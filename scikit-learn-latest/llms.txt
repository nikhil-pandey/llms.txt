about.md

---

# About us

## History

This project was started in 2007 as a Google Summer of Code project by David Cournapeau. Later that year, Matthieu Brucher started working on this project as part of his thesis.

In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent Michel of INRIA took leadership of the project and made the first public release, February the 1st 2010. Since then, several releases have appeared following an approximately 3-month cycle, and a thriving international community has been leading the development. As a result, INRIA holds the copyright over the work done by people who were employed by INRIA at the time of the contribution.

## Governance

The decision making process and governance structure of scikit-learn, like roles and responsibilities, is laid out in the \[governance document \<governance\>\](\#governance-document-\<governance\>).

## The people behind scikit-learn

Scikit-learn is a community project, developed by a large group of people, all across the world. A few core contributor teams, listed below, have central roles, however a more complete list of contributors can be found [on github](https://github.com/scikit-learn/scikit-learn/graphs/contributors).

### Active Core Contributors

#### Maintainers Team

The following people are currently maintainers, in charge of consolidating scikit-learn's development and maintenance:

\> **Note** \> Please do not email the authors directly to ask for assistance or report issues. Instead, please see [What's the best way to ask questions about scikit-learn](https://scikit-learn.org/stable/faq.html#what-s-the-best-way-to-get-help-on-scikit-learn-usage) in the FAQ.

<div class="seealso">

How you can \[contribute to the project \<contributing\>\](\#contribute-to-the-project-\<contributing\>).

</div>

#### Documentation Team

The following people help with documenting the project:

#### Contributor Experience Team

The following people are active contributors who also help with \[triaging issues \<bug\_triaging\>\](\#triaging-issues-\<bug\_triaging\>), PRs, and general maintenance:

#### Communication Team

The following people help with \[communication around scikit-learn \<communication\_team\>\](\#communication-around-scikit-learn \<communication\_team\>).

### Emeritus Core Contributors

#### Emeritus Maintainers Team

The following people have been active contributors in the past, but are no longer active in the project:

  - Mathieu Blondel
  - Joris Van den Bossche
  - Matthieu Brucher
  - Lars Buitinck
  - David Cournapeau
  - Noel Dawe
  - Vincent Dubourg
  - Edouard Duchesnay
  - Alexander Fabisch
  - Virgile Fritsch
  - Satrajit Ghosh
  - Angel Soler Gollonet
  - Chris Gorgolewski
  - Jaques Grobler
  - Yaroslav Halchenko
  - Brian Holt
  - Arnaud Joly
  - Thouis (Ray) Jones
  - Kyle Kastner
  - manoj kumar
  - Robert Layton
  - Wei Li
  - Paolo Losi
  - Gilles Louppe
  - Jan Hendrik Metzen
  - Vincent Michel
  - Jarrod Millman
  - Vlad Niculae
  - Alexandre Passos
  - Fabian Pedregosa
  - Peter Prettenhofer
  - Hanmin Qin
  - (Venkat) Raghav, Rajagopalan
  - Jacob Schreiber
  - 杜世橋 Du Shiqiao
  - Bertrand Thirion
  - Tom Dupré la Tour
  - Jake Vanderplas
  - Nelle Varoquaux
  - David Warde-Farley
  - Ron Weiss
  - Roman Yurchak

#### Emeritus Communication Team

The following people have been active in the communication team in the past, but no longer have communication responsibilities:

  - Reshama Shaikh

#### Emeritus Contributor Experience Team

The following people have been active in the contributor experience team in the past:

  - Chiara Marmo

## Citing scikit-learn

If you use scikit-learn in a scientific publication, we would appreciate citations to the following paper:

[Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.

Bibtex entry:

    @article{scikit-learn,
      title={Scikit-learn: Machine Learning in {P}ython},
      author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
              and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
              and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
              Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
      journal={Journal of Machine Learning Research},
      volume={12},
      pages={2825--2830},
      year={2011}
    }

If you want to cite scikit-learn for its API or design, you may also want to consider the following paper:

`API design for machine learning software: experiences from the scikit-learn
project <1309.0238>`, Buitinck *et al.*, 2013.

Bibtex entry:

    @inproceedings{sklearn_api,
      author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
                    Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
                    Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
                    and Jaques Grobler and Robert Layton and Jake VanderPlas and
                    Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
      title     = {{API} design for machine learning software: experiences from the scikit-learn
                    project},
      booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
      year      = {2013},
      pages = {108--122},
    }

## Artwork

High quality PNG and SVG logos are available in the [doc/logos/](https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos) source directory.

![image](images/scikit-learn-logo-notext.png)

## Funding

Scikit-learn is a community driven project, however institutional and private grants help to assure its sustainability.

The project would like to thank the following funders.

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[:probabl.](https://probabl.ai) employs Adrin Jalali, Arturo Amor, François Goupil, Guillaume Lemaitre, Jérémie du Boisberranger, Loïc Estève, Olivier Grisel, and Stefanie Senger.

</div>

<div class="div">

image-box

[![image](images/probabl.png)](https://probabl.ai)

</div>

</div>

-----

<style>
  table.image-subtable tr {
    border-color: transparent;
  }

  table.image-subtable td {
    width: 50%;
    vertical-align: middle;
    text-align: center;
  }

  table.image-subtable td img {
    max-height: 40px !important;
    max-width: 90% !important;
  }
</style>

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

The [Members](https://scikit-learn.fondation-inria.fr/en/home/#sponsors) of the [Scikit-learn Consortium at Inria Foundation](https://scikit-learn.fondation-inria.fr/en/home/) help at maintaining and improving the project through their financial support.

</div>

<div class="div">

image-box

<table style="width:32%;">
<colgroup>
<col style="width: 15%" />
<col style="width: 16%" />
</colgroup>
<tbody>
<tr class="odd">
<td><blockquote>
<p>|cha</p>
</blockquote></td>
<td>nel|</td>
</tr>
<tr class="even">
<td><blockquote>
<p><a href="https://www.axa.fr/"><img src="images/axa.png" alt="axa" /></a></p>
</blockquote></td>
<td><blockquote>
<p><a href="https://www.bnpparibascardif.com/"><img src="images/bnp.png" alt="bnp" /></a></p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>|nvi</p>
</blockquote></td>
<td>dia|</td>
</tr>
<tr class="even">
<td><blockquote>
<p>|dat</p>
</blockquote></td>
<td>aiku|</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>|in</p>
</blockquote></td>
<td>ria|</td>
</tr>
</tbody>
</table>

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[NVidia](https://nvidia.com) funds Tim Head since 2022 and is part of the scikit-learn consortium at Inria.

</div>

<div class="div">

image-box

[![image](images/nvidia.png)](https://nvidia.com)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Microsoft](https://microsoft.com/) funds Andreas Müller since 2020.

</div>

<div class="div">

image-box

[![image](images/microsoft.png)](https://microsoft.com)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Quansight Labs](https://labs.quansight.org) funds Lucy Liu since 2022.

</div>

<div class="div">

image-box

[![image](images/quansight-labs.png)](https://labs.quansight.org)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[The Chan-Zuckerberg Initiative](https://chanzuckerberg.com/) and [Wellcome Trust](https://wellcome.org/) fund scikit-learn through the [Essential Open Source Software for Science (EOSS)](https://chanzuckerberg.com/eoss/) cycle 6.

It supports Lucy Liu and diversity & inclusion initiatives that will be announced in the future.

</div>

<div class="div">

image-box

<table style="width:39%;">
<colgroup>
<col style="width: 15%" />
<col style="width: 23%" />
</colgroup>
<tbody>
<tr class="odd">
<td><blockquote>
<p><a href="https://chanzuckerberg.com"><img src="images/czi.png" alt="czi" /></a></p>
</blockquote></td>
<td><blockquote>
<p><a href="https://wellcome.org/"><img src="images/wellcome-trust.png" alt="wellcome" /></a></p>
</blockquote></td>
</tr>
</tbody>
</table>

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Tidelift](https://tidelift.com/) supports the project via their service agreement.

</div>

<div class="div">

image-box

[![image](images/Tidelift-logo-on-light.svg)](https://tidelift.com/)

</div>

</div>

-----

### Past Sponsors

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Quansight Labs](https://labs.quansight.org) funded Meekail Zain in 2022 and 2023, and funded Thomas J. Fan from 2021 to 2023.

</div>

<div class="div">

image-box

[![image](images/quansight-labs.png)](https://labs.quansight.org)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Columbia University](https://columbia.edu/) funded Andreas Müller (2016-2020).

</div>

<div class="div">

image-box

[![image](images/columbia.png)](https://columbia.edu)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[The University of Sydney](https://sydney.edu.au/) funded Joel Nothman (2017-2021).

</div>

<div class="div">

image-box

[![image](images/sydney-primary.jpeg)](https://sydney.edu.au/)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

Andreas Müller received a grant to improve scikit-learn from the [Alfred P. Sloan Foundation](https://sloan.org) . This grant supported the position of Nicolas Hug and Thomas J. Fan.

</div>

<div class="div">

image-box

[![image](images/sloan_banner.png)](https://sloan.org/)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[INRIA](https://www.inria.fr) actively supports this project. It has provided funding for Fabian Pedregosa (2010-2012), Jaques Grobler (2012-2013) and Olivier Grisel (2013-2017) to work on this project full-time. It also hosts coding sprints and other events.

</div>

<div class="div">

image-box

[![image](images/inria-logo.jpg)](https://www.inria.fr)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Paris-Saclay Center for Data Science](http://www.datascience-paris-saclay.fr/) funded one year for a developer to work on the project full-time (2014-2015), 50% of the time of Guillaume Lemaitre (2016-2017) and 50% of the time of Joris van den Bossche (2017-2018).

</div>

<div class="div">

image-box

[![image](images/cds-logo.png)](http://www.datascience-paris-saclay.fr/)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[NYU Moore-Sloan Data Science Environment](https://cds.nyu.edu/mooresloan/) funded Andreas Mueller (2014-2016) to work on this project. The Moore-Sloan Data Science Environment also funds several students to work on the project part-time.

</div>

<div class="div">

image-box

[![image](images/nyu_short_color.png)](https://cds.nyu.edu/mooresloan/)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[Télécom Paristech](https://www.telecom-paristech.fr/) funded Manoj Kumar (2014), Tom Dupré la Tour (2015), Raghav RV (2015-2017), Thierry Guillemot (2016-2017) and Albert Thomas (2017) to work on scikit-learn.

</div>

<div class="div">

image-box

[![image](images/telecom.png)](https://www.telecom-paristech.fr/)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[The Labex DigiCosme](https://digicosme.lri.fr) funded Nicolas Goix (2015-2016), Tom Dupré la Tour (2015-2016 and 2017-2018), Mathurin Massias (2018-2019) to work part time on scikit-learn during their PhDs. It also funded a scikit-learn coding sprint in 2015.

</div>

<div class="div">

image-box

[![image](images/digicosme.png)](https://digicosme.lri.fr)

</div>

</div>

-----

<div class="div">

sk-text-image-grid-small

<div class="div">

text-box

[The Chan-Zuckerberg Initiative](https://chanzuckerberg.com/) funded Nicolas Hug to work full-time on scikit-learn in 2020.

</div>

<div class="div">

image-box

[![image](images/czi.png)](https://chanzuckerberg.com)

</div>

</div>

-----

The following students were sponsored by [Google](https://opensource.google/) to work on scikit-learn through the [Google Summer of Code](https://en.wikipedia.org/wiki/Google_Summer_of_Code) program.

  - 2007 - David Cournapeau
  - 2011 - [Vlad Niculae](https://vene.ro/)
  - 2012 - [Vlad Niculae](https://vene.ro/), Immanuel Bayer
  - 2013 - Kemal Eren, Nicolas Trésegnie
  - 2014 - Hamzeh Alsalhi, Issam Laradji, Maheshakya Wijewardena, Manoj Kumar
  - 2015 - [Raghav RV](https://github.com/raghavrv), Wei Xue
  - 2016 - [Nelson Liu](http://nelsonliu.me), [YenChen Lin](https://yenchenlin.me/)

-----

The [NeuroDebian](http://neuro.debian.net) project providing [Debian](https://www.debian.org/) packaging and contributions is supported by [Dr. James V. Haxby](http://haxbylab.dartmouth.edu/) ([Dartmouth College](https://pbs.dartmouth.edu/)).

-----

The following organizations funded the scikit-learn consortium at Inria in the past:

<style>
  div.image-subgrid img {
    max-height: 50px;
    max-width: 90%;
  }
</style>

<div class="grid" data-class-row="image-subgrid" data-gutter="1">

2 2 4 4

<div class="grid-item sd-text-center" data-child-align="center">

[![msn](images/microsoft.png)](https://www.microsoft.com/)

</div>

<div class="grid-item sd-text-center" data-child-align="center">

[![bcg](images/bcg.png)](https://www.bcg.com/beyond-consulting/bcg-gamma/default.aspx)

</div>

<div class="grid-item sd-text-center" data-child-align="center">

[![fujitsu](images/fujitsu.png)](https://www.fujitsu.com/global/)

</div>

<div class="grid-item sd-text-center" data-child-align="center">

[![aphp](images/logo_APHP_text.png)](https://aphp.fr/)

</div>

<div class="grid-item sd-text-center" data-child-align="center">

[![hf](images/huggingface_logo-noborder.png)](https://huggingface.co)

</div>

</div>

## Coding Sprints

The scikit-learn project has a long history of [open source coding sprints](https://blog.scikit-learn.org/events/sprints-value/) with over 50 sprint events from 2010 to present day. There are scores of sponsors who contributed to costs which include venue, food, travel, developer time and more. See [scikit-learn sprints](https://blog.scikit-learn.org/sprints/) for a full list of events.

## Donating to the project

If you are interested in donating to the project or to one of our code-sprints, please donate via the [NumFOCUS Donations Page](https://numfocus.org/donate-to-scikit-learn).

<p class="text-center">
  <a class="btn sk-btn-orange mb-1" href="https://numfocus.org/donate-to-scikit-learn">
    Help us, <strong>donate!</strong>
  </a>
</p>

All donations will be handled by [NumFOCUS](https://numfocus.org/), a non-profit organization which is managed by a board of [Scipy community members](https://numfocus.org/board.html). NumFOCUS's mission is to foster scientific computing software, in particular in Python. As a fiscal home of scikit-learn, it ensures that money is available when needed to keep the project funded and available while in compliance with tax regulations.

The received donations for the scikit-learn project mostly will go towards covering travel-expenses for code sprints, as well as towards the organization budget of the project\[1\].

**Notes**

## Infrastructure support

We would also like to thank [Microsoft Azure](https://azure.microsoft.com/en-us/), [Cirrus Cl](https://cirrus-ci.org), [CircleCl](https://circleci.com/) for free CPU time on their Continuous Integration servers, and [Anaconda Inc.](https://www.anaconda.com) for the storage they provide for our staging and nightly builds.

1.  Regarding the organization budget, in particular, we might use some of the donated funds to pay for other project expenses such as DNS, hosting or continuous integration services.

---

common_pitfalls.md

---

# Common pitfalls and recommended practices

The purpose of this chapter is to illustrate some common pitfalls and anti-patterns that occur when using scikit-learn. It provides examples of what **not** to do, along with a corresponding correct example.

## Inconsistent preprocessing

scikit-learn provides a library of \[data-transforms\](\#data-transforms), which may clean (see \[preprocessing\](\#preprocessing)), reduce (see \[data\_reduction\](\#data\_reduction)), expand (see \[kernel\_approximation\](\#kernel\_approximation)) or generate (see \[feature\_extraction\](\#feature\_extraction)) feature representations. If these data transforms are used when training a model, they also must be used on subsequent datasets, whether it's test data or data in a production system. Otherwise, the feature space will change, and the model will not be able to perform effectively.

For the following example, let's create a synthetic dataset with a single feature:

    >>> from sklearn.datasets import make_regression
    >>> from sklearn.model_selection import train_test_split
    
    >>> random_state = 42
    >>> X, y = make_regression(random_state=random_state, n_features=1, noise=1)
    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, test_size=0.4, random_state=random_state)

**Wrong**

The train dataset is scaled, but not the test dataset, so model performance on the test dataset is worse than expected:

    >>> from sklearn.metrics import mean_squared_error
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.preprocessing import StandardScaler
    
    >>> scaler = StandardScaler()
    >>> X_train_transformed = scaler.fit_transform(X_train)
    >>> model = LinearRegression().fit(X_train_transformed, y_train)
    >>> mean_squared_error(y_test, model.predict(X_test))
    62.80...

**Right**

Instead of passing the non-transformed <span class="title-ref">X\_test</span> to <span class="title-ref">predict</span>, we should transform the test data, the same way we transformed the training data:

    >>> X_test_transformed = scaler.transform(X_test)
    >>> mean_squared_error(y_test, model.predict(X_test_transformed))
    0.90...

Alternatively, we recommend using a <span class="title-ref">Pipeline \<sklearn.pipeline.Pipeline\></span>, which makes it easier to chain transformations with estimators, and reduces the possibility of forgetting a transformation:

    >>> from sklearn.pipeline import make_pipeline
    
    >>> model = make_pipeline(StandardScaler(), LinearRegression())
    >>> model.fit(X_train, y_train)
    Pipeline(steps=[('standardscaler', StandardScaler()),
                    ('linearregression', LinearRegression())])
    >>> mean_squared_error(y_test, model.predict(X_test))
    0.90...

Pipelines also help avoiding another common pitfall: leaking the test data into the training data.

## Data leakage

Data leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from \[cross-validation \<cross\_validation\>\](\#cross-validation \<cross\_validation\>), and thus poorer performance when the model is used on actually novel data, for example during production.

A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. **The general rule is to never call** <span class="title-ref">fit</span> **on the test data**. While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps.

Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, **not** the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model.

### How to avoid data leakage

Below are some tips on avoiding data leakage:

  - Always split the data into train and test subsets first, particularly before any preprocessing steps.

  - Never include test data when using the <span class="title-ref">fit</span> and <span class="title-ref">fit\_transform</span> methods. Using all the data, e.g., <span class="title-ref">fit(X)</span>, can result in overly optimistic scores.
    
    Conversely, the <span class="title-ref">transform</span> method should be used on both train and test subsets as the same preprocessing should be applied to all the data. This can be achieved by using <span class="title-ref">fit\_transform</span> on the train subset and <span class="title-ref">transform</span> on the test subset.

  - The scikit-learn \[pipeline \<pipeline\>\](\#pipeline-\<pipeline\>) is a great way to prevent data leakage as it ensures that the appropriate method is performed on the correct data subset. The pipeline is ideal for use in cross-validation and hyper-parameter tuning functions.

An example of data leakage during preprocessing is detailed below.

### Data leakage during pre-processing

<div class="note">

<div class="title">

Note

</div>

We here choose to illustrate data leakage with a feature selection step. This risk of leakage is however relevant with almost all transformations in scikit-learn, including (but not limited to) <span class="title-ref">\~sklearn.preprocessing.StandardScaler</span>, <span class="title-ref">\~sklearn.impute.SimpleImputer</span>, and <span class="title-ref">\~sklearn.decomposition.PCA</span>.

</div>

A number of \[feature\_selection\](\#feature\_selection) functions are available in scikit-learn. They can help remove irrelevant, redundant and noisy features as well as improve your model build time and performance. As with any other type of preprocessing, feature selection should **only** use the training data. Including the test data in feature selection will optimistically bias your model.

To demonstrate we will create this binary classification problem with 10,000 randomly generated features:

    >>> import numpy as np
    >>> n_samples, n_features, n_classes = 200, 10000, 2
    >>> rng = np.random.RandomState(42)
    >>> X = rng.standard_normal((n_samples, n_features))
    >>> y = rng.choice(n_classes, n_samples)

**Wrong**

Using all the data to perform feature selection results in an accuracy score much higher than chance, even though our targets are completely random. This randomness means that our <span class="title-ref">X</span> and <span class="title-ref">y</span> are independent and we thus expect the accuracy to be around 0.5. However, since the feature selection step 'sees' the test data, the model has an unfair advantage. In the incorrect example below we first use all the data for feature selection and then split the data into training and test subsets for model fitting. The result is a much higher than expected accuracy score:

    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.ensemble import GradientBoostingClassifier
    >>> from sklearn.metrics import accuracy_score
    
    >>> # Incorrect preprocessing: the entire data is transformed
    >>> X_selected = SelectKBest(k=25).fit_transform(X, y)
    
    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X_selected, y, random_state=42)
    >>> gbc = GradientBoostingClassifier(random_state=1)
    >>> gbc.fit(X_train, y_train)
    GradientBoostingClassifier(random_state=1)
    
    >>> y_pred = gbc.predict(X_test)
    >>> accuracy_score(y_test, y_pred)
    0.76

**Right**

To prevent data leakage, it is good practice to split your data into train and test subsets **first**. Feature selection can then be formed using just the train dataset. Notice that whenever we use <span class="title-ref">fit</span> or <span class="title-ref">fit\_transform</span>, we only use the train dataset. The score is now what we would expect for the data, close to chance:

    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, random_state=42)
    >>> select = SelectKBest(k=25)
    >>> X_train_selected = select.fit_transform(X_train, y_train)
    
    >>> gbc = GradientBoostingClassifier(random_state=1)
    >>> gbc.fit(X_train_selected, y_train)
    GradientBoostingClassifier(random_state=1)
    
    >>> X_test_selected = select.transform(X_test)
    >>> y_pred = gbc.predict(X_test_selected)
    >>> accuracy_score(y_test, y_pred)
    0.46

Here again, we recommend using a <span class="title-ref">\~sklearn.pipeline.Pipeline</span> to chain together the feature selection and model estimators. The pipeline ensures that only the training data is used when performing <span class="title-ref">fit</span> and the test data is used only for calculating the accuracy score:

    >>> from sklearn.pipeline import make_pipeline
    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, random_state=42)
    >>> pipeline = make_pipeline(SelectKBest(k=25),
    ...                          GradientBoostingClassifier(random_state=1))
    >>> pipeline.fit(X_train, y_train)
    Pipeline(steps=[('selectkbest', SelectKBest(k=25)),
                    ('gradientboostingclassifier',
                    GradientBoostingClassifier(random_state=1))])
    
    >>> y_pred = pipeline.predict(X_test)
    >>> accuracy_score(y_test, y_pred)
    0.46

The pipeline can also be fed into a cross-validation function such as <span class="title-ref">\~sklearn.model\_selection.cross\_val\_score</span>. Again, the pipeline ensures that the correct data subset and estimator method is used during fitting and predicting:

    >>> from sklearn.model_selection import cross_val_score
    >>> scores = cross_val_score(pipeline, X, y)
    >>> print(f"Mean accuracy: {scores.mean():.2f}+/-{scores.std():.2f}")
    Mean accuracy: 0.46+/-0.07

## Controlling randomness

Some scikit-learn objects are inherently random. These are usually estimators (e.g. <span class="title-ref">\~sklearn.ensemble.RandomForestClassifier</span>) and cross-validation splitters (e.g. <span class="title-ref">\~sklearn.model\_selection.KFold</span>). The randomness of these objects is controlled via their <span class="title-ref">random\_state</span> parameter, as described in the `Glossary <random_state>`. This section expands on the glossary entry, and describes good practices and common pitfalls w.r.t. this subtle parameter.

<div class="note">

<div class="title">

Note

</div>

Recommendation summary

For an optimal robustness of cross-validation (CV) results, pass <span class="title-ref">RandomState</span> instances when creating estimators, or leave <span class="title-ref">random\_state</span> to <span class="title-ref">None</span>. Passing integers to CV splitters is usually the safest option and is preferable; passing <span class="title-ref">RandomState</span> instances to splitters may sometimes be useful to achieve very specific use-cases. For both estimators and splitters, passing an integer vs passing an instance (or <span class="title-ref">None</span>) leads to subtle but significant differences, especially for CV procedures. These differences are important to understand when reporting results.

For reproducible results across executions, remove any use of <span class="title-ref">random\_state=None</span>.

</div>

### Using <span class="title-ref">None</span> or <span class="title-ref">RandomState</span> instances, and repeated calls to <span class="title-ref">fit</span> and <span class="title-ref">split</span>

The <span class="title-ref">random\_state</span> parameter determines whether multiple calls to `fit` (for estimators) or to `split` (for CV splitters) will produce the same results, according to these rules:

  - If an integer is passed, calling <span class="title-ref">fit</span> or <span class="title-ref">split</span> multiple times always yields the same results.
  - If <span class="title-ref">None</span> or a <span class="title-ref">RandomState</span> instance is passed: <span class="title-ref">fit</span> and <span class="title-ref">split</span> will yield different results each time they are called, and the succession of calls explores all sources of entropy. <span class="title-ref">None</span> is the default value for all <span class="title-ref">random\_state</span> parameters.

We here illustrate these rules for both estimators and CV splitters.

<div class="note">

<div class="title">

Note

</div>

Since passing <span class="title-ref">random\_state=None</span> is equivalent to passing the global <span class="title-ref">RandomState</span> instance from <span class="title-ref">numpy</span> (<span class="title-ref">random\_state=np.random.mtrand.\_rand</span>), we will not explicitly mention <span class="title-ref">None</span> here. Everything that applies to instances also applies to using <span class="title-ref">None</span>.

</div>

#### Estimators

Passing instances means that calling <span class="title-ref">fit</span> multiple times will not yield the same results, even if the estimator is fitted on the same data and with the same hyper-parameters:

    >>> from sklearn.linear_model import SGDClassifier
    >>> from sklearn.datasets import make_classification
    >>> import numpy as np
    
    >>> rng = np.random.RandomState(0)
    >>> X, y = make_classification(n_features=5, random_state=rng)
    >>> sgd = SGDClassifier(random_state=rng)
    
    >>> sgd.fit(X, y).coef_
    array([[ 8.85418642,  4.79084103, -3.13077794,  8.11915045, -0.56479934]])
    
    >>> sgd.fit(X, y).coef_
    array([[ 6.70814003,  5.25291366, -7.55212743,  5.18197458,  1.37845099]])

We can see from the snippet above that repeatedly calling <span class="title-ref">sgd.fit</span> has produced different models, even if the data was the same. This is because the Random Number Generator (RNG) of the estimator is consumed (i.e. mutated) when <span class="title-ref">fit</span> is called, and this mutated RNG will be used in the subsequent calls to <span class="title-ref">fit</span>. In addition, the <span class="title-ref">rng</span> object is shared across all objects that use it, and as a consequence, these objects become somewhat inter-dependent. For example, two estimators that share the same <span class="title-ref">RandomState</span> instance will influence each other, as we will see later when we discuss cloning. This point is important to keep in mind when debugging.

If we had passed an integer to the <span class="title-ref">random\_state</span> parameter of the <span class="title-ref">\~sklearn.linear\_model.SGDClassifier</span>, we would have obtained the same models, and thus the same scores each time. When we pass an integer, the same RNG is used across all calls to <span class="title-ref">fit</span>. What internally happens is that even though the RNG is consumed when <span class="title-ref">fit</span> is called, it is always reset to its original state at the beginning of <span class="title-ref">fit</span>.

#### CV splitters

Randomized CV splitters have a similar behavior when a <span class="title-ref">RandomState</span> instance is passed; calling <span class="title-ref">split</span> multiple times yields different data splits:

    >>> from sklearn.model_selection import KFold
    >>> import numpy as np
    
    >>> X = y = np.arange(10)
    >>> rng = np.random.RandomState(0)
    >>> cv = KFold(n_splits=2, shuffle=True, random_state=rng)
    
    >>> for train, test in cv.split(X, y):
    ...     print(train, test)
    [0 3 5 6 7] [1 2 4 8 9]
    [1 2 4 8 9] [0 3 5 6 7]
    
    >>> for train, test in cv.split(X, y):
    ...     print(train, test)
    [0 4 6 7 8] [1 2 3 5 9]
    [1 2 3 5 9] [0 4 6 7 8]

We can see that the splits are different from the second time <span class="title-ref">split</span> is called. This may lead to unexpected results if you compare the performance of multiple estimators by calling <span class="title-ref">split</span> many times, as we will see in the next section.

### Common pitfalls and subtleties

While the rules that govern the <span class="title-ref">random\_state</span> parameter are seemingly simple, they do however have some subtle implications. In some cases, this can even lead to wrong conclusions.

#### Estimators

**Different \`random\_state\` types lead to different cross-validation procedures**

Depending on the type of the <span class="title-ref">random\_state</span> parameter, estimators will behave differently, especially in cross-validation procedures. Consider the following snippet:

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.datasets import make_classification
    >>> from sklearn.model_selection import cross_val_score
    >>> import numpy as np
    
    >>> X, y = make_classification(random_state=0)
    
    >>> rf_123 = RandomForestClassifier(random_state=123)
    >>> cross_val_score(rf_123, X, y)
    array([0.85, 0.95, 0.95, 0.9 , 0.9 ])
    
    >>> rf_inst = RandomForestClassifier(random_state=np.random.RandomState(0))
    >>> cross_val_score(rf_inst, X, y)
    array([0.9 , 0.95, 0.95, 0.9 , 0.9 ])

We see that the cross-validated scores of <span class="title-ref">rf\_123</span> and <span class="title-ref">rf\_inst</span> are different, as should be expected since we didn't pass the same <span class="title-ref">random\_state</span> parameter. However, the difference between these scores is more subtle than it looks, and **the cross-validation procedures that were performed by** <span class="title-ref">\~sklearn.model\_selection.cross\_val\_score</span> **significantly differ in each case**:

  - Since <span class="title-ref">rf\_123</span> was passed an integer, every call to <span class="title-ref">fit</span> uses the same RNG: this means that all random characteristics of the random forest estimator will be the same for each of the 5 folds of the CV procedure. In particular, the (randomly chosen) subset of features of the estimator will be the same across all folds.
  - Since <span class="title-ref">rf\_inst</span> was passed a <span class="title-ref">RandomState</span> instance, each call to <span class="title-ref">fit</span> starts from a different RNG. As a result, the random subset of features will be different for each folds.

While having a constant estimator RNG across folds isn't inherently wrong, we usually want CV results that are robust w.r.t. the estimator's randomness. As a result, passing an instance instead of an integer may be preferable, since it will allow the estimator RNG to vary for each fold.

<div class="note">

<div class="title">

Note

</div>

Here, <span class="title-ref">\~sklearn.model\_selection.cross\_val\_score</span> will use a non-randomized CV splitter (as is the default), so both estimators will be evaluated on the same splits. This section is not about variability in the splits. Also, whether we pass an integer or an instance to <span class="title-ref">\~sklearn.datasets.make\_classification</span> isn't relevant for our illustration purpose: what matters is what we pass to the <span class="title-ref">\~sklearn.ensemble.RandomForestClassifier</span> estimator.

</div>

<div class="dropdown">

Cloning

Another subtle side effect of passing <span class="title-ref">RandomState</span> instances is how <span class="title-ref">\~sklearn.base.clone</span> will work:

    >>> from sklearn import clone
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> import numpy as np
    
    >>> rng = np.random.RandomState(0)
    >>> a = RandomForestClassifier(random_state=rng)
    >>> b = clone(a)

Since a <span class="title-ref">RandomState</span> instance was passed to <span class="title-ref">a</span>, <span class="title-ref">a</span> and <span class="title-ref">b</span> are not clones in the strict sense, but rather clones in the statistical sense: <span class="title-ref">a</span> and <span class="title-ref">b</span> will still be different models, even when calling <span class="title-ref">fit(X, y)</span> on the same data. Moreover, <span class="title-ref">a</span> and <span class="title-ref">b</span> will influence each-other since they share the same internal RNG: calling <span class="title-ref">a.fit</span> will consume <span class="title-ref">b</span>'s RNG, and calling <span class="title-ref">b.fit</span> will consume <span class="title-ref">a</span>'s RNG, since they are the same. This bit is true for any estimators that share a <span class="title-ref">random\_state</span> parameter; it is not specific to clones.

If an integer were passed, <span class="title-ref">a</span> and <span class="title-ref">b</span> would be exact clones and they would not influence each other.

<div class="warning">

<div class="title">

Warning

</div>

Even though <span class="title-ref">\~sklearn.base.clone</span> is rarely used in user code, it is called pervasively throughout scikit-learn codebase: in particular, most meta-estimators that accept non-fitted estimators call <span class="title-ref">\~sklearn.base.clone</span> internally (<span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span>, <span class="title-ref">\~sklearn.ensemble.StackingClassifier</span>, <span class="title-ref">\~sklearn.calibration.CalibratedClassifierCV</span>, etc.).

</div>

</div>

#### CV splitters

When passed a <span class="title-ref">RandomState</span> instance, CV splitters yield different splits each time <span class="title-ref">split</span> is called. When comparing different estimators, this can lead to overestimating the variance of the difference in performance between the estimators:

    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    >>> from sklearn.datasets import make_classification
    >>> from sklearn.model_selection import KFold
    >>> from sklearn.model_selection import cross_val_score
    >>> import numpy as np
    
    >>> rng = np.random.RandomState(0)
    >>> X, y = make_classification(random_state=rng)
    >>> cv = KFold(shuffle=True, random_state=rng)
    >>> lda = LinearDiscriminantAnalysis()
    >>> nb = GaussianNB()
    
    >>> for est in (lda, nb):
    ...     print(cross_val_score(est, X, y, cv=cv))
    [0.8  0.75 0.75 0.7  0.85]
    [0.85 0.95 0.95 0.85 0.95]

Directly comparing the performance of the <span class="title-ref">\~sklearn.discriminant\_analysis.LinearDiscriminantAnalysis</span> estimator vs the <span class="title-ref">\~sklearn.naive\_bayes.GaussianNB</span> estimator **on each fold** would be a mistake: **the splits on which the estimators are evaluated are different**. Indeed, <span class="title-ref">\~sklearn.model\_selection.cross\_val\_score</span> will internally call <span class="title-ref">cv.split</span> on the same <span class="title-ref">\~sklearn.model\_selection.KFold</span> instance, but the splits will be different each time. This is also true for any tool that performs model selection via cross-validation, e.g. <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> and \`\~sklearn.model\_selection.RandomizedSearchCV\`: scores are not comparable fold-to-fold across different calls to <span class="title-ref">search.fit</span>, since <span class="title-ref">cv.split</span> would have been called multiple times. Within a single call to <span class="title-ref">search.fit</span>, however, fold-to-fold comparison is possible since the search estimator only calls <span class="title-ref">cv.split</span> once.

For comparable fold-to-fold results in all scenarios, one should pass an integer to the CV splitter: <span class="title-ref">cv = KFold(shuffle=True, random\_state=0)</span>.

<div class="note">

<div class="title">

Note

</div>

While fold-to-fold comparison is not advisable with <span class="title-ref">RandomState</span> instances, one can however expect that average scores allow to conclude whether one estimator is better than another, as long as enough folds and data are used.

</div>

<div class="note">

<div class="title">

Note

</div>

What matters in this example is what was passed to <span class="title-ref">\~sklearn.model\_selection.KFold</span>. Whether we pass a <span class="title-ref">RandomState</span> instance or an integer to <span class="title-ref">\~sklearn.datasets.make\_classification</span> is not relevant for our illustration purpose. Also, neither <span class="title-ref">\~sklearn.discriminant\_analysis.LinearDiscriminantAnalysis</span> nor <span class="title-ref">\~sklearn.naive\_bayes.GaussianNB</span> are randomized estimators.

</div>

### General recommendations

#### Getting reproducible results across multiple executions

In order to obtain reproducible (i.e. constant) results across multiple *program executions*, we need to remove all uses of <span class="title-ref">random\_state=None</span>, which is the default. The recommended way is to declare a <span class="title-ref">rng</span> variable at the top of the program, and pass it down to any object that accepts a <span class="title-ref">random\_state</span> parameter:

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.datasets import make_classification
    >>> from sklearn.model_selection import train_test_split
    >>> import numpy as np
    
    >>> rng = np.random.RandomState(0)
    >>> X, y = make_classification(random_state=rng)
    >>> rf = RandomForestClassifier(random_state=rng)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,
    ...                                                     random_state=rng)
    >>> rf.fit(X_train, y_train).score(X_test, y_test)
    0.84

We are now guaranteed that the result of this script will always be 0.84, no matter how many times we run it. Changing the global <span class="title-ref">rng</span> variable to a different value should affect the results, as expected.

It is also possible to declare the <span class="title-ref">rng</span> variable as an integer. This may however lead to less robust cross-validation results, as we will see in the next section.

<div class="note">

<div class="title">

Note

</div>

We do not recommend setting the global <span class="title-ref">numpy</span> seed by calling <span class="title-ref">np.random.seed(0)</span>. See [here](https://stackoverflow.com/questions/5836335/consistently-create-same-random-numpy-array/5837352#comment6712034_5837352) for a discussion.

</div>

#### Robustness of cross-validation results

When we evaluate a randomized estimator performance by cross-validation, we want to make sure that the estimator can yield accurate predictions for new data, but we also want to make sure that the estimator is robust w.r.t. its random initialization. For example, we would like the random weights initialization of a <span class="title-ref">\~sklearn.linear\_model.SGDClassifier</span> to be consistently good across all folds: otherwise, when we train that estimator on new data, we might get unlucky and the random initialization may lead to bad performance. Similarly, we want a random forest to be robust w.r.t the set of randomly selected features that each tree will be using.

For these reasons, it is preferable to evaluate the cross-validation performance by letting the estimator use a different RNG on each fold. This is done by passing a <span class="title-ref">RandomState</span> instance (or <span class="title-ref">None</span>) to the estimator initialization.

When we pass an integer, the estimator will use the same RNG on each fold: if the estimator performs well (or bad), as evaluated by CV, it might just be because we got lucky (or unlucky) with that specific seed. Passing instances leads to more robust CV results, and makes the comparison between various algorithms fairer. It also helps limiting the temptation to treat the estimator's RNG as a hyper-parameter that can be tuned.

Whether we pass <span class="title-ref">RandomState</span> instances or integers to CV splitters has no impact on robustness, as long as <span class="title-ref">split</span> is only called once. When <span class="title-ref">split</span> is called multiple times, fold-to-fold comparison isn't possible anymore. As a result, passing integer to CV splitters is usually safer and covers most use-cases.

---

communication_team_emeritus.md

---

- Reshama Shaikh

---

computational_performance.md

---

<div id="computational_performance">

<div class="currentmodule">

sklearn

</div>

</div>

# Computational Performance

For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).

We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.

Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile).

Prediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second).

An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.

## Prediction Latency

One of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment.

The main factors that influence the prediction latency are

1.  Number of features
2.  Input data representation and sparsity
3.  Model complexity
4.  Feature extraction

A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.

### Bulk versus Atomic mode

In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:

<div class="centered">

[![atomic\_prediction\_latency](../auto_examples/applications/images/sphx_glr_plot_prediction_latency_001.png)](../auto_examples/applications/plot_prediction_latency.html)

</div>

<div class="centered">

[![bulk\_prediction\_latency](../auto_examples/applications/images/sphx_glr_plot_prediction_latency_002.png)](../auto_examples/applications/plot_prediction_latency.html)

</div>

To benchmark different estimators for your case you can simply change the `n_features` parameter in this example: \[sphx\_glr\_auto\_examples\_applications\_plot\_prediction\_latency.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_prediction\_latency.py). This should give you an estimate of the order of magnitude of the prediction latency.

### Configuring Scikit-learn for reduced validation overhead

Scikit-learn does some validation on data that increases the overhead per call to `predict` and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable `SKLEARN_ASSUME_FINITE` to a non-empty string before importing scikit-learn, or configure it in Python with <span class="title-ref">set\_config</span>. For more control than these global settings, a <span class="title-ref">config\_context</span> allows you to set this configuration within a specified context:

    >>> import sklearn
    >>> with sklearn.config_context(assume_finite=True):
    ...     pass  # do learning/prediction here with reduced validation

Note that this will affect all uses of <span class="title-ref">\~utils.assert\_all\_finite</span> within the context.

### Influence of the Number of Features

Obviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of \(M\) instances with \(N\) features, the space complexity is in \(O(NM)\). From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features:

<div class="centered">

[![influence\_of\_n\_features\_on\_latency](../auto_examples/applications/images/sphx_glr_plot_prediction_latency_003.png)](../auto_examples/applications/plot_prediction_latency.html)

</div>

Overall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator).

### Influence of the Input Data Representation

Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you don't store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse ([CSR or CSC](https://docs.scipy.org/doc/scipy/reference/sparse.html)) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6.

Calculation over a dense representation, however, may leverage highly optimized vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation.

Here is sample code to test the sparsity of your input:

    def sparsity_ratio(X):
        return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])
    print("input sparsity ratio:", sparsity_ratio(X))

As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy's sparse matrix formats [documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html) for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the `CSR` and `CSC` formats work best.

### Influence of the Model Complexity

Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.

For `sklearn.linear_model` (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression...) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.

Here is an example using <span class="title-ref">\~linear\_model.SGDClassifier</span> with the `elasticnet` penalty. The regularization strength is globally controlled by the `alpha` parameter. With a sufficiently high `alpha`, one can then increase the `l1_ratio` parameter of `elasticnet` to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.

<div class="centered">

[![en\_model\_complexity](../auto_examples/applications/images/sphx_glr_plot_model_complexity_influence_001.png)](../auto_examples/applications/plot_model_complexity_influence.html)

</div>

For the `sklearn.svm` family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the `nu` parameter of <span class="title-ref">\~svm.NuSVR</span> was used to influence the number of support vectors.

<div class="centered">

[![nusvr\_model\_complexity](../auto_examples/applications/images/sphx_glr_plot_model_complexity_influence_002.png)](../auto_examples/applications/plot_model_complexity_influence.html)

</div>

For `sklearn.ensemble` of trees (e.g. RandomForest, GBT, ExtraTrees, etc.) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the `n_estimators` parameter of <span class="title-ref">\~ensemble.GradientBoostingRegressor</span>.

<div class="centered">

[![gbt\_model\_complexity](../auto_examples/applications/images/sphx_glr_plot_model_complexity_influence_003.png)](../auto_examples/applications/plot_model_complexity_influence.html)

</div>

In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process.

### Feature Extraction Latency

Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.

<div class="centered">

[![prediction\_time](../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_004.png)](../auto_examples/applications/plot_out_of_core_classification.html)

</div>

In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.

## Prediction Throughput

Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the \[sphx\_glr\_auto\_examples\_applications\_plot\_prediction\_latency.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_prediction\_latency.py) example that measures this quantity for a number of estimators on synthetic data:

<div class="centered">

[![throughput\_benchmark](../auto_examples/applications/images/sphx_glr_plot_prediction_latency_004.png)](../auto_examples/applications/plot_prediction_latency.html)

</div>

These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.

## Tips and Tricks

### Linear algebra libraries

As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) / [LAPACK](https://en.wikipedia.org/wiki/LAPACK) library.

Not all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (`SVC`, `SVR`, `NuSVC`, `NuSVR`). On the other hand a linear model implemented with a BLAS DGEMM call (via `numpy.dot`) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS.

You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following command:

    python -c "import sklearn; sklearn.show_versions()"

Optimized BLAS / LAPACK implementations include:

  - Atlas (need hardware specific tuning by rebuilding on the target machine)
  - OpenBLAS
  - MKL
  - Apple Accelerate and vecLib frameworks (OSX only)

More information can be found on the [NumPy install page](https://numpy.org/install/) and in this [blog post](https://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/) from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.

### Limiting Working Memory

Some calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using <span class="title-ref">set\_config</span> or <span class="title-ref">config\_context</span>. The following suggests to limit temporary working memory to 128 MiB:

    >>> import sklearn
    >>> with sklearn.config_context(working_memory=128):
    ...     pass  # do chunked work here

An example of a chunked operation adhering to this setting is <span class="title-ref">\~metrics.pairwise\_distances\_chunked</span>, which facilitates computing row-wise reductions of a pairwise distance matrix.

### Model Compression

Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.

Here is sample code that illustrates the use of the `sparsify()` method:

    clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)
    clf.fit(X_train, y_train).sparsify()
    clf.predict(X_test)

In this example we prefer the `elasticnet` penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the `l1_ratio` parameter (in combination with the regularization strength `alpha`) to control this tradeoff.

A typical [benchmark](https://github.com/scikit-learn/scikit-learn/blob/main/benchmarks/bench_sparsify.py) on synthetic data yields a \>30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.

### Model Reshaping

Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in `CSR` format), it is generally sufficient to not generate the relevant features, leaving their columns empty.

### Links

  - \[scikit-learn developer performance documentation \<performance-howto\>\](\#scikit-learn-developer-performance-documentation-\<performance-howto\>)
  - [Scipy sparse matrix formats documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html)

---

parallelism.md

---

# Parallelism, resource management, and configuration

## Parallelism

Some scikit-learn estimators and utilities parallelize costly operations using multiple CPU cores.

Depending on the type of estimator and sometimes the values of the constructor parameters, this is either done:

  - with higher-level parallelism via [joblib](https://joblib.readthedocs.io/en/latest/).
  - with lower-level parallelism via OpenMP, used in C or Cython code.
  - with lower-level parallelism via BLAS, used by NumPy and SciPy for generic operations on arrays.

The <span class="title-ref">n\_jobs</span> parameters of estimators always controls the amount of parallelism managed by joblib (processes or threads depending on the joblib backend). The thread-level parallelism managed by OpenMP in scikit-learn's own Cython code or by BLAS & LAPACK libraries used by NumPy and SciPy operations used in scikit-learn is always controlled by environment variables or <span class="title-ref">threadpoolctl</span> as explained below. Note that some estimators can leverage all three kinds of parallelism at different points of their training and prediction methods.

We describe these 3 types of parallelism in the following subsections in more details.

### Higher-level parallelism with joblib

When the underlying implementation uses joblib, the number of workers (threads or processes) that are spawned in parallel can be controlled via the `n_jobs` parameter.

\> **Note** \> Where (and how) parallelization happens in the estimators using joblib by specifying <span class="title-ref">n\_jobs</span> is currently poorly documented. Please help us by improving our docs and tackle [issue 14228](https://github.com/scikit-learn/scikit-learn/issues/14228)\!

Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the **backend** that it's using.

scikit-learn generally relies on the `loky` backend, which is joblib's default backend. Loky is a multi-processing backend. When doing multi-processing, in order to avoid duplicating the memory in each process (which isn't reasonable with big datasets), joblib will create a [memmap](https://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html) that all processes can share, when the data is bigger than 1MB.

In some specific cases (when the code that is run in parallel releases the GIL), scikit-learn will indicate to `joblib` that a multi-threading backend is preferable.

As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager:

    from joblib import parallel_backend
    
    with parallel_backend('threading', n_jobs=2):
        # Your scikit-learn code here

Please refer to the [joblib's docs](https://joblib.readthedocs.io/en/latest/parallel.html#thread-based-parallelism-vs-process-based-parallelism) for more details.

In practice, whether parallelism is helpful at improving runtime depends on many factors. It is usually a good idea to experiment rather than assuming that increasing the number of workers is always a good thing. In some cases it can be highly detrimental to performance to run multiple copies of some estimators or functions in parallel (see oversubscription below).

### Lower-level parallelism with OpenMP

OpenMP is used to parallelize code written in Cython or C, relying on multi-threading exclusively. By default, the implementations using OpenMP will use as many threads as possible, i.e. as many threads as logical cores.

You can control the exact number of threads that are used either:

  - via the `OMP_NUM_THREADS` environment variable, for instance when: running a python script:
    
    <div class="prompt">
    
    bash $
    
    OMP\_NUM\_THREADS=4 python my\_script.py
    
    </div>

  - or via <span class="title-ref">threadpoolctl</span> as explained by [this piece of documentation](https://github.com/joblib/threadpoolctl/#setting-the-maximum-size-of-thread-pools).

### Parallel NumPy and SciPy routines from numerical libraries

scikit-learn relies heavily on NumPy and SciPy, which internally call multi-threaded linear algebra routines (BLAS & LAPACK) implemented in libraries such as MKL, OpenBLAS or BLIS.

You can control the exact number of threads used by BLAS for each library using environment variables, namely:

  - `MKL_NUM_THREADS` sets the number of thread MKL uses,
  - `OPENBLAS_NUM_THREADS` sets the number of threads OpenBLAS uses
  - `BLIS_NUM_THREADS` sets the number of threads BLIS uses

Note that BLAS & LAPACK implementations can also be impacted by <span class="title-ref">OMP\_NUM\_THREADS</span>. To check whether this is the case in your environment, you can inspect how the number of threads effectively used by those libraries is affected when running the following command in a bash or zsh terminal for different values of \`OMP\_NUM\_THREADS\`:

<div class="prompt">

bash $

OMP\_NUM\_THREADS=2 python -m threadpoolctl -i numpy scipy

</div>

<div class="note">

<div class="title">

Note

</div>

At the time of writing (2022), NumPy and SciPy packages which are distributed on pypi.org (i.e. the ones installed via `pip install`) and on the conda-forge channel (i.e. the ones installed via `conda install --channel conda-forge`) are linked with OpenBLAS, while NumPy and SciPy packages packages shipped on the `defaults` conda channel from Anaconda.org (i.e. the ones installed via `conda install`) are linked by default with MKL.

</div>

### Oversubscription: spawning too many threads

It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time.

Suppose you have a machine with 8 CPUs. Consider a case where you're running a <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> (parallelized with joblib) with `n_jobs=8` over a <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingClassifier</span> (parallelized with OpenMP). Each instance of <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingClassifier</span> will spawn 8 threads (since you have 8 CPUs). That's a total of `8 * 8 = 64` threads, which leads to oversubscription of threads for physical CPU resources and thus to scheduling overhead.

Oversubscription can arise in the exact same fashion with parallelized routines from MKL, OpenBLAS or BLIS that are nested in joblib calls.

Starting from `joblib >= 0.14`, when the `loky` backend is used (which is the default), joblib will tell its child **processes** to limit the number of threads they can use, so as to avoid oversubscription. In practice the heuristic that joblib uses is to tell the processes to use `max_threads = n_cpus // n_jobs`, via their corresponding environment variable. Back to our example from above, since the joblib backend of <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> is `loky`, each process will only be able to use 1 thread instead of 8, thus mitigating the oversubscription issue.

Note that:

  - Manually setting one of the environment variables (`OMP_NUM_THREADS`, `MKL_NUM_THREADS`, `OPENBLAS_NUM_THREADS`, or `BLIS_NUM_THREADS`) will take precedence over what joblib tries to do. The total number of threads will be `n_jobs * <LIB>_NUM_THREADS`. Note that setting this limit will also impact your computations in the main process, which will only use `<LIB>_NUM_THREADS`. Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below).
  - When joblib is configured to use the `threading` backend, there is no mechanism to avoid oversubscriptions when calling into parallel native libraries in the joblib-managed threads.
  - All scikit-learn estimators that explicitly rely on OpenMP in their Cython code always use <span class="title-ref">threadpoolctl</span> internally to automatically adapt the numbers of threads used by OpenMP and potentially nested BLAS calls so as to avoid oversubscription.

You will find additional details about joblib mitigation of oversubscription in [joblib documentation](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources).

You will find additional details about parallelism in numerical python libraries in [this document from Thomas J. Fan](https://thomasjpfan.github.io/parallelism-python-libraries-design/).

## Configuration switches

### Python API

<span class="title-ref">sklearn.set\_config</span> and <span class="title-ref">sklearn.config\_context</span> can be used to change parameters of the configuration which control aspect of parallelism.

### Environment variables

These environment variables should be set before importing scikit-learn.

#### <span class="title-ref">SKLEARN\_ASSUME\_FINITE</span>

Sets the default value for the <span class="title-ref">assume\_finite</span> argument of <span class="title-ref">sklearn.set\_config</span>.

#### <span class="title-ref">SKLEARN\_WORKING\_MEMORY</span>

Sets the default value for the <span class="title-ref">working\_memory</span> argument of <span class="title-ref">sklearn.set\_config</span>.

#### <span class="title-ref">SKLEARN\_SEED</span>

Sets the seed of the global random generator when running the tests, for reproducibility.

Note that scikit-learn tests are expected to run deterministically with explicit seeding of their own independent RNG instances instead of relying on the numpy or Python standard library RNG singletons to make sure that test results are independent of the test execution order. However some tests might forget to use explicit seeding and this variable is a way to control the initial state of the aforementioned singletons.

#### <span class="title-ref">SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED</span>

Controls the seeding of the random number generator used in tests that rely on the <span class="title-ref">global\_random\_seed</span>\` fixture.

All tests that use this fixture accept the contract that they should deterministically pass for any seed value from 0 to 99 included.

In nightly CI builds, the <span class="title-ref">SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED</span> environment variable is drawn randomly in the above range and all fixtured tests will run for that specific seed. The goal is to ensure that, over time, our CI will run all tests with different seeds while keeping the test duration of a single run of the full test suite limited. This will check that the assertions of tests written to use this fixture are not dependent on a specific seed value.

The range of admissible seed values is limited to \[0, 99\] because it is often not possible to write a test that can work for any possible seed and we want to avoid having tests that randomly fail on the CI.

Valid values for \`SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED\`:

  - \`SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED="42"\`: run tests with a fixed seed of 42
  - \`SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED="40-42"\`: run the tests with all seeds between 40 and 42 included
  - \`SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED="all"\`: run the tests with all seeds between 0 and 99 included. This can take a long time: only use for individual tests, not the full test suite\!

If the variable is not set, then 42 is used as the global seed in a deterministic manner. This ensures that, by default, the scikit-learn test suite is as deterministic as possible to avoid disrupting our friendly third-party package maintainers. Similarly, this variable should not be set in the CI config of pull-requests to make sure that our friendly contributors are not the first people to encounter a seed-sensitivity regression in a test unrelated to the changes of their own PR. Only the scikit-learn maintainers who watch the results of the nightly builds are expected to be annoyed by this.

When writing a new test function that uses this fixture, please use the following command to make sure that it passes deterministically for all admissible seeds on your local machine:

<div class="prompt">

bash $

SKLEARN\_TESTS\_GLOBAL\_RANDOM\_SEED="all" pytest -v -k test\_your\_test\_name

</div>

#### <span class="title-ref">SKLEARN\_SKIP\_NETWORK\_TESTS</span>

When this environment variable is set to a non zero value, the tests that need network access are skipped. When this environment variable is not set then network tests are skipped.

#### <span class="title-ref">SKLEARN\_RUN\_FLOAT32\_TESTS</span>

When this environment variable is set to '1', the tests using the <span class="title-ref">global\_dtype</span> fixture are also run on float32 data. When this environment variable is not set, the tests are only run on float64 data.

#### <span class="title-ref">SKLEARN\_ENABLE\_DEBUG\_CYTHON\_DIRECTIVES</span>

When this environment variable is set to a non zero value, the <span class="title-ref">Cython</span> derivative, <span class="title-ref">boundscheck</span> is set to <span class="title-ref">True</span>. This is useful for finding segfaults.

#### <span class="title-ref">SKLEARN\_BUILD\_ENABLE\_DEBUG\_SYMBOLS</span>

When this environment variable is set to a non zero value, the debug symbols will be included in the compiled C extensions. Only debug symbols for POSIX systems is configured.

#### <span class="title-ref">SKLEARN\_PAIRWISE\_DIST\_CHUNK\_SIZE</span>

This sets the size of chunk to be used by the underlying <span class="title-ref">PairwiseDistancesReductions</span> implementations. The default value is <span class="title-ref">256</span> which has been showed to be adequate on most machines.

Users looking for the best performance might want to tune this variable using powers of 2 so as to get the best parallelism behavior for their hardware, especially with respect to their caches' sizes.

#### <span class="title-ref">SKLEARN\_WARNINGS\_AS\_ERRORS</span>

This environment variable is used to turn warnings into errors in tests and documentation build.

Some CI (Continuous Integration) builds set <span class="title-ref">SKLEARN\_WARNINGS\_AS\_ERRORS=1</span>, for example to make sure that we catch deprecation warnings from our dependencies and that we adapt our code.

To locally run with the same "warnings as errors" setting as in these CI builds you can set <span class="title-ref">SKLEARN\_WARNINGS\_AS\_ERRORS=1</span>.

By default, warnings are not turned into errors. This is the case if <span class="title-ref">SKLEARN\_WARNINGS\_AS\_ERRORS</span> is unset, or <span class="title-ref">SKLEARN\_WARNINGS\_AS\_ERRORS=0</span>.

This environment variable use specific warning filters to ignore some warnings, since sometimes warnings originate from third-party libraries and there is not much we can do about it. You can see the warning filters in the <span class="title-ref">\_get\_warnings\_filters\_info\_list</span> function in <span class="title-ref">sklearn/utils/\_testing.py</span>.

Note that for documentation build, <span class="title-ref">SKLEARN\_WARNING\_AS\_ERRORS=1</span> is checking that the documentation build, in particular running examples, does not produce any warnings. This is different from the <span class="title-ref">-W</span> <span class="title-ref">sphinx-build</span> argument that catches syntax warnings in the rst files.

---

scaling_strategies.md

---

# Strategies to scale computationally: bigger data

For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.

## Scaling with instances using out-of-core learning

Out-of-core (or "external memory") learning is a technique used to learn from data that cannot fit in a computer's main memory (RAM).

Here is a sketch of a system designed to achieve this goal:

1.  a way to stream instances
2.  a way to extract features from instances
3.  an incremental algorithm

### Streaming instances

Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.

### Extracting features

2\. could be any relevant way to extract features among the different \[feature extraction \<feature\_extraction\>\](\#feature-extraction-\<feature\_extraction\>) methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called \[hashing trick\<feature\_hashing\>\](\#hashing-trick\<feature\_hashing\>) as implemented by <span class="title-ref">sklearn.feature\_extraction.FeatureHasher</span> for datasets with categorical variables represented as list of Python dicts or <span class="title-ref">sklearn.feature\_extraction.text.HashingVectorizer</span> for text documents.

### Incremental learning

Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the `partial_fit` API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called "online learning") is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning\[1\].

Here is a list of incremental estimators for different tasks:

  -   - Classification
        
          - <span class="title-ref">sklearn.naive\_bayes.MultinomialNB</span>
          - <span class="title-ref">sklearn.naive\_bayes.BernoulliNB</span>
          - <span class="title-ref">sklearn.linear\_model.Perceptron</span>
          - <span class="title-ref">sklearn.linear\_model.SGDClassifier</span>
          - <span class="title-ref">sklearn.linear\_model.PassiveAggressiveClassifier</span>
          - <span class="title-ref">sklearn.neural\_network.MLPClassifier</span>

  -   - Regression
        
          - <span class="title-ref">sklearn.linear\_model.SGDRegressor</span>
          - <span class="title-ref">sklearn.linear\_model.PassiveAggressiveRegressor</span>
          - <span class="title-ref">sklearn.neural\_network.MLPRegressor</span>

  -   - Clustering
        
          - <span class="title-ref">sklearn.cluster.MiniBatchKMeans</span>
          - <span class="title-ref">sklearn.cluster.Birch</span>

  -   - Decomposition / feature Extraction
        
          - <span class="title-ref">sklearn.decomposition.MiniBatchDictionaryLearning</span>
          - <span class="title-ref">sklearn.decomposition.IncrementalPCA</span>
          - <span class="title-ref">sklearn.decomposition.LatentDirichletAllocation</span>
          - <span class="title-ref">sklearn.decomposition.MiniBatchNMF</span>

  -   - Preprocessing
        
          - <span class="title-ref">sklearn.preprocessing.StandardScaler</span>
          - <span class="title-ref">sklearn.preprocessing.MinMaxScaler</span>
          - <span class="title-ref">sklearn.preprocessing.MaxAbsScaler</span>

For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first `partial_fit` call using the `classes=` parameter.

Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the `Perceptron` is still sensitive to badly labeled examples even after many examples whereas the `SGD*` and `PassiveAggressive*` families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.

### Examples

Finally, we have a full-fledged example of \[sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py). It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above.

Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.

<div class="centered">

[![accuracy\_over\_time](../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_001.png)](../auto_examples/applications/plot_out_of_core_classification.html)

</div>

Now looking at the computation time of the different parts, we see that the vectorization is much more expensive than learning itself. From the different algorithms, `MultinomialNB` is the most expensive, but its overhead can be mitigated by increasing the size of the mini-batches (exercise: change `minibatch_size` to 100 and 10000 in the program and compare).

<div class="centered">

[![computation\_time](../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_003.png)](../auto_examples/applications/plot_out_of_core_classification.html)

</div>

### Notes

1.  Depending on the algorithm the mini-batch size can influence results or not. SGD\*, PassiveAggressive\*, and discrete NaiveBayes are truly online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint can vary dramatically with batch size.

---

computing.md

---

# Computing with scikit-learn

<div class="toctree" data-maxdepth="2">

computing/scaling\_strategies computing/computational\_performance computing/parallelism

</div>

---

contributor_experience_team_emeritus.md

---

- Chiara Marmo

---

data_transforms.md

---

# Dataset transformations

scikit-learn provides a library of transformers, which may clean (see \[preprocessing\](\#preprocessing)), reduce (see \[data\_reduction\](\#data\_reduction)), expand (see \[kernel\_approximation\](\#kernel\_approximation)) or generate (see \[feature\_extraction\](\#feature\_extraction)) feature representations.

Like other estimators, these are represented by classes with a `fit` method, which learns model parameters (e.g. mean and standard deviation for normalization) from a training set, and a `transform` method which applies this transformation model to unseen data. `fit_transform` may be more convenient and efficient for modelling and transforming the training data simultaneously.

Combining such transformers, either in parallel or series is covered in \[combining\_estimators\](\#combining\_estimators). \[metrics\](\#metrics) covers transforming feature spaces into affinity matrices, while \[preprocessing\_targets\](\#preprocessing\_targets) considers transformations of the target space (e.g. categorical labels) for use in scikit-learn.

<div class="toctree" data-maxdepth="2">

modules/compose modules/feature\_extraction modules/preprocessing modules/impute modules/unsupervised\_reduction modules/random\_projection modules/kernel\_approximation modules/metrics modules/preprocessing\_targets

</div>

---

loading_other_datasets.md

---

# Loading other datasets

<div class="currentmodule">

sklearn.datasets

</div>

## Sample images

Scikit-learn also embeds a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipelines on 2D data.

<div class="autosummary">

load\_sample\_images load\_sample\_image

</div>

<div class="plot" data-context="close-figs" data-scale="30" data-align="right" data-include-source="False">

import matplotlib.pyplot as plt from sklearn.datasets import load\_sample\_image

china = load\_sample\_image("china.jpg") plt.imshow(china) plt.axis('off') plt.tight\_layout() plt.show()

</div>

\> **Warning** \> The default coding of images is based on the `uint8` dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use `matplotlib.pyplpt.imshow`, don't forget to scale to the range 0 - 1 as done in the following example.

## Datasets in svmlight / libsvm format

scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form `<label> <feature-id>:<feature-value> <feature-id>:<feature-value> ...`. This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for `X` and numpy arrays are used for `y`.

You may load a dataset like as follows:

    >>> from sklearn.datasets import load_svmlight_file
    >>> X_train, y_train = load_svmlight_file("/path/to/train_dataset.txt")
    ...                                                         # doctest: +SKIP

You may also load two (or more) datasets at once:

    >>> X_train, y_train, X_test, y_test = load_svmlight_files(
    ...     ("/path/to/train_dataset.txt", "/path/to/test_dataset.txt"))
    ...                                                         # doctest: +SKIP

In this case, `X_train` and `X_test` are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:

    >>> X_test, y_test = load_svmlight_file(
    ...     "/path/to/test_dataset.txt", n_features=X_train.shape[1])
    ...                                                         # doctest: +SKIP

**Related links**

  - \`Public datasets in svmlight / libsvm format\`: <https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets>
  - \`Faster API-compatible implementation\`: <https://github.com/mblondel/svmlight-loader>

## Downloading datasets from the openml.org repository

[openml.org](https://openml.org) is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.

The `sklearn.datasets` package is able to download datasets from the repository using the function <span class="title-ref">sklearn.datasets.fetch\_openml</span>.

For example, to download a dataset of gene expressions in mice brains:

    >>> from sklearn.datasets import fetch_openml
    >>> mice = fetch_openml(name='miceprotein', version=4)

To fully specify a dataset, you need to provide a name and a version, though the version is optional, see \[openml\_versions\](\#openml\_versions) below. The dataset contains a total of 1080 examples belonging to 8 different classes:

    >>> mice.data.shape
    (1080, 77)
    >>> mice.target.shape
    (1080,)
    >>> np.unique(mice.target)
    array(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)

You can get more information on the dataset by looking at the `DESCR` and `details` attributes:

    >>> print(mice.DESCR) # doctest: +SKIP
    **Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios
    **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015
    **Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing
    Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down
    Syndrome. PLoS ONE 10(6): e0129126...
    
    >>> mice.details # doctest: +SKIP
    {'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',
    'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',
    'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',
    'file_id': '17928620', 'default_target_attribute': 'class',
    'row_id_attribute': 'MouseID',
    'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],
    'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],
    'visibility': 'public', 'status': 'active',
    'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}

The `DESCR` contains a free-text description of the data, while `details` contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the [OpenML documentation](https://docs.openml.org/#data) The `data_id` of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:

    >>> mice.url
    'https://www.openml.org/d/40966'

The `data_id` also uniquely identifies a dataset from OpenML:

    >>> mice = fetch_openml(data_id=40966)
    >>> mice.details # doctest: +SKIP
    {'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
    'creator': ...,
    'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
    'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':
    '1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,
    Gardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins
    Critical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):
    e0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',
    'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':
    '3c479a6885bfa0438971388283a1ce32'}

### Dataset Versions

A dataset is uniquely specified by its `data_id`, but not necessarily by its name. Several different "versions" of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that `fetch_openml(name="miceprotein")` can yield different results at different times if earlier versions become inactive. You can see that the dataset with `data_id` 40966 that we fetched above is the first version of the "miceprotein" dataset:

    >>> mice.details['version']  #doctest: +SKIP
    '1'

In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:

    >>> iris = fetch_openml(name="iris")
    >>> iris.details['version']  #doctest: +SKIP
    '1'
    >>> iris.details['id']  #doctest: +SKIP
    '61'
    
    >>> iris_61 = fetch_openml(data_id=61)
    >>> iris_61.details['version']
    '1'
    >>> iris_61.details['id']
    '61'
    
    >>> iris_969 = fetch_openml(data_id=969)
    >>> iris_969.details['version']
    '3'
    >>> iris_969.details['id']
    '969'

Specifying the dataset by the name "iris" yields the lowest version, version 1, with the `data_id` 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset `data_id`. The other dataset, with `data_id` 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:

    >>> np.unique(iris_969.target)
    array(['N', 'P'], dtype=object)

You can also specify both the name and the version, which also uniquely identifies the dataset:

    >>> iris_version_3 = fetch_openml(name="iris", version=3)
    >>> iris_version_3.details['version']
    '3'
    >>> iris_version_3.details['id']
    '969'

**References**

  - `Vanschoren, van Rijn, Bischl and Torgo. "OpenML: networked science in
    machine learning" ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.
    <1407.7722>`

### ARFF parser

From version 1.2, scikit-learn provides a new keyword argument <span class="title-ref">parser</span> that provides several options to parse the ARFF files provided by OpenML. The legacy parser (i.e. <span class="title-ref">parser="liac-arff"</span>) is based on the project [LIAC-ARFF](https://github.com/renatopp/liac-arff). This parser is however slow and consume more memory than required. A new parser based on pandas (i.e. <span class="title-ref">parser="pandas"</span>) is both faster and more memory efficient. However, this parser does not support sparse data. Therefore, we recommend using <span class="title-ref">parser="auto"</span> which will use the best parser available for the requested dataset.

The <span class="title-ref">"pandas"</span> and <span class="title-ref">"liac-arff"</span> parsers can lead to different data types in the output. The notable differences are the following:

  - The <span class="title-ref">"liac-arff"</span> parser always encodes categorical features as <span class="title-ref">str</span> objects. To the contrary, the <span class="title-ref">"pandas"</span> parser instead infers the type while reading and numerical categories will be casted into integers whenever possible.
  - The <span class="title-ref">"liac-arff"</span> parser uses float64 to encode numerical features tagged as 'REAL' and 'NUMERICAL' in the metadata. The <span class="title-ref">"pandas"</span> parser instead infers if these numerical features corresponds to integers and uses panda's Integer extension dtype.
  - In particular, classification datasets with integer categories are typically loaded as such <span class="title-ref">(0, 1, ...)</span> with the <span class="title-ref">"pandas"</span> parser while <span class="title-ref">"liac-arff"</span> will force the use of string encoded class labels such as <span class="title-ref">"0"</span>, <span class="title-ref">"1"</span> and so on.
  - The <span class="title-ref">"pandas"</span> parser will not strip single quotes - i.e. <span class="title-ref">'</span> - from string columns. For instance, a string <span class="title-ref">'my string'</span> will be kept as is while the <span class="title-ref">"liac-arff"</span> parser will strip the single quotes. For categorical columns, the single quotes are stripped from the values.

In addition, when <span class="title-ref">as\_frame=False</span> is used, the <span class="title-ref">"liac-arff"</span> parser returns ordinally encoded data where the categories are provided in the attribute <span class="title-ref">categories</span> of the <span class="title-ref">Bunch</span> instance. Instead, <span class="title-ref">"pandas"</span> returns a NumPy array were the categories. Then it's up to the user to design a feature engineering pipeline with an instance of <span class="title-ref">OneHotEncoder</span> or <span class="title-ref">OrdinalEncoder</span> typically wrapped in a <span class="title-ref">ColumnTransformer</span> to preprocess the categorical columns explicitly. See for instance: \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py).

## Loading from external datasets

scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.

Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:

  - [pandas.io](https://pandas.pydata.org/pandas-docs/stable/io.html) provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn.
  - [scipy.io](https://docs.scipy.org/doc/scipy/reference/io.html) specializes in binary formats often used in scientific computing context such as .mat and .arff
  - [numpy/routines.io](https://docs.scipy.org/doc/numpy/reference/routines.io.html) for standard loading of columnar data into numpy arrays
  - scikit-learn's <span class="title-ref">load\_svmlight\_file</span> for the svmlight or libSVM sparse format
  - scikit-learn's <span class="title-ref">load\_files</span> for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category

For some miscellaneous data such as images, videos, and audio, you may wish to refer to:

  - [skimage.io](https://scikit-image.org/docs/dev/api/skimage.io.html) or [Imageio](https://imageio.readthedocs.io/en/stable/reference/core_v3.html) for loading images and videos into numpy arrays
  - [scipy.io.wavfile.read](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html) for reading WAV files into a numpy array

Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using <span class="title-ref">\~sklearn.preprocessing.OneHotEncoder</span> or <span class="title-ref">\~sklearn.preprocessing.OrdinalEncoder</span> or similar. See \[preprocessing\](\#preprocessing).

Note: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.

---

real_world.md

---

# Real world datasets

<div class="currentmodule">

sklearn.datasets

</div>

scikit-learn provides tools to load larger datasets, downloading them if necessary.

They can be loaded using the following functions:

<div class="autosummary">

fetch\_olivetti\_faces fetch\_20newsgroups fetch\_20newsgroups\_vectorized fetch\_lfw\_people fetch\_lfw\_pairs fetch\_covtype fetch\_rcv1 fetch\_kddcup99 fetch\_california\_housing fetch\_species\_distributions

</div>

## The Olivetti faces dataset

[This dataset contains a set of face images](https://cam-orl.co.uk/facedatabase.html) taken between April 1992 and April 1994 at AT\&T Laboratories Cambridge. The <span class="title-ref">sklearn.datasets.fetch\_olivetti\_faces</span> function is the data fetching / caching function that downloads the data archive from AT\&T.

As described on the original website:

> There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).

**Data Set Characteristics:**

<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><blockquote>
<p>40</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Samples total</td>
<td><blockquote>
<p>400</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Dimensionality</td>
<td><blockquote>
<p>4096</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Features</td>
<td>real, between 0 and 1</td>
</tr>
</tbody>
</table>

The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval \[0, 1\], which are easier to work with for many algorithms.

The "target" for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.

The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.

When using these images, please give credit to AT\&T Laboratories Cambridge.

## The 20 newsgroups text dataset

The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.

This module contains two loaders. The first one, <span class="title-ref">sklearn.datasets.fetch\_20newsgroups</span>, returns a list of the raw texts that can be fed to text feature extractors such as <span class="title-ref">\~sklearn.feature\_extraction.text.CountVectorizer</span> with custom parameters so as to extract feature vectors. The second one, <span class="title-ref">sklearn.datasets.fetch\_20newsgroups\_vectorized</span>, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.

**Data Set Characteristics:**

<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><blockquote>
<p>20</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Samples total</td>
<td><blockquote>
<p>18846</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Dimensionality</td>
<td><blockquote>
<p>1</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Features</td>
<td><blockquote>
<p>text</p>
</blockquote></td>
</tr>
</tbody>
</table>

<div class="dropdown">

Usage

The <span class="title-ref">sklearn.datasets.fetch\_20newsgroups</span> function is a data fetching / caching functions that downloads the data archive from the original [20 newsgroups website](http://people.csail.mit.edu/jrennie/20Newsgroups/), extracts the archive contents in the `~/scikit_learn_data/20news_home` folder and calls the <span class="title-ref">sklearn.datasets.load\_files</span> on either the training or testing set folder, or both of them:

    >>> from sklearn.datasets import fetch_20newsgroups
    >>> newsgroups_train = fetch_20newsgroups(subset='train')
    
    >>> from pprint import pprint
    >>> pprint(list(newsgroups_train.target_names))
    ['alt.atheism',
     'comp.graphics',
     'comp.os.ms-windows.misc',
     'comp.sys.ibm.pc.hardware',
     'comp.sys.mac.hardware',
     'comp.windows.x',
     'misc.forsale',
     'rec.autos',
     'rec.motorcycles',
     'rec.sport.baseball',
     'rec.sport.hockey',
     'sci.crypt',
     'sci.electronics',
     'sci.med',
     'sci.space',
     'soc.religion.christian',
     'talk.politics.guns',
     'talk.politics.mideast',
     'talk.politics.misc',
     'talk.religion.misc']

The real data lies in the `filenames` and `target` attributes. The target attribute is the integer index of the category:

    >>> newsgroups_train.filenames.shape
    (11314,)
    >>> newsgroups_train.target.shape
    (11314,)
    >>> newsgroups_train.target[:10]
    array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])

It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the <span class="title-ref">sklearn.datasets.fetch\_20newsgroups</span> function:

    >>> cats = ['alt.atheism', 'sci.space']
    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)
    
    >>> list(newsgroups_train.target_names)
    ['alt.atheism', 'sci.space']
    >>> newsgroups_train.filenames.shape
    (1073,)
    >>> newsgroups_train.target.shape
    (1073,)
    >>> newsgroups_train.target[:10]
    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])

</div>

<div class="dropdown">

Converting text to vectors

In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the `sklearn.feature_extraction.text` as demonstrated in the following example that extract [TF-IDF](https://en.wikipedia.org/wiki/Tf-idf) vectors of unigram tokens from a subset of 20news:

    >>> from sklearn.feature_extraction.text import TfidfVectorizer
    >>> categories = ['alt.atheism', 'talk.religion.misc',
    ...               'comp.graphics', 'sci.space']
    >>> newsgroups_train = fetch_20newsgroups(subset='train',
    ...                                       categories=categories)
    >>> vectorizer = TfidfVectorizer()
    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)
    >>> vectors.shape
    (2034, 34118)

The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):

    >>> vectors.nnz / float(vectors.shape[0])
    159.01327...

<span class="title-ref">sklearn.datasets.fetch\_20newsgroups\_vectorized</span> is a function which returns ready-to-use token counts features instead of file names.

</div>

<div class="dropdown">

Filtering text for more realistic training

It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren't from this window of time.

For example, let's look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:

    >>> from sklearn.naive_bayes import MultinomialNB
    >>> from sklearn import metrics
    >>> newsgroups_test = fetch_20newsgroups(subset='test',
    ...                                      categories=categories)
    >>> vectors_test = vectorizer.transform(newsgroups_test.data)
    >>> clf = MultinomialNB(alpha=.01)
    >>> clf.fit(vectors, newsgroups_train.target)
    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
    
    >>> pred = clf.predict(vectors_test)
    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
    0.88213...

(The example \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py) shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of what's going on inside this classifier?)

Let's take a look at what the most informative features are:

> \>\>\> import numpy as np \>\>\> def show\_top10(classifier, vectorizer, categories): ... feature\_names = vectorizer.get\_feature\_names\_out() ... for i, category in enumerate(categories): ... top10 = np.argsort([classifier.coef]()\[i\])\[-10:\] ... print("%s: %s" % (category, " ".join(feature\_names\[top10\]))) ... \>\>\> show\_top10(clf, vectorizer, newsgroups\_train.target\_names) alt.atheism: edu it and in you that is of to the comp.graphics: edu in graphics it is for and of to the sci.space: edu it that is in and space to of the talk.religion.misc: not it you in is that and to of the

You can now see many things that these features have overfit to:

  - Almost every group is distinguished by whether headers such as `NNTP-Posting-Host:` and `Distribution:` appear more or less often.
  - Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature.
  - The word "article" is a significant feature, based on how often people quote previous posts like this: "In article \[article ID\], \[name\] \<\[e-mail address\]\> wrote:"
  - Other features match the names and e-mail addresses of particular people who were posting at the time.

With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.

For this reason, the functions that load 20 Newsgroups data provide a parameter called **remove**, telling it what kinds of information to strip out of each file. **remove** should be a tuple containing any subset of `('headers', 'footers', 'quotes')`, telling it to remove headers, signature blocks, and quotation blocks respectively.

> \>\>\> newsgroups\_test = fetch\_20newsgroups(subset='test', ... remove=('headers', 'footers', 'quotes'), ... categories=categories) \>\>\> vectors\_test = vectorizer.transform(newsgroups\_test.data) \>\>\> pred = clf.predict(vectors\_test) \>\>\> metrics.f1\_score(pred, newsgroups\_test.target, average='macro') 0.77310...

This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:

> \>\>\> newsgroups\_train = fetch\_20newsgroups(subset='train', ... remove=('headers', 'footers', 'quotes'), ... categories=categories) \>\>\> vectors = vectorizer.fit\_transform(newsgroups\_train.data) \>\>\> clf = MultinomialNB(alpha=.01) \>\>\> clf.fit(vectors, newsgroups\_train.target) MultinomialNB(alpha=0.01, class\_prior=None, fit\_prior=True)
> 
> \>\>\> vectors\_test = vectorizer.transform(newsgroups\_test.data) \>\>\> pred = clf.predict(vectors\_test) \>\>\> metrics.f1\_score(newsgroups\_test.target, pred, average='macro') 0.76995...

Some other classifiers cope better with this harder version of the task. Try the \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py) example with and without the <span class="title-ref">remove</span> option to compare the results.

</div>

**Data Considerations**

The Cleveland Indians is a major league baseball team based in Cleveland, Ohio, USA. In December 2020, it was reported that "After several months of discussion sparked by the death of George Floyd and a national reckoning over race and colonialism, the Cleveland Indians have decided to change their name." Team owner Paul Dolan "did make it clear that the team will not make its informal nickname -- the Tribe -- its new team name." "It's not going to be a half-step away from the Indians," Dolan said."We will not have a Native American-themed name."

<https://www.mlb.com/news/cleveland-indians-team-name-change>

**Recommendation**

  - When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting `remove=('headers', 'footers', 'quotes')`. The F-score will be lower because it is more realistic.
  - This text dataset contains data which may be inappropriate for certain NLP applications. An example is listed in the "Data Considerations" section above. The challenge with using current text datasets in NLP for tasks such as sentence completion, clustering, and other applications is that text that is culturally biased and inflammatory will propagate biases. This should be taken into consideration when using the dataset, reviewing the output, and the bias should be documented.

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py)
  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py)
  - \[sphx\_glr\_auto\_examples\_text\_plot\_hashing\_vs\_dict\_vectorizer.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_hashing\_vs\_dict\_vectorizer.py)
  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py)

## The Labeled Faces in the Wild face recognition dataset

This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:

<http://vis-www.cs.umass.edu/lfw/>

Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.

An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.

Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.

**Data Set Characteristics:**

<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><blockquote>
<p>5749</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Samples total</td>
<td><blockquote>
<p>13233</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Dimensionality</td>
<td><blockquote>
<p>5828</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Features</td>
<td>real, between 0 and 255</td>
</tr>
</tbody>
</table>

<div class="dropdown">

Usage

`scikit-learn` provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the `~/scikit_learn_data/lfw_home/` folder using `joblib`.

The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):

    >>> from sklearn.datasets import fetch_lfw_people
    >>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
    
    >>> for name in lfw_people.target_names:
    ...     print(name)
    ...
    Ariel Sharon
    Colin Powell
    Donald Rumsfeld
    George W Bush
    Gerhard Schroeder
    Hugo Chavez
    Tony Blair

The default slice is a rectangular shape around the face, removing most of the background:

    >>> lfw_people.data.dtype
    dtype('float32')
    
    >>> lfw_people.data.shape
    (1288, 1850)
    
    >>> lfw_people.images.shape
    (1288, 50, 37)

Each of the `1140` faces is assigned to a single person id in the `target` array:

    >>> lfw_people.target.shape
    (1288,)
    
    >>> list(lfw_people.target[:10])
    [5, 6, 3, 1, 0, 1, 3, 4, 3, 0]

The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:

    >>> from sklearn.datasets import fetch_lfw_pairs
    >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')
    
    >>> list(lfw_pairs_train.target_names)
    ['Different persons', 'Same person']
    
    >>> lfw_pairs_train.pairs.shape
    (2200, 2, 62, 47)
    
    >>> lfw_pairs_train.data.shape
    (2200, 5828)
    
    >>> lfw_pairs_train.target.shape
    (2200,)

Both for the <span class="title-ref">sklearn.datasets.fetch\_lfw\_people</span> and <span class="title-ref">sklearn.datasets.fetch\_lfw\_pairs</span> function it is possible to get an additional dimension with the RGB color channels by passing `color=True`, in that case the shape will be `(2200, 2, 62, 47, 3)`.

The <span class="title-ref">sklearn.datasets.fetch\_lfw\_pairs</span> datasets is subdivided into 3 subsets: the development `train` set, the development `test` set and an evaluation `10_folds` set meant to compute performance metrics using a 10-folds cross validation scheme.

</div>

**References**

  - [Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.](http://vis-www.cs.umass.edu/lfw/lfw.pdf) Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.

**Examples**

  - \[sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py)

## Forest covertypes

The samples in this dataset correspond to 30×30m patches of forest in the US, collected for the task of predicting each patch's cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the [dataset's homepage](https://archive.ics.uci.edu/ml/datasets/Covertype). Some of the features are boolean indicators, while others are discrete or continuous measurements.

**Data Set Characteristics:**

<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><blockquote>
<p>7</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Samples total</td>
<td><blockquote>
<p>581012</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Dimensionality</td>
<td><blockquote>
<p>54</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Features</td>
<td><blockquote>
<p>int</p>
</blockquote></td>
</tr>
</tbody>
</table>

<span class="title-ref">sklearn.datasets.fetch\_covtype</span> will load the covertype dataset; it returns a dictionary-like 'Bunch' object with the feature matrix in the `data` member and the target values in `target`. If optional argument 'as\_frame' is set to 'True', it will return `data` and `target` as pandas data frame, and there will be an additional member `frame` as well. The dataset will be downloaded from the web if necessary.

## RCV1 dataset

Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in\[1\].

**Data Set Characteristics:**

<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><blockquote>
<p>103</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Samples total</td>
<td><blockquote>
<p>804414</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Dimensionality</td>
<td><blockquote>
<p>47236</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Features</td>
<td>real, between 0 and 1</td>
</tr>
</tbody>
</table>

<span class="title-ref">sklearn.datasets.fetch\_rcv1</span> will load the following version: RCV1-v2, vectors, full sets, topics multilabels:

    >>> from sklearn.datasets import fetch_rcv1
    >>> rcv1 = fetch_rcv1()

It returns a dictionary-like object, with the following attributes:

`data`: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in\[2\]: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values:

    >>> rcv1.data.shape
    (804414, 47236)

`target`: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:

    >>> rcv1.target.shape
    (804414, 103)

`sample_id`: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:

    >>> rcv1.sample_id[:3]
    array([2286, 2287, 2288], dtype=uint32)

`target_names`: The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for 'GMIL', to 381327 for 'CCAT':

    >>> rcv1.target_names[:3].tolist()  # doctest: +SKIP
    ['E11', 'ECAT', 'M11']

The dataset will be downloaded from the [rcv1 homepage](http://jmlr.csail.mit.edu/papers/volume5/lewis04a/) if necessary. The compressed size is about 656 MB.

**References**

## Kddcup 99 dataset

The KDD Cup '99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab\[3\]. The artificial data (described on the [dataset's homepage](https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting 'abnormal' data, i.e.:

  - qualitatively different from normal data
  - in large minority among the observations.

We thus transform the KDD Data set into two different data sets: SA and SF.

  - SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives an anomaly proportion of 1%.
  - SF is obtained as in\[4\] by simply picking up the data whose attribute logged\_in is positive, thus focusing on the intrusion attack, which gives a proportion of 0.3% of attack.
  - http and smtp are two subsets of SF corresponding with third feature equal to 'http' (resp. to 'smtp').

General KDD structure:

|                |                                            |
| -------------- | ------------------------------------------ |
| Samples total  | 4898431                                    |
| Dimensionality | 41                                         |
| Features       | discrete (int) or continuous (float)       |
| Targets        | str, 'normal.' or name of the anomaly type |

SA structure:

|                |                                            |
| -------------- | ------------------------------------------ |
| Samples total  | 976158                                     |
| Dimensionality | 41                                         |
| Features       | discrete (int) or continuous (float)       |
| Targets        | str, 'normal.' or name of the anomaly type |

SF structure:

|                |                                            |
| -------------- | ------------------------------------------ |
| Samples total  | 699691                                     |
| Dimensionality | 4                                          |
| Features       | discrete (int) or continuous (float)       |
| Targets        | str, 'normal.' or name of the anomaly type |

http structure:

|                |                                            |
| -------------- | ------------------------------------------ |
| Samples total  | 619052                                     |
| Dimensionality | 3                                          |
| Features       | discrete (int) or continuous (float)       |
| Targets        | str, 'normal.' or name of the anomaly type |

smtp structure:

|                |                                            |
| -------------- | ------------------------------------------ |
| Samples total  | 95373                                      |
| Dimensionality | 3                                          |
| Features       | discrete (int) or continuous (float)       |
| Targets        | str, 'normal.' or name of the anomaly type |

<span class="title-ref">sklearn.datasets.fetch\_kddcup99</span> will load the kddcup99 dataset; it returns a dictionary-like object with the feature matrix in the `data` member and the target values in `target`. The "as\_frame" optional argument converts `data` into a pandas DataFrame and `target` into a pandas Series. The dataset will be downloaded from the web if necessary.

**References**

## California Housing dataset

**Data Set Characteristics:**

  - Number of Instances  
    20640

  - Number of Attributes  
    8 numeric, predictive attributes and the target

  - Attribute Information
    
      - MedInc median income in block group
      - HouseAge median house age in block group
      - AveRooms average number of rooms per household
      - AveBedrms average number of bedrooms per household
      - Population block group population
      - AveOccup average number of household members
      - Latitude block group latitude
      - Longitude block group longitude

  - Missing Attribute Values  
    None

This dataset was obtained from the StatLib repository. <https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>

The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surprisingly large values for block groups with few households and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the <span class="title-ref">sklearn.datasets.fetch\_california\_housing</span> function.

**References**

  - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297

## Species distribution dataset

This dataset represents the geographic distribution of two species in Central and South America. The two species are:

  - ["Bradypus variegatus"](http://www.iucnredlist.org/details/3038/0) , the Brown-throated Sloth.
  - ["Microryzomys minutus"](http://www.iucnredlist.org/details/13408/0) , also known as the Forest Small Rice Rat, a rodent that lives in Peru, Colombia, Ecuador, Peru, and Venezuela.

The dataset is not a typical dataset since a <span class="title-ref">\~sklearn.datasets.base.Bunch</span> containing the attributes <span class="title-ref">data</span> and <span class="title-ref">target</span> is not returned. Instead, we have information allowing to create a "density" map of the different species.

The grid for the map can be built using the attributes <span class="title-ref">x\_left\_lower\_corner</span>, <span class="title-ref">y\_left\_lower\_corner</span>, <span class="title-ref">Nx</span>, <span class="title-ref">Ny</span> and <span class="title-ref">grid\_size</span>, which respectively correspond to the x and y coordinates of the lower left corner of the grid, the number of points along the x- and y-axis and the size of the step on the grid.

The density at each location of the grid is contained in the <span class="title-ref">coverage</span> attribute.

Finally, the <span class="title-ref">train</span> and <span class="title-ref">test</span> attributes contain information regarding the location of a species at a specific location.

The dataset is provided by Phillips et. al. (2006).

**References**

  - ["Maximum entropy modeling of species geographic distributions"](http://rob.schapire.net/papers/ecolmod.pdf) S. J. Phillips,
    18. 16. Anderson, R. E. Schapire - Ecological Modelling, 190:231-259, 2006.

**Examples**

  - \[sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py)

<!-- end list -->

1.  Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.

2.  Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.

3.  Analysis and Results of the 1999 DARPA Off-Line Intrusion Detection Evaluation, Richard Lippmann, Joshua W. Haines, David J. Fried, Jonathan Korba, Kumar Das.

4.  K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online unsupervised outlier detection using finite mixtures with discounting learning algorithms. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 320-324. ACM Press, 2000.

---

sample_generators.md

---

# Generated datasets

<div class="currentmodule">

sklearn.datasets

</div>

In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.

## Generators for classification and clustering

These generators produce a matrix of features and corresponding discrete targets.

### Single label

<span class="title-ref">make\_blobs</span> creates a multiclass dataset by allocating each class to one normally-distributed cluster of points. It provides control over the centers and standard deviations of each cluster. This dataset is used to demonstrate clustering.

<div class="plot" data-context="close-figs" data-scale="70" data-align="center">

import matplotlib.pyplot as plt from sklearn.datasets import make\_blobs

X, y = make\_blobs(centers=3, cluster\_std=0.5, random\_state=0)

plt.scatter(X\[:, 0\], X\[:, 1\], c=y) plt.title("Three normally-distributed clusters") plt.show()

</div>

<span class="title-ref">make\_classification</span> also creates multiclass datasets but specializes in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.

<div class="plot" data-context="close-figs" data-scale="70" data-align="center">

import matplotlib.pyplot as plt from sklearn.datasets import make\_classification

fig, axs = plt.subplots(1, 3, figsize=(12, 4), sharey=True, sharex=True) titles = \["Two classes,none informative feature,none cluster per class", "Two classes,ntwo informative features,ntwo clusters per class", "Three classes,ntwo informative features,none cluster per class"\] params = \[ {"n\_informative": 1, "n\_clusters\_per\_class": 1, "n\_classes": 2}, {"n\_informative": 2, "n\_clusters\_per\_class": 2, "n\_classes": 2}, {"n\_informative": 2, "n\_clusters\_per\_class": 1, "n\_classes": 3} \]

  - for i, param in enumerate(params):  
    X, Y = make\_classification(n\_features=2, n\_redundant=0, random\_state=1, \*\*param) axs\[i\].scatter(X\[:, 0\], X\[:, 1\], c=Y) axs\[i\].set\_title(titles\[i\])

plt.tight\_layout() plt.show()

</div>

<span class="title-ref">make\_gaussian\_quantiles</span> divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres.

<div class="plot" data-context="close-figs" data-scale="70" data-align="center">

import matplotlib.pyplot as plt from sklearn.datasets import make\_gaussian\_quantiles

X, Y = make\_gaussian\_quantiles(n\_features=2, n\_classes=3, random\_state=0) plt.scatter(X\[:, 0\], X\[:, 1\], c=Y) plt.title("Gaussian divided into three quantiles") plt.show()

</div>

<span class="title-ref">make\_hastie\_10\_2</span> generates a similar binary, 10-dimensional problem.

<span class="title-ref">make\_circles</span> and <span class="title-ref">make\_moons</span> generate 2D binary classification datasets that are challenging to certain algorithms (e.g., centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualization. <span class="title-ref">make\_circles</span> produces Gaussian data with a spherical decision boundary for binary classification, while <span class="title-ref">make\_moons</span> produces two interleaving half-circles.

<div class="plot" data-context="close-figs" data-scale="70" data-align="center">

import matplotlib.pyplot as plt from sklearn.datasets import make\_circles, make\_moons

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))

X, Y = make\_circles(noise=0.1, factor=0.3, random\_state=0) ax1.scatter(X\[:, 0\], X\[:, 1\], c=Y) ax1.set\_title("make\_circles")

X, Y = make\_moons(noise=0.1, random\_state=0) ax2.scatter(X\[:, 0\], X\[:, 1\], c=Y) ax2.set\_title("make\_moons")

plt.tight\_layout() plt.show()

</div>

### Multilabel

<span class="title-ref">make\_multilabel\_classification</span> generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:

  - Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base distribution, and would be correlated.
  - For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.
  - Documents without labels words at random, rather than from a base distribution.

[![image](../auto_examples/datasets/images/sphx_glr_plot_random_multilabel_dataset_001.png)](../auto_examples/datasets/plot_random_multilabel_dataset.html)

### Biclustering

<div class="autosummary">

make\_biclusters make\_checkerboard

</div>

## Generators for regression

<span class="title-ref">make\_regression</span> produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance).

Other regression generators generate functions deterministically from randomized features. <span class="title-ref">make\_sparse\_uncorrelated</span> produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: <span class="title-ref">make\_friedman1</span> is related by polynomial and sine transforms; <span class="title-ref">make\_friedman2</span> includes feature multiplication and reciprocation; and <span class="title-ref">make\_friedman3</span> is similar with an arctan transformation on the target.

## Generators for manifold learning

<div class="autosummary">

make\_s\_curve make\_swiss\_roll

</div>

## Generators for decomposition

<div class="autosummary">

make\_low\_rank\_matrix make\_sparse\_coded\_signal make\_spd\_matrix make\_sparse\_spd\_matrix

</div>

---

toy_dataset.md

---

# Toy datasets

<div class="currentmodule">

sklearn.datasets

</div>

scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.

They can be loaded using the following functions:

<div class="autosummary">

load\_iris load\_diabetes load\_digits load\_linnerud load\_wine load\_breast\_cancer

</div>

These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.

## Iris plants dataset

**Data Set Characteristics:**

  - Number of Instances  
    150 (50 in each of three classes)

  - Number of Attributes  
    4 numeric, predictive attributes and the class

  - Attribute Information
    
      - sepal length in cm
    
      - sepal width in cm
    
      - petal length in cm
    
      - petal width in cm
    
      -   - class:
            
              - Iris-Setosa
              - Iris-Versicolour
              - Iris-Virginica

  - Summary Statistics

<table>
<thead>
<tr class="header">
<th></th>
<th>Min</th>
<th>Max</th>
<th>Mean</th>
<th>SD</th>
<th>Class Correlation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sepal length:</td>
<td><blockquote>
<p>4.3</p>
</blockquote></td>
<td><blockquote>
<p>7.9</p>
</blockquote></td>
<td><blockquote>
<p>5.84</p>
</blockquote></td>
<td><blockquote>
<p>0.83</p>
</blockquote></td>
<td><blockquote>
<p>0.7826</p>
</blockquote></td>
</tr>
<tr class="even">
<td>sepal width:</td>
<td><blockquote>
<p>2.0</p>
</blockquote></td>
<td><blockquote>
<p>4.4</p>
</blockquote></td>
<td><blockquote>
<p>3.05</p>
</blockquote></td>
<td><blockquote>
<p>0.43</p>
</blockquote></td>
<td><blockquote>
<p>-0.4194</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>petal length:</td>
<td><blockquote>
<p>1.0</p>
</blockquote></td>
<td><blockquote>
<p>6.9</p>
</blockquote></td>
<td><blockquote>
<p>3.76</p>
</blockquote></td>
<td><blockquote>
<p>1.76</p>
</blockquote></td>
<td><blockquote>
<p>0.9490 (high!)</p>
</blockquote></td>
</tr>
<tr class="even">
<td>petal width:</td>
<td><blockquote>
<p>0.1</p>
</blockquote></td>
<td><blockquote>
<p>2.5</p>
</blockquote></td>
<td><blockquote>
<p>1.20</p>
</blockquote></td>
<td><blockquote>
<p>0.76</p>
</blockquote></td>
<td><blockquote>
<p>0.9565 (high!)</p>
</blockquote></td>
</tr>
</tbody>
</table>

  - Missing Attribute Values  
    None

  - Class Distribution  
    33.3% for each of 3 classes.

  - Creator  
    R.A. Fisher

  - Donor  
    Michael Marshall (<MARSHALL%PLU@io.arc.nasa.gov>)

  - Date  
    July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher's paper. Note that it's the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.

<div class="dropdown">

References

  - Fisher, R.A. "The use of multiple measurements in taxonomic problems" Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to Mathematical Statistics" (John Wiley, NY, 1950).
  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218.
  - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71.
  - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule". IEEE Transactions on Information Theory, May 1972, 431-433.
  - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al"s AUTOCLASS II conceptual clustering system finds 3 classes in the data.
  - Many, many more ...

</div>

## Diabetes dataset

Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.

**Data Set Characteristics:**

  - Number of Instances  
    442

  - Number of Attributes  
    First 10 columns are numeric predictive values

  - Target  
    Column 11 is a quantitative measure of disease progression one year after baseline

  - Attribute Information
    
      - age age in years
      - sex
      - bmi body mass index
      - bp average blood pressure
      - s1 tc, total serum cholesterol
      - s2 ldl, low-density lipoproteins
      - s3 hdl, high-density lipoproteins
      - s4 tch, total cholesterol / HDL
      - s5 ltg, possibly log of serum triglycerides level
      - s6 glu, blood sugar level

Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of <span class="title-ref">n\_samples</span> (i.e. the sum of squares of each column totals 1).

Source URL: <https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html>

For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) "Least Angle Regression," Annals of Statistics (with discussion), 407-499. (<https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf>)

## Optical recognition of handwritten digits dataset

**Data Set Characteristics:**

  - Number of Instances  
    1797

  - Number of Attributes  
    64

  - Attribute Information  
    8x8 image of integer pixels in the range 0..16.

  - Missing Attribute Values  
    None

  - Creator
    
    5.  Alpaydin (alpaydin '@' boun.edu.tr)

  - Date  
    July; 1998

This is a copy of the test set of the UCI ML hand-written digits datasets <https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits>

The data set contains images of hand-written digits: 10 classes where each class refers to a digit.

Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.

For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.

<div class="dropdown">

References

  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University.

  - 5.  Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.

  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.

  - Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000.

</div>

## Linnerrud dataset

**Data Set Characteristics:**

  - Number of Instances  
    20

  - Number of Attributes  
    3

  - Missing Attribute Values  
    None

The Linnerud dataset is a multi-output regression dataset. It consists of three exercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club:

  -   - *physiological* - CSV containing 20 observations on 3 physiological variables:  
        Weight, Waist and Pulse.

  -   - *exercise* - CSV containing 20 observations on 3 exercise variables:  
        Chins, Situps and Jumps.

<div class="dropdown">

References

  - Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.

</div>

## Wine recognition dataset

**Data Set Characteristics:**

  - Number of Instances  
    178

  - Number of Attributes  
    13 numeric, predictive attributes and the class

  - Attribute Information
    
      - Alcohol
    
      - Malic acid
    
      - Ash
    
      - Alcalinity of ash
    
      - Magnesium
    
      - Total phenols
    
      - Flavanoids
    
      - Nonflavanoid phenols
    
      - Proanthocyanins
    
      - Color intensity
    
      - Hue
    
      - OD280/OD315 of diluted wines
    
      - Proline
    
      -   - class:
            
              - class\_0
              - class\_1
              - class\_2

  - Summary Statistics

<table>
<thead>
<tr class="header">
<th></th>
<th>Min</th>
<th>Max</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alcohol:</td>
<td>11.0</td>
<td><blockquote>
<p>14.8</p>
</blockquote></td>
<td><blockquote>
<p>13.0</p>
</blockquote></td>
<td><blockquote>
<p>0.8</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Malic Acid:</td>
<td>0.74</td>
<td><blockquote>
<p>5.80</p>
</blockquote></td>
<td><blockquote>
<p>2.34</p>
</blockquote></td>
<td><blockquote>
<p>1.12</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Ash:</td>
<td>1.36</td>
<td><blockquote>
<p>3.23</p>
</blockquote></td>
<td><blockquote>
<p>2.36</p>
</blockquote></td>
<td><blockquote>
<p>0.27</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Alcalinity of Ash:</td>
<td>10.6</td>
<td><blockquote>
<p>30.0</p>
</blockquote></td>
<td><blockquote>
<p>19.5</p>
</blockquote></td>
<td><blockquote>
<p>3.3</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Magnesium:</td>
<td>70.0</td>
<td>162.0</td>
<td><blockquote>
<p>99.7</p>
</blockquote></td>
<td><blockquote>
<p>14.3</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Total Phenols:</td>
<td>0.98</td>
<td><blockquote>
<p>3.88</p>
</blockquote></td>
<td><blockquote>
<p>2.29</p>
</blockquote></td>
<td><blockquote>
<p>0.63</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Flavanoids:</td>
<td>0.34</td>
<td><blockquote>
<p>5.08</p>
</blockquote></td>
<td><blockquote>
<p>2.03</p>
</blockquote></td>
<td><blockquote>
<p>1.00</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Nonflavanoid Phenols:</td>
<td>0.13</td>
<td><blockquote>
<p>0.66</p>
</blockquote></td>
<td><blockquote>
<p>0.36</p>
</blockquote></td>
<td><blockquote>
<p>0.12</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Proanthocyanins:</td>
<td>0.41</td>
<td><blockquote>
<p>3.58</p>
</blockquote></td>
<td><blockquote>
<p>1.59</p>
</blockquote></td>
<td><blockquote>
<p>0.57</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Colour Intensity:</td>
<td><blockquote>
<p>1.3</p>
</blockquote></td>
<td><blockquote>
<p>13.0</p>
</blockquote></td>
<td><blockquote>
<p>5.1</p>
</blockquote></td>
<td><blockquote>
<p>2.3</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Hue:</td>
<td>0.48</td>
<td><blockquote>
<p>1.71</p>
</blockquote></td>
<td><blockquote>
<p>0.96</p>
</blockquote></td>
<td><blockquote>
<p>0.23</p>
</blockquote></td>
</tr>
<tr class="even">
<td>OD280/OD315 of diluted wines:</td>
<td>1.27</td>
<td><blockquote>
<p>4.00</p>
</blockquote></td>
<td><blockquote>
<p>2.61</p>
</blockquote></td>
<td><blockquote>
<p>0.71</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Proline:</td>
<td><blockquote>
<p>278</p>
</blockquote></td>
<td><blockquote>
<p>1680</p>
</blockquote></td>
<td><blockquote>
<p>746</p>
</blockquote></td>
<td><blockquote>
<p>315</p>
</blockquote></td>
</tr>
</tbody>
</table>

  - Missing Attribute Values  
    None

  - Class Distribution  
    class\_0 (59), class\_1 (71), class\_2 (48)

  - Creator  
    R.A. Fisher

  - Donor  
    Michael Marshall (<MARSHALL%PLU@io.arc.nasa.gov>)

  - Date  
    July, 1988

This is a copy of UCI ML Wine recognition datasets. <https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data>

The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.

Original Owners:

Forina, M. et al, PARVUS -An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.

Citation:

Lichman, M. (2013). UCI Machine Learning Repository \[<https://archive.ics.uci.edu/ml>\]. Irvine, CA: University of California, School of Information and Computer Science.

<div class="dropdown">

References

(1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics).

The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)

(2) S. Aeberhard, D. Coomans and O. de Vel, "THE CLASSIFICATION PERFORMANCE OF RDA" Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics).

</div>

## Breast cancer wisconsin (diagnostic) dataset

**Data Set Characteristics:**

  - Number of Instances  
    569

  - Number of Attributes  
    30 numeric, predictive attributes and the class

  - Attribute Information
    
      - radius (mean of distances from center to points on the perimeter)
      - texture (standard deviation of gray-scale values)
      - perimeter
      - area
      - smoothness (local variation in radius lengths)
      - compactness (perimeter^2 / area - 1.0)
      - concavity (severity of concave portions of the contour)
      - concave points (number of concave portions of the contour)
      - symmetry
      - fractal dimension ("coastline approximation" - 1)
    
    The mean, standard error, and "worst" or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.
    
      -   - class:
            
              - WDBC-Malignant
              - WDBC-Benign

  - Summary Statistics

|                                     | Min   | Max    |
| ----------------------------------- | ----- | ------ |
| radius (mean):                      | 6.981 | 28.11  |
| texture (mean):                     | 9.71  | 39.28  |
| perimeter (mean):                   | 43.79 | 188.5  |
| area (mean):                        | 143.5 | 2501.0 |
| smoothness (mean):                  | 0.053 | 0.163  |
| compactness (mean):                 | 0.019 | 0.345  |
| concavity (mean):                   | 0.0   | 0.427  |
| concave points (mean):              | 0.0   | 0.201  |
| symmetry (mean):                    | 0.106 | 0.304  |
| fractal dimension (mean):           | 0.05  | 0.097  |
| radius (standard error):            | 0.112 | 2.873  |
| texture (standard error):           | 0.36  | 4.885  |
| perimeter (standard error):         | 0.757 | 21.98  |
| area (standard error):              | 6.802 | 542.2  |
| smoothness (standard error):        | 0.002 | 0.031  |
| compactness (standard error):       | 0.002 | 0.135  |
| concavity (standard error):         | 0.0   | 0.396  |
| concave points (standard error):    | 0.0   | 0.053  |
| symmetry (standard error):          | 0.008 | 0.079  |
| fractal dimension (standard error): | 0.001 | 0.03   |
| radius (worst):                     | 7.93  | 36.04  |
| texture (worst):                    | 12.02 | 49.54  |
| perimeter (worst):                  | 50.41 | 251.2  |
| area (worst):                       | 185.2 | 4254.0 |
| smoothness (worst):                 | 0.071 | 0.223  |
| compactness (worst):                | 0.027 | 1.058  |
| concavity (worst):                  | 0.0   | 1.252  |
| concave points (worst):             | 0.0   | 0.291  |
| symmetry (worst):                   | 0.156 | 0.664  |
| fractal dimension (worst):          | 0.055 | 0.208  |

  - Missing Attribute Values  
    None

  - Class Distribution  
    212 - Malignant, 357 - Benign

  - Creator  
    Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian

  - Donor  
    Nick Street

  - Date  
    November, 1995

This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. <https://goo.gl/U2Uwz2>

Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.

Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) \[K. P. Bennett, "Decision Tree Construction Via Linear Programming." Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992\], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.

The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: \[K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34\].

This database is also available through the UW CS ftp server:

ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/

<div class="dropdown">

References

  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS\&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.
  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995.
  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171.

</div>

---

datasets.md

---

# Dataset loading utilities

<div class="currentmodule">

sklearn.datasets

</div>

The `sklearn.datasets` package embeds some small toy datasets and provides helpers to fetch larger datasets commonly used by the machine learning community to benchmark algorithms on data that comes from the 'real world'.

To evaluate the impact of the scale of the dataset (`n_samples` and `n_features`) while controlling the statistical properties of the data (typically the correlation and informativeness of the features), it is also possible to generate synthetic data.

**General dataset API.** There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.

**The dataset loaders.** They can be used to load small standard datasets, described in the \[toy\_datasets\](\#toy\_datasets) section.

**The dataset fetchers.** They can be used to download and load larger datasets, described in the \[real\_world\_datasets\](\#real\_world\_datasets) section.

Both loaders and fetchers functions return a <span class="title-ref">\~sklearn.utils.Bunch</span> object holding at least two items: an array of shape `n_samples` \* `n_features` with key `data` (except for 20newsgroups) and a numpy array of length `n_samples`, containing the target values, with key `target`.

The Bunch object is a dictionary that exposes its keys as attributes. For more information about Bunch object, see <span class="title-ref">\~sklearn.utils.Bunch</span>.

It's also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the `return_X_y` parameter to `True`.

The datasets also contain a full description in their `DESCR` attribute and some contain `feature_names` and `target_names`. See the dataset descriptions below for details.

**The dataset generation functions.** They can be used to generate controlled synthetic datasets, described in the \[sample\_generators\](\#sample\_generators) section.

These functions return a tuple `(X, y)` consisting of a `n_samples` \* `n_features` numpy array `X` and an array of length `n_samples` containing the targets `y`.

In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the \[loading\_other\_datasets\](\#loading\_other\_datasets) section.

<div class="toctree" data-maxdepth="2">

datasets/toy\_dataset datasets/real\_world datasets/sample\_generators datasets/loading\_other\_datasets

</div>

---

advanced_installation.md

---

<div id="advanced-installation">

\<\!-- Failed to include ../min\_dependency\_substitutions.rst --\>

</div>

# Installing the development version of scikit-learn

This section introduces how to install the **main branch** of scikit-learn. This can be done by either installing a nightly build or building from source.

## Installing nightly builds

The continuous integration servers of the scikit-learn project build, test and upload wheel packages for the most recent Python version on a nightly basis.

Installing a nightly build is the quickest way to:

  - try a new feature that will be shipped in the next release (that is, a feature from a pull-request that was recently merged to the main branch);
  - check whether a bug you encountered has been fixed since the last release.

You can install the nightly build of scikit-learn using the <span class="title-ref">scientific-python-nightly-wheels</span> index from the PyPI registry of \`anaconda.org\`:

<div class="prompt">

bash $

pip install --pre --extra-index <https://pypi.anaconda.org/scientific-python-nightly-wheels/simple> scikit-learn

</div>

Note that first uninstalling scikit-learn might be required to be able to install nightly builds of scikit-learn.

## Building from source

Building from source is required to work on a contribution (bug fix, new feature, code or documentation improvement).

<div id="git_repo">

1.  Use [Git](https://git-scm.com/) to check out the latest source from the [scikit-learn repository](https://github.com/scikit-learn/scikit-learn) on Github.:
    
    <div class="prompt">
    
    bash $
    
    git clone <git@github.com>:scikit-learn/scikit-learn.git \# add --depth 1 if your connection is slow cd scikit-learn
    
    </div>
    
    If you plan on submitting a pull-request, you should clone from your fork instead.

2.  Install a recent version of Python (3.9 or later at the time of writing) for instance using [Miniforge3](https://github.com/conda-forge/miniforge#miniforge3). Miniforge provides a conda-based distribution of Python and the most popular scientific libraries.
    
    If you installed Python with conda, we recommend to create a dedicated [conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) with all the build dependencies of scikit-learn (namely [NumPy](https://numpy.org), [SciPy](https://www.scipy.org), [Cython](https://cython.org), [meson-python](https://mesonbuild.com/meson-python) and [Ninja](https://ninja-build.org/)):
    
    <div class="prompt">
    
    bash $
    
    conda create -n sklearn-env -c conda-forge python numpy scipy cython meson-python ninja
    
    </div>
    
    It is not always necessary but it is safer to open a new prompt before activating the newly created conda environment.
    
    <div class="prompt">
    
    bash $
    
    conda activate sklearn-env
    
    </div>

3.  **Alternative to conda:** You can use alternative installations of Python provided they are recent enough (3.9 or higher at the time of writing). Here is an example on how to create a build environment for a Linux system's Python. Build dependencies are installed with <span class="title-ref">pip</span> in a dedicated [virtualenv](https://docs.python.org/3/tutorial/venv.html) to avoid disrupting other Python programs installed on the system:
    
    <div class="prompt">
    
    bash $
    
    python3 -m venv sklearn-env source sklearn-env/bin/activate pip install wheel numpy scipy cython meson-python ninja
    
    </div>

4.  Install a compiler with [OpenMP](https://en.wikipedia.org/wiki/OpenMP) support for your platform. See instructions for \[compiler\_windows\](\#compiler\_windows), \[compiler\_macos\](\#compiler\_macos), \[compiler\_linux\](\#compiler\_linux) and \[compiler\_freebsd\](\#compiler\_freebsd).

5.  Build the project with pip:
    
    <div class="prompt">
    
    bash $
    
    pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true
    
    </div>

6.  Check that the installed scikit-learn has a version number ending with \`.dev0\`:
    
    <div class="prompt">
    
    bash $
    
    python -c "import sklearn; sklearn.show\_versions()"
    
    </div>

7.  Please refer to the \[developers\_guide\](\#developers\_guide) and \[pytest\_tips\](\#pytest\_tips) to run the tests on the module of your choice.

</div>

\> **Note** \> <span class="title-ref">--config-settings editable-verbose=true</span> is optional but recommended to avoid surprises when you import <span class="title-ref">sklearn</span>. <span class="title-ref">meson-python</span> implements editable installs by rebuilding <span class="title-ref">sklearn</span> when executing <span class="title-ref">import sklearn</span>. With the recommended setting you will see a message when this happens, rather than potentially waiting without feed-back and wondering what is taking so long. Bonus: this means you only have to run the <span class="title-ref">pip install</span> command once, <span class="title-ref">sklearn</span> will automatically be rebuilt when importing <span class="title-ref">sklearn</span>.

> Note that <span class="title-ref">--config-settings</span> is only supported in <span class="title-ref">pip</span> version 23.1 or later. To upgrade <span class="title-ref">pip</span> to a compatible version, run <span class="title-ref">pip install -U pip</span>.

### Dependencies

#### Runtime dependencies

Scikit-learn requires the following dependencies both at build time and at runtime:

  - Python (\>= 3.8),
  - NumPy (\>= ),
  - SciPy (\>= ),
  - Joblib (\>= ),
  - threadpoolctl (\>= ).

#### Build dependencies

Building Scikit-learn also requires:

  - Cython \>=

<!-- end list -->

  - \- A C/C++ compiler and a matching [OpenMP](https://en.wikipedia.org/wiki/OpenMP) runtime library. See the  
    \[platform system specific instructions \<platform\_specific\_instructions\>\](\#platform-system-specific-instructions

\--\<platform\_specific\_instructions\>) for more details.

\> **Note** \> If OpenMP is not supported by the compiler, the build will be done with OpenMP functionalities disabled. This is not recommended since it will force some estimators to run in sequential mode instead of leveraging thread-based parallelism. Setting the `SKLEARN_FAIL_NO_OPENMP` environment variable (before cythonization) will force the build to fail if OpenMP is not supported.

Since version 0.21, scikit-learn automatically detects and uses the linear algebra library used by SciPy **at runtime**. Scikit-learn has therefore no build dependency on BLAS/LAPACK implementations such as OpenBlas, Atlas, Blis or MKL.

#### Test dependencies

Running tests requires:

  - pytest \>=

Some tests also require [pandas](https://pandas.pydata.org).

### Building a specific version from a tag

If you want to build a stable version, you can `git checkout <VERSION>` to get the code for that particular version, or download an zip archive of the version from github.

## Platform-specific instructions

Here are instructions to install a working C/C++ compiler with OpenMP support to build scikit-learn Cython extensions for each supported platform.

### Windows

First, download the [Build Tools for Visual Studio 2019 installer](https://aka.ms/vs/17/release/vs_buildtools.exe).

Run the downloaded <span class="title-ref">vs\_buildtools.exe</span> file, during the installation you will need to make sure you select "Desktop development with C++", similarly to this screenshot:

![image](../images/visual-studio-build-tools-selection.png)

Secondly, find out if you are running 64-bit or 32-bit Python. The building command depends on the architecture of the Python interpreter. You can check the architecture by running the following in `cmd` or `powershell` console:

<div class="prompt">

bash $

python -c "import struct; print(struct.calcsize('P') \* 8)"

</div>

For 64-bit Python, configure the build environment by running the following commands in `cmd` or an Anaconda Prompt (if you use Anaconda):

<div class="prompt">

bash C:\>

SET DISTUTILS\_USE\_SDK=1 "C:Program Files (x86)Microsoft Visual Studio2019BuildToolsVCAuxiliaryBuildvcvarsall.bat" x64

</div>

Replace `x64` by `x86` to build for 32-bit Python.

Please be aware that the path above might be different from user to user. The aim is to point to the "vcvarsall.bat" file that will set the necessary environment variables in the current command prompt.

Finally, build scikit-learn with this command prompt:

<div class="prompt">

bash $

pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true

</div>

### macOS

The default C compiler on macOS, Apple clang (confusingly aliased as <span class="title-ref">/usr/bin/gcc</span>), does not directly support OpenMP. We present two alternatives to enable OpenMP support:

  - either install <span class="title-ref">conda-forge::compilers</span> with conda;
  - or install <span class="title-ref">libomp</span> with Homebrew to extend the default Apple clang compiler.

For Apple Silicon M1 hardware, only the conda-forge method below is known to work at the time of writing (January 2021). You can install the <span class="title-ref">macos/arm64</span> distribution of conda using the [miniforge installer](https://github.com/conda-forge/miniforge#miniforge)

#### macOS compilers from conda-forge

If you use the conda package manager (version \>= 4.7), you can install the `compilers` meta-package from the conda-forge channel, which provides OpenMP-enabled C/C++ compilers based on the llvm toolchain.

First install the macOS command line tools:

<div class="prompt">

bash $

xcode-select --install

</div>

It is recommended to use a dedicated [conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) to build scikit-learn from source:

<div class="prompt">

bash $

conda create -n sklearn-dev -c conda-forge python numpy scipy cython joblib threadpoolctl pytest compilers llvm-openmp meson-python ninja

</div>

It is not always necessary but it is safer to open a new prompt before activating the newly created conda environment.

<div class="prompt">

bash $

conda activate sklearn-dev make clean pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true

</div>

\> **Note** \> If you get any conflicting dependency error message, try commenting out any custom conda configuration in the `$HOME/.condarc` file. In particular the `channel_priority: strict` directive is known to cause problems for this setup.

You can check that the custom compilers are properly installed from conda forge using the following command:

<div class="prompt">

bash $

conda list

</div>

which should include `compilers` and `llvm-openmp`.

The compilers meta-package will automatically set custom environment variables:

<div class="prompt">

bash $

echo $CC echo $CXX echo $CFLAGS echo $CXXFLAGS echo $LDFLAGS

</div>

They point to files and folders from your `sklearn-dev` conda environment (in particular in the bin/, include/ and lib/ subfolders). For instance `-L/path/to/conda/envs/sklearn-dev/lib` should appear in `LDFLAGS`.

In the log, you should see the compiled extension being built with the clang and clang++ compilers installed by conda with the `-fopenmp` command line flag.

#### macOS compilers from Homebrew

Another solution is to enable OpenMP support for the clang compiler shipped by default on macOS.

First install the macOS command line tools:

<div class="prompt">

bash $

xcode-select --install

</div>

Install the [Homebrew](https://brew.sh) package manager for macOS.

Install the LLVM OpenMP library:

<div class="prompt">

bash $

brew install libomp

</div>

Set the following environment variables:

<div class="prompt">

bash $

export CC=/usr/bin/clang export CXX=/usr/bin/clang++ export CPPFLAGS="$CPPFLAGS -Xpreprocessor -fopenmp" export CFLAGS="$CFLAGS -I/usr/local/opt/libomp/include" export CXXFLAGS="$CXXFLAGS -I/usr/local/opt/libomp/include" export LDFLAGS="$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib -L/usr/local/opt/libomp/lib -lomp"

</div>

Finally, build scikit-learn in verbose mode (to check for the presence of the `-fopenmp` flag in the compiler commands):

<div class="prompt">

bash $

make clean pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true

</div>

### Linux

#### Linux compilers from the system

Installing scikit-learn from source without using conda requires you to have installed the scikit-learn Python development headers and a working C/C++ compiler with OpenMP support (typically the GCC toolchain).

Install build dependencies for Debian-based operating systems, e.g. Ubuntu:

<div class="prompt">

bash $

sudo apt-get install build-essential python3-dev python3-pip

</div>

then proceed as usual:

<div class="prompt">

bash $

pip3 install cython pip3 install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true

</div>

Cython and the pre-compiled wheels for the runtime dependencies (numpy, scipy and joblib) should automatically be installed in `$HOME/.local/lib/pythonX.Y/site-packages`. Alternatively you can run the above commands from a [virtualenv](https://docs.python.org/3/tutorial/venv.html) or a [conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) to get full isolation from the Python packages installed via the system packager. When using an isolated environment, `pip3` should be replaced by `pip` in the above commands.

When precompiled wheels of the runtime dependencies are not available for your architecture (e.g. ARM), you can install the system versions:

<div class="prompt">

bash $

sudo apt-get install cython3 python3-numpy python3-scipy

</div>

On Red Hat and clones (e.g. CentOS), install the dependencies using:

<div class="prompt">

bash $

sudo yum -y install gcc gcc-c++ python3-devel numpy scipy

</div>

#### Linux compilers from conda-forge

Alternatively, install a recent version of the GNU C Compiler toolchain (GCC) in the user folder using conda:

<div class="prompt">

bash $

conda create -n sklearn-dev -c conda-forge python numpy scipy cython joblib threadpoolctl pytest compilers meson-python ninja

</div>

It is not always necessary but it is safer to open a new prompt before activating the newly created conda environment.

<div class="prompt">

bash $

conda activate sklearn-dev pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true

</div>

### FreeBSD

The clang compiler included in FreeBSD 12.0 and 11.2 base systems does not include OpenMP support. You need to install the <span class="title-ref">openmp</span> library from packages (or ports):

<div class="prompt">

bash $

sudo pkg install openmp

</div>

This will install header files in `/usr/local/include` and libs in `/usr/local/lib`. Since these directories are not searched by default, you can set the environment variables to these locations:

<div class="prompt">

bash $

export CFLAGS="$CFLAGS -I/usr/local/include" export CXXFLAGS="$CXXFLAGS -I/usr/local/include" export LDFLAGS="$LDFLAGS -Wl,-rpath,/usr/local/lib -L/usr/local/lib -lomp"

</div>

Finally, build the package using the standard command:

<div class="prompt">

bash $

pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true

</div>

For the upcoming FreeBSD 12.1 and 11.3 versions, OpenMP will be included in the base system and these steps will not be necessary.

---

bug_triaging.md

---

# Bug triaging and issue curation

The [issue tracker](https://github.com/scikit-learn/scikit-learn/issues) is important to the communication in the project: it helps developers identify major projects to work on, as well as to discuss priorities. For this reason, it is important to curate it, adding labels to issues and closing issues that are not necessary.

## Working on issues to improve them

Improving issues increases their chances of being successfully resolved. Guidelines on submitting good issues can be found \[here \<filing\_bugs\>\](\#here \<filing\_bugs\>). A third party can give useful feedback or even add comments on the issue. The following actions are typically useful:

  - documenting issues that are missing elements to reproduce the problem such as code samples
  - suggesting better use of code formatting
  - suggesting to reformulate the title and description to make them more explicit about the problem to be solved
  - linking to related issues or discussions while briefly describing how they are related, for instance "See also \#xyz for a similar attempt at this" or "See also \#xyz where the same thing happened in SomeEstimator" provides context and helps the discussion.

<div class="topic">

**Fruitful discussions**

Online discussions may be harder than it seems at first glance, in particular given that a person new to open-source may have a very different understanding of the process than a seasoned maintainer.

Overall, it is useful to stay positive and assume good will. [The following article](https://gael-varoquaux.info/programming/technical-discussions-are-hard-a-few-tips.html) explores how to lead online discussions in the context of open source.

</div>

## Working on PRs to help review

Reviewing code is also encouraged. Contributors and users are welcome to participate to the review process following our \[review guidelines \<code\_review\>\](\#review-guidelines \<code\_review\>).

## Triaging operations for members of the core and contributor experience teams

In addition to the above, members of the core team and the contributor experience team can do the following important tasks:

  - Update \[labels for issues and PRs \<issue\_tracker\_tags\>\](\#labels-for-issues-and-prs-\<issue\_tracker\_tags\>): see the list of the [available github labels](https://github.com/scikit-learn/scikit-learn/labels).
  - \[Determine if a PR must be relabeled as stalled \<stalled\_pull\_request\>\](\#determine-if-a-pr-must-be-relabeled-as-stalled-\<stalled\_pull\_request\>) or needs help (this is typically very important in the context of sprints, where the risk is to create many unfinished PRs)
  - If a stalled PR is taken over by a newer PR, then label the stalled PR as "Superseded", leave a comment on the stalled PR linking to the new PR, and likely close the stalled PR.
  - Triage issues:
      - **close usage questions** and politely point the reporter to use Stack Overflow instead.
      - **close duplicate issues**, after checking that they are indeed duplicate. Ideally, the original submitter moves the discussion to the older, duplicate issue
      - **close issues that cannot be replicated**, after leaving time (at least a week) to add extra information

\[Saved replies \<saved\_replies\>\](\#saved-replies-\<saved\_replies\>) are useful to gain time and yet be welcoming and polite when triaging.

See the github description for [roles in the organization](https://docs.github.com/en/github/setting-up-and-managing-organizations-and-teams/repository-permission-levels-for-an-organization).

<div class="topic">

**Closing issues: a tough call**

When uncertain on whether an issue should be closed or not, it is best to strive for consensus with the original poster, and possibly to seek relevant expertise. However, when the issue is a usage question, or when it has been considered as unclear for many years it should be closed.

</div>

## A typical workflow for triaging issues

The following workflow\[1\] is a good way to approach issue triaging:

1.  Thank the reporter for opening an issue
    
    The issue tracker is many people's first interaction with the scikit-learn project itself, beyond just using the library. As such, we want it to be a welcoming, pleasant experience.

2.  Is this a usage question? If so close it with a polite message (\[here is an example \<saved\_replies\>\](\#here-is-an-example-\<saved\_replies\>)).

3.  Is the necessary information provided?
    
    If crucial information (like the version of scikit-learn used), is missing feel free to ask for that and label the issue with "Needs info".

4.  Is this a duplicate issue?
    
    We have many open issues. If a new issue seems to be a duplicate, point to the original issue. If it is a clear duplicate, or consensus is that it is redundant, close it. Make sure to still thank the reporter, and encourage them to chime in on the original issue, and perhaps try to fix it.
    
    If the new issue provides relevant information, such as a better or slightly different example, add it to the original issue as a comment or an edit to the original post.

5.  Make sure that the title accurately reflects the issue. If you have the necessary permissions edit it yourself if it's not clear.

6.  Is the issue minimal and reproducible?
    
    For bug reports, we ask that the reporter provide a minimal reproducible example. See [this useful post](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) by Matthew Rocklin for a good explanation. If the example is not reproducible, or if it's clearly not minimal, feel free to ask the reporter if they can provide and example or simplify the provided one. Do acknowledge that writing minimal reproducible examples is hard work. If the reporter is struggling, you can try to write one yourself.
    
    If a reproducible example is provided, but you see a simplification, add your simpler reproducible example.

7.  Add the relevant labels, such as "Documentation" when the issue is about documentation, "Bug" if it is clearly a bug, "Enhancement" if it is an enhancement request, ...
    
    If the issue is clearly defined and the fix seems relatively straightforward, label the issue as “Good first issue”.
    
    An additional useful step can be to tag the corresponding module e.g. <span class="title-ref">sklearn.linear\_models</span> when relevant.

8.  Remove the "Needs Triage" label from the issue if the label exists.

<!-- end list -->

1.  Adapted from the pandas project [maintainers guide](https://pandas.pydata.org/docs/development/maintaining.html)

---

contributing.md

---

# Contributing

<div class="currentmodule">

sklearn

</div>

This project is a community effort, and everyone is welcome to contribute. It is hosted on <https://github.com/scikit-learn/scikit-learn>. The decision making process and governance structure of scikit-learn is laid out in \[governance\](\#governance).

Scikit-learn is somewhat \[selective \<selectiveness\>\](\#selective-\<selectiveness\>) when it comes to adding new algorithms, and the best way to contribute and to help the project is to start working on known issues. See \[new\_contributors\](\#new\_contributors) to get started.

<div class="topic">

****Our community, our values****

We are a community based on openness and friendly, didactic, discussions.

We aspire to treat everybody equally, and value their contributions. We are particularly seeking people from underrepresented backgrounds in Open Source Software and scikit-learn in particular to participate and contribute their expertise and experience.

Decisions are made based on technical merit and consensus.

Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions.

We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: <https://www.python.org/psf/codeofconduct/>

</div>

In case you experience issues using this package, do not hesitate to submit a ticket to the [GitHub issue tracker](https://github.com/scikit-learn/scikit-learn/issues). You are also welcome to post feature requests or pull requests.

## Ways to contribute

There are many ways to contribute to scikit-learn, with the most common ones being contribution of code or documentation to the project. Improving the documentation is no less important than improving the library itself. If you find a typo in the documentation, or have made improvements, do not hesitate to create a GitHub issue or preferably submit a GitHub pull request. Full documentation can be found under the doc/ directory.

But there are many other ways to help. In particular helping to \[improve, triage, and investigate issues \<bug\_triaging\>\](\#improve,-triage,-and-investigate-issues-\<bug\_triaging\>) and \[reviewing other developers' pull requests \<code\_review\>\](\#reviewing-other-developers'-pull-requests-\<code\_review\>) are very valuable contributions that decrease the burden on the project maintainers.

Another way to contribute is to report issues you're facing, and give a "thumbs up" on issues that others reported and that are relevant to you. It also helps us if you spread the word: reference the project from your blog and articles, link to it from your website, or simply star to say "I use it":

<p>
  <object
    data="https://img.shields.io/github/stars/scikit-learn/scikit-learn?style=for-the-badge&logo=github"
    type="image/svg+xml">
  </object>
</p>

In case a contribution/issue involves changes to the API principles or changes to dependencies or supported versions, it must be backed by a \[slep\](\#slep), where a SLEP must be submitted as a pull-request to [enhancement proposals](https://scikit-learn-enhancement-proposals.readthedocs.io) using the [SLEP template](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep_template.html) and follows the decision-making process outlined in \[governance\](\#governance).

<div class="dropdown">

Contributing to related projects

Scikit-learn thrives in an ecosystem of several related projects, which also may have relevant issues to work on, including smaller projects such as:

  - [scikit-learn-contrib](https://github.com/search?q=org%3Ascikit-learn-contrib+is%3Aissue+is%3Aopen+sort%3Aupdated-desc&type=Issues)
  - [joblib](https://github.com/joblib/joblib/issues)
  - [sphinx-gallery](https://github.com/sphinx-gallery/sphinx-gallery/issues)
  - [numpydoc](https://github.com/numpy/numpydoc/issues)
  - [liac-arff](https://github.com/renatopp/liac-arff/issues)

and larger projects:

  - [numpy](https://github.com/numpy/numpy/issues)
  - [scipy](https://github.com/scipy/scipy/issues)
  - [matplotlib](https://github.com/matplotlib/matplotlib/issues)
  - and so on.

Look for issues marked "help wanted" or similar. Helping these projects may help scikit-learn too. See also \[related\_projects\](\#related\_projects).

</div>

## Automated Contributions Policy

Please refrain from submitting issues or pull requests generated by fully-automated tools. Maintainers reserve the right, at their sole discretion, to close such submissions and to block any account responsible for them.

Ideally, contributions should follow from a human-to-human discussion in the form of an issue.

## Submitting a bug report or a feature request

We use GitHub issues to track all bugs and feature requests; feel free to open an issue if you have found a bug or wish to see a feature implemented.

In case you experience issues using this package, do not hesitate to submit a ticket to the [Bug Tracker](https://github.com/scikit-learn/scikit-learn/issues). You are also welcome to post feature requests or pull requests.

It is recommended to check that your issue complies with the following rules before submitting:

  - Verify that your issue is not being currently addressed by other [issues](https://github.com/scikit-learn/scikit-learn/issues?q=) or [pull requests](https://github.com/scikit-learn/scikit-learn/pulls?q=).
  - If you are submitting an algorithm or feature request, please verify that the algorithm fulfills our [new algorithm requirements](https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms).
  - If you are submitting a bug report, we strongly encourage you to follow the guidelines in \[filing\_bugs\](\#filing\_bugs).

### How to make a good bug report

When you submit an issue to [GitHub](https://github.com/scikit-learn/scikit-learn/issues), please do your best to follow these guidelines\! This will make it a lot easier to provide you with good feedback:

  - \- The ideal bug report contains a \[short reproducible code snippet  
    \<minimal\_reproducer\>\](\#short-reproducible-code-snippet

  - \--\<minimal\_reproducer\>), this way anyone can try to reproduce the bug easily. If your  
    snippet is longer than around 50 lines, please link to a [Gist](https://gist.github.com) or a GitHub repo.

  - \- If not feasible to include a reproducible snippet, please be specific about  
    what **estimators and/or functions are involved and the shape of the data**.

<!-- end list -->

  - If an exception is raised, please **provide the full traceback**.

  - Please include your **operating system type and version number**, as well as your **Python, scikit-learn, numpy, and scipy versions**. This information can be found by running:
    
    <div class="prompt">
    
    bash
    
    python -c "import sklearn; sklearn.show\_versions()"
    
    </div>

  - Please ensure all **code snippets and error messages are formatted in appropriate code blocks**. See [Creating and highlighting code blocks](https://help.github.com/articles/creating-and-highlighting-code-blocks) for more details.

If you want to help curate issues, read about \[bug\_triaging\](\#bug\_triaging).

## Contributing code

\> **Note** \> To avoid duplicating work, it is highly advised that you search through the [issue tracker](https://github.com/scikit-learn/scikit-learn/issues) and the [PR list](https://github.com/scikit-learn/scikit-learn/pulls). If in doubt about duplicated work, or if you want to work on a non-trivial feature, it's recommended to first open an issue in the [issue tracker](https://github.com/scikit-learn/scikit-learn/issues) to get some feedbacks from core developers.

> One easy way to find an issue to work on is by applying the "help wanted" label in your search. This lists all the issues that have been unclaimed so far. In order to claim an issue for yourself, please comment exactly `/take` on it for the CI to automatically assign the issue to you.

To maintain the quality of the codebase and ease the review process, any contribution must conform to the project's \[coding guidelines \<coding-guidelines\>\](\#coding-guidelines \<coding-guidelines\>), in particular:

  - Don't modify unrelated lines to keep the PR focused on the scope stated in its description or issue.
  - Only write inline comments that add value and avoid stating the obvious: explain the "why" rather than the "what".
  - **Most importantly**: Do not contribute code that you don't understand.

### Video resources

These videos are step-by-step introductions on how to contribute to scikit-learn, and are a great companion to the following text guidelines. Please make sure to still check our guidelines below, since they describe our latest up-to-date workflow.

  - Crash Course in Contributing to Scikit-Learn & Open Source Projects: [Video](https://youtu.be/5OL8XoMMOfA), [Transcript](https://github.com/data-umbrella/event-transcripts/blob/main/2020/05-andreas-mueller-contributing.md)
  - Example of Submitting a Pull Request to scikit-learn: [Video](https://youtu.be/PU1WyDPGePI), [Transcript](https://github.com/data-umbrella/event-transcripts/blob/main/2020/06-reshama-shaikh-sklearn-pr.md)
  - Sprint-specific instructions and practical tips: [Video](https://youtu.be/p_2Uw2BxdhA), [Transcript](https://github.com/data-umbrella/data-umbrella-scikit-learn-sprint/blob/master/3_transcript_ACM_video_vol2.md)
  - 3 Components of Reviewing a Pull Request: [Video](https://youtu.be/dyxS9KKCNzA), [Transcript](https://github.com/data-umbrella/event-transcripts/blob/main/2021/27-thomas-pr.md)

<div class="note">

<div class="title">

Note

</div>

In January 2021, the default branch name changed from `master` to `main` for the scikit-learn GitHub repository to use more inclusive terms. These videos were created prior to the renaming of the branch. For contributors who are viewing these videos to set up their working environment and submitting a PR, `master` should be replaced to `main`.

</div>

### How to contribute

The preferred way to contribute to scikit-learn is to fork the [main repository](https://github.com/scikit-learn/scikit-learn/) on GitHub, then submit a "pull request" (PR).

In the first few steps, we explain how to locally install scikit-learn, and how to set up your git repository:

1.  [Create an account](https://github.com/join) on GitHub if you do not already have one.

2.  Fork the [project repository](https://github.com/scikit-learn/scikit-learn): click on the 'Fork' button near the top of the page. This creates a copy of the code under your account on the GitHub user account. For more details on how to fork a repository see [this guide](https://help.github.com/articles/fork-a-repo/).

3.  Clone your fork of the scikit-learn repo from your GitHub account to your local disk:
    
    <div class="prompt">
    
    bash
    
    git clone <git@github.com>:YourLogin/scikit-learn.git \# add --depth 1 if your connection is slow cd scikit-learn
    
    </div>

4.  Follow steps 2-6 in \[install\_bleeding\_edge\](\#install\_bleeding\_edge) to build scikit-learn in development mode and return to this document.

5.  Install the development dependencies:
    
    <div class="prompt">
    
    bash
    
    pip install pytest pytest-cov ruff mypy numpydoc black==24.3.0
    
    </div>

<div id="upstream">

6.  Add the `upstream` remote. This saves a reference to the main scikit-learn repository, which you can use to keep your repository synchronized with the latest changes:
    
    <div class="prompt">
    
    bash
    
    git remote add upstream <git@github.com>:scikit-learn/scikit-learn.git
    
    </div>

7.  Check that the <span class="title-ref">upstream</span> and <span class="title-ref">origin</span> remote aliases are configured correctly by running <span class="title-ref">git remote -v</span> which should display:
    
      - \`\`\`text  
        origin <git@github.com>:YourLogin/scikit-learn.git (fetch) origin <git@github.com>:YourLogin/scikit-learn.git (push) upstream <git@github.com>:scikit-learn/scikit-learn.git (fetch) upstream <git@github.com>:scikit-learn/scikit-learn.git (push)

</div>

You should now have a working installation of scikit-learn, and your git repository `` ` properly configured. It could be useful to run some test to verify your installation. Please refer to [pytest_tips](#pytest_tips) for examples.  The next steps now describe the process of modifying code and submitting a PR:  8. Synchronize your ``main`branch with the`upstream/main``branch,    more details on `GitHub Docs <https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork>`_:     .. prompt:: bash          git checkout main         git fetch upstream         git merge upstream/main  9. Create a feature branch to hold your development changes:     .. prompt:: bash          git checkout -b my_feature     and start making changes. Always use a feature branch. It's good    practice to never work on the``main``branch!  10. (**Optional**) Install `pre-commit <https://pre-commit.com/#install>`_ to     run code style checks before each commit:      .. prompt:: bash            pip install pre-commit           pre-commit install      pre-commit checks can be disabled for a particular commit with     `git commit -n`.  11. Develop the feature on your feature branch on your computer, using Git to     do the version control. When you're done editing, add changed files using``git add`and then`git commit``:      .. prompt:: bash          git add modified_files         git commit      to record your changes in Git, then push the changes to your GitHub     account with:      .. prompt:: bash         git push -u origin my_feature  12. Follow `these     <https://help.github.com/articles/creating-a-pull-request-from-a-fork>`_     instructions to create a pull request from your fork. This will send an     notification to potential reviewers. You may want to consider sending an message to     the `discord <https://discord.com/invite/h9qyrK8Jc8>`_ in the development     channel for more visibility if your pull request does not receive attention after     a couple of days (instant replies are not guaranteed though).  It is often helpful to keep your local feature branch synchronized with the latest changes of the main scikit-learn repository:  .. prompt:: bash      git fetch upstream     git merge upstream/main  Subsequently, you might need to solve the conflicts. You can refer to the `Git documentation related to resolving merge conflict using the command line <https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/>`_.  .. topic:: Learning Git      The `Git documentation <https://git-scm.com/doc>`_ and     http://try.github.io are excellent resources to get started with git,     and understanding all of the commands shown here.  .. _pr_checklist:  Pull request checklist ----------------------  Before a PR can be merged, it needs to be approved by two core developers. An incomplete contribution -- where you expect to do more work before receiving a full review -- should be marked as a `draft pull request <https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/changing-the-stage-of-a-pull-request>`__ and changed to "ready for review" when it matures. Draft PRs may be useful to: indicate you are working on something to avoid duplicated work, request broad review of functionality or API, or seek collaborators. Draft PRs often benefit from the inclusion of a `task list <https://github.com/blog/1375-task-lists-in-gfm-issues-pulls-comments>`_ in the PR description.  In order to ease the reviewing process, we recommend that your contribution complies with the following rules before marking a PR as "ready for review". The **bolded** ones are especially important:  1. **Give your pull request a helpful title** that summarizes what your    contribution does. This title will often become the commit message once    merged so it should summarize your contribution for posterity. In some    cases "Fix <ISSUE TITLE>" is enough. "Fix #<ISSUE NUMBER>" is never a    good title.  2. **Make sure your code passes the tests**. The whole test suite can be run    with `pytest`, but it is usually not recommended since it takes a long    time. It is often enough to only run the test related to your changes:    for example, if you changed something in    `sklearn/linear_model/_logistic.py`, running the following commands will    usually be enough:     - `pytest sklearn/linear_model/_logistic.py` to make sure the doctest      examples are correct    - `pytest sklearn/linear_model/tests/test_logistic.py` to run the tests      specific to the file    - `pytest sklearn/linear_model` to test the whole      :mod:`~sklearn.linear_model` module    - `pytest doc/modules/linear_model.rst` to make sure the user guide      examples are correct.    - `pytest sklearn/tests/test_common.py -k LogisticRegression` to run all our      estimator checks (specifically for `LogisticRegression`, if that's the      estimator you changed).     There may be other failing tests, but they will be caught by the CI so    you don't need to run the whole test suite locally. For guidelines on how    to use``pytest``efficiently, see the [pytest_tips](#pytest_tips).  3. **Make sure your code is properly commented and documented**, and **make    sure the documentation renders properly**. To build the documentation, please    refer to our [contribute_documentation](#contribute_documentation) guidelines. The CI will also    build the docs: please refer to [generated_doc_CI](#generated_doc_ci).  4. **Tests are necessary for enhancements to be    accepted**. Bug-fixes or new features should be provided with    `non-regression tests    <https://en.wikipedia.org/wiki/Non-regression_testing>`_. These tests    verify the correct behavior of the fix or feature. In this manner, further    modifications on the code base are granted to be consistent with the    desired behavior. In the case of bug fixes, at the time of the PR, the    non-regression tests should fail for the code base in the``main``branch    and pass for the PR code.  5. If your PR is likely to affect users, you need to add a changelog entry describing    your PR changes, see the `following README <https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md>`    for more details.  6. Follow the [coding-guidelines](#coding-guidelines).  7. When applicable, use the validation tools and scripts in the :mod:`sklearn.utils`    module. A list of utility routines available for developers can be found in the    [developers-utils](#developers-utils) page.  8. Often pull requests resolve one or more other issues (or pull requests).    If merging your pull request means that some other issues/PRs should    be closed, you should `use keywords to create link to them    <https://github.com/blog/1506-closing-issues-via-pull-requests/>`_    (e.g.,``Fixes \#1234`; multiple issues/PRs are allowed as long as each    one is preceded by a keyword). Upon merging, those issues/PRs will    automatically be closed by GitHub. If your pull request is simply    related to some other issues/PRs, or it only partially resolves the target    issue, create a link to them without using the keywords (e.g.,`Towards \#1234``).  9. PRs should often substantiate the change, through benchmarks of    performance and efficiency (see [monitoring_performances](#monitoring_performances)) or through    examples of usage. Examples also illustrate the features and intricacies of    the library to users. Have a look at other examples in the `examples/    <https://github.com/scikit-learn/scikit-learn/tree/main/examples>`_    directory for reference. Examples should demonstrate why the new    functionality is useful in practice and, if possible, compare it to other    methods available in scikit-learn.  10. New features have some maintenance overhead. We expect PR authors     to take part in the maintenance for the code they submit, at least     initially. New features need to be illustrated with narrative     documentation in the user guide, with small code snippets.     If relevant, please also add references in the literature, with PDF links     when possible.  11. The user guide should also include expected time and space complexity     of the algorithm and scalability, e.g. "this algorithm can scale to a     large number of samples > 100000, but does not scale in dimensionality:     `n_features` is expected to be lower than 100".  You can also check our [code_review](#code_review) to get an idea of what reviewers will expect.  You can check for common programming errors with the following tools:  * Code with a good unit test coverage (at least 80%, better 100%), check with:    .. prompt:: bash      pip install pytest pytest-cov     pytest --cov sklearn path/to/tests    See also [testing_coverage](#testing_coverage).  * Run static analysis with `mypy`:    .. prompt:: bash        mypy sklearn    This must not produce new errors in your pull request. Using `# type: ignore`   annotation can be a workaround for a few cases that are not supported by   mypy, in particular,    - when importing C or Cython modules,   - on properties with decorators.  Bonus points for contributions that include a performance analysis with a benchmark script and profiling output (see [monitoring_performances](#monitoring_performances)). Also check out the [performance-howto](#performance-howto) guide for more details on profiling and Cython optimizations.  > **Note** >    The current state of the scikit-learn code base is not compliant with   all of those guidelines, but we expect that enforcing those constraints   on all new contributions will get the overall code base quality in the   right direction.  .. seealso::     For two very well documented and more detailed guides on development    workflow, please pay a visit to the `Scipy Development Workflow    <http://scipy.github.io/devdocs/dev/dev_quickstart.html>`_ -    and the `Astropy Workflow for Developers    <https://astropy.readthedocs.io/en/latest/development/workflow/development_workflow.html>`_    sections.  Continuous Integration (CI) ---------------------------  * Azure pipelines are used for testing scikit-learn on Linux, Mac and Windows,   with different dependencies and settings. * CircleCI is used to build the docs for viewing. * Github Actions are used for various tasks, including building wheels and   source distributions. * Cirrus CI is used to build on ARM.  .. _commit_markers:  Commit message markers ^^^^^^^^^^^^^^^^^^^^^^  Please note that if one of the following markers appear in the latest commit message, the following actions are taken.  ====================== =================== Commit Message Marker  Action Taken by CI ====================== =================== [ci skip]              CI is skipped completely [cd build]             CD is run (wheels and source distribution are built) [cd build gh]          CD is run only for GitHub Actions [cd build cirrus]      CD is run only for Cirrus CI [lint skip]            Azure pipeline skips linting [scipy-dev]            Build & test with our dependencies (numpy, scipy, etc.) development builds [free-threaded]        Build & test with CPython 3.13 free-threaded [pyodide]              Build & test with Pyodide [azure parallel]       Run Azure CI jobs in parallel [cirrus arm]           Run Cirrus CI ARM test [float32]              Run float32 tests by setting `SKLEARN_RUN_FLOAT32_TESTS=1`. See [environment_variable](#environment_variable) for more details [doc skip]             Docs are not built [doc quick]            Docs built, but excludes example gallery plots [doc build]            Docs built including example gallery plots (very long) ====================== ===================  Note that, by default, the documentation is built but only the examples that are directly modified by the pull request are executed.  Lock files ^^^^^^^^^^  CIs use lock files to build environments with specific versions of dependencies. When a PR needs to modify the dependencies or their versions, the lock files should be updated accordingly. This can be done by commenting in the PR:``\`text @scikit-learn-bot update lock-files

A bot will push a commit to your PR branch with the updated lock files in a few minutes. `` ` Make sure to tick the *Allow edits from maintainers* checkbox located at the bottom of the right sidebar of the PR. You can also specify the options `--select-build`, `--skip-build`, and `--select-tag` as in a command line. Use `--help` on the script `build_tools/update_environments_and_lock_files.py` for more information. For example, ``\`text @scikit-learn-bot update lock-files --select-tag main-ci --skip-build doc

The bot will automatically add \[commit message markers \<commit\_markers\>\](\#commit-message-markers-\<commit\_markers\>) to the `` ` commit for certain tags. If you want to add more markers manually, you can do so using the `--commit-marker` option. For example, the following comment will trigger the bot to update documentation-related lock files and add the `[doc build]` marker to the commit: ``\`text @scikit-learn-bot update lock-files --select-build doc --commit-marker "\[doc build\]"

<div id="stalled_pull_request">

Stalled pull requests `` ` ---------------------  As contributing a feature can be a lengthy process, some pull requests appear inactive but unfinished. In such a case, taking them over is a great service for the project. A good etiquette to take over is:  * **Determine if a PR is stalled**    * A pull request may have the label "stalled" or "help wanted" if we     have already identified it as a candidate for other contributors.    * To decide whether an inactive PR is stalled, ask the contributor if     she/he plans to continue working on the PR in the near future.     Failure to respond within 2 weeks with an activity that moves the PR     forward suggests that the PR is stalled and will result in tagging     that PR with "help wanted".      Note that if a PR has received earlier comments on the contribution     that have had no reply in a month, it is safe to assume that the PR     is stalled and to shorten the wait time to one day.      After a sprint, follow-up for un-merged PRs opened during sprint will     be communicated to participants at the sprint, and those PRs will be     tagged "sprint". PRs tagged with "sprint" can be reassigned or     declared stalled by sprint leaders.  * **Taking over a stalled PR**: To take over a PR, it is important to   comment on the stalled PR that you are taking over and to link from the   new PR to the old one. The new PR should be created by pulling from the   old one.  Stalled and Unclaimed Issues ----------------------------  Generally speaking, issues which are up for grabs will have a `"help wanted" <https://github.com/scikit-learn/scikit-learn/labels/help%20wanted>`_. tag. However, not all issues which need contributors will have this tag, as the "help wanted" tag is not always up-to-date with the state of the issue. Contributors can find issues which are still up for grabs using the following guidelines:  * First, to **determine if an issue is claimed**:    * Check for linked pull requests   * Check the conversation to see if anyone has said that they're working on     creating a pull request  * If a contributor comments on an issue to say they are working on it,   a pull request is expected within 2 weeks (new contributor) or 4 weeks   (contributor or core dev), unless an larger time frame is explicitly given.   Beyond that time, another contributor can take the issue and make a   pull request for it. We encourage contributors to comment directly on the   stalled or unclaimed issue to let community members know that they will be   working on it.  * If the issue is linked to a [stalled pull request <stalled_pull_request>](#stalled-pull-request-<stalled_pull_request>),   we recommend that contributors follow the procedure   described in the [stalled_pull_request](#stalled_pull_request)   section rather than working directly on the issue.  .. _new_contributors:  Issues for New Contributors ---------------------------  New contributors should look for the following tags when looking for issues.  We strongly recommend that new contributors tackle "easy" issues first: this helps the contributor become familiar with the contribution workflow, and for the core devs to become acquainted with the contributor; besides which, we frequently underestimate how easy an issue is to solve!  - **Good first issue tag**    A great way to start contributing to scikit-learn is to pick an item from   the list of `good first issues   <https://github.com/scikit-learn/scikit-learn/labels/good%20first%20issue>`_   in the issue tracker. Resolving these issues allow you to start contributing   to the project without much prior knowledge. If you have already contributed   to scikit-learn, you should look at Easy issues instead.  - **Easy tag**    If you have already contributed to scikit-learn, another great way to contribute   to scikit-learn is to pick an item from the list of `Easy issues   <https://github.com/scikit-learn/scikit-learn/labels/Easy>`_ in the issue   tracker. Your assistance in this area will be greatly appreciated by the   more experienced developers as it helps free up their time to concentrate on   other issues.  - **Help wanted tag**    We often use the help wanted tag to mark issues regardless of difficulty.   Additionally, we use the help wanted tag to mark Pull Requests which have been   abandoned by their original contributor and are available for someone to pick up where   the original contributor left off. The list of issues with the help wanted tag can be   found `here <https://github.com/scikit-learn/scikit-learn/labels/help%20wanted>`_.   Note that not all issues which need contributors will have this tag.  .. _contribute_documentation:  Documentation =============  We are glad to accept any sort of documentation:  * **Function/method/class docstrings:** Also known as "API documentation", these   describe what the object does and details any parameters, attributes and   methods. Docstrings live alongside the code in `sklearn/   <https://github.com/scikit-learn/scikit-learn/tree/main/sklearn>`_, and are generated   generated according to `doc/api_reference.py   <https://github.com/scikit-learn/scikit-learn/blob/main/doc/api_reference.py>`_. To   add, update, remove, or deprecate a public API that is listed in [api_ref](#api_ref), this   is the place to look at. * **User guide:** These provide more detailed information about the algorithms   implemented in scikit-learn and generally live in the root   `doc/ <https://github.com/scikit-learn/scikit-learn/tree/main/doc>`_ directory   and   `doc/modules/ <https://github.com/scikit-learn/scikit-learn/tree/main/doc/modules>`_. * **Examples:** These provide full code examples that may demonstrate the use   of scikit-learn modules, compare different algorithms or discuss their   interpretation, etc. Examples live in   `examples/ <https://github.com/scikit-learn/scikit-learn/tree/main/examples>`_. * **Other reStructuredText documents:** These provide various other useful information   (e.g., the [contributing](#contributing) guide) and live in   `doc/ <https://github.com/scikit-learn/scikit-learn/tree/main/doc>`_.   .. dropdown:: Guidelines for writing docstrings    * When documenting the parameters and attributes, here is a list of some     well-formatted examples ``\`text n\_clusters : int, default=3 The number of clusters detected by the algorithm.

</div>

>   - some\_param : {"hello", "goodbye"}, bool or int, default=True  
>     The parameter description goes here, which can be either a string literal (either <span class="title-ref">hello</span> or <span class="title-ref">goodbye</span>), a bool, or an int. The default value is True.
> 
>   - array\_parameter : {array-like, sparse matrix} of shape (n\_samples, n\_features) or (n\_samples,)  
>     This parameter accepts data in either of the mentioned forms, with one of the mentioned shapes. The default value is <span class="title-ref">np.ones(shape=(n\_samples,))</span>.
> 
> list\_param : list of int
> 
> typed\_ndarray : ndarray of shape (n\_samples,), dtype=np.int32
> 
> sample\_weight : array-like of shape (n\_samples,), default=None
> 
> multioutput\_array : ndarray of shape (n\_samples, n\_classes) or list of such arrays
> 
> In general have the following in mind:
> 
>   - Use Python basic types. (`bool` instead of `boolean`)
> 
> \* Use parenthesis for defining shapes: `array-like of shape (n_samples,)` or `array-like of shape (n_samples, n_features)` \* For strings with multiple options, use brackets: `input: {'log', 'squared', 'multinomial'}` \* 1D or 2D data can be a subset of `{array-like, ndarray, sparse matrix, dataframe}`. Note that `array-like` can also be a `list`, while `ndarray` is explicitly only a `numpy.ndarray`. \* Specify `dataframe` when "frame-like" features are being used, such as the column names. \* When specifying the data type of a list, use `of` as a delimiter: `list of int`. When the parameter supports arrays giving details about the shape and/or data type and a list of such arrays, you can use one of `array-like of shape (n_samples,) or list of such arrays`. \* When specifying the dtype of an ndarray, use e.g. `dtype=np.int32` after defining the shape: `ndarray of shape (n_samples,), dtype=np.int32`. You can specify multiple dtype as a set: `array-like of shape (n_samples,), dtype={np.float64, np.float32}`. If one wants to mention arbitrary precision, use <span class="title-ref">integral</span> and <span class="title-ref">floating</span> rather than the Python dtype <span class="title-ref">int</span> and <span class="title-ref">float</span>. When both <span class="title-ref">int</span> and <span class="title-ref">floating</span> are supported, there is no need to specify the dtype. \* When the default is `None`, `None` only needs to be specified at the end with `default=None`. Be sure to include in the docstring, what it means for the parameter or attribute to be `None`.
> 
>   - Add "See Also" in docstrings for related classes/functions.
> 
> \* "See Also" in docstrings should be one line per reference, with a colon and an explanation, for example:
> 
> ``` text
> ```
> 
> ### See Also
> 
> SelectKBest : Select features based on the k highest scores. SelectFpr : Select features based on a false positive rate test.
> 
>   - Add one or two snippets of code in "Example" section to show how it can be used.

<div class="dropdown">

Guidelines for writing the user guide and other reStructuredText documents

It is important to keep a good compromise between mathematical and algorithmic details, and give intuition to the reader on what the algorithm does.

  - Begin with a concise, hand-waving explanation of what the algorithm/code does on the data.

  - Highlight the usefulness of the feature and its recommended application. Consider including the algorithm's complexity (\(O\left(g\left(n\right)\right)\)) if available, as "rules of thumb" can be very machine-dependent. Only if those complexities are not available, then rules of thumb may be provided instead.

  - Incorporate a relevant figure (generated from an example) to provide intuitions.

  - Include one or two short code examples to demonstrate the feature's usage.

  - Introduce any necessary mathematical equations, followed by references. By deferring the mathematical aspects, the documentation becomes more accessible to users primarily interested in understanding the feature's practical implications rather than its underlying mechanics.

  - When editing reStructuredText (`.rst`) files, try to keep line length under 88 characters when possible (exceptions include links and tables).

  - In scikit-learn reStructuredText files both single and double backticks surrounding text will render as inline literal (often used for code, e.g., <span class="title-ref">list</span>). This is due to specific configurations we have set. Single backticks should be used nowadays.

  - Too much information makes it difficult for users to access the content they are interested in. Use dropdowns to factorize it by using the following syntax
    
    ``` rst
    .. dropdown:: Dropdown title
    
      Dropdown content.
    ```
    
    The snippet above will result in the following dropdown:
    
    <div class="dropdown">
    
    Dropdown title
    
    Dropdown content.
    
    </div>

  - Information that can be hidden by default using dropdowns is:
    
      - low hierarchy sections such as <span class="title-ref">References</span>, <span class="title-ref">Properties</span>, etc. (see for instance the subsections in \[det\_curve\](\#det\_curve));
      - in-depth mathematical details;
      - narrative that is use-case specific;
      - in general, narrative that may only interest users that want to go beyond the pragmatics of a given tool.

  - Do not use dropdowns for the low level section <span class="title-ref">Examples</span>, as it should stay visible to all users. Make sure that the <span class="title-ref">Examples</span> section comes right after the main discussion with the least possible folded section in-between.

  - Be aware that dropdowns break cross-references. If that makes sense, hide the reference along with the text mentioning it. Else, do not use dropdown.

</div>

<div class="dropdown">

Guidelines for writing references

  - When bibliographic references are available with [arxiv](https://arxiv.org/) or [Digital Object Identifier](https://www.doi.org/) identification numbers, use the sphinx directives <span class="title-ref">:arxiv:</span> or <span class="title-ref">:doi:</span>. For example, see references in \[Spectral Clustering Graphs \<spectral\_clustering\_graph\>\](\#spectral-clustering-graphs-\<spectral\_clustering\_graph\>).
  - For the "References" section in docstrings, see <span class="title-ref">sklearn.metrics.silhouette\_score</span> as an example.
  - To cross-reference to other pages in the scikit-learn documentation use the reStructuredText cross-referencing syntax:
      - **Section:** to link to an arbitrary section in the documentation, use reference labels (see [Sphinx docs](https://www.sphinx-doc.org/en/master/usage/restructuredtext/roles.html#ref-role)). For example:
        
        ``` rst
        .. _my-section:
        
        My section
        ----------
        
        This is the text of the section.
        
        To refer to itself use [my-section](#my-section).
        ```
        
        You should not modify existing sphinx reference labels as this would break existing cross references and external links pointing to specific sections in the scikit-learn documentation.
    
      - **Glossary:** linking to a term in the \[glossary\](\#glossary):
        
        ``` rst
        :term:`cross_validation`
        ```
    
      - **Function:** to link to the documentation of a function, use the full import path to the function:
        
        ``` rst
        `~sklearn.model_selection.cross_val_score`
        ```
        
        However, if there is a <span class="title-ref">.. currentmodule::</span> directive above you in the document, you will only need to use the path to the function succeeding the current module specified. For example:
        
        ``` rst
        .. currentmodule:: sklearn.model_selection
        
        `cross_val_score`
        ```
    
      - **Class:** to link to documentation of a class, use the full import path to the class, unless there is a <span class="title-ref">.. currentmodule::</span> directive in the document above (see above):
        
        ``` rst
        `~sklearn.preprocessing.StandardScaler`
        ```

</div>

You can edit the documentation using any text editor, and then generate the `` ` HTML output by following [building_documentation](#building_documentation). The resulting HTML files will be placed in ``\_build/html/`and are viewable in a web browser, for instance by opening the local`\_build/html/index.html`file or by running a local server  .. prompt:: bash    python -m http.server -d _build/html   .. _building_documentation:  Building the documentation --------------------------  **Before submitting a pull request check if your modifications have introduced new sphinx warnings by building the documentation locally and try to fix them.**  First, make sure you have [properly installed <install_bleeding_edge>](#properly-installed-<install_bleeding_edge>) the development version. On top of that, building the documentation requires installing some additional packages:  ..     packaging is not needed once setuptools starts shipping packaging>=17.0  .. prompt:: bash      pip install sphinx sphinx-gallery numpydoc matplotlib Pillow pandas \                 polars scikit-image packaging seaborn sphinx-prompt \                 sphinxext-opengraph sphinx-copybutton plotly pooch \                 pydata-sphinx-theme sphinxcontrib-sass sphinx-design \                 sphinx-remove-toctrees  To build the documentation, you need to be in the`doc`folder:  .. prompt:: bash      cd doc  In the vast majority of cases, you only need to generate the web site without the example gallery:  .. prompt:: bash      make  The documentation will be generated in the`\_build/html/stable`directory and are viewable in a web browser, for instance by opening the local`\_build/html/stable/index.html``file. To also generate the example gallery you can use:  .. prompt:: bash      make html  This will run all the examples, which takes a while. You can also run only a few examples based on their file names. Here is a way to run all examples with filenames containing `plot_calibration`:  .. prompt:: bash      EXAMPLES_PATTERN="plot_calibration" make html  You can use regular expressions for more advanced use cases.  Set the environment variable `NO_MATHJAX=1` if you intend to view the documentation in an offline setting. To build the PDF manual, run:  .. prompt:: bash      make latexpdf  .. admonition:: Sphinx version    :class: warning     While we do our best to have the documentation build under as many    versions of Sphinx as possible, the different versions tend to    behave slightly differently. To get the best results, you should    use the same version as the one we used on CircleCI. Look at this    `GitHub search <https://github.com/search?q=repo%3Ascikit-learn%2Fscikit-learn+%2F%5C%2Fsphinx-%5B0-9.%5D%2B%2F+path%3Abuild_tools%2Fcircle%2Fdoc_linux-64_conda.lock&type=code>`_    to know the exact version.   .. _generated_doc_CI:  Generated documentation on GitHub Actions -----------------------------------------  When you change the documentation in a pull request, GitHub Actions automatically builds it. To view the documentation generated by GitHub Actions, simply go to the bottom of your PR page, look for the item "Check the rendered docs here!" and click on 'details' next to it:  .. image:: ../images/generated-doc-ci.png    :align: center  .. _testing_coverage:  Testing and improving test coverage ===================================  High-quality `unit testing <https://en.wikipedia.org/wiki/Unit_testing>`_ is a corner-stone of the scikit-learn development process. For this purpose, we use the `pytest <https://docs.pytest.org>`_ package. The tests are functions appropriately named, located in `tests` subdirectories, that check the validity of the algorithms and the different options of the code.  Running `pytest` in a folder will run all the tests of the corresponding subpackages. For a more detailed `pytest` workflow, please refer to the [pr_checklist](#pr_checklist).  We expect code coverage of new features to be at least around 90%.  .. dropdown:: Writing matplotlib-related tests    Test fixtures ensure that a set of tests will be executing with the appropriate   initialization and cleanup. The scikit-learn test suite implements a``pyplot`fixture which can be used with`matplotlib`.    The`pyplot`fixture should be used when a test function is dealing with`matplotlib`.`matplotlib`is a soft dependency and is not required.   This fixture is in charge of skipping the tests if`matplotlib``is not   installed. In addition, figures created during the tests will be   automatically closed once the test function has been executed.    To use this fixture in a test function, one needs to pass it as an   argument::        def test_requiring_mpl_fixture(pyplot):           # you can now safely use matplotlib  .. dropdown:: Workflow to improve test coverage    To test code coverage, you need to install the `coverage   <https://pypi.org/project/coverage/>`_ package in addition to `pytest`.    1. Run `pytest --cov sklearn /path/to/tests`. The output lists for each file the line      numbers that are not tested.    2. Find a low hanging fruit, looking at which lines are not tested,      write or adapt a test specifically for these lines.    3. Loop.  .. _monitoring_performances:  Monitoring performance ======================  *This section is heavily inspired from the* `pandas documentation <https://pandas.pydata.org/docs/development/contributing_codebase.html#running-the-performance-test-suite>`_.  When proposing changes to the existing code base, it's important to make sure that they don't introduce performance regressions. Scikit-learn uses `asv benchmarks <https://github.com/airspeed-velocity/asv>`_ to monitor the performance of a selection of common estimators and functions. You can view these benchmarks on the `scikit-learn benchmark page <https://scikit-learn.org/scikit-learn-benchmarks>`_. The corresponding benchmark suite can be found in the `asv_benchmarks/` directory.  To use all features of asv, you will need either `conda` or `virtualenv`. For more details please check the `asv installation webpage <https://asv.readthedocs.io/en/latest/installing.html>`_.  First of all you need to install the development version of asv:  .. prompt:: bash      pip install git+https://github.com/airspeed-velocity/asv  and change your directory to `asv_benchmarks/`:  .. prompt:: bash    cd asv_benchmarks  The benchmark suite is configured to run against your local clone of scikit-learn. Make sure it is up to date:  .. prompt:: bash    git fetch upstream  In the benchmark suite, the benchmarks are organized following the same structure as scikit-learn. For example, you can compare the performance of a specific estimator between``upstream/main``and the branch you are working on:  .. prompt:: bash    asv continuous -b LogisticRegression upstream/main HEAD  The command uses conda by default for creating the benchmark environments. If you want to use virtualenv instead, use the `-E` flag:  .. prompt:: bash    asv continuous -E virtualenv -b LogisticRegression upstream/main HEAD  You can also specify a whole module to benchmark:  .. prompt:: bash    asv continuous -b linear_model upstream/main HEAD  You can replace `HEAD` by any local branch. By default it will only report the benchmarks that have change by at least 10%. You can control this ratio with the `-f` flag.  To run the full benchmark suite, simply remove the `-b` flag :  .. prompt:: bash    asv continuous upstream/main HEAD  However this can take up to two hours. The `-b` flag also accepts a regular expression for a more complex subset of benchmarks to run.  To run the benchmarks without comparing to another branch, use the `run` command:  .. prompt:: bash    asv run -b linear_model HEAD^!  You can also run the benchmark suite using the version of scikit-learn already installed in your current Python environment:  .. prompt:: bash    asv run --python=same  It's particularly useful when you installed scikit-learn in editable mode to avoid creating a new environment each time you run the benchmarks. By default the results are not saved when using an existing installation. To save the results you must specify a commit hash:  .. prompt:: bash    asv run --python=same --set-commit-hash=<commit hash>  Benchmarks are saved and organized by machine, environment and commit. To see the list of all saved benchmarks:  .. prompt:: bash    asv show  and to see the report of a specific run:  .. prompt:: bash    asv show <commit hash>  When running benchmarks for a pull request you're working on please report the results on github.  The benchmark suite supports additional configurable options which can be set in the `benchmarks/config.json` configuration file. For example, the benchmarks can run for a provided list of values for the `n_jobs` parameter.  More information on how to write a benchmark and how to use asv can be found in the `asv documentation <https://asv.readthedocs.io/en/latest/index.html>`_.  .. _issue_tracker_tags:  Issue Tracker Tags ==================  All issues and pull requests on the `GitHub issue tracker <https://github.com/scikit-learn/scikit-learn/issues>`_ should have (at least) one of the following tags:  :Bug:     Something is happening that clearly shouldn't happen.     Wrong results as well as unexpected errors from estimators go here.  :Enhancement:     Improving performance, usability, consistency.  :Documentation:     Missing, incorrect or sub-standard documentations and examples.  :New Feature:     Feature requests and pull requests implementing a new feature.  There are four other tags to help new contributors:  :Good first issue:     This issue is ideal for a first contribution to scikit-learn. Ask for help     if the formulation is unclear. If you have already contributed to     scikit-learn, look at Easy issues instead.  :Easy:     This issue can be tackled without much prior experience.  :Moderate:     Might need some knowledge of machine learning or the package,     but is still approachable for someone new to the project.  :Help wanted:     This tag marks an issue which currently lacks a contributor or a     PR that needs another contributor to take over the work. These     issues can range in difficulty, and may not be approachable     for new contributors. Note that not all issues which need     contributors will have this tag.  .. _backwards-compatibility:  Maintaining backwards compatibility ===================================  .. _contributing_deprecation:  Deprecation -----------  If any publicly accessible class, function, method, attribute or parameter is renamed, we still support the old one for two releases and issue a deprecation warning when it is called, passed, or accessed.  .. rubric:: Deprecating a class or a function  Suppose the function``zero\_one`is renamed to`zero\_one\_loss``, we add the decorator `utils.deprecated` to``zero\_one`and call`zero\_one\_loss``from that function::      from ..utils import deprecated      def zero_one_loss(y_true, y_pred, normalize=True):         # actual implementation         pass      @deprecated(         "Function `zero_one` was renamed to `zero_one_loss` in 0.13 and will be "         "removed in 0.15. Default behavior is changed from `normalize=False` to "         "`normalize=True`"     )     def zero_one(y_true, y_pred, normalize=False):         return zero_one_loss(y_true, y_pred, normalize)  One also needs to move``zero\_one`from`API\_REFERENCE`to`DEPRECATED\_API\_REFERENCE`and add`zero\_one\_loss`to`API\_REFERENCE`in the`doc/api\_reference.py``file to reflect the changes in [api_ref](#api_ref).  .. rubric:: Deprecating an attribute or a method  If an attribute or a method is to be deprecated, use the decorator `~utils.deprecated` on the property. Please note that the `~utils.deprecated` decorator should be placed before the``property`decorator if there is one, so that the docstrings can be rendered properly. For instance, renaming an attribute`[labels]()`to`[classes]()``can be done as::      @deprecated(         "Attribute `labels_` was deprecated in 0.13 and will be removed in 0.15. Use "         "`classes_` instead"     )     @property     def labels_(self):         return self.classes_  .. rubric:: Deprecating a parameter  If a parameter has to be deprecated, a``FutureWarning`warning must be raised manually. In the following example,`k``is deprecated and renamed to n_clusters::      import warnings      def example_function(n_clusters=8, k="deprecated"):         if k != "deprecated":             warnings.warn(                 "`k` was renamed to `n_clusters` in 0.13 and will be removed in 0.15",                 FutureWarning,             )             n_clusters = k  When the change is in a class, we validate and raise warning in``fit``::    import warnings    class ExampleEstimator(BaseEstimator):       def __init__(self, n_clusters=8, k='deprecated'):           self.n_clusters = n_clusters           self.k = k        def fit(self, X, y):           if self.k != "deprecated":               warnings.warn(                   "`k` was renamed to `n_clusters` in 0.13 and will be removed in 0.15.",                   FutureWarning,               )               self._n_clusters = self.k           else:               self._n_clusters = self.n_clusters  As in these examples, the warning message should always give both the version in which the deprecation happened and the version in which the old behavior will be removed. If the deprecation happened in version 0.x-dev, the message should say deprecation occurred in version 0.x and the removal will be in 0.(x+2), so that users will have enough time to adapt their code to the new behaviour. For example, if the deprecation happened in version 0.18-dev, the message should say it happened in version 0.18 and the old behavior will be removed in version 0.20.  The warning message should also include a brief explanation of the change and point users to an alternative.  In addition, a deprecation note should be added in the docstring, recalling the same information as the deprecation warning as explained above. Use the``.. deprecated::`directive:`<span class="title-ref">rst .. deprecated:: 0.13 </span><span class="title-ref">k</span><span class="title-ref"> was renamed to </span><span class="title-ref">n\_clusters</span>\` in version 0.13 and will be removed in 0.15.

What's more, a deprecation requires a test which ensures that the warning is `` ` raised in relevant cases but not in other cases. The warning should be caught in all other tests (using e.g., ``@pytest.mark.filterwarnings`), and there should be no warning in the examples.   Change the default value of a parameter ---------------------------------------  If the default value of a parameter needs to be changed, please replace the default value with a specific value (e.g.,`"warn"`) and raise`FutureWarning`when users are using the default value. The following example assumes that the current version is 0.20 and that we change the default value of`n\_clusters``from 5 (old default for 0.20) to 10 (new default for 0.22)::      import warnings      def example_function(n_clusters="warn"):         if n_clusters == "warn":             warnings.warn(                 "The default value of `n_clusters` will change from 5 to 10 in 0.22.",                 FutureWarning,             )             n_clusters = 5  When the change is in a class, we validate and raise warning in``fit``::    import warnings    class ExampleEstimator:       def __init__(self, n_clusters="warn"):           self.n_clusters = n_clusters        def fit(self, X, y):           if self.n_clusters == "warn":               warnings.warn(                   "The default value of `n_clusters` will change from 5 to 10 in 0.22.",                   FutureWarning,               )               self._n_clusters = 5  Similar to deprecations, the warning message should always give both the version in which the change happened and the version in which the old behavior will be removed.  The parameter description in the docstring needs to be updated accordingly by adding a``versionchanged`directive with the old and new default value, pointing to the version when the change will be effective:`<span class="title-ref">rst .. versionchanged:: 0.22 The default value for \`n\_clusters</span> will change from 5 to 10 in version 0.22.

Finally, we need a test which ensures that the warning is raised in relevant cases but `` ` not in other cases. The warning should be caught in all other tests (using e.g., ``@pytest.mark.filterwarnings``), and there should be no warning in the examples.  .. _code_review:  Code Review Guidelines ======================  Reviewing code contributed to the project as PRs is a crucial component of scikit-learn development. We encourage anyone to start reviewing code of other developers. The code review process is often highly educational for everybody involved. This is particularly appropriate if it is a feature you would like to use, and so can respond critically about whether the PR meets your needs. While each pull request needs to be signed off by two core developers, you can speed up this process by providing your feedback.  > **Note** >    The difference between an objective improvement and a subjective nit isn't   always clear. Reviewers should recall that code review is primarily about   reducing risk in the project. When reviewing code, one should aim at   preventing situations which may require a bug fix, a deprecation, or a   retraction. Regarding docs: typos, grammar issues and disambiguations are   better addressed immediately.  .. dropdown:: Important aspects to be covered in any code review    Here are a few important aspects that need to be covered in any code review,   from high-level questions to a more detailed check-list.    - Do we want this in the library? Is it likely to be used? Do you, as     a scikit-learn user, like the change and intend to use it? Is it in     the scope of scikit-learn? Will the cost of maintaining a new     feature be worth its benefits?    - Is the code consistent with the API of scikit-learn? Are public     functions/classes/parameters well named and intuitively designed?    - Are all public functions/classes and their parameters, return types, and     stored attributes named according to scikit-learn conventions and documented clearly?    - Is any new functionality described in the user-guide and illustrated with examples?    - Is every public function/class tested? Are a reasonable set of     parameters, their values, value types, and combinations tested? Do     the tests validate that the code is correct, i.e. doing what the     documentation says it does? If the change is a bug-fix, is a     non-regression test included? Look at `this     <https://jeffknupp.com/blog/2013/12/09/improve-your-python-understanding-unit-testing>`__     to get started with testing in Python.    - Do the tests pass in the continuous integration build? If     appropriate, help the contributor understand why tests failed.    - Do the tests cover every line of code (see the coverage report in the build     log)? If not, are the lines missing coverage good exceptions?    - Is the code easy to read and low on redundancy? Should variable names be     improved for clarity or consistency? Should comments be added? Should comments     be removed as unhelpful or extraneous?    - Could the code easily be rewritten to run much more efficiently for     relevant settings?    - Is the code backwards compatible with previous versions? (or is a     deprecation cycle necessary?)    - Will the new code add any dependencies on other libraries? (this is     unlikely to be accepted)    - Does the documentation render properly (see the     [contribute_documentation](#contribute_documentation) section for more details), and are the plots     instructive?    [saved_replies](#saved_replies) includes some frequent comments that reviewers may make.  .. _communication:  .. dropdown:: Communication Guidelines    Reviewing open pull requests (PRs) helps move the project forward. It is a   great way to get familiar with the codebase and should motivate the   contributor to keep involved in the project. [1]_    - Every PR, good or bad, is an act of generosity. Opening with a positive     comment will help the author feel rewarded, and your subsequent remarks may     be heard more clearly. You may feel good also.   - Begin if possible with the large issues, so the author knows they've been     understood. Resist the temptation to immediately go line by line, or to open     with small pervasive issues.   - Do not let perfect be the enemy of the good. If you find yourself making     many small suggestions that don't fall into the [code_review](#code_review), consider     the following approaches:      - refrain from submitting these;     - prefix them as "Nit" so that the contributor knows it's OK not to address;     - follow up in a subsequent PR, out of courtesy, you may want to let the       original contributor know.    - Do not rush, take the time to make your comments clear and justify your     suggestions.   - You are the face of the project. Bad days occur to everyone, in that     occasion you deserve a break: try to take your time and stay offline.    .. [1] Adapted from the numpy `communication guidelines         <https://numpy.org/devdocs/dev/reviewer_guidelines.html#communication-guidelines>`_.  Reading the existing code base ==============================  Reading and digesting an existing code base is always a difficult exercise that takes time and experience to master. Even though we try to write simple code in general, understanding the code can seem overwhelming at first, given the sheer size of the project. Here is a list of tips that may help make this task easier and faster (in no particular order).  - Get acquainted with the [api_overview](#api_overview): understand what :term:`fit`,   :term:`predict`, :term:`transform`, etc. are used for. - Before diving into reading the code of a function / class, go through the   docstrings first and try to get an idea of what each parameter / attribute   is doing. It may also help to stop a minute and think *how would I do this   myself if I had to?* - The trickiest thing is often to identify which portions of the code are   relevant, and which are not. In scikit-learn **a lot** of input checking   is performed, especially at the beginning of the :term:`fit` methods.   Sometimes, only a very small portion of the code is doing the actual job.   For example looking at the `~linear_model.LinearRegression.fit` method of   `~linear_model.LinearRegression`, what you're looking for   might just be the call the `scipy.linalg.lstsq`, but it is buried into   multiple lines of input checking and the handling of different kinds of   parameters. - Due to the use of `Inheritance   <https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)>`_,   some methods may be implemented in parent classes. All estimators inherit   at least from `~base.BaseEstimator`, and   from a``Mixin``class (e.g. `~base.ClassifierMixin`) that enables default   behaviour depending on the nature of the estimator (classifier, regressor,   transformer, etc.). - Sometimes, reading the tests for a given function will give you an idea of   what its intended purpose is. You can use``git grep`(see below) to find   all the tests written for a function. Most tests for a specific   function/class are placed under the`tests/`folder of the module - You'll often see code looking like this:`out = Parallel(...)(delayed(some\_function)(param) for param in some\_iterable)`. This runs`some\_function``in parallel using `Joblib   <https://joblib.readthedocs.io/>`_.``out`is then an iterable containing   the values returned by`some\_function``for each call. - We use `Cython <https://cython.org/>`_ to write fast code. Cython code is   located in``.pyx`and`.pxd``files. Cython code has a more C-like flavor:   we use pointers, perform manual memory allocation, etc. Having some minimal   experience in C / C++ is pretty much mandatory here. For more information see   [cython](#cython). - Master your tools.    - With such a big project, being efficient with your favorite editor or     IDE goes a long way towards digesting the code base. Being able to quickly     jump (or *peek*) to a function/class/attribute definition helps a lot.     So does being able to quickly see where a given name is used in a file.   - `Git <https://git-scm.com/book/en>`_ also has some built-in killer     features. It is often useful to understand how a file changed over time,     using e.g.``git blame``(`manual     <https://git-scm.com/docs/git-blame>`_). This can also be done directly     on GitHub.``git grep\`<span class="title-ref"> (\`examples \<https://git-scm.com/docs/git-grep\#\_examples\></span>\_) is also extremely useful to see every occurrence of a pattern (e.g. a function call or a variable) in the code base.

  - Configure <span class="title-ref">git blame</span> to ignore the commit that migrated the code style to <span class="title-ref">black</span>.
    
    <div class="prompt">
    
    bash
    
    git config blame.ignoreRevsFile .git-blame-ignore-revs
    
    </div>
    
    Find out more information in black's [documentation for avoiding ruining git blame](https://black.readthedocs.io/en/stable/guides/introducing_black_to_your_project.html#avoiding-ruining-git-blame).

---

cython.md

---

# Cython Best Practices, Conventions and Knowledge

This documents tips to develop Cython code in scikit-learn.

## Tips for developing with Cython in scikit-learn

### Tips to ease development

  - Time spent reading [Cython's documentation](https://cython.readthedocs.io/en/latest/) is not time lost.

  - If you intend to use OpenMP: On MacOS, system's distribution of `clang` does not implement OpenMP. You can install the `compilers` package available on `conda-forge` which comes with an implementation of OpenMP.

  - Activating [checks](https://github.com/scikit-learn/scikit-learn/blob/62a017efa047e9581ae7df8bbaa62cf4c0544ee4/sklearn/_build_utils/__init__.py#L68-L87) might help. E.g. for activating boundscheck use:
    
      - \`\`\`bash  
        export SKLEARN\_ENABLE\_DEBUG\_CYTHON\_DIRECTIVES=1

  - [Start from scratch in a notebook](https://cython.readthedocs.io/en/latest/src/quickstart/build.html#using-the-jupyter-notebook) to understand how to use Cython and to get feedback on your work quickly. If you plan to use OpenMP for your implementations in your Jupyter Notebook, do add extra compiler and linkers arguments in the Cython magic.
    
    ``` python
    # For GCC and for clang
    %%cython --compile-args=-fopenmp --link-args=-fopenmp
    # For Microsoft's compilers
    %%cython --compile-args=/openmp --link-args=/openmp
    ```

  - To debug C code (e.g. a segfault), do use `gdb` with:
    
    ``` bash
    gdb --ex r --args python ./entrypoint_to_bug_reproducer.py
    ```

  - To have access to some value in place to debug in `cdef (nogil)` context, use:
    
    ``` cython
    with gil:
        print(state_to_print)
    ```

  - Note that Cython cannot parse f-strings with `{var=}` expressions, e.g.
    
    ``` bash
    print(f"{test_val=}")
    ```

  - scikit-learn codebase has a lot of non-unified (fused) types (re)definitions. There currently is [ongoing work to simplify and unify that across the codebase](https://github.com/scikit-learn/scikit-learn/issues/25572). For now, make sure you understand which concrete types are used ultimately.

  - You might find this alias to compile individual Cython extension handy:
    
        # You might want to add this alias to your shell script config.
        alias cythonX="cython -X language_level=3 -X boundscheck=False -X wraparound=False -X initializedcheck=False -X nonecheck=False -X cdivision=True"
        
        # This generates `source.c` as if you had recompiled scikit-learn entirely.
        cythonX --annotate source.pyx

  - Using the `--annotate` option with this flag allows generating a HTML report of code annotation. This report indicates interactions with the CPython interpreter on a line-by-line basis. Interactions with the CPython interpreter must be avoided as much as possible in the computationally intensive sections of the algorithms. For more information, please refer to [this section of Cython's tutorial](https://cython.readthedocs.io/en/latest/src/tutorial/cython_tutorial.html#primes)
    
        # This generates a HTML report (`source.html`) for `source.c`.
        cythonX --annotate source.pyx

Tips for performance `` ` ^^^^^^^^^^^^^^^^^^^^  * Understand the GIL in context for CPython (which problems it solves, what are its limitations)   and get a good understanding of when Cython will be mapped to C code free of interactions with   CPython, when it will not, and when it cannot (e.g. presence of interactions with Python   objects, which include functions). In this regard, `PEP073 <https://peps.python.org/pep-0703/>`_   provides a good overview and context and pathways for removal.  * Make sure you have deactivated `checks <https://github.com/scikit-learn/scikit-learn/blob/62a017efa047e9581ae7df8bbaa62cf4c0544ee4/sklearn/_build_utils/__init__.py#L68-L87>`_.  * Always prefer memoryviews instead over ``cnp.ndarray`when possible: memoryviews are lightweight.  * Avoid memoryview slicing: memoryview slicing might be costly or misleading in some cases and   we better not use it, even if handling fewer dimensions in some context would be preferable.  * Decorate final classes or methods with`@final`(this allows removing virtual tables when needed)  * Inline methods and function when it makes sense  * In doubt, read the generated C or C++ code if you can: "The fewer C instructions and indirections   for a line of Cython code, the better" is a good rule of thumb.  *`nogil`declarations are just hints: when declaring the`cdef`functions   as nogil, it means that they can be called without holding the GIL, but it does not release   the GIL when entering them. You have to do that yourself either by passing`nogil=True`to`cython.parallel.prange`explicitly, or by using an explicit context manager:`\`cython cdef inline void my\_func(self) nogil:

> \# Some logic interacting with CPython, e.g. allocating arrays via NumPy.
> 
>   - with nogil:  
>     \# The code here is run as is it were written in C.
> 
> return 0
> 
> This item is based on [this comment from Stéfan's Benhel](https://github.com/cython/cython/issues/2798#issuecomment-459971828)

  - Direct calls to BLAS routines are possible via interfaces defined in `sklearn.utils._cython_blas`.

Using OpenMP `` ` ^^^^^^^^^^^^  Since scikit-learn can be built without OpenMP, it's necessary to protect each direct call to OpenMP.  The `_openmp_helpers` module, available in `sklearn/utils/_openmp_helpers.pyx <https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_openmp_helpers.pyx>`_ provides protected versions of the OpenMP routines. To use OpenMP routines, they must be ``cimported`from this module and not from the OpenMP library directly:`\`cython from sklearn.utils.\_openmp\_helpers cimport omp\_get\_max\_threads max\_threads = omp\_get\_max\_threads()

The parallel loop, <span class="title-ref">prange</span>, is already protected by cython and can be used directly `` ` from `cython.parallel`.  Types ~~~~~  Cython code requires to use explicit types. This is one of the reasons you get a performance boost. In order to avoid code duplication, we have a central place for the most used types in `sklearn/utils/_typedefs.pyd <https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_typedefs.pyd>`_. Ideally you start by having a look there and `cimport` types you need, for example ``<span class="title-ref">cython from sklear.utils.\_typedefs cimport float32, float64 </span>\`\`

---

develop.md

---

# Developing scikit-learn estimators

Whether you are proposing an estimator for inclusion in scikit-learn, developing a separate package compatible with scikit-learn, or implementing custom components for your own projects, this chapter details how to develop objects that safely interact with scikit-learn pipelines and model selection tools.

This section details the public API you should use and implement for a scikit-learn compatible estimator. Inside scikit-learn itself, we experiment and use some private tools and our goal is always to make them public once they are stable enough, so that you can also use them in your own projects.

<div class="currentmodule">

sklearn

</div>

## APIs of scikit-learn objects

There are two major types of estimators. You can think of the first group as simple estimators, which consists most estimators, such as <span class="title-ref">\~sklearn.linear\_model.LogisticRegression</span> or <span class="title-ref">\~sklearn.ensemble.RandomForestClassifier</span>. And the second group are meta-estimators, which are estimators that wrap other estimators. <span class="title-ref">\~sklearn.pipeline.Pipeline</span> and <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> are two examples of meta-estimators.

Here we start with a few vocabulary, and then we illustrate how you can implement your own estimators.

Elements of the scikit-learn API are described more definitively in the \[glossary\](\#glossary).

### Different objects

The main objects in scikit-learn are (one class can implement multiple interfaces):

  - Estimator

> The base object, implements a `fit` method to learn from data, either:
> 
>     estimator = estimator.fit(data, targets)
> 
> or:
> 
>     estimator = estimator.fit(data)

  - Predictor

> For supervised learning, or some unsupervised problems, implements:
> 
>     prediction = predictor.predict(data)
> 
> Classification algorithms usually also offer a way to quantify certainty of a prediction, either using `decision_function` or `predict_proba`:
> 
>     probability = predictor.predict_proba(data)

  - Transformer

> For modifying the data in a supervised or unsupervised way (e.g. by adding, changing, or removing columns, but not by adding or removing rows). Implements:
> 
>     new_data = transformer.transform(data)
> 
> When fitting and transforming can be performed much more efficiently together than separately, implements:
> 
>     new_data = transformer.fit_transform(data)

  - Model

> A model that can give a [goodness of fit](https://en.wikipedia.org/wiki/Goodness_of_fit) measure or a likelihood of unseen data, implements (higher is better):
> 
>     score = model.score(data)

### Estimators

The API has one predominant object: the estimator. An estimator is an object that fits a model based on some training data and is capable of inferring some properties on new data. It can be, for instance, a classifier or a regressor. All estimators implement the fit method:

    estimator.fit(X, y)

Out of all the methods that an estimator implements, `fit` is usually the one you want to implement yourself. Other methods such as `set_params`, `get_params`, etc. are implemented in <span class="title-ref">\~sklearn.base.BaseEstimator</span>, which you should inherit from. You might need to inherit from more mixins, which we will explain later.

#### Instantiation

This concerns the creation of an object. The object's `__init__` method might accept constants as arguments that determine the estimator's behavior (like the `alpha` constant in <span class="title-ref">\~sklearn.linear\_model.SGDClassifier</span>). It should not, however, take the actual training data as an argument, as this is left to the `fit()` method:

    clf2 = SGDClassifier(alpha=2.3)
    clf3 = SGDClassifier([[1, 2], [2, 3]], [-1, 1]) # WRONG!

Ideally, the arguments accepted by `__init__` should all be keyword arguments with a default value. In other words, a user should be able to instantiate an estimator without passing any arguments to it. In some cases, where there are no sane defaults for an argument, they can be left without a default value. In scikit-learn itself, we have very few places, only in some meta-estimators, where the sub-estimator(s) argument is a required argument.

Most arguments correspond to hyperparameters describing the model or the optimisation problem the estimator tries to solve. Other parameters might define how the estimator behaves, e.g. defining the location of a cache to store some data. These initial arguments (or parameters) are always remembered by the estimator. Also note that they should not be documented under the "Attributes" section, but rather under the "Parameters" section for that estimator.

In addition, **every keyword argument accepted by** `__init__` **should correspond to an attribute on the instance**. Scikit-learn relies on this to find the relevant attributes to set on an estimator when doing model selection.

To summarize, an `__init__` should look like:

    def __init__(self, param1=1, param2=2):
        self.param1 = param1
        self.param2 = param2

There should be no logic, not even input validation, and the parameters should not be changed; which also means ideally they should not be mutable objects such as lists or dictionaries. If they're mutable, they should be copied before being modified. The corresponding logic should be put where the parameters are used, typically in `fit`. The following is wrong:

    def __init__(self, param1=1, param2=2, param3=3):
        # WRONG: parameters should not be modified
        if param1 > 1:
            param2 += 1
        self.param1 = param1
        # WRONG: the object's attributes should have exactly the name of
        # the argument in the constructor
        self.param3 = param2

The reason for postponing the validation is that if `__init__` includes input validation, then the same validation would have to be performed in `set_params`, which is used in algorithms like <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span>.

Also it is expected that parameters with trailing `_` are **not to be set inside the** `__init__` **method**. More details on attributes that are not init arguments come shortly.

#### Fitting

The next thing you will probably want to do is to estimate some parameters in the model. This is implemented in the `fit()` method, and it's where the training happens. For instance, this is where you have the computation to learn or estimate coefficients for a linear model.

The `fit()` method takes the training data as arguments, which can be one array in the case of unsupervised learning, or two arrays in the case of supervised learning. Other metadata that come with the training data, such as `sample_weight`, can also be passed to `fit` as keyword arguments.

Note that the model is fitted using `X` and `y`, but the object holds no reference to `X` and `y`. There are, however, some exceptions to this, as in the case of precomputed kernels where this data must be stored for use by the predict method.

| Parameters |                                               |
| ---------- | --------------------------------------------- |
| X          | array-like of shape (n\_samples, n\_features) |
| y          | array-like of shape (n\_samples,)             |
| kwargs     | optional data-dependent parameters            |

The number of samples, i.e. `X.shape[0]` should be the same as `y.shape[0]`. If this requirement is not met, an exception of type `ValueError` should be raised.

`y` might be ignored in the case of unsupervised learning. However, to make it possible to use the estimator as part of a pipeline that can mix both supervised and unsupervised transformers, even unsupervised estimators need to accept a `y=None` keyword argument in the second position that is just ignored by the estimator. For the same reason, `fit_predict`, `fit_transform`, `score` and `partial_fit` methods need to accept a `y` argument in the second place if they are implemented.

The method should return the object (`self`). This pattern is useful to be able to implement quick one liners in an IPython session such as:

    y_predicted = SGDClassifier(alpha=10).fit(X_train, y_train).predict(X_test)

Depending on the nature of the algorithm, `fit` can sometimes also accept additional keywords arguments. However, any parameter that can have a value assigned prior to having access to the data should be an `__init__` keyword argument. Ideally, **fit parameters should be restricted to directly data dependent variables**. For instance a Gram matrix or an affinity matrix which are precomputed from the data matrix `X` are data dependent. A tolerance stopping criterion `tol` is not directly data dependent (although the optimal value according to some scoring function probably is).

When `fit` is called, any previous call to `fit` should be ignored. In general, calling `estimator.fit(X1)` and then `estimator.fit(X2)` should be the same as only calling `estimator.fit(X2)`. However, this may not be true in practice when `fit` depends on some random process, see `random_state`. Another exception to this rule is when the hyper-parameter `warm_start` is set to `True` for estimators that support it. `warm_start=True` means that the previous state of the trainable parameters of the estimator are reused instead of using the default initialization strategy.

#### Estimated Attributes

According to scikit-learn conventions, attributes which you'd want to expose to your users as public attributes and have been estimated or learned from the data must always have a name ending with trailing underscore, for example the coefficients of some regression estimator would be stored in a `coef_` attribute after `fit` has been called. Similarly, attributes that you learn in the process and you'd like to store yet not expose to the user, should have a leading underscure, e.g. `_intermediate_coefs`. You'd need to document the first group (with a trailing underscore) as "Attributes" and no need to document the second group (with a leading underscore).

The estimated attributes are expected to be overridden when you call `fit` a second time.

#### Universal attributes

Estimators that expect tabular input should set a <span class="title-ref">n\_features\_in\_</span> attribute at <span class="title-ref">fit</span> time to indicate the number of features that the estimator expects for subsequent calls to `predict` or `transform`. See [SLEP010](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for details.

Similarly, if estimators are given dataframes such as pandas or polars, they should set a `feature_names_in_` attribute to indicate the features names of the input data, detailed in [SLEP007](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep007/proposal.html). Using <span class="title-ref">\~sklearn.utils.validation.validate\_data</span> would automatically set these attributes for you.

## Rolling your own estimator

If you want to implement a new estimator that is scikit-learn compatible, there are several internals of scikit-learn that you should be aware of in addition to the scikit-learn API outlined above. You can check whether your estimator adheres to the scikit-learn interface and standards by running <span class="title-ref">\~sklearn.utils.estimator\_checks.check\_estimator</span> on an instance. The <span class="title-ref">\~sklearn.utils.estimator\_checks.parametrize\_with\_checks</span> pytest decorator can also be used (see its docstring for details and possible interactions with <span class="title-ref">pytest</span>):

    >>> from sklearn.utils.estimator_checks import check_estimator
    >>> from sklearn.tree import DecisionTreeClassifier
    >>> check_estimator(DecisionTreeClassifier())  # passes

The main motivation to make a class compatible to the scikit-learn estimator interface might be that you want to use it together with model evaluation and selection tools such as <span class="title-ref">\~model\_selection.GridSearchCV</span> and <span class="title-ref">\~pipeline.Pipeline</span>.

Before detailing the required interface below, we describe two ways to achieve the correct interface more easily.

<div class="topic">

**Project template:**

We provide a [project template](https://github.com/scikit-learn-contrib/project-template/) which helps in the creation of Python packages containing scikit-learn compatible estimators. It provides:

  - an initial git repository with Python package directory structure
  - a template of a scikit-learn estimator
  - an initial test suite including use of <span class="title-ref">\~utils.parametrize\_with\_checks</span>
  - directory structures and scripts to compile documentation and example galleries
  - scripts to manage continuous integration (testing on Linux, MacOS, and Windows)
  - instructions from getting started to publishing on [PyPi](https://pypi.org/)

</div>

<div class="topic">

**<span class="title-ref">base.BaseEstimator</span> and mixins:**

We tend to use "duck typing" instead of checking for <span class="title-ref">isinstance</span>, which means it's technically possible to implement estimator without inheriting from scikit-learn classes. However, if you don't inherit from the right mixins, either there will be a large amount of boilerplate code for you to implement and keep in sync with scikit-learn development, or your estimator might not function the same way as a scikit-learn estimator. Here we only document how to develop an estimator using our mixins. If you're interested in implementing your estimator without inheriting from scikit-learn mixins, you'd need to check our implementations.

For example, below is a custom classifier, with more examples included in the scikit-learn-contrib [project template](https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py).

It is particularly important to notice that mixins should be "on the left" while the `BaseEstimator` should be "on the right" in the inheritance list for proper MRO.

> \>\>\> import numpy as np \>\>\> from sklearn.base import BaseEstimator, ClassifierMixin \>\>\> from sklearn.utils.validation import validate\_data, check\_is\_fitted \>\>\> from sklearn.utils.multiclass import unique\_labels \>\>\> from sklearn.metrics import euclidean\_distances \>\>\> class TemplateClassifier(ClassifierMixin, BaseEstimator): ... ... def \_\_init\_\_(self, demo\_param='demo'): ... self.demo\_param = demo\_param ... ... def fit(self, X, y): ... ... \# Check that X and y have correct shape, set [n\_features\_in](), etc. ... X, y = validate\_data(self, X, y) ... \# Store the classes seen during fit ... [self.classes]() = unique\_labels(y) ... ... [self.X]() = X ... [self.y]() = y ... \# Return the classifier ... return self ... ... def predict(self, X): ... ... \# Check if fit has been called ... check\_is\_fitted(self) ... ... \# Input validation ... X = validate\_data(self, X, reset=False) ... ... closest = np.argmin(euclidean\_distances(X, [self.X]()), axis=1) ... return [self.y]()\[closest\]

</div>

And you can check that the above estimator passes all common checks:

    >>> from sklearn.utils.estimator_checks import check_estimator
    >>> check_estimator(TemplateClassifier())  # passes

### get\_params and set\_params

All scikit-learn estimators have `get_params` and `set_params` functions.

The `get_params` function takes no arguments and returns a dict of the `__init__` parameters of the estimator, together with their values.

It take one keyword argument, `deep`, which receives a boolean value that determines whether the method should return the parameters of sub-estimators (only relevant for meta-estimators). The default value for `deep` is `True`. For instance considering the following estimator:

    >>> from sklearn.base import BaseEstimator
    >>> from sklearn.linear_model import LogisticRegression
    >>> class MyEstimator(BaseEstimator):
    ...     def __init__(self, subestimator=None, my_extra_param="random"):
    ...         self.subestimator = subestimator
    ...         self.my_extra_param = my_extra_param

The parameter <span class="title-ref">deep</span> controls control whether or not the parameters of the <span class="title-ref">subestimator</span> should be reported. Thus when <span class="title-ref">deep=True</span>, the output will be:

    >>> my_estimator = MyEstimator(subestimator=LogisticRegression())
    >>> for param, value in my_estimator.get_params(deep=True).items():
    ...     print(f"{param} -> {value}")
    my_extra_param -> random
    subestimator__C -> 1.0
    subestimator__class_weight -> None
    subestimator__dual -> False
    subestimator__fit_intercept -> True
    subestimator__intercept_scaling -> 1
    subestimator__l1_ratio -> None
    subestimator__max_iter -> 100
    subestimator__multi_class -> deprecated
    subestimator__n_jobs -> None
    subestimator__penalty -> l2
    subestimator__random_state -> None
    subestimator__solver -> lbfgs
    subestimator__tol -> 0.0001
    subestimator__verbose -> 0
    subestimator__warm_start -> False
    subestimator -> LogisticRegression()

If the meta-estimator takes multiple sub-estimators, often, those sub-estimators have names (as e.g. named steps in a <span class="title-ref">\~pipeline.Pipeline</span> object), in which case the key should become <span class="title-ref">\<name\>\_\_C</span>, <span class="title-ref">\<name\>\_\_class\_weight</span>, etc.

When `deep=False`, the output will be:

    >>> for param, value in my_estimator.get_params(deep=False).items():
    ...     print(f"{param} -> {value}")
    my_extra_param -> random
    subestimator -> LogisticRegression()

On the other hand, `set_params` takes the parameters of `__init__` as keyword arguments, unpacks them into a dict of the form `'parameter': value` and sets the parameters of the estimator using this dict. It returns the estimator itself.

The <span class="title-ref">\~base.BaseEstimator.set\_params</span> function is used to set parameters during grid search for instance.

### Cloning

As already mentioned that when constructor arguments are mutable, they should be copied before modifying them. This also applies to constructor arguments which are estimators. That's why meta-estimators such as <span class="title-ref">\~model\_selection.GridSearchCV</span> create a copy of the given estimator before modifying it.

However, in scikit-learn, when we copy an estimator, we get an unfitted estimator where only the constructor arguments are copied (with some exceptions, e.g. attributes related to certain internal machinery such as metadata routing).

The function responsible for this behavior is <span class="title-ref">\~base.clone</span>.

Estimators can customize the behavior of <span class="title-ref">base.clone</span> by overriding the <span class="title-ref">base.BaseEstimator.\_\_sklearn\_clone\_\_</span> method. <span class="title-ref">\_\_sklearn\_clone\_\_</span> must return an instance of the estimator. <span class="title-ref">\_\_sklearn\_clone\_\_</span> is useful when an estimator needs to hold on to some state when <span class="title-ref">base.clone</span> is called on the estimator. For example, <span class="title-ref">\~sklearn.frozen.FrozenEstimator</span> makes use of this.

### Estimator types

Among simple estimators (as opposed to meta-estimators), the most common types are transformers, classifiers, regressors, and clustering algorithms.

**Transformers** inherit from <span class="title-ref">\~base.TransformerMixin</span>, and implement a <span class="title-ref">transform</span> method. These are estimators which take the input, and transform it in some way. Note that they should never change the number of input samples, and the output of <span class="title-ref">transform</span> should correspond to its input samples in the same given order.

**Regressors** inherit from <span class="title-ref">\~base.RegressorMixin</span>, and implement a <span class="title-ref">predict</span> method. They should accept numerical `y` in their <span class="title-ref">fit</span> method. Regressors use <span class="title-ref">\~metrics.r2\_score</span> by default in their <span class="title-ref">\~base.RegressorMixin.score</span> method.

**Classifiers** inherit from <span class="title-ref">\~base.ClassifierMixin</span>. If it applies, classifiers can implement `decision_function` to return raw decision values, based on which `predict` can make its decision. If calculating probabilities is supported, classifiers can also implement `predict_proba` and `predict_log_proba`.

Classifiers should accept `y` (target) arguments to `fit` that are sequences (lists, arrays) of either strings or integers. They should not assume that the class labels are a contiguous range of integers; instead, they should store a list of classes in a `classes_` attribute or property. The order of class labels in this attribute should match the order in which `predict_proba`, `predict_log_proba` and `decision_function` return their values. The easiest way to achieve this is to put:

    self.classes_, y = np.unique(y, return_inverse=True)

in `fit`. This returns a new `y` that contains class indexes, rather than labels, in the range \[0, `n_classes`).

A classifier's `predict` method should return arrays containing class labels from `classes_`. In a classifier that implements `decision_function`, this can be achieved with:

    def predict(self, X):
        D = self.decision_function(X)
        return self.classes_[np.argmax(D, axis=1)]

The `~sklearn.utils.multiclass` module contains useful functions for working with multiclass and multilabel problems.

**Clustering algorithms** inherit from <span class="title-ref">\~base.ClusterMixin</span>. Ideally, they should accept a `y` parameter in their `fit` method, but it should be ignored. Clustering algorithms should set a `labels_` attribute, storing the labels assigned to each sample. If applicale, they can also implement a `predict` method, returning the labels assigned to newly given samples.

If one needs to check the type of a given estimator, e.g. in a meta-estimator, one can check if the given object implements a `transform` method for transformers, and otherwise use helper functions such as <span class="title-ref">\~base.is\_classifier</span> or <span class="title-ref">\~base.is\_regressor</span>.

### Estimator Tags

\> **Note** \> Scikit-learn introduced estimator tags in version 0.21 as a private API and mostly used in tests. However, these tags expanded over time and many third party developers also need to use them. Therefore in version 1.6 the API for the tags were revamped and exposed as public API.

The estimator tags are annotations of estimators that allow programmatic inspection of their capabilities, such as sparse matrix support, supported output types and supported methods. The estimator tags are an instance of <span class="title-ref">\~sklearn.utils.Tags</span> returned by the method <span class="title-ref">\~sklearn.base.BaseEstimator.\_\_sklearn\_tags\_\_()</span>. These tags are used in different places, such as <span class="title-ref">\~base.is\_regressor</span> or the common checks run by <span class="title-ref">\~sklearn.utils.estimator\_checks.check\_estimator</span> and <span class="title-ref">\~sklearn.utils.estimator\_checks.parametrize\_with\_checks</span>, where tags determine which checks to run and what input data is appropriate. Tags can depend on estimator parameters or even system architecture and can in general only be determined at runtime and are therefore instance attributes rather than class attributes. See <span class="title-ref">\~sklearn.utils.Tags</span> for more information about individual tags.

It is unlikely that the default values for each tag will suit the needs of your specific estimator. You can change the default values by defining a <span class="title-ref">\_\_sklearn\_tags\_\_()</span> method which returns the new values for your estimator's tags. For example:

    class MyMultiOutputEstimator(BaseEstimator):
    
        def __sklearn_tags__(self):
            tags = super().__sklearn_tags__()
            tags.target_tags.single_output = False
            tags.non_deterministic = True
            return tags

You can create a new subclass of <span class="title-ref">\~sklearn.utils.Tags</span> if you wish to add new tags to the existing set. Note that all attributes that you add in a child class need to have a default value. It can be of the form:

    from dataclasses import dataclass, asdict
    
    @dataclass
    class MyTags(Tags):
        my_tag: bool = True
    
    class MyEstimator(BaseEstimator):
        def __sklearn_tags__(self):
            tags_orig = super().__sklearn_tags__()
            as_dict = {
                field.name: getattr(tags_orig, field.name)
                for field in fields(tags_orig)
            }
            tags = MyTags(**as_dict)
            tags.my_tag = True
            return tags

## Developer API for <span class="title-ref">set\_output</span>

With [SLEP018](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep018/proposal.html), scikit-learn introduces the <span class="title-ref">set\_output</span> API for configuring transformers to output pandas DataFrames. The <span class="title-ref">set\_output</span> API is automatically defined if the transformer defines `get_feature_names_out` and subclasses <span class="title-ref">base.TransformerMixin</span>. `get_feature_names_out` is used to get the column names of pandas output.

<span class="title-ref">base.OneToOneFeatureMixin</span> and <span class="title-ref">base.ClassNamePrefixFeaturesOutMixin</span> are helpful mixins for defining `get_feature_names_out`. <span class="title-ref">base.OneToOneFeatureMixin</span> is useful when the transformer has a one-to-one correspondence between input features and output features, such as <span class="title-ref">\~preprocessing.StandardScaler</span>. <span class="title-ref">base.ClassNamePrefixFeaturesOutMixin</span> is useful when the transformer needs to generate its own feature names out, such as <span class="title-ref">\~decomposition.PCA</span>.

You can opt-out of the <span class="title-ref">set\_output</span> API by setting <span class="title-ref">auto\_wrap\_output\_keys=None</span> when defining a custom subclass:

    class MyTransformer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):
    
        def fit(self, X, y=None):
            return self
        def transform(self, X, y=None):
            return X
        def get_feature_names_out(self, input_features=None):
            ...

The default value for <span class="title-ref">auto\_wrap\_output\_keys</span> is <span class="title-ref">("transform",)</span>, which automatically wraps <span class="title-ref">fit\_transform</span> and <span class="title-ref">transform</span>. The <span class="title-ref">TransformerMixin</span> uses the <span class="title-ref">\_\_init\_subclass\_\_</span> mechanism to consume <span class="title-ref">auto\_wrap\_output\_keys</span> and pass all other keyword arguments to it's super class. Super classes' <span class="title-ref">\_\_init\_subclass\_\_</span> should **not** depend on <span class="title-ref">auto\_wrap\_output\_keys</span>.

For transformers that return multiple arrays in <span class="title-ref">transform</span>, auto wrapping will only wrap the first array and not alter the other arrays.

See \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_set\_output.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_set\_output.py) for an example on how to use the API.

## Developer API for <span class="title-ref">check\_is\_fitted</span>

By default <span class="title-ref">\~sklearn.utils.validation.check\_is\_fitted</span> checks if there are any attributes in the instance with a trailing underscore, e.g. <span class="title-ref">coef\_</span>. An estimator can change the behavior by implementing a <span class="title-ref">\_\_sklearn\_is\_fitted\_\_</span> method taking no input and returning a boolean. If this method exists, <span class="title-ref">\~sklearn.utils.validation.check\_is\_fitted</span> simply returns its output.

See \[sphx\_glr\_auto\_examples\_developing\_estimators\_sklearn\_is\_fitted.py\](\#sphx\_glr\_auto\_examples\_developing\_estimators\_sklearn\_is\_fitted.py) for an example on how to use the API.

## Developer API for HTML representation

\> **Warning** \> The HTML representation API is experimental and the API is subject to change.

Estimators inheriting from <span class="title-ref">\~sklearn.base.BaseEstimator</span> display a HTML representation of themselves in interactive programming environments such as Jupyter notebooks. For instance, we can display this HTML diagram:

    from sklearn.base import BaseEstimator
    
    BaseEstimator()

The raw HTML representation is obtained by invoking the function <span class="title-ref">\~sklearn.utils.estimator\_html\_repr</span> on an estimator instance.

To customize the URL linking to an estimator's documentation (i.e. when clicking on the "?" icon), override the <span class="title-ref">\_doc\_link\_module</span> and <span class="title-ref">\_doc\_link\_template</span> attributes. In addition, you can provide a <span class="title-ref">\_doc\_link\_url\_param\_generator</span> method. Set <span class="title-ref">\_doc\_link\_module</span> to the name of the (top level) module that contains your estimator. If the value does not match the top level module name, the HTML representation will not contain a link to the documentation. For scikit-learn estimators this is set to <span class="title-ref">"sklearn"</span>.

The <span class="title-ref">\_doc\_link\_template</span> is used to construct the final URL. By default, it can contain two variables: <span class="title-ref">estimator\_module</span> (the full name of the module containing the estimator) and <span class="title-ref">estimator\_name</span> (the class name of the estimator). If you need more variables you should implement the <span class="title-ref">\_doc\_link\_url\_param\_generator</span> method which should return a dictionary of the variables and their values. This dictionary will be used to render the <span class="title-ref">\_doc\_link\_template</span>.

## Coding guidelines

The following are some guidelines on how new code should be written for inclusion in scikit-learn, and which may be appropriate to adopt in external projects. Of course, there are special cases and there will be exceptions to these rules. However, following these rules when submitting new code makes the review easier so new code can be integrated in less time.

Uniformly formatted code makes it easier to share code ownership. The scikit-learn project tries to closely follow the official Python guidelines detailed in [PEP8](https://www.python.org/dev/peps/pep-0008) that detail how code should be formatted and indented. Please read it and follow it.

In addition, we add the following guidelines:

  - Use underscores to separate words in non class names: `n_samples` rather than `nsamples`.
  - Avoid multiple statements on one line. Prefer a line return after a control flow statement (`if`/`for`).
  - Use relative imports for references inside scikit-learn.
  - Unit tests are an exception to the previous rule; they should use absolute imports, exactly as client code would. A corollary is that, if `sklearn.foo` exports a class or function that is implemented in `sklearn.foo.bar.baz`, the test should import it from `sklearn.foo`.
  - **Please don't use** `import *` **in any case**. It is considered harmful by the [official Python recommendations](https://docs.python.org/3.1/howto/doanddont.html#at-module-level). It makes the code harder to read as the origin of symbols is no longer explicitly referenced, but most important, it prevents using a static analysis tool like [pyflakes](https://divmod.readthedocs.io/en/latest/products/pyflakes.html) to automatically find bugs in scikit-learn.
  - Use the [numpy docstring standard](https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard) in all your docstrings.

A good example of code that we like can be found [here](https://gist.github.com/nateGeorge/5455d2c57fb33c1ae04706f2dc4fee01).

### Input validation

<div class="currentmodule">

sklearn.utils

</div>

The module `sklearn.utils` contains various functions for doing input validation and conversion. Sometimes, `np.asarray` suffices for validation; do *not* use `np.asanyarray` or `np.atleast_2d`, since those let NumPy's `np.matrix` through, which has a different API (e.g., `*` means dot product on `np.matrix`, but Hadamard product on `np.ndarray`).

In other cases, be sure to call <span class="title-ref">check\_array</span> on any array-like argument passed to a scikit-learn API function. The exact parameters to use depends mainly on whether and which `scipy.sparse` matrices must be accepted.

For more information, refer to the \[developers-utils\](\#developers-utils) page.

### Random Numbers

If your code depends on a random number generator, do not use `numpy.random.random()` or similar routines. To ensure repeatability in error checking, the routine should accept a keyword `random_state` and use this to construct a `numpy.random.RandomState` object. See <span class="title-ref">sklearn.utils.check\_random\_state</span> in \[developers-utils\](\#developers-utils).

Here's a simple example of code using some of the above guidelines:

    from sklearn.utils import check_array, check_random_state
    
    def choose_random_sample(X, random_state=0):
        """Choose a random point from X.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            An array representing the data.
        random_state : int or RandomState instance, default=0
            The seed of the pseudo random number generator that selects a
            random sample. Pass an int for reproducible output across multiple
            function calls.
            See :term:`Glossary <random_state>`.
    
        Returns
        -------
        x : ndarray of shape (n_features,)
            A random point selected from X.
        """
        X = check_array(X)
        random_state = check_random_state(random_state)
        i = random_state.randint(X.shape[0])
        return X[i]

If you use randomness in an estimator instead of a freestanding function, some additional guidelines apply.

First off, the estimator should take a `random_state` argument to its `__init__` with a default value of `None`. It should store that argument's value, **unmodified**, in an attribute `random_state`. `fit` can call `check_random_state` on that attribute to get an actual random number generator. If, for some reason, randomness is needed after `fit`, the RNG should be stored in an attribute `random_state_`. The following example should make this clear:

    class GaussianNoise(BaseEstimator, TransformerMixin):
        """This estimator ignores its input and returns random Gaussian noise.
    
        It also does not adhere to all scikit-learn conventions,
        but showcases how to handle randomness.
        """
    
        def __init__(self, n_components=100, random_state=None):
            self.random_state = random_state
            self.n_components = n_components
    
        # the arguments are ignored anyway, so we make them optional
        def fit(self, X=None, y=None):
            self.random_state_ = check_random_state(self.random_state)
    
        def transform(self, X):
            n_samples = X.shape[0]
            return self.random_state_.randn(n_samples, self.n_components)

The reason for this setup is reproducibility: when an estimator is `fit` twice to the same data, it should produce an identical model both times, hence the validation in `fit`, not `__init__`.

### Numerical assertions in tests

When asserting the quasi-equality of arrays of continuous values, do use <span class="title-ref">sklearn.utils.\_testing.assert\_allclose</span>.

The relative tolerance is automatically inferred from the provided arrays dtypes (for float32 and float64 dtypes in particular) but you can override via `rtol`.

When comparing arrays of zero-elements, please do provide a non-zero value for the absolute tolerance via `atol`.

For more information, please refer to the docstring of <span class="title-ref">sklearn.utils.\_testing.assert\_allclose</span>.

---

index.md

---

# Developer's Guide

<div class="toctree">

contributing minimal\_reproducer develop tips utilities performance cython advanced\_installation bug\_triaging maintainer plotting

</div>

---

minimal_reproducer.md

---

# Crafting a minimal reproducer for scikit-learn

Whether submitting a bug report, designing a suite of tests, or simply posting a question in the discussions, being able to craft minimal, reproducible examples (or minimal, workable examples) is the key to communicating effectively and efficiently with the community.

There are very good guidelines on the internet such as [this StackOverflow document](https://stackoverflow.com/help/mcve) or [this blogpost by Matthew Rocklin](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) on crafting Minimal Complete Verifiable Examples (referred below as MCVE). Our goal is not to be repetitive with those references but rather to provide a step-by-step guide on how to narrow down a bug until you have reached the shortest possible code to reproduce it.

The first step before submitting a bug report to scikit-learn is to read the [Issue template](https://github.com/scikit-learn/scikit-learn/blob/main/.github/ISSUE_TEMPLATE/bug_report.yml). It is already quite informative about the information you will be asked to provide.

## Good practices

In this section we will focus on the **Steps/Code to Reproduce** section of the [Issue template](https://github.com/scikit-learn/scikit-learn/blob/main/.github/ISSUE_TEMPLATE/bug_report.yml). We will start with a snippet of code that already provides a failing example but that has room for readability improvement. We then craft a MCVE from it.

**Example**

`` `python     # I am currently working in a ML project and when I tried to fit a     # GradientBoostingRegressor instance to my_data.csv I get a UserWarning:     # "X has feature names, but DecisionTreeRegressor was fitted without     # feature names". You can get a copy of my dataset from     # https://example.com/my_data.csv and verify my features do have     # names. The problem seems to arise during fit when I pass an integer     # to the n_iter_no_change parameter.      df = pd.read_csv('my_data.csv')     X = df[["feature_name"]] # my features do have names     y = df["target"]      # We set random_state=42 for the train_test_split     X_train, X_test, y_train, y_test = train_test_split(         X, y, test_size=0.33, random_state=42     )      scaler = StandardScaler(with_mean=False)     X_train = scaler.fit_transform(X_train)     X_test = scaler.transform(X_test)      # An instance with default n_iter_no_change raises no error nor warnings     gbdt = GradientBoostingRegressor(random_state=0)     gbdt.fit(X_train, y_train)     default_score = gbdt.score(X_test, y_test)      # the bug appears when I change the value for n_iter_no_change     gbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=5)     gbdt.fit(X_train, y_train)     other_score = gbdt.score(X_test, y_test)      other_score = gbdt.score(X_test, y_test)   Provide a failing code example with minimal comments ``\` ----------------------------------------------------

Writing instructions to reproduce the problem in English is often ambiguous. Better make sure that all the necessary details to reproduce the problem are illustrated in the Python code snippet to avoid any ambiguity. Besides, by this point you already provided a concise description in the **Describe the bug** section of the [Issue template](https://github.com/scikit-learn/scikit-learn/blob/main/.github/ISSUE_TEMPLATE/bug_report.yml).

The following code, while **still not minimal**, is already **much better** because it can be copy-pasted in a Python terminal to reproduce the problem in one step. In particular:

  - it contains **all necessary imports statements**;
  - it can fetch the public dataset without having to manually download a file and put it in the expected location on the disk.

**Improved example**

`` `python     import pandas as pd      df = pd.read_csv("https://example.com/my_data.csv")     X = df[["feature_name"]]     y = df["target"]      from sklearn.model_selection import train_test_split      X_train, X_test, y_train, y_test = train_test_split(         X, y, test_size=0.33, random_state=42     )      from sklearn.preprocessing import StandardScaler      scaler = StandardScaler(with_mean=False)     X_train = scaler.fit_transform(X_train)     X_test = scaler.transform(X_test)      from sklearn.ensemble import GradientBoostingRegressor      gbdt = GradientBoostingRegressor(random_state=0)     gbdt.fit(X_train, y_train)  # no warning     default_score = gbdt.score(X_test, y_test)      gbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=5)     gbdt.fit(X_train, y_train)  # raises warning     other_score = gbdt.score(X_test, y_test)     other_score = gbdt.score(X_test, y_test)   Boil down your script to something as small as possible ``\` -------------------------------------------------------

You have to ask yourself which lines of code are relevant and which are not for reproducing the bug. Deleting unnecessary lines of code or simplifying the function calls by omitting unrelated non-default options will help you and other contributors narrow down the cause of the bug.

In particular, for this specific example:

  - the warning has nothing to do with the <span class="title-ref">train\_test\_split</span> since it already appears in the training step, before we use the test set.
  - similarly, the lines that compute the scores on the test set are not necessary;
  - the bug can be reproduced for any value of <span class="title-ref">random\_state</span> so leave it to its default;
  - the bug can be reproduced without preprocessing the data with the <span class="title-ref">StandardScaler</span>.

**Improved example**

`` `python     import pandas as pd     df = pd.read_csv("https://example.com/my_data.csv")     X = df[["feature_name"]]     y = df["target"]      from sklearn.ensemble import GradientBoostingRegressor      gbdt = GradientBoostingRegressor()     gbdt.fit(X, y)  # no warning      gbdt = GradientBoostingRegressor(n_iter_no_change=5)     gbdt.fit(X, y)  # raises warning   **DO NOT** report your data unless it is extremely necessary ``\` ------------------------------------------------------------

The idea is to make the code as self-contained as possible. For doing so, you can use a \[synth\_data\](\#synth\_data). It can be generated using numpy, pandas or the `sklearn.datasets` module. Most of the times the bug is not related to a particular structure of your data. Even if it is, try to find an available dataset that has similar characteristics to yours and that reproduces the problem. In this particular case, we are interested in data that has labeled feature names.

**Improved example**

`` `python     import pandas as pd     from sklearn.ensemble import GradientBoostingRegressor      df = pd.DataFrame(         {             "feature_name": [-12.32, 1.43, 30.01, 22.17],             "target": [72, 55, 32, 43],         }     )     X = df[["feature_name"]]     y = df["target"]      gbdt = GradientBoostingRegressor()     gbdt.fit(X, y) # no warning     gbdt = GradientBoostingRegressor(n_iter_no_change=5)     gbdt.fit(X, y) # raises warning  As already mentioned, the key to communication is the readability of the code ``\` and good formatting can really be a plus. Notice that in the previous snippet we:

  - try to limit all lines to a maximum of 79 characters to avoid horizontal scrollbars in the code snippets blocks rendered on the GitHub issue;
  - use blank lines to separate groups of related functions;
  - place all the imports in their own group at the beginning.

The simplification steps presented in this guide can be implemented in a different order than the progression we have shown here. The important points are:

  - a minimal reproducer should be runnable by a simple copy-and-paste in a python terminal;
  - it should be simplified as much as possible by removing any code steps that are not strictly needed to reproducing the original problem;
  - it should ideally only rely on a minimal dataset generated on-the-fly by running the code instead of relying on external data, if possible.

### Use markdown formatting

To format code or text into its own distinct block, use triple backticks. [Markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) supports an optional language identifier to enable syntax highlighting in your fenced code block. For example:

    ```python
    from sklearn.datasets import make_blobs
    
    n_samples = 100
    n_components = 3
    X, y = make_blobs(n_samples=n_samples, centers=n_components)
    ```

will render a python formatted snippet as follows

`` `python     from sklearn.datasets import make_blobs      n_samples = 100     n_components = 3     X, y = make_blobs(n_samples=n_samples, centers=n_components)  It is not necessary to create several blocks of code when submitting a bug ``\` report. Remember other reviewers are going to copy-paste your code and having a single cell will make their task easier.

In the section named **Actual results** of the [Issue template](https://github.com/scikit-learn/scikit-learn/blob/main/.github/ISSUE_TEMPLATE/bug_report.yml) you are asked to provide the error message including the full traceback of the exception. In this case, use the <span class="title-ref">python-traceback</span> qualifier. For example:

    ```python-traceback
    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    <ipython-input-1-a674e682c281> in <module>
        4 vectorizer = CountVectorizer(input=docs, analyzer='word')
        5 lda_features = vectorizer.fit_transform(docs)
    ----> 6 lda_model = LatentDirichletAllocation(
        7     n_topics=10,
        8     learning_method='online',
    
    TypeError: __init__() got an unexpected keyword argument 'n_topics'
    ```

yields the following when rendered:

`` `python     ---------------------------------------------------------------------------     TypeError                                 Traceback (most recent call last)     <ipython-input-1-a674e682c281> in <module>         4 vectorizer = CountVectorizer(input=docs, analyzer='word')         5 lda_features = vectorizer.fit_transform(docs)     ----> 6 lda_model = LatentDirichletAllocation(         7     n_topics=10,         8     learning_method='online',      TypeError: __init__() got an unexpected keyword argument 'n_topics'   .. _synth_data:  Synthetic dataset ``\` =================

Before choosing a particular synthetic dataset, first you have to identify the type of problem you are solving: Is it a classification, a regression, a clustering, etc?

Once that you narrowed down the type of problem, you need to provide a synthetic dataset accordingly. Most of the times you only need a minimalistic dataset. Here is a non-exhaustive list of tools that may help you.

### NumPy

NumPy tools such as [numpy.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html) and [numpy.random.randint](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) can be used to create dummy numeric data.

  - regression
    
    Regressions take continuous numeric data as features and target.
    
      - \`\`\`python  
        import numpy as np
        
        rng = np.random.RandomState(0) n\_samples, n\_features = 5, 5 X = rng.randn(n\_samples, n\_features) y = rng.randn(n\_samples)

A similar snippet can be used as synthetic data when testing scaling tools such `` ` as `sklearn.preprocessing.StandardScaler`.  - classification    If the bug is not raised during when encoding a categorical variable, you can   feed numeric data to a classifier. Just remember to ensure that the target   is indeed an integer. ``\`python import numpy as np

> rng = np.random.RandomState(0) n\_samples, n\_features = 5, 5 X = rng.randn(n\_samples, n\_features) y = rng.randint(0, 2, n\_samples) \# binary target with values in {0, 1}
> 
> If the bug only happens with non-numeric class labels, you might want to generate a random target with [numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html).
> 
> ``` python
> ```
> 
> import numpy as np
> 
> rng = np.random.RandomState(0) n\_samples, n\_features = 50, 5 X = rng.randn(n\_samples, n\_features) y = np.random.choice( \["male", "female", "other"\], size=n\_samples, p=\[0.49, 0.49, 0.02\] )

Pandas `` ` ------  Some scikit-learn objects expect pandas dataframes as input. In this case you can transform numpy arrays into pandas objects using `pandas.DataFrame <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html>`_, or `pandas.Series <https://pandas.pydata.org/docs/reference/api/pandas.Series.html>`_. ``\`python import numpy as np import pandas as pd

> rng = np.random.RandomState(0) n\_samples, n\_features = 5, 5 X = pd.DataFrame( { "continuous\_feature": rng.randn(n\_samples), "positive\_feature": rng.uniform(low=0.0, high=100.0, size=n\_samples), "categorical\_feature": rng.choice(\["a", "b", "c"\], size=n\_samples), } ) y = pd.Series(rng.randn(n\_samples))

In addition, scikit-learn includes various \[sample\_generators\](\#sample\_generators) that can be `` ` used to build artificial datasets of controlled size and complexity.  `make_regression` -----------------  As hinted by the name, `sklearn.datasets.make_regression` produces regression targets with noise as an optionally-sparse random linear combination of random features. ``\`python from sklearn.datasets import make\_regression

> X, y = make\_regression(n\_samples=1000, n\_features=20)

<span class="title-ref">make\_classification</span> `` ` ---------------------  `sklearn.datasets.make_classification` creates multiclass datasets with multiple Gaussian clusters per class. Noise can be introduced by means of correlated, redundant or uninformative features. ``\`python from sklearn.datasets import make\_classification

>   - X, y = make\_classification(  
>     n\_features=2, n\_redundant=0, n\_informative=2, n\_clusters\_per\_class=1
> 
> )

<span class="title-ref">make\_blobs</span> `` ` ------------  Similarly to `make_classification`, `sklearn.datasets.make_blobs` creates multiclass datasets using normally-distributed clusters of points. It provides greater control regarding the centers and standard deviations of each cluster, and therefore it is useful to demonstrate clustering. ``\`python from sklearn.datasets import make\_blobs

> X, y = make\_blobs(n\_samples=10, centers=3, n\_features=2)

Dataset loading utilities `` ` -------------------------  You can use the [datasets](#datasets) to load and fetch several popular reference datasets. This option is useful when the bug relates to the particular structure of the data, e.g. dealing with missing values or image recognition. ``\`python from sklearn.datasets import load\_breast\_cancer

X, y = load\_breast\_cancer(return\_X\_y=True) \`\`\`

---

performance.md

---

# How to optimize for speed

The following gives some practical guidelines to help you write efficient code for the scikit-learn project.

\> **Note** \> While it is always useful to profile your code so as to **check performance assumptions**, it is also highly recommended to **review the literature** to ensure that the implemented algorithm is the state of the art for the task before investing into costly implementation optimization.

> Times and times, hours of efforts invested in optimizing complicated implementation details have been rendered irrelevant by the subsequent discovery of simple **algorithmic tricks**, or by using another algorithm altogether that is better suited to the problem.
> 
> The section \[warm-restarts\](\#warm-restarts) gives an example of such a trick.

## Python, Cython or C/C++?

<div class="currentmodule">

sklearn

</div>

In general, the scikit-learn project emphasizes the **readability** of the source code to make it easy for the project users to dive into the source code so as to understand how the algorithm behaves on their data but also for ease of maintainability (by the developers).

When implementing a new algorithm is thus recommended to **start implementing it in Python using Numpy and Scipy** by taking care of avoiding looping code using the vectorized idioms of those libraries. In practice this means trying to **replace any nested for loops by calls to equivalent Numpy array methods**. The goal is to avoid the CPU wasting time in the Python interpreter rather than crunching numbers to fit your statistical model. It's generally a good idea to consider NumPy and SciPy performance tips: <https://scipy.github.io/old-wiki/pages/PerformanceTips>

Sometimes however an algorithm cannot be expressed efficiently in simple vectorized Numpy code. In this case, the recommended strategy is the following:

1.  **Profile** the Python implementation to find the main bottleneck and isolate it in a **dedicated module level function**. This function will be reimplemented as a compiled extension module.
2.  If there exists a well maintained BSD or MIT **C/C++** implementation of the same algorithm that is not too big, you can write a **Cython wrapper** for it and include a copy of the source code of the library in the scikit-learn source tree: this strategy is used for the classes <span class="title-ref">svm.LinearSVC</span>, <span class="title-ref">svm.SVC</span> and <span class="title-ref">linear\_model.LogisticRegression</span> (wrappers for liblinear and libsvm).
3.  Otherwise, write an optimized version of your Python function using **Cython** directly. This strategy is used for the <span class="title-ref">linear\_model.ElasticNet</span> and <span class="title-ref">linear\_model.SGDClassifier</span> classes for instance.
4.  **Move the Python version of the function in the tests** and use it to check that the results of the compiled extension are consistent with the gold standard, easy to debug Python version.
5.  Once the code is optimized (not simple bottleneck spottable by profiling), check whether it is possible to have **coarse grained parallelism** that is amenable to **multi-processing** by using the `joblib.Parallel` class.

## Profiling Python code

In order to profile Python code we recommend to write a script that loads and prepare you data and then use the IPython integrated profiler for interactively exploring the relevant part for the code.

Suppose we want to profile the Non Negative Matrix Factorization module of scikit-learn. Let us setup a new IPython session and load the digits dataset and as in the \[sphx\_glr\_auto\_examples\_classification\_plot\_digits\_classification.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_digits\_classification.py) example:

    In [1]: from sklearn.decomposition import NMF
    
    In [2]: from sklearn.datasets import load_digits
    
    In [3]: X, _ = load_digits(return_X_y=True)

Before starting the profiling session and engaging in tentative optimization iterations, it is important to measure the total execution time of the function we want to optimize without any kind of profiler overhead and save it somewhere for later reference:

    In [4]: %timeit NMF(n_components=16, tol=1e-2).fit(X)
    1 loops, best of 3: 1.7 s per loop

To have a look at the overall performance profile using the `%prun` magic command:

    In [5]: %prun -l nmf.py NMF(n_components=16, tol=1e-2).fit(X)
             14496 function calls in 1.682 CPU seconds
    
       Ordered by: internal time
       List reduced from 90 to 9 due to restriction <'nmf.py'>
    
       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
           36    0.609    0.017    1.499    0.042 nmf.py:151(_nls_subproblem)
         1263    0.157    0.000    0.157    0.000 nmf.py:18(_pos)
            1    0.053    0.053    1.681    1.681 nmf.py:352(fit_transform)
          673    0.008    0.000    0.057    0.000 nmf.py:28(norm)
            1    0.006    0.006    0.047    0.047 nmf.py:42(_initialize_nmf)
           36    0.001    0.000    0.010    0.000 nmf.py:36(_sparseness)
           30    0.001    0.000    0.001    0.000 nmf.py:23(_neg)
            1    0.000    0.000    0.000    0.000 nmf.py:337(__init__)
            1    0.000    0.000    1.681    1.681 nmf.py:461(fit)

The `tottime` column is the most interesting: it gives to total time spent executing the code of a given function ignoring the time spent in executing the sub-functions. The real total time (local code + sub-function calls) is given by the `cumtime` column.

Note the use of the `-l nmf.py` that restricts the output to lines that contains the "nmf.py" string. This is useful to have a quick look at the hotspot of the nmf Python module it-self ignoring anything else.

Here is the beginning of the output of the same command without the `-l nmf.py` filter:

    In [5] %prun NMF(n_components=16, tol=1e-2).fit(X)
             16159 function calls in 1.840 CPU seconds
    
       Ordered by: internal time
    
       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
         2833    0.653    0.000    0.653    0.000 {numpy.core._dotblas.dot}
           46    0.651    0.014    1.636    0.036 nmf.py:151(_nls_subproblem)
         1397    0.171    0.000    0.171    0.000 nmf.py:18(_pos)
         2780    0.167    0.000    0.167    0.000 {method 'sum' of 'numpy.ndarray' objects}
            1    0.064    0.064    1.840    1.840 nmf.py:352(fit_transform)
         1542    0.043    0.000    0.043    0.000 {method 'flatten' of 'numpy.ndarray' objects}
          337    0.019    0.000    0.019    0.000 {method 'all' of 'numpy.ndarray' objects}
         2734    0.011    0.000    0.181    0.000 fromnumeric.py:1185(sum)
            2    0.010    0.005    0.010    0.005 {numpy.linalg.lapack_lite.dgesdd}
          748    0.009    0.000    0.065    0.000 nmf.py:28(norm)
    ...

The above results show that the execution is largely dominated by dot products operations (delegated to blas). Hence there is probably no huge gain to expect by rewriting this code in Cython or C/C++: in this case out of the 1.7s total execution time, almost 0.7s are spent in compiled code we can consider optimal. By rewriting the rest of the Python code and assuming we could achieve a 1000% boost on this portion (which is highly unlikely given the shallowness of the Python loops), we would not gain more than a 2.4x speed-up globally.

Hence major improvements can only be achieved by **algorithmic improvements** in this particular example (e.g. trying to find operation that are both costly and useless to avoid computing then rather than trying to optimize their implementation).

It is however still interesting to check what's happening inside the `_nls_subproblem` function which is the hotspot if we only consider Python code: it takes around 100% of the accumulated time of the module. In order to better understand the profile of this specific function, let us install `line_profiler` and wire it to IPython:

<div class="prompt">

bash $

pip install line\_profiler

</div>

**Under IPython 0.13+**, first create a configuration profile:

<div class="prompt">

bash $

ipython profile create

</div>

Then register the line\_profiler extension in `~/.ipython/profile_default/ipython_config.py`:

    c.TerminalIPythonApp.extensions.append('line_profiler')
    c.InteractiveShellApp.extensions.append('line_profiler')

This will register the `%lprun` magic command in the IPython terminal application and the other frontends such as qtconsole and notebook.

Now restart IPython and let us use this new toy:

    In [1]: from sklearn.datasets import load_digits
    
    In [2]: from sklearn.decomposition import NMF
      ... : from sklearn.decomposition._nmf import _nls_subproblem
    
    In [3]: X, _ = load_digits(return_X_y=True)
    
    In [4]: %lprun -f _nls_subproblem NMF(n_components=16, tol=1e-2).fit(X)
    Timer unit: 1e-06 s
    
    File: sklearn/decomposition/nmf.py
    Function: _nls_subproblem at line 137
    Total time: 1.73153 s
    
    Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
       137                                           def _nls_subproblem(V, W, H_init, tol, max_iter):
       138                                               """Non-negative least square solver
       ...
       170                                               """
       171        48         5863    122.1      0.3      if (H_init < 0).any():
       172                                                   raise ValueError("Negative values in H_init passed to NLS solver.")
       173
       174        48          139      2.9      0.0      H = H_init
       175        48       112141   2336.3      5.8      WtV = np.dot(W.T, V)
       176        48        16144    336.3      0.8      WtW = np.dot(W.T, W)
       177
       178                                               # values justified in the paper
       179        48          144      3.0      0.0      alpha = 1
       180        48          113      2.4      0.0      beta = 0.1
       181       638         1880      2.9      0.1      for n_iter in range(1, max_iter + 1):
       182       638       195133    305.9     10.2          grad = np.dot(WtW, H) - WtV
       183       638       495761    777.1     25.9          proj_gradient = norm(grad[np.logical_or(grad < 0, H > 0)])
       184       638         2449      3.8      0.1          if proj_gradient < tol:
       185        48          130      2.7      0.0              break
       186
       187      1474         4474      3.0      0.2          for inner_iter in range(1, 20):
       188      1474        83833     56.9      4.4              Hn = H - alpha * grad
       189                                                       # Hn = np.where(Hn > 0, Hn, 0)
       190      1474       194239    131.8     10.1              Hn = _pos(Hn)
       191      1474        48858     33.1      2.5              d = Hn - H
       192      1474       150407    102.0      7.8              gradd = np.sum(grad * d)
       193      1474       515390    349.7     26.9              dQd = np.sum(np.dot(WtW, d) * d)
       ...

By looking at the top values of the `% Time` column it is really easy to pin-point the most expensive expressions that would deserve additional care.

## Memory usage profiling

You can analyze in detail the memory usage of any Python code with the help of [memory\_profiler](https://pypi.org/project/memory_profiler/). First, install the latest version:

<div class="prompt">

bash $

pip install -U memory\_profiler

</div>

Then, setup the magics in a manner similar to `line_profiler`.

**Under IPython 0.11+**, first create a configuration profile:

<div class="prompt">

bash $

ipython profile create

</div>

Then register the extension in `~/.ipython/profile_default/ipython_config.py` alongside the line profiler:

    c.TerminalIPythonApp.extensions.append('memory_profiler')
    c.InteractiveShellApp.extensions.append('memory_profiler')

This will register the `%memit` and `%mprun` magic commands in the IPython terminal application and the other frontends such as qtconsole and notebook.

`%mprun` is useful to examine, line-by-line, the memory usage of key functions in your program. It is very similar to `%lprun`, discussed in the previous section. For example, from the `memory_profiler` `examples` directory:

    In [1] from example import my_func
    
    In [2] %mprun -f my_func my_func()
    Filename: example.py
    
    Line #    Mem usage  Increment   Line Contents
    ==============================================
         3                           @profile
         4      5.97 MB    0.00 MB   def my_func():
         5     13.61 MB    7.64 MB       a = [1] * (10 ** 6)
         6    166.20 MB  152.59 MB       b = [2] * (2 * 10 ** 7)
         7     13.61 MB -152.59 MB       del b
         8     13.61 MB    0.00 MB       return a

Another useful magic that `memory_profiler` defines is `%memit`, which is analogous to `%timeit`. It can be used as follows:

    In [1]: import numpy as np
    
    In [2]: %memit np.zeros(1e7)
    maximum of 3: 76.402344 MB per loop

For more details, see the docstrings of the magics, using `%memit?` and `%mprun?`.

## Using Cython

If profiling of the Python code reveals that the Python interpreter overhead is larger by one order of magnitude or more than the cost of the actual numerical computation (e.g. `for` loops over vector components, nested evaluation of conditional expression, scalar arithmetic...), it is probably adequate to extract the hotspot portion of the code as a standalone function in a `.pyx` file, add static type declarations and then use Cython to generate a C program suitable to be compiled as a Python extension module.

The [Cython's documentation](http://docs.cython.org/) contains a tutorial and reference guide for developing such a module. For more information about developing in Cython for scikit-learn, see \[cython\](\#cython).

## Profiling compiled extensions

When working with compiled extensions (written in C/C++ with a wrapper or directly as Cython extension), the default Python profiler is useless: we need a dedicated tool to introspect what's happening inside the compiled extension it-self.

### Using yep and gperftools

Easy profiling without special compilation options use yep:

  - <https://pypi.org/project/yep/>
  - <https://fa.bianp.net/blog/2011/a-profiler-for-python-extensions>

### Using a debugger, gdb

  - It is helpful to use `gdb` to debug. In order to do so, one must use a Python interpreter built with debug support (debug symbols and proper optimization). To create a new conda environment (which you might need to deactivate and reactivate after building/installing) with a source-built CPython interpreter:
      - \`\`\`bash  
        git clone <https://github.com/python/cpython.git> conda create -n debug-scikit-dev conda activate debug-scikit-dev cd cpython mkdir debug cd debug ../configure --prefix=$CONDA\_PREFIX --with-pydebug make EXTRA\_CFLAGS='-DPy\_DEBUG' -j\<num\_cores\> make install

Using gprof `` ` -----------  In order to profile compiled Python extensions one could use ``gprof`after having recompiled the project with`gcc -pg`and using the`python-dbg`variant of the interpreter on debian / ubuntu: however this approach requires to also have`numpy`and`scipy`recompiled with`-pg`which is rather complicated to get working.  Fortunately there exist two alternative profilers that don't require you to recompile everything.  Using valgrind / callgrind / kcachegrind ----------------------------------------  kcachegrind ~~~~~~~~~~~`yep`can be used to create a profiling report.`kcachegrind`provides a graphical environment to visualize this report:  .. prompt:: bash $    # Run yep to profile some python script   python -m yep -c my_file.py  .. prompt:: bash $    # open my_file.py.callgrin with kcachegrind   kcachegrind my_file.py.prof  > **Note** >`yep`can be executed with the argument`--lines`or`-l`to compile    a profiling report 'line by line'.  Multi-core parallelism using`joblib.Parallel\`\` ================================================

See [joblib documentation](https://joblib.readthedocs.io)

## A simple algorithmic trick: warm restarts

See the glossary entry for `warm_start`

---

plotting.md

---

# Developing with the Plotting API

Scikit-learn defines a simple API for creating visualizations for machine learning. The key features of this API is to run calculations once and to have the flexibility to adjust the visualizations after the fact. This section is intended for developers who wish to develop or maintain plotting tools. For usage, users should refer to the \[User Guide \<visualizations\>\](\#user-guide-\<visualizations\>).

## Plotting API Overview

This logic is encapsulated into a display object where the computed data is stored and the plotting is done in a <span class="title-ref">plot</span> method. The display object's <span class="title-ref">\_\_init\_\_</span> method contains only the data needed to create the visualization. The <span class="title-ref">plot</span> method takes in parameters that only have to do with visualization, such as a matplotlib axes. The <span class="title-ref">plot</span> method will store the matplotlib artists as attributes allowing for style adjustments through the display object. The <span class="title-ref">Display</span> class should define one or both class methods: <span class="title-ref">from\_estimator</span> and <span class="title-ref">from\_predictions</span>. These methods allows to create the <span class="title-ref">Display</span> object from the estimator and some data or from the true and predicted values. After these class methods create the display object with the computed values, then call the display's plot method. Note that the <span class="title-ref">plot</span> method defines attributes related to matplotlib, such as the line artist. This allows for customizations after calling the <span class="title-ref">plot</span> method.

For example, the <span class="title-ref">RocCurveDisplay</span> defines the following methods and attributes:

    class RocCurveDisplay:
        def __init__(self, fpr, tpr, roc_auc, estimator_name):
            ...
            self.fpr = fpr
            self.tpr = tpr
            self.roc_auc = roc_auc
            self.estimator_name = estimator_name
    
        @classmethod
        def from_estimator(cls, estimator, X, y):
            # get the predictions
            y_pred = estimator.predict_proba(X)[:, 1]
            return cls.from_predictions(y, y_pred, estimator.__class__.__name__)
    
        @classmethod
        def from_predictions(cls, y, y_pred, estimator_name):
            # do ROC computation from y and y_pred
            fpr, tpr, roc_auc = ...
            viz = RocCurveDisplay(fpr, tpr, roc_auc, estimator_name)
            return viz.plot()
    
        def plot(self, ax=None, name=None, **kwargs):
            ...
            self.line_ = ...
            self.ax_ = ax
            self.figure_ = ax.figure_

Read more in \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_roc\_curve\_visualization\_api.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_roc\_curve\_visualization\_api.py) and the \[User Guide \<visualizations\>\](\#user-guide-\<visualizations\>).

## Plotting with Multiple Axes

Some of the plotting tools like <span class="title-ref">\~sklearn.inspection.PartialDependenceDisplay.from\_estimator</span> and <span class="title-ref">\~sklearn.inspection.PartialDependenceDisplay</span> support plotting on multiple axes. Two different scenarios are supported:

1\. If a list of axes is passed in, <span class="title-ref">plot</span> will check if the number of axes is consistent with the number of axes it expects and then draws on those axes. 2. If a single axes is passed in, that axes defines a space for multiple axes to be placed. In this case, we suggest using matplotlib's <span class="title-ref">\~matplotlib.gridspec.GridSpecFromSubplotSpec</span> to split up the space:

    import matplotlib.pyplot as plt
    from matplotlib.gridspec import GridSpecFromSubplotSpec
    
    fig, ax = plt.subplots()
    gs = GridSpecFromSubplotSpec(2, 2, subplot_spec=ax.get_subplotspec())
    
    ax_top_left = fig.add_subplot(gs[0, 0])
    ax_top_right = fig.add_subplot(gs[0, 1])
    ax_bottom = fig.add_subplot(gs[1, :])

By default, the <span class="title-ref">ax</span> keyword in <span class="title-ref">plot</span> is <span class="title-ref">None</span>. In this case, the single axes is created and the gridspec api is used to create the regions to plot in.

See for example, <span class="title-ref">\~sklearn.inspection.PartialDependenceDisplay.from\_estimator</span> which plots multiple lines and contours using this API. The axes defining the bounding box is saved in a <span class="title-ref">bounding\_ax\_</span> attribute. The individual axes created are stored in an <span class="title-ref">axes\_</span> ndarray, corresponding to the axes position on the grid. Positions that are not used are set to <span class="title-ref">None</span>. Furthermore, the matplotlib Artists are stored in <span class="title-ref">lines\_</span> and <span class="title-ref">contours\_</span> where the key is the position on the grid. When a list of axes is passed in, the <span class="title-ref">axes\_</span>, <span class="title-ref">lines\_</span>, and <span class="title-ref">contours\_</span> is a 1d ndarray corresponding to the list of axes passed in.

---

tips.md

---

# Developers' Tips and Tricks

## Productivity and sanity-preserving tips

In this section we gather some useful advice and tools that may increase your quality-of-life when reviewing pull requests, running unit tests, and so forth. Some of these tricks consist of userscripts that require a browser extension such as [TamperMonkey](https://tampermonkey.net/) or [GreaseMonkey](https://www.greasespot.net/); to set up userscripts you must have one of these extensions installed, enabled and running. We provide userscripts as GitHub gists; to install them, click on the "Raw" button on the gist page.

### Folding and unfolding outdated diffs on pull requests

GitHub hides discussions on PRs when the corresponding lines of code have been changed in the mean while. This [userscript](https://raw.githubusercontent.com/lesteve/userscripts/master/github-expand-all.user.js) provides a shortcut (Control-Alt-P at the time of writing but look at the code to be sure) to unfold all such hidden discussions at once, so you can catch up.

### Checking out pull requests as remote-tracking branches

In your local fork, add to your `.git/config`, under the `[remote "upstream"]` heading, the line:

    fetch = +refs/pull/*/head:refs/remotes/upstream/pr/*

You may then use `git checkout pr/PR_NUMBER` to navigate to the code of the pull-request with the given number. ([Read more in this gist.](https://gist.github.com/piscisaureus/3342247))

### Display code coverage in pull requests

To overlay the code coverage reports generated by the CodeCov continuous integration, consider [this browser extension](https://github.com/codecov/browser-extension). The coverage of each line will be displayed as a color background behind the line number.

### Useful pytest aliases and flags

The full test suite takes fairly long to run. For faster iterations, it is possibly to select a subset of tests using pytest selectors. In particular, one can run a [single test based on its node ID](https://docs.pytest.org/en/latest/example/markers.html#selecting-tests-based-on-their-node-id):

<div class="prompt">

bash $

pytest -v sklearn/linear\_model/tests/test\_logistic.py::test\_sparsify

</div>

or use the [-k pytest parameter](https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name) to select tests based on their name. For instance,:

<div class="prompt">

bash $

pytest sklearn/tests/test\_common.py -v -k LogisticRegression

</div>

will run all `common tests` for the `LogisticRegression` estimator.

When a unit test fails, the following tricks can make debugging easier:

1.  The command line argument `pytest -l` instructs pytest to print the local variables when a failure occurs.

2.  The argument `pytest --pdb` drops into the Python debugger on failure. To instead drop into the rich IPython debugger `ipdb`, you may set up a shell alias to:
    
    <div class="prompt">
    
    bash $
    
    pytest --pdbcls=IPython.terminal.debugger:TerminalPdb --capture no
    
    </div>

Other <span class="title-ref">pytest</span> options that may become useful include:

  - `-x` which exits on the first failed test,
  - `--lf` to rerun the tests that failed on the previous run,
  - `--ff` to rerun all previous tests, running the ones that failed first,
  - `-s` so that pytest does not capture the output of `print()` statements,
  - `--tb=short` or `--tb=line` to control the length of the logs,
  - `--runxfail` also run tests marked as a known failure (XFAIL) and report errors.

Since our continuous integration tests will error if `FutureWarning` isn't properly caught, it is also recommended to run `pytest` along with the `-Werror::FutureWarning` flag.

### Standard replies for reviewing

It may be helpful to store some of these in GitHub's [saved replies](https://github.com/settings/replies/) for reviewing:

Issue: Usage questions

``` none
You are asking a usage question. The issue tracker is for bugs and new features. For usage questions, it is recommended to try [Stack Overflow](https://stackoverflow.com/questions/tagged/scikit-learn) or [the Mailing List](https://mail.python.org/mailman/listinfo/scikit-learn).

Unfortunately, we need to close this issue as this issue tracker is a communication tool used for the development of scikit-learn. The additional activity created by usage questions crowds it too much and impedes this development. The conversation can continue here, however there is no guarantee that it will receive attention from core developers.
```

Issue: You're welcome to update the docs

``` none
Please feel free to offer a pull request updating the documentation if you feel it could be improved.
```

Issue: Self-contained example for bug

``` none
Please provide [self-contained example code](https://scikit-learn.org/dev/developers/minimal_reproducer.html), including imports and data (if possible), so that other contributors can just run it and reproduce your issue. Ideally your example code should be minimal.
```

Issue: Software versions

```` none
To help diagnose your issue, please paste the output of:
```py
import sklearn; sklearn.show_versions()
```
Thanks.
````

Issue: Code blocks

```` none
Readability can be greatly improved if you [format](https://help.github.com/articles/creating-and-highlighting-code-blocks/) your code snippets and complete error messages appropriately. For example:

    ```python
    print(something)
    ```

generates:

```python
print(something)
```

And:

    ```pytb
    Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
    ImportError: No module named 'hello'
    ```

generates:

```pytb
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ImportError: No module named 'hello'
```

You can edit your issue descriptions and comments at any time to improve readability. This helps maintainers a lot. Thanks!
````

Issue/Comment: Linking to code

``` none
Friendly advice: for clarity's sake, you can link to code like [this](https://help.github.com/articles/creating-a-permanent-link-to-a-code-snippet/).
```

Issue/Comment: Linking to comments

``` none
Please use links to comments, which make it a lot easier to see what you are referring to, rather than just linking to the issue. See [this](https://stackoverflow.com/questions/25163598/how-do-i-reference-a-specific-issue-comment-on-github) for more details.
```

PR-NEW: Better description and title

``` none
Thanks for the pull request! Please make the title of the PR more descriptive. The title will become the commit message when this is merged. You should state what issue (or PR) it fixes/resolves in the description using the syntax described [here](https://scikit-learn.org/dev/developers/contributing.html#contributing-pull-requests).
```

PR-NEW: Fix \#

``` none
Please use "Fix #issueNumber" in your PR description (and you can do it more than once). This way the associated issue gets closed automatically when the PR is merged. For more details, look at [this](https://github.com/blog/1506-closing-issues-via-pull-requests).
```

PR-NEW or Issue: Maintenance cost

``` none
Every feature we include has a [maintenance cost](https://scikit-learn.org/dev/faq.html#why-are-you-so-selective-on-what-algorithms-you-include-in-scikit-learn). Our maintainers are mostly volunteers. For a new feature to be included, we need evidence that it is often useful and, ideally, [well-established](https://scikit-learn.org/dev/faq.html#what-are-the-inclusion-criteria-for-new-algorithms) in the literature or in practice. Also, we expect PR authors to take part in the maintenance for the code they submit, at least initially. That doesn't stop you implementing it for yourself and publishing it in a separate repository, or even [scikit-learn-contrib](https://scikit-learn-contrib.github.io).
```

PR-WIP: What's needed before merge?

``` none
Please clarify (perhaps as a TODO list in the PR description) what work you believe still needs to be done before it can be reviewed for merge. When it is ready, please prefix the PR title with `[MRG]`.
```

PR-WIP: Regression test needed

``` none
Please add a [non-regression test](https://en.wikipedia.org/wiki/Non-regression_testing) that would fail at main but pass in this PR.
```

PR-WIP: PEP8

``` none
You have some [PEP8](https://www.python.org/dev/peps/pep-0008/) violations, whose details you can see in the Circle CI `lint` job. It might be worth configuring your code editor to check for such errors on the fly, so you can catch them before committing.
```

PR-MRG: Patience

``` none
Before merging, we generally require two core developers to agree that your pull request is desirable and ready. [Please be patient](https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention), as we mostly rely on volunteered time from busy core developers. (You are also welcome to help us out with [reviewing other PRs](https://scikit-learn.org/dev/developers/contributing.html#code-review-guidelines).)
```

PR-MRG: Add to what's new

``` none
Please add an entry to the future changelog by adding an RST fragment into the module associated with your change located in `doc/whats_new/upcoming_changes`. Refer to the following [README](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md) for full instructions.
```

PR: Don't change unrelated

``` none
Please do not change unrelated lines. It makes your contribution harder to review and may introduce merge conflicts to other pull requests.
```

### Debugging CI issues

CI issues may arise for a variety of reasons, so this is by no means a comprehensive guide, but rather a list of useful tips and tricks.

#### Using a lock-file to get an environment close to the CI

<span class="title-ref">conda-lock</span> can be used to create a conda environment with the exact same conda and pip packages as on the CI. For example, the following command will create a conda environment named <span class="title-ref">scikit-learn-doc</span> that is similar to the CI:

<div class="prompt">

bash $

conda-lock install -n scikit-learn-doc build\_tools/circle/doc\_linux-64\_conda.lock

</div>

\> **Note** \> It only works if you have the same OS as the CI build (check <span class="title-ref">platform:</span> in the lock-file). For example, the previous command will only work if you are on a Linux machine. Also this may not allow you to reproduce some of the issues that are more tied to the particularities of the CI environment, for example CPU architecture reported by OpenBLAS in <span class="title-ref">sklearn.show\_versions()</span>.

If you don't have the same OS as the CI build you can still create a conda environment from the right environment yaml file, although it won't be as close as the CI environment as using the associated lock-file. For example for the doc build:

<div class="prompt">

bash $

conda env create -n scikit-learn-doc -f build\_tools/circle/doc\_environment.yml -y

</div>

This may not give you exactly the same package versions as in the CI for a variety of reasons, for example:

  - some packages may have had new releases between the time the lock files were last updated in the <span class="title-ref">main</span> branch and the time you run the <span class="title-ref">conda create</span> command. You can always try to look at the version in the lock-file and specify the versions by hand for some specific packages that you think would help reproducing the issue.
  - different packages may be installed by default depending on the OS. For example, the default BLAS library when installing numpy is OpenBLAS on Linux and MKL on Windows.

Also the problem may be OS specific so the only way to be able to reproduce would be to have the same OS as the CI build.

## Debugging memory errors in Cython with valgrind

While python/numpy's built-in memory management is relatively robust, it can lead to performance penalties for some routines. For this reason, much of the high-performance code in scikit-learn is written in cython. This performance gain comes with a tradeoff, however: it is very easy for memory bugs to crop up in cython code, especially in situations where that code relies heavily on pointer arithmetic.

Memory errors can manifest themselves a number of ways. The easiest ones to debug are often segmentation faults and related glibc errors. Uninitialized variables can lead to unexpected behavior that is difficult to track down. A very useful tool when debugging these sorts of errors is [valgrind](https://valgrind.org).

Valgrind is a command-line tool that can trace memory errors in a variety of code. Follow these steps:

1.  Install [valgrind](https://valgrind.org) on your system.

2.  Download the python valgrind suppression file: [valgrind-python.supp](https://github.com/python/cpython/blob/master/Misc/valgrind-python.supp).

3.  Follow the directions in the [README.valgrind](https://github.com/python/cpython/blob/master/Misc/README.valgrind) file to customize your python suppressions. If you don't, you will have spurious output coming related to the python interpreter instead of your own code.

4.  Run valgrind as follows:
    
    <div class="prompt">
    
    bash $
    
    valgrind -v --suppressions=valgrind-python.supp python my\_test\_script.py
    
    </div>

The result will be a list of all the memory-related errors, which reference lines in the C-code generated by cython from your .pyx file. If you examine the referenced lines in the .c file, you will see comments which indicate the corresponding location in your .pyx source file. Hopefully the output will give you clues as to the source of your memory error.

For more information on valgrind and the array of options it has, see the tutorials and documentation on the [valgrind web site](https://valgrind.org).

## Building and testing for the ARM64 platform on a x86\_64 machine

ARM-based machines are a popular target for mobile, edge or other low-energy deployments (including in the cloud, for instance on Scaleway or AWS Graviton).

Here are instructions to setup a local dev environment to reproduce ARM-specific bugs or test failures on a x86\_64 host laptop or workstation. This is based on QEMU user mode emulation using docker for convenience (see <https://github.com/multiarch/qemu-user-static>).

\> **Note** \> The following instructions are illustrated for ARM64 but they also apply to ppc64le, after changing the Docker image and Miniforge paths appropriately.

Prepare a folder on the host filesystem and download the necessary tools and source code:

<div class="prompt">

bash $

mkdir arm64 pushd arm64 wget <https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh> git clone <https://github.com/scikit-learn/scikit-learn.git>

</div>

Use docker to install QEMU user mode and run an ARM64v8 container with access to your shared folder under the <span class="title-ref">/io</span> mount point:

<div class="prompt">

bash $

docker run --rm --privileged multiarch/qemu-user-static --reset -p yes docker run -v\`pwd\`:/io --rm -it arm64v8/ubuntu /bin/bash

</div>

In the container, install miniforge3 for the ARM64 (a.k.a. aarch64) architecture:

<div class="prompt">

bash $

bash Miniforge3-Linux-aarch64.sh \# Choose to install miniforge3 under: <span class="title-ref">/io/miniforge3</span>

</div>

Whenever you restart a new container, you will need to reinit the conda env previously installed under \`/io/miniforge3\`:

<div class="prompt">

bash $

/io/miniforge3/bin/conda init source /root/.bashrc

</div>

as the <span class="title-ref">/root</span> home folder is part of the ephemeral docker container. Every file or directory stored under <span class="title-ref">/io</span> is persistent on the other hand.

You can then build scikit-learn as usual (you will need to install compiler tools and dependencies using apt or conda as usual). Building scikit-learn takes a lot of time because of the emulation layer, however it needs to be done only once if you put the scikit-learn folder under the <span class="title-ref">/io</span> mount point.

Then use pytest to run only the tests of the module you are interested in debugging.

## The Meson Build Backend

Since scikit-learn 1.5.0 we use meson-python as the build tool. Meson is a new tool for scikit-learn and the PyData ecosystem. It is used by several other packages that have written good guides about what it is and how it works.

  - [pandas setup doc](https://pandas.pydata.org/docs/development/contributing_environment.html#step-3-build-and-install-pandas): pandas has a similar setup as ours (no spin or dev.py)
  - [scipy Meson doc](https://scipy.github.io/devdocs/building/understanding_meson.html) gives more background about how Meson works behind the scenes

---

utilities.md

---

# Utilities for Developers

Scikit-learn contains a number of utilities to help with development. These are located in `sklearn.utils`, and include tools in a number of categories. All the following functions and classes are in the module `sklearn.utils`.

<div class="currentmodule">

sklearn.utils

</div>

## Validation Tools

These are tools used to check and validate input. When you write a function which accepts arrays, matrices, or sparse matrices as arguments, the following should be used when applicable.

  - \`assert\_all\_finite\`: Throw an error if array contains NaNs or Infs.
  - \`as\_float\_array\`: convert input to an array of floats. If a sparse matrix is passed, a sparse matrix will be returned.
  - \`check\_array\`: check that input is a 2D array, raise error on sparse matrices. Allowed sparse matrix formats can be given optionally, as well as allowing 1D or N-dimensional arrays. Calls <span class="title-ref">assert\_all\_finite</span> by default.
  - \`check\_X\_y\`: check that X and y have consistent length, calls check\_array on X, and column\_or\_1d on y. For multilabel classification or multitarget regression, specify multi\_output=True, in which case check\_array will be called on y.
  - \`indexable\`: check that all input arrays have consistent length and can be sliced or indexed using safe\_index. This is used to validate input for cross-validation.
  - <span class="title-ref">validation.check\_memory</span> checks that input is `joblib.Memory`-like, which means that it can be converted into a `sklearn.utils.Memory` instance (typically a str denoting the `cachedir`) or has the same interface.

If your code relies on a random number generator, it should never use functions like `numpy.random.random` or `numpy.random.normal`. This approach can lead to repeatability issues in unit tests. Instead, a `numpy.random.RandomState` object should be used, which is built from a `random_state` argument passed to the class or function. The function <span class="title-ref">check\_random\_state</span>, below, can then be used to create a random number generator object.

  - \`check\_random\_state\`: create a `np.random.RandomState` object from a parameter `random_state`.
      - If `random_state` is `None` or `np.random`, then a randomly-initialized `RandomState` object is returned.
      - If `random_state` is an integer, then it is used to seed a new `RandomState` object.
      - If `random_state` is a `RandomState` object, then it is passed through.

For example:

    >>> from sklearn.utils import check_random_state
    >>> random_state = 0
    >>> random_state = check_random_state(random_state)
    >>> random_state.rand(4)
    array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])

When developing your own scikit-learn compatible estimator, the following helpers are available.

  - \`validation.check\_is\_fitted\`: check that the estimator has been fitted before calling `transform`, `predict`, or similar methods. This helper allows to raise a standardized error message across estimator.
  - \`validation.has\_fit\_parameter\`: check that a given parameter is supported in the `fit` method of a given estimator.

## Efficient Linear Algebra & Array Operations

  - \`extmath.randomized\_range\_finder\`: construct an orthonormal matrix whose range approximates the range of the input. This is used in <span class="title-ref">extmath.randomized\_svd</span>, below.
  - \`extmath.randomized\_svd\`: compute the k-truncated randomized SVD. This algorithm finds the exact truncated singular values decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components.
  - \`arrayfuncs.cholesky\_delete\`: (used in <span class="title-ref">\~sklearn.linear\_model.lars\_path</span>) Remove an item from a cholesky factorization.
  - \`arrayfuncs.min\_pos\`: (used in `sklearn.linear_model.least_angle`) Find the minimum of the positive values within an array.
  - \`extmath.fast\_logdet\`: efficiently compute the log of the determinant of a matrix.
  - \`extmath.density\`: efficiently compute the density of a sparse vector
  - \`extmath.safe\_sparse\_dot\`: dot product which will correctly handle `scipy.sparse` inputs. If the inputs are dense, it is equivalent to `numpy.dot`.
  - \`extmath.weighted\_mode\`: an extension of `scipy.stats.mode` which allows each item to have a real-valued weight.
  - \`resample\`: Resample arrays or sparse matrices in a consistent way. used in <span class="title-ref">shuffle</span>, below.
  - \`shuffle\`: Shuffle arrays or sparse matrices in a consistent way. Used in <span class="title-ref">\~sklearn.cluster.k\_means</span>.

## Efficient Random Sampling

  - \`random.sample\_without\_replacement\`: implements efficient algorithms for sampling `n_samples` integers from a population of size `n_population` without replacement.

## Efficient Routines for Sparse Matrices

The `sklearn.utils.sparsefuncs` cython module hosts compiled extensions to efficiently process `scipy.sparse` data.

  - \`sparsefuncs.mean\_variance\_axis\`: compute the means and variances along a specified axis of a CSR matrix. Used for normalizing the tolerance stopping criterion in <span class="title-ref">\~sklearn.cluster.KMeans</span>.
  - <span class="title-ref">sparsefuncs\_fast.inplace\_csr\_row\_normalize\_l1</span> and \`sparsefuncs\_fast.inplace\_csr\_row\_normalize\_l2\`: can be used to normalize individual sparse samples to unit L1 or L2 norm as done in <span class="title-ref">\~sklearn.preprocessing.Normalizer</span>.
  - \`sparsefuncs.inplace\_csr\_column\_scale\`: can be used to multiply the columns of a CSR matrix by a constant scale (one scale per column). Used for scaling features to unit standard deviation in <span class="title-ref">\~sklearn.preprocessing.StandardScaler</span>.
  - \`\~sklearn.neighbors.sort\_graph\_by\_row\_values\`: can be used to sort a CSR sparse matrix such that each row is stored with increasing values. This is useful to improve efficiency when using precomputed sparse distance matrices in estimators relying on nearest neighbors graph.

## Graph Routines

  - \`graph.single\_source\_shortest\_path\_length\`: (not currently used in scikit-learn) Return the shortest path from a single source to all connected nodes on a graph. Code is adapted from [networkx](https://networkx.github.io/). If this is ever needed again, it would be far faster to use a single iteration of Dijkstra's algorithm from `graph_shortest_path`.

## Testing Functions

  - <span class="title-ref">discovery.all\_estimators</span> : returns a list of all estimators in scikit-learn to test for consistent behavior and interfaces.
  - <span class="title-ref">discovery.all\_displays</span> : returns a list of all displays (related to plotting API) in scikit-learn to test for consistent behavior and interfaces.
  - <span class="title-ref">discovery.all\_functions</span> : returns a list all functions in scikit-learn to test for consistent behavior and interfaces.

## Multiclass and multilabel utility function

  - \`multiclass.is\_multilabel\`: Helper function to check if the task is a multi-label classification one.
  - \`multiclass.unique\_labels\`: Helper function to extract an ordered array of unique labels from different formats of target.

## Helper Functions

  - \`gen\_even\_slices\`: generator to create `n`-packs of slices going up to `n`. Used in <span class="title-ref">\~sklearn.decomposition.dict\_learning</span> and <span class="title-ref">\~sklearn.cluster.k\_means</span>.
  - \`gen\_batches\`: generator to create slices containing batch size elements from 0 to `n`
  - \`safe\_mask\`: Helper function to convert a mask to the format expected by the numpy array or scipy sparse matrix on which to use it (sparse matrices support integer indices only while numpy arrays support both boolean masks and integer indices).
  - \`safe\_sqr\`: Helper function for unified squaring (`**2`) of array-likes, matrices and sparse matrices.

## Hash Functions

  - <span class="title-ref">murmurhash3\_32</span> provides a python wrapper for the `MurmurHash3_x86_32` C++ non cryptographic hash function. This hash function is suitable for implementing lookup tables, Bloom filters, Count Min Sketch, feature hashing and implicitly defined sparse random projections:
    
        >>> from sklearn.utils import murmurhash3_32
        >>> murmurhash3_32("some feature", seed=0) == -384616559
        True
        
        >>> murmurhash3_32("some feature", seed=0, positive=True) == 3910350737
        True
    
    The `sklearn.utils.murmurhash` module can also be "cimported" from other cython modules so as to benefit from the high performance of MurmurHash while skipping the overhead of the Python interpreter.

## Warnings and Exceptions

  - \`deprecated\`: Decorator to mark a function or class as deprecated.
  - \`\~sklearn.exceptions.ConvergenceWarning\`: Custom warning to catch convergence problems. Used in `sklearn.covariance.graphical_lasso`.

---

dispatching.md

---

# Dispatching

<div class="toctree" data-maxdepth="2">

modules/array\_api

</div>

---

faq.md

---

<style>
  /* h3 headings on this page are the questions; make them rubric-like */
  h3 {
    font-size: 1rem;
    font-weight: bold;
    padding-bottom: 0.2rem;
    margin: 2rem 0 1.15rem 0;
    border-bottom: 1px solid var(--pst-color-border);
  }

  /* Increase top margin for first question in each section */
  h2 + section > h3 {
    margin-top: 2.5rem;
  }

  /* Make the headerlinks a bit more visible */
  h3 > a.headerlink {
    font-size: 0.9rem;
  }

  /* Remove the backlink decoration on the titles */
  h2 > a.toc-backref,
  h3 > a.toc-backref {
    text-decoration: none;
  }
</style>

# Frequently Asked Questions

<div class="currentmodule">

sklearn

</div>

Here we try to give some answers to questions that regularly pop up on the mailing list.

<div class="contents" data-local="" data-depth="2">

Table of Contents

</div>

## About the project

### What is the project name (a lot of people get it wrong)?

scikit-learn, but not scikit or SciKit nor sci-kit learn. Also not scikits.learn or scikits-learn, which were previously used.

### How do you pronounce the project name?

sy-kit learn. sci stands for science\!

### Why scikit?

There are multiple scikits, which are scientific toolboxes built around SciPy. Apart from scikit-learn, another popular one is [scikit-image](https://scikit-image.org/).

### Do you support PyPy?

Due to limited maintainer resources and small number of users, using scikit-learn with [PyPy](https://pypy.org/) (an alternative Python implementation with a built-in just-in-time compiler) is not officially supported.

### How can I obtain permission to use the images in scikit-learn for my work?

The images contained in the [scikit-learn repository](https://github.com/scikit-learn/scikit-learn) and the images generated within the [scikit-learn documentation](https://scikit-learn.org/stable/index.html) can be used via the [BSD 3-Clause License](https://github.com/scikit-learn/scikit-learn?tab=BSD-3-Clause-1-ov-file) for your work. Citations of scikit-learn are highly encouraged and appreciated. See \[citing scikit-learn \<citing-scikit-learn\>\](\#citing-scikit-learn-\<citing-scikit-learn\>).

## Implementation decisions

### Why is there no support for deep or reinforcement learning? Will there be such support in the future?

Deep learning and reinforcement learning both require a rich vocabulary to define an architecture, with deep learning additionally requiring GPUs for efficient computing. However, neither of these fit within the design constraints of scikit-learn. As a result, deep learning and reinforcement learning are currently out of scope for what scikit-learn seeks to achieve.

You can find more information about the addition of GPU support at [Will you add GPU support?](#will-you-add-gpu-support).

Note that scikit-learn currently implements a simple multilayer perceptron in `sklearn.neural_network`. We will only accept bug fixes for this module. If you want to implement more complex deep learning models, please turn to popular deep learning frameworks such as [tensorflow](https://www.tensorflow.org/), [keras](https://keras.io/), and [pytorch](https://pytorch.org/).

### Will you add graphical models or sequence prediction to scikit-learn?

Not in the foreseeable future. scikit-learn tries to provide a unified API for the basic tasks in machine learning, with pipelines and meta-algorithms like grid search to tie everything together. The required concepts, APIs, algorithms and expertise required for structured learning are different from what scikit-learn has to offer. If we started doing arbitrary structured learning, we'd need to redesign the whole package and the project would likely collapse under its own weight.

There are two projects with API similar to scikit-learn that do structured prediction:

  - [pystruct](https://pystruct.github.io/) handles general structured learning (focuses on SSVMs on arbitrary graph structures with approximate inference; defines the notion of sample as an instance of the graph structure).
  - [seqlearn](https://larsmans.github.io/seqlearn/) handles sequences only (focuses on exact inference; has HMMs, but mostly for the sake of completeness; treats a feature vector as a sample and uses an offset encoding for the dependencies between feature vectors).

### Why did you remove HMMs from scikit-learn?

See \[adding\_graphical\_models\](\#adding\_graphical\_models).

### Will you add GPU support?

Adding GPU support by default would introduce heavy harware-specific software dependencies and existing algorithms would need to be reimplemented. This would make it both harder for the average user to install scikit-learn and harder for the developers to maintain the code.

However, since 2023, a limited but growing \[list of scikit-learn estimators \<array\_api\_supported\>\](\#list-of-scikit-learn estimators-\<array\_api\_supported\>) can already run on GPUs if the input data is provided as a PyTorch or CuPy array and if scikit-learn has been configured to accept such inputs as explained in \[array\_api\](\#array\_api). This Array API support allows scikit-learn to run on GPUs without introducing heavy and hardware-specific software dependencies to the main package.

Most estimators that rely on NumPy for their computationally intensive operations can be considered for Array API support and therefore GPU support.

However, not all scikit-learn estimators are amenable to efficiently running on GPUs via the Array API for fundamental algorithmic reasons. For instance, tree-based models currently implemented with Cython in scikit-learn are fundamentally not array-based algorithms. Other algorithms such as k-means or k-nearest neighbors rely on array-based algorithms but are also implemented in Cython. Cython is used to manually interleave consecutive array operations to avoid introducing performance killing memory access to large intermediate arrays: this low-level algorithmic rewrite is called "kernel fusion" and cannot be expressed via the Array API for the foreseeable future.

Adding efficient GPU support to estimators that cannot be efficiently implemented with the Array API would require designing and adopting a more flexible extension system for scikit-learn. This possibility is being considered in the following GitHub issue (under discussion):

  - <https://github.com/scikit-learn/scikit-learn/issues/22438>

### Why do categorical variables need preprocessing in scikit-learn, compared to other tools?

Most of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices of a single numeric dtype. These do not explicitly represent categorical variables at present. Thus, unlike R's `data.frames` or <span class="title-ref">pandas.DataFrame</span>, we require explicit conversion of categorical features to numeric values, as discussed in \[preprocessing\_categorical\_features\](\#preprocessing\_categorical\_features). See also \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py) for an example of working with heterogeneous (e.g. categorical and numeric) data.

Note that recently, <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingRegressor</span> gained native support for categorical features through the option <span class="title-ref">categorical\_features="from\_dtype"</span>. This option relies on inferring which columns of the data are categorical based on the <span class="title-ref">pandas.CategoricalDtype</span> and <span class="title-ref">polars.datatypes.Categorical</span> dtypes.

### Does scikit-learn work natively with various types of dataframes?

Scikit-learn has limited support for <span class="title-ref">pandas.DataFrame</span> and <span class="title-ref">polars.DataFrame</span>. Scikit-learn estimators can accept both these dataframe types as input, and scikit-learn transformers can output dataframes using the <span class="title-ref">set\_output</span> API. For more details, refer to \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_set\_output.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_set\_output.py).

However, the internal computations in scikit-learn estimators rely on numerical operations that are more efficiently performed on homogeneous data structures such as NumPy arrays or SciPy sparse matrices. As a result, most scikit-learn estimators will internally convert dataframe inputs into these homogeneous data structures. Similarly, dataframe outputs are generated from these homogeneous data structures.

Also note that <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> makes it convenient to handle heterogeneous pandas dataframes by mapping homogeneous subsets of dataframe columns selected by name or dtype to dedicated scikit-learn transformers. Therefore <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> are often used in the first step of scikit-learn pipelines when dealing with heterogeneous dataframes (see \[pipeline\](\#pipeline) for more details).

See also \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py) for an example of working with heterogeneous (e.g. categorical and numeric) data.

### Do you plan to implement transform for target `y` in a pipeline?

Currently transform only works for features `X` in a pipeline. There's a long-standing discussion about not being able to transform `y` in a pipeline. Follow on GitHub issue `4143`. Meanwhile, you can check out <span class="title-ref">\~compose.TransformedTargetRegressor</span>, [pipegraph](https://github.com/mcasl/PipeGraph), and [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn). Note that scikit-learn solved for the case where `y` has an invertible transformation applied before training and inverted after prediction. scikit-learn intends to solve for use cases where `y` should be transformed at training time and not at test time, for resampling and similar uses, like at [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn). In general, these use cases can be solved with a custom meta estimator rather than a <span class="title-ref">\~pipeline.Pipeline</span>.

### Why are there so many different estimators for linear models?

Usually, there is one classifier and one regressor per model type, e.g. <span class="title-ref">\~ensemble.GradientBoostingClassifier</span> and <span class="title-ref">\~ensemble.GradientBoostingRegressor</span>. Both have similar options and both have the parameter <span class="title-ref">loss</span>, which is especially useful in the regression case as it enables the estimation of conditional mean as well as conditional quantiles.

For linear models, there are many estimator classes which are very close to each other. Let us have a look at

  - <span class="title-ref">\~linear\_model.LinearRegression</span>, no penalty
  - <span class="title-ref">\~linear\_model.Ridge</span>, L2 penalty
  - <span class="title-ref">\~linear\_model.Lasso</span>, L1 penalty (sparse models)
  - <span class="title-ref">\~linear\_model.ElasticNet</span>, L1 + L2 penalty (less sparse models)
  - <span class="title-ref">\~linear\_model.SGDRegressor</span> with <span class="title-ref">loss="squared\_loss"</span>

**Maintainer perspective:** They all do in principle the same and are different only by the penalty they impose. This, however, has a large impact on the way the underlying optimization problem is solved. In the end, this amounts to usage of different methods and tricks from linear algebra. A special case is <span class="title-ref">\~linear\_model.SGDRegressor</span> which comprises all 4 previous models and is different by the optimization procedure. A further side effect is that the different estimators favor different data layouts (<span class="title-ref">X</span> C-contiguous or F-contiguous, sparse csr or csc). This complexity of the seemingly simple linear models is the reason for having different estimator classes for different penalties.

**User perspective:** First, the current design is inspired by the scientific literature where linear regression models with different regularization/penalty were given different names, e.g. *ridge regression*. Having different model classes with according names makes it easier for users to find those regression models. Secondly, if all the 5 above mentioned linear models were unified into a single class, there would be parameters with a lot of options like the `solver` parameter. On top of that, there would be a lot of exclusive interactions between different parameters. For example, the possible options of the parameters `solver`, `precompute` and `selection` would depend on the chosen values of the penalty parameters `alpha` and `l1_ratio`.

## Contributing

### How can I contribute to scikit-learn?

See \[contributing\](\#contributing). Before wanting to add a new algorithm, which is usually a major and lengthy undertaking, it is recommended to start with \[known issues \<new\_contributors\>\](\#known-issues-\<new\_contributors\>). Please do not contact the contributors of scikit-learn directly regarding contributing to scikit-learn.

### Why is my pull request not getting any attention?

The scikit-learn review process takes a significant amount of time, and contributors should not be discouraged by a lack of activity or review on their pull request. We care a lot about getting things right the first time, as maintenance and later change comes at a high cost. We rarely release any "experimental" code, so all of our contributions will be subject to high use immediately and should be of the highest quality possible initially.

Beyond that, scikit-learn is limited in its reviewing bandwidth; many of the reviewers and core developers are working on scikit-learn on their own time. If a review of your pull request comes slowly, it is likely because the reviewers are busy. We ask for your understanding and request that you not close your pull request or discontinue your work solely because of this reason.

### What are the inclusion criteria for new algorithms?

We only consider well-established algorithms for inclusion. A rule of thumb is at least 3 years since publication, 200+ citations, and wide use and usefulness. A technique that provides a clear-cut improvement (e.g. an enhanced data structure or a more efficient approximation technique) on a widely-used method will also be considered for inclusion.

From the algorithms or techniques that meet the above criteria, only those which fit well within the current API of scikit-learn, that is a `fit`, `predict/transform` interface and ordinarily having input/output that is a numpy array or sparse matrix, are accepted.

The contributor should support the importance of the proposed addition with research papers and/or implementations in other similar packages, demonstrate its usefulness via common use-cases/applications and corroborate performance improvements, if any, with benchmarks and/or plots. It is expected that the proposed algorithm should outperform the methods that are already implemented in scikit-learn at least in some areas.

Inclusion of a new algorithm speeding up an existing model is easier if:

  - it does not introduce new hyper-parameters (as it makes the library more future-proof),
  - it is easy to document clearly when the contribution improves the speed and when it does not, for instance, "when `n_features >> n_samples`",
  - benchmarks clearly show a speed up.

Also, note that your implementation need not be in scikit-learn to be used together with scikit-learn tools. You can implement your favorite algorithm in a scikit-learn compatible way, upload it to GitHub and let us know. We will be happy to list it under \[related\_projects\](\#related\_projects). If you already have a package on GitHub following the scikit-learn API, you may also be interested to look at [scikit-learn-contrib](https://scikit-learn-contrib.github.io).

### Why are you so selective on what algorithms you include in scikit-learn?

Code comes with maintenance cost, and we need to balance the amount of code we have with the size of the team (and add to this the fact that complexity scales non linearly with the number of features). The package relies on core developers using their free time to fix bugs, maintain code and review contributions. Any algorithm that is added needs future attention by the developers, at which point the original author might long have lost interest. See also \[new\_algorithms\_inclusion\_criteria\](\#new\_algorithms\_inclusion\_criteria). For a great read about long-term maintenance issues in open-source software, look at [the Executive Summary of Roads and Bridges](https://www.fordfoundation.org/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf#page=8).

## Using scikit-learn

### What's the best way to get help on scikit-learn usage?

  - General machine learning questions: use [Cross Validated](https://stats.stackexchange.com/) with the `[machine-learning]` tag.
  - scikit-learn usage questions: use [Stack Overflow](https://stackoverflow.com/questions/tagged/scikit-learn) with the `[scikit-learn]` and `[python]` tags. You can alternatively use the [mailing list](https://mail.python.org/mailman/listinfo/scikit-learn).

Please make sure to include a minimal reproduction code snippet (ideally shorter than 10 lines) that highlights your problem on a toy dataset (for instance from `sklearn.datasets` or randomly generated with functions of `numpy.random` with a fixed random seed). Please remove any line of code that is not necessary to reproduce your problem.

The problem should be reproducible by simply copy-pasting your code snippet in a Python shell with scikit-learn installed. Do not forget to include the import statements. More guidance to write good reproduction code snippets can be found at: <https://stackoverflow.com/help/mcve>.

If your problem raises an exception that you do not understand (even after googling it), please make sure to include the full traceback that you obtain when running the reproduction script.

For bug reports or feature requests, please make use of the [issue tracker on GitHub](https://github.com/scikit-learn/scikit-learn/issues).

<div class="warning">

<div class="title">

Warning

</div>

Please do not email any authors directly to ask for assistance, report bugs, or for any other issue related to scikit-learn.

</div>

### How should I save, export or deploy estimators for production?

See \[model\_persistence\](\#model\_persistence).

### How can I create a bunch object?

Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, <span class="title-ref">bunch\["value\_key"\]</span>, or by an attribute, <span class="title-ref">bunch.value\_key</span>.

They should not be used as an input. Therefore you almost never need to create a <span class="title-ref">\~utils.Bunch</span> object, unless you are extending scikit-learn's API.

### How can I load my own datasets into a format usable by scikit-learn?

Generally, scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as <span class="title-ref">pandas.DataFrame</span> are also acceptable.

For more information on loading your data files into these usable data structures, please refer to \[loading external datasets \<external\_datasets\>\](\#loading-external-datasets-\<external\_datasets\>).

### How do I deal with string data (or trees, graphs...)?

scikit-learn estimators assume you'll feed them real-valued feature vectors. This assumption is hard-coded in pretty much all of the library. However, you can feed non-numerical inputs to estimators in several ways.

If you have text documents, you can use a term frequency features; see \[text\_feature\_extraction\](\#text\_feature\_extraction) for the built-in *text vectorizers*. For more general feature extraction from any kind of data, see \[dict\_feature\_extraction\](\#dict\_feature\_extraction) and \[feature\_hashing\](\#feature\_hashing).

Another common case is when you have non-numerical data and a custom distance (or similarity) metric on these data. Examples include strings with edit distance (aka. Levenshtein distance), for instance, DNA or RNA sequences. These can be encoded as numbers, but doing so is painful and error-prone. Working with distance metrics on arbitrary data can be done in two ways.

Firstly, many estimators take precomputed distance/similarity matrices, so if the dataset is not too large, you can compute distances for all pairs of inputs. If the dataset is large, you can use feature vectors with only one "feature", which is an index into a separate data structure, and supply a custom metric function that looks up the actual data in this data structure. For instance, to use <span class="title-ref">\~cluster.dbscan</span> with Levenshtein distances:

    >>> import numpy as np
    >>> from leven import levenshtein  # doctest: +SKIP
    >>> from sklearn.cluster import dbscan
    >>> data = ["ACCTCCTAGAAG", "ACCTACTAGAAGTT", "GAATATTAGGCCGA"]
    >>> def lev_metric(x, y):
    ...     i, j = int(x[0]), int(y[0])  # extract indices
    ...     return levenshtein(data[i], data[j])
    ...
    >>> X = np.arange(len(data)).reshape(-1, 1)
    >>> X
    array([[0],
           [1],
           [2]])
    >>> # We need to specify algorithm='brute' as the default assumes
    >>> # a continuous feature space.
    >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2, algorithm='brute')  # doctest: +SKIP
    (array([0, 1]), array([ 0,  0, -1]))

Note that the example above uses the third-party edit distance package [leven](https://pypi.org/project/leven/). Similar tricks can be used, with some care, for tree kernels, graph kernels, etc.

### Why do I sometimes get a crash/freeze with `n_jobs > 1` under OSX or Linux?

Several scikit-learn tools such as <span class="title-ref">\~model\_selection.GridSearchCV</span> and <span class="title-ref">\~model\_selection.cross\_val\_score</span> rely internally on Python's `multiprocessing` module to parallelize execution onto several Python processes by passing `n_jobs > 1` as an argument.

The problem is that Python `multiprocessing` does a `fork` system call without following it with an `exec` system call for performance reasons. Many libraries like (some versions of) Accelerate or vecLib under OSX, (some versions of) MKL, the OpenMP runtime of GCC, nvidia's Cuda (and probably many others), manage their own internal thread pool. Upon a call to <span class="title-ref">fork</span>, the thread pool state in the child process is corrupted: the thread pool believes it has many threads while only the main thread state has been forked. It is possible to change the libraries to make them detect when a fork happens and reinitialize the thread pool in that case: we did that for OpenBLAS (merged upstream in main since 0.2.10) and we contributed a [patch](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=60035) to GCC's OpenMP runtime (not yet reviewed).

But in the end the real culprit is Python's `multiprocessing` that does `fork` without `exec` to reduce the overhead of starting and using new Python processes for parallel computing. Unfortunately this is a violation of the POSIX standard and therefore some software editors like Apple refuse to consider the lack of fork-safety in Accelerate and vecLib as a bug.

In Python 3.4+ it is now possible to configure `multiprocessing` to use the `"forkserver"` or `"spawn"` start methods (instead of the default `"fork"`) to manage the process pools. To work around this issue when using scikit-learn, you can set the `JOBLIB_START_METHOD` environment variable to `"forkserver"`. However the user should be aware that using the `"forkserver"` method prevents <span class="title-ref">joblib.Parallel</span> to call function interactively defined in a shell session.

If you have custom code that uses `multiprocessing` directly instead of using it via `joblib` you can enable the `"forkserver"` mode globally for your program. Insert the following instructions in your main script:

    import multiprocessing
    
    # other imports, custom code, load data, define model...
    
    if __name__ == "__main__":
        multiprocessing.set_start_method("forkserver")
    
        # call scikit-learn utils with n_jobs > 1 here

You can find more default on the new start methods in the [multiprocessing documentation](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods).

### Why does my job use more cores than specified with `n_jobs`?

This is because `n_jobs` only controls the number of jobs for routines that are parallelized with `joblib`, but parallel code can come from other sources:

  - some routines may be parallelized with OpenMP (for code written in C or Cython),
  - scikit-learn relies a lot on numpy, which in turn may rely on numerical libraries like MKL, OpenBLAS or BLIS which can provide parallel implementations.

For more details, please refer to our \[notes on parallelism \<parallelism\>\](\#notes-on-parallelism-\<parallelism\>).

### How do I set a `random_state` for an entire execution?

Please refer to \[randomness\](\#randomness).

---

getting_started.md

---

# Getting Started

The purpose of this guide is to illustrate some of the main features that `scikit-learn` provides. It assumes a very basic working knowledge of machine learning practices (model fitting, predicting, cross-validation, etc.). Please refer to our \[installation instructions \<installation-instructions\>\](\#installation-instructions \<installation-instructions\>) for installing `scikit-learn`.

`Scikit-learn` is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.

## Fitting and predicting: estimator basics

`Scikit-learn` provides dozens of built-in machine learning algorithms and models, called `estimators`. Each estimator can be fitted to some data using its `fit` method.

Here is a simple example where we fit a <span class="title-ref">\~sklearn.ensemble.RandomForestClassifier</span> to some very basic data:

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> clf = RandomForestClassifier(random_state=0)
    >>> X = [[ 1,  2,  3],  # 2 samples, 3 features
    ...      [11, 12, 13]]
    >>> y = [0, 1]  # classes of each sample
    >>> clf.fit(X, y)
    RandomForestClassifier(random_state=0)

The `fit` method generally accepts 2 inputs:

  - The samples matrix (or design matrix) `X`. The size of `X` is typically `(n_samples, n_features)`, which means that samples are represented as rows and features are represented as columns.
  - The target values `y` which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervised learning tasks, `y` does not need to be specified. `y` is usually a 1d array where the `i` th entry corresponds to the target of the `i` th sample (row) of `X`.

Both `X` and `y` are usually expected to be numpy arrays or equivalent `array-like` data types, though some estimators work with other formats such as sparse matrices.

Once the estimator is fitted, it can be used for predicting target values of new data. You don't need to re-train the estimator:

    >>> clf.predict(X)  # predict classes of the training data
    array([0, 1])
    >>> clf.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data
    array([0, 1])

You can check \[ml\_map\](\#ml\_map) on how to choose the right model for your use case.

## Transformers and pre-processors

Machine learning workflows are often composed of different parts. A typical pipeline consists of a pre-processing step that transforms or imputes the data, and a final predictor that predicts target values.

In `scikit-learn`, pre-processors and transformers follow the same API as the estimator objects (they actually all inherit from the same `BaseEstimator` class). The transformer objects don't have a `predict` method but rather a `transform` method that outputs a newly transformed sample matrix `X`:

    >>> from sklearn.preprocessing import StandardScaler
    >>> X = [[0, 15],
    ...      [1, -10]]
    >>> # scale data according to computed scaling values
    >>> StandardScaler().fit(X).transform(X)
    array([[-1.,  1.],
           [ 1., -1.]])

Sometimes, you want to apply different transformations to different features: the \[ColumnTransformer\<column\_transformer\>\](\#columntransformer\<column\_transformer\>) is designed for these use-cases.

## Pipelines: chaining pre-processors and estimators

Transformers and estimators (predictors) can be combined together into a single unifying object: a <span class="title-ref">\~sklearn.pipeline.Pipeline</span>. The pipeline offers the same API as a regular estimator: it can be fitted and used for prediction with `fit` and `predict`. As we will see later, using a pipeline will also prevent you from data leakage, i.e. disclosing some testing data in your training data.

In the following example, we \[load the Iris dataset \<datasets\>\](\#load-the-iris-dataset-\<datasets\>), split it into train and test sets, and compute the accuracy score of a pipeline on the test data:

    >>> from sklearn.preprocessing import StandardScaler
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.pipeline import make_pipeline
    >>> from sklearn.datasets import load_iris
    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn.metrics import accuracy_score
    ...
    >>> # create a pipeline object
    >>> pipe = make_pipeline(
    ...     StandardScaler(),
    ...     LogisticRegression()
    ... )
    ...
    >>> # load the iris dataset and split it into train and test sets
    >>> X, y = load_iris(return_X_y=True)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    ...
    >>> # fit the whole pipeline
    >>> pipe.fit(X_train, y_train)
    Pipeline(steps=[('standardscaler', StandardScaler()),
                    ('logisticregression', LogisticRegression())])
    >>> # we can now use it like any other estimator
    >>> accuracy_score(pipe.predict(X_test), y_test)
    0.97...

## Model evaluation

Fitting a model to some data does not entail that it will predict well on unseen data. This needs to be directly evaluated. We have just seen the <span class="title-ref">\~sklearn.model\_selection.train\_test\_split</span> helper that splits a dataset into train and test sets, but `scikit-learn` provides many other tools for model evaluation, in particular for \[cross-validation \<cross\_validation\>\](\#cross-validation \<cross\_validation\>).

We here briefly show how to perform a 5-fold cross-validation procedure, using the <span class="title-ref">\~sklearn.model\_selection.cross\_validate</span> helper. Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring functions. Please refer to our \[User Guide \<cross\_validation\>\](\#user-guide-\<cross\_validation\>) for more details:

    >>> from sklearn.datasets import make_regression
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.model_selection import cross_validate
    ...
    >>> X, y = make_regression(n_samples=1000, random_state=0)
    >>> lr = LinearRegression()
    ...
    >>> result = cross_validate(lr, X, y)  # defaults to 5-fold CV
    >>> result['test_score']  # r_squared score is high because dataset is easy
    array([1., 1., 1., 1., 1.])

## Automatic parameter searches

All estimators have parameters (often called hyper-parameters in the literature) that can be tuned. The generalization power of an estimator often critically depends on a few parameters. For example a <span class="title-ref">\~sklearn.ensemble.RandomForestRegressor</span> has a `n_estimators` parameter that determines the number of trees in the forest, and a `max_depth` parameter that determines the maximum depth of each tree. Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.

`Scikit-learn` provides tools to automatically find the best parameter combinations (via cross-validation). In the following example, we randomly search over the parameter space of a random forest with a <span class="title-ref">\~sklearn.model\_selection.RandomizedSearchCV</span> object. When the search is over, the <span class="title-ref">\~sklearn.model\_selection.RandomizedSearchCV</span> behaves as a <span class="title-ref">\~sklearn.ensemble.RandomForestRegressor</span> that has been fitted with the best set of parameters. Read more in the \[User Guide \<grid\_search\>\](\#user-guide \<grid\_search\>):

    >>> from sklearn.datasets import fetch_california_housing
    >>> from sklearn.ensemble import RandomForestRegressor
    >>> from sklearn.model_selection import RandomizedSearchCV
    >>> from sklearn.model_selection import train_test_split
    >>> from scipy.stats import randint
    ...
    >>> X, y = fetch_california_housing(return_X_y=True)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    ...
    >>> # define the parameter space that will be searched over
    >>> param_distributions = {'n_estimators': randint(1, 5),
    ...                        'max_depth': randint(5, 10)}
    ...
    >>> # now create a searchCV object and fit it to the data
    >>> search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),
    ...                             n_iter=5,
    ...                             param_distributions=param_distributions,
    ...                             random_state=0)
    >>> search.fit(X_train, y_train)
    RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,
                       param_distributions={'max_depth': ...,
                                            'n_estimators': ...},
                       random_state=0)
    >>> search.best_params_
    {'max_depth': 9, 'n_estimators': 4}
    
    >>> # the search object now acts like a normal random forest estimator
    >>> # with max_depth=9 and n_estimators=4
    >>> search.score(X_test, y_test)
    0.73...

\> **Note** \> In practice, you almost always want to \[search over a pipeline \<composite\_grid\_search\>\](\#search-over-a-pipeline ----\<composite\_grid\_search\>), instead of a single estimator. One of the main reasons is that if you apply a pre-processing step to the whole dataset without using a pipeline, and then perform any kind of cross-validation, you would be breaking the fundamental assumption of independence between training and testing data. Indeed, since you pre-processed the data using the whole dataset, some information about the test sets are available to the train sets. This will lead to over-estimating the generalization power of the estimator (you can read more in this [Kaggle post](https://www.kaggle.com/alexisbcook/data-leakage)).

> Using a pipeline for cross-validation and searching will largely keep you from this common pitfall.

## Next steps

We have briefly covered estimator fitting and predicting, pre-processing steps, pipelines, cross-validation tools and automatic hyper-parameter searches. This guide should give you an overview of some of the main features of the library, but there is much more to `scikit-learn`\!

Please refer to our \[user\_guide\](\#user\_guide) for details on all the tools that we provide. You can also find an exhaustive list of the public API in the \[api\_ref\](\#api\_ref).

You can also look at our numerous \[examples \<general\_examples\>\](\#examples-\<general\_examples\>) that illustrate the use of `scikit-learn` in many different contexts.

---

glossary.md

---

<div class="currentmodule">

sklearn

</div>

# Glossary of Common Terms and API Elements

This glossary hopes to definitively represent the tacit and explicit conventions applied in Scikit-learn and its API, while providing a reference for users and contributors. It aims to describe the concepts and either detail their corresponding API or link to other relevant parts of the documentation which do so. By linking to glossary entries from the API Reference and User Guide, we may minimize redundancy and inconsistency.

We begin by listing general concepts (and any that didn't fit elsewhere), but more specific sets of related terms are listed below: \[glossary\_estimator\_types\](\#glossary\_estimator\_types), \[glossary\_target\_types\](\#glossary\_target\_types), \[glossary\_methods\](\#glossary\_methods), \[glossary\_parameters\](\#glossary\_parameters), \[glossary\_attributes\](\#glossary\_attributes), \[glossary\_sample\_props\](\#glossary\_sample\_props).

## General Concepts

<div class="glossary">

1d 1d array One-dimensional array. A NumPy array whose `.shape` has length 1. A vector.

2d 2d array Two-dimensional array. A NumPy array whose `.shape` has length 2. Often represents a matrix.

  - API  
    Refers to both the *specific* interfaces for estimators implemented in Scikit-learn and the *generalized* conventions across types of estimators as described in this glossary and \[overviewed in the contributor documentation \<api\_overview\>\](\#overviewed-in-the

</div>

\--------contributor-documentation-\<api\_overview\>).

> The specific interfaces that constitute Scikit-learn's public API are largely documented in \[api\_ref\](\#api\_ref). However, we less formally consider anything as public API if none of the identifiers required to access it begins with `_`. We generally try to maintain `backwards
> compatibility` for all objects in the public API.
> 
> Private API, including functions, modules and methods beginning `_` are not assured to be stable.
> 
> array-like The most common data format for *input* to Scikit-learn estimators and functions, array-like is any type object for which <span class="title-ref">numpy.asarray</span> will produce an array of appropriate shape (usually 1 or 2-dimensional) of appropriate dtype (usually numeric).
> 
> This includes:
> 
>   - a numpy array
>   - a list of numbers
>   - a list of length-k lists of numbers for some fixed length k
>   - a <span class="title-ref">pandas.DataFrame</span> with all columns numeric
>   - a numeric <span class="title-ref">pandas.Series</span>
> 
> It excludes:
> 
>   - a `sparse matrix`
>   - a sparse array
>   - an iterator
>   - a generator
> 
> Note that *output* from scikit-learn estimators and functions (e.g. predictions) should generally be arrays or sparse matrices, or lists thereof (as in multi-output <span class="title-ref">tree.DecisionTreeClassifier</span>'s `predict_proba`). An estimator where `predict()` returns a list or a <span class="title-ref">pandas.Series</span> is not valid.
> 
> attribute attributes We mostly use attribute to refer to how model information is stored on an estimator during fitting. Any public attribute stored on an estimator instance is required to begin with an alphabetic character and end in a single underscore if it is set in `fit` or `partial_fit`. These are what is documented under an estimator's *Attributes* documentation. The information stored in attributes is usually either: sufficient statistics used for prediction or transformation; `transductive` outputs such as `labels_` or `embedding_`; or diagnostic data, such as `feature_importances_`. Common attributes are listed \[below \<glossary\_attributes\>\](\#below-\<glossary\_attributes\>).
> 
> A public attribute may have the same name as a constructor `parameter`, with a `_` appended. This is used to store a validated or estimated version of the user's input. For example, <span class="title-ref">decomposition.PCA</span> is constructed with an `n_components` parameter. From this, together with other parameters and the data, PCA estimates the attribute `n_components_`.
> 
> Further private attributes used in prediction/transformation/etc. may also be set when fitting. These begin with a single underscore and are not assured to be stable for public access.
> 
> A public attribute on an estimator instance that does not end in an underscore should be the stored, unmodified value of an `__init__` `parameter` of the same name. Because of this equivalence, these are documented under an estimator's *Parameters* documentation.
> 
> backwards compatibility We generally try to maintain backward compatibility (i.e. interfaces and behaviors may be extended but not changed or removed) from release to release but this comes with some exceptions:
> 
>   - Public API only  
>     The behavior of objects accessed through private identifiers (those beginning `_`) may be changed arbitrarily between versions.
> 
>   - As documented  
>     We will generally assume that the users have adhered to the documented parameter types and ranges. If the documentation asks for a list and the user gives a tuple, we do not assure consistent behavior from version to version.
> 
>   - Deprecation  
>     Behaviors may change following a `deprecation` period (usually two releases long). Warnings are issued using Python's `warnings` module.
> 
>   - Keyword arguments  
>     We may sometimes assume that all optional parameters (other than X and y to `fit` and similar methods) are passed as keyword arguments only and may be positionally reordered.
> 
>   - Bug fixes and enhancements  
>     Bug fixes and -- less often -- enhancements may change the behavior of estimators, including the predictions of an estimator trained on the same data and `random_state`. When this happens, we attempt to note it clearly in the changelog.
> 
>   - Serialization  
>     We make no assurances that pickling an estimator in one version will allow it to be unpickled to an equivalent model in the subsequent version. (For estimators in the sklearn package, we issue a warning when this unpickling is attempted, even if it may happen to work.) See \[persistence\_limitations\](\#persistence\_limitations).
> 
>   - <span class="title-ref">utils.estimator\_checks.check\_estimator</span>  
>     We provide limited backwards compatibility assurances for the estimator checks: we may add extra requirements on estimators tested with this function, usually when these were informally assumed but not formally tested.
> 
> Despite this informal contract with our users, the software is provided as is, as stated in the license. When a release inadvertently introduces changes that are not backward compatible, these are known as software regressions.
> 
> callable A function, class or an object which implements the `__call__` method; anything that returns True when the argument of [callable()](https://docs.python.org/3/library/functions.html#callable).
> 
> categorical feature A categorical or nominal `feature` is one that has a finite set of discrete values across the population of data. These are commonly represented as columns of integers or strings. Strings will be rejected by most scikit-learn estimators, and integers will be treated as ordinal or count-valued. For the use with most estimators, categorical variables should be one-hot encoded. Notable exceptions include tree-based models such as random forests and gradient boosting models that often work better and faster with integer-coded categorical variables. <span class="title-ref">\~sklearn.preprocessing.OrdinalEncoder</span> helps encoding string-valued categorical features as ordinal integers, and <span class="title-ref">\~sklearn.preprocessing.OneHotEncoder</span> can be used to one-hot encode categorical features. See also \[preprocessing\_categorical\_features\](\#preprocessing\_categorical\_features) and the [categorical-encoding](https://github.com/scikit-learn-contrib/category_encoders) package for tools related to encoding categorical features.
> 
> clone cloned To copy an `estimator instance` and create a new one with identical `parameters`, but without any fitted `attributes`, using <span class="title-ref">\~sklearn.base.clone</span>.
> 
> When `fit` is called, a `meta-estimator` usually clones a wrapped estimator instance before fitting the cloned instance. (Exceptions, for legacy reasons, include <span class="title-ref">\~pipeline.Pipeline</span> and <span class="title-ref">\~pipeline.FeatureUnion</span>.)
> 
> If the estimator's <span class="title-ref">random\_state</span> parameter is an integer (or if the estimator doesn't have a <span class="title-ref">random\_state</span> parameter), an *exact clone* is returned: the clone and the original estimator will give the exact same results. Otherwise, *statistical clone* is returned: the clone might yield different results from the original estimator. More details can be found in \[randomness\](\#randomness).
> 
> common tests This refers to the tests run on almost every estimator class in Scikit-learn to check they comply with basic API conventions. They are available for external use through <span class="title-ref">utils.estimator\_checks.check\_estimator</span> or <span class="title-ref">utils.estimator\_checks.parametrize\_with\_checks</span>, with most of the implementation in `sklearn/utils/estimator_checks.py`.
> 
> Note: Some exceptions to the common testing regime are currently hard-coded into the library, but we hope to replace this by marking exceptional behaviours on the estimator using semantic `estimator
> tags`.
> 
> cross-fitting cross fitting A resampling method that iteratively partitions data into mutually exclusive subsets to fit two stages. During the first stage, the mutually exclusive subsets enable predictions or transformations to be computed on data not seen during training. The computed data is then used in the second stage. The objective is to avoid having any overfitting in the first stage introduce bias into the input data distribution of the second stage. For examples of its use, see: <span class="title-ref">\~preprocessing.TargetEncoder</span>, <span class="title-ref">\~ensemble.StackingClassifier</span>, <span class="title-ref">\~ensemble.StackingRegressor</span> and <span class="title-ref">\~calibration.CalibratedClassifierCV</span>.
> 
> cross-validation cross validation A resampling method that iteratively partitions data into mutually exclusive 'train' and 'test' subsets so model performance can be evaluated on unseen data. This conserves data as avoids the need to hold out a 'validation' dataset and accounts for variability as multiple rounds of cross validation are generally performed. See \[User Guide \<cross\_validation\>\](\#user-guide-\<cross\_validation\>) for more details.
> 
> deprecation We use deprecation to slowly violate our `backwards
> compatibility` assurances, usually to:
> 
>   - change the default value of a parameter; or
>   - remove a parameter, attribute, method, class, etc.
> 
> We will ordinarily issue a warning when a deprecated element is used, although there may be limitations to this. For instance, we will raise a warning when someone sets a parameter that has been deprecated, but may not when they access that parameter's attribute on the estimator instance.
> 
> See the \[Contributors' Guide \<contributing\_deprecation\>\](\#contributors'-guide-\<contributing\_deprecation\>).
> 
> dimensionality May be used to refer to the number of `features` (i.e. `n_features`), or columns in a 2d feature matrix. Dimensions are, however, also used to refer to the length of a NumPy array's shape, distinguishing a 1d array from a 2d matrix.
> 
> docstring The embedded documentation for a module, class, function, etc., usually in code as a string at the beginning of the object's definition, and accessible as the object's `__doc__` attribute.
> 
> We try to adhere to [PEP257](https://www.python.org/dev/peps/pep-0257/), and follow [NumpyDoc conventions](https://numpydoc.readthedocs.io/en/latest/format.html).
> 
> double underscore double underscore notation When specifying parameter names for nested estimators, `__` may be used to separate between parent and child in some contexts. The most common use is when setting parameters through a meta-estimator with `set_params` and hence in specifying a search grid in \[parameter search \<grid\_search\>\](\#parameter-search-\<grid\_search\>). See `parameter`. It is also used in <span class="title-ref">pipeline.Pipeline.fit</span> for passing `sample properties` to the `fit` methods of estimators in the pipeline.
> 
> dtype data type NumPy arrays assume a homogeneous data type throughout, available in the `.dtype` attribute of an array (or sparse matrix). We generally assume simple data types for scikit-learn data: float or integer. We may support object or string data types for arrays before encoding or vectorizing. Our estimators do not work with struct arrays, for instance.
> 
> Our documentation can sometimes give information about the dtype precision, e.g. <span class="title-ref">np.int32</span>, <span class="title-ref">np.int64</span>, etc. When the precision is provided, it refers to the NumPy dtype. If an arbitrary precision is used, the documentation will refer to dtype <span class="title-ref">integer</span> or <span class="title-ref">floating</span>. Note that in this case, the precision can be platform dependent. The <span class="title-ref">numeric</span> dtype refers to accepting both <span class="title-ref">integer</span> and <span class="title-ref">floating</span>.
> 
> When it comes to choosing between 64-bit dtype (i.e. <span class="title-ref">np.float64</span> and <span class="title-ref">np.int64</span>) and 32-bit dtype (i.e. <span class="title-ref">np.float32</span> and <span class="title-ref">np.int32</span>), it boils down to a trade-off between efficiency and precision. The 64-bit types offer more accurate results due to their lower floating-point error, but demand more computational resources, resulting in slower operations and increased memory usage. In contrast, 32-bit types promise enhanced operation speed and reduced memory consumption, but introduce a larger floating-point error. The efficiency improvement are dependent on lower level optimization such as like vectorization, single instruction multiple dispatch (SIMD), or cache optimization but crucially on the compatibility of the algorithm in use.
> 
> Specifically, the choice of precision should account for whether the employed algorithm can effectively leverage <span class="title-ref">np.float32</span>. Some algorithms, especially certain minimization methods, are exclusively coded for <span class="title-ref">np.float64</span>, meaning that even if <span class="title-ref">np.float32</span> is passed, it triggers an automatic conversion back to <span class="title-ref">np.float64</span>. This not only negates the intended computational savings but also introduces additional overhead, making operations with <span class="title-ref">np.float32</span> unexpectedly slower and more memory-intensive due to this extra conversion step.
> 
> duck typing We try to apply [duck typing](https://en.wikipedia.org/wiki/Duck_typing) to determine how to handle some input values (e.g. checking whether a given estimator is a classifier). That is, we avoid using `isinstance` where possible, and rely on the presence or absence of attributes to determine an object's behaviour. Some nuance is required when following this approach:
> 
>   - For some estimators, an attribute may only be available once it is `fitted`. For instance, we cannot a priori determine if `predict_proba` is available in a grid search where the grid includes alternating between a probabilistic and a non-probabilistic predictor in the final step of the pipeline. In the following, we can only determine if `clf` is probabilistic after fitting it on some data:
>     
>         >>> from sklearn.model_selection import GridSearchCV
>         >>> from sklearn.linear_model import SGDClassifier
>         >>> clf = GridSearchCV(SGDClassifier(),
>         ...                    param_grid={'loss': ['log_loss', 'hinge']})
>     
>     This means that we can only check for duck-typed attributes after fitting, and that we must be careful to make `meta-estimators` only present attributes according to the state of the underlying estimator after fitting.
> 
>   - Checking if an attribute is present (using `hasattr`) is in general just as expensive as getting the attribute (`getattr` or dot notation). In some cases, getting the attribute may indeed be expensive (e.g. for some implementations of `feature_importances_`, which may suggest this is an API design flaw). So code which does `hasattr` followed by `getattr` should be avoided; `getattr` within a try-except block is preferred.
> 
>   - For determining some aspects of an estimator's expectations or support for some feature, we use `estimator tags` instead of duck typing.
> 
> early stopping This consists in stopping an iterative optimization method before the convergence of the training loss, to avoid over-fitting. This is generally done by monitoring the generalization score on a validation set. When available, it is activated through the parameter `early_stopping` or by setting a positive `n_iter_no_change`.
> 
> estimator instance We sometimes use this terminology to distinguish an `estimator` class from a constructed instance. For example, in the following, `cls` is an estimator class, while `est1` and `est2` are instances:
> 
>     cls = RandomForestClassifier
>     est1 = cls()
>     est2 = RandomForestClassifier()
> 
> examples We try to give examples of basic usage for most functions and classes in the API:
> 
>   - as doctests in their docstrings (i.e. within the `sklearn/` library code itself).
>   - as examples in the \[example gallery \<general\_examples\>\](\#example-gallery-\<general\_examples\>) rendered (using [sphinx-gallery](https://sphinx-gallery.readthedocs.io/)) from scripts in the `examples/` directory, exemplifying key features or parameters of the estimator/function. These should also be referenced from the User Guide.
>   - sometimes in the \[User Guide \<user\_guide\>\](\#user-guide-\<user\_guide\>) (built from `doc/`) alongside a technical description of the estimator.
> 
> experimental An experimental tool is already usable but its public API, such as default parameter values or fitted attributes, is still subject to change in future versions without the usual `deprecation` warning policy.
> 
> evaluation metric evaluation metrics Evaluation metrics give a measure of how well a model performs. We may use this term specifically to refer to the functions in `~sklearn.metrics` (disregarding `~sklearn.metrics.pairwise`), as distinct from the `score` method and the `scoring` API used in cross validation. See \[model\_evaluation\](\#model\_evaluation).
> 
> These functions usually accept a ground truth (or the raw data where the metric evaluates clustering without a ground truth) and a prediction, be it the output of `predict` (`y_pred`), of `predict_proba` (`y_proba`), or of an arbitrary score function including `decision_function` (`y_score`). Functions are usually named to end with `_score` if a greater score indicates a better model, and `_loss` if a lesser score indicates a better model. This diversity of interface motivates the scoring API.
> 
> Note that some estimators can calculate metrics that are not included in `~sklearn.metrics` and are estimator-specific, notably model likelihoods.
> 
> estimator tags Estimator tags describe certain capabilities of an estimator. This would enable some runtime behaviors based on estimator inspection, but it also allows each estimator to be tested for appropriate invariances while being excepted from other `common tests`.
> 
> Some aspects of estimator tags are currently determined through the `duck typing` of methods like `predict_proba` and through some special attributes on estimator objects:
> 
> For more detailed info, see \[estimator\_tags\](\#estimator\_tags).
> 
> feature features feature vector In the abstract, a feature is a function (in its mathematical sense) mapping a sampled object to a numeric or categorical quantity. "Feature" is also commonly used to refer to these quantities, being the individual elements of a vector representing a sample. In a data matrix, features are represented as columns: each column contains the result of applying a feature function to a set of samples.
> 
> Elsewhere features are known as attributes, predictors, regressors, or independent variables.
> 
> Nearly all estimators in scikit-learn assume that features are numeric, finite and not missing, even when they have semantically distinct domains and distributions (categorical, ordinal, count-valued, real-valued, interval). See also `categorical feature` and `missing values`.
> 
> `n_features` indicates the number of features in a dataset.
> 
> fitting Calling `fit` (or `fit_transform`, `fit_predict`, etc.) on an estimator.
> 
> fitted The state of an estimator after `fitting`.
> 
> There is no conventional procedure for checking if an estimator is fitted. However, an estimator that is not fitted:
> 
>   - should raise <span class="title-ref">exceptions.NotFittedError</span> when a prediction method (`predict`, `transform`, etc.) is called. (<span class="title-ref">utils.validation.check\_is\_fitted</span> is used internally for this purpose.)
>   - should not have any `attributes` beginning with an alphabetic character and ending with an underscore. (Note that a descriptor for the attribute may still be present on the class, but hasattr should return False)
> 
> function We provide ad hoc function interfaces for many algorithms, while `estimator` classes provide a more consistent interface.
> 
> In particular, Scikit-learn may provide a function interface that fits a model to some data and returns the learnt model parameters, as in <span class="title-ref">linear\_model.enet\_path</span>. For transductive models, this also returns the embedding or cluster labels, as in <span class="title-ref">manifold.spectral\_embedding</span> or <span class="title-ref">cluster.dbscan</span>. Many preprocessing transformers also provide a function interface, akin to calling `fit_transform`, as in <span class="title-ref">preprocessing.maxabs\_scale</span>. Users should be careful to avoid `data leakage` when making use of these `fit_transform`-equivalent functions.
> 
> We do not have a strict policy about when to or when not to provide function forms of estimators, but maintainers should consider consistency with existing interfaces, and whether providing a function would lead users astray from best practices (as regards data leakage, etc.)
> 
> gallery See `examples`.
> 
> hyperparameter hyper-parameter See `parameter`.
> 
> impute imputation Most machine learning algorithms require that their inputs have no `missing values`, and will not work if this requirement is violated. Algorithms that attempt to fill in (or impute) missing values are referred to as imputation algorithms.
> 
> indexable An `array-like`, `sparse matrix`, pandas DataFrame or sequence (usually a list).
> 
> induction inductive Inductive (contrasted with `transductive`) machine learning builds a model of some data that can then be applied to new instances. Most estimators in Scikit-learn are inductive, having `predict` and/or `transform` methods.
> 
> joblib A Python library (<https://joblib.readthedocs.io>) used in Scikit-learn to facilite simple parallelism and caching. Joblib is oriented towards efficiently working with numpy arrays, such as through use of `memory mapping`. See \[parallelism\](\#parallelism) for more information.
> 
> label indicator matrix multilabel indicator matrix multilabel indicator matrices The format used to represent multilabel data, where each row of a 2d array or sparse matrix corresponds to a sample, each column corresponds to a class, and each element is 1 if the sample is labeled with the class and 0 if not.
> 
> leakage data leakage A problem in cross validation where generalization performance can be over-estimated since knowledge of the test data was inadvertently included in training a model. This is a risk, for instance, when applying a `transformer` to the entirety of a dataset rather than each training portion in a cross validation split.
> 
> We aim to provide interfaces (such as `~sklearn.pipeline` and `~sklearn.model_selection`) that shield the user from data leakage.
> 
> memmapping memory map memory mapping A memory efficiency strategy that keeps data on disk rather than copying it into main memory. Memory maps can be created for arrays that can be read, written, or both, using `numpy.memmap`. When using `joblib` to parallelize operations in Scikit-learn, it may automatically memmap large arrays to reduce memory duplication overhead in multiprocessing.
> 
> missing values Most Scikit-learn estimators do not work with missing values. When they do (e.g. in <span class="title-ref">impute.SimpleImputer</span>), NaN is the preferred representation of missing values in float arrays. If the array has integer dtype, NaN cannot be represented. For this reason, we support specifying another `missing_values` value when `imputation` or learning can be performed in integer space. `Unlabeled data <unlabeled data>` is a special case of missing values in the `target`.
> 
> `n_features` The number of `features`.
> 
> `n_outputs` The number of `outputs` in the `target`.
> 
> `n_samples` The number of `samples`.
> 
> `n_targets` Synonym for `n_outputs`.
> 
> narrative docs narrative documentation An alias for \[User Guide \<user\_guide\>\](\#user-guide-\<user\_guide\>), i.e. documentation written in `doc/modules/`. Unlike the \[API reference \<api\_ref\>\](\#api-reference-\<api\_ref\>) provided through docstrings, the User Guide aims to:
> 
>   - group tools provided by Scikit-learn together thematically or in terms of usage;
>   - motivate why someone would use each particular tool, often through comparison;
>   - provide both intuitive and technical descriptions of tools;
>   - provide or link to `examples` of using key features of a tool.
> 
> np A shorthand for Numpy due to the conventional import statement:
> 
>     import numpy as np
> 
> online learning Where a model is iteratively updated by receiving each batch of ground truth `targets` soon after making predictions on corresponding batch of data. Intrinsically, the model must be usable for prediction after each batch. See `partial_fit`.
> 
> out-of-core An efficiency strategy where not all the data is stored in main memory at once, usually by performing learning on batches of data. See `partial_fit`.
> 
> outputs Individual scalar/categorical variables per sample in the `target`. For example, in multilabel classification each possible label corresponds to a binary output. Also called *responses*, *tasks* or *targets*. See `multiclass multioutput` and `continuous multioutput`.
> 
> pair A tuple of length two.
> 
> parameter parameters param params We mostly use *parameter* to refer to the aspects of an estimator that can be specified in its construction. For example, `max_depth` and `random_state` are parameters of <span class="title-ref">\~ensemble.RandomForestClassifier</span>. Parameters to an estimator's constructor are stored unmodified as attributes on the estimator instance, and conventionally start with an alphabetic character and end with an alphanumeric character. Each estimator's constructor parameters are described in the estimator's docstring.
> 
> We do not use parameters in the statistical sense, where parameters are values that specify a model and can be estimated from data. What we call parameters might be what statisticians call hyperparameters to the model: aspects for configuring model structure that are often not directly learnt from data. However, our parameters are also used to prescribe modeling operations that do not affect the learnt model, such as `n_jobs` for controlling parallelism.
> 
> When talking about the parameters of a `meta-estimator`, we may also be including the parameters of the estimators wrapped by the meta-estimator. Ordinarily, these nested parameters are denoted by using a `double underscore` (`__`) to separate between the estimator-as-parameter and its parameter. Thus `clf = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=3))` has a deep parameter `estimator__max_depth` with value `3`, which is accessible with `clf.estimator.max_depth` or `clf.get_params()['estimator__max_depth']`.
> 
> The list of parameters and their current values can be retrieved from an `estimator instance` using its `get_params` method.
> 
> Between construction and fitting, parameters may be modified using `set_params`. To enable this, parameters are not ordinarily validated or altered when the estimator is constructed, or when each parameter is set. Parameter validation is performed when `fit` is called.
> 
> Common parameters are listed \[below \<glossary\_parameters\>\](\#below-\<glossary\_parameters\>).
> 
> pairwise metric pairwise metrics
> 
> In its broad sense, a pairwise metric defines a function for measuring similarity or dissimilarity between two samples (with each ordinarily represented as a `feature vector`). We particularly provide implementations of distance metrics (as well as improper metrics like Cosine Distance) through <span class="title-ref">metrics.pairwise\_distances</span>, and of kernel functions (a constrained class of similarity functions) in <span class="title-ref">metrics.pairwise.pairwise\_kernels</span>. These can compute pairwise distance matrices that are symmetric and hence store data redundantly.
> 
> See also `precomputed` and `metric`.
> 
> Note that for most distance metrics, we rely on implementations from `scipy.spatial.distance`, but may reimplement for efficiency in our context. The <span class="title-ref">metrics.DistanceMetric</span> interface is used to implement distance metrics for integration with efficient neighbors search.
> 
> pd A shorthand for [Pandas](https://pandas.pydata.org) due to the conventional import statement:
> 
>     import pandas as pd
> 
> precomputed Where algorithms rely on `pairwise metrics`, and can be computed from pairwise metrics alone, we often allow the user to specify that the `X` provided is already in the pairwise (dis)similarity space, rather than in a feature space. That is, when passed to `fit`, it is a square, symmetric matrix, with each vector indicating (dis)similarity to every sample, and when passed to prediction/transformation methods, each row corresponds to a testing sample and each column to a training sample.
> 
> Use of precomputed X is usually indicated by setting a `metric`, `affinity` or `kernel` parameter to the string 'precomputed'. If this is the case, then the estimator should set the <span class="title-ref">pairwise</span> estimator tag as True.
> 
> rectangular Data that can be represented as a matrix with `samples` on the first axis and a fixed, finite set of `features` on the second is called rectangular.
> 
> This term excludes samples with non-vectorial structures, such as text, an image of arbitrary size, a time series of arbitrary length, a set of vectors, etc. The purpose of a `vectorizer` is to produce rectangular forms of such data.
> 
> sample samples We usually use this term as a noun to indicate a single feature vector. Elsewhere a sample is called an instance, data point, or observation. `n_samples` indicates the number of samples in a dataset, being the number of rows in a data array `X`. Note that this definition is standard in machine learning and deviates from statistics where it means *a set of individuals or objects collected or selected*.
> 
> sample property sample properties A sample property is data for each sample (e.g. an array of length n\_samples) passed to an estimator method or a similar function, alongside but distinct from the `features` (`X`) and `target` (`y`). The most prominent example is `sample_weight`; see others at \[glossary\_sample\_props\](\#glossary\_sample\_props).
> 
> As of version 0.19 we do not have a consistent approach to handling sample properties and their routing in `meta-estimators`, though a `fit_params` parameter is often used.
> 
> scikit-learn-contrib A venue for publishing Scikit-learn-compatible libraries that are broadly authorized by the core developers and the contrib community, but not maintained by the core developer team. See <https://scikit-learn-contrib.github.io>.
> 
> scikit-learn enhancement proposals SLEP SLEPs Changes to the API principles and changes to dependencies or supported versions happen via a \[SLEP \<slep\>\](\#slep-\<slep\>) and follows the decision-making process outlined in \[governance\](\#governance). For all votes, a proposal must have been made public and discussed before the vote. Such a proposal must be a consolidated document, in the form of a "Scikit-Learn Enhancement Proposal" (SLEP), rather than a long discussion on an issue. A SLEP must be submitted as a pull-request to [enhancement proposals](https://scikit-learn-enhancement-proposals.readthedocs.io) using the [SLEP template](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep_template.html).
> 
> semi-supervised semi-supervised learning semisupervised Learning where the expected prediction (label or ground truth) is only available for some samples provided as training data when `fitting` the model. We conventionally apply the label `-1` to `unlabeled` samples in semi-supervised classification.
> 
> sparse matrix sparse graph A representation of two-dimensional numeric data that is more memory efficient the corresponding dense numpy array where almost all elements are zero. We use the `scipy.sparse` framework, which provides several underlying sparse data representations, or *formats*. Some formats are more efficient than others for particular tasks, and when a particular format provides especial benefit, we try to document this fact in Scikit-learn parameter descriptions.
> 
> Some sparse matrix formats (notably CSR, CSC, COO and LIL) distinguish between *implicit* and *explicit* zeros. Explicit zeros are stored (i.e. they consume memory in a `data` array) in the data structure, while implicit zeros correspond to every element not otherwise defined in explicit storage.
> 
> Two semantics for sparse matrices are used in Scikit-learn:
> 
>   - matrix semantics  
>     The sparse matrix is interpreted as an array with implicit and explicit zeros being interpreted as the number 0. This is the interpretation most often adopted, e.g. when sparse matrices are used for feature matrices or `multilabel indicator
>     matrices`.
> 
>   - graph semantics  
>     As with `scipy.sparse.csgraph`, explicit zeros are interpreted as the number 0, but implicit zeros indicate a masked or absent value, such as the absence of an edge between two vertices of a graph, where an explicit value indicates an edge's weight. This interpretation is adopted to represent connectivity in clustering, in representations of nearest neighborhoods (e.g. <span class="title-ref">neighbors.kneighbors\_graph</span>), and for precomputed distance representation where only distances in the neighborhood of each point are required.
> 
> When working with sparse matrices, we assume that it is sparse for a good reason, and avoid writing code that densifies a user-provided sparse matrix, instead maintaining sparsity or raising an error if not possible (i.e. if an estimator does not / cannot support sparse matrices).
> 
> stateless An estimator is stateless if it does not store any information that is obtained during `fit`. This information can be either parameters learned during `fit` or statistics computed from the training data. An estimator is stateless if it has no `attributes` apart from ones set in <span class="title-ref">\_\_init\_\_</span>. Calling `fit` for these estimators will only validate the public `attributes` passed in <span class="title-ref">\_\_init\_\_</span>.
> 
> supervised supervised learning Learning where the expected prediction (label or ground truth) is available for each sample when `fitting` the model, provided as `y`. This is the approach taken in a `classifier` or `regressor` among other estimators.
> 
> target targets The *dependent variable* in `supervised` (and `semisupervised`) learning, passed as `y` to an estimator's `fit` method. Also known as *dependent variable*, *outcome variable*, *response variable*, *ground truth* or *label*. Scikit-learn works with targets that have minimal structure: a class from a finite set, a finite real-valued number, multiple classes, or multiple numbers. See \[glossary\_target\_types\](\#glossary\_target\_types).
> 
> transduction transductive A transductive (contrasted with `inductive`) machine learning method is designed to model a specific dataset, but not to apply that model to unseen data. Examples include <span class="title-ref">manifold.TSNE</span>, <span class="title-ref">cluster.AgglomerativeClustering</span> and <span class="title-ref">neighbors.LocalOutlierFactor</span>.
> 
> unlabeled unlabeled data Samples with an unknown ground truth when fitting; equivalently, `missing values` in the `target`. See also `semisupervised` and `unsupervised` learning.
> 
> unsupervised unsupervised learning Learning where the expected prediction (label or ground truth) is not available for each sample when `fitting` the model, as in `clusterers` and `outlier detectors`. Unsupervised estimators ignore any `y` passed to `fit`.

## Class APIs and Estimator Types

<div class="glossary">

classifier classifiers A `supervised` (or `semi-supervised`) `predictor` with a finite set of discrete possible output values.

> A classifier supports modeling some of `binary`, `multiclass`, `multilabel`, or `multiclass
> multioutput` targets. Within scikit-learn, all classifiers support multi-class classification, defaulting to using a one-vs-rest strategy over the binary classification problem.
> 
> Classifiers must store a `classes_` attribute after fitting, and inherit from <span class="title-ref">base.ClassifierMixin</span>, which sets their corresponding `estimator tags` correctly.
> 
> A classifier can be distinguished from other estimators with <span class="title-ref">\~base.is\_classifier</span>.
> 
> A classifier must implement:
> 
>   - `fit`
>   - `predict`
>   - `score`
> 
> It may also be appropriate to implement `decision_function`, `predict_proba` and `predict_log_proba`.

clusterer clusterers A `unsupervised` `predictor` with a finite set of discrete output values.

> A clusterer usually stores `labels_` after fitting, and must do so if it is `transductive`.
> 
> A clusterer must implement:
> 
>   - `fit`
>   - `fit_predict` if `transductive`
>   - `predict` if `inductive`

  - density estimator  
    An `unsupervised` estimation of input probability density function. Commonly used techniques are:
    
      - \[kernel\_density\](\#kernel\_density) - uses a kernel function, controlled by the bandwidth parameter to represent density;
      - \[Gaussian mixture \<mixture\>\](\#gaussian-mixture-\<mixture\>) - uses mixture of Gaussian models to represent density.

estimator estimators An object which manages the estimation and decoding of a model. The model is estimated as a deterministic function of:

>   - `parameters` provided in object construction or with `set_params`;
>   - the global `numpy.random` random state if the estimator's `random_state` parameter is set to None; and
>   - any data or `sample properties` passed to the most recent call to `fit`, `fit_transform` or `fit_predict`, or data similarly passed in a sequence of calls to `partial_fit`.
> 
> The estimated model is stored in public and private `attributes` on the estimator instance, facilitating decoding through prediction and transformation methods.
> 
> Estimators must provide a `fit` method, and should provide `set_params` and `get_params`, although these are usually provided by inheritance from <span class="title-ref">base.BaseEstimator</span>.
> 
> The core functionality of some estimators may also be available as a `function`.

feature extractor feature extractors A `transformer` which takes input where each sample is not represented as an `array-like` object of fixed length, and produces an `array-like` object of `features` for each sample (and thus a 2-dimensional array-like for a set of samples). In other words, it (lossily) maps a non-rectangular data representation into `rectangular` data.

> Feature extractors must implement at least:
> 
>   - `fit`
>   - `transform`
>   - `get_feature_names_out`

meta-estimator meta-estimators metaestimator metaestimators An `estimator` which takes another estimator as a parameter. Examples include <span class="title-ref">pipeline.Pipeline</span>, <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">feature\_selection.SelectFromModel</span> and <span class="title-ref">ensemble.BaggingClassifier</span>.

> In a meta-estimator's `fit` method, any contained estimators should be `cloned` before they are fit (although FIXME: Pipeline and FeatureUnion do not do this currently). An exception to this is that an estimator may explicitly document that it accepts a pre-fitted estimator (e.g. using `prefit=True` in <span class="title-ref">feature\_selection.SelectFromModel</span>). One known issue with this is that the pre-fitted estimator will lose its model if the meta-estimator is cloned. A meta-estimator should have `fit` called before prediction, even if all contained estimators are pre-fitted.
> 
> In cases where a meta-estimator's primary behaviors (e.g. `predict` or `transform` implementation) are functions of prediction/transformation methods of the provided *base estimator* (or multiple base estimators), a meta-estimator should provide at least the standard methods provided by the base estimator. It may not be possible to identify which methods are provided by the underlying estimator until the meta-estimator has been `fitted` (see also `duck typing`), for which <span class="title-ref">utils.metaestimators.available\_if</span> may help. It should also provide (or modify) the `estimator tags` and `classes_` attribute provided by the base estimator.
> 
> Meta-estimators should be careful to validate data as minimally as possible before passing it to an underlying estimator. This saves computation time, and may, for instance, allow the underlying estimator to easily work with data that is not `rectangular`.

outlier detector outlier detectors An `unsupervised` binary `predictor` which models the distinction between core and outlying samples.

> Outlier detectors must implement:
> 
>   - `fit`
>   - `fit_predict` if `transductive`
>   - `predict` if `inductive`
> 
> Inductive outlier detectors may also implement `decision_function` to give a normalized inlier score where outliers have score below 0. `score_samples` may provide an unnormalized score per sample.

predictor predictors An `estimator` supporting `predict` and/or `fit_predict`. This encompasses `classifier`, `regressor`, `outlier detector` and `clusterer`.

> In statistics, "predictors" refers to `features`.

regressor regressors A `supervised` (or `semi-supervised`) `predictor` with `continuous` output values.

> Regressors inherit from <span class="title-ref">base.RegressorMixin</span>, which sets their `estimator tags` correctly.
> 
> A regressor can be distinguished from other estimators with <span class="title-ref">\~base.is\_regressor</span>.
> 
> A regressor must implement:
> 
>   - `fit`
>   - `predict`
>   - `score`

transformer transformers An estimator supporting `transform` and/or `fit_transform`. A purely `transductive` transformer, such as <span class="title-ref">manifold.TSNE</span>, may not implement `transform`.

vectorizer vectorizers See `feature extractor`.

</div>

There are further APIs specifically related to a small family of estimators, such as:

<div class="glossary">

cross-validation splitter CV splitter cross-validation generator A non-estimator family of classes used to split a dataset into a sequence of train and test portions (see \[cross\_validation\](\#cross\_validation)), by providing `split` and `get_n_splits` methods. Note that unlike estimators, these do not have `fit` methods and do not provide `set_params` or `get_params`. Parameter validation may be performed in `__init__`.

  - cross-validation estimator  
    An estimator that has built-in cross-validation capabilities to automatically select the best hyper-parameters (see the \[User Guide \<grid\_search\>\](\#user

</div>

  - \--------guide-\<grid\_search\>)). Some example of cross-validation estimators  
    are <span class="title-ref">ElasticNetCV \<linear\_model.ElasticNetCV\></span> and <span class="title-ref">LogisticRegressionCV \<linear\_model.LogisticRegressionCV\></span>. Cross-validation estimators are named <span class="title-ref">EstimatorCV</span> and tend to be roughly equivalent to <span class="title-ref">GridSearchCV(Estimator(), ...)</span>. The advantage of using a cross-validation estimator over the canonical `estimator` class along with \[grid search \<grid\_search\>\](\#grid-search-\<grid\_search\>) is that they can take advantage of warm-starting by reusing precomputed results in the previous steps of the cross-validation process. This generally leads to speed improvements. An exception is the <span class="title-ref">RidgeCV \<linear\_model.RidgeCV\></span> class, which can instead perform efficient Leave-One-Out (LOO) CV. By default, all these estimators, apart from <span class="title-ref">RidgeCV \<linear\_model.RidgeCV\></span> with an LOO-CV, will be refitted on the full training dataset after finding the best combination of hyper-parameters.
    
    scorer A non-estimator callable object which evaluates an estimator on given test data, returning a number. Unlike `evaluation metrics`, a greater returned number must correspond with a *better* score. See \[scoring\_parameter\](\#scoring\_parameter).

Further examples:

  - <span class="title-ref">metrics.DistanceMetric</span>
  - <span class="title-ref">gaussian\_process.kernels.Kernel</span>
  - `tree.Criterion`

## Metadata Routing

<div class="glossary">

  - consumer  
    An object which consumes `metadata`. This object is usually an `estimator`, a `scorer`, or a `CV splitter`. Consuming metadata means using it in calculations, e.g. using `sample_weight` to calculate a certain type of score. Being a consumer doesn't mean that the object always receives a certain metadata, rather it means it can use it if it is provided.

  - metadata  
    Data which is related to the given `X` and `y` data, but is not directly a part of the data, e.g. `sample_weight` or `groups`, and is passed along to different objects and methods, e.g. to a `scorer` or a `CV splitter`.

  - router  
    An object which routes metadata to `consumers <consumer>`. This object is usually a `meta-estimator`, e.g. <span class="title-ref">\~pipeline.Pipeline</span> or <span class="title-ref">\~model\_selection.GridSearchCV</span>. Some routers can also be a consumer. This happens for example when a meta-estimator uses the given `groups`, and it also passes it along to some of its sub-objects, such as a `CV splitter`.

</div>

Please refer to \[Metadata Routing User Guide \<metadata\_routing\>\](\#metadata-routing-user-guide-\<metadata\_routing\>) for more information.

## Target Types

<div class="glossary">

  - binary  
    A classification problem consisting of two classes. A binary target may be represented as for a `multiclass` problem but with only two labels. A binary decision function is represented as a 1d array.
    
    Semantically, one class is often considered the "positive" class. Unless otherwise specified (e.g. using `pos_label` in `evaluation metrics`), we consider the class label with the greater value (numerically or lexicographically) as the positive class: of labels \[0, 1\], 1 is the positive class; of \[1, 2\], 2 is the positive class; of \['no', 'yes'\], 'yes' is the positive class; of \['no', 'YES'\], 'no' is the positive class. This affects the output of `decision_function`, for instance.
    
    Note that a dataset sampled from a multiclass `y` or a continuous `y` may appear to be binary.
    
    <span class="title-ref">\~utils.multiclass.type\_of\_target</span> will return 'binary' for binary input, or a similar array with only a single class present.

  - continuous  
    A regression problem where each sample's target is a finite floating point number represented as a 1-dimensional array of floats (or sometimes ints).
    
    <span class="title-ref">\~utils.multiclass.type\_of\_target</span> will return 'continuous' for continuous input, but if the data is all integers, it will be identified as 'multiclass'.

continuous multioutput continuous multi-output multioutput continuous multi-output continuous A regression problem where each sample's target consists of `n_outputs` `outputs`, each one a finite floating point number, for a fixed int `n_outputs > 1` in a particular dataset.

> Continuous multioutput targets are represented as multiple `continuous` targets, horizontally stacked into an array of shape `(n_samples, n_outputs)`.
> 
> <span class="title-ref">\~utils.multiclass.type\_of\_target</span> will return 'continuous-multioutput' for continuous multioutput input, but if the data is all integers, it will be identified as 'multiclass-multioutput'.

multiclass multi-class A classification problem consisting of more than two classes. A multiclass target may be represented as a 1-dimensional array of strings or integers. A 2d column vector of integers (i.e. a single output in `multioutput` terms) is also accepted.

> We do not officially support other orderable, hashable objects as class labels, even if estimators may happen to work when given classification targets of such type.
> 
> For semi-supervised classification, `unlabeled` samples should have the special label -1 in `y`.
> 
> Within scikit-learn, all estimators supporting binary classification also support multiclass classification, using One-vs-Rest by default.
> 
> A <span class="title-ref">preprocessing.LabelEncoder</span> helps to canonicalize multiclass targets as integers.
> 
> <span class="title-ref">\~utils.multiclass.type\_of\_target</span> will return 'multiclass' for multiclass input. The user may also want to handle 'binary' input identically to 'multiclass'.

multiclass multioutput multi-class multi-output multioutput multiclass multi-output multi-class A classification problem where each sample's target consists of `n_outputs` `outputs`, each a class label, for a fixed int `n_outputs > 1` in a particular dataset. Each output has a fixed set of available classes, and each sample is labeled with a class for each output. An output may be binary or multiclass, and in the case where all outputs are binary, the target is `multilabel`.

> Multiclass multioutput targets are represented as multiple `multiclass` targets, horizontally stacked into an array of shape `(n_samples, n_outputs)`.
> 
> XXX: For simplicity, we may not always support string class labels for multiclass multioutput, and integer class labels should be used.
> 
> `~sklearn.multioutput` provides estimators which estimate multi-output problems using multiple single-output estimators. This may not fully account for dependencies among the different outputs, which methods natively handling the multioutput case (e.g. decision trees, nearest neighbors, neural networks) may do better.
> 
> <span class="title-ref">\~utils.multiclass.type\_of\_target</span> will return 'multiclass-multioutput' for multiclass multioutput input.

multilabel multi-label A `multiclass multioutput` target where each output is `binary`. This may be represented as a 2d (dense) array or sparse matrix of integers, such that each column is a separate binary target, where positive labels are indicated with 1 and negative labels are usually -1 or 0. Sparse multilabel targets are not supported everywhere that dense multilabel targets are supported.

> Semantically, a multilabel target can be thought of as a set of labels for each sample. While not used internally, <span class="title-ref">preprocessing.MultiLabelBinarizer</span> is provided as a utility to convert from a list of sets representation to a 2d array or sparse matrix. One-hot encoding a multiclass target with <span class="title-ref">preprocessing.LabelBinarizer</span> turns it into a multilabel problem.
> 
> <span class="title-ref">\~utils.multiclass.type\_of\_target</span> will return 'multilabel-indicator' for multilabel input, whether sparse or dense.

multioutput multi-output A target where each sample has multiple classification/regression labels. See `multiclass multioutput` and `continuous
    multioutput`. We do not currently support modelling mixed classification and regression targets.

</div>

## Methods

<div class="glossary">

  - `decision_function`  
    In a fitted `classifier` or `outlier detector`, predicts a "soft" score for each sample in relation to each class, rather than the "hard" categorical prediction produced by `predict`. Its input is usually only some observed data, `X`.
    
    If the estimator was not already `fitted`, calling this method should raise a <span class="title-ref">exceptions.NotFittedError</span>.
    
    Output conventions:
    
      - binary classification  
        A 1-dimensional array, where values strictly greater than zero indicate the positive class (i.e. the last class in `classes_`).
    
      - multiclass classification  
        A 2-dimensional array, where the row-wise arg-maximum is the predicted class. Columns are ordered according to `classes_`.
    
      - multilabel classification  
        Scikit-learn is inconsistent in its representation of `multilabel` decision functions. It may be represented one of two ways:
        
          - List of 2d arrays, each array of shape: (<span class="title-ref">n\_samples</span>, 2), like in multiclass multioutput. List is of length <span class="title-ref">n\_labels</span>.
          - Single 2d array of shape (<span class="title-ref">n\_samples</span>, <span class="title-ref">n\_labels</span>), with each 'column' in the array corresponding to the individual binary classification decisions. This is identical to the multiclass classification format, though its semantics differ: it should be interpreted, like in the binary case, by thresholding at 0.
    
      - multioutput classification  
        A list of 2d arrays, corresponding to each multiclass decision function.
    
      - outlier detection  
        A 1-dimensional array, where a value greater than or equal to zero indicates an inlier.

  - `fit`  
    The `fit` method is provided on every estimator. It usually takes some `samples` `X`, `targets` `y` if the model is supervised, and potentially other `sample properties` such as `sample_weight`. It should:
    
      - clear any prior `attributes` stored on the estimator, unless `warm_start` is used;
      - validate and interpret any `parameters`, ideally raising an error if invalid;
      - validate the input data;
      - estimate and store model attributes from the estimated parameters and provided data; and
      - return the now `fitted` estimator to facilitate method chaining.
    
    \[glossary\_target\_types\](\#glossary\_target\_types) describes possible formats for `y`.

  - `fit_predict`  
    Used especially for `unsupervised`, `transductive` estimators, this fits the model and returns the predictions (similar to `predict`) on the training data. In clusterers, these predictions are also stored in the `labels_` attribute, and the output of `.fit_predict(X)` is usually equivalent to `.fit(X).predict(X)`. The parameters to `fit_predict` are the same as those to `fit`.

  - `fit_transform`  
    A method on `transformers` which fits the estimator and returns the transformed training data. It takes parameters as in `fit` and its output should have the same shape as calling `.fit(X, ...).transform(X)`. There are nonetheless rare cases where `.fit_transform(X, ...)` and `.fit(X, ...).transform(X)` do not return the same value, wherein training data needs to be handled differently (due to model blending in stacked ensembles, for instance; such cases should be clearly documented). `Transductive <transductive>` transformers may also provide `fit_transform` but not `transform`.
    
    One reason to implement `fit_transform` is that performing `fit` and `transform` separately would be less efficient than together. <span class="title-ref">base.TransformerMixin</span> provides a default implementation, providing a consistent interface across transformers where `fit_transform` is or is not specialized.
    
    In `inductive` learning -- where the goal is to learn a generalized model that can be applied to new data -- users should be careful not to apply `fit_transform` to the entirety of a dataset (i.e. training and test data together) before further modelling, as this results in `data leakage`.

  - `get_feature_names_out`  
    Primarily for `feature extractors`, but also used for other transformers to provide string names for each column in the output of the estimator's `transform` method. It outputs an array of strings and may take an array-like of strings as input, corresponding to the names of input columns from which output column names can be generated. If <span class="title-ref">input\_features</span> is not passed in, then the <span class="title-ref">feature\_names\_in\_</span> attribute will be used. If the <span class="title-ref">feature\_names\_in\_</span> attribute is not defined, then the input names are named <span class="title-ref">\[x0, x1, ..., x(n\_features\_in\_ - 1)\]</span>.

  - `get_n_splits`  
    On a `CV splitter` (not an estimator), returns the number of elements one would get if iterating through the return value of `split` given the same parameters. Takes the same parameters as split.

  - `get_params`  
    Gets all `parameters`, and their values, that can be set using `set_params`. A parameter `deep` can be used, when set to False to only return those parameters not including `__`, i.e. not due to indirection via contained estimators.
    
    Most estimators adopt the definition from <span class="title-ref">base.BaseEstimator</span>, which simply adopts the parameters defined for `__init__`. <span class="title-ref">pipeline.Pipeline</span>, among others, reimplements `get_params` to declare the estimators named in its `steps` parameters as themselves being parameters.

  - `partial_fit`  
    Facilitates fitting an estimator in an online fashion. Unlike `fit`, repeatedly calling `partial_fit` does not clear the model, but updates it with the data provided. The portion of data provided to `partial_fit` may be called a mini-batch. Each mini-batch must be of consistent shape, etc. In iterative estimators, `partial_fit` often only performs a single iteration.
    
    `partial_fit` may also be used for `out-of-core` learning, although usually limited to the case where learning can be performed online, i.e. the model is usable after each `partial_fit` and there is no separate processing needed to finalize the model. <span class="title-ref">cluster.Birch</span> introduces the convention that calling `partial_fit(X)` will produce a model that is not finalized, but the model can be finalized by calling `partial_fit()` i.e. without passing a further mini-batch.
    
    Generally, estimator parameters should not be modified between calls to `partial_fit`, although `partial_fit` should validate them as well as the new mini-batch of data. In contrast, `warm_start` is used to repeatedly fit the same estimator with the same data but varying parameters.
    
    Like `fit`, `partial_fit` should return the estimator object.
    
    To clear the model, a new estimator should be constructed, for instance with <span class="title-ref">base.clone</span>.
    
    NOTE: Using `partial_fit` after `fit` results in undefined behavior.

  - `predict`  
    Makes a prediction for each sample, usually only taking `X` as input (but see under regressor output conventions below). In a `classifier` or `regressor`, this prediction is in the same target space used in fitting (e.g. one of {'red', 'amber', 'green'} if the `y` in fitting consisted of these strings). Despite this, even when `y` passed to `fit` is a list or other array-like, the output of `predict` should always be an array or sparse matrix. In a `clusterer` or `outlier detector` the prediction is an integer.
    
    If the estimator was not already `fitted`, calling this method should raise a <span class="title-ref">exceptions.NotFittedError</span>.
    
    Output conventions:
    
      - classifier  
        An array of shape `(n_samples,)` `(n_samples, n_outputs)`. `Multilabel <multilabel>` data may be represented as a sparse matrix if a sparse matrix was used in fitting. Each element should be one of the values in the classifier's `classes_` attribute.
    
      - clusterer  
        An array of shape `(n_samples,)` where each value is from 0 to `n_clusters - 1` if the corresponding sample is clustered, and -1 if the sample is not clustered, as in <span class="title-ref">cluster.dbscan</span>.
    
      - outlier detector  
        An array of shape `(n_samples,)` where each value is -1 for an outlier and 1 otherwise.
    
      - regressor  
        A numeric array of shape `(n_samples,)`, usually float64. Some regressors have extra options in their `predict` method, allowing them to return standard deviation (`return_std=True`) or covariance (`return_cov=True`) relative to the predicted value. In this case, the return value is a tuple of arrays corresponding to (prediction mean, std, cov) as required.

  - `predict_log_proba`  
    The natural logarithm of the output of `predict_proba`, provided to facilitate numerical stability.

  - `predict_proba`  
    A method in `classifiers` and `clusterers` that can return probability estimates for each class/cluster. Its input is usually only some observed data, `X`.
    
    If the estimator was not already `fitted`, calling this method should raise a <span class="title-ref">exceptions.NotFittedError</span>.
    
    Output conventions are like those for `decision_function` except in the `binary` classification case, where one column is output for each class (while `decision_function` outputs a 1d array). For binary and multiclass predictions, each row should add to 1.
    
    Like other methods, `predict_proba` should only be present when the estimator can make probabilistic predictions (see `duck typing`). This means that the presence of the method may depend on estimator parameters (e.g. in <span class="title-ref">linear\_model.SGDClassifier</span>) or training data (e.g. in <span class="title-ref">model\_selection.GridSearchCV</span>) and may only appear after fitting.

  - `score`  
    A method on an estimator, usually a `predictor`, which evaluates its predictions on a given dataset, and returns a single numerical score. A greater return value should indicate better predictions; accuracy is used for classifiers and R^2 for regressors by default.
    
    If the estimator was not already `fitted`, calling this method should raise a <span class="title-ref">exceptions.NotFittedError</span>.
    
    Some estimators implement a custom, estimator-specific score function, often the likelihood of the data under the model.

  - `score_samples`  
    A method that returns a score for each given sample. The exact definition of *score* varies from one class to another. In the case of density estimation, it can be the log density model on the data, and in the case of outlier detection, it can be the opposite of the outlier factor of the data.
    
    If the estimator was not already `fitted`, calling this method should raise a <span class="title-ref">exceptions.NotFittedError</span>.

  - `set_params`  
    Available in any estimator, takes keyword arguments corresponding to keys in `get_params`. Each is provided a new value to assign such that calling `get_params` after `set_params` will reflect the changed `parameters`. Most estimators use the implementation in <span class="title-ref">base.BaseEstimator</span>, which handles nested parameters and otherwise sets the parameter as an attribute on the estimator. The method is overridden in <span class="title-ref">pipeline.Pipeline</span> and related estimators.

  - `split`  
    On a `CV splitter` (not an estimator), this method accepts parameters (`X`, `y`, `groups`), where all may be optional, and returns an iterator over `(train_idx, test_idx)` pairs. Each of {train,test}\_idx is a 1d integer array, with values from 0 from `X.shape[0] - 1` of any length, such that no values appear in both some `train_idx` and its corresponding `test_idx`.

  - `transform`  
    In a `transformer`, transforms the input, usually only `X`, into some transformed space (conventionally notated as `Xt`). Output is an array or sparse matrix of length `n_samples` and with the number of columns fixed after `fitting`.
    
    If the estimator was not already `fitted`, calling this method should raise a <span class="title-ref">exceptions.NotFittedError</span>.

</div>

## Parameters

These common parameter names, specifically used in estimator construction (see concept `parameter`), sometimes also appear as parameters of functions or non-estimator constructors.

<div class="glossary">

  - `class_weight`  
    Used to specify sample weights when fitting classifiers as a function of the `target` class. Where `sample_weight` is also supported and given, it is multiplied by the `class_weight` contribution. Similarly, where `class_weight` is used in a `multioutput` (including `multilabel`) tasks, the weights are multiplied across outputs (i.e. columns of `y`).
    
    By default, all samples have equal weight such that classes are effectively weighted by their prevalence in the training data. This could be achieved explicitly with `class_weight={label1: 1, label2: 1, ...}` for all class labels.
    
    More generally, `class_weight` is specified as a dict mapping class labels to weights (`{class_label: weight}`), such that each sample of the named class is given that weight.
    
    `class_weight='balanced'` can be used to give all classes equal weight by giving each sample a weight inversely related to its class's prevalence in the training data: `n_samples / (n_classes * np.bincount(y))`. Class weights will be used differently depending on the algorithm: for linear models (such as linear SVM or logistic regression), the class weights will alter the loss function by weighting the loss of each sample by its class weight. For tree-based algorithms, the class weights will be used for reweighting the splitting criterion. **Note** however that this rebalancing does not take the weight of samples in each class into account.
    
    For multioutput classification, a list of dicts is used to specify weights for each output. For example, for four-class multilabel classification weights should be `[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}]` instead of `[{1:1}, {2:5}, {3:1}, {4:1}]`.
    
    The `class_weight` parameter is validated and interpreted with <span class="title-ref">utils.class\_weight.compute\_class\_weight</span>.

  - `cv`  
    Determines a cross validation splitting strategy, as used in cross-validation based routines. `cv` is also available in estimators such as <span class="title-ref">multioutput.ClassifierChain</span> or <span class="title-ref">calibration.CalibratedClassifierCV</span> which use the predictions of one estimator as training data for another, to not overfit the training supervision.
    
    Possible inputs for `cv` are usually:
    
      - An integer, specifying the number of folds in K-fold cross validation. K-fold will be stratified over classes if the estimator is a classifier (determined by <span class="title-ref">base.is\_classifier</span>) and the `targets` may represent a binary or multiclass (but not multioutput) classification problem (determined by <span class="title-ref">utils.multiclass.type\_of\_target</span>).
      - A `cross-validation splitter` instance. Refer to the \[User Guide \<cross\_validation\>\](\#user-guide-\<cross\_validation\>) for splitters available within Scikit-learn.
      - An iterable yielding train/test splits.
    
    With some exceptions (especially where not using cross validation at all is an option), the default is 5-fold.
    
    `cv` values are validated and interpreted with <span class="title-ref">model\_selection.check\_cv</span>.

  - `kernel`  
    Specifies the kernel function to be used by Kernel Method algorithms. For example, the estimators <span class="title-ref">svm.SVC</span> and <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span> both have a `kernel` parameter that takes the name of the kernel to use as string or a callable kernel function used to compute the kernel matrix. For more reference, see the \[kernel\_approximation\](\#kernel\_approximation) and the \[gaussian\_process\](\#gaussian\_process) user guides.

  - `max_iter`  
    For estimators involving iterative optimization, this determines the maximum number of iterations to be performed in `fit`. If `max_iter` iterations are run without convergence, a <span class="title-ref">exceptions.ConvergenceWarning</span> should be raised. Note that the interpretation of "a single iteration" is inconsistent across estimators: some, but not all, use it to mean a single epoch (i.e. a pass over every sample in the data).
    
    FIXME perhaps we should have some common tests about the relationship between ConvergenceWarning and max\_iter.

  - `memory`  
    Some estimators make use of <span class="title-ref">joblib.Memory</span> to store partial solutions during fitting. Thus when `fit` is called again, those partial solutions have been memoized and can be reused.
    
    A `memory` parameter can be specified as a string with a path to a directory, or a <span class="title-ref">joblib.Memory</span> instance (or an object with a similar interface, i.e. a `cache` method) can be used.
    
    `memory` values are validated and interpreted with <span class="title-ref">utils.validation.check\_memory</span>.

  - `metric`  
    As a parameter, this is the scheme for determining the distance between two data points. See <span class="title-ref">metrics.pairwise\_distances</span>. In practice, for some algorithms, an improper distance metric (one that does not obey the triangle inequality, such as Cosine Distance) may be used.
    
    XXX: hierarchical clustering uses `affinity` with this meaning.
    
    We also use *metric* to refer to `evaluation metrics`, but avoid using this sense as a parameter name.

  - `n_components`  
    The number of features which a `transformer` should transform the input into. See `components_` for the special case of affine projection.

  - `n_iter_no_change`  
    Number of iterations with no improvement to wait before stopping the iterative procedure. This is also known as a *patience* parameter. It is typically used with `early stopping` to avoid stopping too early.

  - `n_jobs`  
    This parameter is used to specify how many concurrent processes or threads should be used for routines that are parallelized with `joblib`.
    
    `n_jobs` is an integer, specifying the maximum number of concurrently running workers. If 1 is given, no joblib parallelism is used at all, which is useful for debugging. If set to -1, all CPUs are used. For `n_jobs` below -1, (n\_cpus + 1 + n\_jobs) are used. For example with `n_jobs=-2`, all CPUs but one are used.
    
    `n_jobs` is `None` by default, which means *unset*; it will generally be interpreted as `n_jobs=1`, unless the current <span class="title-ref">joblib.Parallel</span> backend context specifies otherwise.
    
    Note that even if `n_jobs=1`, low-level parallelism (via Numpy and OpenMP) might be used in some configuration.
    
    For more details on the use of `joblib` and its interactions with scikit-learn, please refer to our \[parallelism notes \<parallelism\>\](\#parallelism-notes

</div>

\--------\<parallelism\>).

>   - `pos_label`  
>     Value with which positive labels must be encoded in binary classification problems in which the positive class is not assumed. This value is typically required to compute asymmetric evaluation metrics such as precision and recall.
> 
>   - `random_state`  
>     Whenever randomization is part of a Scikit-learn algorithm, a `random_state` parameter may be provided to control the random number generator used. Note that the mere presence of `random_state` doesn't mean that randomization is always used, as it may be dependent on another parameter, e.g. `shuffle`, being set.
>     
>     The passed value will have an effect on the reproducibility of the results returned by the function (`fit`, `split`, or any other function like <span class="title-ref">\~sklearn.cluster.k\_means</span>). <span class="title-ref">random\_state</span>'s value may be:
>     
>       - None (default)  
>         Use the global random state instance from `numpy.random`. Calling the function multiple times will reuse the same instance, and will produce different results.
>     
>       - An integer  
>         Use a new random number generator seeded by the given integer. Using an int will produce the same results across different calls. However, it may be worthwhile checking that your results are stable across a number of different distinct random seeds. Popular integer random seeds are 0 and [42](https://en.wikipedia.org/wiki/Answer_to_the_Ultimate_Question_of_Life%2C_the_Universe%2C_and_Everything). Integer values must be in the range <span class="title-ref">\[0, 2\*\*32 - 1\]</span>.
>     
>       - A <span class="title-ref">numpy.random.RandomState</span> instance  
>         Use the provided random state, only affecting other users of that same random state instance. Calling the function multiple times will reuse the same instance, and will produce different results.
>     
>     <span class="title-ref">utils.check\_random\_state</span> is used internally to validate the input `random_state` and return a <span class="title-ref">\~numpy.random.RandomState</span> instance.
>     
>     For more details on how to control the randomness of scikit-learn objects and avoid common pitfalls, you may refer to \[randomness\](\#randomness).
> 
>   - `scoring`  
>     Specifies the score function to be maximized (usually by \[cross validation \<cross\_validation\>\](\#cross

  - \--------validation-\<cross\_validation\>)), or -- in some cases -- multiple score  
    functions to be reported. The score function can be a string accepted by <span class="title-ref">metrics.get\_scorer</span> or a callable `scorer`, not to be confused with an `evaluation metric`, as the latter have a more diverse API. `scoring` may also be set to None, in which case the estimator's `score` method is used. See \[scoring\_parameter\](\#scoring\_parameter) in the User Guide.
    
    Where multiple metrics can be evaluated, `scoring` may be given either as a list of unique strings, a dictionary with names as keys and callables as values or a callable that returns a dictionary. Note that this does *not* specify which score function is to be maximized, and another parameter such as `refit` maybe used for this purpose.
    
    The `scoring` parameter is validated and interpreted using <span class="title-ref">metrics.check\_scoring</span>.
    
    `verbose` Logging is not handled very consistently in Scikit-learn at present, but when it is provided as an option, the `verbose` parameter is usually available to choose no logging (set to False). Any True value should enable some logging, but larger integers (e.g. above 10) may be needed for full verbosity. Verbose logs are usually printed to Standard Output. Estimators should not produce any output on Standard Output with the default `verbose` setting.
    
    `warm_start`
    
    When fitting an estimator repeatedly on the same dataset, but for multiple parameter values (such as to find the value maximizing performance as in \[grid search \<grid\_search\>\](\#grid-search-\<grid\_search\>)), it may be possible to reuse aspects of the model learned from the previous parameter value, saving time. When `warm_start` is true, the existing `fitted` model `attributes` are used to initialize the new model in a subsequent call to `fit`.
    
    Note that this is only applicable for some models and some parameters, and even some orders of parameter values. In general, there is an interaction between `warm_start` and the parameter controlling the number of iterations of the estimator.
    
    For estimators imported from `~sklearn.ensemble`, `warm_start` will interact with `n_estimators` or `max_iter`. For these models, the number of iterations, reported via `len(estimators_)` or `n_iter_`, corresponds the total number of estimators/iterations learnt since the initialization of the model. Thus, if a model was already initialized with <span class="title-ref">N</span> estimators, and <span class="title-ref">fit</span> is called with `n_estimators` or `max_iter` set to <span class="title-ref">M</span>, the model will train <span class="title-ref">M - N</span> new estimators.
    
    Other models, usually using gradient-based solvers, have a different behavior. They all expose a `max_iter` parameter. The reported `n_iter_` corresponds to the number of iteration done during the last call to `fit` and will be at most `max_iter`. Thus, we do not consider the state of the estimator since the initialization.
    
    `partial_fit` also retains the model between calls, but differs: with `warm_start` the parameters change and the data is (more-or-less) constant across calls to `fit`; with `partial_fit`, the mini-batch of data changes and model parameters stay fixed.
    
    There are cases where you want to use `warm_start` to fit on different, but closely related data. For example, one may initially fit to a subset of the data, then fine-tune the parameter search on the full dataset. For classification, all data in a sequence of `warm_start` calls to `fit` must include samples from each class.

## Attributes

See concept `attribute`.

<div class="glossary">

  - `classes_`  
    A list of class labels known to the `classifier`, mapping each label to a numerical index used in the model representation our output. For instance, the array output from `predict_proba` has columns aligned with `classes_`. For `multi-output` classifiers, `classes_` should be a list of lists, with one class listing for each output. For each output, the classes should be sorted (numerically, or lexicographically for strings).
    
    `classes_` and the mapping to indices is often managed with <span class="title-ref">preprocessing.LabelEncoder</span>.

  - `components_`  
    An affine transformation matrix of shape `(n_components, n_features)` used in many linear `transformers` where `n_components` is the number of output features and `n_features` is the number of input features.
    
    See also `components_` which is a similar attribute for linear predictors.

  - `coef_`  
    The weight/coefficient matrix of a generalized linear model `predictor`, of shape `(n_features,)` for binary classification and single-output regression, `(n_classes, n_features)` for multiclass classification and `(n_targets, n_features)` for multi-output regression. Note this does not include the intercept (or bias) term, which is stored in `intercept_`.
    
    When available, `feature_importances_` is not usually provided as well, but can be calculated as the norm of each feature's entry in `coef_`.
    
    See also `components_` which is a similar attribute for linear transformers.

  - `embedding_`  
    An embedding of the training data in \[manifold learning \<manifold\>\](\#manifold-learning

</div>

  - \--------\<manifold\>) estimators, with shape `(n_samples, n_components)`,  
    identical to the output of `fit_transform`. See also `labels_`.
    
    `n_iter_` The number of iterations actually performed when fitting an iterative estimator that may stop upon convergence. See also `max_iter`.
    
    `feature_importances_` A vector of shape `(n_features,)` available in some `predictors` to provide a relative measure of the importance of each feature in the predictions of the model.
    
    `labels_` A vector containing a cluster label for each sample of the training data in `clusterers`, identical to the output of `fit_predict`. See also `embedding_`.

## Data and sample properties

See concept `sample property`.

<div class="glossary">

  - `groups`  
    Used in cross-validation routines to identify samples that are correlated. Each value is an identifier such that, in a supporting `CV splitter`, samples from some `groups` value may not appear in both a training set and its corresponding test set. See \[group\_cv\](\#group\_cv).

  - `sample_weight`  
    A relative weight for each sample. Intuitively, if all weights are integers, a weighted model or score should be equivalent to that calculated when repeating the sample the number of times specified in the weight. Weights may be specified as floats, so that sample weights are usually equivalent up to a constant positive scaling factor.
    
    FIXME Is this interpretation always the case in practice? We have no common tests.
    
    Some estimators, such as decision trees, support negative weights. FIXME: This feature or its absence may not be tested or documented in many estimators.
    
    This is not entirely the case where other parameters of the model consider the number of samples in a region, as with `min_samples` in <span class="title-ref">cluster.DBSCAN</span>. In this case, a count of samples becomes to a sum of their weights.
    
    In classification, sample weights can also be specified as a function of class with the `class_weight` estimator `parameter`.

  - `X`  
    Denotes data that is observed at training and prediction time, used as independent variables in learning. The notation is uppercase to denote that it is ordinarily a matrix (see `rectangular`). When a matrix, each sample may be represented by a `feature` vector, or a vector of `precomputed` (dis)similarity with each training sample. `X` may also not be a matrix, and may require a `feature extractor` or a `pairwise metric` to turn it into one before learning a model.

  - `Xt`  
    Shorthand for "transformed `X`".

`y` `Y` Denotes data that may be observed at training time as the dependent variable in learning, but which is unavailable at prediction time, and is usually the `target` of prediction. The notation may be uppercase to denote that it is a matrix, representing `multi-output` targets, for instance; but usually we use `y` and sometimes do so even when multiple outputs are assumed.

</div>

---

governance.md

---

# Scikit-learn governance and decision-making

The purpose of this document is to formalize the governance process used by the scikit-learn project, to clarify how decisions are made and how the various elements of our community interact. This document establishes a decision-making structure that takes into account feedback from all members of the community and strives to find consensus, while avoiding any deadlocks.

This is a meritocratic, consensus-based community project. Anyone with an interest in the project can join the community, contribute to the project design and participate in the decision making process. This document describes how that participation takes place and how to set about earning merit within the project community.

## Roles And Responsibilities

We distinguish between contributors, core contributors, and the technical committee. A key distinction between them is their voting rights: contributors have no voting rights, whereas the other two groups all have voting rights, as well as permissions to the tools relevant to their roles.

### Contributors

Contributors are community members who contribute in concrete ways to the project. Anyone can become a contributor, and contributions can take many forms – not only code – as detailed in the \[contributors guide \<contributing\>\](\#contributors-guide-\<contributing\>). There is no process to become a contributor: once somebody contributes to the project in any way, they are a contributor.

### Core Contributors

All core contributor members have the same voting rights and right to propose new members to any of the roles listed below. Their membership is represented as being an organization member on the scikit-learn [GitHub organization](https://github.com/orgs/scikit-learn/people).

They are also welcome to join our [monthly core contributor meetings](https://github.com/scikit-learn/administrative/tree/master/meeting_notes).

New members can be nominated by any existing member. Once they have been nominated, there will be a vote by the current core contributors. Voting on new members is one of the few activities that takes place on the project's private mailing list. While it is expected that most votes will be unanimous, a two-thirds majority of the cast votes is enough. The vote needs to be open for at least 1 week.

Core contributors that have not contributed to the project, corresponding to their role, in the past 12 months will be asked if they want to become emeritus members and recant their rights until they become active again. The list of members, active and emeritus (with dates at which they became active) is public on the scikit-learn website. It is the responsibility of the active core contributors to send such a yearly reminder email.

The following teams form the core contributors group:

  - **Contributor Experience Team** The contributor experience team improves the experience of contributors by helping with the triage of issues and pull requests, as well as noticing any repeating patterns where people might struggle, and to help with improving those aspects of the project.
    
    To this end, they have the required permissions on GitHub to label and close issues. \[Their work \<bug\_triaging\>\](\#their-work-\<bug\_triaging\>) is crucial to improve the communication in the project and limit the crowding of the issue tracker.

  - **Communication Team** Members of the communication team help with outreach and communication for scikit-learn. The goal of the team is to develop public awareness of scikit-learn, of its features and usage, as well as branding.
    
    For this, they can operate the scikit-learn accounts on various social networks and produce materials. They also have the required rights to our blog repository and other relevant accounts and platforms.

  - **Documentation Team** Members of the documentation team engage with the documentation of the project among other things. They might also be involved in other aspects of the project, but their reviews on documentation contributions are considered authoritative, and can merge such contributions.
    
    To this end, they have permissions to merge pull requests in scikit-learn's repository.

  - **Maintainers Team** Maintainers are community members who have shown that they are dedicated to the continued development of the project through ongoing engagement with the community. They have shown they can be trusted to maintain scikit-learn with care. Being a maintainer allows contributors to more easily carry on with their project related activities by giving them direct access to the project's repository. Maintainers are expected to review code contributions, merge approved pull requests, cast votes for and against merging a pull-request, and to be involved in deciding major changes to the API.

### Technical Committee

The Technical Committee (TC) members are maintainers who have additional responsibilities to ensure the smooth running of the project. TC members are expected to participate in strategic planning, and approve changes to the governance model. The purpose of the TC is to ensure a smooth progress from the big-picture perspective. Indeed changes that impact the full project require a synthetic analysis and a consensus that is both explicit and informed. In cases that the core contributor community (which includes the TC members) fails to reach such a consensus in the required time frame, the TC is the entity to resolve the issue. Membership of the TC is by nomination by a core contributor. A nomination will result in discussion which cannot take more than a month and then a vote by the core contributors which will stay open for a week. TC membership votes are subject to a two-third majority of all cast votes as well as a simple majority approval of all the current TC members. TC members who do not actively engage with the TC duties are expected to resign.

The Technical Committee of scikit-learn consists of `Thomas Fan
<thomasjpfan>`, `Alexandre Gramfort <agramfort>`, `Olivier Grisel
<ogrisel>`, `Adrin Jalali <adrinjalali>`, `Andreas Müller
<amueller>`, `Joel Nothman <jnothman>` and `Gaël Varoquaux
<GaelVaroquaux>`.

## Decision Making Process

Decisions about the future of the project are made through discussion with all members of the community. All non-sensitive project management discussion takes place on the project contributors' [mailing list](mailto:scikit-learn@python.org) and the [issue tracker](https://github.com/scikit-learn/scikit-learn/issues). Occasionally, sensitive discussion occurs on a private list.

Scikit-learn uses a "consensus seeking" process for making decisions. The group tries to find a resolution that has no open objections among core contributors. At any point during the discussion, any core contributor can call for a vote, which will conclude one month from the call for the vote. Most votes have to be backed by a \[SLEP \<slep\>\](\#slep-\<slep\>). If no option can gather two thirds of the votes cast, the decision is escalated to the TC, which in turn will use consensus seeking with the fallback option of a simple majority vote if no consensus can be found within a month. This is what we hereafter may refer to as "**the decision making process**".

Decisions (in addition to adding core contributors and TC membership as above) are made according to the following rules:

  - **Minor Documentation changes**, such as typo fixes, or addition / correction of a sentence, but no change of the `scikit-learn.org` landing page or the “about” page: Requires +1 by a maintainer, no -1 by a maintainer (lazy consensus), happens on the issue or pull request page. Maintainers are expected to give “reasonable time” to others to give their opinion on the pull request if they're not confident others would agree.
  - **Code changes and major documentation changes** require +1 by two maintainers, no -1 by a maintainer (lazy consensus), happens on the issue of pull-request page.
  - **Changes to the API principles and changes to dependencies or supported versions** happen via \[slep\](\#slep) and follows the decision-making process outlined above.
  - **Changes to the governance model** follow the process outlined in [SLEP020](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep020/proposal.html).

If a veto -1 vote is cast on a lazy consensus, the proposer can appeal to the community and maintainers and the change can be approved or rejected using the decision making procedure outlined above.

### Governance Model Changes

Governance model changes occur through an enhancement proposal or a GitHub Pull Request. An enhancement proposal will go through "**the decision-making process**" described in the previous section. Alternatively, an author may propose a change directly to the governance model with a GitHub Pull Request. Logistically, an author can open a Draft Pull Request for feedback and follow up with a new revised Pull Request for voting. Once that author is happy with the state of the Pull Request, they can call for a vote on the public mailing list. During the one-month voting period, the Pull Request can not change. A Pull Request Approval will count as a positive vote, and a "Request Changes" review will count as a negative vote. If two-thirds of the cast votes are positive, then the governance model change is accepted.

## Enhancement proposals (SLEPs)

For all votes, a proposal must have been made public and discussed before the vote. Such proposal must be a consolidated document, in the form of a "Scikit-Learn Enhancement Proposal" (SLEP), rather than a long discussion on an issue. A SLEP must be submitted as a pull-request to [enhancement proposals](https://scikit-learn-enhancement-proposals.readthedocs.io) using the [SLEP template](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep_template.html). [SLEP000](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep000/proposal.html) describes the process in more detail.

---

ml_map.README.md

---

The scikit-learn machine learning cheat sheet was originally created by Andreas Mueller: <https://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html>

The current version of the chart is located at <span class="title-ref">doc/images/ml\_map.svg</span> in SVG+XML format, created using \[draw.io\](<https://draw.io/>). To edit the chart, open the file in draw.io, make changes, and save. This should update the chart in-place. Another option would be to re-export the chart as SVG and replace the existing file. The options used for exporting the chart are:

  - Zoom: 100%
  - Border width: 15
  - Size: Diagram
  - Transparent Background: False
  - Appearance: Light

Each node in the chart that contains an estimator should have a link, where the root directory is at <span class="title-ref">../../</span>. Note that after updating or re-exporting the SVG, the links may be prefixed with e.g. <span class="title-ref">https://app.diagrams.net/</span>. Remember to check and remove them, for instance by replacing all occurrences of <span class="title-ref">https://app.diagrams.net/../../</span> with <span class="title-ref">../../</span>.

---

inspection.md

---

# Inspection

Predictive performance is often the main goal of developing machine learning models. Yet summarizing performance with an evaluation metric is often insufficient: it assumes that the evaluation metric and test dataset perfectly reflect the target domain, which is rarely true. In certain domains, a model needs a certain level of interpretability before it can be deployed. A model that is exhibiting performance issues needs to be debugged for one to understand the model's underlying issue. The `sklearn.inspection` module provides tools to help understand the predictions from a model and what affects them. This can be used to evaluate assumptions and biases of a model, design a better model, or to diagnose issues with model performance.

**Examples**

  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_linear\_model\_coefficient\_interpretation.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_linear\_model\_coefficient\_interpretation.py)

<div class="toctree">

modules/partial\_dependence modules/permutation\_importance

</div>

---

install.md

---

# Installing scikit-learn

There are different ways to install scikit-learn:

  - \[Install the latest official release \<install\_official\_release\>\](\#install-the-latest-official-release-\<install\_official\_release\>). This is the best approach for most users. It will provide a stable version and pre-built packages are available for most platforms.
  - Install the version of scikit-learn provided by your \[operating system or Python distribution \<install\_by\_distribution\>\](\#operating-system-or-python-distribution-\<install\_by\_distribution\>). This is a quick option for those who have operating systems or Python distributions that distribute scikit-learn. It might not provide the latest release version.

<!-- end list -->

  - \* \[Building the package from source  
    \<install\_bleeding\_edge\>\](\#building-the-package-from-source

  - \--\<install\_bleeding\_edge\>). This is best for users who want the  
    latest-and-greatest features and aren't afraid of running brand-new code. This is also needed for users who wish to contribute to the project.

## Installing the latest release

<style>
  /* Show caption on large screens */
  @media screen and (min-width: 960px) {
    .install-instructions .sd-tab-set {
      --tab-caption-width: 20%;
    }

    .install-instructions .sd-tab-set.tabs-os::before {
      content: "Operating System";
    }

    .install-instructions .sd-tab-set.tabs-package-manager::before {
      content: "Package Manager";
    }
  }
</style>

<div class="div">

install-instructions

<div class="tab-set tabs-os">

<div class="tab-item" data-class-label="tab-4">

Windows

<div class="tab-set tabs-package-manager">

<div class="tab-item" data-class-label="tab-6" data-sync="package-manager-pip">

pip

Install the 64-bit version of Python 3, for instance from the [official website](https://www.python.org/downloads/windows/).

Now create a [virtual environment (venv)](https://docs.python.org/3/tutorial/venv.html) and install scikit-learn. Note that the virtual environment is optional but strongly recommended, in order to avoid potential conflicts with other packages.

<div class="prompt">

powershell

python -m venv sklearn-env sklearn-envScriptsactivate \# activate pip install -U scikit-learn

</div>

In order to check your installation, you can use:

<div class="prompt">

powershell

python -m pip show scikit-learn \# show scikit-learn version and location python -m pip freeze \# show all installed packages in the environment python -c "import sklearn; sklearn.show\_versions()"

</div>

</div>

<div class="tab-item" data-class-label="tab-6" data-sync="package-manager-conda">

conda

Install conda using the

</div>

</div>

</div>

</div>

</div>

[miniforge installers](https://github.com/conda-forge/miniforge#miniforge) (no administrator permission required). Then run:

<div class="prompt">

bash

conda create -n sklearn-env -c conda-forge scikit-learn conda activate sklearn-env

</div>

In order to check your installation, you can use:

<div class="prompt">

bash

conda list scikit-learn \# show scikit-learn version and location conda list \# show all installed packages in the environment python -c "import sklearn; sklearn.show\_versions()"

> 
> 
> <div class="tab-item" data-class-label="tab-4">
> 
> MacOS
> 
> <div class="tab-set tabs-package-manager">
> 
> <div class="tab-item" data-class-label="tab-6" data-sync="package-manager-pip">
> 
> pip
> 
> Install Python 3 using [homebrew](https://brew.sh/) (<span class="title-ref">brew install python</span>) or by manually installing the package from the [official website](https://www.python.org/downloads/macos/).
> 
> Now create a [virtual environment (venv)](https://docs.python.org/3/tutorial/venv.html) and install scikit-learn. Note that the virtual environment is optional but strongly recommended, in order to avoid potential conflicts with other packges.
> 
> <div class="prompt">
> 
> bash
> 
> python -m venv sklearn-env source sklearn-env/bin/activate \# activate pip install -U scikit-learn
> 
> </div>
> 
> In order to check your installation, you can use:
> 
> <div class="prompt">
> 
> bash
> 
> python -m pip show scikit-learn \# show scikit-learn version and location python -m pip freeze \# show all installed packages in the environment python -c "import sklearn; sklearn.show\_versions()"
> 
> </div>
> 
> </div>
> 
> <div class="tab-item" data-class-label="tab-6" data-sync="package-manager-conda">
> 
> conda
> 
> Install conda using the
> 
> </div>
> 
> </div>
> 
> </div>

</div>

[miniforge installers](https://github.com/conda-forge/miniforge#miniforge) (no administrator permission required). Then run:

<div class="prompt">

bash

conda create -n sklearn-env -c conda-forge scikit-learn conda activate sklearn-env

</div>

In order to check your installation, you can use:

<div class="prompt">

bash

conda list scikit-learn \# show scikit-learn version and location conda list \# show all installed packages in the environment python -c "import sklearn; sklearn.show\_versions()"

> 
> 
> <div class="tab-item" data-class-label="tab-4">
> 
> Linux
> 
> <div class="tab-set tabs-package-manager">
> 
> <div class="tab-item" data-class-label="tab-6" data-sync="package-manager-pip">
> 
> pip
> 
> Python 3 is usually installed by default on most Linux distributions. To check if you have it installed, try:
> 
> <div class="prompt">
> 
> bash
> 
> python3 --version pip3 --version
> 
> </div>
> 
> If you don't have Python 3 installed, please install <span class="title-ref">python3</span> and <span class="title-ref">python3-pip</span> from your distribution's package manager.
> 
> Now create a [virtual environment (venv)](https://docs.python.org/3/tutorial/venv.html) and install scikit-learn. Note that the virtual environment is optional but strongly recommended, in order to avoid potential conflicts with other packages.
> 
> <div class="prompt">
> 
> bash
> 
> python3 -m venv sklearn-env source sklearn-env/bin/activate \# activate pip3 install -U scikit-learn
> 
> </div>
> 
> In order to check your installation, you can use:
> 
> <div class="prompt">
> 
> bash
> 
> python3 -m pip show scikit-learn \# show scikit-learn version and location python3 -m pip freeze \# show all installed packages in the environment python3 -c "import sklearn; sklearn.show\_versions()"
> 
> </div>
> 
> </div>
> 
> <div class="tab-item" data-class-label="tab-6" data-sync="package-manager-conda">
> 
> conda
> 
> Install conda using the
> 
> </div>
> 
> </div>
> 
> </div>

</div>

[miniforge installers](https://github.com/conda-forge/miniforge#miniforge) (no administrator permission required). Then run:

<div class="prompt">

bash

conda create -n sklearn-env -c conda-forge scikit-learn conda activate sklearn-env

</div>

In order to check your installation, you can use:

<div class="prompt">

bash

conda list scikit-learn \# show scikit-learn version and location conda list \# show all installed packages in the environment python -c "import sklearn; sklearn.show\_versions()"

</div>

Using an isolated environment such as pip venv or conda makes it possible to install a specific version of scikit-learn with pip or conda and its dependencies independently of any previously installed Python packages. In particular under Linux it is discouraged to install pip packages alongside the packages managed by the package manager of the distribution (apt, dnf, pacman...).

Note that you should always remember to activate the environment of your choice prior to running any Python command whenever you start a new terminal session.

If you have not installed NumPy or SciPy yet, you can also install these using conda or pip. When using pip, please ensure that *binary wheels* are used, and NumPy and SciPy are not recompiled from source, which can happen when using particular configurations of operating system and hardware (such as Linux on a Raspberry Pi).

Scikit-learn plotting capabilities (i.e., functions starting with <span class="title-ref">plot\_</span> and classes ending with <span class="title-ref">Display</span>) require Matplotlib. The examples require Matplotlib and some examples require scikit-image, pandas, or seaborn. The minimum version of scikit-learn dependencies are listed below along with its purpose.

\<\!-- Failed to include min\_dependency\_table.rst --\>

\> **Warning** \> Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4. Scikit-learn 0.21 supported Python 3.5-3.7. Scikit-learn 0.22 supported Python 3.5-3.8. Scikit-learn 0.23-0.24 required Python 3.6 or newer. Scikit-learn 1.0 supported Python 3.7-3.10. Scikit-learn 1.1, 1.2 and 1.3 support Python 3.8-3.12 Scikit-learn 1.4 requires Python 3.9 or newer.

## Third party distributions of scikit-learn

Some third-party distributions provide versions of scikit-learn integrated with their package-management systems.

These can make installation and upgrading much easier for users since the integration includes the ability to automatically install dependencies (numpy, scipy) that scikit-learn requires.

The following is an incomplete list of OS and python distributions that provide their own version of scikit-learn.

### Alpine Linux

Alpine Linux's package is provided through the [official repositories](https://pkgs.alpinelinux.org/packages?name=py3-scikit-learn) as `py3-scikit-learn` for Python. It can be installed by typing the following command:

<div class="prompt">

bash

sudo apk add py3-scikit-learn

</div>

### Arch Linux

Arch Linux's package is provided through the [official repositories](https://www.archlinux.org/packages/?q=scikit-learn) as `python-scikit-learn` for Python. It can be installed by typing the following command:

<div class="prompt">

bash

sudo pacman -S python-scikit-learn

</div>

### Debian/Ubuntu

The Debian/Ubuntu package is split in three different packages called `python3-sklearn` (python modules), `python3-sklearn-lib` (low-level implementations and bindings), `python-sklearn-doc` (documentation). Note that scikit-learn requires Python 3, hence the need to use the <span class="title-ref">python3-</span> suffixed package names. Packages can be installed using `apt-get`:

<div class="prompt">

bash

sudo apt-get install python3-sklearn python3-sklearn-lib python-sklearn-doc

</div>

### Fedora

The Fedora package is called `python3-scikit-learn` for the python 3 version, the only one available in Fedora. It can be installed using `dnf`:

<div class="prompt">

bash

sudo dnf install python3-scikit-learn

</div>

### NetBSD

scikit-learn is available via [pkgsrc-wip](http://pkgsrc-wip.sourceforge.net/): <https://pkgsrc.se/math/py-scikit-learn>

### MacPorts for Mac OSX

The MacPorts package is named `py<XY>-scikits-learn`, where `XY` denotes the Python version. It can be installed by typing the following command:

<div class="prompt">

bash

sudo port install py39-scikit-learn

</div>

### Anaconda and Enthought Deployment Manager for all supported platforms

[Anaconda](https://www.anaconda.com/download) and [Enthought Deployment Manager](https://assets.enthought.com/downloads/) both ship with scikit-learn in addition to a large set of scientific python library for Windows, Mac OSX and Linux.

Anaconda offers scikit-learn as part of its free distribution.

### Intel Extension for Scikit-learn

Intel maintains an optimized x86\_64 package, available in PyPI (via <span class="title-ref">pip</span>), and in the <span class="title-ref">main</span>, <span class="title-ref">conda-forge</span> and <span class="title-ref">intel</span> conda channels:

<div class="prompt">

bash

conda install scikit-learn-intelex

</div>

This package has an Intel optimized version of many estimators. Whenever an alternative implementation doesn't exist, scikit-learn implementation is used as a fallback. Those optimized solvers come from the oneDAL C++ library and are optimized for the x86\_64 architecture, and are optimized for multi-core Intel CPUs.

Note that those solvers are not enabled by default, please refer to the [scikit-learn-intelex](https://intel.github.io/scikit-learn-intelex/latest/what-is-patching.html) documentation for more details on usage scenarios. Direct export example:

<div class="prompt">

python \>\>\>

from sklearnex.neighbors import NearestNeighbors

</div>

Compatibility with the standard scikit-learn solvers is checked by running the full scikit-learn test suite via automated continuous integration as reported on <https://github.com/intel/scikit-learn-intelex>. If you observe any issue with <span class="title-ref">scikit-learn-intelex</span>, please report the issue on their [issue tracker](https://github.com/intel/scikit-learn-intelex/issues).

### WinPython for Windows

The [WinPython](https://winpython.github.io/) project distributes scikit-learn as an additional plugin.

## Troubleshooting

If you encounter unexpected failures when installing scikit-learn, you may submit an issue to the [issue tracker](https://github.com/scikit-learn/scikit-learn/issues). Before that, please also make sure to check the following common issues.

### Error caused by file path length limit on Windows

It can happen that pip fails to install packages when reaching the default path size limit of Windows if Python is installed in a nested location such as the <span class="title-ref">AppData</span> folder structure under the user home directory, for instance:

    C:\Users\username>C:\Users\username\AppData\Local\Microsoft\WindowsApps\python.exe -m pip install scikit-learn
    Collecting scikit-learn
    ...
    Installing collected packages: scikit-learn
    ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\Users\\username\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\sklearn\\datasets\\tests\\data\\openml\\292\\api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz'

In this case it is possible to lift that limit in the Windows registry by using the `regedit` tool:

1.  Type "regedit" in the Windows start menu to launch `regedit`.

2.  Go to the `Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem` key.

3.  Edit the value of the `LongPathsEnabled` property of that key and set it to 1.

4.  Reinstall scikit-learn (ignoring the previous broken installation):
    
    <div class="prompt">
    
    powershell
    
    pip install --exists-action=i scikit-learn
    
    </div>

---

install_instructions_conda.md

---

Install conda using the [miniforge installers](https://github.com/conda-forge/miniforge#miniforge) (no administrator permission required). Then run:

<div class="prompt">

bash

conda create -n sklearn-env -c conda-forge scikit-learn conda activate sklearn-env

</div>

In order to check your installation, you can use:

<div class="prompt">

bash

conda list scikit-learn \# show scikit-learn version and location conda list \# show all installed packages in the environment python -c "import sklearn; sklearn.show\_versions()"

</div>

---

machine_learning_map.md

---

- html\_theme.sidebar\_secondary.remove

# Choosing the right estimator

Often the hardest part of solving a machine learning problem can be finding the right estimator for the job. Different estimators are better suited for different types of data and different problems.

The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data. Click on any estimator in the chart below to see its documentation. The 😭 emoji is to be read as "if this estimator does not achieve the desired outcome, then follow the arrow and try the next one". Use scroll wheel to zoom in and out, and click and drag to pan around. You can also download the chart: `ml_map.svg <images/ml_map.svg>`.

<style>
  #sk-ml-map {
    height: 80vh;
    margin: 1.5rem 0;
  }

  #sk-ml-map svg {
    height: 100%;
    width: 100%;
    border: 2px solid var(--pst-color-border);
    border-radius: 0.5rem;
  }

  html[data-theme="dark"] #sk-ml-map svg {
    filter: invert(90%) hue-rotate(180deg);
  }
</style>

<script src="_static/scripts/vendor/svg-pan-zoom.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    const beforePan = function (oldPan, newPan) {
      const gutterWidth = 100, gutterHeight = 100;
      const sizes = this.getSizes();

      // Compute pan limits
      const leftLimit = -((sizes.viewBox.x + sizes.viewBox.width) * sizes.realZoom) + gutterWidth;
      const rightLimit = sizes.width - gutterWidth - (sizes.viewBox.x * sizes.realZoom);
      const topLimit = -((sizes.viewBox.y + sizes.viewBox.height) * sizes.realZoom) + gutterHeight;
      const bottomLimit = sizes.height - gutterHeight - (sizes.viewBox.y * sizes.realZoom);

      return {
        x: Math.max(leftLimit, Math.min(rightLimit, newPan.x)),
        y: Math.max(topLimit, Math.min(bottomLimit, newPan.y))
      };
    };

    // Limit the pan
    svgPanZoom("#sk-ml-map svg", {
      zoomEnabled: true,
      controlIconsEnabled: true,
      fit: 1,
      center: 1,
      beforePan: beforePan,
    });
  });
</script>

<div id="sk-ml-map">

</div>

---

maintainers_emeritus.md

---

- Mathieu Blondel
  - Joris Van den Bossche
  - Matthieu Brucher
  - Lars Buitinck
  - David Cournapeau
  - Noel Dawe
  - Vincent Dubourg
  - Edouard Duchesnay
  - Alexander Fabisch
  - Virgile Fritsch
  - Satrajit Ghosh
  - Angel Soler Gollonet
  - Chris Gorgolewski
  - Jaques Grobler
  - Yaroslav Halchenko
  - Brian Holt
  - Arnaud Joly
  - Thouis (Ray) Jones
  - Kyle Kastner
  - manoj kumar
  - Robert Layton
  - Wei Li
  - Paolo Losi
  - Gilles Louppe
  - Jan Hendrik Metzen
  - Vincent Michel
  - Jarrod Millman
  - Vlad Niculae
  - Alexandre Passos
  - Fabian Pedregosa
  - Peter Prettenhofer
  - Hanmin Qin
  - (Venkat) Raghav, Rajagopalan
  - Jacob Schreiber
  - 杜世橋 Du Shiqiao
  - Bertrand Thirion
  - Tom Dupré la Tour
  - Jake Vanderplas
  - Nelle Varoquaux
  - David Warde-Farley
  - Ron Weiss
  - Roman Yurchak

---

metadata_routing.md

---

<div class="currentmodule">

sklearn

</div>

# Metadata Routing

<div class="note">

<div class="title">

Note

</div>

The Metadata Routing API is experimental, and is not yet implemented for all estimators. Please refer to the \[list of supported and unsupported models \<metadata\_routing\_models\>\](\#list-of-supported-and-unsupported

</div>

  - \--models-\<metadata\_routing\_models\>) for more information. It may change without  
    the usual deprecation cycle. By default this feature is not enabled. You can enable it by setting the `enable_metadata_routing` flag to `True`:
    
        >>> import sklearn
        >>> sklearn.set_config(enable_metadata_routing=True)
    
    Note that the methods and requirements introduced in this document are only relevant if you want to pass `metadata` (e.g. `sample_weight`) to a method. If you're only passing `X` and `y` and no other parameter / metadata to methods such as `fit`, `transform`, etc., then you don't need to set anything.

This guide demonstrates how `metadata` can be routed and passed between objects in scikit-learn. If you are developing a scikit-learn compatible estimator or meta-estimator, you can check our related developer guide: \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_metadata\_routing.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_metadata\_routing.py).

Metadata is data that an estimator, scorer, or CV splitter takes into account if the user explicitly passes it as a parameter. For instance, <span class="title-ref">\~cluster.KMeans</span> accepts <span class="title-ref">sample\_weight</span> in its <span class="title-ref">fit()</span> method and considers it to calculate its centroids. <span class="title-ref">classes</span> are consumed by some classifiers and <span class="title-ref">groups</span> are used in some splitters, but any data that is passed into an object's methods apart from X and y can be considered as metadata. Prior to scikit-learn version 1.3, there was no single API for passing metadata like that if these objects were used in conjunction with other objects, e.g. a scorer accepting <span class="title-ref">sample\_weight</span> inside a <span class="title-ref">\~model\_selection.GridSearchCV</span>.

With the Metadata Routing API, we can transfer metadata to estimators, scorers, and CV splitters using `meta-estimators` (such as <span class="title-ref">\~pipeline.Pipeline</span> or <span class="title-ref">\~model\_selection.GridSearchCV</span>) or functions such as <span class="title-ref">\~model\_selection.cross\_validate</span> which route data to other objects. In order to pass metadata to a method like `fit` or `score`, the object consuming the metadata, must *request* it. This is done via <span class="title-ref">set\_{method}\_request()</span> methods, where <span class="title-ref">{method}</span> is substituted by the name of the method that requests the metadata. For instance, estimators that use the metadata in their <span class="title-ref">fit()</span> method would use <span class="title-ref">set\_fit\_request()</span>, and scorers would use <span class="title-ref">set\_score\_request()</span>. These methods allow us to specify which metadata to request, for instance <span class="title-ref">set\_fit\_request(sample\_weight=True)</span>.

For grouped splitters such as <span class="title-ref">\~model\_selection.GroupKFold</span>, a `groups` parameter is requested by default. This is best demonstrated by the following examples.

## Usage Examples

Here we present a few examples to show some common use-cases. Our goal is to pass <span class="title-ref">sample\_weight</span> and <span class="title-ref">groups</span> through <span class="title-ref">\~model\_selection.cross\_validate</span>, which routes the metadata to <span class="title-ref">\~linear\_model.LogisticRegressionCV</span> and to a custom scorer made with <span class="title-ref">\~metrics.make\_scorer</span>, both of which *can* use the metadata in their methods. In these examples we want to individually set whether to use the metadata within the different `consumers <consumer>`.

The examples in this section require the following imports and data:

    >>> import numpy as np
    >>> from sklearn.metrics import make_scorer, accuracy_score
    >>> from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
    >>> from sklearn.model_selection import cross_validate, GridSearchCV, GroupKFold
    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.pipeline import make_pipeline
    >>> n_samples, n_features = 100, 4
    >>> rng = np.random.RandomState(42)
    >>> X = rng.rand(n_samples, n_features)
    >>> y = rng.randint(0, 2, size=n_samples)
    >>> my_groups = rng.randint(0, 10, size=n_samples)
    >>> my_weights = rng.rand(n_samples)
    >>> my_other_weights = rng.rand(n_samples)

### Weighted scoring and fitting

The splitter used internally in <span class="title-ref">\~linear\_model.LogisticRegressionCV</span>, <span class="title-ref">\~model\_selection.GroupKFold</span>, requests `groups` by default. However, we need to explicitly request <span class="title-ref">sample\_weight</span> for it and for our custom scorer by specifying <span class="title-ref">sample\_weight=True</span> in <span class="title-ref">\~linear\_model.LogisticRegressionCV\`s \`set\_fit\_request()</span> method and in <span class="title-ref">\~metrics.make\_scorer\`s \`set\_score\_request()</span> method. Both `consumers <consumer>` know how to use `sample_weight` in their <span class="title-ref">fit()</span> or <span class="title-ref">score()</span> methods. We can then pass the metadata in <span class="title-ref">\~model\_selection.cross\_validate</span> which will route it to any active consumers:

    >>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)
    >>> lr = LogisticRegressionCV(
    ...     cv=GroupKFold(),
    ...     scoring=weighted_acc
    ... ).set_fit_request(sample_weight=True)
    >>> cv_results = cross_validate(
    ...     lr,
    ...     X,
    ...     y,
    ...     params={"sample_weight": my_weights, "groups": my_groups},
    ...     cv=GroupKFold(),
    ...     scoring=weighted_acc,
    ... )

Note that in this example, <span class="title-ref">\~model\_selection.cross\_validate</span> routes `my_weights` to both the scorer and <span class="title-ref">\~linear\_model.LogisticRegressionCV</span>.

If we would pass <span class="title-ref">sample\_weight</span> in the params of <span class="title-ref">\~model\_selection.cross\_validate</span>, but not set any object to request it, <span class="title-ref">UnsetMetadataPassedError</span> would be raised, hinting to us that we need to explicitly set where to route it. The same applies if `params={"sample_weights": my_weights, ...}` were passed (note the typo, i.e. `weights` instead of `weight`), since `sample_weights` was not requested by any of its underlying objects.

### Weighted scoring and unweighted fitting

When passing metadata such as `sample_weight` into a `router` (`meta-estimators` or routing function), all `sample_weight` `consumers
<consumer>` require weights to be either explicitly requested or explicitly not requested (i.e. `True` or `False`). Thus, to perform an unweighted fit, we need to configure <span class="title-ref">\~linear\_model.LogisticRegressionCV</span> to not request sample weights, so that <span class="title-ref">\~model\_selection.cross\_validate</span> does not pass the weights along:

    >>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)
    >>> lr = LogisticRegressionCV(
    ...     cv=GroupKFold(), scoring=weighted_acc,
    ... ).set_fit_request(sample_weight=False)
    >>> cv_results = cross_validate(
    ...     lr,
    ...     X,
    ...     y,
    ...     cv=GroupKFold(),
    ...     params={"sample_weight": my_weights, "groups": my_groups},
    ...     scoring=weighted_acc,
    ... )

If <span class="title-ref">linear\_model.LogisticRegressionCV.set\_fit\_request</span> had not been called, <span class="title-ref">\~model\_selection.cross\_validate</span> would raise an error because `sample_weight` is passed but <span class="title-ref">\~linear\_model.LogisticRegressionCV</span> would not be explicitly configured to recognize the weights.

### Unweighted feature selection

Routing metadata is only possible if the object's method knows how to use the metadata, which in most cases means they have it as an explicit parameter. Only then we can set request values for metadata using <span class="title-ref">set\_fit\_request(sample\_weight=True)</span>, for instance. This makes the object a `consumer <consumer>`.

Unlike <span class="title-ref">\~linear\_model.LogisticRegressionCV</span>, <span class="title-ref">\~feature\_selection.SelectKBest</span> can't consume weights and therefore no request value for `sample_weight` on its instance is set and `sample_weight` is not routed to it:

    >>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)
    >>> lr = LogisticRegressionCV(
    ...     cv=GroupKFold(), scoring=weighted_acc,
    ... ).set_fit_request(sample_weight=True)
    >>> sel = SelectKBest(k=2)
    >>> pipe = make_pipeline(sel, lr)
    >>> cv_results = cross_validate(
    ...     pipe,
    ...     X,
    ...     y,
    ...     cv=GroupKFold(),
    ...     params={"sample_weight": my_weights, "groups": my_groups},
    ...     scoring=weighted_acc,
    ... )

### Different scoring and fitting weights

Despite <span class="title-ref">\~metrics.make\_scorer</span> and <span class="title-ref">\~linear\_model.LogisticRegressionCV</span> both expecting the key `sample_weight`, we can use aliases to pass different weights to different consumers. In this example, we pass `scoring_weight` to the scorer, and `fitting_weight` to \`\~linear\_model.LogisticRegressionCV\`:

    >>> weighted_acc = make_scorer(accuracy_score).set_score_request(
    ...    sample_weight="scoring_weight"
    ... )
    >>> lr = LogisticRegressionCV(
    ...     cv=GroupKFold(), scoring=weighted_acc,
    ... ).set_fit_request(sample_weight="fitting_weight")
    >>> cv_results = cross_validate(
    ...     lr,
    ...     X,
    ...     y,
    ...     cv=GroupKFold(),
    ...     params={
    ...         "scoring_weight": my_weights,
    ...         "fitting_weight": my_other_weights,
    ...         "groups": my_groups,
    ...     },
    ...     scoring=weighted_acc,
    ... )

## API Interface

A `consumer` is an object (estimator, meta-estimator, scorer, splitter) which accepts and uses some `metadata` in at least one of its methods (for instance `fit`, `predict`, `inverse_transform`, `transform`, `score`, `split`). Meta-estimators which only forward the metadata to other objects (child estimators, scorers, or splitters) and don't use the metadata themselves are not consumers. (Meta-)Estimators which route metadata to other objects are `routers <router>`. A(n) (meta-)estimator can be a `consumer` and a `router` at the same time. (Meta-)Estimators and splitters expose a <span class="title-ref">set\_{method}\_request</span> method for each method which accepts at least one metadata. For instance, if an estimator supports `sample_weight` in `fit` and `score`, it exposes `estimator.set_fit_request(sample_weight=value)` and `estimator.set_score_request(sample_weight=value)`. Here `value` can be:

  - `True`: method requests a `sample_weight`. This means if the metadata is provided, it will be used, otherwise no error is raised.
  - `False`: method does not request a `sample_weight`.
  - `None`: router will raise an error if `sample_weight` is passed. This is in almost all cases the default value when an object is instantiated and ensures the user sets the metadata requests explicitly when a metadata is passed. The only exception are `Group*Fold` splitters.
  - `"param_name"`: alias for `sample_weight` if we want to pass different weights to different consumers. If aliasing is used the meta-estimator should not forward `"param_name"` to the consumer, but `sample_weight` instead, because the consumer will expect a param called `sample_weight`. This means the mapping between the metadata required by the object, e.g. `sample_weight` and the variable name provided by the user, e.g. `my_weights` is done at the router level, and not by the consuming object itself.

Metadata are requested in the same way for scorers using `set_score_request`.

If a metadata, e.g. `sample_weight`, is passed by the user, the metadata request for all objects which potentially can consume `sample_weight` should be set by the user, otherwise an error is raised by the router object. For example, the following code raises an error, since it hasn't been explicitly specified whether `sample_weight` should be passed to the estimator's scorer or not:

    >>> param_grid = {"C": [0.1, 1]}
    >>> lr = LogisticRegression().set_fit_request(sample_weight=True)
    >>> try:
    ...     GridSearchCV(
    ...         estimator=lr, param_grid=param_grid
    ...     ).fit(X, y, sample_weight=my_weights)
    ... except ValueError as e:
    ...     print(e)
    [sample_weight] are passed but are not explicitly set as requested or not
    requested for LogisticRegression.score, which is used within GridSearchCV.fit.
    Call `LogisticRegression.set_score_request({metadata}=True/False)` for each metadata
    you want to request/ignore.

The issue can be fixed by explicitly setting the request value:

    >>> lr = LogisticRegression().set_fit_request(
    ...     sample_weight=True
    ... ).set_score_request(sample_weight=False)

At the end of the **Usage Examples** section, we disable the configuration flag for metadata routing:

    >>> sklearn.set_config(enable_metadata_routing=False)

## Metadata Routing Support Status

All consumers (i.e. simple estimators which only consume metadata and don't route them) support metadata routing, meaning they can be used inside meta-estimators which support metadata routing. However, development of support for metadata routing for meta-estimators is in progress, and here is a list of meta-estimators and tools which support and don't yet support metadata routing.

Meta-estimators and functions supporting metadata routing:

  - <span class="title-ref">sklearn.calibration.CalibratedClassifierCV</span>
  - <span class="title-ref">sklearn.compose.ColumnTransformer</span>
  - <span class="title-ref">sklearn.compose.TransformedTargetRegressor</span>
  - <span class="title-ref">sklearn.covariance.GraphicalLassoCV</span>
  - <span class="title-ref">sklearn.ensemble.StackingClassifier</span>
  - <span class="title-ref">sklearn.ensemble.StackingRegressor</span>
  - <span class="title-ref">sklearn.ensemble.VotingClassifier</span>
  - <span class="title-ref">sklearn.ensemble.VotingRegressor</span>
  - <span class="title-ref">sklearn.ensemble.BaggingClassifier</span>
  - <span class="title-ref">sklearn.ensemble.BaggingRegressor</span>
  - <span class="title-ref">sklearn.feature\_selection.RFE</span>
  - <span class="title-ref">sklearn.feature\_selection.RFECV</span>
  - <span class="title-ref">sklearn.feature\_selection.SelectFromModel</span>
  - <span class="title-ref">sklearn.feature\_selection.SequentialFeatureSelector</span>
  - <span class="title-ref">sklearn.impute.IterativeImputer</span>
  - <span class="title-ref">sklearn.linear\_model.ElasticNetCV</span>
  - <span class="title-ref">sklearn.linear\_model.LarsCV</span>
  - <span class="title-ref">sklearn.linear\_model.LassoCV</span>
  - <span class="title-ref">sklearn.linear\_model.LassoLarsCV</span>
  - <span class="title-ref">sklearn.linear\_model.LogisticRegressionCV</span>
  - <span class="title-ref">sklearn.linear\_model.MultiTaskElasticNetCV</span>
  - <span class="title-ref">sklearn.linear\_model.MultiTaskLassoCV</span>
  - <span class="title-ref">sklearn.linear\_model.OrthogonalMatchingPursuitCV</span>
  - <span class="title-ref">sklearn.linear\_model.RANSACRegressor</span>
  - <span class="title-ref">sklearn.linear\_model.RidgeClassifierCV</span>
  - <span class="title-ref">sklearn.linear\_model.RidgeCV</span>
  - <span class="title-ref">sklearn.model\_selection.GridSearchCV</span>
  - <span class="title-ref">sklearn.model\_selection.HalvingGridSearchCV</span>
  - <span class="title-ref">sklearn.model\_selection.HalvingRandomSearchCV</span>
  - <span class="title-ref">sklearn.model\_selection.RandomizedSearchCV</span>
  - <span class="title-ref">sklearn.model\_selection.permutation\_test\_score</span>
  - <span class="title-ref">sklearn.model\_selection.cross\_validate</span>
  - <span class="title-ref">sklearn.model\_selection.cross\_val\_score</span>
  - <span class="title-ref">sklearn.model\_selection.cross\_val\_predict</span>
  - <span class="title-ref">sklearn.model\_selection.learning\_curve</span>
  - <span class="title-ref">sklearn.model\_selection.validation\_curve</span>
  - <span class="title-ref">sklearn.multiclass.OneVsOneClassifier</span>
  - <span class="title-ref">sklearn.multiclass.OneVsRestClassifier</span>
  - <span class="title-ref">sklearn.multiclass.OutputCodeClassifier</span>
  - <span class="title-ref">sklearn.multioutput.ClassifierChain</span>
  - <span class="title-ref">sklearn.multioutput.MultiOutputClassifier</span>
  - <span class="title-ref">sklearn.multioutput.MultiOutputRegressor</span>
  - <span class="title-ref">sklearn.multioutput.RegressorChain</span>
  - <span class="title-ref">sklearn.pipeline.FeatureUnion</span>
  - <span class="title-ref">sklearn.pipeline.Pipeline</span>
  - <span class="title-ref">sklearn.semi\_supervised.SelfTrainingClassifier</span>

Meta-estimators and tools not supporting metadata routing yet:

  - <span class="title-ref">sklearn.ensemble.AdaBoostClassifier</span>
  - <span class="title-ref">sklearn.ensemble.AdaBoostRegressor</span>

---

model_persistence.md

---

# Model persistence

<table>
<caption>Summary of model persistence methods</caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Persistence method</th>
<th>Pros</th>
<th>Risks / Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[ONNX &lt;onnx_persistence&gt;](#onnx-&lt;onnx_persistence&gt;)</td>
<td><ul>
<li>Serve models without a Python environment</li>
<li>Serving and training environments independent of one another</li>
<li>Most secure option</li>
</ul></td>
<td><ul>
<li>Not all scikit-learn models are supported</li>
<li>Custom estimators require more work to support</li>
<li>Original Python object is lost and cannot be reconstructed</li>
</ul></td>
</tr>
<tr class="even">
<td>[skops_persistence](#skops_persistence)</td>
<td><ul>
<li>More secure than <span class="title-ref">pickle</span> based formats</li>
<li>Contents can be partly validated without loading</li>
</ul></td>
<td><ul>
<li>Not as fast as <span class="title-ref">pickle</span> based formats</li>
<li>Supports less types than <span class="title-ref">pickle</span> based formats</li>
<li>Requires the same environment as the training environment</li>
</ul></td>
</tr>
<tr class="odd">
<td><code class="interpreted-text" role="mod">pickle</code></td>
<td><ul>
<li>Native to Python</li>
<li>Can serialize most Python objects</li>
<li>Efficient memory usage with <span class="title-ref">protocol=5</span></li>
</ul></td>
<td><ul>
<li>Loading can execute arbitrary code</li>
<li>Requires the same environment as the training environment</li>
</ul></td>
</tr>
<tr class="even">
<td><code class="interpreted-text" role="mod">joblib</code></td>
<td><ul>
<li>Efficient memory usage</li>
<li>Supports memory mapping</li>
<li>Easy shortcuts for compression and decompression</li>
</ul></td>
<td><ul>
<li>Pickle based format</li>
<li>Loading can execute arbitrary code</li>
<li>Requires the same environment as the training environment</li>
</ul></td>
</tr>
<tr class="odd">
<td><a href="https://github.com/cloudpipe/cloudpickle">cloudpickle</a></td>
<td><ul>
<li>Can serialize non-packaged, custom Python code</li>
<li>Comparable loading efficiency as <code class="interpreted-text" role="mod">pickle</code> with <span class="title-ref">protocol=5</span></li>
</ul></td>
<td><ul>
<li>Pickle based format</li>
<li>Loading can execute arbitrary code</li>
<li>No forward compatibility guarantees</li>
<li>Requires the same environment as the training environment</li>
</ul></td>
</tr>
</tbody>
</table>

After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. Based on your use-case, there are a few different ways to persist a scikit-learn model, and here we help you decide which one suits you best. In order to make a decision, you need to answer the following questions:

1.  Do you need the Python object after persistence, or do you only need to persist in order to serve the model and get predictions out of it?

If you only need to serve the model and no further investigation on the Python object itself is required, then \[ONNX \<onnx\_persistence\>\](\#onnx-\<onnx\_persistence\>) might be the best fit for you. Note that not all models are supported by ONNX.

In case ONNX is not suitable for your use-case, the next question is:

2.  Do you absolutely trust the source of the model, or are there any security concerns regarding where the persisted model comes from?

If you have security concerns, then you should consider using \[skops.io \<skops\_persistence\>\](\#skops.io \<skops\_persistence\>) which gives you back the Python object, but unlike <span class="title-ref">pickle</span> based persistence solutions, loading the persisted model doesn't automatically allow arbitrary code execution. Note that this requires manual investigation of the persisted file, which `skops.io` allows you to do.

The other solutions assume you absolutely trust the source of the file to be loaded, as they are all susceptible to arbitrary code execution upon loading the persisted file since they all use the pickle protocol under the hood.

3.  Do you care about the performance of loading the model, and sharing it between processes where a memory mapped object on disk is beneficial?

If yes, then you can consider using \[joblib \<pickle\_persistence\>\](\#joblib-\<pickle\_persistence\>). If this is not a major concern for you, then you can use the built-in `pickle` module.

4.  Did you try `pickle` or `joblib` and found that the model cannot be persisted? It can happen for instance when you have user defined functions in your model.

If yes, then you can use [cloudpickle](https://github.com/cloudpipe/cloudpickle) which can serialize certain objects which cannot be serialized by `pickle` or `joblib`.

## Workflow Overview

In a typical workflow, the first step is to train the model using scikit-learn and scikit-learn compatible libraries. Note that support for scikit-learn and third party estimators varies across the different persistence methods.

### Train and Persist the Model

Creating an appropriate model depends on your use-case. As an example, here we train a <span class="title-ref">sklearn.ensemble.HistGradientBoostingClassifier</span> on the iris dataset:

    >>> from sklearn import ensemble
    >>> from sklearn import datasets
    >>> clf = ensemble.HistGradientBoostingClassifier()
    >>> X, y = datasets.load_iris(return_X_y=True)
    >>> clf.fit(X, y)
    HistGradientBoostingClassifier()

Once the model is trained, you can persist it using your desired method, and then you can load the model in a separate environment and get predictions from it given input data. Here there are two major paths depending on how you persist and plan to serve the model:

  - \[ONNX \<onnx\_persistence\>\](\#onnx-\<onnx\_persistence\>): You need an <span class="title-ref">ONNX</span> runtime and an environment with appropriate dependencies installed to load the model and use the runtime to get predictions. This environment can be minimal and does not necessarily even require Python to be installed to load the model and compute predictions. Also note that <span class="title-ref">onnxruntime</span> typically requires much less RAM than Python to compute predictions from small models.
  - `skops.io`, `pickle`, `joblib`, [cloudpickle](https://github.com/cloudpipe/cloudpickle): You need a Python environment with the appropriate dependencies installed to load the model and get predictions from it. This environment should have the same **packages** and the same **versions** as the environment where the model was trained. Note that none of these methods support loading a model trained with a different version of scikit-learn, and possibly different versions of other dependencies such as <span class="title-ref">numpy</span> and <span class="title-ref">scipy</span>. Another concern would be running the persisted model on a different hardware, and in most cases you should be able to load your persisted model on a different hardware.

## ONNX

<span class="title-ref">ONNX</span>, or [Open Neural Network Exchange](https://onnx.ai/) format is best suitable in use-cases where one needs to persist the model and then use the persisted artifact to get predictions without the need to load the Python object itself. It is also useful in cases where the serving environment needs to be lean and minimal, since the <span class="title-ref">ONNX</span> runtime does not require <span class="title-ref">python</span>.

<span class="title-ref">ONNX</span> is a binary serialization of the model. It has been developed to improve the usability of the interoperable representation of data models. It aims to facilitate the conversion of the data models between different machine learning frameworks, and to improve their portability on different computing architectures. More details are available from the [ONNX tutorial](https://onnx.ai/get-started.html). To convert scikit-learn model to <span class="title-ref">ONNX</span> [sklearn-onnx](http://onnx.ai/sklearn-onnx/) has been developed. However, not all scikit-learn models are supported, and it is limited to the core scikit-learn and does not support most third party estimators. One can write a custom converter for third party or custom estimators, but the documentation to do that is sparse and it might be challenging to do so.

<div class="dropdown">

Using ONNX

To convert the model to <span class="title-ref">ONNX</span> format, you need to give the converter some information about the input as well, about which you can read more [here](http://onnx.ai/sklearn-onnx/index.html):

    from skl2onnx import to_onnx
    onx = to_onnx(clf, X[:1].astype(numpy.float32), target_opset=12)
    with open("filename.onnx", "wb") as f:
        f.write(onx.SerializeToString())

You can load the model in Python and use the <span class="title-ref">ONNX</span> runtime to get predictions:

    from onnxruntime import InferenceSession
    with open("filename.onnx", "rb") as f:
        onx = f.read()
    sess = InferenceSession(onx, providers=["CPUExecutionProvider"])
    pred_ort = sess.run(None, {"X": X_test.astype(numpy.float32)})[0]

</div>

## <span class="title-ref">skops.io</span>

`skops.io` avoids using `pickle` and only loads files which have types and references to functions which are trusted either by default or by the user. Therefore it provides a more secure format than `pickle`, `joblib`, and [cloudpickle](https://github.com/cloudpipe/cloudpickle).

<div class="dropdown">

Using skops

The API is very similar to `pickle`, and you can persist your models as explained in the [documentation](https://skops.readthedocs.io/en/stable/persistence.html) using <span class="title-ref">skops.io.dump</span> and \`skops.io.dumps\`:

    import skops.io as sio
    obj = sio.dump(clf, "filename.skops")

And you can load them back using <span class="title-ref">skops.io.load</span> and <span class="title-ref">skops.io.loads</span>. However, you need to specify the types which are trusted by you. You can get existing unknown types in a dumped object / file using <span class="title-ref">skops.io.get\_untrusted\_types</span>, and after checking its contents, pass it to the load function:

    unknown_types = sio.get_untrusted_types(file="filename.skops")
    # investigate the contents of unknown_types, and only load if you trust
    # everything you see.
    clf = sio.load("filename.skops", trusted=unknown_types)

Please report issues and feature requests related to this format on the [skops issue tracker](https://github.com/skops-dev/skops/issues).

</div>

## <span class="title-ref">pickle</span>, <span class="title-ref">joblib</span>, and <span class="title-ref">cloudpickle</span>

These three modules / packages, use the <span class="title-ref">pickle</span> protocol under the hood, but come with slight variations:

  - `pickle` is a module from the Python Standard Library. It can serialize and deserialize any Python object, including custom Python classes and objects.
  - `joblib` is more efficient than <span class="title-ref">pickle</span> when working with large machine learning models or large numpy arrays.
  - [cloudpickle](https://github.com/cloudpipe/cloudpickle) can serialize certain objects which cannot be serialized by `pickle` or `joblib`, such as user defined functions and lambda functions. This can happen for instance, when using a <span class="title-ref">\~sklearn.preprocessing.FunctionTransformer</span> and using a custom function to transform the data.

<div class="dropdown">

Using <span class="title-ref">pickle</span>, <span class="title-ref">joblib</span>, or <span class="title-ref">cloudpickle</span>

Depending on your use-case, you can choose one of these three methods to persist and load your scikit-learn model, and they all follow the same API:

    # Here you can replace pickle with joblib or cloudpickle
    from pickle import dump
    with open("filename.pkl", "wb") as f:
        dump(clf, f, protocol=5)

Using <span class="title-ref">protocol=5</span> is recommended to reduce memory usage and make it faster to store and load any large NumPy array stored as a fitted attribute in the model. You can alternatively pass <span class="title-ref">protocol=pickle.HIGHEST\_PROTOCOL</span> which is equivalent to <span class="title-ref">protocol=5</span> in Python 3.8 and later (at the time of writing).

And later when needed, you can load the same object from the persisted file:

    # Here you can replace pickle with joblib or cloudpickle
    from pickle import load
    with open("filename.pkl", "rb") as f:
        clf = load(f)

</div>

## Security & Maintainability Limitations

`pickle` (and `joblib` and `clouldpickle` by extension), has many documented security vulnerabilities by design and should only be used if the artifact, i.e. the pickle-file, is coming from a trusted and verified source. You should never load a pickle file from an untrusted source, similarly to how you should never execute code from an untrusted source.

Also note that arbitrary computations can be represented using the <span class="title-ref">ONNX</span> format, and it is therefore recommended to serve models using <span class="title-ref">ONNX</span> in a sandboxed environment to safeguard against computational and memory exploits.

Also note that there are no supported ways to load a model trained with a different version of scikit-learn. While using `skops.io`, `joblib`, `pickle`, or [cloudpickle](https://github.com/cloudpipe/cloudpickle), models saved using one version of scikit-learn might load in other versions, however, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results, or even crash your Python process.

In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:

  - The training data, e.g. a reference to an immutable snapshot
  - The Python source code used to generate the model
  - The versions of scikit-learn and its dependencies
  - The cross validation score obtained on the training data

This should make it possible to check that the cross-validation score is in the same range as before.

Aside for a few exceptions, persisted models should be portable across operating systems and hardware architectures assuming the same versions of dependencies and Python are used. If you encounter an estimator that is not portable, please open an issue on GitHub. Persisted models are often deployed in production using containers like Docker, in order to freeze the environment and dependencies.

If you want to know more about these issues, please refer to these talks:

  - [Adrin Jalali: Let's exploit pickle, and skops to the rescue\! | PyData Amsterdam 2023](https://www.youtube.com/watch?v=9w_H5OSTO9A).
  - [Alex Gaynor: Pickles are for Delis, not Software - PyCon 2014](https://pyvideo.org/video/2566/pickles-are-for-delis-not-software).

### Replicating the training environment in production

If the versions of the dependencies used may differ from training to production, it may result in unexpected behaviour and errors while using the trained model. To prevent such situations it is recommended to use the same dependencies and versions in both the training and production environment. These transitive dependencies can be pinned with the help of package management tools like <span class="title-ref">pip</span>, <span class="title-ref">mamba</span>, <span class="title-ref">conda</span>, <span class="title-ref">poetry</span>, <span class="title-ref">conda-lock</span>, <span class="title-ref">pixi</span>, etc.

It is not always possible to load an model trained with older versions of the scikit-learn library and its dependencies in an updated software environment. Instead, you might need to retrain the model with the new versions of the all the libraries. So when training a model, it is important to record the training recipe (e.g. a Python script) and training set information, and metadata about all the dependencies to be able to automatically reconstruct the same training environment for the updated software.

<div class="dropdown">

InconsistentVersionWarning

When an estimator is loaded with a scikit-learn version that is inconsistent with the version the estimator was pickled with, a <span class="title-ref">\~sklearn.exceptions.InconsistentVersionWarning</span> is raised. This warning can be caught to obtain the original version the estimator was pickled with:

    from sklearn.exceptions import InconsistentVersionWarning
    warnings.simplefilter("error", InconsistentVersionWarning)
    
    try:
        with open("model_from_prevision_version.pickle", "rb") as f:
            est = pickle.load(f)
    except InconsistentVersionWarning as w:
        print(w.original_sklearn_version)

</div>

### Serving the model artifact

The last step after training a scikit-learn model is serving the model. Once the trained model is successfully loaded, it can be served to manage different prediction requests. This can involve deploying the model as a web service using containerization, or other model deployment strategies, according to the specifications.

## Summarizing the key points

Based on the different approaches for model persistence, the key points for each approach can be summarized as follows:

  - \`ONNX\`: It provides a uniform format for persisting any machine learning or deep learning model (other than scikit-learn) and is useful for model inference (predictions). It can however, result in compatibility issues with different frameworks.
  - `skops.io`: Trained scikit-learn models can be easily shared and put into production using `skops.io`. It is more secure compared to alternate approaches based on `pickle` because it does not load arbitrary code unless explicitly asked for by the user. Such code needs to be packaged and importable in the target Python environment.
  - `joblib`: Efficient memory mapping techniques make it faster when using the same persisted model in multiple Python processes when using <span class="title-ref">mmap\_mode="r"</span>. It also gives easy shortcuts to compress and decompress the persisted object without the need for extra code. However, it may trigger the execution of malicious code when loading a model from an untrusted source as any other pickle-based persistence mechanism.
  - `pickle`: It is native to Python and most Python objects can be serialized and deserialized using `pickle`, including custom Python classes and functions as long as they are defined in a package that can be imported in the target environment. While `pickle` can be used to easily save and load scikit-learn models, it may trigger the execution of malicious code while loading a model from an untrusted source. `pickle` can also be very efficient memorywise if the model was persisted with <span class="title-ref">protocol=5</span> but it does not support memory mapping.
  - [cloudpickle](https://github.com/cloudpipe/cloudpickle): It has comparable loading efficiency as `pickle` and `joblib` (without memory mapping), but offers additional flexibility to serialize custom Python code such as lambda expressions and interactively defined functions and classes. It might be a last resort to persist pipelines with custom Python components such as a <span class="title-ref">sklearn.preprocessing.FunctionTransformer</span> that wraps a function defined in the training script itself or more generally outside of any importable Python package. Note that [cloudpickle](https://github.com/cloudpipe/cloudpickle) offers no forward compatibility guarantees and you might need the same version of [cloudpickle](https://github.com/cloudpipe/cloudpickle) to load the persisted model along with the same version of all the libraries used to define the model. As the other pickle-based persistence mechanisms, it may trigger the execution of malicious code while loading a model from an untrusted source.

---

model_selection.md

---

# Model selection and evaluation

<div class="toctree" data-maxdepth="2">

modules/cross\_validation modules/grid\_search modules/classification\_threshold modules/model\_evaluation modules/learning\_curve

</div>

---

array_api.md

---

# Array API support (experimental)

<div class="currentmodule">

sklearn

</div>

The [Array API](https://data-apis.org/array-api/latest/) specification defines a standard API for all array manipulation libraries with a NumPy-like API. Scikit-learn's Array API support requires [array-api-compat](https://github.com/data-apis/array-api-compat) to be installed.

Some scikit-learn estimators that primarily rely on NumPy (as opposed to using Cython) to implement the algorithmic logic of their <span class="title-ref">fit</span>, <span class="title-ref">predict</span> or <span class="title-ref">transform</span> methods can be configured to accept any Array API compatible input datastructures and automatically dispatch operations to the underlying namespace instead of relying on NumPy.

At this stage, this support is **considered experimental** and must be enabled explicitly as explained in the following.

<div class="note">

<div class="title">

Note

</div>

Currently, only <span class="title-ref">array-api-strict</span>, <span class="title-ref">cupy</span>, and <span class="title-ref">PyTorch</span> are known to work with scikit-learn's estimators.

</div>

## Example usage

Here is an example code snippet to demonstrate how to use [CuPy](https://cupy.dev/) to run <span class="title-ref">\~discriminant\_analysis.LinearDiscriminantAnalysis</span> on a GPU:

    >>> from sklearn.datasets import make_classification
    >>> from sklearn import config_context
    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    >>> import cupy
    
    >>> X_np, y_np = make_classification(random_state=0)
    >>> X_cu = cupy.asarray(X_np)
    >>> y_cu = cupy.asarray(y_np)
    >>> X_cu.device
    <CUDA Device 0>
    
    >>> with config_context(array_api_dispatch=True):
    ...     lda = LinearDiscriminantAnalysis()
    ...     X_trans = lda.fit_transform(X_cu, y_cu)
    >>> X_trans.device
    <CUDA Device 0>

After the model is trained, fitted attributes that are arrays will also be from the same Array API namespace as the training data. For example, if CuPy's Array API namespace was used for training, then fitted attributes will be on the GPU. We provide a experimental <span class="title-ref">\_estimator\_with\_converted\_arrays</span> utility that transfers an estimator attributes from Array API to a ndarray:

    >>> from sklearn.utils._array_api import _estimator_with_converted_arrays
    >>> cupy_to_ndarray = lambda array : array.get()
    >>> lda_np = _estimator_with_converted_arrays(lda, cupy_to_ndarray)
    >>> X_trans = lda_np.transform(X_np)
    >>> type(X_trans)
    <class 'numpy.ndarray'>

### PyTorch Support

PyTorch Tensors are supported by setting <span class="title-ref">array\_api\_dispatch=True</span> and passing in the tensors directly:

    >>> import torch
    >>> X_torch = torch.asarray(X_np, device="cuda", dtype=torch.float32)
    >>> y_torch = torch.asarray(y_np, device="cuda", dtype=torch.float32)
    
    >>> with config_context(array_api_dispatch=True):
    ...     lda = LinearDiscriminantAnalysis()
    ...     X_trans = lda.fit_transform(X_torch, y_torch)
    >>> type(X_trans)
    <class 'torch.Tensor'>
    >>> X_trans.device.type
    'cuda'

## Support for <span class="title-ref">Array API</span>-compatible inputs

Estimators and other tools in scikit-learn that support Array API compatible inputs.

### Estimators

  - <span class="title-ref">decomposition.PCA</span> (with <span class="title-ref">svd\_solver="full"</span>, <span class="title-ref">svd\_solver="randomized"</span> and <span class="title-ref">power\_iteration\_normalizer="QR"</span>)
  - <span class="title-ref">linear\_model.Ridge</span> (with <span class="title-ref">solver="svd"</span>)
  - <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> (with <span class="title-ref">solver="svd"</span>)
  - <span class="title-ref">preprocessing.KernelCenterer</span>
  - <span class="title-ref">preprocessing.MaxAbsScaler</span>
  - <span class="title-ref">preprocessing.MinMaxScaler</span>
  - <span class="title-ref">preprocessing.Normalizer</span>

### Meta-estimators

Meta-estimators that accept Array API inputs conditioned on the fact that the base estimator also does:

  - <span class="title-ref">model\_selection.GridSearchCV</span>
  - <span class="title-ref">model\_selection.RandomizedSearchCV</span>
  - <span class="title-ref">model\_selection.HalvingGridSearchCV</span>
  - <span class="title-ref">model\_selection.HalvingRandomSearchCV</span>

### Metrics

  - <span class="title-ref">sklearn.metrics.cluster.entropy</span>
  - <span class="title-ref">sklearn.metrics.accuracy\_score</span>
  - <span class="title-ref">sklearn.metrics.d2\_tweedie\_score</span>
  - <span class="title-ref">sklearn.metrics.max\_error</span>
  - <span class="title-ref">sklearn.metrics.mean\_absolute\_error</span>
  - <span class="title-ref">sklearn.metrics.mean\_absolute\_percentage\_error</span>
  - <span class="title-ref">sklearn.metrics.mean\_gamma\_deviance</span>
  - <span class="title-ref">sklearn.metrics.mean\_poisson\_deviance</span> (requires [enabling array API support for SciPy](https://docs.scipy.org/doc/scipy/dev/api-dev/array_api.html#using-array-api-standard-support))
  - <span class="title-ref">sklearn.metrics.mean\_squared\_error</span>
  - <span class="title-ref">sklearn.metrics.mean\_squared\_log\_error</span>
  - <span class="title-ref">sklearn.metrics.mean\_tweedie\_deviance</span>
  - <span class="title-ref">sklearn.metrics.pairwise.additive\_chi2\_kernel</span>
  - <span class="title-ref">sklearn.metrics.pairwise.chi2\_kernel</span>
  - <span class="title-ref">sklearn.metrics.pairwise.cosine\_similarity</span>
  - <span class="title-ref">sklearn.metrics.pairwise.cosine\_distances</span>
  - <span class="title-ref">sklearn.metrics.pairwise.euclidean\_distances</span> (see \[device\_support\_for\_float64\](\#device\_support\_for\_float64))
  - <span class="title-ref">sklearn.metrics.pairwise.linear\_kernel</span>
  - <span class="title-ref">sklearn.metrics.pairwise.paired\_cosine\_distances</span>
  - <span class="title-ref">sklearn.metrics.pairwise.paired\_euclidean\_distances</span>
  - <span class="title-ref">sklearn.metrics.pairwise.polynomial\_kernel</span>
  - <span class="title-ref">sklearn.metrics.pairwise.rbf\_kernel</span> (see \[device\_support\_for\_float64\](\#device\_support\_for\_float64))
  - <span class="title-ref">sklearn.metrics.pairwise.sigmoid\_kernel</span>
  - <span class="title-ref">sklearn.metrics.r2\_score</span>
  - <span class="title-ref">sklearn.metrics.root\_mean\_squared\_error</span>
  - <span class="title-ref">sklearn.metrics.root\_mean\_squared\_log\_error</span>
  - <span class="title-ref">sklearn.metrics.zero\_one\_loss</span>

### Tools

  - <span class="title-ref">model\_selection.train\_test\_split</span>

Coverage is expected to grow over time. Please follow the dedicated [meta-issue on GitHub](https://github.com/scikit-learn/scikit-learn/issues/22352) to track progress.

### Type of return values and fitted attributes

When calling functions or methods with Array API compatible inputs, the convention is to return array values of the same array container type and device as the input data.

Similarly, when an estimator is fitted with Array API compatible inputs, the fitted attributes will be arrays from the same library as the input and stored on the same device. The <span class="title-ref">predict</span> and <span class="title-ref">transform</span> method subsequently expect inputs from the same array library and device as the data passed to the <span class="title-ref">fit</span> method.

Note however that scoring functions that return scalar values return Python scalars (typically a <span class="title-ref">float</span> instance) instead of an array scalar value.

## Common estimator checks

Add the <span class="title-ref">array\_api\_support</span> tag to an estimator's set of tags to indicate that it supports the Array API. This will enable dedicated checks as part of the common tests to verify that the estimators result's are the same when using vanilla NumPy and Array API inputs.

To run these checks you need to install [array\_api\_compat](https://github.com/data-apis/array-api-compat) in your test environment. To run the full set of checks you need to install both [PyTorch](https://pytorch.org/) and [CuPy](https://cupy.dev/) and have a GPU. Checks that can not be executed or have missing dependencies will be automatically skipped. Therefore it's important to run the tests with the <span class="title-ref">-v</span> flag to see which checks are skipped:

<div class="prompt">

bash $

pip install array-api-compat \# and other libraries as needed pytest -k "array\_api" -v

</div>

### Note on MPS device support

On macOS, PyTorch can use the Metal Performance Shaders (MPS) to access hardware accelerators (e.g. the internal GPU component of the M1 or M2 chips). However, the MPS device support for PyTorch is incomplete at the time of writing. See the following github issue for more details:

  - <https://github.com/pytorch/pytorch/issues/77764>

To enable the MPS support in PyTorch, set the environment variable <span class="title-ref">PYTORCH\_ENABLE\_MPS\_FALLBACK=1</span> before running the tests:

<div class="prompt">

bash $

PYTORCH\_ENABLE\_MPS\_FALLBACK=1 pytest -k "array\_api" -v

</div>

At the time of writing all scikit-learn tests should pass, however, the computational speed is not necessarily better than with the CPU device.

### Note on device support for `float64`

Certain operations within scikit-learn will automatically perform operations on floating-point values with <span class="title-ref">float64</span> precision to prevent overflows and ensure correctness (e.g., <span class="title-ref">metrics.pairwise.euclidean\_distances</span>). However, certain combinations of array namespaces and devices, such as <span class="title-ref">PyTorch on MPS</span> (see \[mps\_support\](\#mps\_support)) do not support the <span class="title-ref">float64</span> data type. In these cases, scikit-learn will revert to using the <span class="title-ref">float32</span> data type instead. This can result in different behavior (typically numerically unstable results) compared to not using array API dispatching or using a device with <span class="title-ref">float64</span> support.

---

biclustering.md

---

# Biclustering

Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.

For instance, given a matrix of shape `(10, 10)`, one possible bicluster with three rows and two columns induces a submatrix of shape `(3, 2)`:

    >>> import numpy as np
    >>> data = np.arange(100).reshape(10, 10)
    >>> rows = np.array([0, 2, 3])[:, np.newaxis]
    >>> columns = np.array([1, 2])
    >>> data[rows, columns]
    array([[ 1,  2],
           [21, 22],
           [31, 32]])

For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.

Algorithms differ in how they define biclusters. Some of the common types include:

  - constant values, constant rows, or constant columns
  - unusually high or low values
  - submatrices with low variance
  - correlated rows or columns

Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions.

If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:

![An example of biclusters formed by partitioning rows and columns.](../auto_examples/bicluster/images/sphx_glr_plot_spectral_coclustering_003.png)

In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small:

![An example of checkerboard biclusters.](../auto_examples/bicluster/images/sphx_glr_plot_spectral_biclustering_003.png)

After fitting a model, row and column cluster membership can be found in the `rows_` and `columns_` attributes. `rows_[i]` is a binary vector with nonzero entries corresponding to rows that belong to bicluster `i`. Similarly, `columns_[i]` indicates which columns belong to bicluster `i`.

Some models also have `row_labels_` and `column_labels_` attributes. These models partition the rows and columns, such as in the block diagonal and checkerboard bicluster structures.

\> **Note** \> Biclustering has many other names in different fields including co-clustering, two-mode clustering, two-way clustering, block clustering, coupled two-way clustering, etc. The names of some algorithms, such as the Spectral Co-Clustering algorithm, reflect these alternate names.

<div class="currentmodule">

sklearn.cluster

</div>

## Spectral Co-Clustering

The <span class="title-ref">SpectralCoclustering</span> algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:

\> **Note** \> The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.

### Mathematical formulation

An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix \(A\) has shape \(m
\times n\), the Laplacian matrix for the corresponding bipartite graph has shape \((m + n) \times (m + n)\). However, in this case it is possible to work directly with \(A\), which is smaller and more efficient.

The input matrix \(A\) is preprocessed as follows:

\[A_n = R^{-1/2} A C^{-1/2}\]

Where \(R\) is the diagonal matrix with entry \(i\) equal to \(\sum_{j} A_{ij}\) and \(C\) is the diagonal matrix with entry \(j\) equal to \(\sum_{i} A_{ij}\).

The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.

The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):

\[\begin{aligned}
Z = \begin{bmatrix} R^{-1/2} U \\\\
C^{-1/2} V
\end{bmatrix}
\end{aligned}\]

where the columns of \(U\) are \(u_2, \dots, u_{\ell +
1}\), and similarly for \(V\).

Then the rows of \(Z\) are clustered using \[k-means \<k\_means\>\](\#k-means \<k\_means\>). The first `n_rows` labels provide the row partitioning, and the remaining `n_columns` labels provide the column partitioning.

**Examples**

  - \[sphx\_glr\_auto\_examples\_bicluster\_plot\_spectral\_coclustering.py\](\#sphx\_glr\_auto\_examples\_bicluster\_plot\_spectral\_coclustering.py): A simple example showing how to generate a data matrix with biclusters and apply this method to it.
  - \[sphx\_glr\_auto\_examples\_bicluster\_plot\_bicluster\_newsgroups.py\](\#sphx\_glr\_auto\_examples\_bicluster\_plot\_bicluster\_newsgroups.py): An example of finding biclusters in the twenty newsgroup dataset.

**References**

  - Dhillon, Inderjit S, 2001. `Co-clustering documents and words using
    bipartite spectral graph partitioning
    <10.1145/502512.502550>`

## Spectral Biclustering

The <span class="title-ref">SpectralBiclustering</span> algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.

The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.

### Mathematical formulation

The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:

1.  *Independent row and column normalization*, as in Spectral Co-Clustering. This method makes the rows sum to a constant and the columns sum to a different constant.
2.  **Bistochastization**: repeated row and column normalization until convergence. This method makes both rows and columns sum to the same constant.
3.  **Log normalization**: the log of the data matrix is computed: \(L =
    \log A\). Then the column mean \(\overline{L_{i \cdot}}\), row mean \(\overline{L_{\cdot j}}\), and overall mean \(\overline{L_{\cdot
    \cdot}}\) of \(L\) are computed. The final matrix is computed according to the formula

\[K_{ij} = L_{ij} - \overline{L_{i \cdot}} - \overline{L_{\cdot
j}} + \overline{L_{\cdot \cdot}}\]

After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.

If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \(u_1\) and \(v_1\). are discarded. From now on, the "first" singular vectors refers to \(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the case of log normalization.

Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.

For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q<p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.

**Examples**

  - \[sphx\_glr\_auto\_examples\_bicluster\_plot\_spectral\_biclustering.py\](\#sphx\_glr\_auto\_examples\_bicluster\_plot\_spectral\_biclustering.py): a simple example showing how to generate a checkerboard matrix and bicluster it.

**References**

  - Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray
    data: coclustering genes and conditions
    <10.1101/gr.648603>`

<div id="biclustering_evaluation">

<div class="currentmodule">

sklearn.metrics

</div>

</div>

## Biclustering evaluation

There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.

To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score.

To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:

\[J(A, B) = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}\]

where \(A\) and \(B\) are biclusters, \(|A \cap B|\) is the number of elements in their intersection. The Jaccard index achieves its minimum of 0 when the biclusters to not overlap at all and its maximum of 1 when they are identical.

Several methods have been developed to compare two sets of biclusters. For now, only <span class="title-ref">consensus\_score</span> (Hochreiter et. al., 2010) is available:

1.  Compute bicluster similarities for pairs of biclusters, one in each set, using the Jaccard index or a similar measure.
2.  Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using <span class="title-ref">scipy.optimize.linear\_sum\_assignment</span>, which uses a modified Jonker-Volgenant algorithm.
3.  The final sum of similarities is divided by the size of the larger set.

The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.

**References**

  - Hochreiter, Bodenhofer, et. al., 2010. [FABIA: factor analysis for bicluster acquisition](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/).

---

calibration.md

---

# Probability calibration

<div class="currentmodule">

sklearn.calibration

</div>

When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction (e.g., some instances of <span class="title-ref">\~sklearn.linear\_model.SGDClassifier</span>). The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.

Well calibrated classifiers are probabilistic classifiers for which the output of the `predict_proba` method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a `predict_proba` value close to, say, 0.8, approximately 80% actually belong to the positive class.

Before we show how to re-calibrate a classifier, we first need a way to detect how good a classifier is calibrated.

<div class="note">

<div class="title">

Note

</div>

Strictly proper scoring rules for probabilistic predictions like <span class="title-ref">sklearn.metrics.brier\_score\_loss</span> and <span class="title-ref">sklearn.metrics.log\_loss</span> assess calibration (reliability) and discriminative power (resolution) of a model, as well as the randomness of the data (uncertainty) at the same time. This follows from the well-known Brier score decomposition of Murphy\[1\]. As it is not clear which term dominates, the score is of limited use for assessing calibration alone (unless one computes each term of the decomposition). A lower Brier loss, for instance, does not necessarily mean a better calibrated model, it could also mean a worse calibrated model with much more discriminatory power, e.g. using many more features.

</div>

## Calibration curves

Calibration curves, also referred to as *reliability diagrams* (Wilks 1995\[2\]), compare how well the probabilistic predictions of a binary classifier are calibrated. It plots the frequency of the positive label (to be more precise, an estimation of the *conditional event probability* \(P(Y=1|\text{predict_proba})\)) on the y-axis against the predicted probability `predict_proba` of a model on the x-axis. The tricky part is to get values for the y-axis. In scikit-learn, this is accomplished by binning the predictions such that the x-axis represents the average predicted probability in each bin. The y-axis is then the *fraction of positives* given the predictions of that bin, i.e. the proportion of samples whose class is the positive class (in each bin).

The top calibration curve plot is created with <span class="title-ref">CalibrationDisplay.from\_estimator</span>, which uses <span class="title-ref">calibration\_curve</span> to calculate the per bin average predicted probabilities and fraction of positives. <span class="title-ref">CalibrationDisplay.from\_estimator</span> takes as input a fitted classifier, which is used to calculate the predicted probabilities. The classifier thus must have `predict_proba` method. For the few classifiers that do not have a `predict_proba` method, it is possible to use <span class="title-ref">CalibratedClassifierCV</span> to calibrate the classifier outputs to probabilities.

The bottom histogram gives some insight into the behavior of each classifier by showing the number of samples in each predicted probability bin.

![](../auto_examples/calibration/images/sphx_glr_plot_compare_calibration_001.png)

<div class="currentmodule">

sklearn.linear\_model

</div>

<span class="title-ref">LogisticRegression</span> is more likely to return well calibrated predictions by itself as it has a canonical link function for its loss, i.e. the logit-link for the \[log\_loss\](\#log\_loss). In the unpenalized case, this leads to the so-called **balance property**, see\[3\] and \[Logistic\_regression\](\#logistic\_regression). In the plot above, data is generated according to a linear mechanism, which is consistent with the <span class="title-ref">LogisticRegression</span> model (the model is 'well specified'), and the value of the regularization parameter <span class="title-ref">C</span> is tuned to be appropriate (neither too strong nor too low). As a consequence, this model returns accurate predictions from its <span class="title-ref">predict\_proba</span> method. In contrast to that, the other shown models return biased probabilities; with different biases per model.

<div class="currentmodule">

sklearn.naive\_bayes

</div>

<span class="title-ref">GaussianNB</span> (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.

<div class="currentmodule">

sklearn.ensemble

</div>

<span class="title-ref">RandomForestClassifier</span> shows the opposite behavior: the histograms show peaks at probabilities approximately 0.2 and 0.9, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil and Caruana\[4\]: "Methods such as bagging and random forests that average predictions from a base set of models can have difficulty making predictions near 0 and 1 because variance in the underlying base models will bias predictions that should be near zero or one away from these values. Because predictions are restricted to the interval \[0,1\], errors caused by variance tend to be one-sided near zero and one. For example, if a model should predict p = 0 for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this effect most strongly with random forests because the base-level trees trained with random forests have relatively high variance due to feature subsetting." As a result, the calibration curve shows a characteristic sigmoid shape, indicating that the classifier could trust its "intuition" more and return probabilities closer to 0 or 1 typically.

<div class="currentmodule">

sklearn.svm

</div>

<span class="title-ref">LinearSVC</span> (SVC) shows an even more sigmoid curve than the random forest, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana\[5\]), which focus on difficult to classify samples that are close to the decision boundary (the support vectors).

## Calibrating a classifier

<div class="currentmodule">

sklearn.calibration

</div>

Calibrating a classifier consists of fitting a regressor (called a *calibrator*) that maps the output of the classifier (as given by `decision_function` or `predict_proba`) to a calibrated probability in \[0, 1\]. Denoting the output of the classifier for a given sample by \(f_i\), the calibrator tries to predict the conditional event probability \(P(y_i = 1 | f_i)\).

Ideally, the calibrator is fit on a dataset independent of the training data used to fit the classifier in the first place. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should.

## Usage

The <span class="title-ref">CalibratedClassifierCV</span> class is used to calibrate a classifier.

<span class="title-ref">CalibratedClassifierCV</span> uses a cross-validation approach to ensure unbiased data is always used to fit the calibrator. The data is split into k <span class="title-ref">(train\_set, test\_set)</span> couples (as determined by <span class="title-ref">cv</span>). When <span class="title-ref">ensemble=True</span> (default), the following procedure is repeated independently for each cross-validation split:

1.  a clone of <span class="title-ref">base\_estimator</span> is trained on the train subset
2.  the trained <span class="title-ref">base\_estimator</span> makes predictions on the test subset
3.  the predictions are used to fit a calibrator (either a sigmoid or isotonic regressor) (when the data is multiclass, a calibrator is fit for every class)

This results in an ensemble of k <span class="title-ref">(classifier, calibrator)</span> couples where each calibrator maps the output of its corresponding classifier into \[0, 1\]. Each couple is exposed in the <span class="title-ref">calibrated\_classifiers\_</span> attribute, where each entry is a calibrated classifier with a `predict_proba` method that outputs calibrated probabilities. The output of `predict_proba` for the main <span class="title-ref">CalibratedClassifierCV</span> instance corresponds to the average of the predicted probabilities of the <span class="title-ref">k</span> estimators in the <span class="title-ref">calibrated\_classifiers\_</span> list. The output of `predict` is the class that has the highest probability.

It is important to choose <span class="title-ref">cv</span> carefully when using <span class="title-ref">ensemble=True</span>. All classes should be present in both train and test subsets for every split. When a class is absent in the train subset, the predicted probability for that class will default to 0 for the <span class="title-ref">(classifier, calibrator)</span> couple of that split. This skews the `predict_proba` as it averages across all couples. When a class is absent in the test subset, the calibrator for that class (within the <span class="title-ref">(classifier, calibrator)</span> couple of that split) is fit on data with no positive class. This results in ineffective calibration.

When <span class="title-ref">ensemble=False</span>, cross-validation is used to obtain 'unbiased' predictions for all the data, via <span class="title-ref">\~sklearn.model\_selection.cross\_val\_predict</span>. These unbiased predictions are then used to train the calibrator. The attribute <span class="title-ref">calibrated\_classifiers\_</span> consists of only one <span class="title-ref">(classifier, calibrator)</span> couple where the classifier is the <span class="title-ref">base\_estimator</span> trained on all the data. In this case the output of `predict_proba` for <span class="title-ref">CalibratedClassifierCV</span> is the predicted probabilities obtained from the single <span class="title-ref">(classifier, calibrator)</span> couple.

The main advantage of <span class="title-ref">ensemble=True</span> is to benefit from the traditional ensembling effect (similar to \[bagging\](\#bagging)). The resulting ensemble should both be well calibrated and slightly more accurate than with <span class="title-ref">ensemble=False</span>. The main advantage of using <span class="title-ref">ensemble=False</span> is computational: it reduces the overall fit time by training only a single base classifier and calibrator pair, decreases the final model size and increases prediction speed.

Alternatively an already fitted classifier can be calibrated by using a <span class="title-ref">\~sklearn.frozen.FrozenEstimator</span> as `CalibratedClassifierCV(estimator=FrozenEstimator(estimator))`. It is up to the user to make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor. data used for fitting the regressor.

<span class="title-ref">CalibratedClassifierCV</span> supports the use of two regression techniques for calibration via the <span class="title-ref">method</span> parameter: <span class="title-ref">"sigmoid"</span> and <span class="title-ref">"isotonic"</span>.

### Sigmoid

The sigmoid regressor, <span class="title-ref">method="sigmoid"</span> is based on Platt's logistic model\[6\]:

\[p(y_i = 1 | f_i) = \frac{1}{1 + \exp(A f_i + B)} \,,\]

where \(y_i\) is the true label of sample \(i\) and \(f_i\) is the output of the un-calibrated classifier for sample \(i\). \(A\) and \(B\) are real numbers to be determined when fitting the regressor via maximum likelihood.

The sigmoid method assumes the \[calibration curve \<calibration\_curve\>\](\#calibration-curve-\<calibration\_curve\>) can be corrected by applying a sigmoid function to the raw predictions. This assumption has been empirically justified in the case of \[svm\](\#svm) with common kernel functions on various benchmark datasets in section 2.1 of Platt 1999\[7\] but does not necessarily hold in general. Additionally, the logistic model works best if the calibration error is symmetrical, meaning the classifier output for each binary class is normally distributed with the same variance\[8\]. This can be a problem for highly imbalanced classification problems, where outputs do not have equal variance.

In general this method is most effective for small sample sizes or when the un-calibrated model is under-confident and has similar calibration errors for both high and low outputs.

### Isotonic

The <span class="title-ref">method="isotonic"</span> fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function, see `sklearn.isotonic`. It minimizes:

\[\sum_{i=1}^{n} (y_i - \hat{f}_i)^2\]

subject to \(\hat{f}_i \geq \hat{f}_j\) whenever \(f_i \geq f_j\). \(y_i\) is the true label of sample \(i\) and \(\hat{f}_i\) is the output of the calibrated classifier for sample \(i\) (i.e., the calibrated probability). This method is more general when compared to 'sigmoid' as the only restriction is that the mapping function is monotonically increasing. It is thus more powerful as it can correct any monotonic distortion of the un-calibrated model. However, it is more prone to overfitting, especially on small datasets\[9\].

Overall, 'isotonic' will perform as well as or better than 'sigmoid' when there is enough data (greater than \~ 1000 samples) to avoid overfitting\[10\].

<div class="note">

<div class="title">

Note

</div>

Impact on ranking metrics like AUC

It is generally expected that calibration does not affect ranking metrics such as ROC-AUC. However, these metrics might differ after calibration when using <span class="title-ref">method="isotonic"</span> since isotonic regression introduces ties in the predicted probabilities. This can be seen as within the uncertainty of the model predictions. In case, you strictly want to keep the ranking and thus AUC scores, use <span class="title-ref">method="sigmoid"</span> which is a strictly monotonic transformation and thus keeps the ranking.

</div>

### Multiclass support

Both isotonic and sigmoid regressors only support 1-dimensional data (e.g., binary classification output) but are extended for multiclass classification if the <span class="title-ref">base\_estimator</span> supports multiclass predictions. For multiclass predictions, <span class="title-ref">CalibratedClassifierCV</span> calibrates for each class separately in a \[ovr\_classification\](\#ovr\_classification) fashion\[11\]. When predicting probabilities, the calibrated probabilities for each class are predicted separately. As those probabilities do not necessarily sum to one, a postprocessing is performed to normalize them.

**Examples**

  - \[sphx\_glr\_auto\_examples\_calibration\_plot\_calibration\_curve.py\](\#sphx\_glr\_auto\_examples\_calibration\_plot\_calibration\_curve.py)
  - \[sphx\_glr\_auto\_examples\_calibration\_plot\_calibration\_multiclass.py\](\#sphx\_glr\_auto\_examples\_calibration\_plot\_calibration\_multiclass.py)
  - \[sphx\_glr\_auto\_examples\_calibration\_plot\_calibration.py\](\#sphx\_glr\_auto\_examples\_calibration\_plot\_calibration.py)
  - \[sphx\_glr\_auto\_examples\_calibration\_plot\_compare\_calibration.py\](\#sphx\_glr\_auto\_examples\_calibration\_plot\_compare\_calibration.py)

**References**

1.  Allan H. Murphy (1973). `"A New Vector Partition of the Probability Score"
    <10.1175/1520-0450(1973)012%3C0595:ANVPOT%3E2.0.CO;2>` Journal of Applied Meteorology and Climatology

2.  [On the combination of forecast probabilities for consecutive precipitation periods.](https://journals.ametsoc.org/waf/article/5/4/640/40179) Wea. Forecasting, 5, 640–650., Wilks, D. S., 1990a

3.  Mario V. Wüthrich, Michael Merz (2023). `"Statistical Foundations of Actuarial Learning and its Applications"
    <10.1007/978-3-031-12409-9>` Springer Actuarial

4.  [Predicting Good Probabilities with Supervised Learning](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf), A. Niculescu-Mizil & R. Caruana, ICML 2005

5.  [Predicting Good Probabilities with Supervised Learning](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf), A. Niculescu-Mizil & R. Caruana, ICML 2005

6.  [Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf) J. Platt, (1999)

7.  [Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf) J. Platt, (1999)

8.  [Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration](https://projecteuclid.org/euclid.ejs/1513306867) Kull, M., Silva Filho, T. M., & Flach, P. (2017).

9.  [Predicting accurate probabilities with a ranking loss.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4180410/) Menon AK, Jiang XJ, Vembu S, Elkan C, Ohno-Machado L. Proc Int Conf Mach Learn. 2012;2012:703-710

10. [Predicting Good Probabilities with Supervised Learning](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf), A. Niculescu-Mizil & R. Caruana, ICML 2005

11. [Transforming Classifier Scores into Accurate Multiclass Probability Estimates.](https://dl.acm.org/doi/pdf/10.1145/775047.775151) B. Zadrozny & C. Elkan, (KDD 2002)

---

classification_threshold.md

---

<div class="currentmodule">

sklearn.model\_selection

</div>

# Tuning the decision threshold for class prediction

Classification is best divided into two parts:

  - the statistical problem of learning a model to predict, ideally, class probabilities;
  - the decision problem to take concrete action based on those probability predictions.

Let's take a straightforward example related to weather forecasting: the first point is related to answering "what is the chance that it will rain tomorrow?" while the second point is related to answering "should I take an umbrella tomorrow?".

When it comes to the scikit-learn API, the first point is addressed providing scores using `predict_proba` or `decision_function`. The former returns conditional probability estimates \(P(y|X)\) for each class, while the latter returns a decision score for each class.

The decision corresponding to the labels are obtained with `predict`. In binary classification, a decision rule or action is then defined by thresholding the scores, leading to the prediction of a single class label for each sample. For binary classification in scikit-learn, class labels predictions are obtained by hard-coded cut-off rules: a positive class is predicted when the conditional probability \(P(y|X)\) is greater than 0.5 (obtained with `predict_proba`) or if the decision score is greater than 0 (obtained with `decision_function`).

Here, we show an example that illustrates the relation between conditional probability estimates \(P(y|X)\) and class labels:

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.tree import DecisionTreeClassifier
    >>> X, y = make_classification(random_state=0)
    >>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)
    >>> classifier.predict_proba(X[:4])
    array([[0.94     , 0.06     ],
           [0.94     , 0.06     ],
           [0.0416..., 0.9583...],
           [0.0416..., 0.9583...]])
    >>> classifier.predict(X[:4])
    array([0, 0, 1, 1])

While these hard-coded rules might at first seem reasonable as default behavior, they are most certainly not ideal for most use cases. Let's illustrate with an example.

Consider a scenario where a predictive model is being deployed to assist physicians in detecting tumors. In this setting, physicians will most likely be interested in identifying all patients with cancer and not missing anyone with cancer so that they can provide them with the right treatment. In other words, physicians prioritize achieving a high recall rate. This emphasis on recall comes, of course, with the trade-off of potentially more false-positive predictions, reducing the precision of the model. That is a risk physicians are willing to take because the cost of a missed cancer is much higher than the cost of further diagnostic tests. Consequently, when it comes to deciding whether to classify a patient as having cancer or not, it may be more beneficial to classify them as positive for cancer when the conditional probability estimate is much lower than 0.5.

## Post-tuning the decision threshold

One solution to address the problem stated in the introduction is to tune the decision threshold of the classifier once the model has been trained. The <span class="title-ref">\~sklearn.model\_selection.TunedThresholdClassifierCV</span> tunes this threshold using an internal cross-validation. The optimum threshold is chosen to maximize a given metric.

The following image illustrates the tuning of the decision threshold for a gradient boosting classifier. While the vanilla and tuned classifiers provide the same `predict_proba` outputs and thus the same Receiver Operating Characteristic (ROC) and Precision-Recall curves, the class label predictions differ because of the tuned decision threshold. The vanilla classifier predicts the class of interest for a conditional probability greater than 0.5 while the tuned classifier predicts the class of interest for a very low probability (around 0.02). This decision threshold optimizes a utility metric defined by the business (in this case an insurance company).

![](../auto_examples/model_selection/images/sphx_glr_plot_cost_sensitive_learning_002.png)

### Options to tune the decision threshold

The decision threshold can be tuned through different strategies controlled by the parameter <span class="title-ref">scoring</span>.

One way to tune the threshold is by maximizing a pre-defined scikit-learn metric. These metrics can be found by calling the function <span class="title-ref">\~sklearn.metrics.get\_scorer\_names</span>. By default, the balanced accuracy is the metric used but be aware that one should choose a meaningful metric for their use case.

\> **Note** \> It is important to notice that these metrics come with default parameters, notably the label of the class of interest (i.e. <span class="title-ref">pos\_label</span>). Thus, if this label is not the right one for your application, you need to define a scorer and pass the right <span class="title-ref">pos\_label</span> (and additional parameters) using the <span class="title-ref">\~sklearn.metrics.make\_scorer</span>. Refer to \[scoring\](\#scoring) to get information to define your own scoring function. For instance, we show how to pass the information to the scorer that the label of interest is <span class="title-ref">0</span> when maximizing the \`\~sklearn.metrics.f1\_score\`:

    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.model_selection import TunedThresholdClassifierCV
    >>> from sklearn.metrics import make_scorer, f1_score
    >>> X, y = make_classification(
    ...   n_samples=1_000, weights=[0.1, 0.9], random_state=0)
    >>> pos_label = 0
    >>> scorer = make_scorer(f1_score, pos_label=pos_label)
    >>> base_model = LogisticRegression()
    >>> model = TunedThresholdClassifierCV(base_model, scoring=scorer)
    >>> scorer(model.fit(X, y), X, y)
    0.88...
    >>> # compare it with the internal score found by cross-validation
    >>> model.best_score_
    0.86...

### Important notes regarding the internal cross-validation

By default <span class="title-ref">\~sklearn.model\_selection.TunedThresholdClassifierCV</span> uses a 5-fold stratified cross-validation to tune the decision threshold. The parameter <span class="title-ref">cv</span> allows to control the cross-validation strategy. It is possible to bypass cross-validation by setting <span class="title-ref">cv="prefit"</span> and providing a fitted classifier. In this case, the decision threshold is tuned on the data provided to the <span class="title-ref">fit</span> method.

However, you should be extremely careful when using this option. You should never use the same data for training the classifier and tuning the decision threshold due to the risk of overfitting. Refer to the following example section for more details (cf. \[TunedThresholdClassifierCV\_no\_cv\](\#tunedthresholdclassifiercv\_no\_cv)). If you have limited resources, consider using a float number for <span class="title-ref">cv</span> to limit to an internal single train-test split.

The option <span class="title-ref">cv="prefit"</span> should only be used when the provided classifier was already trained, and you just want to find the best decision threshold using a new validation set.

### Manually setting the decision threshold

The previous sections discussed strategies to find an optimal decision threshold. It is also possible to manually set the decision threshold using the class <span class="title-ref">\~sklearn.model\_selection.FixedThresholdClassifier</span>. In case that you don't want to refit the model when calling <span class="title-ref">fit</span>, wrap your sub-estimator with a <span class="title-ref">\~sklearn.frozen.FrozenEstimator</span> and do `FixedThresholdClassifier(FrozenEstimator(estimator), ...)`.

### Examples

  - See the example entitled \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_tuned\_decision\_threshold.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_tuned\_decision\_threshold.py), to get insights on the post-tuning of the decision threshold.
  - See the example entitled \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_cost\_sensitive\_learning.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_cost\_sensitive\_learning.py), to learn about cost-sensitive learning and decision threshold tuning.

---

clustering.md

---

# Clustering

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) of unlabeled data can be performed with the module `sklearn.cluster`.

Each clustering algorithm comes in two variants: a class, that implements the `fit` method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the `labels_` attribute.

<div class="currentmodule">

sklearn.cluster

</div>

<div class="topic">

**Input data**

One important thing to note is that the algorithms implemented in this module can take different kinds of matrix as input. All the methods accept standard data matrices of shape `(n_samples, n_features)`. These can be obtained from the classes in the `sklearn.feature_extraction` module. For <span class="title-ref">AffinityPropagation</span>, <span class="title-ref">SpectralClustering</span> and <span class="title-ref">DBSCAN</span> one can also input similarity matrices of shape `(n_samples, n_samples)`. These can be obtained from the functions in the `sklearn.metrics.pairwise` module.

</div>

## Overview of clustering methods

![A comparison of the clustering algorithms in scikit-learn](../auto_examples/cluster/images/sphx_glr_plot_cluster_comparison_001.png)

| Method name                                                                                                                | Parameters                                                       | Scalability                                                                                                                           | Usecase                                                                                                        | Geometry (metric used)                       |
| -------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| \[K-Means \<k\_means\>\](\#k-means-\<k\_means\>)                                                                           | number of clusters                                               | Very large `n_samples`, medium `n_clusters` with \[MiniBatch code \<mini\_batch\_kmeans\>\](\#minibatch-code-\<mini\_batch\_kmeans\>) | General-purpose, even cluster size, flat geometry, not too many clusters, inductive                            | Distances between points                     |
| \[Affinity propagation \<affinity\_propagation\>\](\#affinity-propagation-\<affinity\_propagation\>)                       | damping, sample preference                                       | Not scalable with n\_samples                                                                                                          | Many clusters, uneven cluster size, non-flat geometry, inductive                                               | Graph distance (e.g. nearest-neighbor graph) |
| \[Mean-shift \<mean\_shift\>\](\#mean-shift-\<mean\_shift\>)                                                               | bandwidth                                                        | Not scalable with `n_samples`                                                                                                         | Many clusters, uneven cluster size, non-flat geometry, inductive                                               | Distances between points                     |
| \[Spectral clustering \<spectral\_clustering\>\](\#spectral-clustering-\<spectral\_clustering\>)                           | number of clusters                                               | Medium `n_samples`, small `n_clusters`                                                                                                | Few clusters, even cluster size, non-flat geometry, transductive                                               | Graph distance (e.g. nearest-neighbor graph) |
| \[Ward hierarchical clustering \<hierarchical\_clustering\>\](\#ward-hierarchical-clustering-\<hierarchical\_clustering\>) | number of clusters or distance threshold                         | Large `n_samples` and `n_clusters`                                                                                                    | Many clusters, possibly connectivity constraints, transductive                                                 | Distances between points                     |
| \[Agglomerative clustering \<hierarchical\_clustering\>\](\#agglomerative-clustering-\<hierarchical\_clustering\>)         | number of clusters or distance threshold, linkage type, distance | Large `n_samples` and `n_clusters`                                                                                                    | Many clusters, possibly connectivity constraints, non Euclidean distances, transductive                        | Any pairwise distance                        |
| \[DBSCAN \<dbscan\>\](\#dbscan-\<dbscan\>)                                                                                 | neighborhood size                                                | Very large `n_samples`, medium `n_clusters`                                                                                           | Non-flat geometry, uneven cluster sizes, outlier removal, transductive                                         | Distances between nearest points             |
| \[HDBSCAN \<hdbscan\>\](\#hdbscan-\<hdbscan\>)                                                                             | minimum cluster membership, minimum point neighbors              | large `n_samples`, medium `n_clusters`                                                                                                | Non-flat geometry, uneven cluster sizes, outlier removal, transductive, hierarchical, variable cluster density | Distances between nearest points             |
| \[OPTICS \<optics\>\](\#optics-\<optics\>)                                                                                 | minimum cluster membership                                       | Very large `n_samples`, large `n_clusters`                                                                                            | Non-flat geometry, uneven cluster sizes, variable cluster density, outlier removal, transductive               | Distances between points                     |
| \[Gaussian mixtures \<mixture\>\](\#gaussian-mixtures-\<mixture\>)                                                         | many                                                             | Not scalable                                                                                                                          | Flat geometry, good for density estimation, inductive                                                          | Mahalanobis distances to centers             |
| \[BIRCH \<birch\>\](\#birch-\<birch\>)                                                                                     | branching factor, threshold, optional global clusterer.          | Large `n_clusters` and `n_samples`                                                                                                    | Large dataset, outlier removal, data reduction, inductive                                                      | Euclidean distance between points            |
| \[Bisecting K-Means \<bisect\_k\_means\>\](\#bisecting-k-means-\<bisect\_k\_means\>)                                       | number of clusters                                               | Very large `n_samples`, medium `n_clusters`                                                                                           | General-purpose, even cluster size, flat geometry, no empty clusters, inductive, hierarchical                  | Distances between points                     |

Non-flat geometry clustering is useful when the clusters have a specific shape, i.e. a non-flat manifold, and the standard euclidean distance is not the right metric. This case arises in the two top rows of the figure above.

Gaussian mixture models, useful for clustering, are described in \[another chapter of the documentation \<mixture\>\](\#another-chapter-of-the-documentation-\<mixture\>) dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.

`Transductive <transductive>` clustering methods (in contrast to `inductive` clustering methods) are not designed to be applied to new, unseen data.

## K-means

The <span class="title-ref">KMeans</span> algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the *inertia* or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large numbers of samples and has been used across a large range of application areas in many different fields.

The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster "centroids"; note that they are not, in general, points from \(X\), although they live in the same space.

The K-means algorithm aims to choose centroids that minimise the **inertia**, or **within-cluster sum-of-squares criterion**:

\[\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)\]

Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:

  - Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.
  - Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called "curse of dimensionality"). Running a dimensionality reduction algorithm such as \[PCA\](\#pca) prior to k-means clustering can alleviate this problem and speed up the computations.

[![image](../auto_examples/cluster/images/sphx_glr_plot_kmeans_assumptions_002.png)](../auto_examples/cluster/plot_kmeans_assumptions.html)

For more detailed descriptions of the issues shown above and how to address them, refer to the examples \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_assumptions.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_assumptions.py) and \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_silhouette\_analysis.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_silhouette\_analysis.py).

K-means is often referred to as Lloyd's algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.

[![image](../auto_examples/cluster/images/sphx_glr_plot_kmeans_digits_001.png)](../auto_examples/cluster/plot_kmeans_digits.html)

K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.

The algorithm can also be understood through the concept of [Voronoi diagrams](https://en.wikipedia.org/wiki/Voronoi_diagram). First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.

Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the `init='k-means++'` parameter). This initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization, as shown in the reference. For a detailed example of comaparing different initialization schemes, refer to \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_digits.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_digits.py).

K-means++ can also be called independently to select seeds for other clustering algorithms, see <span class="title-ref">sklearn.cluster.kmeans\_plusplus</span> for details and example usage.

The algorithm supports sample weights, which can be given by a parameter `sample_weight`. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).

**Examples**

  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py): Document clustering using <span class="title-ref">KMeans</span> and <span class="title-ref">MiniBatchKMeans</span> based on sparse data
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_plusplus.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_plusplus.py): Using K-means++ to select seeds for other clustering algorithms.

### Low-level parallelism

<span class="title-ref">KMeans</span> benefits from OpenMP based parallelism through Cython. Small chunks of data (256 samples) are processed in parallel, which in addition yields a low memory footprint. For more details on how to control the number of threads, please refer to our \[parallelism\](\#parallelism) notes.

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_assumptions.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_assumptions.py): Demonstrating when k-means performs intuitively and when it does not
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_digits.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_digits.py): Clustering handwritten digits

<div class="dropdown">

References

  - ["k-means++: The advantages of careful seeding"](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf) Arthur, David, and Sergei Vassilvitskii, *Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms*, Society for Industrial and Applied Mathematics (2007)

</div>

### Mini Batch K-Means

The <span class="title-ref">MiniBatchKMeans</span> is a variant of the <span class="title-ref">KMeans</span> algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.

The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.

<span class="title-ref">MiniBatchKMeans</span> converges faster than <span class="title-ref">KMeans</span>, but the quality of the results is reduced. In practice this difference in quality can be quite small, as shown in the example and cited reference.

![](../auto_examples/cluster/images/sphx_glr_plot_mini_batch_kmeans_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_mini\_batch\_kmeans.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_mini\_batch\_kmeans.py): Comparison of <span class="title-ref">KMeans</span> and <span class="title-ref">MiniBatchKMeans</span>
  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py): Document clustering using <span class="title-ref">KMeans</span> and <span class="title-ref">MiniBatchKMeans</span> based on sparse data
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_dict\_face\_patches.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_dict\_face\_patches.py)

<div class="dropdown">

References

  - ["Web Scale K-Means clustering"](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)
    D. Sculley, *Proceedings of the 19th international conference on World wide web* (2010)

</div>

## Affinity Propagation

<span class="title-ref">AffinityPropagation</span> creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.

![](../auto_examples/cluster/images/sphx_glr_plot_affinity_propagation_001.png)

Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the *preference*, which controls how many exemplars are used, and the *damping factor* which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.

The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.

<div class="dropdown">

Algorithm description

The messages sent between points belong to one of two categories. The first is the responsibility \(r(i, k)\), which is the accumulated evidence that sample \(k\) should be the exemplar for sample \(i\). The second is the availability \(a(i, k)\) which is the accumulated evidence that sample \(i\) should choose sample \(k\) to be its exemplar, and considers the values for all other samples that \(k\) should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves.

More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:

\[r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]\]

Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\). The availability of sample \(k\) to be the exemplar of sample \(i\) is given by:

\[a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i',
k)}]\]

To begin with, all values for \(r\) and \(a\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \(\lambda\) is introduced to iteration process:

\[r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)\]

\[a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)\]

where \(t\) indicates the iteration times.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_affinity\_propagation.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_affinity\_propagation.py): Affinity Propagation on a synthetic 2D datasets with 3 classes
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_stock\_market.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_stock\_market.py) Affinity Propagation on financial time series to find groups of companies

## Mean Shift

<span class="title-ref">MeanShift</span> clustering aims to discover *blobs* in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.

<div class="dropdown">

Mathematical details

The position of centroid candidates is iteratively adjusted using a technique called hill climbing, which finds local maxima of the estimated probability density. Given a candidate centroid \(x\) for iteration \(t\), the candidate is updated according to the following equation:

\[x^{t+1} = x^t + m(x^t)\]

Where \(m\) is the *mean shift* vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. To compute \(m\) we define \(N(x)\) as the neighborhood of samples within a given distance around \(x\). Then \(m\) is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:

\[m(x) = \frac{1}{|N(x)|} \sum_{x_j \in N(x)}x_j - x\]

In general, the equation for \(m\) depends on a kernel used for density estimation. The generic formula is:

\[m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j -
x)} - x\]

In our implementation, \(K(x)\) is equal to 1 if \(x\) is small enough and is equal to 0 otherwise. Effectively \(K(y - x)\) indicates whether \(y\) is in the neighborhood of \(x\).

</div>

The algorithm automatically sets the number of clusters, instead of relying on a parameter `bandwidth`, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided `estimate_bandwidth` function, which is called if the bandwidth is not set.

The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.

Labelling a new sample is performed by finding the nearest centroid for a given sample.

![](../auto_examples/cluster/images/sphx_glr_plot_mean_shift_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_mean\_shift.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_mean\_shift.py): Mean Shift clustering on a synthetic 2D datasets with 3 classes.

<div class="dropdown">

References

  - `"Mean shift: A robust approach toward feature space analysis"
    <10.1109/34.1000236>` D. Comaniciu and P. Meer, *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2002)

</div>

## Spectral clustering

<span class="title-ref">SpectralClustering</span> performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space. It is especially computationally efficient if the affinity matrix is sparse and the <span class="title-ref">amg</span> solver is used for the eigenvalue problem (Note, the <span class="title-ref">amg</span> solver requires that the [pyamg](https://github.com/pyamg/pyamg) module is installed.)

The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters.

For two clusters, SpectralClustering solves a convex relaxation of the [normalized cuts](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf) problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image.

<div class="centered">

[![noisy\_img](../auto_examples/cluster/images/sphx_glr_plot_segmentation_toy_001.png)](../auto_examples/cluster/plot_segmentation_toy.html) [![segmented\_img](../auto_examples/cluster/images/sphx_glr_plot_segmentation_toy_002.png)](../auto_examples/cluster/plot_segmentation_toy.html)

</div>

<div class="warning">

<div class="title">

Warning

</div>

Transforming distance to well-behaved similarities

Note that if the values of your similarity matrix are not well distributed, e.g. with negative values or with a distance matrix rather than a similarity, the spectral problem will be singular and the problem not solvable. In which case it is advised to apply a transformation to the entries of the matrix. For instance, in the case of a signed distance matrix, is common to apply a heat kernel:

    similarity = np.exp(-beta * distance / distance.std())

See the examples for such an application.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_segmentation\_toy.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_segmentation\_toy.py): Segmenting objects from a noisy background using spectral clustering.
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_segmentation.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_segmentation.py): Spectral clustering to split the image of coins in regions.

### Different label assignment strategies

Different label assignment strategies can be used, corresponding to the `assign_labels` parameter of <span class="title-ref">SpectralClustering</span>. `"kmeans"` strategy can match finer details, but can be unstable. In particular, unless you control the `random_state`, it may not be reproducible from run-to-run, as it depends on random initialization. The alternative `"discretize"` strategy is 100% reproducible, but tends to create parcels of fairly even and geometrical shape. The recently added `"cluster_qr"` option is a deterministic alternative that tends to create the visually best partitioning on the example application below.

<table>
<thead>
<tr class="header">
<th><code>assign_labels="kmeans"</code></th>
<th><code>assign_labels="discretize"</code></th>
<th><code>assign_labels="cluster_qr"</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="../auto_examples/cluster/plot_coin_segmentation.html"><img src="../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_001.png" alt="coin_kmeans" /></a></td>
<td><blockquote>
<p><a href="../auto_examples/cluster/plot_coin_segmentation.html"><img src="../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_002.png" alt="coin_discretize" /></a></p>
</blockquote></td>
<td><blockquote>
<p><a href="../auto_examples/cluster/plot_coin_segmentation.html"><img src="../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_003.png" alt="coin_cluster_qr" /></a></p>
</blockquote></td>
</tr>
</tbody>
</table>

<div class="dropdown">

References

  - ["Multiclass spectral clustering"](https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf) Stella X. Yu, Jianbo Shi, 2003
  - `"Simple, direct, and efficient multi-way spectral clustering"<10.1093/imaiai/iay008>` Anil Damle, Victor Minden, Lexing Ying, 2019

</div>

### Spectral Clustering Graphs

Spectral Clustering can also be used to partition graphs via their spectral embeddings. In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with \`affinity='precomputed'\`:

    >>> from sklearn.cluster import SpectralClustering
    >>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,
    ...                         assign_labels='discretize')
    >>> sc.fit_predict(adjacency_matrix)  # doctest: +SKIP

<div class="dropdown">

References

  - `"A Tutorial on Spectral Clustering" <10.1007/s11222-007-9033-z>` Ulrike von Luxburg, 2007
  - `"Normalized cuts and image segmentation" <10.1109/34.868688>` Jianbo Shi, Jitendra Malik, 2000
  - ["A Random Walks View of Spectral Segmentation"](https://citeseerx.ist.psu.edu/doc_view/pid/84a86a69315e994cfd1e0c7debb86d62d7bd1f44) Marina Meila, Jianbo Shi, 2001
  - ["On Spectral Clustering: Analysis and an algorithm"](https://citeseerx.ist.psu.edu/doc_view/pid/796c5d6336fc52aa84db575fb821c78918b65f58) Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
  - `"Preconditioned Spectral Clustering for Stochastic Block Partition
    Streaming Graph Challenge" <1708.07481>` David Zhuzhunashvili, Andrew Knyazev

</div>

## Hierarchical clustering

Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the [Wikipedia page](https://en.wikipedia.org/wiki/Hierarchical_clustering) for more details.

The <span class="title-ref">AgglomerativeClustering</span> object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:

  - **Ward** minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.
  - **Maximum** or **complete linkage** minimizes the maximum distance between observations of pairs of clusters.
  - **Average linkage** minimizes the average of the distances between all observations of pairs of clusters.
  - **Single linkage** minimizes the distance between the closest observations of pairs of clusters.

<span class="title-ref">AgglomerativeClustering</span> can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges.

<div class="topic">

**<span class="title-ref">FeatureAgglomeration</span>**

The <span class="title-ref">FeatureAgglomeration</span> uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see \[data\_reduction\](\#data\_reduction).

</div>

### Different linkage type: Ward, complete, average, and single linkage

<span class="title-ref">AgglomerativeClustering</span> supports Ward, single, average, and complete linkage strategies.

[![image](../auto_examples/cluster/images/sphx_glr_plot_linkage_comparison_001.png)](../auto_examples/cluster/plot_linkage_comparison.html)

Agglomerative cluster has a "rich get richer" behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data.

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_digits\_linkage.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_digits\_linkage.py): exploration of the different linkage strategies in a real dataset.
      - \[sphx\_glr\_auto\_examples\_cluster\_plot\_linkage\_comparison.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_linkage\_comparison.py): exploration of the different linkage strategies in toy datasets.

### Visualization of cluster hierarchy

It's possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes.

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_dendrogram_001.png)](../auto_examples/cluster/plot_agglomerative_dendrogram.html)

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_dendrogram.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_dendrogram.py)

### Adding connectivity constraints

An interesting aspect of <span class="title-ref">AgglomerativeClustering</span> is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll.

<div class="centered">

[![unstructured](../auto_examples/cluster/images/sphx_glr_plot_ward_structured_vs_unstructured_001.png)](../auto_examples/cluster/plot_ward_structured_vs_unstructured.html) [![structured](../auto_examples/cluster/images/sphx_glr_plot_ward_structured_vs_unstructured_002.png)](../auto_examples/cluster/plot_ward_structured_vs_unstructured.html)

</div>

These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.

The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using <span class="title-ref">sklearn.neighbors.kneighbors\_graph</span> to restrict merging to nearest neighbors as in \[this example \<sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering.py\>\](\#this-example \<sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering.py\>), or using <span class="title-ref">sklearn.feature\_extraction.image.grid\_to\_graph</span> to enable only merging of neighboring pixels on an image, as in the \[coin \<sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_ward\_segmentation.py\>\](\#coin-\<sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_ward\_segmentation.py\>) example.

<div class="warning">

<div class="title">

Warning

</div>

**Connectivity constraints with single, average and complete linkage**

Connectivity constraints and single, complete or average linkage can enhance the 'rich getting richer' aspect of agglomerative clustering, particularly so if they are built with <span class="title-ref">sklearn.neighbors.kneighbors\_graph</span>. In the limit of a small number of clusters, they tend to give a few macroscopically occupied clusters and almost empty ones. (see the discussion in \[sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering.py)). Single linkage is the most brittle linkage option with regard to this issue.

</div>

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_001.png)](../auto_examples/cluster/plot_agglomerative_clustering.html)

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_002.png)](../auto_examples/cluster/plot_agglomerative_clustering.html)

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_003.png)](../auto_examples/cluster/plot_agglomerative_clustering.html)

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_004.png)](../auto_examples/cluster/plot_agglomerative_clustering.html)

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_ward\_segmentation.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_ward\_segmentation.py): Ward clustering to split the image of coins in regions.
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_ward\_structured\_vs\_unstructured.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_ward\_structured\_vs\_unstructured.py): Example of Ward algorithm on a swiss-roll, comparison of structured approaches versus unstructured approaches.
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_feature\_agglomeration\_vs\_univariate\_selection.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_feature\_agglomeration\_vs\_univariate\_selection.py): Example of dimensionality reduction with feature agglomeration based on Ward hierarchical clustering.
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering.py)

### Varying the metric

Single, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance (*l2*), Manhattan distance (or Cityblock, or *l1*), cosine distance, or any precomputed affinity matrix.

  - *l1* distance is often good for sparse features, or sparse noise: i.e. many of the features are zero, as in text mining using occurrences of rare words.
  - *cosine* distance is interesting because it is invariant to global scalings of the signal.

The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_005.png)](../auto_examples/cluster/plot_agglomerative_clustering_metrics.html)

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_006.png)](../auto_examples/cluster/plot_agglomerative_clustering_metrics.html)

[![image](../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_007.png)](../auto_examples/cluster/plot_agglomerative_clustering_metrics.html)

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering\_metrics.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_agglomerative\_clustering\_metrics.py)

### Bisecting K-Means

<div id="bisect_k_means">

The <span class="title-ref">BisectingKMeans</span> is an iterative variant of <span class="title-ref">KMeans</span>, using divisive hierarchical clustering. Instead of creating all centroids at once, centroids are picked progressively based on a previous clustering: a cluster is split into two new clusters repeatedly until the target number of clusters is reached.

</div>

<span class="title-ref">BisectingKMeans</span> is more efficient than <span class="title-ref">KMeans</span> when the number of clusters is large since it only works on a subset of the data at each bisection while <span class="title-ref">KMeans</span> always works on the entire dataset.

Although <span class="title-ref">BisectingKMeans</span> can't benefit from the advantages of the <span class="title-ref">"k-means++"</span> initialization by design, it will still produce comparable results than <span class="title-ref">KMeans(init="k-means++")</span> in terms of inertia at cheaper computational costs, and will likely produce better results than <span class="title-ref">KMeans</span> with a random initialization.

This variant is more efficient to agglomerative clustering if the number of clusters is small compared to the number of data points.

This variant also does not produce empty clusters.

  - There exist two strategies for selecting the cluster to split:
    
      - `bisecting_strategy="largest_cluster"` selects the cluster having the most points
      - `bisecting_strategy="biggest_inertia"` selects the cluster with biggest inertia (cluster with biggest Sum of Squared Errors within)

Picking by largest amount of data points in most cases produces result as accurate as picking by inertia and is faster (especially for larger amount of data points, where calculating error may be costly).

Picking by largest amount of data points will also likely produce clusters of similar sizes while <span class="title-ref">KMeans</span> is known to produce clusters of different sizes.

Difference between Bisecting K-Means and regular K-Means can be seen on example \[sphx\_glr\_auto\_examples\_cluster\_plot\_bisect\_kmeans.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_bisect\_kmeans.py). While the regular K-Means algorithm tends to create non-related clusters, clusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.

<div class="dropdown">

References

  - ["A Comparison of Document Clustering Techniques"](http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf) Michael Steinbach, George Karypis and Vipin Kumar, Department of Computer Science and Egineering, University of Minnesota (June 2000)
  - ["Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data"](https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf) K.Abirami and Dr.P.Mayilvahanan, International Journal of Emerging Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)
  - ["Bisecting K-means Algorithm Based on K-valued Self-determining and Clustering Center Optimization"](http://www.jcomputers.us/vol13/jcp1306-01.pdf) Jian Di, Xinyue Gou School of Control and Computer Engineering,North China Electric Power University, Baoding, Hebei, China (August 2017)

</div>

## DBSCAN

The <span class="title-ref">DBSCAN</span> algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of *core samples*, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, `min_samples` and `eps`, which define formally what we mean when we say *dense*. Higher `min_samples` or lower `eps` indicate higher density necessary to form a cluster.

More formally, we define a core sample as being a sample in the dataset such that there exist `min_samples` other samples within a distance of `eps`, which are defined as *neighbors* of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of *their* neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.

Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least `eps` in distance from any core sample, is considered an outlier by the algorithm.

While the parameter `min_samples` primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter `eps` is *crucial to choose appropriately* for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as `-1` for "noise"). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below).

In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.

<div class="centered">

[![dbscan\_results](../auto_examples/cluster/images/sphx_glr_plot_dbscan_002.png)](../auto_examples/cluster/plot_dbscan.html)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_dbscan.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_dbscan.py)

<div class="dropdown">

Implementation

The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than `eps` to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than `eps` from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.

The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see <span class="title-ref">NearestNeighbors</span>.

</div>

<div class="dropdown">

Memory consumption for large sample sizes

This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume \(n^2\) floats. A couple of mechanisms for getting around this are:

  - Use \[OPTICS \<optics\>\](\#optics-\<optics\>) clustering in conjunction with the <span class="title-ref">extract\_dbscan</span> method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n).
  - A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with `metric='precomputed'`. See <span class="title-ref">sklearn.neighbors.NearestNeighbors.radius\_neighbors\_graph</span>.
  - The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a `sample_weight` when fitting DBSCAN.

</div>

<div class="dropdown">

References

</div>

  - [A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf) Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
  - `DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
    <10.1145/3068335>` Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu,
    24. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.

## HDBSCAN

The <span class="title-ref">HDBSCAN</span> algorithm can be seen as an extension of <span class="title-ref">DBSCAN</span> and <span class="title-ref">OPTICS</span>. Specifically, <span class="title-ref">DBSCAN</span> assumes that the clustering criterion (i.e. density requirement) is *globally homogeneous*. In other words, <span class="title-ref">DBSCAN</span> may struggle to successfully capture clusters with different densities. <span class="title-ref">HDBSCAN</span> alleviates this assumption and explores all possible density scales by building an alternative representation of the clustering problem.

\> **Note** \> This implementation is adapted from the original implementation of HDBSCAN, [scikit-learn-contrib/hdbscan](https://github.com/scikit-learn-contrib/hdbscan) based on [\[LJ2017\]](#LJ2017).

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_hdbscan.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_hdbscan.py)

### Mutual Reachability Graph

HDBSCAN first defines \(d_c(x_p)\), the *core distance* of a sample \(x_p\), as the distance to its <span class="title-ref">min\_samples</span> th-nearest neighbor, counting itself. For example, if <span class="title-ref">min\_samples=5</span> and \(x_*\) is the 5th-nearest neighbor of \(x_p\) then the core distance is:

\[d_c(x_p)=d(x_p, x_*).\]

Next it defines \(d_m(x_p, x_q)\), the *mutual reachability distance* of two points \(x_p, x_q\), as:

\[d_m(x_p, x_q) = \max\{d_c(x_p), d_c(x_q), d(x_p, x_q)\}\]

These two notions allow us to construct the *mutual reachability graph* \(G_{ms}\) defined for a fixed choice of <span class="title-ref">min\_samples</span> by associating each sample \(x_p\) with a vertex of the graph, and thus edges between points \(x_p, x_q\) are the mutual reachability distance \(d_m(x_p, x_q)\) between them. We may build subsets of this graph, denoted as \(G_{ms,\varepsilon}\), by removing any edges with value greater than \(\varepsilon\): from the original graph. Any points whose core distance is less than \(\varepsilon\): are at this staged marked as noise. The remaining points are then clustered by finding the connected components of this trimmed graph.

\> **Note** \> Taking the connected components of a trimmed graph \(G_{ms,\varepsilon}\) is equivalent to running DBSCAN\* with <span class="title-ref">min\_samples</span> and \(\varepsilon\). DBSCAN\* is a slightly modified version of DBSCAN mentioned in [\[CM2013\]](#CM2013).

### Hierarchical Clustering

HDBSCAN can be seen as an algorithm which performs DBSCAN\* clustering across all values of \(\varepsilon\). As mentioned prior, this is equivalent to finding the connected components of the mutual reachability graphs for all values of \(\varepsilon\). To do this efficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully -connected mutual reachability graph, then greedily cuts the edges with highest weight. An outline of the HDBSCAN algorithm is as follows:

1.  Extract the MST of \(G_{ms}\).
2.  Extend the MST by adding a "self edge" for each vertex, with weight equal to the core distance of the underlying sample.
3.  Initialize a single cluster and label for the MST.
4.  Remove the edge with the greatest weight from the MST (ties are removed simultaneously).
5.  Assign cluster labels to the connected components which contain the end points of the now-removed edge. If the component does not have at least one edge it is instead assigned a "null" label marking it as noise.
6.  Repeat 4-5 until there are no more connected components.

HDBSCAN is therefore able to obtain all possible partitions achievable by DBSCAN\* for a fixed choice of <span class="title-ref">min\_samples</span> in a hierarchical fashion. Indeed, this allows HDBSCAN to perform clustering across multiple densities and as such it no longer needs \(\varepsilon\) to be given as a hyperparameter. Instead it relies solely on the choice of <span class="title-ref">min\_samples</span>, which tends to be a more robust hyperparameter.

<div class="centered">

[![hdbscan\_ground\_truth](../auto_examples/cluster/images/sphx_glr_plot_hdbscan_005.png)](../auto_examples/cluster/plot_hdbscan.html)

</div>

<div class="centered">

[![hdbscan\_results](../auto_examples/cluster/images/sphx_glr_plot_hdbscan_007.png)](../auto_examples/cluster/plot_hdbscan.html)

</div>

HDBSCAN can be smoothed with an additional hyperparameter <span class="title-ref">min\_cluster\_size</span> which specifies that during the hierarchical clustering, components with fewer than <span class="title-ref">minimum\_cluster\_size</span> many samples are considered noise. In practice, one can set <span class="title-ref">minimum\_cluster\_size = min\_samples</span> to couple the parameters and simplify the hyperparameter space.

**References**

## OPTICS

The <span class="title-ref">OPTICS</span> algorithm shares many similarities with the <span class="title-ref">DBSCAN</span> algorithm, and can be considered a generalization of DBSCAN that relaxes the `eps` requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a *reachability* graph, which assigns each sample both a `reachability_` distance, and a spot within the cluster `ordering_` attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of *inf* set for `max_eps`, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given `eps` value using the `cluster_optics_dbscan` method. Setting `max_eps` to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points.

<div class="centered">

[![optics\_results](../auto_examples/cluster/images/sphx_glr_plot_optics_001.png)](../auto_examples/cluster/plot_optics.html)

</div>

The *reachability* distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining *reachability* distances and data set `ordering_` produces a *reachability plot*, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. 'Cutting' the reachability plot at a single value produces DBSCAN like results; all points above the 'cut' are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter `xi`. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the `cluster_hierarchy_` parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_optics.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_optics.py)

<div class="dropdown">

Comparison with DBSCAN

The results from OPTICS `cluster_optics_dbscan` method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise.

Note that for any single value of `eps`, DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying `eps` values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS' output is close to DBSCAN's only if `eps` and `max_eps` are close.

</div>

<div class="dropdown">

Computational Complexity

Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the `metric` keyword.

For large datasets, similar (but not identical) results can be obtained via <span class="title-ref">HDBSCAN</span>. The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain \(n\) (as opposed to \(n^2\)) memory scaling; however, tuning of the `max_eps` parameter will likely need to be used to give a solution in a reasonable amount of wall time.

</div>

<div class="dropdown">

References

  - "OPTICS: ordering points to identify the clustering structure." Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.

</div>

## BIRCH

The <span class="title-ref">Birch</span> builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.

The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:

  - Number of samples in a subcluster.
  - Linear Sum - An n-dimensional vector holding the sum of all samples
  - Squared Sum - Sum of the squared L2 norm of all samples.
  - Centroids - To avoid recalculation linear sum / n\_samples.
  - Squared norm of the centroids.

The BIRCH algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.

This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by `n_clusters`. If `n_clusters` is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.

<div class="dropdown">

Algorithm description

  - A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions. If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After finding the nearest subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated.
  - If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters.
  - If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root.

</div>

<div class="dropdown">

BIRCH or MiniBatchKMeans?

  - BIRCH does not scale very well to high dimensional data. As a rule of thumb if `n_features` is greater than twenty, it is generally better to use MiniBatchKMeans.
  - If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, BIRCH is more useful than MiniBatchKMeans.

[![image](../auto_examples/cluster/images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png)](../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html)

</div>

<div class="dropdown">

How to use partial\_fit?

To avoid the computation of global clustering, for every call of `partial_fit` the user is advised:

1.  To set `n_clusters=None` initially.
2.  Train all data by multiple calls to partial\_fit.
3.  Set `n_clusters` to a required value using `brc.set_params(n_clusters=n_clusters)`.
4.  Call `partial_fit` finally with no arguments, i.e. `brc.partial_fit()` which performs the global clustering.

</div>

<div class="dropdown">

References

  - Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. <https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf>
  - Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm <https://code.google.com/archive/p/jbirch>

</div>

## Clustering performance evaluation

Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric.

<div class="currentmodule">

sklearn.metrics

</div>

### Rand index<span id="rand_score"></span>

Given the knowledge of the ground truth class assignments `labels_true` and our clustering algorithm assignments of the same samples `labels_pred`, the **(adjusted or unadjusted) Rand index** is a function that measures the **similarity** of the two assignments, ignoring permutations:

    >>> from sklearn import metrics
    >>> labels_true = [0, 0, 0, 1, 1, 1]
    >>> labels_pred = [0, 0, 1, 1, 2, 2]
    >>> metrics.rand_score(labels_true, labels_pred)
    0.66...

The Rand index does not ensure to obtain a value close to 0.0 for a random labelling. The adjusted Rand index **corrects for chance** and will give such a baseline.

> \>\>\> metrics.adjusted\_rand\_score(labels\_true, labels\_pred) 0.24...

As with all clustering metrics, one can permute 0 and 1 in the predicted labels, rename 2 to 3, and get the same score:

    >>> labels_pred = [1, 1, 0, 0, 3, 3]
    >>> metrics.rand_score(labels_true, labels_pred)
    0.66...
    >>> metrics.adjusted_rand_score(labels_true, labels_pred)
    0.24...

Furthermore, both <span class="title-ref">rand\_score</span> <span class="title-ref">adjusted\_rand\_score</span> are **symmetric**: swapping the argument does not change the scores. They can thus be used as **consensus measures**:

    >>> metrics.rand_score(labels_pred, labels_true)
    0.66...
    >>> metrics.adjusted_rand_score(labels_pred, labels_true)
    0.24...

Perfect labeling is scored 1.0:

    >>> labels_pred = labels_true[:]
    >>> metrics.rand_score(labels_true, labels_pred)
    1.0
    >>> metrics.adjusted_rand_score(labels_true, labels_pred)
    1.0

Poorly agreeing labels (e.g. independent labelings) have lower scores, and for the adjusted Rand index the score will be negative or close to zero. However, for the unadjusted Rand index the score, while lower, will not necessarily be close to zero.:

    >>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]
    >>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]
    >>> metrics.rand_score(labels_true, labels_pred)
    0.39...
    >>> metrics.adjusted_rand_score(labels_true, labels_pred)
    -0.07...

<div class="topic">

**Advantages:**

  - **Interpretability**: The unadjusted Rand index is proportional to the number of sample pairs whose labels are the same in both <span class="title-ref">labels\_pred</span> and <span class="title-ref">labels\_true</span>, or are different in both.
  - **Random (uniform) label assignments have an adjusted Rand index score close to 0.0** for any value of `n_clusters` and `n_samples` (which is not the case for the unadjusted Rand index or the V-measure for instance).
  - **Bounded range**: Lower values indicate different labelings, similar clusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the perfect match score. The score range is \[0, 1\] for the unadjusted Rand index and \[-0.5, 1\] for the adjusted Rand index.
  - **No assumption is made on the cluster structure**: The (adjusted or unadjusted) Rand index can be used to compare all kinds of clustering algorithms, and can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with "folded" shapes.

</div>

<div class="topic">

**Drawbacks:**

  - Contrary to inertia, the **(adjusted or unadjusted) Rand index requires knowledge of the ground truth classes** which is almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).
    
    However (adjusted or unadjusted) Rand index can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).

  - The **unadjusted Rand index is often close to 1.0** even if the clusterings themselves differ significantly. This can be understood when interpreting the Rand index as the accuracy of element pair labeling resulting from the clusterings: In practice there often is a majority of element pairs that are assigned the `different` pair label under both the predicted and the ground truth clustering resulting in a high proportion of pair labels that agree, which leads subsequently to a high score.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_adjusted\_for\_chance\_measures.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_adjusted\_for\_chance\_measures.py): Analysis of the impact of the dataset size on the value of clustering measures for random assignments.

<div class="dropdown">

Mathematical formulation

If C is a ground truth class assignment and K the clustering, let us define \(a\) and \(b\) as:

  - \(a\), the number of pairs of elements that are in the same set in C and in the same set in K
  - \(b\), the number of pairs of elements that are in different sets in C and in different sets in K

The unadjusted Rand index is then given by:

\[\text{RI} = \frac{a + b}{C_2^{n_{samples}}}\]

where \(C_2^{n_{samples}}\) is the total number of possible pairs in the dataset. It does not matter if the calculation is performed on ordered pairs or unordered pairs as long as the calculation is performed consistently.

However, the Rand index does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).

To counter this effect we can discount the expected RI \(E[\text{RI}]\) of random labelings by defining the adjusted Rand index as follows:

\[\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}\]

</div>

<div class="dropdown">

References

  - [Comparing Partitions](https://link.springer.com/article/10.1007%2FBF01908075) L. Hubert and P. Arabie, Journal of Classification 1985
  - [Properties of the Hubert-Arabie adjusted Rand index](https://psycnet.apa.org/record/2004-17801-007) D. Steinley, Psychological Methods 2004
  - [Wikipedia entry for the Rand index](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index)
  - `Minimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo <10.1007/s11634-022-00491-w>`

</div>

### Mutual Information based scores

Given the knowledge of the ground truth class assignments `labels_true` and our clustering algorithm assignments of the same samples `labels_pred`, the **Mutual Information** is a function that measures the **agreement** of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, **Normalized Mutual Information (NMI)** and **Adjusted Mutual Information (AMI)**. NMI is often used in the literature, while AMI was proposed more recently and is **normalized against chance**:

    >>> from sklearn import metrics
    >>> labels_true = [0, 0, 0, 1, 1, 1]
    >>> labels_pred = [0, 0, 1, 1, 2, 2]
    
    >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
    0.22504...

One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:

    >>> labels_pred = [1, 1, 0, 0, 3, 3]
    >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
    0.22504...

All, <span class="title-ref">mutual\_info\_score</span>, <span class="title-ref">adjusted\_mutual\_info\_score</span> and <span class="title-ref">normalized\_mutual\_info\_score</span> are symmetric: swapping the argument does not change the score. Thus they can be used as a **consensus measure**:

    >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +SKIP
    0.22504...

Perfect labeling is scored 1.0:

    >>> labels_pred = labels_true[:]
    >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
    1.0
    
    >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
    1.0

This is not true for `mutual_info_score`, which is therefore harder to judge:

    >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
    0.69...

Bad (e.g. independent labelings) have non-positive scores:

    >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
    >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
    >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
    -0.10526...

<div class="topic">

**Advantages:**

  - **Random (uniform) label assignments have a AMI score close to 0.0** for any value of `n_clusters` and `n_samples` (which is not the case for raw Mutual Information or the V-measure for instance).
  - **Upper bound of 1**: Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, an AMI of exactly 1 indicates that the two label assignments are equal (with or without permutation).

</div>

<div class="topic">

**Drawbacks:**

  - Contrary to inertia, **MI-based measures require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).
    
    However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection.

  - NMI and MI are not adjusted against chance.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_adjusted\_for\_chance\_measures.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_adjusted\_for\_chance\_measures.py): Analysis of the impact of the dataset size on the value of clustering measures for random assignments. This example also includes the Adjusted Rand Index.

<div class="dropdown">

Mathematical formulation

Assume two label assignments (of the same N objects), \(U\) and \(V\). Their entropy is the amount of uncertainty for a partition set, defined by:

\[H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))\]

where \(P(i) = |U_i| / N\) is the probability that an object picked at random from \(U\) falls into class \(U_i\). Likewise for \(V\):

\[H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))\]

With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\) and \(V\) is calculated by:

\[\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)\]

where \(P(i, j) = |U_i \cap V_j| / N\) is the probability that an object picked at random falls into both classes \(U_i\) and \(V_j\).

It also can be expressed in set cardinality formulation:

\[\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)\]

The normalized mutual information is defined as

\[\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}\]

This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of "mutual information" between the label assignments.

The expected value for the mutual information can be calculated using the following equation [\[VEB2009\]](). In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).

\[E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}\]

Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:

\[\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}\]

For normalized mutual information and adjusted mutual information, the normalizing value is typically some *generalized* mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides "qualitatively similar behaviours" [\[YAT2016\]](). In our implementation, this is controlled by the `average_method` parameter.

Vinh et al. (2010) named variants of NMI and AMI by their averaging method [\[VEB2010\]](). Their 'sqrt' and 'sum' averages are the geometric and arithmetic means; we use these more broadly common names.

**References**

  - Strehl, Alexander, and Joydeep Ghosh (2002). "Cluster ensembles - a knowledge reuse framework for combining multiple partitions". Journal of Machine Learning Research 3: 583-617. [doi:10.1162/153244303321897735](http://strehl.com/download/strehl-jmlr02.pdf).
  - [Wikipedia entry for the (normalized) Mutual Information](https://en.wikipedia.org/wiki/Mutual_Information)
  - [Wikipedia entry for the Adjusted Mutual Information](https://en.wikipedia.org/wiki/Adjusted_Mutual_Information)

</div>

### Homogeneity, completeness and V-measure

Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.

In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:

  - **homogeneity**: each cluster contains only members of a single class.
  - **completeness**: all members of a given class are assigned to the same cluster.

We can turn those concept as scores <span class="title-ref">homogeneity\_score</span> and <span class="title-ref">completeness\_score</span>. Both are bounded below by 0.0 and above by 1.0 (higher is better):

    >>> from sklearn import metrics
    >>> labels_true = [0, 0, 0, 1, 1, 1]
    >>> labels_pred = [0, 0, 1, 1, 2, 2]
    
    >>> metrics.homogeneity_score(labels_true, labels_pred)
    0.66...
    
    >>> metrics.completeness_score(labels_true, labels_pred)
    0.42...

Their harmonic mean called **V-measure** is computed by \`v\_measure\_score\`:

    >>> metrics.v_measure_score(labels_true, labels_pred)
    0.51...

This function's formula is as follows:

\[v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}\]

<span class="title-ref">beta</span> defaults to a value of 1.0, but for using a value less than 1 for beta:

    >>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)
    0.54...

more weight will be attributed to homogeneity, and using a value greater than 1:

    >>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)
    0.48...

more weight will be attributed to completeness.

The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean [\[B2011\]](#B2011).

Homogeneity, completeness and V-measure can be computed at once using <span class="title-ref">homogeneity\_completeness\_v\_measure</span> as follows:

    >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
    (0.66..., 0.42..., 0.51...)

The following clustering assignment is slightly better, since it is homogeneous but not complete:

    >>> labels_pred = [0, 0, 0, 1, 2, 2]
    >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
    (1.0, 0.68..., 0.81...)

\> **Note** \> <span class="title-ref">v\_measure\_score</span> is **symmetric**: it can be used to evaluate the **agreement** of two independent assignments on the same dataset.

> This is not the case for <span class="title-ref">completeness\_score</span> and \`homogeneity\_score\`: both are bound by the relationship:
> 
>     homogeneity_score(a, b) == completeness_score(b, a)

<div class="topic">

**Advantages:**

  - **Bounded scores**: 0.0 is as bad as it can be, 1.0 is a perfect score.
  - Intuitive interpretation: clustering with bad V-measure can be **qualitatively analyzed in terms of homogeneity and completeness** to better feel what 'kind' of mistakes is done by the assignment.
  - **No assumption is made on the cluster structure**: can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with "folded" shapes.

</div>

<div class="topic">

**Drawbacks:**

  - The previously introduced metrics are **not normalized with regards to random labeling**: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular **random labeling won't yield zero scores especially when the number of clusters is large**.
    
    This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. **For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)**.

![](../auto_examples/cluster/images/sphx_glr_plot_adjusted_for_chance_measures_001.png)

  - These metrics **require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_adjusted\_for\_chance\_measures.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_adjusted\_for\_chance\_measures.py): Analysis of the impact of the dataset size on the value of clustering measures for random assignments.

<div class="dropdown">

Mathematical formulation

Homogeneity and completeness scores are formally given by:

\[h = 1 - \frac{H(C|K)}{H(C)}\]

\[c = 1 - \frac{H(K|C)}{H(K)}\]

where \(H(C|K)\) is the **conditional entropy of the classes given the cluster assignments** and is given by:

\[H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)\]

and \(H(C)\) is the **entropy of the classes** and is given by:

\[H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)\]

with \(n\) the total number of samples, \(n_c\) and \(n_k\) the number of samples respectively belonging to class \(c\) and cluster \(k\), and finally \(n_{c,k}\) the number of samples from class \(c\) assigned to cluster \(k\).

The **conditional entropy of clusters given class** \(H(K|C)\) and the **entropy of clusters** \(H(K)\) are defined in a symmetric manner.

Rosenberg and Hirschberg further define **V-measure** as the **harmonic mean of homogeneity and completeness**:

\[v = 2 \cdot \frac{h \cdot c}{h + c}\]

</div>

**References**

  - [V-Measure: A conditional entropy-based external cluster evaluation measure](https://aclweb.org/anthology/D/D07/D07-1043.pdf) Andrew Rosenberg and Julia Hirschberg, 2007

### Fowlkes-Mallows scores

The original Fowlkes-Mallows index (FMI) was intended to measure the similarity between two clustering results, which is inherently an unsupervised comparison. The supervised adaptation of the Fowlkes-Mallows index (as implemented in <span class="title-ref">sklearn.metrics.fowlkes\_mallows\_score</span>) can be used when the ground truth class assignments of the samples are known. The FMI is defined as the geometric mean of the pairwise precision and recall:

\[\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}\]

In the above formula:

  - `TP` (**True Positive**): The number of pairs of points that are clustered together both in the true labels and in the predicted labels.
  - `FP` (**False Positive**): The number of pairs of points that are clustered together in the predicted labels but not in the true labels.
  - `FN` (**False Negative**): The number of pairs of points that are clustered together in the true labels but not in the predicted labels.

The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.

> \>\>\> from sklearn import metrics \>\>\> labels\_true = \[0, 0, 0, 1, 1, 1\] \>\>\> labels\_pred = \[0, 0, 1, 1, 2, 2\]
> 
> \>\>\> metrics.fowlkes\_mallows\_score(labels\_true, labels\_pred) 0.47140...

One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:

    >>> labels_pred = [1, 1, 0, 0, 3, 3]
    
    >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
    0.47140...

Perfect labeling is scored 1.0:

    >>> labels_pred = labels_true[:]
    >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
    1.0

Bad (e.g. independent labelings) have zero scores:

    >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
    >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
    >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
    0.0

<div class="topic">

**Advantages:**

  - **Random (uniform) label assignments have a FMI score close to 0.0** for any value of `n_clusters` and `n_samples` (which is not the case for raw Mutual Information or the V-measure for instance).
  - **Upper-bounded at 1**: Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, values of exactly 0 indicate **purely** independent label assignments and a FMI of exactly 1 indicates that the two label assignments are equal (with or without permutation).
  - **No assumption is made on the cluster structure**: can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with "folded" shapes.

</div>

<div class="topic">

**Drawbacks:**

  - Contrary to inertia, **FMI-based measures require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).

</div>

<div class="dropdown">

References

  - E. B. Fowkles and C. L. Mallows, 1983. "A method for comparing two hierarchical clusterings". Journal of the American Statistical Association. <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>
  - [Wikipedia entry for the Fowlkes-Mallows Index](https://en.wikipedia.org/wiki/Fowlkes-Mallows_index)

</div>

### Silhouette Coefficient

If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (<span class="title-ref">sklearn.metrics.silhouette\_score</span>) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:

  - **a**: The mean distance between a sample and all other points in the same class.
  - **b**: The mean distance between a sample and all other points in the *next nearest cluster*.

The Silhouette Coefficient *s* for a single sample is then given as:

\[s = \frac{b - a}{max(a, b)}\]

The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.

> \>\>\> from sklearn import metrics \>\>\> from sklearn.metrics import pairwise\_distances \>\>\> from sklearn import datasets \>\>\> X, y = datasets.load\_iris(return\_X\_y=True)

In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.

> \>\>\> import numpy as np \>\>\> from sklearn.cluster import KMeans \>\>\> kmeans\_model = KMeans(n\_clusters=3, random\_state=1).fit(X) \>\>\> labels = [kmeans\_model.labels]() \>\>\> metrics.silhouette\_score(X, labels, metric='euclidean') 0.55...

<div class="topic">

**Advantages:**

  - The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.
  - The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.

</div>

<div class="topic">

**Drawbacks:**

  - The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_silhouette\_analysis.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_silhouette\_analysis.py) : In this example the silhouette analysis is used to choose an optimal value for n\_clusters.

<div class="dropdown">

References

  - Peter J. Rousseeuw (1987). `"Silhouettes: a Graphical Aid to the
    Interpretation and Validation of Cluster Analysis"<10.1016/0377-0427(87)90125-7>`. Computational and Applied Mathematics 20: 53-65.

</div>

### Calinski-Harabasz Index

If the ground truth labels are not known, the Calinski-Harabasz index (<span class="title-ref">sklearn.metrics.calinski\_harabasz\_score</span>) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters.

The index is the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):

> \>\>\> from sklearn import metrics \>\>\> from sklearn.metrics import pairwise\_distances \>\>\> from sklearn import datasets \>\>\> X, y = datasets.load\_iris(return\_X\_y=True)

In normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis:

> \>\>\> import numpy as np \>\>\> from sklearn.cluster import KMeans \>\>\> kmeans\_model = KMeans(n\_clusters=3, random\_state=1).fit(X) \>\>\> labels = [kmeans\_model.labels]() \>\>\> metrics.calinski\_harabasz\_score(X, labels) 561.59...

<div class="topic">

**Advantages:**

  - The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.
  - The score is fast to compute.

</div>

<div class="topic">

**Drawbacks:**

  - The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.

</div>

<div class="dropdown">

Mathematical formulation

For a set of data \(E\) of size \(n_E\) which has been clustered into \(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:

\[s = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1}\]

where \(\mathrm{tr}(B_k)\) is trace of the between group dispersion matrix and \(\mathrm{tr}(W_k)\) is the trace of the within-cluster dispersion matrix defined by:

\[W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T\]

\[B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\]

with \(C_q\) the set of points in cluster \(q\), \(c_q\) the center of cluster \(q\), \(c_E\) the center of \(E\), and \(n_q\) the number of points in cluster \(q\).

</div>

<div class="dropdown">

References

  - Caliński, T., & Harabasz, J. (1974). ["A Dendrite Method for Cluster Analysis"](https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis). `Communications in Statistics-theory and Methods 3: 1-27
    <10.1080/03610927408827101>`.

</div>

### Davies-Bouldin Index

If the ground truth labels are not known, the Davies-Bouldin index (<span class="title-ref">sklearn.metrics.davies\_bouldin\_score</span>) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.

This index signifies the average 'similarity' between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves.

Zero is the lowest possible score. Values closer to zero indicate a better partition.

In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:

> \>\>\> from sklearn import datasets \>\>\> iris = datasets.load\_iris() \>\>\> X = iris.data \>\>\> from sklearn.cluster import KMeans \>\>\> from sklearn.metrics import davies\_bouldin\_score \>\>\> kmeans = KMeans(n\_clusters=3, random\_state=1).fit(X) \>\>\> labels = [kmeans.labels]() \>\>\> davies\_bouldin\_score(X, labels) 0.666...

<div class="topic">

**Advantages:**

  - The computation of Davies-Bouldin is simpler than that of Silhouette scores.
  - The index is solely based on quantities and features inherent to the dataset as its computation only uses point-wise distances.

</div>

<div class="topic">

**Drawbacks:**

  - The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.
  - The usage of centroid distance limits the distance metric to Euclidean space.

</div>

<div class="dropdown">

Mathematical formulation

The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:

  - \(s_i\), the average distance between each point of cluster \(i\) and the centroid of that cluster -- also know as cluster diameter.
  - \(d_{ij}\), the distance between cluster centroids \(i\) and \(j\).

A simple choice to construct \(R_{ij}\) so that it is nonnegative and symmetric is:

\[R_{ij} = \frac{s_i + s_j}{d_{ij}}\]

Then the Davies-Bouldin index is defined as:

\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]

</div>

<div class="dropdown">

References

  - Davies, David L.; Bouldin, Donald W. (1979). `"A Cluster Separation
    Measure" <10.1109/TPAMI.1979.4766909>` IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227.
  - Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). `"On
    Clustering Validation Techniques" <10.1023/A:1012801612483>` Journal of Intelligent Information Systems, 17(2-3), 107-145.
  - [Wikipedia entry for Davies-Bouldin index](https://en.wikipedia.org/wiki/Davies-Bouldin_index).

</div>

### Contingency Matrix

Contingency matrix (<span class="title-ref">sklearn.metrics.cluster.contingency\_matrix</span>) reports the intersection cardinality for every true/predicted cluster pair. The contingency matrix provides sufficient statistics for all clustering metrics where the samples are independent and identically distributed and one doesn't need to account for some instances not being clustered.

Here is an example:

    >>> from sklearn.metrics.cluster import contingency_matrix
    >>> x = ["a", "a", "a", "b", "b", "b"]
    >>> y = [0, 0, 1, 1, 2, 2]
    >>> contingency_matrix(x, y)
    array([[2, 1, 0],
           [0, 1, 2]])

The first row of output array indicates that there are three samples whose true cluster is "a". Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is "b". Of them, none is in predicted cluster 0, one is in 1 and two are in 2.

A \[confusion matrix \<confusion\_matrix\>\](\#confusion-matrix-\<confusion\_matrix\>) for classification is a square contingency matrix where the order of rows and columns correspond to a list of classes.

<div class="topic">

**Advantages:**

  - Allows to examine the spread of each true cluster across predicted clusters and vice versa.
  - The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.

</div>

<div class="topic">

**Drawbacks:**

  - Contingency matrix is easy to interpret for a small number of clusters, but becomes very hard to interpret for a large number of clusters.
  - It doesn't give a single metric to use as an objective for clustering optimisation.

</div>

<div class="dropdown">

References

  - [Wikipedia entry for contingency matrix](https://en.wikipedia.org/wiki/Contingency_table)

</div>

### Pair Confusion Matrix

The pair confusion matrix (<span class="title-ref">sklearn.metrics.cluster.pair\_confusion\_matrix</span>) is a 2x2 similarity matrix

\[\begin{aligned}
C = \left[\begin{matrix}
C_{00} & C_{01} \\
C_{10} & C_{11}
\end{matrix}\right]
\end{aligned}\]

between two clusterings computed by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings.

It has the following entries:

\(C_{00}\) : number of pairs with both clusterings having the samples not clustered together

\(C_{10}\) : number of pairs with the true label clustering having the samples clustered together but the other clustering not having the samples clustered together

\(C_{01}\) : number of pairs with the true label clustering not having the samples clustered together but the other clustering having the samples clustered together

\(C_{11}\) : number of pairs with both clusterings having the samples clustered together

Considering a pair of samples that is clustered together a positive pair, then as in binary classification the count of true negatives is \(C_{00}\), false negatives is \(C_{10}\), true positives is \(C_{11}\) and false positives is \(C_{01}\).

Perfectly matching labelings have all non-zero entries on the diagonal regardless of actual label values:

    >>> from sklearn.metrics.cluster import pair_confusion_matrix
    >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])
    array([[8, 0],
           [0, 4]])

    >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
    array([[8, 0],
           [0, 4]])

Labelings that assign all classes members to the same clusters are complete but may not always be pure, hence penalized, and have some off-diagonal non-zero entries:

    >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
    array([[8, 2],
           [0, 2]])

The matrix is not symmetric:

    >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])
    array([[8, 0],
           [2, 2]])

If classes members are completely split across different clusters, the assignment is totally incomplete, hence the matrix has all zero diagonal entries:

    >>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])
    array([[ 0,  0],
           [12,  0]])

<div class="dropdown">

References

  - `"Comparing Partitions" <10.1007/BF01908075>` L. Hubert and P. Arabie, Journal of Classification 1985

</div>

<div id="citations">

  - <span id="B2011" class="citation-label">B2011</span>  
    [Identification and Characterization of Events in Social Media](http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf), Hila Becker, PhD Thesis.

  - <span id="CM2013" class="citation-label">CM2013</span>  
    Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based Clustering Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S., Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer, Berlin, Heidelberg. `Density-Based Clustering Based on Hierarchical
    Density Estimates <10.1007/978-3-642-37456-2_14>`

  - <span id="LJ2017" class="citation-label">LJ2017</span>  
    L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density Based Clustering. In: IEEE International Conference on Data Mining Workshops (ICDMW), 2017, pp. 33-42. `Accelerated Hierarchical Density Based
    Clustering <10.1109/ICDMW.2017.12>`

</div>

---

compose.md

---

# Pipelines and composite estimators

To build a composite estimator, transformers are usually combined with other transformers or with `predictors` (such as classifiers or regressors). The most common tool used for composing estimators is a \[Pipeline \<pipeline\>\](\#pipeline \<pipeline\>). Pipelines require all steps except the last to be a `transformer`. The last step can be anything, a transformer, a `predictor`, or a clustering estimator which might have or not have a <span class="title-ref">.predict(...)</span> method. A pipeline exposes all methods provided by the last estimator: if the last step provides a <span class="title-ref">transform</span> method, then the pipeline would have a <span class="title-ref">transform</span> method and behave like a transformer. If the last step provides a <span class="title-ref">predict</span> method, then the pipeline would expose that method, and given a data `X`, use all steps except the last to transform the data, and then give that transformed data to the <span class="title-ref">predict</span> method of the last step of the pipeline. The class <span class="title-ref">Pipeline</span> is often used in combination with \[ColumnTransformer \<column\_transformer\>\](\#columntransformer-\<column\_transformer\>) or \[FeatureUnion \<feature\_union\>\](\#featureunion-\<feature\_union\>) which concatenate the output of transformers into a composite feature space. \[TransformedTargetRegressor \<transformed\_target\_regressor\>\](\#transformedtargetregressor-\<transformed\_target\_regressor\>) deals with transforming the `target` (i.e. log-transform `y`).

## Pipeline: chaining estimators

<div class="currentmodule">

sklearn.pipeline

</div>

<span class="title-ref">Pipeline</span> can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. <span class="title-ref">Pipeline</span> serves multiple purposes here:

  - Convenience and encapsulation  
    You only have to call `fit` and `predict` once on your data to fit a whole sequence of estimators.

  - Joint parameter selection  
    You can \[grid search \<grid\_search\>\](\#grid-search-\<grid\_search\>) over parameters of all estimators in the pipeline at once.

  - Safety  
    Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.

All estimators in a pipeline, except the last one, must be transformers (i.e. must have a `transform` method). The last estimator may be any type (transformer, classifier, etc.).

\> **Note** \> Calling `fit` on the pipeline is the same as calling `fit` on each estimator in turn, `transform` the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the <span class="title-ref">Pipeline</span> can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.

### Usage

#### Build a pipeline

The <span class="title-ref">Pipeline</span> is built using a list of `(key, value)` pairs, where the `key` is a string containing the name you want to give this step and `value` is an estimator object:

    >>> from sklearn.pipeline import Pipeline
    >>> from sklearn.svm import SVC
    >>> from sklearn.decomposition import PCA
    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
    >>> pipe = Pipeline(estimators)
    >>> pipe
    Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])

<div class="dropdown">

Shorthand version using <span class="title-ref">make\_pipeline</span>

The utility function <span class="title-ref">make\_pipeline</span> is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:

    >>> from sklearn.pipeline import make_pipeline
    >>> make_pipeline(PCA(), SVC())
    Pipeline(steps=[('pca', PCA()), ('svc', SVC())])

</div>

#### Access pipeline steps

The estimators of a pipeline are stored as a list in the `steps` attribute. A sub-pipeline can be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse):

> \>\>\> pipe\[:1\] Pipeline(steps=\[('reduce\_dim', PCA())\]) \>\>\> pipe\[-1:\] Pipeline(steps=\[('clf', SVC())\])

<div class="dropdown">

Accessing a step by name or position

A specific step can also be accessed by index or name by indexing (with `[idx]`) the pipeline:

    >>> pipe.steps[0]
    ('reduce_dim', PCA())
    >>> pipe[0]
    PCA()
    >>> pipe['reduce_dim']
    PCA()

<span class="title-ref">Pipeline</span>'s <span class="title-ref">named\_steps</span> attribute allows accessing steps by name with tab completion in interactive environments:

    >>> pipe.named_steps.reduce_dim is pipe['reduce_dim']
    True

</div>

#### Tracking feature names in a pipeline

To enable model inspection, <span class="title-ref">\~sklearn.pipeline.Pipeline</span> has a `get_feature_names_out()` method, just like all transformers. You can use pipeline slicing to get the feature names going into each step:

    >>> from sklearn.datasets import load_iris
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.feature_selection import SelectKBest
    >>> iris = load_iris()
    >>> pipe = Pipeline(steps=[
    ...    ('select', SelectKBest(k=2)),
    ...    ('clf', LogisticRegression())])
    >>> pipe.fit(iris.data, iris.target)
    Pipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))])
    >>> pipe[:-1].get_feature_names_out()
    array(['x2', 'x3'], ...)

<div class="dropdown">

Customize feature names

You can also provide custom feature names for the input data using `get_feature_names_out`:

    >>> pipe[:-1].get_feature_names_out(iris.feature_names)
    array(['petal length (cm)', 'petal width (cm)'], ...)

</div>

#### Access to nested parameters

It is common to adjust the parameters of an estimator within a pipeline. This parameter is therefore nested because it belongs to a particular sub-step. Parameters of the estimators in the pipeline are accessible using the `<estimator>__<parameter>` syntax:

    >>> pipe = Pipeline(steps=[("reduce_dim", PCA()), ("clf", SVC())])
    >>> pipe.set_params(clf__C=10)
    Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])

<div class="dropdown">

When does it matter?

This is particularly important for doing grid searches:

    >>> from sklearn.model_selection import GridSearchCV
    >>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],
    ...                   clf__C=[0.1, 10, 100])
    >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)

Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to `'passthrough'`:

    >>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],
    ...                   clf=[SVC(), LogisticRegression()],
    ...                   clf__C=[0.1, 10, 100])
    >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)

<div class="seealso">

  - \[composite\_grid\_search\](\#composite\_grid\_search)

</div>

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_feature\_selection\_pipeline.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_feature\_selection\_pipeline.py)
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py)
  - \[sphx\_glr\_auto\_examples\_compose\_plot\_digits\_pipe.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_digits\_pipe.py)
  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_kernel\_approximation.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_kernel\_approximation.py)
  - \[sphx\_glr\_auto\_examples\_svm\_plot\_svm\_anova.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_svm\_anova.py)
  - \[sphx\_glr\_auto\_examples\_compose\_plot\_compare\_reduction.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_compare\_reduction.py)
  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_pipeline\_display.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_pipeline\_display.py)

### Caching transformers: avoid repeated computation

<div class="currentmodule">

sklearn.pipeline

</div>

Fitting transformers may be computationally expensive. With its `memory` parameter set, <span class="title-ref">Pipeline</span> will cache each transformer after calling `fit`. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration. The last step will never be cached, even if it is a transformer.

The parameter `memory` is needed in order to cache the transformers. `memory` can be either a string containing the directory where to cache the transformers or a [joblib.Memory](https://joblib.readthedocs.io/en/latest/memory.html) object:

    >>> from tempfile import mkdtemp
    >>> from shutil import rmtree
    >>> from sklearn.decomposition import PCA
    >>> from sklearn.svm import SVC
    >>> from sklearn.pipeline import Pipeline
    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
    >>> cachedir = mkdtemp()
    >>> pipe = Pipeline(estimators, memory=cachedir)
    >>> pipe
    Pipeline(memory=...,
             steps=[('reduce_dim', PCA()), ('clf', SVC())])
    >>> # Clear the cache directory when you don't need it anymore
    >>> rmtree(cachedir)

<div class="dropdown" color="warning">

Side effect of caching transformers

Using a <span class="title-ref">Pipeline</span> without cache enabled, it is possible to inspect the original instance such as:

    >>> from sklearn.datasets import load_digits
    >>> X_digits, y_digits = load_digits(return_X_y=True)
    >>> pca1 = PCA(n_components=10)
    >>> svm1 = SVC()
    >>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])
    >>> pipe.fit(X_digits, y_digits)
    Pipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])
    >>> # The pca instance can be inspected directly
    >>> pca1.components_.shape
    (10, 64)

Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. In following example, accessing the <span class="title-ref">\~sklearn.decomposition.PCA</span> instance `pca2` will raise an `AttributeError` since `pca2` will be an unfitted transformer. Instead, use the attribute `named_steps` to inspect estimators within the pipeline:

    >>> cachedir = mkdtemp()
    >>> pca2 = PCA(n_components=10)
    >>> svm2 = SVC()
    >>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],
    ...                        memory=cachedir)
    >>> cached_pipe.fit(X_digits, y_digits)
    Pipeline(memory=...,
             steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])
    >>> cached_pipe.named_steps['reduce_dim'].components_.shape
    (10, 64)
    >>> # Remove the cache directory
    >>> rmtree(cachedir)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_compose\_plot\_compare\_reduction.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_compare\_reduction.py)

## Transforming target in regression

<span class="title-ref">\~sklearn.compose.TransformedTargetRegressor</span> transforms the targets `y` before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable:

    >>> import numpy as np
    >>> from sklearn.datasets import fetch_california_housing
    >>> from sklearn.compose import TransformedTargetRegressor
    >>> from sklearn.preprocessing import QuantileTransformer
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.model_selection import train_test_split
    >>> X, y = fetch_california_housing(return_X_y=True)
    >>> X, y = X[:2000, :], y[:2000]  # select a subset of data
    >>> transformer = QuantileTransformer(output_distribution='normal')
    >>> regressor = LinearRegression()
    >>> regr = TransformedTargetRegressor(regressor=regressor,
    ...                                   transformer=transformer)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    >>> regr.fit(X_train, y_train)
    TransformedTargetRegressor(...)
    >>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
    R2 score: 0.61
    >>> raw_target_regr = LinearRegression().fit(X_train, y_train)
    >>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))
    R2 score: 0.59

For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:

    >>> def func(x):
    ...     return np.log(x)
    >>> def inverse_func(x):
    ...     return np.exp(x)

Subsequently, the object is created as:

    >>> regr = TransformedTargetRegressor(regressor=regressor,
    ...                                   func=func,
    ...                                   inverse_func=inverse_func)
    >>> regr.fit(X_train, y_train)
    TransformedTargetRegressor(...)
    >>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
    R2 score: 0.51

By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting `check_inverse` to `False`:

    >>> def inverse_func(x):
    ...     return x
    >>> regr = TransformedTargetRegressor(regressor=regressor,
    ...                                   func=func,
    ...                                   inverse_func=inverse_func,
    ...                                   check_inverse=False)
    >>> regr.fit(X_train, y_train)
    TransformedTargetRegressor(...)
    >>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
    R2 score: -1.57

\> **Note** \> The transformation can be triggered by setting either `transformer` or the pair of functions `func` and `inverse_func`. However, setting both options will raise an error.

**Examples**

  - \[sphx\_glr\_auto\_examples\_compose\_plot\_transformed\_target.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_transformed\_target.py)

## FeatureUnion: composite feature spaces

<div class="currentmodule">

sklearn.pipeline

</div>

<span class="title-ref">FeatureUnion</span> combines several transformer objects into a new transformer that combines their output. A <span class="title-ref">FeatureUnion</span> takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.

When you want to apply different transformations to each field of the data, see the related class <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> (see \[user guide \<column\_transformer\>\](\#user-guide-\<column\_transformer\>)).

<span class="title-ref">FeatureUnion</span> serves the same purposes as <span class="title-ref">Pipeline</span> -convenience and joint parameter estimation and validation.

<span class="title-ref">FeatureUnion</span> and <span class="title-ref">Pipeline</span> can be combined to create complex models.

(A <span class="title-ref">FeatureUnion</span> has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are is the caller's responsibility.)

### Usage

A <span class="title-ref">FeatureUnion</span> is built using a list of `(key, value)` pairs, where the `key` is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and `value` is an estimator object:

    >>> from sklearn.pipeline import FeatureUnion
    >>> from sklearn.decomposition import PCA
    >>> from sklearn.decomposition import KernelPCA
    >>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]
    >>> combined = FeatureUnion(estimators)
    >>> combined
    FeatureUnion(transformer_list=[('linear_pca', PCA()),
                                   ('kernel_pca', KernelPCA())])

Like pipelines, feature unions have a shorthand constructor called <span class="title-ref">make\_union</span> that does not require explicit naming of the components.

Like `Pipeline`, individual steps may be replaced using `set_params`, and ignored by setting to `'drop'`:

    >>> combined.set_params(kernel_pca='drop')
    FeatureUnion(transformer_list=[('linear_pca', PCA()),
                                   ('kernel_pca', 'drop')])

**Examples**

  - \[sphx\_glr\_auto\_examples\_compose\_plot\_feature\_union.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_feature\_union.py)

## ColumnTransformer for heterogeneous data

Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using [pandas](https://pandas.pydata.org/). Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:

1.  Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as *data leakage*), for example in the case of scalers or imputing missing values.
2.  You may want to include the parameters of the preprocessors in a \[parameter search \<grid\_search\>\](\#parameter-search-\<grid\_search\>).

The <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> helps performing different transformations for different columns of the data, within a <span class="title-ref">\~sklearn.pipeline.Pipeline</span> that is safe from data leakage and that can be parametrized. <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> works on arrays, sparse matrices, and [pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/).

To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:

    >>> import pandas as pd
    >>> X = pd.DataFrame(
    ...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],
    ...      'title': ["His Last Bow", "How Watson Learned the Trick",
    ...                "A Moveable Feast", "The Grapes of Wrath"],
    ...      'expert_rating': [5, 3, 4, 5],
    ...      'user_rating': [4, 5, 4, 3]})

For this data, we might want to encode the `'city'` column as a categorical variable using <span class="title-ref">\~sklearn.preprocessing.OneHotEncoder</span> but apply a <span class="title-ref">\~sklearn.feature\_extraction.text.CountVectorizer</span> to the `'title'` column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say `'city_category'` and `'title_bow'`. By default, the remaining rating columns are ignored (`remainder='drop'`):

    >>> from sklearn.compose import ColumnTransformer
    >>> from sklearn.feature_extraction.text import CountVectorizer
    >>> from sklearn.preprocessing import OneHotEncoder
    >>> column_trans = ColumnTransformer(
    ...     [('categories', OneHotEncoder(dtype='int'), ['city']),
    ...      ('title_bow', CountVectorizer(), 'title')],
    ...     remainder='drop', verbose_feature_names_out=False)
    
    >>> column_trans.fit(X)
    ColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'),
                                     ['city']),
                                    ('title_bow', CountVectorizer(), 'title')],
                      verbose_feature_names_out=False)
    
    >>> column_trans.get_feature_names_out()
    array(['city_London', 'city_Paris', 'city_Sallisaw', 'bow', 'feast',
    'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the',
     'trick', 'watson', 'wrath'], ...)
    
    >>> column_trans.transform(X).toarray()
    array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
           [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],
           [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
           [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)

In the above example, the <span class="title-ref">\~sklearn.feature\_extraction.text.CountVectorizer</span> expects a 1D array as input and therefore the columns were specified as a string (`'title'`). However, <span class="title-ref">\~sklearn.preprocessing.OneHotEncoder</span> as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (`['city']`).

Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a <span class="title-ref">\~sklearn.compose.make\_column\_selector</span>. The <span class="title-ref">\~sklearn.compose.make\_column\_selector</span> is used to select columns based on data type or column name:

    >>> from sklearn.preprocessing import StandardScaler
    >>> from sklearn.compose import make_column_selector
    >>> ct = ColumnTransformer([
    ...       ('scale', StandardScaler(),
    ...       make_column_selector(dtype_include=np.number)),
    ...       ('onehot',
    ...       OneHotEncoder(),
    ...       make_column_selector(pattern='city', dtype_include=object))])
    >>> ct.fit_transform(X)
    array([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],
           [-1.507...,  1.414...,  1. ,  0. ,  0. ],
           [-0.301...,  0.      ,  0. ,  1. ,  0. ],
           [ 0.904..., -1.414...,  0. ,  0. ,  1. ]])

Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.

We can keep the remaining rating columns by setting `remainder='passthrough'`. The values are appended to the end of the transformation:

    >>> column_trans = ColumnTransformer(
    ...     [('city_category', OneHotEncoder(dtype='int'),['city']),
    ...      ('title_bow', CountVectorizer(), 'title')],
    ...     remainder='passthrough')
    
    >>> column_trans.fit_transform(X)
    array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],
           [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],
           [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],
           [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)

The `remainder` parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:

    >>> from sklearn.preprocessing import MinMaxScaler
    >>> column_trans = ColumnTransformer(
    ...     [('city_category', OneHotEncoder(), ['city']),
    ...      ('title_bow', CountVectorizer(), 'title')],
    ...     remainder=MinMaxScaler())
    
    >>> column_trans.fit_transform(X)[:, -2:]
    array([[1. , 0.5],
           [0. , 1. ],
           [0.5, 0.5],
           [1. , 0. ]])

<div id="make_column_transformer">

The <span class="title-ref">\~sklearn.compose.make\_column\_transformer</span> function is available to more easily create a <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> object. Specifically, the names will be given automatically. The equivalent for the above example would be:

    >>> from sklearn.compose import make_column_transformer
    >>> column_trans = make_column_transformer(
    ...     (OneHotEncoder(), ['city']),
    ...     (CountVectorizer(), 'title'),
    ...     remainder=MinMaxScaler())
    >>> column_trans
    ColumnTransformer(remainder=MinMaxScaler(),
                      transformers=[('onehotencoder', OneHotEncoder(), ['city']),
                                    ('countvectorizer', CountVectorizer(),
                                     'title')])

</div>

If <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> is fitted with a dataframe and the dataframe only has string column names, then transforming a dataframe will use the column names to select the columns:

    >>> ct = ColumnTransformer(
    ...          [("scale", StandardScaler(), ["expert_rating"])]).fit(X)
    >>> X_new = pd.DataFrame({"expert_rating": [5, 6, 1],
    ...                       "ignored_new_col": [1.2, 0.3, -0.1]})
    >>> ct.transform(X_new)
    array([[ 0.9...],
           [ 2.1...],
           [-3.9...]])

## Visualizing Composite Estimators

Estimators are displayed with an HTML representation when shown in a jupyter notebook. This is useful to diagnose or visualize a Pipeline with many estimators. This visualization is activated by default:

    >>> column_trans  # doctest: +SKIP

It can be deactivated by setting the <span class="title-ref">display</span> option in <span class="title-ref">\~sklearn.set\_config</span> to 'text':

    >>> from sklearn import set_config
    >>> set_config(display='text')  # doctest: +SKIP
    >>> # displays text representation in a jupyter context
    >>> column_trans  # doctest: +SKIP

An example of the HTML output can be seen in the **HTML representation of Pipeline** section of \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py). As an alternative, the HTML can be written to a file using \`\~sklearn.utils.estimator\_html\_repr\`:

    >>> from sklearn.utils import estimator_html_repr
    >>> with open('my_estimator.html', 'w') as f:  # doctest: +SKIP
    ...     f.write(estimator_html_repr(clf))

**Examples**

  - \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer.py)
  - \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer\_mixed\_types.py)

---

covariance.md

---

# Covariance estimation

<div class="currentmodule">

sklearn.covariance

</div>

Many statistical problems require the estimation of a population's covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation's quality. The `sklearn.covariance` package provides tools for accurately estimating a population's covariance matrix under various settings.

We assume that the observations are independent and identically distributed (i.i.d.).

## Empirical covariance

The covariance matrix of a data set is known to be well approximated by the classical *maximum likelihood estimator* (or "empirical covariance"), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an asymptotically unbiased estimator of the corresponding population's covariance matrix.

The empirical covariance matrix of a sample can be computed using the <span class="title-ref">empirical\_covariance</span> function of the package, or by fitting an <span class="title-ref">EmpiricalCovariance</span> object to the data sample with the <span class="title-ref">EmpiricalCovariance.fit</span> method. Be careful that results depend on whether the data are centered, so one may want to use the `assume_centered` parameter accurately. More precisely, if `assume_centered=False`, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and `assume_centered=True` should be used.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py) for an example on how to fit an <span class="title-ref">EmpiricalCovariance</span> object to data.

## Shrunk Covariance

### Basic shrinkage

Despite being an asymptotically unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good estimator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate. Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the `shrinkage`.

In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the <span class="title-ref">shrunk\_covariance</span> method. Also, a shrunk estimator of the covariance can be fitted to data with a <span class="title-ref">ShrunkCovariance</span> object and its <span class="title-ref">ShrunkCovariance.fit</span> method. Again, results depend on whether the data are centered, so one may want to use the `assume_centered` parameter accurately.

Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm
shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm
Tr}\hat{\Sigma}}{p}\rm Id\).

Choosing the amount of shrinkage, \(\alpha\) amounts to setting a bias/variance trade-off, and is discussed below.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py) for an example on how to fit a <span class="title-ref">ShrunkCovariance</span> object to data.

### Ledoit-Wolf shrinkage

In their 2004 paper\[1\], O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.

The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the <span class="title-ref">ledoit\_wolf</span> function of the `sklearn.covariance` package, or it can be otherwise obtained by fitting a <span class="title-ref">LedoitWolf</span> object to the same sample.

<div class="note">

<div class="title">

Note

</div>

**Case when population covariance matrix is isotropic**

It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.

This, however, is not the case in the Ledoit-Wolf procedure when the population covariance happens to be a multiple of the identity matrix. In this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples increases. This indicates that the optimal estimate of the covariance matrix in the Ledoit-Wolf sense is multiple of the identity. Since the population covariance is already a multiple of the identity matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.

</div>

**Examples**

  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py) for an example on how to fit a <span class="title-ref">LedoitWolf</span> object to data and for visualizing the performances of the Ledoit-Wolf estimator in terms of likelihood.

**References**

### Oracle Approximating Shrinkage

Under the assumption that the data are Gaussian distributed, Chen et al.\[2\] derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf's formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.

The OAS estimator of the covariance matrix can be computed on a sample with the <span class="title-ref">oas</span> function of the `sklearn.covariance` package, or it can be otherwise obtained by fitting an <span class="title-ref">OAS</span> object to the same sample.

![Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estimators](../auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_001.png)

**References**

**Examples**

  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_covariance\_estimation.py) for an example on how to fit an <span class="title-ref">OAS</span> object to data.
  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_lw\_vs\_oas.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_lw\_vs\_oas.py) to visualize the Mean Squared Error difference between a <span class="title-ref">LedoitWolf</span> and an <span class="title-ref">OAS</span> estimator of the covariance.

![](../auto_examples/covariance/images/sphx_glr_plot_lw_vs_oas_001.png)

## Sparse inverse covariance

The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as *covariance selection*.

In the small-samples situation, in which `n_samples` is on the order of `n_features` or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure.

The <span class="title-ref">GraphicalLasso</span> estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its `alpha` parameter, the more sparse the precision matrix. The corresponding <span class="title-ref">GraphicalLassoCV</span> object uses cross-validation to automatically set the `alpha` parameter.

![*A comparison of maximum likelihood, shrinkage and sparse estimates of the covariance and precision matrix in the very small samples settings.*](../auto_examples/covariance/images/sphx_glr_plot_sparse_cov_001.png)

<div class="note">

<div class="title">

Note

</div>

**Structure recovery**

Recovering a graphical structure from correlations in the data is a challenging thing. If you are interested in such recovery keep in mind that:

  - Recovery is easier from a correlation matrix than a covariance matrix: standardize your observations before running <span class="title-ref">GraphicalLasso</span>
  - If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections.
  - If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it.
  - Even if you are in favorable recovery conditions, the alpha parameter chosen by cross-validation (e.g. using the <span class="title-ref">GraphicalLassoCV</span> object) will lead to selecting too many edges. However, the relevant edges will have heavier weights than the irrelevant ones.

</div>

The mathematical formulation is the following:

\[\hat{K} = \mathrm{argmin}_K \big(
            \mathrm{tr} S K - \mathrm{log} \mathrm{det} K
            + \alpha \|K\|_1
            \big)\]

Where \(K\) is the precision matrix to be estimated, and \(S\) is the sample covariance matrix. \(\|K\|_1\) is the sum of the absolute values of off-diagonal coefficients of \(K\). The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R `glasso` package.

**Examples**

  - \[sphx\_glr\_auto\_examples\_covariance\_plot\_sparse\_cov.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_sparse\_cov.py): example on synthetic data showing some recovery of a structure, and comparing to other covariance estimators.
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_stock\_market.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_stock\_market.py): example on real stock market data, finding which symbols are most linked.

**References**

  - Friedman et al, ["Sparse inverse covariance estimation with the graphical lasso"](https://biostatistics.oxfordjournals.org/content/9/3/432.short), Biostatistics 9, pp 432, 2008

## Robust Covariance Estimation

Real data sets are often subject to measurement or recording errors. Regular but uncommon observations may also appear for a variety of reasons. Observations which are very uncommon are called outliers. The empirical covariance estimator and the shrunk covariance estimators presented above are very sensitive to the presence of outliers in the data. Therefore, one should use robust covariance estimators to estimate the covariance of its real data sets. Alternatively, robust covariance estimators can be used to perform outlier detection and discard/downweight some observations according to further processing of the data.

The `sklearn.covariance` package implements a robust estimator of covariance, the Minimum Covariance Determinant\[3\].

### Minimum Covariance Determinant

The Minimum Covariance Determinant estimator is a robust estimator of a data set's covariance introduced by P.J. Rousseeuw in\[4\]. The idea is to find a given proportion (h) of "good" observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations ("consistency step"). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set ("reweighting step").

Rousseeuw and Van Driessen\[5\] developed the FastMCD algorithm in order to compute the Minimum Covariance Determinant. This algorithm is used in scikit-learn when fitting an MCD object to data. The FastMCD algorithm also computes a robust estimate of the data set location at the same time.

Raw estimates can be accessed as `raw_location_` and `raw_covariance_` attributes of a <span class="title-ref">MinCovDet</span> robust covariance estimator object.

**References**

**Examples**

  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_robust\_vs\_empirical\_covariance.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_robust\_vs\_empirical\_covariance.py) for an example on how to fit a <span class="title-ref">MinCovDet</span> object to data and see how the estimate remains accurate despite the presence of outliers.
  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_mahalanobis\_distances.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_mahalanobis\_distances.py) to visualize the difference between <span class="title-ref">EmpiricalCovariance</span> and <span class="title-ref">MinCovDet</span> covariance estimators in terms of Mahalanobis distance (so we get a better estimate of the precision matrix too).

| Influence of outliers on location and covariance estimates                                                                                                                          | Separating inliers from outliers using a Mahalanobis distance                                                                                                 |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [![robust\_vs\_emp](../auto_examples/covariance/images/sphx_glr_plot_robust_vs_empirical_covariance_001.png)](../auto_examples/covariance/plot_robust_vs_empirical_covariance.html) | [![mahalanobis](../auto_examples/covariance/images/sphx_glr_plot_mahalanobis_distances_001.png)](../auto_examples/covariance/plot_mahalanobis_distances.html) |

1.  O. Ledoit and M. Wolf, "A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices", Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.

2.  `"Shrinkage algorithms for MMSE covariance estimation.",
    Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.
    IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.
    <0907.4698>`

3.  16. 10. Rousseeuw. Least median of squares regression.
    
    17. Am Stat Ass, 79:871, 1984.

4.  16. 10. Rousseeuw. Least median of squares regression.
    
    17. Am Stat Ass, 79:871, 1984.

5.  A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS.

---

cross_decomposition.md

---

# Cross decomposition

<div class="currentmodule">

sklearn.cross\_decomposition

</div>

The cross decomposition module contains **supervised** estimators for dimensionality reduction and regression, belonging to the "Partial Least Squares" family.

![](../auto_examples/cross_decomposition/images/sphx_glr_plot_compare_cross_decomposition_001.png)

Cross decomposition algorithms find the fundamental relations between two matrices (X and Y). They are latent variable approaches to modeling the covariance structures in these two spaces. They will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. In other words, PLS projects both <span class="title-ref">X</span> and <span class="title-ref">Y</span> into a lower-dimensional subspace such that the covariance between <span class="title-ref">transformed(X)</span> and <span class="title-ref">transformed(Y)</span> is maximal.

PLS draws similarities with [Principal Component Regression](https://en.wikipedia.org/wiki/Principal_component_regression) (PCR), where the samples are first projected into a lower-dimensional subspace, and the targets <span class="title-ref">y</span> are predicted using <span class="title-ref">transformed(X)</span>. One issue with PCR is that the dimensionality reduction is unsupervised, and may lose some important variables: PCR would keep the features with the most variance, but it's possible that features with a small variances are relevant from predicting the target. In a way, PLS allows for the same kind of dimensionality reduction, but by taking into account the targets <span class="title-ref">y</span>. An illustration of this fact is given in the following example: \* \[sphx\_glr\_auto\_examples\_cross\_decomposition\_plot\_pcr\_vs\_pls.py\](\#sphx\_glr\_auto\_examples\_cross\_decomposition\_plot\_pcr\_vs\_pls.py).

Apart from CCA, the PLS estimators are particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among the features. By contrast, standard linear regression would fail in these cases unless it is regularized.

Classes included in this module are <span class="title-ref">PLSRegression</span>, <span class="title-ref">PLSCanonical</span>, <span class="title-ref">CCA</span> and <span class="title-ref">PLSSVD</span>

## PLSCanonical

We here describe the algorithm used in <span class="title-ref">PLSCanonical</span>. The other estimators use variants of this algorithm, and are detailed below. We recommend section\[1\] for more details and comparisons between these algorithms. In\[2\], <span class="title-ref">PLSCanonical</span> corresponds to "PLSW2A".

Given two centered matrices \(X \in \mathbb{R}^{n \times d}\) and \(Y \in \mathbb{R}^{n \times t}\), and a number of components \(K\), <span class="title-ref">PLSCanonical</span> proceeds as follows:

Set \(X_1\) to \(X\) and \(Y_1\) to \(Y\). Then, for each \(k \in [1, K]\):

  - a) compute \(u_k \in \mathbb{R}^d\) and \(v_k \in \mathbb{R}^t\), the first left and right singular vectors of the cross-covariance matrix \(C = X_k^T Y_k\). \(u_k\) and \(v_k\) are called the *weights*. By definition, \(u_k\) and \(v_k\) are chosen so that they maximize the covariance between the projected \(X_k\) and the projected target, that is \(\text{Cov}(X_k u_k,
    Y_k v_k)\).
  - b) Project \(X_k\) and \(Y_k\) on the singular vectors to obtain *scores*: \(\xi_k = X_k u_k\) and \(\omega_k = Y_k v_k\)
  - c) Regress \(X_k\) on \(\xi_k\), i.e. find a vector \(\gamma_k
    \in \mathbb{R}^d\) such that the rank-1 matrix \(\xi_k \gamma_k^T\) is as close as possible to \(X_k\). Do the same on \(Y_k\) with \(\omega_k\) to obtain \(\delta_k\). The vectors \(\gamma_k\) and \(\delta_k\) are called the *loadings*.
  - d) *deflate* \(X_k\) and \(Y_k\), i.e. subtract the rank-1 approximations: \(X_{k+1} = X_k - \xi_k \gamma_k^T\), and \(Y_{k + 1} = Y_k - \omega_k \delta_k^T\).

At the end, we have approximated \(X\) as a sum of rank-1 matrices: \(X = \Xi \Gamma^T\) where \(\Xi \in \mathbb{R}^{n \times K}\) contains the scores in its columns, and \(\Gamma^T \in \mathbb{R}^{K
\times d}\) contains the loadings in its rows. Similarly for \(Y\), we have \(Y = \Omega \Delta^T\).

Note that the scores matrices \(\Xi\) and \(\Omega\) correspond to the projections of the training data \(X\) and \(Y\), respectively.

Step *a)* may be performed in two ways: either by computing the whole SVD of \(C\) and only retain the singular vectors with the biggest singular values, or by directly computing the singular vectors using the power method (cf section 11.3 in\[3\]), which corresponds to the <span class="title-ref">'nipals'</span> option of the <span class="title-ref">algorithm</span> parameter.

<div class="dropdown">

Transforming data

To transform \(X\) into \(\bar{X}\), we need to find a projection matrix \(P\) such that \(\bar{X} = XP\). We know that for the training data, \(\Xi = XP\), and \(X = \Xi \Gamma^T\). Setting \(P = U(\Gamma^T U)^{-1}\) where \(U\) is the matrix with the \(u_k\) in the columns, we have \(XP = X U(\Gamma^T U)^{-1} = \Xi
(\Gamma^T U) (\Gamma^T U)^{-1} = \Xi\) as desired. The rotation matrix \(P\) can be accessed from the <span class="title-ref">x\_rotations\_</span> attribute.

Similarly, \(Y\) can be transformed using the rotation matrix \(V(\Delta^T V)^{-1}\), accessed via the <span class="title-ref">y\_rotations\_</span> attribute.

</div>

<div class="dropdown">

Predicting the targets <span class="title-ref">Y</span>

To predict the targets of some data \(X\), we are looking for a coefficient matrix \(\beta \in R^{d \times t}\) such that \(Y =
X\beta\).

The idea is to try to predict the transformed targets \(\Omega\) as a function of the transformed samples \(\Xi\), by computing \(\alpha
\in \mathbb{R}\) such that \(\Omega = \alpha \Xi\).

Then, we have \(Y = \Omega \Delta^T = \alpha \Xi \Delta^T\), and since \(\Xi\) is the transformed training data we have that \(Y = X \alpha
P \Delta^T\), and as a result the coefficient matrix \(\beta = \alpha P
\Delta^T\).

\(\beta\) can be accessed through the <span class="title-ref">coef\_</span> attribute.

</div>

## PLSSVD

<span class="title-ref">PLSSVD</span> is a simplified version of <span class="title-ref">PLSCanonical</span> described earlier: instead of iteratively deflating the matrices \(X_k\) and \(Y_k\), <span class="title-ref">PLSSVD</span> computes the SVD of \(C = X^TY\) only *once*, and stores the <span class="title-ref">n\_components</span> singular vectors corresponding to the biggest singular values in the matrices <span class="title-ref">U</span> and <span class="title-ref">V</span>, corresponding to the <span class="title-ref">x\_weights\_</span> and <span class="title-ref">y\_weights\_</span> attributes. Here, the transformed data is simply <span class="title-ref">transformed(X) = XU</span> and <span class="title-ref">transformed(Y) = YV</span>.

If <span class="title-ref">n\_components == 1</span>, <span class="title-ref">PLSSVD</span> and <span class="title-ref">PLSCanonical</span> are strictly equivalent.

## PLSRegression

The <span class="title-ref">PLSRegression</span> estimator is similar to <span class="title-ref">PLSCanonical</span> with <span class="title-ref">algorithm='nipals'</span>, with 2 significant differences:

  - at step a) in the power method to compute \(u_k\) and \(v_k\), \(v_k\) is never normalized.
  - at step c), the targets \(Y_k\) are approximated using the projection of \(X_k\) (i.e. \(\xi_k\)) instead of the projection of \(Y_k\) (i.e. \(\omega_k\)). In other words, the loadings computation is different. As a result, the deflation in step d) will also be affected.

These two modifications affect the output of <span class="title-ref">predict</span> and <span class="title-ref">transform</span>, which are not the same as for <span class="title-ref">PLSCanonical</span>. Also, while the number of components is limited by <span class="title-ref">min(n\_samples, n\_features, n\_targets)</span> in <span class="title-ref">PLSCanonical</span>, here the limit is the rank of \(X^TX\), i.e. <span class="title-ref">min(n\_samples, n\_features)</span>.

<span class="title-ref">PLSRegression</span> is also known as PLS1 (single targets) and PLS2 (multiple targets). Much like <span class="title-ref">\~sklearn.linear\_model.Lasso</span>, <span class="title-ref">PLSRegression</span> is a form of regularized linear regression where the number of components controls the strength of the regularization.

## Canonical Correlation Analysis

Canonical Correlation Analysis was developed prior and independently to PLS. But it turns out that <span class="title-ref">CCA</span> is a special case of PLS, and corresponds to PLS in "Mode B" in the literature.

<span class="title-ref">CCA</span> differs from <span class="title-ref">PLSCanonical</span> in the way the weights \(u_k\) and \(v_k\) are computed in the power method of step a). Details can be found in section 10 of\[4\].

Since <span class="title-ref">CCA</span> involves the inversion of \(X_k^TX_k\) and \(Y_k^TY_k\), this estimator can be unstable if the number of features or targets is greater than the number of samples.

**References**

**Examples**

  - \[sphx\_glr\_auto\_examples\_cross\_decomposition\_plot\_compare\_cross\_decomposition.py\](\#sphx\_glr\_auto\_examples\_cross\_decomposition\_plot\_compare\_cross\_decomposition.py)
  - \[sphx\_glr\_auto\_examples\_cross\_decomposition\_plot\_pcr\_vs\_pls.py\](\#sphx\_glr\_auto\_examples\_cross\_decomposition\_plot\_pcr\_vs\_pls.py)

<!-- end list -->

1.  [A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case](https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf), JA Wegelin

2.  [A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case](https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf), JA Wegelin

3.  [A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case](https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf), JA Wegelin

4.  [A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case](https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf), JA Wegelin

---

cross_validation.md

---

# Cross-validation: evaluating estimator performance

<div class="currentmodule">

sklearn.model\_selection

</div>

Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a **test set** `X_test, y_test`. Note that the word "experiment" is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by \[grid search \<grid\_search\>\](\#grid-search-\<grid\_search\>) techniques.

![Grid Search Workflow](../images/grid_search_workflow.png)

In scikit-learn a random split into training and test sets can be quickly computed with the <span class="title-ref">train\_test\_split</span> helper function. Let's load the iris data set to fit a linear support vector machine on it:

    >>> import numpy as np
    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn import datasets
    >>> from sklearn import svm
    
    >>> X, y = datasets.load_iris(return_X_y=True)
    >>> X.shape, y.shape
    ((150, 4), (150,))

We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:

    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, test_size=0.4, random_state=0)
    
    >>> X_train.shape, y_train.shape
    ((90, 4), (90,))
    >>> X_test.shape, y_test.shape
    ((60, 4), (60,))
    
    >>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
    >>> clf.score(X_test, y_test)
    0.96...

When evaluating different settings ("hyperparameters") for estimators, such as the `C` setting that must be manually set for an SVM, there is still a risk of overfitting *on the test set* because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can "leak" into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called "validation set": training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.

However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.

A solution to this problem is a procedure called [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_\(statistics\)) (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called *k*-fold CV, the training set is split into *k* smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the *k* "folds":

  - A model is trained using \(k-1\) of the folds as training data;
  - the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).

The performance measure reported by *k*-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.

![A depiction of a 5 fold cross validation on a training set, while holding out a test set.](../images/grid_search_cross_validation.png)

## Computing cross-validated metrics

The simplest way to use cross-validation is to call the <span class="title-ref">cross\_val\_score</span> helper function on the estimator and the dataset.

The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):

    >>> from sklearn.model_selection import cross_val_score
    >>> clf = svm.SVC(kernel='linear', C=1, random_state=42)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores
    array([0.96..., 1. , 0.96..., 0.96..., 1. ])

The mean score and the standard deviation are hence given by:

    >>> print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
    0.98 accuracy with a standard deviation of 0.02

By default, the score computed at each CV iteration is the `score` method of the estimator. It is possible to change this by using the scoring parameter:

    >>> from sklearn import metrics
    >>> scores = cross_val_score(
    ...     clf, X, y, cv=5, scoring='f1_macro')
    >>> scores
    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])

See \[scoring\_parameter\](\#scoring\_parameter) for details. In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal.

When the `cv` argument is an integer, <span class="title-ref">cross\_val\_score</span> uses the <span class="title-ref">KFold</span> or <span class="title-ref">StratifiedKFold</span> strategies by default, the latter being used if the estimator derives from <span class="title-ref">ClassifierMixin \<sklearn.base.ClassifierMixin\></span>.

It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:

    >>> from sklearn.model_selection import ShuffleSplit
    >>> n_samples = X.shape[0]
    >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
    >>> cross_val_score(clf, X, y, cv=cv)
    array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])

Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example:

    >>> def custom_cv_2folds(X):
    ...     n = X.shape[0]
    ...     i = 1
    ...     while i <= 2:
    ...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)
    ...         yield idx, idx
    ...         i += 1
    ...
    >>> custom_cv = custom_cv_2folds(X)
    >>> cross_val_score(clf, X, y, cv=custom_cv)
    array([1.        , 0.973...])

<div class="dropdown">

Data transformation with held-out data

Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar \[data transformations \<data-transforms\>\](\#data-transformations-\<data-transforms\>) similarly should be learnt from a training set and applied to held-out data for prediction:

    >>> from sklearn import preprocessing
    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, test_size=0.4, random_state=0)
    >>> scaler = preprocessing.StandardScaler().fit(X_train)
    >>> X_train_transformed = scaler.transform(X_train)
    >>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
    >>> X_test_transformed = scaler.transform(X_test)
    >>> clf.score(X_test_transformed, y_test)
    0.9333...

A <span class="title-ref">Pipeline \<sklearn.pipeline.Pipeline\></span> makes it easier to compose estimators, providing this behavior under cross-validation:

    >>> from sklearn.pipeline import make_pipeline
    >>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
    >>> cross_val_score(clf, X, y, cv=cv)
    array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])

See \[combining\_estimators\](\#combining\_estimators).

</div>

### The cross\_validate function and multiple metric evaluation

The <span class="title-ref">cross\_validate</span> function differs from <span class="title-ref">cross\_val\_score</span> in two ways:

  - It allows specifying multiple metrics for evaluation.
  - It returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score.

For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - `['test_score', 'fit_time', 'score_time']`

And for multiple metric evaluation, the return value is a dict with the following keys -`['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']`

`return_train_score` is set to `False` by default to save computation time. To evaluate the scores on the training set as well you need to set it to `True`. You may also retain the estimator fitted on each training set by setting `return_estimator=True`. Similarly, you may set <span class="title-ref">return\_indices=True</span> to retain the training and testing indices used to split the dataset into train and test sets for each cv split.

The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:

    >>> from sklearn.model_selection import cross_validate
    >>> from sklearn.metrics import recall_score
    >>> scoring = ['precision_macro', 'recall_macro']
    >>> clf = svm.SVC(kernel='linear', C=1, random_state=0)
    >>> scores = cross_validate(clf, X, y, scoring=scoring)
    >>> sorted(scores.keys())
    ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']
    >>> scores['test_recall_macro']
    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])

Or as a dict mapping scorer name to a predefined or custom scoring function:

    >>> from sklearn.metrics import make_scorer
    >>> scoring = {'prec_macro': 'precision_macro',
    ...            'rec_macro': make_scorer(recall_score, average='macro')}
    >>> scores = cross_validate(clf, X, y, scoring=scoring,
    ...                         cv=5, return_train_score=True)
    >>> sorted(scores.keys())
    ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',
     'train_prec_macro', 'train_rec_macro']
    >>> scores['train_rec_macro']
    array([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])

Here is an example of `cross_validate` using a single metric:

    >>> scores = cross_validate(clf, X, y,
    ...                         scoring='precision_macro', cv=5,
    ...                         return_estimator=True)
    >>> sorted(scores.keys())
    ['estimator', 'fit_time', 'score_time', 'test_score']

### Obtaining predictions by cross-validation

The function <span class="title-ref">cross\_val\_predict</span> has a similar interface to <span class="title-ref">cross\_val\_score</span>, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).

<div class="warning">

<div class="title">

Warning

</div>

Note on inappropriate usage of cross\_val\_predict

The result of <span class="title-ref">cross\_val\_predict</span> may be different from those obtained using <span class="title-ref">cross\_val\_score</span> as the elements are grouped in different ways. The function <span class="title-ref">cross\_val\_score</span> takes an average over cross-validation folds, whereas <span class="title-ref">cross\_val\_predict</span> simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, <span class="title-ref">cross\_val\_predict</span> is not an appropriate measure of generalization error.

</div>

  - The function <span class="title-ref">cross\_val\_predict</span> is appropriate for:
    
      - Visualization of predictions obtained from different models.
      - Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.

The available cross validation iterators are introduced in the following section.

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_roc\_crossval.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_roc\_crossval.py),
  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_with\_cross\_validation.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_with\_cross\_validation.py),
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_digits.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_digits.py),
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py),
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_cv\_predict.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_cv\_predict.py),
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_nested\_cross\_validation\_iris.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_nested\_cross\_validation\_iris.py).

## Cross validation iterators

The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.

### Cross-validation iterators for i.i.d. data

Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.

The following cross-validators can be used in such cases.

\> **Note** \> While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it is safer to use a \[time-series aware cross-validation scheme \<timeseries\_cv\>\](\#time-series-aware-cross-validation-scheme-\<timeseries\_cv\>). Similarly, if we know that the generative process has a group structure (samples collected from different subjects, experiments, measurement devices), it is safer to use \[group-wise cross-validation \<group\_cv\>\](\#group-wise-cross-validation-\<group\_cv\>).

#### K-fold

<span class="title-ref">KFold</span> divides all the samples in \(k\) groups of samples, called folds (if \(k = n\), this is equivalent to the *Leave One Out* strategy), of equal sizes (if possible). The prediction function is learned using \(k - 1\) folds, and the fold left out is used for test.

Example of 2-fold cross-validation on a dataset with 4 samples:

    >>> import numpy as np
    >>> from sklearn.model_selection import KFold
    
    >>> X = ["a", "b", "c", "d"]
    >>> kf = KFold(n_splits=2)
    >>> for train, test in kf.split(X):
    ...     print("%s %s" % (train, test))
    [2 3] [0 1]
    [0 1] [2 3]

Here is a visualization of the cross-validation behavior. Note that <span class="title-ref">KFold</span> is not affected by classes or groups.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_006.png)

Each fold is constituted by two arrays: the first one is related to the *training set*, and the second one to the *test set*. Thus, one can create the training/test sets using numpy indexing:

    >>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])
    >>> y = np.array([0, 1, 0, 1])
    >>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]

#### Repeated K-Fold

<span class="title-ref">RepeatedKFold</span> repeats K-Fold n times. It can be used when one requires to run <span class="title-ref">KFold</span> n times, producing different splits in each repetition.

Example of 2-fold K-Fold repeated 2 times:

    >>> import numpy as np
    >>> from sklearn.model_selection import RepeatedKFold
    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
    >>> random_state = 12883823
    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)
    >>> for train, test in rkf.split(X):
    ...     print("%s %s" % (train, test))
    ...
    [2 3] [0 1]
    [0 1] [2 3]
    [0 2] [1 3]
    [1 3] [0 2]

Similarly, <span class="title-ref">RepeatedStratifiedKFold</span> repeats Stratified K-Fold n times with different randomization in each repetition.

#### Leave One Out (LOO)

<span class="title-ref">LeaveOneOut</span> (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for \(n\) samples, we have \(n\) different training sets and \(n\) different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:

    >>> from sklearn.model_selection import LeaveOneOut
    
    >>> X = [1, 2, 3, 4]
    >>> loo = LeaveOneOut()
    >>> for train, test in loo.split(X):
    ...     print("%s %s" % (train, test))
    [1 2 3] [0]
    [0 2 3] [1]
    [0 1 3] [2]
    [0 1 2] [3]

Potential users of LOO for model selection should weigh a few known caveats. When compared with \(k\)-fold cross validation, one builds \(n\) models from \(n\) samples instead of \(k\) models, where \(n > k\). Moreover, each is trained on \(n - 1\) samples rather than \((k-1) n / k\). In both ways, assuming \(k\) is not too large and \(k < n\), LOO is more computationally expensive than \(k\)-fold cross validation.

In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since \(n - 1\) of the \(n\) samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.

However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.

As a general rule, most authors, and empirical evidence, suggest that 5- or 10-fold cross validation should be preferred to LOO.

<div class="dropdown">

References

  - <http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>;
  - T. Hastie, R. Tibshirani, J. Friedman, [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/), Springer 2009
  - L. Breiman, P. Spector [Submodel selection and evaluation in regression: The X-random case](https://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf), International Statistical Review 1992;
  - R. Kohavi, [A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection](https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf), Intl. Jnt. Conf. AI
  - R. Bharat Rao, G. Fung, R. Rosales, [On the Dangers of Cross-Validation. An Experimental Evaluation](https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf), SIAM 2008;
  - G. James, D. Witten, T. Hastie, R Tibshirani, [An Introduction to Statistical Learning](https://www.statlearning.com), Springer 2013.

</div>

#### Leave P Out (LPO)

<span class="title-ref">LeavePOut</span> is very similar to <span class="title-ref">LeaveOneOut</span> as it creates all the possible training/test sets by removing \(p\) samples from the complete set. For \(n\) samples, this produces \({n \choose p}\) train-test pairs. Unlike <span class="title-ref">LeaveOneOut</span> and <span class="title-ref">KFold</span>, the test sets will overlap for \(p > 1\).

Example of Leave-2-Out on a dataset with 4 samples:

    >>> from sklearn.model_selection import LeavePOut
    
    >>> X = np.ones(4)
    >>> lpo = LeavePOut(p=2)
    >>> for train, test in lpo.split(X):
    ...     print("%s %s" % (train, test))
    [2 3] [0 1]
    [1 3] [0 2]
    [1 2] [0 3]
    [0 3] [1 2]
    [0 2] [1 3]
    [0 1] [2 3]

#### Random permutations cross-validation a.k.a. Shuffle & Split

The <span class="title-ref">ShuffleSplit</span> iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.

It is possible to control the randomness for reproducibility of the results by explicitly seeding the `random_state` pseudo random number generator.

Here is a usage example:

    >>> from sklearn.model_selection import ShuffleSplit
    >>> X = np.arange(10)
    >>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)
    >>> for train_index, test_index in ss.split(X):
    ...     print("%s %s" % (train_index, test_index))
    [9 1 6 7 3 0 5] [2 8 4]
    [2 9 8 0 6 7 4] [3 5 1]
    [4 5 1 0 6 9 7] [2 3 8]
    [2 7 5 8 0 3 4] [6 1 9]
    [4 1 0 6 8 9 3] [5 2 7]

Here is a visualization of the cross-validation behavior. Note that <span class="title-ref">ShuffleSplit</span> is not affected by classes or groups.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_008.png)

<span class="title-ref">ShuffleSplit</span> is thus a good alternative to <span class="title-ref">KFold</span> cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.

### Cross-validation iterators with stratification based on class labels

Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in <span class="title-ref">StratifiedKFold</span> and <span class="title-ref">StratifiedShuffleSplit</span> to ensure that relative class frequencies is approximately preserved in each train and validation fold.

#### Stratified k-fold

<span class="title-ref">StratifiedKFold</span> is a variation of *k-fold* which returns *stratified* folds: each set contains approximately the same percentage of samples of each target class as the complete set.

Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with <span class="title-ref">KFold</span>.

> \>\>\> from sklearn.model\_selection import StratifiedKFold, KFold \>\>\> import numpy as np \>\>\> X, y = np.ones((50, 1)), np.hstack((\[0\] \* 45, \[1\] \* 5)) \>\>\> skf = StratifiedKFold(n\_splits=3) \>\>\> for train, test in skf.split(X, y): ... print('train - {} | test - {}'.format( ... np.bincount(y\[train\]), np.bincount(y\[test\]))) train - \[30 3\] | test - \[15 2\] train - \[30 3\] | test - \[15 2\] train - \[30 4\] | test - \[15 1\] \>\>\> kf = KFold(n\_splits=3) \>\>\> for train, test in kf.split(X, y): ... print('train - {} | test - {}'.format( ... np.bincount(y\[train\]), np.bincount(y\[test\]))) train - \[28 5\] | test - \[17\] train - \[28 5\] | test - \[17\] train - \[34\] | test - \[11 5\]

We can see that <span class="title-ref">StratifiedKFold</span> preserves the class ratios (approximately 1 / 10) in both train and test dataset.

Here is a visualization of the cross-validation behavior.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_009.png)

<span class="title-ref">RepeatedStratifiedKFold</span> can be used to repeat Stratified K-Fold n times with different randomization in each repetition.

#### Stratified Shuffle Split

<span class="title-ref">StratifiedShuffleSplit</span> is a variation of *ShuffleSplit*, which returns stratified splits, *i.e* which creates splits by preserving the same percentage for each target class as in the complete set.

Here is a visualization of the cross-validation behavior.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_012.png)

### Predefined fold-splits / Validation-sets

For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using <span class="title-ref">PredefinedSplit</span> it is possible to use these folds e.g. when searching for hyperparameters.

For example, when using a validation set, set the `test_fold` to 0 for all samples that are part of the validation set, and to -1 for all other samples.

### Cross-validation iterators for grouped data

The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.

Such a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier.

In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.

The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the `groups` parameter.

#### Group k-fold

<span class="title-ref">GroupKFold</span> is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. <span class="title-ref">GroupKFold</span> makes it possible to detect this kind of overfitting situations.

Imagine you have three subjects, each with an associated number from 1 to 3:

    >>> from sklearn.model_selection import GroupKFold
    
    >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
    >>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
    >>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]
    
    >>> gkf = GroupKFold(n_splits=3)
    >>> for train, test in gkf.split(X, y, groups=groups):
    ...     print("%s %s" % (train, test))
    [0 1 2 3 4 5] [6 7 8 9]
    [0 1 2 6 7 8 9] [3 4 5]
    [3 4 5 6 7 8 9] [0 1 2]

Each subject is in a different testing fold, and the same subject is never in both testing and training. Notice that the folds do not have exactly the same size due to the imbalance in the data. If class proportions must be balanced across folds, <span class="title-ref">StratifiedGroupKFold</span> is a better option.

Here is a visualization of the cross-validation behavior.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_007.png)

Similar to <span class="title-ref">KFold</span>, the test sets from <span class="title-ref">GroupKFold</span> will form a complete partition of all the data.

While <span class="title-ref">GroupKFold</span> attempts to place the same number of samples in each fold when `shuffle=False`, when `shuffle=True` it attempts to place equal number of distinct groups in each fold (but doesn not account for group sizes).

#### StratifiedGroupKFold

<span class="title-ref">StratifiedGroupKFold</span> is a cross-validation scheme that combines both <span class="title-ref">StratifiedKFold</span> and <span class="title-ref">GroupKFold</span>. The idea is to try to preserve the distribution of classes in each split while keeping each group within a single split. That might be useful when you have an unbalanced dataset so that using just <span class="title-ref">GroupKFold</span> might produce skewed splits.

Example:

    >>> from sklearn.model_selection import StratifiedGroupKFold
    >>> X = list(range(18))
    >>> y = [1] * 6 + [0] * 12
    >>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]
    >>> sgkf = StratifiedGroupKFold(n_splits=3)
    >>> for train, test in sgkf.split(X, y, groups=groups):
    ...     print("%s %s" % (train, test))
    [ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]
    [ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]
    [ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]

<div class="dropdown">

Implementation notes

  - With the current implementation full shuffle is not possible in most scenarios. When shuffle=True, the following happens:
    
    1.  All groups are shuffled.
    2.  Groups are sorted by standard deviation of classes using stable sort.
    3.  Sorted groups are iterated over and assigned to folds.
    
    That means that only groups with the same standard deviation of class distribution will be shuffled, which might be useful when each group has only a single class.

  - The algorithm greedily assigns each group to one of n\_splits test sets, choosing the test set that minimises the variance in class distribution across test sets. Group assignment proceeds from groups with highest to lowest variance in class frequency, i.e. large groups peaked on one or few classes are assigned first.

  - This split is suboptimal in a sense that it might produce imbalanced splits even if perfect stratification is possible. If you have relatively close distribution of classes in each group, using <span class="title-ref">GroupKFold</span> is better.

</div>

Here is a visualization of cross-validation behavior for uneven groups:

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_005.png)

#### Leave One Group Out

<span class="title-ref">LeaveOneGroupOut</span> is a cross-validation scheme where each split holds out samples belonging to one specific group. Group information is provided via an array that encodes the group of each sample.

Each training set is thus constituted by all the samples except the ones related to a specific group. This is the same as <span class="title-ref">LeavePGroupsOut</span> with <span class="title-ref">n\_groups=1</span> and the same as <span class="title-ref">GroupKFold</span> with <span class="title-ref">n\_splits</span> equal to the number of unique labels passed to the <span class="title-ref">groups</span> parameter.

For example, in the cases of multiple experiments, <span class="title-ref">LeaveOneGroupOut</span> can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:

    >>> from sklearn.model_selection import LeaveOneGroupOut
    
    >>> X = [1, 5, 10, 50, 60, 70, 80]
    >>> y = [0, 1, 1, 2, 2, 2, 2]
    >>> groups = [1, 1, 2, 2, 3, 3, 3]
    >>> logo = LeaveOneGroupOut()
    >>> for train, test in logo.split(X, y, groups=groups):
    ...     print("%s %s" % (train, test))
    [2 3 4 5 6] [0 1]
    [0 1 4 5 6] [2 3]
    [0 1 2 3] [4 5 6]

Another common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.

#### Leave P Groups Out

<span class="title-ref">LeavePGroupsOut</span> is similar as <span class="title-ref">LeaveOneGroupOut</span>, but removes samples related to \(P\) groups for each training/test set. All possible combinations of \(P\) groups are left out, meaning test sets will overlap for \(P>1\).

Example of Leave-2-Group Out:

    >>> from sklearn.model_selection import LeavePGroupsOut
    
    >>> X = np.arange(6)
    >>> y = [1, 1, 1, 2, 2, 2]
    >>> groups = [1, 1, 2, 2, 3, 3]
    >>> lpgo = LeavePGroupsOut(n_groups=2)
    >>> for train, test in lpgo.split(X, y, groups=groups):
    ...     print("%s %s" % (train, test))
    [4 5] [0 1 2 3]
    [2 3] [0 1 4 5]
    [0 1] [2 3 4 5]

#### Group Shuffle Split

The <span class="title-ref">GroupShuffleSplit</span> iterator behaves as a combination of <span class="title-ref">ShuffleSplit</span> and <span class="title-ref">LeavePGroupsOut</span>, and generates a sequence of randomized partitions in which a subset of groups are held out for each split. Each train/test split is performed independently meaning there is no guaranteed relationship between successive test sets.

Here is a usage example:

    >>> from sklearn.model_selection import GroupShuffleSplit
    
    >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]
    >>> y = ["a", "b", "b", "b", "c", "c", "c", "a"]
    >>> groups = [1, 1, 2, 2, 3, 3, 4, 4]
    >>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
    >>> for train, test in gss.split(X, y, groups=groups):
    ...     print("%s %s" % (train, test))
    ...
    [0 1 2 3] [4 5 6 7]
    [2 3 6 7] [0 1 4 5]
    [2 3 4 5] [0 1 6 7]
    [4 5 6 7] [0 1 2 3]

Here is a visualization of the cross-validation behavior.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_011.png)

This class is useful when the behavior of <span class="title-ref">LeavePGroupsOut</span> is desired, but the number of groups is large enough that generating all possible partitions with \(P\) groups withheld would be prohibitively expensive. In such a scenario, <span class="title-ref">GroupShuffleSplit</span> provides a random sample (with replacement) of the train / test splits generated by <span class="title-ref">LeavePGroupsOut</span>.

### Using cross-validation iterators to split train and test

The above group cross-validation functions may also be useful for splitting a dataset into training and testing subsets. Note that the convenience function <span class="title-ref">train\_test\_split</span> is a wrapper around <span class="title-ref">ShuffleSplit</span> and thus only allows for stratified splitting (using the class labels) and cannot account for groups.

To perform the train and test split, use the indices for the train and test subsets yielded by the generator output by the <span class="title-ref">split()</span> method of the cross-validation splitter. For example:

    >>> import numpy as np
    >>> from sklearn.model_selection import GroupShuffleSplit
    
    >>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])
    >>> y = np.array(["a", "b", "b", "b", "c", "c", "c", "a"])
    >>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])
    >>> train_indx, test_indx = next(
    ...     GroupShuffleSplit(random_state=7).split(X, y, groups)
    ... )
    >>> X_train, X_test, y_train, y_test = \
    ...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]
    >>> X_train.shape, X_test.shape
    ((6,), (2,))
    >>> np.unique(groups[train_indx]), np.unique(groups[test_indx])
    (array([1, 2, 4]), array([3]))

### Cross validation of time series data

Time series data is characterized by the correlation between observations that are near in time (*autocorrelation*). However, classical cross-validation techniques such as <span class="title-ref">KFold</span> and <span class="title-ref">ShuffleSplit</span> assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalization error) on time series data. Therefore, it is very important to evaluate our model for time series data on the "future" observations least like those that are used to train the model. To achieve this, one solution is provided by <span class="title-ref">TimeSeriesSplit</span>.

#### Time Series Split

<span class="title-ref">TimeSeriesSplit</span> is a variation of *k-fold* which returns first \(k\) folds as train set and the \((k+1)\) th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model.

This class can be used to cross-validate time series data samples that are observed at fixed time intervals.

Example of 3-split time series cross-validation on a dataset with 6 samples:

    >>> from sklearn.model_selection import TimeSeriesSplit
    
    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
    >>> y = np.array([1, 2, 3, 4, 5, 6])
    >>> tscv = TimeSeriesSplit(n_splits=3)
    >>> print(tscv)
    TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)
    >>> for train, test in tscv.split(X):
    ...     print("%s %s" % (train, test))
    [0 1 2] [3]
    [0 1 2 3] [4]
    [0 1 2 3 4] [5]

Here is a visualization of the cross-validation behavior.

![](../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_013.png)

## A note on shuffling

If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross-validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.

Some cross validation iterators, such as <span class="title-ref">KFold</span>, have an inbuilt option to shuffle the data indices before splitting them. Note that:

  - This consumes less memory than shuffling the data directly.
  - By default no shuffling occurs, including for the (stratified) K fold cross-validation performed by specifying `cv=some_integer` to <span class="title-ref">cross\_val\_score</span>, grid search, etc. Keep in mind that <span class="title-ref">train\_test\_split</span> still returns a random split.
  - The `random_state` parameter defaults to `None`, meaning that the shuffling will be different every time `KFold(..., shuffle=True)` is iterated. However, `GridSearchCV` will use the same shuffling for each set of parameters validated by a single call to its `fit` method.
  - To get identical results for each split, set `random_state` to an integer.

For more details on how to control the randomness of cv splitters and avoid common pitfalls, see \[randomness\](\#randomness).

## Cross validation and model selection

Cross validation iterators can also be used to directly perform model selection using Grid Search for the optimal hyperparameters of the model. This is the topic of the next section: \[grid\_search\](\#grid\_search).

## Permutation test score

<span class="title-ref">\~sklearn.model\_selection.permutation\_test\_score</span> offers another way to evaluate the performance of classifiers. It provides a permutation-based p-value, which represents how likely an observed performance of the classifier would be obtained by chance. The null hypothesis in this test is that the classifier fails to leverage any statistical dependency between the features and the labels to make correct predictions on left out data. <span class="title-ref">\~sklearn.model\_selection.permutation\_test\_score</span> generates a null distribution by calculating <span class="title-ref">n\_permutations</span> different permutations of the data. In each permutation the labels are randomly shuffled, thereby removing any dependency between the features and the labels. The p-value output is the fraction of permutations for which the average cross-validation score obtained by the model is better than the cross-validation score obtained by the model using the original data. For reliable results `n_permutations` should typically be larger than 100 and `cv` between 3-10 folds.

A low p-value provides evidence that the dataset contains real dependency between features and labels and the classifier was able to utilize this to obtain good results. A high p-value could be due to a lack of dependency between features and labels (there is no difference in feature values between the classes) or because the classifier was not able to use the dependency in the data. In the latter case, using a more appropriate classifier that is able to utilize the structure in the data, would result in a lower p-value.

Cross-validation provides information about how well a classifier generalizes, specifically the range of expected errors of the classifier. However, a classifier trained on a high dimensional dataset with no structure may still perform better than expected on cross-validation, just by chance. This can typically happen with small datasets with less than a few hundred samples. <span class="title-ref">\~sklearn.model\_selection.permutation\_test\_score</span> provides information on whether the classifier has found a real class structure and can help in evaluating the performance of the classifier.

It is important to note that this test has been shown to produce low p-values even if there is only weak structure in the data because in the corresponding permutated datasets there is absolutely no structure. This test is therefore only able to show when the model reliably outperforms random guessing.

Finally, <span class="title-ref">\~sklearn.model\_selection.permutation\_test\_score</span> is computed using brute force and internally fits `(n_permutations + 1) * n_cv` models. It is therefore only tractable with small datasets for which fitting an individual model is very fast.

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_permutation\_tests\_for\_classification.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_permutation\_tests\_for\_classification.py)

<div class="dropdown">

References

  - Ojala and Garriga. [Permutation Tests for Studying Classifier Performance](http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf).
    10. Mach. Learn. Res. 2010.

</div>

---

decomposition.md

---

# Decomposing signals in components (matrix factorization problems)

<div class="currentmodule">

sklearn.decomposition

</div>

## Principal component analysis (PCA)

### Exact PCA and probabilistic interpretation

PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, <span class="title-ref">PCA</span> is implemented as a *transformer* object that learns \(n\) components in its `fit` method, and can be used on new data to project it on these components.

PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter `whiten=True` makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.

Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:

![](../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_lda_001.png)

The <span class="title-ref">PCA</span> object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a `score` method that can be used in cross-validation:

![](../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_iris.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_iris.py)
  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_lda.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_lda.py)
  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_fa\_model\_selection.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_fa\_model\_selection.py)

### Incremental PCA

The <span class="title-ref">PCA</span> object is very useful, but has certain limitations for large datasets. The biggest limitation is that <span class="title-ref">PCA</span> only supports batch processing, which means all of the data to be processed must fit in main memory. The <span class="title-ref">IncrementalPCA</span> object uses a different form of processing and allows for partial computations which almost exactly match the results of <span class="title-ref">PCA</span> while processing the data in a minibatch fashion. <span class="title-ref">IncrementalPCA</span> makes it possible to implement out-of-core Principal Component Analysis either by:

  - Using its `partial_fit` method on chunks of data fetched sequentially from the local hard drive or a network database.
  - Calling its fit method on a memory mapped file using `numpy.memmap`.

<span class="title-ref">IncrementalPCA</span> only stores estimates of component and noise variances, in order update `explained_variance_ratio_` incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset.

As in <span class="title-ref">PCA</span>, <span class="title-ref">IncrementalPCA</span> centers but does not scale the input data for each feature before applying the SVD.

![](../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_001.png)

![](../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_002.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_incremental\_pca.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_incremental\_pca.py)

### PCA using randomized SVD

It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.

For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.

The class <span class="title-ref">PCA</span> used with the optional parameter `svd_solver='randomized'` is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.

For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:

<div class="centered">

[![orig\_img](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_001.png)](../auto_examples/decomposition/plot_faces_decomposition.html) [![pca\_img](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png)](../auto_examples/decomposition/plot_faces_decomposition.html)

</div>

If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and \(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity of the randomized <span class="title-ref">PCA</span> is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method implemented in <span class="title-ref">PCA</span>.

The memory footprint of randomized <span class="title-ref">PCA</span> is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max}
\cdot n_{\min}\) for the exact method.

Note: the implementation of `inverse_transform` in <span class="title-ref">PCA</span> with `svd_solver='randomized'` is not the exact inverse transform of `transform` even when `whiten=False` (default).

**Examples**

  - \[sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py)
  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py)

**References**

  - Algorithm 4.3 in `"Finding structure with randomness: Stochastic algorithms for
    constructing approximate matrix decompositions" <0909.4061>` Halko, et al., 2009
  - `"An implementation of a randomized algorithm for principal component
    analysis" <1412.3510>` A. Szlam et al. 2014

### Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)

<span class="title-ref">SparsePCA</span> is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data.

Mini-batch sparse PCA (<span class="title-ref">MiniBatchSparsePCA</span>) is a variant of <span class="title-ref">SparsePCA</span> that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.

Principal component analysis (<span class="title-ref">PCA</span>) has the disadvantage that the components extracted by this method have exclusively dense expressions, i.e. they have non-zero coefficients when expressed as linear combinations of the original variables. This can make interpretation difficult. In many cases, the real underlying components can be more naturally imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces.

Sparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples.

The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see [\[Jen09\]](#Jen09) for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.

<div class="centered">

[![pca\_img](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png)](../auto_examples/decomposition/plot_faces_decomposition.html) [![spca\_img](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_005.png)](../auto_examples/decomposition/plot_faces_decomposition.html)

</div>

Note that there are many different formulations for the Sparse PCA problem. The one implemented here is based on [\[Mrl09\]](#Mrl09) . The optimization problem solved is a PCA problem (dictionary learning) with an \(\ell_1\) penalty on the components:

\[\begin{aligned}
(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2}
||X-UV||_{\text{Fro}}^2+\alpha||V||_{1,1} \\
\text{subject to } & ||U_k||_2 <= 1 \text{ for all }
0 \leq k < n_{components}
\end{aligned}\]

\(||.||_{\text{Fro}}\) stands for the Frobenius norm and \(||.||_{1,1}\) stands for the entry-wise matrix norm which is the sum of the absolute values of all the entries in the matrix. The sparsity-inducing \(||.||_{1,1}\) matrix norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter `alpha`. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.

\> **Note** \> While in the spirit of an online algorithm, the class <span class="title-ref">MiniBatchSparsePCA</span> does not implement `partial_fit` because the algorithm is online along the features direction, not the samples direction.

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py)

**References**

## Kernel Principal Component Analysis (kPCA)

### Exact Kernel PCA

<span class="title-ref">KernelPCA</span> is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels (see \[metrics\](\#metrics)) [\[Scholkopf1997\]](#Scholkopf1997). It has many applications including denoising, compression and structured prediction (kernel dependency estimation). <span class="title-ref">KernelPCA</span> supports both `transform` and `inverse_transform`.

![](../auto_examples/decomposition/images/sphx_glr_plot_kernel_pca_002.png)

<div class="note">

<div class="title">

Note

</div>

<span class="title-ref">KernelPCA.inverse\_transform</span> relies on a kernel ridge to learn the function mapping samples from the PCA basis into the original feature space [\[Bakir2003\]](#Bakir2003). Thus, the reconstruction obtained with <span class="title-ref">KernelPCA.inverse\_transform</span> is an approximation. See the example linked below for more details.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_kernel\_pca.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_kernel\_pca.py)
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_digits\_denoising.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_digits\_denoising.py)

**References**

### Choice of solver for Kernel PCA

While in <span class="title-ref">PCA</span> the number of components is bounded by the number of features, in <span class="title-ref">KernelPCA</span> the number of components is bounded by the number of samples. Many real-world datasets have large number of samples\! In these cases finding *all* the components with a full kPCA is a waste of computation time, as data is mostly described by the first few components (e.g. `n_components<=100`). In other words, the centered Gram matrix that is eigendecomposed in the Kernel PCA fitting process has an effective rank that is much smaller than its size. This is a situation where approximate eigensolvers can provide speedup with very low precision loss.

<div class="dropdown">

Eigensolvers

The optional parameter `eigen_solver='randomized'` can be used to *significantly* reduce the computation time when the number of requested `n_components` is small compared with the number of samples. It relies on randomized decomposition methods to find an approximate solution in a shorter time.

The time complexity of the randomized <span class="title-ref">KernelPCA</span> is \(O(n_{\mathrm{samples}}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\mathrm{samples}}^3)\) for the exact method implemented with `eigen_solver='dense'`.

The memory footprint of randomized <span class="title-ref">KernelPCA</span> is also proportional to \(2 \cdot n_{\mathrm{samples}} \cdot n_{\mathrm{components}}\) instead of \(n_{\mathrm{samples}}^2\) for the exact method.

Note: this technique is the same as in \[RandomizedPCA\](\#randomizedpca).

In addition to the above two solvers, `eigen_solver='arpack'` can be used as an alternate way to get an approximate decomposition. In practice, this method only provides reasonable execution times when the number of components to find is extremely small. It is enabled by default when the desired number of components is less than 10 (strict) and the number of samples is more than 200 (strict). See <span class="title-ref">KernelPCA</span> for details.

**References**

  - *dense* solver: [scipy.linalg.eigh documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html)
  - *randomized* solver:
      - Algorithm 4.3 in `"Finding structure with randomness: Stochastic
        algorithms for constructing approximate matrix decompositions" <0909.4061>` Halko, et al. (2009)
      - `"An implementation of a randomized algorithm
        for principal component analysis" <1412.3510>`
        1.  Szlam et al. (2014)
  - *arpack* solver: [scipy.sparse.linalg.eigsh documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html)
    18. 2.  Lehoucq, D. C. Sorensen, and C. Yang, (1998)

</div>

## Truncated singular value decomposition and latent semantic analysis

<span class="title-ref">TruncatedSVD</span> implements a variant of singular value decomposition (SVD) that only computes the \(k\) largest singular values, where \(k\) is a user-specified parameter.

<span class="title-ref">TruncatedSVD</span> is very similar to <span class="title-ref">PCA</span>, but differs in that the matrix \(X\) does not need to be centered. When the columnwise (per-feature) means of \(X\) are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA.

<div class="dropdown">

About truncated SVD and latent semantic analysis (LSA)

When truncated SVD is applied to term-document matrices (as returned by <span class="title-ref">\~sklearn.feature\_extraction.text.CountVectorizer</span> or <span class="title-ref">\~sklearn.feature\_extraction.text.TfidfVectorizer</span>), this transformation is known as [latent semantic analysis](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf) (LSA), because it transforms such matrices to a "semantic" space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.

<div class="note">

<div class="title">

Note

</div>

LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.

</div>

Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):

\[X \approx X_k = U_k \Sigma_k V_k^\top\]

After this operation, \(U_k \Sigma_k\) is the transformed training set with \(k\) features (called `n_components` in the API).

To also transform a test set \(X\), we multiply it with \(V_k\):

\[X' = X V_k\]

<div class="note">

<div class="title">

Note

</div>

Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape `(n_features, n_samples)`. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.

</div>

While the <span class="title-ref">TruncatedSVD</span> transformer works with any feature matrix, using it on tf-idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (`sublinear_tf=True, use_idf=True`) to bring the feature values closer to a Gaussian distribution, compensating for LSA's erroneous assumptions about textual data.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py)

**References**

  - Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), *Introduction to Information Retrieval*, Cambridge University Press, chapter 18: [Matrix decompositions & latent semantic indexing](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf)

## Dictionary Learning

### Sparse coding with a precomputed dictionary

The <span class="title-ref">SparseCoder</span> object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a `fit` method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the `transform_method` initialization parameter:

  - Orthogonal matching pursuit (\[omp\](\#omp))
  - Least-angle regression (\[least\_angle\_regression\](\#least\_angle\_regression))
  - Lasso computed by least-angle regression
  - Lasso using coordinate descent (\[lasso\](\#lasso))
  - Thresholding

Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction.

The dictionary learning objects offer, via the `split_code` parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.

The split code for a single sample has length `2 * n_components` and is constructed using the following rule: First, the regular code of length `n_components` is computed. Then, the first `n_components` entries of the `split_code` are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split\_code is non-negative.

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_sparse\_coding.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_sparse\_coding.py)

### Generic dictionary learning

Dictionary learning (<span class="title-ref">DictionaryLearning</span>) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data.

Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks.

Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code.

\[\begin{aligned}
(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2}
||X-UV||_{\text{Fro}}^2+\alpha||U||_{1,1} \\
\text{subject to } & ||V_k||_2 <= 1 \text{ for all }
0 \leq k < n_{\mathrm{atoms}}
\end{aligned}\]

<div class="centered">

[![pca\_img2](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png)](../auto_examples/decomposition/plot_faces_decomposition.html) [![dict\_img2](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_007.png)](../auto_examples/decomposition/plot_faces_decomposition.html)

</div>

\(||.||_{\text{Fro}}\) stands for the Frobenius norm and \(||.||_{1,1}\) stands for the entry-wise matrix norm which is the sum of the absolute values of all the entries in the matrix. After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see \[SparseCoder\](\#sparsecoder)).

It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.

<div class="centered">

[![dict\_img\_pos1](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_010.png)](../auto_examples/decomposition/plot_image_denoising.html) [![dict\_img\_pos2](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_011.png)](../auto_examples/decomposition/plot_image_denoising.html)

</div>

<div class="centered">

[![dict\_img\_pos3](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_012.png)](../auto_examples/decomposition/plot_image_denoising.html) [![dict\_img\_pos4](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_013.png)](../auto_examples/decomposition/plot_image_denoising.html)

</div>

The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.

![](../auto_examples/decomposition/images/sphx_glr_plot_image_denoising_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_image\_denoising.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_image\_denoising.py)

**References**

  - ["Online dictionary learning for sparse coding"](https://www.di.ens.fr/sierra/pdfs/icml09.pdf)
    10. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009

### Mini-batch dictionary learning

<span class="title-ref">MiniBatchDictionaryLearning</span> implements a faster, but less accurate version of the dictionary learning algorithm that is better suited for large datasets.

By default, <span class="title-ref">MiniBatchDictionaryLearning</span> divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition.

The estimator also implements `partial_fit`, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.

<div class="currentmodule">

sklearn.cluster

</div>

[![image](../auto_examples/cluster/images/sphx_glr_plot_dict_face_patches_001.png)](../auto_examples/cluster/plot_dict_face_patches.html)

<div class="topic">

****Clustering for dictionary learning****

Note that when using dictionary learning to extract a representation (e.g. for sparse coding) clustering can be a good proxy to learn the dictionary. For instance the <span class="title-ref">MiniBatchKMeans</span> estimator is computationally efficient and implements on-line learning with a `partial_fit` method.

Example: \[sphx\_glr\_auto\_examples\_cluster\_plot\_dict\_face\_patches.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_dict\_face\_patches.py)

</div>

<div class="currentmodule">

sklearn.decomposition

</div>

## Factor Analysis

In unsupervised learning we only have a dataset \(X = \{x_1, x_2, \dots, x_n
\}\). How can this dataset be described mathematically? A very simple <span class="title-ref">continuous latent variable</span> model for \(X\) is

\[x_i = W h_i + \mu + \epsilon\]

The vector \(h_i\) is called "latent" because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called "generative" as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)'s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)'s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):

\[\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}\]

In other words, we *decomposed* matrix \(\mathbf{X}\).

If \(h_i\) is given, the above equation automatically implies the following probabilistic interpretation:

\[p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)\]

For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0,
\mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):

\[p(x) = \mathcal{N}(\mu, WW^T + \Psi)\]

Now, without any further assumptions the idea of having a latent variable \(h\) would be superfluous -- \(x\) can be completely modelled with a mean and a covariance. We need to impose some more specific structure on one of these two parameters. A simple additional assumption regards the structure of the error covariance \(\Psi\):

  - \(\Psi = \sigma^2 \mathbf{I}\): This assumption leads to the probabilistic model of <span class="title-ref">PCA</span>.
  - \(\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)\): This model is called <span class="title-ref">FactorAnalysis</span>, a classical statistical model. The matrix W is sometimes called the "factor loading matrix".

Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. <span class="title-ref">FastICA</span>) if non-Gaussian priors on the latent variables are assumed.

Factor analysis *can* produce similar components (the columns of its loading matrix) to <span class="title-ref">PCA</span>. However, one can not make any general statements about these components (e.g. whether they are orthogonal):

<div class="centered">

[![pca\_img3](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png)](../auto_examples/decomposition/plot_faces_decomposition.html) [![fa\_img3](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_008.png)](../auto_examples/decomposition/plot_faces_decomposition.html)

</div>

The main advantage for Factor Analysis over <span class="title-ref">PCA</span> is that it can model the variance in every direction of the input space independently (heteroscedastic noise):

![](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_009.png)

This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:

![](../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_002.png)

Factor Analysis is often followed by a rotation of the factors (with the parameter <span class="title-ref">rotation</span>), usually to improve interpretability. For example, Varimax rotation maximizes the sum of the variances of the squared loadings, i.e., it tends to produce sparser factors, which are influenced by only a few features each (the "simple structure"). See e.g., the first example below.

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_varimax\_fa.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_varimax\_fa.py)
  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_fa\_model\_selection.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_fa\_model\_selection.py)

## Independent component analysis (ICA)

Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the <span class="title-ref">Fast ICA \<FastICA\></span> algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.

It is classically used to separate mixed signals (a problem known as *blind source separation*), as in the example below:

![](../auto_examples/decomposition/images/sphx_glr_plot_ica_blind_source_separation_001.png)

ICA can also be used as yet another non linear decomposition that finds components with some sparsity:

<div class="centered">

[![pca\_img4](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png)](../auto_examples/decomposition/plot_faces_decomposition.html) [![ica\_img4](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_004.png)](../auto_examples/decomposition/plot_faces_decomposition.html)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_ica\_blind\_source\_separation.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_ica\_blind\_source\_separation.py)
  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_ica\_vs\_pca.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_ica\_vs\_pca.py)
  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py)

## Non-negative matrix factorization (NMF or NNMF)

### NMF with the Frobenius norm

<span class="title-ref">NMF</span>\[1\] is an alternative approach to decomposition that assumes that the data and the components are non-negative. <span class="title-ref">NMF</span> can be plugged in instead of <span class="title-ref">PCA</span> or its variants, in the cases where the data matrix does not contain negative values. It finds a decomposition of samples \(X\) into two matrices \(W\) and \(H\) of non-negative elements, by optimizing the distance \(d\) between \(X\) and the matrix product \(WH\). The most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:

\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]

Unlike <span class="title-ref">PCA</span>, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text.

It has been observed in \[Hoyer, 2004\]\[2\] that, when carefully constrained, <span class="title-ref">NMF</span> can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by <span class="title-ref">NMF</span> from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.

<div class="centered">

[![pca\_img5](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png)](../auto_examples/decomposition/plot_faces_decomposition.html) [![nmf\_img5](../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_003.png)](../auto_examples/decomposition/plot_faces_decomposition.html)

</div>

The <span class="title-ref">init</span> attribute determines the initialization method applied, which has a great impact on the performance of the method. <span class="title-ref">NMF</span> implements the method Nonnegative Double Singular Value Decomposition. NNDSVD\[3\] is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.

Note that the Multiplicative Update ('mu') solver cannot update zeros present in the initialization, so it leads to poorer results when used jointly with the basic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or NNDSVDar should be preferred.

<span class="title-ref">NMF</span> can also be initialized with correctly scaled random non-negative matrices by setting <span class="title-ref">init="random"</span>. An integer seed or a `RandomState` can also be passed to <span class="title-ref">random\_state</span> to control reproducibility.

In <span class="title-ref">NMF</span>, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in <span class="title-ref">\~sklearn.linear\_model.ElasticNet</span>, we control the combination of L1 and L2 with the <span class="title-ref">l1\_ratio</span> (\(\rho\)) parameter, and the intensity of the regularization with the <span class="title-ref">alpha\_W</span> and <span class="title-ref">alpha\_H</span> (\(\alpha_W\) and \(\alpha_H\)) parameters. The priors are scaled by the number of samples (\(n\_samples\)) for <span class="title-ref">H</span> and the number of features (\(n\_features\)) for <span class="title-ref">W</span> to keep their impact balanced with respect to one another and to the data fit term as independent as possible of the size of the training set. Then the priors terms are:

\[(\alpha_W \rho ||W||_1 + \frac{\alpha_W(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2) * n\_features
+ (\alpha_H \rho ||H||_1 + \frac{\alpha_H(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2) * n\_samples\]

and the regularized objective function is:

\[d_{\mathrm{Fro}}(X, WH)
+ (\alpha_W \rho ||W||_1 + \frac{\alpha_W(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2) * n\_features
+ (\alpha_H \rho ||H||_1 + \frac{\alpha_H(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2) * n\_samples\]

### NMF with a beta-divergence

As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:

\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{Fro}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]

Other distance functions can be used in NMF as, for example, the (generalized) Kullback-Leibler (KL) divergence, also referred as I-divergence:

\[d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\]

Or, the Itakura-Saito (IS) divergence:

\[d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)\]

These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively\[4\]. The beta-divergence are defined by :

\[d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})\]

![image](../images/beta_divergence.png)

Note that this definition is not valid if \(\beta \in (0; 1)\), yet it can be continuously extended to the definitions of \(d_{KL}\) and \(d_{IS}\) respectively.

<div class="dropdown">

NMF implemented solvers

<span class="title-ref">NMF</span> implements two solvers, using Coordinate Descent ('cd')\[5\], and Multiplicative Update ('mu')\[6\]. The 'mu' solver can optimize every beta-divergence, including of course the Frobenius norm (\(\beta=2\)), the (generalized) Kullback-Leibler divergence (\(\beta=1\)) and the Itakura-Saito divergence (\(\beta=0\)). Note that for \(\beta \in (1; 2)\), the 'mu' solver is significantly faster than for other values of \(\beta\). Note also that with a negative (or 0, i.e. 'itakura-saito') \(\beta\), the input matrix cannot contain zero values.

The 'cd' solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function.

</div>

NMF is best used with the `fit_transform` method, which returns the matrix W. The matrix H is stored into the fitted model in the `components_` attribute; the method `transform` will decompose a new matrix X\_new based on these stored components:

    >>> import numpy as np
    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
    >>> from sklearn.decomposition import NMF
    >>> model = NMF(n_components=2, init='random', random_state=0)
    >>> W = model.fit_transform(X)
    >>> H = model.components_
    >>> X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])
    >>> W_new = model.transform(X_new)

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_faces\_decomposition.py)
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_topics\_extraction\_with\_nmf\_lda.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_topics\_extraction\_with\_nmf\_lda.py)

### Mini-batch Non Negative Matrix Factorization

<span class="title-ref">MiniBatchNMF</span>\[7\] implements a faster, but less accurate version of the non negative matrix factorization (i.e. <span class="title-ref">\~sklearn.decomposition.NMF</span>), better suited for large datasets.

By default, <span class="title-ref">MiniBatchNMF</span> divides the data into mini-batches and optimizes the NMF model in an online manner by cycling over the mini-batches for the specified number of iterations. The `batch_size` parameter controls the size of the batches.

In order to speed up the mini-batch algorithm it is also possible to scale past batches, giving them less importance than newer batches. This is done introducing a so-called forgetting factor controlled by the `forget_factor` parameter.

The estimator also implements `partial_fit`, which updates `H` by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or when the data does not fit into memory.

**References**

## Latent Dirichlet Allocation (LDA)

Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.

The graphical model of LDA is a three-level generative model:

![image](../images/lda_model_graph.png)

Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013):

  - The corpus is a collection of \(D\) documents.
  - A document is a sequence of \(N\) words.
  - There are \(K\) topics in the corpus.
  - The boxes represent repeated sampling.

In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure.

<div class="dropdown">

Details on modeling text corpora

When modeling text corpora, the model assumes the following generative process for a corpus with \(D\) documents and \(K\) topics, with \(K\) corresponding to <span class="title-ref">n\_components</span> in the API:

1.  For each topic \(k \in K\), draw \(\beta_k \sim
    \mathrm{Dirichlet}(\eta)\). This provides a distribution over the words, i.e. the probability of a word appearing in topic \(k\). \(\eta\) corresponds to <span class="title-ref">topic\_word\_prior</span>.
2.  For each document \(d \in D\), draw the topic proportions \(\theta_d \sim \mathrm{Dirichlet}(\alpha)\). \(\alpha\) corresponds to <span class="title-ref">doc\_topic\_prior</span>.
3.  For each word \(i\) in document \(d\):
    1.  Draw the topic assignment \(z_{di} \sim \mathrm{Multinomial}
        (\theta_d)\)
    2.  Draw the observed word \(w_{ij} \sim \mathrm{Multinomial}
        (\beta_{z_{di}})\)

For parameter estimation, the posterior distribution is:

\[p(z, \theta, \beta |w, \alpha, \eta) =
\frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}\]

Since the posterior is intractable, variational Bayesian method uses a simpler distribution \(q(z,\theta,\beta | \lambda, \phi, \gamma)\) to approximate it, and those variational parameters \(\lambda\), \(\phi\), \(\gamma\) are optimized to maximize the Evidence Lower Bound (ELBO):

\[\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]\]

Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).

</div>

<span class="title-ref">LatentDirichletAllocation</span> implements the online variational Bayes algorithm and supports both online and batch update methods. While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points.

\> **Note** \> Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.

When <span class="title-ref">LatentDirichletAllocation</span> is applied on a "document-term" matrix, the matrix will be decomposed into a "topic-term" matrix and a "document-topic" matrix. While "topic-term" matrix is stored as <span class="title-ref">components\_</span> in the model, "document-topic" matrix can be calculated from `transform` method.

<span class="title-ref">LatentDirichletAllocation</span> also implements `partial_fit` method. This is used when data can be fetched sequentially.

**Examples**

  - \[sphx\_glr\_auto\_examples\_applications\_plot\_topics\_extraction\_with\_nmf\_lda.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_topics\_extraction\_with\_nmf\_lda.py)

**References**

  - ["Latent Dirichlet Allocation"](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
    4.  Blei, A. Ng, M. Jordan, 2003
  - ["Online Learning for Latent Dirichlet Allocation”](https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf)
    13. Hoffman, D. Blei, F. Bach, 2010
  - ["Stochastic Variational Inference"](https://www.cs.columbia.edu/~blei/papers/HoffmanBleiWangPaisley2013.pdf)
    13. Hoffman, D. Blei, C. Wang, J. Paisley, 2013
  - ["The varimax criterion for analytic rotation in factor analysis"](https://link.springer.com/article/10.1007%2FBF02289233)
    8.  6.  Kaiser, 1958

See also \[nca\_dim\_reduction\](\#nca\_dim\_reduction) for dimensionality reduction with Neighborhood Components Analysis.

<div id="citations">

  - <span id="Bakir2003" class="citation-label">Bakir2003</span>  
    Bakır, Gökhan H., Jason Weston, and Bernhard Schölkopf. ["Learning to find pre-images."](https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf) Advances in neural information processing systems 16 (2003): 449-456.

  - <span id="Jen09" class="citation-label">Jen09</span>  
    ["Structured Sparse Principal Component Analysis"](https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf) R. Jenatton, G. Obozinski, F. Bach, 2009

  - <span id="Mrl09" class="citation-label">Mrl09</span>  
    ["Online Dictionary Learning for Sparse Coding"](https://www.di.ens.fr/sierra/pdfs/icml09.pdf) J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009

  - <span id="Scholkopf1997" class="citation-label">Scholkopf1997</span>  
    Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller. ["Kernel principal component analysis."](https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf) International conference on artificial neural networks. Springer, Berlin, Heidelberg, 1997.

</div>

1.  ["Learning the parts of objects by non-negative matrix factorization"](http://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf) D. Lee, S. Seung, 1999

2.  ["Non-negative Matrix Factorization with Sparseness Constraints"](https://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf) P. Hoyer, 2004

3.  ["SVD based initialization: A head start for nonnegative matrix factorization"](https://www.boutsidis.org/Boutsidis_PRE_08.pdf) C. Boutsidis, E. Gallopoulos, 2008

4.  `"Algorithms for nonnegative matrix factorization with
    the beta-divergence" <1010.1763>` C. Fevotte, J. Idier, 2011

5.  ["Fast local algorithms for large scale nonnegative matrix and tensor factorizations."](https://www.researchgate.net/profile/Anh-Huy-Phan/publication/220241471_Fast_Local_Algorithms_for_Large_Scale_Nonnegative_Matrix_and_Tensor_Factorizations) A. Cichocki, A. Phan, 2009

6.  `"Algorithms for nonnegative matrix factorization with
    the beta-divergence" <1010.1763>` C. Fevotte, J. Idier, 2011

7.  `"Online algorithms for nonnegative matrix factorization with the
    Itakura-Saito divergence" <1106.4198>` A. Lefevre, F. Bach, C. Fevotte, 2011

---

density.md

---

# Density Estimation

<div class="sectionauthor">

Jake Vanderplas \<<vanderplas@astro.washington.edu>\>

</div>

Density estimation walks the line between unsupervised learning, feature engineering, and data modeling. Some of the most popular and useful density estimation techniques are mixture models such as Gaussian Mixtures (<span class="title-ref">\~sklearn.mixture.GaussianMixture</span>), and neighbor-based approaches such as the kernel density estimate (<span class="title-ref">\~sklearn.neighbors.KernelDensity</span>). Gaussian Mixtures are discussed more fully in the context of \[clustering \<clustering\>\](\#clustering-\<clustering\>), because the technique is also useful as an unsupervised clustering scheme.

Density estimation is a very simple concept, and most people are already familiar with one common density estimation technique: the histogram.

## Density Estimation: Histograms

A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure:

<div class="centered">

[![hist\_to\_kde](../auto_examples/neighbors/images/sphx_glr_plot_kde_1d_001.png)](../auto_examples/neighbors/plot_kde_1d.html)

</div>

A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data.

Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.

This visualization is an example of a *kernel density estimation*, in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points.

## Kernel Density Estimation

Kernel density estimation in scikit-learn is implemented in the <span class="title-ref">\~sklearn.neighbors.KernelDensity</span> estimator, which uses the Ball Tree or KD Tree for efficient queries (see \[neighbors\](\#neighbors) for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.

In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels:

<div class="centered">

[![kde\_1d\_distribution](../auto_examples/neighbors/images/sphx_glr_plot_kde_1d_003.png)](../auto_examples/neighbors/plot_kde_1d.html)

</div>

It's clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:

> \>\>\> from sklearn.neighbors import KernelDensity \>\>\> import numpy as np \>\>\> X = np.array(\[\[-1, -1\], \[-2, -1\], \[-3, -2\], \[1, 1\], \[2, 1\], \[3, 2\]\]) \>\>\> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X) \>\>\> kde.score\_samples(X) array(\[-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698, -0.41076071\])

Here we have used `kernel='gaussian'`, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:

\[\rho_K(y) = \sum_{i=1}^{N} K(y - x_i; h)\]

The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.

The parameter <span class="title-ref">bandwidth</span> controls this smoothing. One can either set manually this parameter or use Scott's and Silverman's estimation methods.

<span class="title-ref">\~sklearn.neighbors.KernelDensity</span> implements several common kernel forms, which are shown in the following figure:

<div class="centered">

[![kde\_kernels](../auto_examples/neighbors/images/sphx_glr_plot_kde_1d_002.png)](../auto_examples/neighbors/plot_kde_1d.html)

</div>

<div class="dropdown">

Kernels' mathematical expressions

The form of these kernels is as follows:

  - Gaussian kernel (`kernel = 'gaussian'`)
    
    \(K(x; h) \propto \exp(- \frac{x^2}{2h^2} )\)

  - Tophat kernel (`kernel = 'tophat'`)
    
    \(K(x; h) \propto 1\) if \(x < h\)

  - Epanechnikov kernel (`kernel = 'epanechnikov'`)
    
    \(K(x; h) \propto 1 - \frac{x^2}{h^2}\)

  - Exponential kernel (`kernel = 'exponential'`)
    
    \(K(x; h) \propto \exp(-x/h)\)

  - Linear kernel (`kernel = 'linear'`)
    
    \(K(x; h) \propto 1 - x/h\) if \(x < h\)

  - Cosine kernel (`kernel = 'cosine'`)
    
    \(K(x; h) \propto \cos(\frac{\pi x}{2h})\) if \(x < h\)

</div>

The kernel density estimator can be used with any of the valid distance metrics (see <span class="title-ref">\~sklearn.metrics.DistanceMetric</span> for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the [Haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:

<div class="centered">

[![species\_kde](../auto_examples/neighbors/images/sphx_glr_plot_species_kde_001.png)](../auto_examples/neighbors/plot_species_kde.html)

</div>

One other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in order to efficiently draw new samples from this generative model. Here is an example of using this process to create a new set of hand-written digits, using a Gaussian kernel learned on a PCA projection of the data:

<div class="centered">

[![digits\_kde](../auto_examples/neighbors/images/sphx_glr_plot_digits_kde_sampling_001.png)](../auto_examples/neighbors/plot_digits_kde_sampling.html)

</div>

The "new" data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model.

**Examples**

  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_kde\_1d.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_kde\_1d.py): computation of simple kernel density estimates in one dimension.
  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_digits\_kde\_sampling.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_digits\_kde\_sampling.py): an example of using Kernel Density estimation to learn a generative model of the hand-written digits data, and drawing new samples from this model.
  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_species\_kde.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_species\_kde.py): an example of Kernel Density estimation using the Haversine distance metric to visualize geospatial data

---

ensemble.md

---

# Ensembles: Gradient boosting, random forests, bagging, voting, stacking

<div class="currentmodule">

sklearn.ensemble

</div>

**Ensemble methods** combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.

Two very famous examples of ensemble methods are \[gradient-boosted trees \<gradient\_boosting\>\](\#gradient-boosted-trees \<gradient\_boosting\>) and \[random forests \<forest\>\](\#random-forests-\<forest\>).

More generally, ensemble models can be applied to any base learner beyond trees, in averaging methods such as \[Bagging methods \<bagging\>\](\#bagging-methods-\<bagging\>), \[model stacking \<stacking\>\](\#model-stacking-\<stacking\>), or \[Voting \<voting\_classifier\>\](\#voting-\<voting\_classifier\>), or in boosting, as \[AdaBoost \<adaboost\>\](\#adaboost-\<adaboost\>).

## Gradient-boosted trees

[Gradient Tree Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions, see the seminal work of [\[Friedman2001\]](#Friedman2001). GBDT is an excellent model for both regression and classification, in particular for tabular data.

<div class="topic">

**<span class="title-ref">GradientBoostingClassifier</span> vs <span class="title-ref">HistGradientBoostingClassifier</span>**

Scikit-learn provides two implementations of gradient-boosted trees: <span class="title-ref">HistGradientBoostingClassifier</span> vs <span class="title-ref">GradientBoostingClassifier</span> for classification, and the corresponding classes for regression. The former can be **orders of magnitude faster** than the latter when the number of samples is larger than tens of thousands of samples.

Missing values and categorical data are natively supported by the Hist... version, removing the need for additional preprocessing such as imputation.

<span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span>, might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting.

</div>

### Histogram-Based Gradient Boosting

Scikit-learn 0.21 introduced two new implementations of gradient boosted trees, namely <span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span>, inspired by [LightGBM](https://github.com/Microsoft/LightGBM) (See [\[LightGBM\]](#LightGBM)).

These histogram-based estimators can be **orders of magnitude faster** than <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span> when the number of samples is larger than tens of thousands of samples.

They also have built-in support for missing values, which avoids the need for an imputer.

These fast estimators first bin the input samples `X` into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span> are not yet supported, for instance some loss functions.

**Examples**

  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py)
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_hist\_grad\_boosting\_comparison.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_hist\_grad\_boosting\_comparison.py)

#### Usage

Most of the parameters are unchanged from <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span>. One exception is the `max_iter` parameter that replaces `n_estimators`, and controls the number of iterations of the boosting process:

    >>> from sklearn.ensemble import HistGradientBoostingClassifier
    >>> from sklearn.datasets import make_hastie_10_2
    
    >>> X, y = make_hastie_10_2(random_state=0)
    >>> X_train, X_test = X[:2000], X[2000:]
    >>> y_train, y_test = y[:2000], y[2000:]
    
    >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)
    >>> clf.score(X_test, y_test)
    0.8965

Available losses for **regression** are:

  - 'squared\_error', which is the default loss;
  - 'absolute\_error', which is less sensitive to outliers than the squared error;
  - 'gamma', which is well suited to model strictly positive outcomes;
  - 'poisson', which is well suited to model counts and frequencies;
  - 'quantile', which allows for estimating a conditional quantile that can later be used to obtain prediction intervals.

For **classification**, 'log\_loss' is the only option. For binary classification it uses the binary log loss, also known as binomial deviance or binary cross-entropy. For <span class="title-ref">n\_classes \>= 3</span>, it uses the multi-class log loss function, with multinomial deviance and categorical cross-entropy as alternative names. The appropriate loss version is selected based on `y` passed to `fit`.

The size of the trees can be controlled through the `max_leaf_nodes`, `max_depth`, and `min_samples_leaf` parameters.

The number of bins used to bin the data is controlled with the `max_bins` parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible (255), which is the default.

The `l2_regularization` parameter acts as a regularizer for the loss function, and corresponds to \(\lambda\) in the following expression (see equation (2) in [\[XGBoost\]](#XGBoost)):

\[\mathcal{L}(\phi) =  \sum_i l(\hat{y}_i, y_i) + \frac12 \sum_k \lambda ||w_k||^2\]

<div class="dropdown">

Details on l2 regularization

It is important to notice that the loss term \(l(\hat{y}_i, y_i)\) describes only half of the actual loss function except for the pinball loss and absolute error.

The index \(k\) refers to the k-th tree in the ensemble of trees. In the case of regression and binary classification, gradient boosting models grow one tree per iteration, then \(k\) runs up to <span class="title-ref">max\_iter</span>. In the case of multiclass classification problems, the maximal value of the index \(k\) is <span class="title-ref">n\_classes</span> \(\times\) <span class="title-ref">max\_iter</span>.

If \(T_k\) denotes the number of leaves in the k-th tree, then \(w_k\) is a vector of length \(T_k\), which contains the leaf values of the form <span class="title-ref">w = -sum\_gradient / (sum\_hessian + l2\_regularization)</span> (see equation (5) in [\[XGBoost\]](#XGBoost)).

The leaf values \(w_k\) are derived by dividing the sum of the gradients of the loss function by the combined sum of hessians. Adding the regularization to the denominator penalizes the leaves with small hessians (flat regions), resulting in smaller updates. Those \(w_k\) values contribute then to the model's prediction for a given input that ends up in the corresponding leaf. The final prediction is the sum of the base prediction and the contributions from each tree. The result of that sum is then transformed by the inverse link function depending on the choice of the loss function (see \[gradient\_boosting\_formulation\](\#gradient\_boosting\_formulation)).

Notice that the original paper [\[XGBoost\]](#XGBoost) introduces a term \(\gamma\sum_k
T_k\) that penalizes the number of leaves (making it a smooth version of <span class="title-ref">max\_leaf\_nodes</span>) not presented here as it is not implemented in scikit-learn; whereas \(\lambda\) penalizes the magnitude of the individual tree predictions before being rescaled by the learning rate, see \[gradient\_boosting\_shrinkage\](\#gradient\_boosting\_shrinkage).

</div>

Note that **early-stopping is enabled by default if the number of samples is larger than 10,000**. The early-stopping behaviour is controlled via the `early_stopping`, `scoring`, `validation_fraction`, `n_iter_no_change`, and `tol` parameters. It is possible to early-stop using an arbitrary `scorer`, or just the training or validation loss. Note that for technical reasons, using a callable as a scorer is significantly slower than using the loss. By default, early-stopping is performed if there are at least 10,000 samples in the training set, using the validation loss.

#### Missing values support

<span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span> have built-in support for missing values (NaNs).

During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently:

    >>> from sklearn.ensemble import HistGradientBoostingClassifier
    >>> import numpy as np
    
    >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)
    >>> y = [0, 0, 1, 1]
    
    >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)
    >>> gbdt.predict(X)
    array([0, 0, 1, 1])

When the missingness pattern is predictive, the splits can be performed on whether the feature value is missing or not:

    >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)
    >>> y = [0, 1, 0, 0, 1]
    >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,
    ...                                       max_depth=2,
    ...                                       learning_rate=1,
    ...                                       max_iter=1).fit(X, y)
    >>> gbdt.predict(X)
    array([0, 1, 0, 0, 1])

If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_hgbt\_regression.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_hgbt\_regression.py)

#### Sample weight support

<span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span> support sample weights during `fit`.

The following toy example demonstrates that samples with a sample weight of zero are ignored:

> \>\>\> X = \[\[1, 0\], ... \[1, 0\], ... \[1, 0\], ... \[0, 1\]\] \>\>\> y = \[0, 0, 1, 0\] \>\>\> \# ignore the first 2 training samples by setting their weight to 0 \>\>\> sample\_weight = \[0, 0, 1, 1\] \>\>\> gb = HistGradientBoostingClassifier(min\_samples\_leaf=1) \>\>\> gb.fit(X, y, sample\_weight=sample\_weight) HistGradientBoostingClassifier(...) \>\>\> gb.predict(\[\[1, 0\]\]) array(\[1\]) \>\>\> gb.predict\_proba(\[\[1, 0\]\])\[0, 1\] 0.99...

As you can see, the <span class="title-ref">\[1, 0\]</span> is comfortably classified as <span class="title-ref">1</span> since the first two samples are ignored due to their sample weights.

Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights. Note that the binning stage (specifically the quantiles computation) does not take the weights into account.

#### Categorical Features Support

<span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span> have native support for categorical features: they can consider splits on non-ordered, categorical data.

For datasets with categorical features, using the native categorical support is often better than relying on one-hot encoding (<span class="title-ref">\~sklearn.preprocessing.OneHotEncoder</span>), because one-hot encoding requires more tree depth to achieve equivalent splits. It is also usually better to rely on the native categorical support rather than to treat categorical features as continuous (ordinal), which happens for ordinal-encoded categorical data, since categories are nominal quantities where order does not matter.

To enable categorical support, a boolean mask can be passed to the <span class="title-ref">categorical\_features</span> parameter, indicating which feature is categorical. In the following, the first feature will be treated as categorical and the second feature as numerical:

    >>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])

Equivalently, one can pass a list of integers indicating the indices of the categorical features:

    >>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])

When the input is a DataFrame, it is also possible to pass a list of column names:

    >>> gbdt = HistGradientBoostingClassifier(categorical_features=["site", "manufacturer"])

Finally, when the input is a DataFrame we can use <span class="title-ref">categorical\_features="from\_dtype"</span> in which case all columns with a categorical <span class="title-ref">dtype</span> will be treated as categorical features.

The cardinality of each categorical feature must be less than the <span class="title-ref">max\_bins</span> parameter. For an example using histogram-based gradient boosting on categorical features, see \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_categorical.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_categorical.py).

If there are missing values during training, the missing values will be treated as a proper category. If there are no missing values during training, then at prediction time, missing values are mapped to the child node that has the most samples (just like for continuous features). When predicting, categories that were not seen during fit time will be treated as missing values.

<div class="dropdown">

Split finding with categorical features

The canonical way of considering categorical splits in a tree is to consider all of the \(2^{K - 1} - 1\) partitions, where \(K\) is the number of categories. This can quickly become prohibitive when \(K\) is large. Fortunately, since gradient boosting trees are always regression trees (even for classification problems), there exist a faster strategy that can yield equivalent splits. First, the categories of a feature are sorted according to the variance of the target, for each category <span class="title-ref">k</span>. Once the categories are sorted, one can consider *continuous partitions*, i.e. treat the categories as if they were ordered continuous values (see Fisher [\[Fisher1958\]](#Fisher1958) for a formal proof). As a result, only \(K - 1\) splits need to be considered instead of \(2^{K - 1} - 1\). The initial sorting is a \(\mathcal{O}(K \log(K))\) operation, leading to a total complexity of \(\mathcal{O}(K \log(K) + K)\), instead of \(\mathcal{O}(2^K)\).

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_categorical.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_categorical.py)

#### Monotonic Constraints

Depending on the problem at hand, you may have prior knowledge indicating that a given feature should in general have a positive (or negative) effect on the target value. For example, all else being equal, a higher credit score should increase the probability of getting approved for a loan. Monotonic constraints allow you to incorporate such prior knowledge into the model.

For a predictor \(F\) with two features:

  - a **monotonic increase constraint** is a constraint of the form:
    
    \[x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)\]

  - a **monotonic decrease constraint** is a constraint of the form:
    
    \[x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)\]

You can specify a monotonic constraint on each feature using the <span class="title-ref">monotonic\_cst</span> parameter. For each feature, a value of 0 indicates no constraint, while 1 and -1 indicate a monotonic increase and monotonic decrease constraint, respectively:

    >>> from sklearn.ensemble import HistGradientBoostingRegressor
    
    ... # monotonic increase, monotonic decrease, and no constraint on the 3 features
    >>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])

In a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed to have a positive (negative) effect on the probability of samples to belong to the positive class.

Nevertheless, monotonic constraints only marginally constrain feature effects on the output. For instance, monotonic increase and decrease constraints cannot be used to enforce the following modelling constraint:

\[x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2')\]

Also, monotonic constraints are not supported for multiclass classification.

<div class="note">

<div class="title">

Note

</div>

Since categories are unordered quantities, it is not possible to enforce monotonic constraints on categorical features.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_monotonic\_constraints.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_monotonic\_constraints.py)
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_hgbt\_regression.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_hgbt\_regression.py)

#### Interaction constraints

A priori, the histogram gradient boosted trees are allowed to use any feature to split a node into child nodes. This creates so called interactions between features, i.e. usage of different features as split along a branch. Sometimes, one wants to restrict the possible interactions, see [\[Mayer2022\]](#Mayer2022). This can be done by the parameter `interaction_cst`, where one can specify the indices of features that are allowed to interact. For instance, with 3 features in total, `interaction_cst=[{0}, {1}, {2}]` forbids all interactions. The constraints `[{0, 1}, {1, 2}]` specifies two groups of possibly interacting features. Features 0 and 1 may interact with each other, as well as features 1 and 2. But note that features 0 and 2 are forbidden to interact. The following depicts a tree and the possible splits of the tree:

`` `none       1      <- Both constraint groups could be applied from now on      / \     1   2    <- Left split still fulfills both constraint groups.    / \ / \      Right split at feature 2 has only group {1, 2} from now on.  LightGBM uses the same logic for overlapping groups.  Note that features not listed in ``interaction\_cst`are automatically`<span class="title-ref"> assigned an interaction group for themselves. With again 3 features, this means that </span><span class="title-ref">\[{0}\]</span><span class="title-ref"> is equivalent to </span><span class="title-ref">\[{0}, {1, 2}\]</span>\`.

**Examples**

  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py)

**References**

#### Low-level parallelism

<span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span> use OpenMP for parallelization through Cython. For more details on how to control the number of threads, please refer to our \[parallelism\](\#parallelism) notes.

The following parts are parallelized:

  - mapping samples from real values to integer-valued bins (finding the bin thresholds is however sequential)
  - building histograms is parallelized over features
  - finding the best split point at a node is parallelized over features
  - during fit, mapping samples into the left and right children is parallelized over samples
  - gradient and hessians computations are parallelized over samples
  - predicting is parallelized over samples

#### Why it's faster

The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span>) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\) is the number of samples at the node.

<span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span>, in contrast, do not require sorting the feature values and instead use a data-structure called a histogram, where the samples are implicitly ordered. Building a histogram has a \(\mathcal{O}(n)\) complexity, so the node splitting procedure has a \(\mathcal{O}(n_\text{features} \times n)\) complexity, much smaller than the previous one. In addition, instead of considering \(n\) split points, we consider only `max_bins` split points, which might be much smaller.

In order to build histograms, the input data <span class="title-ref">X</span> needs to be binned into integer-valued bins. This binning procedure does require sorting the feature values, but it only happens once at the very beginning of the boosting process (not at each node, like in <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span>).

Finally, many parts of the implementation of <span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span> are parallelized.

**References**

### <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span>

The usage and the parameters of <span class="title-ref">GradientBoostingClassifier</span> and <span class="title-ref">GradientBoostingRegressor</span> are described below. The 2 most important parameters of these estimators are <span class="title-ref">n\_estimators</span> and <span class="title-ref">learning\_rate</span>.

<div class="dropdown">

Classification

<span class="title-ref">GradientBoostingClassifier</span> supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners:

    >>> from sklearn.datasets import make_hastie_10_2
    >>> from sklearn.ensemble import GradientBoostingClassifier
    
    >>> X, y = make_hastie_10_2(random_state=0)
    >>> X_train, X_test = X[:2000], X[2000:]
    >>> y_train, y_test = y[:2000], y[2000:]
    
    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    ...     max_depth=1, random_state=0).fit(X_train, y_train)
    >>> clf.score(X_test, y_test)
    0.913...

The number of weak learners (i.e. regression trees) is controlled by the parameter `n_estimators`; \[The size of each tree \<gradient\_boosting\_tree\_size\>\](\#the-size-of-each-tree

</div>

  - \--\<gradient\_boosting\_tree\_size\>) can be controlled either by setting the tree  
    depth via `max_depth` or by setting the number of leaf nodes via `max_leaf_nodes`. The `learning_rate` is a hyper-parameter in the range (0.0, 1.0\] that controls overfitting via \[shrinkage \<gradient\_boosting\_shrinkage\>\](\#shrinkage

\--\<gradient\_boosting\_shrinkage\>) .

> \> **Note**

  - \>  
    Classification with more than 2 classes requires the induction of `n_classes` regression trees at each iteration, thus, the total number of induced trees equals `n_classes * n_estimators`. For datasets with a large number of classes we strongly recommend to use <span class="title-ref">HistGradientBoostingClassifier</span> as an alternative to <span class="title-ref">GradientBoostingClassifier</span> .

<div class="dropdown">

Regression

<span class="title-ref">GradientBoostingRegressor</span> supports a number of \[different loss functions \<gradient\_boosting\_loss\>\](\#different-loss-functions-\<gradient\_boosting\_loss\>) for regression which can be specified via the argument `loss`; the default loss function for regression is squared error (`'squared_error'`).

    >>> import numpy as np
    >>> from sklearn.metrics import mean_squared_error
    >>> from sklearn.datasets import make_friedman1
    >>> from sklearn.ensemble import GradientBoostingRegressor
    
    >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
    >>> X_train, X_test = X[:200], X[200:]
    >>> y_train, y_test = y[:200], y[200:]
    >>> est = GradientBoostingRegressor(
    ...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,
    ...     loss='squared_error'
    ... ).fit(X_train, y_train)
    >>> mean_squared_error(y_test, est.predict(X_test))
    5.00...

The figure below shows the results of applying <span class="title-ref">GradientBoostingRegressor</span> with least squares loss and 500 base learners to the diabetes dataset (<span class="title-ref">sklearn.datasets.load\_diabetes</span>). The plot shows the train and test error at each iteration. The train error at each iteration is stored in the <span class="title-ref">train\_score\_</span> attribute of the gradient boosting model. The test error at each iterations can be obtained via the <span class="title-ref">\~GradientBoostingRegressor.staged\_predict</span> method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. `n_estimators`) by early stopping.

![](../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regression_001.png)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py)
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_oob.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_oob.py)

#### Fitting additional weak-learners

Both <span class="title-ref">GradientBoostingRegressor</span> and <span class="title-ref">GradientBoostingClassifier</span> support `warm_start=True` which allows you to add more estimators to an already fitted model.

    >>> import numpy as np
    >>> from sklearn.metrics import mean_squared_error
    >>> from sklearn.datasets import make_friedman1
    >>> from sklearn.ensemble import GradientBoostingRegressor
    
    >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
    >>> X_train, X_test = X[:200], X[200:]
    >>> y_train, y_test = y[:200], y[200:]
    >>> est = GradientBoostingRegressor(
    ...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,
    ...     loss='squared_error'
    ... )
    >>> est = est.fit(X_train, y_train)  # fit with 100 trees
    >>> mean_squared_error(y_test, est.predict(X_test))
    5.00...
    >>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees
    >>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est
    >>> mean_squared_error(y_test, est.predict(X_test))
    3.84...

#### Controlling the tree size

The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth `h` can capture interactions of order `h` . There are two ways in which the size of the individual regression trees can be controlled.

If you specify `max_depth=h` then complete binary trees of depth `h` will be grown. Such trees will have (at most) `2**h` leaf nodes and `2**h - 1` split nodes.

Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter `max_leaf_nodes`. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with `max_leaf_nodes=k` has `k - 1` split nodes and thus can model interactions of up to order `max_leaf_nodes - 1` .

We found that `max_leaf_nodes=k` gives comparable results to `max_depth=k-1` but is significantly faster to train at the expense of a slightly higher training error. The parameter `max_leaf_nodes` corresponds to the variable `J` in the chapter on gradient boosting in [\[Friedman2001\]](#Friedman2001) and is related to the parameter `interaction.depth` in R's gbm package where `max_leaf_nodes == interaction.depth + 1` .

#### Mathematical formulation

We first present GBRT for regression, and then detail the classification case.

<div class="dropdown">

Regression

GBRT regressors are additive models whose prediction \(\hat{y}_i\) for a given input \(x_i\) is of the following form:

\[\hat{y}_i = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)\]

where the \(h_m\) are estimators called *weak learners* in the context of boosting. Gradient Tree Boosting uses \[decision tree regressors \<tree\>\](\#decision-tree-regressors

</div>

  - \--\<tree\>) of fixed size as weak learners. The constant M corresponds to the  
    <span class="title-ref">n\_estimators</span> parameter.
    
    Similar to other boosting algorithms, a GBRT is built in a greedy fashion:
    
    \[F_m(x) = F_{m-1}(x) + h_m(x),\]
    
    where the newly added tree \(h_m\) is fitted in order to minimize a sum of losses \(L_m\), given the previous ensemble \(F_{m-1}\):
    
    \[h_m =  \arg\min_{h} L_m = \arg\min_{h} \sum_{i=1}^{n}
    l(y_i, F_{m-1}(x_i) + h(x_i)),\]
    
    where \(l(y_i, F(x_i))\) is defined by the <span class="title-ref">loss</span> parameter, detailed in the next section.
    
    By default, the initial model \(F_{0}\) is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. The initial model can also be specified via the `init` argument.
    
    Using a first-order Taylor approximation, the value of \(l\) can be approximated as follows:
    
    \[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx
    l(y_i, F_{m-1}(x_i))
    + h_m(x_i)
    \left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}.\]
    
    \> **Note**

  - \>  
    Briefly, a first-order Taylor approximation says that \(l(z) \approx l(a) + (z - a) \frac{\partial l}{\partial z}(a)\). Here, \(z\) corresponds to \(F_{m - 1}(x_i) + h_m(x_i)\), and \(a\) corresponds to \(F_{m-1}(x_i)\)
    
    The quantity \(\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)}
    \right]_{F=F_{m - 1}}\) is the derivative of the loss with respect to its second parameter, evaluated at \(F_{m-1}(x)\). It is easy to compute for any given \(F_{m - 1}(x_i)\) in a closed form since the loss is differentiable. We will denote it by \(g_i\).
    
    Removing the constant terms, we have:
    
    h\_m approx argm[in](){h} s[um](){i=1}^{n} h(x\_i) g\_i
    
    This is minimized if \(h(x_i)\) is fitted to predict a value that is proportional to the negative gradient \(-g_i\). Therefore, at each iteration, **the estimator** \(h_m\) **is fitted to predict the negative gradients of the samples**. The gradients are updated at each iteration. This can be considered as some kind of gradient descent in a functional space.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    </div>
    
    For some losses, e.g. `'absolute_error'` where the gradients are \(\pm 1\), the values predicted by a fitted \(h_m\) are not accurate enough: the tree can only output integer values. As a result, the leaves values of the tree \(h_m\) are modified once the tree is fitted, such that the leaves values minimize the loss \(L_m\). The update is loss-dependent: for the absolute error loss, the value of a leaf is updated to the median of the samples in that leaf.

<div class="dropdown">

Classification

Gradient boosting for classification is very similar to the regression case. However, the sum of the trees \(F_M(x_i) = \sum_m h_m(x_i)\) is not homogeneous to a prediction: it cannot be a class, since the trees predict continuous values.

The mapping from the value \(F_M(x_i)\) to a class or a probability is loss-dependent. For the log-loss, the probability that \(x_i\) belongs to the positive class is modeled as \(p(y_i = 1 |
x_i) = \sigma(F_M(x_i))\) where \(\sigma\) is the sigmoid or expit function.

For multiclass classification, K trees (for K classes) are built at each of the \(M\) iterations. The probability that \(x_i\) belongs to class k is modeled as a softmax of the \(F_{M,k}(x_i)\) values.

Note that even for a classification task, the \(h_m\) sub-estimator is still a regressor, not a classifier. This is because the sub-estimators are trained to predict (negative) *gradients*, which are always continuous quantities.

</div>

#### Loss Functions

The following loss functions are supported and can be specified using the parameter `loss`:

<div class="dropdown">

Regression

  - Squared error (`'squared_error'`): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.
  - Absolute error (`'absolute_error'`): A robust loss function for regression. The initial model is given by the median of the target values.
  - Huber (`'huber'`): Another robust loss function that combines least squares and least absolute deviation; use `alpha` to control the sensitivity with regards to outliers (see [\[Friedman2001\]](#Friedman2001) for more details).
  - Quantile (`'quantile'`): A loss function for quantile regression. Use `0 < alpha < 1` to specify the quantile. This loss function can be used to create prediction intervals (see \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py)).

</div>

<div class="dropdown">

Classification

  - Binary log-loss (`'log-loss'`): The binomial negative log-likelihood loss function for binary classification. It provides probability estimates. The initial model is given by the log odds-ratio.
  - Multi-class log-loss (`'log-loss'`): The multinomial negative log-likelihood loss function for multi-class classification with `n_classes` mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration `n_classes` regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.
  - Exponential loss (`'exponential'`): The same loss function as <span class="title-ref">AdaBoostClassifier</span>. Less robust to mislabeled examples than `'log-loss'`; can only be used for binary classification.

</div>

#### Shrinkage via learning rate

[\[Friedman2001\]](#Friedman2001) proposed a simple regularization strategy that scales the contribution of each weak learner by a constant factor \(\nu\):

\[F_m(x) = F_{m-1}(x) + \nu h_m(x)\]

The parameter \(\nu\) is also called the **learning rate** because it scales the step length the gradient descent procedure; it can be set via the `learning_rate` parameter.

The parameter `learning_rate` strongly interacts with the parameter `n_estimators`, the number of weak learners to fit. Smaller values of `learning_rate` require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of `learning_rate` favor better test error. [\[HTF\]](#HTF) recommend to set the learning rate to a small constant (e.g. `learning_rate <= 0.1`) and choose `n_estimators` large enough that early stopping applies, see \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_early\_stopping.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_early\_stopping.py) for a more detailed discussion of the interaction between `learning_rate` and `n_estimators` see [\[R2007\]](#R2007).

#### Subsampling

[\[Friedman2002\]](#Friedman2002) proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction `subsample` of the available training data. The subsample is drawn without replacement. A typical value of `subsample` is 0.5.

The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.

![](../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regularization_001.png)

Another strategy to reduce the variance is by subsampling the features analogous to the random splits in <span class="title-ref">RandomForestClassifier</span>. The number of subsampled features can be controlled via the `max_features` parameter.

<div class="note">

<div class="title">

Note

</div>

Using a small `max_features` value can significantly decrease the runtime.

</div>

Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute <span class="title-ref">oob\_improvement\_</span>. `oob_improvement_[i]` holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming.

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regularization.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regularization.py)
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_oob.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_oob.py)
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_ensemble\_oob.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_ensemble\_oob.py)

#### Interpretation with feature importance

Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models.

Often features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response?

Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree (see \[random\_forest\_feature\_importance\](\#random\_forest\_feature\_importance) for more details).

The feature importance scores of a fit gradient boosting model can be accessed via the `feature_importances_` property:

    >>> from sklearn.datasets import make_hastie_10_2
    >>> from sklearn.ensemble import GradientBoostingClassifier
    
    >>> X, y = make_hastie_10_2(random_state=0)
    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    ...     max_depth=1, random_state=0).fit(X, y)
    >>> clf.feature_importances_
    array([0.10..., 0.10..., 0.11..., ...

Note that this computation of feature importance is based on entropy, and it is distinct from <span class="title-ref">sklearn.inspection.permutation\_importance</span> which is based on permutation of the features.

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py)

**References**

## Random forests and other randomized tree ensembles

The `sklearn.ensemble` module includes two averaging algorithms based on randomized \[decision trees \<tree\>\](\#decision-trees-\<tree\>): the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques [\[B1998\]](#B1998) specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.

As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of shape `(n_samples, n_features)` holding the training samples, and an array Y of shape `(n_samples,)` holding the target values (class labels) for the training samples:

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> X = [[0, 0], [1, 1]]
    >>> Y = [0, 1]
    >>> clf = RandomForestClassifier(n_estimators=10)
    >>> clf = clf.fit(X, Y)

Like \[decision trees \<tree\>\](\#decision-trees-\<tree\>), forests of trees also extend to \[multi-output problems \<tree\_multioutput\>\](\#multi-output-problems-\<tree\_multioutput\>) (if Y is an array of shape `(n_samples, n_outputs)`).

### Random Forests

In random forests (see <span class="title-ref">RandomForestClassifier</span> and <span class="title-ref">RandomForestRegressor</span> classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.

Furthermore, when splitting each node during the construction of a tree, the best split is found through an exhaustive search of the features values of either all input features or a random subset of size `max_features`. (See the \[parameter tuning guidelines \<random\_forest\_parameters\>\](\#parameter-tuning-guidelines-\<random\_forest\_parameters\>) for more details.)

The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.

In contrast to the original publication [\[B2001\]](#B2001), the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.

A competitive alternative to random forests are \[histogram\_based\_gradient\_boosting\](\#histogram\_based\_gradient\_boosting) (HGBT) models:

  - Building trees: Random forests typically rely on deep trees (that overfit individually) which uses much computational resources, as they require several splittings and evaluations of candidate splits. Boosting models build shallow trees (that underfit individually) which are faster to fit and predict.
  - Sequential boosting: In HGBT, the decision trees are built sequentially, where each tree is trained to correct the errors made by the previous ones. This allows them to iteratively improve the model's performance using relatively few trees. In contrast, random forests use a majority vote to predict the outcome, which can require a larger number of trees to achieve the same level of accuracy.
  - Efficient binning: HGBT uses an efficient binning algorithm that can handle large datasets with a high number of features. The binning algorithm can pre-process the data to speed up the subsequent tree construction (see \[Why it's faster \<Why\_it's\_faster\>\](\#why-it's-faster-\<why\_it's\_faster\>)). In contrast, the scikit-learn implementation of random forests does not use binning and relies on exact splitting, which can be computationally expensive.

Overall, the computational cost of HGBT versus RF depends on the specific characteristics of the dataset and the modeling task. It's a good idea to try both models and compare their performance and computational efficiency on your specific problem to determine which model is the best fit.

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_hist\_grad\_boosting\_comparison.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_hist\_grad\_boosting\_comparison.py)

### Extremely Randomized Trees

In extremely randomized trees (see <span class="title-ref">ExtraTreesClassifier</span> and <span class="title-ref">ExtraTreesRegressor</span> classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:

    >>> from sklearn.model_selection import cross_val_score
    >>> from sklearn.datasets import make_blobs
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.ensemble import ExtraTreesClassifier
    >>> from sklearn.tree import DecisionTreeClassifier
    
    >>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
    ...     random_state=0)
    
    >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
    ...     random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.98...
    
    >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
    ...     min_samples_split=2, random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.999...
    
    >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
    ...     min_samples_split=2, random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean() > 0.999
    True

![](../auto_examples/ensemble/images/sphx_glr_plot_forest_iris_001.png)

### Parameters

The main parameters to adjust when using these methods is `n_estimators` and `max_features`. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are `max_features=1.0` or equivalently `max_features=None` (always considering all features instead of a random subset) for regression problems, and `max_features="sqrt"` (using a random subset of size `sqrt(n_features)`) for classification tasks (where `n_features` is the number of features in the data). The default value of `max_features=1.0` is equivalent to bagged trees and more randomness can be achieved by setting smaller values (e.g. 0.3 is a typical default in the literature). Good results are often achieved when setting `max_depth=None` in combination with `min_samples_split=2` (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (`bootstrap=True`) while the default strategy for extra-trees is to use the whole dataset (`bootstrap=False`). When using bootstrap sampling the generalization error can be estimated on the left out or out-of-bag samples. This can be enabled by setting `oob_score=True`.

\> **Note** \> The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: `min_samples_split`, `max_leaf_nodes`, `max_depth` and `min_samples_leaf`.

### Parallelization

Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the `n_jobs` parameter. If `n_jobs=k` then computations are partitioned into `k` jobs, and run on `k` cores of the machine. If `n_jobs=-1` then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using `k` jobs will unfortunately not be `k` times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_iris.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_iris.py)
  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py)

**References**

  - P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees", Machine Learning, 63(1), 3-42, 2006.

### Feature importance evaluation

The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The **expected fraction of the samples** they contribute to can thus be used as an estimate of the **relative importance of the features**. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.

By **averaging** the estimates of predictive ability over several randomized trees one can **reduce the variance** of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to [\[L2014\]](#L2014) for more information on MDI and feature importance evaluation with Random Forests.

\> **Warning** \> The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore **do not necessarily inform us on which features are most important to make good predictions on held-out dataset**. Secondly, **they favor high cardinality features**, that is features with many unique values. \[permutation\_importance\](\#permutation\_importance) is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: \[sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py).

In practice those estimates are stored as an attribute named `feature_importances_` on the fitted model. This is an array with shape `(n_features,)` whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_importances.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_importances.py)

**References**

### Totally Random Trees Embedding

<span class="title-ref">RandomTreesEmbedding</span> implements an unsupervised transformation of the data. Using a forest of completely random trees, <span class="title-ref">RandomTreesEmbedding</span> encodes the data by the indices of the leaves a data point ends up in. This index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be computed very efficiently and can then be used as a basis for other learning tasks. The size and sparsity of the code can be influenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the coding contains one entry of one. The size of the coding is at most `n_estimators * 2 ** max_depth`, the maximum number of leaves in the forest.

As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_random\_forest\_embedding.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_random\_forest\_embedding.py)
  - \[sphx\_glr\_auto\_examples\_manifold\_plot\_lle\_digits.py\](\#sphx\_glr\_auto\_examples\_manifold\_plot\_lle\_digits.py) compares non-linear dimensionality reduction techniques on handwritten digits.
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_feature\_transformation.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_feature\_transformation.py) compares supervised and unsupervised tree based feature transformations.

<div class="seealso">

\[manifold\](\#manifold) techniques can also be useful to derive non-linear representations of feature space, also these approaches focus also on dimensionality reduction.

</div>

### Fitting additional trees

RandomForest, Extra-Trees and <span class="title-ref">RandomTreesEmbedding</span> estimators all support `warm_start=True` which allows you to add more trees to an already fitted model.

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.ensemble import RandomForestClassifier
    
    >>> X, y = make_classification(n_samples=100, random_state=1)
    >>> clf = RandomForestClassifier(n_estimators=10)
    >>> clf = clf.fit(X, y)  # fit with 10 trees
    >>> len(clf.estimators_)
    10
    >>> # set warm_start and increase num of estimators
    >>> _ = clf.set_params(n_estimators=20, warm_start=True)
    >>> _ = clf.fit(X, y) # fit additional 10 trees
    >>> len(clf.estimators_)
    20

When `random_state` is also set, the internal random state is also preserved between `fit` calls. This means that training a model once with `n` estimators is the same as building the model iteratively via multiple `fit` calls, where the final number of estimators is equal to `n`.

    >>> clf = RandomForestClassifier(n_estimators=20)  # set `n_estimators` to 10 + 10
    >>> _ = clf.fit(X, y)  # fit `estimators_` will be the same as `clf` above

Note that this differs from the usual behavior of `random_state` in that it does *not* result in the same result across different calls.

## Bagging meta-estimator

In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).

Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:

  - When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [\[B1999\]](#B1999).
  - When samples are drawn with replacement, then the method is known as Bagging [\[B1996\]](#B1996).
  - When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [\[H1998\]](#H1998).
  - Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [\[LG2012\]](#LG2012).

In scikit-learn, bagging methods are offered as a unified <span class="title-ref">BaggingClassifier</span> meta-estimator (resp. <span class="title-ref">BaggingRegressor</span>), taking as input a user-specified estimator along with parameters specifying the strategy to draw random subsets. In particular, `max_samples` and `max_features` control the size of the subsets (in terms of samples and features), while `bootstrap` and `bootstrap_features` control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting `oob_score=True`. As an example, the snippet below illustrates how to instantiate a bagging ensemble of <span class="title-ref">\~sklearn.neighbors.KNeighborsClassifier</span> estimators, each built on random subsets of 50% of the samples and 50% of the features.

> \>\>\> from sklearn.ensemble import BaggingClassifier \>\>\> from sklearn.neighbors import KNeighborsClassifier \>\>\> bagging = BaggingClassifier(KNeighborsClassifier(), ... max\_samples=0.5, max\_features=0.5)

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_bias\_variance.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_bias\_variance.py)

**References**

## Voting Classifier

The idea behind the <span class="title-ref">VotingClassifier</span> is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing models in order to balance out their individual weaknesses.

### Majority Class Labels (Majority/Hard Voting)

In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.

E.g., if the prediction for a given sample is

  - classifier 1 -\> class 1
  - classifier 2 -\> class 1
  - classifier 3 -\> class 2

the VotingClassifier (with `voting='hard'`) would classify the sample as "class 1" based on the majority class label.

In the cases of a tie, the <span class="title-ref">VotingClassifier</span> will select the class based on the ascending sort order. E.g., in the following scenario

  - classifier 1 -\> class 2
  - classifier 2 -\> class 1

the class label 1 will be assigned to the sample.

### Usage

The following example shows how to fit the majority rule classifier:

    >>> from sklearn import datasets
    >>> from sklearn.model_selection import cross_val_score
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.ensemble import VotingClassifier
    
    >>> iris = datasets.load_iris()
    >>> X, y = iris.data[:, 1:3], iris.target
    
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
    >>> clf3 = GaussianNB()
    
    >>> eclf = VotingClassifier(
    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...     voting='hard')
    
    >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
    ...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
    ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
    Accuracy: 0.94 (+/- 0.04) [Random Forest]
    Accuracy: 0.91 (+/- 0.04) [naive Bayes]
    Accuracy: 0.95 (+/- 0.04) [Ensemble]

### Weighted Average Probabilities (Soft Voting)

In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.

Specific weights can be assigned to each classifier via the `weights` parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.

To illustrate this with a simple example, let's assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.

The weighted average probabilities for a sample would then be calculated as follows:

| classifier       | class 1   | class 2   | class 3   |
| ---------------- | --------- | --------- | --------- |
| classifier 1     | w1 \* 0.2 | w1 \* 0.5 | w1 \* 0.3 |
| classifier 2     | w2 \* 0.6 | w2 \* 0.3 | w2 \* 0.1 |
| classifier 3     | w3 \* 0.3 | w3 \* 0.4 | w3 \* 0.3 |
| weighted average | 0.37      | 0.4       | 0.23      |

Here, the predicted class label is 2, since it has the highest average probability.

The following example illustrates how the decision regions may change when a soft <span class="title-ref">VotingClassifier</span> is used based on a linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:

    >>> from sklearn import datasets
    >>> from sklearn.tree import DecisionTreeClassifier
    >>> from sklearn.neighbors import KNeighborsClassifier
    >>> from sklearn.svm import SVC
    >>> from itertools import product
    >>> from sklearn.ensemble import VotingClassifier
    
    >>> # Loading some example data
    >>> iris = datasets.load_iris()
    >>> X = iris.data[:, [0, 2]]
    >>> y = iris.target
    
    >>> # Training classifiers
    >>> clf1 = DecisionTreeClassifier(max_depth=4)
    >>> clf2 = KNeighborsClassifier(n_neighbors=7)
    >>> clf3 = SVC(kernel='rbf', probability=True)
    >>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
    ...                         voting='soft', weights=[2, 1, 2])
    
    >>> clf1 = clf1.fit(X, y)
    >>> clf2 = clf2.fit(X, y)
    >>> clf3 = clf3.fit(X, y)
    >>> eclf = eclf.fit(X, y)

![](../auto_examples/ensemble/images/sphx_glr_plot_voting_decision_regions_001.png)

### Usage

In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support `predict_proba` method):

    >>> eclf = VotingClassifier(
    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...     voting='soft'
    ... )

Optionally, weights can be provided for the individual classifiers:

    >>> eclf = VotingClassifier(
    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...     voting='soft', weights=[2,5,1]
    ... )

<div class="dropdown">

Using the <span class="title-ref">VotingClassifier</span> with <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span>

The <span class="title-ref">VotingClassifier</span> can also be used together with <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> in order to tune the hyperparameters of the individual estimators:

    >>> from sklearn.model_selection import GridSearchCV
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> eclf = VotingClassifier(
    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...     voting='soft'
    ... )
    
    >>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}
    
    >>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
    >>> grid = grid.fit(iris.data, iris.target)

</div>

## Voting Regressor

The idea behind the <span class="title-ref">VotingRegressor</span> is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.

### Usage

The following example shows how to fit the VotingRegressor:

    >>> from sklearn.datasets import load_diabetes
    >>> from sklearn.ensemble import GradientBoostingRegressor
    >>> from sklearn.ensemble import RandomForestRegressor
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.ensemble import VotingRegressor
    
    >>> # Loading some example data
    >>> X, y = load_diabetes(return_X_y=True)
    
    >>> # Training classifiers
    >>> reg1 = GradientBoostingRegressor(random_state=1)
    >>> reg2 = RandomForestRegressor(random_state=1)
    >>> reg3 = LinearRegression()
    >>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
    >>> ereg = ereg.fit(X, y)

![](../auto_examples/ensemble/images/sphx_glr_plot_voting_regressor_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_voting\_regressor.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_voting\_regressor.py)

## Stacked generalization

Stacked generalization is a method for combining estimators to reduce their biases [\[W1992\]](#W1992) [\[HTF\]](#HTF). More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation.

The <span class="title-ref">StackingClassifier</span> and <span class="title-ref">StackingRegressor</span> provide such strategies which can be applied to classification and regression problems.

The <span class="title-ref">estimators</span> parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators:

    >>> from sklearn.linear_model import RidgeCV, LassoCV
    >>> from sklearn.neighbors import KNeighborsRegressor
    >>> estimators = [('ridge', RidgeCV()),
    ...               ('lasso', LassoCV(random_state=42)),
    ...               ('knr', KNeighborsRegressor(n_neighbors=20,
    ...                                           metric='euclidean'))]

The <span class="title-ref">final\_estimator</span> will use the predictions of the <span class="title-ref">estimators</span> as input. It needs to be a classifier or a regressor when using <span class="title-ref">StackingClassifier</span> or <span class="title-ref">StackingRegressor</span>, respectively:

    >>> from sklearn.ensemble import GradientBoostingRegressor
    >>> from sklearn.ensemble import StackingRegressor
    >>> final_estimator = GradientBoostingRegressor(
    ...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,
    ...     random_state=42)
    >>> reg = StackingRegressor(
    ...     estimators=estimators,
    ...     final_estimator=final_estimator)

To train the <span class="title-ref">estimators</span> and <span class="title-ref">final\_estimator</span>, the <span class="title-ref">fit</span> method needs to be called on the training data:

    >>> from sklearn.datasets import load_diabetes
    >>> X, y = load_diabetes(return_X_y=True)
    >>> from sklearn.model_selection import train_test_split
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,
    ...                                                     random_state=42)
    >>> reg.fit(X_train, y_train)
    StackingRegressor(...)

During training, the <span class="title-ref">estimators</span> are fitted on the whole training data <span class="title-ref">X\_train</span>. They will be used when calling <span class="title-ref">predict</span> or <span class="title-ref">predict\_proba</span>. To generalize and avoid over-fitting, the <span class="title-ref">final\_estimator</span> is trained on out-samples using <span class="title-ref">sklearn.model\_selection.cross\_val\_predict</span> internally.

For <span class="title-ref">StackingClassifier</span>, note that the output of the `estimators` is controlled by the parameter <span class="title-ref">stack\_method</span> and it is called by each estimator. This parameter is either a string, being estimator method names, or <span class="title-ref">'auto'</span> which will automatically identify an available method depending on the availability, tested in the order of preference: <span class="title-ref">predict\_proba</span>, <span class="title-ref">decision\_function</span> and <span class="title-ref">predict</span>.

A <span class="title-ref">StackingRegressor</span> and <span class="title-ref">StackingClassifier</span> can be used as any other regressor or classifier, exposing a <span class="title-ref">predict</span>, <span class="title-ref">predict\_proba</span>, or <span class="title-ref">decision\_function</span> method, e.g.:

    >>> y_pred = reg.predict(X_test)
    >>> from sklearn.metrics import r2_score
    >>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))
    R2 score: 0.53

Note that it is also possible to get the output of the stacked <span class="title-ref">estimators</span> using the <span class="title-ref">transform</span> method:

    >>> reg.transform(X_test[:5])
    array([[142..., 138..., 146...],
           [179..., 182..., 151...],
           [139..., 132..., 158...],
           [286..., 292..., 225...],
           [126..., 124..., 164...]])

In practice, a stacking predictor predicts as good as the best predictor of the base layer and even sometimes outperforms it by combining the different strengths of the these predictors. However, training a stacking predictor is computationally expensive.

<div class="note">

<div class="title">

Note

</div>

For <span class="title-ref">StackingClassifier</span>, when using <span class="title-ref">stack\_method\_='predict\_proba'</span>, the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear.

</div>

<div class="note">

<div class="title">

Note

</div>

Multiple stacking layers can be achieved by assigning <span class="title-ref">final\_estimator</span> to a <span class="title-ref">StackingClassifier</span> or \`StackingRegressor\`:

    >>> final_layer_rfr = RandomForestRegressor(

... n\_estimators=10, max\_features=1, max\_leaf\_nodes=5,random\_state=42) \>\>\> final\_layer\_gbr = GradientBoostingRegressor( ... n\_estimators=10, max\_features=1, max\_leaf\_nodes=5,random\_state=42) \>\>\> final\_layer = StackingRegressor( ... estimators=\[('rf', final\_layer\_rfr), ... ('gbrt', final\_layer\_gbr)\], ... final\_estimator=RidgeCV() ... ) \>\>\> multi\_layer\_regressor = StackingRegressor( ... estimators=\[('ridge', RidgeCV()), ... ('lasso', LassoCV(random\_state=42)), ... ('knr', KNeighborsRegressor(n\_neighbors=20, ... metric='euclidean'))\], ... final\_estimator=final\_layer ... ) \>\>\> multi\_layer\_regressor.fit(X\_train, y\_train) StackingRegressor(...) \>\>\> print('R2 score: {:.2f}' ... .format(multi\_layer\_regressor.score(X\_test, y\_test))) R2 score: 0.53

</div>

**References**

## AdaBoost

The module `sklearn.ensemble` includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [\[FS1995\]](#FS1995).

The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consists of applying weights \(w_1\), \(w_2\), ..., \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence [\[HTF\]](#HTF).

![](../auto_examples/ensemble/images/sphx_glr_plot_adaboost_multiclass_001.png)

AdaBoost can be used both for classification and regression problems:

  - For multi-class classification, <span class="title-ref">AdaBoostClassifier</span> implements AdaBoost.SAMME [\[ZZRH2009\]](#ZZRH2009).
  - For regression, <span class="title-ref">AdaBoostRegressor</span> implements AdaBoost.R2 [\[D1997\]](#D1997).

### Usage

The following example shows how to fit an AdaBoost classifier with 100 weak learners:

    >>> from sklearn.model_selection import cross_val_score
    >>> from sklearn.datasets import load_iris
    >>> from sklearn.ensemble import AdaBoostClassifier
    
    >>> X, y = load_iris(return_X_y=True)
    >>> clf = AdaBoostClassifier(n_estimators=100)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.9...

The number of weak learners is controlled by the parameter `n_estimators`. The `learning_rate` parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the `estimator` parameter. The main parameters to tune to obtain good results are `n_estimators` and the complexity of the base estimators (e.g., its depth `max_depth` or minimum required number of samples to consider a split `min_samples_split`).

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_adaboost\_multiclass.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_adaboost\_multiclass.py) shows the performance of AdaBoost on a multi-class problem.
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_adaboost\_twoclass.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_adaboost\_twoclass.py) shows the decision boundary and decision function values for a non-linearly separable two-class problem using AdaBoost-SAMME.
  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_adaboost\_regression.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_adaboost\_regression.py) demonstrates regression with the AdaBoost.R2 algorithm.

**References**

<div id="citations">

  - <span id="B1996" class="citation-label">B1996</span>  
    L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140, 1996.

  - <span id="B1998" class="citation-label">B1998</span>
    
    12. Breiman, "Arcing Classifiers", Annals of Statistics 1998.

  - <span id="B1999" class="citation-label">B1999</span>  
    L. Breiman, "Pasting small votes for classification in large databases and on-line", Machine Learning, 36(1), 85-103, 1999.

  - <span id="B2001" class="citation-label">B2001</span>
    
    12. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.

  - <span id="D1997" class="citation-label">D1997</span>
    
    8.  Drucker. "Improving Regressors using Boosting Techniques", 1997.

  - <span id="FS1995" class="citation-label">FS1995</span>  
    Y. Freund, and R. Schapire, "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting", 1997.

  - <span id="Fisher1958" class="citation-label">Fisher1958</span>  
    Fisher, W.D. (1958). ["On Grouping for Maximum Homogeneity"](http://csiss.ncgia.ucsb.edu/SPACE/workshops/2004/SAC/files/fisher.pdf) Journal of the American Statistical Association, 53, 789-798.

  - <span id="Friedman2001" class="citation-label">Friedman2001</span>  
    Friedman, J.H. (2001). `Greedy function approximation: A gradient
    boosting machine <10.1214/aos/1013203451>`. Annals of Statistics, 29, 1189-1232.

  - <span id="Friedman2002" class="citation-label">Friedman2002</span>  
    Friedman, J.H. (2002). [Stochastic gradient boosting.](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf). Computational Statistics & Data Analysis, 38, 367-378.

  - <span id="H1998" class="citation-label">H1998</span>  
    T. Ho, "The random subspace method for constructing decision forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.

  - <span id="HTF" class="citation-label">HTF</span>  
    T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical Learning Ed. 2", Springer, 2009.

  - <span id="L2014" class="citation-label">L2014</span>  
    G. Louppe, `"Understanding Random Forests: From Theory to
    Practice" <1407.7502>`, PhD Thesis, U. of Liege, 2014.

  - <span id="LG2012" class="citation-label">LG2012</span>  
    G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.

  - <span id="LightGBM" class="citation-label">LightGBM</span>  
    Ke et. al. ["LightGBM: A Highly Efficient Gradient BoostingDecision Tree" \<https://papers.nips.cc/paper/ 6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree\>]()

  - <span id="Mayer2022" class="citation-label">Mayer2022</span>  
    M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio. 2022. `Machine Learning Applications to Land and Structure Valuation
    <10.3390/jrfm15050193>`. Journal of Risk and Financial Management 15, no. 5: 193

  - <span id="R2007" class="citation-label">R2007</span>  
    G. Ridgeway (2006). [Generalized Boosted Models: A guide to the gbm package](https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf)

  - <span id="W1992" class="citation-label">W1992</span>  
    Wolpert, David H. "Stacked generalization." Neural networks 5.2 (1992): 241-259.

  - <span id="XGBoost" class="citation-label">XGBoost</span>  
    Tianqi Chen, Carlos Guestrin, `"XGBoost: A Scalable Tree
    Boosting System" <1603.02754>`

  - <span id="ZZRH2009" class="citation-label">ZZRH2009</span>
    
    10. Zhu, H. Zou, S. Rosset, T. Hastie. "Multi-class AdaBoost", 2009.

</div>

---

feature_extraction.md

---

# Feature extraction

<div class="currentmodule">

sklearn.feature\_extraction

</div>

The `sklearn.feature_extraction` module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.

\> **Note** \> Feature extraction is very different from \[feature\_selection\](\#feature\_selection): the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features.

## Loading features from dicts

The class <span class="title-ref">DictVectorizer</span> can be used to convert feature arrays represented as lists of standard Python `dict` objects to the NumPy/SciPy representation used by scikit-learn estimators.

While not particularly fast to process, Python's `dict` has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values.

<span class="title-ref">DictVectorizer</span> implements what is called one-of-K or "one-hot" coding for categorical (aka nominal, discrete) features. Categorical features are "attribute-value" pairs where the value is restricted to a list of discrete possibilities without ordering (e.g. topic identifiers, types of objects, tags, names...).

In the following, "city" is a categorical attribute while "temperature" is a traditional numerical feature:

    >>> measurements = [
    ...     {'city': 'Dubai', 'temperature': 33.},
    ...     {'city': 'London', 'temperature': 12.},
    ...     {'city': 'San Francisco', 'temperature': 18.},
    ... ]
    
    >>> from sklearn.feature_extraction import DictVectorizer
    >>> vec = DictVectorizer()
    
    >>> vec.fit_transform(measurements).toarray()
    array([[ 1.,  0.,  0., 33.],
           [ 0.,  1.,  0., 12.],
           [ 0.,  0.,  1., 18.]])
    
    >>> vec.get_feature_names_out()
    array(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)

<span class="title-ref">DictVectorizer</span> accepts multiple string values for one feature, like, e.g., multiple categories for a movie.

Assume a database classifies each movie using some categories (not mandatories) and its year of release.

> \>\>\> movie\_entry = \[{'category': \['thriller', 'drama'\], 'year': 2003}, ... {'category': \['animation', 'family'\], 'year': 2011}, ... {'year': 1974}\] \>\>\> vec.fit\_transform(movie\_entry).toarray() array(\[\[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03\], \[1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03\], \[0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03\]\]) \>\>\> vec.get\_feature\_names\_out() array(\['category=animation', 'category=drama', 'category=family', 'category=thriller', 'year'\], ...) \>\>\> vec.transform({'category': \['thriller'\], ... 'unseen\_feature': '3'}).toarray() array(\[\[0., 0., 0., 1., 0.\]\])

<span class="title-ref">DictVectorizer</span> is also a useful representation transformation for training sequence classifiers in Natural Language Processing models that typically work by extracting feature windows around a particular word of interest.

For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word 'sat' in the sentence 'The cat sat on the mat.':

    >>> pos_window = [
    ...     {
    ...         'word-2': 'the',
    ...         'pos-2': 'DT',
    ...         'word-1': 'cat',
    ...         'pos-1': 'NN',
    ...         'word+1': 'on',
    ...         'pos+1': 'PP',
    ...     },
    ...     # in a real application one would extract many such dictionaries
    ... ]

This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a <span class="title-ref">\~text.TfidfTransformer</span> for normalization):

    >>> vec = DictVectorizer()
    >>> pos_vectorized = vec.fit_transform(pos_window)
    >>> pos_vectorized
    <Compressed Sparse...dtype 'float64'
      with 6 stored elements and shape (1, 6)>
    >>> pos_vectorized.toarray()
    array([[1., 1., 1., 1., 1., 1.]])
    >>> vec.get_feature_names_out()
    array(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',
           'word-2=the'], ...)

As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the `DictVectorizer` class uses a `scipy.sparse` matrix by default instead of a `numpy.ndarray`.

## Feature hashing

<div class="currentmodule">

sklearn.feature\_extraction

</div>

The class <span class="title-ref">FeatureHasher</span> is a high-speed, low-memory vectorizer that uses a technique known as [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing), or the "hashing trick". Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of <span class="title-ref">FeatureHasher</span> apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no `inverse_transform` method.

Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature's value is zero. This mechanism is enabled by default with `alternate_sign=True` and is particularly useful for small hash table sizes (`n_features < 10000`). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like <span class="title-ref">\~sklearn.naive\_bayes.MultinomialNB</span> or <span class="title-ref">\~sklearn.feature\_selection.chi2</span> feature selectors that expect non-negative inputs.

<span class="title-ref">FeatureHasher</span> accepts either mappings (like Python's `dict` and its variants in the `collections` module), `(feature, value)` pairs, or strings, depending on the constructor parameter `input_type`. Mapping are treated as lists of `(feature, value)` pairs, while single strings have an implicit value of 1, so `['feat1', 'feat2', 'feat3']` is interpreted as `[('feat1', 1), ('feat2', 1), ('feat3', 1)]`. If a single feature occurs multiple times in a sample, the associated values will be summed (so `('feat', 2)` and `('feat', 3.5)` become `('feat', 5.5)`). The output from <span class="title-ref">FeatureHasher</span> is always a `scipy.sparse` matrix in the CSR format.

Feature hashing can be employed in document classification, but unlike <span class="title-ref">\~text.CountVectorizer</span>, <span class="title-ref">FeatureHasher</span> does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see \[hashing\_vectorizer\](\#hashing\_vectorizer), below, for a combined tokenizer/hasher.

As an example, consider a word-level natural language processing task that needs features extracted from `(token, part_of_speech)` pairs. One could use a Python generator function to extract features:

    def token_features(token, part_of_speech):
        if token.isdigit():
            yield "numeric"
        else:
            yield "token={}".format(token.lower())
            yield "token,pos={},{}".format(token, part_of_speech)
        if token[0].isupper():
            yield "uppercase_initial"
        if token.isupper():
            yield "all_uppercase"
        yield "pos={}".format(part_of_speech)

Then, the `raw_X` to be fed to `FeatureHasher.transform` can be constructed using:

    raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)

and fed to a hasher with:

    hasher = FeatureHasher(input_type='string')
    X = hasher.transform(raw_X)

to get a `scipy.sparse` matrix `X`.

Note the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only processed on demand from the hasher.

<div class="dropdown">

Implementation details

<span class="title-ref">FeatureHasher</span> uses the signed 32-bit variant of MurmurHash3. As a result (and because of limitations in `scipy.sparse`), the maximum number of features supported is currently \(2^{31} - 1\).

The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.

Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the `n_features` parameter; otherwise the features will not be mapped evenly to the columns.

**References**

  - [MurmurHash3](https://github.com/aappleby/smhasher).

</div>

**References**

  - Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). [Feature hashing for large scale multitask learning](https://alex.smola.org/papers/2009/Weinbergeretal09.pdf). Proc. ICML.

## Text feature extraction

<div class="currentmodule">

sklearn.feature\_extraction.text

</div>

### The Bag of Words representation

Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.

In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:

  - **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.
  - **counting** the occurrences of tokens in each document.
  - **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents.

In this scheme, features and samples are defined as follows:

  - each **individual token occurrence frequency** (normalized or not) is treated as a **feature**.
  - the vector of all the token frequencies for a given **document** is considered a multivariate **sample**.

A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.

We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or "Bag of n-grams" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.

### Sparsity

As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).

For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.

In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the `scipy.sparse` package.

### Common Vectorizer usage

<span class="title-ref">CountVectorizer</span> implements both tokenization and occurrence counting in a single class:

    >>> from sklearn.feature_extraction.text import CountVectorizer

This model has many parameters, however the default values are quite reasonable (please see the \[reference documentation \<feature\_extraction\_ref-from-text\>\](\#reference-documentation \<feature\_extraction\_ref-from-text\>) for the details):

    >>> vectorizer = CountVectorizer()
    >>> vectorizer
    CountVectorizer()

Let's use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:

    >>> corpus = [
    ...     'This is the first document.',
    ...     'This is the second second document.',
    ...     'And the third one.',
    ...     'Is this the first document?',
    ... ]
    >>> X = vectorizer.fit_transform(corpus)
    >>> X
    <Compressed Sparse...dtype 'int64'
      with 19 stored elements and shape (4, 9)>

The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:

    >>> analyze = vectorizer.build_analyzer()
    >>> analyze("This is a text document to analyze.") == (
    ...     ['this', 'is', 'text', 'document', 'to', 'analyze'])
    True

Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows:

    >>> vectorizer.get_feature_names_out()
    array(['and', 'document', 'first', 'is', 'one', 'second', 'the',
           'third', 'this'], ...)
    
    >>> X.toarray()
    array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
           [0, 1, 0, 1, 0, 2, 1, 0, 1],
           [1, 0, 0, 0, 1, 0, 1, 1, 0],
           [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)

The converse mapping from feature name to column index is stored in the `vocabulary_` attribute of the vectorizer:

    >>> vectorizer.vocabulary_.get('document')
    1

Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:

    >>> vectorizer.transform(['Something completely new.']).toarray()
    array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)

Note that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. In particular we lose the information that the last document is an interrogative form. To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words):

    >>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
    ...                                     token_pattern=r'\b\w+\b', min_df=1)
    >>> analyze = bigram_vectorizer.build_analyzer()
    >>> analyze('Bi-grams are cool!') == (
    ...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])
    True

The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:

    >>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
    >>> X_2
    array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
           [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
           [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
           [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)

In particular the interrogative form "Is this" is only present in the last document:

    >>> feature_index = bigram_vectorizer.vocabulary_.get('is this')
    >>> X_2[:, feature_index]
    array([0, 0, 0, 1]...)

### Using stop words

Stop words are words like "and", "the", "him", which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.

There are several known issues in our provided 'english' stop word list. It does not aim to be a general, 'one-size-fits-all' solution as some tasks may require a more custom solution. See [\[NQY18\]](#NQY18) for more details.

Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as *computer*.

You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word *we've* is split into *we* and *ve* by CountVectorizer's default tokenizer, so if *we've* is in `stop_words`, but *ve* is not, *ve* will be retained from *we've* in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.

**References**

### Tf–idf term weighting

In a large text corpus, some words will be very present (e.g. "the", "a", "is" in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.

In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.

Tf means **term-frequency** while tf–idf means term-frequency times **inverse document-frequency**: \(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\).

Using the `TfidfTransformer`'s default settings, `TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)` the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as

\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\),

where \(n\) is the total number of documents in the document set, and \(\text{df}(t)\) is the number of documents in the document set that contain term \(t\). The resulting tf-idf vectors are then normalized by the Euclidean norm:

\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}\).

This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.

The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn's <span class="title-ref">TfidfTransformer</span> and <span class="title-ref">TfidfVectorizer</span> differ slightly from the standard textbook notation that defines the idf as

\(\text{idf}(t) = \log{\frac{n}{1+\text{df}(t)}}.\)

In the <span class="title-ref">TfidfTransformer</span> and <span class="title-ref">TfidfVectorizer</span> with `smooth_idf=False`, the "1" count is added to the idf instead of the idf's denominator:

\(\text{idf}(t) = \log{\frac{n}{\text{df}(t)}} + 1\)

This normalization is implemented by the <span class="title-ref">TfidfTransformer</span> class:

    >>> from sklearn.feature_extraction.text import TfidfTransformer
    >>> transformer = TfidfTransformer(smooth_idf=False)
    >>> transformer
    TfidfTransformer(smooth_idf=False)

Again please see the \[reference documentation \<feature\_extraction\_ref-from-text\>\](\#reference-documentation \<feature\_extraction\_ref-from-text\>) for the details on all the parameters.

<div class="dropdown">

Numeric example of a tf-idf matrix

Let's take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:

    >>> counts = [[3, 0, 1],
    ...           [2, 0, 0],
    ...           [3, 0, 0],
    ...           [4, 0, 0],
    ...           [3, 2, 0],
    ...           [3, 0, 2]]
    ...
    >>> tfidf = transformer.fit_transform(counts)
    >>> tfidf
    <Compressed Sparse...dtype 'float64'
      with 9 stored elements and shape (6, 3)>
    
    >>> tfidf.toarray()
    array([[0.81940995, 0.        , 0.57320793],
          [1.        , 0.        , 0.        ],
          [1.        , 0.        , 0.        ],
          [1.        , 0.        , 0.        ],
          [0.47330339, 0.88089948, 0.        ],
          [0.58149261, 0.        , 0.81355169]])

Each row is normalized to have unit Euclidean norm:

\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}\)

For example, we can compute the tf-idf of the first term in the first document in the <span class="title-ref">counts</span> array as follows:

\(n = 6\)

\(\text{df}(t)_{\text{term1}} = 6\)

\(\text{idf}(t)_{\text{term1}} =
\log \frac{n}{\text{df}(t)} + 1 = \log(1)+1 = 1\)

\(\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3\)

Now, if we repeat this computation for the remaining 2 terms in the document, we get

\(\text{tf-idf}_{\text{term2}} = 0 \times (\log(6/1)+1) = 0\)

\(\text{tf-idf}_{\text{term3}} = 1 \times (\log(6/2)+1) \approx 2.0986\)

and the vector of raw tf-idfs:

\(\text{tf-idf}_{\text{raw}} = [3, 0, 2.0986].\)

Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:

\(\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}}
= [ 0.819,  0,  0.573].\)

Furthermore, the default parameter `smooth_idf=True` adds "1" to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:

\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\)

Using this modification, the tf-idf of the third term in document 1 changes to 1.8473:

\(\text{tf-idf}_{\text{term3}} = 1 \times \log(7/3)+1 \approx 1.8473\)

And the L2-normalized tf-idf changes to

\(\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}}
= [0.8515, 0, 0.5243]\):

    >>> transformer = TfidfTransformer()
    >>> transformer.fit_transform(counts).toarray()
    array([[0.85151335, 0.        , 0.52433293],
          [1.        , 0.        , 0.        ],
          [1.        , 0.        , 0.        ],
          [1.        , 0.        , 0.        ],
          [0.55422893, 0.83236428, 0.        ],
          [0.63035731, 0.        , 0.77630514]])

The weights of each feature computed by the `fit` method call are stored in a model attribute:

    >>> transformer.idf_
    array([1. ..., 2.25..., 1.84...])

As tf-idf is very often used for text features, there is also another class called <span class="title-ref">TfidfVectorizer</span> that combines all the options of <span class="title-ref">CountVectorizer</span> and <span class="title-ref">TfidfTransformer</span> in a single model:

    >>> from sklearn.feature_extraction.text import TfidfVectorizer
    >>> vectorizer = TfidfVectorizer()
    >>> vectorizer.fit_transform(corpus)
    <Compressed Sparse...dtype 'float64'
      with 19 stored elements and shape (4, 9)>

While the tf-idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the `binary` parameter of <span class="title-ref">CountVectorizer</span>. In particular, some estimators such as \[bernoulli\_naive\_bayes\](\#bernoulli\_naive\_bayes) explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf-idf values while the binary occurrence info is more stable.

As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier:

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py)

</div>

### Decoding text files

Text is made of characters, but files are made of bytes. These bytes represent characters according to some *encoding*. To work with text files in Python, their bytes must be *decoded* to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist.

<div class="note">

<div class="title">

Note

</div>

An encoding can also be called a 'character set', but this term is less accurate: several encodings can exist for a single character set.

</div>

The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The <span class="title-ref">CountVectorizer</span> takes an `encoding` parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (`encoding="utf-8"`).

If the text you are loading is not actually encoded with UTF-8, however, you will get a `UnicodeDecodeError`. The vectorizers can be told to be silent about decoding errors by setting the `decode_error` parameter to either `"ignore"` or `"replace"`. See the documentation for the Python function `bytes.decode` for more details (type `help(bytes.decode)` at the Python prompt).

<div class="dropdown">

Troubleshooting decoding text

If you are having trouble decoding text, here are some things to try:

  - Find out what the actual encoding of the text is. The file might come with a header or README that tells you the encoding, or there might be some standard encoding you can assume based on where the text comes from.
  - You may be able to find out what kind of encoding it is in general using the UNIX command `file`. The Python `chardet` module comes with a script called `chardetect.py` that will guess the specific encoding, though you cannot rely on its guess being correct.
  - You could try UTF-8 and disregard the errors. You can decode byte strings with `bytes.decode(errors='replace')` to replace all decoding errors with a meaningless character, or set `decode_error='replace'` in the vectorizer. This may damage the usefulness of your features.
  - Real text may come from a variety of sources that may have used different encodings, or even be sloppily decoded in a different encoding than the one it was encoded with. This is common in text retrieved from the Web. The Python package [ftfy](https://github.com/LuminosoInsight/python-ftfy) can automatically sort out some classes of decoding errors, so you could try decoding the unknown text as `latin-1` and then using `ftfy` to fix errors.
  - If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as `latin-1`. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature.

For example, the following snippet uses `chardet` (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.

> \>\>\> import chardet \# doctest: +SKIP \>\>\> text1 = b"Sei mir gegrxc3xbcxc3x9ft mein Sauerkraut" \>\>\> text2 = b"holdselig sind deine Gerxfcche" \>\>\> text3 = b"xffxfeAx00ux00fx00 x00Fx00lx00xfcx00gx00ex00lx00nx00 x00dx00ex00sx00 x00Gx00ex00sx00ax00nx00gx00ex00sx00,x00 x00Hx00ex00rx00zx00lx00ix00ex00bx00cx00hx00ex00nx00,x00 x00tx00rx00ax00gx00 x00ix00cx00hx00 x00dx00ix00cx00hx00 x00fx00ox00rx00tx00" \>\>\> decoded = \[x.decode(chardet.detect(x)\['encoding'\]) ... for x in (text1, text2, text3)\] \# doctest: +SKIP \>\>\> v = CountVectorizer().fit(decoded).[vocabulary]() \# doctest: +SKIP \>\>\> for term in v: print(v) \# doctest: +SKIP

(Depending on the version of `chardet`, it might get the first one wrong.)

For an introduction to Unicode and character encodings in general, see Joel Spolsky's [Absolute Minimum Every Software Developer Must Know About Unicode](https://www.joelonsoftware.com/articles/Unicode.html).

</div>

### Applications and examples

The bag of words representation is quite simplistic but surprisingly useful in practice.

In particular in a **supervised setting** it can be successfully combined with fast and scalable linear models to train **document classifiers**, for instance:

  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py)

In an **unsupervised setting** it can be used to group similar documents together by applying clustering algorithms such as \[k\_means\](\#k\_means):

  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_clustering.py)

Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using \[NMF\](\#nmf):

  - \[sphx\_glr\_auto\_examples\_applications\_plot\_topics\_extraction\_with\_nmf\_lda.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_topics\_extraction\_with\_nmf\_lda.py)

### Limitations of the Bag of Words representation

A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn't account for potential misspellings or word derivations.

N-grams to the rescue\! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.

One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations.

For example, let's say we're dealing with a corpus of two documents: `['words', 'wprds']`. The second document contains a misspelling of the word 'words'. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:

    >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))
    >>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])
    >>> ngram_vectorizer.get_feature_names_out()
    array([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)
    >>> counts.toarray().astype(int)
    array([[1, 1, 1, 0, 1, 1, 1, 0],
           [1, 1, 0, 1, 1, 1, 0, 1]])

In the above example, `char_wb` analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The `char` analyzer, alternatively, creates n-grams that span across words:

    >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))
    >>> ngram_vectorizer.fit_transform(['jumpy fox'])
    <Compressed Sparse...dtype 'int64'
      with 4 stored elements and shape (1, 4)>
    
    >>> ngram_vectorizer.get_feature_names_out()
    array([' fox ', ' jump', 'jumpy', 'umpy '], ...)
    
    >>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))
    >>> ngram_vectorizer.fit_transform(['jumpy fox'])
    <Compressed Sparse...dtype 'int64'
      with 5 stored elements and shape (1, 5)>
    >>> ngram_vectorizer.get_feature_names_out()
    array(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)

The word boundaries-aware variant `char_wb` is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw `char` variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.

While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure.

In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as "Structured output" problems which are currently outside of the scope of scikit-learn.

### Vectorizing a large text corpus with the hashing trick

The above vectorization scheme is simple but the fact that it holds an **in-memory mapping from the string tokens to the integer feature indices** (the `vocabulary_` attribute) causes several **problems when dealing with large datasets**:

  - the larger the corpus, the larger the vocabulary will grow and hence the memory use too,
  - fitting requires the allocation of intermediate data structures of size proportional to that of the original dataset.
  - building the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner.
  - pickling and un-pickling vectorizers with a large `vocabulary_` can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size),
  - it is not easily possible to split the vectorization work into concurrent sub tasks as the `vocabulary_` attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers' performance to the point of making them slower than the sequential variant.

It is possible to overcome those limitations by combining the "hashing trick" (\[Feature\_hashing\](\#feature\_hashing)) implemented by the <span class="title-ref">\~sklearn.feature\_extraction.FeatureHasher</span> class and the text preprocessing and tokenization features of the <span class="title-ref">CountVectorizer</span>.

This combination is implementing in <span class="title-ref">HashingVectorizer</span>, a transformer class that is mostly API compatible with <span class="title-ref">CountVectorizer</span>. <span class="title-ref">HashingVectorizer</span> is stateless, meaning that you don't have to call `fit` on it:

    >>> from sklearn.feature_extraction.text import HashingVectorizer
    >>> hv = HashingVectorizer(n_features=10)
    >>> hv.transform(corpus)
    <Compressed Sparse...dtype 'float64'
      with 16 stored elements and shape (4, 10)>

You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros extracted previously by the <span class="title-ref">CountVectorizer</span> on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the `n_features` parameter.

In a real world setting, the `n_features` parameter can be left to its default value of `2 ** 20` (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as `2 ** 18` might help without introducing too many additional collisions on typical text classification tasks.

Note that the dimensionality does not affect the CPU training time of algorithms which operate on CSR matrices (`LinearSVC(dual=True)`, `Perceptron`, `SGDClassifier`, `PassiveAggressive`) but it does for algorithms that work with CSC matrices (`LinearSVC(dual=False)`, `Lasso()`, etc.).

Let's try again with the default setting:

    >>> hv = HashingVectorizer()
    >>> hv.transform(corpus)
    <Compressed Sparse...dtype 'float64'
      with 19 stored elements and shape (4, 1048576)>

We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other.

The <span class="title-ref">HashingVectorizer</span> also comes with the following limitations:

  - it is not possible to invert the model (no `inverse_transform` method), nor to access the original string representation of the features, because of the one-way nature of the hash function that performs the mapping.
  - it does not provide IDF weighting as that would introduce statefulness in the model. A <span class="title-ref">TfidfTransformer</span> can be appended to it in a pipeline if required.

<div class="dropdown">

Performing out-of-core scaling with HashingVectorizer

An interesting development of using a <span class="title-ref">HashingVectorizer</span> is the ability to perform [out-of-core]() scaling. This means that we can learn from data that does not fit into the computer's main memory.

A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is vectorized using <span class="title-ref">HashingVectorizer</span> so as to guarantee that the input space of the estimator has always the same dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning time is often limited by the CPU time one wants to spend on the task.

For a full-fledged example of out-of-core scaling in a text classification task see \[sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py).

</div>

### Customizing the vectorizer classes

It is possible to customize the behavior by passing a callable to the vectorizer constructor:

    >>> def my_tokenizer(s):
    ...     return s.split()
    ...
    >>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)
    >>> vectorizer.build_analyzer()(u"Some... punctuation!") == (
    ...     ['some...', 'punctuation!'])
    True

In particular we name:

  - `preprocessor`: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc.
  - `tokenizer`: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these.
  - `analyzer`: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps.

(Lucene users might recognize these names, but be aware that scikit-learn concepts may not map one-to-one onto Lucene concepts.)

To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the `build_preprocessor`, `build_tokenizer` and `build_analyzer` factory methods instead of passing custom functions.

<div class="dropdown" color="success">

Tips and tricks

  - If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass `analyzer=str.split`

  - Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here's a `CountVectorizer` with a tokenizer and lemmatizer using [NLTK](https://www.nltk.org/):
    
        >>> from nltk import word_tokenize          # doctest: +SKIP
        >>> from nltk.stem import WordNetLemmatizer # doctest: +SKIP
        >>> class LemmaTokenizer:
        ...     def __init__(self):
        ...         self.wnl = WordNetLemmatizer()
        ...     def __call__(self, doc):
        ...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]
        ...
        >>> vect = CountVectorizer(tokenizer=LemmaTokenizer())  # doctest: +SKIP
    
    (Note that this will not filter out punctuation.)
    
    The following example will, for instance, transform some British spelling to American spelling:
    
        >>> import re
        >>> def to_british(tokens):
        ...     for t in tokens:
        ...         t = re.sub(r"(...)our$", r"\1or", t)
        ...         t = re.sub(r"([bt])re$", r"\1er", t)
        ...         t = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2", t)
        ...         t = re.sub(r"ogue$", "og", t)
        ...         yield t
        ...
        >>> class CustomVectorizer(CountVectorizer):
        ...     def build_tokenizer(self):
        ...         tokenize = super().build_tokenizer()
        ...         return lambda doc: list(to_british(tokenize(doc)))
        ...
        >>> print(CustomVectorizer().build_analyzer()(u"color colour"))
        [...'color', ...'color']
    
    for other styles of preprocessing; examples include stemming, lemmatization, or normalizing numerical tokens, with the latter illustrated in:
    
      - \[sphx\_glr\_auto\_examples\_bicluster\_plot\_bicluster\_newsgroups.py\](\#sphx\_glr\_auto\_examples\_bicluster\_plot\_bicluster\_newsgroups.py)

Customizing the vectorizer can also be useful when handling Asian languages that do not use an explicit word separator such as whitespace.

</div>

## Image feature extraction

<div class="currentmodule">

sklearn.feature\_extraction.image

</div>

### Patch extraction

The <span class="title-ref">extract\_patches\_2d</span> function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use <span class="title-ref">reconstruct\_from\_patches\_2d</span>. For example let us generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):

    >>> import numpy as np
    >>> from sklearn.feature_extraction import image
    
    >>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))
    >>> one_image[:, :, 0]  # R channel of a fake RGB picture
    array([[ 0,  3,  6,  9],
           [12, 15, 18, 21],
           [24, 27, 30, 33],
           [36, 39, 42, 45]])
    
    >>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,
    ...     random_state=0)
    >>> patches.shape
    (2, 2, 2, 3)
    >>> patches[:, :, :, 0]
    array([[[ 0,  3],
            [12, 15]],
    <BLANKLINE>
           [[15, 18],
            [27, 30]]])
    >>> patches = image.extract_patches_2d(one_image, (2, 2))
    >>> patches.shape
    (9, 2, 2, 3)
    >>> patches[4, :, :, 0]
    array([[15, 18],
           [27, 30]])

Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:

    >>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))
    >>> np.testing.assert_array_equal(one_image, reconstructed)

The <span class="title-ref">PatchExtractor</span> class works in the same way as <span class="title-ref">extract\_patches\_2d</span>, only it supports multiple images as input. It is implemented as a scikit-learn transformer, so it can be used in pipelines. See:

    >>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)
    >>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)
    >>> patches.shape
    (45, 2, 2, 3)

### Connectivity graph of an image

Several estimators in the scikit-learn can use connectivity information between features or samples. For instance Ward clustering (\[hierarchical\_clustering\](\#hierarchical\_clustering)) can cluster together only neighboring pixels of an image, thus forming contiguous patches:

![](../auto_examples/cluster/images/sphx_glr_plot_coin_ward_segmentation_001.png)

For this purpose, the estimators use a 'connectivity' matrix, giving which samples are connected.

The function <span class="title-ref">img\_to\_graph</span> returns such a matrix from a 2D or 3D image. Similarly, <span class="title-ref">grid\_to\_graph</span> build a connectivity matrix for images given the shape of these image.

These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (\[hierarchical\_clustering\](\#hierarchical\_clustering)), but also to build precomputed kernels, or similarity matrices.

<div class="note">

<div class="title">

Note

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_ward\_segmentation.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_coin\_ward\_segmentation.py)
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_segmentation\_toy.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_segmentation\_toy.py)
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_feature\_agglomeration\_vs\_univariate\_selection.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_feature\_agglomeration\_vs\_univariate\_selection.py)

</div>

<div id="citations">

  - <span id="NQY18" class="citation-label">NQY18</span>  
    J. Nothman, H. Qin and R. Yurchak (2018). ["Stop Word Lists in Free Open-source Software Packages"](https://aclweb.org/anthology/W18-2502). In *Proc. Workshop for NLP Open Source Software*.

</div>

---

feature_selection.md

---

<div class="currentmodule">

sklearn.feature\_selection

</div>

# Feature selection

The classes in the `sklearn.feature_selection` module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators' accuracy scores or to boost their performance on very high-dimensional datasets.

## Removing features with low variance

<span class="title-ref">VarianceThreshold</span> is a simple baseline approach to feature selection. It removes all features whose variance doesn't meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.

As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by

\[\mathrm{Var}[X] = p(1 - p)\]

so we can select using the threshold `.8 * (1 - .8)`:

    >>> from sklearn.feature_selection import VarianceThreshold
    >>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
    >>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
    >>> sel.fit_transform(X)
    array([[0, 1],
           [1, 0],
           [0, 0],
           [1, 1],
           [1, 0],
           [1, 1]])

As expected, `VarianceThreshold` has removed the first column, which has a probability \(p = 5/6 > .8\) of containing a zero.

## Univariate feature selection

Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the `transform` method:

  - <span class="title-ref">SelectKBest</span> removes all but the \(k\) highest scoring features
  - <span class="title-ref">SelectPercentile</span> removes all but a user-specified highest scoring percentage of features
  - using common univariate statistical tests for each feature: false positive rate <span class="title-ref">SelectFpr</span>, false discovery rate <span class="title-ref">SelectFdr</span>, or family wise error <span class="title-ref">SelectFwe</span>.
  - <span class="title-ref">GenericUnivariateSelect</span> allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.

For instance, we can use a F-test to retrieve the two best features for a dataset as follows:

> \>\>\> from sklearn.datasets import load\_iris \>\>\> from sklearn.feature\_selection import SelectKBest \>\>\> from sklearn.feature\_selection import f\_classif \>\>\> X, y = load\_iris(return\_X\_y=True) \>\>\> X.shape (150, 4) \>\>\> X\_new = SelectKBest(f\_classif, k=2).fit\_transform(X, y) \>\>\> X\_new.shape (150, 2)

These objects take as input a scoring function that returns univariate scores and p-values (or only scores for <span class="title-ref">SelectKBest</span> and <span class="title-ref">SelectPercentile</span>):

  - For regression: <span class="title-ref">r\_regression</span>, <span class="title-ref">f\_regression</span>, <span class="title-ref">mutual\_info\_regression</span>
  - For classification: <span class="title-ref">chi2</span>, <span class="title-ref">f\_classif</span>, <span class="title-ref">mutual\_info\_classif</span>

The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation. Note that the \(\chi^2\)-test should only be applied to non-negative features, such as frequencies.

<div class="topic">

**Feature selection with sparse data**

If you use sparse data (i.e. data represented as sparse matrices), <span class="title-ref">chi2</span>, <span class="title-ref">mutual\_info\_regression</span>, <span class="title-ref">mutual\_info\_classif</span> will deal with the data without making it dense.

</div>

\> **Warning** \> Beware not to use a regression scoring function with a classification problem, you will get useless results.

\> **Note** \> The <span class="title-ref">SelectPercentile</span> and <span class="title-ref">SelectKBest</span> support unsupervised feature selection as well. One needs to provide a <span class="title-ref">score\_func</span> where <span class="title-ref">y=None</span>. The <span class="title-ref">score\_func</span> should use internally <span class="title-ref">X</span> to compute the scores.

**Examples**

  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_feature\_selection.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_feature\_selection.py)
  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_f\_test\_vs\_mi.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_f\_test\_vs\_mi.py)

## Recursive feature elimination

Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (<span class="title-ref">RFE</span>) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as `coef_`, `feature_importances_`) or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.

<span class="title-ref">RFECV</span> performs RFE in a cross-validation loop to find the optimal number of features. In more details, the number of features selected is tuned automatically by fitting an <span class="title-ref">RFE</span> selector on the different cross-validation splits (provided by the <span class="title-ref">cv</span> parameter). The performance of the <span class="title-ref">RFE</span> selector are evaluated using <span class="title-ref">scorer</span> for different number of selected features and aggregated together. Finally, the scores are averaged across folds and the number of features selected is set to the number of features that maximize the cross-validation score.

**Examples**

  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_digits.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_digits.py): A recursive feature elimination example showing the relevance of pixels in a digit classification task.
  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_with\_cross\_validation.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_with\_cross\_validation.py): A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.

## Feature selection using SelectFromModel

<span class="title-ref">SelectFromModel</span> is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`) or via an <span class="title-ref">importance\_getter</span> callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided `threshold` parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are "mean", "median" and float multiples of these like "0.1\*mean". In combination with the <span class="title-ref">threshold</span> criteria, one can use the <span class="title-ref">max\_features</span> parameter to set a limit on the number of features to select.

For examples on how it is to be used refer to the sections below.

**Examples**

  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_select\_from\_model\_diabetes.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_select\_from\_model\_diabetes.py)

### L1-based feature selection

<div class="currentmodule">

sklearn

</div>

\[Linear models \<linear\_model\>\](\#linear-models-\<linear\_model\>) penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with <span class="title-ref">\~feature\_selection.SelectFromModel</span> to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the <span class="title-ref">\~linear\_model.Lasso</span> for regression, and of <span class="title-ref">\~linear\_model.LogisticRegression</span> and <span class="title-ref">\~svm.LinearSVC</span> for classification:

    >>> from sklearn.svm import LinearSVC
    >>> from sklearn.datasets import load_iris
    >>> from sklearn.feature_selection import SelectFromModel
    >>> X, y = load_iris(return_X_y=True)
    >>> X.shape
    (150, 4)
    >>> lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
    >>> model = SelectFromModel(lsvc, prefit=True)
    >>> X_new = model.transform(X)
    >>> X_new.shape
    (150, 3)

With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_dense\_vs\_sparse\_data.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_dense\_vs\_sparse\_data.py).

<div id="compressive_sensing">

<div class="dropdown">

L1-recovery and compressive sensing

For a good choice of alpha, the \[lasso\](\#lasso) can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be "sufficiently large", or L1 models will perform at random, where "sufficiently large" depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated. On the use of Lasso for sparse signal recovery, see this example on compressive sensing: \[sphx\_glr\_auto\_examples\_applications\_plot\_tomography\_l1\_reconstruction.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_tomography\_l1\_reconstruction.py).

There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (<span class="title-ref">\~sklearn.linear\_model.LassoCV</span> or <span class="title-ref">\~sklearn.linear\_model.LassoLarsCV</span>), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (<span class="title-ref">\~sklearn.linear\_model.LassoLarsIC</span>) tends, on the opposite, to set high values of alpha.

**References**

Richard G. Baraniuk "Compressive Sensing", IEEE Signal Processing Magazine \[120\] July 2007 <http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf>

</div>

</div>

### Tree-based feature selection

Tree-based estimators (see the `sklearn.tree` module and forest of trees in the `sklearn.ensemble` module) can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (when coupled with the <span class="title-ref">\~feature\_selection.SelectFromModel</span> meta-transformer):

    >>> from sklearn.ensemble import ExtraTreesClassifier
    >>> from sklearn.datasets import load_iris
    >>> from sklearn.feature_selection import SelectFromModel
    >>> X, y = load_iris(return_X_y=True)
    >>> X.shape
    (150, 4)
    >>> clf = ExtraTreesClassifier(n_estimators=50)
    >>> clf = clf.fit(X, y)
    >>> clf.feature_importances_  # doctest: +SKIP
    array([ 0.04...,  0.05...,  0.4...,  0.4...])
    >>> model = SelectFromModel(clf, prefit=True)
    >>> X_new = model.transform(X)
    >>> X_new.shape               # doctest: +SKIP
    (150, 2)

**Examples**

  - \[sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_importances.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_forest\_importances.py): example on synthetic data showing the recovery of the actually meaningful features.
  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py): example discussing the caveats of using impurity-based feature importances as a proxy for feature relevance.

## Sequential Feature Selection

Sequential Feature Selection [\[sfs\]]() (SFS) is available in the <span class="title-ref">\~sklearn.feature\_selection.SequentialFeatureSelector</span> transformer. SFS can be either forward or backward:

Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero features and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the <span class="title-ref">n\_features\_to\_select</span> parameter.

Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no features and greedily adding features, we start with *all* the features and greedily *remove* features from the set. The <span class="title-ref">direction</span> parameter controls whether forward or backward SFS is used.

<div class="dropdown">

Details on Sequential Feature Selection

In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.

SFS differs from <span class="title-ref">\~sklearn.feature\_selection.RFE</span> and <span class="title-ref">\~sklearn.feature\_selection.SelectFromModel</span> in that it does not require the underlying model to expose a <span class="title-ref">coef\_</span> or <span class="title-ref">feature\_importances\_</span> attribute. It may however be slower considering that more models need to be evaluated, compared to the other approaches. For example in backward selection, the iteration going from <span class="title-ref">m</span> features to <span class="title-ref">m - 1</span> features using k-fold cross-validation requires fitting <span class="title-ref">m \* k</span> models, while <span class="title-ref">\~sklearn.feature\_selection.RFE</span> would require only a single fit, and <span class="title-ref">\~sklearn.feature\_selection.SelectFromModel</span> always just does a single fit and requires no iterations.

**References**

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_select\_from\_model\_diabetes.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_select\_from\_model\_diabetes.py)

## Feature selection as part of a pipeline

Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a \`\~pipeline.Pipeline\`:

    clf = Pipeline([
      ('feature_selection', SelectFromModel(LinearSVC(penalty="l1"))),
      ('classification', RandomForestClassifier())
    ])
    clf.fit(X, y)

In this snippet we make use of a <span class="title-ref">\~svm.LinearSVC</span> coupled with <span class="title-ref">\~feature\_selection.SelectFromModel</span> to evaluate feature importances and select the most relevant features. Then, a <span class="title-ref">\~ensemble.RandomForestClassifier</span> is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the <span class="title-ref">\~pipeline.Pipeline</span> examples for more details.

---

gaussian_process.md

---

# Gaussian Processes

<div class="currentmodule">

sklearn.gaussian\_process

</div>

**Gaussian Processes (GP)** are a nonparametric supervised learning method used to solve *regression* and *probabilistic classification* problems.

The advantages of Gaussian processes are:

  - The prediction interpolates the observations (at least for regular kernels).
  - The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.

<!-- end list -->

  - \- Versatile: different \[kernels  
    \<gp\_kernels\>\](\#kernels

  - \--\<gp\_kernels\>) can be specified. Common kernels are provided, but  
    it is also possible to specify custom kernels.

The disadvantages of Gaussian processes include:

  - Our implementation is not sparse, i.e., they use the whole samples/features information to perform the prediction.
  - They lose efficiency in high dimensional spaces -- namely when the number of features exceeds a few dozens.

## Gaussian Process Regression (GPR)

<div class="currentmodule">

sklearn.gaussian\_process

</div>

The <span class="title-ref">GaussianProcessRegressor</span> implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. GP will combine this prior and the likelihood function based on training samples. It allows to give a probabilistic approach to prediction by giving the mean and standard deviation as output when predicting.

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_targets_002.png)

The prior mean is assumed to be constant and zero (for <span class="title-ref">normalize\_y=False</span>) or the training data's mean (for <span class="title-ref">normalize\_y=True</span>). The prior's covariance is specified by passing a \[kernel \<gp\_kernels\>\](\#kernel-\<gp\_kernels\>) object. The hyperparameters of the kernel are optimized when fitting the <span class="title-ref">GaussianProcessRegressor</span> by maximizing the log-marginal-likelihood (LML) based on the passed <span class="title-ref">optimizer</span>. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying <span class="title-ref">n\_restarts\_optimizer</span>. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, <span class="title-ref">None</span> can be passed as optimizer.

The noise level in the targets can be specified by passing it via the parameter <span class="title-ref">alpha</span>, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric instabilities during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a <span class="title-ref">\~sklearn.gaussian\_process.kernels.WhiteKernel</span> component into the kernel, which can estimate the global noise level from the data (see example below). The figure below shows the effect of noisy target handled by setting the parameter <span class="title-ref">alpha</span>.

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_targets_003.png)

The implementation is based on Algorithm 2.1 of [\[RW2006\]](#RW2006). In addition to the API of standard scikit-learn estimators, \`GaussianProcessRegressor\`:

  - allows prediction without prior fitting (based on the GP prior)
  - provides an additional method `sample_y(X)`, which evaluates samples drawn from the GPR (prior or posterior) at given inputs
  - exposes a method `log_marginal_likelihood(theta)`, which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo.

**Examples**

  - \[sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_gpr\_noisy\_targets.py\](\#sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_gpr\_noisy\_targets.py)
  - \[sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_gpr\_noisy.py\](\#sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_gpr\_noisy.py)
  - \[sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_compare\_gpr\_krr.py\](\#sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_compare\_gpr\_krr.py)
  - \[sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_gpr\_co2.py\](\#sphx\_glr\_auto\_examples\_gaussian\_process\_plot\_gpr\_co2.py)

## Gaussian Process Classification (GPC)

<div class="currentmodule">

sklearn.gaussian\_process

</div>

The <span class="title-ref">GaussianProcessClassifier</span> implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function \(f\), which is then squashed through a link function to obtain the probabilistic classification. The latent function \(f\) is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and \(f\) is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.

In contrast to the regression setting, the posterior of the latent function \(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [\[RW2006\]](#RW2006).

The GP prior mean is assumed to be zero. The prior's covariance is specified by passing a \[kernel \<gp\_kernels\>\](\#kernel-\<gp\_kernels\>) object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed `optimizer`. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying `n_restarts_optimizer`. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, <span class="title-ref">None</span> can be passed as optimizer.

<span class="title-ref">GaussianProcessClassifier</span> supports multi-class classification by performing either one-versus-rest or one-versus-one based training and prediction. In one-versus-rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In "one\_vs\_one", one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. See the section on \[multi-class classification \<multiclass\>\](\#multi-class-classification-\<multiclass\>) for more details.

In the case of Gaussian process classification, "one\_vs\_one" might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that "one\_vs\_one" does not support predicting probability estimates but only plain predictions. Moreover, note that <span class="title-ref">GaussianProcessClassifier</span> does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.

## GPC examples

### Probabilistic predictions with GPC

This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).

While the hyperparameters chosen by optimizing LML have a considerably larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC.

The second figure shows the log-marginal-likelihood for different choices of the kernel's hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png)

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_002.png)

### Illustration of GPC on the XOR dataset

<div class="currentmodule">

sklearn.gaussian\_process.kernels

</div>

This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (<span class="title-ref">RBF</span>) and a non-stationary kernel (<span class="title-ref">DotProduct</span>). On this particular dataset, the <span class="title-ref">DotProduct</span> kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as <span class="title-ref">RBF</span> often obtain better results.

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_xor_001.png)

<div class="currentmodule">

sklearn.gaussian\_process

</div>

### Gaussian process classification (GPC) on iris dataset

This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_iris_001.png)

## Kernels for Gaussian Processes

<div class="currentmodule">

sklearn.gaussian\_process.kernels

</div>

Kernels (also called "covariance functions" in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the "similarity" of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [\[RW2006\]](#RW2006). For guidance on how to best combine different kernels, we refer to [\[Duv2014\]](#Duv2014).

<div class="dropdown">

Gaussian Process Kernel API

The main usage of a <span class="title-ref">Kernel</span> is to compute the GP's covariance between datapoints. For this, the method `__call__` of the kernel can be called. This method can either be used to compute the "auto-covariance" of all pairs of datapoints in a 2d array X, or the "cross-covariance" of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the <span class="title-ref">WhiteKernel</span>): `k(X) == K(X, Y=X)`

If only the diagonal of the auto-covariance is being used, the method `diag()` of a kernel can be called, which is more computationally efficient than the equivalent call to `__call__`: `np.diag(k(X, X)) == k.diag(X)`

Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel's auto-covariance with respect to \(log(\theta)\) via setting `eval_gradient=True` in the `__call__` method. That is, a `(len(X), len(X), len(theta))` array is returned where the entry `[i, j, l]` contains \(\frac{\partial k_\theta(x_i, x_j)}{\partial log(\theta_l)}\). This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property `theta` of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property `bounds` of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of <span class="title-ref">Hyperparameter</span> in the respective kernel. Note that a kernel using a hyperparameter with name "x" must have the attributes self.x and self.x\_bounds.

The abstract base class for all kernels is <span class="title-ref">Kernel</span>. Kernel implements a similar interface as <span class="title-ref">\~sklearn.base.BaseEstimator</span>, providing the methods `get_params()`, `set_params()`, and `clone()`. This allows setting kernel values also via meta-estimators such as <span class="title-ref">\~sklearn.pipeline.Pipeline</span> or <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span>. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with `k1__` and parameters of the right operand with `k2__`. An additional convenience method is `clone_with_theta(theta)`, which returns a cloned version of the kernel but with the hyperparameters set to `theta`. An illustrative example:

> \>\>\> from sklearn.gaussian\_process.kernels import ConstantKernel, RBF \>\>\> kernel = ConstantKernel(constant\_value=1.0, constant\_value\_bounds=(0.0, 10.0)) \* RBF(length\_scale=0.5, length\_scale\_bounds=(0.0, 10.0)) + RBF(length\_scale=2.0, length\_scale\_bounds=(0.0, 10.0)) \>\>\> for hyperparameter in kernel.hyperparameters: print(hyperparameter) Hyperparameter(name='k1\_\_k1\_\_constant\_value', value\_type='numeric', bounds=array(\[\[ 0., 10.\]\]), n\_elements=1, fixed=False) Hyperparameter(name='k1\_\_k2\_\_length\_scale', value\_type='numeric', bounds=array(\[\[ 0., 10.\]\]), n\_elements=1, fixed=False) Hyperparameter(name='k2\_\_length\_scale', value\_type='numeric', bounds=array(\[\[ 0., 10.\]\]), n\_elements=1, fixed=False) \>\>\> params = kernel.get\_params() \>\>\> for key in sorted(params): print("%s : %s" % (key, params\[key\])) k1 : 1\**2* RBF(length\_scale=0.5) k1\_\_k1 : 1\*\*2 k1\_\_k1\_\_constant\_value : 1.0 k1\_\_k1\_\_constant\_value\_bounds : (0.0, 10.0) k1\_\_k2 : RBF(length\_scale=0.5) k1\_\_k2\_\_length\_scale : 0.5 k1\_\_k2\_\_length\_scale\_bounds : (0.0, 10.0) k2 : RBF(length\_scale=2) k2\_\_length\_scale : 2.0 k2\_\_length\_scale\_bounds : (0.0, 10.0) \>\>\> print(kernel.theta) \# Note: log-transformed \[ 0. -0.69314718 0.69314718\] \>\>\> print(kernel.bounds) \# Note: log-transformed \[\[ -inf 2.30258509\] \[ -inf 2.30258509\] \[ -inf 2.30258509\]\]

All Gaussian process kernels are interoperable with `sklearn.metrics.pairwise` and vice versa: instances of subclasses of <span class="title-ref">Kernel</span> can be passed as `metric` to `pairwise_kernels` from `sklearn.metrics.pairwise`. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class <span class="title-ref">PairwiseKernel</span>. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter `gamma` is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.

</div>

### Basic kernels

The <span class="title-ref">ConstantKernel</span> kernel can be used as part of a <span class="title-ref">Product</span> kernel where it scales the magnitude of the other factor (kernel) or as part of a <span class="title-ref">Sum</span> kernel, where it modifies the mean of the Gaussian process. It depends on a parameter \(constant\_value\). It is defined as:

\[k(x_i, x_j) = constant\_value \;\forall\; x_1, x_2\]

The main use-case of the <span class="title-ref">WhiteKernel</span> kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:

\[k(x_i, x_j) = noise\_level \text{ if } x_i == x_j \text{ else } 0\]

### Kernel operators

Kernel operators take one or two base kernels and combine them into a new kernel. The <span class="title-ref">Sum</span> kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\). The <span class="title-ref">Product</span> kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\). The <span class="title-ref">Exponentiation</span> kernel takes one base kernel and a scalar parameter \(p\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^p\). Note that magic methods `__add__`, `__mul___` and `__pow__` are overridden on the Kernel objects, so one can use e.g. `RBF() + RBF()` as a shortcut for `Sum(RBF(), RBF())`.

### Radial basis function (RBF) kernel

The <span class="title-ref">RBF</span> kernel is a stationary kernel. It is also known as the "squared exponential" kernel. It is parameterized by a length-scale parameter \(l>0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:

\[k(x_i, x_j) = \text{exp}\left(- \frac{d(x_i, x_j)^2}{2l^2} \right)\]

where \(d(\cdot, \cdot)\) is the Euclidean distance. This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png)

### Matérn kernel

The <span class="title-ref">Matern</span> kernel is a stationary kernel and a generalization of the <span class="title-ref">RBF</span> kernel. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter \(l>0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel).

<div class="dropdown">

Mathematical implementation of Matérn kernel

The kernel is given by:

\[k(x_i, x_j) = \frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg)^\nu K_\nu\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg),\]

where \(d(\cdot,\cdot)\) is the Euclidean distance, \(K_\nu(\cdot)\) is a modified Bessel function and \(\Gamma(\cdot)\) is the gamma function. As \(\nu\rightarrow\infty\), the Matérn kernel converges to the RBF kernel. When \(\nu = 1/2\), the Matérn kernel becomes identical to the absolute exponential kernel, i.e.,

\[k(x_i, x_j) = \exp \Bigg(- \frac{1}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{1}{2}\]

In particular, \(\nu = 3/2\):

\[k(x_i, x_j) =  \Bigg(1 + \frac{\sqrt{3}}{l} d(x_i , x_j )\Bigg) \exp \Bigg(-\frac{\sqrt{3}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{3}{2}\]

and \(\nu = 5/2\):

\[k(x_i, x_j) = \Bigg(1 + \frac{\sqrt{5}}{l} d(x_i , x_j ) +\frac{5}{3l} d(x_i , x_j )^2 \Bigg) \exp \Bigg(-\frac{\sqrt{5}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{5}{2}\]

are popular choices for learning functions that are not infinitely differentiable (as assumed by the RBF kernel) but at least once (\(\nu =
3/2\)) or twice differentiable (\(\nu = 5/2\)).

The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation.

</div>

The prior and posterior of a GP resulting from a Matérn kernel are shown in the following figure:

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_005.png)

See [\[RW2006\]](#RW2006), pp84 for further details regarding the different variants of the Matérn kernel.

### Rational quadratic kernel

The <span class="title-ref">RationalQuadratic</span> kernel can be seen as a scale mixture (an infinite sum) of <span class="title-ref">RBF</span> kernels with different characteristic length-scales. It is parameterized by a length-scale parameter \(l>0\) and a scale mixture parameter \(\alpha>0\) Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:

\[k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^{-\alpha}\]

The prior and posterior of a GP resulting from a <span class="title-ref">RationalQuadratic</span> kernel are shown in the following figure:

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png)

### Exp-Sine-Squared kernel

The <span class="title-ref">ExpSineSquared</span> kernel allows modeling periodic functions. It is parameterized by a length-scale parameter \(l>0\) and a periodicity parameter \(p>0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:

\[k(x_i, x_j) = \text{exp}\left(- \frac{ 2\sin^2(\pi d(x_i, x_j) / p) }{ l^ 2} \right)\]

The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png)

### Dot-Product kernel

The <span class="title-ref">DotProduct</span> kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The <span class="title-ref">DotProduct</span> kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter \(\sigma_0^2\). For \(\sigma_0^2 = 0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by

\[k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j\]

The <span class="title-ref">DotProduct</span> kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:

![](../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png)

### References

<div class="currentmodule">

sklearn.gaussian\_process

</div>

<div id="citations">

  - <span id="Duv2014" class="citation-label">Duv2014</span>  
    [David Duvenaud, "The Kernel Cookbook: Advice on Covariance functions", 2014](https://www.cs.toronto.edu/~duvenaud/cookbook/)

  - <span id="RW2006" class="citation-label">RW2006</span>  
    [Carl E. Rasmussen and Christopher K.I. Williams, "Gaussian Processes for Machine Learning", MIT Press 2006](https://www.gaussianprocess.org/gpml/chapters/RW.pdf)

</div>

---

grid_search.md

---

<div class="currentmodule">

sklearn.model\_selection

</div>

# Tuning the hyper-parameters of an estimator

Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include `C`, `kernel` and `gamma` for Support Vector Classifier, `alpha` for Lasso, etc.

It is possible and recommended to search the hyper-parameter space for the best \[cross validation \<cross\_validation\>\](\#cross-validation-\<cross\_validation\>) score.

Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:

    estimator.get_params()

A search consists of:

  - an estimator (regressor or classifier such as `sklearn.svm.SVC()`);
  - a parameter space;
  - a method for searching or sampling candidates;
  - a cross-validation scheme; and
  - a \[score function \<gridsearch\_scoring\>\](\#score-function-\<gridsearch\_scoring\>).

Two generic approaches to parameter search are provided in scikit-learn: for given values, <span class="title-ref">GridSearchCV</span> exhaustively considers all parameter combinations, while <span class="title-ref">RandomizedSearchCV</span> can sample a given number of candidates from a parameter space with a specified distribution. Both these tools have successive halving counterparts <span class="title-ref">HalvingGridSearchCV</span> and <span class="title-ref">HalvingRandomSearchCV</span>, which can be much faster at finding a good parameter combination.

After describing these tools we detail \[best practices \<grid\_search\_tips\>\](\#best-practices \<grid\_search\_tips\>) applicable to these approaches. Some models allow for specialized, efficient parameter search strategies, outlined in \[alternative\_cv\](\#alternative\_cv).

Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.

## Exhaustive Grid Search

The grid search provided by <span class="title-ref">GridSearchCV</span> exhaustively generates candidates from a grid of parameter values specified with the `param_grid` parameter. For instance, the following `param_grid`:

    param_grid = [
      {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
      {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
     ]

specifies that two grids should be explored: one with a linear kernel and C values in \[1, 10, 100, 1000\], and the second one with an RBF kernel, and the cross-product of C values ranging in \[1, 10, 100, 1000\] and gamma values in \[0.001, 0.0001\].

The <span class="title-ref">GridSearchCV</span> instance implements the usual estimator API: when "fitting" it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.

<div class="currentmodule">

sklearn.model\_selection

</div>

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_nested\_cross\_validation\_iris.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_nested\_cross\_validation\_iris.py) for an example of Grid Search within a cross validation loop on the iris dataset. This is the best practice for evaluating the performance of a model with grid search.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_text\_feature\_extraction.py) for an example of Grid Search coupling parameters from a text documents feature extractor (n-gram count vectorizer and TF-IDF transformer) with a classifier (here a linear SVM trained with SGD with either elastic net or L2 penalty) using a <span class="title-ref">\~sklearn.pipeline.Pipeline</span> instance.

<div class="dropdown">

Advanced examples

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_nested\_cross\_validation\_iris.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_nested\_cross\_validation\_iris.py) for an example of Grid Search within a cross validation loop on the iris dataset. This is the best practice for evaluating the performance of a model with grid search.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_multi\_metric\_evaluation.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_multi\_metric\_evaluation.py) for an example of <span class="title-ref">GridSearchCV</span> being used to evaluate multiple metrics simultaneously.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_refit\_callable.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_refit\_callable.py) for an example of using `refit=callable` interface in <span class="title-ref">GridSearchCV</span>. The example shows how this interface adds certain amount of flexibility in identifying the "best" estimator. This interface can also be used in multiple metrics evaluation.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_stats.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_stats.py) for an example of how to do a statistical comparison on the outputs of <span class="title-ref">GridSearchCV</span>.

</div>

## Randomized Parameter Optimization

While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favorable properties. <span class="title-ref">RandomizedSearchCV</span> implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:

  - A budget can be chosen independent of the number of parameters and possible values.
  - Adding parameters that do not influence the performance does not decrease efficiency.

Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for <span class="title-ref">GridSearchCV</span>. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:

    {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),
      'kernel': ['rbf'], 'class_weight':['balanced', None]}

This example uses the `scipy.stats` module, which contains many useful distributions for sampling parameters, such as `expon`, `gamma`, `uniform`, `loguniform` or `randint`.

In principle, any function can be passed that provides a `rvs` (random variate sample) method to sample a value. A call to the `rvs` function should provide independent random samples from possible parameter values on consecutive calls.

\> **Warning** \> The distributions in `scipy.stats` prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via `np.random.seed` or set using `np.random.set_state`. However, beginning scikit-learn 0.18, the `sklearn.model_selection` module sets the random state provided by the user if scipy \>= 0.16 is also available.

For continuous parameters, such as `C` above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing `n_iter` will always lead to a finer search.

A continuous log-uniform random variable is the continuous version of a log-spaced parameter. For example to specify the equivalent of `C` from above, `loguniform(1, 100)` can be used instead of `[1, 10, 100]`.

Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between `1e0` and `1e3`:

    from sklearn.utils.fixes import loguniform
    {'C': loguniform(1e0, 1e3),
     'gamma': loguniform(1e-4, 1e-3),
     'kernel': ['rbf'],
     'class_weight':['balanced', None]}

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_randomized\_search.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_randomized\_search.py) compares the usage and efficiency of randomized search and grid search.

**References**

  - Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)

## Searching for optimal parameters with successive halving

Scikit-learn also provides the <span class="title-ref">HalvingGridSearchCV</span> and <span class="title-ref">HalvingRandomSearchCV</span> estimators that can be used to search a parameter space using successive halving. Successive halving (SH) is like a tournament among candidate parameter combinations. SH is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small amount of resources at the first iteration. Only some of these candidates are selected for the next iteration, which will be allocated more resources. For parameter tuning, the resource is typically the number of training samples, but it can also be an arbitrary numeric parameter such as <span class="title-ref">n\_estimators</span> in a random forest.

<div class="note">

<div class="title">

Note

</div>

The resource increase chosen should be large enough so that a large improvement in scores is obtained when taking into account statistical significance.

</div>

As illustrated in the figure below, only a subset of candidates 'survive' until the last iteration. These are the candidates that have consistently ranked among the top-scoring candidates across all iterations. Each iteration is allocated an increasing amount of resources per candidate, here the number of samples.

![](../auto_examples/model_selection/images/sphx_glr_plot_successive_halving_iterations_001.png)

We here briefly describe the main parameters, but each parameter and their interactions are described more in detail in the dropdown section below. The `factor` (\> 1) parameter controls the rate at which the resources grow, and the rate at which the number of candidates decreases. In each iteration, the number of resources per candidate is multiplied by `factor` and the number of candidates is divided by the same factor. Along with `resource` and `min_resources`, `factor` is the most important parameter to control the search in our implementation, though a value of 3 usually works well. `factor` effectively controls the number of iterations in <span class="title-ref">HalvingGridSearchCV</span> and the number of candidates (by default) and iterations in <span class="title-ref">HalvingRandomSearchCV</span>. `aggressive_elimination=True` can also be used if the number of available resources is small. More control is available through tuning the `min_resources` parameter.

These estimators are still **experimental**: their predictions and their API might change without any deprecation cycle. To use them, you need to explicitly import `enable_halving_search_cv`:

    >>> from sklearn.experimental import enable_halving_search_cv  # noqa
    >>> from sklearn.model_selection import HalvingGridSearchCV
    >>> from sklearn.model_selection import HalvingRandomSearchCV

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_successive\_halving\_heatmap.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_successive\_halving\_heatmap.py)
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_successive\_halving\_iterations.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_successive\_halving\_iterations.py)

The sections below dive into technical aspects of successive halving.

<div class="dropdown">

Choosing `min_resources` and the number of candidates

Beside `factor`, the two main parameters that influence the behaviour of a successive halving search are the `min_resources` parameter, and the number of candidates (or parameter combinations) that are evaluated. `min_resources` is the amount of resources allocated at the first iteration for each candidate. The number of candidates is specified directly in <span class="title-ref">HalvingRandomSearchCV</span>, and is determined from the `param_grid` parameter of <span class="title-ref">HalvingGridSearchCV</span>.

Consider a case where the resource is the number of samples, and where we have 1000 samples. In theory, with `min_resources=10` and `factor=2`, we are able to run **at most** 7 iterations with the following number of samples: `[10, 20, 40, 80, 160, 320, 640]`.

But depending on the number of candidates, we might run less than 7 iterations: if we start with a **small** number of candidates, the last iteration might use less than 640 samples, which means not using all the available resources (samples). For example if we start with 5 candidates, we only need 2 iterations: 5 candidates for the first iteration, then <span class="title-ref">5 // 2 = 2</span> candidates at the second iteration, after which we know which candidate performs the best (so we don't need a third one). We would only be using at most 20 samples which is a waste since we have 1000 samples at our disposal. On the other hand, if we start with a **high** number of candidates, we might end up with a lot of candidates at the last iteration, which may not always be ideal: it means that many candidates will run with the full resources, basically reducing the procedure to standard search.

In the case of <span class="title-ref">HalvingRandomSearchCV</span>, the number of candidates is set by default such that the last iteration uses as much of the available resources as possible. For <span class="title-ref">HalvingGridSearchCV</span>, the number of candidates is determined by the <span class="title-ref">param\_grid</span> parameter. Changing the value of `min_resources` will impact the number of possible iterations, and as a result will also have an effect on the ideal number of candidates.

Another consideration when choosing `min_resources` is whether or not it is easy to discriminate between good and bad candidates with a small amount of resources. For example, if you need a lot of samples to distinguish between good and bad parameters, a high `min_resources` is recommended. On the other hand if the distinction is clear even with a small amount of samples, then a small `min_resources` may be preferable since it would speed up the computation.

Notice in the example above that the last iteration does not use the maximum amount of resources available: 1000 samples are available, yet only 640 are used, at most. By default, both <span class="title-ref">HalvingRandomSearchCV</span> and <span class="title-ref">HalvingGridSearchCV</span> try to use as many resources as possible in the last iteration, with the constraint that this amount of resources must be a multiple of both <span class="title-ref">min\_resources</span> and <span class="title-ref">factor</span> (this constraint will be clear in the next section). <span class="title-ref">HalvingRandomSearchCV</span> achieves this by sampling the right amount of candidates, while <span class="title-ref">HalvingGridSearchCV</span> achieves this by properly setting <span class="title-ref">min\_resources</span>.

</div>

<div class="dropdown">

Amount of resource and number of candidates at each iteration

At any iteration <span class="title-ref">i</span>, each candidate is allocated a given amount of resources which we denote <span class="title-ref">n\_resources\_i</span>. This quantity is controlled by the parameters `factor` and `min_resources` as follows (<span class="title-ref">factor</span> is strictly greater than 1):

    n_resources_i = factor**i * min_resources,

or equivalently:

    n_resources_{i+1} = n_resources_i * factor

where `min_resources == n_resources_0` is the amount of resources used at the first iteration. `factor` also defines the proportions of candidates that will be selected for the next iteration:

    n_candidates_i = n_candidates // (factor ** i)

or equivalently:

    n_candidates_0 = n_candidates
    n_candidates_{i+1} = n_candidates_i // factor

So in the first iteration, we use `min_resources` resources `n_candidates` times. In the second iteration, we use `min_resources * factor` resources `n_candidates // factor` times. The third again multiplies the resources per candidate and divides the number of candidates. This process stops when the maximum amount of resource per candidate is reached, or when we have identified the best candidate. The best candidate is identified at the iteration that is evaluating <span class="title-ref">factor</span> or less candidates (see just below for an explanation).

Here is an example with `min_resources=3` and `factor=2`, starting with 70 candidates:

| `n_resources_i`     | `n_candidates_i`    |
| ------------------- | ------------------- |
| 3 (=min\_resources) | 70 (=n\_candidates) |
| 3 \* 2 = 6          | 70 // 2 = 35        |
| 6 \* 2 = 12         | 35 // 2 = 17        |
| 12 \* 2 = 24        | 17 // 2 = 8         |
| 24 \* 2 = 48        | 8 // 2 = 4          |
| 48 \* 2 = 96        | 4 // 2 = 2          |

We can note that:

  - the process stops at the first iteration which evaluates <span class="title-ref">factor=2</span> candidates: the best candidate is the best out of these 2 candidates. It is not necessary to run an additional iteration, since it would only evaluate one candidate (namely the best one, which we have already identified). For this reason, in general, we want the last iteration to run at most `factor` candidates. If the last iteration evaluates more than <span class="title-ref">factor</span> candidates, then this last iteration reduces to a regular search (as in <span class="title-ref">RandomizedSearchCV</span> or <span class="title-ref">GridSearchCV</span>).
  - each `n_resources_i` is a multiple of both `factor` and `min_resources` (which is confirmed by its definition above).

The amount of resources that is used at each iteration can be found in the <span class="title-ref">n\_resources\_</span> attribute.

</div>

<div class="dropdown">

Choosing a resource

By default, the resource is defined in terms of number of samples. That is, each iteration will use an increasing amount of samples to train on. You can however manually specify a parameter to use as the resource with the `resource` parameter. Here is an example where the resource is defined in terms of the number of estimators of a random forest:

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.experimental import enable_halving_search_cv  # noqa
    >>> from sklearn.model_selection import HalvingGridSearchCV
    >>> import pandas as pd
    >>> param_grid = {'max_depth': [3, 5, 10],
    ...               'min_samples_split': [2, 5, 10]}
    >>> base_estimator = RandomForestClassifier(random_state=0)
    >>> X, y = make_classification(n_samples=1000, random_state=0)
    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
    ...                          factor=2, resource='n_estimators',
    ...                          max_resources=30).fit(X, y)
    >>> sh.best_estimator_
    RandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)

Note that it is not possible to budget on a parameter that is part of the parameter grid.

</div>

<div class="dropdown">

Exhausting the available resources

As mentioned above, the number of resources that is used at each iteration depends on the <span class="title-ref">min\_resources</span> parameter. If you have a lot of resources available but start with a low number of resources, some of them might be wasted (i.e. not used):

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.svm import SVC
    >>> from sklearn.experimental import enable_halving_search_cv  # noqa
    >>> from sklearn.model_selection import HalvingGridSearchCV
    >>> import pandas as pd
    >>> param_grid= {'kernel': ('linear', 'rbf'),
    ...              'C': [1, 10, 100]}
    >>> base_estimator = SVC(gamma='scale')
    >>> X, y = make_classification(n_samples=1000)
    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
    ...                          factor=2, min_resources=20).fit(X, y)
    >>> sh.n_resources_
    [20, 40, 80]

The search process will only use 80 resources at most, while our maximum amount of available resources is `n_samples=1000`. Here, we have `min_resources = r_0 = 20`.

For <span class="title-ref">HalvingGridSearchCV</span>, by default, the <span class="title-ref">min\_resources</span> parameter is set to 'exhaust'. This means that <span class="title-ref">min\_resources</span> is automatically set such that the last iteration can use as many resources as possible, within the <span class="title-ref">max\_resources</span> limit:

    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
    ...                          factor=2, min_resources='exhaust').fit(X, y)
    >>> sh.n_resources_
    [250, 500, 1000]

<span class="title-ref">min\_resources</span> was here automatically set to 250, which results in the last iteration using all the resources. The exact value that is used depends on the number of candidate parameter, on <span class="title-ref">max\_resources</span> and on <span class="title-ref">factor</span>.

For <span class="title-ref">HalvingRandomSearchCV</span>, exhausting the resources can be done in 2 ways:

  - by setting <span class="title-ref">min\_resources='exhaust'</span>, just like for <span class="title-ref">HalvingGridSearchCV</span>;
  - by setting <span class="title-ref">n\_candidates='exhaust'</span>.

Both options are mutually exclusive: using <span class="title-ref">min\_resources='exhaust'</span> requires knowing the number of candidates, and symmetrically <span class="title-ref">n\_candidates='exhaust'</span> requires knowing <span class="title-ref">min\_resources</span>.

In general, exhausting the total number of resources leads to a better final candidate parameter, and is slightly more time-intensive.

</div>

### Aggressive elimination of candidates

Using the `aggressive_elimination` parameter, you can force the search process to end up with less than `factor` candidates at the last iteration.

<div class="dropdown">

Code example of aggressive elimination

Ideally, we want the last iteration to evaluate `factor` candidates. We then just have to pick the best one. When the number of available resources is small with respect to the number of candidates, the last iteration may have to evaluate more than `factor` candidates:

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.svm import SVC
    >>> from sklearn.experimental import enable_halving_search_cv  # noqa
    >>> from sklearn.model_selection import HalvingGridSearchCV
    >>> import pandas as pd
    >>> param_grid = {'kernel': ('linear', 'rbf'),
    ...               'C': [1, 10, 100]}
    >>> base_estimator = SVC(gamma='scale')
    >>> X, y = make_classification(n_samples=1000)
    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
    ...                          factor=2, max_resources=40,
    ...                          aggressive_elimination=False).fit(X, y)
    >>> sh.n_resources_
    [20, 40]
    >>> sh.n_candidates_
    [6, 3]

Since we cannot use more than `max_resources=40` resources, the process has to stop at the second iteration which evaluates more than `factor=2` candidates.

When using `aggressive_elimination`, the process will eliminate as many candidates as necessary using `min_resources` resources:

    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
    ...                            factor=2,
    ...                            max_resources=40,
    ...                            aggressive_elimination=True,
    ...                            ).fit(X, y)
    >>> sh.n_resources_
    [20, 20, 40]
    >>> sh.n_candidates_
    [6, 3, 2]

Notice that we end with 2 candidates at the last iteration since we have eliminated enough candidates during the first iterations, using `n_resources = min_resources = 20`.

</div>

### Analyzing results with the <span class="title-ref">cv\_results\_</span> attribute

The `cv_results_` attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with `df = pd.DataFrame(est.cv_results_)`. The `cv_results_` attribute of <span class="title-ref">HalvingGridSearchCV</span> and <span class="title-ref">HalvingRandomSearchCV</span> is similar to that of <span class="title-ref">GridSearchCV</span> and <span class="title-ref">RandomizedSearchCV</span>, with additional information related to the successive halving process.

<div class="dropdown">

Example of a (truncated) output dataframe:

<table>
<thead>
<tr class="header">
<th>..</th>
<th>iter</th>
<th>n_resources</th>
<th>mean_test_score</th>
<th>params</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>0</p>
</blockquote></td>
<td><blockquote>
<p>0</p>
</blockquote></td>
<td><blockquote>
<p>125</p>
</blockquote></td>
<td><blockquote>
<p>0.983667</p>
</blockquote></td>
<td>{'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 5}</td>
</tr>
<tr class="even">
<td><blockquote>
<p>1</p>
</blockquote></td>
<td><blockquote>
<p>0</p>
</blockquote></td>
<td><blockquote>
<p>125</p>
</blockquote></td>
<td><blockquote>
<p>0.983667</p>
</blockquote></td>
<td>{'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_split': 7}</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>2</p>
</blockquote></td>
<td><blockquote>
<p>0</p>
</blockquote></td>
<td><blockquote>
<p>125</p>
</blockquote></td>
<td><blockquote>
<p>0.983667</p>
</blockquote></td>
<td>{'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}</td>
</tr>
<tr class="even">
<td><blockquote>
<p>3</p>
</blockquote></td>
<td><blockquote>
<p>0</p>
</blockquote></td>
<td><blockquote>
<p>125</p>
</blockquote></td>
<td><blockquote>
<p>0.983667</p>
</blockquote></td>
<td>{'criterion': 'log_loss', 'max_depth': None, 'max_features': 6, 'min_samples_split': 6}</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>...</p>
</blockquote></td>
<td><blockquote>
<p>...</p>
</blockquote></td>
<td><blockquote>
<p>...</p>
</blockquote></td>
<td><blockquote>
<p>...</p>
</blockquote></td>
<td>...</td>
</tr>
<tr class="even">
<td><blockquote>
<p>15</p>
</blockquote></td>
<td><blockquote>
<p>2</p>
</blockquote></td>
<td><blockquote>
<p>500</p>
</blockquote></td>
<td><blockquote>
<p>0.951958</p>
</blockquote></td>
<td>{'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>16</p>
</blockquote></td>
<td><blockquote>
<p>2</p>
</blockquote></td>
<td><blockquote>
<p>500</p>
</blockquote></td>
<td><blockquote>
<p>0.947958</p>
</blockquote></td>
<td>{'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}</td>
</tr>
<tr class="even">
<td><blockquote>
<p>17</p>
</blockquote></td>
<td><blockquote>
<p>2</p>
</blockquote></td>
<td><blockquote>
<p>500</p>
</blockquote></td>
<td><blockquote>
<p>0.951958</p>
</blockquote></td>
<td>{'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>18</p>
</blockquote></td>
<td><blockquote>
<p>3</p>
</blockquote></td>
<td><blockquote>
<p>1000</p>
</blockquote></td>
<td><blockquote>
<p>0.961009</p>
</blockquote></td>
<td>{'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}</td>
</tr>
<tr class="even">
<td><blockquote>
<p>19</p>
</blockquote></td>
<td><blockquote>
<p>3</p>
</blockquote></td>
<td><blockquote>
<p>1000</p>
</blockquote></td>
<td><blockquote>
<p>0.955989</p>
</blockquote></td>
<td>{'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}</td>
</tr>
</tbody>
</table>

Each row corresponds to a given parameter combination (a candidate) and a given iteration. The iteration is given by the `iter` column. The `n_resources` column tells you how many resources were used.

In the example above, the best parameter combination is `{'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}` since it has reached the last iteration (3) with the highest score: 0.96.

**References**

</div>

## Tips for parameter search

### Specifying an objective metric

By default, parameter search uses the `score` function of the estimator to evaluate a parameter setting. These are the <span class="title-ref">sklearn.metrics.accuracy\_score</span> for classification and <span class="title-ref">sklearn.metrics.r2\_score</span> for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the `scoring` parameter of most parameter search tools. See \[scoring\_parameter\](\#scoring\_parameter) for more details.

### Specifying multiple metrics for evaluation

<span class="title-ref">GridSearchCV</span> and <span class="title-ref">RandomizedSearchCV</span> allow specifying multiple metrics for the `scoring` parameter.

Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See \[multimetric\_scoring\](\#multimetric\_scoring) for more details.

When specifying multiple metrics, the `refit` parameter must be set to the metric (string) for which the `best_params_` will be found and used to build the `best_estimator_` on the whole dataset. If the search should not be refit, set `refit=False`. Leaving refit to the default value `None` will result in an error when using multiple metrics.

See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_multi\_metric\_evaluation.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_multi\_metric\_evaluation.py) for an example usage.

<span class="title-ref">HalvingRandomSearchCV</span> and <span class="title-ref">HalvingGridSearchCV</span> do not support multimetric scoring.

### Composite estimators and parameter spaces

<span class="title-ref">GridSearchCV</span> and <span class="title-ref">RandomizedSearchCV</span> allow searching over parameters of composite or nested estimators such as <span class="title-ref">\~sklearn.pipeline.Pipeline</span>, <span class="title-ref">\~sklearn.compose.ColumnTransformer</span>, <span class="title-ref">\~sklearn.ensemble.VotingClassifier</span> or <span class="title-ref">\~sklearn.calibration.CalibratedClassifierCV</span> using a dedicated `<estimator>__<parameter>` syntax:

    >>> from sklearn.model_selection import GridSearchCV
    >>> from sklearn.calibration import CalibratedClassifierCV
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.datasets import make_moons
    >>> X, y = make_moons()
    >>> calibrated_forest = CalibratedClassifierCV(
    ...    estimator=RandomForestClassifier(n_estimators=10))
    >>> param_grid = {
    ...    'estimator__max_depth': [2, 4, 6, 8]}
    >>> search = GridSearchCV(calibrated_forest, param_grid, cv=5)
    >>> search.fit(X, y)
    GridSearchCV(cv=5,
                 estimator=CalibratedClassifierCV(estimator=RandomForestClassifier(n_estimators=10)),
                 param_grid={'estimator__max_depth': [2, 4, 6, 8]})

Here, `<estimator>` is the parameter name of the nested estimator, in this case `estimator`. If the meta-estimator is constructed as a collection of estimators as in <span class="title-ref">pipeline.Pipeline</span>, then `<estimator>` refers to the name of the estimator, see \[pipeline\_nested\_parameters\](\#pipeline\_nested\_parameters). In practice, there can be several levels of nesting:

    >>> from sklearn.pipeline import Pipeline
    >>> from sklearn.feature_selection import SelectKBest
    >>> pipe = Pipeline([
    ...    ('select', SelectKBest()),
    ...    ('model', calibrated_forest)])
    >>> param_grid = {
    ...    'select__k': [1, 2],
    ...    'model__estimator__max_depth': [2, 4, 6, 8]}
    >>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)

Please refer to \[pipeline\](\#pipeline) for performing parameter searches over pipelines.

### Model selection: development and evaluation

Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to "train" the parameters of the grid.

When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a **development set** (to be fed to the <span class="title-ref">GridSearchCV</span> instance) and an **evaluation set** to compute performance metrics.

This can be done by using the <span class="title-ref">train\_test\_split</span> utility function.

### Parallelism

The parameter search tools evaluate each parameter combination on each data fold independently. Computations can be run in parallel by using the keyword `n_jobs=-1`. See function signature for more details, and also the Glossary entry for `n_jobs`.

### Robustness to failure

Some parameter settings may result in a failure to `fit` one or more folds of the data. By default, the score for those settings will be <span class="title-ref">np.nan</span>. This can be controlled by setting <span class="title-ref">error\_score="raise"</span> to raise an exception if one fit fails, or for example <span class="title-ref">error\_score=0</span> to set another value for the score of failing parameter combinations.

## Alternatives to brute force parameter search

### Model specific cross-validation

Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter.

The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the **regularization path** of the estimator.

Here is the list of such models:

<div class="currentmodule">

sklearn

</div>

<div class="autosummary">

linear\_model.ElasticNetCV linear\_model.LarsCV linear\_model.LassoCV linear\_model.LassoLarsCV linear\_model.LogisticRegressionCV linear\_model.MultiTaskElasticNetCV linear\_model.MultiTaskLassoCV linear\_model.OrthogonalMatchingPursuitCV linear\_model.RidgeCV linear\_model.RidgeClassifierCV

</div>

### Information Criterion

Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation).

Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:

<div class="autosummary">

linear\_model.LassoLarsIC

</div>

### Out of Bag Estimates

When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.

This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes "for free" as no additional data is needed and can be used for model selection.

This is currently implemented in the following classes:

<div class="autosummary">

ensemble.RandomForestClassifier ensemble.RandomForestRegressor ensemble.ExtraTreesClassifier ensemble.ExtraTreesRegressor ensemble.GradientBoostingClassifier ensemble.GradientBoostingRegressor

</div>

---

impute.md

---

# Imputation of missing values

<div class="currentmodule">

sklearn.impute

</div>

For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the glossary entry on `imputation`.

## Univariate vs. Multivariate Imputation

One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. <span class="title-ref">SimpleImputer</span>). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. <span class="title-ref">IterativeImputer</span>).

## Univariate feature imputation

The <span class="title-ref">SimpleImputer</span> class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.

The following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean value of the columns (axis 0) that contain the missing values:

    >>> import numpy as np
    >>> from sklearn.impute import SimpleImputer
    >>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')
    >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
    SimpleImputer()
    >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
    >>> print(imp.transform(X))
    [[4.          2.        ]
     [6.          3.666...]
     [7.          6.        ]]

The <span class="title-ref">SimpleImputer</span> class also supports sparse matrices:

    >>> import scipy.sparse as sp
    >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])
    >>> imp = SimpleImputer(missing_values=-1, strategy='mean')
    >>> imp.fit(X)
    SimpleImputer(missing_values=-1)
    >>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])
    >>> print(imp.transform(X_test).toarray())
    [[3. 2.]
     [6. 3.]
     [7. 6.]]

Note that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input.

The <span class="title-ref">SimpleImputer</span> class also supports categorical data represented as string values or pandas categoricals when using the `'most_frequent'` or `'constant'` strategy:

    >>> import pandas as pd
    >>> df = pd.DataFrame([["a", "x"],
    ...                    [np.nan, "y"],
    ...                    ["a", np.nan],
    ...                    ["b", "y"]], dtype="category")
    ...
    >>> imp = SimpleImputer(strategy="most_frequent")
    >>> print(imp.fit_transform(df))
    [['a' 'x']
     ['a' 'y']
     ['a' 'y']
     ['b' 'y']]

For another example on usage, see \[sphx\_glr\_auto\_examples\_impute\_plot\_missing\_values.py\](\#sphx\_glr\_auto\_examples\_impute\_plot\_missing\_values.py).

## Multivariate feature imputation

A more sophisticated approach is to use the <span class="title-ref">IterativeImputer</span> class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output `y` and the other feature columns are treated as inputs `X`. A regressor is fit on `(X, y)` for known `y`. Then, the regressor is used to predict the missing values of `y`. This is done for each feature in an iterative fashion, and then is repeated for `max_iter` imputation rounds. The results of the final imputation round are returned.

\> **Note** \> This estimator is still **experimental** for now: default parameters or details of behaviour might change without any deprecation cycle. Resolving the following issues would help stabilize \`IterativeImputer\`: convergence criteria (`14338`), default estimators (`13286`), and use of random state (`15611`). To use it, you need to explicitly import `enable_iterative_imputer`.

    >>> import numpy as np
    >>> from sklearn.experimental import enable_iterative_imputer
    >>> from sklearn.impute import IterativeImputer
    >>> imp = IterativeImputer(max_iter=10, random_state=0)
    >>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])
    IterativeImputer(random_state=0)
    >>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]
    >>> # the model learns that the second feature is double the first
    >>> print(np.round(imp.transform(X_test)))
    [[ 1.  2.]
     [ 6. 12.]
     [ 3.  6.]]

Both <span class="title-ref">SimpleImputer</span> and <span class="title-ref">IterativeImputer</span> can be used in a Pipeline as a way to build a composite estimator that supports imputation. See \[sphx\_glr\_auto\_examples\_impute\_plot\_missing\_values.py\](\#sphx\_glr\_auto\_examples\_impute\_plot\_missing\_values.py).

### Flexibility of IterativeImputer

There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with <span class="title-ref">IterativeImputer</span> by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See \[sphx\_glr\_auto\_examples\_impute\_plot\_iterative\_imputer\_variants\_comparison.py\](\#sphx\_glr\_auto\_examples\_impute\_plot\_iterative\_imputer\_variants\_comparison.py).

### Multiple vs. Single Imputation

In the statistics community, it is common practice to perform multiple imputations, generating, for example, `m` separate imputations for a single feature matrix. Each of these `m` imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The `m` final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation.

Our implementation of <span class="title-ref">IterativeImputer</span> was inspired by the R MICE package (Multivariate Imputation by Chained Equations)\[1\], but differs from it by returning a single imputation instead of multiple imputations. However, <span class="title-ref">IterativeImputer</span> can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when `sample_posterior=True`. See\[2\], chapter 4 for more discussion on multiple vs. single imputations.

It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.

Note that a call to the `transform` method of <span class="title-ref">IterativeImputer</span> is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to `transform`.

### References

## Nearest neighbors imputation

The <span class="title-ref">KNNImputer</span> class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, <span class="title-ref">\~sklearn.metrics.pairwise.nan\_euclidean\_distances</span>, is used to find the nearest neighbors. Each missing feature is imputed using values from `n_neighbors` nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than <span class="title-ref">n\_neighbors</span> and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during <span class="title-ref">transform</span>. For more information on the methodology, see ref. [\[OL2001\]](#OL2001).

The following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean feature value of the two nearest neighbors of samples with missing values:

    >>> import numpy as np
    >>> from sklearn.impute import KNNImputer
    >>> nan = np.nan
    >>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
    >>> imputer = KNNImputer(n_neighbors=2, weights="uniform")
    >>> imputer.fit_transform(X)
    array([[1. , 2. , 4. ],
           [3. , 4. , 3. ],
           [5.5, 6. , 5. ],
           [8. , 8. , 7. ]])

For another example on usage, see \[sphx\_glr\_auto\_examples\_impute\_plot\_missing\_values.py\](\#sphx\_glr\_auto\_examples\_impute\_plot\_missing\_values.py).

**References**

## Keeping the number of features constant

By default, the scikit-learn imputers will drop fully empty features, i.e. columns containing only missing values. For instance:

    >>> imputer = SimpleImputer()
    >>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])
    >>> imputer.fit_transform(X)
    array([[1.],
           [2.],
           [3.]])

The first feature in <span class="title-ref">X</span> containing only <span class="title-ref">np.nan</span> was dropped after the imputation. While this feature will not help in predictive setting, dropping the columns will change the shape of <span class="title-ref">X</span> which could be problematic when using imputers in a more complex machine-learning pipeline. The parameter <span class="title-ref">keep\_empty\_features</span> offers the option to keep the empty features by imputing with a constant values. In most of the cases, this constant value is zero:

    >>> imputer.set_params(keep_empty_features=True)
    SimpleImputer(keep_empty_features=True)
    >>> imputer.fit_transform(X)
    array([[0., 1.],
           [0., 2.],
           [0., 3.]])

## Marking imputed values

The <span class="title-ref">MissingIndicator</span> transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. Note that both the <span class="title-ref">SimpleImputer</span> and <span class="title-ref">IterativeImputer</span> have the boolean parameter `add_indicator` (`False` by default) which when set to `True` provides a convenient way of stacking the output of the <span class="title-ref">MissingIndicator</span> transformer with the output of the imputer.

`NaN` is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter `missing_values` allows to specify other placeholder such as integer. In the following example, we will use `-1` as missing values:

    >>> from sklearn.impute import MissingIndicator
    >>> X = np.array([[-1, -1, 1, 3],
    ...               [4, -1, 0, -1],
    ...               [8, -1, 1, 0]])
    >>> indicator = MissingIndicator(missing_values=-1)
    >>> mask_missing_values_only = indicator.fit_transform(X)
    >>> mask_missing_values_only
    array([[ True,  True, False],
           [False,  True,  True],
           [False,  True, False]])

The `features` parameter is used to choose the features for which the mask is constructed. By default, it is `'missing-only'` which returns the imputer mask of the features containing missing values at `fit` time:

    >>> indicator.features_
    array([0, 1, 3])

The `features` parameter can be set to `'all'` to return all features whether or not they contain missing values:

    >>> indicator = MissingIndicator(missing_values=-1, features="all")
    >>> mask_all = indicator.fit_transform(X)
    >>> mask_all
    array([[ True,  True, False, False],
           [False,  True, False,  True],
           [False,  True, False, False]])
    >>> indicator.features_
    array([0, 1, 2, 3])

When using the <span class="title-ref">MissingIndicator</span> in a <span class="title-ref">\~sklearn.pipeline.Pipeline</span>, be sure to use the <span class="title-ref">\~sklearn.pipeline.FeatureUnion</span> or <span class="title-ref">\~sklearn.compose.ColumnTransformer</span> to add the indicator features to the regular features. First we obtain the <span class="title-ref">iris</span> dataset, and add some missing values to it.

> \>\>\> from sklearn.datasets import load\_iris \>\>\> from sklearn.impute import SimpleImputer, MissingIndicator \>\>\> from sklearn.model\_selection import train\_test\_split \>\>\> from sklearn.pipeline import FeatureUnion, make\_pipeline \>\>\> from sklearn.tree import DecisionTreeClassifier \>\>\> X, y = load\_iris(return\_X\_y=True) \>\>\> mask = np.random.randint(0, 2, size=X.shape).astype(bool) \>\>\> X\[mask\] = np.nan \>\>\> X\_train, X\_test, y\_train, \_ = train\_test\_split(X, y, test\_size=100, ... random\_state=0)

Now we create a <span class="title-ref">\~sklearn.pipeline.FeatureUnion</span>. All features will be imputed using <span class="title-ref">SimpleImputer</span>, in order to enable classifiers to work with this data. Additionally, it adds the indicator variables from <span class="title-ref">MissingIndicator</span>.

> \>\>\> transformer = FeatureUnion( ... transformer\_list=\[ ... ('features', SimpleImputer(strategy='mean')), ... ('indicators', MissingIndicator())\]) \>\>\> transformer = transformer.fit(X\_train, y\_train) \>\>\> results = transformer.transform(X\_test) \>\>\> results.shape (100, 8)

Of course, we cannot use the transformer to make any predictions. We should wrap this in a <span class="title-ref">\~sklearn.pipeline.Pipeline</span> with a classifier (e.g., a <span class="title-ref">\~sklearn.tree.DecisionTreeClassifier</span>) to be able to make predictions.

> \>\>\> clf = make\_pipeline(transformer, DecisionTreeClassifier()) \>\>\> clf = clf.fit(X\_train, y\_train) \>\>\> results = clf.predict(X\_test) \>\>\> results.shape (100,)

## Estimators that handle NaN values

Some estimators are designed to handle NaN values without preprocessing. Below is the list of these estimators, classified by type (cluster, regressor, classifier, transform):

<div id="citations">

  - <span id="OL2001" class="citation-label">OL2001</span>  
    [Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525.](https://academic.oup.com/bioinformatics/article/17/6/520/272365)

</div>

1.  [Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice: Multivariate Imputation by Chained Equations in R". Journal of Statistical Software 45: 1-67.](https://www.jstatsoft.org/article/view/v045i03)

2.  Roderick J A Little and Donald B Rubin (1986). "Statistical Analysis with Missing Data". John Wiley & Sons, Inc., New York, NY, USA.

---

isotonic.md

---

# Isotonic regression

<div class="currentmodule">

sklearn.isotonic

</div>

The class <span class="title-ref">IsotonicRegression</span> fits a non-decreasing real function to 1-dimensional data. It solves the following problem:

\[\min \sum_i w_i (y_i - \hat{y}_i)^2\]

subject to \(\hat{y}_i \le \hat{y}_j\) whenever \(X_i \le X_j\), where the weights \(w_i\) are strictly positive, and both <span class="title-ref">X</span> and <span class="title-ref">y</span> are arbitrary real quantities.

The <span class="title-ref">increasing</span> parameter changes the constraint to \(\hat{y}_i \ge \hat{y}_j\) whenever \(X_i \le X_j\). Setting it to 'auto' will automatically choose the constraint based on [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).

<span class="title-ref">IsotonicRegression</span> produces a series of predictions \(\hat{y}_i\) for the training data which are the closest to the targets \(y\) in terms of mean squared error. These predictions are interpolated for predicting to unseen data. The predictions of <span class="title-ref">IsotonicRegression</span> thus form a function that is piecewise linear:

![](../auto_examples/miscellaneous/images/sphx_glr_plot_isotonic_regression_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_isotonic\_regression.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_isotonic\_regression.py)

---

kernel_approximation.md

---

# Kernel Approximation

This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see \[svm\](\#svm)). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms.

<div class="currentmodule">

sklearn.linear\_model

</div>

The advantage of using approximate explicit feature maps compared to the [kernel trick](https://en.wikipedia.org/wiki/Kernel_trick), which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with <span class="title-ref">SGDClassifier</span> can make non-linear learning on large datasets possible.

Since there has not been much empirical work using approximate embeddings, it is advisable to compare results against exact kernel methods when possible.

<div class="seealso">

\[polynomial\_regression\](\#polynomial\_regression) for an exact polynomial transformation.

</div>

<div class="currentmodule">

sklearn.kernel\_approximation

</div>

## Nystroem Method for Kernel Approximation

The Nystroem method, as implemented in <span class="title-ref">Nystroem</span> is a general method for reduced rank approximations of kernels. It achieves this by subsampling without replacement rows/columns of the data on which the kernel is evaluated. While the computational complexity of the exact method is \(\mathcal{O}(n^3_{\text{samples}})\), the complexity of the approximation is \(\mathcal{O}(n^2_{\text{components}} \cdot n_{\text{samples}})\), where one can set \(n_{\text{components}} \ll n_{\text{samples}}\) without a significative decrease in performance [\[WS2001\]](#WS2001).

We can construct the eigendecomposition of the kernel matrix \(K\), based on the features of the data, and then split it into sampled and unsampled data points.

\[\begin{aligned}
K = U \Lambda U^T
= \begin{bmatrix} U_1 \\ U_2\end{bmatrix} \Lambda \begin{bmatrix} U_1 \\ U_2 \end{bmatrix}^T
= \begin{bmatrix} U_1 \Lambda U_1^T & U_1 \Lambda U_2^T \\ U_2 \Lambda U_1^T & U_2 \Lambda U_2^T \end{bmatrix}
\equiv \begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22} \end{bmatrix}
\end{aligned}\]

where:

  - \(U\) is orthonormal
  - \(\Lambda\) is diagonal matrix of eigenvalues
  - \(U_1\) is orthonormal matrix of samples that were chosen
  - \(U_2\) is orthonormal matrix of samples that were not chosen

Given that \(U_1 \Lambda U_1^T\) can be obtained by orthonormalization of the matrix \(K_{11}\), and \(U_2 \Lambda U_1^T\) can be evaluated (as well as its transpose), the only remaining term to elucidate is \(U_2 \Lambda U_2^T\). To do this we can express it in terms of the already evaluated matrices:

\[\begin{aligned}
\begin{align} U_2 \Lambda U_2^T &= \left(K_{21} U_1 \Lambda^{-1}\right) \Lambda \left(K_{21} U_1 \Lambda^{-1}\right)^T
\\&= K_{21} U_1 (\Lambda^{-1} \Lambda) \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} U_1 \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} K_{11}^{-1} K_{21}^T
\\&= \left( K_{21} K_{11}^{-\frac12} \right) \left( K_{21} K_{11}^{-\frac12} \right)^T
.\end{align}
\end{aligned}\]

During `fit`, the class <span class="title-ref">Nystroem</span> evaluates the basis \(U_1\), and computes the normalization constant, \(K_{11}^{-\frac12}\). Later, during `transform`, the kernel matrix is determined between the basis (given by the <span class="title-ref">components\_</span> attribute) and the new data points, `X`. This matrix is then multiplied by the `normalization_` matrix for the final result.

By default <span class="title-ref">Nystroem</span> uses the `rbf` kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter `n_components`.

**Examples**

  - See the example entitled \[sphx\_glr\_auto\_examples\_applications\_plot\_cyclical\_feature\_engineering.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_cyclical\_feature\_engineering.py), that shows an efficient machine learning pipeline that uses a <span class="title-ref">Nystroem</span> kernel.

## Radial Basis Function Kernel

The <span class="title-ref">RBFSampler</span> constructs an approximate mapping for the radial basis function kernel, also known as *Random Kitchen Sinks* [\[RR2007\]](#RR2007). This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:

    >>> from sklearn.kernel_approximation import RBFSampler
    >>> from sklearn.linear_model import SGDClassifier
    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
    >>> y = [0, 0, 1, 1]
    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)
    >>> X_features = rbf_feature.fit_transform(X)
    >>> clf = SGDClassifier(max_iter=5)
    >>> clf.fit(X_features, y)
    SGDClassifier(max_iter=5)
    >>> clf.score(X_features, y)
    1.0

The mapping relies on a Monte Carlo approximation to the kernel values. The `fit` function performs the Monte Carlo sampling, whereas the `transform` method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the `fit` function.

The `fit` function takes two arguments: `n_components`, which is the target dimensionality of the feature transform, and `gamma`, the parameter of the RBF-kernel. A higher `n_components` will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that "fitting" the feature function does not actually depend on the data given to the `fit` function. Only the dimensionality of the data is used. Details on the method can be found in [\[RR2007\]](#RR2007).

For a given value of `n_components` <span class="title-ref">RBFSampler</span> is often less accurate as <span class="title-ref">Nystroem</span>. <span class="title-ref">RBFSampler</span> is cheaper to compute, though, making use of larger feature spaces more efficient.

![Comparing an exact RBF kernel (left) with the approximation (right)](../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_approximation_002.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_kernel\_approximation.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_kernel\_approximation.py)

## Additive Chi Squared Kernel

The additive chi squared kernel is a kernel on histograms, often used in computer vision.

The additive chi squared kernel as used here is given by

\[k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}\]

This is not exactly the same as <span class="title-ref">sklearn.metrics.pairwise.additive\_chi2\_kernel</span>. The authors of [\[VZ2010\]](#VZ2010) prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \(x_i\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.

The class <span class="title-ref">AdditiveChi2Sampler</span> implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size `n_samples * 5 * n_features` (in the case of \(n=2\)).

The approximate feature map provided by <span class="title-ref">AdditiveChi2Sampler</span> can be combined with the approximate feature map provided by <span class="title-ref">RBFSampler</span> to yield an approximate feature map for the exponentiated chi squared kernel. See the [\[VZ2010\]](#VZ2010) for details and [\[VVZ2010\]](#VVZ2010) for combination with the <span class="title-ref">RBFSampler</span>.

## Skewed Chi Squared Kernel

The skewed chi squared kernel is given by:

\[k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}\]

It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.

The usage of the <span class="title-ref">SkewedChi2Sampler</span> is the same as the usage described above for the <span class="title-ref">RBFSampler</span>. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see [\[LS2010\]](#LS2010).

## Polynomial Kernel Approximation via Tensor Sketch

The \[polynomial kernel \<polynomial\_kernel\>\](\#polynomial-kernel-\<polynomial\_kernel\>) is a popular type of kernel function given by:

\[k(x, y) = (\gamma x^\top y +c_0)^d\]

where:

  - `x`, `y` are the input vectors
  - `d` is the kernel degree

Intuitively, the feature space of the polynomial kernel of degree <span class="title-ref">d</span> consists of all possible degree-<span class="title-ref">d</span> products among input features, which enables learning algorithms using this kernel to account for interactions between features.

The TensorSketch [\[PP2013\]](#PP2013) method, as implemented in <span class="title-ref">PolynomialCountSketch</span>, is a scalable, input data independent method for polynomial kernel approximation. It is based on the concept of Count sketch [\[WIKICS\]](#WIKICS) [\[CCF2002\]](#CCF2002) , a dimensionality reduction technique similar to feature hashing, which instead uses several independent hash functions. TensorSketch obtains a Count Sketch of the outer product of two vectors (or a vector with itself), which can be used as an approximation of the polynomial kernel feature space. In particular, instead of explicitly computing the outer product, TensorSketch computes the Count Sketch of the vectors and then uses polynomial multiplication via the Fast Fourier Transform to compute the Count Sketch of their outer product.

Conveniently, the training phase of TensorSketch simply consists of initializing some random variables. It is thus independent of the input data, i.e. it only depends on the number of input features, but not the data values. In addition, this method can transform samples in \(\mathcal{O}(n_{\text{samples}}(n_{\text{features}} + n_{\text{components}} \log(n_{\text{components}})))\) time, where \(n_{\text{components}}\) is the desired output dimension, determined by `n_components`.

**Examples**

  - \[sphx\_glr\_auto\_examples\_kernel\_approximation\_plot\_scalable\_poly\_kernels.py\](\#sphx\_glr\_auto\_examples\_kernel\_approximation\_plot\_scalable\_poly\_kernels.py)

## Mathematical Details

Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that

\[k(x,y) = \langle \phi(x), \phi(y) \rangle\]

Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the Hilbert space.

If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \(x_i\), one may use the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm to the mapped data points \(\phi(x_i)\). The advantage of using \(k\) is that the mapping \(\phi\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite).

One drawback of kernel methods is, that it might be necessary to store many kernel values \(k(x_i, x_j)\) during optimization. If a kernelized classifier is applied to new data \(y_j\), \(k(x_i, y_j)\) needs to be computed to make predictions, possibly for many different \(x_i\) in the training set.

The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.

**References**

<div id="citations">

  - <span id="CCF2002" class="citation-label">CCF2002</span>  
    ["Finding frequent items in data streams"](https://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf) Charikar, M., Chen, K., & Farach-Colton - 2002

  - <span id="LS2010" class="citation-label">LS2010</span>  
    ["Random Fourier approximations for skewed multiplicative histogram kernels"](https://www.researchgate.net/publication/221114584_Random_Fourier_Approximations_for_Skewed_Multiplicative_Histogram_Kernels) Li, F., Ionescu, C., and Sminchisescu, C. - Pattern Recognition, DAGM 2010, Lecture Notes in Computer Science.

  - <span id="PP2013" class="citation-label">PP2013</span>  
    `"Fast and scalable polynomial kernels via explicit feature maps"
    <10.1145/2487575.2487591>` Pham, N., & Pagh, R. - 2013

  - <span id="RR2007" class="citation-label">RR2007</span>  
    ["Random features for large-scale kernel machines"](https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html) Rahimi, A. and Recht, B. - Advances in neural information processing 2007,

  - <span id="VVZ2010" class="citation-label">VVZ2010</span>  
    ["Generalized RBF feature maps for Efficient Detection"](https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf) Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010

  - <span id="VZ2010" class="citation-label">VZ2010</span>  
    ["Efficient additive kernels via explicit feature maps"](https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf) Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010

  - <span id="WIKICS" class="citation-label">WIKICS</span>  
    ["Wikipedia: Count sketch"](https://en.wikipedia.org/wiki/Count_sketch)

  - <span id="WS2001" class="citation-label">WS2001</span>  
    ["Using the Nyström method to speed up kernel machines"](https://papers.nips.cc/paper_files/paper/2000/hash/19de10adbaa1b2ee13f77f679fa1483a-Abstract.html) Williams, C.K.I.; Seeger, M. - 2001.

</div>

---

kernel_ridge.md

---

# Kernel ridge regression

<div class="currentmodule">

sklearn.kernel\_ridge

</div>

Kernel ridge regression (KRR) [\[M2012\]](#M2012) combines \[ridge\_regression\](\#ridge\_regression) (linear least squares with l2-norm regularization) with the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method). It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.

The form of the model learned by <span class="title-ref">KernelRidge</span> is identical to support vector regression (<span class="title-ref">\~sklearn.svm.SVR</span>). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to <span class="title-ref">\~sklearn.svm.SVR</span>, fitting <span class="title-ref">KernelRidge</span> can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than <span class="title-ref">\~sklearn.svm.SVR</span>, which learns a sparse model for \(\epsilon > 0\), at prediction-time.

The following figure compares <span class="title-ref">KernelRidge</span> and <span class="title-ref">\~sklearn.svm.SVR</span> on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of <span class="title-ref">KernelRidge</span> and <span class="title-ref">\~sklearn.svm.SVR</span> is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting <span class="title-ref">KernelRidge</span> is approximately seven times faster than fitting <span class="title-ref">\~sklearn.svm.SVR</span> (both with grid-search). However, prediction of 100000 target values is more than three times faster with <span class="title-ref">\~sklearn.svm.SVR</span> since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors.

![](../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_ridge_regression_001.png)

The next figure compares the time for fitting and prediction of <span class="title-ref">KernelRidge</span> and <span class="title-ref">\~sklearn.svm.SVR</span> for different sizes of the training set. Fitting <span class="title-ref">KernelRidge</span> is faster than <span class="title-ref">\~sklearn.svm.SVR</span> for medium-sized training sets (less than 1000 samples); however, for larger training sets <span class="title-ref">\~sklearn.svm.SVR</span> scales better. With regard to prediction time, <span class="title-ref">\~sklearn.svm.SVR</span> is faster than <span class="title-ref">KernelRidge</span> for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the <span class="title-ref">\~sklearn.svm.SVR</span>; \(\epsilon = 0\) would correspond to a dense model.

![](../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_ridge_regression_002.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_kernel\_ridge\_regression.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_kernel\_ridge\_regression.py)

**References**

<div id="citations">

  - <span id="M2012" class="citation-label">M2012</span>  
    "Machine Learning: A Probabilistic Perspective" Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012

</div>

---

lda_qda.md

---

# Linear and Quadratic Discriminant Analysis

<div class="currentmodule">

sklearn

</div>

Linear Discriminant Analysis (<span class="title-ref">\~discriminant\_analysis.LinearDiscriminantAnalysis</span>) and Quadratic Discriminant Analysis (<span class="title-ref">\~discriminant\_analysis.QuadraticDiscriminantAnalysis</span>) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.

These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.

<div class="centered">

[![ldaqda](../auto_examples/classification/images/sphx_glr_plot_lda_qda_001.png)](../auto_examples/classification/plot_lda_qda.html)

</div>

The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.

**Examples**

  - \[sphx\_glr\_auto\_examples\_classification\_plot\_lda\_qda.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_lda\_qda.py): Comparison of LDA and QDA on synthetic data.

## Dimensionality reduction using Linear Discriminant Analysis

<span class="title-ref">\~discriminant\_analysis.LinearDiscriminantAnalysis</span> can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is in general a rather strong dimensionality reduction, and only makes sense in a multiclass setting.

This is implemented in the <span class="title-ref">transform</span> method. The desired dimensionality can be set using the `n_components` parameter. This parameter has no influence on the <span class="title-ref">fit</span> and <span class="title-ref">predict</span> methods.

**Examples**

  - \[sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_lda.py\](\#sphx\_glr\_auto\_examples\_decomposition\_plot\_pca\_vs\_lda.py): Comparison of LDA and PCA for dimensionality reduction of the Iris dataset

## Mathematical formulation of the LDA and QDA classifiers

Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes' rule, for each training sample \(x \in \mathcal{R}^d\):

\[P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}\]

and we select the class \(k\) which maximizes this posterior probability.

More specifically, for linear and quadratic discriminant analysis, \(P(x|y)\) is modeled as a multivariate Gaussian distribution with density:

\[P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)\]

where \(d\) is the number of features.

### QDA

According to the model above, the log of the posterior is:

\[\begin{aligned}
\log P(y=k | x) &= \log P(x | y=k) + \log P(y = k) + Cst \\
&= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,
\end{aligned}\]

where the constant term \(Cst\) corresponds to the denominator \(P(x)\), in addition to other constant terms from the Gaussian. The predicted class is the one that maximises this log-posterior.

<div class="note">

<div class="title">

Note

</div>

**Relation with Gaussian Naive Bayes**

If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier <span class="title-ref">naive\_bayes.GaussianNB</span>.

</div>

### LDA

LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This reduces the log posterior to:

\[\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst.\]

The term \((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\) corresponds to the [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) between the sample \(x\) and the mean \(\mu_k\). The Mahalanobis distance tells how close \(x\) is from \(\mu_k\), while also accounting for the variance of each feature. We can thus interpret LDA as assigning \(x\) to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities.

The log-posterior of LDA can also be written\[1\] as:

\[\log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst.\]

where \(\omega_k = \Sigma^{-1} \mu_k\) and \(\omega_{k0} =
-\frac{1}{2} \mu_k^t\Sigma^{-1}\mu_k + \log P (y = k)\). These quantities correspond to the <span class="title-ref">coef\_</span> and <span class="title-ref">intercept\_</span> attributes, respectively.

From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See\[2\] for more details.

## Mathematical formulation of LDA dimensionality reduction

First note that the K means \(\mu_k\) are vectors in \(\mathcal{R}^d\), and they lie in an affine subspace \(H\) of dimension at most \(K - 1\) (2 points lie on a line, 3 points lie on a plane, etc.).

As mentioned above, we can interpret LDA as assigning \(x\) to the class whose mean \(\mu_k\) is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first *sphering* the data so that the covariance matrix is the identity, and then assigning \(x\) to the closest mean in terms of Euclidean distance (still accounting for the class priors).

Computing Euclidean distances in this d-dimensional space is equivalent to first projecting the data points into \(H\), and computing the distances there (since the other dimensions will contribute equally to each class in terms of distance). In other words, if \(x\) is closest to \(\mu_k\) in the original space, it will also be the case in \(H\). This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.

We can reduce the dimension even more, to a chosen \(L\), by projecting onto the linear subspace \(H_L\) which maximizes the variance of the \(\mu^*_k\) after projection (in effect, we are doing a form of PCA for the transformed class means \(\mu^*_k\)). This \(L\) corresponds to the `n_components` parameter used in the <span class="title-ref">\~discriminant\_analysis.LinearDiscriminantAnalysis.transform</span> method. See \[3\] for more details.

## Shrinkage and Covariance Estimator

Shrinkage is a form of regularization used to improve the estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator, and shrinkage helps improving the generalization performance of the classifier. Shrinkage LDA can be used by setting the `shrinkage` parameter of the <span class="title-ref">\~discriminant\_analysis.LinearDiscriminantAnalysis</span> class to 'auto'. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf\[4\]. Note that currently shrinkage only works when setting the `solver` parameter to 'lsqr' or 'eigen'.

The `shrinkage` parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.

The shrunk Ledoit and Wolf estimator of covariance may not always be the best choice. For example if the distribution of the data is normally distributed, the Oracle Approximating Shrinkage estimator <span class="title-ref">sklearn.covariance.OAS</span> yields a smaller Mean Squared Error than the one given by Ledoit and Wolf's formula used with shrinkage="auto". In LDA, the data are assumed to be gaussian conditionally to the class. If these assumptions hold, using LDA with the OAS estimator of covariance will yield a better classification accuracy than if Ledoit and Wolf or the empirical covariance estimator is used.

The covariance estimator can be chosen using with the `covariance_estimator` parameter of the <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> class. A covariance estimator should have a `fit` method and a `covariance_` attribute like all covariance estimators in the `sklearn.covariance` module.

<div class="centered">

[![shrinkage](../auto_examples/classification/images/sphx_glr_plot_lda_001.png)](../auto_examples/classification/plot_lda.html)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_classification\_plot\_lda.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_lda.py): Comparison of LDA classifiers with Empirical, Ledoit Wolf and OAS covariance estimator.

## Estimation algorithms

Using LDA and QDA requires computing the log-posterior which depends on the class priors \(P(y=k)\), the class means \(\mu_k\), and the covariance matrices.

The 'svd' solver is the default solver used for <span class="title-ref">\~sklearn.discriminant\_analysis.LinearDiscriminantAnalysis</span>, and it is the only available solver for <span class="title-ref">\~sklearn.discriminant\_analysis.QuadraticDiscriminantAnalysis</span>. It can perform both classification and transform (for LDA). As it does not rely on the calculation of the covariance matrix, the 'svd' solver may be preferable in situations where the number of features is large. The 'svd' solver cannot be used with shrinkage. For QDA, the use of the SVD solver relies on the fact that the covariance matrix \(\Sigma_k\) is, by definition, equal to \(\frac{1}{n - 1}
X_k^tX_k = \frac{1}{n - 1} V S^2 V^t\) where \(V\) comes from the SVD of the (centered) matrix: \(X_k = U S V^t\). It turns out that we can compute the log-posterior above without having to explicitly compute \(\Sigma\): computing \(S\) and \(V\) via the SVD of \(X\) is enough. For LDA, two SVDs are computed: the SVD of the centered input matrix \(X\) and the SVD of the class-wise mean vectors.

The 'lsqr' solver is an efficient algorithm that only works for classification. It needs to explicitly compute the covariance matrix \(\Sigma\), and supports shrinkage and custom covariance estimators. This solver computes the coefficients \(\omega_k = \Sigma^{-1}\mu_k\) by solving for \(\Sigma \omega =
\mu_k\), thus avoiding the explicit computation of the inverse \(\Sigma^{-1}\).

The 'eigen' solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the 'eigen' solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.

**References**

1.  R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification (Second Edition), section 2.6.2.

2.  "The Elements of Statistical Learning", Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.

3.  "The Elements of Statistical Learning", Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.

4.  Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.

---

learning_curve.md

---

# Validation curves: plotting scores to evaluate models

<div class="currentmodule">

sklearn.model\_selection

</div>

Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The **bias** of an estimator is its average error for different training sets. The **variance** of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.

In the following plot, we see a function \(f(x) = \cos (\frac{3}{2} \pi x)\) and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).

![](../auto_examples/model_selection/images/sphx_glr_plot_underfitting_overfitting_001.png)

Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see [Bias-variance dilemma](https://en.wikipedia.org/wiki/Bias-variance_dilemma)). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.

In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_underfitting\_overfitting.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_underfitting\_overfitting.py)
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_train\_error\_vs\_test\_error.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_train\_error\_vs\_test\_error.py)
  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_learning\_curve.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_learning\_curve.py)

## Validation curve

To validate a model we need a scoring function (see \[model\_evaluation\](\#model\_evaluation)), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator is of course grid search or similar methods (see \[grid\_search\](\#grid\_search)) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimize the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.

However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.

The function <span class="title-ref">validation\_curve</span> can help in this case:

    >>> import numpy as np
    >>> from sklearn.model_selection import validation_curve
    >>> from sklearn.datasets import load_iris
    >>> from sklearn.svm import SVC
    
    >>> np.random.seed(0)
    >>> X, y = load_iris(return_X_y=True)
    >>> indices = np.arange(y.shape[0])
    >>> np.random.shuffle(indices)
    >>> X, y = X[indices], y[indices]
    
    >>> train_scores, valid_scores = validation_curve(
    ...     SVC(kernel="linear"), X, y, param_name="C", param_range=np.logspace(-7, 3, 3),
    ... )
    >>> train_scores
    array([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],
           [0.9... , 0.92..., 0.93..., 0.92..., 0.93...],
           [0.97..., 1...   , 0.98..., 0.97..., 0.99...]])
    >>> valid_scores
    array([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],
           [0.9..., 0.83..., 0.96..., 0.96..., 0.93...],
           [1.... , 0.93..., 1....  , 1....  , 0.9... ]])

If you intend to plot the validation curves only, the class <span class="title-ref">\~sklearn.model\_selection.ValidationCurveDisplay</span> is more direct than using matplotlib manually on the results of a call to <span class="title-ref">validation\_curve</span>. You can use the method <span class="title-ref">\~sklearn.model\_selection.ValidationCurveDisplay.from\_estimator</span> similarly to <span class="title-ref">validation\_curve</span> to generate and plot the validation curve:

<div class="plot" data-context="close-figs" data-align="center">

from sklearn.datasets import load\_iris from sklearn.model\_selection import ValidationCurveDisplay from sklearn.svm import SVC from sklearn.utils import shuffle X, y = load\_iris(return\_X\_y=True) X, y = shuffle(X, y, random\_state=0) ValidationCurveDisplay.from\_estimator( SVC(kernel="linear"), X, y, param\_name="C", param\_range=np.logspace(-7, 3, 10) )

</div>

If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible.

## Learning curve

A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Consider the following example where we plot the learning curve of a naive Bayes classifier and an SVM.

For the naive Bayes, both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Thus, we will probably not benefit much from more training data.

In contrast, for small amounts of data, the training score of the SVM is much greater than the validation score. Adding more training samples will most likely increase generalization.

![](../auto_examples/model_selection/images/sphx_glr_plot_learning_curve_001.png)

We can use the function <span class="title-ref">learning\_curve</span> to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):

    >>> from sklearn.model_selection import learning_curve
    >>> from sklearn.svm import SVC
    
    >>> train_sizes, train_scores, valid_scores = learning_curve(
    ...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)
    >>> train_sizes
    array([ 50, 80, 110])
    >>> train_scores
    array([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],
           [0.98..., 1.   , 0.98..., 0.98..., 0.98...],
           [0.98..., 1.   , 0.98..., 0.98..., 0.99...]])
    >>> valid_scores
    array([[1. ,  0.93...,  1. ,  1. ,  0.96...],
           [1. ,  0.96...,  1. ,  1. ,  0.96...],
           [1. ,  0.96...,  1. ,  1. ,  0.96...]])

If you intend to plot the learning curves only, the class <span class="title-ref">\~sklearn.model\_selection.LearningCurveDisplay</span> will be easier to use. You can use the method <span class="title-ref">\~sklearn.model\_selection.LearningCurveDisplay.from\_estimator</span> similarly to <span class="title-ref">learning\_curve</span> to generate and plot the learning curve:

<div class="plot" data-context="close-figs" data-align="center">

from sklearn.datasets import load\_iris from sklearn.model\_selection import LearningCurveDisplay from sklearn.svm import SVC from sklearn.utils import shuffle X, y = load\_iris(return\_X\_y=True) X, y = shuffle(X, y, random\_state=0) LearningCurveDisplay.from\_estimator( SVC(kernel="linear"), X, y, train\_sizes=\[50, 80, 110\], cv=5)

</div>

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_learning\_curve.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_learning\_curve.py) for an example of using learning curves to check the scalability of a predictive model.

---

linear_model.md

---

# Linear Models

<div class="currentmodule">

sklearn.linear\_model

</div>

The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \(\hat{y}\) is the predicted value.

\[\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\]

Across the module, we designate the vector \(w = (w_1,
..., w_p)\) as `coef_` and \(w_0\) as `intercept_`.

To perform classification with generalized linear models, see \[Logistic\_regression\](\#logistic\_regression).

## Ordinary Least Squares

<span class="title-ref">LinearRegression</span> fits a linear model with coefficients \(w = (w_1, ..., w_p)\) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:

\[\min_{w} || X w - y||_2^2\]

![](../auto_examples/linear_model/images/sphx_glr_plot_ols_001.png)

<span class="title-ref">LinearRegression</span> will take in its `fit` method arrays `X`, `y` and will store the coefficients \(w\) of the linear model in its `coef_` member:

    >>> from sklearn import linear_model
    >>> reg = linear_model.LinearRegression()
    >>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
    LinearRegression()
    >>> reg.coef_
    array([0.5, 0.5])

The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix \(X\) have an approximately linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of *multicollinearity* can arise, for example, when data are collected without an experimental design.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_ols.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_ols.py)

### Non-Negative Least Squares

It is possible to constrain all the coefficients to be non-negative, which may be useful when they represent some physical or naturally non-negative quantities (e.g., frequency counts or prices of goods). <span class="title-ref">LinearRegression</span> accepts a boolean `positive` parameter: when set to <span class="title-ref">True</span> [Non-Negative Least Squares](https://en.wikipedia.org/wiki/Non-negative_least_squares) are then applied.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_nnls.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_nnls.py)

### Ordinary Least Squares Complexity

The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape <span class="title-ref">(n\_samples, n\_features)</span> this method has a cost of \(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that \(n_{\text{samples}} \geq n_{\text{features}}\).

## Ridge regression and classification

### Regression

<span class="title-ref">Ridge</span> regression addresses some of the problems of \[ordinary\_least\_squares\](\#ordinary\_least\_squares) by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:

\[\min_{w} || X w - y||_2^2 + \alpha ||w||_2^2\]

The complexity parameter \(\alpha \geq 0\) controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.

![](../auto_examples/linear_model/images/sphx_glr_plot_ridge_path_001.png)

As with other linear models, <span class="title-ref">Ridge</span> will take in its `fit` method arrays `X`, `y` and will store the coefficients \(w\) of the linear model in its `coef_` member:

    >>> from sklearn import linear_model
    >>> reg = linear_model.Ridge(alpha=.5)
    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
    Ridge(alpha=0.5)
    >>> reg.coef_
    array([0.34545455, 0.34545455])
    >>> reg.intercept_
    0.13636...

Note that the class <span class="title-ref">Ridge</span> allows for the user to specify that the solver be automatically chosen by setting <span class="title-ref">solver="auto"</span>. When this option is specified, <span class="title-ref">Ridge</span> will choose between the <span class="title-ref">"lbfgs"</span>, <span class="title-ref">"cholesky"</span>, and <span class="title-ref">"sparse\_cg"</span> solvers. <span class="title-ref">Ridge</span> will begin checking the conditions shown in the following table from top to bottom. If the condition is true, the corresponding solver is chosen.

|              |                                             |
| ------------ | ------------------------------------------- |
| **Solver**   | **Condition**                               |
| 'lbfgs'      | The `positive=True` option is specified.    |
| 'cholesky'   | The input array X is not sparse.            |
| 'sparse\_cg' | None of the above conditions are fulfilled. |

### Classification

The <span class="title-ref">Ridge</span> regressor has a classifier variant: <span class="title-ref">RidgeClassifier</span>. This classifier first converts binary targets to `{-1, 1}` and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressor's prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value.

It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However, in practice, all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the <span class="title-ref">RidgeClassifier</span> allows for a very different choice of the numerical solvers with distinct computational performance profiles.

The <span class="title-ref">RidgeClassifier</span> can be significantly faster than e.g. <span class="title-ref">LogisticRegression</span> with a high number of classes because it can compute the projection matrix \((X^T X)^{-1} X^T\) only once.

This classifier is sometimes referred to as a [Least Squares Support Vector Machines](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine) with a linear kernel.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_ridge\_path.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_ridge\_path.py)
  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py)
  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_linear\_model\_coefficient\_interpretation.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_linear\_model\_coefficient\_interpretation.py)

### Ridge Complexity

This method has the same order of complexity as \[ordinary\_least\_squares\](\#ordinary\_least\_squares).

### Setting the regularization parameter: leave-one-out Cross-Validation

<span class="title-ref">RidgeCV</span> and <span class="title-ref">RidgeClassifierCV</span> implement ridge regression/classification with built-in cross-validation of the alpha parameter. They work in the same way as <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> except that it defaults to efficient Leave-One-Out `cross-validation`. When using the default `cross-validation`, alpha cannot be 0 due to the formulation used to calculate Leave-One-Out error. See [\[RL2007\]]() for details.

Usage example:

    >>> import numpy as np
    >>> from sklearn import linear_model
    >>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
    RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,
          1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))
    >>> reg.alpha_
    0.01

Specifying the value of the `cv` attribute will trigger the use of cross-validation with <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span>, for example <span class="title-ref">cv=10</span> for 10-fold cross-validation, rather than Leave-One-Out Cross-Validation.

<div class="dropdown">

References

</div>

## Lasso

The <span class="title-ref">Lasso</span> is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason, Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see \[sphx\_glr\_auto\_examples\_applications\_plot\_tomography\_l1\_reconstruction.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_tomography\_l1\_reconstruction.py)).

Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:

\[\min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}\]

The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the coefficient vector.

The implementation in the class <span class="title-ref">Lasso</span> uses coordinate descent as the algorithm to fit the coefficients. See \[least\_angle\_regression\](\#least\_angle\_regression) for another implementation:

    >>> from sklearn import linear_model
    >>> reg = linear_model.Lasso(alpha=0.1)
    >>> reg.fit([[0, 0], [1, 1]], [0, 1])
    Lasso(alpha=0.1)
    >>> reg.predict([[1, 1]])
    array([0.8])

The function <span class="title-ref">lasso\_path</span> is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_and\_elasticnet.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_and\_elasticnet.py)
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_tomography\_l1\_reconstruction.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_tomography\_l1\_reconstruction.py)
  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_linear\_model\_coefficient\_interpretation.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_linear\_model\_coefficient\_interpretation.py)

<div class="note">

<div class="title">

Note

</div>

**Feature selection with Lasso**

As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in \[l1\_feature\_selection\](\#l1\_feature\_selection).

</div>

<div class="dropdown">

References

The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.

  - "Regularization Path For Generalized linear Models by Coordinate Descent", Friedman, Hastie & Tibshirani, J Stat Softw, 2010 ([Paper](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf)).
  - "An Interior-Point Method for Large-Scale L1-Regularized Least Squares,"
    S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007 ([Paper](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf))

</div>

### Setting regularization parameter

The `alpha` parameter controls the degree of sparsity of the estimated coefficients.

#### Using cross-validation

scikit-learn exposes objects that set the Lasso `alpha` parameter by cross-validation: <span class="title-ref">LassoCV</span> and <span class="title-ref">LassoLarsCV</span>. <span class="title-ref">LassoLarsCV</span> is based on the \[least\_angle\_regression\](\#least\_angle\_regression) algorithm explained below.

For high-dimensional datasets with many collinear features, <span class="title-ref">LassoCV</span> is most often preferable. However, <span class="title-ref">LassoLarsCV</span> has the advantage of exploring more relevant values of <span class="title-ref">alpha</span> parameter, and if the number of samples is very small compared to the number of features, it is often faster than <span class="title-ref">LassoCV</span>.

<div class="centered">

[![lasso\_cv\_1](../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_002.png)](../auto_examples/linear_model/plot_lasso_model_selection.html) [![lasso\_cv\_2](../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_003.png)](../auto_examples/linear_model/plot_lasso_model_selection.html)

</div>

#### Information-criteria based model selection

Alternatively, the estimator <span class="title-ref">LassoLarsIC</span> proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation.

Indeed, these criteria are computed on the in-sample training set. In short, they penalize the over-optimistic scores of the different Lasso models by their flexibility (cf. to "Mathematical details" section below).

However, such criteria need a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the correct model is candidates under investigation. They also tend to break when the problem is badly conditioned (e.g. more features than samples).

![](../auto_examples/linear_model/images/sphx_glr_plot_lasso_lars_ic_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_model\_selection.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_model\_selection.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_lars\_ic.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_lars\_ic.py)

#### AIC and BIC criteria

The definition of AIC (and thus BIC) might differ in the literature. In this section, we give more information regarding the criterion computed in scikit-learn.

<div class="dropdown">

Mathematical details

The AIC criterion is defined as:

\[AIC = -2 \log(\hat{L}) + 2 d\]

where \(\hat{L}\) is the maximum likelihood of the model and \(d\) is the number of parameters (as well referred to as degrees of freedom in the previous section).

The definition of BIC replace the constant \(2\) by \(\log(N)\):

\[BIC = -2 \log(\hat{L}) + \log(N) d\]

where \(N\) is the number of samples.

For a linear Gaussian model, the maximum log-likelihood is defined as:

\[\log(\hat{L}) = - \frac{n}{2} \log(2 \pi) - \frac{n}{2} \ln(\sigma^2) - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{2\sigma^2}\]

where \(\sigma^2\) is an estimate of the noise variance, \(y_i\) and \(\hat{y}_i\) are respectively the true and predicted targets, and \(n\) is the number of samples.

Plugging the maximum log-likelihood in the AIC formula yields:

\[AIC = n \log(2 \pi \sigma^2) + \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sigma^2} + 2 d\]

The first term of the above expression is sometimes discarded since it is a constant when \(\sigma^2\) is provided. In addition, it is sometimes stated that the AIC is equivalent to the \(C_p\) statistic . In a strict sense, however, it is equivalent only up to some constant and a multiplicative factor.

At last, we mentioned above that \(\sigma^2\) is an estimate of the noise variance. In <span class="title-ref">LassoLarsIC</span> when the parameter <span class="title-ref">noise\_variance</span> is not provided (default), the noise variance is estimated via the unbiased estimator defined as:

\[\sigma^2 = \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n - p}\]

where \(p\) is the number of features and \(\hat{y}_i\) is the predicted target using an ordinary least squares regression. Note, that this formula is valid only when <span class="title-ref">n\_samples \> n\_features</span>.

**References**

</div>

#### Comparison with the regularization parameter of SVM

The equivalence between `alpha` and the regularization parameter of SVM, `C` is given by `alpha = 1 / C` or `alpha = 1 / (n_samples * C)`, depending on the estimator and the exact objective function optimized by the model.

## Multi-task Lasso

The <span class="title-ref">MultiTaskLasso</span> is a linear model that estimates sparse coefficients for multiple regression problems jointly: `y` is a 2D array, of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks.

The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.

<div class="centered">

[![multi\_task\_lasso\_1](../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_001.png)](../auto_examples/linear_model/plot_multi_task_lasso_support.html) [![multi\_task\_lasso\_2](../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_002.png)](../auto_examples/linear_model/plot_multi_task_lasso_support.html)

</div>

<div class="centered">

Fitting a time-series model, imposing that any active feature be active at all times.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_multi\_task\_lasso\_support.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_multi\_task\_lasso\_support.py)

<div class="dropdown">

Mathematical details

Mathematically, it consists of a linear model trained with a mixed \(\ell_1\) \(\ell_2\)-norm for regularization. The objective function to minimize is:

\[\min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}} ^ 2 + \alpha ||W||_{21}}\]

where \(\text{Fro}\) indicates the Frobenius norm

\[||A||_{\text{Fro}} = \sqrt{\sum_{ij} a_{ij}^2}\]

and \(\ell_1\) \(\ell_2\) reads

\[||A||_{2 1} = \sum_i \sqrt{\sum_j a_{ij}^2}.\]

The implementation in the class <span class="title-ref">MultiTaskLasso</span> uses coordinate descent as the algorithm to fit the coefficients.

</div>

## Elastic-Net

<span class="title-ref">ElasticNet</span> is a linear regression model trained with both \(\ell_1\) and \(\ell_2\)-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like <span class="title-ref">Lasso</span>, while still maintaining the regularization properties of <span class="title-ref">Ridge</span>. We control the convex combination of \(\ell_1\) and \(\ell_2\) using the `l1_ratio` parameter.

Elastic-net is useful when there are multiple features that are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.

A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge's stability under rotation.

The objective function to minimize is in this case

\[\min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}\]

![](../auto_examples/linear_model/images/sphx_glr_plot_lasso_lasso_lars_elasticnet_path_002.png)

The class <span class="title-ref">ElasticNetCV</span> can be used to set the parameters `alpha` (\(\alpha\)) and `l1_ratio` (\(\rho\)) by cross-validation.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_and\_elasticnet.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_and\_elasticnet.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_lasso\_lars\_elasticnet\_path.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_lasso\_lars\_elasticnet\_path.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_elastic\_net\_precomputed\_gram\_matrix\_with\_weighted\_samples.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_elastic\_net\_precomputed\_gram\_matrix\_with\_weighted\_samples.py)

<div class="dropdown">

References

The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.

  - "Regularization Path For Generalized linear Models by Coordinate Descent", Friedman, Hastie & Tibshirani, J Stat Softw, 2010 ([Paper](https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf)).
  - "An Interior-Point Method for Large-Scale L1-Regularized Least Squares,"
    S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007 ([Paper](https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf))

</div>

## Multi-task Elastic-Net

The <span class="title-ref">MultiTaskElasticNet</span> is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: `Y` is a 2D array of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks.

Mathematically, it consists of a linear model trained with a mixed \(\ell_1\) \(\ell_2\)-norm and \(\ell_2\)-norm for regularization. The objective function to minimize is:

\[\min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}}^2 + \alpha \rho ||W||_{2 1} +
\frac{\alpha(1-\rho)}{2} ||W||_{\text{Fro}}^2}\]

The implementation in the class <span class="title-ref">MultiTaskElasticNet</span> uses coordinate descent as the algorithm to fit the coefficients.

The class <span class="title-ref">MultiTaskElasticNetCV</span> can be used to set the parameters `alpha` (\(\alpha\)) and `l1_ratio` (\(\rho\)) by cross-validation.

## Least Angle Regression

Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.

The advantages of LARS are:

  - It is numerically efficient in contexts where the number of features is significantly greater than the number of samples.
  - It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares.
  - It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.
  - If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.
  - It is easily modified to produce solutions for other estimators, like the Lasso.

The disadvantages of the LARS method include:

  - Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article.

The LARS model can be used via the estimator <span class="title-ref">Lars</span>, or its low-level implementation <span class="title-ref">lars\_path</span> or <span class="title-ref">lars\_path\_gram</span>.

## LARS Lasso

<span class="title-ref">LassoLars</span> is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients.

![](../auto_examples/linear_model/images/sphx_glr_plot_lasso_lasso_lars_elasticnet_path_001.png)

    >>> from sklearn import linear_model
    >>> reg = linear_model.LassoLars(alpha=.1)
    >>> reg.fit([[0, 0], [1, 1]], [0, 1])
    LassoLars(alpha=0.1)
    >>> reg.coef_
    array([0.6..., 0.        ])

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_lasso\_lars\_elasticnet\_path.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_lasso\_lars\_elasticnet\_path.py)

The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions <span class="title-ref">lars\_path</span> or <span class="title-ref">lars\_path\_gram</span>.

<div class="dropdown">

Mathematical formulation

The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one's correlations with the residual.

Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the \(\ell_1\) norm of the parameter vector. The full coefficients path is stored in the array `coef_path_` of shape <span class="title-ref">(n\_features, max\_features + 1)</span>. The first column is always zero.

**References**

  - Original Algorithm is detailed in the paper [Least Angle Regression](https://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf) by Hastie et al.

</div>

## Orthogonal Matching Pursuit (OMP)

<span class="title-ref">OrthogonalMatchingPursuit</span> and <span class="title-ref">orthogonal\_mp</span> implement the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (ie. the \(\ell_0\) pseudo-norm).

Being a forward feature selection method like \[least\_angle\_regression\](\#least\_angle\_regression), orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements:

\[\underset{w}{\operatorname{arg\,min\,}}  ||y - Xw||_2^2 \text{ subject to } ||w||_0 \leq n_{\text{nonzero_coefs}}\]

Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as:

\[\underset{w}{\operatorname{arg\,min\,}} ||w||_0 \text{ subject to } ||y-Xw||_2^2 \leq \text{tol}\]

OMP is based on a greedy algorithm that includes at each step the atom most highly correlated with the current residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is recomputed using an orthogonal projection on the space of the previously chosen dictionary elements.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_omp.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_omp.py)

<div class="dropdown">

References

  - <https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf>
  - [Matching pursuits with time-frequency dictionaries](https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf),
    19. 7.  Mallat, Z. Zhang,

</div>

## Bayesian Regression

Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.

This can be done by introducing [uninformative priors](https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors) over the hyper parameters of the model. The \(\ell_{2}\) regularization used in \[ridge\_regression\](\#ridge\_regression) is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients \(w\) with precision \(\lambda^{-1}\). Instead of setting <span class="title-ref">lambda</span> manually, it is possible to treat it as a random variable to be estimated from the data.

To obtain a fully probabilistic model, the output \(y\) is assumed to be Gaussian distributed around \(X w\):

\[p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha^{-1})\]

where \(\alpha\) is again treated as a random variable that is to be estimated from the data.

The advantages of Bayesian Regression are:

  - It adapts to the data at hand.
  - It can be used to include regularization parameters in the estimation procedure.

The disadvantages of Bayesian regression include:

  - Inference of the model can be time consuming.

<div class="dropdown">

References

  - A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning
  - Original Algorithm is detailed in the book <span class="title-ref">Bayesian learning for neural networks</span> by Radford M. Neal

</div>

### Bayesian Ridge Regression

<span class="title-ref">BayesianRidge</span> estimates a probabilistic model of the regression problem as described above. The prior for the coefficient \(w\) is given by a spherical Gaussian:

\[p(w|\lambda) =
\mathcal{N}(w|0,\lambda^{-1}\mathbf{I}_{p})\]

The priors over \(\alpha\) and \(\lambda\) are chosen to be [gamma distributions](https://en.wikipedia.org/wiki/Gamma_distribution), the conjugate prior for the precision of the Gaussian. The resulting model is called *Bayesian Ridge Regression*, and is similar to the classical <span class="title-ref">Ridge</span>.

The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model, the regularization parameters \(\alpha\) and \(\lambda\) being estimated by maximizing the *log marginal likelihood*. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \(\alpha\) and \(\lambda\) is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters `alpha_init` and `lambda_init`.

There are four more hyperparameters, \(\alpha_1\), \(\alpha_2\), \(\lambda_1\) and \(\lambda_2\) of the gamma prior distributions over \(\alpha\) and \(\lambda\). These are usually chosen to be *non-informative*. By default \(\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}\).

Bayesian Ridge Regression is used for regression:

    >>> from sklearn import linear_model
    >>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
    >>> Y = [0., 1., 2., 3.]
    >>> reg = linear_model.BayesianRidge()
    >>> reg.fit(X, Y)
    BayesianRidge()

After being fitted, the model can then be used to predict new values:

    >>> reg.predict([[1, 0.]])
    array([0.50000013])

The coefficients \(w\) of the model can be accessed:

    >>> reg.coef_
    array([0.49999993, 0.49999993])

Due to the Bayesian framework, the weights found are slightly different to the ones found by \[ordinary\_least\_squares\](\#ordinary\_least\_squares). However, Bayesian Ridge Regression is more robust to ill-posed problems.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_bayesian\_ridge\_curvefit.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_bayesian\_ridge\_curvefit.py)

<div class="dropdown">

References

  - Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006
  - David J. C. MacKay, [Bayesian Interpolation](https://citeseerx.ist.psu.edu/doc_view/pid/b14c7cc3686e82ba40653c6dff178356a33e5e2c), 1992.
  - Michael E. Tipping, [Sparse Bayesian Learning and the Relevance Vector Machine](https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf), 2001.

</div>

### Automatic Relevance Determination - ARD

The Automatic Relevance Determination (as being implemented in <span class="title-ref">ARDRegression</span>) is a kind of linear model which is very similar to the [Bayesian Ridge Regression](#bayesian-ridge-regression), but that leads to sparser coefficients \(w\) \[1\]\[2\].

<span class="title-ref">ARDRegression</span> poses a different prior over \(w\): it drops the spherical Gaussian distribution for a centered elliptic Gaussian distribution. This means each coefficient \(w_{i}\) can itself be drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):

\[p(w|\lambda) = \mathcal{N}(w|0,A^{-1})\]

with \(A\) being a positive definite diagonal matrix and \(\text{diag}(A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}\).

In contrast to the [Bayesian Ridge Regression](#bayesian-ridge-regression), each coordinate of \(w_{i}\) has its own standard deviation \(\frac{1}{\lambda_i}\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by the hyperparameters \(\lambda_1\) and \(\lambda_2\).

ARD is also known in the literature as *Sparse Bayesian Learning* and *Relevance Vector Machine*\[3\]\[4\]. For a worked-out comparison between ARD and [Bayesian Ridge Regression](#bayesian-ridge-regression), see the example below.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_ard.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_ard.py)

**References**

## Logistic regression

The logistic regression is implemented in <span class="title-ref">LogisticRegression</span>. Despite its name, it is implemented as a linear model for classification rather than regression in terms of the scikit-learn/ML nomenclature. The logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a [logistic function](https://en.wikipedia.org/wiki/Logistic_function).

This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \(\ell_1\), \(\ell_2\) or Elastic-Net regularization.

<div class="note">

<div class="title">

Note

</div>

**Regularization**

Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting C to a very high value.

</div>

<div class="note">

<div class="title">

Note

</div>

**Logistic Regression as a special case of the Generalized Linear Models (GLM)**

Logistic regression is a special case of \[generalized\_linear\_models\](\#generalized\_linear\_models) with a Binomial / Bernoulli conditional distribution and a Logit link. The numerical output of the logistic regression, which is the predicted probability, can be used as a classifier by applying a threshold (by default 0.5) to it. This is how it is implemented in scikit-learn, so it expects a categorical target, making the Logistic Regression a classifier.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_logistic\_l1\_l2\_sparsity.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_logistic\_l1\_l2\_sparsity.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_logistic\_path.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_logistic\_path.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_logistic\_multinomial.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_logistic\_multinomial.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sparse\_logistic\_regression\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sparse\_logistic\_regression\_20newsgroups.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sparse\_logistic\_regression\_mnist.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sparse\_logistic\_regression\_mnist.py)
  - \[sphx\_glr\_auto\_examples\_classification\_plot\_classification\_probability.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_classification\_probability.py)

### Binary Case

For notational ease, we assume that the target \(y_i\) takes values in the set \(\{0, 1\}\) for data point \(i\). Once fitted, the <span class="title-ref">\~sklearn.linear\_model.LogisticRegression.predict\_proba</span> method of <span class="title-ref">\~sklearn.linear\_model.LogisticRegression</span> predicts the probability of the positive class \(P(y_i=1|X_i)\) as

\[\hat{p}(X_i) = \operatorname{expit}(X_i w + w_0) = \frac{1}{1 + \exp(-X_i w - w_0)}.\]

As an optimization problem, binary class logistic regression with regularization term \(r(w)\) minimizes the following cost function:

\[\min_{w} \frac{1}{S}\sum_{i=1}^n s_i
\left(-y_i \log(\hat{p}(X_i)) - (1 - y_i) \log(1 - \hat{p}(X_i))\right)
+ \frac{r(w)}{S C}\,,\]

where \({s_i}\) corresponds to the weights assigned by the user to a specific training sample (the vector \(s\) is formed by element-wise multiplication of the class weights and sample weights), and the sum \(S = \sum_{i=1}^n s_i\).

We currently provide four choices for the regularization term \(r(w)\) via the <span class="title-ref">penalty</span> argument:

| penalty                                   | \(r(w)\)                                    |
| ----------------------------------------- | ------------------------------------------- |
| <span class="title-ref">None</span>       | \(0\)                                       |
| \(\ell_1\)                                | \(\|w\|_1\)                                 |
| \(\ell_2\)                                | \(\frac{1}{2}\|w\|_2^2 = \frac{1}{2}w^T w\) |
| <span class="title-ref">ElasticNet</span> | \(\frac{1 - \rho}{2}w^T w + \rho \|w\|_1\)  |

For ElasticNet, \(\rho\) (which corresponds to the <span class="title-ref">l1\_ratio</span> parameter) controls the strength of \(\ell_1\) regularization vs. \(\ell_2\) regularization. Elastic-Net is equivalent to \(\ell_1\) when \(\rho = 1\) and equivalent to \(\ell_2\) when \(\rho=0\).

Note that the scale of the class weights and the sample weights will influence the optimization problem. For instance, multiplying the sample weights by a constant \(b>0\) is equivalent to multiplying the (inverse) regularization strength <span class="title-ref">C</span> by \(b\).

### Multinomial Case

The binary case can be extended to \(K\) classes leading to the multinomial logistic regression, see also [log-linear model](https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model).

<div class="note">

<div class="title">

Note

</div>

It is possible to parameterize a \(K\)-class classification model using only \(K-1\) weight vectors, leaving one class probability fully determined by the other class probabilities by leveraging the fact that all class probabilities must sum to one. We deliberately choose to overparameterize the model using \(K\) weight vectors for ease of implementation and to preserve the symmetrical inductive bias regarding ordering of classes, see. This effect becomes especially important when using regularization. The choice of overparameterization can be detrimental for unpenalized models since then the solution may not be unique, as shown in.

</div>

<div class="dropdown">

Mathematical details

Let \(y_i \in {1, \ldots, K}\) be the label (ordinal) encoded target variable for observation \(i\). Instead of a single coefficient vector, we now have a matrix of coefficients \(W\) where each row vector \(W_k\) corresponds to class \(k\). We aim at predicting the class probabilities \(P(y_i=k|X_i)\) via <span class="title-ref">\~sklearn.linear\_model.LogisticRegression.predict\_proba</span> as:

\[\hat{p}_k(X_i) = \frac{\exp(X_i W_k + W_{0, k})}{\sum_{l=0}^{K-1} \exp(X_i W_l + W_{0, l})}.\]

The objective for the optimization becomes

\[\min_W -\frac{1}{S}\sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik} [y_i = k] \log(\hat{p}_k(X_i))
+ \frac{r(W)}{S C}\,,\]

where \([P]\) represents the Iverson bracket which evaluates to \(0\) if \(P\) is false, otherwise it evaluates to \(1\).

Again, \(s_{ik}\) are the weights assigned by the user (multiplication of sample weights and class weights) with their sum \(S = \sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik}\).

We currently provide four choices for the regularization term \(r(W)\) via the <span class="title-ref">penalty</span> argument, where \(m\) is the number of features:

| penalty                                   | \(r(W)\)                                                                   |
| ----------------------------------------- | -------------------------------------------------------------------------- |
| <span class="title-ref">None</span>       | \(0\)                                                                      |
| \(\ell_1\)                                | \(\|W\|_{1,1} = \sum_{i=1}^m\sum_{j=1}^{K}|W_{i,j}|\)                      |
| \(\ell_2\)                                | \(\frac{1}{2}\|W\|_F^2 = \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^{K} W_{i,j}^2\) |
| <span class="title-ref">ElasticNet</span> | \(\frac{1 - \rho}{2}\|W\|_F^2 + \rho \|W\|_{1,1}\)                         |

</div>

### Solvers

The solvers implemented in the class <span class="title-ref">LogisticRegression</span> are "lbfgs", "liblinear", "newton-cg", "newton-cholesky", "sag" and "saga":

The following table summarizes the penalties and multinomial multiclass supported by each solver:

<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 13%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 18%" />
<col style="width: 9%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td><blockquote>
<p>**Solvers</p>
</blockquote></td>
<td>**</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Penalties</strong></td>
<td><strong>'lbfgs'</strong> | **'</td>
<td>liblinear'**</td>
<td><strong>'newton-cg'</strong></td>
<td><strong>'newton-cholesky'</strong></td>
<td><strong>'sag'</strong></td>
<td><strong>'saga'</strong></td>
</tr>
<tr class="odd">
<td>L2 penalty</td>
<td><blockquote>
<p>yes |</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
</tr>
<tr class="even">
<td>L1 penalty</td>
<td><blockquote>
<p>no |</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Elastic-Net (L1 + L2)</td>
<td><blockquote>
<p>no |</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
</tr>
<tr class="even">
<td>No penalty ('none')</td>
<td><blockquote>
<p>yes |</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>Multiclass support</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>multinomial multiclass</td>
<td><blockquote>
<p>yes |</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>Behaviors</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Penalize the intercept (bad)</td>
<td><blockquote>
<p>no |</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Faster for large datasets</td>
<td><blockquote>
<p>no |</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Robust to unscaled datasets</td>
<td><blockquote>
<p>yes |</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>yes</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
<td><blockquote>
<p>no</p>
</blockquote></td>
</tr>
</tbody>
</table>

The "lbfgs" solver is used by default for its robustness. For large datasets the "saga" solver is usually faster. For large dataset, you may also consider using <span class="title-ref">SGDClassifier</span> with <span class="title-ref">loss="log\_loss"</span>, which might be even faster but requires more tuning.

#### Differences between solvers

There might be a difference in the scores obtained between <span class="title-ref">LogisticRegression</span> with `solver=liblinear` or <span class="title-ref">\~sklearn.svm.LinearSVC</span> and the external liblinear library directly, when `fit_intercept=False` and the fit `coef_` (or) the data to be predicted are zeroes. This is because for the sample(s) with `decision_function` zero, <span class="title-ref">LogisticRegression</span> and <span class="title-ref">\~sklearn.svm.LinearSVC</span> predict the negative class, while liblinear predicts the positive class. Note that a model with `fit_intercept=False` and having many samples with `decision_function` zero, is likely to be a underfit, bad model and you are advised to set `fit_intercept=True` and increase the `intercept_scaling`.

<div class="dropdown">

Solvers' details

  - The solver "liblinear" uses a coordinate descent (CD) algorithm, and relies on the excellent C++ [LIBLINEAR library](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a "one-vs-rest" fashion so separate binary classifiers are trained for all classes. This happens under the hood, so <span class="title-ref">LogisticRegression</span> instances using this solver behave as multiclass classifiers. For \(\ell_1\) regularization <span class="title-ref">sklearn.svm.l1\_min\_c</span> allows to calculate the lower bound for C in order to get a non "null" (all feature weights to zero) model.
  - The "lbfgs", "newton-cg" and "sag" solvers only support \(\ell_2\) regularization or no regularization, and are found to converge faster for some high-dimensional data. Setting <span class="title-ref">multi\_class</span> to "multinomial" with these solvers learns a true multinomial logistic regression model, which means that its probability estimates should be better calibrated than the default "one-vs-rest" setting.
  - The "sag" solver uses Stochastic Average Gradient descent. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.
  - The "saga" solver is a variant of "sag" that also supports the non-smooth <span class="title-ref">penalty="l1"</span>. This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports <span class="title-ref">penalty="elasticnet"</span>.
  - The "lbfgs" is an optimization algorithm that approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm, which belongs to quasi-Newton methods. As such, it can deal with a wide range of different training data and is therefore the default solver. Its performance, however, suffers on poorly scaled datasets and on datasets with one-hot encoded categorical features with rare categories.
  - The "newton-cholesky" solver is an exact Newton solver that calculates the hessian matrix and solves the resulting linear system. It is a very good choice for <span class="title-ref">n\_samples</span> \>\> <span class="title-ref">n\_features</span>, but has a few shortcomings: Only \(\ell_2\) regularization is supported. Furthermore, because the hessian matrix is explicitly computed, the memory usage has a quadratic dependency on <span class="title-ref">n\_features</span> as well as on <span class="title-ref">n\_classes</span>. As a consequence, only the one-vs-rest scheme is implemented for the multiclass case.

For a comparison of some of these solvers, see.

**References**

</div>

<div class="note">

<div class="title">

Note

</div>

**Feature selection with sparse logistic regression**

A logistic regression with \(\ell_1\) penalty yields sparse models, and can thus be used to perform feature selection, as detailed in \[l1\_feature\_selection\](\#l1\_feature\_selection).

</div>

<div class="note">

<div class="title">

Note

</div>

**P-value estimation**

It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The [statsmodels package](https://pypi.org/project/statsmodels/) natively supports this. Within sklearn, one could use bootstrapping instead as well.

</div>

<span class="title-ref">LogisticRegressionCV</span> implements Logistic Regression with built-in cross-validation support, to find the optimal <span class="title-ref">C</span> and <span class="title-ref">l1\_ratio</span> parameters according to the `scoring` attribute. The "newton-cg", "sag", "saga" and "lbfgs" solvers are found to be faster for high-dimensional dense data, due to warm-starting (see `Glossary <warm_start>`).

## Generalized Linear Models<span id="Generalized_linear_regression"></span>

Generalized Linear Models (GLM) extend linear models in two ways \[5\]. First, the predicted values \(\hat{y}\) are linked to a linear combination of the input variables \(X\) via an inverse link function \(h\) as

\[\hat{y}(w, X) = h(Xw).\]

Secondly, the squared loss function is replaced by the unit deviance \(d\) of a distribution in the exponential family (or more precisely, a reproductive exponential dispersion model (EDM)\[6\]).

The minimization problem becomes:

\[\min_{w} \frac{1}{2 n_{\text{samples}}} \sum_i d(y_i, \hat{y}_i) + \frac{\alpha}{2} ||w||_2^2,\]

where \(\alpha\) is the L2 regularization penalty. When sample weights are provided, the average becomes a weighted average.

The following table lists some specific EDMs and their unit deviance :

| Distribution     | Target Domain               | Unit Deviance \(d(y, \hat{y})\)                                                           |
| ---------------- | --------------------------- | ----------------------------------------------------------------------------------------- |
| Normal           | \(y \in (-\infty, \infty)\) | \((y-\hat{y})^2\)                                                                         |
| Bernoulli        | \(y \in \{0, 1\}\)          | \(2({y}\log\frac{y}{\hat{y}}+({1}-{y})\log\frac{{1}-{y}}{{1}-\hat{y}})\)                  |
| Categorical      | \(y \in \{0, 1, ..., k\}\)  | \(2\sum_{i \in \{0, 1, ..., k\}} I(y = i) y_\text{i}\log\frac{I(y = i)}{\hat{I(y = i)}}\) |
| Poisson          | \(y \in [0, \infty)\)       | \(2(y\log\frac{y}{\hat{y}}-y+\hat{y})\)                                                   |
| Gamma            | \(y \in (0, \infty)\)       | \(2(\log\frac{\hat{y}}{y}+\frac{y}{\hat{y}}-1)\)                                          |
| Inverse Gaussian | \(y \in (0, \infty)\)       | \(\frac{(y-\hat{y})^2}{y\hat{y}^2}\)                                                      |

The Probability Density Functions (PDF) of these distributions are illustrated in the following figure,

![PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma distributions with different mean values (\(\mu\)). Observe the point mass at \(Y=0\) for the Poisson distribution and the Tweedie (power=1.5) distribution, but not for the Gamma distribution which has a strictly positive target domain.](./glm_data/poisson_gamma_tweedie_distributions.png)

The Bernoulli distribution is a discrete probability distribution modelling a Bernoulli trial - an event that has only two mutually exclusive outcomes. The Categorical distribution is a generalization of the Bernoulli distribution for a categorical random variable. While a random variable in a Bernoulli distribution has two possible outcomes, a Categorical random variable can take on one of K possible categories, with the probability of each category specified separately.

The choice of the distribution depends on the problem at hand:

  - If the target values \(y\) are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson distribution with a log-link.
  - If the target values are positive valued and skewed, you might try a Gamma distribution with a log-link.
  - If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian distribution (or even higher variance powers of the Tweedie family).
  - If the target values \(y\) are probabilities, you can use the Bernoulli distribution. The Bernoulli distribution with a logit link can be used for binary classification. The Categorical distribution with a softmax link can be used for multiclass classification.

<div class="dropdown">

Examples of use cases

  - Agriculture / weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie / Compound Poisson Gamma).
  - Risk modeling / insurance policy pricing: number of claim events / policyholder per year (Poisson), cost per event (Gamma), total cost per policyholder per year (Tweedie / Compound Poisson Gamma).
  - Credit Default: probability that a loan can't be paid back (Bernoulli).
  - Fraud Detection: probability that a financial transaction like a cash transfer is a fraudulent transaction (Bernoulli).
  - Predictive maintenance: number of production interruption events per year (Poisson), duration of interruption (Gamma), total interruption time per year (Tweedie / Compound Poisson Gamma).
  - Medical Drug Testing: probability of curing a patient in a set of trials or probability that a patient will experience side effects (Bernoulli).
  - News Classification: classification of news articles into three categories namely Business News, Politics and Entertainment news (Categorical).

</div>

**References**

### Usage

<span class="title-ref">TweedieRegressor</span> implements a generalized linear model for the Tweedie distribution, that allows to model any of the above mentioned distributions using the appropriate `power` parameter. In particular:

  - `power = 0`: Normal distribution. Specific estimators such as <span class="title-ref">Ridge</span>, <span class="title-ref">ElasticNet</span> are generally more appropriate in this case.
  - `power = 1`: Poisson distribution. <span class="title-ref">PoissonRegressor</span> is exposed for convenience. However, it is strictly equivalent to <span class="title-ref">TweedieRegressor(power=1, link='log')</span>.
  - `power = 2`: Gamma distribution. <span class="title-ref">GammaRegressor</span> is exposed for convenience. However, it is strictly equivalent to <span class="title-ref">TweedieRegressor(power=2, link='log')</span>.
  - `power = 3`: Inverse Gaussian distribution.

The link function is determined by the <span class="title-ref">link</span> parameter.

Usage example:

    >>> from sklearn.linear_model import TweedieRegressor
    >>> reg = TweedieRegressor(power=1, alpha=0.5, link='log')
    >>> reg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2])
    TweedieRegressor(alpha=0.5, link='log', power=1)
    >>> reg.coef_
    array([0.2463..., 0.4337...])
    >>> reg.intercept_
    -0.7638...

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_poisson\_regression\_non\_normal\_loss.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_poisson\_regression\_non\_normal\_loss.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_tweedie\_regression\_insurance\_claims.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_tweedie\_regression\_insurance\_claims.py)

<div class="dropdown">

Practical considerations

The feature matrix <span class="title-ref">X</span> should be standardized before fitting. This ensures that the penalty treats features equally.

Since the linear predictor \(Xw\) can be negative and Poisson, Gamma and Inverse Gaussian distributions don't support negative values, it is necessary to apply an inverse link function that guarantees the non-negativeness. For example with <span class="title-ref">link='log'</span>, the inverse link function becomes \(h(Xw)=\exp(Xw)\).

If you want to model a relative frequency, i.e. counts per exposure (time, volume, ...) you can do so by using a Poisson distribution and passing \(y=\frac{\mathrm{counts}}{\mathrm{exposure}}\) as target values together with \(\mathrm{exposure}\) as sample weights. For a concrete example see e.g. \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_tweedie\_regression\_insurance\_claims.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_tweedie\_regression\_insurance\_claims.py).

When performing cross-validation for the <span class="title-ref">power</span> parameter of <span class="title-ref">TweedieRegressor</span>, it is advisable to specify an explicit <span class="title-ref">scoring</span> function, because the default scorer <span class="title-ref">TweedieRegressor.score</span> is a function of <span class="title-ref">power</span> itself.

</div>

## Stochastic Gradient Descent - SGD

Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The `partial_fit` method allows online/out-of-core learning.

The classes <span class="title-ref">SGDClassifier</span> and <span class="title-ref">SGDRegressor</span> provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with `loss="log"`, <span class="title-ref">SGDClassifier</span> fits a logistic regression model, while with `loss="hinge"` it fits a linear support vector machine (SVM).

You can refer to the dedicated \[sgd\](\#sgd) documentation section for more details.

## Perceptron

The <span class="title-ref">Perceptron</span> is another simple classification algorithm suitable for large scale learning. By default:

  - It does not require a learning rate.
  - It is not regularized (penalized).
  - It updates its model only on mistakes.

The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.

In fact, the <span class="title-ref">Perceptron</span> is a wrapper around the <span class="title-ref">SGDClassifier</span> class using a perceptron loss and a constant learning rate. Refer to \[mathematical section \<sgd\_mathematical\_formulation\>\](\#mathematical-section-\<sgd\_mathematical\_formulation\>) of the SGD procedure for more details.

## Passive Aggressive Algorithms

The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter `C`.

For classification, <span class="title-ref">PassiveAggressiveClassifier</span> can be used with `loss='hinge'` (PA-I) or `loss='squared_hinge'` (PA-II). For regression, <span class="title-ref">PassiveAggressiveRegressor</span> can be used with `loss='epsilon_insensitive'` (PA-I) or `loss='squared_epsilon_insensitive'` (PA-II).

<div class="dropdown">

References

  - ["Online Passive-Aggressive Algorithms"](http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)
    11. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)

</div>

## Robustness regression: outliers and modeling errors

Robust regression aims to fit a regression model in the presence of corrupt data: either outliers, or error in the model.

![](../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png)

### Different scenario and useful concepts

There are different things to keep in mind when dealing with data corrupted by outliers:

  - **Outliers in X or in y**?
    
    | Outliers in the y direction                                                                                                                 | Outliers in the X direction                                                                                                                 |
    | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
    | [![y\_outliers](../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_003.png)](../auto_examples/linear_model/plot_robust_fit.html) | [![X\_outliers](../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_002.png)](../auto_examples/linear_model/plot_robust_fit.html) |

  - **Fraction of outliers versus amplitude of error**
    
    The number of outlying points matters, but also how much they are outliers.
    
    | Small outliers                                                                                                                              | Large outliers                                                                                                                                     |
    | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
    | [![y\_outliers](../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_003.png)](../auto_examples/linear_model/plot_robust_fit.html) | [![large\_y\_outliers](../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_005.png)](../auto_examples/linear_model/plot_robust_fit.html) |

An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data.

Note that in general, robust fitting in high-dimensional setting (large <span class="title-ref">n\_features</span>) is very hard. The robust models here will probably not work in these settings.

<div class="topic">

**Trade-offs: which estimator ?**

Scikit-learn provides 3 robust regression estimators: \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>), \[Theil Sen \<theil\_sen\_regression\>\](\#theil-sen-\<theil\_sen\_regression\>) and \[HuberRegressor \<huber\_regression\>\](\#huberregressor-\<huber\_regression\>).

  - \[HuberRegressor \<huber\_regression\>\](\#huberregressor-\<huber\_regression\>) should be faster than \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>) and \[Theil Sen \<theil\_sen\_regression\>\](\#theil-sen-\<theil\_sen\_regression\>) unless the number of samples are very large, i.e. `n_samples` \>\> `n_features`. This is because \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>) and \[Theil Sen \<theil\_sen\_regression\>\](\#theil-sen-\<theil\_sen\_regression\>) fit on smaller subsets of the data. However, both \[Theil Sen \<theil\_sen\_regression\>\](\#theil-sen-\<theil\_sen\_regression\>) and \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>) are unlikely to be as robust as \[HuberRegressor \<huber\_regression\>\](\#huberregressor-\<huber\_regression\>) for the default parameters.
  - \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>) is faster than \[Theil Sen \<theil\_sen\_regression\>\](\#theil-sen-\<theil\_sen\_regression\>) and scales much better with the number of samples.
  - \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>) will deal better with large outliers in the y direction (most common situation).
  - \[Theil Sen \<theil\_sen\_regression\>\](\#theil-sen-\<theil\_sen\_regression\>) will cope better with medium-size outliers in the X direction, but this property will disappear in high-dimensional settings.

When in doubt, use \[RANSAC \<ransac\_regression\>\](\#ransac-\<ransac\_regression\>).

</div>

### RANSAC: RANdom SAmple Consensus

RANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set.

RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations (see <span class="title-ref">max\_trials</span> parameter). It is typically used for linear and non-linear regression problems and is especially popular in the field of photogrammetric computer vision.

The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.

![](../auto_examples/linear_model/images/sphx_glr_plot_ransac_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_ransac.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_ransac.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_robust\_fit.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_robust\_fit.py)

<div class="dropdown">

Details of the algorithm

Each iteration performs the following steps:

1.  Select `min_samples` random samples from the original data and check whether the set of data is valid (see `is_data_valid`).
2.  Fit a model to the random subset (`estimator.fit`) and check whether the estimated model is valid (see `is_model_valid`).
3.  Classify all data as inliers or outliers by calculating the residuals to the estimated model (`estimator.predict(X) - y`) - all data samples with absolute residuals smaller than or equal to the `residual_threshold` are considered as inliers.
4.  Save fitted model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has better score.

These steps are performed either a maximum number of times (`max_trials`) or until one of the special stop criteria are met (see `stop_n_inliers` and `stop_score`). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.

The `is_data_valid` and `is_model_valid` functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, `is_data_valid` should be used as it is called prior to fitting the model and thus leading to better computational performance.

</div>

<div class="dropdown">

References

  - <https://en.wikipedia.org/wiki/RANSAC>
  - ["Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography"](https://www.cs.ait.ac.th/~mdailey/cvreadings/Fischler-RANSAC.pdf) Martin A. Fischler and Robert C. Bolles - SRI International (1981)
  - ["Performance Evaluation of RANSAC Family"](http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf) Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)

</div>

### Theil-Sen estimator: generalized-median-based estimator

The <span class="title-ref">TheilSenRegressor</span> estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It loses its robustness properties and becomes no better than an ordinary least squares in high dimension.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_theilsen.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_theilsen.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_robust\_fit.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_robust\_fit.py)

<div class="dropdown">

Theoretical considerations

<span class="title-ref">TheilSenRegressor</span> is comparable to the \[Ordinary Least Squares (OLS) \<ordinary\_least\_squares\>\](\#ordinary-least-squares

</div>

  - \--(ols)-\<ordinary\_least\_squares\>) in terms of asymptotic efficiency and as an  
    unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric method which means it makes no assumption about the underlying distribution of the data. Since Theil-Sen is a median-based estimator, it is more robust against corrupted data aka outliers. In univariate setting, Theil-Sen has a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary corrupted data of up to 29.3%.
    
    ![](../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png)
    
    The implementation of <span class="title-ref">TheilSenRegressor</span> in scikit-learn follows a generalization to a multivariate linear regression model using the spatial median which is a generalization of the median to multiple dimensions.
    
    In terms of time and space complexity, Theil-Sen scales according to
    
    \[\binom{n_{\text{samples}}}{n_{\text{subsamples}}}\]
    
    which makes it infeasible to be applied exhaustively to problems with a large number of samples and features. Therefore, the magnitude of a subpopulation can be chosen to limit the time and space complexity by considering only a random subset of all possible combinations.
    
    **References**
    
    Also see the [Wikipedia page](https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator)

### Huber Regression

The <span class="title-ref">HuberRegressor</span> is different to <span class="title-ref">Ridge</span> because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from <span class="title-ref">TheilSenRegressor</span> and <span class="title-ref">RANSACRegressor</span> because it does not ignore the effect of the outliers but gives a lesser weight to them.

![](/auto_examples/linear_model/images/sphx_glr_plot_huber_vs_ridge_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_huber\_vs\_ridge.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_huber\_vs\_ridge.py)

<div class="dropdown">

Mathematical details

The loss function that <span class="title-ref">HuberRegressor</span> minimizes is given by

\[\min_{w, \sigma} {\sum_{i=1}^n\left(\sigma + H_{\epsilon}\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}\]

where

\[\begin{aligned}
H_{\epsilon}(z) = \begin{cases}
      z^2, & \text {if } |z| < \epsilon, \\
      2\epsilon|z| - \epsilon^2, & \text{otherwise}
\end{cases}
\end{aligned}\]

It is advised to set the parameter `epsilon` to 1.35 to achieve 95% statistical efficiency.

**References**

  - Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172

</div>

The <span class="title-ref">HuberRegressor</span> differs from using <span class="title-ref">SGDRegressor</span> with loss set to <span class="title-ref">huber</span> in the following ways.

  - <span class="title-ref">HuberRegressor</span> is scaling invariant. Once `epsilon` is set, scaling `X` and `y` down or up by different values would produce the same robustness to outliers as before. as compared to <span class="title-ref">SGDRegressor</span> where `epsilon` has to be set again when `X` and `y` are scaled.
  - <span class="title-ref">HuberRegressor</span> should be more efficient to use on data with small number of samples while <span class="title-ref">SGDRegressor</span> needs a number of passes on the training data to produce the same robustness.

Note that this estimator is different from the R implementation of Robust Regression (<https://stats.oarc.ucla.edu/r/dae/robust-regression/>) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold.

## Quantile Regression

Quantile regression estimates the median or other quantiles of \(y\) conditional on \(X\), while ordinary least squares (OLS) estimates the conditional mean.

Quantile regression may be useful if one is interested in predicting an interval instead of point prediction. Sometimes, prediction intervals are calculated based on the assumption that prediction error is distributed normally with zero mean and constant variance. Quantile regression provides sensible prediction intervals even for errors with non-constant (but predictable) variance or non-normal distribution.

![](/auto_examples/linear_model/images/sphx_glr_plot_quantile_regression_002.png)

Based on minimizing the pinball loss, conditional quantiles can also be estimated by models other than linear models. For example, <span class="title-ref">\~sklearn.ensemble.GradientBoostingRegressor</span> can predict conditional quantiles if its parameter `loss` is set to `"quantile"` and parameter `alpha` is set to the quantile that should be predicted. See the example in \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py).

Most implementations of quantile regression are based on linear programming problem. The current implementation is based on <span class="title-ref">scipy.optimize.linprog</span>.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_quantile\_regression.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_quantile\_regression.py)

<div class="dropdown">

Mathematical details

As a linear model, the <span class="title-ref">QuantileRegressor</span> gives linear predictions \(\hat{y}(w, X) = Xw\) for the \(q\)-th quantile, \(q \in (0, 1)\). The weights or coefficients \(w\) are then found by the following minimization problem:

\[\min_{w} {\frac{1}{n_{\text{samples}}}
\sum_i PB_q(y_i - X_i w) + \alpha ||w||_1}.\]

This consists of the pinball loss (also known as linear loss), see also <span class="title-ref">\~sklearn.metrics.mean\_pinball\_loss</span>,

\[\begin{aligned}
PB_q(t) = q \max(t, 0) + (1 - q) \max(-t, 0) =
\begin{cases}
q t, & t > 0, \\
0,    & t = 0, \\
(q-1) t, & t < 0
\end{cases}
\end{aligned}\]

and the L1 penalty controlled by parameter `alpha`, similar to <span class="title-ref">Lasso</span>.

As the pinball loss is only linear in the residuals, quantile regression is much more robust to outliers than squared error based estimation of the mean. Somewhat in between is the <span class="title-ref">HuberRegressor</span>.

</div>

<div class="dropdown">

References

  - Koenker, R., & Bassett Jr, G. (1978). [Regression quantiles.](https://gib.people.uic.edu/RQ.pdf) Econometrica: journal of the Econometric Society, 33-50.
  - Portnoy, S., & Koenker, R. (1997). `The Gaussian hare and the Laplacian
    tortoise: computability of squared-error versus absolute-error estimators.
    Statistical Science, 12, 279-300 <10.1214/ss/1030037960>`.
  - Koenker, R. (2005). `Quantile Regression <10.1017/CBO9780511754098>`. Cambridge University Press.

</div>

## Polynomial regression: extending linear models with basis functions

<div class="currentmodule">

sklearn.preprocessing

</div>

One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.

<div class="dropdown">

Mathematical details

For example, a simple linear regression can be extended by constructing **polynomial features** from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:

\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2\]

If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:

\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\]

The (sometimes surprising) observation is that this is *still a linear model*: to see this, imagine creating a new set of features

\[z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\]

With this re-labeling of the data, our problem can be written

\[\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\]

We see that the resulting *polynomial regression* is in the same class of linear models we considered above (i.e. the model is linear in \(w\)) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.

</div>

Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:

![](../auto_examples/linear_model/images/sphx_glr_plot_polynomial_interpolation_001.png)

This figure is created using the <span class="title-ref">PolynomialFeatures</span> transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:

    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> import numpy as np
    >>> X = np.arange(6).reshape(3, 2)
    >>> X
    array([[0, 1],
           [2, 3],
           [4, 5]])
    >>> poly = PolynomialFeatures(degree=2)
    >>> poly.fit_transform(X)
    array([[ 1.,  0.,  1.,  0.,  0.,  1.],
           [ 1.,  2.,  3.,  4.,  6.,  9.],
           [ 1.,  4.,  5., 16., 20., 25.]])

The features of `X` have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.

This sort of preprocessing can be streamlined with the \[Pipeline \<pipeline\>\](\#pipeline-\<pipeline\>) tools. A single object representing a simple polynomial regression can be created and used as follows:

    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.pipeline import Pipeline
    >>> import numpy as np
    >>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),
    ...                   ('linear', LinearRegression(fit_intercept=False))])
    >>> # fit to an order-3 polynomial data
    >>> x = np.arange(5)
    >>> y = 3 - 2 * x + x ** 2 - x ** 3
    >>> model = model.fit(x[:, np.newaxis], y)
    >>> model.named_steps['linear'].coef_
    array([ 3., -2.,  1., -1.])

The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.

In some cases it's not necessary to include higher powers of any single feature, but only the so-called *interaction features* that multiply together at most \(d\) distinct features. These can be gotten from <span class="title-ref">PolynomialFeatures</span> with the setting `interaction_only=True`.

For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:

    >>> from sklearn.linear_model import Perceptron
    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> import numpy as np
    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    >>> y = X[:, 0] ^ X[:, 1]
    >>> y
    array([0, 1, 1, 0])
    >>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)
    >>> X
    array([[1, 0, 0, 0],
           [1, 0, 1, 0],
           [1, 1, 0, 0],
           [1, 1, 1, 1]])
    >>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,
    ...                  shuffle=False).fit(X, y)

And the classifier "predictions" are perfect:

    >>> clf.predict(X)
    array([0, 1, 1, 0])
    >>> clf.score(X, y)
    1.0

1.  Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1

2.  David Wipf and Srikantan Nagarajan: [A New View of Automatic Relevance Determination](https://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf)

3.  Michael E. Tipping: [Sparse Bayesian Learning and the Relevance Vector Machine](https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf)

4.  Tristan Fletcher: [Relevance Vector Machines Explained](https://citeseerx.ist.psu.edu/doc_view/pid/3dc9d625404fdfef6eaccc3babddefe4c176abd4)

5.  McCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.

6.  Jørgensen, B. (1992). The theory of exponential dispersion models and analysis of deviance. Monografias de matemática, no. 51. See also [Exponential dispersion model.](https://en.wikipedia.org/wiki/Exponential_dispersion_model)

---

manifold.md

---

<div class="currentmodule">

sklearn.manifold

</div>

# Manifold learning

Look for the bare necessities  
The simple bare necessities  
Forget about your worries and your strife  
I mean the bare necessities  
Old Mother Nature's recipes  
That bring the bare necessities of life  
  
            -- Baloo's song \[The Jungle Book\]

![](../auto_examples/manifold/images/sphx_glr_plot_compare_methods_001.png)

<div class="centered">

[![manifold\_img3](../auto_examples/manifold/images/sphx_glr_plot_compare_methods_003.png)](../auto_examples/manifold/plot_compare_methods.html) [![manifold\_img4](../auto_examples/manifold/images/sphx_glr_plot_compare_methods_004.png)](../auto_examples/manifold/plot_compare_methods.html) [![manifold\_img5](../auto_examples/manifold/images/sphx_glr_plot_compare_methods_005.png)](../auto_examples/manifold/plot_compare_methods.html) [![manifold\_img6](../auto_examples/manifold/images/sphx_glr_plot_compare_methods_006.png)](../auto_examples/manifold/plot_compare_methods.html)

</div>

Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.

## Introduction

High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.

The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.

<div class="centered">

[![digits\_img](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_001.png)](../auto_examples/manifold/plot_lle_digits.html) [![projected\_img](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_002.png)](../auto_examples/manifold/plot_lle_digits.html)

</div>

To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an "interesting" linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data.

<div class="centered">

[![PCA\_img](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_003.png)](../auto_examples/manifold/plot_lle_digits.html) [![LDA\_img](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_004.png)](../auto_examples/manifold/plot_lle_digits.html)

</div>

Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_manifold\_plot\_lle\_digits.py\](\#sphx\_glr\_auto\_examples\_manifold\_plot\_lle\_digits.py) for an example of dimensionality reduction on handwritten digits.
  - See \[sphx\_glr\_auto\_examples\_manifold\_plot\_compare\_methods.py\](\#sphx\_glr\_auto\_examples\_manifold\_plot\_compare\_methods.py) for an example of dimensionality reduction on a toy "S-curve" dataset.
  - See \[sphx\_glr\_auto\_examples\_applications\_plot\_stock\_market.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_stock\_market.py) for an example of using manifold learning to map the stock market structure based on historical stock prices.

The manifold learning implementations available in scikit-learn are summarized below

## Isomap

One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object <span class="title-ref">Isomap</span>.

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_005.png)

<div class="dropdown">

Complexity

The Isomap algorithm comprises three stages:

1.  **Nearest neighbor search.** Isomap uses <span class="title-ref">\~sklearn.neighbors.BallTree</span> for efficient neighbor search. The cost is approximately \(O[D \log(k) N \log(N)]\), for \(k\) nearest neighbors of \(N\) points in \(D\) dimensions.
2.  **Shortest-path graph search.** The most efficient known algorithms for this are *Dijkstra's Algorithm*, which is approximately \(O[N^2(k + \log(N))]\), or the *Floyd-Warshall algorithm*, which is \(O[N^3]\). The algorithm can be selected by the user with the `path_method` keyword of `Isomap`. If unspecified, the code attempts to choose the best algorithm for the input data.
3.  **Partial eigenvalue decomposition.** The embedding is encoded in the eigenvectors corresponding to the \(d\) largest eigenvalues of the \(N \times N\) isomap kernel. For a dense solver, the cost is approximately \(O[d N^2]\). This cost can often be improved using the `ARPACK` solver. The eigensolver can be specified by the user with the `eigen_solver` keyword of `Isomap`. If unspecified, the code attempts to choose the best algorithm for the input data.

The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).

  - \(N\) : number of training data points
  - \(D\) : input dimension
  - \(k\) : number of nearest neighbors
  - \(d\) : output dimension

</div>

**References**

  - ["A global geometric framework for nonlinear dimensionality reduction"](http://science.sciencemag.org/content/290/5500/2319.full) Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. Science 290 (5500)

## Locally Linear Embedding

Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.

Locally linear embedding can be performed with function <span class="title-ref">locally\_linear\_embedding</span> or its object-oriented counterpart <span class="title-ref">LocallyLinearEmbedding</span>.

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_006.png)

<div class="dropdown">

Complexity

The standard LLE algorithm comprises three stages:

1.  **Nearest Neighbors Search**. See discussion under Isomap above.
2.  **Weight Matrix Construction**. \(O[D N k^3]\). The construction of the LLE weight matrix involves the solution of a \(k \times k\) linear equation for each of the \(N\) local neighborhoods.
3.  **Partial Eigenvalue Decomposition**. See discussion under Isomap above.

The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).

  - \(N\) : number of training data points
  - \(D\) : input dimension
  - \(k\) : number of nearest neighbors
  - \(d\) : output dimension

</div>

**References**

  - ["Nonlinear dimensionality reduction by locally linear embedding"](http://www.sciencemag.org/content/290/5500/2323.full) Roweis, S. & Saul, L. Science 290:2323 (2000)

## Modified Locally Linear Embedding

One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient. To address this, standard LLE applies an arbitrary regularization parameter \(r\), which is chosen relative to the trace of the local weight matrix. Though it can be shown formally that as \(r \to 0\), the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for \(r > 0\). This problem manifests itself in embeddings which distort the underlying geometry of the manifold.

One method to address the regularization problem is to use multiple weight vectors in each neighborhood. This is the essence of *modified locally linear embedding* (MLLE). MLLE can be performed with function <span class="title-ref">locally\_linear\_embedding</span> or its object-oriented counterpart <span class="title-ref">LocallyLinearEmbedding</span>, with the keyword `method = 'modified'`. It requires `n_neighbors > n_components`.

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_007.png)

<div class="dropdown">

Complexity

The MLLE algorithm comprises three stages:

1.  **Nearest Neighbors Search**. Same as standard LLE
2.  **Weight Matrix Construction**. Approximately \(O[D N k^3] + O[N (k-D) k^2]\). The first term is exactly equivalent to that of standard LLE. The second term has to do with constructing the weight matrix from multiple weights. In practice, the added cost of constructing the MLLE weight matrix is relatively small compared to the cost of stages 1 and 3.
3.  **Partial Eigenvalue Decomposition**. Same as standard LLE

The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).

  - \(N\) : number of training data points
  - \(D\) : input dimension
  - \(k\) : number of nearest neighbors
  - \(d\) : output dimension

</div>

**References**

  - ["MLLE: Modified Locally Linear Embedding Using Multiple Weights"](https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3) Zhang, Z. & Wang, J.

## Hessian Eigenmapping

Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, `sklearn` implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function <span class="title-ref">locally\_linear\_embedding</span> or its object-oriented counterpart <span class="title-ref">LocallyLinearEmbedding</span>, with the keyword `method = 'hessian'`. It requires `n_neighbors > n_components * (n_components + 3) / 2`.

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_008.png)

<div class="dropdown">

Complexity

The HLLE algorithm comprises three stages:

1.  **Nearest Neighbors Search**. Same as standard LLE
2.  **Weight Matrix Construction**. Approximately \(O[D N k^3] + O[N d^6]\). The first term reflects a similar cost to that of standard LLE. The second term comes from a QR decomposition of the local hessian estimator.
3.  **Partial Eigenvalue Decomposition**. Same as standard LLE.

The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).

  - \(N\) : number of training data points
  - \(D\) : input dimension
  - \(k\) : number of nearest neighbors
  - \(d\) : output dimension

</div>

**References**

  - ["Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data"](http://www.pnas.org/content/100/10/5591) Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)

## Spectral Embedding

Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function <span class="title-ref">spectral\_embedding</span> or its object-oriented counterpart <span class="title-ref">SpectralEmbedding</span>.

<div class="dropdown">

Complexity

The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:

1.  **Weighted Graph Construction**. Transform the raw input data into graph representation using affinity (adjacency) matrix representation.
2.  **Graph Laplacian Construction**. unnormalized Graph Laplacian is constructed as \(L = D - A\) for and normalized one as \(L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}\).
3.  **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is done on graph Laplacian.

The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).

  - \(N\) : number of training data points
  - \(D\) : input dimension
  - \(k\) : number of nearest neighbors
  - \(d\) : output dimension

</div>

**References**

  - ["Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"](https://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf)
    13. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396

## Local Tangent Space Alignment

Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function <span class="title-ref">locally\_linear\_embedding</span> or its object-oriented counterpart <span class="title-ref">LocallyLinearEmbedding</span>, with the keyword `method = 'ltsa'`.

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_009.png)

<div class="dropdown">

Complexity

The LTSA algorithm comprises three stages:

1.  **Nearest Neighbors Search**. Same as standard LLE
2.  **Weight Matrix Construction**. Approximately \(O[D N k^3] + O[k^2 d]\). The first term reflects a similar cost to that of standard LLE.
3.  **Partial Eigenvalue Decomposition**. Same as standard LLE

The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).

  - \(N\) : number of training data points
  - \(D\) : input dimension
  - \(k\) : number of nearest neighbors
  - \(d\) : output dimension

</div>

**References**

  - `"Principal manifolds and nonlinear dimensionality reduction via
    tangent space alignment"
    <cs/0212008>` Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)

## Multi-dimensional Scaling (MDS)

[Multidimensional scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling) (<span class="title-ref">MDS</span>) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.

In general, <span class="title-ref">MDS</span> is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.

There exists two types of MDS algorithm: metric and non metric. In scikit-learn, the class <span class="title-ref">MDS</span> implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_010.png)

Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(\sum_{i < j} d_{ij}(X) - \hat{d}_{ij}(X)\)

<div class="dropdown">

Metric MDS

The simplest metric <span class="title-ref">MDS</span> model, called *absolute MDS*, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.

Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).

</div>

<div class="dropdown">

Nonmetric MDS

Non metric <span class="title-ref">MDS</span> focuses on the ordination of the data. If \(S_{ij} > S_{jk}\), then the embedding should enforce \(d_{ij} <
d_{jk}\). For this reason, we discuss it in terms of dissimilarities (\(\delta_{ij}\)) instead of similarities (\(S_{ij}\)). Note that dissimilarities can easily be obtained from similarities through a simple transform, e.g. \(\delta_{ij}=c_1-c_2 S_{ij}\) for some real constants \(c_1, c_2\). A simple algorithm to enforce proper ordination is to use a monotonic regression of \(d_{ij}\) on \(\delta_{ij}\), yielding disparities \(\hat{d}_{ij}\) in the same order as \(\delta_{ij}\).

A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities \(\hat{d}_{ij}\) are normalized. Note that since we only care about relative ordering, our objective should be invariant to simple translation and scaling, however the stress used in metric MDS is sensitive to scaling. To address this, non-metric MDS may use a normalized stress, known as Stress-1 defined as

\[\sqrt{\frac{\sum_{i < j} (d_{ij} - \hat{d}_{ij})^2}{\sum_{i < j} d_{ij}^2}}.\]

The use of normalized Stress-1 can be enabled by setting <span class="title-ref">normalized\_stress=True</span>, however it is only compatible with the non-metric MDS problem and will be ignored in the metric case.

![](../auto_examples/manifold/images/sphx_glr_plot_mds_001.png)

</div>

**References**

  - ["Modern Multidimensional Scaling - Theory and Applications"](https://www.springer.com/fr/book/9780387251509) Borg, I.; Groenen P. Springer Series in Statistics (1997)
  - ["Nonmetric multidimensional scaling: a numerical method"](http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964b.pdf) Kruskal, J. Psychometrika, 29 (1964)
  - ["Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis"](http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964a.pdf) Kruskal, J. Psychometrika, 29, (1964)

## t-distributed Stochastic Neighbor Embedding (t-SNE)

t-SNE (<span class="title-ref">TSNE</span>) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student's t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:

  - Revealing the structure at many scales on a single map
  - Revealing data that lie in multiple, different, manifolds or clusters
  - Reducing the tendency to crowd points together at the center

While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.

The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.

The disadvantages to using t-SNE are roughly:

  - t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes
  - The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.
  - The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.
  - Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using <span class="title-ref">init='pca'</span>).

![](../auto_examples/manifold/images/sphx_glr_plot_lle_digits_013.png)

<div class="dropdown">

Optimizing t-SNE

The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.

Optimizing the KL divergence can be a little bit tricky sometimes. There are five parameters that control the optimization of t-SNE and therefore possibly the quality of the resulting embedding:

  - perplexity
  - early exaggeration factor
  - learning rate
  - maximum number of iterations
  - angle (not used in the exact method)

The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.

The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. A heuristic suggested in Belkina et al. (2019) is to set the learning rate to the sample size divided by the early exaggeration factor. We implement this heuristic as <span class="title-ref">learning\_rate='auto'</span> argument. More tips can be found in Laurens van der Maaten's FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.

["How to Use t-SNE Effectively"](https://distill.pub/2016/misread-tsne/) provides a good discussion of the effects of the various parameters, as well as interactive plots to explore the effects of different parameters.

</div>

<div class="dropdown">

Barnes-Hut t-SNE

The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:

  - The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.
  - Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using <span class="title-ref">\~sklearn.decomposition.PCA</span>
  - Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when method="exact"
  - Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable

For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.

Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not high enough to accurately represent the internal structure of the data.

</div>

**References**

  - ["Visualizing High-Dimensional Data Using t-SNE"](https://jmlr.org/papers/v9/vandermaaten08a.html) van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)
  - ["t-Distributed Stochastic Neighbor Embedding"](https://lvdmaaten.github.io/tsne/) van der Maaten, L.J.P.
  - ["Accelerating t-SNE using Tree-Based Algorithms"](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf) van der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014.
  - ["Automated optimized parameters for T-distributed stochastic neighbor embedding improve visualization and analysis of large datasets"](https://www.nature.com/articles/s41467-019-13055-y) Belkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J., Snyder-Cappione, J.E., Nature Communications 10, 5415 (2019).

## Tips on practical use

  - Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See \[StandardScaler \<preprocessing\_scaler\>\](\#standardscaler-\<preprocessing\_scaler\>) for convenient ways of scaling heterogeneous data.
  - The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as `n_components` is increased until `n_components == d`.
  - Note that noisy data can "short-circuit" the manifold, in essence acting as a bridge between parts of the manifold that would otherwise be well-separated. Manifold learning on noisy and/or incomplete data is an active area of research.
  - Certain input configurations can lead to singular weight matrices, for example when more than two points in the dataset are identical, or when the data is split into disjointed groups. In this case, `solver='arpack'` will fail to find the null space. The easiest way to address this is to use `solver='dense'` which will work on a singular matrix, though it may be very slow depending on the number of input points. Alternatively, one can attempt to understand the source of the singularity: if it is due to disjoint sets, increasing `n_neighbors` may help. If it is due to identical points in the dataset, removing these points may help.

<div class="seealso">

\[random\_trees\_embedding\](\#random\_trees\_embedding) can also be useful to derive non-linear representations of feature space, also it does not perform dimensionality reduction.

</div>

---

metrics.md

---

# Pairwise metrics, Affinities and Kernels

The `sklearn.metrics.pairwise` submodule implements utilities to evaluate pairwise distances or affinity of sets of samples.

This module contains both distance metrics and kernels. A brief summary is given on the two here.

Distance metrics are functions `d(a, b)` such that `d(a, b) < d(a, c)` if objects `a` and `b` are considered "more similar" than objects `a` and `c`. Two objects exactly alike would have a distance of zero. One of the most popular examples is Euclidean distance. To be a 'true' metric, it must obey the following four conditions:

    1. d(a, b) >= 0, for all a and b
    2. d(a, b) == 0, if and only if a = b, positive definiteness
    3. d(a, b) == d(b, a), symmetry
    4. d(a, c) <= d(a, b) + d(b, c), the triangle inequality

Kernels are measures of similarity, i.e. `s(a, b) > s(a, c)` if objects `a` and `b` are considered "more similar" than objects `a` and `c`. A kernel must also be positive semi-definite.

There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let `D` be the distance, and `S` be the kernel:

1.    - `S = np.exp(-D * gamma)`, where one heuristic for choosing  
        `gamma` is `1 / num_features`

2.  `S = 1. / (D / np.max(D))`

<div class="currentmodule">

sklearn.metrics

</div>

The distances between the row vectors of `X` and the row vectors of `Y` can be evaluated using <span class="title-ref">pairwise\_distances</span>. If `Y` is omitted the pairwise distances of the row vectors of `X` are calculated. Similarly, <span class="title-ref">pairwise.pairwise\_kernels</span> can be used to calculate the kernel between <span class="title-ref">X</span> and <span class="title-ref">Y</span> using different kernel functions. See the API reference for more details.

> \>\>\> import numpy as np \>\>\> from sklearn.metrics import pairwise\_distances \>\>\> from sklearn.metrics.pairwise import pairwise\_kernels \>\>\> X = np.array(\[\[2, 3\], \[3, 5\], \[5, 8\]\]) \>\>\> Y = np.array(\[\[1, 0\], \[2, 1\]\]) \>\>\> pairwise\_distances(X, Y, metric='manhattan') array(\[\[ 4., 2.\], \[ 7., 5.\], \[12., 10.\]\]) \>\>\> pairwise\_distances(X, metric='manhattan') array(\[\[0., 3., 8.\], \[3., 0., 5.\], \[8., 5., 0.\]\]) \>\>\> pairwise\_kernels(X, Y, metric='linear') array(\[\[ 2., 7.\], \[ 3., 11.\], \[ 5., 18.\]\])

<div class="currentmodule">

sklearn.metrics.pairwise

</div>

## Cosine similarity

<span class="title-ref">cosine\_similarity</span> computes the L2-normalized dot product of vectors. That is, if \(x\) and \(y\) are row vectors, their cosine similarity \(k\) is defined as:

\[k(x, y) = \frac{x y^\top}{\|x\| \|y\|}\]

This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.

This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. <span class="title-ref">cosine\_similarity</span> accepts `scipy.sparse` matrices. (Note that the tf-idf functionality in `sklearn.feature_extraction.text` can produce normalized vectors, in which case <span class="title-ref">cosine\_similarity</span> is equivalent to <span class="title-ref">linear\_kernel</span>, only slower.)

**References**

  - C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. <https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html>

## Linear kernel

The function <span class="title-ref">linear\_kernel</span> computes the linear kernel, that is, a special case of <span class="title-ref">polynomial\_kernel</span> with `degree=1` and `coef0=0` (homogeneous). If `x` and `y` are column vectors, their linear kernel is:

\[k(x, y) = x^\top y\]

## Polynomial kernel

The function <span class="title-ref">polynomial\_kernel</span> computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.

The polynomial kernel is defined as:

\[k(x, y) = (\gamma x^\top y +c_0)^d\]

where:

  - `x`, `y` are the input vectors
  - `d` is the kernel degree

If \(c_0 = 0\) the kernel is said to be homogeneous.

## Sigmoid kernel

The function <span class="title-ref">sigmoid\_kernel</span> computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:

\[k(x, y) = \tanh( \gamma x^\top y + c_0)\]

where:

  - `x`, `y` are the input vectors
  - \(\gamma\) is known as slope
  - \(c_0\) is known as intercept

## RBF kernel

The function <span class="title-ref">rbf\_kernel</span> computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:

\[k(x, y) = \exp( -\gamma \| x-y \|^2)\]

where `x` and `y` are the input vectors. If \(\gamma = \sigma^{-2}\) the kernel is known as the Gaussian kernel of variance \(\sigma^2\).

## Laplacian kernel

The function <span class="title-ref">laplacian\_kernel</span> is a variant on the radial basis function kernel defined as:

\[k(x, y) = \exp( -\gamma \| x-y \|_1)\]

where `x` and `y` are the input vectors and \(\|x-y\|_1\) is the Manhattan distance between the input vectors.

It has proven useful in ML applied to noiseless data. See e.g. [Machine learning for quantum mechanics in a nutshell](https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/).

## Chi-squared kernel

The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using <span class="title-ref">chi2\_kernel</span> and then passed to an <span class="title-ref">\~sklearn.svm.SVC</span> with `kernel="precomputed"`:

    >>> from sklearn.svm import SVC
    >>> from sklearn.metrics.pairwise import chi2_kernel
    >>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]
    >>> y = [0, 1, 0, 1]
    >>> K = chi2_kernel(X, gamma=.5)
    >>> K
    array([[1.        , 0.36787944, 0.89483932, 0.58364548],
           [0.36787944, 1.        , 0.51341712, 0.83822343],
           [0.89483932, 0.51341712, 1.        , 0.7768366 ],
           [0.58364548, 0.83822343, 0.7768366 , 1.        ]])
    
    >>> svm = SVC(kernel='precomputed').fit(K, y)
    >>> svm.predict(K)
    array([0, 1, 0, 1])

It can also be directly used as the `kernel` argument:

    >>> svm = SVC(kernel=chi2_kernel).fit(X, y)
    >>> svm.predict(X)
    array([0, 1, 0, 1])

The chi squared kernel is given by

\[k(x, y) = \exp \left (-\gamma \sum_i \frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \right )\]

The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.

The chi squared kernel is most commonly used on histograms (bags) of visual words.

**References**

  - Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 <https://hal.archives-ouvertes.fr/hal-00171412/document>

---

mixture.md

---

# Gaussian mixture models<span id="mixture"></span>

<div class="currentmodule">

sklearn.mixture

</div>

`sklearn.mixture` is a package which enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and full covariance matrices supported), sample them, and estimate them from data. Facilities to help determine the appropriate number of components are also provided.

![**Two-component Gaussian mixture model:** *data points, and equi-probability surfaces of the model.*](../auto_examples/mixture/images/sphx_glr_plot_gmm_pdf_001.png)

A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.

Scikit-learn implements different classes to estimate Gaussian mixture models, that correspond to different estimation strategies, detailed below.

## Gaussian Mixture

The <span class="title-ref">GaussianMixture</span> object implements the \[expectation-maximization \<expectation\_maximization\>\](\#expectation-maximization-\<expectation\_maximization\>) (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A <span class="title-ref">GaussianMixture.fit</span> method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it most probably belongs to using the <span class="title-ref">GaussianMixture.predict</span> method.

The <span class="title-ref">GaussianMixture</span> comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.

![](../auto_examples/mixture/images/sphx_glr_plot_gmm_covariances_001.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_covariances.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_covariances.py) for an example of using the Gaussian mixture as clustering on the iris dataset.
  - See \[sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_pdf.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_pdf.py) for an example on plotting the density estimation.

<div class="dropdown">

Pros and cons of class GaussianMixture

**Pros**

  - Speed  
    It is the fastest algorithm for learning mixture models

  - Agnostic  
    As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.

**Cons**

  - Singularities  
    When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially.

  - Number of components  
    This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.

</div>

<div class="dropdown">

Selecting the number of components in a classical Gaussian Mixture model

The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a \[Variational Bayesian Gaussian mixture \<bgmm\>\](\#variational-bayesian-gaussian-mixture-\<bgmm\>) avoids the specification of the number of components for a Gaussian mixture model.

![](../auto_examples/mixture/images/sphx_glr_plot_gmm_selection_002.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_selection.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_selection.py) for an example of model selection performed with classical Gaussian mixture.

</div>

<div id="expectation_maximization">

<div class="dropdown">

Estimation algorithm expectation-maximization

The main difficulty in learning Gaussian mixture models from unlabeled data is that one usually doesn't know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). [Expectation-maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.

</div>

</div>

<div class="dropdown">

Choice of the Initialization method

There is a choice of four initialization methods (as well as inputting user defined initial means) to generate the initial centers for the model components:

  - k-means (default)  
    This applies a traditional k-means clustering algorithm. This can be computationally expensive compared to other initialization methods.

  - k-means++  
    This uses the initialization method of k-means clustering: k-means++. This will pick the first center at random from the data. Subsequent centers will be chosen from a weighted distribution of the data favouring points further away from existing centers. k-means++ is the default initialization for k-means so will be quicker than running a full k-means but can still take a significant amount of time for large data sets with many components.

  - random\_from\_data  
    This will pick random data points from the input data as the initial centers. This is a very fast method of initialization but can produce non-convergent results if the chosen points are too close to each other.

  - random  
    Centers are chosen as a small perturbation away from the mean of all data. This method is simple but can lead to the model taking longer to converge.

![](../auto_examples/mixture/images/sphx_glr_plot_gmm_init_001.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_init.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_init.py) for an example of using different initializations in Gaussian Mixture.

</div>

## Variational Bayesian Gaussian Mixture

The <span class="title-ref">BayesianGaussianMixture</span> object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar to the one defined by <span class="title-ref">GaussianMixture</span>.

<div id="variational_inference">

**Estimation algorithm: variational inference**

</div>

Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical.

Due to its Bayesian nature, the variational algorithm needs more hyperparameters than expectation-maximization, the most important of these being the concentration parameter `weight_concentration_prior`. Specifying a low value for the concentration prior will make the model put most of the weight on a few components and set the remaining components' weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.

The parameters implementation of the <span class="title-ref">BayesianGaussianMixture</span> class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.

The next figure compares the results obtained for the different type of the weight concentration prior (parameter `weight_concentration_prior_type`) for different values of `weight_concentration_prior`. Here, we can see the value of the `weight_concentration_prior` parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is 'dirichlet\_distribution' while this is not necessarily the case for the 'dirichlet\_process' type (used by default).

<div class="centered">

[![plot\_bgmm](../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_001.png)](../auto_examples/mixture/plot_concentration_prior.html) [![plot\_dpgmm](../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_002.png)](../auto_examples/mixture/plot_concentration_prior.html)

</div>

The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected `n_components=5` which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.

![](../auto_examples/mixture/images/sphx_glr_plot_gmm_001.png)

On the following figure we are fitting a dataset not well-depicted by a Gaussian mixture. Adjusting the `weight_concentration_prior`, parameter of the <span class="title-ref">BayesianGaussianMixture</span> controls the number of components used to fit this data. We also present on the last two plots a random sampling generated from the two resulting mixtures.

![](../auto_examples/mixture/images/sphx_glr_plot_gmm_sin_001.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_mixture\_plot\_gmm.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_gmm.py) for an example on plotting the confidence ellipsoids for both <span class="title-ref">GaussianMixture</span> and <span class="title-ref">BayesianGaussianMixture</span>.
  - \[sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_sin.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_gmm\_sin.py) shows using <span class="title-ref">GaussianMixture</span> and <span class="title-ref">BayesianGaussianMixture</span> to fit a sine wave.
  - See \[sphx\_glr\_auto\_examples\_mixture\_plot\_concentration\_prior.py\](\#sphx\_glr\_auto\_examples\_mixture\_plot\_concentration\_prior.py) for an example plotting the confidence ellipsoids for the <span class="title-ref">BayesianGaussianMixture</span> with different `weight_concentration_prior_type` for different values of the parameter `weight_concentration_prior`.

<div class="dropdown">

Pros and cons of variational inference with BayesianGaussianMixture

**Pros**

  - Automatic selection  
    When `weight_concentration_prior` is small enough and `n_components` is larger than what is found necessary by the model, the Variational Bayesian mixture model has a natural tendency to set some mixture weights values close to zero. This makes it possible to let the model choose a suitable number of effective components automatically. Only an upper bound of this number needs to be provided. Note however that the "ideal" number of active components is very application specific and is typically ill-defined in a data exploration setting.

  - Less sensitivity to the number of parameters  
    Unlike finite models, which will almost always use all components as much as they can, and hence will produce wildly different solutions for different numbers of components, the variational inference with a Dirichlet process prior (`weight_concentration_prior_type='dirichlet_process'`) won't change much with changes to the parameters, leading to more stability and less tuning.

  - Regularization  
    Due to the incorporation of prior information, variational solutions have less pathological special cases than expectation-maximization solutions.

**Cons**

  - Speed  
    The extra parametrization necessary for variational inference makes inference slower, although not by much.

  - Hyperparameters  
    This algorithm needs an extra hyperparameter that might need experimental tuning via cross-validation.

  - Bias  
    There are many implicit biases in the inference algorithms (and also in the Dirichlet process if used), and whenever there is a mismatch between these biases and the data it might be possible to fit better models using a finite mixture.

</div>

### The Dirichlet Process

Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on *clusterings with an infinite, unbounded, number of partitions*. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.

An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesn't fit this manual, one can think of its [stick breaking process](https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process) analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that don't fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller values of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights).

Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the "true" number of components, affects only algorithmic complexity, not the actual number of components used).

---

model_evaluation.md

---

<div class="currentmodule">

sklearn

</div>

# Metrics and scoring: quantifying the quality of predictions

There are 3 different APIs for evaluating the quality of a model's predictions:

  - **Estimator score method**: Estimators have a `score` method providing a default evaluation criterion for the problem they are designed to solve. This is not discussed on this page, but in each estimator's documentation.
  - **Scoring parameter**: Model-evaluation tools using \[cross-validation \<cross\_validation\>\](\#cross-validation-\<cross\_validation\>) (such as <span class="title-ref">model\_selection.cross\_val\_score</span> and <span class="title-ref">model\_selection.GridSearchCV</span>) rely on an internal *scoring* strategy. This is discussed in the section \[scoring\_parameter\](\#scoring\_parameter).
  - **Metric functions**: The `sklearn.metrics` module implements functions assessing prediction error for specific purposes. These metrics are detailed in sections on \[classification\_metrics\](\#classification\_metrics), \[multilabel\_ranking\_metrics\](\#multilabel\_ranking\_metrics), \[regression\_metrics\](\#regression\_metrics) and \[clustering\_metrics\](\#clustering\_metrics).

Finally, \[dummy\_estimators\](\#dummy\_estimators) are useful to get a baseline value of those metrics for random predictions.

<div class="seealso">

For "pairwise" metrics, between *samples* and not estimators or predictions, see the \[metrics\](\#metrics) section.

</div>

## The `scoring` parameter: defining model evaluation rules

Model selection and evaluation using tools, such as <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.cross\_val\_score</span>, take a `scoring` parameter that controls what metric they apply to the estimators evaluated.

### Common cases: predefined values

For the most common use cases, you can designate a scorer object with the `scoring` parameter; the table below shows all possible values. All scorer objects follow the convention that **higher return values are better than lower return values**. Thus metrics which measure the distance between the model and the data, like <span class="title-ref">metrics.mean\_squared\_error</span>, are available as neg\_mean\_squared\_error which return the negated value of the metric.

<table>
<thead>
<tr class="header">
<th>Scoring</th>
<th>Function</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><strong>Classification</strong> 'accuracy' 'balanced_accuracy' 'top_k_accuracy' 'average_precision' 'neg_brier_score'</p></td>
<td><p><span class="title-ref">metrics.accuracy_score</span> <span class="title-ref">metrics.balanced_accuracy_score</span> <span class="title-ref">metrics.top_k_accuracy_score</span> <span class="title-ref">metrics.average_precision_score</span> <span class="title-ref">metrics.brier_score_loss</span></p></td>
<td></td>
</tr>
<tr class="even">
<td>'f1'</td>
<td><span class="title-ref">metrics.f1_score</span> for bi</td>
<td>nary targets</td>
</tr>
<tr class="odd">
<td>'f1_micro'</td>
<td><span class="title-ref">metrics.f1_score</span> micro-</td>
<td>averaged</td>
</tr>
<tr class="even">
<td>'f1_macro'</td>
<td><span class="title-ref">metrics.f1_score</span> macro-</td>
<td>averaged</td>
</tr>
<tr class="odd">
<td>'f1_weighted'</td>
<td><span class="title-ref">metrics.f1_score</span> weight</td>
<td>ed average</td>
</tr>
<tr class="even">
<td>'f1_samples'</td>
<td><span class="title-ref">metrics.f1_score</span> by mul</td>
<td>tilabel sample</td>
</tr>
<tr class="odd">
<td>'neg_log_loss'</td>
<td><span class="title-ref">metrics.log_loss</span> requir</td>
<td>es <code>predict_proba</code> support</td>
</tr>
<tr class="even">
<td>'precision' etc.</td>
<td><span class="title-ref">metrics.precision_score</span> suffix</td>
<td>es apply as with 'f1'</td>
</tr>
<tr class="odd">
<td>'recall' etc.</td>
<td><span class="title-ref">metrics.recall_score</span> suffix</td>
<td>es apply as with 'f1'</td>
</tr>
<tr class="even">
<td><p>'jaccard' etc. 'roc_auc' 'roc_auc_ovr' 'roc_auc_ovo' 'roc_auc_ovr_weighted' 'roc_auc_ovo_weighted' 'd2_log_loss_score'</p>
<p><strong>Clustering</strong> 'adjusted_mutual_info_score' 'adjusted_rand_score' 'completeness_score' 'fowlkes_mallows_score' 'homogeneity_score' 'mutual_info_score' 'normalized_mutual_info_score' 'rand_score' 'v_measure_score'</p>
<p><strong>Regression</strong> 'explained_variance' 'neg_max_error' 'neg_mean_absolute_error' 'neg_mean_squared_error' 'neg_root_mean_squared_error' 'neg_mean_squared_log_error' 'neg_root_mean_squared_log_error' 'neg_median_absolute_error' 'r2' 'neg_mean_poisson_deviance' 'neg_mean_gamma_deviance' 'neg_mean_absolute_percentage_error' 'd2_absolute_error_score'</p></td>
<td><p><span class="title-ref">metrics.jaccard_score</span> suffix <span class="title-ref">metrics.roc_auc_score</span> <span class="title-ref">metrics.roc_auc_score</span> <span class="title-ref">metrics.roc_auc_score</span> <span class="title-ref">metrics.roc_auc_score</span> <span class="title-ref">metrics.roc_auc_score</span> <span class="title-ref">metrics.d2_log_loss_score</span></p>
<p><span class="title-ref">metrics.adjusted_mutual_info_score</span> <span class="title-ref">metrics.adjusted_rand_score</span> <span class="title-ref">metrics.completeness_score</span> <span class="title-ref">metrics.fowlkes_mallows_score</span> <span class="title-ref">metrics.homogeneity_score</span> <span class="title-ref">metrics.mutual_info_score</span> <span class="title-ref">metrics.normalized_mutual_info_score</span> <span class="title-ref">metrics.rand_score</span> <span class="title-ref">metrics.v_measure_score</span></p>
<p><span class="title-ref">metrics.explained_variance_score</span> <span class="title-ref">metrics.max_error</span> <span class="title-ref">metrics.mean_absolute_error</span> <span class="title-ref">metrics.mean_squared_error</span> <span class="title-ref">metrics.root_mean_squared_error</span> <span class="title-ref">metrics.mean_squared_log_error</span> <span class="title-ref">metrics.root_mean_squared_log_error</span> <span class="title-ref">metrics.median_absolute_error</span> <span class="title-ref">metrics.r2_score</span> <span class="title-ref">metrics.mean_poisson_deviance</span> <span class="title-ref">metrics.mean_gamma_deviance</span> <span class="title-ref">metrics.mean_absolute_percentage_error</span> <span class="title-ref">metrics.d2_absolute_error_score</span></p></td>
<td><p>es apply as with 'f1'</p></td>
</tr>
</tbody>
</table>

Usage examples:

> \>\>\> from sklearn import svm, datasets \>\>\> from sklearn.model\_selection import cross\_val\_score \>\>\> X, y = datasets.load\_iris(return\_X\_y=True) \>\>\> clf = svm.SVC(random\_state=0) \>\>\> cross\_val\_score(clf, X, y, cv=5, scoring='recall\_macro') array(\[0.96..., 0.96..., 0.96..., 0.93..., 1. \])

\> **Note** \> If a wrong scoring name is passed, an `InvalidParameterError` is raised. You can retrieve the names of all available scorers by calling <span class="title-ref">\~sklearn.metrics.get\_scorer\_names</span>.

<div class="currentmodule">

sklearn.metrics

</div>

### Defining your scoring strategy from metric functions

The following metrics functions are not implemented as named scorers, sometimes because they require additional parameters, such as <span class="title-ref">fbeta\_score</span>. They cannot be passed to the `scoring` parameters; instead their callable needs to be passed to <span class="title-ref">make\_scorer</span> together with the value of the user-settable parameters.

<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Parameter</th>
<th>Example usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Classification</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><p><span class="title-ref">metrics.fbeta_score</span> ``beta</p>
<p><strong>Regression</strong></p></td>
<td><p><code></code>make</p></td>
<td><p>_scorer(fbeta_score, beta=2)``</p></td>
</tr>
<tr class="odd">
<td><span class="title-ref">metrics.mean_tweedie_deviance</span> ``powe</td>
<td>r<code></code>make</td>
<td>_scorer(mean_tweedie_deviance, power=1.5)``</td>
</tr>
<tr class="even">
<td><span class="title-ref">metrics.mean_pinball_loss</span> ``alph</td>
<td>a<code></code>make</td>
<td>_scorer(mean_pinball_loss, alpha=0.95)``</td>
</tr>
<tr class="odd">
<td><span class="title-ref">metrics.d2_tweedie_score</span> ``powe</td>
<td>r<code></code>make</td>
<td>_scorer(d2_tweedie_score, power=1.5)``</td>
</tr>
<tr class="even">
<td><span class="title-ref">metrics.d2_pinball_score</span> ``alph</td>
<td>a<code></code>make</td>
<td>_scorer(d2_pinball_score, alpha=0.95)``</td>
</tr>
</tbody>
</table>

One typical use case is to wrap an existing metric function from the library with non-default values for its parameters, such as the `beta` parameter for the <span class="title-ref">fbeta\_score</span> function:

    >>> from sklearn.metrics import fbeta_score, make_scorer
    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
    >>> from sklearn.model_selection import GridSearchCV
    >>> from sklearn.svm import LinearSVC
    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
    ...                     scoring=ftwo_scorer, cv=5)

The module `sklearn.metrics` also exposes a set of simple functions measuring a prediction error given ground truth and prediction:

  - functions ending with `_score` return a value to maximize, the higher the better.
  - functions ending with `_error`, `_loss`, or `_deviance` return a value to minimize, the lower the better. When converting into a scorer object using <span class="title-ref">make\_scorer</span>, set the `greater_is_better` parameter to `False` (`True` by default; see the parameter description below).

<div class="dropdown">

Custom scorer objects

The second use case is to build a completely custom scorer object from a simple python function using <span class="title-ref">make\_scorer</span>, which can take several parameters:

  - the python function you want to use (`my_custom_loss_func` in the example below)
  - whether the python function returns a score (`greater_is_better=True`, the default) or a loss (`greater_is_better=False`). If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models.
  - for classification metrics only: whether the python function you provided requires continuous decision certainties. If the scoring function only accepts probability estimates (e.g. <span class="title-ref">metrics.log\_loss</span>) then one needs to set the parameter <span class="title-ref">response\_method</span>, thus in this case <span class="title-ref">response\_method="predict\_proba"</span>. Some scoring function do not necessarily require probability estimates but rather non-thresholded decision values (e.g. <span class="title-ref">metrics.roc\_auc\_score</span>). In this case, one provides a list such as <span class="title-ref">response\_method=\["decision\_function", "predict\_proba"\]</span>. In this case, the scorer will use the first available method, in the order given in the list, to compute the scores.
  - any additional parameters, such as `beta` or `labels` in <span class="title-ref">f1\_score</span>.

Here is an example of building custom scorers, and of using the `greater_is_better` parameter:

    >>> import numpy as np
    >>> def my_custom_loss_func(y_true, y_pred):
    ...     diff = np.abs(y_true - y_pred).max()
    ...     return np.log1p(diff)
    ...
    >>> # score will negate the return value of my_custom_loss_func,
    >>> # which will be np.log(2), 0.693, given the values for X
    >>> # and y defined below.
    >>> score = make_scorer(my_custom_loss_func, greater_is_better=False)
    >>> X = [[1], [1]]
    >>> y = [0, 1]
    >>> from sklearn.dummy import DummyClassifier
    >>> clf = DummyClassifier(strategy='most_frequent', random_state=0)
    >>> clf = clf.fit(X, y)
    >>> my_custom_loss_func(y, clf.predict(X))
    0.69...
    >>> score(clf, X, y)
    -0.69...

</div>

### Implementing your own scoring object

You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the <span class="title-ref">make\_scorer</span> factory.

<div class="dropdown">

How to build a scorer from scratch

For a callable to be a scorer, it needs to meet the protocol specified by the following two rules:

  - It can be called with parameters `(estimator, X, y)`, where `estimator` is the model that should be evaluated, `X` is validation data, and `y` is the ground truth target for `X` (in the supervised case) or `None` (in the unsupervised case).
  - It returns a floating point number that quantifies the `estimator` prediction quality on `X`, with reference to `y`. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.
  - Advanced: If it requires extra metadata to be passed to it, it should expose a `get_metadata_routing` method returning the requested metadata. The user should be able to set the requested metadata via a `set_score_request` method. Please see \[User Guide \<metadata\_routing\>\](\#user-guide-\<metadata\_routing\>) and \[Developer Guide \<sphx\_glr\_auto\_examples\_miscellaneous\_plot\_metadata\_routing.py\>\](\#developer

</div>

  - \----guide-\<sphx\_glr\_auto\_examples\_miscellaneous\_plot\_metadata\_routing.py\>) for  
    more details.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    **Using custom scorers in functions where n\_jobs \> 1**
    
    While defining the custom scoring function alongside the calling function should work out of the box with the default joblib backend (loky), importing it from another module will be a more robust approach and work independently of the joblib backend.
    
    For example, to use `n_jobs` greater than 1 in the example below, `custom_scoring_function` function is saved in a user-created module (`custom_scorer_module.py`) and imported:
    
        >>> from custom_scorer_module import custom_scoring_function # doctest: +SKIP
        >>> cross_val_score(model,
        ...  X_train,
        ...  y_train,
        ...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),
        ...  cv=5,
        ...  n_jobs=-1) # doctest: +SKIP
    
    </div>

### Using multiple metric evaluation

Scikit-learn also permits evaluation of multiple metrics in `GridSearchCV`, `RandomizedSearchCV` and `cross_validate`.

There are three ways to specify multiple scoring metrics for the `scoring` parameter:

  - As an iterable of string metrics:
    
        >>> scoring = ['accuracy', 'precision']

  - As a `dict` mapping the scorer name to the scoring function:
    
        >>> from sklearn.metrics import accuracy_score
        >>> from sklearn.metrics import make_scorer
        >>> scoring = {'accuracy': make_scorer(accuracy_score),
        ...            'prec': 'precision'}
    
    Note that the dict values can either be scorer functions or one of the predefined metric strings.

  - As a callable that returns a dictionary of scores:
    
        >>> from sklearn.model_selection import cross_validate
        >>> from sklearn.metrics import confusion_matrix
        >>> # A sample toy binary classification dataset
        >>> X, y = datasets.make_classification(n_classes=2, random_state=0)
        >>> svm = LinearSVC(random_state=0)
        >>> def confusion_matrix_scorer(clf, X, y):
        ...      y_pred = clf.predict(X)
        ...      cm = confusion_matrix(y, y_pred)
        ...      return {'tn': cm[0, 0], 'fp': cm[0, 1],
        ...              'fn': cm[1, 0], 'tp': cm[1, 1]}
        >>> cv_results = cross_validate(svm, X, y, cv=5,
        ...                             scoring=confusion_matrix_scorer)
        >>> # Getting the test set true positive scores
        >>> print(cv_results['test_tp'])
        [10  9  8  7  8]
        >>> # Getting the test set false negative scores
        >>> print(cv_results['test_fn'])
        [0 1 2 3 2]

## Classification metrics

<div class="currentmodule">

sklearn.metrics

</div>

The `sklearn.metrics` module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the `sample_weight` parameter.

Some of these are restricted to the binary classification case:

<div class="autosummary">

precision\_recall\_curve roc\_curve class\_likelihood\_ratios det\_curve

</div>

Others also work in the multiclass case:

<div class="autosummary">

balanced\_accuracy\_score cohen\_kappa\_score confusion\_matrix hinge\_loss matthews\_corrcoef roc\_auc\_score top\_k\_accuracy\_score

</div>

Some also work in the multilabel case:

<div class="autosummary">

accuracy\_score classification\_report f1\_score fbeta\_score hamming\_loss jaccard\_score log\_loss multilabel\_confusion\_matrix precision\_recall\_fscore\_support precision\_score recall\_score roc\_auc\_score zero\_one\_loss d2\_log\_loss\_score

</div>

And some work with binary and multilabel (but not multiclass) problems:

<div class="autosummary">

average\_precision\_score

</div>

In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.

### From binary to multiclass and multilabel

Some metrics are essentially defined for binary classification tasks (e.g. <span class="title-ref">f1\_score</span>, <span class="title-ref">roc\_auc\_score</span>). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled `1` (though this may be configurable through the `pos_label` parameter).

In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the `average` parameter.

  - `"macro"` simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.
  - `"weighted"` accounts for class imbalance by computing the average of binary metrics in which each class's score is weighted by its presence in the true data sample.
  - `"micro"` gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.
  - `"samples"` applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their (`sample_weight`-weighted) average.
  - Selecting `average=None` will return an array with the score for each class.

While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell `[i, j]` has value 1 if sample `i` has label `j` and value 0 otherwise.

### Accuracy score

The <span class="title-ref">accuracy\_score</span> function computes the [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision), either the fraction (default) or the count (normalize=False) of correct predictions.

In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as

\[\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)\]

where \(1(x)\) is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).

> \>\>\> import numpy as np \>\>\> from sklearn.metrics import accuracy\_score \>\>\> y\_pred = \[0, 2, 1, 3\] \>\>\> y\_true = \[0, 1, 2, 3\] \>\>\> accuracy\_score(y\_true, y\_pred) 0.5 \>\>\> accuracy\_score(y\_true, y\_pred, normalize=False) 2.0

In the multilabel case with binary label indicators:

    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
    0.5

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_permutation\_tests\_for\_classification.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_permutation\_tests\_for\_classification.py) for an example of accuracy score usage using permutations of the dataset.

### Top-k accuracy score

The <span class="title-ref">top\_k\_accuracy\_score</span> function is a generalization of <span class="title-ref">accuracy\_score</span>. The difference is that a prediction is considered correct as long as the true label is associated with one of the `k` highest predicted scores. <span class="title-ref">accuracy\_score</span> is the special case of <span class="title-ref">k = 1</span>.

The function covers the binary and multiclass classification cases but not the multilabel case.

If \(\hat{f}_{i,j}\) is the predicted class for the \(i\)-th sample corresponding to the \(j\)-th largest predicted score and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as

\[\texttt{top-k accuracy}(y, \hat{f}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} \sum_{j=1}^{k} 1(\hat{f}_{i,j} = y_i)\]

where \(k\) is the number of guesses allowed and \(1(x)\) is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).

> \>\>\> import numpy as np \>\>\> from sklearn.metrics import top\_k\_accuracy\_score \>\>\> y\_true = np.array(\[0, 1, 2, 2\]) \>\>\> y\_score = np.array(\[\[0.5, 0.2, 0.2\], ... \[0.3, 0.4, 0.2\], ... \[0.2, 0.4, 0.3\], ... \[0.7, 0.2, 0.1\]\]) \>\>\> top\_k\_accuracy\_score(y\_true, y\_score, k=2) 0.75 \>\>\> \# Not normalizing gives the number of "correctly" classified samples \>\>\> top\_k\_accuracy\_score(y\_true, y\_score, k=2, normalize=False) 3

### Balanced accuracy score

The <span class="title-ref">balanced\_accuracy\_score</span> function computes the [balanced accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision), which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.

In the binary case, balanced accuracy is equal to the arithmetic mean of [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (true positive rate) and [specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (true negative rate), or the area under the ROC curve with binary predictions rather than scores:

\[\texttt{balanced-accuracy} = \frac{1}{2}\left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right )\]

If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).

In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{n\_classes}\).

The score ranges from 0 to 1, or when `adjusted=True` is used, it rescaled to the range \(\frac{1}{1 - n\_classes}\) to 1, inclusive, with performance at random scoring 0.

If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\) is the corresponding sample weight, then we adjust the sample weight to:

\[\hat{w}_i = \frac{w_i}{\sum_j{1(y_j = y_i) w_j}}\]

where \(1(x)\) is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function). Given predicted \(\hat{y}_i\) for sample \(i\), balanced accuracy is defined as:

\[\texttt{balanced-accuracy}(y, \hat{y}, w) = \frac{1}{\sum{\hat{w}_i}} \sum_i 1(\hat{y}_i = y_i) \hat{w}_i\]

With `adjusted=True`, balanced accuracy reports the relative increase from \(\texttt{balanced-accuracy}(y, \mathbf{0}, w) =
\frac{1}{n\_classes}\). In the binary case, this is also known as [\*Youden's J statistic\*](https://en.wikipedia.org/wiki/Youden%27s_J_statistic), or *informedness*.

\> **Note** \> The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:

>   - Our definition: [\[Mosley2013\]](#Mosley2013), [\[Kelleher2015\]](#Kelleher2015) and [\[Guyon2015\]](#Guyon2015), where [\[Guyon2015\]](#Guyon2015) adopt the adjusted version to ensure that random predictions have a score of \(0\) and perfect predictions have a score of \(1\)..
>   - Class balanced accuracy as described in [\[Mosley2013\]](#Mosley2013): the minimum between the precision and the recall for each class is computed. Those values are then averaged over the total number of classes to get the balanced accuracy.
>   - Balanced Accuracy as described in [\[Urbanowicz2015\]](#Urbanowicz2015): the average of sensitivity and specificity is computed for each class and then averaged over total number of classes.

**References**

### Cohen's kappa

The function <span class="title-ref">cohen\_kappa\_score</span> computes [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.

The kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).

Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.

> \>\>\> from sklearn.metrics import cohen\_kappa\_score \>\>\> labeling1 = \[2, 0, 2, 2, 0, 1\] \>\>\> labeling2 = \[0, 0, 2, 2, 0, 2\] \>\>\> cohen\_kappa\_score(labeling1, labeling2) 0.4285714285714286

### Confusion matrix

The <span class="title-ref">confusion\_matrix</span> function evaluates classification accuracy by computing the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) with each row corresponding to the true class (Wikipedia and other references may use different convention for axes).

By definition, entry \(i, j\) in a confusion matrix is the number of observations actually in group \(i\), but predicted to be in group \(j\). Here is an example:

    >>> from sklearn.metrics import confusion_matrix
    >>> y_true = [2, 0, 2, 2, 0, 1]
    >>> y_pred = [0, 0, 2, 2, 0, 2]
    >>> confusion_matrix(y_true, y_pred)
    array([[2, 0, 0],
           [0, 0, 1],
           [1, 0, 2]])

<span class="title-ref">ConfusionMatrixDisplay</span> can be used to visually represent a confusion matrix as shown in the \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_confusion\_matrix.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_confusion\_matrix.py) example, which creates the following figure:

[![image](../auto_examples/model_selection/images/sphx_glr_plot_confusion_matrix_001.png)](../auto_examples/model_selection/plot_confusion_matrix.html)

The parameter `normalize` allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: `'pred'`, `'true'`, and `'all'` which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively.

> \>\>\> y\_true = \[0, 0, 0, 1, 1, 1, 1, 1\] \>\>\> y\_pred = \[0, 1, 0, 1, 0, 1, 0, 1\] \>\>\> confusion\_matrix(y\_true, y\_pred, normalize='all') array(\[\[0.25 , 0.125\], \[0.25 , 0.375\]\])

For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:

    >>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]
    >>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
    >>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    >>> tn, fp, fn, tp
    (2, 1, 2, 3)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_confusion\_matrix.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_confusion\_matrix.py) for an example of using a confusion matrix to evaluate classifier output quality.
  - See \[sphx\_glr\_auto\_examples\_classification\_plot\_digits\_classification.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_digits\_classification.py) for an example of using a confusion matrix to classify hand-written digits.
  - See \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py) for an example of using a confusion matrix to classify text documents.

### Classification report

The <span class="title-ref">classification\_report</span> function builds a text report showing the main classification metrics. Here is a small example with custom `target_names` and inferred labels:

    >>> from sklearn.metrics import classification_report
    >>> y_true = [0, 1, 2, 2, 0]
    >>> y_pred = [0, 0, 2, 1, 0]
    >>> target_names = ['class 0', 'class 1', 'class 2']
    >>> print(classification_report(y_true, y_pred, target_names=target_names))
                  precision    recall  f1-score   support
    <BLANKLINE>
         class 0       0.67      1.00      0.80         2
         class 1       0.00      0.00      0.00         1
         class 2       1.00      0.50      0.67         2
    <BLANKLINE>
        accuracy                           0.60         5
       macro avg       0.56      0.50      0.49         5
    weighted avg       0.67      0.60      0.59         5
    <BLANKLINE>

**Examples**

  - See \[sphx\_glr\_auto\_examples\_classification\_plot\_digits\_classification.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_digits\_classification.py) for an example of classification report usage for hand-written digits.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_digits.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_digits.py) for an example of classification report usage for grid search with nested cross-validation.

### Hamming loss

The <span class="title-ref">hamming\_loss</span> computes the average Hamming loss or [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) between two sets of samples.

If \(\hat{y}_{i,j}\) is the predicted value for the \(j\)-th label of a given sample \(i\), \(y_{i,j}\) is the corresponding true value, \(n_\text{samples}\) is the number of samples and \(n_\text{labels}\) is the number of labels, then the Hamming loss \(L_{Hamming}\) is defined as:

\[L_{Hamming}(y, \hat{y}) = \frac{1}{n_\text{samples} * n_\text{labels}} \sum_{i=0}^{n_\text{samples}-1} \sum_{j=0}^{n_\text{labels} - 1} 1(\hat{y}_{i,j} \not= y_{i,j})\]

where \(1(x)\) is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).

The equation above does not hold true in the case of multiclass classification. Please refer to the note below for more information. :

    >>> from sklearn.metrics import hamming_loss
    >>> y_pred = [1, 2, 3, 4]
    >>> y_true = [2, 2, 3, 4]
    >>> hamming_loss(y_true, y_pred)
    0.25

In the multilabel case with binary label indicators:

    >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
    0.75

\> **Note** \> In multiclass classification, the Hamming loss corresponds to the Hamming distance between `y_true` and `y_pred` which is similar to the \[zero\_one\_loss\](\#zero\_one\_loss) function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive.

### Precision, recall and F-measures

Intuitively, [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) is the ability of the classifier not to label as positive a sample that is negative, and [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) is the ability of the classifier to find all the positive samples.

The [F-measure](https://en.wikipedia.org/wiki/F1_score) (\(F_\beta\) and \(F_1\) measures) can be interpreted as a weighted harmonic mean of the precision and recall. A \(F_\beta\) measure reaches its best value at 1 and its worst score at 0. With \(\beta = 1\), \(F_\beta\) and \(F_1\) are equivalent, and the recall and the precision are equally important.

The <span class="title-ref">precision\_recall\_curve</span> computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.

The <span class="title-ref">average\_precision\_score</span> function computes the [average precision](https://en.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision) (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as

\[\text{AP} = \sum_n (R_n - R_{n-1}) P_n\]

where \(P_n\) and \(R_n\) are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of positive samples.

References [\[Manning2008\]](#Manning2008) and [\[Everingham2010\]](#Everingham2010) present alternative variants of AP that interpolate the precision-recall curve. Currently, <span class="title-ref">average\_precision\_score</span> does not implement any interpolated variant. References [\[Davis2006\]](#Davis2006) and [\[Flach2015\]](#Flach2015) describe why a linear interpolation of points on the precision-recall curve provides an overly-optimistic measure of classifier performance. This linear interpolation is used when computing area under the curve with the trapezoidal rule in <span class="title-ref">auc</span>.

Several functions allow you to analyze the precision, recall and F-measures score:

<div class="autosummary">

average\_precision\_score f1\_score fbeta\_score precision\_recall\_curve precision\_recall\_fscore\_support precision\_score recall\_score

</div>

Note that the <span class="title-ref">precision\_recall\_curve</span> function is restricted to the binary case. The <span class="title-ref">average\_precision\_score</span> function supports multiclass and multilabel formats by computing each class score in a One-vs-the-rest (OvR) fashion and averaging them or not depending of its `average` argument value.

The <span class="title-ref">PrecisionRecallDisplay.from\_estimator</span> and <span class="title-ref">PrecisionRecallDisplay.from\_predictions</span> functions will plot the precision-recall curve as follows.

[![image](../auto_examples/model_selection/images/sphx_glr_plot_precision_recall_001.png)](../auto_examples/model_selection/plot_precision_recall.html#plot-the-precision-recall-curve)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_digits.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_digits.py) for an example of <span class="title-ref">precision\_score</span> and <span class="title-ref">recall\_score</span> usage to estimate parameters using grid search with nested cross-validation.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_precision\_recall.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_precision\_recall.py) for an example of <span class="title-ref">precision\_recall\_curve</span> usage to evaluate classifier output quality.

**References**

#### Binary classification

In a binary classification task, the terms ''positive'' and ''negative'' refer to the classifier's prediction, and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation''). Given these definitions, we can formulate the following table:

<table style="width:96%;">
<colgroup>
<col style="width: 27%" />
<col style="width: 68%" />
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><blockquote>
<p>Actual class (observation)</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>Predicted class (expectation) +</p>
</blockquote></td>
<td><blockquote>
<p>tp (true positive) | fp (false positive) Correct result | Unexpected result</p>
</blockquote>
<dl>
<dt>---------------------+--------------------------+</dt>
<dd><p>fn (false negative) | tn (true negative) Missing result | Correct absence of result</p>
</dd>
</dl></td>
</tr>
</tbody>
</table>

In this context, we can define the notions of precision and recall:

\[\text{precision} = \frac{\text{tp}}{\text{tp} + \text{fp}},\]

\[\text{recall} = \frac{\text{tp}}{\text{tp} + \text{fn}},\]

(Sometimes recall is also called ''sensitivity'')

F-measure is the weighted harmonic mean of precision and recall, with precision's contribution to the mean weighted by some parameter \(\beta\):

\[F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}\]

To avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this otherwise-equivalent formula:

\[F_\beta = \frac{(1 + \beta^2) \text{tp}}{(1 + \beta^2) \text{tp} + \text{fp} + \beta^2 \text{fn}}\]

Note that this formula is still undefined when there are no true positives, false positives, or false negatives. By default, F-1 for a set of exclusively true negatives is calculated as 0, however this behavior can be changed using the <span class="title-ref">zero\_division</span> parameter. Here are some small examples in binary classification:

    >>> from sklearn import metrics
    >>> y_pred = [0, 1, 0, 0]
    >>> y_true = [0, 1, 0, 1]
    >>> metrics.precision_score(y_true, y_pred)
    1.0
    >>> metrics.recall_score(y_true, y_pred)
    0.5
    >>> metrics.f1_score(y_true, y_pred)
    0.66...
    >>> metrics.fbeta_score(y_true, y_pred, beta=0.5)
    0.83...
    >>> metrics.fbeta_score(y_true, y_pred, beta=1)
    0.66...
    >>> metrics.fbeta_score(y_true, y_pred, beta=2)
    0.55...
    >>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)
    (array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))

    >>> import numpy as np
    >>> from sklearn.metrics import precision_recall_curve
    >>> from sklearn.metrics import average_precision_score
    >>> y_true = np.array([0, 0, 1, 1])
    >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
    >>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)
    >>> precision
    array([0.5       , 0.66..., 0.5       , 1.        , 1.        ])
    >>> recall
    array([1. , 1. , 0.5, 0.5, 0. ])
    >>> threshold
    array([0.1 , 0.35, 0.4 , 0.8 ])
    >>> average_precision_score(y_true, y_scores)
    0.83...

#### Multiclass and multilabel classification

In a multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the `average` argument to the <span class="title-ref">average\_precision\_score</span>, <span class="title-ref">f1\_score</span>, <span class="title-ref">fbeta\_score</span>, <span class="title-ref">precision\_recall\_fscore\_support</span>, <span class="title-ref">precision\_score</span> and <span class="title-ref">recall\_score</span> functions, as described \[above \<average\>\](\#above-\<average\>).

Note the following behaviors when averaging:

  - If all labels are included, "micro"-averaging in a multiclass setting will produce precision, recall and \(F\) that are all identical to accuracy.
  - "weighted" averaging may produce a F-score that is not between precision and recall.
  - "macro" averaging for F-measures is calculated as the arithmetic mean over per-label/class F-measures, not the harmonic mean over the arithmetic precision and recall means. Both calculations can be seen in the literature but are not equivalent, see [\[OB2019\]](#OB2019) for details.

To make this more explicit, consider the following notation:

  - \(y\) the set of *true* \((sample, label)\) pairs
  - \(\hat{y}\) the set of *predicted* \((sample, label)\) pairs
  - \(L\) the set of labels
  - \(S\) the set of samples
  - \(y_s\) the subset of \(y\) with sample \(s\), i.e. \(y_s := \left\{(s', l) \in y | s' = s\right\}\)
  - \(y_l\) the subset of \(y\) with label \(l\)
  - similarly, \(\hat{y}_s\) and \(\hat{y}_l\) are subsets of \(\hat{y}\)
  - \(P(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}\) for some sets \(A\) and \(B\)
  - \(R(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}\) (Conventions vary on handling \(A = \emptyset\); this implementation uses \(R(A, B):=0\), and similar for \(P\).)
  - \(F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}\)

Then the metrics are defined as:

| `average`    | Precision                                                                                       | Recall                                                                                          | F\_beta                                                                                               |
| ------------ | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| `"micro"`    | \(P(y, \hat{y})\)                                                                               | \(R(y, \hat{y})\)                                                                               | \(F_\beta(y, \hat{y})\)                                                                               |
| `"samples"`  | \(\frac{1}{\left|S\right|} \sum_{s \in S} P(y_s, \hat{y}_s)\)                                   | \(\frac{1}{\left|S\right|} \sum_{s \in S} R(y_s, \hat{y}_s)\)                                   | \(\frac{1}{\left|S\right|} \sum_{s \in S} F_\beta(y_s, \hat{y}_s)\)                                   |
| `"macro"`    | \(\frac{1}{\left|L\right|} \sum_{l \in L} P(y_l, \hat{y}_l)\)                                   | \(\frac{1}{\left|L\right|} \sum_{l \in L} R(y_l, \hat{y}_l)\)                                   | \(\frac{1}{\left|L\right|} \sum_{l \in L} F_\beta(y_l, \hat{y}_l)\)                                   |
| `"weighted"` | \(\frac{1}{\sum_{l \in L} \left|y_l\right|} \sum_{l \in L} \left|y_l\right| P(y_l, \hat{y}_l)\) | \(\frac{1}{\sum_{l \in L} \left|y_l\right|} \sum_{l \in L} \left|y_l\right| R(y_l, \hat{y}_l)\) | \(\frac{1}{\sum_{l \in L} \left|y_l\right|} \sum_{l \in L} \left|y_l\right| F_\beta(y_l, \hat{y}_l)\) |
| `None`       | \(\langle P(y_l, \hat{y}_l) | l \in L \rangle\)                                                 | \(\langle R(y_l, \hat{y}_l) | l \in L \rangle\)                                                 | \(\langle F_\beta(y_l, \hat{y}_l) | l \in L \rangle\)                                                 |

> \>\>\> from sklearn import metrics \>\>\> y\_true = \[0, 1, 2, 0, 1, 2\] \>\>\> y\_pred = \[0, 2, 1, 0, 0, 1\] \>\>\> metrics.precision\_score(y\_true, y\_pred, average='macro') 0.22... \>\>\> metrics.recall\_score(y\_true, y\_pred, average='micro') 0.33... \>\>\> metrics.f1\_score(y\_true, y\_pred, average='weighted') 0.26... \>\>\> metrics.fbeta\_score(y\_true, y\_pred, average='macro', beta=0.5) 0.23... \>\>\> metrics.precision\_recall\_fscore\_support(y\_true, y\_pred, beta=0.5, average=None) (array(\[0.66..., 0. , 0. \]), array(\[1., 0., 0.\]), array(\[0.71..., 0. , 0. \]), array(\[2, 2, 2\]...))

For multiclass classification with a "negative class", it is possible to exclude some labels:

> \>\>\> metrics.recall\_score(y\_true, y\_pred, labels=\[1, 2\], average='micro') ... \# excluding 0, no labels were correctly recalled 0.0

Similarly, labels not present in the data sample may be accounted for in macro-averaging.

> \>\>\> metrics.precision\_score(y\_true, y\_pred, labels=\[0, 1, 2, 3\], average='macro') 0.166...

**References**

### Jaccard similarity coefficient score

The <span class="title-ref">jaccard\_score</span> function computes the average of [Jaccard similarity coefficients](https://en.wikipedia.org/wiki/Jaccard_index), also called the Jaccard index, between pairs of label sets.

The Jaccard similarity coefficient with a ground truth label set \(y\) and predicted label set \(\hat{y}\), is defined as

\[J(y, \hat{y}) = \frac{|y \cap \hat{y}|}{|y \cup \hat{y}|}.\]

The <span class="title-ref">jaccard\_score</span> (like <span class="title-ref">precision\_recall\_fscore\_support</span>) applies natively to binary targets. By computing it set-wise it can be extended to apply to multilabel and multiclass through the use of <span class="title-ref">average</span> (see \[above \<average\>\](\#above-\<average\>)).

In the binary case:

    >>> import numpy as np
    >>> from sklearn.metrics import jaccard_score
    >>> y_true = np.array([[0, 1, 1],
    ...                    [1, 1, 0]])
    >>> y_pred = np.array([[1, 1, 1],
    ...                    [1, 0, 0]])
    >>> jaccard_score(y_true[0], y_pred[0])
    0.6666...

In the 2D comparison case (e.g. image similarity):

> \>\>\> jaccard\_score(y\_true, y\_pred, average="micro") 0.6

In the multilabel case with binary label indicators:

    >>> jaccard_score(y_true, y_pred, average='samples')
    0.5833...
    >>> jaccard_score(y_true, y_pred, average='macro')
    0.6666...
    >>> jaccard_score(y_true, y_pred, average=None)
    array([0.5, 0.5, 1. ])

Multiclass problems are binarized and treated like the corresponding multilabel problem:

    >>> y_pred = [0, 2, 1, 2]
    >>> y_true = [0, 1, 2, 2]
    >>> jaccard_score(y_true, y_pred, average=None)
    array([1. , 0. , 0.33...])
    >>> jaccard_score(y_true, y_pred, average='macro')
    0.44...
    >>> jaccard_score(y_true, y_pred, average='micro')
    0.33...

### Hinge loss

The <span class="title-ref">hinge\_loss</span> function computes the average distance between the model and the data using [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss), a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)

If the true label \(y_i\) of a binary classification task is encoded as \(y_i=\left\{-1, +1\right\}\) for every sample \(i\); and \(w_i\) is the corresponding predicted decision (an array of shape (<span class="title-ref">n\_samples</span>,) as output by the <span class="title-ref">decision\_function</span> method), then the hinge loss is defined as:

\[L_\text{Hinge}(y, w) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} \max\left\{1 - w_i y_i, 0\right\}\]

If there are more than two labels, <span class="title-ref">hinge\_loss</span> uses a multiclass variant due to Crammer & Singer. [Here](https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf) is the paper describing it.

In this case the predicted decision is an array of shape (<span class="title-ref">n\_samples</span>, <span class="title-ref">n\_labels</span>). If \(w_{i, y_i}\) is the predicted decision for the true label \(y_i\) of the \(i\)-th sample; and \(\hat{w}_{i, y_i} = \max\left\{w_{i, y_j}~|~y_j \ne y_i \right\}\) is the maximum of the predicted decisions for all the other labels, then the multi-class hinge loss is defined by:

\[L_\text{Hinge}(y, w) = \frac{1}{n_\text{samples}}
\sum_{i=0}^{n_\text{samples}-1} \max\left\{1 + \hat{w}_{i, y_i}
- w_{i, y_i}, 0\right\}\]

Here is a small example demonstrating the use of the <span class="title-ref">hinge\_loss</span> function with a svm classifier in a binary class problem:

    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(random_state=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)
    0.3...

Here is an example demonstrating the use of the <span class="title-ref">hinge\_loss</span> function with a svm classifier in a multiclass problem:

    >>> X = np.array([[0], [1], [2], [3]])
    >>> Y = np.array([0, 1, 2, 3])
    >>> labels = np.array([0, 1, 2, 3])
    >>> est = svm.LinearSVC()
    >>> est.fit(X, Y)
    LinearSVC()
    >>> pred_decision = est.decision_function([[-1], [2], [3]])
    >>> y_true = [0, 2, 3]
    >>> hinge_loss(y_true, pred_decision, labels=labels)
    0.56...

### Log loss

Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (`predict_proba`) of a classifier instead of its discrete predictions.

For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:

\[L_{\log}(y, p) = -\log \operatorname{Pr}(y|p) = -(y \log (p) + (1 - y) \log (1 - p))\]

This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \(Y\), i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\) taken from a set of \(K\) labels. Let \(P\) be a matrix of probability estimates, with \(p_{i,k} = \operatorname{Pr}(y_{i,k} = 1)\). Then the log loss of the whole set is

\[L_{\log}(Y, P) = -\log \operatorname{Pr}(Y|P) = - \frac{1}{N} \sum_{i=0}^{N-1} \sum_{k=0}^{K-1} y_{i,k} \log p_{i,k}\]

To see how this generalizes the binary log loss given above, note that in the binary case, \(p_{i,0} = 1 - p_{i,1}\) and \(y_{i,0} = 1 - y_{i,1}\), so expanding the inner sum over \(y_{i,k} \in \{0,1\}\) gives the binary log loss.

The <span class="title-ref">log\_loss</span> function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator's `predict_proba` method.

> \>\>\> from sklearn.metrics import log\_loss \>\>\> y\_true = \[0, 0, 1, 1\] \>\>\> y\_pred = \[\[.9, .1\], \[.8, .2\], \[.3, .7\], \[.01, .99\]\] \>\>\> log\_loss(y\_true, y\_pred) 0.1738...

The first `[.9, .1]` in `y_pred` denotes 90% probability that the first sample has label 0. The log loss is non-negative.

### Matthews correlation coefficient

The <span class="title-ref">matthews\_corrcoef</span> function computes the [Matthew's correlation coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) for binary classes. Quoting Wikipedia:

> "The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient."

In the binary (two-class) case, \(tp\), \(tn\), \(fp\) and \(fn\) are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as

\[MCC = \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\]

In the multiclass case, the Matthews correlation coefficient can be [defined](http://rk.kvl.dk/introduction/index.html) in terms of a <span class="title-ref">confusion\_matrix</span> \(C\) for \(K\) classes. To simplify the definition consider the following intermediate variables:

  - \(t_k=\sum_{i}^{K} C_{ik}\) the number of times class \(k\) truly occurred,
  - \(p_k=\sum_{i}^{K} C_{ki}\) the number of times class \(k\) was predicted,
  - \(c=\sum_{k}^{K} C_{kk}\) the total number of samples correctly predicted,
  - \(s=\sum_{i}^{K} \sum_{j}^{K} C_{ij}\) the total number of samples.

Then the multiclass MCC is defined as:

\[MCC = \frac{
c \times s - \sum_{k}^{K} p_k \times t_k
}{\sqrt{
(s^2 - \sum_{k}^{K} p_k^2) \times
(s^2 - \sum_{k}^{K} t_k^2)
}}\]

When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground true labels. The maximum value is always +1. For additional information, see [\[WikipediaMCC2021\]](#WikipediaMCC2021).

Here is a small example illustrating the usage of the <span class="title-ref">matthews\_corrcoef</span> function:

> \>\>\> from sklearn.metrics import matthews\_corrcoef \>\>\> y\_true = \[+1, +1, +1, -1\] \>\>\> y\_pred = \[+1, -1, +1, +1\] \>\>\> matthews\_corrcoef(y\_true, y\_pred) -0.33...

**References**

### Multi-label confusion matrix

The <span class="title-ref">multilabel\_confusion\_matrix</span> function computes class-wise (default) or sample-wise (samplewise=True) multilabel confusion matrix to evaluate the accuracy of a classification. multilabel\_confusion\_matrix also treats multiclass data as if it were multilabel, as this is a transformation commonly applied to evaluate multiclass problems with binary classification metrics (such as precision, recall, etc.).

When calculating class-wise multilabel confusion matrix \(C\), the count of true negatives for class \(i\) is \(C_{i,0,0}\), false negatives is \(C_{i,1,0}\), true positives is \(C_{i,1,1}\) and false positives is \(C_{i,0,1}\).

Here is an example demonstrating the use of the <span class="title-ref">multilabel\_confusion\_matrix</span> function with `multilabel indicator matrix` input:

    >>> import numpy as np
    >>> from sklearn.metrics import multilabel_confusion_matrix
    >>> y_true = np.array([[1, 0, 1],
    ...                    [0, 1, 0]])
    >>> y_pred = np.array([[1, 0, 0],
    ...                    [0, 1, 1]])
    >>> multilabel_confusion_matrix(y_true, y_pred)
    array([[[1, 0],
            [0, 1]],
    <BLANKLINE>
           [[1, 0],
            [0, 1]],
    <BLANKLINE>
           [[0, 1],
            [1, 0]]])

Or a confusion matrix can be constructed for each sample's labels:

> \>\>\> multilabel\_confusion\_matrix(y\_true, y\_pred, samplewise=True) array(\[\[\[1, 0\], \[1, 1\]\], \<BLANKLINE\> \[\[1, 1\], \[0, 1\]\]\])

Here is an example demonstrating the use of the <span class="title-ref">multilabel\_confusion\_matrix</span> function with `multiclass` input:

    >>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
    >>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
    >>> multilabel_confusion_matrix(y_true, y_pred,
    ...                             labels=["ant", "bird", "cat"])
    array([[[3, 1],
            [0, 2]],
    <BLANKLINE>
           [[5, 0],
            [1, 0]],
    <BLANKLINE>
           [[2, 1],
            [1, 2]]])

Here are some examples demonstrating the use of the <span class="title-ref">multilabel\_confusion\_matrix</span> function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input.

Calculating [recall](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (also called the true positive rate or the sensitivity) for each class:

    >>> y_true = np.array([[0, 0, 1],
    ...                    [0, 1, 0],
    ...                    [1, 1, 0]])
    >>> y_pred = np.array([[0, 1, 0],
    ...                    [0, 0, 1],
    ...                    [1, 1, 0]])
    >>> mcm = multilabel_confusion_matrix(y_true, y_pred)
    >>> tn = mcm[:, 0, 0]
    >>> tp = mcm[:, 1, 1]
    >>> fn = mcm[:, 1, 0]
    >>> fp = mcm[:, 0, 1]
    >>> tp / (tp + fn)
    array([1. , 0.5, 0. ])

Calculating [specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (also called the true negative rate) for each class:

    >>> tn / (tn + fp)
    array([1. , 0. , 0.5])

Calculating [fall out](https://en.wikipedia.org/wiki/False_positive_rate) (also called the false positive rate) for each class:

    >>> fp / (fp + tn)
    array([0. , 1. , 0.5])

Calculating [miss rate](https://en.wikipedia.org/wiki/False_positives_and_false_negatives) (also called the false negative rate) for each class:

    >>> fn / (fn + tp)
    array([0. , 0.5, 1. ])

### Receiver operating characteristic (ROC)

The function <span class="title-ref">roc\_curve</span> computes the [receiver operating characteristic curve, or ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Quoting Wikipedia :

> "A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate."

This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the <span class="title-ref">roc\_curve</span> function:

    >>> import numpy as np
    >>> from sklearn.metrics import roc_curve
    >>> y = np.array([1, 1, 2, 2])
    >>> scores = np.array([0.1, 0.4, 0.35, 0.8])
    >>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)
    >>> fpr
    array([0. , 0. , 0.5, 0.5, 1. ])
    >>> tpr
    array([0. , 0.5, 0.5, 1. , 1. ])
    >>> thresholds
    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])

Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn't require optimizing a threshold for each label.

The <span class="title-ref">roc\_auc\_score</span> function, denoted by ROC-AUC or AUROC, computes the area under the ROC curve. By doing so, the curve information is summarized in one number.

The following figure shows the ROC curve and ROC-AUC score for a classifier aimed to distinguish the virginica flower from the rest of the species in the \[iris\_dataset\](\#iris\_dataset):

[![image](../auto_examples/model_selection/images/sphx_glr_plot_roc_001.png)](../auto_examples/model_selection/plot_roc.html)

For more information see the [Wikipedia article on AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).

#### Binary case

In the **binary case**, you can either provide the probability estimates, using the <span class="title-ref">classifier.predict\_proba()</span> method, or the non-thresholded decision values given by the <span class="title-ref">classifier.decision\_function()</span> method. In the case of providing the probability estimates, the probability of the class with the "greater label" should be provided. The "greater label" corresponds to <span class="title-ref">classifier.classes\_\[1\]</span> and thus <span class="title-ref">classifier.predict\_proba(X)\[:, 1\]</span>. Therefore, the <span class="title-ref">y\_score</span> parameter is of size (n\_samples,).

> \>\>\> from sklearn.datasets import load\_breast\_cancer \>\>\> from sklearn.linear\_model import LogisticRegression \>\>\> from sklearn.metrics import roc\_auc\_score \>\>\> X, y = load\_breast\_cancer(return\_X\_y=True) \>\>\> clf = LogisticRegression(solver="liblinear").fit(X, y) \>\>\> [clf.classes]() array(\[0, 1\])

We can use the probability estimates corresponding to <span class="title-ref">clf.classes\_\[1\]</span>.

> \>\>\> y\_score = clf.predict\_proba(X)\[:, 1\] \>\>\> roc\_auc\_score(y, y\_score) 0.99...

Otherwise, we can use the non-thresholded decision values

> \>\>\> roc\_auc\_score(y, clf.decision\_function(X)) 0.99...

#### Multi-class case

The <span class="title-ref">roc\_auc\_score</span> function can also be used in **multi-class classification**. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the predicted labels are provided in an array with values from 0 to `n_classes`, and the scores correspond to the probability estimates that a sample belongs to a particular class. The OvO and OvR algorithms support weighting uniformly (`average='macro'`) and by prevalence (`average='weighted'`).

<div class="dropdown">

One-vs-one Algorithm

Computes the average AUC of all possible pairwise combinations of classes. [\[HT2001\]](#HT2001) defines a multiclass AUC metric weighted uniformly:

\[\frac{1}{c(c-1)}\sum_{j=1}^{c}\sum_{k > j}^c (\text{AUC}(j | k) +
\text{AUC}(k | j))\]

where \(c\) is the number of classes and \(\text{AUC}(j | k)\) is the AUC with class \(j\) as the positive class and class \(k\) as the negative class. In general, \(\text{AUC}(j | k) \neq \text{AUC}(k | j))\) in the multiclass case. This algorithm is used by setting the keyword argument `multiclass` to `'ovo'` and `average` to `'macro'`.

The [\[HT2001\]](#HT2001) multiclass AUC metric can be extended to be weighted by the prevalence:

\[\frac{1}{c(c-1)}\sum_{j=1}^{c}\sum_{k > j}^c p(j \cup k)(
\text{AUC}(j | k) + \text{AUC}(k | j))\]

where \(c\) is the number of classes. This algorithm is used by setting the keyword argument `multiclass` to `'ovo'` and `average` to `'weighted'`. The `'weighted'` option returns a prevalence-weighted average as described in [\[FC2009\]](#FC2009).

</div>

<div class="dropdown">

One-vs-rest Algorithm

Computes the AUC of each class against the rest [\[PD2000\]](#PD2000). The algorithm is functionally the same as the multilabel case. To enable this algorithm set the keyword argument `multiclass` to `'ovr'`. Additionally to `'macro'` [\[F2006\]](#F2006) and `'weighted'` [\[F2001\]](#F2001) averaging, OvR supports `'micro'` averaging.

In applications where a high false positive rate is not tolerable the parameter `max_fpr` of <span class="title-ref">roc\_auc\_score</span> can be used to summarize the ROC curve up to the given limit.

The following figure shows the micro-averaged ROC curve and its corresponding ROC-AUC score for a classifier aimed to distinguish the different species in the \[iris\_dataset\](\#iris\_dataset):

[![image](../auto_examples/model_selection/images/sphx_glr_plot_roc_002.png)](../auto_examples/model_selection/plot_roc.html)

</div>

#### Multi-label case

In **multi-label classification**, the <span class="title-ref">roc\_auc\_score</span> function is extended by averaging over the labels as \[above \<average\>\](\#above-\<average\>). In this case, you should provide a <span class="title-ref">y\_score</span> of shape <span class="title-ref">(n\_samples, n\_classes)</span>. Thus, when using the probability estimates, one needs to select the probability of the class with the greater label for each output.

> \>\>\> from sklearn.datasets import make\_multilabel\_classification \>\>\> from sklearn.multioutput import MultiOutputClassifier \>\>\> X, y = make\_multilabel\_classification(random\_state=0) \>\>\> inner\_clf = LogisticRegression(solver="liblinear", random\_state=0) \>\>\> clf = MultiOutputClassifier(inner\_clf).fit(X, y) \>\>\> y\_score = np.transpose(\[y\_pred\[:, 1\] for y\_pred in clf.predict\_proba(X)\]) \>\>\> roc\_auc\_score(y, y\_score, average=None) array(\[0.82..., 0.86..., 0.94..., 0.85... , 0.94...\])

And the decision values do not require such processing.

> \>\>\> from sklearn.linear\_model import RidgeClassifierCV \>\>\> clf = RidgeClassifierCV().fit(X, y) \>\>\> y\_score = clf.decision\_function(X) \>\>\> roc\_auc\_score(y, y\_score, average=None) array(\[0.81..., 0.84... , 0.93..., 0.87..., 0.94...\])

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_roc.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_roc.py) for an example of using ROC to evaluate the quality of the output of a classifier.
  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_roc\_crossval.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_roc\_crossval.py) for an example of using ROC to evaluate classifier output quality, using cross-validation.
  - See \[sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py) for an example of using ROC to model species distribution.

**References**

### Detection error tradeoff (DET)

The function <span class="title-ref">det\_curve</span> computes the detection error tradeoff curve (DET) curve [\[WikipediaDET2017\]](#WikipediaDET2017). Quoting Wikipedia:

> "A detection error tradeoff (DET) graph is a graphical plot of error rates for binary classification systems, plotting false reject rate vs. false accept rate. The x- and y-axes are scaled non-linearly by their standard normal deviates (or just by logarithmic transformation), yielding tradeoff curves that are more linear than ROC curves, and use most of the image area to highlight the differences of importance in the critical operating region."

DET curves are a variation of receiver operating characteristic (ROC) curves where False Negative Rate is plotted on the y-axis instead of True Positive Rate. DET curves are commonly plotted in normal deviate scale by transformation with \(\phi^{-1}\) (with \(\phi\) being the cumulative distribution function). The resulting performance curves explicitly visualize the tradeoff of error types for given classification algorithms. See [\[Martin1997\]](#Martin1997) for examples and further motivation.

This figure compares the ROC and DET curves of two example classifiers on the same classification task:

[![image](../auto_examples/model_selection/images/sphx_glr_plot_det_001.png)](../auto_examples/model_selection/plot_det.html)

<div class="dropdown">

Properties

  - DET curves form a linear curve in normal deviate scale if the detection scores are normally (or close-to normally) distributed. It was shown by [\[Navratil2007\]](#Navratil2007) that the reverse is not necessarily true and even more general distributions are able to produce linear DET curves.
  - The normal deviate scale transformation spreads out the points such that a comparatively larger space of plot is occupied. Therefore curves with similar classification performance might be easier to distinguish on a DET plot.
  - With False Negative Rate being "inverse" to True Positive Rate the point of perfection for DET curves is the origin (in contrast to the top left corner for ROC curves).

</div>

<div class="dropdown">

Applications and limitations

DET curves are intuitive to read and hence allow quick visual assessment of a classifier's performance. Additionally DET curves can be consulted for threshold analysis and operating point selection. This is particularly helpful if a comparison of error types is required.

On the other hand DET curves do not provide their metric as a single number. Therefore for either automated evaluation or comparison to other classification tasks metrics like the derived area under ROC curve might be better suited.

</div>

**Examples**

  - See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_det.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_det.py) for an example comparison between receiver operating characteristic (ROC) curves and Detection error tradeoff (DET) curves.

**References**

### Zero one loss

The <span class="title-ref">zero\_one\_loss</span> function computes the sum or the average of the 0-1 classification loss (\(L_{0-1}\)) over \(n_{\text{samples}}\). By default, the function normalizes over the sample. To get the sum of the \(L_{0-1}\), set `normalize` to `False`.

In multilabel classification, the <span class="title-ref">zero\_one\_loss</span> scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set `normalize` to `False`

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the 0-1 loss \(L_{0-1}\) is defined as:

\[L_{0-1}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i \not= y_i)\]

where \(1(x)\) is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function). The zero one loss can also be computed as \(zero-one loss = 1 - accuracy\).

> \>\>\> from sklearn.metrics import zero\_one\_loss \>\>\> y\_pred = \[1, 2, 3, 4\] \>\>\> y\_true = \[2, 2, 3, 4\] \>\>\> zero\_one\_loss(y\_true, y\_pred) 0.25 \>\>\> zero\_one\_loss(y\_true, y\_pred, normalize=False) 1.0

In the multilabel case with binary label indicators, where the first label set \[0,1\] has an error:

    >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
    0.5
    
    >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)
    1.0

**Examples**

  - See \[sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_with\_cross\_validation.py\](\#sphx\_glr\_auto\_examples\_feature\_selection\_plot\_rfe\_with\_cross\_validation.py) for an example of zero one loss usage to perform recursive feature elimination with cross-validation.

### Brier score loss

The <span class="title-ref">brier\_score\_loss</span> function computes the [Brier score](https://en.wikipedia.org/wiki/Brier_score) for binary classes [\[Brier1950\]](#Brier1950). Quoting Wikipedia:

> "The Brier score is a proper score function that measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes."

This function returns the mean squared error of the actual outcome \(y \in \{0,1\}\) and the predicted probability estimate \(p = \operatorname{Pr}(y = 1)\) (`predict_proba`) as outputted by:

\[BS = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}} - 1}(y_i - p_i)^2\]

The Brier score loss is also between 0 to 1 and the lower the value (the mean square difference is smaller), the more accurate the prediction is.

Here is a small example of usage of this function:

    >>> import numpy as np
    >>> from sklearn.metrics import brier_score_loss
    >>> y_true = np.array([0, 1, 1, 0])
    >>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
    >>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])
    >>> y_pred = np.array([0, 1, 1, 0])
    >>> brier_score_loss(y_true, y_prob)
    0.055
    >>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)
    0.055
    >>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
    0.055
    >>> brier_score_loss(y_true, y_prob > 0.5)
    0.0

The Brier score can be used to assess how well a classifier is calibrated. However, a lower Brier score loss does not always mean a better calibration. This is because, by analogy with the bias-variance decomposition of the mean squared error, the Brier score loss can be decomposed as the sum of calibration loss and refinement loss [\[Bella2012\]](#Bella2012). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. Refinement loss can change independently from calibration loss, thus a lower Brier score loss does not necessarily mean a better calibrated model. "Only when refinement loss remains the same does a lower Brier score loss always mean better calibration" [\[Bella2012\]](#Bella2012), [\[Flach2008\]](#Flach2008).

**Examples**

  - See \[sphx\_glr\_auto\_examples\_calibration\_plot\_calibration.py\](\#sphx\_glr\_auto\_examples\_calibration\_plot\_calibration.py) for an example of Brier score loss usage to perform probability calibration of classifiers.

**References**

### Class likelihood ratios

The <span class="title-ref">class\_likelihood\_ratios</span> function computes the [positive and negative likelihood ratios](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing) \(LR_\pm\) for binary classes, which can be interpreted as the ratio of post-test to pre-test odds as explained below. As a consequence, this metric is invariant w.r.t. the class prevalence (the number of samples in the positive class divided by the total number of samples) and **can be extrapolated between populations regardless of any possible class imbalance.**

The \(LR_\pm\) metrics are therefore very useful in settings where the data available to learn and evaluate a classifier is a study population with nearly balanced classes, such as a case-control study, while the target application, i.e. the general population, has very low prevalence.

The positive likelihood ratio \(LR_+\) is the probability of a classifier to correctly predict that a sample belongs to the positive class divided by the probability of predicting the positive class for a sample belonging to the negative class:

\[LR_+ = \frac{\text{PR}(P+|T+)}{\text{PR}(P+|T-)}.\]

The notation here refers to predicted (\(P\)) or true (\(T\)) label and the sign \(+\) and \(-\) refer to the positive and negative class, respectively, e.g. \(P+\) stands for "predicted positive".

Analogously, the negative likelihood ratio \(LR_-\) is the probability of a sample of the positive class being classified as belonging to the negative class divided by the probability of a sample of the negative class being correctly classified:

\[LR_- = \frac{\text{PR}(P-|T+)}{\text{PR}(P-|T-)}.\]

For classifiers above chance \(LR_+\) above 1 **higher is better**, while \(LR_-\) ranges from 0 to 1 and **lower is better**. Values of \(LR_\pm\approx 1\) correspond to chance level.

Notice that probabilities differ from counts, for instance \(\operatorname{PR}(P+|T+)\) is not equal to the number of true positive counts `tp` (see [the wikipedia page](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing) for the actual formulas).

**Examples**

  - \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_likelihood\_ratios.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_likelihood\_ratios.py)

<div class="dropdown">

Interpretation across varying prevalence

Both class likelihood ratios are interpretable in terms of an odds ratio (pre-test and post-tests):

\[\text{post-test odds} = \text{Likelihood ratio} \times \text{pre-test odds}.\]

Odds are in general related to probabilities via

\[\text{odds} = \frac{\text{probability}}{1 - \text{probability}},\]

or equivalently

\[\text{probability} = \frac{\text{odds}}{1 + \text{odds}}.\]

On a given population, the pre-test probability is given by the prevalence. By converting odds to probabilities, the likelihood ratios can be translated into a probability of truly belonging to either class before and after a classifier prediction:

\[\text{post-test odds} = \text{Likelihood ratio} \times
\frac{\text{pre-test probability}}{1 - \text{pre-test probability}},\]

\[\text{post-test probability} = \frac{\text{post-test odds}}{1 + \text{post-test odds}}.\]

</div>

<div class="dropdown">

Mathematical divergences

The positive likelihood ratio is undefined when \(fp = 0\), which can be interpreted as the classifier perfectly identifying positive cases. If \(fp
= 0\) and additionally \(tp = 0\), this leads to a zero/zero division. This happens, for instance, when using a <span class="title-ref">DummyClassifier</span> that always predicts the negative class and therefore the interpretation as a perfect classifier is lost.

The negative likelihood ratio is undefined when \(tn = 0\). Such divergence is invalid, as \(LR_- > 1\) would indicate an increase in the odds of a sample belonging to the positive class after being classified as negative, as if the act of classifying caused the positive condition. This includes the case of a <span class="title-ref">DummyClassifier</span> that always predicts the positive class (i.e. when \(tn=fn=0\)).

Both class likelihood ratios are undefined when \(tp=fn=0\), which means that no samples of the positive class were present in the testing set. This can also happen when cross-validating highly imbalanced data.

In all the previous cases the <span class="title-ref">class\_likelihood\_ratios</span> function raises by default an appropriate warning message and returns <span class="title-ref">nan</span> to avoid pollution when averaging over cross-validation folds.

For a worked-out demonstration of the <span class="title-ref">class\_likelihood\_ratios</span> function, see the example below.

</div>

<div class="dropdown">

References

  - [Wikipedia entry for Likelihood ratios in diagnostic testing](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing)
  - Brenner, H., & Gefeller, O. (1997). Variation of sensitivity, specificity, likelihood ratios and predictive values with disease prevalence. Statistics in medicine, 16(9), 981-991.

</div>

### D² score for classification

The D² score computes the fraction of deviance explained. It is a generalization of R², where the squared error is generalized and replaced by a classification deviance of choice \(\text{dev}(y, \hat{y})\) (e.g., Log loss). D² is a form of a *skill score*. It is calculated as

\[D^2(y, \hat{y}) = 1 - \frac{\text{dev}(y, \hat{y})}{\text{dev}(y, y_{\text{null}})} \,.\]

Where \(y_{\text{null}}\) is the optimal prediction of an intercept-only model (e.g., the per-class proportion of <span class="title-ref">y\_true</span> in the case of the Log loss).

Like R², the best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts \(y_{\text{null}}\), disregarding the input features, would get a D² score of 0.0.

<div class="dropdown">

D2 log loss score

The <span class="title-ref">d2\_log\_loss\_score</span> function implements the special case of D² with the log loss, see \[log\_loss\](\#log\_loss), i.e.:

\[\text{dev}(y, \hat{y}) = \text{log_loss}(y, \hat{y}).\]

Here are some usage examples of the <span class="title-ref">d2\_log\_loss\_score</span> function:

    >>> from sklearn.metrics import d2_log_loss_score
    >>> y_true = [1, 1, 2, 3]
    >>> y_pred = [
    ...    [0.5, 0.25, 0.25],
    ...    [0.5, 0.25, 0.25],
    ...    [0.5, 0.25, 0.25],
    ...    [0.5, 0.25, 0.25],
    ... ]
    >>> d2_log_loss_score(y_true, y_pred)
    0.0
    >>> y_true = [1, 2, 3]
    >>> y_pred = [
    ...     [0.98, 0.01, 0.01],
    ...     [0.01, 0.98, 0.01],
    ...     [0.01, 0.01, 0.98],
    ... ]
    >>> d2_log_loss_score(y_true, y_pred)
    0.981...
    >>> y_true = [1, 2, 3]
    >>> y_pred = [
    ...     [0.1, 0.6, 0.3],
    ...     [0.1, 0.6, 0.3],
    ...     [0.4, 0.5, 0.1],
    ... ]
    >>> d2_log_loss_score(y_true, y_pred)
    -0.552...

</div>

## Multilabel ranking metrics

<div class="currentmodule">

sklearn.metrics

</div>

In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.

### Coverage error

The <span class="title-ref">coverage\_error</span> function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.

\> **Note** \> Our implementation's score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels.

Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as

\[coverage(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
\sum_{i=0}^{n_{\text{samples}} - 1} \max_{j:y_{ij} = 1} \text{rank}_{ij}\]

with \(\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|\). Given the rank definition, ties in `y_scores` are broken by giving the maximal rank that would have been assigned to all tied values.

Here is a small example of usage of this function:

    >>> import numpy as np
    >>> from sklearn.metrics import coverage_error
    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
    >>> coverage_error(y_true, y_score)
    2.5

### Label ranking average precision

The <span class="title-ref">label\_ranking\_average\_precision\_score</span> function implements label ranking average precision (LRAP). This metric is linked to the <span class="title-ref">average\_precision\_score</span> function, but is based on the notion of label ranking instead of precision and recall.

Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the [mean reciprocal rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank).

Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as

\[LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
\sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0}
\sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}\]

where \(\mathcal{L}_{ij} = \left\{k: y_{ik} = 1, \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\), \(\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|\), \(|\cdot|\) computes the cardinality of the set (i.e., the number of elements in the set), and \(||\cdot||_0\) is the \(\ell_0\) "norm" (which computes the number of nonzero elements in a vector).

Here is a small example of usage of this function:

    >>> import numpy as np
    >>> from sklearn.metrics import label_ranking_average_precision_score
    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
    >>> label_ranking_average_precision_score(y_true, y_score)
    0.416...

### Ranking loss

The <span class="title-ref">label\_ranking\_loss</span> function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.

Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as

\[ranking\_loss(y, \hat{f}) =  \frac{1}{n_{\text{samples}}}
\sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0(n_\text{labels} - ||y_i||_0)}
\left|\left\{(k, l): \hat{f}_{ik} \leq \hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \right\}\right|\]

where \(|\cdot|\) computes the cardinality of the set (i.e., the number of elements in the set) and \(||\cdot||_0\) is the \(\ell_0\) "norm" (which computes the number of nonzero elements in a vector).

Here is a small example of usage of this function:

    >>> import numpy as np
    >>> from sklearn.metrics import label_ranking_loss
    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
    >>> label_ranking_loss(y_true, y_score)
    0.75...
    >>> # With the following prediction, we have perfect and minimal loss
    >>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])
    >>> label_ranking_loss(y_true, y_score)
    0.0

<div class="dropdown">

References

  - Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US.

</div>

### Normalized Discounted Cumulative Gain

Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain (NDCG) are ranking metrics implemented in <span class="title-ref">\~sklearn.metrics.dcg\_score</span> and <span class="title-ref">\~sklearn.metrics.ndcg\_score</span> ; they compare a predicted order to ground-truth scores, such as the relevance of answers to a query.

From the Wikipedia page for Discounted Cumulative Gain:

"Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks"

DCG orders the true targets (e.g. relevance of query answers) in the predicted order, then multiplies them by a logarithmic decay and sums the result. The sum can be truncated after the first \(K\) results, in which case we call it <DCG@K>. NDCG, or <NDCG@K> is DCG divided by the DCG obtained by a perfect prediction, so that it is always between 0 and 1. Usually, NDCG is preferred to DCG.

Compared with the ranking loss, NDCG can take into account relevance scores, rather than a ground-truth ranking. So if the ground-truth consists only of an ordering, the ranking loss should be preferred; if the ground-truth consists of actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very relevant), NDCG can be used.

For one sample, given the vector of continuous ground-truth values for each target \(y \in \mathbb{R}^{M}\), where \(M\) is the number of outputs, and the prediction \(\hat{y}\), which induces the ranking function \(f\), the DCG score is

\[\sum_{r=1}^{\min(K, M)}\frac{y_{f(r)}}{\log(1 + r)}\]

and the NDCG score is the DCG score divided by the DCG score obtained for \(y\).

<div class="dropdown">

References

  - [Wikipedia entry for Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
  - Jarvelin, K., & Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446.
  - Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)
  - McSherry, F., & Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg.

</div>

## Regression metrics

<div class="currentmodule">

sklearn.metrics

</div>

The `sklearn.metrics` module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: <span class="title-ref">mean\_squared\_error</span>, <span class="title-ref">mean\_absolute\_error</span>, <span class="title-ref">r2\_score</span>, <span class="title-ref">explained\_variance\_score</span>, <span class="title-ref">mean\_pinball\_loss</span>, <span class="title-ref">d2\_pinball\_score</span> and <span class="title-ref">d2\_absolute\_error\_score</span>.

These functions have a `multioutput` keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is `'uniform_average'`, which specifies a uniformly weighted mean over outputs. If an `ndarray` of shape `(n_outputs,)` is passed, then its entries are interpreted as weights and an according weighted average is returned. If `multioutput` is `'raw_values'`, then all unaltered individual scores or losses will be returned in an array of shape `(n_outputs,)`.

The <span class="title-ref">r2\_score</span> and <span class="title-ref">explained\_variance\_score</span> accept an additional value `'variance_weighted'` for the `multioutput` parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on explaining the higher variance variables.

### R² score, the coefficient of determination

The <span class="title-ref">r2\_score</span> function computes the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), usually denoted as \(R^2\).

It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.

As such variance is dataset dependent, \(R^2\) may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected (average) value of y, disregarding the input features, would get an \(R^2\) score of 0.0.

Note: when the prediction residuals have zero mean, the \(R^2\) score and the \[explained\_variance\_score\](\#explained\_variance\_score) are identical.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value for total \(n\) samples, the estimated \(R^2\) is defined as:

\[R^2(y, \hat{y}) = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\]

where \(\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i\) and \(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \epsilon_i^2\).

Note that <span class="title-ref">r2\_score</span> calculates unadjusted \(R^2\) without correcting for bias in sample variance of y.

In the particular case where the true target is constant, the \(R^2\) score is not finite: it is either `NaN` (perfect predictions) or `-Inf` (imperfect predictions). Such non-finite scores may prevent correct model optimization such as grid-search cross-validation to be performed correctly. For this reason the default behaviour of <span class="title-ref">r2\_score</span> is to replace them with 1.0 (perfect predictions) or 0.0 (imperfect predictions). If `force_finite` is set to `False`, this score falls back on the original \(R^2\) definition.

Here is a small example of usage of the <span class="title-ref">r2\_score</span> function:

    >>> from sklearn.metrics import r2_score
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> r2_score(y_true, y_pred)
    0.948...
    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]
    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')
    0.938...
    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]
    >>> r2_score(y_true, y_pred, multioutput='uniform_average')
    0.936...
    >>> r2_score(y_true, y_pred, multioutput='raw_values')
    array([0.965..., 0.908...])
    >>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])
    0.925...
    >>> y_true = [-2, -2, -2]
    >>> y_pred = [-2, -2, -2]
    >>> r2_score(y_true, y_pred)
    1.0
    >>> r2_score(y_true, y_pred, force_finite=False)
    nan
    >>> y_true = [-2, -2, -2]
    >>> y_pred = [-2, -2, -2 + 1e-8]
    >>> r2_score(y_true, y_pred)
    0.0
    >>> r2_score(y_true, y_pred, force_finite=False)
    -inf

**Examples**

  - See \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_and\_elasticnet.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_lasso\_and\_elasticnet.py) for an example of R² score usage to evaluate Lasso and Elastic Net on sparse signals.

### Mean absolute error

The <span class="title-ref">mean\_absolute\_error</span> function computes [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error), a risk metric corresponding to the expected value of the absolute error loss or \(l1\)-norm loss.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean absolute error (MAE) estimated over \(n_{\text{samples}}\) is defined as

\[\text{MAE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \left| y_i - \hat{y}_i \right|.\]

Here is a small example of usage of the <span class="title-ref">mean\_absolute\_error</span> function:

    >>> from sklearn.metrics import mean_absolute_error
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> mean_absolute_error(y_true, y_pred)
    0.5
    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]
    >>> mean_absolute_error(y_true, y_pred)
    0.75
    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
    array([0.5, 1. ])
    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
    0.85...

### Mean squared error

The <span class="title-ref">mean\_squared\_error</span> function computes [mean square error](https://en.wikipedia.org/wiki/Mean_squared_error), a risk metric corresponding to the expected value of the squared (quadratic) error or loss.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared error (MSE) estimated over \(n_{\text{samples}}\) is defined as

\[\text{MSE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (y_i - \hat{y}_i)^2.\]

Here is a small example of usage of the <span class="title-ref">mean\_squared\_error</span> function:

    >>> from sklearn.metrics import mean_squared_error
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> mean_squared_error(y_true, y_pred)
    0.375
    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]
    >>> mean_squared_error(y_true, y_pred)
    0.7083...

**Examples**

  - See \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regression.py) for an example of mean squared error usage to evaluate gradient boosting regression.

Taking the square root of the MSE, called the root mean squared error (RMSE), is another common metric that provides a measure in the same units as the target variable. RSME is available through the <span class="title-ref">root\_mean\_squared\_error</span> function.

### Mean squared logarithmic error

The <span class="title-ref">mean\_squared\_log\_error</span> function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is defined as

\[\text{MSLE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (\log_e (1 + y_i) - \log_e (1 + \hat{y}_i) )^2.\]

Where \(\log_e (x)\) means the natural logarithm of \(x\). This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.

Here is a small example of usage of the <span class="title-ref">mean\_squared\_log\_error</span> function:

    >>> from sklearn.metrics import mean_squared_log_error
    >>> y_true = [3, 5, 2.5, 7]
    >>> y_pred = [2.5, 5, 4, 8]
    >>> mean_squared_log_error(y_true, y_pred)
    0.039...
    >>> y_true = [[0.5, 1], [1, 2], [7, 6]]
    >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]
    >>> mean_squared_log_error(y_true, y_pred)
    0.044...

The root mean squared logarithmic error (RMSLE) is available through the <span class="title-ref">root\_mean\_squared\_log\_error</span> function.

### Mean absolute percentage error

The <span class="title-ref">mean\_absolute\_percentage\_error</span> (MAPE), also known as mean absolute percentage deviation (MAPD), is an evaluation metric for regression problems. The idea of this metric is to be sensitive to relative errors. It is for example not changed by a global scaling of the target variable.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the mean absolute percentage error (MAPE) estimated over \(n_{\text{samples}}\) is defined as

\[\text{MAPE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \frac{{}\left| y_i - \hat{y}_i \right|}{\max(\epsilon, \left| y_i \right|)}\]

where \(\epsilon\) is an arbitrary small yet strictly positive number to avoid undefined results when y is zero.

The <span class="title-ref">mean\_absolute\_percentage\_error</span> function supports multioutput.

Here is a small example of usage of the <span class="title-ref">mean\_absolute\_percentage\_error</span> function:

    >>> from sklearn.metrics import mean_absolute_percentage_error
    >>> y_true = [1, 10, 1e6]
    >>> y_pred = [0.9, 15, 1.2e6]
    >>> mean_absolute_percentage_error(y_true, y_pred)
    0.2666...

In above example, if we had used <span class="title-ref">mean\_absolute\_error</span>, it would have ignored the small magnitude values and only reflected the error in prediction of highest magnitude value. But that problem is resolved in case of MAPE because it calculates relative percentage error with respect to actual output.

\> **Note** \> The MAPE formula here does not represent the common "percentage" definition: the percentage in the range \[0, 100\] is converted to a relative value in the range \[0, 1\] by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2. The motivation here is to have a range of values that is more consistent with other error metrics in scikit-learn, such as <span class="title-ref">accuracy\_score</span>.

> To obtain the mean absolute percentage error as per the Wikipedia formula, multiply the <span class="title-ref">mean\_absolute\_percentage\_error</span> computed here by 100.

<div class="dropdown">

References

  - [Wikipedia entry for Mean Absolute Percentage Error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)

</div>

### Median absolute error

The <span class="title-ref">median\_absolute\_error</span> is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the median absolute error (MedAE) estimated over \(n_{\text{samples}}\) is defined as

\[\text{MedAE}(y, \hat{y}) = \text{median}(\mid y_1 - \hat{y}_1 \mid, \ldots, \mid y_n - \hat{y}_n \mid).\]

The <span class="title-ref">median\_absolute\_error</span> does not support multioutput.

Here is a small example of usage of the <span class="title-ref">median\_absolute\_error</span> function:

    >>> from sklearn.metrics import median_absolute_error
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> median_absolute_error(y_true, y_pred)
    0.5

### Max error

The <span class="title-ref">max\_error</span> function computes the maximum [residual error](https://en.wikipedia.org/wiki/Errors_and_residuals) , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, `max_error` would be `0` on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the max error is defined as

\[\text{Max Error}(y, \hat{y}) = \max(| y_i - \hat{y}_i |)\]

Here is a small example of usage of the <span class="title-ref">max\_error</span> function:

    >>> from sklearn.metrics import max_error
    >>> y_true = [3, 2, 7, 1]
    >>> y_pred = [9, 2, 7, 1]
    >>> max_error(y_true, y_pred)
    6

The <span class="title-ref">max\_error</span> does not support multioutput.

### Explained variance score

The <span class="title-ref">explained\_variance\_score</span> computes the [explained variance regression score](https://en.wikipedia.org/wiki/Explained_variation).

If \(\hat{y}\) is the estimated target output, \(y\) the corresponding (correct) target output, and \(Var\) is [Variance](https://en.wikipedia.org/wiki/Variance), the square of the standard deviation, then the explained variance is estimated as follow:

\[explained\_{}variance(y, \hat{y}) = 1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}\]

The best possible score is 1.0, lower values are worse.

<div class="topic">

**Link to \[r2\_score\](\#r2\_score)**

The difference between the explained variance score and the \[r2\_score\](\#r2\_score) is that the explained variance score does not account for systematic offset in the prediction. For this reason, the \[r2\_score\](\#r2\_score) should be preferred in general.

</div>

In the particular case where the true target is constant, the Explained Variance score is not finite: it is either `NaN` (perfect predictions) or `-Inf` (imperfect predictions). Such non-finite scores may prevent correct model optimization such as grid-search cross-validation to be performed correctly. For this reason the default behaviour of <span class="title-ref">explained\_variance\_score</span> is to replace them with 1.0 (perfect predictions) or 0.0 (imperfect predictions). You can set the `force_finite` parameter to `False` to prevent this fix from happening and fallback on the original Explained Variance score.

Here is a small example of usage of the <span class="title-ref">explained\_variance\_score</span> function:

    >>> from sklearn.metrics import explained_variance_score
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> explained_variance_score(y_true, y_pred)
    0.957...
    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]
    >>> explained_variance_score(y_true, y_pred, multioutput='raw_values')
    array([0.967..., 1.        ])
    >>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])
    0.990...
    >>> y_true = [-2, -2, -2]
    >>> y_pred = [-2, -2, -2]
    >>> explained_variance_score(y_true, y_pred)
    1.0
    >>> explained_variance_score(y_true, y_pred, force_finite=False)
    nan
    >>> y_true = [-2, -2, -2]
    >>> y_pred = [-2, -2, -2 + 1e-8]
    >>> explained_variance_score(y_true, y_pred)
    0.0
    >>> explained_variance_score(y_true, y_pred, force_finite=False)
    -inf

### Mean Poisson, Gamma, and Tweedie deviances

The <span class="title-ref">mean\_tweedie\_deviance</span> function computes the [mean Tweedie deviance error](https://en.wikipedia.org/wiki/Tweedie_distribution#The_Tweedie_deviance) with a `power` parameter (\(p\)). This is a metric that elicits predicted expectation values of regression targets.

Following special cases exist,

  - when `power=0` it is equivalent to <span class="title-ref">mean\_squared\_error</span>.
  - when `power=1` it is equivalent to <span class="title-ref">mean\_poisson\_deviance</span>.
  - when `power=2` it is equivalent to <span class="title-ref">mean\_gamma\_deviance</span>.

If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean Tweedie deviance error (D) for power \(p\), estimated over \(n_{\text{samples}}\) is defined as

\[\begin{aligned}
\text{D}(y, \hat{y}) = \frac{1}{n_\text{samples}}
\sum_{i=0}^{n_\text{samples} - 1}
\begin{cases}
(y_i-\hat{y}_i)^2, & \text{for }p=0\text{ (Normal)}\\
2(y_i \log(y_i/\hat{y}_i) + \hat{y}_i - y_i),  & \text{for }p=1\text{ (Poisson)}\\
2(\log(\hat{y}_i/y_i) + y_i/\hat{y}_i - 1),  & \text{for }p=2\text{ (Gamma)}\\
2\left(\frac{\max(y_i,0)^{2-p}}{(1-p)(2-p)}-
\frac{y_i\,\hat{y}_i^{1-p}}{1-p}+\frac{\hat{y}_i^{2-p}}{2-p}\right),
& \text{otherwise}
\end{cases}
\end{aligned}\]

Tweedie deviance is a homogeneous function of degree `2-power`. Thus, Gamma distribution with `power=2` means that simultaneously scaling `y_true` and `y_pred` has no effect on the deviance. For Poisson distribution `power=1` the deviance scales linearly, and for Normal distribution (`power=0`), quadratically. In general, the higher `power` the less weight is given to extreme deviations between true and predicted targets.

For instance, let's compare the two predictions 1.5 and 150 that are both 50% larger than their corresponding true value.

The mean squared error (`power=0`) is very sensitive to the prediction difference of the second point,:

    >>> from sklearn.metrics import mean_tweedie_deviance
    >>> mean_tweedie_deviance([1.0], [1.5], power=0)
    0.25
    >>> mean_tweedie_deviance([100.], [150.], power=0)
    2500.0

If we increase `power` to 1,:

    >>> mean_tweedie_deviance([1.0], [1.5], power=1)
    0.18...
    >>> mean_tweedie_deviance([100.], [150.], power=1)
    18.9...

the difference in errors decreases. Finally, by setting, `power=2`:

    >>> mean_tweedie_deviance([1.0], [1.5], power=2)
    0.14...
    >>> mean_tweedie_deviance([100.], [150.], power=2)
    0.14...

we would get identical errors. The deviance when `power=2` is thus only sensitive to relative errors.

### Pinball loss

The <span class="title-ref">mean\_pinball\_loss</span> function is used to evaluate the predictive performance of [quantile regression](https://en.wikipedia.org/wiki/Quantile_regression) models.

\[\text{pinball}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1}  \alpha \max(y_i - \hat{y}_i, 0) + (1 - \alpha) \max(\hat{y}_i - y_i, 0)\]

The value of pinball loss is equivalent to half of <span class="title-ref">mean\_absolute\_error</span> when the quantile parameter `alpha` is set to 0.5.

Here is a small example of usage of the <span class="title-ref">mean\_pinball\_loss</span> function:

    >>> from sklearn.metrics import mean_pinball_loss
    >>> y_true = [1, 2, 3]
    >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)
    0.03...
    >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)
    0.3...
    >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)
    0.3...
    >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)
    0.03...
    >>> mean_pinball_loss(y_true, y_true, alpha=0.1)
    0.0
    >>> mean_pinball_loss(y_true, y_true, alpha=0.9)
    0.0

It is possible to build a scorer object with a specific choice of `alpha`:

    >>> from sklearn.metrics import make_scorer
    >>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)

Such a scorer can be used to evaluate the generalization performance of a quantile regressor via cross-validation:

> \>\>\> from sklearn.datasets import make\_regression \>\>\> from sklearn.model\_selection import cross\_val\_score \>\>\> from sklearn.ensemble import GradientBoostingRegressor \>\>\> \>\>\> X, y = make\_regression(n\_samples=100, random\_state=0) \>\>\> estimator = GradientBoostingRegressor( ... loss="quantile", ... alpha=0.95, ... random\_state=0, ... ) \>\>\> cross\_val\_score(estimator, X, y, cv=5, scoring=mean\_pinball\_loss\_95p) array(\[13.6..., 9.7..., 23.3..., 9.5..., 10.4...\])

It is also possible to build scorer objects for hyper-parameter tuning. The sign of the loss must be switched to ensure that greater means better as explained in the example linked below.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py) for an example of using the pinball loss to evaluate and tune the hyper-parameters of quantile regression models on data with non-symmetric noise and outliers.

### D² score

The D² score computes the fraction of deviance explained. It is a generalization of R², where the squared error is generalized and replaced by a deviance of choice \(\text{dev}(y, \hat{y})\) (e.g., Tweedie, pinball or mean absolute error). D² is a form of a *skill score*. It is calculated as

\[D^2(y, \hat{y}) = 1 - \frac{\text{dev}(y, \hat{y})}{\text{dev}(y, y_{\text{null}})} \,.\]

Where \(y_{\text{null}}\) is the optimal prediction of an intercept-only model (e.g., the mean of <span class="title-ref">y\_true</span> for the Tweedie case, the median for absolute error and the alpha-quantile for pinball loss).

Like R², the best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts \(y_{\text{null}}\), disregarding the input features, would get a D² score of 0.0.

<div class="dropdown">

D² Tweedie score

The <span class="title-ref">d2\_tweedie\_score</span> function implements the special case of D² where \(\text{dev}(y, \hat{y})\) is the Tweedie deviance, see \[mean\_tweedie\_deviance\](\#mean\_tweedie\_deviance). It is also known as D² Tweedie and is related to McFadden's likelihood ratio index.

The argument `power` defines the Tweedie power as for <span class="title-ref">mean\_tweedie\_deviance</span>. Note that for <span class="title-ref">power=0</span>, <span class="title-ref">d2\_tweedie\_score</span> equals <span class="title-ref">r2\_score</span> (for single targets).

A scorer object with a specific choice of `power` can be built by:

    >>> from sklearn.metrics import d2_tweedie_score, make_scorer
    >>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)

</div>

<div class="dropdown">

D² pinball score

The <span class="title-ref">d2\_pinball\_score</span> function implements the special case of D² with the pinball loss, see \[pinball\_loss\](\#pinball\_loss), i.e.:

\[\text{dev}(y, \hat{y}) = \text{pinball}(y, \hat{y}).\]

The argument `alpha` defines the slope of the pinball loss as for <span class="title-ref">mean\_pinball\_loss</span> (\[pinball\_loss\](\#pinball\_loss)). It determines the quantile level `alpha` for which the pinball loss and also D² are optimal. Note that for <span class="title-ref">alpha=0.5</span> (the default) <span class="title-ref">d2\_pinball\_score</span> equals <span class="title-ref">d2\_absolute\_error\_score</span>.

A scorer object with a specific choice of `alpha` can be built by:

    >>> from sklearn.metrics import d2_pinball_score, make_scorer
    >>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)

</div>

<div class="dropdown">

D² absolute error score

The <span class="title-ref">d2\_absolute\_error\_score</span> function implements the special case of the \[mean\_absolute\_error\](\#mean\_absolute\_error):

\[\text{dev}(y, \hat{y}) = \text{MAE}(y, \hat{y}).\]

Here are some usage examples of the <span class="title-ref">d2\_absolute\_error\_score</span> function:

    >>> from sklearn.metrics import d2_absolute_error_score
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> d2_absolute_error_score(y_true, y_pred)
    0.764...
    >>> y_true = [1, 2, 3]
    >>> y_pred = [1, 2, 3]
    >>> d2_absolute_error_score(y_true, y_pred)
    1.0
    >>> y_true = [1, 2, 3]
    >>> y_pred = [2, 2, 2]
    >>> d2_absolute_error_score(y_true, y_pred)
    0.0

</div>

### Visual evaluation of regression models

Among methods to assess the quality of regression models, scikit-learn provides the <span class="title-ref">\~sklearn.metrics.PredictionErrorDisplay</span> class. It allows to visually inspect the prediction errors of a model in two different manners.

[![image](../auto_examples/model_selection/images/sphx_glr_plot_cv_predict_001.png)](../auto_examples/model_selection/plot_cv_predict.html)

The plot on the left shows the actual values vs predicted values. For a noise-free regression task aiming to predict the (conditional) expectation of <span class="title-ref">y</span>, a perfect regression model would display data points on the diagonal defined by predicted equal to actual values. The further away from this optimal line, the larger the error of the model. In a more realistic setting with irreducible noise, that is, when not all the variations of <span class="title-ref">y</span> can be explained by features in <span class="title-ref">X</span>, then the best model would lead to a cloud of points densely arranged around the diagonal.

Note that the above only holds when the predicted values is the expected value of <span class="title-ref">y</span> given <span class="title-ref">X</span>. This is typically the case for regression models that minimize the mean squared error objective function or more generally the \[mean Tweedie deviance \<mean\_tweedie\_deviance\>\](\#mean-tweedie-deviance-\<mean\_tweedie\_deviance\>) for any value of its "power" parameter.

When plotting the predictions of an estimator that predicts a quantile of <span class="title-ref">y</span> given <span class="title-ref">X</span>, e.g. <span class="title-ref">\~sklearn.linear\_model.QuantileRegressor</span> or any other model minimizing the \[pinball loss \<pinball\_loss\>\](\#pinball-loss-\<pinball\_loss\>), a fraction of the points are either expected to lie above or below the diagonal depending on the estimated quantile level.

All in all, while intuitive to read, this plot does not really inform us on what to do to obtain a better model.

The right-hand side plot shows the residuals (i.e. the difference between the actual and the predicted values) vs. the predicted values.

This plot makes it easier to visualize if the residuals follow and [homoscedastic or heteroschedastic](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) distribution.

In particular, if the true distribution of <span class="title-ref">y|X</span> is Poisson or Gamma distributed, it is expected that the variance of the residuals of the optimal model would grow with the predicted value of <span class="title-ref">E\[y|X\]</span> (either linearly for Poisson or quadratically for Gamma).

When fitting a linear least squares regression model (see <span class="title-ref">\~sklearn.linear\_model.LinearRegression</span> and <span class="title-ref">\~sklearn.linear\_model.Ridge</span>), we can use this plot to check if some of the [model assumptions](https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions) are met, in particular that the residuals should be uncorrelated, their expected value should be null and that their variance should be constant (homoschedasticity).

If this is not the case, and in particular if the residuals plot show some banana-shaped structure, this is a hint that the model is likely mis-specified and that non-linear feature engineering or switching to a non-linear regression model might be useful.

Refer to the example below to see a model evaluation that makes use of this display.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_compose\_plot\_transformed\_target.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_transformed\_target.py) for an example on how to use <span class="title-ref">\~sklearn.metrics.PredictionErrorDisplay</span> to visualize the prediction quality improvement of a regression model obtained by transforming the target before learning.

## Clustering metrics

<div class="currentmodule">

sklearn.metrics

</div>

The `sklearn.metrics` module implements several loss, score, and utility functions. For more information see the \[clustering\_evaluation\](\#clustering\_evaluation) section for instance clustering, and \[biclustering\_evaluation\](\#biclustering\_evaluation) for biclustering.

## Dummy estimators

<div class="currentmodule">

sklearn.dummy

</div>

When doing supervised learning, a simple sanity check consists of comparing one's estimator against simple rules of thumb. <span class="title-ref">DummyClassifier</span> implements several such simple strategies for classification:

  - `stratified` generates random predictions by respecting the training set class distribution.

  - `most_frequent` always predicts the most frequent label in the training set.

  - `prior` always predicts the class that maximizes the class prior (like `most_frequent`) and `predict_proba` returns the class prior.

  - `uniform` generates predictions uniformly at random.

  -   - `constant` always predicts a constant label that is provided by the user.  
        A major motivation of this method is F1-scoring, when the positive class is in the minority.

Note that with all these strategies, the `predict` method completely ignores the input data\!

To illustrate <span class="title-ref">DummyClassifier</span>, first let's create an imbalanced dataset:

    >>> from sklearn.datasets import load_iris
    >>> from sklearn.model_selection import train_test_split
    >>> X, y = load_iris(return_X_y=True)
    >>> y[y != 1] = -1
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

Next, let's compare the accuracy of `SVC` and `most_frequent`:

    >>> from sklearn.dummy import DummyClassifier
    >>> from sklearn.svm import SVC
    >>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
    >>> clf.score(X_test, y_test)
    0.63...
    >>> clf = DummyClassifier(strategy='most_frequent', random_state=0)
    >>> clf.fit(X_train, y_train)
    DummyClassifier(random_state=0, strategy='most_frequent')
    >>> clf.score(X_test, y_test)
    0.57...

We see that `SVC` doesn't do much better than a dummy classifier. Now, let's change the kernel:

    >>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)
    >>> clf.score(X_test, y_test)
    0.94...

We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the \[cross\_validation\](\#cross\_validation) section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the \[grid\_search\](\#grid\_search) section for details.

More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc...

<span class="title-ref">DummyRegressor</span> also implements four simple rules of thumb for regression:

  - `mean` always predicts the mean of the training targets.
  - `median` always predicts the median of the training targets.
  - `quantile` always predicts a user provided quantile of the training targets.
  - `constant` always predicts a constant value that is provided by the user.

In all these strategies, the `predict` method completely ignores the input data.

<div id="citations">

  - <span id="Bella2012" class="citation-label">Bella2012</span>  
    Bella, Ferri, Hernández-Orallo, and Ramírez-Quintana ["Calibration of Machine Learning Models"](http://dmip.webs.upv.es/papers/BFHRHandbook2010.pdf) in Khosrow-Pour, M. "Machine learning: concepts, methodologies, tools and applications." Hershey, PA: Information Science Reference (2012).

  - <span id="Brier1950" class="citation-label">Brier1950</span>  
    G. Brier, [Verification of forecasts expressed in terms of probability](ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf), Monthly weather review 78.1 (1950)

  - <span id="Davis2006" class="citation-label">Davis2006</span>  
    J. Davis, M. Goadrich, [The Relationship Between Precision-Recall and ROC Curves](https://www.biostat.wisc.edu/~page/rocpr.pdf), ICML 2006.

  - <span id="Everingham2010" class="citation-label">Everingham2010</span>  
    M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, [The Pascal Visual Object Classes (VOC) Challenge](https://citeseerx.ist.psu.edu/doc_view/pid/b6bebfd529b233f00cb854b7d8070319600cf59d), IJCV 2010.

  - <span id="F2001" class="citation-label">F2001</span>  
    Fawcett, T., 2001. [Using rule sets to maximize ROC performance](https://ieeexplore.ieee.org/document/989510/) In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138.

  - <span id="F2006" class="citation-label">F2006</span>  
    Fawcett, T., 2006. [An introduction to ROC analysis.](http://www.sciencedirect.com/science/article/pii/S016786550500303X) Pattern Recognition Letters, 27(8), pp. 861-874.

  - <span id="FC2009" class="citation-label">FC2009</span>  
    Ferri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009). [An Experimental Comparison of Performance Measures for Classification.](https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf) Pattern Recognition Letters. 30. 27-38.

  - <span id="Flach2008" class="citation-label">Flach2008</span>  
    Flach, Peter, and Edson Matsubara. ["On classification, ranking, and probability estimation."](https://drops.dagstuhl.de/opus/volltexte/2008/1382/) Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).

  - <span id="Flach2015" class="citation-label">Flach2015</span>  
    P.A. Flach, M. Kull, [Precision-Recall-Gain Curves: PR Analysis Done Right](https://papers.nips.cc/paper/5867-precision-recall-gain-curves-pr-analysis-done-right.pdf), NIPS 2015.

  - <span id="Guyon2015" class="citation-label">Guyon2015</span>  
    I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, [Design of the 2015 ChaLearn AutoML Challenge](https://ieeexplore.ieee.org/document/7280767), IJCNN 2015.

  - <span id="HT2001" class="citation-label">HT2001</span>  
    Hand, D.J. and Till, R.J., (2001). [A simple generalisation of the area under the ROC curve for multiple class classification problems.](http://link.springer.com/article/10.1023/A:1010920819831) Machine learning, 45(2), pp. 171-186.

  - <span id="Kelleher2015" class="citation-label">Kelleher2015</span>  
    John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, [Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies](https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics), 2015.

  - <span id="Manning2008" class="citation-label">Manning2008</span>  
    C.D. Manning, P. Raghavan, H. Schütze, [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html), 2008.

  - <span id="Martin1997" class="citation-label">Martin1997</span>  
    A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki, [The DET Curve in Assessment of Detection Task Performance](https://ccc.inaoep.mx/~villasen/bib/martin97det.pdf), NIST 1997.

  - <span id="Mosley2013" class="citation-label">Mosley2013</span>  
    L. Mosley, [A balanced approach to the multi-class imbalance problem](https://lib.dr.iastate.edu/etd/13537/), IJCV 2010.

  - <span id="Navratil2007" class="citation-label">Navratil2007</span>  
    J. Navractil and D. Klusacek, ["On Linear DETs"](https://ieeexplore.ieee.org/document/4218079), 2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07, Honolulu, HI, 2007, pp. IV-229-IV-232.

  - <span id="OB2019" class="citation-label">OB2019</span>  
    `Opitz, J., & Burst, S. (2019). "Macro f1 and macro f1."
    <1911.03347>`

  - <span id="PD2000" class="citation-label">PD2000</span>  
    Provost, F., Domingos, P. (2000). [Well-trained PETs: Improving probability estimation trees](https://fosterprovost.com/publication/well-trained-pets-improving-probability-estimation-trees/) (Section 6.2), CeDER Working Paper \#IS-00-04, Stern School of Business, New York University.

  - <span id="Urbanowicz2015" class="citation-label">Urbanowicz2015</span>  
    Urbanowicz R.J., Moore, J.H. `ExSTraCS 2.0: description
    and evaluation of a scalable learning classifier
    system <10.1007/s12065-015-0128-8>`, Evol. Intel. (2015) 8: 89.

  - <span id="WikipediaDET2017" class="citation-label">WikipediaDET2017</span>  
    Wikipedia contributors. Detection error tradeoff. Wikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC. Available at: <https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054>. Accessed February 19, 2018.

  - <span id="WikipediaMCC2021" class="citation-label">WikipediaMCC2021</span>  
    Wikipedia contributors. Phi coefficient. Wikipedia, The Free Encyclopedia. April 21, 2021, 12:21 CEST. Available at: <https://en.wikipedia.org/wiki/Phi_coefficient> Accessed April 21, 2021.

</div>

---

multiclass.md

---

# Multiclass and multioutput algorithms

This section of the user guide covers functionality related to multi-learning problems, including `multiclass`, `multilabel`, and `multioutput` classification and regression.

The modules in this section implement `meta-estimators`, which require a base estimator to be provided in their constructor. Meta-estimators extend the functionality of the base estimator to support multi-learning problems, which is accomplished by transforming the multi-learning problem into a set of simpler problems, then fitting one estimator per problem.

This section covers two modules: `sklearn.multiclass` and `sklearn.multioutput`. The chart below demonstrates the problem types that each module is responsible for, and the corresponding meta-estimators that each module provides.

![image](../images/multi_org_chart.png)

The table below provides a quick reference on the differences between problem types. More detailed explanations can be found in subsequent sections of this guide.

<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 18%" />
<col style="width: 19%" />
<col style="width: 38%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Number of targets</th>
<th>Target cardinality</th>
<th>Valid <span class="title-ref">~sklearn.utils.multiclass.type_of_target</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Multiclass classification</td>
<td><blockquote>
<p>1</p>
</blockquote></td>
<td>&gt;2</td>
<td>'multiclass'</td>
</tr>
<tr class="even">
<td>Multilabel classification</td>
<td>&gt;1</td>
<td><blockquote>
<p>2 (0 or 1)</p>
</blockquote></td>
<td>'multilabel-indicator'</td>
</tr>
<tr class="odd">
<td>Multiclass-multioutput classification</td>
<td>&gt;1</td>
<td>&gt;2</td>
<td>'multiclass-multioutput'</td>
</tr>
<tr class="even">
<td>Multioutput regression</td>
<td>&gt;1</td>
<td>Continuous</td>
<td>'continuous-multioutput'</td>
</tr>
</tbody>
</table>

Below is a summary of scikit-learn estimators that have multi-learning support built-in, grouped by strategy. You don't need the meta-estimators provided by this section if you're using one of these estimators. However, meta-estimators can provide additional strategies beyond what is built-in:

<div class="currentmodule">

sklearn

</div>

  - **Inherently multiclass:**
      - <span class="title-ref">naive\_bayes.BernoulliNB</span>
      - <span class="title-ref">tree.DecisionTreeClassifier</span>
      - <span class="title-ref">tree.ExtraTreeClassifier</span>
      - <span class="title-ref">ensemble.ExtraTreesClassifier</span>
      - <span class="title-ref">naive\_bayes.GaussianNB</span>
      - <span class="title-ref">neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">semi\_supervised.LabelPropagation</span>
      - <span class="title-ref">semi\_supervised.LabelSpreading</span>
      - <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span>
      - <span class="title-ref">svm.LinearSVC</span> (setting multi\_class="crammer\_singer")
      - <span class="title-ref">linear\_model.LogisticRegression</span> (with most solvers)
      - <span class="title-ref">linear\_model.LogisticRegressionCV</span> (with most solvers)
      - <span class="title-ref">neural\_network.MLPClassifier</span>
      - <span class="title-ref">neighbors.NearestCentroid</span>
      - <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis</span>
      - <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">ensemble.RandomForestClassifier</span>
      - <span class="title-ref">linear\_model.RidgeClassifier</span>
      - <span class="title-ref">linear\_model.RidgeClassifierCV</span>
  - **Multiclass as One-Vs-One:**
      - <span class="title-ref">svm.NuSVC</span>
      - <span class="title-ref">svm.SVC</span>.
      - <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span> (setting multi\_class = "one\_vs\_one")
  - **Multiclass as One-Vs-The-Rest:**
      - <span class="title-ref">ensemble.GradientBoostingClassifier</span>
      - <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span> (setting multi\_class = "one\_vs\_rest")
      - <span class="title-ref">svm.LinearSVC</span> (setting multi\_class="ovr")
      - <span class="title-ref">linear\_model.LogisticRegression</span> (most solvers)
      - <span class="title-ref">linear\_model.LogisticRegressionCV</span> (most solvers)
      - <span class="title-ref">linear\_model.SGDClassifier</span>
      - <span class="title-ref">linear\_model.Perceptron</span>
      - <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>
  - **Support multilabel:**
      - <span class="title-ref">tree.DecisionTreeClassifier</span>
      - <span class="title-ref">tree.ExtraTreeClassifier</span>
      - <span class="title-ref">ensemble.ExtraTreesClassifier</span>
      - <span class="title-ref">neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">neural\_network.MLPClassifier</span>
      - <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">ensemble.RandomForestClassifier</span>
      - <span class="title-ref">linear\_model.RidgeClassifier</span>
      - <span class="title-ref">linear\_model.RidgeClassifierCV</span>
  - **Support multiclass-multioutput:**
      - <span class="title-ref">tree.DecisionTreeClassifier</span>
      - <span class="title-ref">tree.ExtraTreeClassifier</span>
      - <span class="title-ref">ensemble.ExtraTreesClassifier</span>
      - <span class="title-ref">neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">ensemble.RandomForestClassifier</span>

## Multiclass classification

<div class="warning">

<div class="title">

Warning

</div>

All classifiers in scikit-learn do multiclass classification out-of-the-box. You don't need to use the `sklearn.multiclass` module unless you want to experiment with different multiclass strategies.

</div>

**Multiclass classification** is a classification task with more than two classes. Each sample can only be labeled as one class.

For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear. Each image is one sample and is labeled as one of the 3 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a pear and an apple.

While all scikit-learn classifiers are capable of multiclass classification, the meta-estimators offered by `sklearn.multiclass` permit changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error or required computational resources).

### Target format

Valid `multiclass` representations for <span class="title-ref">\~sklearn.utils.multiclass.type\_of\_target</span> (<span class="title-ref">y</span>) are:

  - 1d or column vector containing more than two discrete values. An example of a vector `y` for 4 samples:
    
    > \>\>\> import numpy as np \>\>\> y = np.array(\['apple', 'pear', 'apple', 'orange'\]) \>\>\> print(y) \['apple' 'pear' 'apple' 'orange'\]

  - Dense or sparse `binary` matrix of shape `(n_samples, n_classes)` with a single sample per row, where each column represents one class. An example of both a dense and sparse `binary` matrix `y` for 4 samples, where the columns, in order, are apple, orange, and pear:
    
    > \>\>\> import numpy as np \>\>\> from sklearn.preprocessing import LabelBinarizer \>\>\> y = np.array(\['apple', 'pear', 'apple', 'orange'\]) \>\>\> y\_dense = LabelBinarizer().fit\_transform(y) \>\>\> print(y\_dense) \[\[1 0 0\] \[0 0 1\] \[1 0 0\] \[0 1 0\]\] \>\>\> from scipy import sparse \>\>\> y\_sparse = sparse.csr\_matrix(y\_dense) \>\>\> print(y\_sparse) \<Compressed Sparse Row sparse matrix of dtype 'int64' with 4 stored elements and shape (4, 3)\> Coords Values (0, 0) 1 (1, 2) 1 (2, 0) 1 (3, 1) 1

For more information about <span class="title-ref">\~sklearn.preprocessing.LabelBinarizer</span>, refer to \[preprocessing\_targets\](\#preprocessing\_targets).

### OneVsRestClassifier

The **one-vs-rest** strategy, also known as **one-vs-all**, is implemented in <span class="title-ref">\~sklearn.multiclass.OneVsRestClassifier</span>. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only <span class="title-ref">n\_classes</span> classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.

Below is an example of multiclass learning using OvR:

    >>> from sklearn import datasets
    >>> from sklearn.multiclass import OneVsRestClassifier
    >>> from sklearn.svm import LinearSVC
    >>> X, y = datasets.load_iris(return_X_y=True)
    >>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

<span class="title-ref">\~sklearn.multiclass.OneVsRestClassifier</span> also supports multilabel classification. To use this feature, feed the classifier an indicator matrix, in which cell \[i, j\] indicates the presence of label j in sample i.

![](../auto_examples/miscellaneous/images/sphx_glr_plot_multilabel_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multilabel.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multilabel.py)
  - \[sphx\_glr\_auto\_examples\_classification\_plot\_classification\_probability.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_classification\_probability.py)

### OneVsOneClassifier

<span class="title-ref">\~sklearn.multiclass.OneVsOneClassifier</span> constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers.

Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers, this method is usually slower than one-vs-the-rest, due to its O(n\_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don't scale well with `n_samples`. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used `n_classes` times. The decision function is the result of a monotonic transformation of the one-versus-one classification.

Below is an example of multiclass learning using OvO:

    >>> from sklearn import datasets
    >>> from sklearn.multiclass import OneVsOneClassifier
    >>> from sklearn.svm import LinearSVC
    >>> X, y = datasets.load_iris(return_X_y=True)
    >>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

**References**

  - "Pattern Recognition and Machine Learning. Springer", Christopher M. Bishop, page 183, (First Edition)

### OutputCodeClassifier

Error-Correcting Output Code-based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy. In this implementation, we simply use a randomly-generated code book as advocated in\[1\] although more elaborate methods may be added in the future.

At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen.

In <span class="title-ref">\~sklearn.multiclass.OutputCodeClassifier</span>, the `code_size` attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.

A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, `log2(n_classes) / n_classes` is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since `log2(n_classes)` is much smaller than <span class="title-ref">n\_classes</span>.

A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name "error-correcting". In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging.

Below is an example of multiclass learning using Output-Codes:

    >>> from sklearn import datasets
    >>> from sklearn.multiclass import OutputCodeClassifier
    >>> from sklearn.svm import LinearSVC
    >>> X, y = datasets.load_iris(return_X_y=True)
    >>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)
    >>> clf.fit(X, y).predict(X)
    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
           1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

**References**

  - "Solving multiclass learning problems via error-correcting output codes", Dietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995.
  - "The Elements of Statistical Learning", Hastie T., Tibshirani R., Friedman J., page 606 (second-edition), 2008.

## Multilabel classification

**Multilabel classification** (closely related to **multioutput** **classification**) is a classification task labeling each sample with `m` labels from `n_classes` possible classes, where `m` can be 0 to `n_classes` inclusive. This can be thought of as predicting properties of a sample that are not mutually exclusive. Formally, a binary output is assigned to each class, for every sample. Positive classes are indicated with 1 and negative classes with 0 or -1. It is thus comparable to running `n_classes` binary classification tasks, for example with <span class="title-ref">\~sklearn.multioutput.MultiOutputClassifier</span>. This approach treats each label independently whereas multilabel classifiers *may* treat the multiple classes simultaneously, accounting for correlated behavior among them.

For example, prediction of the topics relevant to a text document or video. The document or video may be about one of 'religion', 'politics', 'finance' or 'education', several of the topic classes or all of the topic classes.

### Target format

A valid representation of `multilabel` <span class="title-ref">y</span> is an either dense or sparse `binary` matrix of shape `(n_samples, n_classes)`. Each column represents a class. The `1`'s in each row denote the positive classes a sample has been labeled with. An example of a dense matrix `y` for 3 samples:

> \>\>\> y = np.array(\[\[1, 0, 0, 1\], \[0, 0, 1, 1\], \[0, 0, 0, 0\]\]) \>\>\> print(y) \[\[1 0 0 1\] \[0 0 1 1\] \[0 0 0 0\]\]

Dense binary matrices can also be created using <span class="title-ref">\~sklearn.preprocessing.MultiLabelBinarizer</span>. For more information, refer to \[preprocessing\_targets\](\#preprocessing\_targets).

An example of the same `y` in sparse matrix form:

> \>\>\> y\_sparse = sparse.csr\_matrix(y) \>\>\> print(y\_sparse) \<Compressed Sparse Row sparse matrix of dtype 'int64' with 4 stored elements and shape (3, 4)\> Coords Values (0, 0) 1 (0, 3) 1 (1, 2) 1 (1, 3) 1

### MultiOutputClassifier

Multilabel classification support can be added to any classifier with <span class="title-ref">\~sklearn.multioutput.MultiOutputClassifier</span>. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3...,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3...,yn).

You can find a usage example for <span class="title-ref">\~sklearn.multioutput.MultiOutputClassifier</span> as part of the section on \[multiclass\_multioutput\_classification\](\#multiclass\_multioutput\_classification) since it is a generalization of multilabel classification to multiclass outputs instead of binary outputs.

### ClassifierChain

Classifier chains (see <span class="title-ref">\~sklearn.multioutput.ClassifierChain</span>) are a way of combining a number of binary classifiers into a single multi-label model that is capable of exploiting correlations among targets.

For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.

When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features.

Clearly the order of the chain is important. The first model in the chain has no information about the other labels while the last model in the chain has features indicating the presence of all of the other labels. In general one does not know the optimal ordering of the models in the chain so typically many randomly ordered chains are fit and their predictions are averaged together.

**References**

  - Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, "Classifier Chains for Multi-label Classification", 2009.

## Multiclass-multioutput classification

**Multiclass-multioutput classification** (also known as **multitask classification**) is a classification task which labels each sample with a set of **non-binary** properties. Both the number of properties and the number of classes per property is greater than 2. A single estimator thus handles several joint classification tasks. This is both a generalization of the multi*label* classification task, which only considers binary attributes, as well as a generalization of the multi*class* classification task, where only one property is considered.

For example, classification of the properties "type of fruit" and "colour" for a set of images of fruit. The property "type of fruit" has the possible classes: "apple", "pear" and "orange". The property "colour" has the possible classes: "green", "red", "yellow" and "orange". Each sample is an image of a fruit, a label is output for both properties and each label is one of the possible classes of the corresponding property.

Note that all classifiers handling multiclass-multioutput (also known as multitask classification) tasks, support the multilabel classification task as a special case. Multitask classification is similar to the multioutput classification task with different model formulations. For more information, see the relevant estimator documentation.

Below is an example of multiclass-multioutput classification:

> \>\>\> from sklearn.datasets import make\_classification \>\>\> from sklearn.multioutput import MultiOutputClassifier \>\>\> from sklearn.ensemble import RandomForestClassifier \>\>\> from sklearn.utils import shuffle \>\>\> import numpy as np \>\>\> X, y1 = make\_classification(n\_samples=10, n\_features=100, ... n\_informative=30, n\_classes=3, ... random\_state=1) \>\>\> y2 = shuffle(y1, random\_state=1) \>\>\> y3 = shuffle(y1, random\_state=2) \>\>\> Y = np.vstack((y1, y2, y3)).T \>\>\> n\_samples, n\_features = X.shape \# 10,100 \>\>\> n\_outputs = Y.shape\[1\] \# 3 \>\>\> n\_classes = 3 \>\>\> forest = RandomForestClassifier(random\_state=1) \>\>\> multi\_target\_forest = MultiOutputClassifier(forest, n\_jobs=2) \>\>\> multi\_target\_forest.fit(X, Y).predict(X) array(\[\[2, 2, 0\], \[1, 2, 1\], \[2, 1, 0\], \[0, 0, 2\], \[0, 2, 1\], \[0, 0, 2\], \[1, 1, 0\], \[1, 1, 1\], \[0, 0, 2\], \[2, 0, 0\]\])

<div class="warning">

<div class="title">

Warning

</div>

At present, no metric in `sklearn.metrics` supports the multiclass-multioutput classification task.

</div>

### Target format

A valid representation of `multioutput` <span class="title-ref">y</span> is a dense matrix of shape `(n_samples, n_classes)` of class labels. A column wise concatenation of 1d `multiclass` variables. An example of `y` for 3 samples:

> \>\>\> y = np.array(\[\['apple', 'green'\], \['orange', 'orange'\], \['pear', 'green'\]\]) \>\>\> print(y) \[\['apple' 'green'\] \['orange' 'orange'\] \['pear' 'green'\]\]

## Multioutput regression

**Multioutput regression** predicts multiple numerical properties for each sample. Each property is a numerical variable and the number of properties to be predicted for each sample is greater than or equal to 2. Some estimators that support multioutput regression are faster than just running `n_output` estimators.

For example, prediction of both wind speed and wind direction, in degrees, using data obtained at a certain location. Each sample would be data obtained at one location and both wind speed and direction would be output for each sample.

The following regressors natively support multioutput regression:

  - <span class="title-ref">cross\_decomposition.CCA</span>
  - <span class="title-ref">tree.DecisionTreeRegressor</span>
  - <span class="title-ref">dummy.DummyRegressor</span>
  - <span class="title-ref">linear\_model.ElasticNet</span>
  - <span class="title-ref">tree.ExtraTreeRegressor</span>
  - <span class="title-ref">ensemble.ExtraTreesRegressor</span>
  - <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>
  - <span class="title-ref">neighbors.KNeighborsRegressor</span>
  - <span class="title-ref">kernel\_ridge.KernelRidge</span>
  - <span class="title-ref">linear\_model.Lars</span>
  - <span class="title-ref">linear\_model.Lasso</span>
  - <span class="title-ref">linear\_model.LassoLars</span>
  - <span class="title-ref">linear\_model.LinearRegression</span>
  - <span class="title-ref">multioutput.MultiOutputRegressor</span>
  - <span class="title-ref">linear\_model.MultiTaskElasticNet</span>
  - <span class="title-ref">linear\_model.MultiTaskElasticNetCV</span>
  - <span class="title-ref">linear\_model.MultiTaskLasso</span>
  - <span class="title-ref">linear\_model.MultiTaskLassoCV</span>
  - <span class="title-ref">linear\_model.OrthogonalMatchingPursuit</span>
  - <span class="title-ref">cross\_decomposition.PLSCanonical</span>
  - <span class="title-ref">cross\_decomposition.PLSRegression</span>
  - <span class="title-ref">linear\_model.RANSACRegressor</span>
  - <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>
  - <span class="title-ref">ensemble.RandomForestRegressor</span>
  - <span class="title-ref">multioutput.RegressorChain</span>
  - <span class="title-ref">linear\_model.Ridge</span>
  - <span class="title-ref">linear\_model.RidgeCV</span>
  - <span class="title-ref">compose.TransformedTargetRegressor</span>

### Target format

A valid representation of `multioutput` <span class="title-ref">y</span> is a dense matrix of shape `(n_samples, n_output)` of floats. A column wise concatenation of `continuous` variables. An example of `y` for 3 samples:

> \>\>\> y = np.array(\[\[31.4, 94\], \[40.5, 109\], \[25.0, 30\]\]) \>\>\> print(y) \[\[ 31.4 94. \] \[ 40.5 109. \] \[ 25. 30. \]\]

### MultiOutputRegressor

Multioutput regression support can be added to any regressor with <span class="title-ref">\~sklearn.multioutput.MultiOutputRegressor</span>. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As <span class="title-ref">\~sklearn.multioutput.MultiOutputRegressor</span> fits one regressor per target it can not take advantage of correlations between targets.

Below is an example of multioutput regression:

> \>\>\> from sklearn.datasets import make\_regression \>\>\> from sklearn.multioutput import MultiOutputRegressor \>\>\> from sklearn.ensemble import GradientBoostingRegressor \>\>\> X, y = make\_regression(n\_samples=10, n\_targets=3, random\_state=1) \>\>\> MultiOutputRegressor(GradientBoostingRegressor(random\_state=0)).fit(X, y).predict(X) array(\[\[-154.75474165, -147.03498585, -50.03812219\], \[ 7.12165031, 5.12914884, -81.46081961\], \[-187.8948621 , -100.44373091, 13.88978285\], \[-141.62745778, 95.02891072, -191.48204257\], \[ 97.03260883, 165.34867495, 139.52003279\], \[ 123.92529176, 21.25719016, -7.84253 \], \[-122.25193977, -85.16443186, -107.12274212\], \[ -30.170388 , -94.80956739, 12.16979946\], \[ 140.72667194, 176.50941682, -17.50447799\], \[ 149.37967282, -81.15699552, -5.72850319\]\])

### RegressorChain

Regressor chains (see <span class="title-ref">\~sklearn.multioutput.RegressorChain</span>) is analogous to <span class="title-ref">\~sklearn.multioutput.ClassifierChain</span> as a way of combining a number of regressions into a single multi-target model that is capable of exploiting correlations among targets.

1.  "The error coding method and PICTs", James G., Hastie T., Journal of Computational and Graphical statistics 7, 1998.

---

naive_bayes.md

---

# Naive Bayes

<div class="currentmodule">

sklearn.naive\_bayes

</div>

Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the "naive" assumption of conditional independence between every pair of features given the value of the class variable. Bayes' theorem states the following relationship, given class variable \(y\) and dependent feature vector \(x_1\) through \(x_n\), :

\[P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots, x_n \mid y)}
                                 {P(x_1, \dots, x_n)}\]

Using the naive conditional independence assumption that

\[P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),\]

for all \(i\), this relationship is simplified to

\[P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
                                 {P(x_1, \dots, x_n)}\]

Since \(P(x_1, \dots, x_n)\) is constant given the input, we can use the following classification rule:

\[P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)\]\[\Downarrow\]\[\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),\]

and we can use Maximum A Posteriori (MAP) estimation to estimate \(P(y)\) and \(P(x_i \mid y)\); the former is then the relative frequency of class \(y\) in the training set.

The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).

In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)

Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.

On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from `predict_proba` are not to be taken too seriously.

<div class="dropdown">

References

  - H. Zhang (2004). [The optimality of Naive Bayes.](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf) Proc. FLAIRS.

</div>

## Gaussian Naive Bayes

<span class="title-ref">GaussianNB</span> implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:

\[P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)\]

The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.

> \>\>\> from sklearn.datasets import load\_iris \>\>\> from sklearn.model\_selection import train\_test\_split \>\>\> from sklearn.naive\_bayes import GaussianNB \>\>\> X, y = load\_iris(return\_X\_y=True) \>\>\> X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, test\_size=0.5, random\_state=0) \>\>\> gnb = GaussianNB() \>\>\> y\_pred = gnb.fit(X\_train, y\_train).predict(X\_test) \>\>\> print("Number of mislabeled points out of a total %d points : %d" ... % (X\_test.shape\[0\], (y\_test \!= y\_pred).sum())) Number of mislabeled points out of a total 75 points : 4

## Multinomial Naive Bayes

<span class="title-ref">MultinomialNB</span> implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors \(\theta_y = (\theta_{y1},\ldots,\theta_{yn})\) for each class \(y\), where \(n\) is the number of features (in text classification, the size of the vocabulary) and \(\theta_{yi}\) is the probability \(P(x_i \mid y)\) of feature \(i\) appearing in a sample belonging to class \(y\).

The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:

\[\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}\]

where \(N_{yi} = \sum_{x \in T} x_i\) is the number of times feature \(i\) appears in all samples of class \(y\) in the training set \(T\), and \(N_{y} = \sum_{i=1}^{n} N_{yi}\) is the total count of all features for class \(y\).

The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha < 1\) is called Lidstone smoothing.

## Complement Naive Bayes

<span class="title-ref">ComplementNB</span> implements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the *complement* of each class to compute the model's weights. The inventors of CNB show empirically that the parameter estimates for CNB are more stable than those for MNB. Further, CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks.

<div class="dropdown">

Weights calculation

The procedure for calculating the weights is as follows:

\[\hat{\theta}_{ci} = \frac{\alpha_i + \sum_{j:y_j \neq c} d_{ij}}
                        {\alpha + \sum_{j:y_j \neq c} \sum_{k} d_{kj}}\]\[w_{ci} = \log \hat{\theta}_{ci}\]\[w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}\]

where the summations are over all documents \(j\) not in class \(c\), \(d_{ij}\) is either the count or tf-idf value of term \(i\) in document \(j\), \(\alpha_i\) is a smoothing hyperparameter like that found in MNB, and \(\alpha = \sum_{i} \alpha_i\). The second normalization addresses the tendency for longer documents to dominate parameter estimates in MNB. The classification rule is:

\[\hat{c} = \arg\min_c \sum_{i} t_i w_{ci}\]

i.e., a document is assigned to the class that is the *poorest* complement match.

</div>

<div class="dropdown">

References

  - Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). [Tackling the poor assumptions of naive bayes text classifiers.](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) In ICML (Vol. 3, pp. 616-623).

</div>

## Bernoulli Naive Bayes

<span class="title-ref">BernoulliNB</span> implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a <span class="title-ref">BernoulliNB</span> instance may binarize its input (depending on the `binarize` parameter).

The decision rule for Bernoulli naive Bayes is based on

\[P(x_i \mid y) = P(x_i = 1 \mid y) x_i + (1 - P(x_i = 1 \mid y)) (1 - x_i)\]

which differs from multinomial NB's rule in that it explicitly penalizes the non-occurrence of a feature \(i\) that is an indicator for class \(y\), where the multinomial variant would simply ignore a non-occurring feature.

In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. <span class="title-ref">BernoulliNB</span> might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.

<div class="dropdown">

References

  - C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265.
  - A. McCallum and K. Nigam (1998). [A comparison of event models for Naive Bayes text classification.](https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc) Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.
  - V. Metsis, I. Androutsopoulos and G. Paliouras (2006). [Spam filtering with Naive Bayes -- Which Naive Bayes?](https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757) 3rd Conf. on Email and Anti-Spam (CEAS).

</div>

## Categorical Naive Bayes

<span class="title-ref">CategoricalNB</span> implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index \(i\), has its own categorical distribution.

For each feature \(i\) in the training set \(X\), <span class="title-ref">CategoricalNB</span> estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as \(J = \{ 1, \dots, m \}\), with \(m\) as the number of samples.

<div class="dropdown">

Probability calculation

The probability of category \(t\) in feature \(i\) given class \(c\) is estimated as:

\[P(x_i = t \mid y = c \: ;\, \alpha) = \frac{ N_{tic} + \alpha}{N_{c} +
                                       \alpha n_i},\]

where \(N_{tic} = |\{j \in J \mid x_{ij} = t, y_j = c\}|\) is the number of times category \(t\) appears in the samples \(x_{i}\), which belong to class \(c\), \(N_{c} = |\{ j \in J\mid y_j = c\}|\) is the number of samples with class c, \(\alpha\) is a smoothing parameter and \(n_i\) is the number of available categories of feature \(i\).

</div>

<span class="title-ref">CategoricalNB</span> assumes that the sample matrix \(X\) is encoded (for instance with the help of <span class="title-ref">\~sklearn.preprocessing.OrdinalEncoder</span>) such that all categories for each feature \(i\) are represented with numbers \(0, ..., n_i - 1\) where \(n_i\) is the number of available categories of feature \(i\).

## Out-of-core naive Bayes model fitting

Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, <span class="title-ref">MultinomialNB</span>, <span class="title-ref">BernoulliNB</span>, and <span class="title-ref">GaussianNB</span> expose a `partial_fit` method that can be used incrementally as done with other classifiers as demonstrated in \[sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py). All naive Bayes classifiers support sample weighting.

Contrary to the `fit` method, the first call to `partial_fit` needs to be passed the list of all the expected class labels.

For an overview of available strategies in scikit-learn, see also the \[out-of-core learning \<scaling\_strategies\>\](\#out-of-core-learning-\<scaling\_strategies\>) documentation.

\> **Note** \> The `partial_fit` method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.

---

neighbors.md

---

# Nearest Neighbors

<div class="sectionauthor">

Jake Vanderplas \<<vanderplas@astro.washington.edu>\>

</div>

<div class="currentmodule">

sklearn.neighbors

</div>

`sklearn.neighbors` provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: [classification](#classification) for data with discrete labels, and [regression](#regression) for data with continuous labels.

The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as *non-generalizing* machine learning methods, since they simply "remember" all of its training data (possibly transformed into a fast indexing structure such as a \[Ball Tree \<ball\_tree\>\](\#ball-tree-\<ball\_tree\>) or \[KD Tree \<kd\_tree\>\](\#kd-tree-\<kd\_tree\>)).

Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.

The classes in `sklearn.neighbors` can handle either NumPy arrays or <span class="title-ref">scipy.sparse</span> matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.

There are many learning routines which rely on nearest neighbors at their core. One example is \[kernel density estimation \<kernel\_density\>\](\#kernel-density-estimation-\<kernel\_density\>), discussed in the \[density estimation \<density\_estimation\>\](\#density-estimation-\<density\_estimation\>) section.

## Unsupervised Nearest Neighbors

<span class="title-ref">NearestNeighbors</span> implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: <span class="title-ref">BallTree</span>, <span class="title-ref">KDTree</span>, and a brute-force algorithm based on routines in `sklearn.metrics.pairwise`. The choice of neighbors search algorithm is controlled through the keyword `'algorithm'`, which must be one of `['auto', 'ball_tree', 'kd_tree', 'brute']`. When the default value `'auto'` is passed, the algorithm attempts to determine the best approach from the training data. For a discussion of the strengths and weaknesses of each option, see [Nearest Neighbor Algorithms](#nearest-neighbor-algorithms).

\> **Warning** \> Regarding the Nearest Neighbors algorithms, if two neighbors \(k+1\) and \(k\) have identical distances but different labels, the result will depend on the ordering of the training data.

### Finding the Nearest Neighbors

For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within `sklearn.neighbors` can be used:

> \>\>\> from sklearn.neighbors import NearestNeighbors \>\>\> import numpy as np \>\>\> X = np.array(\[\[-1, -1\], \[-2, -1\], \[-3, -2\], \[1, 1\], \[2, 1\], \[3, 2\]\]) \>\>\> nbrs = NearestNeighbors(n\_neighbors=2, algorithm='ball\_tree').fit(X) \>\>\> distances, indices = nbrs.kneighbors(X) \>\>\> indices array(\[\[0, 1\], \[1, 0\], \[2, 1\], \[3, 4\], \[4, 3\], \[5, 4\]\]...) \>\>\> distances array(\[\[0. , 1. \], \[0. , 1. \], \[0. , 1.41421356\], \[0. , 1. \], \[0. , 1. \], \[0. , 1.41421356\]\])

Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero.

It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:

> \>\>\> nbrs.kneighbors\_graph(X).toarray() array(\[\[1., 1., 0., 0., 0., 0.\], \[1., 1., 0., 0., 0., 0.\], \[0., 1., 1., 0., 0., 0.\], \[0., 0., 0., 1., 1., 0.\], \[0., 0., 0., 1., 1., 0.\], \[0., 0., 0., 0., 1., 1.\]\])

The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see <span class="title-ref">\~sklearn.manifold.Isomap</span>, <span class="title-ref">\~sklearn.manifold.LocallyLinearEmbedding</span>, and <span class="title-ref">\~sklearn.cluster.SpectralClustering</span>.

### KDTree and BallTree Classes

Alternatively, one can use the <span class="title-ref">KDTree</span> or <span class="title-ref">BallTree</span> classes directly to find nearest neighbors. This is the functionality wrapped by the <span class="title-ref">NearestNeighbors</span> class used above. The Ball Tree and KD Tree have the same interface; we'll show an example of using the KD Tree here:

> \>\>\> from sklearn.neighbors import KDTree \>\>\> import numpy as np \>\>\> X = np.array(\[\[-1, -1\], \[-2, -1\], \[-3, -2\], \[1, 1\], \[2, 1\], \[3, 2\]\]) \>\>\> kdt = KDTree(X, leaf\_size=30, metric='euclidean') \>\>\> kdt.query(X, k=2, return\_distance=False) array(\[\[0, 1\], \[1, 0\], \[2, 1\], \[3, 4\], \[4, 3\], \[5, 4\]\]...)

Refer to the <span class="title-ref">KDTree</span> and <span class="title-ref">BallTree</span> class documentation for more information on the options available for nearest neighbors searches, including specification of query strategies, distance metrics, etc. For a list of valid metrics use <span class="title-ref">KDTree.valid\_metrics</span> and \`BallTree.valid\_metrics\`:

> \>\>\> from sklearn.neighbors import KDTree, BallTree \>\>\> KDTree.valid\_metrics \['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity'\] \>\>\> BallTree.valid\_metrics \['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity', 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc'\]

## Nearest Neighbors Classification

Neighbors-based classification is a type of *instance-based learning* or *non-generalizing learning*: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.

scikit-learn implements two different nearest neighbors classifiers: <span class="title-ref">KNeighborsClassifier</span> implements learning based on the \(k\) nearest neighbors of each query point, where \(k\) is an integer value specified by the user. <span class="title-ref">RadiusNeighborsClassifier</span> implements learning based on the number of neighbors within a fixed radius \(r\) of each training point, where \(r\) is a floating-point value specified by the user.

The \(k\)-neighbors classification in <span class="title-ref">KNeighborsClassifier</span> is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.

In cases where the data is not uniformly sampled, radius-based neighbors classification in <span class="title-ref">RadiusNeighborsClassifier</span> can be a better choice. The user specifies a fixed radius \(r\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called "curse of dimensionality".

The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the `weights` keyword. The default value, `weights = 'uniform'`, assigns uniform weights to each neighbor. `weights = 'distance'` assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.

<div class="centered">

[![classification\_1](../auto_examples/neighbors/images/sphx_glr_plot_classification_001.png)](../auto_examples/neighbors/plot_classification.html)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_classification.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_classification.py): an example of classification using nearest neighbors.

## Nearest Neighbors Regression

Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors.

scikit-learn implements two different neighbors regressors: <span class="title-ref">KNeighborsRegressor</span> implements learning based on the \(k\) nearest neighbors of each query point, where \(k\) is an integer value specified by the user. <span class="title-ref">RadiusNeighborsRegressor</span> implements learning based on the neighbors within a fixed radius \(r\) of the query point, where \(r\) is a floating-point value specified by the user.

The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the `weights` keyword. The default value, `weights = 'uniform'`, assigns equal weights to all points. `weights = 'distance'` assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.

![](../auto_examples/neighbors/images/sphx_glr_plot_regression_001.png)

The use of multi-output nearest neighbors for regression is demonstrated in \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py). In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.

![](../auto_examples/miscellaneous/images/sphx_glr_plot_multioutput_face_completion_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_regression.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_regression.py): an example of regression using nearest neighbors.
  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py): an example of multi-output regression using nearest neighbors.

## Nearest Neighbor Algorithms

### Brute Force

Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for \(N\) samples in \(D\) dimensions, this approach scales as \(O[D N^2]\). Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples \(N\) grows, the brute-force approach quickly becomes infeasible. In the classes within `sklearn.neighbors`, brute-force neighbors searches are specified using the keyword `algorithm = 'brute'`, and are computed using the routines available in `sklearn.metrics.pairwise`.

### K-D Tree

To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point \(A\) is very distant from point \(B\), and point \(B\) is very close to point \(C\), then we know that points \(A\) and \(C\) are very distant, *without having to explicitly calculate their distance*. In this way, the computational cost of a nearest neighbors search can be reduced to \(O[D N \log(N)]\) or better. This is a significant improvement over brute-force for large \(N\).

An early approach to taking advantage of this aggregate information was the *KD tree* data structure (short for *K-dimensional tree*), which generalizes two-dimensional *Quad-trees* and 3-dimensional *Oct-trees* to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no \(D\)-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only \(O[\log(N)]\) distance computations. Though the KD tree approach is very fast for low-dimensional (\(D < 20\)) neighbors searches, it becomes inefficient as \(D\) grows very large: this is one manifestation of the so-called "curse of dimensionality". In scikit-learn, KD tree neighbors searches are specified using the keyword `algorithm = 'kd_tree'`, and are computed using the class <span class="title-ref">KDTree</span>.

<div class="dropdown">

References

  - ["Multidimensional binary search trees used for associative searching"](https://dl.acm.org/citation.cfm?doid=361002.361007), Bentley, J.L., Communications of the ACM (1975)

</div>

### Ball Tree

To address the inefficiencies of KD Trees in higher dimensions, the *ball tree* data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.

A ball tree recursively divides the data into nodes defined by a centroid \(C\) and radius \(r\), such that each point in the node lies within the hyper-sphere defined by \(r\) and \(C\). The number of candidate points for a neighbor search is reduced through use of the *triangle inequality*:

\[|x+y| \leq |x| + |y|\]

With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a *KD-tree* in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword `algorithm = 'ball_tree'`, and are computed using the class <span class="title-ref">BallTree</span>. Alternatively, the user can work with the <span class="title-ref">BallTree</span> class directly.

<div class="dropdown">

References

  - ["Five Balltree Construction Algorithms"](https://citeseerx.ist.psu.edu/doc_view/pid/17ac002939f8e950ffb32ec4dc8e86bdd8cb5ff1), Omohundro, S.M., International Computer Science Institute Technical Report (1989)

</div>

<div class="dropdown">

Choice of Nearest Neighbors Algorithm

The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:

  - number of samples \(N\) (i.e. `n_samples`) and dimensionality \(D\) (i.e. `n_features`).
    
      - *Brute force* query time grows as \(O[D N]\)
      - *Ball tree* query time grows as approximately \(O[D \log(N)]\)
      - *KD tree* query time changes with \(D\) in a way that is difficult to precisely characterise. For small \(D\) (less than 20 or so) the cost is approximately \(O[D\log(N)]\), and the KD tree query can be very efficient. For larger \(D\), the cost increases to nearly \(O[DN]\), and the overhead due to the tree structure can lead to queries which are slower than brute force.
    
    For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both <span class="title-ref">KDTree</span> and <span class="title-ref">BallTree</span> address this through providing a *leaf size* parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).

  - data structure: *intrinsic dimensionality* of the data and/or *sparsity* of the data. Intrinsic dimensionality refers to the dimension \(d \le D\) of a manifold on which the data lies, which can be linearly or non-linearly embedded in the parameter space. Sparsity refers to the degree to which the data fills the parameter space (this is to be distinguished from the concept as used in "sparse" matrices. The data matrix may have no zero entries, but the **structure** can still be "sparse" in this sense).
    
      - *Brute force* query time is unchanged by data structure.
      - *Ball tree* and *KD tree* query times can be greatly influenced by data structure. In general, sparser data with a smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily structured data.
    
    Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries.

  - number of neighbors \(k\) requested for a query point.
    
      - *Brute force* query time is largely unaffected by the value of \(k\)
      - *Ball tree* and *KD tree* query time will become slower as \(k\) increases. This is due to two effects: first, a larger \(k\) leads to the necessity to search a larger portion of the parameter space. Second, using \(k > 1\) requires internal queueing of results as the tree is traversed.
    
    As \(k\) becomes large compared to \(N\), the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.

  - number of query points. Both the ball tree and the KD Tree require a construction phase. The cost of this construction becomes negligible when amortized over many queries. If only a small number of queries will be performed, however, the construction can make up a significant fraction of the total cost. If very few query points will be required, brute force is better than a tree-based method.

Currently, `algorithm = 'auto'` selects `'brute'` if any of the following conditions are verified:

  - input data is sparse
  - `metric = 'precomputed'`
  - \(D > 15\)
  - \(k >= N/2\)
  - `effective_metric_` isn't in the `VALID_METRICS` list for either `'kd_tree'` or `'ball_tree'`

Otherwise, it selects the first out of `'kd_tree'` and `'ball_tree'` that has `effective_metric_` in its `VALID_METRICS` list. This heuristic is based on the following assumptions:

  - the number of query points is at least the same order as the number of training points
  - `leaf_size` is close to its default value of `30`
  - when \(D > 15\), the intrinsic dimensionality of the data is generally too high for tree-based methods

</div>

<div class="dropdown">

Effect of `leaf_size`

As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter `leaf_size`. This parameter choice has many effects:

  - **construction time**  
    A larger `leaf_size` leads to a faster tree construction time, because fewer nodes need to be created

  - **query time**  
    Both a large or small `leaf_size` can lead to suboptimal query cost. For `leaf_size` approaching 1, the overhead involved in traversing nodes can significantly slow query times. For `leaf_size` approaching the size of the training set, queries become essentially brute force. A good compromise between these is `leaf_size = 30`, the default value of the parameter.

  - **memory**  
    As `leaf_size` increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a \(D\)-dimensional centroid for each node. The required storage space for <span class="title-ref">BallTree</span> is approximately `1 / leaf_size` times the size of the training set.

`leaf_size` is not referenced for brute force queries.

</div>

<div class="dropdown">

Valid Metrics for Nearest Neighbor Algorithms

For a list of available metrics, see the documentation of the <span class="title-ref">\~sklearn.metrics.DistanceMetric</span> class and the metrics listed in <span class="title-ref">sklearn.metrics.pairwise.PAIRWISE\_DISTANCE\_FUNCTIONS</span>. Note that the "cosine" metric uses <span class="title-ref">\~sklearn.metrics.pairwise.cosine\_distances</span>.

A list of valid metrics for any of the above algorithms can be obtained by using their `valid_metric` attribute. For example, valid metrics for `KDTree` can be generated by:

> \>\>\> from sklearn.neighbors import KDTree \>\>\> print(sorted(KDTree.valid\_metrics)) \['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p'\]

</div>

## Nearest Centroid Classifier

The <span class="title-ref">NearestCentroid</span> classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the <span class="title-ref">\~sklearn.cluster.KMeans</span> algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (<span class="title-ref">\~sklearn.discriminant\_analysis.LinearDiscriminantAnalysis</span>) and Quadratic Discriminant Analysis (<span class="title-ref">\~sklearn.discriminant\_analysis.QuadraticDiscriminantAnalysis</span>) for more complex methods that do not make this assumption. Usage of the default <span class="title-ref">NearestCentroid</span> is simple:

> \>\>\> from sklearn.neighbors import NearestCentroid \>\>\> import numpy as np \>\>\> X = np.array(\[\[-1, -1\], \[-2, -1\], \[-3, -2\], \[1, 1\], \[2, 1\], \[3, 2\]\]) \>\>\> y = np.array(\[1, 1, 1, 2, 2, 2\]) \>\>\> clf = NearestCentroid() \>\>\> clf.fit(X, y) NearestCentroid() \>\>\> print(clf.predict(\[\[-0.8, -1\]\])) \[1\]

### Nearest Shrunken Centroid

The <span class="title-ref">NearestCentroid</span> classifier has a `shrink_threshold` parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by `shrink_threshold`. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.

In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.

<div class="centered">

[![nearest\_centroid\_1](../auto_examples/neighbors/images/sphx_glr_plot_nearest_centroid_001.png)](../auto_examples/neighbors/plot_nearest_centroid.html) [![nearest\_centroid\_2](../auto_examples/neighbors/images/sphx_glr_plot_nearest_centroid_002.png)](../auto_examples/neighbors/plot_nearest_centroid.html)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_nearest\_centroid.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_nearest\_centroid.py): an example of classification using nearest centroid with different shrink thresholds.

## Nearest Neighbors Transformer

Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as <span class="title-ref">KNeighborsClassifier</span> and <span class="title-ref">KNeighborsRegressor</span>, but also some clustering methods such as <span class="title-ref">\~sklearn.cluster.DBSCAN</span> and <span class="title-ref">\~sklearn.cluster.SpectralClustering</span>, and some manifold embeddings such as <span class="title-ref">\~sklearn.manifold.TSNE</span> and <span class="title-ref">\~sklearn.manifold.Isomap</span>.

All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors `sparse graph`, as given by <span class="title-ref">\~sklearn.neighbors.kneighbors\_graph</span> and <span class="title-ref">\~sklearn.neighbors.radius\_neighbors\_graph</span>. With mode <span class="title-ref">mode='connectivity'</span>, these functions return a binary adjacency sparse graph as required, for instance, in <span class="title-ref">\~sklearn.cluster.SpectralClustering</span>. Whereas with <span class="title-ref">mode='distance'</span>, they return a distance sparse graph as required, for instance, in <span class="title-ref">\~sklearn.cluster.DBSCAN</span>. To include these functions in a scikit-learn pipeline, one can also use the corresponding classes <span class="title-ref">KNeighborsTransformer</span> and <span class="title-ref">RadiusNeighborsTransformer</span>. The benefits of this sparse graph API are multiple.

First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline:

> \>\>\> import tempfile \>\>\> from sklearn.manifold import Isomap \>\>\> from sklearn.neighbors import KNeighborsTransformer \>\>\> from sklearn.pipeline import make\_pipeline \>\>\> from sklearn.datasets import make\_regression \>\>\> cache\_path = tempfile.gettempdir() \# we use a temporary folder here \>\>\> X, \_ = make\_regression(n\_samples=50, n\_features=25, random\_state=0) \>\>\> estimator = make\_pipeline( ... KNeighborsTransformer(mode='distance'), ... Isomap(n\_components=3, metric='precomputed'), ... memory=cache\_path) \>\>\> X\_embedded = estimator.fit\_transform(X) \>\>\> X\_embedded.shape (50, 3)

Second, precomputing the graph can give finer control on the nearest neighbors estimation, for instance enabling multiprocessing though the parameter <span class="title-ref">n\_jobs</span>, which might not be available in all estimators.

Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors `sparse graph` needs to be formatted as in <span class="title-ref">\~sklearn.neighbors.radius\_neighbors\_graph</span> output:

  - a CSR matrix (although COO, CSC or LIL will be accepted).
  - only explicitly store nearest neighborhoods of each sample with respect to the training data. This should include those at 0 distance from a query point, including the matrix diagonal when computing the nearest neighborhoods between the training data and itself.
  - each row's <span class="title-ref">data</span> should store the distance in increasing order (optional. Unsorted data will be stable-sorted, adding a computational overhead).
  - all values in data should be non-negative.
  - there should be no duplicate <span class="title-ref">indices</span> in any row (see <https://github.com/scipy/scipy/issues/5807>).
  - if the algorithm being passed the precomputed matrix uses k nearest neighbors (as opposed to radius neighborhood), at least k neighbors must be stored in each row (or k+1, as explained in the following note).

<div class="note">

<div class="title">

Note

</div>

When a specific number of neighbors is queried (using <span class="title-ref">KNeighborsTransformer</span>), the definition of <span class="title-ref">n\_neighbors</span> is ambiguous since it can either include each training point as its own neighbor, or exclude them. Neither choice is perfect, since including them leads to a different number of non-self neighbors during training and testing, while excluding them leads to a difference between <span class="title-ref">fit(X).transform(X)</span> and <span class="title-ref">fit\_transform(X)</span>, which is against scikit-learn API. In <span class="title-ref">KNeighborsTransformer</span> we use the definition which includes each training point as its own neighbor in the count of <span class="title-ref">n\_neighbors</span>. However, for compatibility reasons with other estimators which use the other definition, one extra neighbor will be computed when <span class="title-ref">mode == 'distance'</span>. To maximise compatibility with all estimators, a safe choice is to always include one extra neighbor in a custom nearest neighbors estimator, since unnecessary neighbors will be filtered by following estimators.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_neighbors\_approximate\_nearest\_neighbors.py\](\#sphx\_glr\_auto\_examples\_neighbors\_approximate\_nearest\_neighbors.py): an example of pipelining <span class="title-ref">KNeighborsTransformer</span> and <span class="title-ref">\~sklearn.manifold.TSNE</span>. Also proposes two custom nearest neighbors estimators based on external packages.
  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_caching\_nearest\_neighbors.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_caching\_nearest\_neighbors.py): an example of pipelining <span class="title-ref">KNeighborsTransformer</span> and <span class="title-ref">KNeighborsClassifier</span> to enable caching of the neighbors graph during a hyper-parameter grid-search.

## Neighborhood Components Analysis

<div class="sectionauthor">

William de Vazelhes \<<william.de-vazelhes@inria.fr>\>

</div>

Neighborhood Components Analysis (NCA, <span class="title-ref">NeighborhoodComponentsAnalysis</span>) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification.

<div class="centered">

[![nca\_illustration\_1](../auto_examples/neighbors/images/sphx_glr_plot_nca_illustration_001.png)](../auto_examples/neighbors/plot_nca_illustration.html) [![nca\_illustration\_2](../auto_examples/neighbors/images/sphx_glr_plot_nca_illustration_002.png)](../auto_examples/neighbors/plot_nca_illustration.html)

</div>

In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the \[mathematical formulation \<nca\_mathematical\_formulation\>\](\#mathematical-formulation-\<nca\_mathematical\_formulation\>) for more details.

### Classification

Combined with a nearest neighbors classifier (<span class="title-ref">KNeighborsClassifier</span>), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user.

NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries.

To use this model for classification, one needs to combine a <span class="title-ref">NeighborhoodComponentsAnalysis</span> instance that learns the optimal transformation with a <span class="title-ref">KNeighborsClassifier</span> instance that performs the classification in the projected space. Here is an example using the two classes:

> \>\>\> from sklearn.neighbors import (NeighborhoodComponentsAnalysis, ... KNeighborsClassifier) \>\>\> from sklearn.datasets import load\_iris \>\>\> from sklearn.model\_selection import train\_test\_split \>\>\> from sklearn.pipeline import Pipeline \>\>\> X, y = load\_iris(return\_X\_y=True) \>\>\> X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, ... stratify=y, test\_size=0.7, random\_state=42) \>\>\> nca = NeighborhoodComponentsAnalysis(random\_state=42) \>\>\> knn = KNeighborsClassifier(n\_neighbors=3) \>\>\> nca\_pipe = Pipeline(\[('nca', nca), ('knn', knn)\]) \>\>\> nca\_pipe.fit(X\_train, y\_train) Pipeline(...) \>\>\> print(nca\_pipe.score(X\_test, y\_test)) 0.96190476...

<div class="centered">

[![nca\_classification\_1](../auto_examples/neighbors/images/sphx_glr_plot_nca_classification_001.png)](../auto_examples/neighbors/plot_nca_classification.html) [![nca\_classification\_2](../auto_examples/neighbors/images/sphx_glr_plot_nca_classification_002.png)](../auto_examples/neighbors/plot_nca_classification.html)

</div>

The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes.

### Dimensionality reduction

NCA can be used to perform supervised dimensionality reduction. The input data are projected onto a linear subspace consisting of the directions which minimize the NCA objective. The desired dimensionality can be set using the parameter `n_components`. For instance, the following figure shows a comparison of dimensionality reduction with Principal Component Analysis (<span class="title-ref">\~sklearn.decomposition.PCA</span>), Linear Discriminant Analysis (<span class="title-ref">\~sklearn.discriminant\_analysis.LinearDiscriminantAnalysis</span>) and Neighborhood Component Analysis (<span class="title-ref">NeighborhoodComponentsAnalysis</span>) on the Digits dataset, a dataset with size \(n_{samples} = 1797\) and \(n_{features} = 64\). The data set is split into a training and a test set of equal size, then standardized. For evaluation the 3-nearest neighbor classification accuracy is computed on the 2-dimensional projected points found by each method. Each data sample belongs to one of 10 classes.

<div class="centered">

[![nca\_dim\_reduction\_1](../auto_examples/neighbors/images/sphx_glr_plot_nca_dim_reduction_001.png)](../auto_examples/neighbors/plot_nca_dim_reduction.html) [![nca\_dim\_reduction\_2](../auto_examples/neighbors/images/sphx_glr_plot_nca_dim_reduction_002.png)](../auto_examples/neighbors/plot_nca_dim_reduction.html) [![nca\_dim\_reduction\_3](../auto_examples/neighbors/images/sphx_glr_plot_nca_dim_reduction_003.png)](../auto_examples/neighbors/plot_nca_dim_reduction.html)

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_nca\_classification.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_nca\_classification.py)
  - \[sphx\_glr\_auto\_examples\_neighbors\_plot\_nca\_dim\_reduction.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_nca\_dim\_reduction.py)
  - \[sphx\_glr\_auto\_examples\_manifold\_plot\_lle\_digits.py\](\#sphx\_glr\_auto\_examples\_manifold\_plot\_lle\_digits.py)

### Mathematical formulation

The goal of NCA is to learn an optimal linear transformation matrix of size `(n_components, n_features)`, which maximises the sum over all samples \(i\) of the probability \(p_i\) that \(i\) is correctly classified, i.e.:

\[\underset{L}{\arg\max} \sum\limits_{i=0}^{N - 1} p_{i}\]

with \(N\) = `n_samples` and \(p_i\) the probability of sample \(i\) being correctly classified according to a stochastic nearest neighbors rule in the learned embedded space:

\[p_{i}=\sum\limits_{j \in C_i}{p_{i j}}\]

where \(C_i\) is the set of points in the same class as sample \(i\), and \(p_{i j}\) is the softmax over Euclidean distances in the embedded space:

\[p_{i j} = \frac{\exp(-||L x_i - L x_j||^2)}{\sum\limits_{k \ne
          i} {\exp{-(||L x_i - L x_k||^2)}}} , \quad p_{i i} = 0\]

<div class="dropdown">

Mahalanobis distance

NCA can be seen as learning a (squared) Mahalanobis distance metric:

\[|| L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),\]

where \(M = L^T L\) is a symmetric positive semi-definite matrix of size `(n_features, n_features)`.

</div>

### Implementation

This implementation follows what is explained in the original paper\[1\]. For the optimisation method, it currently uses scipy's L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning.

See the examples below and the docstring of <span class="title-ref">NeighborhoodComponentsAnalysis.fit</span> for further information.

### Complexity

#### Training

NCA stores a matrix of pairwise distances, taking `n_samples ** 2` memory. Time complexity depends on the number of iterations done by the optimisation algorithm. However, one can set the maximum number of iterations with the argument `max_iter`. For each iteration, time complexity is `O(n_components x n_samples x min(n_samples, n_features))`.

#### Transform

Here the `transform` operation returns \(LX^T\), therefore its time complexity equals `n_components * n_features * n_samples_test`. There is no added space complexity in the operation.

**References**

  - [Wikipedia entry on Neighborhood Components Analysis](https://en.wikipedia.org/wiki/Neighbourhood_components_analysis)

<!-- end list -->

1.  ["Neighbourhood Components Analysis"](http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf), J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.

---

neural_networks_supervised.md

---

# Neural network models (supervised)

<div class="currentmodule">

sklearn.neural\_network

</div>

\> **Warning** \> This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see \[related\_projects\](\#related\_projects).

## Multi-layer Perceptron

**Multi-layer Perceptron (MLP)** is a supervised learning algorithm that learns a function \(f: R^m \rightarrow R^o\) by training on a dataset, where \(m\) is the number of dimensions for input and \(o\) is the number of dimensions for output. Given a set of features \(X = {x_1, x_2, ..., x_m}\) and a target \(y\), it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output.

![**Figure 1 : One hidden layer MLP.**](../images/multilayerperceptron_network.png)

The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.

The module contains the public attributes `coefs_` and `intercepts_`. `coefs_` is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). `intercepts_` is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).

<div class="dropdown">

Advantages and disadvantages of Multi-layer Perceptron

The advantages of Multi-layer Perceptron are:

  - Capability to learn non-linear models.
  - Capability to learn models in real-time (on-line learning) using `partial_fit`.

The disadvantages of Multi-layer Perceptron (MLP) include:

  - MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.
  - MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.
  - MLP is sensitive to feature scaling.

Please see \[Tips on Practical Use \<mlp\_tips\>\](\#tips-on-practical-use-\<mlp\_tips\>) section that addresses some of these disadvantages.

</div>

## Classification

Class <span class="title-ref">MLPClassifier</span> implements a multi-layer perceptron (MLP) algorithm that trains using [Backpropagation](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm).

MLP trains on two arrays: array X of size (n\_samples, n\_features), which holds the training samples represented as floating point feature vectors; and array y of size (n\_samples,), which holds the target values (class labels) for the training samples:

    >>> from sklearn.neural_network import MLPClassifier
    >>> X = [[0., 0.], [1., 1.]]
    >>> y = [0, 1]
    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
    ...                     hidden_layer_sizes=(5, 2), random_state=1)
    ...
    >>> clf.fit(X, y)
    MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,
                  solver='lbfgs')

After fitting (training), the model can predict labels for new samples:

    >>> clf.predict([[2., 2.], [-1., -2.]])
    array([1, 0])

MLP can fit a non-linear model to the training data. `clf.coefs_` contains the weight matrices that constitute the model parameters:

    >>> [coef.shape for coef in clf.coefs_]
    [(2, 5), (5, 2), (2, 1)]

Currently, <span class="title-ref">MLPClassifier</span> supports only the Cross-Entropy loss function, which allows probability estimates by running the `predict_proba` method.

MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):

    >>> clf.predict_proba([[2., 2.], [1., 2.]])
    array([[1.967...e-04, 9.998...-01],
           [1.967...e-04, 9.998...-01]])

<span class="title-ref">MLPClassifier</span> supports multi-class classification by applying [Softmax](https://en.wikipedia.org/wiki/Softmax_activation_function) as the output function.

Further, the model supports \[multi-label classification \<multiclass\>\](\#multi-label-classification-\<multiclass\>) in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to <span class="title-ref">0.5</span> are rounded to <span class="title-ref">1</span>, otherwise to <span class="title-ref">0</span>. For a predicted output of a sample, the indices where the value is <span class="title-ref">1</span> represents the assigned classes of that sample:

    >>> X = [[0., 0.], [1., 1.]]
    >>> y = [[0, 1], [1, 1]]
    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
    ...                     hidden_layer_sizes=(15,), random_state=1)
    ...
    >>> clf.fit(X, y)
    MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,
                  solver='lbfgs')
    >>> clf.predict([[1., 2.]])
    array([[1, 1]])
    >>> clf.predict([[0., 0.]])
    array([[0, 1]])

See the examples below and the docstring of <span class="title-ref">MLPClassifier.fit</span> for further information.

**Examples**

  - \[sphx\_glr\_auto\_examples\_neural\_networks\_plot\_mlp\_training\_curves.py\](\#sphx\_glr\_auto\_examples\_neural\_networks\_plot\_mlp\_training\_curves.py)
  - See \[sphx\_glr\_auto\_examples\_neural\_networks\_plot\_mnist\_filters.py\](\#sphx\_glr\_auto\_examples\_neural\_networks\_plot\_mnist\_filters.py) for visualized representation of trained weights.

## Regression

Class <span class="title-ref">MLPRegressor</span> implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values.

<span class="title-ref">MLPRegressor</span> also supports multi-output regression, in which a sample can have more than one target.

## Regularization

Both <span class="title-ref">MLPRegressor</span> and <span class="title-ref">MLPClassifier</span> use parameter `alpha` for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha.

![](../auto_examples/neural_networks/images/sphx_glr_plot_mlp_alpha_001.png)

See the examples below for further information.

**Examples**

  - \[sphx\_glr\_auto\_examples\_neural\_networks\_plot\_mlp\_alpha.py\](\#sphx\_glr\_auto\_examples\_neural\_networks\_plot\_mlp\_alpha.py)

## Algorithms

MLP trains using [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), `Adam <1412.6980>`, or [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS). Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.

\[w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial Loss}{\partial w})\]

where \(\eta\) is the learning rate which controls the step-size in the parameter space search. \(Loss\) is the loss function used for the network.

More details can be found in the documentation of [SGD](https://scikit-learn.org/stable/modules/sgd.html)

Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments.

With SGD or Adam, training supports online and mini-batch learning.

L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of [L-BFGS](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html).

If the selected solver is 'L-BFGS', training does not support online nor mini-batch learning.

## Complexity

Suppose there are \(n\) training samples, \(m\) features, \(k\) hidden layers, each containing \(h\) neurons - for simplicity, and \(o\) output neurons. The time complexity of backpropagation is \(O(i \cdot n \cdot (m \cdot h + (k - 1) \cdot h \cdot h + h \cdot o))\), where \(i\) is the number of iterations. Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.

<div class="dropdown">

Mathematical formulation

Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,

\[g(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}\]

For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.

If there are more than two classes, \(f(x)\) itself would be a vector of size (n\_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,

\[\text{softmax}(z)_i = \frac{\exp(z_i)}{\sum_{l=1}^k\exp(z_l)}\]

where \(z_i\) represents the \(i\) th element of the input to softmax, which corresponds to class \(i\), and \(K\) is the number of classes. The result is a vector containing the probabilities that sample \(x\) belong to each class. The output is the class with the highest probability.

In regression, the output remains as \(f(x)\); therefore, output activation function is just the identity function.

MLP uses different loss functions depending on the problem type. The loss function for classification is Average Cross-Entropy, which in binary case is given as,

\[Loss(\hat{y},y,W) = -\dfrac{1}{n}\sum_{i=0}^n(y_i \ln {\hat{y_i}} + (1-y_i) \ln{(1-\hat{y_i})}) + \dfrac{\alpha}{2n} ||W||_2^2\]

where \(\alpha ||W||_2^2\) is an L2-regularization term (aka penalty) that penalizes complex models; and \(\alpha > 0\) is a non-negative hyperparameter that controls the magnitude of the penalty.

For regression, MLP uses the Mean Square Error loss function; written as,

\[Loss(\hat{y},y,W) = \frac{1}{2n}\sum_{i=0}^n||\hat{y}_i - y_i ||_2^2 + \frac{\alpha}{2n} ||W||_2^2\]

Starting from initial random weights, multi-layer perceptron (MLP) minimizes the loss function by repeatedly updating these weights. After computing the loss, a backward pass propagates it from the output layer to the previous layers, providing each weight parameter with an update value meant to decrease the loss.

In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect to the weights is computed and deducted from \(W\). More formally, this is expressed as,

\[W^{i+1} = W^i - \epsilon \nabla {Loss}_{W}^{i}\]

where \(i\) is the iteration step, and \(\epsilon\) is the learning rate with a value larger than 0.

The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.

</div>

## Tips on Practical Use

  - Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to \[0, 1\] or \[-1, +1\], or standardize it to have mean 0 and variance 1. Note that you must apply the *same* scaling to the test set for meaningful results. You can use <span class="title-ref">\~sklearn.preprocessing.StandardScaler</span> for standardization.
    
    > \>\>\> from sklearn.preprocessing import StandardScaler \# doctest: +SKIP \>\>\> scaler = StandardScaler() \# doctest: +SKIP \>\>\> \# Don't cheat - fit only on training data \>\>\> scaler.fit(X\_train) \# doctest: +SKIP \>\>\> X\_train = scaler.transform(X\_train) \# doctest: +SKIP \>\>\> \# apply same transformation to test data \>\>\> X\_test = scaler.transform(X\_test) \# doctest: +SKIP
    
    An alternative and recommended approach is to use <span class="title-ref">\~sklearn.preprocessing.StandardScaler</span> in a <span class="title-ref">\~sklearn.pipeline.Pipeline</span>

  - Finding a reasonable regularization parameter \(\alpha\) is best done using <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span>, usually in the range `10.0 ** -np.arange(1, 7)`.

  - Empirically, we observed that <span class="title-ref">L-BFGS</span> converges faster and with better solutions on small datasets. For relatively large datasets, however, <span class="title-ref">Adam</span> is very robust. It usually converges quickly and gives pretty good performance. <span class="title-ref">SGD</span> with momentum or nesterov's momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.

## More control with warm\_start

If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using `warm_start=True` and `max_iter=1` and iterating yourself can be helpful:

    >>> X = [[0., 0.], [1., 1.]]
    >>> y = [0, 1]
    >>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)
    >>> for i in range(10):
    ...     clf.fit(X, y)
    ...     # additional monitoring / inspection
    MLPClassifier(...

<div class="dropdown">

References

  - ["Learning representations by back-propagating errors."](https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf) Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.
  - ["Stochastic Gradient Descent"](https://leon.bottou.org/projects/sgd) L. Bottou - Website, 2010.
  - ["Backpropagation"](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm) Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.
  - ["Efficient BackProp"](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
    25. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.
  - `"Adam: A method for stochastic optimization." <1412.6980>` Kingma, Diederik, and Jimmy Ba (2014)

</div>

---

neural_networks_unsupervised.md

---

# Neural network models (unsupervised)

<div class="currentmodule">

sklearn.neural\_network

</div>

## Restricted Boltzmann machines

Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron.

The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides <span class="title-ref">BernoulliRBM</span>, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.

The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (\[Stochastic Maximum Likelihood \<sml\>\](\#stochastic maximum-likelihood-\<sml\>)) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.

The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.

![](../auto_examples/neural_networks/images/sphx_glr_plot_rbm_logistic_classification_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_neural\_networks\_plot\_rbm\_logistic\_classification.py\](\#sphx\_glr\_auto\_examples\_neural\_networks\_plot\_rbm\_logistic\_classification.py)

### Graphical model and parametrization

The graphical model of an RBM is a fully-connected bipartite graph.

![image](../images/rbm_graph.png)

The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.

The energy function measures the quality of a joint assignment:

\[E(\mathbf{v}, \mathbf{h}) = -\sum_i \sum_j w_{ij}v_ih_j - \sum_i b_iv_i
  - \sum_j c_jh_j\]

In the formula above, \(\mathbf{b}\) and \(\mathbf{c}\) are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:

\[P(\mathbf{v}, \mathbf{h}) = \frac{e^{-E(\mathbf{v}, \mathbf{h})}}{Z}\]

The word *restricted* refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:

\[\begin{aligned}
h_i \bot h_j | \mathbf{v} \\
v_i \bot v_j | \mathbf{h}
\end{aligned}\]

The bipartite structure allows for the use of efficient block Gibbs sampling for inference.

### Bernoulli Restricted Boltzmann machines

In the <span class="title-ref">BernoulliRBM</span>, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren't. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values.

The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:

\[\begin{aligned}
P(v_i=1|\mathbf{h}) = \sigma(\sum_j w_{ij}h_j + b_i) \\
P(h_i=1|\mathbf{v}) = \sigma(\sum_i w_{ij}v_i + c_j)
\end{aligned}\]

where \(\sigma\) is the logistic sigmoid function:

\[\sigma(x) = \frac{1}{1 + e^{-x}}\]

### Stochastic Maximum Likelihood learning

The training algorithm implemented in <span class="title-ref">BernoulliRBM</span> is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:

\[\log P(v) = \log \sum_h e^{-E(v, h)} - \log \sum_{x, y} e^{-E(x, y)}\]

For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.

In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \(v\) and \(h\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.

The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.

Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated \(k\) Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly.

**References**

  - ["A fast learning algorithm for deep belief nets"](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf),
    7.  Hinton, S. Osindero, Y.-W. Teh, 2006
  - ["Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient"](https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf),
    20. Tieleman, 2008

---

outlier_detection.md

---

# Novelty and Outlier Detection

<div class="currentmodule">

sklearn

</div>

Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an *inlier*), or should be considered as different (it is an *outlier*). Often, this ability is used to clean real data sets. Two important distinctions must be made:

  - outlier detection  
    The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.

  - novelty detection  
    The training data is not polluted by outliers and we are interested in detecting whether a **new** observation is an outlier. In this context an outlier is also called a novelty.

Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection. In the context of outlier detection, the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions. On the contrary, in the context of novelty detection, novelties/anomalies can form a dense cluster as long as they are in a low density region of the training data, considered as normal in this context.

The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:

    estimator.fit(X_train)

new observations can then be sorted as inliers or outliers with a `predict` method:

    estimator.predict(X_test)

Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the `score_samples` method, while the threshold can be controlled by the `contamination` parameter.

The `decision_function` method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers:

    estimator.decision_function(X_test)

Note that <span class="title-ref">neighbors.LocalOutlierFactor</span> does not support `predict`, `decision_function` and `score_samples` methods by default but only a `fit_predict` method, as this estimator was originally meant to be applied for outlier detection. The scores of abnormality of the training samples are accessible through the `negative_outlier_factor_` attribute.

If you really want to use <span class="title-ref">neighbors.LocalOutlierFactor</span> for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the `novelty` parameter set to `True` before fitting the estimator. In this case, `fit_predict` is not available.

<div class="warning">

<div class="title">

Warning

</div>

**Novelty detection with Local Outlier Factor**

When `novelty` is set to `True` be aware that you must only use `predict`, `decision_function` and `score_samples` on new unseen data and not on the training samples as this would lead to wrong results. I.e., the result of `predict` will not be the same as `fit_predict`. The scores of abnormality of the training samples are always accessible through the `negative_outlier_factor_` attribute.

</div>

The behavior of <span class="title-ref">neighbors.LocalOutlierFactor</span> is summarized in the following table.

| Method                     | Outlier detection              | Novelty detection    |
| -------------------------- | ------------------------------ | -------------------- |
| `fit_predict`              | OK                             | Not available        |
| `predict`                  | Not available                  | Use only on new data |
| `decision_function`        | Not available                  | Use only on new data |
| `score_samples`            | Use `negative_outlier_factor_` | Use only on new data |
| `negative_outlier_factor_` | OK                             | OK                   |

## Overview of outlier detection methods

A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection.

![](../auto_examples/miscellaneous/images/sphx_glr_plot_anomaly_comparison_001.png)

<span class="title-ref">ensemble.IsolationForest</span> and <span class="title-ref">neighbors.LocalOutlierFactor</span> perform reasonably well on the data sets considered here. The <span class="title-ref">svm.OneClassSVM</span> is known to be sensitive to outliers and thus does not perform very well for outlier detection. That being said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging. <span class="title-ref">svm.OneClassSVM</span> may still be used with outlier detection but requires fine-tuning of its hyperparameter <span class="title-ref">nu</span> to handle outliers and prevent overfitting. <span class="title-ref">linear\_model.SGDOneClassSVM</span> provides an implementation of a linear One-Class SVM with a linear complexity in the number of samples. This implementation is here used with a kernel approximation technique to obtain results similar to <span class="title-ref">svm.OneClassSVM</span> which uses a Gaussian kernel by default. Finally, <span class="title-ref">covariance.EllipticEnvelope</span> assumes the data is Gaussian and learns an ellipse. For more details on the different estimators refer to the example \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py) and the sections hereunder.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py) for a comparison of the <span class="title-ref">svm.OneClassSVM</span>, the <span class="title-ref">ensemble.IsolationForest</span>, the <span class="title-ref">neighbors.LocalOutlierFactor</span> and <span class="title-ref">covariance.EllipticEnvelope</span>.
  - See \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_outlier\_detection\_bench.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_outlier\_detection\_bench.py) for an example showing how to evaluate outlier detection estimators, the <span class="title-ref">neighbors.LocalOutlierFactor</span> and the <span class="title-ref">ensemble.IsolationForest</span>, using ROC curves from <span class="title-ref">metrics.RocCurveDisplay</span>.

## Novelty Detection

Consider a data set of \(n\) observations from the same distribution described by \(p\) features. Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods.

In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \(p\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.

The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the \[svm\](\#svm) module in the <span class="title-ref">svm.OneClassSVM</span> object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The <span class="title-ref">nu</span> parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.

**References**

  - [Estimating the support of a high-dimensional distribution](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-99-87.pdf) Schölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_svm\_plot\_oneclass.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_oneclass.py) for visualizing the frontier learned around some data by a <span class="title-ref">svm.OneClassSVM</span> object.
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py)

![](../auto_examples/svm/images/sphx_glr_plot_oneclass_001.png)

### Scaling up the One-Class SVM

An online linear version of the One-Class SVM is implemented in <span class="title-ref">linear\_model.SGDOneClassSVM</span>. This implementation scales linearly with the number of samples and can be used with a kernel approximation to approximate the solution of a kernelized <span class="title-ref">svm.OneClassSVM</span> whose complexity is at best quadratic in the number of samples. See section \[sgd\_online\_one\_class\_svm\](\#sgd\_online\_one\_class\_svm) for more details.

**Examples**

  - See \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgdocsvm\_vs\_ocsvm.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgdocsvm\_vs\_ocsvm.py) for an illustration of the approximation of a kernelized One-Class SVM with the <span class="title-ref">linear\_model.SGDOneClassSVM</span> combined with kernel approximation.

## Outlier Detection

Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called *outliers*. Yet, in the case of outlier detection, we don't have a clean data set representing the population of regular observations that can be used to train any tool.

### Fitting an elliptic envelope

One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the "shape" of the data, and can define outlying observations as observations which stand far enough from the fit shape.

The scikit-learn provides an object <span class="title-ref">covariance.EllipticEnvelope</span> that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.

For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.

![](../auto_examples/covariance/images/sphx_glr_plot_mahalanobis_distances_001.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_covariance\_plot\_mahalanobis\_distances.py\](\#sphx\_glr\_auto\_examples\_covariance\_plot\_mahalanobis\_distances.py) for an illustration of the difference between using a standard (<span class="title-ref">covariance.EmpiricalCovariance</span>) or a robust estimate (<span class="title-ref">covariance.MinCovDet</span>) of location and covariance to assess the degree of outlyingness of an observation.
  - See \[sphx\_glr\_auto\_examples\_applications\_plot\_outlier\_detection\_wine.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_outlier\_detection\_wine.py) for an example of robust covariance estimation on a real data set.

**References**

  - Rousseeuw, P.J., Van Driessen, K. "A fast algorithm for the minimum covariance determinant estimator" Technometrics 41(3), 212 (1999)

### Isolation Forest

One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The <span class="title-ref">ensemble.IsolationForest</span> 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.

Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.

This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.

Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.

The implementation of <span class="title-ref">ensemble.IsolationForest</span> is based on an ensemble of <span class="title-ref">tree.ExtraTreeRegressor</span>. Following Isolation Forest original paper, the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).

This algorithm is illustrated below.

![](../auto_examples/ensemble/images/sphx_glr_plot_isolation_forest_003.png)

<div id="iforest_warm_start">

The <span class="title-ref">ensemble.IsolationForest</span> supports `warm_start=True` which allows you to add more trees to an already fitted model:

    >>> from sklearn.ensemble import IsolationForest
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])
    >>> clf = IsolationForest(n_estimators=10, warm_start=True)
    >>> clf.fit(X)  # fit 10 trees  # doctest: +SKIP
    >>> clf.set_params(n_estimators=20)  # add 10 more trees  # doctest: +SKIP
    >>> clf.fit(X)  # fit the added trees  # doctest: +SKIP

</div>

**Examples**

  - See \[sphx\_glr\_auto\_examples\_ensemble\_plot\_isolation\_forest.py\](\#sphx\_glr\_auto\_examples\_ensemble\_plot\_isolation\_forest.py) for an illustration of the use of IsolationForest.
  - See \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py) for a comparison of <span class="title-ref">ensemble.IsolationForest</span> with <span class="title-ref">neighbors.LocalOutlierFactor</span>, <span class="title-ref">svm.OneClassSVM</span> (tuned to perform like an outlier detection method), <span class="title-ref">linear\_model.SGDOneClassSVM</span>, and a covariance-based outlier detection with <span class="title-ref">covariance.EllipticEnvelope</span>.

**References**

  - Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. "Isolation forest." Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.

### Local Outlier Factor

Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm.

The <span class="title-ref">neighbors.LocalOutlierFactor</span> (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.

In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of its k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.

The number k of neighbors considered, (alias parameter n\_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such information is generally not available, and taking n\_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n\_neighbors should be greater (n\_neighbors=35 in the example below).

The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.

When applying LOF for outlier detection, there are no `predict`, `decision_function` and `score_samples` methods but only a `fit_predict` method. The scores of abnormality of the training samples are accessible through the `negative_outlier_factor_` attribute. Note that `predict`, `decision_function` and `score_samples` can be used on new unseen data when LOF is applied for novelty detection, i.e. when the `novelty` parameter is set to `True`, but the result of `predict` may differ from that of `fit_predict`. See \[novelty\_with\_lof\](\#novelty\_with\_lof).

This strategy is illustrated below.

![](../auto_examples/neighbors/images/sphx_glr_plot_lof_outlier_detection_001.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_neighbors\_plot\_lof\_outlier\_detection.py\](\#sphx\_glr\_auto\_examples\_neighbors\_plot\_lof\_outlier\_detection.py) for an illustration of the use of <span class="title-ref">neighbors.LocalOutlierFactor</span>.
  - See \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_anomaly\_comparison.py) for a comparison with other anomaly detection methods.

**References**

  - Breunig, Kriegel, Ng, and Sander (2000) [LOF: identifying density-based local outliers.](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf) Proc. ACM SIGMOD

## Novelty detection with Local Outlier Factor

To use <span class="title-ref">neighbors.LocalOutlierFactor</span> for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the `novelty` parameter set to `True` before fitting the estimator:

    lof = LocalOutlierFactor(novelty=True)
    lof.fit(X_train)

Note that `fit_predict` is not available in this case to avoid inconsistencies.

<div class="warning">

<div class="title">

Warning

</div>

**Novelty detection with Local Outlier Factor\`**

When `novelty` is set to `True` be aware that you must only use `predict`, `decision_function` and `score_samples` on new unseen data and not on the training samples as this would lead to wrong results. I.e., the result of `predict` will not be the same as `fit_predict`. The scores of abnormality of the training samples are always accessible through the `negative_outlier_factor_` attribute.

</div>

Novelty detection with Local Outlier Factor is illustrated below.

![](../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png)

---

partial_dependence.md

---

# Partial Dependence and Individual Conditional Expectation plots

<div class="currentmodule">

sklearn.inspection

</div>

Partial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response\[1\] and a set of input features of interest.

Both PDPs [\[H2009\]](#H2009) and ICEs [\[G2015\]](#G2015) assume that the input features of interest are independent from the complement features, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE [\[M2019\]](#M2019).

## Partial dependence plots

Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the 'complement' features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.

Due to the limits of human perception, the size of the set of input features of interest must be small (usually, one or two) thus the input features of interest are usually chosen among the most important features.

The figure below shows two one-way and one two-way partial dependence plots for the bike sharing dataset, with a \`\~sklearn.ensemble.HistGradientBoostingRegressor\`:

![](../auto_examples/inspection/images/sphx_glr_plot_partial_dependence_006.png)

One-way PDPs tell us about the interaction between the target response and an input feature of interest (e.g. linear, non-linear). The left plot in the above figure shows the effect of the temperature on the number of bike rentals; we can clearly see that a higher temperature is related with a higher number of bike rentals. Similarly, we could analyze the effect of the humidity on the number of bike rentals (middle plot). Thus, these interpretations are marginal, considering a feature at a time.

PDPs with two input features of interest show the interactions among the two features. For example, the two-variable PDP in the above figure shows the dependence of the number of bike rentals on joint values of temperature and humidity. We can clearly see an interaction between the two features: with a temperature higher than 20 degrees Celsius, mainly the humidity has a strong impact on the number of bike rentals. For lower temperatures, both the temperature and the humidity have an impact on the number of bike rentals.

The `sklearn.inspection` module provides a convenience function <span class="title-ref">\~PartialDependenceDisplay.from\_estimator</span> to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features `0` and `1` and a two-way PDP between the two features:

    >>> from sklearn.datasets import make_hastie_10_2
    >>> from sklearn.ensemble import GradientBoostingClassifier
    >>> from sklearn.inspection import PartialDependenceDisplay
    
    >>> X, y = make_hastie_10_2(random_state=0)
    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    ...     max_depth=1, random_state=0).fit(X, y)
    >>> features = [0, 1, (0, 1)]
    >>> PartialDependenceDisplay.from_estimator(clf, X, features)
    <...>

You can access the newly created figure and Axes objects using `plt.gcf()` and `plt.gca()`.

To make a partial dependence plot with categorical features, you need to specify which features are categorical using the parameter <span class="title-ref">categorical\_features</span>. This parameter takes a list of indices, names of the categorical features or a boolean mask. The graphical representation of partial dependence for categorical features is a bar plot or a 2D heatmap.

<div class="dropdown">

PDPs for multi-class classification

For multi-class classification, you need to set the class label for which the PDPs should be created via the `target` argument:

    >>> from sklearn.datasets import load_iris
    >>> iris = load_iris()
    >>> mc_clf = GradientBoostingClassifier(n_estimators=10,
    ...     max_depth=1).fit(iris.data, iris.target)
    >>> features = [3, 2, (3, 2)]
    >>> PartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)
    <...>

The same parameter `target` is used to specify the target in multi-output regression settings.

</div>

If you need the raw values of the partial dependence function rather than the plots, you can use the <span class="title-ref">sklearn.inspection.partial\_dependence</span> function:

    >>> from sklearn.inspection import partial_dependence
    
    >>> results = partial_dependence(clf, X, [0])
    >>> results["average"]
    array([[ 2.466...,  2.466..., ...
    >>> results["grid_values"]
    [array([-1.624..., -1.592..., ...

The values at which the partial dependence should be evaluated are directly generated from `X`. For 2-way partial dependence, a 2D-grid of values is generated. The `values` field returned by <span class="title-ref">sklearn.inspection.partial\_dependence</span> gives the actual values used in the grid for each input feature of interest. They also correspond to the axis of the plots.

## Individual conditional expectation (ICE) plot

Similar to a PDP, an individual conditional expectation (ICE) plot shows the dependence between the target function and an input feature of interest. However, unlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample. Due to the limits of human perception, only one input feature of interest is supported for ICE plots.

The figures below show two ICE plots for the bike sharing dataset, with a \`\~sklearn.ensemble.HistGradientBoostingRegressor\`:. The figures plot the corresponding PD line overlaid on ICE lines.

![](../auto_examples/inspection/images/sphx_glr_plot_partial_dependence_004.png)

While the PDPs are good at showing the average effect of the target features, they can obscure a heterogeneous relationship created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we see that the ICE for the temperature feature gives us some additional information: Some of the ICE lines are flat while some others shows a decrease of the dependence for temperature above 35 degrees Celsius. We observe a similar pattern for the humidity feature: some of the ICE lines show a sharp decrease when the humidity is above 80%.

The `sklearn.inspection` module's <span class="title-ref">PartialDependenceDisplay.from\_estimator</span> convenience function can be used to create ICE plots by setting `kind='individual'`. In the example below, we show how to create a grid of ICE plots:

> \>\>\> from sklearn.datasets import make\_hastie\_10\_2 \>\>\> from sklearn.ensemble import GradientBoostingClassifier \>\>\> from sklearn.inspection import PartialDependenceDisplay
> 
> \>\>\> X, y = make\_hastie\_10\_2(random\_state=0) \>\>\> clf = GradientBoostingClassifier(n\_estimators=100, learning\_rate=1.0, ... max\_depth=1, random\_state=0).fit(X, y) \>\>\> features = \[0, 1\] \>\>\> PartialDependenceDisplay.from\_estimator(clf, X, features, ... kind='individual') \<...\>

In ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use ICE plots alongside PDPs. They can be plotted together with `kind='both'`.

> \>\>\> PartialDependenceDisplay.from\_estimator(clf, X, features, ... kind='both') \<...\>

If there are too many lines in an ICE plot, it can be difficult to see differences between individual samples and interpret the model. Centering the ICE at the first value on the x-axis, produces centered Individual Conditional Expectation (cICE) plots [\[G2015\]](#G2015). This puts emphasis on the divergence of individual conditional expectations from the mean line, thus making it easier to explore heterogeneous relationships. cICE plots can be plotted by setting \`centered=True\`:

> \>\>\> PartialDependenceDisplay.from\_estimator(clf, X, features, ... kind='both', centered=True) \<...\>

## Mathematical Definition

Let \(X_S\) be the set of input features of interest (i.e. the <span class="title-ref">features</span> parameter) and let \(X_C\) be its complement.

The partial dependence of the response \(f\) at a point \(x_S\) is defined as:

\[\begin{aligned}
pd_{X_S}(x_S) &\overset{def}{=} \mathbb{E}_{X_C}\left[ f(x_S, X_C) \right]\\
              &= \int f(x_S, x_C) p(x_C) dx_C,
\end{aligned}\]

where \(f(x_S, x_C)\) is the response function (`predict`, `predict_proba` or `decision_function`) for a given sample whose values are defined by \(x_S\) for the features in \(X_S\), and by \(x_C\) for the features in \(X_C\). Note that \(x_S\) and \(x_C\) may be tuples.

Computing this integral for various values of \(x_S\) produces a PDP plot as above. An ICE line is defined as a single \(f(x_{S}, x_{C}^{(i)})\) evaluated at \(x_{S}\).

## Computation methods

There are two main methods to approximate the integral above, namely the 'brute' and 'recursion' methods. The <span class="title-ref">method</span> parameter controls which method to use.

The 'brute' method is a generic method that works with any estimator. Note that computing ICE plots is only supported with the 'brute' method. It approximates the above integral by computing an average over the data \`X\`:

\[pd_{X_S}(x_S) \approx \frac{1}{n_\text{samples}} \sum_{i=1}^n f(x_S, x_C^{(i)}),\]

where \(x_C^{(i)}\) is the value of the i-th sample for the features in \(X_C\). For each value of \(x_S\), this method requires a full pass over the dataset <span class="title-ref">X</span> which is computationally intensive.

Each of the \(f(x_{S}, x_{C}^{(i)})\) corresponds to one ICE line evaluated at \(x_{S}\). Computing this for multiple values of \(x_{S}\), one obtains a full ICE line. As one can see, the average of the ICE lines correspond to the partial dependence line.

The 'recursion' method is faster than the 'brute' method, but it is only supported for PDP plots by some tree-based estimators. It is computed as follows. For a given point \(x_S\), a weighted tree traversal is performed: if a split node involves an input feature of interest, the corresponding left or right branch is followed; otherwise both branches are followed, each branch being weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all the visited leaves values.

With the 'brute' method, the parameter <span class="title-ref">X</span> is used both for generating the grid of values \(x_S\) and the complement feature values \(x_C\). However with the 'recursion' method, <span class="title-ref">X</span> is only used for the grid values: implicitly, the \(x_C\) values are those of the training data.

By default, the 'recursion' method is used for plotting PDPs on tree-based estimators that support it, and 'brute' is used for the rest.

<div id="pdp_method_differences">

\> **Note** \> While both methods should be close in general, they might differ in some specific settings. The 'brute' method assumes the existence of the data points \((x_S, x_C^{(i)})\). When the features are correlated, such artificial samples may have a very low probability mass. The 'brute' and 'recursion' methods will likely disagree regarding the value of the partial dependence, because they will treat these unlikely samples differently. Remember, however, that the primary assumption for interpreting PDPs is that the features should be independent.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py)

**Footnotes**

**References**

<div id="citations">

  - <span id="G2015" class="citation-label">G2015</span>  
    `A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,
    "Peeking Inside the Black Box: Visualizing Statistical
    Learning With Plots of Individual Conditional Expectation"
    Journal of Computational and Graphical Statistics,
    24(1): 44-65, Springer, 2015. <1309.6392>`

  - <span id="H2009" class="citation-label">H2009</span>  
    T. Hastie, R. Tibshirani and J. Friedman, [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn//), Second Edition, Section 10.13.2, Springer, 2009.

  - <span id="M2019" class="citation-label">M2019</span>  
    C. Molnar, [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/), Section 5.1, 2019.

</div>

1.  For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function.

---

permutation_importance.md

---

# Permutation feature importance

<div class="currentmodule">

sklearn.inspection

</div>

Permutation feature importance is a model inspection technique that measures the contribution of each feature to a `fitted` model's statistical performance on a given tabular dataset. This technique is particularly useful for non-linear or opaque `estimators`, and involves randomly shuffling the values of a single feature and observing the resulting degradation of the model's score \[1\]. By breaking the relationship between the feature and the target, we determine how much the model relies on such particular feature.

In the following figures, we observe the effect of permuting features on the correlation between the feature and the target and consequently on the model statistical performance.

![image](../images/permuted_predictive_feature.png)

![image](../images/permuted_non_predictive_feature.png)

On the top figure, we observe that permuting a predictive feature breaks the correlation between the feature and the target, and consequently the model statistical performance decreases. On the bottom figure, we observe that permuting a non-predictive feature does not significantly degrade the model statistical performance.

One key advantage of permutation feature importance is that it is model-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can be calculated multiple times with different permutations of the feature, further providing a measure of the variance in the estimated feature importances for the specific trained model.

The figure below shows the permutation feature importance of a <span class="title-ref">\~sklearn.ensemble.RandomForestClassifier</span> trained on an augmented version of the titanic dataset that contains a <span class="title-ref">random\_cat</span> and a <span class="title-ref">random\_num</span> features, i.e. a categrical and a numerical feature that are not correlated in any way with the target variable:

![](../auto_examples/inspection/images/sphx_glr_plot_permutation_importance_002.png)

\> **Warning** \> Features that are deemed of **low importance for a bad model** (low cross-validation score) could be **very important for a good model**. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but **how important this feature is for a particular model**.

The <span class="title-ref">permutation\_importance</span> function calculates the feature importance of `estimators` for a given dataset. The `n_repeats` parameter sets the number of times a feature is randomly shuffled and returns a sample of feature importances.

Let's consider the following trained regression model:

    >>> from sklearn.datasets import load_diabetes
    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn.linear_model import Ridge
    >>> diabetes = load_diabetes()
    >>> X_train, X_val, y_train, y_val = train_test_split(
    ...     diabetes.data, diabetes.target, random_state=0)
    ...
    >>> model = Ridge(alpha=1e-2).fit(X_train, y_train)
    >>> model.score(X_val, y_val)
    0.356...

Its validation performance, measured via the \(R^2\) score, is significantly larger than the chance level. This makes it possible to use the <span class="title-ref">permutation\_importance</span> function to probe which features are most predictive:

    >>> from sklearn.inspection import permutation_importance
    >>> r = permutation_importance(model, X_val, y_val,
    ...                            n_repeats=30,
    ...                            random_state=0)
    ...
    >>> for i in r.importances_mean.argsort()[::-1]:
    ...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
    ...         print(f"{diabetes.feature_names[i]:<8}"
    ...               f"{r.importances_mean[i]:.3f}"
    ...               f" +/- {r.importances_std[i]:.3f}")
    ...
    s5      0.204 +/- 0.050
    bmi     0.176 +/- 0.048
    bp      0.088 +/- 0.033
    sex     0.056 +/- 0.023

Note that the importance values for the top features represent a large fraction of the reference score of 0.356.

Permutation importances can be computed either on the training set or on a held-out testing or validation set. Using a held-out set makes it possible to highlight which features contribute the most to the generalization power of the inspected model. Features that are important on the training set but not on the held-out set might cause the model to overfit.

The permutation feature importance depends on the score function that is specified with the <span class="title-ref">scoring</span> argument. This argument accepts multiple scorers, which is more computationally efficient than sequentially calling <span class="title-ref">permutation\_importance</span> several times with a different scorer, as it reuses model predictions.

<div class="dropdown">

Example of permutation feature importance using multiple scorers

In the example below we use a list of metrics, but more input formats are possible, as documented in \[multimetric\_scoring\](\#multimetric\_scoring).

> \>\>\> scoring = \['r2', 'neg\_mean\_absolute\_percentage\_error', 'neg\_mean\_squared\_error'\] \>\>\> r\_multi = permutation\_importance( ... model, X\_val, y\_val, n\_repeats=30, random\_state=0, scoring=scoring) ... \>\>\> for metric in r\_multi: ... print(f"{metric}") ... r = r\_multi\[metric\] ... for i in r.importances\_mean.argsort()\[::-1\]: ... if r.importances\_mean\[i\] - 2 \* r.importances\_std\[i\] \> 0: ... print(f" {diabetes.feature\_names\[i\]:\<8}" ... f"{r.importances\_mean\[i\]:.3f}" ... f" +/- {r.importances\_std\[i\]:.3f}") ... r2 s5 0.204 +/- 0.050 bmi 0.176 +/- 0.048 bp 0.088 +/- 0.033 sex 0.056 +/- 0.023 neg\_mean\_absolute\_percentage\_error s5 0.081 +/- 0.020 bmi 0.064 +/- 0.015 bp 0.029 +/- 0.010 neg\_mean\_squared\_error s5 1013.866 +/- 246.445 bmi 872.726 +/- 240.298 bp 438.663 +/- 163.022 sex 277.376 +/- 115.123

The ranking of the features is approximately the same for different metrics even if the scales of the importance values are very different. However, this is not guaranteed and different metrics might lead to significantly different feature importances, in particular for models trained for imbalanced classification problems, for which **the choice of the classification metric can be critical**.

</div>

## Outline of the permutation importance algorithm

  - Inputs: fitted predictive model \(m\), tabular dataset (training or validation) \(D\).
  - Compute the reference score \(s\) of the model \(m\) on data \(D\) (for instance the accuracy for a classifier or the \(R^2\) for a regressor).
  - For each feature \(j\) (column of \(D\)):
      - For each repetition \(k\) in \({1, ..., K}\):
        
          - Randomly shuffle column \(j\) of dataset \(D\) to generate a corrupted version of the data named \(\tilde{D}_{k,j}\).
          - Compute the score \(s_{k,j}\) of model \(m\) on corrupted data \(\tilde{D}_{k,j}\).
    
      - Compute importance \(i_j\) for feature \(f_j\) defined as:
        
        \[i_j = s - \frac{1}{K} \sum_{k=1}^{K} s_{k,j}\]

## Relation to impurity-based importance in trees

Tree-based models provide an alternative measure of \[feature importances based on the mean decrease in impurity \<random\_forest\_feature\_importance\>\](\#feature-importances based-on-the-mean-decrease-in-impurity-\<random\_forest\_feature\_importance\>) (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Log Loss or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data.

Furthermore, impurity-based feature importance for trees are **strongly biased** and **favor high cardinality features** (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.

Permutation-based feature importances do not exhibit such a bias. Additionally, the permutation feature importance may be computed with any performance metric on the model predictions and can be used to analyze any model class (not just tree-based models).

The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: \[sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py).

## Misleading values on strongly correlated features

When two features are correlated and one of the features is permuted, the model still has access to the latter through its correlated feature. This results in a lower reported importance value for both features, though they might *actually* be important.

The figure below shows the permutation feature importance of a <span class="title-ref">\~sklearn.ensemble.RandomForestClassifier</span> trained using the \[breast\_cancer\_dataset\](\#breast\_cancer\_dataset), which contains strongly correlated features. A naive interpretation would suggest that all features are unimportant:

![](../auto_examples/inspection/images/sphx_glr_plot_permutation_importance_multicollinear_002.png)

One way to handle the issue is to cluster features that are correlated and only keep one feature from each cluster.

![](../auto_examples/inspection/images/sphx_glr_plot_permutation_importance_multicollinear_004.png)

For more details on such strategy, see the example \[sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance\_multicollinear.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance\_multicollinear.py).

**Examples**

  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance.py)
  - \[sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance\_multicollinear.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_permutation\_importance\_multicollinear.py)

**References**

1.  L. Breiman, `"Random Forests" <10.1023/A:1010933404324>`, Machine Learning, 45(1), 5-32, 2001.

---

pipeline.md

---

- orphan

<meta http-equiv="refresh" content="1; url=./compose.html" />
<script>
  window.location.href = "./compose.html";
</script>

This content is now at \[combining\_estimators\](\#combining\_estimators).

---

preprocessing.md

---

# Preprocessing data

<div class="currentmodule">

sklearn.preprocessing

</div>

The `sklearn.preprocessing` package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.

In general, many learning algorithms such as linear models benefit from standardization of the data set (see \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_scaling\_importance.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_scaling\_importance.py)). If some outliers are present in the set, robust scalers or other transformers can be more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_all\_scaling.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_all\_scaling.py).

## Standardization, or mean removal and variance scaling

**Standardization** of datasets is a **common requirement for many machine learning estimators** implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with **zero mean and unit variance**.

In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.

For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.

The `~sklearn.preprocessing` module provides the <span class="title-ref">StandardScaler</span> utility class, which is a quick and easy way to perform the following operation on an array-like dataset:

    >>> from sklearn import preprocessing
    >>> import numpy as np
    >>> X_train = np.array([[ 1., -1.,  2.],
    ...                     [ 2.,  0.,  0.],
    ...                     [ 0.,  1., -1.]])
    >>> scaler = preprocessing.StandardScaler().fit(X_train)
    >>> scaler
    StandardScaler()
    
    >>> scaler.mean_
    array([1. ..., 0. ..., 0.33...])
    
    >>> scaler.scale_
    array([0.81..., 0.81..., 1.24...])
    
    >>> X_scaled = scaler.transform(X_train)
    >>> X_scaled
    array([[ 0.  ..., -1.22...,  1.33...],
           [ 1.22...,  0.  ..., -0.26...],
           [-1.22...,  1.22..., -1.06...]])

Scaled data has zero mean and unit variance:

    >>> X_scaled.mean(axis=0)
    array([0., 0., 0.])
    
    >>> X_scaled.std(axis=0)
    array([1., 1., 1.])

This class implements the `Transformer` API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. This class is hence suitable for use in the early steps of a \`\~sklearn.pipeline.Pipeline\`:

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn.pipeline import make_pipeline
    >>> from sklearn.preprocessing import StandardScaler
    
    >>> X, y = make_classification(random_state=42)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
    >>> pipe = make_pipeline(StandardScaler(), LogisticRegression())
    >>> pipe.fit(X_train, y_train)  # apply scaling on training data
    Pipeline(steps=[('standardscaler', StandardScaler()),
                    ('logisticregression', LogisticRegression())])
    
    >>> pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.
    0.96

It is possible to disable either centering or scaling by either passing `with_mean=False` or `with_std=False` to the constructor of <span class="title-ref">StandardScaler</span>.

### Scaling features to a range

An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using <span class="title-ref">MinMaxScaler</span> or <span class="title-ref">MaxAbsScaler</span>, respectively.

The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.

Here is an example to scale a toy data matrix to the `[0, 1]` range:

    >>> X_train = np.array([[ 1., -1.,  2.],
    ...                     [ 2.,  0.,  0.],
    ...                     [ 0.,  1., -1.]])
    ...
    >>> min_max_scaler = preprocessing.MinMaxScaler()
    >>> X_train_minmax = min_max_scaler.fit_transform(X_train)
    >>> X_train_minmax
    array([[0.5       , 0.        , 1.        ],
           [1.        , 0.5       , 0.33333333],
           [0.        , 1.        , 0.        ]])

The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:

    >>> X_test = np.array([[-3., -1.,  4.]])
    >>> X_test_minmax = min_max_scaler.transform(X_test)
    >>> X_test_minmax
    array([[-1.5       ,  0.        ,  1.66666667]])

It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:

    >>> min_max_scaler.scale_
    array([0.5       , 0.5       , 0.33...])
    
    >>> min_max_scaler.min_
    array([0.        , 0.5       , 0.33...])

If <span class="title-ref">MinMaxScaler</span> is given an explicit `feature_range=(min, max)` the full formula is:

    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
    
    X_scaled = X_std * (max - min) + min

<span class="title-ref">MaxAbsScaler</span> works in a very similar fashion, but scales in a way that the training data lies within the range `[-1, 1]` by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data.

Here is how to use the toy data from the previous example with this scaler:

    >>> X_train = np.array([[ 1., -1.,  2.],
    ...                     [ 2.,  0.,  0.],
    ...                     [ 0.,  1., -1.]])
    ...
    >>> max_abs_scaler = preprocessing.MaxAbsScaler()
    >>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
    >>> X_train_maxabs
    array([[ 0.5, -1. ,  1. ],
           [ 1. ,  0. ,  0. ],
           [ 0. ,  1. , -0.5]])
    >>> X_test = np.array([[ -3., -1.,  4.]])
    >>> X_test_maxabs = max_abs_scaler.transform(X_test)
    >>> X_test_maxabs
    array([[-1.5, -1. ,  2. ]])
    >>> max_abs_scaler.scale_
    array([2.,  1.,  2.])

### Scaling sparse data

Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.

<span class="title-ref">MaxAbsScaler</span> was specifically designed for scaling sparse data, and is the recommended way to go about this. However, <span class="title-ref">StandardScaler</span> can accept `scipy.sparse` matrices as input, as long as `with_mean=False` is explicitly passed to the constructor. Otherwise a `ValueError` will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. <span class="title-ref">RobustScaler</span> cannot be fitted to sparse inputs, but you can use the `transform` method on sparse inputs.

Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see `scipy.sparse.csr_matrix` and `scipy.sparse.csc_matrix`). Any other sparse input will be **converted to the Compressed Sparse Rows representation**. To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream.

Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the `toarray` method of sparse matrices is another option.

### Scaling data with outliers

If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use <span class="title-ref">RobustScaler</span> as a drop-in replacement instead. It uses more robust estimates for the center and range of your data.

<div class="dropdown">

References

Further discussion on the importance of centering and scaling data is available on this FAQ: [Should I normalize/standardize/rescale the data?](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)

</div>

<div class="dropdown">

Scaling vs Whitening

It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.

To address this issue you can use <span class="title-ref">\~sklearn.decomposition.PCA</span> with `whiten=True` to further remove the linear correlation across features.

</div>

### Centering kernel matrices

If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space (possibly implicitly) defined by a function \(\phi(\cdot)\), a <span class="title-ref">KernelCenterer</span> can transform the kernel matrix so that it contains inner products in the feature space defined by \(\phi\) followed by the removal of the mean in that space. In other words, <span class="title-ref">KernelCenterer</span> computes the centered Gram matrix associated to a positive semidefinite kernel \(K\).

<div class="dropdown">

Mathematical formulation

We can have a look at the mathematical formulation now that we have the intuition. Let \(K\) be a kernel matrix of shape <span class="title-ref">(n\_samples, n\_samples)</span> computed from \(X\), a data matrix of shape <span class="title-ref">(n\_samples, n\_features)</span>, during the <span class="title-ref">fit</span> step. \(K\) is defined by

\[K(X, X) = \phi(X) . \phi(X)^{T}\]

\(\phi(X)\) is a function mapping of \(X\) to a Hilbert space. A centered kernel \(\tilde{K}\) is defined as:

\[\tilde{K}(X, X) = \tilde{\phi}(X) . \tilde{\phi}(X)^{T}\]

where \(\tilde{\phi}(X)\) results from centering \(\phi(X)\) in the Hilbert space.

Thus, one could compute \(\tilde{K}\) by mapping \(X\) using the function \(\phi(\cdot)\) and center the data in this new space. However, kernels are often used because they allows some algebra calculations that avoid computing explicitly this mapping using \(\phi(\cdot)\). Indeed, one can implicitly center as shown in Appendix B in [\[Scholkopf1998\]]():

\[\tilde{K} = K - 1_{\text{n}_{samples}} K - K 1_{\text{n}_{samples}} + 1_{\text{n}_{samples}} K 1_{\text{n}_{samples}}\]

\(1_{\text{n}_{samples}}\) is a matrix of <span class="title-ref">(n\_samples, n\_samples)</span> where all entries are equal to \(\frac{1}{\text{n}_{samples}}\). In the <span class="title-ref">transform</span> step, the kernel becomes \(K_{test}(X, Y)\) defined as:

\[K_{test}(X, Y) = \phi(Y) . \phi(X)^{T}\]

\(Y\) is the test dataset of shape <span class="title-ref">(n\_samples\_test, n\_features)</span> and thus \(K_{test}\) is of shape <span class="title-ref">(n\_samples\_test, n\_samples)</span>. In this case, centering \(K_{test}\) is done as:

\[\tilde{K}_{test}(X, Y) = K_{test} - 1'_{\text{n}_{samples}} K - K_{test} 1_{\text{n}_{samples}} + 1'_{\text{n}_{samples}} K 1_{\text{n}_{samples}}\]

\(1'_{\text{n}_{samples}}\) is a matrix of shape <span class="title-ref">(n\_samples\_test, n\_samples)</span> where all entries are equal to \(\frac{1}{\text{n}_{samples}}\).

**References**

</div>

## Non-linear transformation

Two types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature.

Quantile transforms put all features into the same desired distribution based on the formula \(G^{-1}(F(X))\) where \(F\) is the cumulative distribution function of the feature and \(G^{-1}\) the [quantile function](https://en.wikipedia.org/wiki/Quantile_function) of the desired output distribution \(G\). This formula is using the two following facts: (i) if \(X\) is a random variable with a continuous cumulative distribution function \(F\) then \(F(X)\) is uniformly distributed on \([0,1]\); (ii) if \(U\) is a random variable with uniform distribution on \([0,1]\) then \(G^{-1}(U)\) has distribution \(G\). By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.

Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution.

### Mapping to a Uniform distribution

<span class="title-ref">QuantileTransformer</span> provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1:

    >>> from sklearn.datasets import load_iris
    >>> from sklearn.model_selection import train_test_split
    >>> X, y = load_iris(return_X_y=True)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    >>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)
    >>> X_train_trans = quantile_transformer.fit_transform(X_train)
    >>> X_test_trans = quantile_transformer.transform(X_test)
    >>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) # doctest: +SKIP
    array([ 4.3,  5.1,  5.8,  6.5,  7.9])

This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:

    >>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])
    ... # doctest: +SKIP
    array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])

This can be confirmed on a independent testing set with similar remarks:

    >>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])
    ... # doctest: +SKIP
    array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])
    >>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])
    ... # doctest: +SKIP
    array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])

### Mapping to a Gaussian distribution

In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.

<span class="title-ref">PowerTransformer</span> currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform.

<div class="dropdown">

Yeo-Johnson transform

\[\begin{aligned}
x_i^{(\lambda)} =
\begin{cases}
[(x_i + 1)^\lambda - 1] / \lambda & \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt]
\ln{(x_i + 1)} & \text{if } \lambda = 0, x_i \geq 0 \\[8pt]
-[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) & \text{if } \lambda \neq 2, x_i < 0, \\[8pt]
- \ln (- x_i + 1) & \text{if } \lambda = 2, x_i < 0
\end{cases}
\end{aligned}\]

</div>

<div class="dropdown">

Box-Cox transform

\[\begin{aligned}
x_i^{(\lambda)} =
\begin{cases}
\dfrac{x_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\[8pt]
\ln{(x_i)} & \text{if } \lambda = 0,
\end{cases}
\end{aligned}\]

Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \(\lambda\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution:

    >>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)
    >>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))
    >>> X_lognormal
    array([[1.28..., 1.18..., 0.84...],
          [0.94..., 1.60..., 0.38...],
          [1.35..., 0.21..., 1.09...]])
    >>> pt.fit_transform(X_lognormal)
    array([[ 0.49...,  0.17..., -0.15...],
          [-0.05...,  0.58..., -0.57...],
          [ 0.69..., -0.84...,  0.10...]])

While the above example sets the <span class="title-ref">standardize</span> option to <span class="title-ref">False</span>, <span class="title-ref">PowerTransformer</span> will apply zero-mean, unit-variance normalization to the transformed output by default.

</div>

Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation.

![](../auto_examples/preprocessing/images/sphx_glr_plot_map_data_to_normal_001.png)

It is also possible to map data to a normal distribution using <span class="title-ref">QuantileTransformer</span> by setting `output_distribution='normal'`. Using the earlier example with the iris dataset:

    >>> quantile_transformer = preprocessing.QuantileTransformer(
    ...     output_distribution='normal', random_state=0)
    >>> X_trans = quantile_transformer.fit_transform(X)
    >>> quantile_transformer.quantiles_
    array([[4.3, 2. , 1. , 0.1],
           [4.4, 2.2, 1.1, 0.1],
           [4.4, 2.2, 1.2, 0.1],
           ...,
           [7.7, 4.1, 6.7, 2.5],
           [7.7, 4.2, 6.7, 2.5],
           [7.9, 4.4, 6.9, 2.5]])

Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input's minimum and maximum ---corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively --- do not become infinite under the transformation.

## Normalization

**Normalization** is the process of **scaling individual samples to have unit norm**. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.

This assumption is the base of the [Vector Space Model](https://en.wikipedia.org/wiki/Vector_Space_Model) often used in text classification and clustering contexts.

The function <span class="title-ref">normalize</span> provides a quick and easy way to perform this operation on a single array-like dataset, either using the `l1`, `l2`, or `max` norms:

    >>> X = [[ 1., -1.,  2.],
    ...      [ 2.,  0.,  0.],
    ...      [ 0.,  1., -1.]]
    >>> X_normalized = preprocessing.normalize(X, norm='l2')
    
    >>> X_normalized
    array([[ 0.40..., -0.40...,  0.81...],
           [ 1.  ...,  0.  ...,  0.  ...],
           [ 0.  ...,  0.70..., -0.70...]])

The `preprocessing` module further provides a utility class <span class="title-ref">Normalizer</span> that implements the same operation using the `Transformer` API (even though the `fit` method is useless in this case: the class is stateless as this operation treats samples independently).

This class is hence suitable for use in the early steps of a \`\~sklearn.pipeline.Pipeline\`:

    >>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
    >>> normalizer
    Normalizer()

The normalizer instance can then be used on sample vectors as any transformer:

    >>> normalizer.transform(X)
    array([[ 0.40..., -0.40...,  0.81...],
           [ 1.  ...,  0.  ...,  0.  ...],
           [ 0.  ...,  0.70..., -0.70...]])
    
    >>> normalizer.transform([[-1.,  1., 0.]])
    array([[-0.70...,  0.70...,  0.  ...]])

Note: L2 normalization is also known as spatial sign preprocessing.

<div class="dropdown">

Sparse input

<span class="title-ref">normalize</span> and <span class="title-ref">Normalizer</span> accept **both dense array-like and sparse matrices from scipy.sparse as input**.

For sparse input the data is **converted to the Compressed Sparse Rows representation** (see `scipy.sparse.csr_matrix`) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.

</div>

## Encoding categorical features

Often features are not given as continuous values but categorical. For example a person could have features `["male", "female"]`, `["from Europe", "from US", "from Asia"]`, `["uses Firefox", "uses Chrome", "uses Safari", "uses Internet Explorer"]`. Such features can be efficiently coded as integers, for instance `["male", "from US", "uses Internet Explorer"]` could be expressed as `[0, 1, 3]` while `["female", "from Asia", "uses Chrome"]` would be `[1, 2, 1]`.

To convert categorical features to such integer codes, we can use the <span class="title-ref">OrdinalEncoder</span>. This estimator transforms each categorical feature to one new feature of integers (0 to n\_categories - 1):

    >>> enc = preprocessing.OrdinalEncoder()
    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
    >>> enc.fit(X)
    OrdinalEncoder()
    >>> enc.transform([['female', 'from US', 'uses Safari']])
    array([[0., 1., 1.]])

Such integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).

By default, <span class="title-ref">OrdinalEncoder</span> will also passthrough missing values that are indicated by <span class="title-ref">np.nan</span>.

> \>\>\> enc = preprocessing.OrdinalEncoder() \>\>\> X = \[\['male'\], \['female'\], \[np.nan\], \['female'\]\] \>\>\> enc.fit\_transform(X) array(\[\[ 1.\], \[ 0.\], \[nan\], \[ 0.\]\])

<span class="title-ref">OrdinalEncoder</span> provides a parameter <span class="title-ref">encoded\_missing\_value</span> to encode the missing values without the need to create a pipeline and using <span class="title-ref">\~sklearn.impute.SimpleImputer</span>.

> \>\>\> enc = preprocessing.OrdinalEncoder(encoded\_missing\_value=-1) \>\>\> X = \[\['male'\], \['female'\], \[np.nan\], \['female'\]\] \>\>\> enc.fit\_transform(X) array(\[\[ 1.\], \[ 0.\], \[-1.\], \[ 0.\]\])

The above processing is equivalent to the following pipeline:

    >>> from sklearn.pipeline import Pipeline
    >>> from sklearn.impute import SimpleImputer
    >>> enc = Pipeline(steps=[
    ...     ("encoder", preprocessing.OrdinalEncoder()),
    ...     ("imputer", SimpleImputer(strategy="constant", fill_value=-1)),
    ... ])
    >>> enc.fit_transform(X)
    array([[ 1.],
           [ 0.],
           [-1.],
           [ 0.]])

Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the <span class="title-ref">OneHotEncoder</span>, which transforms each categorical feature with `n_categories` possible values into `n_categories` binary features, with one of them 1, and all others 0.

Continuing the example above:

    >>> enc = preprocessing.OneHotEncoder()
    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
    >>> enc.fit(X)
    OneHotEncoder()
    >>> enc.transform([['female', 'from US', 'uses Safari'],
    ...                ['male', 'from Europe', 'uses Safari']]).toarray()
    array([[1., 0., 0., 1., 0., 1.],
           [0., 1., 1., 0., 0., 1.]])

By default, the values each feature can take is inferred automatically from the dataset and can be found in the `categories_` attribute:

    >>> enc.categories_
    [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]

It is possible to specify this explicitly using the parameter `categories`. There are two genders, four possible continents and four web browsers in our dataset:

    >>> genders = ['female', 'male']
    >>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']
    >>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']
    >>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])
    >>> # Note that for there are missing categorical values for the 2nd and 3rd
    >>> # feature
    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
    >>> enc.fit(X)
    OneHotEncoder(categories=[['female', 'male'],
                              ['from Africa', 'from Asia', 'from Europe',
                               'from US'],
                              ['uses Chrome', 'uses Firefox', 'uses IE',
                               'uses Safari']])
    >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()
    array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])

If there is a possibility that the training data might have missing categorical features, it can often be better to specify <span class="title-ref">handle\_unknown='infrequent\_if\_exist'</span> instead of setting the <span class="title-ref">categories</span> manually as above. When <span class="title-ref">handle\_unknown='infrequent\_if\_exist'</span> is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros or considered as an infrequent category if enabled. (<span class="title-ref">handle\_unknown='infrequent\_if\_exist'</span> is only supported for one-hot encoding):

    >>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')
    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
    >>> enc.fit(X)
    OneHotEncoder(handle_unknown='infrequent_if_exist')
    >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()
    array([[1., 0., 0., 0., 0., 0.]])

It is also possible to encode each column into `n_categories - 1` columns instead of `n_categories` columns by using the `drop` parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (<span class="title-ref">LinearRegression \<sklearn.linear\_model.LinearRegression\></span>), since co-linearity would cause the covariance matrix to be non-invertible:

    >>> X = [['male', 'from US', 'uses Safari'],
    ...      ['female', 'from Europe', 'uses Firefox']]
    >>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)
    >>> drop_enc.categories_
    [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),
     array(['uses Firefox', 'uses Safari'], dtype=object)]
    >>> drop_enc.transform(X).toarray()
    array([[1., 1., 1.],
           [0., 0., 0.]])

One might want to drop one of the two columns only for features with 2 categories. In this case, you can set the parameter <span class="title-ref">drop='if\_binary'</span>.

> \>\>\> X = \[\['male', 'US', 'Safari'\], ... \['female', 'Europe', 'Firefox'\], ... \['female', 'Asia', 'Chrome'\]\] \>\>\> drop\_enc = preprocessing.OneHotEncoder(drop='if\_binary').fit(X) \>\>\> [drop\_enc.categories]() \[array(\['female', 'male'\], dtype=object), array(\['Asia', 'Europe', 'US'\], dtype=object), array(\['Chrome', 'Firefox', 'Safari'\], dtype=object)\] \>\>\> drop\_enc.transform(X).toarray() array(\[\[1., 0., 0., 1., 0., 0., 1.\], \[0., 0., 1., 0., 0., 1., 0.\], \[0., 1., 0., 0., 1., 0., 0.\]\])

In the transformed <span class="title-ref">X</span>, the first column is the encoding of the feature with categories "male"/"female", while the remaining 6 columns is the encoding of the 2 features with respectively 3 categories each.

When <span class="title-ref">handle\_unknown='ignore'</span> and <span class="title-ref">drop</span> is not None, unknown categories will be encoded as all zeros:

    >>> drop_enc = preprocessing.OneHotEncoder(drop='first',
    ...                                        handle_unknown='ignore').fit(X)
    >>> X_test = [['unknown', 'America', 'IE']]
    >>> drop_enc.transform(X_test).toarray()
    array([[0., 0., 0., 0., 0.]])

All the categories in <span class="title-ref">X\_test</span> are unknown during transform and will be mapped to all zeros. This means that unknown categories will have the same mapping as the dropped category. <span class="title-ref">OneHotEncoder.inverse\_transform</span> will map all zeros to the dropped category if a category is dropped and <span class="title-ref">None</span> if a category is not dropped:

    >>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,
    ...                                        handle_unknown='ignore').fit(X)
    >>> X_test = [['unknown', 'America', 'IE']]
    >>> X_trans = drop_enc.transform(X_test)
    >>> X_trans
    array([[0., 0., 0., 0., 0., 0., 0.]])
    >>> drop_enc.inverse_transform(X_trans)
    array([['female', None, None]], dtype=object)

<div class="dropdown">

Support of categorical features with missing values

<span class="title-ref">OneHotEncoder</span> supports categorical features with missing values by considering the missing values as an additional category:

    >>> X = [['male', 'Safari'],
    ...      ['female', None],
    ...      [np.nan, 'Firefox']]
    >>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)
    >>> enc.categories_
    [array(['female', 'male', nan], dtype=object),
    array(['Firefox', 'Safari', None], dtype=object)]
    >>> enc.transform(X).toarray()
    array([[0., 1., 0., 0., 1., 0.],
          [1., 0., 0., 0., 0., 1.],
          [0., 0., 1., 1., 0., 0.]])

If a feature contains both <span class="title-ref">np.nan</span> and <span class="title-ref">None</span>, they will be considered separate categories:

    >>> X = [['Safari'], [None], [np.nan], ['Firefox']]
    >>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)
    >>> enc.categories_
    [array(['Firefox', 'Safari', None, nan], dtype=object)]
    >>> enc.transform(X).toarray()
    array([[0., 1., 0., 0.],
          [0., 0., 1., 0.],
          [0., 0., 0., 1.],
          [1., 0., 0., 0.]])

See \[dict\_feature\_extraction\](\#dict\_feature\_extraction) for categorical features that are represented as a dict, not as scalars.

</div>

### Infrequent categories

<span class="title-ref">OneHotEncoder</span> and <span class="title-ref">OrdinalEncoder</span> support aggregating infrequent categories into a single output for each feature. The parameters to enable the gathering of infrequent categories are <span class="title-ref">min\_frequency</span> and <span class="title-ref">max\_categories</span>.

1.  <span class="title-ref">min\_frequency</span> is either an integer greater or equal to 1, or a float in the interval <span class="title-ref">(0.0, 1.0)</span>. If <span class="title-ref">min\_frequency</span> is an integer, categories with a cardinality smaller than <span class="title-ref">min\_frequency</span> will be considered infrequent. If <span class="title-ref">min\_frequency</span> is a float, categories with a cardinality smaller than this fraction of the total number of samples will be considered infrequent. The default value is 1, which means every category is encoded separately.
2.  <span class="title-ref">max\_categories</span> is either <span class="title-ref">None</span> or any integer greater than 1. This parameter sets an upper limit to the number of output features for each input feature. <span class="title-ref">max\_categories</span> includes the feature that combines infrequent categories.

In the following example with <span class="title-ref">OrdinalEncoder</span>, the categories <span class="title-ref">'dog' and 'snake'</span> are considered infrequent:

    >>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +
    ...               ['snake'] * 3], dtype=object).T
    >>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)
    >>> enc.infrequent_categories_
    [array(['dog', 'snake'], dtype=object)]
    >>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))
    array([[2.],
           [0.],
           [1.],
           [2.]])

<span class="title-ref">OrdinalEncoder</span>'s <span class="title-ref">max\_categories</span> do **not** take into account missing or unknown categories. Setting <span class="title-ref">unknown\_value</span> or <span class="title-ref">encoded\_missing\_value</span> to an integer will increase the number of unique integer codes by one each. This can result in up to <span class="title-ref">max\_categories + 2</span> integer codes. In the following example, "a" and "d" are considered infrequent and grouped together into a single category, "b" and "c" are their own categories, unknown values are encoded as 3 and missing values are encoded as 4.

> \>\>\> X\_train = np.array( ... \[\["a"\] \* 5 + \["b"\] \* 20 + \["c"\] \* 10 + \["d"\] \* 3 + \[np.nan\]\], ... dtype=object).T \>\>\> enc = preprocessing.OrdinalEncoder( ... handle\_unknown="use\_encoded\_value", unknown\_value=3, ... max\_categories=3, encoded\_missing\_value=4) \>\>\> \_ = enc.fit(X\_train) \>\>\> X\_test = np.array(\[\["a"\], \["b"\], \["c"\], \["d"\], \["e"\], \[np.nan\]\], dtype=object) \>\>\> enc.transform(X\_test) array(\[\[2.\], \[0.\], \[1.\], \[2.\], \[3.\], \[4.\]\])

Similarity, <span class="title-ref">OneHotEncoder</span> can be configured to group together infrequent categories:

    >>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)
    >>> enc.infrequent_categories_
    [array(['dog', 'snake'], dtype=object)]
    >>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))
    array([[0., 0., 1.],
           [1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.]])

By setting handle\_unknown to <span class="title-ref">'infrequent\_if\_exist'</span>, unknown categories will be considered infrequent:

    >>> enc = preprocessing.OneHotEncoder(
    ...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)
    >>> enc = enc.fit(X)
    >>> enc.transform(np.array([['dragon']]))
    array([[0., 0., 1.]])

<span class="title-ref">OneHotEncoder.get\_feature\_names\_out</span> uses 'infrequent' as the infrequent feature name:

    >>> enc.get_feature_names_out()
    array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)

When <span class="title-ref">'handle\_unknown'</span> is set to <span class="title-ref">'infrequent\_if\_exist'</span> and an unknown category is encountered in transform:

1.  If infrequent category support was not configured or there was no infrequent category during training, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as <span class="title-ref">None</span>.
2.  If there is an infrequent category during training, the unknown category will be considered infrequent. In the inverse transform, 'infrequent\_sklearn' will be used to represent the infrequent category.

Infrequent categories can also be configured using <span class="title-ref">max\_categories</span>. In the following example, we set <span class="title-ref">max\_categories=2</span> to limit the number of features in the output. This will result in all but the <span class="title-ref">'cat'</span> category to be considered infrequent, leading to two features, one for <span class="title-ref">'cat'</span> and one for infrequent categories - which are all the others:

    >>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)
    >>> enc = enc.fit(X)
    >>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])
    array([[0., 1.],
           [1., 0.],
           [0., 1.],
           [0., 1.]])

If both <span class="title-ref">max\_categories</span> and <span class="title-ref">min\_frequency</span> are non-default values, then categories are selected based on <span class="title-ref">min\_frequency</span> first and <span class="title-ref">max\_categories</span> categories are kept. In the following example, <span class="title-ref">min\_frequency=4</span> considers only <span class="title-ref">snake</span> to be infrequent, but <span class="title-ref">max\_categories=3</span>, forces <span class="title-ref">dog</span> to also be infrequent:

    >>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)
    >>> enc = enc.fit(X)
    >>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])
    array([[0., 0., 1.],
           [1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.]])

If there are infrequent categories with the same cardinality at the cutoff of <span class="title-ref">max\_categories</span>, then then the first <span class="title-ref">max\_categories</span> are taken based on lexicon ordering. In the following example, "b", "c", and "d", have the same cardinality and with <span class="title-ref">max\_categories=2</span>, "b" and "c" are infrequent because they have a higher lexicon order.

> \>\>\> X = np.asarray(\[\["a"\] \* 20 + \["b"\] \* 10 + \["c"\] \* 10 + \["d"\] \* 10\], dtype=object).T \>\>\> enc = preprocessing.OneHotEncoder(max\_categories=3).fit(X) \>\>\> [enc.infrequent\_categories]() \[array(\['b', 'c'\], dtype=object)\]

### Target Encoder

<div class="currentmodule">

sklearn.preprocessing

</div>

The <span class="title-ref">TargetEncoder</span> uses the target mean conditioned on the categorical feature for encoding unordered categories, i.e. nominal categories [\[PAR\]](#PAR) [\[MIC\]](#MIC). This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories are location based such as zip code or region.

<div class="dropdown">

Binary classification targets

For the binary classification target, the target encoding is given by:

\[S_i = \lambda_i\frac{n_{iY}}{n_i} + (1 - \lambda_i)\frac{n_Y}{n}\]

where \(S_i\) is the encoding for category \(i\), \(n_{iY}\) is the number of observations with \(Y=1\) and category \(i\), \(n_i\) is the number of observations with category \(i\), \(n_Y\) is the number of observations with \(Y=1\), \(n\) is the number of observations, and \(\lambda_i\) is a shrinkage factor for category \(i\). The shrinkage factor is given by:

\[\lambda_i = \frac{n_i}{m + n_i}\]

where \(m\) is a smoothing factor, which is controlled with the <span class="title-ref">smooth</span> parameter in <span class="title-ref">TargetEncoder</span>. Large smoothing factors will put more weight on the global mean. When <span class="title-ref">smooth="auto"</span>, the smoothing factor is computed as an empirical Bayes estimate: \(m=\sigma_i^2/\tau^2\), where \(\sigma_i^2\) is the variance of <span class="title-ref">y</span> with category \(i\) and \(\tau^2\) is the global variance of <span class="title-ref">y</span>.

</div>

<div class="dropdown">

Multiclass classification targets

For multiclass classification targets, the formulation is similar to binary classification:

\[S_{ij} = \lambda_i\frac{n_{iY_j}}{n_i} + (1 - \lambda_i)\frac{n_{Y_j}}{n}\]

where \(S_{ij}\) is the encoding for category \(i\) and class \(j\), \(n_{iY_j}\) is the number of observations with \(Y=j\) and category \(i\), \(n_i\) is the number of observations with category \(i\), \(n_{Y_j}\) is the number of observations with \(Y=j\), \(n\) is the number of observations, and \(\lambda_i\) is a shrinkage factor for category \(i\).

</div>

<div class="dropdown">

Continuous targets

For continuous targets, the formulation is similar to binary classification:

\[S_i = \lambda_i\frac{\sum_{k\in L_i}Y_k}{n_i} + (1 - \lambda_i)\frac{\sum_{k=1}^{n}Y_k}{n}\]

where \(L_i\) is the set of observations with category \(i\) and \(n_i\) is the number of observations with category \(i\).

</div>

<span class="title-ref">\~TargetEncoder.fit\_transform</span> internally relies on a `cross fitting` scheme to prevent target information from leaking into the train-time representation, especially for non-informative high-cardinality categorical variables, and help prevent the downstream model from overfitting spurious correlations. Note that as a result, <span class="title-ref">fit(X, y).transform(X)</span> does not equal <span class="title-ref">fit\_transform(X, y)</span>. In <span class="title-ref">\~TargetEncoder.fit\_transform</span>, the training data is split into *k* folds (determined by the <span class="title-ref">cv</span> parameter) and each fold is encoded using the encodings learnt using the other *k-1* folds. The following diagram shows the `cross fitting` scheme in <span class="title-ref">\~TargetEncoder.fit\_transform</span> with the default \`cv=5\`:

![image](../images/target_encoder_cross_validation.svg)

<span class="title-ref">\~TargetEncoder.fit\_transform</span> also learns a 'full data' encoding using the whole training set. This is never used in <span class="title-ref">\~TargetEncoder.fit\_transform</span> but is saved to the attribute <span class="title-ref">encodings\_</span>, for use when <span class="title-ref">\~TargetEncoder.transform</span> is called. Note that the encodings learned for each fold during the `cross fitting` scheme are not saved to an attribute.

The <span class="title-ref">\~TargetEncoder.fit</span> method does **not** use any `cross fitting` schemes and learns one encoding on the entire training set, which is used to encode categories in <span class="title-ref">\~TargetEncoder.transform</span>. This encoding is the same as the 'full data' encoding learned in <span class="title-ref">\~TargetEncoder.fit\_transform</span>.

<div class="note">

<div class="title">

Note

</div>

<span class="title-ref">TargetEncoder</span> considers missing values, such as <span class="title-ref">np.nan</span> or <span class="title-ref">None</span>, as another category and encodes them like any other category. Categories that are not seen during <span class="title-ref">fit</span> are encoded with the target mean, i.e. <span class="title-ref">target\_mean\_</span>.

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_target\_encoder.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_target\_encoder.py)
  - \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_target\_encoder\_cross\_val.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_target\_encoder\_cross\_val.py)

**References**

## Discretization

[Discretization](https://en.wikipedia.org/wiki/Discretization_of_continuous_features) (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes.

One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. For more advanced possibilities, in particular smooth ones, see \[generating\_polynomial\_features\](\#generating\_polynomial\_features) further below.

### K-bins discretization

<span class="title-ref">KBinsDiscretizer</span> discretizes features into `k` bins:

    >>> X = np.array([[ -3., 5., 15 ],
    ...               [  0., 6., 14 ],
    ...               [  6., 3., 11 ]])
    >>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)

By default the output is one-hot encoded into a sparse matrix (See \[preprocessing\_categorical\_features\](\#preprocessing\_categorical\_features)) and this can be configured with the `encode` parameter. For each feature, the bin edges are computed during `fit` and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as:

  - feature 1: \({[-\infty, -1), [-1, 2), [2, \infty)}\)
  - feature 2: \({[-\infty, 5), [5, \infty)}\)
  - feature 3: \({[-\infty, 14), [14, \infty)}\)

Based on these bin intervals, `X` is transformed as follows:

    >>> est.transform(X)                      # doctest: +SKIP
    array([[ 0., 1., 1.],
           [ 1., 1., 1.],
           [ 2., 0., 0.]])

The resulting dataset contains ordinal attributes which can be further used in a <span class="title-ref">\~sklearn.pipeline.Pipeline</span>.

Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins.

<span class="title-ref">KBinsDiscretizer</span> implements different binning strategies, which can be selected with the `strategy` parameter. The 'uniform' strategy uses constant-width bins. The 'quantile' strategy uses the quantiles values to have equally populated bins in each feature. The 'kmeans' strategy defines bins based on a k-means clustering procedure performed on each feature independently.

Be aware that one can specify custom bins by passing a callable defining the discretization strategy to <span class="title-ref">\~sklearn.preprocessing.FunctionTransformer</span>. For instance, we can use the Pandas function \`pandas.cut\`:

    >>> import pandas as pd
    >>> import numpy as np
    >>> from sklearn import preprocessing
    >>>
    >>> bins = [0, 1, 13, 20, 60, np.inf]
    >>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']
    >>> transformer = preprocessing.FunctionTransformer(
    ...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}
    ... )
    >>> X = np.array([0.2, 2, 15, 25, 97])
    >>> transformer.fit_transform(X)
    ['infant', 'kid', 'teen', 'adult', 'senior citizen']
    Categories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']

**Examples**

  - \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_discretization.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_discretization.py)
  - \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_discretization\_classification.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_discretization\_classification.py)
  - \[sphx\_glr\_auto\_examples\_preprocessing\_plot\_discretization\_strategies.py\](\#sphx\_glr\_auto\_examples\_preprocessing\_plot\_discretization\_strategies.py)

### Feature binarization

**Feature binarization** is the process of **thresholding numerical features to get boolean values**. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution). For instance, this is the case for the <span class="title-ref">\~sklearn.neural\_network.BernoulliRBM</span>.

It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.

As for the <span class="title-ref">Normalizer</span>, the utility class <span class="title-ref">Binarizer</span> is meant to be used in the early stages of <span class="title-ref">\~sklearn.pipeline.Pipeline</span>. The `fit` method does nothing as each sample is treated independently of others:

    >>> X = [[ 1., -1.,  2.],
    ...      [ 2.,  0.,  0.],
    ...      [ 0.,  1., -1.]]
    
    >>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing
    >>> binarizer
    Binarizer()
    
    >>> binarizer.transform(X)
    array([[1., 0., 1.],
           [1., 0., 0.],
           [0., 1., 0.]])

It is possible to adjust the threshold of the binarizer:

    >>> binarizer = preprocessing.Binarizer(threshold=1.1)
    >>> binarizer.transform(X)
    array([[0., 0., 1.],
           [1., 0., 0.],
           [0., 0., 0.]])

As for the <span class="title-ref">Normalizer</span> class, the preprocessing module provides a companion function <span class="title-ref">binarize</span> to be used when the transformer API is not necessary.

Note that the <span class="title-ref">Binarizer</span> is similar to the <span class="title-ref">KBinsDiscretizer</span> when `k = 2`, and when the bin edge is at the value `threshold`.

<div class="topic">

**Sparse input**

<span class="title-ref">binarize</span> and <span class="title-ref">Binarizer</span> accept **both dense array-like and sparse matrices from scipy.sparse as input**.

For sparse input the data is **converted to the Compressed Sparse Rows representation** (see `scipy.sparse.csr_matrix`). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.

</div>

## Imputation of missing values

Tools for imputing missing values are discussed at \[impute\](\#impute).

## Generating polynomial features

Often it's useful to add complexity to a model by considering nonlinear features of the input data. We show two possibilities that are both based on polynomials: The first one uses pure polynomials, the second one uses splines, i.e. piecewise polynomials.

### Polynomial features

A simple and common method to use is polynomial features, which can get features' high-order and interaction terms. It is implemented in \`PolynomialFeatures\`:

    >>> import numpy as np
    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> X = np.arange(6).reshape(3, 2)
    >>> X
    array([[0, 1],
           [2, 3],
           [4, 5]])
    >>> poly = PolynomialFeatures(2)
    >>> poly.fit_transform(X)
    array([[ 1.,  0.,  1.,  0.,  0.,  1.],
           [ 1.,  2.,  3.,  4.,  6.,  9.],
           [ 1.,  4.,  5., 16., 20., 25.]])

The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).

In some cases, only interaction terms among features are required, and it can be gotten with the setting `interaction_only=True`:

    >>> X = np.arange(9).reshape(3, 3)
    >>> X
    array([[0, 1, 2],
           [3, 4, 5],
           [6, 7, 8]])
    >>> poly = PolynomialFeatures(degree=3, interaction_only=True)
    >>> poly.fit_transform(X)
    array([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],
           [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],
           [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])

The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).

Note that polynomial features are used implicitly in [kernel methods](https://en.wikipedia.org/wiki/Kernel_method) (e.g., <span class="title-ref">\~sklearn.svm.SVC</span>, <span class="title-ref">\~sklearn.decomposition.KernelPCA</span>) when using polynomial \[svm\_kernels\](\#svm\_kernels).

See \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_polynomial\_interpolation.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_polynomial\_interpolation.py) for Ridge regression using created polynomial features.

### Spline transformer

Another way to add nonlinear terms instead of pure polynomials of features is to generate spline basis functions for each feature with the <span class="title-ref">SplineTransformer</span>. Splines are piecewise polynomials, parametrized by their polynomial degree and the positions of the knots. The <span class="title-ref">SplineTransformer</span> implements a B-spline basis, cf. the references below.

\> **Note** \> The <span class="title-ref">SplineTransformer</span> treats each feature separately, i.e. it won't give you interaction terms.

Some of the advantages of splines over polynomials are:

  - B-splines are very flexible and robust if you keep a fixed low degree, usually 3, and parsimoniously adapt the number of knots. Polynomials would need a higher degree, which leads to the next point.
  - B-splines do not have oscillatory behaviour at the boundaries as have polynomials (the higher the degree, the worse). This is known as [Runge's phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon).
  - B-splines provide good options for extrapolation beyond the boundaries, i.e. beyond the range of fitted values. Have a look at the option `extrapolation`.
  - B-splines generate a feature matrix with a banded structure. For a single feature, every row contains only `degree + 1` non-zero elements, which occur consecutively and are even positive. This results in a matrix with good numerical properties, e.g. a low condition number, in sharp contrast to a matrix of polynomials, which goes under the name [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix). A low condition number is important for stable algorithms of linear models.

The following code snippet shows splines in action:

    >>> import numpy as np
    >>> from sklearn.preprocessing import SplineTransformer
    >>> X = np.arange(5).reshape(5, 1)
    >>> X
    array([[0],
           [1],
           [2],
           [3],
           [4]])
    >>> spline = SplineTransformer(degree=2, n_knots=3)
    >>> spline.fit_transform(X)
    array([[0.5  , 0.5  , 0.   , 0.   ],
           [0.125, 0.75 , 0.125, 0.   ],
           [0.   , 0.5  , 0.5  , 0.   ],
           [0.   , 0.125, 0.75 , 0.125],
           [0.   , 0.   , 0.5  , 0.5  ]])

As the `X` is sorted, one can easily see the banded matrix output. Only the three middle diagonals are non-zero for `degree=2`. The higher the degree, the more overlapping of the splines.

Interestingly, a <span class="title-ref">SplineTransformer</span> of `degree=0` is the same as <span class="title-ref">\~sklearn.preprocessing.KBinsDiscretizer</span> with `encode='onehot-dense'` and `n_bins = n_knots - 1` if `knots = strategy`.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_polynomial\_interpolation.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_polynomial\_interpolation.py)
  - \[sphx\_glr\_auto\_examples\_applications\_plot\_cyclical\_feature\_engineering.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_cyclical\_feature\_engineering.py)

<div class="dropdown">

References

  - Eilers, P., & Marx, B. (1996). `Flexible Smoothing with B-splines and
    Penalties <10.1214/ss/1038425655>`. Statist. Sci. 11 (1996), no. 2, 89--121.
  - Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. `A review of
    spline function procedures in R <10.1186/s12874-019-0666-3>`. BMC Med Res Methodol 19, 46 (2019).

</div>

## Custom transformers

Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with <span class="title-ref">FunctionTransformer</span>. For example, to build a transformer that applies a log transformation in a pipeline, do:

    >>> import numpy as np
    >>> from sklearn.preprocessing import FunctionTransformer
    >>> transformer = FunctionTransformer(np.log1p, validate=True)
    >>> X = np.array([[0, 1], [2, 3]])
    >>> # Since FunctionTransformer is no-op during fit, we can call transform directly
    >>> transformer.transform(X)
    array([[0.        , 0.69314718],
           [1.09861229, 1.38629436]])

You can ensure that `func` and `inverse_func` are the inverse of each other by setting `check_inverse=True` and calling `fit` before `transform`. Please note that a warning is raised and can be turned into an error with a `filterwarnings`:

    >>> import warnings
    >>> warnings.filterwarnings("error", message=".*check_inverse*.",
    ...                         category=UserWarning, append=False)

For a full code example that demonstrates using a <span class="title-ref">FunctionTransformer</span> to extract features from text data see \[sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_column\_transformer.py) and \[sphx\_glr\_auto\_examples\_applications\_plot\_cyclical\_feature\_engineering.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_cyclical\_feature\_engineering.py).

<div id="citations">

  - <span id="MIC" class="citation-label">MIC</span>  
    `Micci-Barreca, Daniele. "A preprocessing scheme for high-cardinality
    categorical attributes in classification and prediction problems"
    SIGKDD Explor. Newsl. 3, 1 (July 2001), 27-32. <10.1145/507533.507538>`

  - <span id="PAR" class="citation-label">PAR</span>  
    `Pargent, F., Pfisterer, F., Thomas, J. et al. "Regularized target
    encoding outperforms traditional methods in supervised machine learning with
    high cardinality features" Comput Stat 37, 2671-2692 (2022)
    <10.1007/s00180-022-01207-6>`

</div>

---

preprocessing_targets.md

---

<div class="currentmodule">

sklearn.preprocessing

</div>

# Transforming the prediction target (`y`)

These are transformers that are not intended to be used on features, only on supervised learning targets. See also \[transformed\_target\_regressor\](\#transformed\_target\_regressor) if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.

## Label binarization

### LabelBinarizer

<span class="title-ref">LabelBinarizer</span> is a utility class to help create a `label
indicator matrix` from a list of `multiclass` labels:

    >>> from sklearn import preprocessing
    >>> lb = preprocessing.LabelBinarizer()
    >>> lb.fit([1, 2, 6, 4, 2])
    LabelBinarizer()
    >>> lb.classes_
    array([1, 2, 4, 6])
    >>> lb.transform([1, 6])
    array([[1, 0, 0, 0],
           [0, 0, 0, 1]])

Using this format can enable multiclass classification in estimators that support the label indicator matrix format.

\> **Warning** \> LabelBinarizer is not needed if you are using an estimator that already supports `multiclass` data.

For more information about multiclass classification, refer to \[multiclass\_classification\](\#multiclass\_classification).

### MultiLabelBinarizer

In `multilabel` learning, the joint set of binary classification tasks is expressed with a label binary indicator array: each sample is one row of a 2d array of shape (n\_samples, n\_classes) with binary values where the one, i.e. the non zero elements, corresponds to the subset of labels for that sample. An array such as `np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])` represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.

Producing multilabel data as a list of sets of labels may be more intuitive. The <span class="title-ref">MultiLabelBinarizer \<sklearn.preprocessing.MultiLabelBinarizer\></span> transformer can be used to convert between a collection of collections of labels and the indicator format:

    >>> from sklearn.preprocessing import MultiLabelBinarizer
    >>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]
    >>> MultiLabelBinarizer().fit_transform(y)
    array([[0, 0, 1, 1, 1],
           [0, 0, 1, 0, 0],
           [1, 1, 0, 1, 0],
           [1, 1, 1, 1, 1],
           [1, 1, 1, 0, 0]])

For more information about multilabel classification, refer to \[multilabel\_classification\](\#multilabel\_classification).

## Label encoding

<span class="title-ref">LabelEncoder</span> is a utility class to help normalize labels such that they contain only values between 0 and n\_classes-1. This is sometimes useful for writing efficient Cython routines. <span class="title-ref">LabelEncoder</span> can be used as follows:

    >>> from sklearn import preprocessing
    >>> le = preprocessing.LabelEncoder()
    >>> le.fit([1, 2, 2, 6])
    LabelEncoder()
    >>> le.classes_
    array([1, 2, 6])
    >>> le.transform([1, 1, 2, 6])
    array([0, 0, 1, 2])
    >>> le.inverse_transform([0, 0, 1, 2])
    array([1, 1, 2, 6])

It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:

    >>> le = preprocessing.LabelEncoder()
    >>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
    LabelEncoder()
    >>> list(le.classes_)
    ['amsterdam', 'paris', 'tokyo']
    >>> le.transform(["tokyo", "tokyo", "paris"])
    array([2, 2, 1])
    >>> list(le.inverse_transform([2, 2, 1]))
    ['tokyo', 'tokyo', 'paris']

---

random_projection.md

---

# Random Projection

<div class="currentmodule">

sklearn.random\_projection

</div>

The `sklearn.random_projection` module implements a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. This module implements two types of unstructured random matrix: \[Gaussian random matrix \<gaussian\_random\_matrix\>\](\#gaussian-random-matrix-\<gaussian\_random\_matrix\>) and \[sparse random matrix \<sparse\_random\_matrix\>\](\#sparse-random-matrix-\<sparse\_random\_matrix\>).

The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.

**References**

  - Sanjoy Dasgupta. 2000. [Experiments with random projection.](https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf) In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI'00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.
  - Ella Bingham and Heikki Mannila. 2001. [Random projection in dimensionality reduction: applications to image and text data.](https://citeseerx.ist.psu.edu/doc_view/pid/aed77346f737b0ed5890b61ad02e5eb4ab2f3dc6) In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '01). ACM, New York, NY, USA, 245-250.

## The Johnson-Lindenstrauss lemma

The main theoretical result behind the efficiency of random projection is the [Johnson-Lindenstrauss lemma (quoting Wikipedia)](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma):

> In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.

Knowing only the number of samples, the <span class="title-ref">johnson\_lindenstrauss\_min\_dim</span> estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:

    >>> from sklearn.random_projection import johnson_lindenstrauss_min_dim
    >>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)
    663
    >>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])
    array([    663,   11841, 1112658])
    >>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)
    array([ 7894,  9868, 11841])

![](../auto_examples/miscellaneous/images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png)

![](../auto_examples/miscellaneous/images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png)

**Examples**

  - See \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_johnson\_lindenstrauss\_bound.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_johnson\_lindenstrauss\_bound.py) for a theoretical explication on the Johnson-Lindenstrauss lemma and an empirical validation using sparse random matrices.

**References**

  - Sanjoy Dasgupta and Anupam Gupta, 1999. [An elementary proof of the Johnson-Lindenstrauss Lemma.](https://citeseerx.ist.psu.edu/doc_view/pid/95cd464d27c25c9c8690b378b894d337cdf021f9)

## Gaussian random projection

The <span class="title-ref">GaussianRandomProjection</span> reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution \(N(0, \frac{1}{n_{components}})\).

Here a small excerpt which illustrates how to use the Gaussian random projection transformer:

    >>> import numpy as np
    >>> from sklearn import random_projection
    >>> X = np.random.rand(100, 10000)
    >>> transformer = random_projection.GaussianRandomProjection()
    >>> X_new = transformer.fit_transform(X)
    >>> X_new.shape
    (100, 3947)

## Sparse random projection

The <span class="title-ref">SparseRandomProjection</span> reduces the dimensionality by projecting the original input space using a sparse random matrix.

Sparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.

If we define `s = 1 / density`, the elements of the random matrix are drawn from

\[\begin{aligned}
\left\{
\begin{array}{c c l}
-\sqrt{\frac{s}{n_{\text{components}}}} & & 1 / 2s\\
0 &\text{with probability}  & 1 - 1 / s \\
+\sqrt{\frac{s}{n_{\text{components}}}} & & 1 / 2s\\
\end{array}
\right.
\end{aligned}\]

where \(n_{\text{components}}\) is the size of the projected subspace. By default the density of non zero elements is set to the minimum density as recommended by Ping Li et al.: \(1 / \sqrt{n_{\text{features}}}\).

Here a small excerpt which illustrates how to use the sparse random projection transformer:

    >>> import numpy as np
    >>> from sklearn import random_projection
    >>> X = np.random.rand(100, 10000)
    >>> transformer = random_projection.SparseRandomProjection()
    >>> X_new = transformer.fit_transform(X)
    >>> X_new.shape
    (100, 3947)

**References**

  - D. Achlioptas. 2003. [Database-friendly random projections: Johnson-Lindenstrauss with binary coins](https://www.sciencedirect.com/science/article/pii/S0022000003000254). Journal of Computer and System Sciences 66 (2003) 671-687.
  - Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006. [Very sparse random projections.](https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf) In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '06). ACM, New York, NY, USA, 287-296.

## Inverse Transform

The random projection transformers have `compute_inverse_components` parameter. When set to True, after creating the random `components_` matrix during fitting, the transformer computes the pseudo-inverse of this matrix and stores it as `inverse_components_`. The `inverse_components_` matrix has shape \(n_{features} \times n_{components}\), and it is always a dense matrix, regardless of whether the components matrix is sparse or dense. So depending on the number of features and components, it may use a lot of memory.

When the `inverse_transform` method is called, it computes the product of the input `X` and the transpose of the inverse components. If the inverse components have been computed during fit, they are reused at each call to `inverse_transform`. Otherwise they are recomputed each time, which can be costly. The result is always dense, even if `X` is sparse.

Here a small code example which illustrates how to use the inverse transform feature:

    >>> import numpy as np
    >>> from sklearn.random_projection import SparseRandomProjection
    >>> X = np.random.rand(100, 10000)
    >>> transformer = SparseRandomProjection(
    ...   compute_inverse_components=True
    ... )
    ...
    >>> X_new = transformer.fit_transform(X)
    >>> X_new.shape
    (100, 3947)
    >>> X_new_inversed = transformer.inverse_transform(X_new)
    >>> X_new_inversed.shape
    (100, 10000)
    >>> X_new_again = transformer.transform(X_new_inversed)
    >>> np.allclose(X_new, X_new_again)
    True

---

semi_supervised.md

---

# Semi-supervised learning

<div class="currentmodule">

sklearn.semi\_supervised

</div>

[Semi-supervised learning](https://en.wikipedia.org/wiki/Semi-supervised_learning) is a situation in which in your training data some of the samples are not labeled. The semi-supervised estimators in `sklearn.semi_supervised` are able to make use of this additional unlabeled data to better capture the shape of the underlying data distribution and generalize better to new samples. These algorithms can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.

<div class="topic">

**Unlabeled entries in <span class="title-ref">y</span>**

It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the `fit` method. The identifier that this implementation uses is the integer value \(-1\). Note that for string labels, the dtype of <span class="title-ref">y</span> should be object so that it can contain both strings and integers.

</div>

\> **Note** \> Semi-supervised algorithms need to make assumptions about the distribution of the dataset in order to achieve performance gains. See [here](https://en.wikipedia.org/wiki/Semi-supervised_learning#Assumptions) for more details.

## Self Training

This self-training implementation is based on Yarowsky's\[1\] algorithm. Using this algorithm, a given supervised classifier can function as a semi-supervised classifier, allowing it to learn from unlabeled data.

<span class="title-ref">SelfTrainingClassifier</span> can be called with any classifier that implements <span class="title-ref">predict\_proba</span>, passed as the parameter <span class="title-ref">base\_classifier</span>. In each iteration, the <span class="title-ref">base\_classifier</span> predicts labels for the unlabeled samples and adds a subset of these labels to the labeled dataset.

The choice of this subset is determined by the selection criterion. This selection can be done using a <span class="title-ref">threshold</span> on the prediction probabilities, or by choosing the <span class="title-ref">k\_best</span> samples according to the prediction probabilities.

The labels used for the final fit as well as the iteration in which each sample was labeled are available as attributes. The optional <span class="title-ref">max\_iter</span> parameter specifies how many times the loop is executed at most.

The <span class="title-ref">max\_iter</span> parameter may be set to <span class="title-ref">None</span>, causing the algorithm to iterate until all samples have labels or no new samples are selected in that iteration.

\> **Note** \> When using the self-training classifier, the \[calibration \<calibration\>\](\#calibration-\<calibration\>) of the classifier is important.

**Examples**

  - \[sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_self\_training\_varying\_threshold.py\](\#sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_self\_training\_varying\_threshold.py)
  - \[sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_semi\_supervised\_versus\_svm\_iris.py\](\#sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_semi\_supervised\_versus\_svm\_iris.py)

**References**

## Label Propagation

Label propagation denotes a few variations of semi-supervised graph inference algorithms.

  - A few features available in this model:
    
      - Used for classification tasks
      - Kernel methods to project data into alternate dimensional spaces

<span class="title-ref">scikit-learn</span> provides two label propagation models: <span class="title-ref">LabelPropagation</span> and <span class="title-ref">LabelSpreading</span>. Both work by constructing a similarity graph over all items in the input dataset.

![**An illustration of label-propagation:** *the structure of unlabeled observations is consistent with the class structure, and thus the class label can be propagated to the unlabeled observations of the training set.*](../auto_examples/semi_supervised/images/sphx_glr_plot_label_propagation_structure_001.png)

<span class="title-ref">LabelPropagation</span> and <span class="title-ref">LabelSpreading</span> differ in modifications to the similarity matrix that graph and the clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground labeled data to some degree. The <span class="title-ref">LabelPropagation</span> algorithm performs hard clamping of input labels, which means \(\alpha=0\). This clamping factor can be relaxed, to say \(\alpha=0.2\), which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent.

<span class="title-ref">LabelPropagation</span> uses the raw similarity matrix constructed from the data with no modifications. In contrast, <span class="title-ref">LabelSpreading</span> minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in \[spectral\_clustering\](\#spectral\_clustering).

Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:

  - rbf (\(\exp(-\gamma |x-y|^2), \gamma > 0\)). \(\gamma\) is specified by keyword gamma.
  - knn (\(1[x' \in kNN(x)]\)). \(k\) is specified by keyword n\_neighbors.

The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.

**Examples**

  - \[sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_semi\_supervised\_versus\_svm\_iris.py\](\#sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_semi\_supervised\_versus\_svm\_iris.py)
  - \[sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_label\_propagation\_structure.py\](\#sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_label\_propagation\_structure.py)
  - \[sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_label\_propagation\_digits.py\](\#sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_label\_propagation\_digits.py)
  - \[sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_label\_propagation\_digits\_active\_learning.py\](\#sphx\_glr\_auto\_examples\_semi\_supervised\_plot\_label\_propagation\_digits\_active\_learning.py)

**References**

\[2\] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216

\[3\] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 <https://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf>

1.  `"Unsupervised word sense disambiguation rivaling supervised methods"
    <10.3115/981658.981684>` David Yarowsky, Proceedings of the 33rd annual meeting on Association for Computational Linguistics (ACL '95). Association for Computational Linguistics, Stroudsburg, PA, USA, 189-196.

---

sgd.md

---

# Stochastic Gradient Descent

<div class="currentmodule">

sklearn.linear\_model

</div>

**Stochastic Gradient Descent (SGD)** is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) and [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression). Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.

SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.

Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a *way* to train a model. Often, an instance of <span class="title-ref">SGDClassifier</span> or <span class="title-ref">SGDRegressor</span> will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. For example, using <span class="title-ref">SGDClassifier(loss='log\_loss')</span> results in logistic regression, i.e. a model equivalent to <span class="title-ref">\~sklearn.linear\_model.LogisticRegression</span> which is fitted via SGD instead of being fitted by one of the other solvers in <span class="title-ref">\~sklearn.linear\_model.LogisticRegression</span>. Similarly, <span class="title-ref">SGDRegressor(loss='squared\_error', penalty='l2')</span> and <span class="title-ref">\~sklearn.linear\_model.Ridge</span> solve the same optimization problem, via different means.

The advantages of Stochastic Gradient Descent are:

  - Efficiency.
  - Ease of implementation (lots of opportunities for code tuning).

The disadvantages of Stochastic Gradient Descent include:

  - SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.
  - SGD is sensitive to feature scaling.

\> **Warning** \> Make sure you permute (shuffle) your training data before fitting the model or use `shuffle=True` to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. <span class="title-ref">make\_pipeline(StandardScaler(), SGDClassifier())</span> (see \[Pipelines \<combining\_estimators\>\](\#pipelines --\<combining\_estimators\>)).

## Classification

The class <span class="title-ref">SGDClassifier</span> implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a <span class="title-ref">SGDClassifier</span> trained with the hinge loss, equivalent to a linear SVM.

![](../auto_examples/linear_model/images/sphx_glr_plot_sgd_separating_hyperplane_001.png)

As other classifiers, SGD has to be fitted with two arrays: an array <span class="title-ref">X</span> of shape (n\_samples, n\_features) holding the training samples, and an array y of shape (n\_samples,) holding the target values (class labels) for the training samples:

    >>> from sklearn.linear_model import SGDClassifier
    >>> X = [[0., 0.], [1., 1.]]
    >>> y = [0, 1]
    >>> clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
    >>> clf.fit(X, y)
    SGDClassifier(max_iter=5)

After being fitted, the model can then be used to predict new values:

    >>> clf.predict([[2., 2.]])
    array([1])

SGD fits a linear model to the training data. The `coef_` attribute holds the model parameters:

    >>> clf.coef_
    array([[9.9..., 9.9...]])

The `intercept_` attribute holds the intercept (aka offset or bias):

    >>> clf.intercept_
    array([-9.9...])

Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter `fit_intercept`.

The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by \`SGDClassifier.decision\_function\`:

    >>> clf.decision_function([[2., 2.]])
    array([29.6...])

The concrete loss function can be set via the `loss` parameter. <span class="title-ref">SGDClassifier</span> supports the following loss functions:

  - `loss="hinge"`: (soft-margin) linear Support Vector Machine,
  - `loss="modified_huber"`: smoothed hinge loss,
  - `loss="log_loss"`: logistic regression,
  - and all regression losses below. In this case the target is encoded as -1 or 1, and the problem is treated as a regression problem. The predicted class then correspond to the sign of the predicted target.

Please refer to the \[mathematical section below \<sgd\_mathematical\_formulation\>\](\#mathematical-section-below \<sgd\_mathematical\_formulation\>) for formulas. The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models (i.e. with more zero coefficients), even when L2 penalty is used.

Using `loss="log_loss"` or `loss="modified_huber"` enables the `predict_proba` method, which gives a vector of probability estimates \(P(y|x)\) per sample \(x\):

    >>> clf = SGDClassifier(loss="log_loss", max_iter=5).fit(X, y)
    >>> clf.predict_proba([[1., 1.]]) # doctest: +SKIP
    array([[0.00..., 0.99...]])

The concrete penalty can be set via the `penalty` parameter. SGD supports the following penalties:

  - `penalty="l2"`: L2 norm penalty on `coef_`.
  - `penalty="l1"`: L1 norm penalty on `coef_`.
  - `penalty="elasticnet"`: Convex combination of L2 and L1; `(1 - l1_ratio) * L2 + l1_ratio * L1`.

The default setting is `penalty="l2"`. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net\[1\] solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter `l1_ratio` controls the convex combination of L1 and L2 penalty.

<span class="title-ref">SGDClassifier</span> supports multi-class classification by combining multiple binary classifiers in a "one versus all" (OVA) scheme. For each of the \(K\) classes, a binary classifier is learned that discriminates between that and all other \(K-1\) classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers.

![](../auto_examples/linear_model/images/sphx_glr_plot_sgd_iris_001.png)

In the case of multi-class classification `coef_` is a two-dimensional array of shape (n\_classes, n\_features) and `intercept_` is a one-dimensional array of shape (n\_classes,). The i-th row of `coef_` holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute `classes_`). Note that, in principle, since they allow to create a probability model, `loss="log_loss"` and `loss="modified_huber"` are more suitable for one-vs-all classification.

<span class="title-ref">SGDClassifier</span> supports both weighted classes and weighted instances via the fit parameters `class_weight` and `sample_weight`. See the examples below and the docstring of <span class="title-ref">SGDClassifier.fit</span> for further information.

<span class="title-ref">SGDClassifier</span> supports averaged SGD (ASGD)\[2\]. Averaging can be enabled by setting <span class="title-ref">average=True</span>. ASGD performs the same updates as the regular SGD (see \[sgd\_mathematical\_formulation\](\#sgd\_mathematical\_formulation)), but instead of using the last value of the coefficients as the <span class="title-ref">coef\_</span> attribute (i.e. the values of the last update), <span class="title-ref">coef\_</span> is set instead to the **average** value of the coefficients across all updates. The same is done for the <span class="title-ref">intercept\_</span> attribute. When using ASGD the learning rate can be larger and even constant, leading on some datasets to a speed up in training time.

For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in <span class="title-ref">LogisticRegression</span>.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_separating\_hyperplane.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_separating\_hyperplane.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_iris.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_iris.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_weighted\_samples.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_weighted\_samples.py)
  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_comparison.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_comparison.py)
  - \[sphx\_glr\_auto\_examples\_svm\_plot\_separating\_hyperplane\_unbalanced.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_separating\_hyperplane\_unbalanced.py) (See the Note in the example)

## Regression

The class <span class="title-ref">SGDRegressor</span> implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. <span class="title-ref">SGDRegressor</span> is well suited for regression problems with a large number of training samples (\> 10.000), for other problems we recommend <span class="title-ref">Ridge</span>, <span class="title-ref">Lasso</span>, or <span class="title-ref">ElasticNet</span>.

The concrete loss function can be set via the `loss` parameter. <span class="title-ref">SGDRegressor</span> supports the following loss functions:

  - `loss="squared_error"`: Ordinary least squares,
  - `loss="huber"`: Huber loss for robust regression,
  - `loss="epsilon_insensitive"`: linear Support Vector Regression.

Please refer to the \[mathematical section below \<sgd\_mathematical\_formulation\>\](\#mathematical-section-below \<sgd\_mathematical\_formulation\>) for formulas. The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter `epsilon`. This parameter depends on the scale of the target variables.

The <span class="title-ref">penalty</span> parameter determines the regularization to be used (see description above in the classification section).

<span class="title-ref">SGDRegressor</span> also supports averaged SGD\[3\] (here again, see description above in the classification section).

For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in <span class="title-ref">Ridge</span>.

## Online One-Class SVM

The class <span class="title-ref">sklearn.linear\_model.SGDOneClassSVM</span> implements an online linear version of the One-Class SVM using a stochastic gradient descent. Combined with kernel approximation techniques, <span class="title-ref">sklearn.linear\_model.SGDOneClassSVM</span> can be used to approximate the solution of a kernelized One-Class SVM, implemented in <span class="title-ref">sklearn.svm.OneClassSVM</span>, with a linear complexity in the number of samples. Note that the complexity of a kernelized One-Class SVM is at best quadratic in the number of samples. <span class="title-ref">sklearn.linear\_model.SGDOneClassSVM</span> is thus well suited for datasets with a large number of training samples (\> 10,000) for which the SGD variant can be several orders of magnitude faster.

<div class="dropdown">

Mathematical details

Its implementation is based on the implementation of the stochastic gradient descent. Indeed, the original optimization problem of the One-Class SVM is given by

\[\begin{aligned}
\begin{aligned}
\min_{w, \rho, \xi} & \quad \frac{1}{2}\Vert w \Vert^2 - \rho + \frac{1}{\nu n} \sum_{i=1}^n \xi_i \\
\text{s.t.} & \quad \langle w, x_i \rangle \geq \rho - \xi_i \quad 1 \leq i \leq n \\
& \quad \xi_i \geq 0 \quad 1 \leq i \leq n
\end{aligned}
\end{aligned}\]

where \(\nu \in (0, 1]\) is the user-specified parameter controlling the proportion of outliers and the proportion of support vectors. Getting rid of the slack variables \(\xi_i\) this problem is equivalent to

\[\min_{w, \rho} \frac{1}{2}\Vert w \Vert^2 - \rho + \frac{1}{\nu n} \sum_{i=1}^n \max(0, \rho - \langle w, x_i \rangle) \, .\]

Multiplying by the constant \(\nu\) and introducing the intercept \(b = 1 - \rho\) we obtain the following equivalent optimization problem

\[\min_{w, b} \frac{\nu}{2}\Vert w \Vert^2 + b\nu + \frac{1}{n} \sum_{i=1}^n \max(0, 1 - (\langle w, x_i \rangle + b)) \, .\]

This is similar to the optimization problems studied in section \[sgd\_mathematical\_formulation\](\#sgd\_mathematical\_formulation) with \(y_i = 1, 1 \leq i \leq n\) and \(\alpha = \nu/2\), \(L\) being the hinge loss function and \(R\) being the L2 norm. We just need to add the term \(b\nu\) in the optimization loop.

</div>

As <span class="title-ref">SGDClassifier</span> and <span class="title-ref">SGDRegressor</span>, <span class="title-ref">SGDOneClassSVM</span> supports averaged SGD. Averaging can be enabled by setting `average=True`.

**Examples**

  - \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgdocsvm\_vs\_ocsvm.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgdocsvm\_vs\_ocsvm.py)

## Stochastic Gradient Descent for sparse data

<div class="note">

<div class="title">

Note

</div>

The sparse implementation produces slightly different results from the dense implementation, due to a shrunk learning rate for the intercept. See \[implementation\_details\](\#implementation\_details).

</div>

There is built-in support for sparse data given in any matrix in a format supported by [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html). For maximum efficiency, however, use the CSR matrix format as defined in [scipy.sparse.csr\_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).

**Examples**

  - \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py)

## Complexity

The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.

Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases.

## Stopping criterion

The classes <span class="title-ref">SGDClassifier</span> and <span class="title-ref">SGDRegressor</span> provide two criteria to stop the algorithm when a given level of convergence is reached:

  - With `early_stopping=True`, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using the <span class="title-ref">score</span> method) computed on the validation set. The size of the validation set can be changed with the parameter `validation_fraction`.
  - With `early_stopping=False`, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data.

In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve `n_iter_no_change` times in a row. The improvement is evaluated with absolute tolerance `tol`, and the algorithm stops in any case after a maximum number of iteration `max_iter`.

See \[sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_early\_stopping.py\](\#sphx\_glr\_auto\_examples\_linear\_model\_plot\_sgd\_early\_stopping.py) for an example of the effects of early stopping.

## Tips on Practical Use

  - Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to \[0,1\] or \[-1,+1\], or standardize it to have mean 0 and variance 1. Note that the *same* scaling must be applied to the test vector to obtain meaningful results. This can be easily done using \`\~sklearn.preprocessing.StandardScaler\`:
    
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        scaler.fit(X_train)  # Don't cheat - fit only on training data
        X_train = scaler.transform(X_train)
        X_test = scaler.transform(X_test)  # apply same transformation to test data
        
        # Or better yet: use a pipeline!
        from sklearn.pipeline import make_pipeline
        est = make_pipeline(StandardScaler(), SGDClassifier())
        est.fit(X_train)
        est.predict(X_test)
    
    If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.

  - Finding a reasonable regularization term \(\alpha\) is best done using automatic hyper-parameter search, e.g. <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> or <span class="title-ref">\~sklearn.model\_selection.RandomizedSearchCV</span>, usually in the range `10.0**-np.arange(1,7)`.

  - Empirically, we found that SGD converges after observing approximately 10^6 training samples. Thus, a reasonable first guess for the number of iterations is `max_iter = np.ceil(10**6 / n)`, where `n` is the size of the training set.

  - If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant <span class="title-ref">c</span> such that the average L2 norm of the training data equals one.

  - We found that Averaged SGD works best with a larger number of features and a higher eta0.

**References**

  - ["Efficient BackProp"](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
    Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.

## Mathematical formulation

We describe here the mathematical details of the SGD procedure. A good overview with convergence rates can be found in\[4\].

Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \mathcal{R}\) (\(y_i \in
{-1, 1}\) for classification), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions for binary classification, we simply look at the sign of \(f(x)\). To find the model parameters, we minimize the regularized training error given by

\[E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)\]

where \(L\) is a loss function that measures model (mis)fit and \(R\) is a regularization term (aka penalty) that penalizes model complexity; \(\alpha > 0\) is a non-negative hyperparameter that controls the regularization strength.

<div class="dropdown">

Loss functions details

Different choices for \(L\) entail different classifiers or regressors:

  - Hinge (soft-margin): equivalent to Support Vector Classification. \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\).
  - Perceptron: \(L(y_i, f(x_i)) = \max(0, - y_i f(x_i))\).
  - Modified Huber: \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\) if \(y_i f(x_i) >
    -1\), and \(L(y_i, f(x_i)) = -4 y_i f(x_i)\) otherwise.
  - Log Loss: equivalent to Logistic Regression. \(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\).
  - Squared Error: Linear regression (Ridge or Lasso depending on \(R\)). \(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\).
  - Huber: less sensitive to outliers than least-squares. It is equivalent to least squares when \(|y_i - f(x_i)| \leq \varepsilon\), and \(L(y_i, f(x_i)) = \varepsilon |y_i - f(x_i)| - \frac{1}{2}
    \varepsilon^2\) otherwise.
  - Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression. \(L(y_i, f(x_i)) = \max(0, |y_i - f(x_i)| - \varepsilon)\).

</div>

All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.

![](../auto_examples/linear_model/images/sphx_glr_plot_sgd_loss_functions_001.png)

Popular choices for the regularization term \(R\) (the <span class="title-ref">penalty</span> parameter) include:

  - L2 norm: \(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\),
  - L1 norm: \(R(w) := \sum_{j=1}^{m} |w_j|\), which leads to sparse solutions.
  - Elastic Net: \(R(w) := \frac{\rho}{2} \sum_{j=1}^{n} w_j^2 +
    (1-\rho) \sum_{j=1}^{m} |w_j|\), a convex combination of L2 and L1, where \(\rho\) is given by `1 - l1_ratio`.

The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).

![](../auto_examples/linear_model/images/sphx_glr_plot_sgd_penalties_001.png)

### SGD

Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of \(E(w,b)\) by considering a single training example at a time.

The class <span class="title-ref">SGDClassifier</span> implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by

\[w \leftarrow w - \eta \left[\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w}\right]\]

where \(\eta\) is the learning rate which controls the step-size in the parameter space. The intercept \(b\) is updated similarly but without regularization (and with additional decay for sparse matrices, as detailed in \[implementation\_details\](\#implementation\_details)).

The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (`learning_rate='optimal'`) is given by

\[\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}\]

where \(t\) is the time step (there are a total of <span class="title-ref">n\_samples \* n\_iter</span> time steps), \(t_0\) is determined based on a heuristic proposed by Léon Bottou such that the expected initial updates are comparable with the expected size of the weights (this assuming that the norm of the training samples is approx. 1). The exact definition can be found in `_init_t` in <span class="title-ref">BaseSGD</span>.

For regression the default learning rate schedule is inverse scaling (`learning_rate='invscaling'`), given by

\[\eta^{(t)} = \frac{eta_0}{t^{power\_t}}\]

where \(eta_0\) and \(power\_t\) are hyperparameters chosen by the user via `eta0` and `power_t`, resp.

For a constant learning rate use `learning_rate='constant'` and use `eta0` to specify the learning rate.

For an adaptively decreasing learning rate, use `learning_rate='adaptive'` and use `eta0` to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.

The model parameters can be accessed through the `coef_` and `intercept_` attributes: `coef_` holds the weights \(w\) and `intercept_` holds \(b\).

When using Averaged SGD (with the <span class="title-ref">average</span> parameter), <span class="title-ref">coef\_</span> is set to the average weight across all updates: <span class="title-ref">coef\_</span> \(= \frac{1}{T} \sum_{t=0}^{T-1} w^{(t)}\), where \(T\) is the total number of updates, found in the <span class="title-ref">t\_</span> attribute.

## Implementation details

The implementation of SGD is influenced by the <span class="title-ref">Stochastic Gradient SVM</span> of \[5\]. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input <span class="title-ref">X</span>, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from\[6\]. For multi-class classification, a "one versus all" approach is used. We use the truncated gradient algorithm proposed in\[7\] for L1 regularization (and the Elastic Net). The code is written in Cython.

**References**

1.  `"Regularization and variable selection via the elastic net"
    <10.1111/j.1467-9868.2005.00503.x>` H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320.

2.  `"Towards Optimal One Pass Large Scale Learning with
    Averaged Stochastic Gradient Descent"
    <1107.2490v2>`. Xu, Wei (2011)

3.  `"Towards Optimal One Pass Large Scale Learning with
    Averaged Stochastic Gradient Descent"
    <1107.2490v2>`. Xu, Wei (2011)

4.  `"Solving large scale linear prediction problems using stochastic
    gradient descent algorithms" <10.1145/1015330.1015332>` T. Zhang - In Proceedings of ICML '04.

5.  ["Stochastic Gradient Descent"](https://leon.bottou.org/projects/sgd) L. Bottou - Website, 2010.

6.  `"Pegasos: Primal estimated sub-gradient solver for svm"
    <10.1145/1273496.1273598>` S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07.

7.  ["Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty"](https://www.aclweb.org/anthology/P/P09/P09-1054.pdf) Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL'09.

---

svm.md

---

# Support Vector Machines

<div class="currentmodule">

sklearn.svm

</div>

**Support vector machines (SVMs)** are a set of supervised learning methods used for \[classification \<svm\_classification\>\](\#classification-\<svm\_classification\>), \[regression \<svm\_regression\>\](\#regression-\<svm\_regression\>) and \[outliers detection \<svm\_outlier\_detection\>\](\#outliers-detection \<svm\_outlier\_detection\>).

The advantages of support vector machines are:

  - Effective in high dimensional spaces.
  - Still effective in cases where number of dimensions is greater than the number of samples.
  - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
  - Versatile: different \[svm\_kernels\](\#svm\_kernels) can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.

The disadvantages of support vector machines include:

  - If the number of features is much greater than the number of samples, avoid over-fitting in choosing \[svm\_kernels\](\#svm\_kernels) and regularization term is crucial.
  - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see \[Scores and probabilities \<scores\_probabilities\>\](\#scores-and-probabilities-\<scores\_probabilities\>), below).

The support vector machines in scikit-learn support both dense (`numpy.ndarray` and convertible to that by `numpy.asarray`) and sparse (any `scipy.sparse`) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered `numpy.ndarray` (dense) or `scipy.sparse.csr_matrix` (sparse) with `dtype=float64`.

## Classification

<span class="title-ref">SVC</span>, <span class="title-ref">NuSVC</span> and <span class="title-ref">LinearSVC</span> are classes capable of performing binary and multi-class classification on a dataset.

![](../auto_examples/svm/images/sphx_glr_plot_iris_svc_001.png)

<span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span> are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section \[svm\_mathematical\_formulation\](\#svm\_mathematical\_formulation)). On the other hand, <span class="title-ref">LinearSVC</span> is another (faster) implementation of Support Vector Classification for the case of a linear kernel. It also lacks some of the attributes of <span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span>, like <span class="title-ref">support\_</span>. <span class="title-ref">LinearSVC</span> uses <span class="title-ref">squared\_hinge</span> loss and due to its implementation in <span class="title-ref">liblinear</span> it also regularizes the intercept, if considered. This effect can however be reduced by carefully fine tuning its <span class="title-ref">intercept\_scaling</span> parameter, which allows the intercept term to have a different regularization behavior compared to the other features. The classification results and score can therefore differ from the other two classifiers.

As other classifiers, <span class="title-ref">SVC</span>, <span class="title-ref">NuSVC</span> and <span class="title-ref">LinearSVC</span> take as input two arrays: an array <span class="title-ref">X</span> of shape <span class="title-ref">(n\_samples, n\_features)</span> holding the training samples, and an array <span class="title-ref">y</span> of class labels (strings or integers), of shape \`(n\_samples)\`:

    >>> from sklearn import svm
    >>> X = [[0, 0], [1, 1]]
    >>> y = [0, 1]
    >>> clf = svm.SVC()
    >>> clf.fit(X, y)
    SVC()

After being fitted, the model can then be used to predict new values:

    >>> clf.predict([[2., 2.]])
    array([1])

SVMs decision function (detailed in the \[svm\_mathematical\_formulation\](\#svm\_mathematical\_formulation)) depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in attributes `support_vectors_`, `support_` and `n_support_`:

    >>> # get support vectors
    >>> clf.support_vectors_
    array([[0., 0.],
           [1., 1.]])
    >>> # get indices of support vectors
    >>> clf.support_
    array([0, 1]...)
    >>> # get number of support vectors for each class
    >>> clf.n_support_
    array([1, 1]...)

**Examples**

  - \[sphx\_glr\_auto\_examples\_svm\_plot\_separating\_hyperplane.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_separating\_hyperplane.py)
  - \[sphx\_glr\_auto\_examples\_svm\_plot\_svm\_anova.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_svm\_anova.py)
  - \[sphx\_glr\_auto\_examples\_classification\_plot\_classification\_probability.py\](\#sphx\_glr\_auto\_examples\_classification\_plot\_classification\_probability.py)

### Multi-class classification

<span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span> implement the "one-versus-one" approach for multi-class classification. In total, `n_classes * (n_classes - 1) / 2` classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the `decision_function_shape` option allows to monotonically transform the results of the "one-versus-one" classifiers to a "one-vs-rest" decision function of shape `(n_samples, n_classes)`, which is the default setting of the parameter (default='ovr').

> \>\>\> X = \[\[0\], \[1\], \[2\], \[3\]\] \>\>\> Y = \[0, 1, 2, 3\] \>\>\> clf = svm.SVC(decision\_function\_shape='ovo') \>\>\> clf.fit(X, Y) SVC(decision\_function\_shape='ovo') \>\>\> dec = clf.decision\_function(\[\[1\]\]) \>\>\> dec.shape\[1\] \# 6 classes: 4\*3/2 = 6 6 \>\>\> clf.decision\_function\_shape = "ovr" \>\>\> dec = clf.decision\_function(\[\[1\]\]) \>\>\> dec.shape\[1\] \# 4 classes 4

On the other hand, <span class="title-ref">LinearSVC</span> implements "one-vs-the-rest" multi-class strategy, thus training <span class="title-ref">n\_classes</span> models.

> \>\>\> lin\_clf = svm.LinearSVC() \>\>\> lin\_clf.fit(X, Y) LinearSVC() \>\>\> dec = lin\_clf.decision\_function(\[\[1\]\]) \>\>\> dec.shape\[1\] 4

See \[svm\_mathematical\_formulation\](\#svm\_mathematical\_formulation) for a complete description of the decision function.

<div class="dropdown">

Details on multi-class strategies

Note that the <span class="title-ref">LinearSVC</span> also implements an alternative multi-class strategy, the so-called multi-class SVM formulated by Crammer and Singer \[1\], by using the option `multi_class='crammer_singer'`. In practice, one-vs-rest classification is usually preferred, since the results are mostly similar, but the runtime is significantly less.

For "one-vs-rest" <span class="title-ref">LinearSVC</span> the attributes `coef_` and `intercept_` have the shape `(n_classes, n_features)` and `(n_classes,)` respectively. Each row of the coefficients corresponds to one of the `n_classes` "one-vs-rest" classifiers and similar for the intercepts, in the order of the "one" class.

In the case of "one-vs-one" <span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span>, the layout of the attributes is a little more involved. In the case of a linear kernel, the attributes `coef_` and `intercept_` have the shape `(n_classes * (n_classes - 1) / 2, n_features)` and `(n_classes * (n_classes - 1) / 2)` respectively. This is similar to the layout for <span class="title-ref">LinearSVC</span> described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is "0 vs 1", "0 vs 2" , ... "0 vs n", "1 vs 2", "1 vs 3", "1 vs n", . . . "n-1 vs n".

The shape of `dual_coef_` is `(n_classes-1, n_SV)` with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the `n_classes * (n_classes - 1) / 2` "one-vs-one" classifiers. Each support vector `v` has a dual coefficient in each of the `n_classes - 1` classifiers comparing the class of `v` against another class. Note that some, but not all, of these dual coefficients, may be zero. The `n_classes - 1` entries in each column are these dual coefficients, ordered by the opposing class.

This might be clearer with an example: consider a three class problem with class 0 having three support vectors \(v^{0}_0, v^{1}_0, v^{2}_0\) and class 1 and 2 having two support vectors \(v^{0}_1, v^{1}_1\) and \(v^{0}_2, v^{1}_2\) respectively. For each support vector \(v^{j}_i\), there are two dual coefficients. Let's call the coefficient of support vector \(v^{j}_i\) in the classifier between classes \(i\) and \(k\) \(\alpha^{j}_{i,k}\). Then `dual_coef_` looks like this:

|                                 |                      |                      |                                 |                      |                                 |                      |
| ------------------------------- | -------------------- | -------------------- | ------------------------------- | -------------------- | ------------------------------- | -------------------- |
| \(\alpha^{0}_{0,1}\)            | \(\alpha^{1}_{0,1}\) | \(\alpha^{2}_{0,1}\) | \(\alpha^{0}_{1,0}\)            | \(\alpha^{1}_{1,0}\) | \(\alpha^{0}_{2,0}\)            | \(\alpha^{1}_{2,0}\) |
| \(\alpha^{0}_{0,2}\)            | \(\alpha^{1}_{0,2}\) | \(\alpha^{2}_{0,2}\) | \(\alpha^{0}_{1,2}\)            | \(\alpha^{1}_{1,2}\) | \(\alpha^{0}_{2,1}\)            | \(\alpha^{1}_{2,1}\) |
| Coefficients for SVs of class 0 |                      |                      | Coefficients for SVs of class 1 |                      | Coefficients for SVs of class 2 |                      |

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_svm\_plot\_iris\_svc.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_iris\_svc.py)

### Scores and probabilities

The `decision_function` method of <span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span> gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option `probability` is set to `True`, class membership probability estimates (from the methods `predict_proba` and `predict_log_proba`) are enabled. In the binary case, the probabilities are calibrated using Platt scaling\[2\]: logistic regression on the SVM's scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per\[3\].

\> **Note** \> The same probability calibration procedure is available for all estimators via the <span class="title-ref">\~sklearn.calibration.CalibratedClassifierCV</span> (see \[calibration\](\#calibration)). In the case of <span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span>, this procedure is builtin in [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) which is used under the hood, so it does not rely on scikit-learn's <span class="title-ref">\~sklearn.calibration.CalibratedClassifierCV</span>.

The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:

  - the "argmax" of the scores may not be the argmax of the probabilities
  - in binary classification, a sample may be labeled by `predict` as belonging to the positive class even if the output of <span class="title-ref">predict\_proba</span> is less than 0.5; and similarly, it could be labeled as negative even if the output of <span class="title-ref">predict\_proba</span> is more than 0.5.

Platt's method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set `probability=False` and use `decision_function` instead of `predict_proba`.

Please note that when `decision_function_shape='ovr'` and `n_classes > 2`, unlike `decision_function`, the `predict` method does not try to break ties by default. You can set `break_ties=True` for the output of `predict` to be the same as `np.argmax(clf.decision_function(...), axis=1)`, otherwise the first class among the tied classes will always be returned; but have in mind that it comes with a computational cost. See \[sphx\_glr\_auto\_examples\_svm\_plot\_svm\_tie\_breaking.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_svm\_tie\_breaking.py) for an example on tie breaking.

### Unbalanced problems

In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters `class_weight` and `sample_weight` can be used.

<span class="title-ref">SVC</span> (but not <span class="title-ref">NuSVC</span>) implements the parameter `class_weight` in the `fit` method. It's a dictionary of the form `{class_label : value}`, where value is a floating point number \> 0 that sets the parameter `C` of class `class_label` to `C * value`. The figure below illustrates the decision boundary of an unbalanced problem, with and without weight correction.

![](../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png)

<span class="title-ref">SVC</span>, <span class="title-ref">NuSVC</span>, <span class="title-ref">SVR</span>, <span class="title-ref">NuSVR</span>, <span class="title-ref">LinearSVC</span>, <span class="title-ref">LinearSVR</span> and <span class="title-ref">OneClassSVM</span> implement also weights for individual samples in the <span class="title-ref">fit</span> method through the `sample_weight` parameter. Similar to `class_weight`, this sets the parameter `C` for the i-th example to `C * sample_weight[i]`, which will encourage the classifier to get these samples right. The figure below illustrates the effect of sample weighting on the decision boundary. The size of the circles is proportional to the sample weights:

![](../auto_examples/svm/images/sphx_glr_plot_weighted_samples_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_svm\_plot\_separating\_hyperplane\_unbalanced.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_separating\_hyperplane\_unbalanced.py)
  - \[sphx\_glr\_auto\_examples\_svm\_plot\_weighted\_samples.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_weighted\_samples.py)

## Regression

The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.

The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.

There are three different implementations of Support Vector Regression: <span class="title-ref">SVR</span>, <span class="title-ref">NuSVR</span> and <span class="title-ref">LinearSVR</span>. <span class="title-ref">LinearSVR</span> provides a faster implementation than <span class="title-ref">SVR</span> but only considers the linear kernel, while <span class="title-ref">NuSVR</span> implements a slightly different formulation than <span class="title-ref">SVR</span> and <span class="title-ref">LinearSVR</span>. Due to its implementation in <span class="title-ref">liblinear</span> <span class="title-ref">LinearSVR</span> also regularizes the intercept, if considered. This effect can however be reduced by carefully fine tuning its <span class="title-ref">intercept\_scaling</span> parameter, which allows the intercept term to have a different regularization behavior compared to the other features. The classification results and score can therefore differ from the other two classifiers. See \[svm\_implementation\_details\](\#svm\_implementation\_details) for further details.

As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:

    >>> from sklearn import svm
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> regr = svm.SVR()
    >>> regr.fit(X, y)
    SVR()
    >>> regr.predict([[1, 1]])
    array([1.5])

**Examples**

  - \[sphx\_glr\_auto\_examples\_svm\_plot\_svm\_regression.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_svm\_regression.py)

## Density estimation, novelty detection

The class <span class="title-ref">OneClassSVM</span> implements a One-Class SVM which is used in outlier detection.

See \[outlier\_detection\](\#outlier\_detection) for the description and usage of OneClassSVM.

## Complexity

Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)-based implementation scales between \(O(n_{features} \times n_{samples}^2)\) and \(O(n_{features} \times n_{samples}^3)\) depending on how efficiently the [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) cache is used in practice (dataset dependent). If the data is very sparse \(n_{features}\) should be replaced by the average number of non-zero features in a sample vector.

For the linear case, the algorithm used in <span class="title-ref">LinearSVC</span> by the [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) implementation is much more efficient than its [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)-based <span class="title-ref">SVC</span> counterpart and can scale almost linearly to millions of samples and/or features.

## Tips on Practical Use

  - **Avoiding data copy**: For <span class="title-ref">SVC</span>, <span class="title-ref">SVR</span>, <span class="title-ref">NuSVC</span> and <span class="title-ref">NuSVR</span>, if the data passed to certain methods is not C-ordered contiguous and double precision, it will be copied before calling the underlying C implementation. You can check whether a given numpy array is C-contiguous by inspecting its `flags` attribute.
    
    For <span class="title-ref">LinearSVC</span> (and <span class="title-ref">LogisticRegression \<sklearn.linear\_model.LogisticRegression\></span>) any input passed as a numpy array will be copied and converted to the [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the <span class="title-ref">SGDClassifier \<sklearn.linear\_model.SGDClassifier\></span> class instead. The objective function can be configured to be almost the same as the <span class="title-ref">LinearSVC</span> model.

  - **Kernel cache size**: For <span class="title-ref">SVC</span>, <span class="title-ref">SVR</span>, <span class="title-ref">NuSVC</span> and <span class="title-ref">NuSVR</span>, the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set `cache_size` to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB).

  - **Setting C**: `C` is `1` by default and it's a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization.
    
    <span class="title-ref">LinearSVC</span> and <span class="title-ref">LinearSVR</span> are less sensitive to `C` when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger `C` values will take more time to train, sometimes up to 10 times longer, as shown in\[4\].

  - Support Vector Machine algorithms are not scale invariant, so **it is highly recommended to scale your data**. For example, scale each attribute on the input vector X to \[0,1\] or \[-1,+1\], or standardize it to have mean 0 and variance 1. Note that the *same* scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a \`\~sklearn.pipeline.Pipeline\`:
    
        >>> from sklearn.pipeline import make_pipeline
        >>> from sklearn.preprocessing import StandardScaler
        >>> from sklearn.svm import SVC
        
        >>> clf = make_pipeline(StandardScaler(), SVC())
    
    See section \[preprocessing\](\#preprocessing) for more details on scaling and normalization.

<div id="shrinking_svm">

  - Regarding the <span class="title-ref">shrinking</span> parameter, quoting\[5\]: *We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance), the code without using shrinking may be much faster*

  - Parameter `nu` in <span class="title-ref">NuSVC</span>/<span class="title-ref">OneClassSVM</span>/<span class="title-ref">NuSVR</span> approximates the fraction of training errors and support vectors.

  - In <span class="title-ref">SVC</span>, if the data is unbalanced (e.g. many positive and few negative), set `class_weight='balanced'` and/or try different penalty parameters `C`.

  - **Randomness of the underlying implementations**: The underlying implementations of <span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span> use a random number generator only to shuffle the data for probability estimation (when `probability` is set to `True`). This randomness can be controlled with the `random_state` parameter. If `probability` is set to `False` these estimators are not random and `random_state` has no effect on the results. The underlying <span class="title-ref">OneClassSVM</span> implementation is similar to the ones of <span class="title-ref">SVC</span> and <span class="title-ref">NuSVC</span>. As no probability estimation is provided for <span class="title-ref">OneClassSVM</span>, it is not random.
    
    The underlying <span class="title-ref">LinearSVC</span> implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e. when `dual` is set to `True`). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller <span class="title-ref">tol</span> parameter. This randomness can also be controlled with the `random_state` parameter. When `dual` is set to `False` the underlying implementation of <span class="title-ref">LinearSVC</span> is not random and `random_state` has no effect on the results.

  - Using L1 penalization as provided by `LinearSVC(penalty='l1', dual=False)` yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing `C` yields a more complex model (more features are selected). The `C` value that yields a "null" model (all weights equal to zero) can be calculated using <span class="title-ref">l1\_min\_c</span>.

</div>

## Kernel functions

The *kernel function* can be any of the following:

  - linear: \(\langle x, x'\rangle\).
  - polynomial: \((\gamma \langle x, x'\rangle + r)^d\), where \(d\) is specified by parameter `degree`, \(r\) by `coef0`.
  - rbf: \(\exp(-\gamma \|x-x'\|^2)\), where \(\gamma\) is specified by parameter `gamma`, must be greater than 0.
  - sigmoid \(\tanh(\gamma \langle x,x'\rangle + r)\), where \(r\) is specified by `coef0`.

Different kernels are specified by the <span class="title-ref">kernel</span> parameter:

    >>> linear_svc = svm.SVC(kernel='linear')
    >>> linear_svc.kernel
    'linear'
    >>> rbf_svc = svm.SVC(kernel='rbf')
    >>> rbf_svc.kernel
    'rbf'

See also \[kernel\_approximation\](\#kernel\_approximation) for a solution to use RBF kernels that is much faster and more scalable.

### Parameters of the RBF Kernel

When training an SVM with the *Radial Basis Function* (RBF) kernel, two parameters must be considered: `C` and `gamma`. The parameter `C`, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low `C` makes the decision surface smooth, while a high `C` aims at classifying all training examples correctly. `gamma` defines how much influence a single training example has. The larger `gamma` is, the closer other examples must be to be affected.

Proper choice of `C` and `gamma` is critical to the SVM's performance. One is advised to use <span class="title-ref">\~sklearn.model\_selection.GridSearchCV</span> with `C` and `gamma` spaced exponentially far apart to choose good values.

**Examples**

  - \[sphx\_glr\_auto\_examples\_svm\_plot\_rbf\_parameters.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_rbf\_parameters.py)
  - \[sphx\_glr\_auto\_examples\_svm\_plot\_svm\_scale\_c.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_svm\_scale\_c.py)

### Custom Kernels

You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix.

Classifiers with custom kernels behave the same way as any other classifiers, except that:

  - Field `support_vectors_` is now empty, only indices of support vectors are stored in `support_`
  - A reference (and not a copy) of the first argument in the `fit()` method is stored for future reference. If that array changes between the use of `fit()` and `predict()` you will have unexpected results.

<div class="dropdown">

Using Python functions as kernels

You can use your own defined kernels by passing a function to the `kernel` parameter.

Your kernel must take as arguments two matrices of shape `(n_samples_1, n_features)`, `(n_samples_2, n_features)` and return a kernel matrix of shape `(n_samples_1, n_samples_2)`.

The following code defines a linear kernel and creates a classifier instance that will use that kernel:

    >>> import numpy as np
    >>> from sklearn import svm
    >>> def my_kernel(X, Y):
    ...     return np.dot(X, Y.T)
    ...
    >>> clf = svm.SVC(kernel=my_kernel)

</div>

<div class="dropdown">

Using the Gram matrix

You can pass pre-computed kernels by using the `kernel='precomputed'` option. You should then pass Gram matrix instead of X to the <span class="title-ref">fit</span> and <span class="title-ref">predict</span> methods. The kernel values between *all* training vectors and the test vectors must be provided:

> \>\>\> import numpy as np \>\>\> from sklearn.datasets import make\_classification \>\>\> from sklearn.model\_selection import train\_test\_split \>\>\> from sklearn import svm \>\>\> X, y = make\_classification(n\_samples=10, random\_state=0) \>\>\> X\_train , X\_test , y\_train, y\_test = train\_test\_split(X, y, random\_state=0) \>\>\> clf = svm.SVC(kernel='precomputed') \>\>\> \# linear kernel computation \>\>\> gram\_train = np.dot(X\_train, X\_train.T) \>\>\> clf.fit(gram\_train, y\_train) SVC(kernel='precomputed') \>\>\> \# predict on training examples \>\>\> gram\_test = np.dot(X\_test, X\_train.T) \>\>\> clf.predict(gram\_test) array(\[0, 1, 0\])

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_svm\_plot\_custom\_kernel.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_custom\_kernel.py)

## Mathematical formulation

A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called "support vectors":

![](../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_001.png)

In general, when the problem isn't linearly separable, the support vectors are the samples *within* the margin boundaries.

We recommend\[6\] and\[7\] as good references for the theory and practicalities of SVMs.

### SVC

Given training vectors \(x_i \in \mathbb{R}^p\), i=1,..., n, in two classes, and a vector \(y \in \{1, -1\}^n\), our goal is to find \(w \in
\mathbb{R}^p\) and \(b \in \mathbb{R}\) such that the prediction given by \(\text{sign} (w^T\phi(x) + b)\) is correct for most samples.

SVC solves the following primal problem:

\[\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i\]\[\begin{aligned}
\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
& \zeta_i \geq 0, i=1, ..., n
\end{aligned}\]

Intuitively, we're trying to maximize the margin (by minimizing \(||w||^2 = w^Tw\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \(y_i
(w^T \phi (x_i) + b)\) would be \(\geq 1\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \(\zeta_i\) from their correct margin boundary. The penalty term <span class="title-ref">C</span> controls the strength of this penalty, and as a result, acts as an inverse regularization parameter (see note below).

The dual problem to the primal is

\[\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha\]\[\begin{aligned}
\textrm {subject to } & y^T \alpha = 0\\
& 0 \leq \alpha_i \leq C, i=1, ..., n
\end{aligned}\]

where \(e\) is the vector of all ones, and \(Q\) is an \(n\) by \(n\) positive semidefinite matrix, \(Q_{ij} \equiv y_i y_j K(x_i, x_j)\), where \(K(x_i, x_j) = \phi (x_i)^T \phi (x_j)\) is the kernel. The terms \(\alpha_i\) are called the dual coefficients, and they are upper-bounded by \(C\). This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \(\phi\): see [kernel trick](https://en.wikipedia.org/wiki/Kernel_method).

Once the optimization problem is solved, the output of `decision_function` for a given sample \(x\) becomes:

\[\sum_{i\in SV} y_i \alpha_i K(x_i, x) + b,\]

and the predicted class correspond to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients \(\alpha_i\) are zero for the other samples.

These parameters can be accessed through the attributes `dual_coef_` which holds the product \(y_i \alpha_i\), `support_vectors_` which holds the support vectors, and `intercept_` which holds the independent term \(b\)

\> **Note** \> While SVM models derived from [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) and [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) use `C` as regularization parameter, most other estimators use `alpha`. The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is <span class="title-ref">\~sklearn.linear\_model.Ridge</span> regression, the relation between them is given as \(C = \frac{1}{alpha}\).

<div class="dropdown">

LinearSVC

The primal problem can be equivalently formulated as

\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}^{n}\max(0, 1 - y_i (w^T \phi(x_i) + b)),\]

where we make use of the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss). This is the form that is directly optimized by <span class="title-ref">LinearSVC</span>, but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by <span class="title-ref">LinearSVC</span> (\(\phi\) is the identity function).

</div>

<div id="nu_svc">

<div class="dropdown">

NuSVC

The \(\nu\)-SVC formulation\[8\] is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.

We introduce a new parameter \(\nu\) (instead of \(C\)) which controls the number of support vectors and *margin errors*: \(\nu \in (0, 1]\) is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin.

</div>

</div>

### SVR

Given training vectors \(x_i \in \mathbb{R}^p\), i=1,..., n, and a vector \(y \in \mathbb{R}^n\) \(\varepsilon\)-SVR solves the following primal problem:

\[\min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)\]\[\begin{aligned}
\textrm {subject to } & y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\
                      & w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\
                      & \zeta_i, \zeta_i^* \geq 0, i=1, ..., n
\end{aligned}\]

Here, we are penalizing samples whose prediction is at least \(\varepsilon\) away from their true target. These samples penalize the objective by \(\zeta_i\) or \(\zeta_i^*\), depending on whether their predictions lie above or below the \(\varepsilon\) tube.

The dual problem is

\[\min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)\]\[\begin{aligned}
\textrm {subject to } & e^T (\alpha - \alpha^*) = 0\\
& 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n
\end{aligned}\]

where \(e\) is the vector of all ones, \(Q\) is an \(n\) by \(n\) positive semidefinite matrix, \(Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)\) is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \(\phi\).

The prediction is:

\[\sum_{i \in SV}(\alpha_i - \alpha_i^*) K(x_i, x) + b\]

These parameters can be accessed through the attributes `dual_coef_` which holds the difference \(\alpha_i - \alpha_i^*\), `support_vectors_` which holds the support vectors, and `intercept_` which holds the independent term \(b\)

<div class="dropdown">

LinearSVR

The primal problem can be equivalently formulated as

\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}^{n}\max(0, |y_i - (w^T \phi(x_i) + b)| - \varepsilon),\]

where we make use of the epsilon-insensitive loss, i.e. errors of less than \(\varepsilon\) are ignored. This is the form that is directly optimized by <span class="title-ref">LinearSVR</span>.

</div>

## Implementation details

Internally, we use [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\[9\] and [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/)\[10\] to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers.

**References**

1.  Crammer and Singer [On the Algorithmic Implementation ofMulticlass Kernel-based Vector Machines](http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf), JMLR 2001.

2.  Platt ["Probabilistic outputs for SVMs and comparisons to regularized likelihood methods"](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf).

3.  Wu, Lin and Weng, ["Probability estimates for multi-class classification by pairwise coupling"](https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf), JMLR 5:975-1005, 2004.

4.  Fan, Rong-En, et al., ["LIBLINEAR: A library for large linear classification."](https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf), Journal of machine learning research 9.Aug (2008): 1871-1874.

5.  Chang and Lin, [LIBSVM: A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf).

6.  Bishop, [Pattern recognition and machine learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), chapter 7 Sparse Kernel Machines

7.  `"A Tutorial on Support Vector Regression"
    <10.1023/B:STCO.0000035301.49549.88>` Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive Volume 14 Issue 3, August 2004, p. 199-222.

8.  Schölkopf et. al [New Support Vector Algorithms](https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf)

9.  Chang and Lin, [LIBSVM: A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf).

10. Fan, Rong-En, et al., ["LIBLINEAR: A library for large linear classification."](https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf), Journal of machine learning research 9.Aug (2008): 1871-1874.

---

tree.md

---

# Decision Trees

<div class="currentmodule">

sklearn.tree

</div>

**Decision Trees (DTs)** are a non-parametric supervised learning method used for \[classification \<tree\_classification\>\](\#classification-\<tree\_classification\>) and \[regression \<tree\_regression\>\](\#regression \<tree\_regression\>). The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.

For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.

![](../auto_examples/tree/images/sphx_glr_plot_tree_regression_001.png)

Some advantages of decision trees are:

  - Simple to understand and to interpret. Trees can be visualized.
  - Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support \[missing values \<tree\_missing\_value\_support\>\](\#missing-values-\<tree\_missing\_value\_support\>).
  - The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
  - Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See \[algorithms \<tree\_algorithms\>\](\#algorithms-\<tree\_algorithms\>) for more information.
  - Able to handle multi-output problems.
  - Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.
  - Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
  - Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.

The disadvantages of decision trees include:

  - Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.
  - Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.
  - Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.
  - The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.
  - There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.
  - Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.

## Classification

<span class="title-ref">DecisionTreeClassifier</span> is a class capable of performing multi-class classification on a dataset.

As with other classifiers, <span class="title-ref">DecisionTreeClassifier</span> takes as input two arrays: an array X, sparse or dense, of shape `(n_samples, n_features)` holding the training samples, and an array Y of integer values, shape `(n_samples,)`, holding the class labels for the training samples:

    >>> from sklearn import tree
    >>> X = [[0, 0], [1, 1]]
    >>> Y = [0, 1]
    >>> clf = tree.DecisionTreeClassifier()
    >>> clf = clf.fit(X, Y)

After being fitted, the model can then be used to predict the class of samples:

    >>> clf.predict([[2., 2.]])
    array([1])

In case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes.

As an alternative to outputting a specific class, the probability of each class can be predicted, which is the fraction of training samples of the class in a leaf:

    >>> clf.predict_proba([[2., 2.]])
    array([[0., 1.]])

<span class="title-ref">DecisionTreeClassifier</span> is capable of both binary (where the labels are \[-1, 1\]) classification and multiclass (where the labels are \[0, ..., K-1\]) classification.

Using the Iris dataset, we can construct a tree as follows:

    >>> from sklearn.datasets import load_iris
    >>> from sklearn import tree
    >>> iris = load_iris()
    >>> X, y = iris.data, iris.target
    >>> clf = tree.DecisionTreeClassifier()
    >>> clf = clf.fit(X, y)

Once trained, you can plot the tree with the <span class="title-ref">plot\_tree</span> function:

    >>> tree.plot_tree(clf)
    [...]

![](../auto_examples/tree/images/sphx_glr_plot_iris_dtc_002.png)

<div class="dropdown">

Alternative ways to export trees

We can also export the tree in [Graphviz](https://www.graphviz.org/) format using the <span class="title-ref">export\_graphviz</span> exporter. If you use the [conda](https://conda.io) package manager, the graphviz binaries and the python package can be installed with <span class="title-ref">conda install python-graphviz</span>.

Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with <span class="title-ref">pip install graphviz</span>.

Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file \`iris.pdf\`:

    >>> import graphviz # doctest: +SKIP
    >>> dot_data = tree.export_graphviz(clf, out_file=None) # doctest: +SKIP
    >>> graph = graphviz.Source(dot_data) # doctest: +SKIP
    >>> graph.render("iris") # doctest: +SKIP

The <span class="title-ref">export\_graphviz</span> exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:

    >>> dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP
    ...                      feature_names=iris.feature_names,  # doctest: +SKIP
    ...                      class_names=iris.target_names,  # doctest: +SKIP
    ...                      filled=True, rounded=True,  # doctest: +SKIP
    ...                      special_characters=True)  # doctest: +SKIP
    >>> graph = graphviz.Source(dot_data)  # doctest: +SKIP
    >>> graph # doctest: +SKIP

<div class="only">

html

![](../images/iris.svg)

</div>

<div class="only">

latex

![](../images/iris.pdf)

</div>

![](../auto_examples/tree/images/sphx_glr_plot_iris_dtc_001.png)

Alternatively, the tree can also be exported in textual format with the function <span class="title-ref">export\_text</span>. This method doesn't require the installation of external libraries and is more compact:

> \>\>\> from sklearn.datasets import load\_iris \>\>\> from sklearn.tree import DecisionTreeClassifier \>\>\> from sklearn.tree import export\_text \>\>\> iris = load\_iris() \>\>\> decision\_tree = DecisionTreeClassifier(random\_state=0, max\_depth=2) \>\>\> decision\_tree = decision\_tree.fit(iris.data, iris.target) \>\>\> r = export\_text(decision\_tree, feature\_names=iris\['feature\_names'\]) \>\>\> print(r) --- petal width (cm) \> 0.80 | | | |--- class: 2 \<BLANKLINE\>

</div>

**Examples**

  - \[sphx\_glr\_auto\_examples\_tree\_plot\_iris\_dtc.py\](\#sphx\_glr\_auto\_examples\_tree\_plot\_iris\_dtc.py)
  - \[sphx\_glr\_auto\_examples\_tree\_plot\_unveil\_tree\_structure.py\](\#sphx\_glr\_auto\_examples\_tree\_plot\_unveil\_tree\_structure.py)

## Regression

![](../auto_examples/tree/images/sphx_glr_plot_tree_regression_001.png)

Decision trees can also be applied to regression problems, using the <span class="title-ref">DecisionTreeRegressor</span> class.

As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:

    >>> from sklearn import tree
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> clf = tree.DecisionTreeRegressor()
    >>> clf = clf.fit(X, y)
    >>> clf.predict([[1, 1]])
    array([0.5])

**Examples**

  - \[sphx\_glr\_auto\_examples\_tree\_plot\_tree\_regression.py\](\#sphx\_glr\_auto\_examples\_tree\_plot\_tree\_regression.py)

## Multi-output problems

A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of shape `(n_samples, n_outputs)`.

When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased.

With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:

  - Store n output values in leaves, instead of 1;
  - Use splitting criteria that compute the average reduction across all n outputs.

This module offers support for multi-output problems by implementing this strategy in both <span class="title-ref">DecisionTreeClassifier</span> and <span class="title-ref">DecisionTreeRegressor</span>. If a decision tree is fit on an output array Y of shape `(n_samples, n_outputs)` then the resulting estimator will:

  - Output n\_output values upon `predict`;
  - Output a list of n\_output arrays of class probabilities upon `predict_proba`.

The use of multi-output trees for regression is demonstrated in \[sphx\_glr\_auto\_examples\_tree\_plot\_tree\_regression.py\](\#sphx\_glr\_auto\_examples\_tree\_plot\_tree\_regression.py). In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.

![](../auto_examples/tree/images/sphx_glr_plot_tree_regression_002.png)

The use of multi-output trees for classification is demonstrated in \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py). In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.

![](../auto_examples/miscellaneous/images/sphx_glr_plot_multioutput_face_completion_001.png)

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_multioutput\_face\_completion.py)

**References**

  - M. Dumont et al, [Fast multi-class image annotation with random subwindows and multiple output randomized trees](http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf), International Conference on Computer Vision Theory and Applications 2009

## Complexity

In general, the run time cost to construct a balanced binary tree is \(O(n_{samples}n_{features}\log(n_{samples}))\) and query time \(O(\log(n_{samples}))\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n_{features})\) to find the feature that offers the largest reduction in the impurity criterion, e.g. log loss (which is equivalent to an information gain). This has a cost of \(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).

## Tips on practical use

  - Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit.

  - Consider performing dimensionality reduction (\[PCA \<PCA\>\](\#pca-\<pca\>), \[ICA \<ICA\>\](\#ica-\<ica\>), or \[feature\_selection\](\#feature\_selection)) beforehand to give your tree a better chance of finding features that are discriminative.

  - \[sphx\_glr\_auto\_examples\_tree\_plot\_unveil\_tree\_structure.py\](\#sphx\_glr\_auto\_examples\_tree\_plot\_unveil\_tree\_structure.py) will help in gaining more insights about how the decision tree makes predictions, which is important for understanding the important features in the data.

  - Visualize your tree as you are training by using the `export` function. Use `max_depth=3` as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.

  - Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use `max_depth` to control the size of the tree to prevent overfitting.

  - Use `min_samples_split` or `min_samples_leaf` to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try `min_samples_leaf=5` as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While `min_samples_split` can create arbitrarily small leaves, `min_samples_leaf` guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, `min_samples_leaf=1` is often the best choice.
    
    Note that `min_samples_split` considers samples directly and independent of `sample_weight`, if provided (e.g. a node with m weighted samples is still treated as having exactly m samples). Consider `min_weight_fraction_leaf` or `min_impurity_decrease` if accounting for sample weights is required at splits.

  - Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (`sample_weight`) for each class to the same value. Also note that weight-based pre-pruning criteria, such as `min_weight_fraction_leaf`, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like `min_samples_leaf`.

  - If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as `min_weight_fraction_leaf`, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.

  - All decision trees use `np.float32` arrays internally. If training data is not in this format, a copy of the dataset will be made.

  - If the input matrix X is very sparse, it is recommended to convert to sparse `csc_matrix` before calling fit and sparse `csr_matrix` before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.

## Tree algorithms: ID3, C4.5, C5.0 and CART

What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?

<div class="dropdown">

Various decision tree algorithms

[ID3](https://en.wikipedia.org/wiki/ID3_algorithm) (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalize to unseen data.

C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. The accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule's precondition if the accuracy of the rule improves without it.

C5.0 is Quinlan's latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.

CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.

</div>

scikit-learn uses an optimized version of the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now.

## Mathematical formulation

Given training vectors \(x_i \in R^n\), i=1,..., l and a label vector \(y \in R^l\), a decision tree recursively partitions the feature space such that the samples with the same labels or similar target values are grouped together.

Let the data at node \(m\) be represented by \(Q_m\) with \(n_m\) samples. For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_m^{left}(\theta)\) and \(Q_m^{right}(\theta)\) subsets

\[Q_m^{left}(\theta) = \{(x, y) | x_j \leq t_m\}\]\[Q_m^{right}(\theta) = Q_m \setminus Q_m^{left}(\theta)\]

The quality of a candidate split of node \(m\) is then computed using an impurity function or loss function \(H()\), the choice of which depends on the task being solved (classification or regression)

\[G(Q_m, \theta) = \frac{n_m^{left}}{n_m} H(Q_m^{left}(\theta))
+ \frac{n_m^{right}}{n_m} H(Q_m^{right}(\theta))\]

Select the parameters that minimises the impurity

\[\theta^* = \operatorname{argmin}_\theta  G(Q_m, \theta)\]

Recurse for subsets \(Q_m^{left}(\theta^*)\) and \(Q_m^{right}(\theta^*)\) until the maximum allowable depth is reached, \(n_m < \min_{samples}\) or \(n_m = 1\).

### Classification criteria

If a target is a classification outcome taking on values 0,1,...,K-1, for node \(m\), let

\[p_{mk} = \frac{1}{n_m} \sum_{y \in Q_m} I(y = k)\]

be the proportion of class k observations in node \(m\). If \(m\) is a terminal node, <span class="title-ref">predict\_proba</span> for this region is set to \(p_{mk}\). Common measures of impurity are the following.

Gini:

\[H(Q_m) = \sum_k p_{mk} (1 - p_{mk})\]

Log Loss or Entropy:

\[H(Q_m) = - \sum_k p_{mk} \log(p_{mk})\]

<div class="dropdown">

Shannon entropy

The entropy criterion computes the Shannon entropy of the possible classes. It takes the class frequencies of the training data points that reached a given leaf \(m\) as their probability. Using the **Shannon entropy as tree node splitting criterion is equivalent to minimizing the log loss** (also known as cross-entropy and multinomial deviance) between the true labels \(y_i\) and the probabilistic predictions \(T_k(x_i)\) of the tree model \(T\) for class \(k\).

To see this, first recall that the log loss of a tree model \(T\) computed on a dataset \(D\) is defined as follows:

\[\mathrm{LL}(D, T) = -\frac{1}{n} \sum_{(x_i, y_i) \in D} \sum_k I(y_i = k) \log(T_k(x_i))\]

where \(D\) is a training dataset of \(n\) pairs \((x_i, y_i)\).

In a classification tree, the predicted class probabilities within leaf nodes are constant, that is: for all \((x_i, y_i) \in Q_m\), one has: \(T_k(x_i) = p_{mk}\) for each class \(k\).

This property makes it possible to rewrite \(\mathrm{LL}(D, T)\) as the sum of the Shannon entropies computed for each leaf of \(T\) weighted by the number of training data points that reached each leaf:

\[\mathrm{LL}(D, T) = \sum_{m \in T} \frac{n_m}{n} H(Q_m)\]

</div>

### Regression criteria

If the target is a continuous value, then for node \(m\), common criteria to minimize as for determining locations for future splits are Mean Squared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute Error (MAE or L1 error). MSE and Poisson deviance both set the predicted value of terminal nodes to the learned mean value \(\bar{y}_m\) of the node whereas the MAE sets the predicted value of terminal nodes to the median \(median(y)_m\).

Mean Squared Error:

\[\bar{y}_m = \frac{1}{n_m} \sum_{y \in Q_m} y\]\[H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} (y - \bar{y}_m)^2\]

Mean Poisson deviance:

\[H(Q_m) = \frac{2}{n_m} \sum_{y \in Q_m} (y \log\frac{y}{\bar{y}_m}
- y + \bar{y}_m)\]

Setting <span class="title-ref">criterion="poisson"</span> might be a good choice if your target is a count or a frequency (count per some unit). In any case, \(y >= 0\) is a necessary condition to use this criterion. Note that it fits much slower than the MSE criterion. For performance reasons the actual implementation minimizes the half mean poisson deviance, i.e. the mean poisson deviance divided by 2.

Mean Absolute Error:

\[median(y)_m = \underset{y \in Q_m}{\mathrm{median}}(y)\]\[H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} |y - median(y)_m|\]

Note that it fits much slower than the MSE criterion.

## Missing Values Support

<span class="title-ref">DecisionTreeClassifier</span>, <span class="title-ref">DecisionTreeRegressor</span> have built-in support for missing values using <span class="title-ref">splitter='best'</span>, where the splits are determined in a greedy fashion. <span class="title-ref">ExtraTreeClassifier</span>, and <span class="title-ref">ExtraTreeRegressor</span> have built-in support for missing values for <span class="title-ref">splitter='random'</span>, where the splits are determined randomly. For more details on how the splitter differs on non-missing values, see the \[Forest section \<forest\>\](\#forest-section-\<forest\>).

The criterion supported when there are missing-values are <span class="title-ref">'gini'</span>, <span class="title-ref">'entropy</span>', or <span class="title-ref">'log\_loss'</span>, for classification or <span class="title-ref">'squared\_error'</span>, <span class="title-ref">'friedman\_mse'</span>, or <span class="title-ref">'poisson'</span> for regression.

First we will describe how <span class="title-ref">DecisionTreeClassifier</span>, <span class="title-ref">DecisionTreeRegressor</span> handle missing-values in the data.

For each potential threshold on the non-missing data, the splitter will evaluate the split with all the missing values going to the left node or the right node.

Decisions are made as follows:

  - By default when predicting, the samples with missing values are classified with the class used in the split found during training:
    
        >>> from sklearn.tree import DecisionTreeClassifier
        >>> import numpy as np
        
        >>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)
        >>> y = [0, 0, 1, 1]
        
        >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
        >>> tree.predict(X)
        array([0, 0, 1, 1])

  - If the criterion evaluation is the same for both nodes, then the tie for missing value at predict time is broken by going to the right node. The splitter also checks the split where all the missing values go to one child and non-missing values go to the other:
    
        >>> from sklearn.tree import DecisionTreeClassifier
        >>> import numpy as np
        
        >>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
        >>> y = [0, 0, 1, 1]
        
        >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
        
        >>> X_test = np.array([np.nan]).reshape(-1, 1)
        >>> tree.predict(X_test)
        array([1])

  - If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples:
    
        >>> from sklearn.tree import DecisionTreeClassifier
        >>> import numpy as np
        
        >>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)
        >>> y = [0, 1, 1, 1]
        
        >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
        
        >>> X_test = np.array([np.nan]).reshape(-1, 1)
        >>> tree.predict(X_test)
        array([1])

<span class="title-ref">ExtraTreeClassifier</span>, and <span class="title-ref">ExtraTreeRegressor</span> handle missing values in a slightly different way. When splitting a node, a random threshold will be chosen to split the non-missing values on. Then the non-missing values will be sent to the left and right child based on the randomly selected threshold, while the missing values will also be randomly sent to the left or right child. This is repeated for every feature considered at each split. The best split among these is chosen.

During prediction, the treatment of missing-values is the same as that of the decision tree:

  - By default when predicting, the samples with missing values are classified with the class used in the split found during training.
  - If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples.

## Minimal Cost-Complexity Pruning

Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of [\[BRE\]](#BRE). This algorithm is parameterized by \(\alpha\ge0\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \(R_\alpha(T)\) of a given tree \(T\):

\[R_\alpha(T) = R(T) + \alpha|\widetilde{T}|\]

where \(|\widetilde{T}|\) is the number of terminal nodes in \(T\) and \(R(T)\) is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for \(R(T)\). As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of \(T\) that minimizes \(R_\alpha(T)\).

The cost complexity measure of a single node is \(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a tree where node \(t\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \(R(T_t)<R(t)\). However, the cost complexity measure of a node, \(t\), and its branch, \(T_t\), can be equal depending on \(\alpha\). We define the effective \(\alpha\) of a node to be the value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or \(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node with the smallest value of \(\alpha_{eff}\) is the weakest link and will be pruned. This process stops when the pruned tree's minimal \(\alpha_{eff}\) is greater than the `ccp_alpha` parameter.

**Examples**

  - \[sphx\_glr\_auto\_examples\_tree\_plot\_cost\_complexity\_pruning.py\](\#sphx\_glr\_auto\_examples\_tree\_plot\_cost\_complexity\_pruning.py)

**References**

  - <https://en.wikipedia.org/wiki/Decision_tree_learning>
  - <https://en.wikipedia.org/wiki/Predictive_analytics>
  - J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.
  - T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.

<div id="citations">

  - <span id="BRE" class="citation-label">BRE</span>  
    L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.

</div>

---

unsupervised_reduction.md

---

# Unsupervised dimensionality reduction

If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the \[unsupervised-learning\](\#unsupervised-learning) methods implement a `transform` method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.

<div class="topic">

****Pipelining****

The unsupervised data reduction and the supervised estimator can be chained in one step. See \[pipeline\](\#pipeline).

</div>

<div class="currentmodule">

sklearn

</div>

## PCA: principal component analysis

<span class="title-ref">decomposition.PCA</span> looks for a combination of features that capture well the variance of the original features. See \[decompositions\](\#decompositions).

**Examples**

  - \[sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py)

## Random projections

The module: `~sklearn.random_projection` provides several tools for data reduction by random projections. See the relevant section of the documentation: \[random\_projection\](\#random\_projection).

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_johnson\_lindenstrauss\_bound.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_johnson\_lindenstrauss\_bound.py)

## Feature agglomeration

<span class="title-ref">cluster.FeatureAgglomeration</span> applies \[hierarchical\_clustering\](\#hierarchical\_clustering) to group together features that behave similarly.

**Examples**

  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_feature\_agglomeration\_vs\_univariate\_selection.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_feature\_agglomeration\_vs\_univariate\_selection.py)
  - \[sphx\_glr\_auto\_examples\_cluster\_plot\_digits\_agglomeration.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_digits\_agglomeration.py)

<div class="topic">

****Feature scaling****

Note that if features have very different scaling or statistical properties, <span class="title-ref">cluster.FeatureAgglomeration</span> may not be able to capture the links between related features. Using a <span class="title-ref">preprocessing.StandardScaler</span> can be useful in these settings.

</div>

---

presentations.md

---

# External Resources, Videos and Talks

## New to Scientific Python?

For those that are still new to the scientific Python ecosystem, we highly recommend the [Python Scientific Lecture Notes](https://scipy-lectures.org). This will help you find your footing a bit and will definitely improve your scikit-learn experience. A basic understanding of NumPy arrays is recommended to make the most of scikit-learn.

## External Tutorials

There are several online tutorials available which are geared toward specific subject areas:

  - [Machine Learning for NeuroImaging in Python](https://nilearn.github.io/)
  - [Machine Learning for Astronomical Data Analysis](https://github.com/astroML/sklearn_tutorial)

## Videos

  - An introduction to scikit-learn [Part I](https://conference.scipy.org/scipy2013/tutorial_detail.php?id=107) and [Part II](https://conference.scipy.org/scipy2013/tutorial_detail.php?id=111) at Scipy 2013 by [Gael Varoquaux](https://gael-varoquaux.info), [Jake Vanderplas](http://www.vanderplas.com) and [Olivier Grisel](https://twitter.com/ogrisel). Notebooks on [github](https://github.com/jakevdp/sklearn_scipy2013).

  - [Introduction to scikit-learn](http://videolectures.net/icml2010_varaquaux_scik/) by [Gael Varoquaux](https://gael-varoquaux.info) at ICML 2010
    
    A three minute video from a very early stage of scikit-learn, explaining the basic idea and approach we are following.

  - [Introduction to statistical learning with scikit-learn](https://archive.org/search.php?query=scikit-learn) by [Gael Varoquaux](https://gael-varoquaux.info) at SciPy 2011
    
    An extensive tutorial, consisting of four sessions of one hour. The tutorial covers the basics of machine learning, many algorithms and how to apply them using scikit-learn.

  - [Statistical Learning for Text Classification with scikit-learn and NLTK](https://pyvideo.org/video/417/pycon-2011--statistical-machine-learning-for-text) (and [slides](https://www.slideshare.net/ogrisel/statistical-machine-learning-for-text-classification-with-scikitlearn-and-nltk)) by [Olivier Grisel](https://twitter.com/ogrisel) at PyCon 2011
    
    Thirty minute introduction to text classification. Explains how to use NLTK and scikit-learn to solve real-world text classification tasks and compares against cloud-based solutions.

  - [Introduction to Interactive Predictive Analytics in Python with scikit-learn](https://www.youtube.com/watch?v=Zd5dfooZWG4) by [Olivier Grisel](https://twitter.com/ogrisel) at PyCon 2012
    
    3-hours long introduction to prediction tasks using scikit-learn.

  - [scikit-learn - Machine Learning in Python](https://www.youtube.com/watch?v=cHZONQ2-x7I) by [Jake Vanderplas](http://www.vanderplas.com) at the 2012 PyData workshop at Google
    
    Interactive demonstration of some scikit-learn features. 75 minutes.

  - [scikit-learn tutorial](https://www.youtube.com/watch?v=cHZONQ2-x7I) by [Jake Vanderplas](http://www.vanderplas.com) at PyData NYC 2012
    
    Presentation using the online tutorial, 45 minutes.

---

related_projects.md

---

# Related Projects

Projects implementing the scikit-learn estimator API are encouraged to use the [scikit-learn-contrib template](https://github.com/scikit-learn-contrib/project-template) which facilitates best practices for testing and documenting estimators. The [scikit-learn-contrib GitHub organization](https://github.com/scikit-learn-contrib/scikit-learn-contrib) also accepts high-quality contributions of repositories conforming to this template.

Below is a list of sister-projects, extensions and domain specific packages.

## Interoperability and framework enhancements

These tools adapt scikit-learn for use with other technologies or otherwise enhance the functionality of scikit-learn's estimators.

**Auto-ML**

  - [auto-sklearn](https://github.com/automl/auto-sklearn/) An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator
  - [autoviml](https://github.com/AutoViML/Auto_ViML/) Automatically Build Multiple Machine Learning Models with a Single Line of Code. Designed as a faster way to use scikit-learn models without having to preprocess data.
  - [TPOT](https://github.com/rhiever/tpot) An automated machine learning toolkit that optimizes a series of scikit-learn operators to design a machine learning pipeline, including data and feature preprocessors as well as the estimators. Works as a drop-in replacement for a scikit-learn estimator.
  - [Featuretools](https://github.com/alteryx/featuretools) A framework to perform automated feature engineering. It can be used for transforming temporal and relational datasets into feature matrices for machine learning.
  - [EvalML](https://github.com/alteryx/evalml) EvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-specific objective functions. It incorporates multiple modeling libraries under one API, and the objects that EvalML creates use an sklearn-compatible API.
  - [MLJAR AutoML](https://github.com/mljar/mljar-supervised) Python package for AutoML on Tabular Data with Feature Engineering, Hyper-Parameters Tuning, Explanations and Automatic Documentation.

**Experimentation and model registry frameworks**

  - [MLFlow](https://mlflow.org/) MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry.
  - [Neptune](https://neptune.ai/) Metadata store for MLOps, built for teams that run a lot of experiments. It gives you a single place to log, store, display, organize, compare, and query all your model building metadata.
  - [Sacred](https://github.com/IDSIA/Sacred) Tool to help you configure, organize, log and reproduce experiments
  - [Scikit-Learn Laboratory](https://skll.readthedocs.io/en/latest/index.html) A command-line wrapper around scikit-learn that makes it easy to run machine learning experiments with multiple learners and large feature sets.

**Model inspection and visualization**

  - [dtreeviz](https://github.com/parrt/dtreeviz/) A python library for decision tree visualization and model interpretation.
  - [sklearn-evaluation](https://github.com/ploomber/sklearn-evaluation) Machine learning model evaluation made easy: plots, tables, HTML reports, experiment tracking and Jupyter notebook analysis. Visual analysis, model selection, evaluation and diagnostics.
  - [yellowbrick](https://github.com/DistrictDataLabs/yellowbrick) A suite of custom matplotlib visualizers for scikit-learn estimators to support visual feature analysis, model selection, evaluation, and diagnostics.

**Model export for production**

  - [sklearn-onnx](https://github.com/onnx/sklearn-onnx) Serialization of many Scikit-learn pipelines to [ONNX](https://onnx.ai/) for interchange and prediction.
  - [skops.io](https://skops.readthedocs.io/en/stable/persistence.html) A persistence model more secure than pickle, which can be used instead of pickle in most common cases.
  - [sklearn2pmml](https://github.com/jpmml/sklearn2pmml) Serialization of a wide variety of scikit-learn estimators and transformers into PMML with the help of [JPMML-SkLearn](https://github.com/jpmml/jpmml-sklearn) library.
  - [treelite](https://treelite.readthedocs.io) Compiles tree-based ensemble models into C code for minimizing prediction latency.
  - [emlearn](https://emlearn.org) Implements scikit-learn estimators in C99 for embedded devices and microcontrollers. Supports several classifier, regression and outlier detection models.

**Model throughput**

  - [Intel(R) Extension for scikit-learn](https://github.com/intel/scikit-learn-intelex) Mostly on high end Intel(R) hardware, accelerates some scikit-learn models for both training and inference under certain circumstances. This project is maintained by Intel(R) and scikit-learn's maintainers are not involved in the development of this project. Also note that in some cases using the tools and estimators under `scikit-learn-intelex` would give different results than `scikit-learn` itself. If you encounter issues while using this project, make sure you report potential issues in their respective repositories.

**Interface to R with genomic applications**

  - [BiocSklearn](https://bioconductor.org/packages/BiocSklearn) Exposes a small number of dimension reduction facilities as an illustration of the basilisk protocol for interfacing python with R. Intended as a springboard for more complete interop.

## Other estimators and tasks

Not everything belongs or is mature enough for the central scikit-learn project. The following are projects providing interfaces similar to scikit-learn for additional learning algorithms, infrastructures and tasks.

**Time series and forecasting**

  - [Darts](https://unit8co.github.io/darts/) Darts is a Python library for user-friendly forecasting and anomaly detection on time series. It contains a variety of models, from classics such as ARIMA to deep neural networks. The forecasting models can all be used in the same way, using fit() and predict() functions, similar to scikit-learn.
  - [sktime](https://github.com/alan-turing-institute/sktime) A scikit-learn compatible toolbox for machine learning with time series including time series classification/regression and (supervised/panel) forecasting.
  - [skforecast](https://github.com/JoaquinAmatRodrigo/skforecast) A python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API.
  - [tslearn](https://github.com/tslearn-team/tslearn) A machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression.

**Gradient (tree) boosting**

Note scikit-learn own modern gradient boosting estimators <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingRegressor</span>.

  - [XGBoost](https://github.com/dmlc/xgboost) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
  - [LightGBM](https://lightgbm.readthedocs.io) LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient.

**Structured learning**

  - [HMMLearn](https://github.com/hmmlearn/hmmlearn) Implementation of hidden markov models that was previously part of scikit-learn.
  - [pomegranate](https://github.com/jmschrei/pomegranate) Probabilistic modelling for Python, with an emphasis on hidden Markov models.

**Deep neural networks etc.**

  - [skorch](https://github.com/dnouri/skorch) A scikit-learn compatible neural network library that wraps PyTorch.
  - [scikeras](https://github.com/adriangb/scikeras) provides a wrapper around Keras to interface it with scikit-learn. SciKeras is the successor of <span class="title-ref">tf.keras.wrappers.scikit\_learn</span>.

**Federated Learning**

  - [Flower](https://flower.dev/) A friendly federated learning framework with a unified approach that can federate any workload, any ML framework, and any programming language.

**Privacy Preserving Machine Learning**

  - [Concrete ML](https://github.com/zama-ai/concrete-ml/) A privacy preserving ML framework built on top of [Concrete](https://github.com/zama-ai/concrete), with bindings to traditional ML frameworks, thanks to fully homomorphic encryption. APIs of so-called Concrete ML built-in models are very close to scikit-learn APIs.

**Broad scope**

  - [mlxtend](https://github.com/rasbt/mlxtend) Includes a number of additional estimators as well as model visualization utilities.
  - [scikit-lego](https://github.com/koaning/scikit-lego) A number of scikit-learn compatible custom transformers, models and metrics, focusing on solving practical industry tasks.

**Other regression and classification**

  - [py-earth](https://github.com/scikit-learn-contrib/py-earth) Multivariate adaptive regression splines
  - [gplearn](https://github.com/trevorstephens/gplearn) Genetic Programming for symbolic regression tasks.
  - [scikit-multilearn](https://github.com/scikit-multilearn/scikit-multilearn) Multi-label classification with focus on label space manipulation.

**Decomposition and clustering**

  - [lda](https://github.com/lda-project/lda/): Fast implementation of latent Dirichlet allocation in Cython which uses [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) to sample from the true posterior distribution. (scikit-learn's <span class="title-ref">\~sklearn.decomposition.LatentDirichletAllocation</span> implementation uses [variational inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) to sample from a tractable approximation of a topic model's posterior distribution.)
  - [kmodes](https://github.com/nicodv/kmodes) k-modes clustering algorithm for categorical data, and several of its variations.
  - [hdbscan](https://github.com/scikit-learn-contrib/hdbscan) HDBSCAN and Robust Single Linkage clustering algorithms for robust variable density clustering. As of scikit-learn version 1.3.0, there is <span class="title-ref">\~sklearn.cluster.HDBSCAN</span>.

**Pre-processing**

  - [categorical-encoding](https://github.com/scikit-learn-contrib/categorical-encoding) A library of sklearn compatible categorical variable encoders. As of scikit-learn version 1.3.0, there is <span class="title-ref">\~sklearn.preprocessing.TargetEncoder</span>.
  - [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) Various methods to under- and over-sample datasets.
  - [Feature-engine](https://github.com/solegalli/feature_engine) A library of sklearn compatible transformers for missing data imputation, categorical encoding, variable transformation, discretization, outlier handling and more. Feature-engine allows the application of preprocessing steps to selected groups of variables and it is fully compatible with the Scikit-learn Pipeline.

**Topological Data Analysis**

  - [giotto-tda](https://github.com/giotto-ai/giotto-tda) A library for [Topological Data Analysis](https://en.wikipedia.org/wiki/Topological_data_analysis) aiming to provide a scikit-learn compatible API. It offers tools to transform data inputs (point clouds, graphs, time series, images) into forms suitable for computations of topological summaries, and components dedicated to extracting sets of scalar features of topological origin, which can be used alongside other feature extraction methods in scikit-learn.

## Statistical learning with Python

Other packages useful for data analysis and machine learning.

  - [Pandas](https://pandas.pydata.org/) Tools for working with heterogeneous and columnar data, relational queries, time series and basic statistics.
  - [statsmodels](https://www.statsmodels.org) Estimating and analysing statistical models. More focused on statistical tests and less on prediction than scikit-learn.
  - [PyMC](https://www.pymc.io/) Bayesian statistical models and fitting algorithms.
  - [Seaborn](https://stanford.edu/~mwaskom/software/seaborn/) Visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.
  - [scikit-survival](https://scikit-survival.readthedocs.io/) A library implementing models to learn from censored time-to-event data (also called survival analysis). Models are fully compatible with scikit-learn.

### Recommendation Engine packages

  - [implicit](https://github.com/benfred/implicit), Library for implicit feedback datasets.
  - [lightfm](https://github.com/lyst/lightfm) A Python/Cython implementation of a hybrid recommender system.
  - [Surprise Lib](https://surpriselib.com/) Library for explicit feedback datasets.

### Domain specific packages

  - [scikit-network](https://scikit-network.readthedocs.io/) Machine learning on graphs.
  - [scikit-image](https://scikit-image.org/) Image processing and computer vision in python.
  - [Natural language toolkit (nltk)](https://www.nltk.org/) Natural language processing and some machine learning.
  - [gensim](https://radimrehurek.com/gensim/) A library for topic modelling, document indexing and similarity retrieval
  - [NiLearn](https://nilearn.github.io/) Machine learning for neuro-imaging.
  - [AstroML](https://www.astroml.org/) Machine learning for astronomy.

## Translations of scikit-learn documentation

Translation's purpose is to ease reading and understanding in languages other than English. Its aim is to help people who do not understand English or have doubts about its interpretation. Additionally, some people prefer to read documentation in their native language, but please bear in mind that the only official documentation is the English one\[1\].

Those translation efforts are community initiatives and we have no control on them. If you want to contribute or report an issue with the translation, please contact the authors of the translation. Some available translations are linked here to improve their dissemination and promote community efforts.

  - [Chinese translation](https://sklearn.apachecn.org/) ([source](https://github.com/apachecn/sklearn-doc-zh))
  - [Persian translation](https://sklearn.ir/) ([source](https://github.com/mehrdad-dev/scikit-learn))
  - [Spanish translation](https://qu4nt.github.io/sklearn-doc-es/) ([source](https://github.com/qu4nt/sklearn-doc-es))
  - [Korean translation](https://panda5176.github.io/scikit-learn-korean/) ([source](https://github.com/panda5176/scikit-learn-korean))

**Footnotes**

1.  following [linux documentation Disclaimer](https://www.kernel.org/doc/html/latest/translations/index.html#disclaimer)

---

roadmap.md

---

# Roadmap

## Purpose of this document

This document list general directions that core contributors are interested to see developed in scikit-learn. The fact that an item is listed here is in no way a promise that it will happen, as resources are limited. Rather, it is an indication that help is welcomed on this topic.

## Statement of purpose: Scikit-learn in 2018

Eleven years after the inception of Scikit-learn, much has changed in the world of machine learning. Key changes include:

  - Computational tools: The exploitation of GPUs, distributed programming frameworks like Scala/Spark, etc.
  - High-level Python libraries for experimentation, processing and data management: Jupyter notebook, Cython, Pandas, Dask, Numba...
  - Changes in the focus of machine learning research: artificial intelligence applications (where input structure is key) with deep learning, representation learning, reinforcement learning, domain transfer, etc.

A more subtle change over the last decade is that, due to changing interests in ML, PhD students in machine learning are more likely to contribute to PyTorch, Dask, etc. than to Scikit-learn, so our contributor pool is very different to a decade ago.

Scikit-learn remains very popular in practice for trying out canonical machine learning techniques, particularly for applications in experimental science and in data science. A lot of what we provide is now very mature. But it can be costly to maintain, and we cannot therefore include arbitrary new implementations. Yet Scikit-learn is also essential in defining an API framework for the development of interoperable machine learning components external to the core library.

**Thus our main goals in this era are to**:

  - continue maintaining a high-quality, well-documented collection of canonical tools for data processing and machine learning within the current scope (i.e. rectangular data largely invariant to column and row order; predicting targets with simple structure)
  - improve the ease for users to develop and publish external components
  - improve interoperability with modern data science tools (e.g. Pandas, Dask) and infrastructures (e.g. distributed processing)

Many of the more fine-grained goals can be found under the [API tag](https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3AAPI) on the issue tracker.

## Architectural / general goals

The list is numbered not as an indication of the order of priority, but to make referring to specific points easier. Please add new entries only at the bottom. Note that the crossed out entries are already done, and we try to keep the document up to date as we work on these issues.

1.  Improved handling of Pandas DataFrames
      - document current handling
2.  Improved handling of categorical features
      - Tree-based models should be able to handle both continuous and categorical features `29437`.
      - Handling mixtures of categorical and continuous variables
3.  Improved handling of missing data
      - Making sure meta-estimators are lenient towards missing data by implementing a common test.
      - An amputation sample generator to make parts of a dataset go missing `6284`
4.  More didactic documentation
      - More and more options have been added to scikit-learn. As a result, the documentation is crowded which makes it hard for beginners to get the big picture. Some work could be done in prioritizing the information.
5.  Passing around information that is not (X, y): Feature properties
      - Per-feature handling (e.g. "is this a nominal / ordinal / English language text?") should also not need to be provided to estimator constructors, ideally, but should be available as metadata alongside X. `8480`
6.  Passing around information that is not (X, y): Target information
      - We have problems getting the full set of classes to all components when the data is split/sampled. `6231` `8100`
      - We have no way to handle a mixture of categorical and continuous targets.
7.  Make it easier for external users to write Scikit-learn-compatible components
      - More self-sufficient running of scikit-learn-contrib or a similar resource
8.  Support resampling and sample reduction
      - Allow subsampling of majority classes (in a pipeline?) `3855`
9.  Better interfaces for interactive development
      - Improve the HTML visualisations of estimators via the <span class="title-ref">estimator\_html\_repr</span>.
      - Include more plotting tools, not just as examples.
10. Improved tools for model diagnostics and basic inference
      - work on a unified interface for "feature importance"
      - better ways to handle validation sets when fitting
11. Better tools for selecting hyperparameters with transductive estimators
      - Grid search and cross validation are not applicable to most clustering tasks. Stability-based selection is more relevant.
12. Better support for manual and automatic pipeline building
      - Easier way to construct complex pipelines and valid search spaces `7608` `5082` `8243`
      - provide search ranges for common estimators??
      - cf. [searchgrid](https://searchgrid.readthedocs.io/en/latest/)
13. Improved tracking of fitting
      - Verbose is not very friendly and should use a standard logging library `6929`, `78`
      - Callbacks or a similar system would facilitate logging and early stopping
14. Distributed parallelism
      - Accept data which complies with `__array_function__`
15. A way forward for more out of core
      - Dask enables easy out-of-core computation. While the Dask model probably cannot be adaptable to all machine-learning algorithms, most machine learning is on smaller data than ETL, hence we can maybe adapt to very large scale while supporting only a fraction of the patterns.
16. Backwards-compatible de/serialization of some estimators
      - Currently serialization (with pickle) breaks across versions. While we may not be able to get around other limitations of pickle re security etc, it would be great to offer cross-version safety from version 1.0. Note: Gael and Olivier think that this can cause heavy maintenance burden and we should manage the trade-offs. A possible alternative is presented in the following point.
17. Documentation and tooling for model lifecycle management
      - Document good practices for model deployments and lifecycle: before deploying a model: snapshot the code versions (numpy, scipy, scikit-learn, custom code repo), the training script and an alias on how to retrieve historical training data + snapshot a copy of a small validation set + snapshot of the predictions (predicted probabilities for classifiers) on that validation set.
      - Document and tools to make it easy to manage upgrade of scikit-learn versions:
          - Try to load the old pickle, if it works, use the validation set prediction snapshot to detect that the serialized model still behave the same;
          - If joblib.load / pickle.load not work, use the versioned control training script + historical training set to retrain the model and use the validation set prediction snapshot to assert that it is possible to recover the previous predictive performance: if this is not the case there is probably a bug in scikit-learn that needs to be reported.
18. Everything in scikit-learn should probably conform to our API contract. We are still in the process of making decisions on some of these related issues.
      - <span class="title-ref">Pipeline \<pipeline.Pipeline\></span> and <span class="title-ref">FeatureUnion</span> modify their input parameters in fit. Fixing this requires making sure we have a good grasp of their use cases to make sure all current functionality is maintained. `8157` `7382`
19. (Optional) Improve scikit-learn common tests suite to make sure that (at least for frequently used) models have stable predictions across-versions (to be discussed);
      - Extend documentation to mention how to deploy models in Python-free environments for instance [ONNX](https://github.com/onnx/sklearn-onnx). and use the above best practices to assess predictive consistency between scikit-learn and ONNX prediction functions on validation set.
      - Document good practices to detect temporal distribution drift for deployed model and good practices for re-training on fresh data without causing catastrophic predictive performance regressions.

---

supervised_learning.md

---

# Supervised learning

<div class="toctree" data-maxdepth="2">

modules/linear\_model modules/lda\_qda.rst modules/kernel\_ridge.rst modules/svm modules/sgd modules/neighbors modules/gaussian\_process modules/cross\_decomposition.rst modules/naive\_bayes modules/tree modules/ensemble modules/multiclass modules/feature\_selection.rst modules/semi\_supervised.rst modules/isotonic.rst modules/calibration.rst modules/neural\_networks\_supervised

</div>

---

support.md

---

# Support

There are several channels to connect with scikit-learn developers for assistance, feedback, or contributions.

**Note**: Communications on all channels should respect our [Code of Conduct](https://github.com/scikit-learn/scikit-learn/blob/main/CODE_OF_CONDUCT.md).

## Mailing Lists

  - **Main Mailing List**: Join the primary discussion platform for scikit-learn at [scikit-learn Mailing List](https://mail.python.org/mailman/listinfo/scikitlearn).
  - **Commit Updates**: Stay informed about repository updates and test failures on the [scikit-learn-commits list](https://lists.sourceforge.net/lists/listinfo/scikit-learn-commits).

## User Questions

If you have questions, this is our general workflow.

  - **Stack Overflow**: Some scikit-learn developers support users using the [\[scikit-learn\]](https://stackoverflow.com/questions/tagged/scikit-learn) tag.
  - **General Machine Learning Queries**: For broader machine learning discussions, visit [Stack Exchange](https://stats.stackexchange.com/).

When posting questions:

  - Please use a descriptive question in the title field (e.g. no "Please help with scikit-learn\!" as this is not a question)
  - Provide detailed context, expected results, and actual observations.
  - Include code and data snippets (preferably minimalistic scripts, up to \~20 lines).
  - Describe your data and preprocessing steps, including sample size, feature types (categorical or numerical), and the target for supervised learning tasks (classification type or regression).

**Note**: Avoid asking user questions on the bug tracker to keep the focus on development.

  - [GitHub Discussions](https://github.com/scikit-learn/scikit-learn/discussions) Usage questions such as methodological
  - [Stack Overflow](https://stackoverflow.com/questions/tagged/scikit-learn) Programming/user questions with <span class="title-ref">\[scikit-learn\]</span> tag
  - [GitHub Bug Tracker](https://github.com/scikit-learn/scikit-learn/issues) Bug reports - Please do not ask usage questions on the issue tracker.
  - [Discord Server](https://discord.gg/h9qyrK8Jc8) Current pull requests - Post any specific PR-related questions on your PR, and you can share a link to your PR on this server.

## Bug Tracker

Encountered a bug? Report it on our [issue tracker](https://github.com/scikit-learn/scikit-learn/issues)

Include in your report:

  - Steps or scripts to reproduce the bug.
  - Expected and observed outcomes.
  - Python or gdb tracebacks, if applicable.

<!-- end list -->

  - \- The ideal bug report contains a \[short reproducible code snippet  
    \<minimal\_reproducer\>\](\#short-reproducible-code-snippet

\--\<minimal\_reproducer\>), this way anyone can try to reproduce the bug easily.

  - If your snippet is longer than around 50 lines, please link to a [gist](https://gist.github.com) or a github repo.

**Tip**: Gists are Git repositories; you can push data files to them using Git.

## Social Media

scikit-learn has presence on various social media platforms to share updates with the community. The platforms are not monitored for user questions.

## Gitter

**Note**: The scikit-learn Gitter room is no longer an active community. For live discussions and support, please refer to the other channels mentioned in this document.

## Documentation Resources

This documentation is for . Find documentation for other versions [here](https://scikit-learn.org/dev/versions.html).

Older versions' printable PDF documentation is available [here](https://sourceforge.net/projects/scikit-learn/files/documentation/). Building the PDF documentation is no longer supported in the website, but you can still generate it locally by following the \[building documentation instructions \<building\_documentation\>\](\#building-documentation-instructions-\<building\_documentation\>).

---

base.md

---

{{ objname | escape | underline(line="=") }}

{% if objtype == "module" -%}

<div class="automodule">

{{ fullname }}

</div>

{%- elif objtype == "function" -%}

<div class="currentmodule">

{{ module }}

</div>

<div class="autofunction">

{{ objname }}

</div>

<div class="minigallery" data-add-heading="Gallery examples" data-heading-level="-">

{{ module }}.{{ objname }}

</div>

{%- elif objtype == "class" -%}

<div class="currentmodule">

{{ module }}

</div>

<div class="autoclass" data-members="" data-inherited-members="" data-special-members="__call__">

{{ objname }}

</div>

<div class="minigallery" data-add-heading="Gallery examples" data-heading-level="-">

{{ module }}.{{ objname }} {% for meth in methods %}{{ module }}.{{ objname }}.{{ meth }} {% endfor %}

</div>

{%- else -%}

<div class="currentmodule">

{{ module }}

</div>

{%- endif -%}

---

numpydoc_docstring.md

---

{{index}} {{summary}} {{extended\_summary}} {{parameters}} {{returns}} {{yields}} {{other\_parameters}} {{attributes}} {{raises}} {{warns}} {{warnings}} {{see\_also}} {{notes}} {{references}} {{examples}} {{methods}}

---

testimonials.md

---

- orphan

<div class="title">

Testimonials

</div>

# Who is using scikit-learn?

## [J.P.Morgan](https://www.jpmorgan.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Scikit-learn is an indispensable part of the Python machine learning toolkit at JPMorgan. It is very widely used across all parts of the bank for classification, predictive analytics, and very many other machine learning tasks. Its straightforward API, its breadth of algorithms, and the quality of its documentation combine to make scikit-learn simultaneously very approachable and very powerful.

<div class="rst-class">

annotation

Stephen Simmons, VP, Athena Research, JPMorgan

</div>

</div>

<div class="div">

image-box

[![image](images/jpmorgan.png)](https://www.jpmorgan.com)

</div>

</div>

## [Spotify](https://www.spotify.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Scikit-learn provides a toolbox with solid implementations of a bunch of state-of-the-art models and makes it easy to plug them into existing applications. We've been using it quite a lot for music recommendations at Spotify and I think it's the most well-designed ML package I've seen so far.

<div class="rst-class">

annotation

Erik Bernhardsson, Engineering Manager Music Discovery & Machine Learning, Spotify

</div>

</div>

<div class="div">

image-box

[![image](images/spotify.png)](https://www.spotify.com)

</div>

</div>

## [Inria](https://www.inria.fr/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At INRIA, we use scikit-learn to support leading-edge basic research in many teams: [Parietal](https://team.inria.fr/parietal/) for neuroimaging, [Lear](https://lear.inrialpes.fr/) for computer vision, [Visages](https://team.inria.fr/visages/) for medical image analysis, [Privatics](https://team.inria.fr/privatics) for security. The project is a fantastic tool to address difficult applications of machine learning in an academic environment as it is performant and versatile, but all easy-to-use and well documented, which makes it well suited to grad students.

<div class="rst-class">

annotation

Gaël Varoquaux, research at Parietal

</div>

</div>

<div class="div">

image-box

[![image](images/inria.png)](https://www.inria.fr/)

</div>

</div>

## [betaworks](https://betaworks.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Betaworks is a NYC-based startup studio that builds new products, grows companies, and invests in others. Over the past 8 years we've launched a handful of social data analytics-driven services, such as Bitly, Chartbeat, digg and Scale Model. Consistently the betaworks data science team uses Scikit-learn for a variety of tasks. From exploratory analysis, to product development, it is an essential part of our toolkit. Recent uses are included in [digg's new video recommender system](https://medium.com/i-data/the-digg-video-recommender-2f9ade7c4ba3), and Poncho's [dynamic heuristic subspace clustering](https://medium.com/@DiggData/scaling-poncho-using-data-ca24569d56fd).

<div class="rst-class">

annotation

Gilad Lotan, Chief Data Scientist

</div>

</div>

<div class="div">

image-box

[![image](images/betaworks.png)](https://betaworks.com)

</div>

</div>

## [Hugging Face](https://huggingface.co)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At Hugging Face we're using NLP and probabilistic models to generate conversational Artificial intelligences that are fun to chat with. Despite using deep neural nets for [a few](https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983) of our [NLP tasks](https://huggingface.co/coref/), scikit-learn is still the bread-and-butter of our daily machine learning routine. The ease of use and predictability of the interface, as well as the straightforward mathematical explanations that are here when you need them, is the killer feature. We use a variety of scikit-learn models in production and they are also operationally very pleasant to work with.

<div class="rst-class">

annotation

Julien Chaumond, Chief Technology Officer

</div>

</div>

<div class="div">

image-box

[![image](images/huggingface.png)](https://huggingface.co)

</div>

</div>

## [Evernote](https://evernote.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Building a classifier is typically an iterative process of exploring the data, selecting the features (the attributes of the data believed to be predictive in some way), training the models, and finally evaluating them. For many of these tasks, we relied on the excellent scikit-learn package for Python.

[Read more](http://blog.evernote.com/tech/2013/01/22/stay-classified/)

<div class="rst-class">

annotation

Mark Ayzenshtat, VP, Augmented Intelligence

</div>

</div>

<div class="div">

image-box

[![image](images/evernote.png)](https://evernote.com)

</div>

</div>

## [Télécom ParisTech](https://www.telecom-paristech.fr/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At Telecom ParisTech, scikit-learn is used for hands-on sessions and home assignments in introductory and advanced machine learning courses. The classes are for undergrads and masters students. The great benefit of scikit-learn is its fast learning curve that allows students to quickly start working on interesting and motivating problems.

<div class="rst-class">

annotation

Alexandre Gramfort, Assistant Professor

</div>

</div>

<div class="div">

image-box

[![image](images/telecomparistech.jpg)](https://www.telecom-paristech.fr/)

</div>

</div>

## [Booking.com](https://www.booking.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At Booking.com, we use machine learning algorithms for many different applications, such as recommending hotels and destinations to our customers, detecting fraudulent reservations, or scheduling our customer service agents. Scikit-learn is one of the tools we use when implementing standard algorithms for prediction tasks. Its API and documentations are excellent and make it easy to use. The scikit-learn developers do a great job of incorporating state of the art implementations and new algorithms into the package. Thus, scikit-learn provides convenient access to a wide spectrum of algorithms, and allows us to readily find the right tool for the right job.

<div class="rst-class">

annotation

Melanie Mueller, Data Scientist

</div>

</div>

<div class="div">

image-box

[![image](images/booking.png)](https://www.booking.com)

</div>

</div>

## [AWeber](https://www.aweber.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

The scikit-learn toolkit is indispensable for the Data Analysis and Management team at AWeber. It allows us to do AWesome stuff we would not otherwise have the time or resources to accomplish. The documentation is excellent, allowing new engineers to quickly evaluate and apply many different algorithms to our data. The text feature extraction utilities are useful when working with the large volume of email content we have at AWeber. The RandomizedPCA implementation, along with Pipelining and FeatureUnions, allows us to develop complex machine learning algorithms efficiently and reliably.

Anyone interested in learning more about how AWeber deploys scikit-learn in a production environment should check out talks from PyData Boston by AWeber's Michael Becker available at <https://github.com/mdbecker/pydata_2013>.

<div class="rst-class">

annotation

Michael Becker, Software Engineer, Data Analysis and Management Ninjas

</div>

</div>

<div class="div">

image-box

[![image](images/aweber.png)](https://www.aweber.com)

</div>

</div>

## [Yhat](https://www.yhat.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

The combination of consistent APIs, thorough documentation, and top notch implementation make scikit-learn our favorite machine learning package in Python. scikit-learn makes doing advanced analysis in Python accessible to anyone. At Yhat, we make it easy to integrate these models into your production applications. Thus eliminating the unnecessary dev time encountered productionizing analytical work.

<div class="rst-class">

annotation

Greg Lamp, Co-founder

</div>

</div>

<div class="div">

image-box

[![image](images/yhat.png)](https://www.yhat.com)

</div>

</div>

## [Rangespan](http://www.rangespan.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

The Python scikit-learn toolkit is a core tool in the data science group at Rangespan. Its large collection of well documented models and algorithms allow our team of data scientists to prototype fast and quickly iterate to find the right solution to our learning problems. We find that scikit-learn is not only the right tool for prototyping, but its careful and well tested implementation give us the confidence to run scikit-learn models in production.

<div class="rst-class">

annotation

Jurgen Van Gael, Data Science Director

</div>

</div>

<div class="div">

image-box

[![image](images/rangespan.png)](http://www.rangespan.com)

</div>

</div>

## [Birchbox](https://www.birchbox.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At Birchbox, we face a range of machine learning problems typical to E-commerce: product recommendation, user clustering, inventory prediction, trends detection, etc. Scikit-learn lets us experiment with many models, especially in the exploration phase of a new project: the data can be passed around in a consistent way; models are easy to save and reuse; updates keep us informed of new developments from the pattern discovery research community. Scikit-learn is an important tool for our team, built the right way in the right language.

<div class="rst-class">

annotation

Thierry Bertin-Mahieux, Data Scientist

</div>

</div>

<div class="div">

image-box

[![image](images/birchbox.jpg)](https://www.birchbox.com)

</div>

</div>

## [Bestofmedia Group](http://www.bestofmedia.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Scikit-learn is our \#1 toolkit for all things machine learning at Bestofmedia. We use it for a variety of tasks (e.g. spam fighting, ad click prediction, various ranking models) thanks to the varied, state-of-the-art algorithm implementations packaged into it. In the lab it accelerates prototyping of complex pipelines. In production I can say it has proven to be robust and efficient enough to be deployed for business critical components.

<div class="rst-class">

annotation

Eustache Diemert, Lead Scientist

</div>

</div>

<div class="div">

image-box

[![image](images/bestofmedia-logo.png)](http://www.bestofmedia.com)

</div>

</div>

## [Change.org](https://www.change.org)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At change.org we automate the use of scikit-learn's RandomForestClassifier in our production systems to drive email targeting that reaches millions of users across the world each week. In the lab, scikit-learn's ease-of-use, performance, and overall variety of algorithms implemented has proved invaluable in giving us a single reliable source to turn to for our machine-learning needs.

<div class="rst-class">

annotation

Vijay Ramesh, Software Engineer in Data/science at Change.org

</div>

</div>

<div class="div">

image-box

[![image](images/change-logo.png)](https://www.change.org)

</div>

</div>

## [PHIMECA Engineering](https://www.phimeca.com/?lang=en)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At PHIMECA Engineering, we use scikit-learn estimators as surrogates for expensive-to-evaluate numerical models (mostly but not exclusively finite-element mechanical models) for speeding up the intensive post-processing operations involved in our simulation-based decision making framework. Scikit-learn's fit/predict API together with its efficient cross-validation tools considerably eases the task of selecting the best-fit estimator. We are also using scikit-learn for illustrating concepts in our training sessions. Trainees are always impressed by the ease-of-use of scikit-learn despite the apparent theoretical complexity of machine learning.

<div class="rst-class">

annotation

Vincent Dubourg, PHIMECA Engineering, PhD Engineer

</div>

</div>

<div class="div">

image-box

[![image](images/phimeca.png)](https://www.phimeca.com/?lang=en)

</div>

</div>

## [HowAboutWe](http://www.howaboutwe.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At HowAboutWe, scikit-learn lets us implement a wide array of machine learning techniques in analysis and in production, despite having a small team. We use scikit-learn's classification algorithms to predict user behavior, enabling us to (for example) estimate the value of leads from a given traffic source early in the lead's tenure on our site. Also, our users' profiles consist of primarily unstructured data (answers to open-ended questions), so we use scikit-learn's feature extraction and dimensionality reduction tools to translate these unstructured data into inputs for our matchmaking system.

<div class="rst-class">

annotation

Daniel Weitzenfeld, Senior Data Scientist at HowAboutWe

</div>

</div>

<div class="div">

image-box

[![image](images/howaboutwe.png)](http://www.howaboutwe.com/)

</div>

</div>

## [PeerIndex](https://www.brandwatch.com/peerindex-and-brandwatch)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At PeerIndex we use scientific methodology to build the Influence Graph - a unique dataset that allows us to identify who's really influential and in which context. To do this, we have to tackle a range of machine learning and predictive modeling problems. Scikit-learn has emerged as our primary tool for developing prototypes and making quick progress. From predicting missing data and classifying tweets to clustering communities of social media users, scikit-learn proved useful in a variety of applications. Its very intuitive interface and excellent compatibility with other python tools makes it and indispensable tool in our daily research efforts.

<div class="rst-class">

annotation

Ferenc Huszar, Senior Data Scientist at Peerindex

</div>

</div>

<div class="div">

image-box

[![image](images/peerindex.png)](https://www.brandwatch.com/peerindex-and-brandwatch)

</div>

</div>

## [DataRobot](https://www.datarobot.com)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

DataRobot is building next generation predictive analytics software to make data scientists more productive, and scikit-learn is an integral part of our system. The variety of machine learning techniques in combination with the solid implementations that scikit-learn offers makes it a one-stop-shopping library for machine learning in Python. Moreover, its consistent API, well-tested code and permissive licensing allow us to use it in a production environment. Scikit-learn has literally saved us years of work we would have had to do ourselves to bring our product to market.

<div class="rst-class">

annotation

Jeremy Achin, CEO & Co-founder DataRobot Inc.

</div>

</div>

<div class="div">

image-box

[![image](images/datarobot.png)](https://www.datarobot.com)

</div>

</div>

## [OkCupid](https://www.okcupid.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

We're using scikit-learn at OkCupid to evaluate and improve our matchmaking system. The range of features it has, especially preprocessing utilities, means we can use it for a wide variety of projects, and it's performant enough to handle the volume of data that we need to sort through. The documentation is really thorough, as well, which makes the library quite easy to use.

<div class="rst-class">

annotation

David Koh - Senior Data Scientist at OkCupid

</div>

</div>

<div class="div">

image-box

[![image](images/okcupid.png)](https://www.okcupid.com)

</div>

</div>

## [Lovely](https://livelovely.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At Lovely, we strive to deliver the best apartment marketplace, with respect to our users and our listings. From understanding user behavior, improving data quality, and detecting fraud, scikit-learn is a regular tool for gathering insights, predictive modeling and improving our product. The easy-to-read documentation and intuitive architecture of the API makes machine learning both explorable and accessible to a wide range of python developers. I'm constantly recommending that more developers and scientists try scikit-learn.

<div class="rst-class">

annotation

Simon Frid - Data Scientist, Lead at Lovely

</div>

</div>

<div class="div">

image-box

[![image](images/lovely.png)](https://livelovely.com)

</div>

</div>

## [Data Publica](http://www.data-publica.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Data Publica builds a new predictive sales tool for commercial and marketing teams called C-Radar. We extensively use scikit-learn to build segmentations of customers through clustering, and to predict future customers based on past partnerships success or failure. We also categorize companies using their website communication thanks to scikit-learn and its machine learning algorithm implementations. Eventually, machine learning makes it possible to detect weak signals that traditional tools cannot see. All these complex tasks are performed in an easy and straightforward way thanks to the great quality of the scikit-learn framework.

<div class="rst-class">

annotation

Guillaume Lebourgeois & Samuel Charron - Data Scientists at Data Publica

</div>

</div>

<div class="div">

image-box

[![image](images/datapublica.png)](http://www.data-publica.com/)

</div>

</div>

## [Machinalis](https://www.machinalis.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Scikit-learn is the cornerstone of all the machine learning projects carried at Machinalis. It has a consistent API, a wide selection of algorithms and lots of auxiliary tools to deal with the boilerplate. We have used it in production environments on a variety of projects including click-through rate prediction, [information extraction](https://github.com/machinalis/iepy), and even counting sheep\!

In fact, we use it so much that we've started to freeze our common use cases into Python packages, some of them open-sourced, like [FeatureForge](https://github.com/machinalis/featureforge). Scikit-learn in one word: Awesome.

<div class="rst-class">

annotation

Rafael Carrascosa, Lead developer

</div>

</div>

<div class="div">

image-box

[![image](images/machinalis.png)](https://www.machinalis.com/)

</div>

</div>

## [solido](https://www.solidodesign.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Scikit-learn is helping to drive Moore's Law, via Solido. Solido creates computer-aided design tools used by the majority of top-20 semiconductor companies and fabs, to design the bleeding-edge chips inside smartphones, automobiles, and more. Scikit-learn helps to power Solido's algorithms for rare-event estimation, worst-case verification, optimization, and more. At Solido, we are particularly fond of scikit-learn's libraries for Gaussian Process models, large-scale regularized linear regression, and classification. Scikit-learn has increased our productivity, because for many ML problems we no longer need to “roll our own” code. [This PyData 2014 talk](https://www.youtube.com/watch?v=Jm-eBD9xR3w) has details.

<div class="rst-class">

annotation

Trent McConaghy, founder, Solido Design Automation Inc.

</div>

</div>

<div class="div">

image-box

[![image](images/solido_logo.png)](https://www.solidodesign.com/)

</div>

</div>

## [INFONEA](http://www.infonea.com/en/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

We employ scikit-learn for rapid prototyping and custom-made Data Science solutions within our in-memory based Business Intelligence Software INFONEA®. As a well-documented and comprehensive collection of state-of-the-art algorithms and pipelining methods, scikit-learn enables us to provide flexible and scalable scientific analysis solutions. Thus, scikit-learn is immensely valuable in realizing a powerful integration of Data Science technology within self-service business analytics.

<div class="rst-class">

annotation

Thorsten Kranz, Data Scientist, Coma Soft AG.

</div>

</div>

<div class="div">

image-box

[![image](images/infonea.jpg)](http://www.infonea.com/en/)

</div>

</div>

## [Dataiku](https://www.dataiku.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Our software, Data Science Studio (DSS), enables users to create data services that combine [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) with Machine Learning. Our Machine Learning module integrates many scikit-learn algorithms. The scikit-learn library is a perfect integration with DSS because it offers algorithms for virtually all business cases. Our goal is to offer a transparent and flexible tool that makes it easier to optimize time consuming aspects of building a data service, preparing data, and training machine learning algorithms on all types of data.

<div class="rst-class">

annotation

Florian Douetteau, CEO, Dataiku

</div>

</div>

<div class="div">

image-box

[![image](images/dataiku_logo.png)](https://www.dataiku.com/)

</div>

</div>

## [Otto Group](https://ottogroup.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Here at Otto Group, one of global Big Five B2C online retailers, we are using scikit-learn in all aspects of our daily work from data exploration to development of machine learning application to the productive deployment of those services. It helps us to tackle machine learning problems ranging from e-commerce to logistics. It consistent APIs enabled us to build the [Palladium REST-API framework](https://github.com/ottogroup/palladium/) around it and continuously deliver scikit-learn based services.

<div class="rst-class">

annotation

Christian Rammig, Head of Data Science, Otto Group

</div>

</div>

<div class="div">

image-box

[![image](images/ottogroup_logo.png)](https://ottogroup.com)

</div>

</div>

## [Zopa](https://zopa.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

At Zopa, the first ever Peer-to-Peer lending platform, we extensively use scikit-learn to run the business and optimize our users' experience. It powers our Machine Learning models involved in credit risk, fraud risk, marketing, and pricing, and has been used for originating at least 1 billion GBP worth of Zopa loans. It is very well documented, powerful, and simple to use. We are grateful for the capabilities it has provided, and for allowing us to deliver on our mission of making money simple and fair.

<div class="rst-class">

annotation

Vlasios Vasileiou, Head of Data Science, Zopa

</div>

</div>

<div class="div">

image-box

[![image](images/zopa.png)](https://zopa.com)

</div>

</div>

## [MARS](https://www.mars.com/global)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

Scikit-Learn is integral to the Machine Learning Ecosystem at Mars. Whether we're designing better recipes for petfood or closely analysing our cocoa supply chain, Scikit-Learn is used as a tool for rapidly prototyping ideas and taking them to production. This allows us to better understand and meet the needs of our consumers worldwide. Scikit-Learn's feature-rich toolset is easy to use and equips our associates with the capabilities they need to solve the business challenges they face every day.

<div class="rst-class">

annotation

Michael Fitzke, Next Generation Technologies Sr Leader, Mars Inc.

</div>

</div>

<div class="div">

image-box

[![image](images/mars.png)](https://www.mars.com/global)

</div>

</div>

## [BNP Paribas Cardif](https://www.bnpparibascardif.com/)

<div class="div">

sk-text-image-grid-large

<div class="div">

text-box

BNP Paribas Cardif uses scikit-learn for several of its machine learning models in production. Our internal community of developers and data scientists has been using scikit-learn since 2015, for several reasons: the quality of the developments, documentation and contribution governance, and the sheer size of the contributing community. We even explicitly mention the use of scikit-learn's pipelines in our internal model risk governance as one of our good practices to decrease operational risks and overfitting risk. As a way to support open source software development and in particular scikit-learn project, we decided to participate to scikit-learn's consortium at La Fondation Inria since its creation in 2018.

<div class="rst-class">

annotation

Sébastien Conort, Chief Data Scientist, BNP Paribas Cardif

</div>

</div>

<div class="div">

image-box

[![image](images/bnp_paribas_cardif.png)](https://www.bnpparibascardif.com/)

</div>

</div>

---

unsupervised_learning.md

---

# Unsupervised learning

<div class="toctree" data-maxdepth="2">

modules/mixture modules/manifold modules/clustering modules/biclustering modules/decomposition modules/covariance modules/outlier\_detection modules/density modules/neural\_networks\_unsupervised

</div>

---

user_guide.md

---

# User Guide

<div class="toctree" data-numbered="" data-maxdepth="3">

supervised\_learning.rst unsupervised\_learning.rst model\_selection.rst inspection.rst visualizations.rst data\_transforms.rst datasets.rst computing.rst model\_persistence.rst common\_pitfalls.rst dispatching.rst machine\_learning\_map.rst presentations.rst

</div>

## Under Development

<div class="toctree" data-numbered="" data-maxdepth="1">

metadata\_routing.rst

</div>

---

visualizations.md

---

# Visualizations

Scikit-learn defines a simple API for creating visualizations for machine learning. The key feature of this API is to allow for quick plotting and visual adjustments without recalculation. We provide <span class="title-ref">Display</span> classes that expose two methods for creating plots: <span class="title-ref">from\_estimator</span> and <span class="title-ref">from\_predictions</span>. The <span class="title-ref">from\_estimator</span> method will take a fitted estimator and some data (<span class="title-ref">X</span> and <span class="title-ref">y</span>) and create a <span class="title-ref">Display</span> object. Sometimes, we would like to only compute the predictions once and one should use <span class="title-ref">from\_predictions</span> instead. In the following example, we plot a ROC curve for a fitted support vector machine:

<div class="plot" data-context="close-figs" data-align="center">

from sklearn.model\_selection import train\_test\_split from sklearn.svm import SVC from sklearn.metrics import RocCurveDisplay from sklearn.datasets import load\_wine

X, y = load\_wine(return\_X\_y=True) y = y == 2 \# make binary X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, random\_state=42) svc = SVC(random\_state=42) svc.fit(X\_train, y\_train)

svc\_disp = RocCurveDisplay.from\_estimator(svc, X\_test, y\_test)

</div>

The returned <span class="title-ref">svc\_disp</span> object allows us to continue using the already computed ROC curve for SVC in future plots. In this case, the <span class="title-ref">svc\_disp</span> is a <span class="title-ref">\~sklearn.metrics.RocCurveDisplay</span> that stores the computed values as attributes called <span class="title-ref">roc\_auc</span>, <span class="title-ref">fpr</span>, and <span class="title-ref">tpr</span>. Be aware that we could get the predictions from the support vector machine and then use <span class="title-ref">from\_predictions</span> instead of <span class="title-ref">from\_estimator</span>. Next, we train a random forest classifier and plot the previously computed roc curve again by using the <span class="title-ref">plot</span> method of the <span class="title-ref">Display</span> object.

<div class="plot" data-context="close-figs" data-align="center">

import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n\_estimators=10, random\_state=42) rfc.fit(X\_train, y\_train)

ax = plt.gca() rfc\_disp = RocCurveDisplay.from\_estimator(rfc, X\_test, y\_test, ax=ax, alpha=0.8) svc\_disp.plot(ax=ax, alpha=0.8)

</div>

Notice that we pass <span class="title-ref">alpha=0.8</span> to the plot functions to adjust the alpha values of the curves.

**Examples**

  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_roc\_curve\_visualization\_api.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_roc\_curve\_visualization\_api.py)
  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_partial\_dependence\_visualization\_api.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_partial\_dependence\_visualization\_api.py)
  - \[sphx\_glr\_auto\_examples\_miscellaneous\_plot\_display\_object\_visualization.py\](\#sphx\_glr\_auto\_examples\_miscellaneous\_plot\_display\_object\_visualization.py)
  - \[sphx\_glr\_auto\_examples\_calibration\_plot\_compare\_calibration.py\](\#sphx\_glr\_auto\_examples\_calibration\_plot\_compare\_calibration.py)

## Available Plotting Utilities

### Display Objects

<div class="currentmodule">

sklearn

</div>

<div class="autosummary">

calibration.CalibrationDisplay inspection.PartialDependenceDisplay inspection.DecisionBoundaryDisplay metrics.ConfusionMatrixDisplay metrics.DetCurveDisplay metrics.PrecisionRecallDisplay metrics.PredictionErrorDisplay metrics.RocCurveDisplay model\_selection.LearningCurveDisplay model\_selection.ValidationCurveDisplay

</div>

---

older_versions.md

---

<div class="currentmodule">

sklearn

</div>

# Older Versions

## Version 0.12.1

**October 8, 2012**

The 0.12.1 release is a bug-fix release with no additional features, but is instead a set of bug fixes

### Changelog

  - Improved numerical stability in spectral embedding by [Gael Varoquaux](http://gael-varoquaux.info)
  - Doctest under windows 64bit by [Gael Varoquaux](http://gael-varoquaux.info)
  - Documentation fixes for elastic net by [Andreas Müller](https://amueller.github.io/) and [Alexandre Gramfort](http://alexandre.gramfort.net)
  - Proper behavior with fortran-ordered NumPy arrays by [Gael Varoquaux](http://gael-varoquaux.info)
  - Make GridSearchCV work with non-CSR sparse matrix by [Lars Buitinck](https://github.com/larsmans)
  - Fix parallel computing in MDS by [Gael Varoquaux](http://gael-varoquaux.info)
  - Fix Unicode support in count vectorizer by [Andreas Müller](https://amueller.github.io/)
  - Fix MinCovDet breaking with X.shape = (3, 1) by `Virgile Fritsch <VirgileFritsch>`
  - Fix clone of SGD objects by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - Stabilize GMM by `Virgile Fritsch <VirgileFritsch>`

### People

  - 14 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 12 [Gael Varoquaux](http://gael-varoquaux.info)
  - 10 [Andreas Müller](https://amueller.github.io/)
  - 5 [Lars Buitinck](https://github.com/larsmans)
  - 3 `Virgile Fritsch <VirgileFritsch>`
  - 1 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 1 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 1 [Mathieu Blondel](http://www.mblondel.org)

## Version 0.12

**September 4, 2012**

### Changelog

  - Various speed improvements of the \[decision trees \<tree\>\](\#decision-trees-\<tree\>) module, by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - <span class="title-ref">\~ensemble.GradientBoostingRegressor</span> and <span class="title-ref">\~ensemble.GradientBoostingClassifier</span> now support feature subsampling via the `max_features` argument, by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Added Huber and Quantile loss functions to <span class="title-ref">\~ensemble.GradientBoostingRegressor</span>, by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - \[Decision trees \<tree\>\](\#decision-trees-\<tree\>) and \[forests of randomized trees \<forest\>\](\#forests-of-randomized-trees-\<forest\>) now support multi-output classification and regression problems, by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Added <span class="title-ref">\~preprocessing.LabelEncoder</span>, a simple utility class to normalize labels or transform non-numerical labels, by [Mathieu Blondel](http://www.mblondel.org).
  - Added the epsilon-insensitive loss and the ability to make probabilistic predictions with the modified huber loss in \[sgd\](\#sgd), by [Mathieu Blondel](http://www.mblondel.org).
  - Added \[multidimensional\_scaling\](\#multidimensional\_scaling), by Nelle Varoquaux.
  - SVMlight file format loader now detects compressed (gzip/bzip2) files and decompresses them on the fly, by [Lars Buitinck](https://github.com/larsmans).
  - SVMlight file format serializer now preserves double precision floating point values, by [Olivier Grisel](https://twitter.com/ogrisel).
  - A common testing framework for all estimators was added, by [Andreas Müller](https://amueller.github.io/).
  - Understandable error messages for estimators that do not accept sparse input by [Gael Varoquaux](http://gael-varoquaux.info)
  - Speedups in hierarchical clustering by [Gael Varoquaux](http://gael-varoquaux.info). In particular building the tree now supports early stopping. This is useful when the number of clusters is not small compared to the number of samples.
  - Add MultiTaskLasso and MultiTaskElasticNet for joint feature selection, by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Added <span class="title-ref">metrics.auc\_score</span> and <span class="title-ref">metrics.average\_precision\_score</span> convenience functions by [Andreas Müller](https://amueller.github.io/).
  - Improved sparse matrix support in the \[feature\_selection\](\#feature\_selection) module by [Andreas Müller](https://amueller.github.io/).
  - New word boundaries-aware character n-gram analyzer for the \[text\_feature\_extraction\](\#text\_feature\_extraction) module by `@kernc <kernc>`.
  - Fixed bug in spectral clustering that led to single point clusters by [Andreas Müller](https://amueller.github.io/).
  - In <span class="title-ref">\~feature\_extraction.text.CountVectorizer</span>, added an option to ignore infrequent words, `min_df` by [Andreas Müller](https://amueller.github.io/).
  - Add support for multiple targets in some linear models (ElasticNet, Lasso and OrthogonalMatchingPursuit) by [Vlad Niculae](https://vene.ro/) and [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Fixes in <span class="title-ref">decomposition.ProbabilisticPCA</span> score function by Wei Li.
  - Fixed feature importance computation in \[gradient\_boosting\](\#gradient\_boosting).

### API changes summary

  - The old `scikits.learn` package has disappeared; all code should import from `sklearn` instead, which was introduced in 0.9.
  - In <span class="title-ref">metrics.roc\_curve</span>, the `thresholds` array is now returned with it's order reversed, in order to keep it consistent with the order of the returned `fpr` and `tpr`.
  - In <span class="title-ref">hmm</span> objects, like <span class="title-ref">hmm.GaussianHMM</span>, <span class="title-ref">hmm.MultinomialHMM</span>, etc., all parameters must be passed to the object when initialising it and not through `fit`. Now `fit` will only accept the data as an input parameter.
  - For all SVM classes, a faulty behavior of `gamma` was fixed. Previously, the default gamma value was only computed the first time `fit` was called and then stored. It is now recalculated on every call to `fit`.
  - All `Base` classes are now abstract meta classes so that they can not be instantiated.
  - <span class="title-ref">cluster.ward\_tree</span> now also returns the parent array. This is necessary for early-stopping in which case the tree is not completely built.
  - In <span class="title-ref">\~feature\_extraction.text.CountVectorizer</span> the parameters `min_n` and `max_n` were joined to the parameter `n_gram_range` to enable grid-searching both at once.
  - In <span class="title-ref">\~feature\_extraction.text.CountVectorizer</span>, words that appear only in one document are now ignored by default. To reproduce the previous behavior, set `min_df=1`.
  - Fixed API inconsistency: <span class="title-ref">linear\_model.SGDClassifier.predict\_proba</span> now returns 2d array when fit on two classes.
  - Fixed API inconsistency: <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis.decision\_function</span> and <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis.decision\_function</span> now return 1d arrays when fit on two classes.
  - Grid of alphas used for fitting <span class="title-ref">\~linear\_model.LassoCV</span> and <span class="title-ref">\~linear\_model.ElasticNetCV</span> is now stored in the attribute `alphas_` rather than overriding the init parameter `alphas`.
  - Linear models when alpha is estimated by cross-validation store the estimated value in the `alpha_` attribute rather than just `alpha` or `best_alpha`.
  - <span class="title-ref">\~ensemble.GradientBoostingClassifier</span> now supports <span class="title-ref">\~ensemble.GradientBoostingClassifier.staged\_predict\_proba</span>, and <span class="title-ref">\~ensemble.GradientBoostingClassifier.staged\_predict</span>.
  - <span class="title-ref">svm.sparse.SVC</span> and other sparse SVM classes are now deprecated. The all classes in the \[svm\](\#svm) module now automatically select the sparse or dense representation base on the input.
  - All clustering algorithms now interpret the array `X` given to `fit` as input data, in particular <span class="title-ref">\~cluster.SpectralClustering</span> and <span class="title-ref">\~cluster.AffinityPropagation</span> which previously expected affinity matrices.
  - For clustering algorithms that take the desired number of clusters as a parameter, this parameter is now called `n_clusters`.

### People

  - 267 [Andreas Müller](https://amueller.github.io/)
  - 94 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 89 [Gael Varoquaux](http://gael-varoquaux.info)
  - 79 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 60 [Mathieu Blondel](http://www.mblondel.org)
  - 57 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 52 [Vlad Niculae](https://vene.ro/)
  - 45 [Lars Buitinck](https://github.com/larsmans)
  - 44 Nelle Varoquaux
  - 37 [Jaques Grobler](https://github.com/jaquesgrobler)
  - 30 Alexis Mignon
  - 30 Immanuel Bayer
  - 27 [Olivier Grisel](https://twitter.com/ogrisel)
  - 16 Subhodeep Moitra
  - 13 Yannick Schwartz
  - 12 `@kernc <kernc>`
  - 11 `Virgile Fritsch <VirgileFritsch>`
  - 9 Daniel Duckworth
  - 9 [Fabian Pedregosa](http://fa.bianp.net)
  - 9 [Robert Layton](https://twitter.com/robertlayton)
  - 8 John Benediktsson
  - 7 Marko Burjek
  - 5 [Nicolas Pinto](https://twitter.com/npinto)
  - 4 Alexandre Abraham
  - 4 [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - 3 [Brian Holt](http://personal.ee.surrey.ac.uk/Personal/B.Holt)
  - 3 [Edouard Duchesnay](https://duchesnay.github.io/)
  - 3 Florian Hoenig
  - 3 flyingimmidev
  - 2 Francois Savard
  - 2 Hannes Schulz
  - 2 Peter Welinder
  - 2 [Yaroslav Halchenko](http://www.onerussian.com/)
  - 2 Wei Li
  - 1 Alex Companioni
  - 1 Brandyn A. White
  - 1 Bussonnier Matthias
  - 1 Charles-Pierre Astolfi
  - 1 Dan O'Huiginn
  - 1 David Cournapeau
  - 1 Keith Goodman
  - 1 Ludwig Schwardt
  - 1 Olivier Hervieu
  - 1 Sergio Medina
  - 1 Shiqiao Du
  - 1 Tim Sheerman-Chase
  - 1 buguen

## Version 0.11

**May 7, 2012**

### Changelog

#### Highlights

  - Gradient boosted regression trees (\[gradient\_boosting\](\#gradient\_boosting)) for classification and regression by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/) and [Scott White](https://twitter.com/scottblanc) .
  - Simple dict-based feature loader with support for categorical variables (<span class="title-ref">\~feature\_extraction.DictVectorizer</span>) by [Lars Buitinck](https://github.com/larsmans).
  - Added Matthews correlation coefficient (<span class="title-ref">metrics.matthews\_corrcoef</span>) and added macro and micro average options to <span class="title-ref">\~metrics.precision\_score</span>, <span class="title-ref">metrics.recall\_score</span> and <span class="title-ref">\~metrics.f1\_score</span> by [Satrajit Ghosh](https://www.mit.edu/~satra/).
  - \[out\_of\_bag\](\#out\_of\_bag) of generalization error for \[ensemble\](\#ensemble) by [Andreas Müller](https://amueller.github.io/).
  - Randomized sparse linear models for feature selection, by [Alexandre Gramfort](http://alexandre.gramfort.net) and [Gael Varoquaux](http://gael-varoquaux.info)
  - \[label\_propagation\](\#label\_propagation) for semi-supervised learning, by Clay Woolam. **Note** the semi-supervised API is still work in progress, and may change.
  - Added BIC/AIC model selection to classical \[gmm\](\#gmm) and unified the API with the remainder of scikit-learn, by [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page)
  - Added <span class="title-ref">sklearn.cross\_validation.StratifiedShuffleSplit</span>, which is a <span class="title-ref">sklearn.cross\_validation.ShuffleSplit</span> with balanced splits, by Yannick Schwartz.
  - <span class="title-ref">\~sklearn.neighbors.NearestCentroid</span> classifier added, along with a `shrink_threshold` parameter, which implements **shrunken centroid classification**, by [Robert Layton](https://twitter.com/robertlayton).

#### Other changes

  - Merged dense and sparse implementations of \[sgd\](\#sgd) module and exposed utility extension types for sequential datasets `seq_dataset` and weight vectors `weight_vector` by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Added `partial_fit` (support for online/minibatch learning) and warm\_start to the \[sgd\](\#sgd) module by [Mathieu Blondel](http://www.mblondel.org).
  - Dense and sparse implementations of \[svm\](\#svm) classes and <span class="title-ref">\~linear\_model.LogisticRegression</span> merged by [Lars Buitinck](https://github.com/larsmans).
  - Regressors can now be used as base estimator in the \[multiclass\](\#multiclass) module by [Mathieu Blondel](http://www.mblondel.org).
  - Added n\_jobs option to <span class="title-ref">metrics.pairwise\_distances</span> and <span class="title-ref">metrics.pairwise.pairwise\_kernels</span> for parallel computation, by [Mathieu Blondel](http://www.mblondel.org).
  - \[k\_means\](\#k\_means) can now be run in parallel, using the `n_jobs` argument to either \[k\_means\](\#k\_means) or <span class="title-ref">cluster.KMeans</span>, by [Robert Layton](https://twitter.com/robertlayton).
  - Improved \[cross\_validation\](\#cross\_validation) and \[grid\_search\](\#grid\_search) documentation and introduced the new <span class="title-ref">cross\_validation.train\_test\_split</span> helper function by [Olivier Grisel](https://twitter.com/ogrisel)
  - <span class="title-ref">\~svm.SVC</span> members `coef_` and `intercept_` changed sign for consistency with `decision_function`; for `kernel==linear`, `coef_` was fixed in the one-vs-one case, by [Andreas Müller](https://amueller.github.io/).
  - Performance improvements to efficient leave-one-out cross-validated Ridge regression, esp. for the `n_samples > n_features` case, in <span class="title-ref">\~linear\_model.RidgeCV</span>, by Reuben Fletcher-Costin.
  - Refactoring and simplification of the \[text\_feature\_extraction\](\#text\_feature\_extraction) API and fixed a bug that caused possible negative IDF, by [Olivier Grisel](https://twitter.com/ogrisel).
  - Beam pruning option in <span class="title-ref">\_BaseHMM</span> module has been removed since it is difficult to Cythonize. If you are interested in contributing a Cython version, you can use the python version in the git history as a reference.
  - Classes in \[neighbors\](\#neighbors) now support arbitrary Minkowski metric for nearest neighbors searches. The metric can be specified by argument `p`.

### API changes summary

  - <span class="title-ref">covariance.EllipticEnvelop</span> is now deprecated. Please use <span class="title-ref">\~covariance.EllipticEnvelope</span> instead.
  - `NeighborsClassifier` and `NeighborsRegressor` are gone in the module \[neighbors\](\#neighbors). Use the classes <span class="title-ref">\~neighbors.KNeighborsClassifier</span>, <span class="title-ref">\~neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">\~neighbors.KNeighborsRegressor</span> and/or <span class="title-ref">\~neighbors.RadiusNeighborsRegressor</span> instead.
  - Sparse classes in the \[sgd\](\#sgd) module are now deprecated.
  - In <span class="title-ref">mixture.GMM</span>, <span class="title-ref">mixture.DPGMM</span> and <span class="title-ref">mixture.VBGMM</span>, parameters must be passed to an object when initialising it and not through `fit`. Now `fit` will only accept the data as an input parameter.
  - methods `rvs` and `decode` in <span class="title-ref">GMM</span> module are now deprecated. `sample` and `score` or `predict` should be used instead.
  - attribute `_scores` and `_pvalues` in univariate feature selection objects are now deprecated. `scores_` or `pvalues_` should be used instead.
  - In <span class="title-ref">\~linear\_model.LogisticRegression</span>, <span class="title-ref">\~svm.LinearSVC</span>, <span class="title-ref">\~svm.SVC</span> and <span class="title-ref">\~svm.NuSVC</span>, the `class_weight` parameter is now an initialization parameter, not a parameter to fit. This makes grid searches over this parameter possible.
  - LFW `data` is now always shape `(n_samples, n_features)` to be consistent with the Olivetti faces dataset. Use `images` and `pairs` attribute to access the natural images shapes instead.
  - In <span class="title-ref">\~svm.LinearSVC</span>, the meaning of the `multi_class` parameter changed. Options now are `'ovr'` and `'crammer_singer'`, with `'ovr'` being the default. This does not change the default behavior but hopefully is less confusing.
  - Class <span class="title-ref">feature\_selection.text.Vectorizer</span> is deprecated and replaced by <span class="title-ref">feature\_selection.text.TfidfVectorizer</span>.
  - The preprocessor / analyzer nested structure for text feature extraction has been removed. All those features are now directly passed as flat constructor arguments to <span class="title-ref">feature\_selection.text.TfidfVectorizer</span> and <span class="title-ref">feature\_selection.text.CountVectorizer</span>, in particular the following parameters are now used:
  - `analyzer` can be `'word'` or `'char'` to switch the default analysis scheme, or use a specific python callable (as previously).
  - `tokenizer` and `preprocessor` have been introduced to make it still possible to customize those steps with the new API.
  - `input` explicitly control how to interpret the sequence passed to `fit` and `predict`: filenames, file objects or direct (byte or Unicode) strings.
  - charset decoding is explicit and strict by default.
  - the `vocabulary`, fitted or not is now stored in the `vocabulary_` attribute to be consistent with the project conventions.
  - Class <span class="title-ref">feature\_selection.text.TfidfVectorizer</span> now derives directly from <span class="title-ref">feature\_selection.text.CountVectorizer</span> to make grid search trivial.
  - methods `rvs` in <span class="title-ref">\_BaseHMM</span> module are now deprecated. `sample` should be used instead.
  - Beam pruning option in <span class="title-ref">\_BaseHMM</span> module is removed since it is difficult to be Cythonized. If you are interested, you can look in the history codes by git.
  - The SVMlight format loader now supports files with both zero-based and one-based column indices, since both occur "in the wild".
  - Arguments in class <span class="title-ref">\~model\_selection.ShuffleSplit</span> are now consistent with <span class="title-ref">\~model\_selection.StratifiedShuffleSplit</span>. Arguments `test_fraction` and `train_fraction` are deprecated and renamed to `test_size` and `train_size` and can accept both `float` and `int`.
  - Arguments in class <span class="title-ref">Bootstrap</span> are now consistent with <span class="title-ref">\~model\_selection.StratifiedShuffleSplit</span>. Arguments `n_test` and `n_train` are deprecated and renamed to `test_size` and `train_size` and can accept both `float` and `int`.
  - Argument `p` added to classes in \[neighbors\](\#neighbors) to specify an arbitrary Minkowski metric for nearest neighbors searches.

### People

  - 282 [Andreas Müller](https://amueller.github.io/)
  - 239 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 198 [Gael Varoquaux](http://gael-varoquaux.info)
  - 129 [Olivier Grisel](https://twitter.com/ogrisel)
  - 114 [Mathieu Blondel](http://www.mblondel.org)
  - 103 Clay Woolam
  - 96 [Lars Buitinck](https://github.com/larsmans)
  - 88 [Jaques Grobler](https://github.com/jaquesgrobler)
  - 82 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 50 [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page)
  - 42 [Robert Layton](https://twitter.com/robertlayton)
  - 28 flyingimmidev
  - 26 [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - 26 Shiqiao Du
  - 21 [Satrajit Ghosh](https://www.mit.edu/~satra/)
  - 17 [David Marek](https://davidmarek.cz/)
  - 17 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 14 [Vlad Niculae](https://vene.ro/)
  - 11 Yannick Schwartz
  - 10 [Fabian Pedregosa](http://fa.bianp.net)
  - 9 fcostin
  - 7 Nick Wilson
  - 5 Adrien Gaidon
  - 5 [Nicolas Pinto](https://twitter.com/npinto)
  - 4 [David Warde-Farley](http://www-etud.iro.umontreal.ca/~wardefar/)
  - 5 Nelle Varoquaux
  - 5 Emmanuelle Gouillart
  - 3 Joonas Sillanpää
  - 3 Paolo Losi
  - 2 Charles McCarthy
  - 2 Roy Hyunjin Han
  - 2 Scott White
  - 2 ibayer
  - 1 Brandyn White
  - 1 Carlos Scheidegger
  - 1 Claire Revillet
  - 1 Conrad Lee
  - 1 [Edouard Duchesnay](https://duchesnay.github.io/)
  - 1 Jan Hendrik Metzen
  - 1 Meng Xinfan
  - 1 [Rob Zinkov](https://www.zinkov.com/)
  - 1 Shiqiao
  - 1 Udi Weinsberg
  - 1 Virgile Fritsch
  - 1 Xinfan Meng
  - 1 Yaroslav Halchenko
  - 1 jansoe
  - 1 Leon Palafox

## Version 0.10

**January 11, 2012**

### Changelog

  - Python 2.5 compatibility was dropped; the minimum Python version needed to use scikit-learn is now 2.6.
  - \[sparse\_inverse\_covariance\](\#sparse\_inverse\_covariance) estimation using the graph Lasso, with associated cross-validated estimator, by [Gael Varoquaux](http://gael-varoquaux.info)
  - New \[Tree \<tree\>\](\#tree-\<tree\>) module by [Brian Holt](http://personal.ee.surrey.ac.uk/Personal/B.Holt), [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/), [Satrajit Ghosh](https://www.mit.edu/~satra/) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/). The module comes with complete documentation and examples.
  - Fixed a bug in the RFE module by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/) (issue \#378).
  - Fixed a memory leak in \[svm\](\#svm) module by [Brian Holt](http://personal.ee.surrey.ac.uk/Personal/B.Holt) (issue \#367).
  - Faster tests by [Fabian Pedregosa](http://fa.bianp.net) and others.
  - Silhouette Coefficient cluster analysis evaluation metric added as <span class="title-ref">\~sklearn.metrics.silhouette\_score</span> by Robert Layton.
  - Fixed a bug in \[k\_means\](\#k\_means) in the handling of the `n_init` parameter: the clustering algorithm used to be run `n_init` times but the last solution was retained instead of the best solution by [Olivier Grisel](https://twitter.com/ogrisel).
  - Minor refactoring in \[sgd\](\#sgd) module; consolidated dense and sparse predict methods; Enhanced test time performance by converting model parameters to fortran-style arrays after fitting (only multi-class).
  - Adjusted Mutual Information metric added as <span class="title-ref">\~sklearn.metrics.adjusted\_mutual\_info\_score</span> by Robert Layton.
  - Models like SVC/SVR/LinearSVC/LogisticRegression from libsvm/liblinear now support scaling of C regularization parameter by the number of samples by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - New \[Ensemble Methods \<ensemble\>\](\#ensemble-methods-\<ensemble\>) module by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/) and [Brian Holt](http://personal.ee.surrey.ac.uk/Personal/B.Holt). The module comes with the random forest algorithm and the extra-trees method, along with documentation and examples.
  - \[outlier\_detection\](\#outlier\_detection): outlier and novelty detection, by `Virgile Fritsch <VirgileFritsch>`.
  - \[kernel\_approximation\](\#kernel\_approximation): a transform implementing kernel approximation for fast SGD on non-linear kernels by [Andreas Müller](https://amueller.github.io/).
  - Fixed a bug due to atom swapping in \[OMP\](\#omp) by [Vlad Niculae](https://vene.ro/).
  - \[SparseCoder\](\#sparsecoder) by [Vlad Niculae](https://vene.ro/).
  - \[mini\_batch\_kmeans\](\#mini\_batch\_kmeans) performance improvements by [Olivier Grisel](https://twitter.com/ogrisel).
  - \[k\_means\](\#k\_means) support for sparse matrices by [Mathieu Blondel](http://www.mblondel.org).
  - Improved documentation for developers and for the `sklearn.utils` module, by [Jake Vanderplas](https://staff.washington.edu/jakevdp/).
  - Vectorized 20newsgroups dataset loader (<span class="title-ref">\~sklearn.datasets.fetch\_20newsgroups\_vectorized</span>) by [Mathieu Blondel](http://www.mblondel.org).
  - \[multiclass\](\#multiclass) by [Lars Buitinck](https://github.com/larsmans).
  - Utilities for fast computation of mean and variance for sparse matrices by [Mathieu Blondel](http://www.mblondel.org).
  - Make <span class="title-ref">\~sklearn.preprocessing.scale</span> and <span class="title-ref">sklearn.preprocessing.Scaler</span> work on sparse matrices by [Olivier Grisel](https://twitter.com/ogrisel)
  - Feature importances using decision trees and/or forest of trees, by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Parallel implementation of forests of randomized trees by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - <span class="title-ref">sklearn.cross\_validation.ShuffleSplit</span> can subsample the train sets as well as the test sets by [Olivier Grisel](https://twitter.com/ogrisel).
  - Errors in the build of the documentation fixed by [Andreas Müller](https://amueller.github.io/).

### API changes summary

Here are the code migration instructions when upgrading from scikit-learn version 0.9:

  - Some estimators that may overwrite their inputs to save memory previously had `overwrite_` parameters; these have been replaced with `copy_` parameters with exactly the opposite meaning.
    
    This particularly affects some of the estimators in `~sklearn.linear_model`. The default behavior is still to copy everything passed in.

  - The SVMlight dataset loader <span class="title-ref">\~sklearn.datasets.load\_svmlight\_file</span> no longer supports loading two files at once; use `load_svmlight_files` instead. Also, the (unused) `buffer_mb` parameter is gone.

  - Sparse estimators in the \[sgd\](\#sgd) module use dense parameter vector `coef_` instead of `sparse_coef_`. This significantly improves test time performance.

  - The \[covariance\](\#covariance) module now has a robust estimator of covariance, the Minimum Covariance Determinant estimator.

  - Cluster evaluation metrics in `~sklearn.metrics.cluster` have been refactored but the changes are backwards compatible. They have been moved to the <span class="title-ref">metrics.cluster.supervised</span>, along with <span class="title-ref">metrics.cluster.unsupervised</span> which contains the Silhouette Coefficient.

  - The `permutation_test_score` function now behaves the same way as `cross_val_score` (i.e. uses the mean score across the folds.)

  - Cross Validation generators now use integer indices (`indices=True`) by default instead of boolean masks. This make it more intuitive to use with sparse matrix data.

  - The functions used for sparse coding, `sparse_encode` and `sparse_encode_parallel` have been combined into <span class="title-ref">\~sklearn.decomposition.sparse\_encode</span>, and the shapes of the arrays have been transposed for consistency with the matrix factorization setting, as opposed to the regression setting.

  - Fixed an off-by-one error in the SVMlight/LibSVM file format handling; files generated using <span class="title-ref">\~sklearn.datasets.dump\_svmlight\_file</span> should be re-generated. (They should continue to work, but accidentally had one extra column of zeros prepended.)

  - `BaseDictionaryLearning` class replaced by `SparseCodingMixin`.

  - <span class="title-ref">sklearn.utils.extmath.fast\_svd</span> has been renamed <span class="title-ref">\~sklearn.utils.extmath.randomized\_svd</span> and the default oversampling is now fixed to 10 additional random vectors instead of doubling the number of components to extract. The new behavior follows the reference paper.

### People

The following people contributed to scikit-learn since last release:

  - 246 [Andreas Müller](https://amueller.github.io/)
  - 242 [Olivier Grisel](https://twitter.com/ogrisel)
  - 220 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 183 [Brian Holt](http://personal.ee.surrey.ac.uk/Personal/B.Holt)
  - 166 [Gael Varoquaux](http://gael-varoquaux.info)
  - 144 [Lars Buitinck](https://github.com/larsmans)
  - 73 [Vlad Niculae](https://vene.ro/)
  - 65 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 64 [Fabian Pedregosa](http://fa.bianp.net)
  - 60 Robert Layton
  - 55 [Mathieu Blondel](http://www.mblondel.org)
  - 52 [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - 44 Noel Dawe
  - 38 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 24 `Virgile Fritsch <VirgileFritsch>`
  - 23 [Satrajit Ghosh](https://www.mit.edu/~satra/)
  - 3 Jan Hendrik Metzen
  - 3 Kenneth C. Arnold
  - 3 Shiqiao Du
  - 3 Tim Sheerman-Chase
  - 3 [Yaroslav Halchenko](http://www.onerussian.com/)
  - 2 Bala Subrahmanyam Varanasi
  - 2 DraXus
  - 2 Michael Eickenberg
  - 1 Bogdan Trach
  - 1 Félix-Antoine Fortin
  - 1 Juan Manuel Caicedo Carvajal
  - 1 Nelle Varoquaux
  - 1 [Nicolas Pinto](https://twitter.com/npinto)
  - 1 Tiziano Zito
  - 1 Xinfan Meng

## Version 0.9

**September 21, 2011**

scikit-learn 0.9 was released on September 2011, three months after the 0.8 release and includes the new modules \[manifold\](\#manifold), \[dirichlet\_process\](\#dirichlet\_process) as well as several new algorithms and documentation improvements.

This release also includes the dictionary-learning work developed by [Vlad Niculae](https://vene.ro/) as part of the [Google Summer of Code](https://developers.google.com/open-source/gsoc) program.

[![banner2](../auto_examples/linear_model/images/thumb/sphx_glr_plot_omp_thumb.png)](../auto_examples/linear_model/plot_omp.html) [![banner1](../auto_examples/manifold/images/thumb/sphx_glr_plot_compare_methods_thumb.png)](../auto_examples/manifold/plot_compare_methods.html) [![banner3](../auto_examples/decomposition/images/thumb/sphx_glr_plot_kernel_pca_thumb.png)](../auto_examples/decomposition/plot_kernel_pca.html)

### Changelog

  - New \[manifold\](\#manifold) module by [Jake Vanderplas](https://staff.washington.edu/jakevdp/) and [Fabian Pedregosa](http://fa.bianp.net).
  - New \[Dirichlet Process \<dirichlet\_process\>\](\#dirichlet-process-\<dirichlet\_process\>) Gaussian Mixture Model by [Alexandre Passos](http://atpassos.me)
  - \[neighbors\](\#neighbors) module refactoring by [Jake Vanderplas](https://staff.washington.edu/jakevdp/) : general refactoring, support for sparse matrices in input, speed and documentation improvements. See the next section for a full list of API changes.
  - Improvements on the \[feature\_selection\](\#feature\_selection) module by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/) : refactoring of the RFE classes, documentation rewrite, increased efficiency and minor API changes.
  - \[SparsePCA\](\#sparsepca) by [Vlad Niculae](https://vene.ro/), [Gael Varoquaux](http://gael-varoquaux.info) and [Alexandre Gramfort](http://alexandre.gramfort.net)
  - Printing an estimator now behaves independently of architectures and Python version thanks to `Jean Kossaifi <JeanKossaifi>`.
  - \[Loader for libsvm/svmlight format \<libsvm\_loader\>\](\#loader-for-libsvm/svmlight-format-\<libsvm\_loader\>) by [Mathieu Blondel](http://www.mblondel.org) and [Lars Buitinck](https://github.com/larsmans)
  - Documentation improvements: thumbnails in example gallery by [Fabian Pedregosa](http://fa.bianp.net).
  - Important bugfixes in \[svm\](\#svm) module (segfaults, bad performance) by [Fabian Pedregosa](http://fa.bianp.net).
  - Added \[multinomial\_naive\_bayes\](\#multinomial\_naive\_bayes) and \[bernoulli\_naive\_bayes\](\#bernoulli\_naive\_bayes) by [Lars Buitinck](https://github.com/larsmans)
  - Text feature extraction optimizations by Lars Buitinck
  - Chi-Square feature selection (<span class="title-ref">feature\_selection.chi2</span>) by [Lars Buitinck](https://github.com/larsmans).
  - \[sample\_generators\](\#sample\_generators) module refactoring by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - \[multiclass\](\#multiclass) by [Mathieu Blondel](http://www.mblondel.org)
  - Ball tree rewrite by [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - Implementation of \[dbscan\](\#dbscan) algorithm by Robert Layton
  - Kmeans predict and transform by Robert Layton
  - Preprocessing module refactoring by [Olivier Grisel](https://twitter.com/ogrisel)
  - Faster mean shift by Conrad Lee
  - New `Bootstrap`, \[ShuffleSplit\](\#shufflesplit) and various other improvements in cross validation schemes by [Olivier Grisel](https://twitter.com/ogrisel) and [Gael Varoquaux](http://gael-varoquaux.info)
  - Adjusted Rand index and V-Measure clustering evaluation metrics by [Olivier Grisel](https://twitter.com/ogrisel)
  - Added <span class="title-ref">Orthogonal Matching Pursuit \<linear\_model.OrthogonalMatchingPursuit\></span> by [Vlad Niculae](https://vene.ro/)
  - Added 2D-patch extractor utilities in the \[feature\_extraction\](\#feature\_extraction) module by [Vlad Niculae](https://vene.ro/)
  - Implementation of <span class="title-ref">\~linear\_model.LassoLarsCV</span> (cross-validated Lasso solver using the Lars algorithm) and <span class="title-ref">\~linear\_model.LassoLarsIC</span> (BIC/AIC model selection in Lars) by [Gael Varoquaux](http://gael-varoquaux.info) and [Alexandre Gramfort](http://alexandre.gramfort.net)
  - Scalability improvements to <span class="title-ref">metrics.roc\_curve</span> by Olivier Hervieu
  - Distance helper functions <span class="title-ref">metrics.pairwise\_distances</span> and <span class="title-ref">metrics.pairwise.pairwise\_kernels</span> by Robert Layton
  - <span class="title-ref">Mini-Batch K-Means \<cluster.MiniBatchKMeans\></span> by Nelle Varoquaux and Peter Prettenhofer.
  - mldata utilities by Pietro Berkes.
  - \[olivetti\_faces\_dataset\](\#olivetti\_faces\_dataset) by [David Warde-Farley](http://www-etud.iro.umontreal.ca/~wardefar/).

### API changes summary

Here are the code migration instructions when upgrading from scikit-learn version 0.8:

  - The `scikits.learn` package was renamed `sklearn`. There is still a `scikits.learn` package alias for backward compatibility.
    
    Third-party projects with a dependency on scikit-learn 0.9+ should upgrade their codebase. For instance, under Linux / MacOSX just run (make a backup first\!):
    
        find -name "*.py" | xargs sed -i 's/\bscikits.learn\b/sklearn/g'

  - Estimators no longer accept model parameters as `fit` arguments: instead all parameters must be only be passed as constructor arguments or using the now public `set_params` method inherited from <span class="title-ref">\~base.BaseEstimator</span>.
    
    Some estimators can still accept keyword arguments on the `fit` but this is restricted to data-dependent values (e.g. a Gram matrix or an affinity matrix that are precomputed from the `X` data matrix.

  - The `cross_val` package has been renamed to `cross_validation` although there is also a `cross_val` package alias in place for backward compatibility.
    
    Third-party projects with a dependency on scikit-learn 0.9+ should upgrade their codebase. For instance, under Linux / MacOSX just run (make a backup first\!):
    
        find -name "*.py" | xargs sed -i 's/\bcross_val\b/cross_validation/g'

  - The `score_func` argument of the `sklearn.cross_validation.cross_val_score` function is now expected to accept `y_test` and `y_predicted` as only arguments for classification and regression tasks or `X_test` for unsupervised estimators.

  - `gamma` parameter for support vector machine algorithms is set to `1 / n_features` by default, instead of `1 / n_samples`.

  - The `sklearn.hmm` has been marked as orphaned: it will be removed from scikit-learn in version 0.11 unless someone steps up to contribute documentation, examples and fix lurking numerical stability issues.

  - `sklearn.neighbors` has been made into a submodule. The two previously available estimators, `NeighborsClassifier` and `NeighborsRegressor` have been marked as deprecated. Their functionality has been divided among five new classes: `NearestNeighbors` for unsupervised neighbors searches, `KNeighborsClassifier` & `RadiusNeighborsClassifier` for supervised classification problems, and `KNeighborsRegressor` & `RadiusNeighborsRegressor` for supervised regression problems.

  - `sklearn.ball_tree.BallTree` has been moved to `sklearn.neighbors.BallTree`. Using the former will generate a warning.

  - `sklearn.linear_model.LARS()` and related classes (LassoLARS, LassoLARSCV, etc.) have been renamed to `sklearn.linear_model.Lars()`.

  - All distance metrics and kernels in `sklearn.metrics.pairwise` now have a Y parameter, which by default is None. If not given, the result is the distance (or kernel similarity) between each sample in Y. If given, the result is the pairwise distance (or kernel similarity) between samples in X to Y.

  - `sklearn.metrics.pairwise.l1_distance` is now called `manhattan_distance`, and by default returns the pairwise distance. For the component wise distance, set the parameter `sum_over_features` to `False`.

Backward compatibility package aliases and other deprecated classes and functions will be removed in version 0.11.

### People

38 people contributed to this release.

  - 387 [Vlad Niculae](https://vene.ro/)
  - 320 [Olivier Grisel](https://twitter.com/ogrisel)
  - 192 [Lars Buitinck](https://github.com/larsmans)
  - 179 [Gael Varoquaux](http://gael-varoquaux.info)
  - 168 [Fabian Pedregosa](http://fa.bianp.net) ([INRIA](https://www.inria.fr/), [Parietal Team](http://parietal.saclay.inria.fr/))
  - 127 [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - 120 [Mathieu Blondel](http://www.mblondel.org)
  - 85 [Alexandre Passos](http://atpassos.me)
  - 67 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 57 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 56 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 42 Robert Layton
  - 38 Nelle Varoquaux
  - 32 `Jean Kossaifi <JeanKossaifi>`
  - 30 Conrad Lee
  - 22 Pietro Berkes
  - 18 andy
  - 17 David Warde-Farley
  - 12 Brian Holt
  - 11 Robert
  - 8 Amit Aides
  - 8 `Virgile Fritsch <VirgileFritsch>`
  - 7 [Yaroslav Halchenko](http://www.onerussian.com/)
  - 6 Salvatore Masecchia
  - 5 Paolo Losi
  - 4 Vincent Schut
  - 3 Alexis Metaireau
  - 3 Bryan Silverthorn
  - 3 [Andreas Müller](https://amueller.github.io/)
  - 2 Minwoo Jake Lee
  - 1 Emmanuelle Gouillart
  - 1 Keith Goodman
  - 1 Lucas Wiman
  - 1 [Nicolas Pinto](https://twitter.com/npinto)
  - 1 Thouis (Ray) Jones
  - 1 Tim Sheerman-Chase

## Version 0.8

**May 11, 2011**

scikit-learn 0.8 was released on May 2011, one month after the first "international" [scikit-learn coding sprint](https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events) and is marked by the inclusion of important modules: \[hierarchical\_clustering\](\#hierarchical\_clustering), \[cross\_decomposition\](\#cross\_decomposition), \[NMF\](\#nmf), initial support for Python 3 and by important enhancements and bug fixes.

### Changelog

Several new modules where introduced during this release:

  - New \[hierarchical\_clustering\](\#hierarchical\_clustering) module by Vincent Michel, [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page), [Alexandre Gramfort](http://alexandre.gramfort.net) and [Gael Varoquaux](http://gael-varoquaux.info).
  - \[kernel\_pca\](\#kernel\_pca) implementation by [Mathieu Blondel](http://www.mblondel.org)
  - \[labeled\_faces\_in\_the\_wild\_dataset\](\#labeled\_faces\_in\_the\_wild\_dataset) by [Olivier Grisel](https://twitter.com/ogrisel).
  - New \[cross\_decomposition\](\#cross\_decomposition) module by [Edouard Duchesnay](https://duchesnay.github.io/).
  - \[NMF\](\#nmf) module [Vlad Niculae](https://vene.ro/)
  - Implementation of the \[oracle\_approximating\_shrinkage\](\#oracle\_approximating\_shrinkage) algorithm by `Virgile Fritsch <VirgileFritsch>` in the \[covariance\](\#covariance) module.

Some other modules benefited from significant improvements or cleanups.

  - Initial support for Python 3: builds and imports cleanly, some modules are usable while others have failing tests by [Fabian Pedregosa](http://fa.bianp.net).
  - <span class="title-ref">\~decomposition.PCA</span> is now usable from the Pipeline object by [Olivier Grisel](https://twitter.com/ogrisel).
  - Guide \[performance-howto\](\#performance-howto) by [Olivier Grisel](https://twitter.com/ogrisel).
  - Fixes for memory leaks in libsvm bindings, 64-bit safer BallTree by Lars Buitinck.
  - bug and style fixing in \[k\_means\](\#k\_means) algorithm by Jan Schlüter.
  - Add attribute converged to Gaussian Mixture Models by Vincent Schut.
  - Implemented `transform`, `predict_log_proba` in <span class="title-ref">\~discriminant\_analysis.LinearDiscriminantAnalysis</span> By [Mathieu Blondel](http://www.mblondel.org).
  - Refactoring in the \[svm\](\#svm) module and bug fixes by [Fabian Pedregosa](http://fa.bianp.net), [Gael Varoquaux](http://gael-varoquaux.info) and Amit Aides.
  - Refactored SGD module (removed code duplication, better variable naming), added interface for sample weight by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Wrapped BallTree with Cython by Thouis (Ray) Jones.
  - Added function <span class="title-ref">svm.l1\_min\_c</span> by Paolo Losi.
  - Typos, doc style, etc. by [Yaroslav Halchenko](http://www.onerussian.com/), [Gael Varoquaux](http://gael-varoquaux.info), [Olivier Grisel](https://twitter.com/ogrisel), Yann Malet, [Nicolas Pinto](https://twitter.com/npinto), Lars Buitinck and [Fabian Pedregosa](http://fa.bianp.net).

### People

People that made this release possible preceded by number of commits:

  - 159 [Olivier Grisel](https://twitter.com/ogrisel)
  - 96 [Gael Varoquaux](http://gael-varoquaux.info)
  - 96 [Vlad Niculae](https://vene.ro/)
  - 94 [Fabian Pedregosa](http://fa.bianp.net)
  - 36 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 32 Paolo Losi
  - 31 [Edouard Duchesnay](https://duchesnay.github.io/)
  - 30 [Mathieu Blondel](http://www.mblondel.org)
  - 25 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 22 [Nicolas Pinto](https://twitter.com/npinto)
  - 11 `Virgile Fritsch <VirgileFritsch>`
  - 7 Lars Buitinck
  - 6 Vincent Michel
  - 5 [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page)
  - 4 Thouis (Ray) Jones
  - 4 Vincent Schut
  - 3 Jan Schlüter
  - 2 Julien Miotte
  - 2 [Matthieu Perrot](http://brainvisa.info/biblio/lnao/en/Author/PERROT-M.html)
  - 2 Yann Malet
  - 2 [Yaroslav Halchenko](http://www.onerussian.com/)
  - 1 Amit Aides
  - 1 [Andreas Müller](https://amueller.github.io/)
  - 1 Feth Arezki
  - 1 Meng Xinfan

## Version 0.7

**March 2, 2011**

scikit-learn 0.7 was released in March 2011, roughly three months after the 0.6 release. This release is marked by the speed improvements in existing algorithms like k-Nearest Neighbors and K-Means algorithm and by the inclusion of an efficient algorithm for computing the Ridge Generalized Cross Validation solution. Unlike the preceding release, no new modules where added to this release.

### Changelog

  - Performance improvements for Gaussian Mixture Model sampling \[Jan Schlüter\].
  - Implementation of efficient leave-one-out cross-validated Ridge in <span class="title-ref">\~linear\_model.RidgeCV</span> \[[Mathieu Blondel](http://www.mblondel.org)\]
  - Better handling of collinearity and early stopping in <span class="title-ref">linear\_model.lars\_path</span> \[[Alexandre Gramfort](http://alexandre.gramfort.net) and [Fabian Pedregosa](http://fa.bianp.net)\].
  - Fixes for liblinear ordering of labels and sign of coefficients \[Dan Yamins, Paolo Losi, [Mathieu Blondel](http://www.mblondel.org) and [Fabian Pedregosa](http://fa.bianp.net)\].
  - Performance improvements for Nearest Neighbors algorithm in high-dimensional spaces \[[Fabian Pedregosa](http://fa.bianp.net)\].
  - Performance improvements for <span class="title-ref">\~cluster.KMeans</span> \[[Gael Varoquaux](http://gael-varoquaux.info) and [James Bergstra](http://www-etud.iro.umontreal.ca/~bergstrj/)\].
  - Sanity checks for SVM-based classes \[[Mathieu Blondel](http://www.mblondel.org)\].
  - Refactoring of <span class="title-ref">neighbors.NeighborsClassifier</span> and \`neighbors.kneighbors\_graph\`: added different algorithms for the k-Nearest Neighbor Search and implemented a more stable algorithm for finding barycenter weights. Also added some developer documentation for this module, see [notes\_neighbors](https://github.com/scikit-learn/scikit-learn/wiki/Neighbors-working-notes) for more information \[[Fabian Pedregosa](http://fa.bianp.net)\].
  - Documentation improvements: Added <span class="title-ref">pca.RandomizedPCA</span> and <span class="title-ref">\~linear\_model.LogisticRegression</span> to the class reference. Also added references of matrices used for clustering and other fixes \[[Gael Varoquaux](http://gael-varoquaux.info), [Fabian Pedregosa](http://fa.bianp.net), [Mathieu Blondel](http://www.mblondel.org), [Olivier Grisel](https://twitter.com/ogrisel), Virgile Fritsch , Emmanuelle Gouillart\]
  - Binded decision\_function in classes that make use of [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), dense and sparse variants, like <span class="title-ref">\~svm.LinearSVC</span> or <span class="title-ref">\~linear\_model.LogisticRegression</span> \[[Fabian Pedregosa](http://fa.bianp.net)\].
  - Performance and API improvements to <span class="title-ref">metrics.pairwise.euclidean\_distances</span> and to <span class="title-ref">pca.RandomizedPCA</span> \[[James Bergstra](http://www-etud.iro.umontreal.ca/~bergstrj/)\].
  - Fix compilation issues under NetBSD \[Kamel Ibn Hassen Derouiche\]
  - Allow input sequences of different lengths in <span class="title-ref">hmm.GaussianHMM</span> \[[Ron Weiss](https://www.ee.columbia.edu/~ronw/)\].
  - Fix bug in affinity propagation caused by incorrect indexing \[Xinfan Meng\]

### People

People that made this release possible preceded by number of commits:

  - 85 [Fabian Pedregosa](http://fa.bianp.net)
  - 67 [Mathieu Blondel](http://www.mblondel.org)
  - 20 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 19 [James Bergstra](http://www-etud.iro.umontreal.ca/~bergstrj/)
  - 14 Dan Yamins
  - 13 [Olivier Grisel](https://twitter.com/ogrisel)
  - 12 [Gael Varoquaux](http://gael-varoquaux.info)
  - 4 [Edouard Duchesnay](https://duchesnay.github.io/)
  - 4 [Ron Weiss](https://www.ee.columbia.edu/~ronw/)
  - 2 Satrajit Ghosh
  - 2 Vincent Dubourg
  - 1 Emmanuelle Gouillart
  - 1 Kamel Ibn Hassen Derouiche
  - 1 Paolo Losi
  - 1 VirgileFritsch
  - 1 [Yaroslav Halchenko](http://www.onerussian.com/)
  - 1 Xinfan Meng

## Version 0.6

**December 21, 2010**

scikit-learn 0.6 was released on December 2010. It is marked by the inclusion of several new modules and a general renaming of old ones. It is also marked by the inclusion of new example, including applications to real-world datasets.

### Changelog

  - New [stochastic gradient](https://scikit-learn.org/stable/modules/sgd.html) descent module by Peter Prettenhofer. The module comes with complete documentation and examples.
  - Improved svm module: memory consumption has been reduced by 50%, heuristic to automatically set class weights, possibility to assign weights to samples (see \[sphx\_glr\_auto\_examples\_svm\_plot\_weighted\_samples.py\](\#sphx\_glr\_auto\_examples\_svm\_plot\_weighted\_samples.py) for an example).
  - New \[gaussian\_process\](\#gaussian\_process) module by Vincent Dubourg. This module also has great documentation and some very neat examples. See example\_gaussian\_process\_plot\_gp\_regression.py or example\_gaussian\_process\_plot\_gp\_probabilistic\_classification\_after\_regression.py for a taste of what can be done.
  - It is now possible to use liblinear's Multi-class SVC (option multi\_class in <span class="title-ref">\~svm.LinearSVC</span>)
  - New features and performance improvements of text feature extraction.
  - Improved sparse matrix support, both in main classes (<span class="title-ref">\~model\_selection.GridSearchCV</span>) as in modules sklearn.svm.sparse and sklearn.linear\_model.sparse.
  - Lots of cool new examples and a new section that uses real-world datasets was created. These include: \[sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_face\_recognition.py), \[sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_species\_distribution\_modeling.py), \[sphx\_glr\_auto\_examples\_applications\_wikipedia\_principal\_eigenvector.py\](\#sphx\_glr\_auto\_examples\_applications\_wikipedia\_principal\_eigenvector.py) and others.
  - Faster \[least\_angle\_regression\](\#least\_angle\_regression) algorithm. It is now 2x faster than the R version on worst case and up to 10x times faster on some cases.
  - Faster coordinate descent algorithm. In particular, the full path version of lasso (<span class="title-ref">linear\_model.lasso\_path</span>) is more than 200x times faster than before.
  - It is now possible to get probability estimates from a <span class="title-ref">\~linear\_model.LogisticRegression</span> model.
  - module renaming: the glm module has been renamed to linear\_model, the gmm module has been included into the more general mixture model and the sgd module has been included in linear\_model.
  - Lots of bug fixes and documentation improvements.

### People

People that made this release possible preceded by number of commits:

  - 207 [Olivier Grisel](https://twitter.com/ogrisel)
  - 167 [Fabian Pedregosa](http://fa.bianp.net)
  - 97 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 68 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 59 [Mathieu Blondel](http://www.mblondel.org)
  - 55 [Gael Varoquaux](http://gael-varoquaux.info)
  - 33 Vincent Dubourg
  - 21 [Ron Weiss](https://www.ee.columbia.edu/~ronw/)
  - 9 Bertrand Thirion
  - 3 [Alexandre Passos](http://atpassos.me)
  - 3 Anne-Laure Fouque
  - 2 Ronan Amicel
  - 1 [Christian Osendorfer](https://osdf.github.io)

## Version 0.5

**October 11, 2010**

### Changelog

### New classes

  - Support for sparse matrices in some classifiers of modules `svm` and `linear_model` (see <span class="title-ref">svm.sparse.SVC</span>, <span class="title-ref">svm.sparse.SVR</span>, <span class="title-ref">svm.sparse.LinearSVC</span>, <span class="title-ref">linear\_model.sparse.Lasso</span>, <span class="title-ref">linear\_model.sparse.ElasticNet</span>)
  - New <span class="title-ref">\~pipeline.Pipeline</span> object to compose different estimators.
  - Recursive Feature Elimination routines in module \[feature\_selection\](\#feature\_selection).
  - Addition of various classes capable of cross validation in the linear\_model module (<span class="title-ref">\~linear\_model.LassoCV</span>, <span class="title-ref">\~linear\_model.ElasticNetCV</span>, etc.).
  - New, more efficient LARS algorithm implementation. The Lasso variant of the algorithm is also implemented. See <span class="title-ref">\~linear\_model.lars\_path</span>, <span class="title-ref">\~linear\_model.Lars</span> and <span class="title-ref">\~linear\_model.LassoLars</span>.
  - New Hidden Markov Models module (see classes <span class="title-ref">hmm.GaussianHMM</span>, <span class="title-ref">hmm.MultinomialHMM</span>, <span class="title-ref">hmm.GMMHMM</span>)

<!-- end list -->

  - \- New module feature\_extraction (see \[class reference  
    \<feature\_extraction\_ref\>\](\#class-reference

\--\<feature\_extraction\_ref\>))

  - New FastICA algorithm in module sklearn.fastica

### Documentation

  - Improved documentation for many modules, now separating narrative documentation from the class reference. As an example, see [documentation for the SVM module](https://scikit-learn.org/stable/modules/svm.html) and the complete [class reference](https://scikit-learn.org/stable/modules/classes.html).

### Fixes

  - API changes: adhere variable names to PEP-8, give more meaningful names.
  - Fixes for svm module to run on a shared memory context (multiprocessing).
  - It is again possible to generate latex (and thus PDF) from the sphinx docs.

### Examples

  - new examples using some of the mlcomp datasets: `sphx_glr_auto_examples_mlcomp_sparse_document_classification.py` (since removed) and \[sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py\](\#sphx\_glr\_auto\_examples\_text\_plot\_document\_classification\_20newsgroups.py)
  - Many more examples. [See here](https://scikit-learn.org/stable/auto_examples/index.html) the full list of examples.

### External dependencies

  - Joblib is now a dependency of this package, although it is shipped with (sklearn.externals.joblib).

### Removed modules

  - Module ann (Artificial Neural Networks) has been removed from the distribution. Users wanting this sort of algorithms should take a look into pybrain.

### Misc

  - New sphinx theme for the web page.

### Authors

The following is a list of authors for this release, preceded by number of commits:

  - 262 Fabian Pedregosa
  - 240 Gael Varoquaux
  - 149 Alexandre Gramfort
  - 116 Olivier Grisel
  - 40 Vincent Michel
  - 38 Ron Weiss
  - 23 Matthieu Perrot
  - 10 Bertrand Thirion
  - 7 Yaroslav Halchenko
  - 9 VirgileFritsch
  - 6 Edouard Duchesnay
  - 4 Mathieu Blondel
  - 1 Ariel Rokem
  - 1 Matthieu Brucher

## Version 0.4

**August 26, 2010**

### Changelog

Major changes in this release include:

  - Coordinate Descent algorithm (Lasso, ElasticNet) refactoring & speed improvements (roughly 100x times faster).
  - Coordinate Descent Refactoring (and bug fixing) for consistency with R's package GLMNET.
  - New metrics module.
  - New GMM module contributed by Ron Weiss.
  - Implementation of the LARS algorithm (without Lasso variant for now).
  - feature\_selection module redesign.
  - Migration to GIT as version control system.
  - Removal of obsolete attrselect module.
  - Rename of private compiled extensions (added underscore).
  - Removal of legacy unmaintained code.
  - Documentation improvements (both docstring and rst).
  - Improvement of the build system to (optionally) link with MKL. Also, provide a lite BLAS implementation in case no system-wide BLAS is found.
  - Lots of new examples.
  - Many, many bug fixes ...

### Authors

The committer list for this release is the following (preceded by number of commits):

  - 143 Fabian Pedregosa
  - 35 Alexandre Gramfort
  - 34 Olivier Grisel
  - 11 Gael Varoquaux
  - 5 Yaroslav Halchenko
  - 2 Vincent Michel
  - 1 Chris Filo Gorgolewski

## Earlier versions

Earlier versions included contributions by Fred Mailhot, David Cooke, David Huard, Dave Morrill, Ed Schofield, Travis Oliphant, Pearu Peterson.

---

27096.feature.md

---

- <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span>, <span class="title-ref">model\_selection.HalvingGridSearchCV</span> and <span class="title-ref">model\_selection.HalvingRandomSearchCV</span> now support Array API compatible inputs when their base estimators do. By `Tim Head <betatim>` and `Olivier Grisel <ogrisel>`

---

27381.feature.md

---

- <span class="title-ref">preprocessing.LabelEncoder</span> now supports Array API compatible inputs. By `Omar Salman <OmarManzoor>`

---

27736.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_absolute\_error</span> now supports Array API compatible inputs. By `Edoardo Abati <EdAbati>`

---

28106.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_tweedie\_deviance</span> now supports Array API compatible inputs. By `Thomas Li <lithomas1>`

---

29014.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.cosine\_similarity</span> now supports Array API compatible inputs. By `Edoardo Abati <EdAbati>`

---

29112.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.paired\_cosine\_distances</span> now supports Array API compatible inputs. By `Edoardo Abati <EdAbati>`

---

29141.feature.md

---

- <span class="title-ref">sklearn.metrics.cluster.entropy</span> now supports Array API compatible inputs. By `Yaroslav Korobko <Tialo>`

---

29142.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_squared\_error</span> now supports Array API compatible inputs. By `Yaroslav Korobko <Tialo>`

---

29144.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.additive\_chi2\_kernel</span> now supports Array API compatible inputs. By `Yaroslav Korobko <Tialo>`

---

29207.feature.md

---

- <span class="title-ref">sklearn.metrics.d2\_tweedie\_score</span> now supports Array API compatible inputs. By `Emily Chen <EmilyXinyi>`

---

29212.feature.md

---

- <span class="title-ref">sklearn.metrics.max\_error</span> now supports Array API compatible inputs. By `Edoardo Abati <EdAbati>`

---

29227.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_poisson\_deviance</span> now supports Array API compatible inputs. By `Emily Chen <EmilyXinyi>`

---

29239.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_gamma\_deviance</span> now supports Array API compatible inputs. By `Emily Chen <EmilyXinyi>`

---

29265.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.cosine\_distances</span> now supports Array API compatible inputs. By `Emily Chen <EmilyXinyi>`

---

29267.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.chi2\_kernel</span> now supports Array API compatible inputs. By `Yaroslav Korobko <Tialo>`

---

29300.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_absolute\_percentage\_error</span> now supports Array API compatible inputs. By `Emily Chen <EmilyXinyi>`

---

29389.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.paired\_euclidean\_distances</span> now supports Array API compatible inputs. By `Emily Chen <EmilyXinyi>`

---

29433.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.euclidean\_distances</span> and <span class="title-ref">sklearn.metrics.pairwise.rbf\_kernel</span> now supports Array API compatible inputs. By `Omar Salman <OmarManzoor>`

---

29475.feature.md

---

- <span class="title-ref">sklearn.metrics.pairwise.linear\_kernel</span>, <span class="title-ref">sklearn.metrics.pairwise.sigmoid\_kernel</span>, and <span class="title-ref">sklearn.metrics.pairwise.polynomial\_kernel</span> now supports Array API compatible inputs. By `Omar Salman <OmarManzoor>`

---

29639.other.md

---

- Support for the soon to be deprecated <span class="title-ref">cupy.array\_api</span> module has been removed in favor of directly supporting the top level <span class="title-ref">cupy</span> module, possibly via the <span class="title-ref">array\_api\_compat.cupy</span> compatibility wrapper. By `Olivier Grisel <ogrisel>`

---

29709.feature.md

---

- <span class="title-ref">sklearn.metrics.mean\_squared\_log\_error</span> and <span class="title-ref">sklearn.metrics.root\_mean\_squared\_log\_error</span> now supports Array API compatible inputs. By `Virgil Chan <virchan>`

---

29751.feature.md

---

- <span class="title-ref">preprocessing.MinMaxScaler</span> with <span class="title-ref">clip=True</span> now supports Array API compatible inputs. By `Shreekant Nandiyawar <Shree7676>`

---

29128.other.md

---

# Dropping official support for PyPy

Due to limited maintainer resources and small number of users, official PyPy support has been dropped. Some parts of scikit-learn may still work but PyPy is not tested anymore in the scikit-learn Continuous Integration. By `Loïc Estève <lesteve>`

---

29400.other.md

---

# Dropping support for building with setuptools

From scikit-learn 1.6 onwards, support for building with setuptools has been removed. Meson is the only supported way to build scikit-learn, see \[Building from source \<install\_bleeding\_edge\>\](\#building-from-source-\<install\_bleeding\_edge\>) for more details. By `Loïc Estève <lesteve>`

---

29677.enhancement.md

---

- <span class="title-ref">\_\_sklearn\_tags\_\_</span> was introduced for setting tags in estimators. More details in \[estimator\_tags\](\#estimator\_tags). By `Thomas Fan <thomasjpfan>` and `Adrin Jalali <adrinjalali>`

---

29696.api.md

---

- <span class="title-ref">utils.validation.validate\_data</span> is introduced and replaces previously private <span class="title-ref">base.BaseEstimator.\_validate\_data</span> method. This is intended for third party estimator developers, who should use this function in most cases instead of <span class="title-ref">utils.check\_array</span> and <span class="title-ref">utils.check\_X\_y</span>. By `Adrin Jalali <adrinjalali>`

---

29793.enhancement.md

---

- Scikit-learn classes and functions can be used while only having a <span class="title-ref">import sklearn</span> import line. For example, <span class="title-ref">import sklearn; sklearn.svm.SVC()</span> now works. By `Thomas Fan <thomasjpfan>`

---

30023.fix.md

---

- Classes <span class="title-ref">metrics.ConfusionMatrixDisplay</span>, <span class="title-ref">metrics.RocCurveDisplay</span>, <span class="title-ref">calibration.CalibrationDisplay</span>, <span class="title-ref">metrics.PrecisionRecallDisplay</span>, <span class="title-ref">metrics.PredictionErrorDisplay</span> and <span class="title-ref">inspection.PartialDependenceDisplay</span> now properly handle Matplotlib aliases for style parameters (e.g., <span class="title-ref">c</span> and <span class="title-ref">color</span>, <span class="title-ref">ls</span> and <span class="title-ref">linestyle</span>, etc). By `Joseph Barbier <JosephBARBIERDARNAL>`

---

28494.feature.md

---

- <span class="title-ref">semi\_supervised.SelfTrainingClassifier</span> now supports metadata routing. The fit method now accepts `**fit_params` which are passed to the underlying estimators via their <span class="title-ref">fit</span> methods. In addition, the <span class="title-ref">\~semi\_supervised.SelfTrainingClassifier.predict</span>, <span class="title-ref">\~semi\_supervised.SelfTrainingClassifier.predict\_proba</span>, <span class="title-ref">\~semi\_supervised.SelfTrainingClassifier.predict\_log\_proba</span>, <span class="title-ref">\~semi\_supervised.SelfTrainingClassifier.score</span> and <span class="title-ref">\~semi\_supervised.SelfTrainingClassifier.decision\_function</span> methods also accept `**params` which are passed to the underlying estimators via their respective methods. By `Adam Li <adam2392>`

---

28701.feature.md

---

- <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span> now support metadata routing and pass `**fit_params` to the underlying estimators via their <span class="title-ref">fit</span> methods. By `Stefanie Senger <StefanieSenger>`

---

28975.feature.md

---

- <span class="title-ref">model\_selection.learning\_curve</span> now supports metadata routing for the <span class="title-ref">fit</span> method of its estimator and for its underlying CV splitter and scorer. By `Stefanie Senger <StefanieSenger>`

---

29136.feature.md

---

- <span class="title-ref">compose.TransformedTargetRegressor</span> now supports metadata routing in its <span class="title-ref">\~compose.TransformedTargetRegressor.fit</span> and <span class="title-ref">\~compose.TransformedTargetRegressor.predict</span> methods and routes the corresponding params to the underlying regressor. By `Omar Salman <OmarManzoor>`

---

29260.feature.md

---

- <span class="title-ref">feature\_selection.SequentialFeatureSelector</span> now supports metadata routing in its <span class="title-ref">fit</span> method and passes the corresponding params to the <span class="title-ref">model\_selection.cross\_val\_score</span> function. By `Omar Salman <OmarManzoor>`

---

29266.feature.md

---

- <span class="title-ref">model\_selection.permutation\_test\_score</span> now supports metadata routing for the <span class="title-ref">fit</span> method of its estimator and for its underlying CV splitter and scorer. By `Adam Li <adam2392>`

---

29312.feature.md

---

- <span class="title-ref">feature\_selection.RFE</span> and <span class="title-ref">feature\_selection.RFECV</span> now support metadata routing. By `Omar Salman <OmarManzoor>`

---

29329.feature.md

---

- <span class="title-ref">model\_selection.validation\_curve</span> now supports metadata routing for the <span class="title-ref">fit</span> method of its estimator and for its underlying CV splitter and scorer. By `Stefanie Senger <StefanieSenger>`

---

29634.fix.md

---

- Metadata is routed correctly to grouped CV splitters via <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span> and <span class="title-ref">UnsetMetadataPassedError</span> is fixed for <span class="title-ref">linear\_model.RidgeClassifierCV</span> with default scoring. By `Stefanie Senger <StefanieSenger>`

---

29920.fix.md

---

- Many method arguments which shouldn't be included in the routing mechanism are now excluded and the <span class="title-ref">set\_{method}\_request</span> methods are not generated for them. By [Adrin Jalali]()

---

28936.enhancement.md

---

- Added a function <span class="title-ref">base.is\_clusterer</span> which determines whether a given estimator is of category clusterer. By `Christian Veenhuis <ChVeen>`

---

30122.api.md

---

- Passing a class object to <span class="title-ref">\~sklearn.base.is\_classifier</span>, <span class="title-ref">\~sklearn.base.is\_regressor</span>, <span class="title-ref">\~sklearn.base.is\_transformer</span>, and <span class="title-ref">\~sklearn.base.is\_outlier\_detector</span> is now deprecated. Pass an instance instead. By [Adrin Jalali]()

---

30171.api.md

---

- <span class="title-ref">cv="prefit"</span> is deprecated for <span class="title-ref">\~sklearn.calibration.CalibratedClassifierCV</span>. Use <span class="title-ref">\~sklearn.frozen.FrozenEstimator</span> instead, as <span class="title-ref">CalibratedClassifierCV(FrozenEstimator(estimator))</span>. By [Adrin Jalali]()

---

29124.api.md

---

- The <span class="title-ref">copy</span> parameter of <span class="title-ref">cluster.Birch</span> was deprecated in 1.6 and will be removed in 1.8. It has no effect as the estimator does not perform in-place operations on the input data. By `Yao Xiao <Charlie-XIAO>`

---

28934.enhancement.md

---

- <span class="title-ref">sklearn.compose.ColumnTransformer</span> <span class="title-ref">verbose\_feature\_names\_out</span> now accepts string format or callable to generate feature names. By `Marc Bresson <MarcBresson>`

---

29835.efficiency.md

---

- <span class="title-ref">covariance.MinCovDet</span> fitting is now slightly faster. By `Antony Lee <anntzer>`

---

29710.fix.md

---

- <span class="title-ref">cross\_decomposition.PLSRegression</span> properly raises an error when <span class="title-ref">n\_components</span> is larger than <span class="title-ref">n\_samples</span>. By `Thomas Fan <thomasjpfan>`

---

29354.feature.md

---

- <span class="title-ref">datasets.fetch\_file</span> allows downloading arbitrary data-file from the web. It handles local caching, integrity checks with SHA256 digests and automatic retries in case of HTTP errors. By `Olivier Grisel <ogrisel>`

---

30097.enhancement.md

---

- <span class="title-ref">\~sklearn.decomposition.LatentDirichletAllocation</span> now has a `normalize` parameter in <span class="title-ref">\~sklearn.decomposition.LatentDirichletAllocation.transform</span> and <span class="title-ref">\~sklearn.decomposition.LatentDirichletAllocation.fit\_transform</span> methods to control whether the document topic distribution is normalized. By [Adrin Jalali]()

---

30224.fix.md

---

- <span class="title-ref">\~sklearn.decomposition.IncrementalPCA</span> will now only raise a `ValueError` when the number of samples in the input data to `partial_fit` is less than the number of components on the first call to `partial_fit`. Subsequent calls to `partial_fit` no longer face this restriction. By `Thomas Gessey-Jones <ThomasGesseyJonesPX>`

---

19731.fix.md

---

- <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis</span> will now cause <span class="title-ref">LinAlgWarning</span> in case of collinear variables. These errors can be silenced using the <span class="title-ref">reg\_param</span> attribute. By `Alihan Zihna <azihna>`

---

28064.efficiency.md

---

- Small runtime improvement of fitting <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> by parallelizing the initial search for bin thresholds. By `Christian Lorentzen <lorentzenchr>`

---

28179.enhancement.md

---

- The verbosity of <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> got a more granular control. Now, <span class="title-ref">verbose = 1</span> prints only summary messages, <span class="title-ref">verbose \>= 2</span> prints the full information as before. By `Christian Lorentzen <lorentzenchr>`

---

28268.feature.md

---

- <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span> now support missing-values in the data matrix <span class="title-ref">X</span>. Missing-values are handled by randomly moving all of the samples to the left, or right child node as the tree is traversed. By `Adam Li <adam2392>`

---

28622.efficiency.md

---

- <span class="title-ref">ensemble.IsolationForest</span> now runs parallel jobs during `predict` offering a speedup of up to 2-4x on sample sizes larger than 2000 using <span class="title-ref">joblib</span>. By `Adam Li <adam2392>` and `Sérgio Pereira <sergiormpereira>`

---

29997.api.md

---

- The parameter <span class="title-ref">algorithm</span> of <span class="title-ref">ensemble.AdaBoostClassifier</span> is deprecated and will be removed in 1.8. By `Jérémie du Boisberranger <jeremiedbb>`

---

30022.fix.md

---

- <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span> now correctly preserves the <span class="title-ref">dtype</span> of <span class="title-ref">idf\_</span> based on the input data. By `Guillaume Lemaitre <glemaitre>`

---

29705.major-feature.md

---

- <span class="title-ref">\~sklearn.frozen.FrozenEstimator</span> is now introduced which allows freezing an estimator. This means calling <span class="title-ref">.fit</span> on it has no effect, and doing a <span class="title-ref">clone(frozenestimator)</span> returns the same estimator instead of an unfitted clone. `29705` By [Adrin Jalali]()

---

29135.fix.md

---

- <span class="title-ref">impute.KNNImputer</span> excludes samples with nan distances when computing the mean value for uniform weights. By `Xuefeng Xu <xuefeng-xu>`

---

29451.fix.md

---

- When <span class="title-ref">min\_value</span> and <span class="title-ref">max\_value</span> are array-like and some features are dropped due to <span class="title-ref">keep\_empty\_features=False</span>, <span class="title-ref">impute.IterativeImputer</span> no longer raises an error and now indexes correctly. By `Guntitat Sawadwuthikul <gunsodo>`

---

29779.fix.md

---

- Fixed <span class="title-ref">impute.IterativeImputer</span> to make sure that it does not skip the iterative process when <span class="title-ref">keep\_empty\_features</span> is set to <span class="title-ref">True</span>. By `Arif Qodari <arifqodari>`

---

29950.api.md

---

- Add a warning in <span class="title-ref">impute.SimpleImputer</span> when <span class="title-ref">keep\_empty\_feature=False</span> and <span class="title-ref">strategy="constant"</span>. In this case empty features are not dropped and this behaviour will change in 1.8. By `Arthur Courselle <ArthurCourselle>` and `Simon Riou <simon-riou>`

---

19746.fix.md

---

- In <span class="title-ref">linear\_model.Ridge</span> and <span class="title-ref">linear\_model.RidgeCV</span>, after <span class="title-ref">fit</span>, the <span class="title-ref">coef\_</span> attribute is now of shape <span class="title-ref">(n\_samples,)</span> like other linear models. By `Maxwell Liu<MaxwellLZH>`, [Guillaume Lemaitre](), and [Adrin Jalali]()

---

28840.enhancement.md

---

- The <span class="title-ref">solver="newton-cholesky"</span> in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> is extended to support the full multinomial loss in a multiclass setting. By `Christian Lorentzen <lorentzenchr>`

---

29105.api.md

---

- Deprecates <span class="title-ref">copy\_X</span> in <span class="title-ref">linear\_model.TheilSenRegressor</span> as the parameter has no effect. <span class="title-ref">copy\_X</span> will be removed in 1.8. By `Adam Li <adam2392>`

---

29419.fix.md

---

- <span class="title-ref">linear\_model.LogisticRegressionCV</span> corrects sample weight handling for the calculation of test scores. By `Shruti Nath <snath-xoc>`

---

29442.fix.md

---

- <span class="title-ref">linear\_model.LassoCV</span> and <span class="title-ref">linear\_model.ElasticNetCV</span> now take sample weights into accounts to define the search grid for the internally tuned <span class="title-ref">alpha</span> hyper-parameter. By `John Hopfensperger <s-banach>` and `Shruti Nath <snath-xoc>`

---

29818.fix.md

---

- <span class="title-ref">linear\_model.LogisticRegression</span>, <span class="title-ref">linear\_model.PoissonRegressor</span>, <span class="title-ref">linear\_model.GammaRegressor</span>, <span class="title-ref">linear\_model.TweedieRegressor</span> now take sample weights into account to decide when to fall back to <span class="title-ref">solver='lbfgs'</span> whenever <span class="title-ref">solver='newton-cholesky'</span> becomes numerically unstable. By `Antoine Baker <antoinebaker>`

---

29842.fix.md

---

- <span class="title-ref">linear\_model.RidgeCV</span> now properly uses predictions on the same scale as the target seen during <span class="title-ref">fit</span>. These predictions are stored in <span class="title-ref">cv\_results\_</span> when <span class="title-ref">scoring \!= None</span>. Previously, the predictions were rescaled by the square root of the sample weights and offset by the mean of the target, leading to an incorrect estimate of the score. By `Guillaume Lemaitre <glemaitre>`, `Jérôme Dockes <jeromedockes>` and `Hanmin Qin <qinhanmin2014>`

---

29884.fix.md

---

- <span class="title-ref">linear\_model.RidgeCV</span> now properly supports custom multioutput scorers by letting the scorer manage the multioutput averaging. Previously, the predictions and true targets were both squeezed to a 1D array before computing the error. By `Guillaume Lemaitre <glemaitre>`

---

30040.fix.md

---

- <span class="title-ref">linear\_model.LinearRegression</span> now sets the <span class="title-ref">cond</span> parameter when calling the <span class="title-ref">scipy.linalg.lstsq</span> solver on dense input data. This ensures more numerically robust results on rank-deficient data. In particular, it empirically fixes the expected equivalence property between fitting with reweighted or with repeated data points. By `Antoine Baker <antoinebaker>`

---

30100.fix.md

---

- <span class="title-ref">linear\_model.LogisticRegression</span> and and other linear models that accept <span class="title-ref">solver="newton-cholesky"</span> now report the correct number of iterations when they fall back to the <span class="title-ref">"lbfgs"</span> solver because of a rank deficient Hessian matrix. By `Olivier Grisel <ogrisel>`

---

30227.fix.md

---

- <span class="title-ref">\~sklearn.linear\_model.SGDOneClassSVM</span> now correctly inherits from <span class="title-ref">\~sklearn.base.OutlierMixin</span> and the tags are correctly set. By `Guillaume Lemaitre <glemaitre>`

---

28096.efficiency.md

---

- <span class="title-ref">manifold.locally\_linear\_embedding</span> and <span class="title-ref">manifold.LocallyLinearEmbedding</span> now allocate more efficiently the memory of sparse matrices in the Hessian, Modified and LTSA methods. By `Giorgio Angelotti <giorgioangel>`

---

26367.enhancement.md

---

- <span class="title-ref">metrics.RocCurveDisplay.from\_estimator</span>, <span class="title-ref">metrics.RocCurveDisplay.from\_predictions</span>, <span class="title-ref">metrics.PrecisionRecallDisplay.from\_estimator</span>, and <span class="title-ref">metrics.PrecisionRecallDisplay.from\_predictions</span> now accept a new keyword <span class="title-ref">despine</span> to remove the top and right spines of the plot in order to make it clearer. By `Yao Xiao <Charlie-XIAO>`

---

27412.fix.md

---

- <span class="title-ref">metrics.roc\_auc\_score</span> will now correctly return np.nan and warn user if only one class is present in the labels. By `Gleb Levitski <glevv>` and `Janez Demšar <janezd>`

---

28992.enhancement.md

---

- <span class="title-ref">sklearn.metrics.check\_scoring</span> now accepts <span class="title-ref">raise\_exc</span> to specify whether to raise an exception if a subset of the scorers in multimetric scoring fails or to return an error code. By `Stefanie Senger <StefanieSenger>`

---

29404.api.md

---

- The <span class="title-ref">assert\_all\_finite</span> parameter of functions <span class="title-ref">metrics.pairwise.check\_pairwise\_arrays</span> and <span class="title-ref">metrics.pairwise\_distances</span> is renamed into <span class="title-ref">ensure\_all\_finite</span>. <span class="title-ref">force\_all\_finite</span> will be removed in 1.8. By `Jérémie du Boisberranger <jeremiedb>`

---

29462.api.md

---

- <span class="title-ref">scoring="neg\_max\_error"</span> should be used instead of <span class="title-ref">scoring="max\_error"</span> which is now deprecated. By `Farid "Freddie" Taba <artificialfintelligence>`

---

29709.fix.md

---

- The functions <span class="title-ref">metrics.mean\_squared\_log\_error</span> and <span class="title-ref">metrics.root\_mean\_squared\_log\_error</span> now check whether the inputs are within the correct domain for the function \(y=\log(1+x)\), rather than \(y=\log(x)\). The functions <span class="title-ref">metrics.mean\_absolute\_error</span>, <span class="title-ref">metrics.mean\_absolute\_percentage\_error</span>, <span class="title-ref">metrics.mean\_squared\_error</span> and <span class="title-ref">metrics.root\_mean\_squared\_error</span> now explicitly check whether a scalar will be returned when <span class="title-ref">multioutput=uniform\_average</span>. By `Virgil Chan <virchan>`

---

29738.efficiency.md

---

- <span class="title-ref">sklearn.metrics.classification\_report</span> is now faster by caching classification labels. By `Adrin Jalali <adrinjalali>`

---

30001.api.md

---

- The default value of the <span class="title-ref">response\_method</span> parameter of <span class="title-ref">metrics.make\_scorer</span> will change from <span class="title-ref">None</span> to <span class="title-ref">"predict"</span> and <span class="title-ref">None</span> will be removed in 1.8. In the mean time, <span class="title-ref">None</span> is equivalent to <span class="title-ref">"predict"</span>. By `Jérémie du Boisberranger <jeremiedb>`

---

30013.fix.md

---

- <span class="title-ref">metrics.roc\_auc\_score</span> will now correctly return np.nan and warn user if only one class is present in the labels. By `Gleb Levitski <glevv>` and `Janez Demšar <janezd>`

---

28519.enhancement.md

---

- <span class="title-ref">\~model\_selection.GroupKFold</span> now has the ability to shuffle groups into different folds when <span class="title-ref">shuffle=True</span>. By `Zachary Vealey <zvealey>`

---

29402.fix.md

---

- Improve error message when <span class="title-ref">model\_selection.RepeatedStratifiedKFold.split</span> is called without a <span class="title-ref">y</span> argument By `Anurag Varma <Anurag-Varma>`

---

30172.enhancement.md

---

- There is no need to call <span class="title-ref">fit</span> on a <span class="title-ref">\~sklearn.model\_selection.FixedThresholdClassifier</span> if the underlying estimator is already fitted. By `Adrin Jalali <adrinjalali>`

---

25330.enhancement.md

---

- <span class="title-ref">neighbors.NearestNeighbors</span>, <span class="title-ref">neighbors.KNeighborsClassifier</span>, <span class="title-ref">neighbors.KNeighborsRegressor</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>, <span class="title-ref">neighbors.KNeighborsTransformer</span>, <span class="title-ref">neighbors.RadiusNeighborsTransformer</span>, and <span class="title-ref">neighbors.LocalOutlierFactor</span> now work with <span class="title-ref">metric="nan\_euclidean"</span>, supporting <span class="title-ref">nan</span> inputs. By `Carlo Lemos <vitaliset>`, [Guillaume Lemaitre](), and [Adrin Jalali]()

---

26689.enhancement.md

---

- Add <span class="title-ref">neighbors.NearestCentroid.decision\_function</span>, <span class="title-ref">neighbors.NearestCentroid.predict\_proba</span> and <span class="title-ref">neighbors.NearestCentroid.predict\_log\_proba</span> to the <span class="title-ref">neighbors.NearestCentroid</span> estimator class. Support the case when <span class="title-ref">X</span> is sparse and <span class="title-ref">shrinking\_threshold</span> is not <span class="title-ref">None</span> in <span class="title-ref">neighbors.NearestCentroid</span>. By `Matthew Ning <NoPenguinsLand>`

---

28773.fix.md

---

- <span class="title-ref">neighbors.LocalOutlierFactor</span> raises a warning in the <span class="title-ref">fit</span> method when duplicate values in the training data lead to inaccurate outlier detection. By `Henrique Caroço <HenriqueProj>`

---

30047.enhancement.md

---

- Make <span class="title-ref">predict</span>, <span class="title-ref">predict\_proba</span>, and <span class="title-ref">score</span> of <span class="title-ref">neighbors.KNeighborsClassifier</span> and <span class="title-ref">neighbors.RadiusNeighborsClassifier</span> accept <span class="title-ref">X=None</span> as input. In this case predictions for all training set points are returned, and points are not included into their own neighbors. By `Dmitry Kobak <dkobak>`

---

29773.fix.md

---

- <span class="title-ref">neural\_network.MLPRegressor</span> does no longer crash when the model diverges and that <span class="title-ref">early\_stopping</span> is enabled. By `Marc Bresson <MarcBresson>`

---

28901.major-feature.md

---

- <span class="title-ref">pipeline.Pipeline</span> can now transform metadata up to the step requiring the metadata, which can be set using the <span class="title-ref">transform\_input</span> parameter. By [Adrin Jalali]()

---

29868.enhancement.md

---

- <span class="title-ref">pipeline.Pipeline</span> now warns about not being fitted before calling methods that require the pipeline to be fitted. This warning will become an error in 1.8. By [Adrin Jalali]()

---

30203.fix.md

---

- Fixed an issue with tags and estimator type of <span class="title-ref">\~sklearn.pipeline.Pipeline</span> when pipeline is empty. This allows the HTML representation of an empty pipeline to be rendered correctly. By `Gennaro Daniele Acciaro <gdacciaro>`

---

27875.fix.md

---

- <span class="title-ref">preprocessing.PowerTransformer</span> now uses <span class="title-ref">scipy.special.inv\_boxcox</span> to output <span class="title-ref">nan</span> if the input of BoxCox's inverse is invalid. By `Xuefeng Xu <xuefeng-xu>`

---

28637.enhancement.md

---

- Added <span class="title-ref">warn</span> option to <span class="title-ref">handle\_unknown</span> parameter in <span class="title-ref">preprocessing.OneHotEncoder</span>. By `Gleb Levitski <glevv>`

---

29158.enhancement.md

---

- The HTML representation of <span class="title-ref">preprocessing.FunctionTransformer</span> will show the function name in the label. By `Yao Xiao <Charlie-XIAO>`

---

28494.api.md

---

- <span class="title-ref">semi\_supervised.SelfTrainingClassifier</span> deprecated the <span class="title-ref">base\_estimator</span> parameter in favor of <span class="title-ref">estimator</span>. By `Adam Li <adam2392>`

---

17575.fix.md

---

- Escape double quotes for labels and feature names when exporting trees to Graphviz format. By `Santiago M. Mola <smola>`.

---

27966.feature.md

---

- <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeRegressor</span> now support missing-values in the data matrix `X`. Missing-values are handled by randomly moving all of the samples to the left, or right child node as the tree is traversed. By `Adam Li <adam2392>`

---

29404.api.md

---

- The <span class="title-ref">assert\_all\_finite</span> parameter of functions <span class="title-ref">utils.check\_array</span>, <span class="title-ref">utils.check\_X\_y</span>, <span class="title-ref">utils.as\_float\_array</span> is renamed into <span class="title-ref">ensure\_all\_finite</span>. <span class="title-ref">force\_all\_finite</span> will be removed in 1.8. By `Jérémie du Boisberranger <jeremiedb>`

---

29540.enhancement.md

---

- <span class="title-ref">utils.check\_array</span> now accepts <span class="title-ref">ensure\_non\_negative</span> to check for negative values in the passed array, until now only available through calling <span class="title-ref">utils.check\_non\_negative</span>. By `Tamara Atanasoska <tamaraatanasoska>`

---

29818.api.md

---

- <span class="title-ref">check\_estimators.check\_sample\_weights\_invariance</span> replaced by <span class="title-ref">check\_estimators.check\_sample\_weight\_equivalence</span> which uses integer (including zero) weights. By `Antoine Baker <antoinebaker>`

---

29869.fix.md

---

- <span class="title-ref">utils.estimator\_checks.parametrize\_with\_checks</span> and <span class="title-ref">utils.estimator\_checks.check\_estimator</span> now support estimators that have <span class="title-ref">set\_output</span> called on them. By `Adrin Jalali <adrinjalali>`

---

29874.enhancement.md

---

- <span class="title-ref">\~sklearn.utils.estimator\_checks.check\_estimator</span> and <span class="title-ref">\~sklearn.utils.estimator\_checks.parametrize\_with\_checks</span> now check and fail if the classifier has the <span class="title-ref">tags.classifier\_tags.multi\_class = False</span> tag but does not fail on multi-class data. By [Adrin Jalali]()

---

29880.enhancement.md

---

- <span class="title-ref">utils.validation.check\_is\_fitted</span> now passes on stateless estimators. An estimator can indicate it's stateless by setting the <span class="title-ref">requires\_fit</span> tag. See \[estimator\_tags\](\#estimator\_tags) for more information. By `Adrin Jalali <adrinjalali>`

---

30122.api.md

---

- Using <span class="title-ref">\_estimator\_type</span> to set the estimator type is deprecated. Inherit from <span class="title-ref">\~sklearn.base.ClassifierMixin</span>, <span class="title-ref">\~sklearn.base.RegressorMixin</span>, <span class="title-ref">\~sklearn.base.TransformerMixin</span>, or <span class="title-ref">\~sklearn.base.OutlierMixin</span> instead. Alternatively, you can set <span class="title-ref">estimator\_type</span> in <span class="title-ref">\~sklearn.utils.Tags</span> in the <span class="title-ref">\_\_sklearn\_tags\_\_</span> method. By [Adrin Jalali]()

---

30149.enhancement.md

---

- Changes to <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span> and <span class="title-ref">\~utils.estimator\_checks.parametrize\_with\_checks</span>.
    
      - <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span> introduces new arguments: `on_skip`, `on_fail`, and `callback` to control the behavior of the check runner. Refer to the API documentation for more details.
    
      - `generate_only=True` is deprecated in <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span>. Use <span class="title-ref">\~utils.estimator\_checks.estimator\_checks\_generator</span> instead.
    
      - The `_xfail_checks` estimator tag is now removed, and now in order to indicate which tests are expected to fail, you can pass a dictionary to the <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span> as the `expected_failed_checks` parameter. Similarly, the `expected_failed_checks` parameter in <span class="title-ref">\~utils.estimator\_checks.parametrize\_with\_checks</span> can be used, which is a callable returning a dictionary of the form:
        
            {
                "check_name": "reason to mark this check as xfail",
            }
    
    By [Adrin Jalali]()

---

v0.13.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.13

## Version 0.13.1

**February 23, 2013**

The 0.13.1 release only fixes some bugs and does not add any new functionality.

### Changelog

  - Fixed a testing error caused by the function <span class="title-ref">cross\_validation.train\_test\_split</span> being interpreted as a test by [Yaroslav Halchenko](http://www.onerussian.com/).
  - Fixed a bug in the reassignment of small clusters in the <span class="title-ref">cluster.MiniBatchKMeans</span> by [Gael Varoquaux](http://gael-varoquaux.info).
  - Fixed default value of `gamma` in <span class="title-ref">decomposition.KernelPCA</span> by [Lars Buitinck](https://github.com/larsmans).
  - Updated joblib to `0.7.0d` by [Gael Varoquaux](http://gael-varoquaux.info).
  - Fixed scaling of the deviance in <span class="title-ref">ensemble.GradientBoostingClassifier</span> by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Better tie-breaking in <span class="title-ref">multiclass.OneVsOneClassifier</span> by [Andreas Müller](https://amueller.github.io/).
  - Other small improvements to tests and documentation.

### People

List of contributors for release 0.13.1 by number of commits.

  - 16 [Lars Buitinck](https://github.com/larsmans)
  - 12 [Andreas Müller](https://amueller.github.io/)
  - 8 [Gael Varoquaux](http://gael-varoquaux.info)
  - 5 Robert Marchman
  - 3 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 2 Hrishikesh Huilgolkar
  - 1 Bastiaan van den Berg
  - 1 Diego Molla
  - 1 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 1 [Mathieu Blondel](http://www.mblondel.org)
  - 1 [Nelle Varoquaux](https://github.com/nellev)
  - 1 Rafael Cunha de Almeida
  - 1 Rolando Espinoza La fuente
  - 1 [Vlad Niculae](https://vene.ro/)
  - 1 [Yaroslav Halchenko](http://www.onerussian.com/)

## Version 0.13

**January 21, 2013**

### New Estimator Classes

  - <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span>, two data-independent predictors by [Mathieu Blondel](http://www.mblondel.org). Useful to sanity-check your estimators. See \[dummy\_estimators\](\#dummy\_estimators) in the user guide. Multioutput support added by [Arnaud Joly](http://www.ajoly.org).
  - <span class="title-ref">decomposition.FactorAnalysis</span>, a transformer implementing the classical factor analysis, by [Christian Osendorfer](https://osdf.github.io) and [Alexandre Gramfort](http://alexandre.gramfort.net). See \[FA\](\#fa) in the user guide.
  - <span class="title-ref">feature\_extraction.FeatureHasher</span>, a transformer implementing the "hashing trick" for fast, low-memory feature extraction from string fields by [Lars Buitinck](https://github.com/larsmans) and <span class="title-ref">feature\_extraction.text.HashingVectorizer</span> for text documents by [Olivier Grisel](https://twitter.com/ogrisel) See \[feature\_hashing\](\#feature\_hashing) and \[hashing\_vectorizer\](\#hashing\_vectorizer) for the documentation and sample usage.
  - <span class="title-ref">pipeline.FeatureUnion</span>, a transformer that concatenates results of several other transformers by [Andreas Müller](https://amueller.github.io/). See \[feature\_union\](\#feature\_union) in the user guide.
  - <span class="title-ref">random\_projection.GaussianRandomProjection</span>, <span class="title-ref">random\_projection.SparseRandomProjection</span> and the function <span class="title-ref">random\_projection.johnson\_lindenstrauss\_min\_dim</span>. The first two are transformers implementing Gaussian and sparse random projection matrix by [Olivier Grisel](https://twitter.com/ogrisel) and [Arnaud Joly](http://www.ajoly.org). See \[random\_projection\](\#random\_projection) in the user guide.
  - <span class="title-ref">kernel\_approximation.Nystroem</span>, a transformer for approximating arbitrary kernels by [Andreas Müller](https://amueller.github.io/). See \[nystroem\_kernel\_approx\](\#nystroem\_kernel\_approx) in the user guide.
  - <span class="title-ref">preprocessing.OneHotEncoder</span>, a transformer that computes binary encodings of categorical features by [Andreas Müller](https://amueller.github.io/). See \[preprocessing\_categorical\_features\](\#preprocessing\_categorical\_features) in the user guide.
  - <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span> and <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span>, predictors implementing an efficient stochastic optimization for linear models by [Rob Zinkov](https://www.zinkov.com/) and [Mathieu Blondel](http://www.mblondel.org). See \[passive\_aggressive\](\#passive\_aggressive) in the user guide.
  - <span class="title-ref">ensemble.RandomTreesEmbedding</span>, a transformer for creating high-dimensional sparse representations using ensembles of totally random trees by [Andreas Müller](https://amueller.github.io/). See \[random\_trees\_embedding\](\#random\_trees\_embedding) in the user guide.
  - <span class="title-ref">manifold.SpectralEmbedding</span> and function <span class="title-ref">manifold.spectral\_embedding</span>, implementing the "laplacian eigenmaps" transformation for non-linear dimensionality reduction by Wei Li. See \[spectral\_embedding\](\#spectral\_embedding) in the user guide.
  - <span class="title-ref">isotonic.IsotonicRegression</span> by [Fabian Pedregosa](http://fa.bianp.net), [Alexandre Gramfort](http://alexandre.gramfort.net) and [Nelle Varoquaux](https://github.com/nellev),

### Changelog

  - <span class="title-ref">metrics.zero\_one\_loss</span> (formerly `metrics.zero_one`) now has option for normalized output that reports the fraction of misclassifications, rather than the raw number of misclassifications. By Kyle Beauchamp.
  - <span class="title-ref">tree.DecisionTreeClassifier</span> and all derived ensemble models now support sample weighting, by [Noel Dawe](https://github.com/ndawe) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Speedup improvement when using bootstrap samples in forests of randomized trees, by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Partial dependence plots for \[gradient\_boosting\](\#gradient\_boosting) in <span class="title-ref">ensemble.partial\_dependence.partial\_dependence</span> by [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/). See \[sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py\](\#sphx\_glr\_auto\_examples\_inspection\_plot\_partial\_dependence.py) for an example.
  - The table of contents on the website has now been made expandable by [Jaques Grobler](https://github.com/jaquesgrobler).
  - <span class="title-ref">feature\_selection.SelectPercentile</span> now breaks ties deterministically instead of returning all equally ranked features.
  - <span class="title-ref">feature\_selection.SelectKBest</span> and <span class="title-ref">feature\_selection.SelectPercentile</span> are more numerically stable since they use scores, rather than p-values, to rank results. This means that they might sometimes select different features than they did previously.
  - Ridge regression and ridge classification fitting with `sparse_cg` solver no longer has quadratic memory complexity, by [Lars Buitinck](https://github.com/larsmans) and [Fabian Pedregosa](http://fa.bianp.net).
  - Ridge regression and ridge classification now support a new fast solver called `lsqr`, by [Mathieu Blondel](http://www.mblondel.org).
  - Speed up of <span class="title-ref">metrics.precision\_recall\_curve</span> by Conrad Lee.
  - Added support for reading/writing svmlight files with pairwise preference attribute (qid in svmlight file format) in <span class="title-ref">datasets.dump\_svmlight\_file</span> and <span class="title-ref">datasets.load\_svmlight\_file</span> by [Fabian Pedregosa](http://fa.bianp.net).
  - Faster and more robust <span class="title-ref">metrics.confusion\_matrix</span> and \[clustering\_evaluation\](\#clustering\_evaluation) by Wei Li.
  - <span class="title-ref">cross\_validation.cross\_val\_score</span> now works with precomputed kernels and affinity matrices, by [Andreas Müller](https://amueller.github.io/).
  - LARS algorithm made more numerically stable with heuristics to drop regressors too correlated as well as to stop the path when numerical noise becomes predominant, by [Gael Varoquaux](http://gael-varoquaux.info).
  - Faster implementation of <span class="title-ref">metrics.precision\_recall\_curve</span> by Conrad Lee.
  - New kernel <span class="title-ref">metrics.chi2\_kernel</span> by [Andreas Müller](https://amueller.github.io/), often used in computer vision applications.
  - Fix of longstanding bug in <span class="title-ref">naive\_bayes.BernoulliNB</span> fixed by Shaun Jackman.
  - Implemented `predict_proba` in <span class="title-ref">multiclass.OneVsRestClassifier</span>, by Andrew Winterman.
  - Improve consistency in gradient boosting: estimators <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> use the estimator <span class="title-ref">tree.DecisionTreeRegressor</span> instead of the <span class="title-ref">tree.\_tree.Tree</span> data structure by [Arnaud Joly](http://www.ajoly.org).
  - Fixed a floating point exception in the \[decision trees \<tree\>\](\#decision-trees-\<tree\>) module, by Seberg.
  - Fix <span class="title-ref">metrics.roc\_curve</span> fails when y\_true has only one class by Wei Li.
  - Add the <span class="title-ref">metrics.mean\_absolute\_error</span> function which computes the mean absolute error. The <span class="title-ref">metrics.mean\_squared\_error</span>, <span class="title-ref">metrics.mean\_absolute\_error</span> and <span class="title-ref">metrics.r2\_score</span> metrics support multioutput by [Arnaud Joly](http://www.ajoly.org).
  - Fixed `class_weight` support in <span class="title-ref">svm.LinearSVC</span> and <span class="title-ref">linear\_model.LogisticRegression</span> by [Andreas Müller](https://amueller.github.io/). The meaning of `class_weight` was reversed as erroneously higher weight meant less positives of a given class in earlier releases.
  - Improve narrative documentation and consistency in `sklearn.metrics` for regression and classification metrics by [Arnaud Joly](http://www.ajoly.org).
  - Fixed a bug in <span class="title-ref">sklearn.svm.SVC</span> when using csr-matrices with unsorted indices by Xinfan Meng and [Andreas Müller](https://amueller.github.io/).
  - \`cluster.MiniBatchKMeans\`: Add random reassignment of cluster centers with little observations attached to them, by [Gael Varoquaux](http://gael-varoquaux.info).

### API changes summary

  - Renamed all occurrences of `n_atoms` to `n_components` for consistency. This applies to <span class="title-ref">decomposition.DictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.dict\_learning</span>, <span class="title-ref">decomposition.dict\_learning\_online</span>.
  - Renamed all occurrences of `max_iters` to `max_iter` for consistency. This applies to <span class="title-ref">semi\_supervised.LabelPropagation</span> and <span class="title-ref">semi\_supervised.label\_propagation.LabelSpreading</span>.
  - Renamed all occurrences of `learn_rate` to `learning_rate` for consistency in <span class="title-ref">ensemble.BaseGradientBoosting</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span>.
  - The module `sklearn.linear_model.sparse` is gone. Sparse matrix support was already integrated into the "regular" linear models.
  - <span class="title-ref">sklearn.metrics.mean\_square\_error</span>, which incorrectly returned the accumulated error, was removed. Use <span class="title-ref">metrics.mean\_squared\_error</span> instead.
  - Passing `class_weight` parameters to `fit` methods is no longer supported. Pass them to estimator constructors instead.
  - GMMs no longer have `decode` and `rvs` methods. Use the `score`, `predict` or `sample` methods instead.
  - The `solver` fit option in Ridge regression and classification is now deprecated and will be removed in v0.14. Use the constructor option instead.
  - <span class="title-ref">feature\_extraction.text.DictVectorizer</span> now returns sparse matrices in the CSR format, instead of COO.
  - Renamed `k` in <span class="title-ref">cross\_validation.KFold</span> and <span class="title-ref">cross\_validation.StratifiedKFold</span> to `n_folds`, renamed `n_bootstraps` to `n_iter` in `cross_validation.Bootstrap`.
  - Renamed all occurrences of `n_iterations` to `n_iter` for consistency. This applies to <span class="title-ref">cross\_validation.ShuffleSplit</span>, <span class="title-ref">cross\_validation.StratifiedShuffleSplit</span>, <span class="title-ref">utils.extmath.randomized\_range\_finder</span> and <span class="title-ref">utils.extmath.randomized\_svd</span>.
  - Replaced `rho` in <span class="title-ref">linear\_model.ElasticNet</span> and <span class="title-ref">linear\_model.SGDClassifier</span> by `l1_ratio`. The `rho` parameter had different meanings; `l1_ratio` was introduced to avoid confusion. It has the same meaning as previously `rho` in <span class="title-ref">linear\_model.ElasticNet</span> and `(1-rho)` in <span class="title-ref">linear\_model.SGDClassifier</span>.
  - <span class="title-ref">linear\_model.LassoLars</span> and <span class="title-ref">linear\_model.Lars</span> now store a list of paths in the case of multiple targets, rather than an array of paths.
  - The attribute `gmm` of <span class="title-ref">hmm.GMMHMM</span> was renamed to `gmm_` to adhere more strictly with the API.
  - <span class="title-ref">cluster.spectral\_embedding</span> was moved to <span class="title-ref">manifold.spectral\_embedding</span>.
  - Renamed `eig_tol` in <span class="title-ref">manifold.spectral\_embedding</span>, <span class="title-ref">cluster.SpectralClustering</span> to `eigen_tol`, renamed `mode` to `eigen_solver`.
  - Renamed `mode` in <span class="title-ref">manifold.spectral\_embedding</span> and <span class="title-ref">cluster.SpectralClustering</span> to `eigen_solver`.
  - `classes_` and `n_classes_` attributes of <span class="title-ref">tree.DecisionTreeClassifier</span> and all derived ensemble models are now flat in case of single output problems and nested in case of multi-output problems.
  - The `estimators_` attribute of <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> is now an array of <span class="title-ref">tree.DecisionTreeRegressor</span>.
  - Renamed `chunk_size` to `batch_size` in <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> and <span class="title-ref">decomposition.MiniBatchSparsePCA</span> for consistency.
  - <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.NuSVC</span> now provide a `classes_` attribute and support arbitrary dtypes for labels `y`. Also, the dtype returned by `predict` now reflects the dtype of `y` during `fit` (used to be `np.float`).
  - Changed default test\_size in <span class="title-ref">cross\_validation.train\_test\_split</span> to None, added possibility to infer `test_size` from `train_size` in <span class="title-ref">cross\_validation.ShuffleSplit</span> and <span class="title-ref">cross\_validation.StratifiedShuffleSplit</span>.
  - Renamed function <span class="title-ref">sklearn.metrics.zero\_one</span> to <span class="title-ref">sklearn.metrics.zero\_one\_loss</span>. Be aware that the default behavior in <span class="title-ref">sklearn.metrics.zero\_one\_loss</span> is different from \`sklearn.metrics.zero\_one\`: `normalize=False` is changed to `normalize=True`.
  - Renamed function <span class="title-ref">metrics.zero\_one\_score</span> to <span class="title-ref">metrics.accuracy\_score</span>.
  - <span class="title-ref">datasets.make\_circles</span> now has the same number of inner and outer points.
  - In the Naive Bayes classifiers, the `class_prior` parameter was moved from `fit` to `__init__`.

### People

List of contributors for release 0.13 by number of commits.

  - 364 [Andreas Müller](https://amueller.github.io/)
  - 143 [Arnaud Joly](http://www.ajoly.org)
  - 137 [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/)
  - 131 [Gael Varoquaux](http://gael-varoquaux.info)
  - 117 [Mathieu Blondel](http://www.mblondel.org)
  - 108 [Lars Buitinck](https://github.com/larsmans)
  - 106 Wei Li
  - 101 [Olivier Grisel](https://twitter.com/ogrisel)
  - 65 [Vlad Niculae](https://vene.ro/)
  - 54 [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/)
  - 40 [Jaques Grobler](https://github.com/jaquesgrobler)
  - 38 [Alexandre Gramfort](http://alexandre.gramfort.net)
  - 30 [Rob Zinkov](https://www.zinkov.com/)
  - 19 Aymeric Masurelle
  - 18 Andrew Winterman
  - 17 [Fabian Pedregosa](http://fa.bianp.net)
  - 17 Nelle Varoquaux
  - 16 [Christian Osendorfer](https://osdf.github.io)
  - 14 [Daniel Nouri](http://danielnouri.org)
  - 13 `Virgile Fritsch <VirgileFritsch>`
  - 13 syhw
  - 12 [Satrajit Ghosh](https://www.mit.edu/~satra/)
  - 10 Corey Lynch
  - 10 Kyle Beauchamp
  - 9 Brian Cheung
  - 9 Immanuel Bayer
  - 9 mr.Shu
  - 8 Conrad Lee
  - 8 [James Bergstra](http://www-etud.iro.umontreal.ca/~bergstrj/)
  - 7 Tadej Janež
  - 6 Brian Cajes
  - 6 [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - 6 Michael
  - 6 Noel Dawe
  - 6 Tiago Nunes
  - 6 cow
  - 5 Anze
  - 5 Shiqiao Du
  - 4 Christian Jauvin
  - 4 Jacques Kvam
  - 4 Richard T. Guy
  - 4 [Robert Layton](https://twitter.com/robertlayton)
  - 3 Alexandre Abraham
  - 3 Doug Coleman
  - 3 Scott Dickerson
  - 2 ApproximateIdentity
  - 2 John Benediktsson
  - 2 Mark Veronda
  - 2 Matti Lyra
  - 2 Mikhail Korobov
  - 2 Xinfan Meng
  - 1 Alejandro Weinstein
  - 1 [Alexandre Passos](http://atpassos.me)
  - 1 Christoph Deil
  - 1 Eugene Nizhibitsky
  - 1 Kenneth C. Arnold
  - 1 Luis Pedro Coelho
  - 1 Miroslav Batchkarov
  - 1 Pavel
  - 1 Sebastian Berg
  - 1 Shaun Jackman
  - 1 Subhodeep Moitra
  - 1 bob
  - 1 dengemann
  - 1 emanuele
  - 1 x006

---

v0.14.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.14

## Version 0.14

**August 7, 2013**

### Changelog

  - Missing values with sparse and dense matrices can be imputed with the transformer <span class="title-ref">preprocessing.Imputer</span> by [Nicolas Trésegnie](https://github.com/NicolasTr).
  - The core implementation of decisions trees has been rewritten from scratch, allowing for faster tree induction and lower memory consumption in all tree-based estimators. By [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Added <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span>, by [Noel Dawe](https://github.com/ndawe) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/). See the \[AdaBoost \<adaboost\>\](\#adaboost-\<adaboost\>) section of the user guide for details and examples.
  - Added <span class="title-ref">grid\_search.RandomizedSearchCV</span> and <span class="title-ref">grid\_search.ParameterSampler</span> for randomized hyperparameter optimization. By [Andreas Müller](https://amueller.github.io/).
  - Added \[biclustering \<biclustering\>\](\#biclustering-\<biclustering\>) algorithms (<span class="title-ref">sklearn.cluster.bicluster.SpectralCoclustering</span> and <span class="title-ref">sklearn.cluster.bicluster.SpectralBiclustering</span>), data generation methods (<span class="title-ref">sklearn.datasets.make\_biclusters</span> and <span class="title-ref">sklearn.datasets.make\_checkerboard</span>), and scoring metrics (<span class="title-ref">sklearn.metrics.consensus\_score</span>). By [Kemal Eren](http://www.kemaleren.com).
  - Added \[Restricted Boltzmann Machines\<rbm\>\](\#restricted-boltzmann-machines\<rbm\>) (<span class="title-ref">neural\_network.BernoulliRBM</span>). By [Yann Dauphin](https://ynd.github.io/).
  - Python 3 support by `Justin Vincent <justinvf>`, [Lars Buitinck](https://github.com/larsmans), `Subhodeep Moitra <smoitra87>` and [Olivier Grisel](https://twitter.com/ogrisel). All tests now pass under Python 3.3.
  - Ability to pass one penalty (alpha value) per target in <span class="title-ref">linear\_model.Ridge</span>, by @eickenberg and [Mathieu Blondel](http://www.mblondel.org).
  - Fixed <span class="title-ref">sklearn.linear\_model.stochastic\_gradient.py</span> L2 regularization issue (minor practical significance). By `Norbert Crombach <norbert>` and [Mathieu Blondel](http://www.mblondel.org) .
  - Added an interactive version of [Andreas Müller](https://amueller.github.io/)'s [Machine Learning Cheat Sheet (for scikit-learn)](https://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html) to the documentation. See \[Choosing the right estimator \<ml\_map\>\](\#choosing-the-right-estimator-\<ml\_map\>). By [Jaques Grobler](https://github.com/jaquesgrobler).
  - <span class="title-ref">grid\_search.GridSearchCV</span> and <span class="title-ref">cross\_validation.cross\_val\_score</span> now support the use of advanced scoring function such as area under the ROC curve and f-beta scores. See \[scoring\_parameter\](\#scoring\_parameter) for details. By [Andreas Müller](https://amueller.github.io/) and [Lars Buitinck](https://github.com/larsmans). Passing a function from `sklearn.metrics` as `score_func` is deprecated.
  - Multi-label classification output is now supported by <span class="title-ref">metrics.accuracy\_score</span>, <span class="title-ref">metrics.zero\_one\_loss</span>, <span class="title-ref">metrics.f1\_score</span>, <span class="title-ref">metrics.fbeta\_score</span>, <span class="title-ref">metrics.classification\_report</span>, <span class="title-ref">metrics.precision\_score</span> and <span class="title-ref">metrics.recall\_score</span> by [Arnaud Joly](http://www.ajoly.org).
  - Two new metrics <span class="title-ref">metrics.hamming\_loss</span> and <span class="title-ref">metrics.jaccard\_similarity\_score</span> are added with multi-label support by [Arnaud Joly](http://www.ajoly.org).
  - Speed and memory usage improvements in <span class="title-ref">feature\_extraction.text.CountVectorizer</span> and <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span>, by Jochen Wersdörfer and Roman Sinayev.
  - The `min_df` parameter in <span class="title-ref">feature\_extraction.text.CountVectorizer</span> and <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span>, which used to be 2, has been reset to 1 to avoid unpleasant surprises (empty vocabularies) for novice users who try it out on tiny document collections. A value of at least 2 is still recommended for practical use.
  - <span class="title-ref">svm.LinearSVC</span>, <span class="title-ref">linear\_model.SGDClassifier</span> and <span class="title-ref">linear\_model.SGDRegressor</span> now have a `sparsify` method that converts their `coef_` into a sparse matrix, meaning stored models trained using these estimators can be made much more compact.
  - <span class="title-ref">linear\_model.SGDClassifier</span> now produces multiclass probability estimates when trained under log loss or modified Huber loss.
  - Hyperlinks to documentation in example code on the website by `Martin Luessi <mluessi>`.
  - Fixed bug in <span class="title-ref">preprocessing.MinMaxScaler</span> causing incorrect scaling of the features for non-default `feature_range` settings. By [Andreas Müller](https://amueller.github.io/).
  - `max_features` in <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span> and all derived ensemble estimators now supports percentage values. By [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Performance improvements in <span class="title-ref">isotonic.IsotonicRegression</span> by [Nelle Varoquaux](https://github.com/nellev).
  - <span class="title-ref">metrics.accuracy\_score</span> has an option normalize to return the fraction or the number of correctly classified sample by [Arnaud Joly](http://www.ajoly.org).
  - Added <span class="title-ref">metrics.log\_loss</span> that computes log loss, aka cross-entropy loss. By Jochen Wersdörfer and [Lars Buitinck](https://github.com/larsmans).
  - A bug that caused <span class="title-ref">ensemble.AdaBoostClassifier</span>'s to output incorrect probabilities has been fixed.
  - Feature selectors now share a mixin providing consistent `transform`, `inverse_transform` and `get_support` methods. By [Joel Nothman](https://joelnothman.com/).
  - A fitted <span class="title-ref">grid\_search.GridSearchCV</span> or <span class="title-ref">grid\_search.RandomizedSearchCV</span> can now generally be pickled. By [Joel Nothman](https://joelnothman.com/).
  - Refactored and vectorized implementation of <span class="title-ref">metrics.roc\_curve</span> and <span class="title-ref">metrics.precision\_recall\_curve</span>. By [Joel Nothman](https://joelnothman.com/).
  - The new estimator <span class="title-ref">sklearn.decomposition.TruncatedSVD</span> performs dimensionality reduction using SVD on sparse matrices, and can be used for latent semantic analysis (LSA). By [Lars Buitinck](https://github.com/larsmans).
  - Added self-contained example of out-of-core learning on text data \[sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py\](\#sphx\_glr\_auto\_examples\_applications\_plot\_out\_of\_core\_classification.py). By `Eustache Diemert <oddskool>`.
  - The default number of components for <span class="title-ref">sklearn.decomposition.RandomizedPCA</span> is now correctly documented to be `n_features`. This was the default behavior, so programs using it will continue to work as they did.
  - <span class="title-ref">sklearn.cluster.KMeans</span> now fits several orders of magnitude faster on sparse data (the speedup depends on the sparsity). By [Lars Buitinck](https://github.com/larsmans).
  - Reduce memory footprint of FastICA by [Denis Engemann](http://denis-engemann.de) and [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Verbose output in <span class="title-ref">sklearn.ensemble.gradient\_boosting</span> now uses a column format and prints progress in decreasing frequency. It also shows the remaining time. By [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - <span class="title-ref">sklearn.ensemble.gradient\_boosting</span> provides out-of-bag improvement <span class="title-ref">oob\_improvement\_</span> rather than the OOB score for model selection. An example that shows how to use OOB estimates to select the number of trees was added. By [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Most metrics now support string labels for multiclass classification by [Arnaud Joly](http://www.ajoly.org) and [Lars Buitinck](https://github.com/larsmans).
  - New OrthogonalMatchingPursuitCV class by [Alexandre Gramfort](http://alexandre.gramfort.net) and [Vlad Niculae](https://vene.ro/).
  - Fixed a bug in \`sklearn.covariance.GraphLassoCV\`: the 'alphas' parameter now works as expected when given a list of values. By Philippe Gervais.
  - Fixed an important bug in <span class="title-ref">sklearn.covariance.GraphLassoCV</span> that prevented all folds provided by a CV object to be used (only the first 3 were used). When providing a CV object, execution time may thus increase significantly compared to the previous version (bug results are correct now). By Philippe Gervais.
  - <span class="title-ref">cross\_validation.cross\_val\_score</span> and the <span class="title-ref">grid\_search</span> module is now tested with multi-output data by [Arnaud Joly](http://www.ajoly.org).
  - <span class="title-ref">datasets.make\_multilabel\_classification</span> can now return the output in label indicator multilabel format by [Arnaud Joly](http://www.ajoly.org).
  - K-nearest neighbors, <span class="title-ref">neighbors.KNeighborsRegressor</span> and <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>, and radius neighbors, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span> and <span class="title-ref">neighbors.RadiusNeighborsClassifier</span> support multioutput data by [Arnaud Joly](http://www.ajoly.org).
  - Random state in LibSVM-based estimators (<span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.OneClassSVM</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.NuSVR</span>) can now be controlled. This is useful to ensure consistency in the probability estimates for the classifiers trained with `probability=True`. By [Vlad Niculae](https://vene.ro/).
  - Out-of-core learning support for discrete naive Bayes classifiers <span class="title-ref">sklearn.naive\_bayes.MultinomialNB</span> and <span class="title-ref">sklearn.naive\_bayes.BernoulliNB</span> by adding the `partial_fit` method by [Olivier Grisel](https://twitter.com/ogrisel).
  - New website design and navigation by [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/), [Nelle Varoquaux](https://github.com/nellev), Vincent Michel and [Andreas Müller](https://amueller.github.io/).

<!-- end list -->

  - \- Improved documentation on \[multi-class, multi-label and multi-output  
    classification \<multiclass\>\](\#multi-class,-multi-label-and-multi-output

\--classification-\<multiclass\>) by [Yannick Schwartz](https://team.inria.fr/parietal/schwarty/) and [Arnaud Joly](http://www.ajoly.org).

  - Better input and error handling in the `sklearn.metrics` module by [Arnaud Joly](http://www.ajoly.org) and [Joel Nothman](https://joelnothman.com/).
  - Speed optimization of the <span class="title-ref">hmm</span> module by `Mikhail Korobov <kmike>`
  - Significant speed improvements for <span class="title-ref">sklearn.cluster.DBSCAN</span> by [cleverless](https://github.com/cleverless)

### API changes summary

  - The <span class="title-ref">auc\_score</span> was renamed <span class="title-ref">metrics.roc\_auc\_score</span>.
  - Testing scikit-learn with `sklearn.test()` is deprecated. Use `nosetests sklearn` from the command line.
  - Feature importances in <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span> and all derived ensemble estimators are now computed on the fly when accessing the `feature_importances_` attribute. Setting `compute_importances=True` is no longer required. By [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - <span class="title-ref">linear\_model.lasso\_path</span> and <span class="title-ref">linear\_model.enet\_path</span> can return its results in the same format as that of <span class="title-ref">linear\_model.lars\_path</span>. This is done by setting the `return_models` parameter to `False`. By [Jaques Grobler](https://github.com/jaquesgrobler) and [Alexandre Gramfort](http://alexandre.gramfort.net)
  - <span class="title-ref">grid\_search.IterGrid</span> was renamed to <span class="title-ref">grid\_search.ParameterGrid</span>.
  - Fixed bug in <span class="title-ref">KFold</span> causing imperfect class balance in some cases. By [Alexandre Gramfort](http://alexandre.gramfort.net) and Tadej Janež.
  - <span class="title-ref">sklearn.neighbors.BallTree</span> has been refactored, and a <span class="title-ref">sklearn.neighbors.KDTree</span> has been added which shares the same interface. The Ball Tree now works with a wide variety of distance metrics. Both classes have many new methods, including single-tree and dual-tree queries, breadth-first and depth-first searching, and more advanced queries such as kernel density estimation and 2-point correlation functions. By [Jake Vanderplas](https://staff.washington.edu/jakevdp/)
  - Support for scipy.spatial.cKDTree within neighbors queries has been removed, and the functionality replaced with the new <span class="title-ref">sklearn.neighbors.KDTree</span> class.
  - <span class="title-ref">sklearn.neighbors.KernelDensity</span> has been added, which performs efficient kernel density estimation with a variety of kernels.
  - <span class="title-ref">sklearn.decomposition.KernelPCA</span> now always returns output with `n_components` components, unless the new parameter `remove_zero_eig` is set to `True`. This new behavior is consistent with the way kernel PCA was always documented; previously, the removal of components with zero eigenvalues was tacitly performed on all data.
  - `gcv_mode="auto"` no longer tries to perform SVD on a densified sparse matrix in <span class="title-ref">sklearn.linear\_model.RidgeCV</span>.
  - Sparse matrix support in <span class="title-ref">sklearn.decomposition.RandomizedPCA</span> is now deprecated in favor of the new `TruncatedSVD`.
  - <span class="title-ref">cross\_validation.KFold</span> and <span class="title-ref">cross\_validation.StratifiedKFold</span> now enforce <span class="title-ref">n\_folds \>= 2</span> otherwise a `ValueError` is raised. By [Olivier Grisel](https://twitter.com/ogrisel).
  - <span class="title-ref">datasets.load\_files</span>'s `charset` and `charset_errors` parameters were renamed `encoding` and `decode_errors`.
  - Attribute `oob_score_` in <span class="title-ref">sklearn.ensemble.GradientBoostingRegressor</span> and <span class="title-ref">sklearn.ensemble.GradientBoostingClassifier</span> is deprecated and has been replaced by `oob_improvement_` .
  - Attributes in OrthogonalMatchingPursuit have been deprecated (copy\_X, Gram, ...) and precompute\_gram renamed precompute for consistency. See \#2224.
  - <span class="title-ref">sklearn.preprocessing.StandardScaler</span> now converts integer input to float, and raises a warning. Previously it rounded for dense integer input.
  - <span class="title-ref">sklearn.multiclass.OneVsRestClassifier</span> now has a `decision_function` method. This will return the distance of each sample from the decision boundary for each class, as long as the underlying estimators implement the `decision_function` method. By [Kyle Kastner](https://kastnerkyle.github.io/).
  - Better input validation, warning on unexpected shapes for y.

### People

List of contributors for release 0.14 by number of commits.

  - 277 Gilles Louppe
  - 245 Lars Buitinck
  - 187 Andreas Mueller
  - 124 Arnaud Joly
  - 112 Jaques Grobler
  - 109 Gael Varoquaux
  - 107 Olivier Grisel
  - 102 Noel Dawe
  - 99 Kemal Eren
  - 79 Joel Nothman
  - 75 Jake VanderPlas
  - 73 Nelle Varoquaux
  - 71 Vlad Niculae
  - 65 Peter Prettenhofer
  - 64 Alexandre Gramfort
  - 54 Mathieu Blondel
  - 38 Nicolas Trésegnie
  - 35 eustache
  - 27 Denis Engemann
  - 25 Yann N. Dauphin
  - 19 Justin Vincent
  - 17 Robert Layton
  - 15 Doug Coleman
  - 14 Michael Eickenberg
  - 13 Robert Marchman
  - 11 Fabian Pedregosa
  - 11 Philippe Gervais
  - 10 Jim Holmström
  - 10 Tadej Janež
  - 10 syhw
  - 9 Mikhail Korobov
  - 9 Steven De Gryze
  - 8 sergeyf
  - 7 Ben Root
  - 7 Hrishikesh Huilgolkar
  - 6 Kyle Kastner
  - 6 Martin Luessi
  - 6 Rob Speer
  - 5 Federico Vaggi
  - 5 Raul Garreta
  - 5 Rob Zinkov
  - 4 Ken Geis
  - 3 A. Flaxman
  - 3 Denton Cockburn
  - 3 Dougal Sutherland
  - 3 Ian Ozsvald
  - 3 Johannes Schönberger
  - 3 Robert McGibbon
  - 3 Roman Sinayev
  - 3 Szabo Roland
  - 2 Diego Molla
  - 2 Imran Haque
  - 2 Jochen Wersdörfer
  - 2 Sergey Karayev
  - 2 Yannick Schwartz
  - 2 jamestwebber
  - 1 Abhijeet Kolhe
  - 1 Alexander Fabisch
  - 1 Bastiaan van den Berg
  - 1 Benjamin Peterson
  - 1 Daniel Velkov
  - 1 Fazlul Shahriar
  - 1 Felix Brockherde
  - 1 Félix-Antoine Fortin
  - 1 Harikrishnan S
  - 1 Jack Hale
  - 1 JakeMick
  - 1 James McDermott
  - 1 John Benediktsson
  - 1 John Zwinck
  - 1 Joshua Vredevoogd
  - 1 Justin Pati
  - 1 Kevin Hughes
  - 1 Kyle Kelley
  - 1 Matthias Ekman
  - 1 Miroslav Shubernetskiy
  - 1 Naoki Orii
  - 1 Norbert Crombach
  - 1 Rafael Cunha de Almeida
  - 1 Rolando Espinoza La fuente
  - 1 Seamus Abshere
  - 1 Sergey Feldman
  - 1 Sergio Medina
  - 1 Stefano Lattarini
  - 1 Steve Koch
  - 1 Sturla Molden
  - 1 Thomas Jarosch
  - 1 Yaroslav Halchenko

---

v0.15.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.15

## Version 0.15.2

**September 4, 2014**

### Bug fixes

  - Fixed handling of the `p` parameter of the Minkowski distance that was previously ignored in nearest neighbors models. By `Nikolay
    Mayorov <nmayorov>`.
  - Fixed duplicated alphas in <span class="title-ref">linear\_model.LassoLars</span> with early stopping on 32 bit Python. By [Olivier Grisel](https://twitter.com/ogrisel) and [Fabian Pedregosa](http://fa.bianp.net).
  - Fixed the build under Windows when scikit-learn is built with MSVC while NumPy is built with MinGW. By [Olivier Grisel](https://twitter.com/ogrisel) and `Federico
    Vaggi <FedericoV>`.
  - Fixed an array index overflow bug in the coordinate descent solver. By [Gael Varoquaux](http://gael-varoquaux.info).
  - Better handling of numpy 1.9 deprecation warnings. By [Gael Varoquaux](http://gael-varoquaux.info).
  - Removed unnecessary data copy in <span class="title-ref">cluster.KMeans</span>. By [Gael Varoquaux](http://gael-varoquaux.info).
  - Explicitly close open files to avoid `ResourceWarnings` under Python 3. By Calvin Giles.
  - The `transform` of <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now projects the input on the most discriminant directions. By Martin Billinger.
  - Fixed potential overflow in `_tree.safe_realloc` by [Lars Buitinck](https://github.com/larsmans).
  - Performance optimization in <span class="title-ref">isotonic.IsotonicRegression</span>. By Robert Bradshaw.
  - `nose` is non-longer a runtime dependency to import `sklearn`, only for running the tests. By [Joel Nothman](https://joelnothman.com/).
  - Many documentation and website fixes by [Joel Nothman](https://joelnothman.com/), [Lars Buitinck](https://github.com/larsmans) `Matt Pico <MattpSoftware>`, and others.

## Version 0.15.1

**August 1, 2014**

### Bug fixes

  - Made <span class="title-ref">cross\_validation.cross\_val\_score</span> use <span class="title-ref">cross\_validation.KFold</span> instead of <span class="title-ref">cross\_validation.StratifiedKFold</span> on multi-output classification problems. By `Nikolay Mayorov <nmayorov>`.
  - Support unseen labels <span class="title-ref">preprocessing.LabelBinarizer</span> to restore the default behavior of 0.14.1 for backward compatibility. By `Hamzeh Alsalhi <hamsal>`.
  - Fixed the <span class="title-ref">cluster.KMeans</span> stopping criterion that prevented early convergence detection. By Edward Raff and [Gael Varoquaux](http://gael-varoquaux.info).
  - Fixed the behavior of <span class="title-ref">multiclass.OneVsOneClassifier</span>. in case of ties at the per-class vote level by computing the correct per-class sum of prediction scores. By [Andreas Müller](https://amueller.github.io/).
  - Made <span class="title-ref">cross\_validation.cross\_val\_score</span> and <span class="title-ref">grid\_search.GridSearchCV</span> accept Python lists as input data. This is especially useful for cross-validation and model selection of text processing pipelines. By [Andreas Müller](https://amueller.github.io/).
  - Fixed data input checks of most estimators to accept input data that implements the NumPy `__array__` protocol. This is the case for for `pandas.Series` and `pandas.DataFrame` in recent versions of pandas. By [Gael Varoquaux](http://gael-varoquaux.info).
  - Fixed a regression for <span class="title-ref">linear\_model.SGDClassifier</span> with `class_weight="auto"` on data with non-contiguous labels. By [Olivier Grisel](https://twitter.com/ogrisel).

## Version 0.15

**July 15, 2014**

### Highlights

  - Many speed and memory improvements all across the code
  - Huge speed and memory improvements to random forests (and extra trees) that also benefit better from parallel computing.
  - Incremental fit to <span class="title-ref">BernoulliRBM \<neural\_network.BernoulliRBM\></span>
  - Added <span class="title-ref">cluster.AgglomerativeClustering</span> for hierarchical agglomerative clustering with average linkage, complete linkage and ward strategies.
  - Added <span class="title-ref">linear\_model.RANSACRegressor</span> for robust regression models.
  - Added dimensionality reduction with <span class="title-ref">manifold.TSNE</span> which can be used to visualize high-dimensional data.

### Changelog

#### New features

  - Added <span class="title-ref">ensemble.BaggingClassifier</span> and <span class="title-ref">ensemble.BaggingRegressor</span> meta-estimators for ensembling any kind of base estimator. See the \[Bagging \<bagging\>\](\#bagging-\<bagging\>) section of the user guide for details and examples. By [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - New unsupervised feature selection algorithm <span class="title-ref">feature\_selection.VarianceThreshold</span>, by [Lars Buitinck](https://github.com/larsmans).
  - Added <span class="title-ref">linear\_model.RANSACRegressor</span> meta-estimator for the robust fitting of regression models. By `Johannes Schönberger <ahojnnes>`.
  - Added <span class="title-ref">cluster.AgglomerativeClustering</span> for hierarchical agglomerative clustering with average linkage, complete linkage and ward strategies, by [Nelle Varoquaux](https://github.com/nellev) and [Gael Varoquaux](http://gael-varoquaux.info).
  - Shorthand constructors <span class="title-ref">pipeline.make\_pipeline</span> and <span class="title-ref">pipeline.make\_union</span> were added by [Lars Buitinck](https://github.com/larsmans).
  - Shuffle option for <span class="title-ref">cross\_validation.StratifiedKFold</span>. By `Jeffrey Blackburne <jblackburne>`.
  - Incremental learning (`partial_fit`) for Gaussian Naive Bayes by Imran Haque.
  - Added `partial_fit` to <span class="title-ref">BernoulliRBM \<neural\_network.BernoulliRBM\></span> By `Danny Sullivan <dsullivan7>`.
  - Added <span class="title-ref">learning\_curve</span> utility to chart performance with respect to training size. See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_learning\_curve.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_learning\_curve.py). By Alexander Fabisch.
  - Add positive option in <span class="title-ref">LassoCV \<linear\_model.LassoCV\></span> and <span class="title-ref">ElasticNetCV \<linear\_model.ElasticNetCV\></span>. By Brian Wignall and [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Added <span class="title-ref">linear\_model.MultiTaskElasticNetCV</span> and <span class="title-ref">linear\_model.MultiTaskLassoCV</span>. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Added <span class="title-ref">manifold.TSNE</span>. By Alexander Fabisch.

#### Enhancements

  - Add sparse input support to <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> meta-estimators. By `Hamzeh Alsalhi <hamsal>`.
  - Memory improvements of decision trees, by [Arnaud Joly](http://www.ajoly.org).
  - Decision trees can now be built in best-first manner by using `max_leaf_nodes` as the stopping criteria. Refactored the tree code to use either a stack or a priority queue for tree building. By [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Decision trees can now be fitted on fortran- and c-style arrays, and non-continuous arrays without the need to make a copy. If the input array has a different dtype than `np.float32`, a fortran-style copy will be made since fortran-style memory layout has speed advantages. By [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Speed improvement of regression trees by optimizing the the computation of the mean square error criterion. This lead to speed improvement of the tree, forest and gradient boosting tree modules. By [Arnaud Joly](http://www.ajoly.org)
  - The `img_to_graph` and `grid_tograph` functions in `sklearn.feature_extraction.image` now return `np.ndarray` instead of `np.matrix` when `return_as=np.ndarray`. See the Notes section for more information on compatibility.
  - Changed the internal storage of decision trees to use a struct array. This fixed some small bugs, while improving code and providing a small speed gain. By [Joel Nothman](https://joelnothman.com/).
  - Reduce memory usage and overhead when fitting and predicting with forests of randomized trees in parallel with `n_jobs != 1` by leveraging new threading backend of joblib 0.8 and releasing the GIL in the tree fitting Cython code. By [Olivier Grisel](https://twitter.com/ogrisel) and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Speed improvement of the <span class="title-ref">sklearn.ensemble.gradient\_boosting</span> module. By [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/) and [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Various enhancements to the <span class="title-ref">sklearn.ensemble.gradient\_boosting</span> module: a `warm_start` argument to fit additional trees, a `max_leaf_nodes` argument to fit GBM style trees, a `monitor` fit argument to inspect the estimator during training, and refactoring of the verbose code. By [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Faster <span class="title-ref">sklearn.ensemble.ExtraTrees</span> by caching feature values. By [Arnaud Joly](http://www.ajoly.org).
  - Faster depth-based tree building algorithm such as decision tree, random forest, extra trees or gradient tree boosting (with depth based growing strategy) by avoiding trying to split on found constant features in the sample subset. By [Arnaud Joly](http://www.ajoly.org).
  - Add `min_weight_fraction_leaf` pre-pruning parameter to tree-based methods: the minimum weighted fraction of the input samples required to be at a leaf node. By [Noel Dawe](https://github.com/ndawe).
  - Added <span class="title-ref">metrics.pairwise\_distances\_argmin\_min</span>, by Philippe Gervais.
  - Added predict method to <span class="title-ref">cluster.AffinityPropagation</span> and <span class="title-ref">cluster.MeanShift</span>, by [Mathieu Blondel](http://www.mblondel.org).
  - Vector and matrix multiplications have been optimised throughout the library by [Denis Engemann](http://denis-engemann.de), and [Alexandre Gramfort](http://alexandre.gramfort.net). In particular, they should take less memory with older NumPy versions (prior to 1.7.2).
  - Precision-recall and ROC examples now use train\_test\_split, and have more explanation of why these metrics are useful. By [Kyle Kastner](https://kastnerkyle.github.io/)
  - The training algorithm for <span class="title-ref">decomposition.NMF</span> is faster for sparse matrices and has much lower memory complexity, meaning it will scale up gracefully to large datasets. By [Lars Buitinck](https://github.com/larsmans).
  - Added svd\_method option with default value to "randomized" to <span class="title-ref">decomposition.FactorAnalysis</span> to save memory and significantly speedup computation by [Denis Engemann](http://denis-engemann.de), and [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Changed <span class="title-ref">cross\_validation.StratifiedKFold</span> to try and preserve as much of the original ordering of samples as possible so as not to hide overfitting on datasets with a non-negligible level of samples dependency. By [Daniel Nouri](http://danielnouri.org) and [Olivier Grisel](https://twitter.com/ogrisel).
  - Add multi-output support to <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> by John Novak.
  - Support for precomputed distance matrices in nearest neighbor estimators by [Robert Layton](https://twitter.com/robertlayton) and [Joel Nothman](https://joelnothman.com/).
  - Norm computations optimized for NumPy 1.6 and later versions by [Lars Buitinck](https://github.com/larsmans). In particular, the k-means algorithm no longer needs a temporary data structure the size of its input.
  - <span class="title-ref">dummy.DummyClassifier</span> can now be used to predict a constant output value. By [Manoj Kumar](https://manojbits.wordpress.com).
  - <span class="title-ref">dummy.DummyRegressor</span> has now a strategy parameter which allows to predict the mean, the median of the training set or a constant output value. By `Maheshakya Wijewardena <maheshakya>`.
  - Multi-label classification output in multilabel indicator format is now supported by <span class="title-ref">metrics.roc\_auc\_score</span> and <span class="title-ref">metrics.average\_precision\_score</span> by [Arnaud Joly](http://www.ajoly.org).
  - Significant performance improvements (more than 100x speedup for large problems) in <span class="title-ref">isotonic.IsotonicRegression</span> by [Andrew Tulloch](https://tullo.ch/).
  - Speed and memory usage improvements to the SGD algorithm for linear models: it now uses threads, not separate processes, when `n_jobs>1`. By [Lars Buitinck](https://github.com/larsmans).
  - Grid search and cross validation allow NaNs in the input arrays so that preprocessors such as <span class="title-ref">preprocessing.Imputer</span> can be trained within the cross validation loop, avoiding potentially skewed results.
  - Ridge regression can now deal with sample weights in feature space (only sample space until then). By `Michael Eickenberg <eickenberg>`. Both solutions are provided by the Cholesky solver.
  - Several classification and regression metrics now support weighted samples with the new `sample_weight` argument: <span class="title-ref">metrics.accuracy\_score</span>, <span class="title-ref">metrics.zero\_one\_loss</span>, <span class="title-ref">metrics.precision\_score</span>, <span class="title-ref">metrics.average\_precision\_score</span>, <span class="title-ref">metrics.f1\_score</span>, <span class="title-ref">metrics.fbeta\_score</span>, <span class="title-ref">metrics.recall\_score</span>, <span class="title-ref">metrics.roc\_auc\_score</span>, <span class="title-ref">metrics.explained\_variance\_score</span>, <span class="title-ref">metrics.mean\_squared\_error</span>, <span class="title-ref">metrics.mean\_absolute\_error</span>, <span class="title-ref">metrics.r2\_score</span>. By [Noel Dawe](https://github.com/ndawe).
  - Speed up of the sample generator <span class="title-ref">datasets.make\_multilabel\_classification</span>. By [Joel Nothman](https://joelnothman.com/).

#### Documentation improvements

  - The Working With Text Data tutorial has now been worked in to the main documentation's tutorial section. Includes exercises and skeletons for tutorial presentation. Original tutorial created by several authors including [Olivier Grisel](https://twitter.com/ogrisel), Lars Buitinck and many others. Tutorial integration into the scikit-learn documentation by [Jaques Grobler](https://github.com/jaquesgrobler)
  - Added \[Computational Performance \<computational\_performance\>\](\#computational-performance-\<computational\_performance\>) documentation. Discussion and examples of prediction latency / throughput and different factors that have influence over speed. Additional tips for building faster models and choosing a relevant compromise between speed and predictive power. By `Eustache Diemert <oddskool>`.

#### Bug fixes

  - Fixed bug in <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> : `partial_fit` was not working properly.
  - Fixed bug in <span class="title-ref">linear\_model.stochastic\_gradient</span> : `l1_ratio` was used as `(1.0 - l1_ratio)` .
  - Fixed bug in <span class="title-ref">multiclass.OneVsOneClassifier</span> with string labels
  - Fixed a bug in <span class="title-ref">LassoCV \<linear\_model.LassoCV\></span> and \`ElasticNetCV \<linear\_model.ElasticNetCV\><span class="title-ref">: they would not pre-compute the Gram matrix with </span><span class="title-ref">precompute=True</span><span class="title-ref"> or </span><span class="title-ref">precompute="auto"</span><span class="title-ref"> and </span><span class="title-ref">n\_samples \> n\_features</span><span class="title-ref">. By \`Manoj Kumar</span>\_.
  - Fixed incorrect estimation of the degrees of freedom in <span class="title-ref">feature\_selection.f\_regression</span> when variates are not centered. By `Virgile Fritsch <VirgileFritsch>`.
  - Fixed a race condition in parallel processing with `pre_dispatch != "all"` (for instance, in `cross_val_score`). By [Olivier Grisel](https://twitter.com/ogrisel).
  - Raise error in <span class="title-ref">cluster.FeatureAgglomeration</span> and <span class="title-ref">cluster.WardAgglomeration</span> when no samples are given, rather than returning meaningless clustering.
  - Fixed bug in <span class="title-ref">gradient\_boosting.GradientBoostingRegressor</span> with `loss='huber'`: `gamma` might have not been initialized.
  - Fixed feature importances as computed with a forest of randomized trees when fit with `sample_weight != None` and/or with `bootstrap=True`. By [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).

### API changes summary

  - <span class="title-ref">sklearn.hmm</span> is deprecated. Its removal is planned for the 0.17 release.
  - Use of <span class="title-ref">covariance.EllipticEnvelop</span> has now been removed after deprecation. Please use <span class="title-ref">covariance.EllipticEnvelope</span> instead.
  - <span class="title-ref">cluster.Ward</span> is deprecated. Use <span class="title-ref">cluster.AgglomerativeClustering</span> instead.
  - <span class="title-ref">cluster.WardClustering</span> is deprecated. Use
  - <span class="title-ref">cluster.AgglomerativeClustering</span> instead.
  - <span class="title-ref">cross\_validation.Bootstrap</span> is deprecated. <span class="title-ref">cross\_validation.KFold</span> or <span class="title-ref">cross\_validation.ShuffleSplit</span> are recommended instead.
  - Direct support for the sequence of sequences (or list of lists) multilabel format is deprecated. To convert to and from the supported binary indicator matrix format, use <span class="title-ref">preprocessing.MultiLabelBinarizer</span>. By [Joel Nothman](https://joelnothman.com/).
  - Add score method to <span class="title-ref">decomposition.PCA</span> following the model of probabilistic PCA and deprecate <span class="title-ref">ProbabilisticPCA</span> model whose score implementation is not correct. The computation now also exploits the matrix inversion lemma for faster computation. By [Alexandre Gramfort](http://alexandre.gramfort.net).
  - The score method of <span class="title-ref">decomposition.FactorAnalysis</span> now returns the average log-likelihood of the samples. Use score\_samples to get log-likelihood of each sample. By [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Generating boolean masks (the setting `indices=False`) from cross-validation generators is deprecated. Support for masks will be removed in 0.17. The generators have produced arrays of indices by default since 0.10. By [Joel Nothman](https://joelnothman.com/).
  - 1-d arrays containing strings with `dtype=object` (as used in Pandas) are now considered valid classification targets. This fixes a regression from version 0.13 in some classifiers. By [Joel Nothman](https://joelnothman.com/).
  - Fix wrong `explained_variance_ratio_` attribute in <span class="title-ref">RandomizedPCA</span>. By [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Fit alphas for each `l1_ratio` instead of `mean_l1_ratio` in <span class="title-ref">linear\_model.ElasticNetCV</span> and <span class="title-ref">linear\_model.LassoCV</span>. This changes the shape of `alphas_` from `(n_alphas,)` to `(n_l1_ratio, n_alphas)` if the `l1_ratio` provided is a 1-D array like object of length greater than one. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Fix <span class="title-ref">linear\_model.ElasticNetCV</span> and <span class="title-ref">linear\_model.LassoCV</span> when fitting intercept and input data is sparse. The automatic grid of alphas was not computed correctly and the scaling with normalize was wrong. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Fix wrong maximal number of features drawn (`max_features`) at each split for decision trees, random forests and gradient tree boosting. Previously, the count for the number of drawn features started only after one non constant features in the split. This bug fix will affect computational and generalization performance of those algorithms in the presence of constant features. To get back previous generalization performance, you should modify the value of `max_features`. By [Arnaud Joly](http://www.ajoly.org).
  - Fix wrong maximal number of features drawn (`max_features`) at each split for <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span>. Previously, only non constant features in the split was counted as drawn. Now constant features are counted as drawn. Furthermore at least one feature must be non constant in order to make a valid split. This bug fix will affect computational and generalization performance of extra trees in the presence of constant features. To get back previous generalization performance, you should modify the value of `max_features`. By [Arnaud Joly](http://www.ajoly.org).
  - Fix <span class="title-ref">utils.class\_weight.compute\_class\_weight</span> when `class_weight=="auto"`. Previously it was broken for input of non-integer `dtype` and the weighted array that was returned was wrong. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Fix <span class="title-ref">cross\_validation.Bootstrap</span> to return `ValueError` when `n_train + n_test > n`. By `Ronald Phlypo <rphlypo>`.

### People

List of contributors for release 0.15 by number of commits.

  - 312 Olivier Grisel
  - 275 Lars Buitinck
  - 221 Gael Varoquaux
  - 148 Arnaud Joly
  - 134 Johannes Schönberger
  - 119 Gilles Louppe
  - 113 Joel Nothman
  - 111 Alexandre Gramfort
  - 95 Jaques Grobler
  - 89 Denis Engemann
  - 83 Peter Prettenhofer
  - 83 Alexander Fabisch
  - 62 Mathieu Blondel
  - 60 Eustache Diemert
  - 60 Nelle Varoquaux
  - 49 Michael Bommarito
  - 45 Manoj-Kumar-S
  - 28 Kyle Kastner
  - 26 Andreas Mueller
  - 22 Noel Dawe
  - 21 Maheshakya Wijewardena
  - 21 Brooke Osborn
  - 21 Hamzeh Alsalhi
  - 21 Jake VanderPlas
  - 21 Philippe Gervais
  - 19 Bala Subrahmanyam Varanasi
  - 12 Ronald Phlypo
  - 10 Mikhail Korobov
  - 8 Thomas Unterthiner
  - 8 Jeffrey Blackburne
  - 8 eltermann
  - 8 bwignall
  - 7 Ankit Agrawal
  - 7 CJ Carey
  - 6 Daniel Nouri
  - 6 Chen Liu
  - 6 Michael Eickenberg
  - 6 ugurthemaster
  - 5 Aaron Schumacher
  - 5 Baptiste Lagarde
  - 5 Rajat Khanduja
  - 5 Robert McGibbon
  - 5 Sergio Pascual
  - 4 Alexis Metaireau
  - 4 Ignacio Rossi
  - 4 Virgile Fritsch
  - 4 Sebastian Säger
  - 4 Ilambharathi Kanniah
  - 4 sdenton4
  - 4 Robert Layton
  - 4 Alyssa
  - 4 Amos Waterland
  - 3 Andrew Tulloch
  - 3 murad
  - 3 Steven Maude
  - 3 Karol Pysniak
  - 3 Jacques Kvam
  - 3 cgohlke
  - 3 cjlin
  - 3 Michael Becker
  - 3 hamzeh
  - 3 Eric Jacobsen
  - 3 john collins
  - 3 kaushik94
  - 3 Erwin Marsi
  - 2 csytracy
  - 2 LK
  - 2 Vlad Niculae
  - 2 Laurent Direr
  - 2 Erik Shilts
  - 2 Raul Garreta
  - 2 Yoshiki Vázquez Baeza
  - 2 Yung Siang Liau
  - 2 abhishek thakur
  - 2 James Yu
  - 2 Rohit Sivaprasad
  - 2 Roland Szabo
  - 2 amormachine
  - 2 Alexis Mignon
  - 2 Oscar Carlsson
  - 2 Nantas Nardelli
  - 2 jess010
  - 2 kowalski87
  - 2 Andrew Clegg
  - 2 Federico Vaggi
  - 2 Simon Frid
  - 2 Félix-Antoine Fortin
  - 1 Ralf Gommers
  - 1 t-aft
  - 1 Ronan Amicel
  - 1 Rupesh Kumar Srivastava
  - 1 Ryan Wang
  - 1 Samuel Charron
  - 1 Samuel St-Jean
  - 1 Fabian Pedregosa
  - 1 Skipper Seabold
  - 1 Stefan Walk
  - 1 Stefan van der Walt
  - 1 Stephan Hoyer
  - 1 Allen Riddell
  - 1 Valentin Haenel
  - 1 Vijay Ramesh
  - 1 Will Myers
  - 1 Yaroslav Halchenko
  - 1 Yoni Ben-Meshulam
  - 1 Yury V. Zaytsev
  - 1 adrinjalali
  - 1 ai8rahim
  - 1 alemagnani
  - 1 alex
  - 1 benjamin wilson
  - 1 chalmerlowe
  - 1 dzikie drożdże
  - 1 jamestwebber
  - 1 matrixorz
  - 1 popo
  - 1 samuela
  - 1 François Boulogne
  - 1 Alexander Measure
  - 1 Ethan White
  - 1 Guilherme Trein
  - 1 Hendrik Heuer
  - 1 IvicaJovic
  - 1 Jan Hendrik Metzen
  - 1 Jean Michel Rouly
  - 1 Eduardo Ariño de la Rubia
  - 1 Jelle Zijlstra
  - 1 Eddy L O Jansson
  - 1 Denis
  - 1 John
  - 1 John Schmidt
  - 1 Jorge Cañardo Alastuey
  - 1 Joseph Perla
  - 1 Joshua Vredevoogd
  - 1 José Ricardo
  - 1 Julien Miotte
  - 1 Kemal Eren
  - 1 Kenta Sato
  - 1 David Cournapeau
  - 1 Kyle Kelley
  - 1 Daniele Medri
  - 1 Laurent Luce
  - 1 Laurent Pierron
  - 1 Luis Pedro Coelho
  - 1 DanielWeitzenfeld
  - 1 Craig Thompson
  - 1 Chyi-Kwei Yau
  - 1 Matthew Brett
  - 1 Matthias Feurer
  - 1 Max Linke
  - 1 Chris Filo Gorgolewski
  - 1 Charles Earl
  - 1 Michael Hanke
  - 1 Michele Orrù
  - 1 Bryan Lunt
  - 1 Brian Kearns
  - 1 Paul Butler
  - 1 Paweł Mandera
  - 1 Peter
  - 1 Andrew Ash
  - 1 Pietro Zambelli
  - 1 staubda

---

v0.16.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.16

## Version 0.16.1

**April 14, 2015**

### Changelog

#### Bug fixes

  - Allow input data larger than `block_size` in <span class="title-ref">covariance.LedoitWolf</span> by [Andreas Müller](https://amueller.github.io/).
  - Fix a bug in <span class="title-ref">isotonic.IsotonicRegression</span> deduplication that caused unstable result in <span class="title-ref">calibration.CalibratedClassifierCV</span> by [Jan Hendrik Metzen](https://jmetzen.github.io/).
  - Fix sorting of labels in func:<span class="title-ref">preprocessing.label\_binarize</span> by Michael Heilman.
  - Fix several stability and convergence issues in <span class="title-ref">cross\_decomposition.CCA</span> and <span class="title-ref">cross\_decomposition.PLSCanonical</span> by [Andreas Müller](https://amueller.github.io/)
  - Fix a bug in <span class="title-ref">cluster.KMeans</span> when `precompute_distances=False` on fortran-ordered data.
  - Fix a speed regression in <span class="title-ref">ensemble.RandomForestClassifier</span>'s `predict` and `predict_proba` by [Andreas Müller](https://amueller.github.io/).
  - Fix a regression where `utils.shuffle` converted lists and dataframes to arrays, by [Olivier Grisel](https://twitter.com/ogrisel)

## Version 0.16

**March 26, 2015**

### Highlights

  - Speed improvements (notably in <span class="title-ref">cluster.DBSCAN</span>), reduced memory requirements, bug-fixes and better default settings.
  - Multinomial Logistic regression and a path algorithm in <span class="title-ref">linear\_model.LogisticRegressionCV</span>.
  - Out-of core learning of PCA via <span class="title-ref">decomposition.IncrementalPCA</span>.
  - Probability calibration of classifiers using <span class="title-ref">calibration.CalibratedClassifierCV</span>.
  - <span class="title-ref">cluster.Birch</span> clustering method for large-scale datasets.
  - Scalable approximate nearest neighbors search with Locality-sensitive hashing forests in <span class="title-ref">neighbors.LSHForest</span>.
  - Improved error messages and better validation when using malformed input data.
  - More robust integration with pandas dataframes.

### Changelog

#### New features

  - The new <span class="title-ref">neighbors.LSHForest</span> implements locality-sensitive hashing for approximate nearest neighbors search. By `Maheshakya Wijewardena<maheshakya>`.
  - Added <span class="title-ref">svm.LinearSVR</span>. This class uses the liblinear implementation of Support Vector Regression which is much faster for large sample sizes than <span class="title-ref">svm.SVR</span> with linear kernel. By [Fabian Pedregosa](http://fa.bianp.net) and Qiang Luo.
  - Incremental fit for <span class="title-ref">GaussianNB \<naive\_bayes.GaussianNB\></span>.
  - Added `sample_weight` support to <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span>. By [Arnaud Joly](http://www.ajoly.org).
  - Added the <span class="title-ref">metrics.label\_ranking\_average\_precision\_score</span> metrics. By [Arnaud Joly](http://www.ajoly.org).
  - Add the <span class="title-ref">metrics.coverage\_error</span> metrics. By [Arnaud Joly](http://www.ajoly.org).
  - Added <span class="title-ref">linear\_model.LogisticRegressionCV</span>. By [Manoj Kumar](https://manojbits.wordpress.com), [Fabian Pedregosa](http://fa.bianp.net), [Gael Varoquaux](http://gael-varoquaux.info) and [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Added `warm_start` constructor parameter to make it possible for any trained forest model to grow additional trees incrementally. By `Laurent Direr<ldirer>`.
  - Added `sample_weight` support to <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span>. By [Peter Prettenhofer](https://sites.google.com/site/peterprettenhofer/).
  - Added <span class="title-ref">decomposition.IncrementalPCA</span>, an implementation of the PCA algorithm that supports out-of-core learning with a `partial_fit` method. By [Kyle Kastner](https://kastnerkyle.github.io/).
  - Averaged SGD for <span class="title-ref">SGDClassifier \<linear\_model.SGDClassifier\></span> and <span class="title-ref">SGDRegressor \<linear\_model.SGDRegressor\></span> By `Danny Sullivan <dsullivan7>`.
  - Added <span class="title-ref">cross\_val\_predict</span> function which computes cross-validated estimates. By [Luis Pedro Coelho](http://luispedro.org)
  - Added <span class="title-ref">linear\_model.TheilSenRegressor</span>, a robust generalized-median-based estimator. By `Florian Wilhelm <FlorianWilhelm>`.
  - Added <span class="title-ref">metrics.median\_absolute\_error</span>, a robust metric. By [Gael Varoquaux](http://gael-varoquaux.info) and `Florian Wilhelm <FlorianWilhelm>`.
  - Add <span class="title-ref">cluster.Birch</span>, an online clustering algorithm. By [Manoj Kumar](https://manojbits.wordpress.com), [Alexandre Gramfort](http://alexandre.gramfort.net) and [Joel Nothman](https://joelnothman.com/).
  - Added shrinkage support to <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> using two new solvers. By `Clemens Brunner <cle1109>` and [Martin Billinger](https://tnsre.embs.org/author/martinbillinger/).
  - Added <span class="title-ref">kernel\_ridge.KernelRidge</span>, an implementation of kernelized ridge regression. By [Mathieu Blondel](http://www.mblondel.org) and [Jan Hendrik Metzen](https://jmetzen.github.io/).
  - All solvers in <span class="title-ref">linear\_model.Ridge</span> now support <span class="title-ref">sample\_weight</span>. By [Mathieu Blondel](http://www.mblondel.org).
  - Added <span class="title-ref">cross\_validation.PredefinedSplit</span> cross-validation for fixed user-provided cross-validation folds. By `Thomas Unterthiner <untom>`.
  - Added <span class="title-ref">calibration.CalibratedClassifierCV</span>, an approach for calibrating the predicted probabilities of a classifier. By [Alexandre Gramfort](http://alexandre.gramfort.net), [Jan Hendrik Metzen](https://jmetzen.github.io/), [Mathieu Blondel](http://www.mblondel.org) and `Balazs Kegl <kegl>`.

#### Enhancements

  - Add option `return_distance` in <span class="title-ref">hierarchical.ward\_tree</span> to return distances between nodes for both structured and unstructured versions of the algorithm. By [Matteo Visconti di Oleggio Castello](http://www.mvdoc.me). The same option was added in <span class="title-ref">hierarchical.linkage\_tree</span>. By [Manoj Kumar](https://manojbits.wordpress.com)
  - Add support for sample weights in scorer objects. Metrics with sample weight support will automatically benefit from it. By [Noel Dawe](https://github.com/ndawe) and [Vlad Niculae](https://vene.ro/).
  - Added `newton-cg` and <span class="title-ref">lbfgs</span> solver support in <span class="title-ref">linear\_model.LogisticRegression</span>. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Add `selection="random"` parameter to implement stochastic coordinate descent for <span class="title-ref">linear\_model.Lasso</span>, <span class="title-ref">linear\_model.ElasticNet</span> and related. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Add `sample_weight` parameter to <span class="title-ref">metrics.jaccard\_similarity\_score</span> and <span class="title-ref">metrics.log\_loss</span>. By `Jatin Shah <jatinshah>`.
  - Support sparse multilabel indicator representation in <span class="title-ref">preprocessing.LabelBinarizer</span> and <span class="title-ref">multiclass.OneVsRestClassifier</span> (by `Hamzeh Alsalhi <hamsal>` with thanks to Rohit Sivaprasad), as well as evaluation metrics (by [Joel Nothman](https://joelnothman.com/)).
  - Add `sample_weight` parameter to <span class="title-ref">metrics.jaccard\_similarity\_score</span>. By <span class="title-ref">Jatin Shah</span>.
  - Add support for multiclass in <span class="title-ref">metrics.hinge\_loss</span>. Added `labels=None` as optional parameter. By <span class="title-ref">Saurabh Jha</span>.
  - Add `sample_weight` parameter to <span class="title-ref">metrics.hinge\_loss</span>. By <span class="title-ref">Saurabh Jha</span>.
  - Add `multi_class="multinomial"` option in <span class="title-ref">linear\_model.LogisticRegression</span> to implement a Logistic Regression solver that minimizes the cross-entropy or multinomial loss instead of the default One-vs-Rest setting. Supports <span class="title-ref">lbfgs</span> and <span class="title-ref">newton-cg</span> solvers. By [Lars Buitinck](https://github.com/larsmans) and [Manoj Kumar](https://manojbits.wordpress.com). Solver option <span class="title-ref">newton-cg</span> by Simon Wu.
  - `DictVectorizer` can now perform `fit_transform` on an iterable in a single pass, when giving the option `sort=False`. By `Dan
    Blanchard <dan-blanchard>`.
  - <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> can now be configured to work with estimators that may fail and raise errors on individual folds. This option is controlled by the <span class="title-ref">error\_score</span> parameter. This does not affect errors raised on re-fit. By `Michal Romaniuk <romaniukm>`.
  - Add `digits` parameter to <span class="title-ref">metrics.classification\_report</span> to allow report to show different precision of floating point numbers. By `Ian Gilmore <agileminor>`.
  - Add a quantile prediction strategy to the <span class="title-ref">dummy.DummyRegressor</span>. By `Aaron Staple <staple>`.
  - Add `handle_unknown` option to <span class="title-ref">preprocessing.OneHotEncoder</span> to handle unknown categorical features more gracefully during transform. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Added support for sparse input data to decision trees and their ensembles. By [Fares Hedyati](http://www.eecs.berkeley.edu/~fareshed) and [Arnaud Joly](http://www.ajoly.org).
  - Optimized <span class="title-ref">cluster.AffinityPropagation</span> by reducing the number of memory allocations of large temporary data-structures. By [Antony Lee](https://www.ocf.berkeley.edu/~antonyl/).
  - Parellization of the computation of feature importances in random forest. By [Olivier Grisel](https://twitter.com/ogrisel) and [Arnaud Joly](http://www.ajoly.org).
  - Add `n_iter_` attribute to estimators that accept a `max_iter` attribute in their constructor. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Added decision function for <span class="title-ref">multiclass.OneVsOneClassifier</span> By [Raghav RV](https://github.com/raghavrv) and `Kyle Beauchamp <kyleabeauchamp>`.
  - <span class="title-ref">neighbors.kneighbors\_graph</span> and <span class="title-ref">radius\_neighbors\_graph</span> support non-Euclidean metrics. By [Manoj Kumar](https://manojbits.wordpress.com)
  - Parameter `connectivity` in <span class="title-ref">cluster.AgglomerativeClustering</span> and family now accept callables that return a connectivity matrix. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Sparse support for <span class="title-ref">metrics.pairwise.paired\_distances</span>. By [Joel Nothman](https://joelnothman.com/).
  - <span class="title-ref">cluster.DBSCAN</span> now supports sparse input and sample weights and has been optimized: the inner loop has been rewritten in Cython and radius neighbors queries are now computed in batch. By [Joel Nothman](https://joelnothman.com/) and [Lars Buitinck](https://github.com/larsmans).
  - Add `class_weight` parameter to automatically weight samples by class frequency for <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">tree.ExtraTreeClassifier</span>. By [Trevor Stephens](http://trevorstephens.com/).
  - <span class="title-ref">grid\_search.RandomizedSearchCV</span> now does sampling without replacement if all parameters are given as lists. By [Andreas Müller](https://amueller.github.io/).
  - Parallelized calculation of <span class="title-ref">metrics.pairwise\_distances</span> is now supported for scipy metrics and custom callables. By [Joel Nothman](https://joelnothman.com/).
  - Allow the fitting and scoring of all clustering algorithms in <span class="title-ref">pipeline.Pipeline</span>. By [Andreas Müller](https://amueller.github.io/).
  - More robust seeding and improved error messages in <span class="title-ref">cluster.MeanShift</span> by [Andreas Müller](https://amueller.github.io/).
  - Make the stopping criterion for <span class="title-ref">mixture.GMM</span>, <span class="title-ref">mixture.DPGMM</span> and <span class="title-ref">mixture.VBGMM</span> less dependent on the number of samples by thresholding the average log-likelihood change instead of its sum over all samples. By [Hervé Bredin](https://herve.niderb.fr/).
  - The outcome of <span class="title-ref">manifold.spectral\_embedding</span> was made deterministic by flipping the sign of eigenvectors. By `Hasil Sharma <Hasil-Sharma>`.
  - Significant performance and memory usage improvements in <span class="title-ref">preprocessing.PolynomialFeatures</span>. By [Eric Martin](http://www.ericmart.in).
  - Numerical stability improvements for <span class="title-ref">preprocessing.StandardScaler</span> and <span class="title-ref">preprocessing.scale</span>. By [Nicolas Goix](https://ngoix.github.io/)
  - <span class="title-ref">svm.SVC</span> fitted on sparse input now implements `decision_function`. By [Rob Zinkov](https://www.zinkov.com/) and [Andreas Müller](https://amueller.github.io/).
  - <span class="title-ref">cross\_validation.train\_test\_split</span> now preserves the input type, instead of converting to numpy arrays.

#### Documentation improvements

  - Added example of using <span class="title-ref">pipeline.FeatureUnion</span> for heterogeneous input. By `Matt Terry <mrterry>`
  - Documentation on scorers was improved, to highlight the handling of loss functions. By `Matt Pico <MattpSoftware>`.
  - A discrepancy between liblinear output and scikit-learn's wrappers is now noted. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Improved documentation generation: examples referring to a class or function are now shown in a gallery on the class/function's API reference page. By [Joel Nothman](https://joelnothman.com/).
  - More explicit documentation of sample generators and of data transformation. By [Joel Nothman](https://joelnothman.com/).
  - <span class="title-ref">sklearn.neighbors.BallTree</span> and <span class="title-ref">sklearn.neighbors.KDTree</span> used to point to empty pages stating that they are aliases of BinaryTree. This has been fixed to show the correct class docs. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Added silhouette plots for analysis of KMeans clustering using <span class="title-ref">metrics.silhouette\_samples</span> and <span class="title-ref">metrics.silhouette\_score</span>. See \[sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_silhouette\_analysis.py\](\#sphx\_glr\_auto\_examples\_cluster\_plot\_kmeans\_silhouette\_analysis.py)

#### Bug fixes

  - Metaestimators now support ducktyping for the presence of `decision_function`, `predict_proba` and other methods. This fixes behavior of <span class="title-ref">grid\_search.GridSearchCV</span>, <span class="title-ref">grid\_search.RandomizedSearchCV</span>, <span class="title-ref">pipeline.Pipeline</span>, <span class="title-ref">feature\_selection.RFE</span>, <span class="title-ref">feature\_selection.RFECV</span> when nested. By [Joel Nothman](https://joelnothman.com/)
  - The `scoring` attribute of grid-search and cross-validation methods is no longer ignored when a <span class="title-ref">grid\_search.GridSearchCV</span> is given as a base estimator or the base estimator doesn't have predict.
  - The function <span class="title-ref">hierarchical.ward\_tree</span> now returns the children in the same order for both the structured and unstructured versions. By [Matteo Visconti di Oleggio Castello](http://www.mvdoc.me).
  - <span class="title-ref">feature\_selection.RFECV</span> now correctly handles cases when `step` is not equal to 1. By `Nikolay Mayorov <nmayorov>`
  - The <span class="title-ref">decomposition.PCA</span> now undoes whitening in its `inverse_transform`. Also, its `components_` now always have unit length. By `Michael Eickenberg <eickenberg>`.
  - Fix incomplete download of the dataset when <span class="title-ref">datasets.download\_20newsgroups</span> is called. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Various fixes to the Gaussian processes subpackage by Vincent Dubourg and Jan Hendrik Metzen.
  - Calling `partial_fit` with `class_weight=='auto'` throws an appropriate error message and suggests a work around. By `Danny Sullivan <dsullivan7>`.
  - <span class="title-ref">RBFSampler \<kernel\_approximation.RBFSampler\></span> with `gamma=g` formerly approximated <span class="title-ref">rbf\_kernel \<metrics.pairwise.rbf\_kernel\></span> with `gamma=g/2.`; the definition of `gamma` is now consistent, which may substantially change your results if you use a fixed value. (If you cross-validated over `gamma`, it probably doesn't matter too much.) By `Dougal Sutherland <dougalsutherland>`.
  - Pipeline object delegate the `classes_` attribute to the underlying estimator. It allows, for instance, to make bagging of a pipeline object. By [Arnaud Joly](http://www.ajoly.org)
  - <span class="title-ref">neighbors.NearestCentroid</span> now uses the median as the centroid when metric is set to `manhattan`. It was using the mean before. By [Manoj Kumar](https://manojbits.wordpress.com)
  - Fix numerical stability issues in <span class="title-ref">linear\_model.SGDClassifier</span> and <span class="title-ref">linear\_model.SGDRegressor</span> by clipping large gradients and ensuring that weight decay rescaling is always positive (for large l2 regularization and large learning rate values). By [Olivier Grisel](https://twitter.com/ogrisel)
  - When <span class="title-ref">compute\_full\_tree</span> is set to "auto", the full tree is built when n\_clusters is high and is early stopped when n\_clusters is low, while the behavior should be vice-versa in <span class="title-ref">cluster.AgglomerativeClustering</span> (and friends). This has been fixed By [Manoj Kumar](https://manojbits.wordpress.com)
  - Fix lazy centering of data in <span class="title-ref">linear\_model.enet\_path</span> and <span class="title-ref">linear\_model.lasso\_path</span>. It was centered around one. It has been changed to be centered around the origin. By [Manoj Kumar](https://manojbits.wordpress.com)
  - Fix handling of precomputed affinity matrices in <span class="title-ref">cluster.AgglomerativeClustering</span> when using connectivity constraints. By `Cathy Deng <cathydeng>`
  - Correct `partial_fit` handling of `class_prior` for <span class="title-ref">sklearn.naive\_bayes.MultinomialNB</span> and <span class="title-ref">sklearn.naive\_bayes.BernoulliNB</span>. By [Trevor Stephens](http://trevorstephens.com/).
  - Fixed a crash in <span class="title-ref">metrics.precision\_recall\_fscore\_support</span> when using unsorted `labels` in the multi-label setting. By [Andreas Müller](https://amueller.github.io/).
  - Avoid skipping the first nearest neighbor in the methods `radius_neighbors`, `kneighbors`, `kneighbors_graph` and `radius_neighbors_graph` in <span class="title-ref">sklearn.neighbors.NearestNeighbors</span> and family, when the query data is not the same as fit data. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Fix log-density calculation in the <span class="title-ref">mixture.GMM</span> with tied covariance. By [Will Dawson](http://www.dawsonresearch.com)
  - Fixed a scaling error in <span class="title-ref">feature\_selection.SelectFdr</span> where a factor `n_features` was missing. By [Andrew Tulloch](https://tullo.ch/)
  - Fix zero division in <span class="title-ref">neighbors.KNeighborsRegressor</span> and related classes when using distance weighting and having identical data points. By [Garret-R](https://github.com/Garrett-R).
  - Fixed round off errors with non positive-definite covariance matrices in GMM. By `Alexis Mignon <AlexisMignon>`.
  - Fixed a error in the computation of conditional probabilities in <span class="title-ref">naive\_bayes.BernoulliNB</span>. By [Hanna Wallach](https://dirichlet.net/).
  - Make the method `radius_neighbors` of <span class="title-ref">neighbors.NearestNeighbors</span> return the samples lying on the boundary for `algorithm='brute'`. By [Yan Yi](http://seowyanyi.org).
  - Flip sign of `dual_coef_` of <span class="title-ref">svm.SVC</span> to make it consistent with the documentation and `decision_function`. By Artem Sobolev.
  - Fixed handling of ties in <span class="title-ref">isotonic.IsotonicRegression</span>. We now use the weighted average of targets (secondary method). By [Andreas Müller](https://amueller.github.io/) and [Michael Bommarito](http://bommaritollc.com/).

### API changes summary

  - <span class="title-ref">GridSearchCV</span> and <span class="title-ref">cross\_val\_score</span> and other meta-estimators don't convert pandas DataFrames into arrays any more, allowing DataFrame specific operations in custom estimators.

  - <span class="title-ref">multiclass.fit\_ovr</span>, <span class="title-ref">multiclass.predict\_ovr</span>, <span class="title-ref">predict\_proba\_ovr</span>, <span class="title-ref">multiclass.fit\_ovo</span>, <span class="title-ref">multiclass.predict\_ovo</span>, <span class="title-ref">multiclass.fit\_ecoc</span> and <span class="title-ref">multiclass.predict\_ecoc</span> are deprecated. Use the underlying estimators instead.

  - Nearest neighbors estimators used to take arbitrary keyword arguments and pass these to their distance metric. This will no longer be supported in scikit-learn 0.18; use the `metric_params` argument instead.

  -   - <span class="title-ref">n\_jobs</span> parameter of the fit method shifted to the constructor of the  
        LinearRegression class.

  - The `predict_proba` method of <span class="title-ref">multiclass.OneVsRestClassifier</span> now returns two probabilities per sample in the multiclass case; this is consistent with other estimators and with the method's documentation, but previous versions accidentally returned only the positive probability. Fixed by Will Lamond and [Lars Buitinck](https://github.com/larsmans).

  - Change default value of precompute in <span class="title-ref">linear\_model.ElasticNet</span> and <span class="title-ref">linear\_model.Lasso</span> to False. Setting precompute to "auto" was found to be slower when n\_samples \> n\_features since the computation of the Gram matrix is computationally expensive and outweighs the benefit of fitting the Gram for just one alpha. `precompute="auto"` is now deprecated and will be removed in 0.18 By [Manoj Kumar](https://manojbits.wordpress.com).

  - Expose `positive` option in <span class="title-ref">linear\_model.enet\_path</span> and <span class="title-ref">linear\_model.enet\_path</span> which constrains coefficients to be positive. By [Manoj Kumar](https://manojbits.wordpress.com).

  - Users should now supply an explicit `average` parameter to <span class="title-ref">sklearn.metrics.f1\_score</span>, <span class="title-ref">sklearn.metrics.fbeta\_score</span>, <span class="title-ref">sklearn.metrics.recall\_score</span> and <span class="title-ref">sklearn.metrics.precision\_score</span> when performing multiclass or multilabel (i.e. not binary) classification. By [Joel Nothman](https://joelnothman.com/).

  - <span class="title-ref">scoring</span> parameter for cross validation now accepts <span class="title-ref">'f1\_micro'</span>, <span class="title-ref">'f1\_macro'</span> or <span class="title-ref">'f1\_weighted'</span>. <span class="title-ref">'f1'</span> is now for binary classification only. Similar changes apply to <span class="title-ref">'precision'</span> and <span class="title-ref">'recall'</span>. By [Joel Nothman](https://joelnothman.com/).

  - The `fit_intercept`, `normalize` and `return_models` parameters in <span class="title-ref">linear\_model.enet\_path</span> and <span class="title-ref">linear\_model.lasso\_path</span> have been removed. They were deprecated since 0.14

  - From now onwards, all estimators will uniformly raise `NotFittedError` when any of the `predict` like methods are called before the model is fit. By [Raghav RV](https://github.com/raghavrv).

  - Input data validation was refactored for more consistent input validation. The `check_arrays` function was replaced by `check_array` and `check_X_y`. By [Andreas Müller](https://amueller.github.io/).

  - Allow `X=None` in the methods `radius_neighbors`, `kneighbors`, `kneighbors_graph` and `radius_neighbors_graph` in <span class="title-ref">sklearn.neighbors.NearestNeighbors</span> and family. If set to None, then for every sample this avoids setting the sample itself as the first nearest neighbor. By [Manoj Kumar](https://manojbits.wordpress.com).

  - Add parameter `include_self` in <span class="title-ref">neighbors.kneighbors\_graph</span> and <span class="title-ref">neighbors.radius\_neighbors\_graph</span> which has to be explicitly set by the user. If set to True, then the sample itself is considered as the first nearest neighbor.

  - <span class="title-ref">thresh</span> parameter is deprecated in favor of new <span class="title-ref">tol</span> parameter in <span class="title-ref">GMM</span>, <span class="title-ref">DPGMM</span> and <span class="title-ref">VBGMM</span>. See <span class="title-ref">Enhancements</span> section for details. By [Hervé Bredin](https://herve.niderb.fr/).

  - Estimators will treat input with dtype object as numeric when possible. By [Andreas Müller](https://amueller.github.io/)

  - Estimators now raise <span class="title-ref">ValueError</span> consistently when fitted on empty data (less than 1 sample or less than 1 feature for 2D input). By [Olivier Grisel](https://twitter.com/ogrisel).

  - The `shuffle` option of <span class="title-ref">.linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.Perceptron</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span> and <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> now defaults to `True`.

  - <span class="title-ref">cluster.DBSCAN</span> now uses a deterministic initialization. The <span class="title-ref">random\_state</span> parameter is deprecated. By `Erich Schubert <kno10>`.

### Code Contributors

A. Flaxman, Aaron Schumacher, Aaron Staple, abhishek thakur, Akshay, akshayah3, Aldrian Obaja, Alexander Fabisch, Alexandre Gramfort, Alexis Mignon, Anders Aagaard, Andreas Mueller, Andreas van Cranenburgh, Andrew Tulloch, Andrew Walker, Antony Lee, Arnaud Joly, banilo, Barmaley.exe, Ben Davies, Benedikt Koehler, bhsu, Boris Feld, Borja Ayerdi, Boyuan Deng, Brent Pedersen, Brian Wignall, Brooke Osborn, Calvin Giles, Cathy Deng, Celeo, cgohlke, chebee7i, Christian Stade-Schuldt, Christof Angermueller, Chyi-Kwei Yau, CJ Carey, Clemens Brunner, Daiki Aminaka, Dan Blanchard, danfrankj, Danny Sullivan, David Fletcher, Dmitrijs Milajevs, Dougal J. Sutherland, Erich Schubert, Fabian Pedregosa, Florian Wilhelm, floydsoft, Félix-Antoine Fortin, Gael Varoquaux, Garrett-R, Gilles Louppe, gpassino, gwulfs, Hampus Bengtsson, Hamzeh Alsalhi, Hanna Wallach, Harry Mavroforakis, Hasil Sharma, Helder, Herve Bredin, Hsiang-Fu Yu, Hugues SALAMIN, Ian Gilmore, Ilambharathi Kanniah, Imran Haque, isms, Jake VanderPlas, Jan Dlabal, Jan Hendrik Metzen, Jatin Shah, Javier López Peña, jdcaballero, Jean Kossaifi, Jeff Hammerbacher, Joel Nothman, Jonathan Helmus, Joseph, Kaicheng Zhang, Kevin Markham, Kyle Beauchamp, Kyle Kastner, Lagacherie Matthieu, Lars Buitinck, Laurent Direr, leepei, Loic Esteve, Luis Pedro Coelho, Lukas Michelbacher, maheshakya, Manoj Kumar, Manuel, Mario Michael Krell, Martin, Martin Billinger, Martin Ku, Mateusz Susik, Mathieu Blondel, Matt Pico, Matt Terry, Matteo Visconti dOC, Matti Lyra, Max Linke, Mehdi Cherti, Michael Bommarito, Michael Eickenberg, Michal Romaniuk, MLG, mr.Shu, Nelle Varoquaux, Nicola Montecchio, Nicolas, Nikolay Mayorov, Noel Dawe, Okal Billy, Olivier Grisel, Óscar Nájera, Paolo Puggioni, Peter Prettenhofer, Pratap Vardhan, pvnguyen, queqichao, Rafael Carrascosa, Raghav R V, Rahiel Kasim, Randall Mason, Rob Zinkov, Robert Bradshaw, Saket Choudhary, Sam Nicholls, Samuel Charron, Saurabh Jha, sethdandridge, sinhrks, snuderl, Stefan Otte, Stefan van der Walt, Steve Tjoa, swu, Sylvain Zimmer, tejesh95, terrycojones, Thomas Delteil, Thomas Unterthiner, Tomas Kazmar, trevorstephens, tttthomasssss, Tzu-Ming Kuo, ugurcaliskan, ugurthemaster, Vinayak Mehta, Vincent Dubourg, Vjacheslav Murashkin, Vlad Niculae, wadawson, Wei Xue, Will Lamond, Wu Jiang, x0l, Xinfan Meng, Yan Yi, Yu-Chin

---

v0.17.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.17

## Version 0.17.1

**February 18, 2016**

### Changelog

#### Bug fixes

  - Upgrade vendored joblib to version 0.9.4 that fixes an important bug in `joblib.Parallel` that can silently yield to wrong results when working on datasets larger than 1MB: <https://github.com/joblib/joblib/blob/0.9.4/CHANGES.rst>
  - Fixed reading of Bunch pickles generated with scikit-learn version \<= 0.16. This can affect users who have already downloaded a dataset with scikit-learn 0.16 and are loading it with scikit-learn 0.17. See `6196` for how this affected <span class="title-ref">datasets.fetch\_20newsgroups</span>. By [Loic Esteve](https://github.com/lesteve).
  - Fixed a bug that prevented using ROC AUC score to perform grid search on several CPU / cores on large arrays. See `6147` By [Olivier Grisel](https://twitter.com/ogrisel).
  - Fixed a bug that prevented to properly set the `presort` parameter in <span class="title-ref">ensemble.GradientBoostingRegressor</span>. See `5857` By Andrew McCulloh.
  - Fixed a joblib error when evaluating the perplexity of a <span class="title-ref">decomposition.LatentDirichletAllocation</span> model. See `6258` By Chyi-Kwei Yau.

## Version 0.17

**November 5, 2015**

### Changelog

#### New features

  - All the Scaler classes but <span class="title-ref">preprocessing.RobustScaler</span> can be fitted online by calling <span class="title-ref">partial\_fit</span>. By `Giorgio Patrini <giorgiop>`.
  - The new class <span class="title-ref">ensemble.VotingClassifier</span> implements a "majority rule" / "soft voting" ensemble classifier to combine estimators for classification. By [Sebastian Raschka](https://sebastianraschka.com/).
  - The new class <span class="title-ref">preprocessing.RobustScaler</span> provides an alternative to <span class="title-ref">preprocessing.StandardScaler</span> for feature-wise centering and range normalization that is robust to outliers. By `Thomas Unterthiner <untom>`.
  - The new class <span class="title-ref">preprocessing.MaxAbsScaler</span> provides an alternative to <span class="title-ref">preprocessing.MinMaxScaler</span> for feature-wise range normalization when the data is already centered or sparse. By `Thomas Unterthiner <untom>`.
  - The new class <span class="title-ref">preprocessing.FunctionTransformer</span> turns a Python function into a `Pipeline`-compatible transformer object. By Joe Jevnik.
  - The new classes <span class="title-ref">cross\_validation.LabelKFold</span> and <span class="title-ref">cross\_validation.LabelShuffleSplit</span> generate train-test folds, respectively similar to <span class="title-ref">cross\_validation.KFold</span> and <span class="title-ref">cross\_validation.ShuffleSplit</span>, except that the folds are conditioned on a label array. By [Brian McFee](https://bmcfee.github.io), `Jean
    Kossaifi <JeanKossaifi>` and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - <span class="title-ref">decomposition.LatentDirichletAllocation</span> implements the Latent Dirichlet Allocation topic model with online variational inference. By `Chyi-Kwei Yau <chyikwei>`, with code based on an implementation by Matt Hoffman. (`3659`)
  - The new solver `sag` implements a Stochastic Average Gradient descent and is available in both <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.Ridge</span>. This solver is very efficient for large datasets. By `Danny Sullivan <dsullivan7>` and [Tom Dupre la Tour](https://github.com/TomDLT). (`4738`)
  - The new solver `cd` implements a Coordinate Descent in <span class="title-ref">decomposition.NMF</span>. Previous solver based on Projected Gradient is still available setting new parameter `solver` to `pg`, but is deprecated and will be removed in 0.19, along with <span class="title-ref">decomposition.ProjectedGradientNMF</span> and parameters `sparseness`, `eta`, `beta` and `nls_max_iter`. New parameters `alpha` and `l1_ratio` control L1 and L2 regularization, and `shuffle` adds a shuffling step in the `cd` solver. By [Tom Dupre la Tour](https://github.com/TomDLT) and [Mathieu Blondel](http://www.mblondel.org).

#### Enhancements

  - <span class="title-ref">manifold.TSNE</span> now supports approximate optimization via the Barnes-Hut method, leading to much faster fitting. By Christopher Erick Moody. (`4025`)
  - <span class="title-ref">cluster.MeanShift</span> now supports parallel execution, as implemented in the `mean_shift` function. By `Martino
    Sorbaro <martinosorb>`.
  - <span class="title-ref">naive\_bayes.GaussianNB</span> now supports fitting with `sample_weight`. By [Jan Hendrik Metzen](https://jmetzen.github.io/).
  - <span class="title-ref">dummy.DummyClassifier</span> now supports a prior fitting strategy. By [Arnaud Joly](http://www.ajoly.org).
  - Added a `fit_predict` method for <span class="title-ref">mixture.GMM</span> and subclasses. By `Cory Lorenz <clorenz7>`.
  - Added the <span class="title-ref">metrics.label\_ranking\_loss</span> metric. By [Arnaud Joly](http://www.ajoly.org).
  - Added the <span class="title-ref">metrics.cohen\_kappa\_score</span> metric.
  - Added a `warm_start` constructor parameter to the bagging ensemble models to increase the size of the ensemble. By `Tim Head <betatim>`.
  - Added option to use multi-output regression metrics without averaging. By Konstantin Shmelkov and `Michael Eickenberg<eickenberg>`.
  - Added `stratify` option to <span class="title-ref">cross\_validation.train\_test\_split</span> for stratified splitting. By Miroslav Batchkarov.
  - The <span class="title-ref">tree.export\_graphviz</span> function now supports aesthetic improvements for <span class="title-ref">tree.DecisionTreeClassifier</span> and <span class="title-ref">tree.DecisionTreeRegressor</span>, including options for coloring nodes by their majority class or impurity, showing variable names, and using node proportions instead of raw sample counts. By [Trevor Stephens](http://trevorstephens.com/).
  - Improved speed of `newton-cg` solver in <span class="title-ref">linear\_model.LogisticRegression</span>, by avoiding loss computation. By [Mathieu Blondel](http://www.mblondel.org) and [Tom Dupre la Tour](https://github.com/TomDLT).
  - The `class_weight="auto"` heuristic in classifiers supporting `class_weight` was deprecated and replaced by the `class_weight="balanced"` option, which has a simpler formula and interpretation. By [Hanna Wallach](https://dirichlet.net/) and [Andreas Müller](https://amueller.github.io/).
  - Add `class_weight` parameter to automatically weight samples by class frequency for <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>. By [Trevor Stephens](http://trevorstephens.com/).
  - Added backlinks from the API reference pages to the user guide. By [Andreas Müller](https://amueller.github.io/).
  - The `labels` parameter to <span class="title-ref">sklearn.metrics.f1\_score</span>, <span class="title-ref">sklearn.metrics.fbeta\_score</span>, <span class="title-ref">sklearn.metrics.recall\_score</span> and <span class="title-ref">sklearn.metrics.precision\_score</span> has been extended. It is now possible to ignore one or more labels, such as where a multiclass problem has a majority class to ignore. By [Joel Nothman](https://joelnothman.com/).
  - Add `sample_weight` support to <span class="title-ref">linear\_model.RidgeClassifier</span>. By [Trevor Stephens](http://trevorstephens.com/).
  - Provide an option for sparse output from <span class="title-ref">sklearn.metrics.pairwise.cosine\_similarity</span>. By `Jaidev Deshpande <jaidevd>`.
  - Add <span class="title-ref">preprocessing.minmax\_scale</span> to provide a function interface for <span class="title-ref">preprocessing.MinMaxScaler</span>. By `Thomas Unterthiner <untom>`.
  - `dump_svmlight_file` now handles multi-label datasets. By Chih-Wei Chang.
  - RCV1 dataset loader (<span class="title-ref">sklearn.datasets.fetch\_rcv1</span>). By [Tom Dupre la Tour](https://github.com/TomDLT).
  - The "Wisconsin Breast Cancer" classical two-class classification dataset is now included in scikit-learn, available with <span class="title-ref">datasets.load\_breast\_cancer</span>.
  - Upgraded to joblib 0.9.3 to benefit from the new automatic batching of short tasks. This makes it possible for scikit-learn to benefit from parallelism when many very short tasks are executed in parallel, for instance by the <span class="title-ref">grid\_search.GridSearchCV</span> meta-estimator with `n_jobs > 1` used with a large grid of parameters on a small dataset. By [Vlad Niculae](https://vene.ro/), [Olivier Grisel](https://twitter.com/ogrisel) and [Loic Esteve](https://github.com/lesteve).
  - For more details about changes in joblib 0.9.3 see the release notes: <https://github.com/joblib/joblib/blob/master/CHANGES.rst#release-093>
  - Improved speed (3 times per iteration) of <span class="title-ref">decomposition.DictLearning</span> with coordinate descent method from <span class="title-ref">linear\_model.Lasso</span>. By `Arthur Mensch <arthurmensch>`.
  - Parallel processing (threaded) for queries of nearest neighbors (using the ball-tree) by Nikolay Mayorov.
  - Allow <span class="title-ref">datasets.make\_multilabel\_classification</span> to output a sparse `y`. By Kashif Rasul.
  - <span class="title-ref">cluster.DBSCAN</span> now accepts a sparse matrix of precomputed distances, allowing memory-efficient distance precomputation. By [Joel Nothman](https://joelnothman.com/).
  - <span class="title-ref">tree.DecisionTreeClassifier</span> now exposes an `apply` method for retrieving the leaf indices samples are predicted as. By `Daniel Galvez <galv>` and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Speed up decision tree regressors, random forest regressors, extra trees regressors and gradient boosting estimators by computing a proxy of the impurity improvement during the tree growth. The proxy quantity is such that the split that maximizes this value also maximizes the impurity improvement. By [Arnaud Joly](http://www.ajoly.org), `Jacob Schreiber <jmschrei>` and [Gilles Louppe](http://www.montefiore.ulg.ac.be/~glouppe/).
  - Speed up tree based methods by reducing the number of computations needed when computing the impurity measure taking into account linear relationship of the computed statistics. The effect is particularly visible with extra trees and on datasets with categorical or sparse features. By [Arnaud Joly](http://www.ajoly.org).
  - <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> now expose an `apply` method for retrieving the leaf indices each sample ends up in under each try. By `Jacob Schreiber <jmschrei>`.
  - Add `sample_weight` support to <span class="title-ref">linear\_model.LinearRegression</span>. By Sonny Hu. (`#4881`)
  - Add `n_iter_without_progress` to <span class="title-ref">manifold.TSNE</span> to control the stopping criterion. By Santi Villalba. (`5186`)
  - Added optional parameter `random_state` in <span class="title-ref">linear\_model.Ridge</span> , to set the seed of the pseudo random generator used in `sag` solver. By [Tom Dupre la Tour](https://github.com/TomDLT).
  - Added optional parameter `warm_start` in <span class="title-ref">linear\_model.LogisticRegression</span>. If set to True, the solvers `lbfgs`, `newton-cg` and `sag` will be initialized with the coefficients computed in the previous fit. By [Tom Dupre la Tour](https://github.com/TomDLT).
  - Added `sample_weight` support to <span class="title-ref">linear\_model.LogisticRegression</span> for the `lbfgs`, `newton-cg`, and `sag` solvers. By [Valentin Stolbunov](http://www.vstolbunov.com). Support added to the `liblinear` solver. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Added optional parameter `presort` to <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span>, keeping default behavior the same. This allows gradient boosters to turn off presorting when building deep trees or using sparse data. By `Jacob Schreiber <jmschrei>`.
  - Altered <span class="title-ref">metrics.roc\_curve</span> to drop unnecessary thresholds by default. By `Graham Clenaghan <gclenaghan>`.
  - Added <span class="title-ref">feature\_selection.SelectFromModel</span> meta-transformer which can be used along with estimators that have <span class="title-ref">coef\_</span> or <span class="title-ref">feature\_importances\_</span> attribute to select important features of the input data. By `Maheshakya Wijewardena <maheshakya>`, [Joel Nothman](https://joelnothman.com/) and [Manoj Kumar](https://manojbits.wordpress.com).
  - Added <span class="title-ref">metrics.pairwise.laplacian\_kernel</span>. By [Clyde Fare](https://github.com/Clyde-fare).
  - <span class="title-ref">covariance.GraphLasso</span> allows separate control of the convergence criterion for the Elastic-Net subproblem via the `enet_tol` parameter.
  - Improved verbosity in <span class="title-ref">decomposition.DictionaryLearning</span>.
  - <span class="title-ref">ensemble.RandomForestClassifier</span> and <span class="title-ref">ensemble.RandomForestRegressor</span> no longer explicitly store the samples used in bagging, resulting in a much reduced memory footprint for storing random forest models.
  - Added `positive` option to <span class="title-ref">linear\_model.Lars</span> and <span class="title-ref">linear\_model.lars\_path</span> to force coefficients to be positive. (`5131`)
  - Added the `X_norm_squared` parameter to <span class="title-ref">metrics.pairwise.euclidean\_distances</span> to provide precomputed squared norms for `X`.
  - Added the `fit_predict` method to <span class="title-ref">pipeline.Pipeline</span>.
  - Added the <span class="title-ref">preprocessing.minmax\_scale</span> function.

#### Bug fixes

  - Fixed non-determinism in <span class="title-ref">dummy.DummyClassifier</span> with sparse multi-label output. By [Andreas Müller](https://amueller.github.io/).
  - Fixed the output shape of <span class="title-ref">linear\_model.RANSACRegressor</span> to `(n_samples, )`. By [Andreas Müller](https://amueller.github.io/).
  - Fixed bug in <span class="title-ref">decomposition.DictLearning</span> when `n_jobs < 0`. By [Andreas Müller](https://amueller.github.io/).
  - Fixed bug where <span class="title-ref">grid\_search.RandomizedSearchCV</span> could consume a lot of memory for large discrete grids. By [Joel Nothman](https://joelnothman.com/).
  - Fixed bug in <span class="title-ref">linear\_model.LogisticRegressionCV</span> where <span class="title-ref">penalty</span> was ignored in the final fit. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Fixed bug in <span class="title-ref">ensemble.forest.ForestClassifier</span> while computing oob\_score and X is a sparse.csc\_matrix. By `Ankur Ankan <ankurankan>`.
  - All regressors now consistently handle and warn when given `y` that is of shape `(n_samples, 1)`. By [Andreas Müller](https://amueller.github.io/) and Henry Lin. (`5431`)
  - Fix in <span class="title-ref">cluster.KMeans</span> cluster reassignment for sparse input by [Lars Buitinck](https://github.com/larsmans).
  - Fixed a bug in <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> that could cause asymmetric covariance matrices when using shrinkage. By [Martin Billinger](https://tnsre.embs.org/author/martinbillinger/).
  - Fixed <span class="title-ref">cross\_validation.cross\_val\_predict</span> for estimators with sparse predictions. By Buddha Prakash.
  - Fixed the `predict_proba` method of <span class="title-ref">linear\_model.LogisticRegression</span> to use soft-max instead of one-vs-rest normalization. By [Manoj Kumar](https://manojbits.wordpress.com). (`5182`)
  - Fixed the <span class="title-ref">partial\_fit</span> method of <span class="title-ref">linear\_model.SGDClassifier</span> when called with `average=True`. By `Andrew Lamb <andylamb>`. (`5282`)
  - Dataset fetchers use different filenames under Python 2 and Python 3 to avoid pickling compatibility issues. By [Olivier Grisel](https://twitter.com/ogrisel). (`5355`)
  - Fixed a bug in <span class="title-ref">naive\_bayes.GaussianNB</span> which caused classification results to depend on scale. By [Jake Vanderplas](https://staff.washington.edu/jakevdp/).
  - Fixed temporarily <span class="title-ref">linear\_model.Ridge</span>, which was incorrect when fitting the intercept in the case of sparse data. The fix automatically changes the solver to 'sag' in this case. `5360` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - Fixed a performance bug in <span class="title-ref">decomposition.RandomizedPCA</span> on data with a large number of features and fewer samples. (`4478`) By [Andreas Müller](https://amueller.github.io/), [Loic Esteve](https://github.com/lesteve) and `Giorgio Patrini <giorgiop>`.
  - Fixed bug in <span class="title-ref">cross\_decomposition.PLS</span> that yielded unstable and platform dependent output, and failed on <span class="title-ref">fit\_transform</span>. By `Arthur Mensch <arthurmensch>`.
  - Fixes to the `Bunch` class used to store datasets.
  - Fixed <span class="title-ref">ensemble.plot\_partial\_dependence</span> ignoring the `percentiles` parameter.
  - Providing a `set` as vocabulary in `CountVectorizer` no longer leads to inconsistent results when pickling.
  - Fixed the conditions on when a precomputed Gram matrix needs to be recomputed in <span class="title-ref">linear\_model.LinearRegression</span>, <span class="title-ref">linear\_model.OrthogonalMatchingPursuit</span>, <span class="title-ref">linear\_model.Lasso</span> and <span class="title-ref">linear\_model.ElasticNet</span>.
  - Fixed inconsistent memory layout in the coordinate descent solver that affected <span class="title-ref">linear\_model.DictionaryLearning</span> and <span class="title-ref">covariance.GraphLasso</span>. (`5337`) By [Olivier Grisel](https://twitter.com/ogrisel).
  - <span class="title-ref">manifold.LocallyLinearEmbedding</span> no longer ignores the `reg` parameter.
  - Nearest Neighbor estimators with custom distance metrics can now be pickled. (`4362`)
  - Fixed a bug in <span class="title-ref">pipeline.FeatureUnion</span> where `transformer_weights` were not properly handled when performing grid-searches.
  - Fixed a bug in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> when using `class_weight='balanced'` or `class_weight='auto'`. By [Tom Dupre la Tour](https://github.com/TomDLT).
  - Fixed bug `5495` when doing OVR(SVC(decision\_function\_shape="ovr")). Fixed by `Elvis Dohmatob <dohmatob>`.

### API changes summary

  - Attribute <span class="title-ref">data\_min</span>, <span class="title-ref">data\_max</span> and <span class="title-ref">data\_range</span> in <span class="title-ref">preprocessing.MinMaxScaler</span> are deprecated and won't be available from 0.19. Instead, the class now exposes <span class="title-ref">data\_min\_</span>, <span class="title-ref">data\_max\_</span> and <span class="title-ref">data\_range\_</span>. By `Giorgio Patrini <giorgiop>`.
  - All Scaler classes now have an <span class="title-ref">scale\_</span> attribute, the feature-wise rescaling applied by their <span class="title-ref">transform</span> methods. The old attribute <span class="title-ref">std\_</span> in <span class="title-ref">preprocessing.StandardScaler</span> is deprecated and superseded by <span class="title-ref">scale\_</span>; it won't be available in 0.19. By `Giorgio Patrini <giorgiop>`.
  - <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.NuSVC</span> now have an `decision_function_shape` parameter to make their decision function of shape `(n_samples, n_classes)` by setting `decision_function_shape='ovr'`. This will be the default behavior starting in 0.19. By [Andreas Müller](https://amueller.github.io/).
  - Passing 1D data arrays as input to estimators is now deprecated as it caused confusion in how the array elements should be interpreted as features or as samples. All data arrays are now expected to be explicitly shaped `(n_samples, n_features)`. By `Vighnesh Birodkar <vighneshbirodkar>`.
  - <span class="title-ref">lda.LDA</span> and <span class="title-ref">qda.QDA</span> have been moved to <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> and <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis</span>.
  - The `store_covariance` and `tol` parameters have been moved from the fit method to the constructor in <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> and the `store_covariances` and `tol` parameters have been moved from the fit method to the constructor in <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis</span>.
  - Models inheriting from `_LearntSelectorMixin` will no longer support the transform methods. (i.e, RandomForests, GradientBoosting, LogisticRegression, DecisionTrees, SVMs and SGD related models). Wrap these models around the metatransfomer <span class="title-ref">feature\_selection.SelectFromModel</span> to remove features (according to <span class="title-ref">coefs\_</span> or <span class="title-ref">feature\_importances\_</span>) which are below a certain threshold value instead.
  - <span class="title-ref">cluster.KMeans</span> re-runs cluster-assignments in case of non-convergence, to ensure consistency of `predict(X)` and `labels_`. By `Vighnesh Birodkar <vighneshbirodkar>`.
  - Classifier and Regressor models are now tagged as such using the `_estimator_type` attribute.
  - Cross-validation iterators always provide indices into training and test set, not boolean masks.
  - The `decision_function` on all regressors was deprecated and will be removed in 0.19. Use `predict` instead.
  - <span class="title-ref">datasets.load\_lfw\_pairs</span> is deprecated and will be removed in 0.19. Use <span class="title-ref">datasets.fetch\_lfw\_pairs</span> instead.
  - The deprecated `hmm` module was removed.
  - The deprecated `Bootstrap` cross-validation iterator was removed.
  - The deprecated `Ward` and `WardAgglomerative` classes have been removed. Use <span class="title-ref">cluster.AgglomerativeClustering</span> instead.
  - <span class="title-ref">cross\_validation.check\_cv</span> is now a public function.
  - The property `residues_` of <span class="title-ref">linear\_model.LinearRegression</span> is deprecated and will be removed in 0.19.
  - The deprecated `n_jobs` parameter of <span class="title-ref">linear\_model.LinearRegression</span> has been moved to the constructor.
  - Removed deprecated `class_weight` parameter from <span class="title-ref">linear\_model.SGDClassifier</span>'s `fit` method. Use the construction parameter instead.
  - The deprecated support for the sequence of sequences (or list of lists) multilabel format was removed. To convert to and from the supported binary indicator matrix format, use <span class="title-ref">MultiLabelBinarizer \<preprocessing.MultiLabelBinarizer\></span>.
  - The behavior of calling the `inverse_transform` method of `Pipeline.pipeline` will change in 0.19. It will no longer reshape one-dimensional input to two-dimensional input.
  - The deprecated attributes `indicator_matrix_`, `multilabel_` and `classes_` of <span class="title-ref">preprocessing.LabelBinarizer</span> were removed.
  - Using `gamma=0` in <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.SVR</span> to automatically set the gamma to `1. / n_features` is deprecated and will be removed in 0.19. Use `gamma="auto"` instead.

### Code Contributors

Aaron Schumacher, Adithya Ganesh, akitty, Alexandre Gramfort, Alexey Grigorev, Ali Baharev, Allen Riddell, Ando Saabas, Andreas Mueller, Andrew Lamb, Anish Shah, Ankur Ankan, Anthony Erlinger, Ari Rouvinen, Arnaud Joly, Arnaud Rachez, Arthur Mensch, banilo, Barmaley.exe, benjaminirving, Boyuan Deng, Brett Naul, Brian McFee, Buddha Prakash, Chi Zhang, Chih-Wei Chang, Christof Angermueller, Christoph Gohlke, Christophe Bourguignat, Christopher Erick Moody, Chyi-Kwei Yau, Cindy Sridharan, CJ Carey, Clyde-fare, Cory Lorenz, Dan Blanchard, Daniel Galvez, Daniel Kronovet, Danny Sullivan, Data1010, David, David D Lowe, David Dotson, djipey, Dmitry Spikhalskiy, Donne Martin, Dougal J. Sutherland, Dougal Sutherland, edson duarte, Eduardo Caro, Eric Larson, Eric Martin, Erich Schubert, Fernando Carrillo, Frank C. Eckert, Frank Zalkow, Gael Varoquaux, Ganiev Ibraim, Gilles Louppe, Giorgio Patrini, giorgiop, Graham Clenaghan, Gryllos Prokopis, gwulfs, Henry Lin, Hsuan-Tien Lin, Immanuel Bayer, Ishank Gulati, Jack Martin, Jacob Schreiber, Jaidev Deshpande, Jake Vanderplas, Jan Hendrik Metzen, Jean Kossaifi, Jeffrey04, Jeremy, jfraj, Jiali Mei, Joe Jevnik, Joel Nothman, John Kirkham, John Wittenauer, Joseph, Joshua Loyal, Jungkook Park, KamalakerDadi, Kashif Rasul, Keith Goodman, Kian Ho, Konstantin Shmelkov, Kyler Brown, Lars Buitinck, Lilian Besson, Loic Esteve, Louis Tiao, maheshakya, Maheshakya Wijewardena, Manoj Kumar, MarkTab marktab.net, Martin Ku, Martin Spacek, MartinBpr, martinosorb, MaryanMorel, Masafumi Oyamada, Mathieu Blondel, Matt Krump, Matti Lyra, Maxim Kolganov, mbillinger, mhg, Michael Heilman, Michael Patterson, Miroslav Batchkarov, Nelle Varoquaux, Nicolas, Nikolay Mayorov, Olivier Grisel, Omer Katz, Óscar Nájera, Pauli Virtanen, Peter Fischer, Peter Prettenhofer, Phil Roth, pianomania, Preston Parry, Raghav RV, Rob Zinkov, Robert Layton, Rohan Ramanath, Saket Choudhary, Sam Zhang, santi, saurabh.bansod, scls19fr, Sebastian Raschka, Sebastian Saeger, Shivan Sornarajah, SimonPL, sinhrks, Skipper Seabold, Sonny Hu, sseg, Stephen Hoover, Steven De Gryze, Steven Seguin, Theodore Vasiloudis, Thomas Unterthiner, Tiago Freitas Pereira, Tian Wang, Tim Head, Timothy Hopper, tokoroten, Tom Dupré la Tour, Trevor Stephens, Valentin Stolbunov, Vighnesh Birodkar, Vinayak Mehta, Vincent, Vincent Michel, vstolbunov, wangz10, Wei Xue, Yucheng Low, Yury Zhauniarovich, Zac Stewart, zhai\_pro, Zichen Wang

---

v0.18.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.18

\> **Warning** \> Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6. Later versions of scikit-learn will require Python 2.7 or above.

## Version 0.18.2

**June 20, 2017**

### Changelog

  - Fixes for compatibility with NumPy 1.13.0: `7946` `8355` by [Loic Esteve](https://github.com/lesteve).
  - Minor compatibility changes in the examples `9010` `8040` `9149`.

### Code Contributors

Aman Dalmia, Loic Esteve, Nate Guerin, Sergei Lebedev

## Version 0.18.1

**November 11, 2016**

### Changelog

#### Enhancements

  - Improved `sample_without_replacement` speed by utilizing numpy.random.permutation for most cases. As a result, samples may differ in this release for a fixed random state. Affected estimators:
    
      - <span class="title-ref">ensemble.BaggingClassifier</span>
      - <span class="title-ref">ensemble.BaggingRegressor</span>
      - <span class="title-ref">linear\_model.RANSACRegressor</span>
      - <span class="title-ref">model\_selection.RandomizedSearchCV</span>
      - <span class="title-ref">random\_projection.SparseRandomProjection</span>
    
    This also affects the <span class="title-ref">datasets.make\_classification</span> method.

#### Bug fixes

  - Fix issue where `min_grad_norm` and `n_iter_without_progress` parameters were not being utilised by <span class="title-ref">manifold.TSNE</span>. `6497` by `Sebastian Säger <ssaeger>`
  - Fix bug for svm's decision values when `decision_function_shape` is `ovr` in <span class="title-ref">svm.SVC</span>. <span class="title-ref">svm.SVC</span>'s decision\_function was incorrect from versions 0.17.0 through 0.18.0. `7724` by [Bing Tian Dai](https://github.com/btdai)
  - Attribute `explained_variance_ratio` of <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> calculated with SVD and Eigen solver are now of the same length. `7632` by `JPFrancoia <JPFrancoia>`
  - Fixes issue in \[univariate\_feature\_selection\](\#univariate\_feature\_selection) where score functions were not accepting multi-label targets. `7676` by `Mohammed Affan <affanv14>`
  - Fixed setting parameters when calling `fit` multiple times on <span class="title-ref">feature\_selection.SelectFromModel</span>. `7756` by [Andreas Müller](https://amueller.github.io/)
  - Fixes issue in `partial_fit` method of <span class="title-ref">multiclass.OneVsRestClassifier</span> when number of classes used in `partial_fit` was less than the total number of classes in the data. `7786` by [Srivatsan Ramesh](https://github.com/srivatsan-ramesh)
  - Fixes issue in <span class="title-ref">calibration.CalibratedClassifierCV</span> where the sum of probabilities of each class for a data was not 1, and `CalibratedClassifierCV` now handles the case where the training set has less number of classes than the total data. `7799` by [Srivatsan Ramesh](https://github.com/srivatsan-ramesh)
  - Fix a bug where <span class="title-ref">sklearn.feature\_selection.SelectFdr</span> did not exactly implement Benjamini-Hochberg procedure. It formerly may have selected fewer features than it should. `7490` by `Peng Meng <mpjlu>`.
  - <span class="title-ref">sklearn.manifold.LocallyLinearEmbedding</span> now correctly handles integer inputs. `6282` by [Jake Vanderplas](https://staff.washington.edu/jakevdp/).
  - The `min_weight_fraction_leaf` parameter of tree-based classifiers and regressors now assumes uniform sample weights by default if the `sample_weight` argument is not passed to the `fit` function. Previously, the parameter was silently ignored. `7301` by `Nelson Liu <nelson-liu>`.
  - Numerical issue with <span class="title-ref">linear\_model.RidgeCV</span> on centered data when <span class="title-ref">n\_features \> n\_samples</span>. `6178` by [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page)
  - Tree splitting criterion classes' cloning/pickling is now memory safe `7680` by `Ibraim Ganiev <olologin>`.
  - Fixed a bug where <span class="title-ref">decomposition.NMF</span> sets its `n_iters_` attribute in <span class="title-ref">transform()</span>. `7553` by `Ekaterina
    Krivich <kiote>`.
  - <span class="title-ref">sklearn.linear\_model.LogisticRegressionCV</span> now correctly handles string labels. `5874` by [Raghav RV](https://github.com/raghavrv).
  - Fixed a bug where <span class="title-ref">sklearn.model\_selection.train\_test\_split</span> raised an error when `stratify` is a list of string labels. `7593` by [Raghav RV](https://github.com/raghavrv).
  - Fixed a bug where <span class="title-ref">sklearn.model\_selection.GridSearchCV</span> and <span class="title-ref">sklearn.model\_selection.RandomizedSearchCV</span> were not pickleable because of a pickling bug in `np.ma.MaskedArray`. `7594` by [Raghav RV](https://github.com/raghavrv).
  - All cross-validation utilities in `sklearn.model_selection` now permit one time cross-validation splitters for the `cv` parameter. Also non-deterministic cross-validation splitters (where multiple calls to `split` produce dissimilar splits) can be used as `cv` parameter. The <span class="title-ref">sklearn.model\_selection.GridSearchCV</span> will cross-validate each parameter setting on the split produced by the first `split` call to the cross-validation splitter. `7660` by [Raghav RV](https://github.com/raghavrv).
  - Fix bug where <span class="title-ref">preprocessing.MultiLabelBinarizer.fit\_transform</span> returned an invalid CSR matrix. `7750` by `CJ Carey <perimosocordiae>`.
  - Fixed a bug where <span class="title-ref">metrics.pairwise.cosine\_distances</span> could return a small negative distance. `7732` by `Artsion <asanakoy>`.

### API changes summary

Trees and forests

  - The `min_weight_fraction_leaf` parameter of tree-based classifiers and regressors now assumes uniform sample weights by default if the `sample_weight` argument is not passed to the `fit` function. Previously, the parameter was silently ignored. `7301` by `Nelson
    Liu <nelson-liu>`.
  - Tree splitting criterion classes' cloning/pickling is now memory safe. `7680` by `Ibraim Ganiev <olologin>`.

Linear, kernelized and related models

  - Length of `explained_variance_ratio` of <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> changed for both Eigen and SVD solvers. The attribute has now a length of min(n\_components, n\_classes - 1). `7632` by `JPFrancoia <JPFrancoia>`
  - Numerical issue with <span class="title-ref">linear\_model.RidgeCV</span> on centered data when `n_features > n_samples`. `6178` by [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page)

## Version 0.18

**September 28, 2016**

### Model Selection Enhancements and API Changes

  - **The model\_selection module**
    
    The new module `sklearn.model_selection`, which groups together the functionalities of formerly <span class="title-ref">sklearn.cross\_validation</span>, <span class="title-ref">sklearn.grid\_search</span> and <span class="title-ref">sklearn.learning\_curve</span>, introduces new possibilities such as nested cross-validation and better manipulation of parameter searches with Pandas.
    
    Many things will stay the same but there are some key differences. Read below to know more about the changes.

  - **Data-independent CV splitters enabling nested cross-validation**
    
    The new cross-validation splitters, defined in the `sklearn.model_selection`, are no longer initialized with any data-dependent parameters such as `y`. Instead they expose a <span class="title-ref">split</span> method that takes in the data and yields a generator for the different splits.
    
    This change makes it possible to use the cross-validation splitters to perform nested cross-validation, facilitated by <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> utilities.

  - **The enhanced cv\_results\_ attribute**
    
    The new `cv_results_` attribute (of <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span>) introduced in lieu of the `grid_scores_` attribute is a dict of 1D arrays with elements in each array corresponding to the parameter settings (i.e. search candidates).
    
    The `cv_results_` dict can be easily imported into `pandas` as a `DataFrame` for exploring the search results.
    
    The `cv_results_` arrays include scores for each cross-validation split (with keys such as `'split0_test_score'`), as well as their mean (`'mean_test_score'`) and standard deviation (`'std_test_score'`).
    
    The ranks for the search candidates (based on their mean cross-validation score) is available at `cv_results_['rank_test_score']`.
    
    The parameter values for each parameter is stored separately as numpy masked object arrays. The value, for that search candidate, is masked if the corresponding parameter is not applicable. Additionally a list of all the parameter dicts are stored at `cv_results_['params']`.

  - **Parameters n\_folds and n\_iter renamed to n\_splits**
    
    Some parameter names have changed: The `n_folds` parameter in new <span class="title-ref">model\_selection.KFold</span>, <span class="title-ref">model\_selection.GroupKFold</span> (see below for the name change), and <span class="title-ref">model\_selection.StratifiedKFold</span> is now renamed to `n_splits`. The `n_iter` parameter in <span class="title-ref">model\_selection.ShuffleSplit</span>, the new class <span class="title-ref">model\_selection.GroupShuffleSplit</span> and <span class="title-ref">model\_selection.StratifiedShuffleSplit</span> is now renamed to `n_splits`.

  - **Rename of splitter classes which accepts group labels along with data**
    
    The cross-validation splitters `LabelKFold`, `LabelShuffleSplit`, `LeaveOneLabelOut` and `LeavePLabelOut` have been renamed to <span class="title-ref">model\_selection.GroupKFold</span>, <span class="title-ref">model\_selection.GroupShuffleSplit</span>, <span class="title-ref">model\_selection.LeaveOneGroupOut</span> and <span class="title-ref">model\_selection.LeavePGroupsOut</span> respectively.
    
    Note the change from singular to plural form in <span class="title-ref">model\_selection.LeavePGroupsOut</span>.

  - **Fit parameter labels renamed to groups**
    
    The `labels` parameter in the <span class="title-ref">split</span> method of the newly renamed splitters <span class="title-ref">model\_selection.GroupKFold</span>, <span class="title-ref">model\_selection.LeaveOneGroupOut</span>, <span class="title-ref">model\_selection.LeavePGroupsOut</span>, <span class="title-ref">model\_selection.GroupShuffleSplit</span> is renamed to `groups` following the new nomenclature of their class names.

  - **Parameter n\_labels renamed to n\_groups**
    
    The parameter `n_labels` in the newly renamed <span class="title-ref">model\_selection.LeavePGroupsOut</span> is changed to `n_groups`.

  - Training scores and Timing information
    
    `cv_results_` also includes the training scores for each cross-validation split (with keys such as `'split0_train_score'`), as well as their mean (`'mean_train_score'`) and standard deviation (`'std_train_score'`). To avoid the cost of evaluating training score, set `return_train_score=False`.
    
    Additionally the mean and standard deviation of the times taken to split, train and score the model across all the cross-validation splits is available at the key `'mean_time'` and `'std_time'` respectively.

### Changelog

#### New features

Classifiers and Regressors

  - The Gaussian Process module has been reimplemented and now offers classification and regression estimators through <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span> and <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>. Among other things, the new implementation supports kernel engineering, gradient-based hyperparameter optimization or sampling of functions from GP prior and GP posterior. Extensive documentation and examples are provided. By [Jan Hendrik Metzen](https://jmetzen.github.io/).
  - Added new supervised learning algorithm: \[Multi-layer Perceptron \<multilayer\_perceptron\>\](\#multi-layer-perceptron-\<multilayer\_perceptron\>) `3204` by `Issam H. Laradji <IssamLaradji>`
  - Added <span class="title-ref">linear\_model.HuberRegressor</span>, a linear model robust to outliers. `5291` by [Manoj Kumar](https://manojbits.wordpress.com).
  - Added the <span class="title-ref">multioutput.MultiOutputRegressor</span> meta-estimator. It converts single output regressors to multi-output regressors by fitting one regressor per output. By `Tim Head <betatim>`.

Other estimators

  - New <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span> replace former mixture models, employing faster inference for sounder results. `7295` by `Wei Xue <xuewei4d>` and `Thierry Guillemot <tguillemot>`.
  - Class <span class="title-ref">decomposition.RandomizedPCA</span> is now factored into <span class="title-ref">decomposition.PCA</span> and it is available calling with parameter `svd_solver='randomized'`. The default number of `n_iter` for `'randomized'` has changed to 4. The old behavior of PCA is recovered by `svd_solver='full'`. An additional solver calls `arpack` and performs truncated (non-randomized) SVD. By default, the best solver is selected depending on the size of the input and the number of components requested. `5299` by `Giorgio Patrini <giorgiop>`.
  - Added two functions for mutual information estimation: <span class="title-ref">feature\_selection.mutual\_info\_classif</span> and <span class="title-ref">feature\_selection.mutual\_info\_regression</span>. These functions can be used in <span class="title-ref">feature\_selection.SelectKBest</span> and <span class="title-ref">feature\_selection.SelectPercentile</span> as score functions. By `Andrea Bravi <AndreaBravi>` and `Nikolay Mayorov <nmayorov>`.
  - Added the <span class="title-ref">ensemble.IsolationForest</span> class for anomaly detection based on random forests. By [Nicolas Goix](https://ngoix.github.io/).
  - Added `algorithm="elkan"` to <span class="title-ref">cluster.KMeans</span> implementing Elkan's fast K-Means algorithm. By [Andreas Müller](https://amueller.github.io/).

Model selection and evaluation

  - Added <span class="title-ref">metrics.fowlkes\_mallows\_score</span>, the Fowlkes Mallows Index which measures the similarity of two clusterings of a set of points By `Arnaud Fouchet <afouchet>` and `Thierry Guillemot <tguillemot>`.
  - Added <span class="title-ref">metrics.calinski\_harabaz\_score</span>, which computes the Calinski and Harabaz score to evaluate the resulting clustering of a set of points. By `Arnaud Fouchet <afouchet>` and `Thierry Guillemot <tguillemot>`.
  - Added new cross-validation splitter <span class="title-ref">model\_selection.TimeSeriesSplit</span> to handle time series data. `6586` by `YenChen Lin <yenchenlin>`
  - The cross-validation iterators are replaced by cross-validation splitters available from `sklearn.model_selection`, allowing for nested cross-validation. See \[model\_selection\_changes\](\#model\_selection\_changes) for more information. `4294` by [Raghav RV](https://github.com/raghavrv).

#### Enhancements

Trees and ensembles

  - Added a new splitting criterion for <span class="title-ref">tree.DecisionTreeRegressor</span>, the mean absolute error. This criterion can also be used in <span class="title-ref">ensemble.ExtraTreesRegressor</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, and the gradient boosting estimators. `6667` by `Nelson Liu <nelson-liu>`.
  - Added weighted impurity-based early stopping criterion for decision tree growth. `6954` by `Nelson Liu <nelson-liu>`
  - The random forest, extra tree and decision tree estimators now has a method `decision_path` which returns the decision path of samples in the tree. By [Arnaud Joly](http://www.ajoly.org).
  - A new example has been added unveiling the decision tree structure. By [Arnaud Joly](http://www.ajoly.org).
  - Random forest, extra trees, decision trees and gradient boosting estimator accept the parameter `min_samples_split` and `min_samples_leaf` provided as a percentage of the training samples. By `yelite <yelite>` and [Arnaud Joly](http://www.ajoly.org).
  - Gradient boosting estimators accept the parameter `criterion` to specify to splitting criterion used in built decision trees. `6667` by `Nelson Liu <nelson-liu>`.
  - The memory footprint is reduced (sometimes greatly) for <span class="title-ref">ensemble.bagging.BaseBagging</span> and classes that inherit from it, i.e, <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span>, and <span class="title-ref">ensemble.IsolationForest</span>, by dynamically generating attribute `estimators_samples_` only when it is needed. By `David Staub <staubda>`.
  - Added `n_jobs` and `sample_weight` parameters for <span class="title-ref">ensemble.VotingClassifier</span> to fit underlying estimators in parallel. `5805` by `Ibraim Ganiev <olologin>`.

Linear, kernelized and related models

  - In <span class="title-ref">linear\_model.LogisticRegression</span>, the SAG solver is now available in the multinomial case. `5251` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - <span class="title-ref">linear\_model.RANSACRegressor</span>, <span class="title-ref">svm.LinearSVC</span> and <span class="title-ref">svm.LinearSVR</span> now support `sample_weight`. By `Imaculate <Imaculate>`.
  - Add parameter `loss` to <span class="title-ref">linear\_model.RANSACRegressor</span> to measure the error on the samples for every trial. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Prediction of out-of-sample events with Isotonic Regression (<span class="title-ref">isotonic.IsotonicRegression</span>) is now much faster (over 1000x in tests with synthetic data). By `Jonathan Arfa <jarfa>`.
  - Isotonic regression (<span class="title-ref">isotonic.IsotonicRegression</span>) now uses a better algorithm to avoid <span class="title-ref">O(n^2)</span> behavior in pathological cases, and is also generally faster (`#6691`). By [Antony Lee](https://www.ocf.berkeley.edu/~antonyl/).
  - <span class="title-ref">naive\_bayes.GaussianNB</span> now accepts data-independent class-priors through the parameter `priors`. By `Guillaume Lemaitre <glemaitre>`.
  - <span class="title-ref">linear\_model.ElasticNet</span> and <span class="title-ref">linear\_model.Lasso</span> now works with `np.float32` input data without converting it into `np.float64`. This allows to reduce the memory consumption. `6913` by `YenChen Lin <yenchenlin>`.
  - <span class="title-ref">semi\_supervised.LabelPropagation</span> and <span class="title-ref">semi\_supervised.LabelSpreading</span> now accept arbitrary kernel functions in addition to strings `knn` and `rbf`. `5762` by `Utkarsh Upadhyay <musically-ut>`.

Decomposition, manifold learning and clustering

  - Added `inverse_transform` function to <span class="title-ref">decomposition.NMF</span> to compute data matrix of original shape. By `Anish Shah <AnishShah>`.
  - <span class="title-ref">cluster.KMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span> now works with `np.float32` and `np.float64` input data without converting it. This allows to reduce the memory consumption by using `np.float32`. `6846` by `Sebastian Säger <ssaeger>` and `YenChen Lin <yenchenlin>`.

Preprocessing and feature selection

  - <span class="title-ref">preprocessing.RobustScaler</span> now accepts `quantile_range` parameter. `5929` by `Konstantin Podshumok <podshumok>`.
  - <span class="title-ref">feature\_extraction.FeatureHasher</span> now accepts string values. `6173` by `Ryad Zenine <ryadzenine>` and `Devashish Deshpande <dsquareindia>`.
  - Keyword arguments can now be supplied to `func` in <span class="title-ref">preprocessing.FunctionTransformer</span> by means of the `kw_args` parameter. By [Brian McFee](https://bmcfee.github.io).
  - <span class="title-ref">feature\_selection.SelectKBest</span> and <span class="title-ref">feature\_selection.SelectPercentile</span> now accept score functions that take X, y as input and return only the scores. By `Nikolay Mayorov <nmayorov>`.

Model evaluation and meta-estimators

  - <span class="title-ref">multiclass.OneVsOneClassifier</span> and <span class="title-ref">multiclass.OneVsRestClassifier</span> now support `partial_fit`. By `Asish Panda <kaichogami>` and `Philipp Dowling <phdowling>`.
  - Added support for substituting or disabling <span class="title-ref">pipeline.Pipeline</span> and <span class="title-ref">pipeline.FeatureUnion</span> components using the `set_params` interface that powers <span class="title-ref">sklearn.grid\_search</span>. See \[sphx\_glr\_auto\_examples\_compose\_plot\_compare\_reduction.py\](\#sphx\_glr\_auto\_examples\_compose\_plot\_compare\_reduction.py) By [Joel Nothman](https://joelnothman.com/) and `Robert McGibbon <rmcgibbo>`.
  - The new `cv_results_` attribute of <span class="title-ref">model\_selection.GridSearchCV</span> (and <span class="title-ref">model\_selection.RandomizedSearchCV</span>) can be easily imported into pandas as a `DataFrame`. Ref \[model\_selection\_changes\](\#model\_selection\_changes) for more information. `6697` by [Raghav RV](https://github.com/raghavrv).
  - Generalization of <span class="title-ref">model\_selection.cross\_val\_predict</span>. One can pass method names such as <span class="title-ref">predict\_proba</span> to be used in the cross validation framework instead of the default <span class="title-ref">predict</span>. By `Ori Ziv <zivori>` and `Sears Merritt <merritts>`.
  - The training scores and time taken for training followed by scoring for each search candidate are now available at the `cv_results_` dict. See \[model\_selection\_changes\](\#model\_selection\_changes) for more information. `7325` by `Eugene Chen <eyc88>` and [Raghav RV](https://github.com/raghavrv).

Metrics

  - Added `labels` flag to <span class="title-ref">metrics.log\_loss</span> to explicitly provide the labels when the number of classes in `y_true` and `y_pred` differ. `7239` by `Hong Guangguo <hongguangguo>` with help from `Mads Jensen <indianajensen>` and `Nelson Liu <nelson-liu>`.
  - Support sparse contingency matrices in cluster evaluation (<span class="title-ref">metrics.cluster.supervised</span>) to scale to a large number of clusters. `7419` by `Gregory Stupp <stuppie>` and [Joel Nothman](https://joelnothman.com/).
  - Add `sample_weight` parameter to <span class="title-ref">metrics.matthews\_corrcoef</span>. By `Jatin Shah <jatinshah>` and [Raghav RV](https://github.com/raghavrv).
  - Speed up <span class="title-ref">metrics.silhouette\_score</span> by using vectorized operations. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Add `sample_weight` parameter to <span class="title-ref">metrics.confusion\_matrix</span>. By `Bernardo Stein <DanielSidhion>`.

Miscellaneous

  - Added `n_jobs` parameter to <span class="title-ref">feature\_selection.RFECV</span> to compute the score on the test folds in parallel. By [Manoj Kumar](https://manojbits.wordpress.com)
  - Codebase does not contain C/C++ cython generated files: they are generated during build. Distribution packages will still contain generated C/C++ files. By `Arthur Mensch <arthurmensch>`.
  - Reduce the memory usage for 32-bit float input arrays of <span class="title-ref">utils.sparse\_func.mean\_variance\_axis</span> and <span class="title-ref">utils.sparse\_func.incr\_mean\_variance\_axis</span> by supporting cython fused types. By `YenChen Lin <yenchenlin>`.
  - The <span class="title-ref">ignore\_warnings</span> now accept a category argument to ignore only the warnings of a specified type. By `Thierry Guillemot <tguillemot>`.
  - Added parameter `return_X_y` and return type `(data, target) : tuple` option to <span class="title-ref">datasets.load\_iris</span> dataset `7049`, <span class="title-ref">datasets.load\_breast\_cancer</span> dataset `7152`, <span class="title-ref">datasets.load\_digits</span> dataset, <span class="title-ref">datasets.load\_diabetes</span> dataset, <span class="title-ref">datasets.load\_linnerud</span> dataset, <span class="title-ref">datasets.load\_boston</span> dataset `7154` by `Manvendra Singh<manu-chroma>`.
  - Simplification of the `clone` function, deprecate support for estimators that modify parameters in `__init__`. `5540` by [Andreas Müller](https://amueller.github.io/).

<!-- end list -->

  - \- When unpickling a scikit-learn estimator in a different version than the one  
    the estimator was trained with, a `UserWarning` is raised, see \[the documentation on model persistence \<persistence\_limitations\>\](\#the-documentation

  - \--on-model-persistence-\<persistence\_limitations\>) for more details. (`7248`)  
    By [Andreas Müller](https://amueller.github.io/).

#### Bug fixes

Trees and ensembles

  - Random forest, extra trees, decision trees and gradient boosting won't accept anymore `min_samples_split=1` as at least 2 samples are required to split a decision tree node. By [Arnaud Joly](http://www.ajoly.org)
  - <span class="title-ref">ensemble.VotingClassifier</span> now raises `NotFittedError` if `predict`, `transform` or `predict_proba` are called on the non-fitted estimator. by [Sebastian Raschka](https://sebastianraschka.com/).
  - Fix bug where <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> would perform poorly if the `random_state` was fixed (`7411`). By [Joel Nothman](https://joelnothman.com/).
  - Fix bug in ensembles with randomization where the ensemble would not set `random_state` on base estimators in a pipeline or similar nesting. (`7411`). Note, results for <span class="title-ref">ensemble.BaggingClassifier</span> <span class="title-ref">ensemble.BaggingRegressor</span>, <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> will now differ from previous versions. By [Joel Nothman](https://joelnothman.com/).

Linear, kernelized and related models

  - Fixed incorrect gradient computation for `loss='squared_epsilon_insensitive'` in <span class="title-ref">linear\_model.SGDClassifier</span> and <span class="title-ref">linear\_model.SGDRegressor</span> (`6764`). By `Wenhua Yang <geekoala>`.
  - Fix bug in <span class="title-ref">linear\_model.LogisticRegressionCV</span> where `solver='liblinear'` did not accept `class_weights='balanced`. (`6817`). By [Tom Dupre la Tour](https://github.com/TomDLT).
  - Fix bug in <span class="title-ref">neighbors.RadiusNeighborsClassifier</span> where an error occurred when there were outliers being labelled and a weight function specified (`6902`). By [LeonieBorne](https://github.com/LeonieBorne).
  - Fix <span class="title-ref">linear\_model.ElasticNet</span> sparse decision function to match output with dense in the multioutput case.

Decomposition, manifold learning and clustering

  - <span class="title-ref">decomposition.RandomizedPCA</span> default number of <span class="title-ref">iterated\_power</span> is 4 instead of 3. `5141` by `Giorgio Patrini <giorgiop>`.
  - <span class="title-ref">utils.extmath.randomized\_svd</span> performs 4 power iterations by default, instead or 0. In practice this is enough for obtaining a good approximation of the true eigenvalues/vectors in the presence of noise. When <span class="title-ref">n\_components</span> is small (`< .1 * min(X.shape)`) <span class="title-ref">n\_iter</span> is set to 7, unless the user specifies a higher number. This improves precision with few components. `5299` by `Giorgio Patrini<giorgiop>`.
  - Whiten/non-whiten inconsistency between components of <span class="title-ref">decomposition.PCA</span> and <span class="title-ref">decomposition.RandomizedPCA</span> (now factored into PCA, see the New features) is fixed. <span class="title-ref">components\_</span> are stored with no whitening. `5299` by `Giorgio Patrini <giorgiop>`.
  - Fixed bug in <span class="title-ref">manifold.spectral\_embedding</span> where diagonal of unnormalized Laplacian matrix was incorrectly set to 1. `4995` by `Peter Fischer <yanlend>`.
  - Fixed incorrect initialization of <span class="title-ref">utils.arpack.eigsh</span> on all occurrences. Affects <span class="title-ref">cluster.bicluster.SpectralBiclustering</span>, <span class="title-ref">decomposition.KernelPCA</span>, <span class="title-ref">manifold.LocallyLinearEmbedding</span>, and <span class="title-ref">manifold.SpectralEmbedding</span> (`5012`). By `Peter Fischer <yanlend>`.
  - Attribute `explained_variance_ratio_` calculated with the SVD solver of <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now returns correct results. By `JPFrancoia <JPFrancoia>`

Preprocessing and feature selection

  - <span class="title-ref">preprocessing.data.\_transform\_selected</span> now always passes a copy of `X` to transform function when `copy=True` (`7194`). By [Caio Oliveira](https://github.com/caioaao).

Model evaluation and meta-estimators

  - <span class="title-ref">model\_selection.StratifiedKFold</span> now raises error if all n\_labels for individual classes is less than n\_folds. `6182` by `Devashish Deshpande <dsquareindia>`.
  - Fixed bug in <span class="title-ref">model\_selection.StratifiedShuffleSplit</span> where train and test sample could overlap in some edge cases, see `6121` for more details. By [Loic Esteve](https://github.com/lesteve).
  - Fix in <span class="title-ref">sklearn.model\_selection.StratifiedShuffleSplit</span> to return splits of size `train_size` and `test_size` in all cases (`6472`). By [Andreas Müller](https://amueller.github.io/).
  - Cross-validation of <span class="title-ref">multiclass.OneVsOneClassifier</span> and <span class="title-ref">multiclass.OneVsRestClassifier</span> now works with precomputed kernels. `7350` by `Russell Smith <rsmith54>`.
  - Fix incomplete `predict_proba` method delegation from <span class="title-ref">model\_selection.GridSearchCV</span> to <span class="title-ref">linear\_model.SGDClassifier</span> (`7159`) by [Yichuan Liu](https://github.com/yl565).

Metrics

  - Fix bug in <span class="title-ref">metrics.silhouette\_score</span> in which clusters of size 1 were incorrectly scored. They should get a score of 0. By [Joel Nothman](https://joelnothman.com/).
  - Fix bug in <span class="title-ref">metrics.silhouette\_samples</span> so that it now works with arbitrary labels, not just those ranging from 0 to n\_clusters - 1.
  - Fix bug where expected and adjusted mutual information were incorrect if cluster contingency cells exceeded `2**16`. By [Joel Nothman](https://joelnothman.com/).
  - <span class="title-ref">metrics.pairwise\_distances</span> now converts arrays to boolean arrays when required in `scipy.spatial.distance`. `5460` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - Fix sparse input support in <span class="title-ref">metrics.silhouette\_score</span> as well as example examples/text/document\_clustering.py. By `YenChen Lin <yenchenlin>`.
  - <span class="title-ref">metrics.roc\_curve</span> and <span class="title-ref">metrics.precision\_recall\_curve</span> no longer round `y_score` values when creating ROC curves; this was causing problems for users with very small differences in scores (`7353`).

Miscellaneous

  - <span class="title-ref">model\_selection.tests.\_search.\_check\_param\_grid</span> now works correctly with all types that extends/implements <span class="title-ref">Sequence</span> (except string), including range (Python 3.x) and xrange (Python 2.x). `7323` by Viacheslav Kovalevskyi.
  - <span class="title-ref">utils.extmath.randomized\_range\_finder</span> is more numerically stable when many power iterations are requested, since it applies LU normalization by default. If `n_iter<2` numerical issues are unlikely, thus no normalization is applied. Other normalization options are available: `'none', 'LU'` and `'QR'`. `5141` by `Giorgio Patrini <giorgiop>`.
  - Fix a bug where some formats of `scipy.sparse` matrix, and estimators with them as parameters, could not be passed to <span class="title-ref">base.clone</span>. By [Loic Esteve](https://github.com/lesteve).
  - <span class="title-ref">datasets.load\_svmlight\_file</span> now is able to read long int QID values. `7101` by `Ibraim Ganiev <olologin>`.

### API changes summary

Linear, kernelized and related models

  - `residual_metric` has been deprecated in <span class="title-ref">linear\_model.RANSACRegressor</span>. Use `loss` instead. By [Manoj Kumar](https://manojbits.wordpress.com).
  - Access to public attributes `.X_` and `.y_` has been deprecated in <span class="title-ref">isotonic.IsotonicRegression</span>. By `Jonathan Arfa <jarfa>`.

Decomposition, manifold learning and clustering

  - The old <span class="title-ref">mixture.DPGMM</span> is deprecated in favor of the new <span class="title-ref">mixture.BayesianGaussianMixture</span> (with the parameter `weight_concentration_prior_type='dirichlet_process'`). The new class solves the computational problems of the old class and computes the Gaussian mixture with a Dirichlet process prior faster than before. `7295` by `Wei Xue <xuewei4d>` and `Thierry Guillemot <tguillemot>`.
  - The old <span class="title-ref">mixture.VBGMM</span> is deprecated in favor of the new <span class="title-ref">mixture.BayesianGaussianMixture</span> (with the parameter `weight_concentration_prior_type='dirichlet_distribution'`). The new class solves the computational problems of the old class and computes the Variational Bayesian Gaussian mixture faster than before. `6651` by `Wei Xue <xuewei4d>` and `Thierry Guillemot <tguillemot>`.
  - The old <span class="title-ref">mixture.GMM</span> is deprecated in favor of the new <span class="title-ref">mixture.GaussianMixture</span>. The new class computes the Gaussian mixture faster than before and some of computational problems have been solved. `6666` by `Wei Xue <xuewei4d>` and `Thierry Guillemot <tguillemot>`.

Model evaluation and meta-estimators

  - The <span class="title-ref">sklearn.cross\_validation</span>, <span class="title-ref">sklearn.grid\_search</span> and <span class="title-ref">sklearn.learning\_curve</span> have been deprecated and the classes and functions have been reorganized into the `sklearn.model_selection` module. Ref \[model\_selection\_changes\](\#model\_selection\_changes) for more information. `4294` by [Raghav RV](https://github.com/raghavrv).
  - The `grid_scores_` attribute of <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> is deprecated in favor of the attribute `cv_results_`. Ref \[model\_selection\_changes\](\#model\_selection\_changes) for more information. `6697` by [Raghav RV](https://github.com/raghavrv).
  - The parameters `n_iter` or `n_folds` in old CV splitters are replaced by the new parameter `n_splits` since it can provide a consistent and unambiguous interface to represent the number of train-test splits. `7187` by `YenChen Lin <yenchenlin>`.
  - `classes` parameter was renamed to `labels` in <span class="title-ref">metrics.hamming\_loss</span>. `7260` by `Sebastián Vanrell <srvanrell>`.
  - The splitter classes `LabelKFold`, `LabelShuffleSplit`, `LeaveOneLabelOut` and `LeavePLabelsOut` are renamed to <span class="title-ref">model\_selection.GroupKFold</span>, <span class="title-ref">model\_selection.GroupShuffleSplit</span>, <span class="title-ref">model\_selection.LeaveOneGroupOut</span> and <span class="title-ref">model\_selection.LeavePGroupsOut</span> respectively. Also the parameter `labels` in the <span class="title-ref">split</span> method of the newly renamed splitters <span class="title-ref">model\_selection.LeaveOneGroupOut</span> and <span class="title-ref">model\_selection.LeavePGroupsOut</span> is renamed to `groups`. Additionally in <span class="title-ref">model\_selection.LeavePGroupsOut</span>, the parameter `n_labels` is renamed to `n_groups`. `6660` by [Raghav RV](https://github.com/raghavrv).
  - Error and loss names for `scoring` parameters are now prefixed by `'neg_'`, such as `neg_mean_squared_error`. The unprefixed versions are deprecated and will be removed in version 0.20. `7261` by `Tim Head <betatim>`.

### Code Contributors

Aditya Joshi, Alejandro, Alexander Fabisch, Alexander Loginov, Alexander Minyushkin, Alexander Rudy, Alexandre Abadie, Alexandre Abraham, Alexandre Gramfort, Alexandre Saint, alexfields, Alvaro Ulloa, alyssaq, Amlan Kar, Andreas Mueller, andrew giessel, Andrew Jackson, Andrew McCulloh, Andrew Murray, Anish Shah, Arafat, Archit Sharma, Ariel Rokem, Arnaud Joly, Arnaud Rachez, Arthur Mensch, Ash Hoover, asnt, b0noI, Behzad Tabibian, Bernardo, Bernhard Kratzwald, Bhargav Mangipudi, blakeflei, Boyuan Deng, Brandon Carter, Brett Naul, Brian McFee, Caio Oliveira, Camilo Lamus, Carol Willing, Cass, CeShine Lee, Charles Truong, Chyi-Kwei Yau, CJ Carey, codevig, Colin Ni, Dan Shiebler, Daniel, Daniel Hnyk, David Ellis, David Nicholson, David Staub, David Thaler, David Warshaw, Davide Lasagna, Deborah, definitelyuncertain, Didi Bar-Zev, djipey, dsquareindia, edwinENSAE, Elias Kuthe, Elvis DOHMATOB, Ethan White, Fabian Pedregosa, Fabio Ticconi, fisache, Florian Wilhelm, Francis, Francis O'Donovan, Gael Varoquaux, Ganiev Ibraim, ghg, Gilles Louppe, Giorgio Patrini, Giovanni Cherubin, Giovanni Lanzani, Glenn Qian, Gordon Mohr, govin-vatsan, Graham Clenaghan, Greg Reda, Greg Stupp, Guillaume Lemaitre, Gustav Mörtberg, halwai, Harizo Rajaona, Harry Mavroforakis, hashcode55, hdmetor, Henry Lin, Hobson Lane, Hugo Bowne-Anderson, Igor Andriushchenko, Imaculate, Inki Hwang, Isaac Sijaranamual, Ishank Gulati, Issam Laradji, Iver Jordal, jackmartin, Jacob Schreiber, Jake Vanderplas, James Fiedler, James Routley, Jan Zikes, Janna Brettingen, jarfa, Jason Laska, jblackburne, jeff levesque, Jeffrey Blackburne, Jeffrey04, Jeremy Hintz, jeremynixon, Jeroen, Jessica Yung, Jill-Jênn Vie, Jimmy Jia, Jiyuan Qian, Joel Nothman, johannah, John, John Boersma, John Kirkham, John Moeller, jonathan.striebel, joncrall, Jordi, Joseph Munoz, Joshua Cook, JPFrancoia, jrfiedler, JulianKahnert, juliathebrave, kaichogami, KamalakerDadi, Kenneth Lyons, Kevin Wang, kingjr, kjell, Konstantin Podshumok, Kornel Kielczewski, Krishna Kalyan, krishnakalyan3, Kvle Putnam, Kyle Jackson, Lars Buitinck, ldavid, LeiG, LeightonZhang, Leland McInnes, Liang-Chi Hsieh, Lilian Besson, lizsz, Loic Esteve, Louis Tiao, Léonie Borne, Mads Jensen, Maniteja Nandana, Manoj Kumar, Manvendra Singh, Marco, Mario Krell, Mark Bao, Mark Szepieniec, Martin Madsen, MartinBpr, MaryanMorel, Massil, Matheus, Mathieu Blondel, Mathieu Dubois, Matteo, Matthias Ekman, Max Moroz, Michael Scherer, michiaki ariga, Mikhail Korobov, Moussa Taifi, mrandrewandrade, Mridul Seth, nadya-p, Naoya Kanai, Nate George, Nelle Varoquaux, Nelson Liu, Nick James, NickleDave, Nico, Nicolas Goix, Nikolay Mayorov, ningchi, nlathia, okbalefthanded, Okhlopkov, Olivier Grisel, Panos Louridas, Paul Strickland, Perrine Letellier, pestrickland, Peter Fischer, Pieter, Ping-Yao, Chang, practicalswift, Preston Parry, Qimu Zheng, Rachit Kansal, Raghav RV, Ralf Gommers, Ramana.S, Rammig, Randy Olson, Rob Alexander, Robert Lutz, Robin Schucker, Rohan Jain, Ruifeng Zheng, Ryan Yu, Rémy Léone, saihttam, Saiwing Yeung, Sam Shleifer, Samuel St-Jean, Sartaj Singh, Sasank Chilamkurthy, saurabh.bansod, Scott Andrews, Scott Lowe, seales, Sebastian Raschka, Sebastian Saeger, Sebastián Vanrell, Sergei Lebedev, shagun Sodhani, shanmuga cv, Shashank Shekhar, shawpan, shengxiduan, Shota, shuckle16, Skipper Seabold, sklearn-ci, SmedbergM, srvanrell, Sébastien Lerique, Taranjeet, themrmax, Thierry, Thierry Guillemot, Thomas, Thomas Hallock, Thomas Moreau, Tim Head, tKammy, toastedcornflakes, Tom, TomDLT, Toshihiro Kamishima, tracer0tong, Trent Hauck, trevorstephens, Tue Vo, Varun, Varun Jewalikar, Viacheslav, Vighnesh Birodkar, Vikram, Villu Ruusmann, Vinayak Mehta, walter, waterponey, Wenhua Yang, Wenjian Huang, Will Welch, wyseguy7, xyguo, yanlend, Yaroslav Halchenko, yelite, Yen, YenChenLin, Yichuan Liu, Yoav Ram, Yoshiki, Zheng RuiFeng, zivori, Óscar Nájera

---

v0.19.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.19

## Version 0.19.2

**July, 2018**

This release is exclusively in order to support Python 3.7.

### Related changes

  - `n_iter_` may vary from previous releases in <span class="title-ref">linear\_model.LogisticRegression</span> with `solver='lbfgs'` and <span class="title-ref">linear\_model.HuberRegressor</span>. For Scipy \<= 1.0.0, the optimizer could perform more than the requested maximum number of iterations. Now both estimators will report at most `max_iter` iterations even if more were performed. `10723` by [Joel Nothman](https://joelnothman.com/).

## Version 0.19.1

**October 23, 2017**

This is a bug-fix release with some minor documentation improvements and enhancements to features released in 0.19.0.

Note there may be minor differences in TSNE output in this release (due to `9623`), in the case where multiple samples have equal distance to some sample.

### Changelog

#### API changes

  - Reverted the addition of `metrics.ndcg_score` and `metrics.dcg_score` which had been merged into version 0.19.0 by error. The implementations were broken and undocumented.
  - `return_train_score` which was added to <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span> and <span class="title-ref">model\_selection.cross\_validate</span> in version 0.19.0 will be changing its default value from True to False in version 0.21. We found that calculating training score could have a great effect on cross validation runtime in some cases. Users should explicitly set `return_train_score` to False if prediction or scoring functions are slow, resulting in a deleterious effect on CV runtime, or to True if they wish to use the calculated scores. `9677` by `Kumar Ashutosh <thechargedneutron>` and [Joel Nothman](https://joelnothman.com/).
  - `correlation_models` and `regression_models` from the legacy gaussian processes implementation have been belatedly deprecated. `9717` by `Kumar Ashutosh <thechargedneutron>`.

#### Bug fixes

  - Avoid integer overflows in <span class="title-ref">metrics.matthews\_corrcoef</span>. `9693` by `Sam Steingold <sam-s>`.
  - Fixed a bug in the objective function for <span class="title-ref">manifold.TSNE</span> (both exact and with the Barnes-Hut approximation) when `n_components >= 3`. `9711` by `goncalo-rodrigues`.
  - Fix regression in <span class="title-ref">model\_selection.cross\_val\_predict</span> where it raised an error with `method='predict_proba'` for some probabilistic classifiers. `9641` by `James Bourbeau <jrbourbeau>`.
  - Fixed a bug where <span class="title-ref">datasets.make\_classification</span> modified its input `weights`. `9865` by `Sachin Kelkar <s4chin>`.
  - <span class="title-ref">model\_selection.StratifiedShuffleSplit</span> now works with multioutput multiclass or multilabel data with more than 1000 columns. `9922` by `Charlie Brummitt <crbrummitt>`.
  - Fixed a bug with nested and conditional parameter setting, e.g. setting a pipeline step and its parameter at the same time. `9945` by [Andreas Müller](https://amueller.github.io/) and [Joel Nothman](https://joelnothman.com/).

Regressions in 0.19.0 fixed in 0.19.1:

  - Fixed a bug where parallelised prediction in random forests was not thread-safe and could (rarely) result in arbitrary errors. `9830` by [Joel Nothman](https://joelnothman.com/).
  - Fix regression in <span class="title-ref">model\_selection.cross\_val\_predict</span> where it no longer accepted `X` as a list. `9600` by `Rasul Kerimov
    <CoderINusE>`.
  - Fixed handling of <span class="title-ref">model\_selection.cross\_val\_predict</span> for binary classification with `method='decision_function'`. `9593` by `Reiichiro Nakano <reiinakano>` and core devs.
  - Fix regression in <span class="title-ref">pipeline.Pipeline</span> where it no longer accepted `steps` as a tuple. `9604` by `Joris Van den Bossche
    <jorisvandenbossche>`.
  - Fix bug where `n_iter` was not properly deprecated, leaving `n_iter` unavailable for interim use in <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> and <span class="title-ref">linear\_model.Perceptron</span>. `9558` by [Andreas Müller](https://amueller.github.io/).
  - Dataset fetchers make sure temporary files are closed before removing them, which caused errors on Windows. `9847` by `Joan Massich <massich>`.
  - Fixed a regression in <span class="title-ref">manifold.TSNE</span> where it no longer supported metrics other than 'euclidean' and 'precomputed'. `9623` by `Oli
    Blum <oliblum90>`.

#### Enhancements

  - Our test suite and <span class="title-ref">utils.estimator\_checks.check\_estimator</span> can now be run without Nose installed. `9697` by `Joan Massich <massich>`.
  - To improve usability of version 0.19's <span class="title-ref">pipeline.Pipeline</span> caching, `memory` now allows `joblib.Memory` instances. This make use of the new <span class="title-ref">utils.validation.check\_memory</span> helper. issue:<span class="title-ref">9584</span> by `Kumar Ashutosh <thechargedneutron>`
  - Some fixes to examples: `9750`, `9788`, `9815`
  - Made a FutureWarning in SGD-based estimators less verbose. `9802` by `Vrishank Bhardwaj <vrishank97>`.

### Code and Documentation Contributors

With thanks to:

Joel Nothman, Loic Esteve, Andreas Mueller, Kumar Ashutosh, Vrishank Bhardwaj, Hanmin Qin, Rasul Kerimov, James Bourbeau, Nagarjuna Kumar, Nathaniel Saul, Olivier Grisel, Roman Yurchak, Reiichiro Nakano, Sachin Kelkar, Sam Steingold, Yaroslav Halchenko, diegodlh, felix, goncalo-rodrigues, jkleint, oliblum90, pasbi, Anthony Gitter, Ben Lawson, Charlie Brummitt, Didi Bar-Zev, Gael Varoquaux, Joan Massich, Joris Van den Bossche, nielsenmarkus11

## Version 0.19

**August 12, 2017**

### Highlights

We are excited to release a number of great new features including <span class="title-ref">neighbors.LocalOutlierFactor</span> for anomaly detection, <span class="title-ref">preprocessing.QuantileTransformer</span> for robust feature transformation, and the <span class="title-ref">multioutput.ClassifierChain</span> meta-estimator to simply account for dependencies between classes in multilabel problems. We have some new algorithms in existing estimators, such as multiplicative update in <span class="title-ref">decomposition.NMF</span> and multinomial <span class="title-ref">linear\_model.LogisticRegression</span> with L1 loss (use `solver='saga'`).

Cross validation is now able to return the results from multiple metric evaluations. The new <span class="title-ref">model\_selection.cross\_validate</span> can return many scores on the test data as well as training set performance and timings, and we have extended the `scoring` and `refit` parameters for grid/randomized search \[to handle multiple metrics \<multimetric\_grid\_search\>\](\#to-handle-multiple-metrics-\<multimetric\_grid\_search\>).

You can also learn faster. For instance, the \[new option to cache transformations \<pipeline\_cache\>\](\#new-option-to-cache transformations-\<pipeline\_cache\>) in <span class="title-ref">pipeline.Pipeline</span> makes grid search over pipelines including slow transformations much more efficient. And you can predict faster: if you're sure you know what you're doing, you can turn off validating that the input is finite using <span class="title-ref">config\_context</span>.

We've made some important fixes too. We've fixed a longstanding implementation error in <span class="title-ref">metrics.average\_precision\_score</span>, so please be cautious with prior results reported from that function. A number of errors in the <span class="title-ref">manifold.TSNE</span> implementation have been fixed, particularly in the default Barnes-Hut approximation. <span class="title-ref">semi\_supervised.LabelSpreading</span> and <span class="title-ref">semi\_supervised.LabelPropagation</span> have had substantial fixes. LabelPropagation was previously broken. LabelSpreading should now correctly respect its alpha parameter.

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - <span class="title-ref">cluster.KMeans</span> with sparse X and initial centroids given (bug fix)
  - <span class="title-ref">cross\_decomposition.PLSRegression</span> with `scale=True` (bug fix)
  - <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> where `min_impurity_split` is used (bug fix)
  - gradient boosting `loss='quantile'` (bug fix)
  - <span class="title-ref">ensemble.IsolationForest</span> (bug fix)
  - <span class="title-ref">feature\_selection.SelectFdr</span> (bug fix)
  - <span class="title-ref">linear\_model.RANSACRegressor</span> (bug fix)
  - <span class="title-ref">linear\_model.LassoLars</span> (bug fix)
  - <span class="title-ref">linear\_model.LassoLarsIC</span> (bug fix)
  - <span class="title-ref">manifold.TSNE</span> (bug fix)
  - <span class="title-ref">neighbors.NearestCentroid</span> (bug fix)
  - <span class="title-ref">semi\_supervised.LabelSpreading</span> (bug fix)
  - <span class="title-ref">semi\_supervised.LabelPropagation</span> (bug fix)
  - tree based models where `min_weight_fraction_leaf` is used (enhancement)
  - <span class="title-ref">model\_selection.StratifiedKFold</span> with `shuffle=True` (this change, due to `7823` was not mentioned in the release notes at the time)

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Changelog

#### New features

Classifiers and regressors

  - Added <span class="title-ref">multioutput.ClassifierChain</span> for multi-label classification. By `Adam Kleczewski <adamklec>`.
  - Added solver `'saga'` that implements the improved version of Stochastic Average Gradient, in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.Ridge</span>. It allows the use of L1 penalty with multinomial logistic loss, and behaves marginally better than 'sag' during the first epochs of ridge and logistic regression. `8446` by [Arthur Mensch](https://amensch.fr).

Other estimators

  - Added the <span class="title-ref">neighbors.LocalOutlierFactor</span> class for anomaly detection based on nearest neighbors. `5279` by [Nicolas Goix](https://ngoix.github.io/) and [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Added <span class="title-ref">preprocessing.QuantileTransformer</span> class and <span class="title-ref">preprocessing.quantile\_transform</span> function for features normalization based on quantiles. `8363` by `Denis Engemann <dengemann>`, `Guillaume Lemaitre <glemaitre>`, [Olivier Grisel](https://twitter.com/ogrisel), [Raghav RV](https://github.com/raghavrv), `Thierry Guillemot <tguillemot>`, and [Gael Varoquaux](http://gael-varoquaux.info).
  - The new solver `'mu'` implements a Multiplicate Update in <span class="title-ref">decomposition.NMF</span>, allowing the optimization of all beta-divergences, including the Frobenius norm, the generalized Kullback-Leibler divergence and the Itakura-Saito divergence. `5295` by [Tom Dupre la Tour](https://github.com/TomDLT).

Model selection and evaluation

  - <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> now support simultaneous evaluation of multiple metrics. Refer to the \[multimetric\_grid\_search\](\#multimetric\_grid\_search) section of the user guide for more information. `7388` by [Raghav RV](https://github.com/raghavrv)
  - Added the <span class="title-ref">model\_selection.cross\_validate</span> which allows evaluation of multiple metrics. This function returns a dict with more useful information from cross-validation such as the train scores, fit times and score times. Refer to \[multimetric\_cross\_validation\](\#multimetric\_cross\_validation) section of the userguide for more information. `7388` by [Raghav RV](https://github.com/raghavrv)
  - Added <span class="title-ref">metrics.mean\_squared\_log\_error</span>, which computes the mean square error of the logarithmic transformation of targets, particularly useful for targets with an exponential trend. `7655` by `Karan Desai <karandesai-96>`.
  - Added <span class="title-ref">metrics.dcg\_score</span> and <span class="title-ref">metrics.ndcg\_score</span>, which compute Discounted cumulative gain (DCG) and Normalized discounted cumulative gain (NDCG). `7739` by `David Gasquez <davidgasquez>`.
  - Added the <span class="title-ref">model\_selection.RepeatedKFold</span> and <span class="title-ref">model\_selection.RepeatedStratifiedKFold</span>. `8120` by [Neeraj Gangwar](http://neerajgangwar.in).

Miscellaneous

  - Validation that input data contains no NaN or inf can now be suppressed using <span class="title-ref">config\_context</span>, at your own risk. This will save on runtime, and may be particularly useful for prediction time. `7548` by [Joel Nothman](https://joelnothman.com/).
  - Added a test to ensure parameter listing in docstrings match the function/class signature. `9206` by [Alexandre Gramfort](http://alexandre.gramfort.net) and [Raghav RV](https://github.com/raghavrv).

#### Enhancements

Trees and ensembles

  - The `min_weight_fraction_leaf` constraint in tree construction is now more efficient, taking a fast path to declare a node a leaf if its weight is less than 2 \* the minimum. Note that the constructed tree will be different from previous versions where `min_weight_fraction_leaf` is used. `7441` by `Nelson Liu <nelson-liu>`.
  - <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> now support sparse input for prediction. `6101` by `Ibraim Ganiev <olologin>`.
  - <span class="title-ref">ensemble.VotingClassifier</span> now allows changing estimators by using <span class="title-ref">ensemble.VotingClassifier.set\_params</span>. An estimator can also be removed by setting it to `None`. `7674` by `Yichuan Liu <yl565>`.
  - <span class="title-ref">tree.export\_graphviz</span> now shows configurable number of decimal places. `8698` by `Guillaume Lemaitre <glemaitre>`.
  - Added `flatten_transform` parameter to <span class="title-ref">ensemble.VotingClassifier</span> to change output shape of <span class="title-ref">transform</span> method to 2 dimensional. `7794` by `Ibraim Ganiev <olologin>` and `Herilalaina Rakotoarison <herilalaina>`.

Linear, kernelized and related models

  - <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> and <span class="title-ref">linear\_model.Perceptron</span> now expose `max_iter` and `tol` parameters, to handle convergence more precisely. `n_iter` parameter is deprecated, and the fitted estimator exposes a `n_iter_` attribute, with actual number of iterations before convergence. `5036` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - Added `average` parameter to perform weight averaging in <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>. `4939` by `Andrea Esuli <aesuli>`.
  - <span class="title-ref">linear\_model.RANSACRegressor</span> no longer throws an error when calling `fit` if no inliers are found in its first iteration. Furthermore, causes of skipped iterations are tracked in newly added attributes, `n_skips_*`. `7914` by `Michael Horrell <mthorrell>`.
  - In <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>, method `predict` is a lot faster with `return_std=True`. `8591` by `Hadrien Bertrand <hbertrand>`.
  - Added `return_std` to `predict` method of <span class="title-ref">linear\_model.ARDRegression</span> and <span class="title-ref">linear\_model.BayesianRidge</span>. `7838` by `Sergey Feldman <sergeyf>`.
  - Memory usage enhancements: Prevent cast from float32 to float64 in: <span class="title-ref">linear\_model.MultiTaskElasticNet</span>; <span class="title-ref">linear\_model.LogisticRegression</span> when using newton-cg solver; and <span class="title-ref">linear\_model.Ridge</span> when using svd, sparse\_cg, cholesky or lsqr solvers. `8835`, `8061` by `Joan Massich <massich>` and `Nicolas
    Cordier <ncordier>` and `Thierry Guillemot <tguillemot>`.

Other predictors

  - Custom metrics for the `sklearn.neighbors` binary trees now have fewer constraints: they must take two 1d-arrays and return a float. `6288` by [Jake Vanderplas](https://staff.washington.edu/jakevdp/).
  - `algorithm='auto` in `sklearn.neighbors` estimators now chooses the most appropriate algorithm for all input types and metrics. `9145` by `Herilalaina Rakotoarison <herilalaina>` and `Reddy Chinthala
    <preddy5>`.

Decomposition, manifold learning and clustering

  - <span class="title-ref">cluster.MiniBatchKMeans</span> and <span class="title-ref">cluster.KMeans</span> now use significantly less memory when assigning data points to their nearest cluster center. `7721` by `Jon Crall <Erotemic>`.
  - <span class="title-ref">decomposition.PCA</span>, <span class="title-ref">decomposition.IncrementalPCA</span> and <span class="title-ref">decomposition.TruncatedSVD</span> now expose the singular values from the underlying SVD. They are stored in the attribute `singular_values_`, like in <span class="title-ref">decomposition.IncrementalPCA</span>. `7685` by `Tommy Löfstedt <tomlof>`
  - <span class="title-ref">decomposition.NMF</span> now faster when `beta_loss=0`. `9277` by `hongkahjun`.
  - Memory improvements for method `barnes_hut` in <span class="title-ref">manifold.TSNE</span> `7089` by `Thomas Moreau <tomMoral>` and [Olivier Grisel](https://twitter.com/ogrisel).
  - Optimization schedule improvements for Barnes-Hut <span class="title-ref">manifold.TSNE</span> so the results are closer to the one from the reference implementation [lvdmaaten/bhtsne](https://github.com/lvdmaaten/bhtsne) by `Thomas
    Moreau <tomMoral>` and [Olivier Grisel](https://twitter.com/ogrisel).
  - Memory usage enhancements: Prevent cast from float32 to float64 in <span class="title-ref">decomposition.PCA</span> and <span class="title-ref">decomposition.randomized\_svd\_low\_rank</span>. `9067` by [Raghav RV](https://github.com/raghavrv).

Preprocessing and feature selection

  - Added `norm_order` parameter to <span class="title-ref">feature\_selection.SelectFromModel</span> to enable selection of the norm order when `coef_` is more than 1D. `6181` by `Antoine Wendlinger <antoinewdg>`.
  - Added ability to use sparse matrices in <span class="title-ref">feature\_selection.f\_regression</span> with `center=True`. `8065` by `Daniel LeJeune <acadiansith>`.
  - Small performance improvement to n-gram creation in `sklearn.feature_extraction.text` by binding methods for loops and special-casing unigrams. `7567` by `Jaye Doepke <jtdoepke>`
  - Relax assumption on the data for the <span class="title-ref">kernel\_approximation.SkewedChi2Sampler</span>. Since the Skewed-Chi2 kernel is defined on the open interval \((-skewedness; +\infty)^d\), the transform function should not check whether `X < 0` but whether `X < -self.skewedness`. `7573` by `Romain Brault <RomainBrault>`.
  - Made default kernel parameters kernel-dependent in <span class="title-ref">kernel\_approximation.Nystroem</span>. `5229` by `Saurabh Bansod <mth4saurabh>` and [Andreas Müller](https://amueller.github.io/).

Model evaluation and meta-estimators

  - <span class="title-ref">pipeline.Pipeline</span> is now able to cache transformers within a pipeline by using the `memory` constructor parameter. `7990` by `Guillaume Lemaitre <glemaitre>`.
  - <span class="title-ref">pipeline.Pipeline</span> steps can now be accessed as attributes of its `named_steps` attribute. `8586` by `Herilalaina
    Rakotoarison <herilalaina>`.
  - Added `sample_weight` parameter to <span class="title-ref">pipeline.Pipeline.score</span>. `7723` by `Mikhail Korobov <kmike>`.
  - Added ability to set `n_jobs` parameter to <span class="title-ref">pipeline.make\_union</span>. A `TypeError` will be raised for any other kwargs. `8028` by `Alexander Booth <alexandercbooth>`.
  - <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span> and <span class="title-ref">model\_selection.cross\_val\_score</span> now allow estimators with callable kernels which were previously prohibited. `8005` by [Andreas Müller](https://amueller.github.io/) .
  - <span class="title-ref">model\_selection.cross\_val\_predict</span> now returns output of the correct shape for all values of the argument `method`. `7863` by `Aman Dalmia <dalmia>`.
  - Added `shuffle` and `random_state` parameters to shuffle training data before taking prefixes of it based on training sizes in <span class="title-ref">model\_selection.learning\_curve</span>. `7506` by `Narine Kokhlikyan <NarineK>`.
  - <span class="title-ref">model\_selection.StratifiedShuffleSplit</span> now works with multioutput multiclass (or multilabel) data. `9044` by [Vlad Niculae](https://vene.ro/).
  - Speed improvements to <span class="title-ref">model\_selection.StratifiedShuffleSplit</span>. `5991` by `Arthur Mensch <arthurmensch>` and [Joel Nothman](https://joelnothman.com/).
  - Add `shuffle` parameter to <span class="title-ref">model\_selection.train\_test\_split</span>. `8845` by `themrmax <themrmax>`
  - <span class="title-ref">multioutput.MultiOutputRegressor</span> and <span class="title-ref">multioutput.MultiOutputClassifier</span> now support online learning using `partial_fit`. :issue: <span class="title-ref">8053</span> by `Peng Yu <yupbank>`.
  - Add `max_train_size` parameter to <span class="title-ref">model\_selection.TimeSeriesSplit</span> `8282` by `Aman Dalmia <dalmia>`.
  - More clustering metrics are now available through <span class="title-ref">metrics.get\_scorer</span> and `scoring` parameters. `8117` by [Raghav RV](https://github.com/raghavrv).
  - A scorer based on <span class="title-ref">metrics.explained\_variance\_score</span> is also available. `9259` by `Hanmin Qin <qinhanmin2014>`.

Metrics

  - <span class="title-ref">metrics.matthews\_corrcoef</span> now support multiclass classification. `8094` by `Jon Crall <Erotemic>`.
  - Add `sample_weight` parameter to <span class="title-ref">metrics.cohen\_kappa\_score</span>. `8335` by `Victor Poughon <vpoughon>`.

Miscellaneous

  - <span class="title-ref">utils.estimator\_checks.check\_estimator</span> now attempts to ensure that methods transform, predict, etc. do not set attributes on the estimator. `7533` by `Ekaterina Krivich <kiote>`.
  - Added type checking to the `accept_sparse` parameter in `sklearn.utils.validation` methods. This parameter now accepts only boolean, string, or list/tuple of strings. `accept_sparse=None` is deprecated and should be replaced by `accept_sparse=False`. `7880` by `Josh Karnofsky <jkarno>`.
  - Make it possible to load a chunk of an svmlight formatted file by passing a range of bytes to <span class="title-ref">datasets.load\_svmlight\_file</span>. `935` by `Olivier Grisel <ogrisel>`.
  - <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span> now accept non-finite features. `8931` by `Attractadore`.

#### Bug fixes

Trees and ensembles

  - Fixed a memory leak in trees when using trees with `criterion='mae'`. `8002` by [Raghav RV](https://github.com/raghavrv).
  - Fixed a bug where <span class="title-ref">ensemble.IsolationForest</span> uses an an incorrect formula for the average path length `8549` by [Peter Wang](https://github.com/PTRWang).
  - Fixed a bug where <span class="title-ref">ensemble.AdaBoostClassifier</span> throws `ZeroDivisionError` while fitting data with single class labels. `7501` by `Dominik Krzeminski <dokato>`.
  - Fixed a bug in <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> where a float being compared to `0.0` using `==` caused a divide by zero error. `7970` by `He Chen <chenhe95>`.
  - Fix a bug where <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> ignored the `min_impurity_split` parameter. `8006` by `Sebastian Pölsterl <sebp>`.
  - Fixed `oob_score` in <span class="title-ref">ensemble.BaggingClassifier</span>. `8936` by `Michael Lewis <mlewis1729>`
  - Fixed excessive memory usage in prediction for random forests estimators. `8672` by `Mike Benfield <mikebenfield>`.
  - Fixed a bug where `sample_weight` as a list broke random forests in Python 2 `8068` by `xor`.
  - Fixed a bug where <span class="title-ref">ensemble.IsolationForest</span> fails when `max_features` is less than 1. `5732` by `Ishank Gulati <IshankGulati>`.
  - Fix a bug where gradient boosting with `loss='quantile'` computed negative errors for negative values of `ytrue - ypred` leading to wrong values when calling `__call__`. `8087` by `Alexis Mignon <AlexisMignon>`
  - Fix a bug where <span class="title-ref">ensemble.VotingClassifier</span> raises an error when a numpy array is passed in for weights. `7983` by `Vincent Pham <vincentpham1991>`.
  - Fixed a bug where <span class="title-ref">tree.export\_graphviz</span> raised an error when the length of features\_names does not match n\_features in the decision tree. `8512` by `Li Li <aikinogard>`.

Linear, kernelized and related models

  - Fixed a bug where <span class="title-ref">linear\_model.RANSACRegressor.fit</span> may run until `max_iter` if it finds a large inlier group early. `8251` by `aivision2020`.
  - Fixed a bug where <span class="title-ref">naive\_bayes.MultinomialNB</span> and <span class="title-ref">naive\_bayes.BernoulliNB</span> failed when `alpha=0`. `5814` by `Yichuan Liu <yl565>` and `Herilalaina Rakotoarison
    <herilalaina>`.
  - Fixed a bug where <span class="title-ref">linear\_model.LassoLars</span> does not give the same result as the LassoLars implementation available in R (lars library). `7849` by `Jair Montoya Martinez <jmontoyam>`.
  - Fixed a bug in <span class="title-ref">linear\_model.RandomizedLasso</span>, <span class="title-ref">linear\_model.Lars</span>, <span class="title-ref">linear\_model.LassoLars</span>, <span class="title-ref">linear\_model.LarsCV</span> and <span class="title-ref">linear\_model.LassoLarsCV</span>, where the parameter `precompute` was not used consistently across classes, and some values proposed in the docstring could raise errors. `5359` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - Fix inconsistent results between <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.Ridge</span> when using `normalize=True`. `9302` by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - Fix a bug where <span class="title-ref">linear\_model.LassoLars.fit</span> sometimes left `coef_` as a list, rather than an ndarray. `8160` by `CJ Carey <perimosocordiae>`.
  - Fix <span class="title-ref">linear\_model.BayesianRidge.fit</span> to return ridge parameter `alpha_` and `lambda_` consistent with calculated coefficients `coef_` and `intercept_`. `8224` by `Peter Gedeck <gedeck>`.
  - Fixed a bug in <span class="title-ref">svm.OneClassSVM</span> where it returned floats instead of integer classes. `8676` by `Vathsala Achar <VathsalaAchar>`.
  - Fix AIC/BIC criterion computation in <span class="title-ref">linear\_model.LassoLarsIC</span>. `9022` by [Alexandre Gramfort](http://alexandre.gramfort.net) and `Mehmet Basbug <mehmetbasbug>`.
  - Fixed a memory leak in our LibLinear implementation. `9024` by `Sergei Lebedev <superbobry>`
  - Fix bug where stratified CV splitters did not work with <span class="title-ref">linear\_model.LassoCV</span>. `8973` by `Paulo Haddad <paulochf>`.
  - Fixed a bug in <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> when the standard deviation and covariance predicted without fit would fail with a unmeaningful error by default. `6573` by `Quazi Marufur Rahman <qmaruf>` and [Manoj Kumar](https://manojbits.wordpress.com).

Other predictors

  - Fix <span class="title-ref">semi\_supervised.BaseLabelPropagation</span> to correctly implement `LabelPropagation` and `LabelSpreading` as done in the referenced papers. `9239` by `Andre Ambrosio Boechat <boechat107>`, `Utkarsh Upadhyay
    <musically-ut>`, and [Joel Nothman](https://joelnothman.com/).

Decomposition, manifold learning and clustering

  - Fixed the implementation of \`manifold.TSNE\`:
  - `early_exageration` parameter had no effect and is now used for the first 250 optimization iterations.
  - Fixed the `AssertionError: Tree consistency failed` exception reported in `8992`.
  - Improve the learning schedule to match the one from the reference implementation [lvdmaaten/bhtsne](https://github.com/lvdmaaten/bhtsne). by `Thomas Moreau <tomMoral>` and [Olivier Grisel](https://twitter.com/ogrisel).
  - Fix a bug in <span class="title-ref">decomposition.LatentDirichletAllocation</span> where the `perplexity` method was returning incorrect results because the `transform` method returns normalized document topic distributions as of version 0.18. `7954` by `Gary Foreman <garyForeman>`.
  - Fix output shape and bugs with n\_jobs \> 1 in <span class="title-ref">decomposition.SparseCoder</span> transform and <span class="title-ref">decomposition.sparse\_encode</span> for one-dimensional data and one component. This also impacts the output shape of <span class="title-ref">decomposition.DictionaryLearning</span>. `8086` by [Andreas Müller](https://amueller.github.io/).
  - Fixed the implementation of `explained_variance_` in <span class="title-ref">decomposition.PCA</span>, <span class="title-ref">decomposition.RandomizedPCA</span> and <span class="title-ref">decomposition.IncrementalPCA</span>. `9105` by [Hanmin Qin](https://github.com/qinhanmin2014).
  - Fixed the implementation of `noise_variance_` in <span class="title-ref">decomposition.PCA</span>. `9108` by [Hanmin Qin](https://github.com/qinhanmin2014).
  - Fixed a bug where <span class="title-ref">cluster.DBSCAN</span> gives incorrect result when input is a precomputed sparse matrix with initial rows all zero. `8306` by `Akshay Gupta <Akshay0724>`
  - Fix a bug regarding fitting <span class="title-ref">cluster.KMeans</span> with a sparse array X and initial centroids, where X's means were unnecessarily being subtracted from the centroids. `7872` by `Josh Karnofsky <jkarno>`.
  - Fixes to the input validation in <span class="title-ref">covariance.EllipticEnvelope</span>. `8086` by [Andreas Müller](https://amueller.github.io/).
  - Fixed a bug in <span class="title-ref">covariance.MinCovDet</span> where inputting data that produced a singular covariance matrix would cause the helper method `_c_step` to throw an exception. `3367` by `Jeremy Steward <ThatGeoGuy>`
  - Fixed a bug in <span class="title-ref">manifold.TSNE</span> affecting convergence of the gradient descent. `8768` by `David DeTomaso <deto>`.
  - Fixed a bug in <span class="title-ref">manifold.TSNE</span> where it stored the incorrect `kl_divergence_`. `6507` by `Sebastian Saeger <ssaeger>`.
  - Fixed improper scaling in <span class="title-ref">cross\_decomposition.PLSRegression</span> with `scale=True`. `7819` by `jayzed82 <jayzed82>`.
  - <span class="title-ref">cluster.SpectralCoclustering</span> and <span class="title-ref">cluster.SpectralBiclustering</span> `fit` method conforms with API by accepting `y` and returning the object. `6126`, `7814` by `Laurent Direr <ldirer>` and `Maniteja
    Nandana <maniteja123>`.
  - Fix bug where `sklearn.mixture` `sample` methods did not return as many samples as requested. `7702` by `Levi John Wolf <ljwolf>`.
  - Fixed the shrinkage implementation in <span class="title-ref">neighbors.NearestCentroid</span>. `9219` by [Hanmin Qin](https://github.com/qinhanmin2014).

Preprocessing and feature selection

  - For sparse matrices, <span class="title-ref">preprocessing.normalize</span> with `return_norm=True` will now raise a `NotImplementedError` with 'l1' or 'l2' norm and with norm 'max' the norms returned will be the same as for dense matrices. `7771` by [Ang Lu](https://github.com/luang008).
  - Fix a bug where <span class="title-ref">feature\_selection.SelectFdr</span> did not exactly implement Benjamini-Hochberg procedure. It formerly may have selected fewer features than it should. `7490` by `Peng Meng <mpjlu>`.
  - Fixed a bug where <span class="title-ref">linear\_model.RandomizedLasso</span> and <span class="title-ref">linear\_model.RandomizedLogisticRegression</span> breaks for sparse input. `8259` by `Aman Dalmia <dalmia>`.
  - Fix a bug where <span class="title-ref">feature\_extraction.FeatureHasher</span> mandatorily applied a sparse random projection to the hashed features, preventing the use of <span class="title-ref">feature\_extraction.text.HashingVectorizer</span> in a pipeline with <span class="title-ref">feature\_extraction.text.TfidfTransformer</span>. `7565` by `Roman Yurchak <rth>`.
  - Fix a bug where <span class="title-ref">feature\_selection.mutual\_info\_regression</span> did not correctly use `n_neighbors`. `8181` by `Guillaume Lemaitre
    <glemaitre>`.

Model evaluation and meta-estimators

  - Fixed a bug where <span class="title-ref">model\_selection.BaseSearchCV.inverse\_transform</span> returns `self.best_estimator_.transform()` instead of `self.best_estimator_.inverse_transform()`. `8344` by `Akshay Gupta <Akshay0724>` and `Rasmus Eriksson <MrMjauh>`.
  - Added `classes_` attribute to <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span>, <span class="title-ref">grid\_search.GridSearchCV</span>, and <span class="title-ref">grid\_search.RandomizedSearchCV</span> that matches the `classes_` attribute of `best_estimator_`. `7661` and `8295` by `Alyssa Batula <abatula>`, `Dylan Werner-Meier <unautre>`, and `Stephen Hoover <stephen-hoover>`.
  - Fixed a bug where <span class="title-ref">model\_selection.validation\_curve</span> reused the same estimator for each parameter value. `7365` by `Aleksandr Sandrovskii <Sundrique>`.
  - <span class="title-ref">model\_selection.permutation\_test\_score</span> now works with Pandas types. `5697` by `Stijn Tonk <equialgo>`.
  - Several fixes to input validation in <span class="title-ref">multiclass.OutputCodeClassifier</span> `8086` by [Andreas Müller](https://amueller.github.io/).
  - <span class="title-ref">multiclass.OneVsOneClassifier</span>'s `partial_fit` now ensures all classes are provided up-front. `6250` by `Asish Panda <kaichogami>`.
  - Fix <span class="title-ref">multioutput.MultiOutputClassifier.predict\_proba</span> to return a list of 2d arrays, rather than a 3d array. In the case where different target columns had different numbers of classes, a `ValueError` would be raised on trying to stack matrices with different dimensions. `8093` by `Peter Bull <pjbull>`.
  - Cross validation now works with Pandas datatypes that have a read-only index. `9507` by [Loic Esteve](https://github.com/lesteve).

Metrics

  - <span class="title-ref">metrics.average\_precision\_score</span> no longer linearly interpolates between operating points, and instead weighs precisions by the change in recall since the last operating point, as per the [Wikipedia entry](https://en.wikipedia.org/wiki/Average_precision). ([\#7356](https://github.com/scikit-learn/scikit-learn/pull/7356)). By `Nick Dingwall <ndingwall>` and [Gael Varoquaux](http://gael-varoquaux.info).
  - Fix a bug in <span class="title-ref">metrics.classification.\_check\_targets</span> which would return `'binary'` if `y_true` and `y_pred` were both `'binary'` but the union of `y_true` and `y_pred` was `'multiclass'`. `8377` by [Loic Esteve](https://github.com/lesteve).
  - Fixed an integer overflow bug in <span class="title-ref">metrics.confusion\_matrix</span> and hence <span class="title-ref">metrics.cohen\_kappa\_score</span>. `8354`, `7929` by [Joel Nothman](https://joelnothman.com/) and `Jon Crall <Erotemic>`.
  - Fixed passing of `gamma` parameter to the `chi2` kernel in <span class="title-ref">metrics.pairwise.pairwise\_kernels</span> `5211` by `Nick Rhinehart <nrhine1>`, `Saurabh Bansod <mth4saurabh>` and [Andreas Müller](https://amueller.github.io/).

Miscellaneous

  - Fixed a bug when <span class="title-ref">datasets.make\_classification</span> fails when generating more than 30 features. `8159` by `Herilalaina Rakotoarison <herilalaina>`.
  - Fixed a bug where <span class="title-ref">datasets.make\_moons</span> gives an incorrect result when `n_samples` is odd. `8198` by `Josh Levy <levy5674>`.
  - Some `fetch_` functions in `sklearn.datasets` were ignoring the `download_if_missing` keyword. `7944` by `Ralf Gommers <rgommers>`.
  - Fix estimators to accept a `sample_weight` parameter of type `pandas.Series` in their `fit` function. `7825` by [Kathleen Chen](https://github.com/kchen17).
  - Fix a bug in cases where `numpy.cumsum` may be numerically unstable, raising an exception if instability is identified. `7376` and `7331` by [Joel Nothman](https://joelnothman.com/) and `yangarbiter`.
  - Fix a bug where <span class="title-ref">base.BaseEstimator.\_\_getstate\_\_</span> obstructed pickling customizations of child-classes, when used in a multiple inheritance context. `8316` by `Holger Peters <HolgerPeters>`.
  - Update Sphinx-Gallery from 0.1.4 to 0.1.7 for resolving links in documentation build with Sphinx\>1.5 `8010`, `7986` by `Oscar Najera <Titan-C>`
  - Add `data_home` parameter to <span class="title-ref">sklearn.datasets.fetch\_kddcup99</span>. `9289` by [Loic Esteve](https://github.com/lesteve).
  - Fix dataset loaders using Python 3 version of makedirs to also work in Python 2. `9284` by `Sebastin Santy <SebastinSanty>`.
  - Several minor issues were fixed with thanks to the alerts of [lgtm.com](https://lgtm.com/). `9278` by `Jean Helie <jhelie>`, among others.

### API changes summary

Trees and ensembles

  - Gradient boosting base models are no longer estimators. By [Andreas Müller](https://amueller.github.io/).
  - All tree based estimators now accept a `min_impurity_decrease` parameter in lieu of the `min_impurity_split`, which is now deprecated. The `min_impurity_decrease` helps stop splitting the nodes in which the weighted impurity decrease from splitting is no longer at least `min_impurity_decrease`. `8449` by [Raghav RV](https://github.com/raghavrv).

Linear, kernelized and related models

  - `n_iter` parameter is deprecated in <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> and <span class="title-ref">linear\_model.Perceptron</span>. By [Tom Dupre la Tour](https://github.com/TomDLT).

Other predictors

  - <span class="title-ref">neighbors.LSHForest</span> has been deprecated and will be removed in 0.21 due to poor performance. `9078` by `Laurent Direr <ldirer>`.
  - <span class="title-ref">neighbors.NearestCentroid</span> no longer purports to support `metric='precomputed'` which now raises an error. `8515` by `Sergul Aydore <sergulaydore>`.
  - The `alpha` parameter of <span class="title-ref">semi\_supervised.LabelPropagation</span> now has no effect and is deprecated to be removed in 0.21. `9239` by `Andre Ambrosio Boechat <boechat107>`, `Utkarsh Upadhyay
    <musically-ut>`, and [Joel Nothman](https://joelnothman.com/).

Decomposition, manifold learning and clustering

  - Deprecate the `doc_topic_distr` argument of the `perplexity` method in <span class="title-ref">decomposition.LatentDirichletAllocation</span> because the user no longer has access to the unnormalized document topic distribution needed for the perplexity calculation. `7954` by `Gary Foreman <garyForeman>`.
  - The `n_topics` parameter of <span class="title-ref">decomposition.LatentDirichletAllocation</span> has been renamed to `n_components` and will be removed in version 0.21. `8922` by `Attractadore`.
  - <span class="title-ref">decomposition.SparsePCA.transform</span>'s `ridge_alpha` parameter is deprecated in preference for class parameter. `8137` by `Naoya Kanai <naoyak>`.
  - <span class="title-ref">cluster.DBSCAN</span> now has a `metric_params` parameter. `8139` by `Naoya Kanai <naoyak>`.

Preprocessing and feature selection

  - <span class="title-ref">feature\_selection.SelectFromModel</span> now has a `partial_fit` method only if the underlying estimator does. By [Andreas Müller](https://amueller.github.io/).
  - <span class="title-ref">feature\_selection.SelectFromModel</span> now validates the `threshold` parameter and sets the `threshold_` attribute during the call to `fit`, and no longer during the call to `transform`<span class="title-ref">. By \`Andreas Müller</span>\_.
  - The `non_negative` parameter in <span class="title-ref">feature\_extraction.FeatureHasher</span> has been deprecated, and replaced with a more principled alternative, `alternate_sign`. `7565` by `Roman Yurchak <rth>`.
  - <span class="title-ref">linear\_model.RandomizedLogisticRegression</span>, and <span class="title-ref">linear\_model.RandomizedLasso</span> have been deprecated and will be removed in version 0.21. `8995` by `Ramana.S <sentient07>`.

Model evaluation and meta-estimators

  - Deprecate the `fit_params` constructor input to the <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> in favor of passing keyword parameters to the `fit` methods of those classes. Data-dependent parameters needed for model training should be passed as keyword arguments to `fit`, and conforming to this convention will allow the hyperparameter selection classes to be used with tools such as <span class="title-ref">model\_selection.cross\_val\_predict</span>. `2879` by `Stephen Hoover <stephen-hoover>`.
  - In version 0.21, the default behavior of splitters that use the `test_size` and `train_size` parameter will change, such that specifying `train_size` alone will cause `test_size` to be the remainder. `7459` by `Nelson Liu <nelson-liu>`.
  - <span class="title-ref">multiclass.OneVsRestClassifier</span> now has `partial_fit`, `decision_function` and `predict_proba` methods only when the underlying estimator does. `7812` by [Andreas Müller](https://amueller.github.io/) and `Mikhail Korobov <kmike>`.
  - <span class="title-ref">multiclass.OneVsRestClassifier</span> now has a `partial_fit` method only if the underlying estimator does. By [Andreas Müller](https://amueller.github.io/).
  - The `decision_function` output shape for binary classification in <span class="title-ref">multiclass.OneVsRestClassifier</span> and <span class="title-ref">multiclass.OneVsOneClassifier</span> is now `(n_samples,)` to conform to scikit-learn conventions. `9100` by [Andreas Müller](https://amueller.github.io/).
  - The <span class="title-ref">multioutput.MultiOutputClassifier.predict\_proba</span> function used to return a 3d array (`n_samples`, `n_classes`, `n_outputs`). In the case where different target columns had different numbers of classes, a `ValueError` would be raised on trying to stack matrices with different dimensions. This function now returns a list of arrays where the length of the list is `n_outputs`, and each array is (`n_samples`, `n_classes`) for that particular output. `8093` by `Peter Bull <pjbull>`.
  - Replace attribute `named_steps` `dict` to <span class="title-ref">utils.Bunch</span> in <span class="title-ref">pipeline.Pipeline</span> to enable tab completion in interactive environment. In the case conflict value on `named_steps` and `dict` attribute, `dict` behavior will be prioritized. `8481` by `Herilalaina Rakotoarison <herilalaina>`.

Miscellaneous

  - Deprecate the `y` parameter in `transform` and `inverse_transform`. The method should not accept `y` parameter, as it's used at the prediction time. `8174` by `Tahar Zanouda <tzano>`, [Alexandre Gramfort](http://alexandre.gramfort.net) and [Raghav RV](https://github.com/raghavrv).

  - SciPy \>= 0.13.3 and NumPy \>= 1.8.2 are now the minimum supported versions for scikit-learn. The following backported functions in `sklearn.utils` have been removed or deprecated accordingly. `8854` and `8874` by `Naoya Kanai <naoyak>`

  - The `store_covariances` and `covariances_` parameters of <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis</span> has been renamed to `store_covariance` and `covariance_` to be consistent with the corresponding parameter names of the <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span>. They will be removed in version 0.21. `7998` by `Jiacheng <mrbeann>`
    
    Removed in 0.19:
    
      - `utils.fixes.argpartition`
      - `utils.fixes.array_equal`
      - `utils.fixes.astype`
      - `utils.fixes.bincount`
      - `utils.fixes.expit`
      - `utils.fixes.frombuffer_empty`
      - `utils.fixes.in1d`
      - `utils.fixes.norm`
      - `utils.fixes.rankdata`
      - `utils.fixes.safe_copy`
    
    Deprecated in 0.19, to be removed in 0.21:
    
      - `utils.arpack.eigs`
      - `utils.arpack.eigsh`
      - `utils.arpack.svds`
      - `utils.extmath.fast_dot`
      - `utils.extmath.logsumexp`
      - `utils.extmath.norm`
      - `utils.extmath.pinvh`
      - `utils.graph.graph_laplacian`
      - `utils.random.choice`
      - `utils.sparsetools.connected_components`
      - `utils.stats.rankdata`

  - Estimators with both methods `decision_function` and `predict_proba` are now required to have a monotonic relation between them. The method `check_decision_proba_consistency` has been added in **utils.estimator\_checks** to check their consistency. `7578` by `Shubham Bhardwaj <shubham0704>`

  - All checks in `utils.estimator_checks`, in particular <span class="title-ref">utils.estimator\_checks.check\_estimator</span> now accept estimator instances. Most other checks do not accept estimator classes any more. `9019` by [Andreas Müller](https://amueller.github.io/).

  - Ensure that estimators' attributes ending with `_` are not set in the constructor but only in the `fit` method. Most notably, ensemble estimators (deriving from <span class="title-ref">ensemble.BaseEnsemble</span>) now only have `self.estimators_` available after `fit`. `7464` by [Lars Buitinck](https://github.com/larsmans) and [Loic Esteve](https://github.com/lesteve).

### Code and Documentation Contributors

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.18, including:

Joel Nothman, Loic Esteve, Andreas Mueller, Guillaume Lemaitre, Olivier Grisel, Hanmin Qin, Raghav RV, Alexandre Gramfort, themrmax, Aman Dalmia, Gael Varoquaux, Naoya Kanai, Tom Dupré la Tour, Rishikesh, Nelson Liu, Taehoon Lee, Nelle Varoquaux, Aashil, Mikhail Korobov, Sebastin Santy, Joan Massich, Roman Yurchak, RAKOTOARISON Herilalaina, Thierry Guillemot, Alexandre Abadie, Carol Willing, Balakumaran Manoharan, Josh Karnofsky, Vlad Niculae, Utkarsh Upadhyay, Dmitry Petrov, Minghui Liu, Srivatsan, Vincent Pham, Albert Thomas, Jake VanderPlas, Attractadore, JC Liu, alexandercbooth, chkoar, Óscar Nájera, Aarshay Jain, Kyle Gilliam, Ramana Subramanyam, CJ Carey, Clement Joudet, David Robles, He Chen, Joris Van den Bossche, Karan Desai, Katie Luangkote, Leland McInnes, Maniteja Nandana, Michele Lacchia, Sergei Lebedev, Shubham Bhardwaj, akshay0724, omtcyfz, rickiepark, waterponey, Vathsala Achar, jbDelafosse, Ralf Gommers, Ekaterina Krivich, Vivek Kumar, Ishank Gulati, Dave Elliott, ldirer, Reiichiro Nakano, Levi John Wolf, Mathieu Blondel, Sid Kapur, Dougal J. Sutherland, midinas, mikebenfield, Sourav Singh, Aseem Bansal, Ibraim Ganiev, Stephen Hoover, AishwaryaRK, Steven C. Howell, Gary Foreman, Neeraj Gangwar, Tahar, Jon Crall, dokato, Kathy Chen, ferria, Thomas Moreau, Charlie Brummitt, Nicolas Goix, Adam Kleczewski, Sam Shleifer, Nikita Singh, Basil Beirouti, Giorgio Patrini, Manoj Kumar, Rafael Possas, James Bourbeau, James A. Bednar, Janine Harper, Jaye, Jean Helie, Jeremy Steward, Artsiom, John Wei, Jonathan LIgo, Jonathan Rahn, seanpwilliams, Arthur Mensch, Josh Levy, Julian Kuhlmann, Julien Aubert, Jörn Hees, Kai, shivamgargsya, Kat Hempstalk, Kaushik Lakshmikanth, Kennedy, Kenneth Lyons, Kenneth Myers, Kevin Yap, Kirill Bobyrev, Konstantin Podshumok, Arthur Imbert, Lee Murray, toastedcornflakes, Lera, Li Li, Arthur Douillard, Mainak Jas, tobycheese, Manraj Singh, Manvendra Singh, Marc Meketon, MarcoFalke, Matthew Brett, Matthias Gilch, Mehul Ahuja, Melanie Goetz, Meng, Peng, Michael Dezube, Michal Baumgartner, vibrantabhi19, Artem Golubin, Milen Paskov, Antonin Carette, Morikko, MrMjauh, NALEPA Emmanuel, Namiya, Antoine Wendlinger, Narine Kokhlikyan, NarineK, Nate Guerin, Angus Williams, Ang Lu, Nicole Vavrova, Nitish Pandey, Okhlopkov Daniil Olegovich, Andy Craze, Om Prakash, Parminder Singh, Patrick Carlson, Patrick Pei, Paul Ganssle, Paulo Haddad, Paweł Lorek, Peng Yu, Pete Bachant, Peter Bull, Peter Csizsek, Peter Wang, Pieter Arthur de Jong, Ping-Yao, Chang, Preston Parry, Puneet Mathur, Quentin Hibon, Andrew Smith, Andrew Jackson, 1kastner, Rameshwar Bhaskaran, Rebecca Bilbro, Remi Rampin, Andrea Esuli, Rob Hall, Robert Bradshaw, Romain Brault, Aman Pratik, Ruifeng Zheng, Russell Smith, Sachin Agarwal, Sailesh Choyal, Samson Tan, Samuël Weber, Sarah Brown, Sebastian Pölsterl, Sebastian Raschka, Sebastian Saeger, Alyssa Batula, Abhyuday Pratap Singh, Sergey Feldman, Sergul Aydore, Sharan Yalburgi, willduan, Siddharth Gupta, Sri Krishna, Almer, Stijn Tonk, Allen Riddell, Theofilos Papapanagiotou, Alison, Alexis Mignon, Tommy Boucher, Tommy Löfstedt, Toshihiro Kamishima, Tyler Folkman, Tyler Lanigan, Alexander Junge, Varun Shenoy, Victor Poughon, Vilhelm von Ehrenheim, Aleksandr Sandrovskii, Alan Yee, Vlasios Vasileiou, Warut Vijitbenjaronk, Yang Zhang, Yaroslav Halchenko, Yichuan Liu, Yuichi Fujikawa, affanv14, aivision2020, xor, andreh7, brady salz, campustrampus, Agamemnon Krasoulis, ditenberg, elena-sharova, filipj8, fukatani, gedeck, guiniol, guoci, hakaa1, hongkahjun, i-am-xhy, jakirkham, jaroslaw-weber, jayzed82, jeroko, jmontoyam, jonathan.striebel, josephsalmon, jschendel, leereeves, martin-hahn, mathurinm, mehak-sachdeva, mlewis1729, mlliou112, mthorrell, ndingwall, nuffe, yangarbiter, plagree, pldtc325, Breno Freitas, Brett Olsen, Brian A. Alfano, Brian Burns, polmauri, Brandon Carter, Charlton Austin, Chayant T15h, Chinmaya Pancholi, Christian Danielsen, Chung Yen, Chyi-Kwei Yau, pravarmahajan, DOHMATOB Elvis, Daniel LeJeune, Daniel Hnyk, Darius Morawiec, David DeTomaso, David Gasquez, David Haberthür, David Heryanto, David Kirkby, David Nicholson, rashchedrin, Deborah Gertrude Digges, Denis Engemann, Devansh D, Dickson, Bob Baxley, Don86, E. Lynch-Klarup, Ed Rogers, Elizabeth Ferriss, Ellen-Co2, Fabian Egli, Fang-Chieh Chou, Bing Tian Dai, Greg Stupp, Grzegorz Szpak, Bertrand Thirion, Hadrien Bertrand, Harizo Rajaona, zxcvbnius, Henry Lin, Holger Peters, Icyblade Dai, Igor Andriushchenko, Ilya, Isaac Laughlin, Iván Vallés, Aurélien Bellet, JPFrancoia, Jacob Schreiber, Asish Mahapatra

---

v0.20.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.20

\> **Warning** \> Version 0.20 is the last version of scikit-learn to support Python 2.7 and Python 3.4. Scikit-learn 0.21 will require Python 3.5 or higher.

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 0.20.4

**July 30, 2019**

This is a bug-fix release with some bug fixes applied to version 0.20.3.

### Changelog

The bundled version of joblib was upgraded from 0.13.0 to 0.13.2.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where KMeans++ initialisation could rarely result in an IndexError. `11756` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">compose.ColumnTransformer</span> where using DataFrames whose column order differs between `` `fit ``<span class="title-ref"> and :func:</span><span class="title-ref">transform</span><span class="title-ref"> could lead to silently passing incorrect columns to the </span><span class="title-ref">remainder</span><span class="title-ref"> transformer. :pr:\`14237</span> by <span class="title-ref">Andreas Schuderer \<schuderer\></span>.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cross\_decomposition.CCA</span> improving numerical stability when <span class="title-ref">Y</span> is close to zero. `13903` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.model_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">model\_selection.StratifiedKFold</span> shuffles each class's samples with the same `random_state`, making `shuffle=True` ineffective. `13124` by `Hanmin Qin <qinhanmin2014>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neighbors.KernelDensity</span> which could not be restored from a pickle if `sample_weight` had been used. `13772` by `Aditya Vyas <aditya1702>`.

## Version 0.20.3

**March 1, 2019**

This is a bug-fix release with some minor documentation improvements and enhancements to features released in 0.20.0.

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where computation was single threaded when <span class="title-ref">n\_jobs \> 1</span> or <span class="title-ref">n\_jobs = -1</span>. `12949` by `Prabakaran Kumaresshan <nixphix>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">compose.ColumnTransformer</span> to handle negative indexes in the columns list of the transformers. `12946` by `Pierre Tallotte <pierretallotte>`.

#### `sklearn.covariance`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in <span class="title-ref">covariance.graphical\_lasso</span> so that the case <span class="title-ref">n\_features=2</span> is handled correctly. `13276` by `Aurélien Bellet <bellet>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.sparse\_encode</span> where computation was single threaded when <span class="title-ref">n\_jobs \> 1</span> or <span class="title-ref">n\_jobs = -1</span>. `13005` by `Prabakaran Kumaresshan <nixphix>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">sklearn.datasets.fetch\_openml</span> now loads data by streaming, avoiding high memory usage. `13312` by [Joris Van den Bossche](https://github.com/jorisvandenbossche).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.text.CountVectorizer</span> which would result in the sparse feature matrix having conflicting <span class="title-ref">indptr</span> and <span class="title-ref">indices</span> precisions under very large vocabularies. `11295` by `Gabriel Vacaliuc <gvacaliuc>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` add support for non-numeric data in <span class="title-ref">sklearn.impute.MissingIndicator</span> which was not supported while <span class="title-ref">sklearn.impute.SimpleImputer</span> was supporting this for some imputation strategies. `13046` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.MultiTaskElasticNet</span> and <span class="title-ref">linear\_model.MultiTaskLasso</span> which were breaking when `warm_start = True`. `12360` by `Aakanksha Joshi <joaak>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">preprocessing.KBinsDiscretizer</span> where `strategy='kmeans'` fails with an error during transformation due to unsorted bin edges. `13134` by `Sandro Casagrande <SandroCasagrande>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">preprocessing.OneHotEncoder</span> where the deprecation of `categorical_features` was handled incorrectly in combination with `handle_unknown='ignore'`. `12881` by [Joris Van den Bossche](https://github.com/jorisvandenbossche).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Bins whose width are too small (i.e., \<= 1e-8) are removed with a warning in <span class="title-ref">preprocessing.KBinsDiscretizer</span>. `13165` by `Hanmin Qin <qinhanmin2014>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.NuSVR</span> and <span class="title-ref">svm.OneClassSVM</span> where the `scale` option of parameter `gamma` is erroneously defined as `1 / (n_features * X.std())`. It's now defined as `1 / (n_features * X.var())`. `13221` by `Hanmin Qin <qinhanmin2014>`.

### Code and Documentation Contributors

With thanks to:

Adrin Jalali, Agamemnon Krasoulis, Albert Thomas, Andreas Mueller, Aurélien Bellet, bertrandhaut, Bharat Raghunathan, Dowon, Emmanuel Arias, Fibinse Xavier, Finn O'Shea, Gabriel Vacaliuc, Gael Varoquaux, Guillaume Lemaitre, Hanmin Qin, joaak, Joel Nothman, Joris Van den Bossche, Jérémie Méhault, kms15, Kossori Aruku, Lakshya KD, maikia, Manuel López-Ibáñez, Marco Gorelli, MarcoGorelli, mferrari3, Mickaël Schoentgen, Nicolas Hug, pavlos kallis, Pierre Glaser, pierretallotte, Prabakaran Kumaresshan, Reshama Shaikh, Rohit Kapoor, Roman Yurchak, SandroCasagrande, Tashay Green, Thomas Fan, Vishaal Kapoor, Zhuyi Xue, Zijie (ZJ) Poh

## Version 0.20.2

**December 20, 2018**

This is a bug-fix release with some minor documentation improvements and enhancements to features released in 0.20.0.

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `sklearn.neighbors` when `metric=='jaccard'` (bug fix)
  - use of `'seuclidean'` or `'mahalanobis'` metrics in some cases (bug fix)

### Changelog

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">compose.make\_column\_transformer</span> which raises unexpected error when columns is pandas Index or pandas Series. `12704` by `Hanmin Qin <qinhanmin2014>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.pairwise\_distances</span> and <span class="title-ref">metrics.pairwise\_distances\_chunked</span> where parameters `V` of `"seuclidean"` and `VI` of `"mahalanobis"` metrics were computed after the data was split into chunks instead of being pre-computed on whole data. `12701` by `Jeremie du Boisberranger <jeremiedbb>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed <span class="title-ref">sklearn.neighbors.DistanceMetric</span> jaccard distance function to return 0 when two all-zero vectors are compared. `12685` by `Thomas Fan <thomasjpfan>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Calling <span class="title-ref">utils.check\_array</span> on <span class="title-ref">pandas.Series</span> with categorical data, which raised an error in 0.20.0, now returns the expected output again. `12699` by [Joris Van den Bossche](https://github.com/jorisvandenbossche).

### Code and Documentation Contributors

With thanks to:

adanhawth, Adrin Jalali, Albert Thomas, Andreas Mueller, Dan Stine, Feda Curic, Hanmin Qin, Jan S, jeremiedbb, Joel Nothman, Joris Van den Bossche, josephsalmon, Katrin Leinweber, Loic Esteve, Muhammad Hassaan Rafique, Nicolas Hug, Olivier Grisel, Paul Paczuski, Reshama Shaikh, Sam Waterbury, Shivam Kotwalia, Thomas Fan

## Version 0.20.1

**November 21, 2018**

This is a bug-fix release with some minor documentation improvements and enhancements to features released in 0.20.0. Note that we also include some API changes in this release, so you might get some extra warnings after updating from 0.20.0 to 0.20.1.

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - <span class="title-ref">decomposition.IncrementalPCA</span> (bug fix)

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` make <span class="title-ref">cluster.MeanShift</span> no longer try to do nested parallelism as the overhead would hurt performance significantly when `n_jobs > 1`. `12159` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.DBSCAN</span> with precomputed sparse neighbors graph, which would add explicitly zeros on the diagonal even when already present. `12105` by [Tom Dupre la Tour](https://github.com/TomDLT).

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">compose.ColumnTransformer</span> when stacking columns with types not convertible to a numeric. `11912` by `Adrin Jalali <adrinjalali>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">compose.ColumnTransformer</span> now applies the `sparse_threshold` even if all transformation results are sparse. `12304` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">compose.make\_column\_transformer</span> now expects `(transformer, columns)` instead of `(columns, transformer)` to keep consistent with <span class="title-ref">compose.ColumnTransformer</span>. `12339` by `Adrin Jalali <adrinjalali>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_openml</span> to correctly use the local cache. `12246` by `Jan N. van Rijn <janvanrijn>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_openml</span> to correctly handle ignore attributes and row id attributes. `12330` by `Jan N. van Rijn <janvanrijn>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed integer overflow in <span class="title-ref">datasets.make\_classification</span> for values of `n_informative` parameter larger than 64. `10811` by `Roman Feldbauer <VarIr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed olivetti faces dataset `DESCR` attribute to point to the right location in <span class="title-ref">datasets.fetch\_olivetti\_faces</span>. `12441` by `Jérémie du Boisberranger <jeremiedbb>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_openml</span> to retry downloading when reading from local cache fails. `12517` by `Thomas Fan <thomasjpfan>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in <span class="title-ref">decomposition.IncrementalPCA</span> where 0.20.0 raised an error if the number of samples in the final batch for fitting IncrementalPCA was smaller than n\_components. `12234` by `Ming Li <minggli>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug mostly affecting <span class="title-ref">ensemble.RandomForestClassifier</span> where `class_weight='balanced_subsample'` failed with more than 32 classes. `12165` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug affecting <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span> and <span class="title-ref">ensemble.IsolationForest</span>, where `max_features` was sometimes rounded down to zero. `12388` by `Connor Tann <Connossor>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in v0.20.0 where <span class="title-ref">feature\_extraction.text.CountVectorizer</span> and other text vectorizers could error during stop words validation with custom preprocessors or tokenizers. `12393` by [Roman Yurchak](https://github.com/rth).

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.SGDClassifier</span> and variants with `early_stopping=True` would not use a consistent validation split in the multiclass case and this would cause a crash when using those estimators as part of parallel parameter search or cross-validation. `12122` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug affecting <span class="title-ref">linear\_model.SGDClassifier</span> in the multiclass case. Each one-versus-all step is run in a <span class="title-ref">joblib.Parallel</span> call and mutating a common parameter, causing a segmentation fault if called within a backend using processes and not threads. We now use `require=sharedmem` at the <span class="title-ref">joblib.Parallel</span> instance creation. `12518` by `Pierre Glaser <pierreglaser>` and `Olivier Grisel <ogrisel>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.pairwise.pairwise\_distances\_argmin\_min</span> which returned the square root of the distance when the metric parameter was set to "euclidean". `12481` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.pairwise.pairwise\_distances\_chunked</span> which didn't ensure the diagonal is zero for euclidean distances. `12612` by `Andreas Müller <amueller>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">metrics.calinski\_harabaz\_score</span> has been renamed to <span class="title-ref">metrics.calinski\_harabasz\_score</span> and will be removed in version 0.23. `12211` by `Lisa Thomas <LisaThomas9>`, `Mark Hannel <markhannel>` and `Melissa Ferrari <mferrari3>`.

#### `sklearn.mixture`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Ensure that the `fit_predict` method of <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span> always yield assignments consistent with `fit` followed by `predict` even if the convergence criterion is too loose or not met. `12451` by `Olivier Grisel <ogrisel>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` force the parallelism backend to `threading` for <span class="title-ref">neighbors.KDTree</span> and <span class="title-ref">neighbors.BallTree</span> in Python 2.7 to avoid pickling errors caused by the serialization of their methods. `12171` by `Thomas Moreau <tomMoral>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">preprocessing.OrdinalEncoder</span> when passing manually specified categories. `12365` by [Joris Van den Bossche](https://github.com/jorisvandenbossche).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">preprocessing.KBinsDiscretizer</span> where the `transform` method mutates the `_encoder` attribute. The `transform` method is now thread safe. `12514` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">preprocessing.PowerTransformer</span> where the Yeo-Johnson transform was incorrect for lambda parameters outside of <span class="title-ref">\[0, 2\]</span> `12522` by `Nicolas Hug<NicolasHug>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">preprocessing.OneHotEncoder</span> where transform failed when set to ignore unknown numpy strings of different lengths `12471` by `Gabriel Marzinotto<GMarzinotto>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of the `method` argument in <span class="title-ref">preprocessing.power\_transform</span> will be changed from `box-cox` to `yeo-johnson` to match <span class="title-ref">preprocessing.PowerTransformer</span> in version 0.23. A FutureWarning is raised when the default value is used. `12317` by `Eric Chang <chang>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Use float64 for mean accumulator to avoid floating point precision issues in <span class="title-ref">preprocessing.StandardScaler</span> and <span class="title-ref">decomposition.IncrementalPCA</span> when using float32 datasets. `12338` by `bauks <bauks>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Calling <span class="title-ref">utils.check\_array</span> on <span class="title-ref">pandas.Series</span>, which raised an error in 0.20.0, now returns the expected output again. `12625` by [Andreas Müller](https://amueller.github.io/)

#### Miscellaneous

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` When using site joblib by setting the environment variable <span class="title-ref">SKLEARN\_SITE\_JOBLIB</span>, added compatibility with joblib 0.11 in addition to 0.12+. `12350` by [Joel Nothman](https://joelnothman.com/) and [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Make sure to avoid raising `FutureWarning` when calling `np.vstack` with numpy 1.16 and later (use list comprehensions instead of generator expressions in many locations of the scikit-learn code base). `12467` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Removed all mentions of `sklearn.externals.joblib`, and deprecated joblib methods exposed in `sklearn.utils`, except for <span class="title-ref">utils.parallel\_backend</span> and <span class="title-ref">utils.register\_parallel\_backend</span>, which allow users to configure parallel computation in scikit-learn. Other functionalities are part of [joblib](https://joblib.readthedocs.io/). package and should be used directly, by installing it. The goal of this change is to prepare for unvendoring joblib in future version of scikit-learn. `12345` by `Thomas Moreau <tomMoral>`

### Code and Documentation Contributors

With thanks to:

^\_\_^, Adrin Jalali, Andrea Navarrete, Andreas Mueller, bauks, BenjaStudio, Cheuk Ting Ho, Connossor, Corey Levinson, Dan Stine, daten-kieker, Denis Kataev, Dillon Gardner, Dmitry Vukolov, Dougal J. Sutherland, Edward J Brown, Eric Chang, Federico Caselli, Gabriel Marzinotto, Gael Varoquaux, GauravAhlawat, Gustavo De Mari Pereira, Hanmin Qin, haroldfox, JackLangerman, Jacopo Notarstefano, janvanrijn, jdethurens, jeremiedbb, Joel Nothman, Joris Van den Bossche, Koen, Kushal Chauhan, Lee Yi Jie Joel, Lily Xiong, mail-liam, Mark Hannel, melsyt, Ming Li, Nicholas Smith, Nicolas Hug, Nikolay Shebanov, Oleksandr Pavlyk, Olivier Grisel, Peter Hausamann, Pierre Glaser, Pulkit Maloo, Quentin Batista, Radostin Stoyanov, Ramil Nugmanov, Rebekah Kim, Reshama Shaikh, Rohan Singh, Roman Feldbauer, Roman Yurchak, Roopam Sharma, Sam Waterbury, Scott Lowe, Sebastian Raschka, Stephen Tierney, SylvainLan, TakingItCasual, Thomas Fan, Thomas Moreau, Tom Dupré la Tour, Tulio Casagrande, Utkarsh Upadhyay, Xing Han Lu, Yaroslav Halchenko, Zach Miller

## Version 0.20.0

**September 25, 2018**

This release packs in a mountain of bug fixes, features and enhancements for the Scikit-learn library, and improvements to the documentation and examples. Thanks to our contributors\!

This release is dedicated to the memory of Raghav Rajagopalan.

### Highlights

We have tried to improve our support for common data-science use-cases including missing values, categorical variables, heterogeneous data, and features/targets with unusual distributions. Missing values in features, represented by NaNs, are now accepted in column-wise preprocessing such as scalers. Each feature is fitted disregarding NaNs, and data containing NaNs can be transformed. The new `sklearn.impute` module provides estimators for learning despite missing data.

<span class="title-ref">\~compose.ColumnTransformer</span> handles the case where different features or columns of a pandas.DataFrame need different preprocessing. String or pandas Categorical columns can now be encoded with <span class="title-ref">\~preprocessing.OneHotEncoder</span> or <span class="title-ref">\~preprocessing.OrdinalEncoder</span>.

<span class="title-ref">\~compose.TransformedTargetRegressor</span> helps when the regression target needs to be transformed to be modeled. <span class="title-ref">\~preprocessing.PowerTransformer</span> and <span class="title-ref">\~preprocessing.KBinsDiscretizer</span> join <span class="title-ref">\~preprocessing.QuantileTransformer</span> as non-linear transformations.

Beyond this, we have added `sample_weight` support to several estimators (including <span class="title-ref">\~cluster.KMeans</span>, <span class="title-ref">\~linear\_model.BayesianRidge</span> and <span class="title-ref">\~neighbors.KernelDensity</span>) and improved stopping criteria in others (including <span class="title-ref">\~neural\_network.MLPRegressor</span>, <span class="title-ref">\~ensemble.GradientBoostingRegressor</span> and <span class="title-ref">\~linear\_model.SGDRegressor</span>).

This release is also the first to be accompanied by a \[glossary\](\#glossary) developed by [Joel Nothman](https://joelnothman.com/). The glossary is a reference resource to help users and contributors become familiar with the terminology and conventions used in Scikit-learn.

Sorry if your contribution didn't make it into the highlights. There's a lot here...

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - <span class="title-ref">cluster.MeanShift</span> (bug fix)
  - <span class="title-ref">decomposition.IncrementalPCA</span> in Python 2 (bug fix)
  - <span class="title-ref">decomposition.SparsePCA</span> (bug fix)
  - <span class="title-ref">ensemble.GradientBoostingClassifier</span> (bug fix affecting feature importances)
  - <span class="title-ref">isotonic.IsotonicRegression</span> (bug fix)
  - <span class="title-ref">linear\_model.ARDRegression</span> (bug fix)
  - <span class="title-ref">linear\_model.LogisticRegressionCV</span> (bug fix)
  - <span class="title-ref">linear\_model.OrthogonalMatchingPursuit</span> (bug fix)
  - <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span> (bug fix)
  - <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> (bug fix)
  - <span class="title-ref">linear\_model.Perceptron</span> (bug fix)
  - <span class="title-ref">linear\_model.SGDClassifier</span> (bug fix)
  - <span class="title-ref">linear\_model.SGDRegressor</span> (bug fix)
  - <span class="title-ref">metrics.roc\_auc\_score</span> (bug fix)
  - <span class="title-ref">metrics.roc\_curve</span> (bug fix)
  - <span class="title-ref">neural\_network.BaseMultilayerPerceptron</span> (bug fix)
  - <span class="title-ref">neural\_network.MLPClassifier</span> (bug fix)
  - <span class="title-ref">neural\_network.MLPRegressor</span> (bug fix)
  - The v0.19.0 release notes failed to mention a backwards incompatibility with <span class="title-ref">model\_selection.StratifiedKFold</span> when `shuffle=True` due to `7823`.

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Known Major Bugs

  - `11924`: <span class="title-ref">linear\_model.LogisticRegressionCV</span> with <span class="title-ref">solver='lbfgs'</span> and <span class="title-ref">multi\_class='multinomial'</span> may be non-deterministic or otherwise broken on macOS. This appears to be the case on Travis CI servers, but has not been confirmed on personal MacBooks\! This issue has been present in previous releases.
  - `9354`: <span class="title-ref">metrics.pairwise.euclidean\_distances</span> (which is used several times throughout the library) gives results with poor precision, which particularly affects its use with 32-bit float inputs. This became more problematic in versions 0.18 and 0.19 when some algorithms were changed to avoid casting 32-bit data into 64-bit.

### Changelog

Support for Python 3.3 has been officially dropped.

#### `sklearn.cluster`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">cluster.AgglomerativeClustering</span> now supports Single Linkage clustering via `linkage='single'`. `9372` by `Leland
    McInnes <lmcinnes>` and `Steve Astels <sastels>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">cluster.KMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span> now support sample weights via new parameter `sample_weight` in `fit` function. `10933` by `Johannes Hansen <jnhansen>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.KMeans</span>, <span class="title-ref">cluster.MiniBatchKMeans</span> and <span class="title-ref">cluster.k\_means</span> passed with `algorithm='full'` now enforces row-major ordering, improving runtime. `10471` by `Gaurav Dhingra <gxyd>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.DBSCAN</span> now is parallelized according to `n_jobs` regardless of `algorithm`. `8003` by `Joël Billaud <recamshak>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.KMeans</span> now gives a warning if the number of distinct clusters found is smaller than `n_clusters`. This may occur when the number of distinct points in the data set is actually smaller than the number of cluster one is looking for. `10059` by `Christian Braune <christianbraune79>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where the `fit` method of <span class="title-ref">cluster.AffinityPropagation</span> stored cluster centers as 3d array instead of 2d array in case of non-convergence. For the same class, fixed undefined and arbitrary behavior in case of training data where all samples had equal similarity. `9612`. By `Jonatan Samoocha <jsamoocha>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.spectral\_clustering</span> where the normalization of the spectrum was using a division instead of a multiplication. `8129` by `Jan Margeta <jmargeta>`, `Guillaume Lemaitre <glemaitre>`, and `Devansh D. <devanshdalal>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.k\_means\_elkan</span> where the returned `iteration` was 1 less than the correct value. Also added the missing `n_iter_` attribute in the docstring of <span class="title-ref">cluster.KMeans</span>. `11353` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.mean\_shift</span> where the assigned labels were not deterministic if there were multiple clusters with the same intensities. `11901` by `Adrin Jalali <adrinjalali>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `pooling_func` unused parameter in <span class="title-ref">cluster.AgglomerativeClustering</span>. `9875` by `Kumar Ashutosh <thechargedneutron>`.

#### `sklearn.compose`

  - New module.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">compose.ColumnTransformer</span>, which allows to apply different transformers to different columns of arrays or pandas DataFrames. `9012` by [Andreas Müller](https://amueller.github.io/) and [Joris Van den Bossche](https://github.com/jorisvandenbossche), and `11315` by `Thomas Fan <thomasjpfan>`.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added the <span class="title-ref">compose.TransformedTargetRegressor</span> which transforms the target y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. `9041` by [Andreas Müller](https://amueller.github.io/) and `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.covariance`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Runtime improvements to <span class="title-ref">covariance.GraphicalLasso</span>. `9858` by `Steven Brown <stevendbrown>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">covariance.graph\_lasso</span>, <span class="title-ref">covariance.GraphLasso</span> and <span class="title-ref">covariance.GraphLassoCV</span> have been renamed to <span class="title-ref">covariance.graphical\_lasso</span>, <span class="title-ref">covariance.GraphicalLasso</span> and <span class="title-ref">covariance.GraphicalLassoCV</span> respectively and will be removed in version 0.22. `9993` by `Artiem Krinitsyn <artiemq>`

#### `sklearn.datasets`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">datasets.fetch\_openml</span> to fetch datasets from [OpenML](https://openml.org). OpenML is a free, open data sharing platform and will be used instead of mldata as it provides better service availability. `9908` by [Andreas Müller](https://amueller.github.io/) and `Jan N. van Rijn <janvanrijn>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` In <span class="title-ref">datasets.make\_blobs</span>, one can now pass a list to the `n_samples` parameter to indicate the number of samples to generate per cluster. `8617` by `Maskani Filali Mohamed <maskani-moh>` and `Konstantinos Katrioplas <kkatrio>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add `filename` attribute to `sklearn.datasets` that have a CSV file. `9101` by `alex-33 <alex-33>` and `Maskani Filali Mohamed <maskani-moh>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` `return_X_y` parameter has been added to several dataset loaders. `10774` by `Chris Catalfo <ccatalfo>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">datasets.load\_boston</span> which had a wrong data point. `10795` by `Takeshi Yoshizawa <tarcusx>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">datasets.load\_iris</span> which had two wrong data points. `11082` by `Sadhana Srinivasan <rotuna>` and `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">datasets.fetch\_kddcup99</span>, where data were not properly shuffled. `9731` by [Nicolas Goix](https://ngoix.github.io/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">datasets.make\_circles</span>, where no odd number of data points could be generated. `10045` by `Christian Braune
    <christianbraune79>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecated <span class="title-ref">sklearn.datasets.fetch\_mldata</span> to be removed in version 0.22. mldata.org is no longer operational. Until removal it will remain possible to load cached datasets. `11466` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.decomposition`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">decomposition.dict\_learning</span> functions and models now support positivity constraints. This applies to the dictionary and sparse code. `6374` by `John Kirkham <jakirkham>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.SparsePCA</span> now exposes `normalize_components`. When set to True, the train and test data are centered with the train mean respectively during the fit phase and the transform phase. This fixes the behavior of SparsePCA. When set to False, which is the default, the previous abnormal behaviour still holds. The False value is for backward compatibility and should not be used. `11585` by `Ivan Panico <FollowKenny>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Efficiency improvements in <span class="title-ref">decomposition.dict\_learning</span>. `11420` and others by `John Kirkham <jakirkham>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix for uninformative error in \`decomposition.IncrementalPCA\`: now an error is raised if the number of components is larger than the chosen batch size. The `n_components=None` case was adapted accordingly. `6452`. By `Wally Gauze <wallygauze>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where the `partial_fit` method of <span class="title-ref">decomposition.IncrementalPCA</span> used integer division instead of float division on Python 2. `9492` by `James Bourbeau <jrbourbeau>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` In <span class="title-ref">decomposition.PCA</span> selecting a n\_components parameter greater than the number of samples now raises an error. Similarly, the `n_components=None` case now selects the minimum of `n_samples` and `n_features`. `8484` by `Wally Gauze <wallygauze>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.PCA</span> where users will get unexpected error with large datasets when `n_components='mle'` on Python 3 versions. `9886` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an underflow in calculating KL-divergence for <span class="title-ref">decomposition.NMF</span> `10142` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.SparseCoder</span> when running OMP sparse coding in parallel using read-only memory mapped datastructures. `5956` by `Vighnesh Birodkar <vighneshbirodkar>` and `Olivier Grisel <ogrisel>`.

#### `sklearn.discriminant_analysis`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Memory usage improvement for <span class="title-ref">\_class\_means</span> and <span class="title-ref">\_class\_cov</span> in `sklearn.discriminant_analysis`. `10898` by `Nanxin Chen <bobchennan>`.

#### `sklearn.dummy`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">dummy.DummyRegressor</span> now has a `return_std` option in its `predict` method. The returned standard deviations will be zeros.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span> now only require X to be an object with finite length or shape. `9832` by `Vrishank Bhardwaj <vrishank97>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span> can now be scored without supplying test samples. `11951` by `Rüdiger Busche <JarnoRFB>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.BaggingRegressor</span> and <span class="title-ref">ensemble.BaggingClassifier</span> can now be fit with missing/non-finite values in X and/or multi-output Y to support wrapping pipelines that perform their own imputation. `9707` by `Jimmy Wan <jimmywan>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> now support early stopping via `n_iter_no_change`, `validation_fraction` and `tol`. `7071` by [Raghav RV](https://github.com/raghavrv)
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added `named_estimators_` parameter in <span class="title-ref">ensemble.VotingClassifier</span> to access fitted estimators. `9157` by `Herilalaina Rakotoarison <herilalaina>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug when fitting <span class="title-ref">ensemble.GradientBoostingClassifier</span> or <span class="title-ref">ensemble.GradientBoostingRegressor</span> with `warm_start=True` which previously raised a segmentation fault due to a non-conversion of CSC matrix into CSR format expected by `decision_function`. Similarly, Fortran-ordered arrays are converted to C-ordered arrays in the dense case. `9991` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> to have feature importances summed and then normalized, rather than normalizing on a per-tree basis. The previous behavior over-weighted the Gini importance of features that appear in later stages. This issue only affected feature importances. `11176` by `Gil Forsyth <gforsyth>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of the `n_estimators` parameter of <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>, and <span class="title-ref">ensemble.RandomTreesEmbedding</span> will change from 10 in version 0.20 to 100 in 0.22. A FutureWarning is raised when the default value is used. `11542` by `Anna Ayzenshtat <annaayzenshtat>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Classes derived from <span class="title-ref">ensemble.BaseBagging</span>. The attribute `estimators_samples_` will return a list of arrays containing the indices selected for each bootstrap instead of a list of arrays containing the mask of the samples selected for each bootstrap. Indices allows to repeat samples while mask does not allow this functionality. `9524` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.BaseBagging</span> where one could not deterministically reproduce `fit` result using the object attributes when `random_state` is set. `9723` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Enable the call to <span class="title-ref">get\_feature\_names</span> in unfitted <span class="title-ref">feature\_extraction.text.CountVectorizer</span> initialized with a vocabulary. `10908` by `Mohamed Maskani <maskani-moh>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` `idf_` can now be set on a <span class="title-ref">feature\_extraction.text.TfidfTransformer</span>. `10899` by `Sergey Melderis <serega>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.image.extract\_patches\_2d</span> which would throw an exception if `max_patches` was greater than or equal to the number of all possible patches rather than simply returning the number of possible patches. `10101` by `Varun Agrawal <varunagrawal>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.text.CountVectorizer</span>, <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span>, <span class="title-ref">feature\_extraction.text.HashingVectorizer</span> to support 64 bit sparse array indexing necessary to process large datasets with more than 2·10⁹ tokens (words or n-grams). `9147` by `Claes-Fredrik Mannby <mannby>` and [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span> which was ignoring the parameter `dtype`. In addition, <span class="title-ref">feature\_extraction.text.TfidfTransformer</span> will preserve `dtype` for floating and raise a warning if `dtype` requested is integer. `10441` by `Mayur Kulkarni <maykulkarni>` and `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added select K best features functionality to <span class="title-ref">feature\_selection.SelectFromModel</span>. `6689` by `Nihar Sheth <nsheth12>` and `Quazi Rahman <qmaruf>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added `min_features_to_select` parameter to <span class="title-ref">feature\_selection.RFECV</span> to bound evaluated features counts. `11293` by `Brent Yi <brentyi>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">feature\_selection.RFECV</span>'s fit method now supports `groups`. `9656` by `Adam Greenhall <adamgreenhall>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed computation of `n_features_to_compute` for edge case with tied CV scores in <span class="title-ref">feature\_selection.RFECV</span>. `9222` by `Nick Hoh <nickypie>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` In <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>, method `predict` is faster when using `return_std=True` in particular more when called several times in a row. `9234` by `andrewww <andrewww>` and `Minghui Liu <minghui-liu>`.

#### `sklearn.impute`

  - New module, adopting `preprocessing.Imputer` as <span class="title-ref">impute.SimpleImputer</span> with minor changes (see under preprocessing below).
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">impute.MissingIndicator</span> which generates a binary indicator for missing values. `8075` by `Maniteja Nandana
    <maniteja123>` and `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The <span class="title-ref">impute.SimpleImputer</span> has a new strategy, `'constant'`, to complete missing values with a fixed one, given by the `fill_value` parameter. This strategy supports numeric and non-numeric data, and so does the `'most_frequent'` strategy now. `11211` by `Jeremie du Boisberranger <jeremiedbb>`.

#### `sklearn.isotonic`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">isotonic.IsotonicRegression</span> which incorrectly combined weights when fitting a model to data involving points with identical X values. `9484` by `Dallas Card <dallascard>`

#### `sklearn.linear_model`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> and <span class="title-ref">linear\_model.Perceptron</span> now expose `early_stopping`, `validation_fraction` and `n_iter_no_change` parameters, to stop optimization monitoring the score on a validation set. A new learning rate `"adaptive"` strategy divides the learning rate by 5 each time `n_iter_no_change` consecutive epochs fail to improve the model. `9043` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">sample\_weight</span> parameter to the fit method of <span class="title-ref">linear\_model.BayesianRidge</span> for weighted linear regression. `10112` by `Peter St. John <pstjohn>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">logistic.logistic\_regression\_path</span> to ensure that the returned coefficients are correct when `multiclass='multinomial'`. Previously, some of the coefficients would override each other, leading to incorrect results in <span class="title-ref">linear\_model.LogisticRegressionCV</span>. `11724` by `Nicolas Hug <NicolasHug>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LogisticRegression</span> where when using the parameter `multi_class='multinomial'`, the `predict_proba` method was returning incorrect probabilities in the case of binary outcomes. `9939` by `Roger Westover <rwolst>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LogisticRegressionCV</span> where the `score` method always computes accuracy, not the metric given by the `scoring` parameter. `10998` by `Thomas Fan <thomasjpfan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LogisticRegressionCV</span> where the 'ovr' strategy was always used to compute cross-validation scores in the multiclass setting, even if `'multinomial'` was set. `8720` by `William de Vazelhes <wdevazelhes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.OrthogonalMatchingPursuit</span> that was broken when setting `normalize=False`. `10071` by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.ARDRegression</span> which caused incorrectly updated estimates for the standard deviation and the coefficients. `10153` by `Jörg Döpfert <jdoepfert>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.ARDRegression</span> and <span class="title-ref">linear\_model.BayesianRidge</span> which caused NaN predictions when fitted with a constant target. `10095` by `Jörg Döpfert <jdoepfert>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.RidgeClassifierCV</span> where the parameter `store_cv_values` was not implemented though it was documented in `cv_values` as a way to set up the storage of cross-validation values for different alphas. `10297` by `Mabel Villalba-Jiménez <mabelvj>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.ElasticNet</span> which caused the input to be overridden when using parameter `copy_X=True` and `check_input=False`. `10581` by `Yacine Mazari <ymazari>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">sklearn.linear\_model.Lasso</span> where the coefficient had wrong shape when `fit_intercept=False`. `10687` by `Martin Hahn <martin-hahn>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">sklearn.linear\_model.LogisticRegression</span> where the `multi_class='multinomial'` with binary output `with warm_start=True` `10836` by `Aishwarya Srinivasan <aishgrt1>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.RidgeCV</span> where using integer `alphas` raised an error. `10397` by `Mabel Villalba-Jiménez <mabelvj>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed condition triggering gap computation in <span class="title-ref">linear\_model.Lasso</span> and <span class="title-ref">linear\_model.ElasticNet</span> when working with sparse matrices. `10992` by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span> and <span class="title-ref">linear\_model.Perceptron</span>, where the stopping criterion was stopping the algorithm before convergence. A parameter `n_iter_no_change` was added and set by default to 5. Previous behavior is equivalent to setting the parameter to 1. `9043` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where liblinear and libsvm-based estimators would segfault if passed a scipy.sparse matrix with 64-bit indices. They now raise a ValueError. `11327` by `Karan Dhingra <kdhingra307>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default values of the `solver` and `multi_class` parameters of <span class="title-ref">linear\_model.LogisticRegression</span> will change respectively from `'liblinear'` and `'ovr'` in version 0.20 to `'lbfgs'` and `'auto'` in version 0.22. A FutureWarning is raised when the default values are used. `11905` by [Tom Dupre la Tour](https://github.com/TomDLT) and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `positive=True` option in <span class="title-ref">linear\_model.Lars</span> as the underlying implementation is broken. Use <span class="title-ref">linear\_model.Lasso</span> instead. `9837` by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `n_iter_` may vary from previous releases in <span class="title-ref">linear\_model.LogisticRegression</span> with `solver='lbfgs'` and <span class="title-ref">linear\_model.HuberRegressor</span>. For Scipy \<= 1.0.0, the optimizer could perform more than the requested maximum number of iterations. Now both estimators will report at most `max_iter` iterations even if more were performed. `10723` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.manifold`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Speed improvements for both 'exact' and 'barnes\_hut' methods in <span class="title-ref">manifold.TSNE</span>. `10593` and `10610` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Support sparse input in <span class="title-ref">manifold.Isomap.fit</span>. `8554` by `Leland McInnes <lmcinnes>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">manifold.t\_sne.trustworthiness</span> accepts metrics other than Euclidean. `9775` by `William de Vazelhes <wdevazelhes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">manifold.spectral\_embedding</span> where the normalization of the spectrum was using a division instead of a multiplication. `8129` by `Jan Margeta <jmargeta>`, `Guillaume Lemaitre <glemaitre>`, and `Devansh D.
    <devanshdalal>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Deprecate `precomputed` parameter in function <span class="title-ref">manifold.t\_sne.trustworthiness</span>. Instead, the new parameter `metric` should be used with any compatible metric including 'precomputed', in which case the input matrix `X` should be a matrix of pairwise distances or squared distances. `9775` by `William de Vazelhes
    <wdevazelhes>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `precomputed` parameter in function <span class="title-ref">manifold.t\_sne.trustworthiness</span>. Instead, the new parameter `metric` should be used with any compatible metric including 'precomputed', in which case the input matrix `X` should be a matrix of pairwise distances or squared distances. `9775` by `William de Vazelhes <wdevazelhes>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added the <span class="title-ref">metrics.davies\_bouldin\_score</span> metric for evaluation of clustering models without a ground truth. `10827` by `Luis Osa <logc>`.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added the <span class="title-ref">metrics.balanced\_accuracy\_score</span> metric and a corresponding `'balanced_accuracy'` scorer for binary and multiclass classification. `8066` by `xyguo` and `Aman Dalmia
    <dalmia>`, and `10587` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Partial AUC is available via `max_fpr` parameter in <span class="title-ref">metrics.roc\_auc\_score</span>. `3840` by `Alexander Niederbühl <Alexander-N>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A scorer based on <span class="title-ref">metrics.brier\_score\_loss</span> is also available. `9521` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added control over the normalization in <span class="title-ref">metrics.normalized\_mutual\_info\_score</span> and <span class="title-ref">metrics.adjusted\_mutual\_info\_score</span> via the `average_method` parameter. In version 0.22, the default normalizer for each will become the *arithmetic* mean of the entropies of each clustering. `11124` by `Arya McCarthy <aryamccarthy>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added `output_dict` parameter in <span class="title-ref">metrics.classification\_report</span> to return classification statistics as dictionary. `11160` by `Dan Barkhorn <danielbarkhorn>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.classification\_report</span> now reports all applicable averages on the given data, including micro, macro and weighted average as well as samples average for multilabel data. `11679` by `Alexander Pacha <apacha>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.average\_precision\_score</span> now supports binary `y_true` other than `{0, 1}` or `{-1, 1}` through `pos_label` parameter. `9980` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.label\_ranking\_average\_precision\_score</span> now supports `sample_weight`. `10845` by `Jose Perez-Parras Toledano <jopepato>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add `dense_output` parameter to <span class="title-ref">metrics.pairwise.linear\_kernel</span>. When False and both inputs are sparse, will return a sparse matrix. `10999` by `Taylor G Smith <tgsmith61591>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">metrics.silhouette\_score</span> and <span class="title-ref">metrics.silhouette\_samples</span> are more memory efficient and run faster. This avoids some reported freezes and MemoryErrors. `11135` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.precision\_recall\_fscore\_support</span> when truncated <span class="title-ref">range(n\_labels)</span> is passed as value for <span class="title-ref">labels</span>. `10377` by `Gaurav Dhingra <gxyd>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug due to floating point error in <span class="title-ref">metrics.roc\_auc\_score</span> with non-integer sample weights. `9786` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">metrics.roc\_curve</span> sometimes starts on y-axis instead of (0, 0), which is inconsistent with the document and other implementations. Note that this will not influence the result from <span class="title-ref">metrics.roc\_auc\_score</span> `10093` by `alexryndin
    <alexryndin>` and `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in <span class="title-ref">metrics.mutual\_info\_score</span>. `9772` by `Kumar Ashutosh <thechargedneutron>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">metrics.average\_precision\_score</span> will sometimes return `nan` when `sample_weight` contains 0. `9980` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.fowlkes\_mallows\_score</span> to avoid integer overflow. Casted return value of <span class="title-ref">contingency\_matrix</span> to <span class="title-ref">int64</span> and computed product of square roots rather than square root of product. `9515` by `Alan Liddell <aliddell>` and `Manh Dao <manhdao>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `reorder` parameter in <span class="title-ref">metrics.auc</span> as it's no longer required for <span class="title-ref">metrics.roc\_auc\_score</span>. Moreover using `reorder=True` can hide bugs due to floating point error in the input. `9851` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` In <span class="title-ref">metrics.normalized\_mutual\_info\_score</span> and <span class="title-ref">metrics.adjusted\_mutual\_info\_score</span>, warn that `average_method` will have a new default value. In version 0.22, the default normalizer for each will become the *arithmetic* mean of the entropies of each clustering. Currently, <span class="title-ref">metrics.normalized\_mutual\_info\_score</span> uses the default of `average_method='geometric'`, and <span class="title-ref">metrics.adjusted\_mutual\_info\_score</span> uses the default of `average_method='max'` to match their behaviors in version 0.19. `11124` by `Arya McCarthy <aryamccarthy>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `batch_size` parameter to <span class="title-ref">metrics.pairwise\_distances\_argmin\_min</span> and <span class="title-ref">metrics.pairwise\_distances\_argmin</span> is deprecated to be removed in v0.22. It no longer has any effect, as batch size is determined by global `working_memory` config. See \[working\_memory\](\#working\_memory). `10280` by [Joel Nothman](https://joelnothman.com/) and `Aman Dalmia <dalmia>`.

#### `sklearn.mixture`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added function `fit_predict` to <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.GaussianMixture</span>, which is essentially equivalent to calling `fit` and `predict`. `10336` by `Shu Haoran
    <haoranShu>` and `Andrew Peng <Andrew-peng>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">mixture.BaseMixture</span> where the reported <span class="title-ref">n\_iter\_</span> was missing an iteration. It affected <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span>. `10740` by `Erich
    Schubert <kno10>` and `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">mixture.BaseMixture</span> and its subclasses <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span> where the `lower_bound_` was not the max lower bound across all initializations (when `n_init > 1`), but just the lower bound of the last initialization. `10869` by `Aurélien Géron <ageron>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">return\_estimator</span> parameter in <span class="title-ref">model\_selection.cross\_validate</span> to return estimators fitted on each split. `9686` by `Aurélien Bellet <bellet>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` New `refit_time_` attribute will be stored in <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> if `refit` is set to `True`. This will allow measuring the complete time it takes to perform hyperparameter optimization and refitting the best model on the whole dataset. `11310` by `Matthias Feurer <mfeurer>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Expose <span class="title-ref">error\_score</span> parameter in <span class="title-ref">model\_selection.cross\_validate</span>, <span class="title-ref">model\_selection.cross\_val\_score</span>, <span class="title-ref">model\_selection.learning\_curve</span> and <span class="title-ref">model\_selection.validation\_curve</span> to control the behavior triggered when an error occurs in <span class="title-ref">model\_selection.\_fit\_and\_score</span>. `11576` by `Samuel O. Ronsin <samronsin>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">BaseSearchCV</span> now has an experimental, private interface to support customized parameter search strategies, through its `_run_search` method. See the implementations in <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> and please provide feedback if you use this. Note that we do not assure the stability of this API beyond version 0.20. `9599` by [Joel Nothman](https://joelnothman.com/)
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add improved error message in <span class="title-ref">model\_selection.cross\_val\_score</span> when multiple metrics are passed in `scoring` keyword. `11006` by `Ming Li <minggli>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default number of cross-validation folds `cv` and the default number of splits `n_splits` in the <span class="title-ref">model\_selection.KFold</span>-like splitters will change from 3 to 5 in 0.22 as 3-fold has a lot of variance. `11557` by `Alexandre Boucaud <aboucaud>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default of `iid` parameter of <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> will change from `True` to `False` in version 0.22 to correspond to the standard definition of cross-validation, and the parameter will be removed in version 0.24 altogether. This parameter is of greatest practical significance where the sizes of different test sets in cross-validation were very unequal, i.e. in group-based CV strategies. `9085` by `Laurent Direr <ldirer>` and [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of the `error_score` parameter in <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> will change to `np.NaN` in version 0.22. `10677` by `Kirill Zhdanovich <Zhdanovich>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Changed ValueError exception raised in <span class="title-ref">model\_selection.ParameterSampler</span> to a UserWarning for case where the class is instantiated with a greater value of `n_iter` than the total space of parameters in the parameter grid. `n_iter` now acts as an upper bound on iterations. `10982` by `Juliet Lawton <julietcl>`
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Invalid input for <span class="title-ref">model\_selection.ParameterGrid</span> now raises TypeError. `10928` by `Solutus Immensus <solutusimmensus>`

#### `sklearn.multioutput`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">multioutput.RegressorChain</span> for multi-target regression. `9257` by `Kumar Ashutosh <thechargedneutron>`.

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">naive\_bayes.ComplementNB</span>, which implements the Complement Naive Bayes classifier described in Rennie et al. (2003). `8190` by `Michael A. Alcorn <airalcorn2>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">var\_smoothing</span> parameter in <span class="title-ref">naive\_bayes.GaussianNB</span> to give a precise control over variances calculation. `9681` by `Dmitry Mottl <Mottl>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">naive\_bayes.GaussianNB</span> which incorrectly raised error for prior list which summed to 1. `10005` by `Gaurav Dhingra <gxyd>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">naive\_bayes.MultinomialNB</span> which did not accept vector valued pseudocounts (alpha). `10346` by `Tobias Madsen <TobiasMadsen>`

#### `sklearn.neighbors`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">neighbors.RadiusNeighborsRegressor</span> and <span class="title-ref">neighbors.RadiusNeighborsClassifier</span> are now parallelized according to `n_jobs` regardless of `algorithm`. `10887` by `Joël Billaud <recamshak>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` `sklearn.neighbors` query methods are now more memory efficient when `algorithm='brute'`. `11136` by [Joel Nothman](https://joelnothman.com/) and `Aman Dalmia <dalmia>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add `sample_weight` parameter to the fit method of <span class="title-ref">neighbors.KernelDensity</span> to enable weighting in kernel density estimation. `4394` by `Samuel O. Ronsin <samronsin>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Novelty detection with \`neighbors.LocalOutlierFactor\`: Add a `novelty` parameter to <span class="title-ref">neighbors.LocalOutlierFactor</span>. When `novelty` is set to True, <span class="title-ref">neighbors.LocalOutlierFactor</span> can then be used for novelty detection, i.e. predict on new unseen data. Available prediction methods are `predict`, `decision_function` and `score_samples`. By default, `novelty` is set to `False`, and only the `fit_predict` method is available. By `Albert Thomas <albertcthomas>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neighbors.NearestNeighbors</span> where fitting a NearestNeighbors model fails when a) the distance metric used is a callable and b) the input to the NearestNeighbors model is sparse. `9579` by `Thomas Kober <tttthomasssss>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug so `predict` in <span class="title-ref">neighbors.RadiusNeighborsRegressor</span> can handle empty neighbor set when using non uniform weights. Also raises a new warning when no neighbors are found for samples. `9655` by `Andreas Bjerre-Nielsen
    <abjer>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Fixed a bug in `KDTree` construction that results in faster construction and querying times. `11556` by `Jake VanderPlas <jakevdp>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neighbors.KDTree</span> and <span class="title-ref">neighbors.BallTree</span> where pickled tree objects would change their type to the super class <span class="title-ref">BinaryTree</span>. `11774` by `Nicolas Hug <NicolasHug>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">n\_iter\_no\_change</span> parameter in <span class="title-ref">neural\_network.BaseMultilayerPerceptron</span>, <span class="title-ref">neural\_network.MLPRegressor</span>, and <span class="title-ref">neural\_network.MLPClassifier</span> to give control over maximum number of epochs to not meet `tol` improvement. `9456` by `Nicholas Nadeau <nnadeau>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neural\_network.BaseMultilayerPerceptron</span>, <span class="title-ref">neural\_network.MLPRegressor</span>, and <span class="title-ref">neural\_network.MLPClassifier</span> with new `n_iter_no_change` parameter now at 10 from previously hardcoded 2. `9456` by `Nicholas Nadeau <nnadeau>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neural\_network.MLPRegressor</span> where fitting quit unexpectedly early due to local minima or fluctuations. `9456` by `Nicholas Nadeau <nnadeau>`

#### `sklearn.pipeline`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The `predict` method of <span class="title-ref">pipeline.Pipeline</span> now passes keyword arguments on to the pipeline's last estimator, enabling the use of parameters such as `return_std` in a pipeline with caution. `9304` by `Breno Freitas <brenolf>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">pipeline.FeatureUnion</span> now supports `'drop'` as a transformer to drop features. `11144` by `Thomas Fan <thomasjpfan>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Expanded <span class="title-ref">preprocessing.OneHotEncoder</span> to allow to encode categorical string features as a numeric array using a one-hot (or dummy) encoding scheme, and added <span class="title-ref">preprocessing.OrdinalEncoder</span> to convert to ordinal integers. Those two classes now handle encoding of all feature types (also handles string-valued features) and derives the categories based on the unique values in the features instead of the maximum value in the features. `9151` and `10521` by `Vighnesh
    Birodkar <vighneshbirodkar>` and [Joris Van den Bossche](https://github.com/jorisvandenbossche).
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">preprocessing.KBinsDiscretizer</span> for turning continuous features into categorical or one-hot encoded features. `7668`, `9647`, `10195`, `10192`, `11272`, `11467` and `11505`. by `Henry Lin <hlin117>`, [Hanmin Qin](https://github.com/qinhanmin2014), [Tom Dupre la Tour](https://github.com/TomDLT) and `Giovanni Giuseppe Costa <ggc87>`.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">preprocessing.PowerTransformer</span>, which implements the Yeo-Johnson and Box-Cox power transformations. Power transformations try to find a set of feature-wise parametric transformations to approximately map data to a Gaussian distribution centered at zero and with unit variance. This is useful as a variance-stabilizing transformation in situations where normality and homoscedasticity are desirable. `10210` by `Eric Chang <chang>` and `Maniteja
    Nandana <maniteja123>`, and `11520` by `Nicolas Hug
    <nicolashug>`.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` NaN values are ignored and handled in the following preprocessing methods: <span class="title-ref">preprocessing.MaxAbsScaler</span>, <span class="title-ref">preprocessing.MinMaxScaler</span>, <span class="title-ref">preprocessing.RobustScaler</span>, <span class="title-ref">preprocessing.StandardScaler</span>, <span class="title-ref">preprocessing.PowerTransformer</span>, <span class="title-ref">preprocessing.QuantileTransformer</span> classes and <span class="title-ref">preprocessing.maxabs\_scale</span>, <span class="title-ref">preprocessing.minmax\_scale</span>, <span class="title-ref">preprocessing.robust\_scale</span>, <span class="title-ref">preprocessing.scale</span>, <span class="title-ref">preprocessing.power\_transform</span>, <span class="title-ref">preprocessing.quantile\_transform</span> functions respectively addressed in issues `11011`, `11005`, `11308`, `11206`, `11306`, and `10437`. By `Lucija Gregov <LucijaGregov>` and `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.PolynomialFeatures</span> now supports sparse input. `10452` by `Aman Dalmia <dalmia>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.RobustScaler</span> and <span class="title-ref">preprocessing.robust\_scale</span> can be fitted using sparse matrices. `11308` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OneHotEncoder</span> now supports the <span class="title-ref">get\_feature\_names</span> method to obtain the transformed feature names. `10181` by `Nirvan Anjirbag <Nirvan101>` and [Joris Van den Bossche](https://github.com/jorisvandenbossche).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A parameter `check_inverse` was added to <span class="title-ref">preprocessing.FunctionTransformer</span> to ensure that `func` and `inverse_func` are the inverse of each other. `9399` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The `transform` method of <span class="title-ref">sklearn.preprocessing.MultiLabelBinarizer</span> now ignores any unknown classes. A warning is raised stating the unknown classes classes found which are ignored. `10913` by `Rodrigo Agundez <rragundez>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bugs in <span class="title-ref">preprocessing.LabelEncoder</span> which would sometimes throw errors when `transform` or `inverse_transform` was called with empty arrays. `10458` by `Mayur Kulkarni <maykulkarni>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix ValueError in <span class="title-ref">preprocessing.LabelEncoder</span> when using `inverse_transform` on unseen labels. `9816` by `Charlie Newey
    <newey01c>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix bug in <span class="title-ref">preprocessing.OneHotEncoder</span> which discarded the `dtype` when returning a sparse matrix output. `11042` by `Daniel Morales <DanielMorales9>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix `fit` and `partial_fit` in <span class="title-ref">preprocessing.StandardScaler</span> in the rare case when `with_mean=False` and <span class="title-ref">with\_std=False</span> which was crashing by calling `fit` more than once and giving inconsistent results for `mean_` whether the input was a sparse or a dense matrix. `mean_` will be set to `None` with both sparse and dense inputs. `n_samples_seen_` will be also reported for both input types. `11235` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `n_values` and `categorical_features` parameters and `active_features_`, `feature_indices_` and `n_values_` attributes of <span class="title-ref">preprocessing.OneHotEncoder</span>. The `n_values` parameter can be replaced with the new `categories` parameter, and the attributes with the new `categories_` attribute. Selecting the categorical features with the `categorical_features` parameter is now better supported using the <span class="title-ref">compose.ColumnTransformer</span>. `10521` by [Joris Van den Bossche](https://github.com/jorisvandenbossche).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate <span class="title-ref">preprocessing.Imputer</span> and move the corresponding module to <span class="title-ref">impute.SimpleImputer</span>. `9726` by `Kumar Ashutosh
    <thechargedneutron>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `axis` parameter that was in <span class="title-ref">preprocessing.Imputer</span> is no longer present in <span class="title-ref">impute.SimpleImputer</span>. The behavior is equivalent to `axis=0` (impute along columns). Row-wise imputation can be performed with FunctionTransformer (e.g., `FunctionTransformer(lambda X: SimpleImputer().fit_transform(X.T).T)`). `10829` by `Guillaume Lemaitre <glemaitre>` and `Gilberto Olimpio <gilbertoolimpio>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The NaN marker for the missing values has been changed between the <span class="title-ref">preprocessing.Imputer</span> and the <span class="title-ref">impute.SimpleImputer</span>. `missing_values='NaN'` should now be `missing_values=np.nan`. `11211` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` In <span class="title-ref">preprocessing.FunctionTransformer</span>, the default of `validate` will be from `True` to `False` in 0.22. `10655` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">svm.SVC</span> where when the argument `kernel` is unicode in Python2, the `predict_proba` method was raising an unexpected TypeError given dense inputs. `10412` by `Jiongyan Zhang <qmick>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `random_state` parameter in <span class="title-ref">svm.OneClassSVM</span> as the underlying implementation is not random. `9497` by `Albert Thomas <albertcthomas>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of `gamma` parameter of <span class="title-ref">svm.SVC</span>, <span class="title-ref">\~svm.NuSVC</span>, <span class="title-ref">\~svm.SVR</span>, <span class="title-ref">\~svm.NuSVR</span>, <span class="title-ref">\~svm.OneClassSVM</span> will change from `'auto'` to `'scale'` in version 0.22 to account better for unscaled features. `8361` by `Gaurav Dhingra <gxyd>` and `Ting Neo <neokt>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Although private (and hence not assured API stability), <span class="title-ref">tree.\_criterion.ClassificationCriterion</span> and <span class="title-ref">tree.\_criterion.RegressionCriterion</span> may now be cimported and extended. `10325` by `Camil Staps <camilstaps>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">tree.BaseDecisionTree</span> with <span class="title-ref">splitter="best"</span> where split threshold could become infinite when values in X were near infinite. `10536` by `Jonathan Ohayon <Johayon>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">tree.MAE</span> to ensure sample weights are being used during the calculation of tree MAE impurity. Previous behaviour could cause suboptimal splits to be chosen since the impurity calculation considered all samples to be of equal weight importance. `11464` by `John Stott <JohnStott>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">utils.check\_array</span> and <span class="title-ref">utils.check\_X\_y</span> now have `accept_large_sparse` to control whether scipy.sparse matrices with 64-bit indices should be rejected. `11327` by `Karan Dhingra <kdhingra307>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid copying the data in <span class="title-ref">utils.check\_array</span> when the input data is a memmap (and `copy=False`). `10663` by `Arthur Mensch <arthurmensch>` and `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.check\_array</span> yield a `FutureWarning` indicating that arrays of bytes/strings will be interpreted as decimal numbers beginning in version 0.22. `10229` by `Ryan Lee <rtlee9>`

#### Multiple modules

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` More consistent outlier detection API: Add a `score_samples` method in <span class="title-ref">svm.OneClassSVM</span>, <span class="title-ref">ensemble.IsolationForest</span>, <span class="title-ref">neighbors.LocalOutlierFactor</span>, <span class="title-ref">covariance.EllipticEnvelope</span>. It allows to access raw score functions from original papers. A new `offset_` parameter allows to link `score_samples` and `decision_function` methods. The `contamination` parameter of <span class="title-ref">ensemble.IsolationForest</span> and <span class="title-ref">neighbors.LocalOutlierFactor</span> `decision_function` methods is used to define this `offset_` such that outliers (resp. inliers) have negative (resp. positive) `decision_function` values. By default, `contamination` is kept unchanged to 0.1 for a deprecation period. In 0.22, it will be set to "auto", thus using method-specific score offsets. In <span class="title-ref">covariance.EllipticEnvelope</span> `decision_function` method, the `raw_values` parameter is deprecated as the shifted Mahalanobis distance will be always returned in 0.22. `9015` by [Nicolas Goix](https://ngoix.github.io/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` A `behaviour` parameter has been introduced in <span class="title-ref">ensemble.IsolationForest</span> to ensure backward compatibility. In the old behaviour, the `decision_function` is independent of the `contamination` parameter. A threshold attribute depending on the `contamination` parameter is thus used. In the new behaviour the `decision_function` is dependent on the `contamination` parameter, in such a way that 0 becomes its natural threshold to detect outliers. Setting behaviour to "old" is deprecated and will not be possible in version 0.22. Beside, the behaviour parameter will be removed in 0.24. `11553` by [Nicolas Goix](https://ngoix.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Added convergence warning to <span class="title-ref">svm.LinearSVC</span> and <span class="title-ref">linear\_model.LogisticRegression</span> when `verbose` is set to 0. `10881` by `Alexandre Sevin <AlexandreSev>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Changed warning type from <span class="title-ref">UserWarning</span> to <span class="title-ref">exceptions.ConvergenceWarning</span> for failing convergence in <span class="title-ref">linear\_model.logistic\_regression\_path</span>, <span class="title-ref">linear\_model.RANSACRegressor</span>, <span class="title-ref">linear\_model.ridge\_regression</span>, <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>, <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span>, <span class="title-ref">decomposition.fastica</span>, <span class="title-ref">cross\_decomposition.PLSCanonical</span>, <span class="title-ref">cluster.AffinityPropagation</span>, and <span class="title-ref">cluster.Birch</span>. `10306` by `Jonathan Siebert <jotasi>`.

#### Miscellaneous

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` A new configuration parameter, `working_memory` was added to control memory consumption limits in chunked operations, such as the new <span class="title-ref">metrics.pairwise\_distances\_chunked</span>. See \[working\_memory\](\#working\_memory). `10280` by [Joel Nothman](https://joelnothman.com/) and `Aman Dalmia <dalmia>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The version of `joblib` bundled with Scikit-learn is now 0.12. This uses a new default multiprocessing implementation, named [loky](https://github.com/tomMoral/loky). While this may incur some memory and communication overhead, it should provide greater cross-platform stability than relying on Python standard library multiprocessing. `11741` by the Joblib developers, especially `Thomas Moreau <tomMoral>` and [Olivier Grisel](https://twitter.com/ogrisel).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` An environment variable to use the site joblib instead of the vendored one was added (\[environment\_variable\](\#environment\_variable)). The main API of joblib is now exposed in `sklearn.utils`. `11166` by [Gael Varoquaux](http://gael-varoquaux.info).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add almost complete PyPy 3 support. Known unsupported functionalities are <span class="title-ref">datasets.load\_svmlight\_file</span>, <span class="title-ref">feature\_extraction.FeatureHasher</span> and <span class="title-ref">feature\_extraction.text.HashingVectorizer</span>. For running on PyPy, PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+ are required. `11010` by `Ronan Lamy <rlamy>` and [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A utility method <span class="title-ref">sklearn.show\_versions()</span> was added to print out information relevant for debugging. It includes the user system, the Python executable, the version of the main libraries and BLAS binding information. `11596` by `Alexandre Boucaud <aboucaud>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug when setting parameters on meta-estimator, involving both a wrapped estimator and its parameter. `9999` by `Marcus Voss
    <marcus-voss>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where calling <span class="title-ref">sklearn.base.clone</span> was not thread safe and could result in a "pop from empty list" error. `9569` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of `n_jobs` is changed from `1` to `None` in all related functions and classes. `n_jobs=None` means `unset`. It will generally be interpreted as `n_jobs=1`, unless the current `joblib.Parallel` backend context specifies otherwise (See `Glossary <n_jobs>` for additional information). Note that this change happens immediately (i.e., without a deprecation cycle). `11741` by [Olivier Grisel](https://twitter.com/ogrisel).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in validation helpers where passing a Dask DataFrame results in an error. `12462` by `Zachariah Miller <zwmiller>`

### Changes to estimator checks

These changes mostly affect library developers.

  - Checks for transformers now apply if the estimator implements `transform`, regardless of whether it inherits from <span class="title-ref">sklearn.base.TransformerMixin</span>. `10474` by [Joel Nothman](https://joelnothman.com/).
  - Classifiers are now checked for consistency between `decision_function` and categorical predictions. `10500` by `Narine Kokhlikyan <NarineK>`.
  - Allow tests in <span class="title-ref">utils.estimator\_checks.check\_estimator</span> to test functions that accept pairwise data. `9701` by `Kyle Johnson <gkjohns>`
  - Allow <span class="title-ref">utils.estimator\_checks.check\_estimator</span> to check that there is no private settings apart from parameters during estimator initialization. `9378` by `Herilalaina Rakotoarison <herilalaina>`
  - The set of checks in <span class="title-ref">utils.estimator\_checks.check\_estimator</span> now includes a `check_set_params` test which checks that `set_params` is equivalent to passing parameters in `__init__` and warns if it encounters parameter validation. `7738` by `Alvin Chiang <absolutelyNoWarranty>`
  - Add invariance tests for clustering metrics. `8102` by `Ankita
    Sinha <anki08>` and `Guillaume Lemaitre <glemaitre>`.
  - Add `check_methods_subset_invariance` to <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span>, which checks that estimator methods are invariant if applied to a data subset. `10428` by `Jonathan Ohayon <Johayon>`
  - Add tests in <span class="title-ref">utils.estimator\_checks.check\_estimator</span> to check that an estimator can handle read-only memmap input data. `10663` by `Arthur Mensch <arthurmensch>` and `Loïc Estève <lesteve>`.
  - `check_sample_weights_pandas_series` now uses 8 rather than 6 samples to accommodate for the default number of clusters in <span class="title-ref">cluster.KMeans</span>. `10933` by `Johannes Hansen <jnhansen>`.
  - Estimators are now checked for whether `sample_weight=None` equates to `sample_weight=np.ones(...)`. `11558` by `Sergul Aydore <sergulaydore>`.

### Code and Documentation Contributors

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.19, including:

211217613, Aarshay Jain, absolutelyNoWarranty, Adam Greenhall, Adam Kleczewski, Adam Richie-Halford, adelr, AdityaDaflapurkar, Adrin Jalali, Aidan Fitzgerald, aishgrt1, Akash Shivram, Alan Liddell, Alan Yee, Albert Thomas, Alexander Lenail, Alexander-N, Alexandre Boucaud, Alexandre Gramfort, Alexandre Sevin, Alex Egg, Alvaro Perez-Diaz, Amanda, Aman Dalmia, Andreas Bjerre-Nielsen, Andreas Mueller, Andrew Peng, Angus Williams, Aniruddha Dave, annaayzenshtat, Anthony Gitter, Antonio Quinonez, Anubhav Marwaha, Arik Pamnani, Arthur Ozga, Artiem K, Arunava, Arya McCarthy, Attractadore, Aurélien Bellet, Aurélien Geron, Ayush Gupta, Balakumaran Manoharan, Bangda Sun, Barry Hart, Bastian Venthur, Ben Lawson, Benn Roth, Breno Freitas, Brent Yi, brett koonce, Caio Oliveira, Camil Staps, cclauss, Chady Kamar, Charlie Brummitt, Charlie Newey, chris, Chris, Chris Catalfo, Chris Foster, Chris Holdgraf, Christian Braune, Christian Hirsch, Christian Hogan, Christopher Jenness, Clement Joudet, cnx, cwitte, Dallas Card, Dan Barkhorn, Daniel, Daniel Ferreira, Daniel Gomez, Daniel Klevebring, Danielle Shwed, Daniel Mohns, Danil Baibak, Darius Morawiec, David Beach, David Burns, David Kirkby, David Nicholson, David Pickup, Derek, Didi Bar-Zev, diegodlh, Dillon Gardner, Dillon Niederhut, dilutedsauce, dlovell, Dmitry Mottl, Dmitry Petrov, Dor Cohen, Douglas Duhaime, Ekaterina Tuzova, Eric Chang, Eric Dean Sanchez, Erich Schubert, Eunji, Fang-Chieh Chou, FarahSaeed, felix, Félix Raimundo, fenx, filipj8, FrankHui, Franz Wompner, Freija Descamps, frsi, Gabriele Calvo, Gael Varoquaux, Gaurav Dhingra, Georgi Peev, Gil Forsyth, Giovanni Giuseppe Costa, gkevinyen5418, goncalo-rodrigues, Gryllos Prokopis, Guillaume Lemaitre, Guillaume "Vermeille" Sanchez, Gustavo De Mari Pereira, hakaa1, Hanmin Qin, Henry Lin, Hong, Honghe, Hossein Pourbozorg, Hristo, Hunan Rostomyan, iampat, Ivan PANICO, Jaewon Chung, Jake VanderPlas, jakirkham, James Bourbeau, James Malcolm, Jamie Cox, Jan Koch, Jan Margeta, Jan Schlüter, janvanrijn, Jason Wolosonovich, JC Liu, Jeb Bearer, jeremiedbb, Jimmy Wan, Jinkun Wang, Jiongyan Zhang, jjabl, jkleint, Joan Massich, Joël Billaud, Joel Nothman, Johannes Hansen, JohnStott, Jonatan Samoocha, Jonathan Ohayon, Jörg Döpfert, Joris Van den Bossche, Jose Perez-Parras Toledano, josephsalmon, jotasi, jschendel, Julian Kuhlmann, Julien Chaumond, julietcl, Justin Shenk, Karl F, Kasper Primdal Lauritzen, Katrin Leinweber, Kirill, ksemb, Kuai Yu, Kumar Ashutosh, Kyeongpil Kang, Kye Taylor, kyledrogo, Leland McInnes, Léo DS, Liam Geron, Liutong Zhou, Lizao Li, lkjcalc, Loic Esteve, louib, Luciano Viola, Lucija Gregov, Luis Osa, Luis Pedro Coelho, Luke M Craig, Luke Persola, Mabel, Mabel Villalba, Maniteja Nandana, MarkIwanchyshyn, Mark Roth, Markus Müller, MarsGuy, Martin Gubri, martin-hahn, martin-kokos, mathurinm, Matthias Feurer, Max Copeland, Mayur Kulkarni, Meghann Agarwal, Melanie Goetz, Michael A. Alcorn, Minghui Liu, Ming Li, Minh Le, Mohamed Ali Jamaoui, Mohamed Maskani, Mohammad Shahebaz, Muayyad Alsadi, Nabarun Pal, Nagarjuna Kumar, Naoya Kanai, Narendran Santhanam, NarineK, Nathaniel Saul, Nathan Suh, Nicholas Nadeau, P.Eng., AVS, Nick Hoh, Nicolas Goix, Nicolas Hug, Nicolau Werneck, nielsenmarkus11, Nihar Sheth, Nikita Titov, Nilesh Kevlani, Nirvan Anjirbag, notmatthancock, nzw, Oleksandr Pavlyk, oliblum90, Oliver Rausch, Olivier Grisel, Oren Milman, Osaid Rehman Nasir, pasbi, Patrick Fernandes, Patrick Olden, Paul Paczuski, Pedro Morales, Peter, Peter St. John, pierreablin, pietruh, Pinaki Nath Chowdhury, Piotr Szymański, Pradeep Reddy Raamana, Pravar D Mahajan, pravarmahajan, QingYing Chen, Raghav RV, Rajendra arora, RAKOTOARISON Herilalaina, Rameshwar Bhaskaran, RankyLau, Rasul Kerimov, Reiichiro Nakano, Rob, Roman Kosobrodov, Roman Yurchak, Ronan Lamy, rragundez, Rüdiger Busche, Ryan, Sachin Kelkar, Sagnik Bhattacharya, Sailesh Choyal, Sam Radhakrishnan, Sam Steingold, Samuel Bell, Samuel O. Ronsin, Saqib Nizam Shamsi, SATISH J, Saurabh Gupta, Scott Gigante, Sebastian Flennerhag, Sebastian Raschka, Sebastien Dubois, Sébastien Lerique, Sebastin Santy, Sergey Feldman, Sergey Melderis, Sergul Aydore, Shahebaz, Shalil Awaley, Shangwu Yao, Sharad Vijalapuram, Sharan Yalburgi, shenhanc78, Shivam Rastogi, Shu Haoran, siftikha, Sinclert Pérez, SolutusImmensus, Somya Anand, srajan paliwal, Sriharsha Hatwar, Sri Krishna, Stefan van der Walt, Stephen McDowell, Steven Brown, syonekura, Taehoon Lee, Takanori Hayashi, tarcusx, Taylor G Smith, theriley106, Thomas, Thomas Fan, Thomas Heavey, Tobias Madsen, tobycheese, Tom Augspurger, Tom Dupré la Tour, Tommy, Trevor Stephens, Trishnendu Ghorai, Tulio Casagrande, twosigmajab, Umar Farouk Umar, Urvang Patel, Utkarsh Upadhyay, Vadim Markovtsev, Varun Agrawal, Vathsala Achar, Vilhelm von Ehrenheim, Vinayak Mehta, Vinit, Vinod Kumar L, Viraj Mavani, Viraj Navkal, Vivek Kumar, Vlad Niculae, vqean3, Vrishank Bhardwaj, vufg, wallygauze, Warut Vijitbenjaronk, wdevazelhes, Wenhao Zhang, Wes Barnett, Will, William de Vazelhes, Will Rosenfeld, Xin Xiong, Yiming (Paul) Li, ymazari, Yufeng, Zach Griffith, Zé Vinícius, Zhenqing Hu, Zhiqing Xiao, Zijie (ZJ) Poh

---

v0.21.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.21

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 0.21.3

**July 30, 2019**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - The v0.20.0 release notes failed to mention a backwards incompatibility in <span class="title-ref">metrics.make\_scorer</span> when <span class="title-ref">needs\_proba=True</span> and <span class="title-ref">y\_true</span> is binary. Now, the scorer function is supposed to accept a 1D <span class="title-ref">y\_pred</span> (i.e., probability of the positive class, shape <span class="title-ref">(n\_samples,)</span>), instead of a 2D <span class="title-ref">y\_pred</span> (i.e., shape <span class="title-ref">(n\_samples, 2)</span>).

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where computation with <span class="title-ref">init='random'</span> was single threaded for <span class="title-ref">n\_jobs \> 1</span> or <span class="title-ref">n\_jobs = -1</span>. `12955` by `Prabakaran Kumaresshan <nixphix>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.OPTICS</span> where users were unable to pass float <span class="title-ref">min\_samples</span> and <span class="title-ref">min\_cluster\_size</span>. `14496` by `Fabian Klopfer <someusername1>` and `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where KMeans++ initialisation could rarely result in an IndexError. `11756` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">compose.ColumnTransformer</span> where using DataFrames whose column order differs between `` `fit ``<span class="title-ref"> and :func:</span><span class="title-ref">transform</span><span class="title-ref"> could lead to silently passing incorrect columns to the </span><span class="title-ref">remainder</span><span class="title-ref"> transformer. :pr:\`14237</span> by <span class="title-ref">Andreas Schuderer \<schuderer\></span>.

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_california\_housing</span>, <span class="title-ref">datasets.fetch\_covtype</span>, <span class="title-ref">datasets.fetch\_kddcup99</span>, <span class="title-ref">datasets.fetch\_olivetti\_faces</span>, <span class="title-ref">datasets.fetch\_rcv1</span>, and <span class="title-ref">datasets.fetch\_species\_distributions</span> try to persist the previously cache using the new `joblib` if the cached data was persisted using the deprecated `sklearn.externals.joblib`. This behavior is set to be deprecated and removed in v0.23. `14197` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix zero division error in <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>. `14024` by <span class="title-ref">Nicolas Hug \<NicolasHug\></span>.

#### `sklearn.impute`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">impute.SimpleImputer</span> and <span class="title-ref">impute.IterativeImputer</span> so that no errors are thrown when there are missing values in training data. `13974` by <span class="title-ref">Frank Hoang \<fhoang7\></span>.

#### `sklearn.inspection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">inspection.plot\_partial\_dependence</span> where `target` parameter was not being taken into account for multiclass problems. `14393` by `Guillem G. Subies <guillemgsubies>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LogisticRegressionCV</span> where `refit=False` would fail depending on the `'multiclass'` and `'penalty'` parameters (regression introduced in 0.21). `14087` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Compatibility fix for <span class="title-ref">linear\_model.ARDRegression</span> and Scipy\>=1.3.0. Adapts to upstream changes to the default <span class="title-ref">pinvh</span> cutoff threshold which otherwise results in poor accuracy in some cases. `14067` by `Tim Staley <timstaley>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neighbors.NeighborhoodComponentsAnalysis</span> where the validation of initial parameters `n_components`, `max_iter` and `tol` required too strict types. `14092` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">tree.export\_text</span> when the tree has one feature and a single feature name is passed in. `14053` by <span class="title-ref">Thomas Fan</span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue with <span class="title-ref">tree.plot\_tree</span> where it displayed entropy calculations even for <span class="title-ref">gini</span> criterion in DecisionTreeClassifiers. `13947` by `Frank Hoang <fhoang7>`.

## Version 0.21.2

**24 May 2019**

### Changelog

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cross\_decomposition.CCA</span> improving numerical stability when <span class="title-ref">Y</span> is close to zero. `13903` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.pairwise.euclidean\_distances</span> where a part of the distance matrix was left un-instanciated for sufficiently large float32 datasets (regression introduced in 0.21). `13910` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">preprocessing.OneHotEncoder</span> where the new <span class="title-ref">drop</span> parameter was not reflected in <span class="title-ref">get\_feature\_names</span>. `13894` by `James Myatt <jamesmyatt>`.

#### <span class="title-ref">sklearn.utils.sparsefuncs</span>

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">min\_max\_axis</span> would fail on 32-bit systems for certain large inputs. This affects <span class="title-ref">preprocessing.MaxAbsScaler</span>, <span class="title-ref">preprocessing.normalize</span> and <span class="title-ref">preprocessing.LabelBinarizer</span>. `13741` by `Roddy MacSween <rlms>`.

## Version 0.21.1

**17 May 2019**

This is a bug-fix release to primarily resolve some packaging issues in version 0.21.0. It also includes minor documentation improvements and some bug fixes.

### Changelog

#### `sklearn.inspection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">inspection.partial\_dependence</span> to only check classifier and not regressor for the multiclass-multioutput case. `14309` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.pairwise\_distances</span> where it would raise `AttributeError` for boolean metrics when `X` had a boolean dtype and `Y == None`. `13864` by `Paresh Mathur <rick2047>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed two bugs in <span class="title-ref">metrics.pairwise\_distances</span> when `n_jobs > 1`. First it used to return a distance matrix with same dtype as input, even for integer dtype. Then the diagonal was not zeros for euclidean metric when `Y` is `X`. `13877` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neighbors.KernelDensity</span> which could not be restored from a pickle if `sample_weight` had been used. `13772` by `Aditya Vyas <aditya1702>`.

## Version 0.21.0

**May 2019**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> for multiclass classification. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> with 'eigen' solver. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">linear\_model.BayesianRidge</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - Decision trees and derived ensembles when both <span class="title-ref">max\_depth</span> and <span class="title-ref">max\_leaf\_nodes</span> are set. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> with 'saga' solver. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">ensemble.GradientBoostingClassifier</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">sklearn.feature\_extraction.text.HashingVectorizer</span>, <span class="title-ref">sklearn.feature\_extraction.text.TfidfVectorizer</span>, and <span class="title-ref">sklearn.feature\_extraction.text.CountVectorizer</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">neural\_network.MLPClassifier</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">svm.SVC.decision\_function</span> and <span class="title-ref">multiclass.OneVsOneClassifier.decision\_function</span>. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">linear\_model.SGDClassifier</span> and any derived classifiers. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - Any model using the <span class="title-ref">linear\_model.\_sag.sag\_solver</span> function with a <span class="title-ref">0</span> seed, including <span class="title-ref">linear\_model.LogisticRegression</span>, <span class="title-ref">linear\_model.LogisticRegressionCV</span>, <span class="title-ref">linear\_model.Ridge</span>, and <span class="title-ref">linear\_model.RidgeCV</span> with 'sag' solver. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">linear\_model.RidgeCV</span> when using leave-one-out cross-validation with sparse inputs. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Known Major Bugs

  - The default <span class="title-ref">max\_iter</span> for <span class="title-ref">linear\_model.LogisticRegression</span> is too small for many solvers given the default <span class="title-ref">tol</span>. In particular, we accidentally changed the default <span class="title-ref">max\_iter</span> for the liblinear solver from 1000 to 100 iterations in `3591` released in version 0.16. In a future release we hope to choose better default <span class="title-ref">max\_iter</span> and <span class="title-ref">tol</span> heuristically depending on the solver (see `13317`).

### Changelog

Support for Python 3.4 and below has been officially dropped.

#### `sklearn.base`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The R2 score used when calling `score` on a regressor will use `multioutput='uniform_average'` from version 0.23 to keep consistent with <span class="title-ref">metrics.r2\_score</span>. This will influence the `score` method of all the multioutput regressors (except for <span class="title-ref">multioutput.MultiOutputRegressor</span>). `13157` by `Hanmin Qin <qinhanmin2014>`.

#### `sklearn.calibration`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added support to bin the data passed into <span class="title-ref">calibration.calibration\_curve</span> by quantiles instead of uniformly between 0 and 1. `13086` by `Scott Cole <srcole>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Allow n-dimensional arrays as input for <span class="title-ref">calibration.CalibratedClassifierCV</span>. `13485` by `William de Vazelhes <wdevazelhes>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` A new clustering algorithm: \`cluster.OPTICS\`: an algorithm related to <span class="title-ref">cluster.DBSCAN</span>, that has hyperparameters easier to set and that scales better, by `Shane <espg>`, [Adrin Jalali](https://github.com/adrinjalali), `Erich Schubert <kno10>`, [Hanmin Qin](https://github.com/qinhanmin2014), and `Assia Benbihi <assiaben>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">cluster.Birch</span> could occasionally raise an AttributeError. `13651` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where empty clusters weren't correctly relocated when using sample weights. `13486` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `n_components_` attribute in <span class="title-ref">cluster.AgglomerativeClustering</span> and <span class="title-ref">cluster.FeatureAgglomeration</span> has been renamed to `n_connected_components_`. `13427` by `Stephane Couvreur <scouvreur>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.AgglomerativeClustering</span> and <span class="title-ref">cluster.FeatureAgglomeration</span> now accept a `distance_threshold` parameter which can be used to find the clusters instead of `n_clusters`. `9069` by `Vathsala Achar <VathsalaAchar>` and [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.compose`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">compose.ColumnTransformer</span> is no longer an experimental feature. `13835` by `Hanmin Qin <qinhanmin2014>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Added support for 64-bit group IDs and pointers in SVMLight files. `10727` by `Bryan K Woods <bryan-woods>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.load\_sample\_images</span> returns images with a deterministic order. `13250` by `Thomas Fan <thomasjpfan>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.KernelPCA</span> now has deterministic output (resolved sign ambiguity in eigenvalue decomposition of the kernel matrix). `13241` by `Aurélien Bellet <bellet>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.KernelPCA</span>, <span class="title-ref">fit().transform()</span> now produces the correct output (the same as <span class="title-ref">fit\_transform()</span>) in case of non-removed zero eigenvalues (<span class="title-ref">remove\_zero\_eig=False</span>). <span class="title-ref">fit\_inverse\_transform</span> was also accelerated by using the same trick as <span class="title-ref">fit\_transform</span> to compute the transform of <span class="title-ref">X</span>. `12143` by `Sylvain Marié <smarie>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.NMF</span> where <span class="title-ref">init = 'nndsvd'</span>, <span class="title-ref">init = 'nndsvda'</span>, and <span class="title-ref">init = 'nndsvdar'</span> are allowed when <span class="title-ref">n\_components \< n\_features</span> instead of <span class="title-ref">n\_components \<= min(n\_samples, n\_features)</span>. `11650` by `Hossein Pourbozorg <hossein-pourbozorg>` and `Zijie (ZJ) Poh <zjpoh>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of the `init` argument in <span class="title-ref">decomposition.non\_negative\_factorization</span> will change from `random` to `None` in version 0.23 to make it consistent with <span class="title-ref">decomposition.NMF</span>. A FutureWarning is raised when the default value is used. `12988` by `Zijie (ZJ) Poh <zjpoh>`.

#### `sklearn.discriminant_analysis`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now preserves `float32` and `float64` dtypes. `8769` and `11000` by `Thibault Sejourne <thibsej>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A `ChangedBehaviourWarning` is now raised when <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> is given as parameter `n_components > min(n_features, n_classes - 1)`, and `n_components` is changed to `min(n_features, n_classes - 1)` if so. Previously the change was made, but silently. `11526` by `William de Vazelhes<wdevazelhes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> where the predicted probabilities would be incorrectly computed in the multiclass case. `6848`, by `Agamemnon Krasoulis
    <agamemnonc>` and <span class="title-ref">Guillaume Lemaitre \<glemaitre\></span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> where the predicted probabilities would be incorrectly computed with `eigen` solver. `11727`, by `Agamemnon Krasoulis
    <agamemnonc>`.

#### `sklearn.dummy`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">dummy.DummyClassifier</span> where the `predict_proba` method was returning int32 array instead of float64 for the `stratified` strategy. `13266` by `Christos Aridas<chkoar>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">dummy.DummyClassifier</span> where it was throwing a dimension mismatch error in prediction time if a column vector `y` with `shape=(n, 1)` was given at `fit` time. `13545` by `Nick
    Sorros <nsorros>` and [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Add two new implementations of gradient boosting trees: <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>. The implementation of these estimators is inspired by [LightGBM](https://github.com/Microsoft/LightGBM) and can be orders of magnitude faster than <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> when the number of samples is larger than tens of thousands of samples. The API of these new estimators is slightly different, and some of the features from <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> are not yet supported.
    
    These new estimators are experimental, which means that their results or their API might change without any deprecation cycle. To use them, you need to explicitly import `enable_hist_gradient_boosting`:
    
        >>> # explicitly require this experimental feature
        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
        >>> # now you can import normally from sklearn.ensemble
        >>> from sklearn.ensemble import HistGradientBoostingClassifier
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    Update: since version 1.0, these estimators are not experimental anymore and you don't need to use <span class="title-ref">from sklearn.experimental import enable\_hist\_gradient\_boosting</span>.
    
    </div>
    
    `12807` by `Nicolas Hug<NicolasHug>`.

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">ensemble.VotingRegressor</span> which provides an equivalent of <span class="title-ref">ensemble.VotingClassifier</span> for regression problems. `12513` by `Ramil Nugmanov <stsouko>` and `Mohamed Ali Jamaoui <mohamed-ali>`.

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Make <span class="title-ref">ensemble.IsolationForest</span> prefer threads over processes when running with `n_jobs > 1` as the underlying decision tree fit calls do release the GIL. This changes reduces memory usage and communication overhead. `12543` by `Isaac Storch <istorch>` and [Olivier Grisel](https://twitter.com/ogrisel).

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Make <span class="title-ref">ensemble.IsolationForest</span> more memory efficient by avoiding keeping in memory each tree prediction. `13260` by [Nicolas Goix](https://ngoix.github.io/).

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.IsolationForest</span> now uses chunks of data at prediction step, thus capping the memory usage. `13283` by [Nicolas Goix](https://ngoix.github.io/).

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">sklearn.ensemble.GradientBoostingClassifier</span> and <span class="title-ref">sklearn.ensemble.GradientBoostingRegressor</span> now keep the input `y` as `float64` to avoid it being copied internally by trees. `13524` by [Adrin Jalali](https://github.com/adrinjalali).

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Minimized the validation of X in <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> `13174` by `Christos Aridas <chkoar>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.IsolationForest</span> now exposes `warm_start` parameter, allowing iterative addition of trees to an isolation forest. `13496` by `Peter Marko <petibear>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The values of `feature_importances_` in all random forest based models (i.e. <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>, <span class="title-ref">ensemble.RandomTreesEmbedding</span>, <span class="title-ref">ensemble.GradientBoostingClassifier</span>, and <span class="title-ref">ensemble.GradientBoostingRegressor</span>) now:
    
      - sum up to `1`
      - all the single node trees in feature importance calculation are ignored
      - in case all trees have only one single node (i.e. a root node), feature importances will be an array of all zeros.
    
    `13636` and `13620` by [Adrin Jalali](https://github.com/adrinjalali).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span>, which didn't support scikit-learn estimators as the initial estimator. Also added support of initial estimator which does not support sample weights. `12436` by `Jérémie du Boisberranger <jeremiedbb>` and `12983` by `Nicolas Hug<NicolasHug>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the output of the average path length computed in <span class="title-ref">ensemble.IsolationForest</span> when the input is either 0, 1 or 2. `13251` by `Albert Thomas <albertcthomas>` and `joshuakennethjones <joshuakennethjones>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.GradientBoostingClassifier</span> where the gradients would be incorrectly computed in multiclass classification problems. `12715` by `Nicolas Hug<NicolasHug>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.GradientBoostingClassifier</span> where validation sets for early stopping were not sampled with stratification. `13164` by `Nicolas Hug<NicolasHug>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.GradientBoostingClassifier</span> where the default initial prediction of a multiclass classifier would predict the classes priors instead of the log of the priors. `12983` by `Nicolas Hug<NicolasHug>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.RandomForestClassifier</span> where the `predict` method would error for multiclass multioutput forests models if any targets were strings. `12834` by `Elizabeth Sander
    <elsander>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.gradient\_boosting.LossFunction</span> and <span class="title-ref">ensemble.gradient\_boosting.LeastSquaresError</span> where the default value of `learning_rate` in `update_terminal_regions` is not consistent with the document and the caller functions. Note however that directly using these loss functions is deprecated. `6463` by `movelikeriver <movelikeriver>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.partial\_dependence</span> (and consequently the new version <span class="title-ref">sklearn.inspection.partial\_dependence</span>) now takes sample weights into account for the partial dependence computation when the gradient boosting model has been trained with sample weights. `13193` by `Samuel O. Ronsin <samronsin>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">ensemble.partial\_dependence</span> and <span class="title-ref">ensemble.plot\_partial\_dependence</span> are now deprecated in favor of <span class="title-ref">inspection.partial\_dependence\<sklearn.inspection.partial\_dependence\></span> and <span class="title-ref">inspection.plot\_partial\_dependence\<sklearn.inspection.plot\_partial\_dependence\></span>. `12599` by `Trevor Stephens<trevorstephens>` and `Nicolas Hug<NicolasHug>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span> were failing during `fit` in one of the estimators was set to `None` and `sample_weight` was not `None`. `13779` by `Guillaume Lemaitre <glemaitre>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span> accept `'drop'` to disable an estimator in addition to `None` to be consistent with other estimators (i.e., <span class="title-ref">pipeline.FeatureUnion</span> and <span class="title-ref">compose.ColumnTransformer</span>). `13780` by `Guillaume Lemaitre <glemaitre>`.

#### <span class="title-ref">sklearn.externals</span>

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecated <span class="title-ref">externals.six</span> since we have dropped support for Python 2.7. `12916` by `Hanmin Qin <qinhanmin2014>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` If `input='file'` or `input='filename'`, and a callable is given as the `analyzer`, <span class="title-ref">sklearn.feature\_extraction.text.HashingVectorizer</span>, <span class="title-ref">sklearn.feature\_extraction.text.TfidfVectorizer</span>, and <span class="title-ref">sklearn.feature\_extraction.text.CountVectorizer</span> now read the data from the file(s) and then pass it to the given `analyzer`, instead of passing the file name(s) or the file object(s) to the analyzer. `13641` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.impute`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">impute.IterativeImputer</span>, which is a strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. `8478` and `12177` by `Sergey Feldman <sergeyf>` and `Ben Lawson
    <benlawson>`.
    
    The API of IterativeImputer is experimental and subject to change without any deprecation cycle. To use them, you need to explicitly import `enable_iterative_imputer`:
    
        >>> from sklearn.experimental import enable_iterative_imputer  # noqa
        >>> # now you can import normally from sklearn.impute
        >>> from sklearn.impute import IterativeImputer

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The <span class="title-ref">impute.SimpleImputer</span> and <span class="title-ref">impute.IterativeImputer</span> have a new parameter `'add_indicator'`, which simply stacks a <span class="title-ref">impute.MissingIndicator</span> transform into the output of the imputer's transform. That allows a predictive estimator to account for missingness. `12583`, `13601` by `Danylo Baibak
    <DanilBaibak>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` In <span class="title-ref">impute.MissingIndicator</span> avoid implicit densification by raising an exception if input is sparse add <span class="title-ref">missing\_values</span> property is set to 0. `13240` by `Bartosz Telenczuk <btel>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed two bugs in <span class="title-ref">impute.MissingIndicator</span>. First, when `X` is sparse, all the non-zero non missing values used to become explicit False in the transformed data. Then, when `features='missing-only'`, all features used to be kept if there were no missing values at all. `13562` by `Jérémie du Boisberranger
    <jeremiedbb>`.

#### `sklearn.inspection`

(new subpackage)

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Partial dependence plots (<span class="title-ref">inspection.plot\_partial\_dependence</span>) are now supported for any regressor or classifier (provided that they have a <span class="title-ref">predict\_proba</span> method). `12599` by `Trevor Stephens <trevorstephens>` and `Nicolas Hug <NicolasHug>`.

#### `sklearn.isotonic`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Allow different dtypes (such as float32) in <span class="title-ref">isotonic.IsotonicRegression</span>. `8769` by `Vlad Niculae <vene>`

#### `sklearn.linear_model`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.Ridge</span> now preserves `float32` and `float64` dtypes. `8769` and `11000` by `Guillaume Lemaitre <glemaitre>`, and `Joan Massich <massich>`
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> now support Elastic-Net penalty, with the 'saga' solver. `11646` by `Nicolas Hug <NicolasHug>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">linear\_model.lars\_path\_gram</span>, which is <span class="title-ref">linear\_model.lars\_path</span> in the sufficient stats mode, allowing users to compute <span class="title-ref">linear\_model.lars\_path</span> without providing `X` and `y`. `11699` by `Kuai Yu <yukuairoy>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">linear\_model.make\_dataset</span> now preserves `float32` and `float64` dtypes, reducing memory consumption in stochastic gradient, SAG and SAGA solvers. `8769` and `11000` by `Nelle Varoquaux <NelleV>`, `Arthur Imbert <Henley13>`, `Guillaume Lemaitre <glemaitre>`, and `Joan Massich <massich>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.LogisticRegression</span> now supports an unregularized objective when `penalty='none'` is passed. This is equivalent to setting `C=np.inf` with l2 regularization. Not supported by the liblinear solver. `12860` by `Nicolas Hug
    <NicolasHug>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">sparse\_cg</span> solver in <span class="title-ref">linear\_model.Ridge</span> now supports fitting the intercept (i.e. `fit_intercept=True`) when inputs are sparse. `13336` by `Bartosz Telenczuk <btel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The coordinate descent solver used in <span class="title-ref">Lasso</span>, <span class="title-ref">ElasticNet</span>, etc. now issues a <span class="title-ref">ConvergenceWarning</span> when it completes without meeting the desired toleranbce. `11754` and `13397` by `Brent Fagan <brentfagan>` and `Adrin Jalali <adrinjalali>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> with 'saga' solver, where the weights would not be correctly updated in some cases. `11646` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the posterior mean, posterior covariance and returned regularization parameters in <span class="title-ref">linear\_model.BayesianRidge</span>. The posterior mean and the posterior covariance were not the ones computed with the last update of the regularization parameters and the returned regularization parameters were not the final ones. Also fixed the formula of the log marginal likelihood used to compute the score when <span class="title-ref">compute\_score=True</span>. `12174` by `Albert Thomas <albertcthomas>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LassoLarsIC</span>, where user input `copy_X=False` at instance creation would be overridden by default parameter value `copy_X=True` in `fit`. `12972` by `Lucio Fernandez-Arjona <luk-f-a>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LinearRegression</span> that was not returning the same coeffecients and intercepts with `fit_intercept=True` in sparse and dense case. `13279` by [Alexandre Gramfort](http://alexandre.gramfort.net)
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.HuberRegressor</span> that was broken when `X` was of dtype bool. `13328` by [Alexandre Gramfort](http://alexandre.gramfort.net).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a performance issue of `saga` and `sag` solvers when called in a <span class="title-ref">joblib.Parallel</span> setting with `n_jobs > 1` and `backend="threading"`, causing them to perform worse than in the sequential case. `13389` by `Pierre Glaser <pierreglaser>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.stochastic\_gradient.BaseSGDClassifier</span> that was not deterministic when trained in a multi-class setting on several threads. `13422` by `Clément Doumouro <ClemDoum>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">linear\_model.ridge\_regression</span>, <span class="title-ref">linear\_model.Ridge</span> and <span class="title-ref">linear\_model.RidgeClassifier</span> that caused unhandled exception for arguments `return_intercept=True` and `solver=auto` (default) or any other solver different from `sag`. `13363` by `Bartosz Telenczuk <btel>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.ridge\_regression</span> will now raise an exception if `return_intercept=True` and solver is different from `sag`. Previously, only warning was issued. `13363` by `Bartosz Telenczuk <btel>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.ridge\_regression</span> will choose `sparse_cg` solver for sparse inputs when `solver=auto` and `sample_weight` is provided (previously <span class="title-ref">cholesky</span> solver was selected). `13363` by `Bartosz Telenczuk <btel>`
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The use of <span class="title-ref">linear\_model.lars\_path</span> with `X=None` while passing `Gram` is deprecated in version 0.21 and will be removed in version 0.23. Use <span class="title-ref">linear\_model.lars\_path\_gram</span> instead. `11699` by `Kuai Yu <yukuairoy>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">linear\_model.logistic\_regression\_path</span> is deprecated in version 0.21 and will be removed in version 0.23. `12821` by `Nicolas Hug <NicolasHug>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.RidgeCV</span> with leave-one-out cross-validation now correctly fits an intercept when `fit_intercept=True` and the design matrix is sparse. `13350` by `Jérôme Dockès <jeromedockes>`

#### `sklearn.manifold`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Make <span class="title-ref">manifold.trustworthiness</span> use an inverted index instead of an <span class="title-ref">np.where</span> lookup to find the rank of neighbors in the input space. This improves efficiency in particular when computed with lots of neighbors and/or small datasets. `9907` by `William de Vazelhes <wdevazelhes>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added the <span class="title-ref">metrics.max\_error</span> metric and a corresponding `'max_error'` scorer for single output regression. `12232` by `Krishna Sangeeth <whiletruelearn>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">metrics.multilabel\_confusion\_matrix</span>, which calculates a confusion matrix with true positive, false positive, false negative and true negative counts for each class. This facilitates the calculation of set-wise metrics such as recall, specificity, fall out and miss rate. `11179` by `Shangwu Yao <ShangwuYao>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.jaccard\_score</span> has been added to calculate the Jaccard coefficient as an evaluation metric for binary, multilabel and multiclass tasks, with an interface analogous to <span class="title-ref">metrics.f1\_score</span>. `13151` by `Gaurav Dhingra <gxyd>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">metrics.pairwise.haversine\_distances</span> which can be accessed with <span class="title-ref">metric='pairwise'</span> through <span class="title-ref">metrics.pairwise\_distances</span> and estimators. (Haversine distance was previously available for nearest neighbors calculation.) `12568` by `Wei Xue <xuewei4d>`, `Emmanuel Arias <eamanu>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Faster <span class="title-ref">metrics.pairwise\_distances</span> with <span class="title-ref">n\_jobs</span> \> 1 by using a thread-based backend, instead of process-based backends. `8216` by `Pierre Glaser <pierreglaser>` and `Romuald Menuet <zanospi>`
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The pairwise manhattan distances with sparse input now uses the BLAS shipped with scipy instead of the bundled BLAS. `12732` by `Jérémie du Boisberranger <jeremiedbb>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Use label <span class="title-ref">accuracy</span> instead of <span class="title-ref">micro-average</span> on <span class="title-ref">metrics.classification\_report</span> to avoid confusion. <span class="title-ref">micro-average</span> is only shown for multi-label or multi-class with a subset of classes because it is otherwise identical to accuracy. `12334` by `Emmanuel Arias <eamanu@eamanu.com>`, [Joel Nothman](https://joelnothman.com/) and [Andreas Müller](https://amueller.github.io/)
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added <span class="title-ref">beta</span> parameter to <span class="title-ref">metrics.homogeneity\_completeness\_v\_measure</span> and <span class="title-ref">metrics.v\_measure\_score</span> to configure the tradeoff between homogeneity and completeness. `13607` by `Stephane Couvreur <scouvreur>` and and `Ivan Sanchez <ivsanro1>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The metric <span class="title-ref">metrics.r2\_score</span> is degenerate with a single sample and now it returns NaN and raises <span class="title-ref">exceptions.UndefinedMetricWarning</span>. `12855` by `Pawel Sendyk <psendyk>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">metrics.brier\_score\_loss</span> will sometimes return incorrect result when there's only one class in `y_true`. `13628` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.label\_ranking\_average\_precision\_score</span> where sample\_weight wasn't taken into account for samples with degenerate labels. `13447` by `Dan Ellis <dpwe>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter `labels` in <span class="title-ref">metrics.hamming\_loss</span> is deprecated in version 0.21 and will be removed in version 0.23. `10580` by `Reshama Shaikh <reshamas>` and `Sandra Mitrovic <SandraMNE>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The function <span class="title-ref">metrics.pairwise.euclidean\_distances</span>, and therefore several estimators with `metric='euclidean'`, suffered from numerical precision issues with `float32` features. Precision has been increased at the cost of a small drop of performance. `13554` by `Celelibi` and `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.jaccard\_similarity\_score</span> is deprecated in favour of the more consistent <span class="title-ref">metrics.jaccard\_score</span>. The former behavior for binary and multiclass targets is broken. `13151` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.mixture`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">mixture.BaseMixture</span> and therefore on estimators based on it, i.e. <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span>, where `fit_predict` and `fit.predict` were not equivalent. `13142` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Classes <span class="title-ref">\~model\_selection.GridSearchCV</span> and <span class="title-ref">\~model\_selection.RandomizedSearchCV</span> now allow for refit=callable to add flexibility in identifying the best estimator. See \[sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_refit\_callable.py\](\#sphx\_glr\_auto\_examples\_model\_selection\_plot\_grid\_search\_refit\_callable.py). `11354` by `Wenhao Zhang <wenhaoz@ucla.edu>`, [Joel Nothman](https://joelnothman.com/) and `Adrin Jalali <adrinjalali>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Classes <span class="title-ref">\~model\_selection.GridSearchCV</span>, <span class="title-ref">\~model\_selection.RandomizedSearchCV</span>, and methods <span class="title-ref">\~model\_selection.cross\_val\_score</span>, <span class="title-ref">\~model\_selection.cross\_val\_predict</span>, <span class="title-ref">\~model\_selection.cross\_validate</span>, now print train scores when <span class="title-ref">return\_train\_scores</span> is True and <span class="title-ref">verbose</span> \> 2. For <span class="title-ref">\~model\_selection.learning\_curve</span>, and <span class="title-ref">\~model\_selection.validation\_curve</span> only the latter is required. `12613` and `12669` by `Marc Torrellas <marctorrellas>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Some `CV splitter` classes and <span class="title-ref">model\_selection.train\_test\_split</span> now raise `ValueError` when the resulting training set is empty. `12861` by `Nicolas Hug <NicolasHug>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">model\_selection.StratifiedKFold</span> shuffles each class's samples with the same `random_state`, making `shuffle=True` ineffective. `13124` by `Hanmin Qin <qinhanmin2014>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Added ability for <span class="title-ref">model\_selection.cross\_val\_predict</span> to handle multi-label (and multioutput-multiclass) targets with `predict_proba`-type methods. `8773` by `Stephen Hoover <stephen-hoover>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">\~model\_selection.cross\_val\_predict</span> where <span class="title-ref">method="predict\_proba"</span> returned always <span class="title-ref">0.0</span> when one of the classes was excluded in a cross-validation fold. `13366` by `Guillaume Fournier <gfournier>`

#### `sklearn.multiclass`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">multiclass.OneVsOneClassifier.decision\_function</span> where the decision\_function value of a given sample was different depending on whether the decision\_function was evaluated on the sample alone or on a batch containing this same sample due to the scaling used in decision\_function. `10440` by `Jonathan Ohayon <Johayon>`.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">multioutput.MultiOutputClassifier</span> where the <span class="title-ref">predict\_proba</span> method incorrectly checked for <span class="title-ref">predict\_proba</span> attribute in the estimator object. `12222` by `Rebekah Kim <rebekahkim>`

#### `sklearn.neighbors`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">neighbors.NeighborhoodComponentsAnalysis</span> for metric learning, which implements the Neighborhood Components Analysis algorithm. `10058` by `William de Vazelhes <wdevazelhes>` and `John Chiotellis <johny-c>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Methods in <span class="title-ref">neighbors.NearestNeighbors</span> : <span class="title-ref">\~neighbors.NearestNeighbors.kneighbors</span>, <span class="title-ref">\~neighbors.NearestNeighbors.radius\_neighbors</span>, <span class="title-ref">\~neighbors.NearestNeighbors.kneighbors\_graph</span>, <span class="title-ref">\~neighbors.NearestNeighbors.radius\_neighbors\_graph</span> now raise `NotFittedError`, rather than `AttributeError`, when called before `fit` `12279` by `Krishna Sangeeth
    <whiletruelearn>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span> where the option `shuffle=False` was being ignored. `12582` by `Sam Waterbury <samwaterbury>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">neural\_network.MLPClassifier</span> where validation sets for early stopping were not sampled with stratification. In the multilabel case however, splits are still not stratified. `13164` by `Nicolas Hug<NicolasHug>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">pipeline.Pipeline</span> can now use indexing notation (e.g. `my_pipeline[0:-1]`) to extract a subsequence of steps as another Pipeline instance. A Pipeline can also be indexed directly to extract a particular step (e.g. `my_pipeline['svc']`), rather than accessing `named_steps`. `2568` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added optional parameter `verbose` in <span class="title-ref">pipeline.Pipeline</span>, <span class="title-ref">compose.ColumnTransformer</span> and <span class="title-ref">pipeline.FeatureUnion</span> and corresponding `make_` helpers for showing progress and timing of each step. `11364` by `Baze Petrushev <petrushev>`, `Karan Desai <karandesai-96>`, [Joel Nothman](https://joelnothman.com/), and `Thomas Fan <thomasjpfan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">pipeline.Pipeline</span> now supports using `'passthrough'` as a transformer, with the same effect as `None`. `11144` by `Thomas Fan <thomasjpfan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">pipeline.Pipeline</span> implements `__len__` and therefore `len(pipeline)` returns the number of steps in the pipeline. `13439` by `Lakshya KD <LakshKD>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OneHotEncoder</span> now supports dropping one feature per category with a new drop parameter. `12908` by `Drew Johnston <drewmjohnston>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">preprocessing.OneHotEncoder</span> and <span class="title-ref">preprocessing.OrdinalEncoder</span> now handle pandas DataFrames more efficiently. `13253` by `maikia`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Make <span class="title-ref">preprocessing.MultiLabelBinarizer</span> cache class mappings instead of calculating it every time on the fly. `12116` by `Ekaterina Krivich <kiote>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">preprocessing.PolynomialFeatures</span> now supports compressed sparse row (CSR) matrices as input for degrees 2 and 3. This is typically much faster than the dense case as it scales with matrix density and expansion degree (on the order of density^degree), and is much, much faster than the compressed sparse column (CSC) case. `12197` by `Andrew Nystrom <awnystrom>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Speed improvement in <span class="title-ref">preprocessing.PolynomialFeatures</span>, in the dense case. Also added a new parameter `order` which controls output order for further speed performances. `12251` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the calculation overflow when using a float16 dtype with <span class="title-ref">preprocessing.StandardScaler</span>. `13007` by `Raffaello Baluyot <baluyotraf>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">preprocessing.QuantileTransformer</span> and <span class="title-ref">preprocessing.quantile\_transform</span> to force n\_quantiles to be at most equal to n\_samples. Values of n\_quantiles larger than n\_samples were either useless or resulting in a wrong approximation of the cumulative distribution function estimator. `13333` by `Albert Thomas <albertcthomas>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of <span class="title-ref">copy</span> in <span class="title-ref">preprocessing.quantile\_transform</span> will change from False to True in 0.23 in order to make it more consistent with the default <span class="title-ref">copy</span> values of other functions in `sklearn.preprocessing` and prevent unexpected side effects by modifying the value of <span class="title-ref">X</span> inplace. `13459` by `Hunter McGushion <HunterMcGushion>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue in <span class="title-ref">svm.SVC.decision\_function</span> when `decision_function_shape='ovr'`. The decision\_function value of a given sample was different depending on whether the decision\_function was evaluated on the sample alone or on a batch containing this same sample due to the scaling used in decision\_function. `10440` by `Jonathan Ohayon <Johayon>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Decision Trees can now be plotted with matplotlib using <span class="title-ref">tree.plot\_tree</span> without relying on the `dot` library, removing a hard-to-install dependency. `8508` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Decision Trees can now be exported in a human readable textual format using <span class="title-ref">tree.export\_text</span>. `6261` by <span class="title-ref">Giuseppe Vettigli \<JustGlowing\></span>.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` `get_n_leaves()` and `get_depth()` have been added to <span class="title-ref">tree.BaseDecisionTree</span> and consequently all estimators based on it, including <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span>, and <span class="title-ref">tree.ExtraTreeRegressor</span>. `12300` by `Adrin Jalali <adrinjalali>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Trees and forests did not previously <span class="title-ref">predict</span> multi-output classification targets with string labels, despite accepting them in <span class="title-ref">fit</span>. `11458` by `Mitar Milutinovic <mitar>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an issue with <span class="title-ref">tree.BaseDecisionTree</span> and consequently all estimators based on it, including <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span>, and <span class="title-ref">tree.ExtraTreeRegressor</span>, where they used to exceed the given `max_depth` by 1 while expanding the tree if `max_leaf_nodes` and `max_depth` were both specified by the user. Please note that this also affects all ensemble methods using decision trees. `12344` by `Adrin Jalali <adrinjalali>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">utils.resample</span> now accepts a `stratify` parameter for sampling according to class distributions. `13549` by `Nicolas
    Hug <NicolasHug>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecated `warn_on_dtype` parameter from <span class="title-ref">utils.check\_array</span> and <span class="title-ref">utils.check\_X\_y</span>. Added explicit warning for dtype conversion in <span class="title-ref">check\_pairwise\_arrays</span> if the `metric` being passed is a pairwise boolean metric. `13382` by `Prathmesh Savale <praths007>`.

#### Multiple modules

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` The <span class="title-ref">\_\_repr\_\_()</span> method of all estimators (used when calling <span class="title-ref">print(estimator)</span>) has been entirely re-written, building on Python's pretty printing standard library. All parameters are printed by default, but this can be altered with the `print_changed_only` option in <span class="title-ref">sklearn.set\_config</span>. `11705` by `Nicolas Hug
    <NicolasHug>`.

<!-- end list -->

  - \- `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Add estimators tags: these are annotations of estimators  
    that allow programmatic inspection of their capabilities, such as sparse matrix support, supported output types and supported methods. Estimator tags also determine the tests that are run on an estimator when <span class="title-ref">check\_estimator</span> is called. Read more in the \[User Guide \<estimator\_tags\>\](\#user-guide

\--\<estimator\_tags\>). `8022` by `Andreas Müller <amueller>`.

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Memory copies are avoided when casting arrays to a different dtype in multiple estimators. `11973` by `Roman Yurchak
    <rth>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in the implementation of the <span class="title-ref">our\_rand\_r</span> helper function that was not behaving consistently across platforms. `13422` by `Madhura Parikh <jdnc>` and `Clément Doumouro <ClemDoum>`.

#### Miscellaneous

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Joblib is no longer vendored in scikit-learn, and becomes a dependency. Minimal supported version is joblib 0.11, however using version \>= 0.13 is strongly recommended. `13531` by `Roman Yurchak <rth>`.

### Changes to estimator checks

These changes mostly affect library developers.

  - Add `check_fit_idempotent` to <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span>, which checks that when <span class="title-ref">fit</span> is called twice with the same data, the output of <span class="title-ref">predict</span>, <span class="title-ref">predict\_proba</span>, <span class="title-ref">transform</span>, and <span class="title-ref">decision\_function</span> does not change. `12328` by `Nicolas Hug <NicolasHug>`
  - Many checks can now be disabled or configured with \[estimator\_tags\](\#estimator\_tags). `8022` by `Andreas Müller <amueller>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.20, including:

adanhawth, Aditya Vyas, Adrin Jalali, Agamemnon Krasoulis, Albert Thomas, Alberto Torres, Alexandre Gramfort, amourav, Andrea Navarrete, Andreas Mueller, Andrew Nystrom, assiaben, Aurélien Bellet, Bartosz Michałowski, Bartosz Telenczuk, bauks, BenjaStudio, bertrandhaut, Bharat Raghunathan, brentfagan, Bryan Woods, Cat Chenal, Cheuk Ting Ho, Chris Choe, Christos Aridas, Clément Doumouro, Cole Smith, Connossor, Corey Levinson, Dan Ellis, Dan Stine, Danylo Baibak, daten-kieker, Denis Kataev, Didi Bar-Zev, Dillon Gardner, Dmitry Mottl, Dmitry Vukolov, Dougal J. Sutherland, Dowon, drewmjohnston, Dror Atariah, Edward J Brown, Ekaterina Krivich, Elizabeth Sander, Emmanuel Arias, Eric Chang, Eric Larson, Erich Schubert, esvhd, Falak, Feda Curic, Federico Caselli, Frank Hoang, Fibinse Xavier\`, Finn O'Shea, Gabriel Marzinotto, Gabriel Vacaliuc, Gabriele Calvo, Gael Varoquaux, GauravAhlawat, Giuseppe Vettigli, Greg Gandenberger, Guillaume Fournier, Guillaume Lemaitre, Gustavo De Mari Pereira, Hanmin Qin, haroldfox, hhu-luqi, Hunter McGushion, Ian Sanders, JackLangerman, Jacopo Notarstefano, jakirkham, James Bourbeau, Jan Koch, Jan S, janvanrijn, Jarrod Millman, jdethurens, jeremiedbb, JF, joaak, Joan Massich, Joel Nothman, Jonathan Ohayon, Joris Van den Bossche, josephsalmon, Jérémie Méhault, Katrin Leinweber, ken, kms15, Koen, Kossori Aruku, Krishna Sangeeth, Kuai Yu, Kulbear, Kushal Chauhan, Kyle Jackson, Lakshya KD, Leandro Hermida, Lee Yi Jie Joel, Lily Xiong, Lisa Sarah Thomas, Loic Esteve, louib, luk-f-a, maikia, mail-liam, Manimaran, Manuel López-Ibáñez, Marc Torrellas, Marco Gaido, Marco Gorelli, MarcoGorelli, marineLM, Mark Hannel, Martin Gubri, Masstran, mathurinm, Matthew Roeschke, Max Copeland, melsyt, mferrari3, Mickaël Schoentgen, Ming Li, Mitar, Mohammad Aftab, Mohammed AbdelAal, Mohammed Ibraheem, Muhammad Hassaan Rafique, mwestt, Naoya Iijima, Nicholas Smith, Nicolas Goix, Nicolas Hug, Nikolay Shebanov, Oleksandr Pavlyk, Oliver Rausch, Olivier Grisel, Orestis, Osman, Owen Flanagan, Paul Paczuski, Pavel Soriano, pavlos kallis, Pawel Sendyk, peay, Peter, Peter Cock, Peter Hausamann, Peter Marko, Pierre Glaser, pierretallotte, Pim de Haan, Piotr Szymański, Prabakaran Kumaresshan, Pradeep Reddy Raamana, Prathmesh Savale, Pulkit Maloo, Quentin Batista, Radostin Stoyanov, Raf Baluyot, Rajdeep Dua, Ramil Nugmanov, Raúl García Calvo, Rebekah Kim, Reshama Shaikh, Rohan Lekhwani, Rohan Singh, Rohan Varma, Rohit Kapoor, Roman Feldbauer, Roman Yurchak, Romuald M, Roopam Sharma, Ryan, Rüdiger Busche, Sam Waterbury, Samuel O. Ronsin, SandroCasagrande, Scott Cole, Scott Lowe, Sebastian Raschka, Shangwu Yao, Shivam Kotwalia, Shiyu Duan, smarie, Sriharsha Hatwar, Stephen Hoover, Stephen Tierney, Stéphane Couvreur, surgan12, SylvainLan, TakingItCasual, Tashay Green, thibsej, Thomas Fan, Thomas J Fan, Thomas Moreau, Tom Dupré la Tour, Tommy, Tulio Casagrande, Umar Farouk Umar, Utkarsh Upadhyay, Vinayak Mehta, Vishaal Kapoor, Vivek Kumar, Vlad Niculae, vqean3, Wenhao Zhang, William de Vazelhes, xhan, Xing Han Lu, xinyuliu12, Yaroslav Halchenko, Zach Griffith, Zach Miller, Zayd Hammoudeh, Zhuyi Xue, Zijie (ZJ) Poh, ^\_\_^

---

v0.22.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.22

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_0\_22\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_0\_22\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 0.22.2.post1

**March 3 2020**

The 0.22.2.post1 release includes a packaging fix for the source distribution but the content of the packages is otherwise identical to the content of the wheels with the 0.22.2 version (without the .post1 suffix). Both contain the following changes.

### Changelog

#### `sklearn.impute`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Reduce <span class="title-ref">impute.KNNImputer</span> asymptotic memory usage by chunking pairwise distance computation. `16397` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.plot\_roc\_curve</span> where the name of the estimator was passed in the <span class="title-ref">metrics.RocCurveDisplay</span> instead of the parameter <span class="title-ref">name</span>. It results in a different plot when calling <span class="title-ref">metrics.RocCurveDisplay.plot</span> for the subsequent times. `16500` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.plot\_precision\_recall\_curve</span> where the name of the estimator was passed in the <span class="title-ref">metrics.PrecisionRecallDisplay</span> instead of the parameter <span class="title-ref">name</span>. It results in a different plot when calling <span class="title-ref">metrics.PrecisionRecallDisplay.plot</span> for the subsequent times. `16505` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug which converted a list of arrays into a 2-D object array instead of a 1-D array containing NumPy arrays. This bug was affecting <span class="title-ref">neighbors.NearestNeighbors.radius\_neighbors</span>. `16076` by `Guillaume Lemaitre <glemaitre>` and `Alex Shacked <alexshacked>`.

## Version 0.22.1

**January 2 2020**

This is a bug-fix release to primarily resolve some packaging issues in version 0.22.0. It also includes minor documentation improvements and some bug fixes.

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.KMeans</span> with `algorithm="elkan"` now uses the same stopping criterion as with the default `algorithm="full"`. `15930` by `inder128`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">inspection.permutation\_importance</span> will return the same <span class="title-ref">importances</span> when a <span class="title-ref">random\_state</span> is given for both <span class="title-ref">n\_jobs=1</span> or <span class="title-ref">n\_jobs\>1</span> both with shared memory backends (thread-safety) and isolated memory, process-based backends. Also avoid casting the data as object dtype and avoid read-only error on large dataframes with <span class="title-ref">n\_jobs\>1</span> as reported in `15810`. Follow-up of `15898` by `Shivam Gargsya <shivamgargsya>`. `15933` by `Guillaume Lemaitre <glemaitre>` and [Olivier Grisel](https://twitter.com/ogrisel).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">inspection.plot\_partial\_dependence</span> and <span class="title-ref">inspection.PartialDependenceDisplay.plot</span> now consistently checks the number of axes passed in. `15760` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.plot\_confusion\_matrix</span> now raises error when <span class="title-ref">normalize</span> is invalid. Previously, it runs fine with no normalization. `15888` by [Hanmin Qin](https://github.com/qinhanmin2014).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.plot\_confusion\_matrix</span> now colors the label color correctly to maximize contrast with its background. `15936` by [Thomas Fan](https://github.com/thomasjpfan) and `DizietAsahi`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.classification\_report</span> does no longer ignore the value of the `zero_division` keyword argument. `15879` by `Bibhash Chandra Mitra <Bibyutatsu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.plot\_confusion\_matrix</span> to correctly pass the <span class="title-ref">values\_format</span> parameter to the <span class="title-ref">metrics.ConfusionMatrixDisplay</span> plot() call. `15937` by `Stephen Blystone <blynotes>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> accept scalar values provided in <span class="title-ref">fit\_params</span>. Change in 0.22 was breaking backward compatibility. `15863` by `Adrin Jalali <adrinjalali>` and `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Removed <span class="title-ref">abstractmethod</span> decorator for the method <span class="title-ref">\_check\_X</span> in <span class="title-ref">naive\_bayes.BaseNB</span> that could break downstream projects inheriting from this deprecated public base class. `15996` by `Brigitta Sipőcz <bsipocz>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.QuantileTransformer</span> now guarantees the <span class="title-ref">quantiles\_</span> attribute to be completely sorted in non-decreasing manner. `15751` by `Tirth Patel <tirthasheshpatel>`.

#### `sklearn.semi_supervised`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">semi\_supervised.LabelPropagation</span> and <span class="title-ref">semi\_supervised.LabelSpreading</span> now allow callable kernel function to return sparse weight matrix. `15868` by `Niklas Smedemark-Margulies <nik-sm>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.check\_array</span> now correctly converts pandas DataFrame with boolean columns to floats. `15797` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.validation.check\_is\_fitted</span> accepts back an explicit `attributes` argument to check for specific attributes as explicit markers of a fitted estimator. When no explicit `attributes` are provided, only the attributes that end with a underscore and do not start with double underscore are used as "fitted" markers. The `all_or_any` argument is also no longer deprecated. This change is made to restore some backward compatibility with the behavior of this utility in version 0.21. `15947` by [Thomas Fan](https://github.com/thomasjpfan).

## Version 0.22.0

**December 3 2019**

### Website update

[Our website](https://scikit-learn.org/) was revamped and given a fresh new look. `14849` by [Thomas Fan](https://github.com/thomasjpfan).

### Clear definition of the public API

Scikit-learn has a public API, and a private API.

We do our best not to break the public API, and to only introduce backward-compatible changes that do not require any user action. However, in cases where that's not possible, any change to the public API is subject to a deprecation cycle of two minor versions. The private API isn't publicly documented and isn't subject to any deprecation cycle, so users should not rely on its stability.

A function or object is public if it is documented in the [API Reference](https://scikit-learn.org/dev/modules/classes.html) and if it can be imported with an import path without leading underscores. For example `sklearn.pipeline.make_pipeline` is public, while <span class="title-ref">sklearn.pipeline.\_name\_estimators</span> is private. `sklearn.ensemble._gb.BaseEnsemble` is private too because the whole <span class="title-ref">\_gb</span> module is private.

Up to 0.22, some tools were de-facto public (no leading underscore), while they should have been private in the first place. In version 0.22, these tools have been made properly private, and the public API space has been cleaned. In addition, importing from most sub-modules is now deprecated: you should for example use `from sklearn.cluster import Birch` instead of `from sklearn.cluster.birch import Birch` (in practice, `birch.py` has been moved to `_birch.py`).

\> **Note** \> All the tools in the public API should be documented in the [API Reference](https://scikit-learn.org/dev/modules/classes.html). If you find a public tool (without leading underscore) that isn't in the API reference, that means it should either be private or documented. Please let us know by opening an issue\!

This work was tracked in [issue 9250](https://github.com/scikit-learn/scikit-learn/issues/9250) and [issue 12927](https://github.com/scikit-learn/scikit-learn/issues/12927).

### Deprecations: using `FutureWarning` from now on

When deprecating a feature, previous versions of scikit-learn used to raise a `DeprecationWarning`. Since the `DeprecationWarnings` aren't shown by default by Python, scikit-learn needed to resort to a custom warning filter to always show the warnings. That filter would sometimes interfere with users custom warning filters.

Starting from version 0.22, scikit-learn will show `FutureWarnings` for deprecations, [as recommended by the Python documentation](https://docs.python.org/3/library/exceptions.html#FutureWarning). `FutureWarnings` are always shown by default by Python, so the custom filter has been removed and scikit-learn no longer hinders with user filters. `15080` by [Nicolas Hug](https://github.com/NicolasHug).

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - <span class="title-ref">cluster.KMeans</span> when <span class="title-ref">n\_jobs=1</span>. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">decomposition.SparseCoder</span>, <span class="title-ref">decomposition.DictionaryLearning</span>, and <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">decomposition.SparseCoder</span> with <span class="title-ref">algorithm='lasso\_lars'</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">decomposition.SparsePCA</span> where <span class="title-ref">normalize\_components</span> has no effect due to deprecation.
  - <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`, `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}`, `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}`.
  - <span class="title-ref">impute.IterativeImputer</span> when <span class="title-ref">X</span> has features with no missing values. `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}`
  - <span class="title-ref">linear\_model.Ridge</span> when <span class="title-ref">X</span> is sparse. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">model\_selection.StratifiedKFold</span> and any use of <span class="title-ref">cv=int</span> with a classifier. `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`
  - <span class="title-ref">cross\_decomposition.CCA</span> when using scipy \>= 1.3 `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Changelog

#### `sklearn.base`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` From version 0.24 <span class="title-ref">base.BaseEstimator.get\_params</span> will raise an AttributeError rather than return None for parameters that are in the estimator's constructor but not stored as attributes on the instance. `14464` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug that made <span class="title-ref">calibration.CalibratedClassifierCV</span> fail when given a <span class="title-ref">sample\_weight</span> parameter of type <span class="title-ref">list</span> (in the case where <span class="title-ref">sample\_weights</span> are not supported by the wrapped estimator). `13575` by `William de Vazelhes <wdevazelhes>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">cluster.SpectralClustering</span> now accepts precomputed sparse neighbors graph as input. `10482` by [Tom Dupre la Tour](https://github.com/TomDLT) and `Kumar Ashutosh <thechargedneutron>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.SpectralClustering</span> now accepts a `n_components` parameter. This parameter extends <span class="title-ref">SpectralClustering</span> class functionality to match <span class="title-ref">cluster.spectral\_clustering</span>. `13726` by `Shuzhe Xiao <fdas3213>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">cluster.KMeans</span> produced inconsistent results between <span class="title-ref">n\_jobs=1</span> and <span class="title-ref">n\_jobs\>1</span> due to the handling of the random state. `9288` by `Bryan Yang <bryanyang0528>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">elkan</span> algorithm in <span class="title-ref">cluster.KMeans</span> was producing Segmentation Fault on large arrays due to integer index overflow. `15057` by `Vladimir Korolev <balodja>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">\~cluster.MeanShift</span> now accepts a `max_iter` with a default value of 300 instead of always using the default 300. It also now exposes an `n_iter_` indicating the maximum number of iterations performed on each seed. `15120` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.AgglomerativeClustering</span> and <span class="title-ref">cluster.FeatureAgglomeration</span> now raise an error if <span class="title-ref">affinity='cosine'</span> and <span class="title-ref">X</span> has samples that are all-zeros. `7943` by `mthorrell`.

#### `sklearn.compose`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Adds <span class="title-ref">compose.make\_column\_selector</span> which is used with <span class="title-ref">compose.ColumnTransformer</span> to select DataFrame columns on the basis of name and dtype. `12303` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">compose.ColumnTransformer</span> which failed to select the proper columns when using a boolean list, with NumPy older than 1.12. `14510` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">compose.TransformedTargetRegressor</span> which did not pass <span class="title-ref">\*\*fit\_params</span> to the underlying regressor. `14890` by `Miguel Cabrera <mfcabrera>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">compose.ColumnTransformer</span> now requires the number of features to be consistent between <span class="title-ref">fit</span> and <span class="title-ref">transform</span>. A <span class="title-ref">FutureWarning</span> is raised now, and this will raise an error in 0.24. If the number of features isn't consistent and negative indexing is used, an error is raised. `14544` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.cross_decomposition`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">cross\_decomposition.PLSCanonical</span> and <span class="title-ref">cross\_decomposition.PLSRegression</span> have a new function `inverse_transform` to transform data to the original space. `15304` by `Jaime Ferrando Huertas <jiwidi>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.KernelPCA</span> now properly checks the eigenvalues found by the solver for numerical or conditioning issues. This ensures consistency of results across solvers (different choices for `eigen_solver`), including approximate solvers such as `'randomized'` and `'lobpcg'` (see `12068`). `12145` by `Sylvain Marié <smarie>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">cross\_decomposition.PLSCanonical</span> and <span class="title-ref">cross\_decomposition.PLSRegression</span> were raising an error when fitted with a target matrix <span class="title-ref">Y</span> in which the first column was constant. `13609` by `Camila Williamson <camilaagw>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cross\_decomposition.CCA</span> now produces the same results with scipy 1.3 and previous scipy versions. `15661` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.datasets`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">datasets.fetch\_openml</span> now supports heterogeneous data using pandas by setting <span class="title-ref">as\_frame=True</span>. `13902` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">datasets.fetch\_openml</span> now includes the <span class="title-ref">target\_names</span> in the returned Bunch. `15160` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The parameter <span class="title-ref">return\_X\_y</span> was added to <span class="title-ref">datasets.fetch\_20newsgroups</span> and <span class="title-ref">datasets.fetch\_olivetti\_faces</span> . `14259` by `Sourav Singh <souravsingh>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.make\_classification</span> now accepts array-like <span class="title-ref">weights</span> parameter, i.e. list or numpy.array, instead of list only. `14764` by `Cat Chenal <CatChenal>`.

  -   - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The parameter <span class="title-ref">normalize</span> was added to  
        <span class="title-ref">datasets.fetch\_20newsgroups\_vectorized</span>. `14740` by `Stéphan Tulkens <stephantul>`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">datasets.fetch\_openml</span>, which failed to load an OpenML dataset that contains an ignored feature. `14623` by `Sarra Habchi <HabchiSarra>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">decomposition.NMF</span> with <span class="title-ref">solver="mu"</span> fitted on sparse input matrices now uses batching to avoid briefly allocating an array with size (\#non-zero elements, n\_components). `15257` by `Mart Willocx <Maocx>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.dict\_learning</span> and <span class="title-ref">decomposition.dict\_learning\_online</span> now accept <span class="title-ref">method\_max\_iter</span> and pass it to <span class="title-ref">decomposition.sparse\_encode</span>. `12650` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.SparseCoder</span>, <span class="title-ref">decomposition.DictionaryLearning</span>, and <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> now take a <span class="title-ref">transform\_max\_iter</span> parameter and pass it to either <span class="title-ref">decomposition.dict\_learning()</span> or <span class="title-ref">decomposition.sparse\_encode()</span>. `12650` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.IncrementalPCA</span> now accepts sparse matrices as input, converting them to dense in batches thereby avoiding the need to store the entire dense matrix at once. `13960` by `Scott Gigante <scottgigante>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.sparse\_encode()</span> now passes the <span class="title-ref">max\_iter</span> to the underlying <span class="title-ref">linear\_model.LassoLars</span> when <span class="title-ref">algorithm='lasso\_lars'</span>. `12650` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.dummy`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">dummy.DummyClassifier</span> now handles checking the existence of the provided constant in multiouput cases. `14908` by `Martina G. Vilas <martinagvilas>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of the <span class="title-ref">strategy</span> parameter in <span class="title-ref">dummy.DummyClassifier</span> will change from <span class="title-ref">'stratified'</span> in version 0.22 to <span class="title-ref">'prior'</span> in 0.24. A FutureWarning is raised when the default value is used. `15382` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `outputs_2d_` attribute is deprecated in <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span>. It is equivalent to `n_outputs > 1`. `14933` by [Nicolas Hug](https://github.com/NicolasHug)

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span> to stack predictors using a final classifier or regressor. `11047` by `Guillaume Lemaitre
    <glemaitre>` and `Caio Oliveira <caioaao>` and `15138` by `Jon Cusick <jcusick13>`..

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Many improvements were made to <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and \`ensemble.HistGradientBoostingRegressor\`:
    
      - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Estimators now natively support dense data with missing values both for training and predicting. They also support infinite values. `13911` and `14406` by [Nicolas Hug](https://github.com/NicolasHug), [Adrin Jalali](https://github.com/adrinjalali) and [Olivier Grisel](https://twitter.com/ogrisel).
      - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Estimators now have an additional <span class="title-ref">warm\_start</span> parameter that enables warm starting. `14012` by `Johann Faouzi <johannfaouzi>`.
      - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">inspection.partial\_dependence</span> and <span class="title-ref">inspection.plot\_partial\_dependence</span> now support the fast 'recursion' method for both estimators. `13769` by [Nicolas Hug](https://github.com/NicolasHug).
      - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` for <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> the training loss or score is now monitored on a class-wise stratified subsample to preserve the class balance of the original training set. `14194` by `Johann Faouzi <johannfaouzi>`.
      - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> now supports the 'least\_absolute\_deviation' loss. `13896` by [Nicolas Hug](https://github.com/NicolasHug).
      - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Estimators now bin the training and validation data separately to avoid any data leak. `13933` by [Nicolas Hug](https://github.com/NicolasHug).
      - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where early stopping would break with string targets. `14710` by [Guillaume Lemaitre](https://github.com/glemaitre).
      - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> now raises an error if `categorical_crossentropy` loss is given for a binary classification problem. `14869` by [Adrin Jalali](https://github.com/adrinjalali).
    
    Note that pickles from 0.21 will not work in 0.22.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Addition of `max_samples` argument allows limiting size of bootstrap samples to be less than size of dataset. Added to <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>. `14682` by `Matt Hancock <notmatthancock>` and `5963` by `Pablo Duboue <DrDub>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.VotingClassifier.predict\_proba</span> will no longer be present when <span class="title-ref">voting='hard'</span>. `14287` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">named\_estimators\_</span> attribute in <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span> now correctly maps to dropped estimators. Previously, the <span class="title-ref">named\_estimators\_</span> mapping was incorrect whenever one of the estimators was dropped. `15375` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Run by default <span class="title-ref">utils.estimator\_checks.check\_estimator</span> on both <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span>. It leads to solve issues regarding shape consistency during <span class="title-ref">predict</span> which was failing when the underlying estimators were not outputting consistent array dimensions. Note that it should be replaced by refactoring the common tests in the future. `14305` by [Guillaume Lemaitre](https://github.com/glemaitre).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.AdaBoostClassifier</span> computes probabilities based on the decision function as in the literature. Thus, <span class="title-ref">predict</span> and <span class="title-ref">predict\_proba</span> give consistent results. `14114` by [Guillaume Lemaitre](https://github.com/glemaitre).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Stacking and Voting estimators now ensure that their underlying estimators are either all classifiers or all regressors. <span class="title-ref">ensemble.StackingClassifier</span>, <span class="title-ref">ensemble.StackingRegressor</span>, and <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span> now raise consistent error messages. `15084` by [Guillaume Lemaitre](https://github.com/glemaitre).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.AdaBoostRegressor</span> where the loss should be normalized by the max of the samples with non-null weights only. `14294` by [Guillaume Lemaitre](https://github.com/glemaitre).

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `presort` is now deprecated in <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span>, and the parameter has no effect. Users are recommended to use <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> instead. `14907` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A warning will now be raised if a parameter choice means that another parameter will be unused on calling the fit() method for <span class="title-ref">feature\_extraction.text.HashingVectorizer</span>, <span class="title-ref">feature\_extraction.text.CountVectorizer</span> and <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span>. `14602` by `Gaurav Chawla <getgaurav2>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Functions created by `build_preprocessor` and `build_analyzer` of <span class="title-ref">feature\_extraction.text.VectorizerMixin</span> can now be pickled. `14430` by `Dillon Niederhut <deniederhut>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_extraction.text.strip\_accents\_unicode</span> now correctly removes accents from strings that are in NFKD normalized form. `15100` by `Daniel Grady <DGrady>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug that caused <span class="title-ref">feature\_extraction.DictVectorizer</span> to raise an <span class="title-ref">OverflowError</span> during the <span class="title-ref">transform</span> operation when producing a <span class="title-ref">scipy.sparse</span> matrix on large input data. `15463` by `Norvan Sahiner <norvan>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecated unused <span class="title-ref">copy</span> param for <span class="title-ref">feature\_extraction.text.TfidfVectorizer.transform</span> it will be removed in v0.24. `14520` by `Guillem G. Subies <guillemgsubies>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Updated the following `sklearn.feature_selection` estimators to allow NaN/Inf values in `transform` and `fit`: <span class="title-ref">feature\_selection.RFE</span>, <span class="title-ref">feature\_selection.RFECV</span>, <span class="title-ref">feature\_selection.SelectFromModel</span>, and <span class="title-ref">feature\_selection.VarianceThreshold</span>. Note that if the underlying estimator of the feature selector does not allow NaN/Inf then it will still error, but the feature selectors themselves no longer enforce this restriction unnecessarily. `11635` by `Alec Peters <adpeters>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">feature\_selection.VarianceThreshold</span> with <span class="title-ref">threshold=0</span> did not remove constant features due to numerical instability, by using range rather than variance in this case. `13704` by `Roddy MacSween <rlms>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Gaussian process models on structured data: <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> and <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span> can now accept a list of generic objects (e.g. strings, trees, graphs, etc.) as the `X` argument to their training/prediction methods. A user-defined kernel should be provided for computing the kernel matrix among the generic objects, and should inherit from <span class="title-ref">gaussian\_process.kernels.GenericKernelMixin</span> to notify the GPR/GPC model that it handles non-vectorial samples. `15557` by `Yu-Hang Tang <yhtang>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">gaussian\_process.GaussianProcessClassifier.log\_marginal\_likelihood</span> and <span class="title-ref">gaussian\_process.GaussianProcessRegressor.log\_marginal\_likelihood</span> now accept a `clone_kernel=True` keyword argument. When set to `False`, the kernel attribute is modified, but may result in a performance improvement. `14378` by `Masashi Shibata <c-bata>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` From version 0.24 <span class="title-ref">gaussian\_process.kernels.Kernel.get\_params</span> will raise an `AttributeError` rather than return `None` for parameters that are in the estimator's constructor but not stored as attributes on the instance. `14464` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.impute`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">impute.KNNImputer</span>, to impute missing values using k-Nearest Neighbors. `12852` by `Ashim Bhattarai <ashimb9>` and [Thomas Fan](https://github.com/thomasjpfan) and `15010` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">impute.IterativeImputer</span> has new <span class="title-ref">skip\_compute</span> flag that is False by default, which, when True, will skip computation on features that have no missing values during the fit phase. `13773` by `Sergey Feldman <sergeyf>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">impute.MissingIndicator.fit\_transform</span> avoid repeated computation of the masked matrix. `14356` by `Harsh Soni <harsh020>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">impute.IterativeImputer</span> now works when there is only one feature. By `Sergey Feldman <sergeyf>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">impute.IterativeImputer</span> where features where imputed in the reverse desired order with `imputation_order` either `"ascending"` or `"descending"`. `15393` by `Venkatachalam N <venkyyuvy>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">inspection.permutation\_importance</span> has been added to measure the importance of each feature in an arbitrary trained model with respect to a given scoring function. `13146` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">inspection.partial\_dependence</span> and <span class="title-ref">inspection.plot\_partial\_dependence</span> now support the fast 'recursion' method for <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>. `13769` by [Nicolas Hug](https://github.com/NicolasHug).

<!-- end list -->

  - \- `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">inspection.plot\_partial\_dependence</span> has been extended to  
    now support the new visualization API described in the \[User Guide \<visualizations\>\](\#user-guide

\--\<visualizations\>). `14646` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">inspection.partial\_dependence</span> accepts pandas DataFrame and <span class="title-ref">pipeline.Pipeline</span> containing <span class="title-ref">compose.ColumnTransformer</span>. In addition <span class="title-ref">inspection.plot\_partial\_dependence</span> will use the column names by default when a dataframe is passed. `14028` and `15429` by [Guillaume Lemaitre](https://github.com/glemaitre).

#### `sklearn.kernel_approximation`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">kernel\_approximation.Nystroem</span> raised a <span class="title-ref">KeyError</span> when using <span class="title-ref">kernel="precomputed"</span>. `14706` by `Venkatachalam N <venkyyuvy>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The 'liblinear' logistic regression solver is now faster and requires less memory. `14108`, `14170`, `14296` by `Alex Henrie <alexhenrie>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.BayesianRidge</span> now accepts hyperparameters `alpha_init` and `lambda_init` which can be used to set the initial value of the maximization procedure in `fit`. `13618` by `Yoshihiro Uchida <c56pony>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.Ridge</span> now correctly fits an intercept when <span class="title-ref">X</span> is sparse, <span class="title-ref">solver="auto"</span> and <span class="title-ref">fit\_intercept=True</span>, because the default solver in this configuration has changed to <span class="title-ref">sparse\_cg</span>, which can fit an intercept with sparse data. `13995` by `Jérôme Dockès <jeromedockes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.Ridge</span> with <span class="title-ref">solver='sag'</span> now accepts F-ordered and non-contiguous arrays and makes a conversion instead of failing. `14458` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.LassoCV</span> no longer forces `precompute=False` when fitting the final model. `14591` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span> now correctly scores when <span class="title-ref">cv=None</span>. `14864` by `Venkatachalam N <venkyyuvy>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.LogisticRegressionCV</span> where the `scores_`, `n_iter_` and `coefs_paths_` attribute would have a wrong ordering with `penalty='elastic-net'`. `15044` by [Nicolas Hug](https://github.com/NicolasHug)
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.MultiTaskLassoCV</span> and <span class="title-ref">linear\_model.MultiTaskElasticNetCV</span> with X of dtype int and <span class="title-ref">fit\_intercept=True</span>. `15086` by `Alex Gramfort <agramfort>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The liblinear solver now supports `sample_weight`. `15038` by [Guillaume Lemaitre](https://github.com/glemaitre).

#### `sklearn.manifold`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">manifold.Isomap</span>, <span class="title-ref">manifold.TSNE</span>, and <span class="title-ref">manifold.SpectralEmbedding</span> now accept precomputed sparse neighbors graph as input. `10482` by [Tom Dupre la Tour](https://github.com/TomDLT) and `Kumar Ashutosh <thechargedneutron>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Exposed the `n_jobs` parameter in <span class="title-ref">manifold.TSNE</span> for multi-core calculation of the neighbors graph. This parameter has no impact when `metric="precomputed"` or (`metric="euclidean"` and `method="exact"`). `15082` by [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improved efficiency of <span class="title-ref">manifold.TSNE</span> when `method="barnes-hut"` by computing the gradient in parallel. `13213` by `Thomas Moreau <tommoral>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">manifold.spectral\_embedding</span> (and therefore <span class="title-ref">manifold.SpectralEmbedding</span> and <span class="title-ref">cluster.SpectralClustering</span>) computed wrong eigenvalues with `eigen_solver='amg'` when `n_samples < 5 * n_components`. `14647` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">manifold.spectral\_embedding</span> used in <span class="title-ref">manifold.SpectralEmbedding</span> and <span class="title-ref">cluster.SpectralClustering</span> where `eigen_solver="amg"` would sometimes result in a LinAlgError. `13393` by `Andrew Knyazev <lobpcg>` `13707` by `Scott White <whitews>`
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecate `training_data_` unused attribute in <span class="title-ref">manifold.Isomap</span>. `10482` by [Tom Dupre la Tour](https://github.com/TomDLT).

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">metrics.plot\_roc\_curve</span> has been added to plot roc curves. This function introduces the visualization API described in the \[User Guide \<visualizations\>\](\#user-guide-\<visualizations\>). `14357` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added a new parameter `zero_division` to multiple classification metrics: <span class="title-ref">metrics.precision\_score</span>, <span class="title-ref">metrics.recall\_score</span>, <span class="title-ref">metrics.f1\_score</span>, <span class="title-ref">metrics.fbeta\_score</span>, <span class="title-ref">metrics.precision\_recall\_fscore\_support</span>, <span class="title-ref">metrics.classification\_report</span>. This allows to set returned value for ill-defined metrics. `14900` by `Marc Torrellas Socastro <marctorrellas>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added the <span class="title-ref">metrics.pairwise.nan\_euclidean\_distances</span> metric, which calculates euclidean distances in the presence of missing values. `12852` by `Ashim Bhattarai <ashimb9>` and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` New ranking metrics <span class="title-ref">metrics.ndcg\_score</span> and <span class="title-ref">metrics.dcg\_score</span> have been added to compute Discounted Cumulative Gain and Normalized Discounted Cumulative Gain. `9951` by `Jérôme
    Dockès <jeromedockes>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.plot\_precision\_recall\_curve</span> has been added to plot precision recall curves. `14936` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.plot\_confusion\_matrix</span> has been added to plot confusion matrices. `15083` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added multiclass support to <span class="title-ref">metrics.roc\_auc\_score</span> with corresponding scorers <span class="title-ref">'roc\_auc\_ovr'</span>, <span class="title-ref">'roc\_auc\_ovo'</span>, <span class="title-ref">'roc\_auc\_ovr\_weighted'</span>, and <span class="title-ref">'roc\_auc\_ovo\_weighted'</span>. `12789` and `15274` by `Kathy Chen <kathyxchen>`, `Mohamed Maskani <maskani-moh>`, and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">metrics.mean\_tweedie\_deviance</span> measuring the Tweedie deviance for a given `power` parameter. Also add mean Poisson deviance <span class="title-ref">metrics.mean\_poisson\_deviance</span> and mean Gamma deviance <span class="title-ref">metrics.mean\_gamma\_deviance</span> that are special cases of the Tweedie deviance for `power=1` and `power=2` respectively. `13938` by `Christian Lorentzen <lorentzenchr>` and [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improved performance of <span class="title-ref">metrics.pairwise.manhattan\_distances</span> in the case of sparse matrices. `15049` by <span class="title-ref">Paolo Toccaceli \<ptocca\></span>.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The parameter `beta` in <span class="title-ref">metrics.fbeta\_score</span> is updated to accept the zero and <span class="title-ref">float('+inf')</span> value. `13231` by `Dong-hee Na <corona10>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added parameter `squared` in <span class="title-ref">metrics.mean\_squared\_error</span> to return root mean squared error. `13467` by `Urvang Patel <urvang96>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Allow computing averaged metrics in the case of no true positives. `14595` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Multilabel metrics now supports list of lists as input. `14865` `Srivatsan Ramesh <srivatsan-ramesh>`, `Herilalaina Rakotoarison <herilalaina>`, `Léonard Binet <leonardbinet>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.median\_absolute\_error</span> now supports `multioutput` parameter. `14732` by `Agamemnon Krasoulis <agamemnonc>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` 'roc\_auc\_ovr\_weighted' and 'roc\_auc\_ovo\_weighted' can now be used as the `scoring` parameter of model-selection tools. `14417` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.confusion\_matrix</span> accepts a parameters <span class="title-ref">normalize</span> allowing to normalize the confusion matrix by column, rows, or overall. `15625` by <span class="title-ref">Guillaume Lemaitre \<glemaitre\></span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise a ValueError in <span class="title-ref">metrics.silhouette\_score</span> when a precomputed distance matrix contains non-zero diagonal entries. `12258` by `Stephen Tierney <sjtrny>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `scoring="neg_brier_score"` should be used instead of `scoring="brier_score_loss"` which is now deprecated. `14898` by `Stefan Matcovici <stefan-matcovici>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improved performance of multimetric scoring in <span class="title-ref">model\_selection.cross\_validate</span>, <span class="title-ref">model\_selection.GridSearchCV</span>, and <span class="title-ref">model\_selection.RandomizedSearchCV</span>. `14593` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.learning\_curve</span> now accepts parameter `return_times` which can be used to retrieve computation times in order to plot model scalability (see learning\_curve example). `13938` by `Hadrien Reboul <H4dr1en>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.RandomizedSearchCV</span> now accepts lists of parameter distributions. `14549` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Reimplemented <span class="title-ref">model\_selection.StratifiedKFold</span> to fix an issue where one test set could be <span class="title-ref">n\_classes</span> larger than another. Test sets should now be near-equally sized. `14704` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">cv\_results\_</span> attribute of <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> now only contains unfitted estimators. This potentially saves a lot of memory since the state of the estimators isn't stored. `#15096` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">model\_selection.KFold</span> and <span class="title-ref">model\_selection.StratifiedKFold</span> now raise a warning if <span class="title-ref">random\_state</span> is set but <span class="title-ref">shuffle</span> is False. This will raise an error in 0.24.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">multioutput.MultiOutputClassifier</span> now has attribute `classes_`. `14629` by `Agamemnon Krasoulis <agamemnonc>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">multioutput.MultiOutputClassifier</span> now has <span class="title-ref">predict\_proba</span> as property and can be checked with <span class="title-ref">hasattr</span>. `15488` `15490` by `Rebekah Kim <rebekahkim>`

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">naive\_bayes.CategoricalNB</span> that implements the Categorical Naive Bayes classifier. `12569` by `Tim Bicker <timbicker>` and `Florian Wilhelm <FlorianWilhelm>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">neighbors.KNeighborsTransformer</span> and <span class="title-ref">neighbors.RadiusNeighborsTransformer</span>, which transform input dataset into a sparse neighbors graph. They give finer control on nearest neighbors computations and enable easy pipeline caching for multiple use. `10482` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">neighbors.KNeighborsClassifier</span>, <span class="title-ref">neighbors.KNeighborsRegressor</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>, and <span class="title-ref">neighbors.LocalOutlierFactor</span> now accept precomputed sparse neighbors graph as input. `10482` by [Tom Dupre la Tour](https://github.com/TomDLT) and `Kumar Ashutosh <thechargedneutron>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">neighbors.RadiusNeighborsClassifier</span> now supports predicting probabilities by using <span class="title-ref">predict\_proba</span> and supports more outlier\_label options: 'most\_frequent', or different outlier\_labels for multi-outputs. `9597` by `Wenbo Zhao <webber26232>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Efficiency improvements for <span class="title-ref">neighbors.RadiusNeighborsClassifier.predict</span>. `9597` by `Wenbo Zhao <webber26232>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KNeighborsRegressor</span> now throws error when <span class="title-ref">metric='precomputed'</span> and fit on non-square data. `14336` by `Gregory Dexter <gdex1>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">max\_fun</span> parameter in <span class="title-ref">neural\_network.BaseMultilayerPerceptron</span>, <span class="title-ref">neural\_network.MLPRegressor</span>, and <span class="title-ref">neural\_network.MLPClassifier</span> to give control over maximum number of function evaluation to not meet `tol` improvement. `9274` by `Daniel Perry <daniel-perry>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">pipeline.Pipeline</span> now supports `score_samples` if the final estimator does. `13806` by `Anaël Beaugnon <ab-anssi>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">fit</span> in <span class="title-ref">\~pipeline.FeatureUnion</span> now accepts <span class="title-ref">fit\_params</span> to pass to the underlying transformers. `15119` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">None</span> as a transformer is now deprecated in <span class="title-ref">pipeline.FeatureUnion</span>. Please use <span class="title-ref">'drop'</span> instead. `15053` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">preprocessing.PolynomialFeatures</span> is now faster when the input data is dense. `13290` by `Xavier Dupré <sdpython>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Avoid unnecessary data copy when fitting preprocessors <span class="title-ref">preprocessing.StandardScaler</span>, <span class="title-ref">preprocessing.MinMaxScaler</span>, <span class="title-ref">preprocessing.MaxAbsScaler</span>, <span class="title-ref">preprocessing.RobustScaler</span> and <span class="title-ref">preprocessing.QuantileTransformer</span> which results in a slight performance improvement. `13987` by [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` KernelCenterer now throws error when fit on non-square <span class="title-ref">preprocessing.KernelCenterer</span> `14336` by `Gregory Dexter <gdex1>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> now supports the <span class="title-ref">\_pairwise</span> property, which prevents an error during cross-validation for estimators with pairwise inputs (such as <span class="title-ref">neighbors.KNeighborsClassifier</span> when `metric` is set to 'precomputed'). `13925` by `Isaac S. Robson <isrobson>` and `15524` by `Xun Tang <xun-tang>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.NuSVC</span> now accept a `break_ties` parameter. This parameter results in `predict` breaking the ties according to the confidence values of `decision_function`, if `decision_function_shape='ovr'`, and the number of target classes \> 2. `12557` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` SVM estimators now throw a more specific error when <span class="title-ref">kernel='precomputed'</span> and fit on non-square data. `14336` by `Gregory Dexter <gdex1>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.NuSVR</span> and <span class="title-ref">svm.OneClassSVM</span> when received values negative or zero for parameter `sample_weight` in method fit(), generated an invalid model. This behavior occurred only in some border scenarios. Now in these cases, fit() will fail with an Exception. `14286` by `Alex Shacked <alexshacked>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">n\_support\_</span> attribute of <span class="title-ref">svm.SVR</span> and <span class="title-ref">svm.OneClassSVM</span> was previously non-initialized, and had size 2. It has now size 1 with the correct value. `15099` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` fixed a bug in <span class="title-ref">BaseLibSVM.\_sparse\_fit</span> where n\_SV=0 raised a ZeroDivisionError. `14894` by `Danna Naser <danna-naser>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The liblinear solver now supports `sample_weight`. `15038` by [Guillaume Lemaitre](https://github.com/glemaitre).

#### `sklearn.tree`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Adds minimal cost complexity pruning, controlled by `ccp_alpha`, to <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span>, <span class="title-ref">tree.ExtraTreeRegressor</span>, <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>, <span class="title-ref">ensemble.GradientBoostingClassifier</span>, and <span class="title-ref">ensemble.GradientBoostingRegressor</span>. `12887` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `presort` is now deprecated in <span class="title-ref">tree.DecisionTreeClassifier</span> and <span class="title-ref">tree.DecisionTreeRegressor</span>, and the parameter has no effect. `14907` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `classes_` and `n_classes_` attributes of <span class="title-ref">tree.DecisionTreeRegressor</span> are now deprecated. `15028` by `Mei Guan <meiguan>`, [Nicolas Hug](https://github.com/NicolasHug), and [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.utils`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span> can now generate checks by setting <span class="title-ref">generate\_only=True</span>. Previously, running <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span> will stop when the first check fails. With <span class="title-ref">generate\_only=True</span>, all checks can run independently and report the ones that are failing. Read more in \[rolling\_your\_own\_estimator\](\#rolling\_your\_own\_estimator). `14381` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added a pytest specific decorator, <span class="title-ref">\~utils.estimator\_checks.parametrize\_with\_checks</span>, to parametrize estimator checks for a list of estimators. `14381` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A new random variable, <span class="title-ref">utils.fixes.loguniform</span> implements a log-uniform random variable (e.g., for use in RandomizedSearchCV). For example, the outcomes `1`, `10` and `100` are all equally likely for `loguniform(1, 100)`. See `11232` by `Scott Sievert <stsievert>` and `Nathaniel Saul <sauln>`, and <span class="title-ref">SciPy PR 10815 \<https://github.com/scipy/scipy/pull/10815\></span>.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.safe\_indexing</span> (now deprecated) accepts an `axis` parameter to index array-like across rows and columns. The column indexing can be done on NumPy array, SciPy sparse matrix, and Pandas DataFrame. An additional refactoring was done. `14035` and `14475` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.extmath.safe\_sparse\_dot</span> works between 3D+ ndarray and sparse matrix. `14538` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.check\_array</span> is now raising an error instead of casting NaN to integer. `14872` by [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.check\_array</span> will now correctly detect numeric dtypes in pandas dataframes, fixing a bug where `float32` was upcast to `float64` unnecessarily. `15094` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The following utils have been deprecated and are now private:
      - `choose_check_classifiers_labels`
      - `enforce_estimator_tags_y`
      - `mocking.MockDataFrame`
      - `mocking.CheckingClassifier`
      - `optimize.newton_cg`
      - `random.random_choice_csc`
      - `utils.choose_check_classifiers_labels`
      - `utils.enforce_estimator_tags_y`
      - `utils.optimize.newton_cg`
      - `utils.random.random_choice_csc`
      - `utils.safe_indexing`
      - `utils.mocking`
      - `utils.fast_dict`
      - `utils.seq_dataset`
      - `utils.weight_vector`
      - `utils.fixes.parallel_helper` (removed)
      - All of `utils.testing` except for `all_estimators` which is now in `utils`.

#### `sklearn.isotonic`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">isotonic.IsotonicRegression.fit</span> raised error when <span class="title-ref">X.dtype == 'float32'</span> and <span class="title-ref">X.dtype \!= y.dtype</span>. `14902` by `Lucas <lostcoaster>`.

#### Miscellaneous

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Port <span class="title-ref">lobpcg</span> from SciPy which implement some bug fixes but only available in 1.3+. `13609` and `14971` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Scikit-learn now converts any input data structure implementing a duck array to a numpy array (using `__array__`) to ensure consistent behavior instead of relying on `__array_function__` (see [NEP 18](https://numpy.org/neps/nep-0018-array-function-protocol.html)). `14702` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Replace manual checks with `check_is_fitted`. Errors thrown when using a non-fitted estimators are now more uniform. `13013` by `Agamemnon Krasoulis <agamemnonc>`.

### Changes to estimator checks

These changes mostly affect library developers.

  - Estimators are now expected to raise a `NotFittedError` if `predict` or `transform` is called before `fit`; previously an `AttributeError` or `ValueError` was acceptable. `13013` by by `Agamemnon Krasoulis <agamemnonc>`.
  - Binary only classifiers are now supported in estimator checks. Such classifiers need to have the <span class="title-ref">binary\_only=True</span> estimator tag. `13875` by [Trevor Stephens](http://trevorstephens.com/).
  - Estimators are expected to convert input data (`X`, `y`, `sample_weights`) to <span class="title-ref">numpy.ndarray</span> and never call `__array_function__` on the original datatype that is passed (see [NEP 18](https://numpy.org/neps/nep-0018-array-function-protocol.html)). `14702` by [Andreas Müller](https://amueller.github.io/).
  - <span class="title-ref">requires\_positive\_X</span> estimator tag (for models that require X to be non-negative) is now used by <span class="title-ref">utils.estimator\_checks.check\_estimator</span> to make sure a proper error message is raised if X contains some negative entries. `14680` by `Alex Gramfort <agramfort>`.
  - Added check that pairwise estimators raise error on non-square data `14336` by `Gregory Dexter <gdex1>`.
  - Added two common multioutput estimator tests <span class="title-ref">utils.estimator\_checks.check\_classifier\_multioutput</span> and <span class="title-ref">utils.estimator\_checks.check\_regressor\_multioutput</span>. `13392` by `Rok Mihevc <rok>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Added `check_transformer_data_not_an_array` to checks where missing
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The estimators tags resolution now follows the regular MRO. They used to be overridable only once. `14884` by [Andreas Müller](https://amueller.github.io/).

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.21, including:

Aaron Alphonsus, Abbie Popa, Abdur-Rahmaan Janhangeer, abenbihi, Abhinav Sagar, Abhishek Jana, Abraham K. Lagat, Adam J. Stewart, Aditya Vyas, Adrin Jalali, Agamemnon Krasoulis, Alec Peters, Alessandro Surace, Alexandre de Siqueira, Alexandre Gramfort, alexgoryainov, Alex Henrie, Alex Itkes, alexshacked, Allen Akinkunle, Anaël Beaugnon, Anders Kaseorg, Andrea Maldonado, Andrea Navarrete, Andreas Mueller, Andreas Schuderer, Andrew Nystrom, Angela Ambroz, Anisha Keshavan, Ankit Jha, Antonio Gutierrez, Anuja Kelkar, Archana Alva, arnaudstiegler, arpanchowdhry, ashimb9, Ayomide Bamidele, Baran Buluttekin, barrycg, Bharat Raghunathan, Bill Mill, Biswadip Mandal, blackd0t, Brian G. Barkley, Brian Wignall, Bryan Yang, c56pony, camilaagw, cartman\_nabana, catajara, Cat Chenal, Cathy, cgsavard, Charles Vesteghem, Chiara Marmo, Chris Gregory, Christian Lorentzen, Christos Aridas, Dakota Grusak, Daniel Grady, Daniel Perry, Danna Naser, DatenBergwerk, David Dormagen, deeplook, Dillon Niederhut, Dong-hee Na, Dougal J. Sutherland, DrGFreeman, Dylan Cashman, edvardlindelof, Eric Larson, Eric Ndirangu, Eunseop Jeong, Fanny, federicopisanu, Felix Divo, flaviomorelli, FranciDona, Franco M. Luque, Frank Hoang, Frederic Haase, g0g0gadget, Gabriel Altay, Gabriel do Vale Rios, Gael Varoquaux, ganevgv, gdex1, getgaurav2, Gideon Sonoiya, Gordon Chen, gpapadok, Greg Mogavero, Grzegorz Szpak, Guillaume Lemaitre, Guillem García Subies, H4dr1en, hadshirt, Hailey Nguyen, Hanmin Qin, Hannah Bruce Macdonald, Harsh Mahajan, Harsh Soni, Honglu Zhang, Hossein Pourbozorg, Ian Sanders, Ingrid Spielman, J-A16, jaehong park, Jaime Ferrando Huertas, James Hill, James Myatt, Jay, jeremiedbb, Jérémie du Boisberranger, jeromedockes, Jesper Dramsch, Joan Massich, Joanna Zhang, Joel Nothman, Johann Faouzi, Jonathan Rahn, Jon Cusick, Jose Ortiz, Kanika Sabharwal, Katarina Slama, kellycarmody, Kennedy Kang'ethe, Kensuke Arai, Kesshi Jordan, Kevad, Kevin Loftis, Kevin Winata, Kevin Yu-Sheng Li, Kirill Dolmatov, Kirthi Shankar Sivamani, krishna katyal, Lakshmi Krishnan, Lakshya KD, LalliAcqua, lbfin, Leland McInnes, Léonard Binet, Loic Esteve, loopyme, lostcoaster, Louis Huynh, lrjball, Luca Ionescu, Lutz Roeder, MaggieChege, Maithreyi Venkatesh, Maltimore, Maocx, Marc Torrellas, Marie Douriez, Markus, Markus Frey, Martina G. Vilas, Martin Oywa, Martin Thoma, Masashi SHIBATA, Maxwell Aladago, mbillingr, m-clare, Meghann Agarwal, m.fab, Micah Smith, miguelbarao, Miguel Cabrera, Mina Naghshhnejad, Ming Li, motmoti, mschaffenroth, mthorrell, Natasha Borders, nezar-a, Nicolas Hug, Nidhin Pattaniyil, Nikita Titov, Nishan Singh Mann, Nitya Mandyam, norvan, notmatthancock, novaya, nxorable, Oleg Stikhin, Oleksandr Pavlyk, Olivier Grisel, Omar Saleem, Owen Flanagan, panpiort8, Paolo, Paolo Toccaceli, Paresh Mathur, Paula, Peng Yu, Peter Marko, pierretallotte, poorna-kumar, pspachtholz, qdeffense, Rajat Garg, Raphaël Bournhonesque, Ray, Ray Bell, Rebekah Kim, Reza Gharibi, Richard Payne, Richard W, rlms, Robert Juergens, Rok Mihevc, Roman Feldbauer, Roman Yurchak, R Sanjabi, RuchitaGarde, Ruth Waithera, Sackey, Sam Dixon, Samesh Lakhotia, Samuel Taylor, Sarra Habchi, Scott Gigante, Scott Sievert, Scott White, Sebastian Pölsterl, Sergey Feldman, SeWook Oh, she-dares, Shreya V, Shubham Mehta, Shuzhe Xiao, SimonCW, smarie, smujjiga, Sönke Behrends, Soumirai, Sourav Singh, stefan-matcovici, steinfurt, Stéphane Couvreur, Stephan Tulkens, Stephen Cowley, Stephen Tierney, SylvainLan, th0rwas, theoptips, theotheo, Thierno Ibrahima DIOP, Thomas Edwards, Thomas J Fan, Thomas Moreau, Thomas Schmitt, Tilen Kusterle, Tim Bicker, Timsaur, Tim Staley, Tirth Patel, Tola A, Tom Augspurger, Tom Dupré la Tour, topisan, Trevor Stephens, ttang131, Urvang Patel, Vathsala Achar, veerlosar, Venkatachalam N, Victor Luzgin, Vincent Jeanselme, Vincent Lostanlen, Vladimir Korolev, vnherdeiro, Wenbo Zhao, Wendy Hu, willdarnell, William de Vazelhes, wolframalpha, xavier dupré, xcjason, x-martian, xsat, xun-tang, Yinglr, yokasre, Yu-Hang "Maxin" Tang, Yulia Zamriy, Zhao Feng

---

v0.23.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.23

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_0\_23\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_0\_23\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 0.23.2

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` `inertia_` attribute of <span class="title-ref">cluster.KMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span>.

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where rounding errors could prevent convergence to be declared when <span class="title-ref">tol=0</span>. `17959` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span> where the reported inertia was incorrectly weighted by the sample weights. `17848` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.MeanShift</span> with <span class="title-ref">bin\_seeding=True</span>. When the estimated bandwidth is 0, the behavior is equivalent to <span class="title-ref">bin\_seeding=False</span>. `17742` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.AffinityPropagation</span>, that gives incorrect clusters when the array dtype is float32. `17995` by `Thomaz Santana  <Wikilicious>` and `Amanda Dsouza <amy12xx>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.MiniBatchDictionaryLearning.partial\_fit</span> which should update the dictionary by iterating only once over a mini-batch. `17433` by `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid overflows on Windows in <span class="title-ref">decomposition.IncrementalPCA.partial\_fit</span> for large `batch_size` and `n_samples` values. `17985` by `Alan Butler <aldee153>` and `Amanda Dsouza <amy12xx>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">ensemble.MultinomialDeviance</span> where the average of logloss was incorrectly calculated as sum of logloss. `17694` by `Markus Rempfler <rempfler>` and `Tsutomu Kusanagi <t-kusanagi2>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span> compatibility with estimators that do not define <span class="title-ref">n\_features\_in\_</span>. `17357` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes bug in <span class="title-ref">feature\_extraction.text.CountVectorizer</span> where sample order invariance was broken when <span class="title-ref">max\_features</span> was set and features had the same count. `18016` by [Thomas Fan](https://github.com/thomasjpfan), [Roman Yurchak](https://github.com/rth), and [Joel Nothman](https://joelnothman.com/).

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.lars\_path</span> does not overwrite <span class="title-ref">X</span> when <span class="title-ref">X\_copy=True</span> and <span class="title-ref">Gram='auto'</span>. `17914` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.manifold`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">metrics.pairwise\_distances</span> would raise an error if `metric='seuclidean'` and `X` is not type `np.float64`. `15730` by `Forrest Koch <ForrestCKoch>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.mean\_squared\_error</span> where the average of multiple RMSE values was incorrectly calculated as the root of the average of multiple MSE values. `17309` by `Swier Heeres <swierh>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">pipeline.FeatureUnion</span> raises a deprecation warning when <span class="title-ref">None</span> is included in <span class="title-ref">transformer\_list</span>. `17360` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix <span class="title-ref">utils.estimator\_checks.check\_estimator</span> so that all test cases support the <span class="title-ref">binary\_only</span> estimator tag. `17812` by `Bruno Charron <brcharron>`.

## Version 0.23.1

**May 18 2020**

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.KMeans</span> efficiency has been improved for very small datasets. In particular it cannot spawn idle threads any more. `17210` and `17235` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span> where the sample weights provided by the user were modified in place. `17204` by `Jeremie du Boisberranger <jeremiedbb>`.

#### Miscellaneous

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in the <span class="title-ref">repr</span> of third-party estimators that use a <span class="title-ref">\*\*kwargs</span> parameter in their constructor, when <span class="title-ref">changed\_only</span> is True which is now the default. `17205` by [Nicolas Hug](https://github.com/NicolasHug).

## Version 0.23.0

**May 12 2020**

### Enforcing keyword-only arguments

In an effort to promote clear and non-ambiguous use of the library, most constructor and function parameters are now expected to be passed as keyword arguments (i.e. using the <span class="title-ref">param=value</span> syntax) instead of positional. To ease the transition, a <span class="title-ref">FutureWarning</span> is raised if a keyword-only parameter is used as positional. In version 1.0 (renaming of 0.25), these parameters will be strictly keyword-only, and a <span class="title-ref">TypeError</span> will be raised. `15005` by [Joel Nothman](https://joelnothman.com/), [Adrin Jalali](https://github.com/adrinjalali), [Thomas Fan](https://github.com/thomasjpfan), and [Nicolas Hug](https://github.com/NicolasHug). See [SLEP009](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep009/proposal.html) for more details.

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span>, and <span class="title-ref">ensemble.IsolationForest</span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.KMeans</span> with `algorithm="elkan"` and `algorithm="full"`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.Birch</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer.get\_feature\_names</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer.fit</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.make\_multilabel\_classification</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.PCA</span> with <span class="title-ref">n\_components='mle'</span>
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.NMF</span> and <span class="title-ref">decomposition.non\_negative\_factorization</span> with float32 dtype input.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.KernelPCA.inverse\_transform</span>
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` `estimator_samples_` in <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span> and <span class="title-ref">ensemble.IsolationForest</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span> with <span class="title-ref">sample\_weight</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.RANSACRegressor</span> with `sample_weight`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.RidgeClassifierCV</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.mean\_squared\_error</span> with <span class="title-ref">squared</span> and <span class="title-ref">multioutput='raw\_values'</span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.mutual\_info\_score</span> with negative scores.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.confusion\_matrix</span> with zero length <span class="title-ref">y\_true</span> and <span class="title-ref">y\_pred</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neural\_network.MLPClassifier</span>
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.StandardScaler</span> with <span class="title-ref">partial\_fit</span> and sparse input.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.Normalizer</span> with norm='max'
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Any model using the <span class="title-ref">svm.libsvm</span> or the <span class="title-ref">svm.liblinear</span> solver, including <span class="title-ref">svm.LinearSVC</span>, <span class="title-ref">svm.LinearSVR</span>, <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.OneClassSVM</span>, <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">linear\_model.LogisticRegression</span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> as well as `predict` method of <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeRegressor</span>, and <span class="title-ref">ensemble.GradientBoostingRegressor</span> and read-only float32 input in `predict`, `decision_path` and `predict_proba`.

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.Birch</span> implementation of the predict method avoids high memory footprint by calculating the distances matrix using a chunked scheme. `16149` by `Jeremie du Boisberranger <jeremiedbb>` and `Alex Shacked <alexshacked>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` The critical parts of <span class="title-ref">cluster.KMeans</span> have a more optimized implementation. Parallelism is now over the data instead of over initializations allowing better scalability. `11950` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.KMeans</span> now supports sparse data when <span class="title-ref">solver = "elkan"</span>. `11950` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.AgglomerativeClustering</span> has a faster and more memory efficient implementation of single linkage clustering. `11514` by `Leland McInnes <lmcinnes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.KMeans</span> with `algorithm="elkan"` now converges with `tol=0` as with the default `algorithm="full"`. `16075` by `Erich Schubert <kno10>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.Birch</span> where the <span class="title-ref">n\_clusters</span> parameter could not have a <span class="title-ref">np.int64</span> type. `16484` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.AgglomerativeClustering</span> add specific error when distance matrix is not square and <span class="title-ref">affinity=precomputed</span>. `16257` by `Simona Maggio <simonamaggio>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `n_jobs` parameter of <span class="title-ref">cluster.KMeans</span>, <span class="title-ref">cluster.SpectralCoclustering</span> and <span class="title-ref">cluster.SpectralBiclustering</span> is deprecated. They now use OpenMP based parallelism. For more details on how to control the number of threads, please refer to our \[parallelism\](\#parallelism) notes. `11950` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `precompute_distances` parameter of <span class="title-ref">cluster.KMeans</span> is deprecated. It has no effect. `11950` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `random_state` parameter has been added to <span class="title-ref">cluster.AffinityPropagation</span>. `16801` by `rcwoolston` and `Chiara Marmo <cmarmo>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">compose.ColumnTransformer</span> is now faster when working with dataframes and strings are used to specific subsets of data for transformers. `16431` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">compose.ColumnTransformer</span> method `get_feature_names` now supports <span class="title-ref">'passthrough'</span> columns, with the feature name being either the column name for a dataframe, or <span class="title-ref">'xi'</span> for column index <span class="title-ref">i</span>. `14048` by `Lewis Ball <lrjball>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> method `get_feature_names` now returns correct results when one of the transformer steps applies on an empty list of columns `15963` by [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer.fit</span> will error when selecting a column name that is not unique in the dataframe. `16431` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">datasets.fetch\_openml</span> has reduced memory usage because it no longer stores the full dataset text stream in memory. `16084` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">datasets.fetch\_california\_housing</span> now supports heterogeneous data using pandas by setting <span class="title-ref">as\_frame=True</span>. `15950` by `Stephanie Andrews <gitsteph>` and `Reshama Shaikh <reshamas>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` embedded dataset loaders <span class="title-ref">datasets.load\_breast\_cancer</span>, <span class="title-ref">datasets.load\_diabetes</span>, <span class="title-ref">datasets.load\_digits</span>, <span class="title-ref">datasets.load\_iris</span>, <span class="title-ref">datasets.load\_linnerud</span> and <span class="title-ref">datasets.load\_wine</span> now support loading as a pandas `DataFrame` by setting <span class="title-ref">as\_frame=True</span>. `15980` by `wconnell` and `Reshama Shaikh <reshamas>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added `return_centers` parameter in <span class="title-ref">datasets.make\_blobs</span>, which can be used to return centers for each cluster. `15709` by `shivamgargsya` and `Venkatachalam N <venkyyuvy>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Functions <span class="title-ref">datasets.make\_circles</span> and <span class="title-ref">datasets.make\_moons</span> now accept two-element tuple. `15707` by `Maciej J Mikulski <mjmikulski>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.make\_multilabel\_classification</span> now generates <span class="title-ref">ValueError</span> for arguments <span class="title-ref">n\_classes \< 1</span> OR <span class="title-ref">length \< 1</span>. `16006` by `Rushabh Vasani <rushabh-v>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">StreamHandler</span> was removed from <span class="title-ref">sklearn.logger</span> to avoid double logging of messages in common cases where a handler is attached to the root logger, and to follow the Python logging documentation recommendation for libraries to leave the log message handling to users and application code. `16451` by `Christoph Deil <cdeil>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.NMF</span> and <span class="title-ref">decomposition.non\_negative\_factorization</span> now preserves float32 dtype. `16280` by `Jeremie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.TruncatedSVD.transform</span> is now faster on given sparse `csc` matrices. `16837` by `wornbb`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.PCA</span> with a float <span class="title-ref">n\_components</span> parameter, will exclusively choose the components that explain the variance greater than <span class="title-ref">n\_components</span>. `15669` by `Krishna Chaitanya <krishnachaitanya9>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.PCA</span> with <span class="title-ref">n\_components='mle'</span> now correctly handles small eigenvalues, and does not infer 0 as the correct number of components. `16224` by `Lisa Schwetlick <lschwetlick>`, and `Gelavizh Ahmadi <gelavizh1>` and `Marija Vlajic Wheeler
    <marijavlajic>` and `16841` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.KernelPCA</span> method `inverse_transform` now applies the correct inverse transform to the transformed data. `16655` by `Lewis Ball <lrjball>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug that was causing <span class="title-ref">decomposition.KernelPCA</span> to sometimes raise <span class="title-ref">invalid value encountered in multiply</span> during <span class="title-ref">fit</span>. `16718` by `Gui Miotto <gui-miotto>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">n\_components\_</span> attribute to <span class="title-ref">decomposition.SparsePCA</span> and <span class="title-ref">decomposition.MiniBatchSparsePCA</span>. `16981` by `Mateusz Górski <Reksbril>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> now support `sample_weight`. `14696` by [Adrin Jalali](https://github.com/adrinjalali) and [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Early stopping in <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> is now determined with a new <span class="title-ref">early\_stopping</span> parameter instead of <span class="title-ref">n\_iter\_no\_change</span>. Default value is 'auto', which enables early stopping if there are at least 10,000 samples in the training set. `14516` by `Johann Faouzi
    <johannfaouzi>`.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> now support monotonic constraints, useful when features are supposed to have a positive/negative effect on the target. `15582` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Added boolean <span class="title-ref">verbose</span> flag to classes: <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span>. `16069` by `Sam Bail <spbail>`, `Hanna Bruce MacDonald <hannahbrucemacdonald>`, `Reshama Shaikh <reshamas>`, and `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Fixed a bug in <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> that would not respect the <span class="title-ref">max\_leaf\_nodes</span> parameter if the criteria was reached at the same time as the <span class="title-ref">max\_depth</span> criteria. `16183` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Changed the convention for <span class="title-ref">max\_depth</span> parameter of <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>. The depth now corresponds to the number of edges to go from the root to the deepest leaf. Stumps (trees with one split) are now allowed. `16182` by `Santhosh B <santhoshbala18>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span> and <span class="title-ref">ensemble.IsolationForest</span> where the attribute <span class="title-ref">estimators\_samples\_</span> did not generate the proper indices used during <span class="title-ref">fit</span>. `16437` by `Jin-Hwan CHO <chofchof>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span> where the <span class="title-ref">sample\_weight</span> argument was not being passed to <span class="title-ref">cross\_val\_predict</span> when evaluating the base estimators on cross-validation folds to obtain the input to the meta estimator. `16539` by `Bill DeRose <wderose>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added additional option <span class="title-ref">loss="poisson"</span> to <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>, which adds Poisson deviance with log-link useful for modeling count data. `16692` by `Christian Lorentzen <lorentzenchr>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> would fail with multiple calls to fit when <span class="title-ref">warm\_start=True</span>, <span class="title-ref">early\_stopping=True</span>, and there is no validation set. `16663` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">feature\_extraction.text.CountVectorizer</span> now sorts features after pruning them by document frequency. This improves performances for datasets with large vocabularies combined with `min_df` or `max_df`. `15834` by `Santiago M. Mola <smola>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added support for multioutput data in <span class="title-ref">feature\_selection.RFE</span> and <span class="title-ref">feature\_selection.RFECV</span>. `16103` by `Divyaprabha M <divyaprabha123>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Adds <span class="title-ref">feature\_selection.SelectorMixin</span> back to public API. `16132` by `trimeta`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">gaussian\_process.kernels.Matern</span> returns the RBF kernel when `nu=np.inf`. `15503` by `Sam Dixon <sam-dixon>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> that caused predicted standard deviations to only be between 0 and 1 when WhiteKernel is not used. `15782` by `plgreenLIRU`.

#### `sklearn.impute`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">impute.IterativeImputer</span> accepts both scalar and array-like inputs for `max_value` and `min_value`. Array-like inputs allow a different max and min to be specified for each feature. `16403` by `Narendra Mukherjee <narendramukherjee>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">impute.SimpleImputer</span>, <span class="title-ref">impute.KNNImputer</span>, and <span class="title-ref">impute.IterativeImputer</span> accepts pandas' nullable integer dtype with missing values. `16508` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.inspection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">inspection.partial\_dependence</span> and <span class="title-ref">inspection.plot\_partial\_dependence</span> now support the fast 'recursion' method for <span class="title-ref">ensemble.RandomForestRegressor</span> and <span class="title-ref">tree.DecisionTreeRegressor</span>. `15864` by [Nicolas Hug](https://github.com/NicolasHug).

#### `sklearn.linear_model`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added generalized linear models (GLM) with non normal error distributions, including <span class="title-ref">linear\_model.PoissonRegressor</span>, <span class="title-ref">linear\_model.GammaRegressor</span> and <span class="title-ref">linear\_model.TweedieRegressor</span> which use Poisson, Gamma and Tweedie distributions respectively. `14300` by `Christian Lorentzen <lorentzenchr>`, [Roman Yurchak](https://github.com/rth), and [Olivier Grisel](https://twitter.com/ogrisel).
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Support of <span class="title-ref">sample\_weight</span> in <span class="title-ref">linear\_model.ElasticNet</span> and <span class="title-ref">linear\_model.Lasso</span> for dense feature matrix <span class="title-ref">X</span>. `15436` by `Christian Lorentzen
    <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span> now does not allocate a potentially large array to store dual coefficients for all hyperparameters during its <span class="title-ref">fit</span>, nor an array to store all error or LOO predictions unless <span class="title-ref">store\_cv\_values</span> is <span class="title-ref">True</span>. `15652` by `Jérôme Dockès <jeromedockes>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.LassoLars</span> and <span class="title-ref">linear\_model.Lars</span> now support a <span class="title-ref">jitter</span> parameter that adds random noise to the target. This might help with stability in some edge cases. `15179` by `angelaambroz`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug where if a <span class="title-ref">sample\_weight</span> parameter was passed to the fit method of <span class="title-ref">linear\_model.RANSACRegressor</span>, it would not be passed to the wrapped <span class="title-ref">base\_estimator</span> during the fitting of the final model. `15773` by `Jeremy Alexandre <J-A16>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Add <span class="title-ref">best\_score\_</span> attribute to <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span>. `15655` by `Jérôme Dockès <jeromedockes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.RidgeClassifierCV</span> to pass a specific scoring strategy. Before the internal estimator outputs score instead of predictions. `14848` by `Venkatachalam N <venkyyuvy>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.LogisticRegression</span> will now avoid an unnecessary iteration when <span class="title-ref">solver='newton-cg'</span> by checking for inferior or equal instead of strictly inferior for maximum of <span class="title-ref">absgrad</span> and <span class="title-ref">tol</span> in <span class="title-ref">utils.optimize.\_newton\_cg</span>. `16266` by `Rushabh Vasani <rushabh-v>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecated public attributes <span class="title-ref">standard\_coef\_</span>, <span class="title-ref">standard\_intercept\_</span>, <span class="title-ref">average\_coef\_</span>, and <span class="title-ref">average\_intercept\_</span> in <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span>. `16261` by `Carlos Brandt <chbrandt>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">linear\_model.ARDRegression</span> is more stable and much faster when <span class="title-ref">n\_samples \> n\_features</span>. It can now scale to hundreds of thousands of samples. The stability fix might imply changes in the number of non-zero coefficients and in the predicted output. `16849` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">linear\_model.ElasticNetCV</span>, <span class="title-ref">linear\_model.MultiTaskElasticNetCV</span>, <span class="title-ref">linear\_model.LassoCV</span> and <span class="title-ref">linear\_model.MultiTaskLassoCV</span> where fitting would fail when using joblib loky backend. `14264` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Speed up <span class="title-ref">linear\_model.MultiTaskLasso</span>, <span class="title-ref">linear\_model.MultiTaskLassoCV</span>, <span class="title-ref">linear\_model.MultiTaskElasticNet</span>, <span class="title-ref">linear\_model.MultiTaskElasticNetCV</span> by avoiding slower BLAS Level 2 calls on small arrays `17021` by `Alex Gramfort <agramfort>` and `Mathurin Massias <mathurinm>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.pairwise\_distances\_chunked</span> now allows its `reduce_func` to not have a return value, enabling in-place operations. `16397` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.mean\_squared\_error</span> to not ignore argument <span class="title-ref">squared</span> when argument <span class="title-ref">multioutput='raw\_values'</span>. `16323` by `Rushabh Vasani <rushabh-v>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.mutual\_info\_score</span> where negative scores could be returned. `16362` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.confusion\_matrix</span> that would raise an error when <span class="title-ref">y\_true</span> and <span class="title-ref">y\_pred</span> were length zero and <span class="title-ref">labels</span> was not <span class="title-ref">None</span>. In addition, we raise an error when an empty list is given to the <span class="title-ref">labels</span> parameter. `16442` by `Kyle Parsons <parsons-kyle-89>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Changed the formatting of values in <span class="title-ref">metrics.ConfusionMatrixDisplay.plot</span> and <span class="title-ref">metrics.plot\_confusion\_matrix</span> to pick the shorter format (either '2g' or 'd'). `16159` by `Rick Mackenbach <Rick-Mackenbach>` and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` From version 0.25, <span class="title-ref">metrics.pairwise\_distances</span> will no longer automatically compute the `VI` parameter for Mahalanobis distance and the `V` parameter for seuclidean distance if `Y` is passed. The user will be expected to compute this parameter on the training data of their choice and pass it to <span class="title-ref">pairwise\_distances</span>. `16993` by [Joel Nothman](https://joelnothman.com/).

#### `sklearn.model_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> yields stack trace information in fit failed warning messages in addition to previously emitted type and details. `15622` by `Gregory Morse <GregoryMorse>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.cross\_val\_predict</span> supports <span class="title-ref">method="predict\_proba"</span> when <span class="title-ref">y=None</span>. `15918` by `Luca Kubin <lkubin>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.fit\_grid\_point</span> is deprecated in 0.23 and will be removed in 0.25. `16401` by `Arie Pratama Sutiono <ariepratama>`

#### `sklearn.multioutput`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">multioutput.MultiOutputRegressor.fit</span> and <span class="title-ref">multioutput.MultiOutputClassifier.fit</span> now can accept <span class="title-ref">fit\_params</span> to pass to the <span class="title-ref">estimator.fit</span> method of each step. `15953` `15959` by `Ke Huang <huangk10>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">multioutput.RegressorChain</span> now supports <span class="title-ref">fit\_params</span> for <span class="title-ref">base\_estimator</span> during <span class="title-ref">fit</span>. `16111` by `Venkatachalam N <venkyyuvy>`.

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A correctly formatted error message is shown in <span class="title-ref">naive\_bayes.CategoricalNB</span> when the number of features in the input differs between <span class="title-ref">predict</span> and <span class="title-ref">fit</span>. `16090` by `Madhura Jayaratne <madhuracj>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span> has reduced memory footprint when using stochastic solvers, <span class="title-ref">'sgd'</span> or <span class="title-ref">'adam'</span>, and <span class="title-ref">shuffle=True</span>. `14075` by `meyer89`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Increases the numerical stability of the logistic loss function in <span class="title-ref">neural\_network.MLPClassifier</span> by clipping the probabilities. `16117` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.inspection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">inspection.PartialDependenceDisplay</span> now exposes the deciles lines as attributes so they can be hidden or customized. `15785` by [Nicolas Hug](https://github.com/NicolasHug)

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` argument <span class="title-ref">drop</span> of <span class="title-ref">preprocessing.OneHotEncoder</span> will now accept value 'if\_binary' and will drop the first category of each feature with two categories. `16245` by `Rushabh Vasani <rushabh-v>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">preprocessing.OneHotEncoder</span>'s <span class="title-ref">drop\_idx\_</span> ndarray can now contain <span class="title-ref">None</span>, where <span class="title-ref">drop\_idx\_\[i\] = None</span> means that no category is dropped for index <span class="title-ref">i</span>. `16585` by `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">preprocessing.MaxAbsScaler</span>, <span class="title-ref">preprocessing.MinMaxScaler</span>, <span class="title-ref">preprocessing.StandardScaler</span>, <span class="title-ref">preprocessing.PowerTransformer</span>, <span class="title-ref">preprocessing.QuantileTransformer</span>, <span class="title-ref">preprocessing.RobustScaler</span> now supports pandas' nullable integer dtype with missing values. `16508` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">preprocessing.OneHotEncoder</span> is now faster at transforming. `15762` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">preprocessing.StandardScaler</span> which was incorrectly computing statistics when calling <span class="title-ref">partial\_fit</span> on sparse inputs. `16466` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">preprocessing.Normalizer</span> with norm='max', which was not taking the absolute value of the maximum values before normalizing the vectors. `16632` by `Maura Pintor <Maupin1991>` and `Battista Biggio <bbiggio>`.

#### `sklearn.semi_supervised`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">semi\_supervised.LabelSpreading</span> and <span class="title-ref">semi\_supervised.LabelPropagation</span> avoids divide by zero warnings when normalizing <span class="title-ref">label\_distributions\_</span>. `15946` by `ngshya`.

#### `sklearn.svm`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improved `libsvm` and `liblinear` random number generators used to randomly select coordinates in the coordinate descent algorithms. Platform-dependent C `rand()` was used, which is only able to generate numbers up to `32767` on windows platform (see this [blog post](https://codeforces.com/blog/entry/61587)) and also has poor randomization power as suggested by [this presentation](https://channel9.msdn.com/Events/GoingNative/2013/rand-Considered-Harmful). It was replaced with C++11 `mt19937`, a Mersenne Twister that correctly generates 31bits/63bits random numbers on all platforms. In addition, the crude "modulo" postprocessor used to get a random number in a bounded interval was replaced by the tweaked Lemire method as suggested by [this blog post](http://www.pcg-random.org/posts/bounded-rands.html). Any model using the <span class="title-ref">svm.libsvm</span> or the <span class="title-ref">svm.liblinear</span> solver, including <span class="title-ref">svm.LinearSVC</span>, <span class="title-ref">svm.LinearSVR</span>, <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.OneClassSVM</span>, <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">linear\_model.LogisticRegression</span>, is affected. In particular users can expect a better convergence when the number of samples (LibSVM) or the number of features (LibLinear) is large. `13511` by `Sylvain Marié <smarie>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix use of custom kernel not taking float entries such as string kernels in <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.SVR</span>. Note that custom kennels are now expected to validate their input where they previously received valid numeric arrays. `11296` by [Alexandre Gramfort](http://alexandre.gramfort.net) and `Georgi Peev <georgipeev>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">svm.SVR</span> and <span class="title-ref">svm.OneClassSVM</span> attributes, <span class="title-ref">probA\_</span> and <span class="title-ref">probB\_</span>, are now deprecated as they were not useful. `15558` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">tree.plot\_tree</span> <span class="title-ref">rotate</span> parameter was unused and has been deprecated. `15806` by `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix support of read-only float32 array input in `predict`, `decision_path` and `predict_proba` methods of <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span> as well as `predict` method of <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeRegressor</span>, and <span class="title-ref">ensemble.GradientBoostingRegressor</span>. `16331` by `Alexandre Batisse <batalex>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Estimators can now be displayed with a rich html representation. This can be enabled in Jupyter notebooks by setting <span class="title-ref">display='diagram'</span> in <span class="title-ref">\~sklearn.set\_config</span>. The raw html can be returned by using <span class="title-ref">utils.estimator\_html\_repr</span>. `14180` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` improve error message in <span class="title-ref">utils.validation.column\_or\_1d</span>. `15926` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` add warning in <span class="title-ref">utils.check\_array</span> for pandas sparse DataFrame. `16021` by `Rushabh Vasani <rushabh-v>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.check\_array</span> now constructs a sparse matrix from a pandas DataFrame that contains only <span class="title-ref">SparseArray</span> columns. `16728` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.check\_array</span> supports pandas' nullable integer dtype with missing values when <span class="title-ref">force\_all\_finite</span> is set to <span class="title-ref">False</span> or <span class="title-ref">'allow-nan'</span> in which case the data is converted to floating point values where <span class="title-ref">pd.NA</span> values are replaced by <span class="title-ref">np.nan</span>. As a consequence, all `sklearn.preprocessing` transformers that accept numeric inputs with missing values represented as <span class="title-ref">np.nan</span> now also accepts being directly fed pandas dataframes with <span class="title-ref">pd.Int\* or \`pd.Uint\*</span> typed columns that use <span class="title-ref">pd.NA</span> as a missing value marker. `16508` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Passing classes to <span class="title-ref">utils.estimator\_checks.check\_estimator</span> and <span class="title-ref">utils.estimator\_checks.parametrize\_with\_checks</span> is now deprecated, and support for classes will be removed in 0.24. Pass instances instead. `17032` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The private utility <span class="title-ref">\_safe\_tags</span> in <span class="title-ref">utils.estimator\_checks</span> was removed, hence all tags should be obtained through <span class="title-ref">estimator.\_get\_tags()</span>. Note that Mixins like <span class="title-ref">RegressorMixin</span> must come *before* base classes in the MRO for <span class="title-ref">\_get\_tags()</span> to work properly. `16950` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.all\_estimators</span> now only returns public estimators. `15380` by [Thomas Fan](https://github.com/thomasjpfan).

#### Miscellaneous

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Adds a HTML representation of estimators to be shown in a jupyter notebook or lab. This visualization is activated by setting the <span class="title-ref">display</span> option in <span class="title-ref">sklearn.set\_config</span>. `14180` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` `scikit-learn` now works with `mypy` without errors. `16726` by [Roman Yurchak](https://github.com/rth).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Most estimators now expose a <span class="title-ref">n\_features\_in\_</span> attribute. This attribute is equal to the number of features passed to the <span class="title-ref">fit</span> method. See [SLEP010](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for details. `16112` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Estimators now have a <span class="title-ref">requires\_y</span> tags which is False by default except for estimators that inherit from <span class="title-ref">\~sklearn.base.RegressorMixin</span> or <span class="title-ref">\~sklearn.base.ClassifierMixin</span>. This tag is used to ensure that a proper error message is raised when y was expected but None was passed. `16622` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default setting <span class="title-ref">print\_changed\_only</span> has been changed from False to True. This means that the <span class="title-ref">repr</span> of estimators is now more concise and only shows the parameters whose default value has been changed when printing an estimator. You can restore the previous behaviour by using <span class="title-ref">sklearn.set\_config(print\_changed\_only=False)</span>. Also, note that it is always possible to quickly inspect the parameters of any estimator using <span class="title-ref">est.get\_params(deep=False)</span>. `17061` by [Nicolas Hug](https://github.com/NicolasHug).

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.22, including:

Abbie Popa, Adrin Jalali, Aleksandra Kocot, Alexandre Batisse, Alexandre Gramfort, Alex Henrie, Alex Itkes, Alex Liang, alexshacked, Alonso Silva Allende, Ana Casado, Andreas Mueller, Angela Ambroz, Ankit810, Arie Pratama Sutiono, Arunav Konwar, Baptiste Maingret, Benjamin Beier Liu, bernie gray, Bharathi Srinivasan, Bharat Raghunathan, Bibhash Chandra Mitra, Brian Wignall, brigi, Brigitta Sipőcz, Carlos H Brandt, CastaChick, castor, cgsavard, Chiara Marmo, Chris Gregory, Christian Kastner, Christian Lorentzen, Corrie Bartelheimer, Daniël van Gelder, Daphne, David Breuer, david-cortes, dbauer9, Divyaprabha M, Edward Qian, Ekaterina Borovikova, ELNS, Emily Taylor, Erich Schubert, Eric Leung, Evgeni Chasnovski, Fabiana, Facundo Ferrín, Fan, Franziska Boenisch, Gael Varoquaux, Gaurav Sharma, Geoffrey Bolmier, Georgi Peev, gholdman1, Gonthier Nicolas, Gregory Morse, Gregory R. Lee, Guillaume Lemaitre, Gui Miotto, Hailey Nguyen, Hanmin Qin, Hao Chun Chang, HaoYin, Hélion du Mas des Bourboux, Himanshu Garg, Hirofumi Suzuki, huangk10, Hugo van Kemenade, Hye Sung Jung, indecisiveuser, inderjeet, J-A16, Jérémie du Boisberranger, Jin-Hwan CHO, JJmistry, Joel Nothman, Johann Faouzi, Jon Haitz Legarreta Gorroño, Juan Carlos Alfaro Jiménez, judithabk6, jumon, Kathryn Poole, Katrina Ni, Kesshi Jordan, Kevin Loftis, Kevin Markham, krishnachaitanya9, Lam Gia Thuan, Leland McInnes, Lisa Schwetlick, lkubin, Loic Esteve, lopusz, lrjball, lucgiffon, lucyleeow, Lucy Liu, Lukas Kemkes, Maciej J Mikulski, Madhura Jayaratne, Magda Zielinska, maikia, Mandy Gu, Manimaran, Manish Aradwad, Maren Westermann, Maria, Mariana Meireles, Marie Douriez, Marielle, Mateusz Górski, mathurinm, Matt Hall, Maura Pintor, mc4229, meyer89, m.fab, Michael Shoemaker, Michał Słapek, Mina Naghshhnejad, mo, Mohamed Maskani, Mojca Bertoncelj, narendramukherjee, ngshya, Nicholas Won, Nicolas Hug, nicolasservel, Niklas, @nkish, Noa Tamir, Oleksandr Pavlyk, olicairns, Oliver Urs Lenz, Olivier Grisel, parsons-kyle-89, Paula, Pete Green, Pierre Delanoue, pspachtholz, Pulkit Mehta, Qizhi Jiang, Quang Nguyen, rachelcjordan, raduspaimoc, Reshama Shaikh, Riccardo Folloni, Rick Mackenbach, Ritchie Ng, Roman Feldbauer, Roman Yurchak, Rory Hartong-Redden, Rüdiger Busche, Rushabh Vasani, Sambhav Kothari, Samesh Lakhotia, Samuel Duan, SanthoshBala18, Santiago M. Mola, Sarat Addepalli, scibol, Sebastian Kießling, SergioDSR, Sergul Aydore, Shiki-H, shivamgargsya, SHUBH CHATTERJEE, Siddharth Gupta, simonamaggio, smarie, Snowhite, stareh, Stephen Blystone, Stephen Marsh, Sunmi Yoon, SylvainLan, talgatomarov, tamirlan1, th0rwas, theoptips, Thomas J Fan, Thomas Li, Thomas Schmitt, Tim Nonner, Tim Vink, Tiphaine Viard, Tirth Patel, Titus Christian, Tom Dupré la Tour, trimeta, Vachan D A, Vandana Iyer, Venkatachalam N, waelbenamara, wconnell, wderose, wenliwyan, Windber, wornbb, Yu-Hang "Maxin" Tang

---

v0.24.md

---

<div class="currentmodule">

sklearn

</div>

# Version 0.24

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_0\_24\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_0\_24\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 0.24.2

**April 2021**

### Changelog

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer.get\_feature\_names</span> does not call <span class="title-ref">get\_feature\_names</span> on transformers with an empty column selection. `19579` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.cross_decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in <span class="title-ref">cross\_decomposition.CCA</span>. `19646` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cross\_decomposition.PLSRegression</span> raises warning for constant y residuals instead of a <span class="title-ref">StopIteration</span> error. `19922` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.KernelPCA</span>'s `inverse_transform`. `19732` by `Kei Ishikawa <kstoneriv3>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> <span class="title-ref">fit</span> with <span class="title-ref">sample\_weight</span> parameter and <span class="title-ref">least\_absolute\_deviation</span> loss function. `19407` by `Vadim Ushtanit <vadim-ushtanit>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug to support multiple strings for a category when <span class="title-ref">sparse=False</span> in <span class="title-ref">feature\_extraction.DictVectorizer</span>. `19982` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid explicitly forming inverse covariance matrix in <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> when set to output standard deviation. With certain covariance matrices this inverse is unstable to compute explicitly. Calling Cholesky solver mitigates this issue in computation. `19939` by `Ian Halvic <iwhalvic>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid division by zero when scaling constant target in <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span>. It was due to a std. dev. equal to 0. Now, such case is detected and the std. dev. is affected to 1 avoiding a division by zero and thus the presence of NaN values in the normalized target. `19703` by `sobkevich`, `Boris Villazón-Terrazas <boricles>` and `Alexandr Fonari <afonari>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`: Fixed a bug in \`linear\_model.LogisticRegression\`: the sample\_weight object is not modified anymore. `19182` by `Yosuke KOBAYASHI <m7142yosuke>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.top\_k\_accuracy\_score</span> now supports multiclass problems where only two classes appear in <span class="title-ref">y\_true</span> and all the classes are specified in <span class="title-ref">labels</span>. `19721` by `Joris Clement <flyingdutchman23>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.RandomizedSearchCV</span> and <span class="title-ref">model\_selection.GridSearchCV</span> now correctly shows the score for single metrics and verbose \> 2. `19659` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Some values in the <span class="title-ref">cv\_results\_</span> attribute of <span class="title-ref">model\_selection.HalvingRandomSearchCV</span> and <span class="title-ref">model\_selection.HalvingGridSearchCV</span> were not properly converted to numpy arrays. `19211` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">fit</span> method of the successive halving parameter search (<span class="title-ref">model\_selection.HalvingGridSearchCV</span>, and <span class="title-ref">model\_selection.HalvingRandomSearchCV</span>) now correctly handles the <span class="title-ref">groups</span> parameter. `19847` by `Xiaoyu Chai <xiaoyuchai>`.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">multioutput.MultiOutputRegressor</span> now works with estimators that dynamically define <span class="title-ref">predict</span> during fitting, such as <span class="title-ref">ensemble.StackingRegressor</span>. `19308` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Validate the constructor parameter <span class="title-ref">handle\_unknown</span> in <span class="title-ref">preprocessing.OrdinalEncoder</span> to only allow for <span class="title-ref">'error'</span> and <span class="title-ref">'use\_encoded\_value'</span> strategies. `19234` by <span class="title-ref">Guillaume Lemaitre \<glemaitre\></span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix encoder categories having dtype='S' <span class="title-ref">preprocessing.OneHotEncoder</span> and <span class="title-ref">preprocessing.OrdinalEncoder</span>. `19727` by `Andrew Delong <andrewdelong>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OrdinalEncoder.transform</span> correctly handles unknown values for string dtypes. `19888` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OneHotEncoder.fit</span> no longer alters the <span class="title-ref">drop</span> parameter. `19924` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.semi_supervised`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid NaN during label propagation in <span class="title-ref">\~sklearn.semi\_supervised.LabelPropagation</span>. `19271` by `Zhaowei Wang <ThuWangzw>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">fit</span> of <span class="title-ref">tree.BaseDecisionTree</span> that caused segmentation faults under certain conditions. <span class="title-ref">fit</span> now deep copies the <span class="title-ref">Criterion</span> object to prevent shared concurrent accesses. `19580` by `Samuel Brice <samdbrice>` and `Alex Adamson <aadamson>` and `Wil Yegelwel <wyegelwel>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Better contains the CSS provided by <span class="title-ref">utils.estimator\_html\_repr</span> by giving CSS ids to the html representation. `19417` by [Thomas Fan](https://github.com/thomasjpfan).

## Version 0.24.1

**January 2021**

### Packaging

The 0.24.0 scikit-learn wheels were not working with MacOS \<1.15 due to <span class="title-ref">libomp</span>. The version of <span class="title-ref">libomp</span> used to build the wheels was too recent for older macOS versions. This issue has been fixed for 0.24.1 scikit-learn wheels. Scikit-learn wheels published on PyPI.org now officially support macOS 10.13 and later.

### Changelog

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix numerical stability bug that could happen in <span class="title-ref">metrics.adjusted\_mutual\_info\_score</span> and <span class="title-ref">metrics.mutual\_info\_score</span> with NumPy 1.20+. `19179` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.semi_supervised`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">semi\_supervised.SelfTrainingClassifier</span> is now accepting meta-estimator (e.g. <span class="title-ref">ensemble.StackingClassifier</span>). The validation of this estimator is done on the fitted estimator, once we know the existence of the method <span class="title-ref">predict\_proba</span>. `19126` by `Guillaume Lemaitre <glemaitre>`.

## Version 0.24.0

**December 2020**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.KernelPCA</span> behaviour is now more consistent between 32-bits and 64-bits data when the kernel has small positive eigenvalues.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.TruncatedSVD</span> becomes deterministic by exposing a <span class="title-ref">random\_state</span> parameter.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.Perceptron</span> when <span class="title-ref">penalty='elasticnet'</span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Change in the random sampling procedures for the center initialization of <span class="title-ref">cluster.KMeans</span>.

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Changelog

#### `sklearn.base`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">base.BaseEstimator.get\_params</span> now will raise an <span class="title-ref">AttributeError</span> if a parameter cannot be retrieved as an instance attribute. Previously it would return <span class="title-ref">None</span>. `17448` by `Juan Carlos Alfaro Jiménez <alfaro96>`.

#### `sklearn.calibration`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">calibration.CalibratedClassifierCV.fit</span> now supports parallelization via <span class="title-ref">joblib.Parallel</span> using argument <span class="title-ref">n\_jobs</span>. `17107` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Allow <span class="title-ref">calibration.CalibratedClassifierCV</span> use with prefit <span class="title-ref">pipeline.Pipeline</span> where data is not <span class="title-ref">X</span> is not array-like, sparse matrix or dataframe at the start. `17546` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">ensemble</span> parameter to <span class="title-ref">calibration.CalibratedClassifierCV</span>, which enables implementation of calibration via an ensemble of calibrators (current method) or just one calibrator using all the data (similar to the built-in feature of `sklearn.svm` estimators with the <span class="title-ref">probabilities=True</span> parameter). `17856` by `Lucy Liu <lucyleeow>` and `Andrea Esuli <aesuli>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.AgglomerativeClustering</span> has a new parameter <span class="title-ref">compute\_distances</span>. When set to <span class="title-ref">True</span>, distances between clusters are computed and stored in the <span class="title-ref">distances\_</span> attribute even when the parameter <span class="title-ref">distance\_threshold</span> is not used. This new parameter is useful to produce dendrogram visualizations, but introduces a computational and memory overhead. `17984` by `Michael Riedmann <mriedmann>`, `Emilie Delattre <EmilieDel>`, and `Francesco Casalegno <FrancescoCasalegno>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.SpectralClustering</span> and <span class="title-ref">cluster.spectral\_clustering</span> have a new keyword argument <span class="title-ref">verbose</span>. When set to <span class="title-ref">True</span>, additional messages will be displayed which can aid with debugging. `18052` by `Sean O. Stalley <sstalley>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added <span class="title-ref">cluster.kmeans\_plusplus</span> as public function. Initialization by KMeans++ can now be called separately to generate initial cluster centroids. `17937` by `g-walsh`
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">cluster.MiniBatchKMeans</span> attributes, <span class="title-ref">counts\_</span> and <span class="title-ref">init\_size\_</span>, are deprecated and will be removed in 1.1 (renaming of 0.26). `17864` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> will skip transformers the column selector is a list of bools that are False. `17616` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> now displays the remainder in the diagram display. `18167` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> enforces strict count and order of column names between <span class="title-ref">fit</span> and <span class="title-ref">transform</span> by raising an error instead of a warning, following the deprecation cycle. `18256` by `Madhura Jayratne <madhuracj>`.

#### `sklearn.covariance`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">cv\_alphas\_</span> in favor of <span class="title-ref">cv\_results\_\['alphas'\]</span> and <span class="title-ref">grid\_scores\_</span> in favor of split scores in <span class="title-ref">cv\_results\_</span> in <span class="title-ref">covariance.GraphicalLassoCV</span>. <span class="title-ref">cv\_alphas\_</span> and <span class="title-ref">grid\_scores\_</span> will be removed in version 1.1 (renaming of 0.26). `16392` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.cross_decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cross\_decomposition.PLSSVD</span> which would sometimes return components in the reversed order of importance. `17095` by [Nicolas Hug](https://github.com/NicolasHug).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cross\_decomposition.PLSSVD</span>, <span class="title-ref">cross\_decomposition.CCA</span>, and <span class="title-ref">cross\_decomposition.PLSCanonical</span>, which would lead to incorrect predictions for <span class="title-ref">est.transform(Y)</span> when the training data is single-target. `17095` by [Nicolas Hug](https://github.com/NicolasHug).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Increases the stability of <span class="title-ref">cross\_decomposition.CCA</span> `18746` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The bounds of the <span class="title-ref">n\_components</span> parameter is now restricted:
    
      - into <span class="title-ref">\[1, min(n\_samples, n\_features, n\_targets)\]</span>, for <span class="title-ref">cross\_decomposition.PLSSVD</span>, <span class="title-ref">cross\_decomposition.CCA</span>, and <span class="title-ref">cross\_decomposition.PLSCanonical</span>.
      - into <span class="title-ref">\[1, n\_features\]</span> or <span class="title-ref">cross\_decomposition.PLSRegression</span>.
    
    An error will be raised in 1.1 (renaming of 0.26). `17095` by [Nicolas Hug](https://github.com/NicolasHug).

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` For <span class="title-ref">cross\_decomposition.PLSSVD</span>, <span class="title-ref">cross\_decomposition.CCA</span>, and <span class="title-ref">cross\_decomposition.PLSCanonical</span>, the <span class="title-ref">x\_scores\_</span> and <span class="title-ref">y\_scores\_</span> attributes were deprecated and will be removed in 1.1 (renaming of 0.26). They can be retrieved by calling <span class="title-ref">transform</span> on the training data. The <span class="title-ref">norm\_y\_weights</span> attribute will also be removed. `17095` by [Nicolas Hug](https://github.com/NicolasHug).

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` For <span class="title-ref">cross\_decomposition.PLSRegression</span>, <span class="title-ref">cross\_decomposition.PLSCanonical</span>, <span class="title-ref">cross\_decomposition.CCA</span>, and <span class="title-ref">cross\_decomposition.PLSSVD</span>, the <span class="title-ref">x\_mean\_</span>, <span class="title-ref">y\_mean\_</span>, <span class="title-ref">x\_std\_</span>, and <span class="title-ref">y\_std\_</span> attributes were deprecated and will be removed in 1.1 (renaming of 0.26). `18768` by `Maren Westermann <marenwestermann>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.TruncatedSVD</span> becomes deterministic by using the <span class="title-ref">random\_state</span>. It controls the weights' initialization of the underlying ARPACK solver. `  #18302 ` by `Gaurav Desai <gauravkdesai>` and `Ivan Panico <FollowKenny>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">datasets.fetch\_openml</span> now validates md5 checksum of arff files downloaded or cached to ensure data integrity. `14800` by `Shashank Singh <shashanksingh28>` and [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_openml</span> now allows argument <span class="title-ref">as\_frame</span> to be 'auto', which tries to convert returned data to pandas DataFrame unless data is sparse. `17396` by `Jiaxiang <fujiaxiang>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_covtype</span> now supports the optional argument <span class="title-ref">as\_frame</span>; when it is set to True, the returned Bunch object's <span class="title-ref">data</span> and <span class="title-ref">frame</span> members are pandas DataFrames, and the <span class="title-ref">target</span> member is a pandas Series. `17491` by `Alex Liang <tianchuliang>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_kddcup99</span> now supports the optional argument <span class="title-ref">as\_frame</span>; when it is set to True, the returned Bunch object's <span class="title-ref">data</span> and <span class="title-ref">frame</span> members are pandas DataFrames, and the <span class="title-ref">target</span> member is a pandas Series. `18280` by `Alex Liang <tianchuliang>` and [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_20newsgroups\_vectorized</span> now supports loading as a pandas `DataFrame` by setting `as_frame=True`. `17499` by `Brigitta Sipőcz <bsipocz>` and [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of <span class="title-ref">as\_frame</span> in <span class="title-ref">datasets.fetch\_openml</span> is changed from False to 'auto'. `17610` by `Jiaxiang <fujiaxiang>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` For <span class="title-ref">decomposition.NMF</span>, the <span class="title-ref">init</span> value, when 'init=None' and n\_components \<= min(n\_samples, n\_features) will be changed from <span class="title-ref">'nndsvd'</span> to <span class="title-ref">'nndsvda'</span> in 1.1 (renaming of 0.26). `18525` by `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.FactorAnalysis</span> now supports the optional argument <span class="title-ref">rotation</span>, which can take the value <span class="title-ref">None</span>, <span class="title-ref">'varimax'</span> or <span class="title-ref">'quartimax'</span>. `11064` by `Jona Sassenhagen <jona-sassenhagen>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.NMF</span> now supports the optional parameter <span class="title-ref">regularization</span>, which can take the values <span class="title-ref">None</span>, 'components', 'transformation' or 'both', in accordance with <span class="title-ref">decomposition.NMF.non\_negative\_factorization</span>. `17414` by `Bharat Raghunathan <bharatr21>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.KernelPCA</span> behaviour is now more consistent between 32-bits and 64-bits data input when the kernel has small positive eigenvalues. Small positive eigenvalues were not correctly discarded for 32-bits data. `18149` by `Sylvain Marié <smarie>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix <span class="title-ref">decomposition.SparseCoder</span> such that it follows scikit-learn API and support cloning. The attribute <span class="title-ref">components\_</span> is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). This attribute was redundant with the <span class="title-ref">dictionary</span> attribute and constructor parameter. `17679` by `Xavier Dupré <sdpython>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.TruncatedSVD.fit\_transform</span> consistently returns the same as <span class="title-ref">decomposition.TruncatedSVD.fit</span> followed by <span class="title-ref">decomposition.TruncatedSVD.transform</span>. `18528` by `Albert Villanova del Moral <albertvillanova>` and `Ruifeng Zheng <zhengruifeng>`.

#### `sklearn.discriminant_analysis`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> can now use custom covariance estimate by setting the <span class="title-ref">covariance\_estimator</span> parameter. `14446` by `Hugo Richard <hugorichard>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> now have native support for categorical features with the <span class="title-ref">categorical\_features</span> parameter. `18394` by [Nicolas Hug](https://github.com/NicolasHug) and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> now support the method <span class="title-ref">staged\_predict</span>, which allows monitoring of each stage. `16985` by `Hao Chun Chang <haochunchang>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` break cyclic references in the tree nodes used internally in <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> to allow for the timely garbage collection of large intermediate datastructures and to improve memory usage in <span class="title-ref">fit</span>. `18334` by [Olivier Grisel](https://twitter.com/ogrisel) [Nicolas Hug](https://github.com/NicolasHug), [Thomas Fan](https://github.com/thomasjpfan) and [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Histogram initialization is now done in parallel in <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> which results in speed improvement for problems that build a lot of nodes on multicore machines. `18341` by [Olivier Grisel](https://twitter.com/ogrisel), [Nicolas Hug](https://github.com/NicolasHug), [Thomas Fan](https://github.com/thomasjpfan), and `Egor Smirnov <SmirnovEgorRu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> which can now accept data with <span class="title-ref">uint8</span> dtype in <span class="title-ref">predict</span>. `18410` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter `n_classes_` is now deprecated in <span class="title-ref">ensemble.GradientBoostingRegressor</span> and returns <span class="title-ref">1</span>. `17702` by `Simona Maggio <simonamaggio>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Mean absolute error ('mae') is now deprecated for the parameter `criterion` in <span class="title-ref">ensemble.GradientBoostingRegressor</span> and <span class="title-ref">ensemble.GradientBoostingClassifier</span>. `18326` by `Madhura Jayaratne <madhuracj>`.

#### `sklearn.exceptions`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">exceptions.ChangedBehaviorWarning</span> and <span class="title-ref">exceptions.NonBLASDotWarning</span> are deprecated and will be removed in 1.1 (renaming of 0.26). `17804` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_extraction.DictVectorizer</span> accepts multiple values for one categorical feature. `17367` by `Peng Yu <yupbank>` and `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_extraction.text.CountVectorizer</span> raises an issue if a custom token pattern which capture more than one group is provided. `15427` by `Gangesh Gudmalwar <ggangesh>` and `Erin R Hoffman <hoffm386>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">feature\_selection.SequentialFeatureSelector</span> which implements forward and backward sequential feature selection. `6545` by [Sebastian Raschka](https://sebastianraschka.com/) and `17159` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A new parameter <span class="title-ref">importance\_getter</span> was added to <span class="title-ref">feature\_selection.RFE</span>, <span class="title-ref">feature\_selection.RFECV</span> and <span class="title-ref">feature\_selection.SelectFromModel</span>, allowing the user to specify an attribute name/path or a <span class="title-ref">callable</span> for extracting feature importance from the estimator. `15361` by `Venkatachalam N <venkyyuvy>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Reduce memory footprint in <span class="title-ref">feature\_selection.mutual\_info\_classif</span> and <span class="title-ref">feature\_selection.mutual\_info\_regression</span> by calling <span class="title-ref">neighbors.KDTree</span> for counting nearest neighbors. `17878` by `Noel Rogers <noelano>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_selection.RFE</span> supports the option for the number of <span class="title-ref">n\_features\_to\_select</span> to be given as a float representing the percentage of features to select. `17090` by `Lisa Schwetlick <lschwetlick>` and `Marija Vlajic Wheeler <marijavlajic>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A new method <span class="title-ref">gaussian\_process.kernel.\_check\_bounds\_params</span> is called after fitting a Gaussian Process and raises a `ConvergenceWarning` if the bounds of the hyperparameters are too tight. `12638` by `Sylvain Lannuzel <SylvainLan>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">impute.SimpleImputer</span> now supports a list of strings when `strategy='most_frequent'` or `strategy='constant'`. `17526` by `Ayako YAGI <yagi-3>` and `Juan Carlos Alfaro Jiménez <alfaro96>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added method <span class="title-ref">impute.SimpleImputer.inverse\_transform</span> to revert imputed data to original when instantiated with `add_indicator=True`. `17612` by `Srimukh Sripada <d3b0unce>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` replace the default values in <span class="title-ref">impute.IterativeImputer</span> of <span class="title-ref">min\_value</span> and <span class="title-ref">max\_value</span> parameters to <span class="title-ref">-np.inf</span> and <span class="title-ref">np.inf</span>, respectively instead of <span class="title-ref">None</span>. However, the behaviour of the class does not change since <span class="title-ref">None</span> was defaulting to these values already. `16493` by `Darshan N <DarshanGowda0>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">impute.IterativeImputer</span> will not attempt to set the estimator's <span class="title-ref">random\_state</span> attribute, allowing to use it with more external classes. `15636` by `David Cortes <david-cortes>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">impute.SimpleImputer</span> is now faster with <span class="title-ref">object</span> dtype array. when <span class="title-ref">strategy='most\_frequent'</span> in <span class="title-ref">\~sklearn.impute.SimpleImputer</span>. `18987` by `David Katz <DavidKatz-il>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">inspection.partial\_dependence</span> and <span class="title-ref">inspection.plot\_partial\_dependence</span> now support calculating and plotting Individual Conditional Expectation (ICE) curves controlled by the `kind` parameter. `16619` by `Madhura Jayratne <madhuracj>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">sample\_weight</span> parameter to <span class="title-ref">inspection.permutation\_importance</span>. `16906` by `Roei Kahny <RoeiKa>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Positional arguments are deprecated in <span class="title-ref">inspection.PartialDependenceDisplay.plot</span> and will error in 1.1 (renaming of 0.26). `18293` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.isotonic`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Expose fitted attributes `X_thresholds_` and `y_thresholds_` that hold the de-duplicated interpolation thresholds of an <span class="title-ref">isotonic.IsotonicRegression</span> instance for model inspection purpose. `16289` by `Masashi Kishimoto <kishimoto-banana>` and `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">isotonic.IsotonicRegression</span> now accepts 2d array with 1 feature as input array. `17379` by `Jiaxiang <fujiaxiang>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Add tolerance when determining duplicate X values to prevent inf values from being predicted by <span class="title-ref">isotonic.IsotonicRegression</span>. `18639` by `Lucy Liu <lucyleeow>`.

#### `sklearn.kernel_approximation`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added class <span class="title-ref">kernel\_approximation.PolynomialCountSketch</span> which implements the Tensor Sketch algorithm for polynomial kernel feature map approximation. `13003` by `Daniel López Sánchez <lopeLH>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">kernel\_approximation.Nystroem</span> now supports parallelization via <span class="title-ref">joblib.Parallel</span> using argument <span class="title-ref">n\_jobs</span>. `18545` by `Laurenz Reitsam <LaurenzReitsam>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.LinearRegression</span> now forces coefficients to be all positive when `positive` is set to `True`. `17578` by `Joseph Knox <jknox13>`, `Nelle Varoquaux <NelleV>` and `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.RidgeCV</span> now supports finding an optimal regularization value <span class="title-ref">alpha</span> for each target separately by setting `alpha_per_target=True`. This is only supported when using the default efficient leave-one-out cross-validation scheme `cv=None`. `6624` by `Marijn van Vliet <wmvanvliet>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes bug in <span class="title-ref">linear\_model.TheilSenRegressor</span> where <span class="title-ref">predict</span> and <span class="title-ref">score</span> would fail when <span class="title-ref">fit\_intercept=False</span> and there was one feature during fitting. `18121` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes bug in <span class="title-ref">linear\_model.ARDRegression</span> where <span class="title-ref">predict</span> was raising an error when <span class="title-ref">normalize=True</span> and <span class="title-ref">return\_std=True</span> because <span class="title-ref">X\_offset\_</span> and <span class="title-ref">X\_scale\_</span> were undefined. `18607` by `fhaselbeck <fhaselbeck>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Added the missing <span class="title-ref">l1\_ratio</span> parameter in <span class="title-ref">linear\_model.Perceptron</span>, to be used when <span class="title-ref">penalty='elasticnet'</span>. This changes the default from 0 to 0.15. `18622` by `Haesun Park <rickiepark>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Fixed `10493`. Improve Local Linear Embedding (LLE) that raised <span class="title-ref">MemoryError</span> exception when used with large inputs. `17997` by `Bertrand Maisonneuve <bmaisonn>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">square\_distances</span> parameter to <span class="title-ref">manifold.TSNE</span>, which provides backward compatibility during deprecation of legacy squaring behavior. Distances will be squared by default in 1.1 (renaming of 0.26), and this parameter will be removed in 1.3. `17662` by `Joshua Newton <joshuacwnewton>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.MDS</span> now correctly sets its <span class="title-ref">\_pairwise</span> attribute. `18278` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">metrics.cluster.pair\_confusion\_matrix</span> implementing the confusion matrix arising from pairs of elements from two clusterings. `17412` by `Uwe F Mayer <ufmayer>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` new metric <span class="title-ref">metrics.top\_k\_accuracy\_score</span>. It's a generalization of <span class="title-ref">metrics.top\_k\_accuracy\_score</span>, the difference is that a prediction is considered correct as long as the true label is associated with one of the <span class="title-ref">k</span> highest predicted scores. <span class="title-ref">metrics.accuracy\_score</span> is the special case of <span class="title-ref">k = 1</span>. `16625` by `Geoffrey Bolmier <gbolmier>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">metrics.det\_curve</span> to compute Detection Error Tradeoff curve classification metric. `10591` by `Jeremy Karnowski <jkarnows>` and `Daniel Mohns <dmohns>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">metrics.plot\_det\_curve</span> and <span class="title-ref">metrics.DetCurveDisplay</span> to ease the plot of DET curves. `18176` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">metrics.mean\_absolute\_percentage\_error</span> metric and the associated scorer for regression problems. `10708` fixed with the PR `15007` by `Ashutosh Hathidara <ashutosh1919>`. The scorer and some practical test cases were taken from PR `10711` by `Mohamed Ali Jamaoui <mohamed-ali>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">metrics.rand\_score</span> implementing the (unadjusted) Rand index. `17412` by `Uwe F Mayer <ufmayer>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.plot\_confusion\_matrix</span> now supports making colorbar optional in the matplotlib plot by setting <span class="title-ref">colorbar=False</span>. `17192` by `Avi Gupta <avigupta2612>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">sample\_weight</span> parameter to <span class="title-ref">metrics.median\_absolute\_error</span>. `17225` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">pos\_label</span> parameter in <span class="title-ref">metrics.plot\_precision\_recall\_curve</span> in order to specify the positive class to be used when computing the precision and recall statistics. `17569` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">pos\_label</span> parameter in <span class="title-ref">metrics.plot\_roc\_curve</span> in order to specify the positive class to be used when computing the roc auc statistics. `17651` by `Clara Matos <claramatos>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.classification\_report</span> which was raising AttributeError when called with <span class="title-ref">output\_dict=True</span> for 0-length values. `17777` by `Shubhanshu Mishra <napsternxg>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.classification\_report</span> which was raising AttributeError when called with <span class="title-ref">output\_dict=True</span> for 0-length values. `17777` by `Shubhanshu Mishra <napsternxg>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.jaccard\_score</span> which recommended the <span class="title-ref">zero\_division</span> parameter when called with no true or predicted samples. `17826` by `Richard Decal <crypdick>` and `Joseph Willard <josephwillard>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` bug in <span class="title-ref">metrics.hinge\_loss</span> where error occurs when `y_true` is missing some labels that are provided explicitly in the `labels` parameter. `17935` by `Cary Goltermann <Ultramann>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix scorers that accept a pos\_label parameter and compute their metrics from values returned by <span class="title-ref">decision\_function</span> or <span class="title-ref">predict\_proba</span>. Previously, they would return erroneous values when pos\_label was not corresponding to <span class="title-ref">classifier.classes\_\[1\]</span>. This is especially important when training classifiers directly with string labeled target classes. `18114` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed bug in <span class="title-ref">metrics.plot\_confusion\_matrix</span> where error occurs when <span class="title-ref">y\_true</span> contains labels that were not previously seen by the classifier while the <span class="title-ref">labels</span> and <span class="title-ref">display\_labels</span> parameters are set to <span class="title-ref">None</span>. `18405` by `Thomas J. Fan <thomasjpfan>` and `Yakov Pchelintsev <kyouma>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added (experimental) parameter search estimators <span class="title-ref">model\_selection.HalvingRandomSearchCV</span> and <span class="title-ref">model\_selection.HalvingGridSearchCV</span> which implement Successive Halving, and can be used as a drop-in replacements for <span class="title-ref">model\_selection.RandomizedSearchCV</span> and <span class="title-ref">model\_selection.GridSearchCV</span>. `13900` by [Nicolas Hug](https://github.com/NicolasHug), [Joel Nothman](https://joelnothman.com/) and [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">model\_selection.RandomizedSearchCV</span> and <span class="title-ref">model\_selection.GridSearchCV</span> now have the method `score_samples` `17478` by `Teon Brooks <teonbrooks>` and `Mohamed Maskani <maskani-moh>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.TimeSeriesSplit</span> has two new keyword arguments <span class="title-ref">test\_size</span> and <span class="title-ref">gap</span>. <span class="title-ref">test\_size</span> allows the out-of-sample time series length to be fixed for all folds. <span class="title-ref">gap</span> removes a fixed number of samples between the train and test set on each fold. `13204` by `Kyle Kosic <kykosic>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.permutation\_test\_score</span> and <span class="title-ref">model\_selection.validation\_curve</span> now accept fit\_params to pass additional estimator parameters. `18527` by `Gaurav Dhingra <gxyd>`, `Julien Jerphanion <jjerphan>` and `Amanda Dsouza <amy12xx>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.cross\_val\_score</span>, <span class="title-ref">model\_selection.cross\_validate</span>, <span class="title-ref">model\_selection.GridSearchCV</span>, and <span class="title-ref">model\_selection.RandomizedSearchCV</span> allows estimator to fail scoring and replace the score with <span class="title-ref">error\_score</span>. If <span class="title-ref">error\_score="raise"</span>, the error will be raised. `18343` by [Guillaume Lemaitre](https://github.com/glemaitre) and `Devi Sandeep <dsandeep0138>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.learning\_curve</span> now accept fit\_params to pass additional estimator parameters. `18595` by `Amanda Dsouza <amy12xx>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the <span class="title-ref">len</span> of <span class="title-ref">model\_selection.ParameterSampler</span> when all distributions are lists and <span class="title-ref">n\_iter</span> is more than the number of unique parameter combinations. `18222` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A fix to raise warning when one or more CV splits of <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> results in non-finite scores. `18266` by `Subrat Sahu <subrat93>`, `Nirvan <Nirvan101>` and `Arthur Book <ArthurBook>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span> and <span class="title-ref">model\_selection.cross\_validate</span> support <span class="title-ref">scoring</span> being a callable returning a dictionary of of multiple metric names/values association. `15126` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.multiclass`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">multiclass.OneVsOneClassifier</span> now accepts the inputs with missing values. Hence, estimators which can handle missing values (may be a pipeline with imputation step) can be used as a estimator for multiclass wrappers. `17987` by `Venkatachalam N <venkyyuvy>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A fix to allow <span class="title-ref">multiclass.OutputCodeClassifier</span> to accept sparse input data in its <span class="title-ref">fit</span> and <span class="title-ref">predict</span> methods. The check for validity of the input is now delegated to the base estimator. `17233` by `Zolisa Bleki <zoj613>`.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">multioutput.MultiOutputClassifier</span> and <span class="title-ref">multioutput.MultiOutputRegressor</span> now accepts the inputs with missing values. Hence, estimators which can handle missing values (may be a pipeline with imputation step, HistGradientBoosting estimators) can be used as a estimator for multiclass wrappers. `17987` by `Venkatachalam N <venkyyuvy>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A fix to accept tuples for the `order` parameter in <span class="title-ref">multioutput.ClassifierChain</span>. `18124` by `Gus Brocchini <boldloop>` and `Amanda Dsouza <amy12xx>`.

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds a parameter <span class="title-ref">min\_categories</span> to <span class="title-ref">naive\_bayes.CategoricalNB</span> that allows a minimum number of categories per feature to be specified. This allows categories unseen during training to be accounted for. `16326` by `George Armstrong <gwarmstrong>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The attributes `coef_` and `intercept_` are now deprecated in <span class="title-ref">naive\_bayes.MultinomialNB</span>, <span class="title-ref">naive\_bayes.ComplementNB</span>, <span class="title-ref">naive\_bayes.BernoulliNB</span> and <span class="title-ref">naive\_bayes.CategoricalNB</span>, and will be removed in v1.1 (renaming of 0.26). `17427` by `Juan Carlos Alfaro Jiménez <alfaro96>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Speed up `seuclidean`, `wminkowski`, `mahalanobis` and `haversine` metrics in <span class="title-ref">neighbors.DistanceMetric</span> by avoiding unexpected GIL acquiring in Cython when setting `n_jobs>1` in <span class="title-ref">neighbors.KNeighborsClassifier</span>, <span class="title-ref">neighbors.KNeighborsRegressor</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>, <span class="title-ref">metrics.pairwise\_distances</span> and by validating data out of loops. `17038` by `Wenbo Zhao <webber26232>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">neighbors.NeighborsBase</span> benefits of an improved <span class="title-ref">algorithm = 'auto'</span> heuristic. In addition to the previous set of rules, now, when the number of features exceeds 15, <span class="title-ref">brute</span> is selected, assuming the data intrinsic dimensionality is too high for tree-based methods. `17148` by `Geoffrey Bolmier <gbolmier>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.BinaryTree</span> will raise a <span class="title-ref">ValueError</span> when fitting on data array having points with different dimensions. `18691` by `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.NearestCentroid</span> with a numerical <span class="title-ref">shrink\_threshold</span> will raise a <span class="title-ref">ValueError</span> when fitting on data with all constant features. `18370` by `Trevor Waite <trewaite>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` In methods <span class="title-ref">radius\_neighbors</span> and <span class="title-ref">radius\_neighbors\_graph</span> of <span class="title-ref">neighbors.NearestNeighbors</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>, and <span class="title-ref">neighbors.RadiusNeighborsTransformer</span>, using <span class="title-ref">sort\_results=True</span> now correctly sorts the results even when fitting with the "brute" algorithm. `18612` by [Tom Dupre la Tour](https://github.com/TomDLT).

#### `sklearn.neural_network`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Neural net training and prediction are now a little faster. `17603`, `17604`, `17606`, `17608`, `17609`, `17633`, `17661`, `17932` by `Alex Henrie <alexhenrie>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Avoid converting float32 input to float64 in <span class="title-ref">neural\_network.BernoulliRBM</span>. `16352` by `Arthur Imbert <Henley13>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Support 32-bit computations in <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span>. `17759` by `Srimukh Sripada <d3b0unce>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix method <span class="title-ref">neural\_network.MLPClassifier.fit</span> not iterating to `max_iter` if warm started. `18269` by `Norbert Preining <norbusan>` and `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` References to transformers passed through `transformer_weights` to <span class="title-ref">pipeline.FeatureUnion</span> that aren't present in `transformer_list` will raise a `ValueError`. `17876` by `Cary Goltermann <Ultramann>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A slice of a <span class="title-ref">pipeline.Pipeline</span> now inherits the parameters of the original pipeline (<span class="title-ref">memory</span> and <span class="title-ref">verbose</span>). `18429` by `Albert Villanova del Moral <albertvillanova>` and `Paweł Biernat <pwl>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OneHotEncoder</span> now supports missing values by treating them as a category. `17317` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add a new `handle_unknown` parameter with a `use_encoded_value` option, along with a new `unknown_value` parameter, to <span class="title-ref">preprocessing.OrdinalEncoder</span> to allow unknown categories during transform and set the encoded value of the unknown categories. `17406` by `Felix Wick <FelixWick>` and `18406` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add `clip` parameter to <span class="title-ref">preprocessing.MinMaxScaler</span>, which clips the transformed values of test data to `feature_range`. `17833` by `Yashika Sharma <yashika51>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add `sample_weight` parameter to <span class="title-ref">preprocessing.StandardScaler</span>. Allows setting individual weights for each sample. `18510` and `18447` and `16066` and `18682` by `Maria Telenczuk <maikia>` and `Albert Villanova <albertvillanova>` and `panpiort8` and `Alex Gramfort <agramfort>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Verbose output of <span class="title-ref">model\_selection.GridSearchCV</span> has been improved for readability. `16935` by `Raghav Rajagopalan
    <raghavrv>` and `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add `unit_variance` to <span class="title-ref">preprocessing.RobustScaler</span>, which scales output data such that normally distributed features have a variance of 1. `17193` by `Lucy Liu <lucyleeow>` and `Mabel Villalba <mabelvj>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">dtype</span> parameter to <span class="title-ref">preprocessing.KBinsDiscretizer</span>. `16335` by `Arthur Imbert <Henley13>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise error on <span class="title-ref">sklearn.preprocessing.OneHotEncoder.inverse\_transform</span> when <span class="title-ref">handle\_unknown='error'</span> and <span class="title-ref">drop=None</span> for samples encoded as all zeros. `14982` by `Kevin Winata <kwinata>`.

#### `sklearn.semi_supervised`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">semi\_supervised.SelfTrainingClassifier</span>, a meta-classifier that allows any supervised classifier to function as a semi-supervised classifier that can learn from unlabeled data. `11682` by `Oliver Rausch <orausch>` and `Patrice Becker <pr0duktiv>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix incorrect encoding when using unicode string dtypes in <span class="title-ref">preprocessing.OneHotEncoder</span> and <span class="title-ref">preprocessing.OrdinalEncoder</span>. `15763` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.svm`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` invoke SciPy BLAS API for SVM kernel function in `fit`, `predict` and related methods of <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.OneClassSVM</span>. `16530` by `Shuhua Fan <jim0421>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">tree.DecisionTreeRegressor</span> now supports the new splitting criterion `'poisson'` useful for modeling count data. `17386` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">tree.plot\_tree</span> now uses colors from the matplotlib configuration settings. `17187` by [Andreas Müller](https://amueller.github.io/).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter `X_idx_sorted` is now deprecated in <span class="title-ref">tree.DecisionTreeClassifier.fit</span> and <span class="title-ref">tree.DecisionTreeRegressor.fit</span>, and has not effect. `17614` by `Juan Carlos Alfaro Jiménez <alfaro96>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add `check_methods_sample_order_invariance` to <span class="title-ref">\~utils.estimator\_checks.check\_estimator</span>, which checks that estimator methods are invariant if applied to the same dataset with different sample order `17598` by `Jason Ngo <ngojason9>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add support for weights in <span class="title-ref">utils.sparse\_func.incr\_mean\_variance\_axis</span>. By `Maria Telenczuk <maikia>` and `Alex Gramfort <agramfort>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise ValueError with clear error message in <span class="title-ref">utils.check\_array</span> for sparse DataFrames with mixed types. `17992` by `Thomas J. Fan <thomasjpfan>` and `Alex Shacked <alexshacked>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Allow serialized tree based models to be unpickled on a machine with different endianness. `17644` by `Qi Zhang <qzhang90>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Check that we raise proper error when axis=1 and the dimensions do not match in <span class="title-ref">utils.sparse\_func.incr\_mean\_variance\_axis</span>. By `Alex Gramfort <agramfort>`.

#### Miscellaneous

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Calls to `repr` are now faster when <span class="title-ref">print\_changed\_only=True</span>, especially with meta-estimators. `18508` by `Nathan C. <Xethan>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.23, including:

Abo7atm, Adam Spannbauer, Adrin Jalali, adrinjalali, Agamemnon Krasoulis, Akshay Deodhar, Albert Villanova del Moral, Alessandro Gentile, Alex Henrie, Alex Itkes, Alex Liang, Alexander Lenail, alexandracraciun, Alexandre Gramfort, alexshacked, Allan D Butler, Amanda Dsouza, amy12xx, Anand Tiwari, Anderson Nelson, Andreas Mueller, Ankit Choraria, Archana Subramaniyan, Arthur Imbert, Ashutosh Hathidara, Ashutosh Kushwaha, Atsushi Nukariya, Aura Munoz, AutoViz and Auto\_ViML, Avi Gupta, Avinash Anakal, Ayako YAGI, barankarakus, barberogaston, beatrizsmg, Ben Mainye, Benjamin Bossan, Benjamin Pedigo, Bharat Raghunathan, Bhavika Devnani, Biprateep Dey, bmaisonn, Bo Chang, Boris Villazón-Terrazas, brigi, Brigitta Sipőcz, Bruno Charron, Byron Smith, Cary Goltermann, Cat Chenal, CeeThinwa, chaitanyamogal, Charles Patel, Chiara Marmo, Christian Kastner, Christian Lorentzen, Christoph Deil, Christos Aridas, Clara Matos, clmbst, Coelhudo, crispinlogan, Cristina Mulas, Daniel López, Daniel Mohns, darioka, Darshan N, david-cortes, Declan O'Neill, Deeksha Madan, Elizabeth DuPre, Eric Fiegel, Eric Larson, Erich Schubert, Erin Khoo, Erin R Hoffman, eschibli, Felix Wick, fhaselbeck, Forrest Koch, Francesco Casalegno, Frans Larsson, Gael Varoquaux, Gaurav Desai, Gaurav Sheni, genvalen, Geoffrey Bolmier, George Armstrong, George Kiragu, Gesa Stupperich, Ghislain Antony Vaillant, Gim Seng, Gordon Walsh, Gregory R. Lee, Guillaume Chevalier, Guillaume Lemaitre, Haesun Park, Hannah Bohle, Hao Chun Chang, Harry Scholes, Harsh Soni, Henry, Hirofumi Suzuki, Hitesh Somani, Hoda1394, Hugo Le Moine, hugorichard, indecisiveuser, Isuru Fernando, Ivan Wiryadi, j0rd1smit, Jaehyun Ahn, Jake Tae, James Hoctor, Jan Vesely, Jeevan Anand Anne, JeroenPeterBos, JHayes, Jiaxiang, Jie Zheng, Jigna Panchal, jim0421, Jin Li, Joaquin Vanschoren, Joel Nothman, Jona Sassenhagen, Jonathan, Jorge Gorbe Moya, Joseph Lucas, Joshua Newton, Juan Carlos Alfaro Jiménez, Julien Jerphanion, Justin Huber, Jérémie du Boisberranger, Kartik Chugh, Katarina Slama, kaylani2, Kendrick Cetina, Kenny Huynh, Kevin Markham, Kevin Winata, Kiril Isakov, kishimoto, Koki Nishihara, Krum Arnaudov, Kyle Kosic, Lauren Oldja, Laurenz Reitsam, Lisa Schwetlick, Louis Douge, Louis Guitton, Lucy Liu, Madhura Jayaratne, maikia, Manimaran, Manuel López-Ibáñez, Maren Westermann, Maria Telenczuk, Mariam-ke, Marijn van Vliet, Markus Löning, Martin Scheubrein, Martina G. Vilas, Martina Megasari, Mateusz Górski, mathschy, mathurinm, Matthias Bussonnier, Max Del Giudice, Michael, Milan Straka, Muoki Caleb, N. Haiat, Nadia Tahiri, Ph. D, Naoki Hamada, Neil Botelho, Nicolas Hug, Nils Werner, noelano, Norbert Preining, oj\_lappi, Oleh Kozynets, Olivier Grisel, Pankaj Jindal, Pardeep Singh, Parthiv Chigurupati, Patrice Becker, Pete Green, pgithubs, Poorna Kumar, Prabakaran Kumaresshan, Probinette4, pspachtholz, pwalchessen, Qi Zhang, rachel fischoff, Rachit Toshniwal, Rafey Iqbal Rahman, Rahul Jakhar, Ram Rachum, RamyaNP, rauwuckl, Ravi Kiran Boggavarapu, Ray Bell, Reshama Shaikh, Richard Decal, Rishi Advani, Rithvik Rao, Rob Romijnders, roei, Romain Tavenard, Roman Yurchak, Ruby Werman, Ryotaro Tsukada, sadak, Saket Khandelwal, Sam, Sam Ezebunandu, Sam Kimbinyi, Sarah Brown, Saurabh Jain, Sean O. Stalley, Sergio, Shail Shah, Shane Keller, Shao Yang Hong, Shashank Singh, Shooter23, Shubhanshu Mishra, simonamaggio, Soledad Galli, Srimukh Sripada, Stephan Steinfurt, subrat93, Sunitha Selvan, Swier, Sylvain Marié, SylvainLan, t-kusanagi2, Teon L Brooks, Terence Honles, Thijs van den Berg, Thomas J Fan, Thomas J. Fan, Thomas S Benjamin, Thomas9292, Thorben Jensen, tijanajovanovic, Timo Kaufmann, tnwei, Tom Dupré la Tour, Trevor Waite, ufmayer, Umberto Lupo, Venkatachalam N, Vikas Pandey, Vinicius Rios Fuck, Violeta, watchtheblur, Wenbo Zhao, willpeppo, xavier dupré, Xethan, Xue Qianming, xun-tang, yagi-3, Yakov Pchelintsev, Yashika Sharma, Yi-Yan Ge, Yue Wu, Yutaro Ikeda, Zaccharie Ramzi, zoj613, Zhao Feng.

---

v1.0.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.0

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_0\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_0\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 1.0.2

**December 2021**

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.Birch</span>, <span class="title-ref">feature\_selection.RFECV</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.GradientBoostingRegressor</span>, and <span class="title-ref">ensemble.GradientBoostingClassifier</span> do not raise warning when fitted on a pandas DataFrame anymore. `21578` by [Thomas Fan](https://github.com/thomasjpfan).

### Changelog

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an infinite loop in <span class="title-ref">cluster.SpectralClustering</span> by moving an iteration counter from try to except. `21271` by `Tyler Martin <martintb>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_openml</span> is now thread safe. Data is first downloaded to a temporary subfolder and then renamed. `21833` by `Siavash Rezazadeh <siavrez>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the constraint on the objective function of <span class="title-ref">decomposition.DictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.SparsePCA</span> and <span class="title-ref">decomposition.MiniBatchSparsePCA</span> to be convex and match the referenced article. `19210` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>, and <span class="title-ref">ensemble.RandomTreesEmbedding</span> now raise a `ValueError` when `bootstrap=False` and `max_samples` is not `None`. `21295` `Haoyin Xu <PSSF23>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Solve a bug in <span class="title-ref">ensemble.GradientBoostingClassifier</span> where the exponential loss was computing the positive gradient instead of the negative one. `22050` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed <span class="title-ref">feature\_selection.SelectFromModel</span> by improving support for base estimators that do not set <span class="title-ref">feature\_names\_in\_</span>. `21991` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.impute`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">linear\_model.RidgeClassifierCV</span> where the method <span class="title-ref">predict</span> was performing an <span class="title-ref">argmax</span> on the scores obtained from <span class="title-ref">decision\_function</span> instead of returning the multilabel indicator matrix. `19869` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.LassoLarsIC</span> now correctly computes AIC and BIC. An error is now raised when <span class="title-ref">n\_features \> n\_samples</span> and when the noise variance is not provided. `21481` by `Guillaume Lemaitre <glemaitre>` and `Andrés Babino <ababino>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an unnecessary error when fitting <span class="title-ref">manifold.Isomap</span> with a precomputed dense distance matrix where the neighbors graph has multiple disconnected components. `21915` by [Tom Dupre la Tour](https://github.com/TomDLT).

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` All <span class="title-ref">sklearn.metrics.DistanceMetric</span> subclasses now correctly support read-only buffer attributes. This fixes a regression introduced in 1.0.0 with respect to 0.24.2. `21694` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` All <span class="title-ref">sklearn.metrics.MinkowskiDistance</span> now accepts a weight parameter that makes it possible to write code that behaves consistently both with scipy 1.8 and earlier versions. In turns this means that all neighbors-based estimators (except those that use <span class="title-ref">algorithm="kd\_tree"</span>) now accept a weight parameter with <span class="title-ref">metric="minknowski"</span> to yield results that are always consistent with <span class="title-ref">scipy.spatial.distance.cdist</span>. `21741` by `Olivier Grisel <ogrisel>`.

#### `sklearn.multiclass`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">multiclass.OneVsRestClassifier.predict\_proba</span> does not error when fitted on constant integer targets. `21871` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KDTree</span> and <span class="title-ref">neighbors.BallTree</span> correctly supports read-only buffer attributes. `21845` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes compatibility bug with NumPy 1.22 in <span class="title-ref">preprocessing.OneHotEncoder</span>. `21517` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Prevents <span class="title-ref">tree.plot\_tree</span> from drawing out of the boundary of the figure. `21917` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Support loading pickles of decision tree models when the pickle has been generated on a platform with a different bitness. A typical example is to train and pickle the model on 64 bit machine and load the model on a 32 bit machine for prediction. `21552` by `Loïc Estève <lesteve>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.estimator\_html\_repr</span> now escapes all the estimator descriptions in the generated HTML. `21493` by `Aurélien Geron <ageron>`.

## Version 1.0.1

**October 2021**

### Fixed models

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Non-fit methods in the following classes do not raise a UserWarning when fitted on DataFrames with valid feature names: <span class="title-ref">covariance.EllipticEnvelope</span>, <span class="title-ref">ensemble.IsolationForest</span>, <span class="title-ref">ensemble.AdaBoostClassifier</span>, <span class="title-ref">neighbors.KNeighborsClassifier</span>, <span class="title-ref">neighbors.KNeighborsRegressor</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span>. `21199` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed <span class="title-ref">calibration.CalibratedClassifierCV</span> to take into account <span class="title-ref">sample\_weight</span> when computing the base estimator prediction when <span class="title-ref">ensemble=False</span>. `20638` by `Julien Bohné <JulienB-78>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">calibration.CalibratedClassifierCV</span> with <span class="title-ref">method="sigmoid"</span> that was ignoring the <span class="title-ref">sample\_weight</span> when computing the the Bayesian priors. `21179` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span>, ensuring reproducibility and equivalence between sparse and dense input. `21195` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug that could produce a segfault in rare cases for <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>. `21130` `Christian Lorentzen <lorentzenchr>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Compute <span class="title-ref">y\_std</span> properly with multi-target in <span class="title-ref">sklearn.gaussian\_process.GaussianProcessRegressor</span> allowing proper normalization in multi-target scene. `20761` by `Patrick de C. T. R. Ferreira <patrickctrf>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Fixed an efficiency regression introduced in version 1.0.0 in the <span class="title-ref">transform</span> method of <span class="title-ref">feature\_extraction.text.CountVectorizer</span> which no longer checks for uppercase characters in the provided vocabulary. `21251` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.text.CountVectorizer</span> and <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span> by raising an error when 'min\_idf' or 'max\_idf' are floating-point numbers greater than 1. `20752` by `Alek Lefebvre <AlekLefebvre>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improves stability of <span class="title-ref">linear\_model.LassoLars</span> for different versions of openblas. `21340` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.LogisticRegression</span> now raises a better error message when the solver does not support sparse matrices with int64 indices. `21093` by [Tom Dupre la Tour](https://github.com/TomDLT).

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KNeighborsClassifier</span>, <span class="title-ref">neighbors.KNeighborsRegressor</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsRegressor</span> with <span class="title-ref">metric="precomputed"</span> raises an error for <span class="title-ref">bsr</span> and <span class="title-ref">dok</span> sparse matrices in methods: <span class="title-ref">fit</span>, <span class="title-ref">kneighbors</span> and <span class="title-ref">radius\_neighbors</span>, due to handling of explicit zeros in <span class="title-ref">bsr</span> and <span class="title-ref">dok</span> `sparse graph` formats. `21199` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.pipeline`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">pipeline.Pipeline.get\_feature\_names\_out</span> correctly passes feature names out from one step of a pipeline to the next. `21351` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.svm`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.SVR</span> check for an inconsistency in its internal representation and raise an error instead of segfaulting. This fix also resolves [CVE-2020-28975](https://nvd.nist.gov/vuln/detail/CVE-2020-28975). `21336` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.utils`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.validation.\_check\_sample\_weight</span> can perform a non-negativity check on the sample weights. It can be turned on using the only\_non\_negative bool parameter. Estimators that check for non-negative weights are updated: <span class="title-ref">linear\_model.LinearRegression</span> (here the previous error message was misleading), <span class="title-ref">ensemble.AdaBoostClassifier</span>, <span class="title-ref">ensemble.AdaBoostRegressor</span>, <span class="title-ref">neighbors.KernelDensity</span>. `20880` by `Guillaume Lemaitre <glemaitre>` and `András Simon <simonandras>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Solve a bug in `sklearn.utils.metaestimators.if_delegate_has_method` where the underlying check for an attribute did not work with NumPy arrays. `21145` by `Zahlii <Zahlii>`.

#### Miscellaneous

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fitting an estimator on a dataset that has no feature names, that was previously fitted on a dataset with feature names no longer keeps the old feature names stored in the <span class="title-ref">feature\_names\_in\_</span> attribute. `21389` by `Jérémie du Boisberranger <jeremiedbb>`.

## Version 1.0.0

**September 2021**

### Minimal dependencies

Version 1.0.0 of scikit-learn requires python 3.7+, numpy 1.14.6+ and scipy 1.1.0+. Optional minimal dependency is matplotlib 2.2.2+.

### Enforcing keyword-only arguments

In an effort to promote clear and non-ambiguous use of the library, most constructor and function parameters must now be passed as keyword arguments (i.e. using the <span class="title-ref">param=value</span> syntax) instead of positional. If a keyword-only parameter is used as positional, a <span class="title-ref">TypeError</span> is now raised. `15005` `20002` by [Joel Nothman](https://joelnothman.com/), [Adrin Jalali](https://github.com/adrinjalali), [Thomas Fan](https://github.com/thomasjpfan), [Nicolas Hug](https://github.com/NicolasHug), and [Tom Dupre la Tour](https://github.com/TomDLT). See [SLEP009](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep009/proposal.html) for more details.

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.TSNE</span> now avoids numerical underflow issues during affinity matrix computation.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.Isomap</span> now connects disconnected components of the neighbors graph along some minimum distance pairs, instead of changing every infinite distances to zero.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The splitting criterion of <span class="title-ref">tree.DecisionTreeClassifier</span> and <span class="title-ref">tree.DecisionTreeRegressor</span> can be impacted by a fix in the handling of rounding errors. Previously some extra spurious splits could occur.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.train\_test\_split</span> with a <span class="title-ref">stratify</span> parameter and <span class="title-ref">model\_selection.StratifiedShuffleSplit</span> may lead to slightly different results.

Details are listed in the changelog below.

(While we are trying to better inform users by providing this information, we cannot assure that this list is complete.)

### Changelog

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The option for using the squared error via `loss` and `criterion` parameters was made more consistent. The preferred way is by setting the value to <span class="title-ref">"squared\_error"</span>. Old option names are still valid, produce the same models, but are deprecated and will be removed in version 1.2. `19310` by `Christian Lorentzen <lorentzenchr>`.
      - For <span class="title-ref">ensemble.ExtraTreesRegressor</span>, <span class="title-ref">criterion="mse"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
      - For <span class="title-ref">ensemble.GradientBoostingRegressor</span>, <span class="title-ref">loss="ls"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
      - For <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">criterion="mse"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
      - For <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>, <span class="title-ref">loss="least\_squares"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
      - For <span class="title-ref">linear\_model.RANSACRegressor</span>, <span class="title-ref">loss="squared\_loss"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead.
      - For <span class="title-ref">linear\_model.SGDRegressor</span>, <span class="title-ref">loss="squared\_loss"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
      - For <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">criterion="mse"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
      - For <span class="title-ref">tree.ExtraTreeRegressor</span>, <span class="title-ref">criterion="mse"</span> is deprecated, use <span class="title-ref">"squared\_error"</span> instead which is now the default.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The option for using the absolute error via `loss` and `criterion` parameters was made more consistent. The preferred way is by setting the value to <span class="title-ref">"absolute\_error"</span>. Old option names are still valid, produce the same models, but are deprecated and will be removed in version 1.2. `19733` by `Christian Lorentzen <lorentzenchr>`.
      - For <span class="title-ref">ensemble.ExtraTreesRegressor</span>, <span class="title-ref">criterion="mae"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead.
      - For <span class="title-ref">ensemble.GradientBoostingRegressor</span>, <span class="title-ref">loss="lad"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead.
      - For <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">criterion="mae"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead.
      - For <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>, <span class="title-ref">loss="least\_absolute\_deviation"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead.
      - For <span class="title-ref">linear\_model.RANSACRegressor</span>, <span class="title-ref">loss="absolute\_loss"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead which is now the default.
      - For <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">criterion="mae"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead.
      - For <span class="title-ref">tree.ExtraTreeRegressor</span>, <span class="title-ref">criterion="mae"</span> is deprecated, use <span class="title-ref">"absolute\_error"</span> instead.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">np.matrix</span> usage is deprecated in 1.0 and will raise a <span class="title-ref">TypeError</span> in 1.2. `20165` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `get_feature_names_out` has been added to the transformer API to get the names of the output features. <span class="title-ref">get\_feature\_names</span> has in turn been deprecated. `18444` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` All estimators store <span class="title-ref">feature\_names\_in\_</span> when fitted on pandas Dataframes. These feature names are compared to names seen in non-<span class="title-ref">fit</span> methods, e.g. <span class="title-ref">transform</span> and will raise a <span class="title-ref">FutureWarning</span> if they are not consistent. These `FutureWarning` s will become `ValueError` s in 1.2. `18010` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.base`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">config\_context</span> is now threadsafe. `18736` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.calibration`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">calibration.CalibrationDisplay</span> added to plot calibration curves. `17443` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The `predict` and `predict_proba` methods of <span class="title-ref">calibration.CalibratedClassifierCV</span> can now properly be used on prefitted pipelines. `19641` by `Alek Lefebvre <AlekLefebvre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an error when using a <span class="title-ref">ensemble.VotingClassifier</span> as <span class="title-ref">base\_estimator</span> in <span class="title-ref">calibration.CalibratedClassifierCV</span>. `20087` by `Clément Fauchereau <clement-f>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The `"k-means++"` initialization of <span class="title-ref">cluster.KMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span> is now faster, especially in multicore settings. `19002` by `Jon Crall <Erotemic>` and `Jérémie du
    Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.KMeans</span> with <span class="title-ref">algorithm='elkan'</span> is now faster in multicore settings. `19052` by `Yusuke Nagasaka <YusukeNagasaka>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.MiniBatchKMeans</span> is now faster in multicore settings. `17622` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.OPTICS</span> can now cache the output of the computation of the tree, using the <span class="title-ref">memory</span> parameter. `19024` by `Frankie Robertson <frankier>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">predict</span> and <span class="title-ref">fit\_predict</span> methods of <span class="title-ref">cluster.AffinityPropagation</span> now accept sparse data type for input data. `20117` by `Venkatachalam Natchiappan <venkyyuvy>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.MiniBatchKMeans</span> where the sample weights were partially ignored when the input is sparse. `17622` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improved convergence detection based on center change in <span class="title-ref">cluster.MiniBatchKMeans</span> which was almost never achievable. `17622` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.AgglomerativeClustering</span> now supports readonly memory-mapped datasets. `19883` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.AgglomerativeClustering</span> correctly connects components when connectivity and affinity are both precomputed and the number of connected components is greater than 1. `20597` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.FeatureAgglomeration</span> does not accept a `**params` kwarg in the `fit` function anymore, resulting in a more concise error message. `20899` by `Adam Li <adam2392>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.KMeans</span>, ensuring reproducibility and equivalence between sparse and dense input. `20200` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">cluster.Birch</span> attributes, <span class="title-ref">fit\_</span> and <span class="title-ref">partial\_fit\_</span>, are deprecated and will be removed in 1.2. `19297` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` the default value for the <span class="title-ref">batch\_size</span> parameter of <span class="title-ref">cluster.MiniBatchKMeans</span> was changed from 100 to 1024 due to efficiency reasons. The <span class="title-ref">n\_iter\_</span> attribute of <span class="title-ref">cluster.MiniBatchKMeans</span> now reports the number of started epochs and the <span class="title-ref">n\_steps\_</span> attribute reports the number of mini batches processed. `17622` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">cluster.spectral\_clustering</span> raises an improved error when passed a <span class="title-ref">np.matrix</span>. `20560` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.compose`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">compose.ColumnTransformer</span> now records the output of each transformer in <span class="title-ref">output\_indices\_</span>. `18393` by `Luca Bittarello <lbittarello>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">compose.ColumnTransformer</span> now allows DataFrame input to have its columns appear in a changed order in <span class="title-ref">transform</span>. Further, columns that are dropped will not be required in transform, and additional columns will be ignored if <span class="title-ref">remainder='drop'</span>. `19263` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds <span class="title-ref">\*\*predict\_params</span> keyword argument to <span class="title-ref">compose.TransformedTargetRegressor.predict</span> that passes keyword argument to the regressor. `19244` by `Ricardo <ricardojnf>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer.get\_feature\_names</span> supports non-string feature names returned by any of its transformers. However, note that `get_feature_names` is deprecated, use `get_feature_names_out` instead. `18459` by `Albert Villanova del Moral <albertvillanova>` and `Alonso Silva Allende <alonsosilvaallende>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.TransformedTargetRegressor</span> now takes nD targets with an adequate transformer. `18898` by `Oras Phongpanagnam <panangam>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Adds <span class="title-ref">verbose\_feature\_names\_out</span> to <span class="title-ref">compose.ColumnTransformer</span>. This flag controls the prefixing of feature names out in `get_feature_names_out`. `18444` and `21080` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.covariance`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Adds arrays check to <span class="title-ref">covariance.ledoit\_wolf</span> and <span class="title-ref">covariance.ledoit\_wolf\_shrinkage</span>. `20416` by `Hugo Defois
    <defoishugo>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates the following keys in \`[cv\_results]()<span class="title-ref">: </span>'mean\_score'<span class="title-ref">, </span>'std\_score'<span class="title-ref">, and </span>'split(k)\_score'<span class="title-ref"> in favor of </span>'mean\_test\_score'<span class="title-ref"> </span>'std\_test\_score'<span class="title-ref">, and </span>'split(k)\_test\_score'<span class="title-ref">. :pr:\`20583</span> by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_openml</span> now supports categories with missing values when returning a pandas dataframe. `19365` by [Thomas Fan](https://github.com/thomasjpfan) and `Amanda Dsouza <amy12xx>` and `EL-ATEIF Sara <elateifsara>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_kddcup99</span> raises a better message when the cached file is invalid. `19669` [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Replace usages of `__file__` related to resource file I/O with `importlib.resources` to avoid the assumption that these resource files (e.g. `iris.csv`) already exist on a filesystem, and by extension to enable compatibility with tools such as `PyOxidizer`. `20297` by `Jack Liu <jackzyliu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Shorten data file names in the openml tests to better support installing on Windows and its default 260 character limit on file names. `20209` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_kddcup99</span> returns dataframes when <span class="title-ref">return\_X\_y=True</span> and <span class="title-ref">as\_frame=True</span>. `19011` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">datasets.load\_boston</span> in 1.0 and it will be removed in 1.2. Alternative code snippets to load similar datasets are provided. Please report to the docstring of the function for details. `20729` by [Guillaume Lemaitre](https://github.com/glemaitre).

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` added a new approximate solver (randomized SVD, available with <span class="title-ref">eigen\_solver='randomized'</span>) to <span class="title-ref">decomposition.KernelPCA</span>. This significantly accelerates computation when the number of samples is much larger than the desired number of components. `12069` by `Sylvain Marié <smarie>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes incorrect multiple data-conversion warnings when clustering boolean data. `19046` by `Surya Prakash <jdsurya>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed <span class="title-ref">decomposition.dict\_learning</span>, used by <span class="title-ref">decomposition.DictionaryLearning</span>, to ensure determinism of the output. Achieved by flipping signs of the SVD output which is used to initialize the code. `18433` by `Bruno Charron <brcharron>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchSparsePCA</span> and <span class="title-ref">decomposition.dict\_learning\_online</span> where the update of the dictionary was incorrect. `19198` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.DictionaryLearning</span>, <span class="title-ref">decomposition.SparsePCA</span>, <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchSparsePCA</span>, <span class="title-ref">decomposition.dict\_learning</span> and <span class="title-ref">decomposition.dict\_learning\_online</span> where the restart of unused atoms during the dictionary update was not working as expected. `19198` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` In <span class="title-ref">decomposition.DictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.dict\_learning</span> and <span class="title-ref">decomposition.dict\_learning\_online</span>, <span class="title-ref">transform\_alpha</span> will be equal to <span class="title-ref">alpha</span> instead of 1.0 by default starting from version 1.2 `19159` by `Benoît Malézieux <bmalezieux>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Rename variable names in <span class="title-ref">decomposition.KernelPCA</span> to improve readability. <span class="title-ref">lambdas\_</span> and <span class="title-ref">alphas\_</span> are renamed to <span class="title-ref">eigenvalues\_</span> and <span class="title-ref">eigenvectors\_</span>, respectively. <span class="title-ref">lambdas\_</span> and <span class="title-ref">alphas\_</span> are deprecated and will be removed in 1.2. `19908` by `Kei Ishikawa <kstoneriv3>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">alpha</span> and <span class="title-ref">regularization</span> parameters of <span class="title-ref">decomposition.NMF</span> and <span class="title-ref">decomposition.non\_negative\_factorization</span> are deprecated and will be removed in 1.2. Use the new parameters <span class="title-ref">alpha\_W</span> and <span class="title-ref">alpha\_H</span> instead. `20512` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.dummy`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Attribute <span class="title-ref">n\_features\_in\_</span> in <span class="title-ref">dummy.DummyRegressor</span> and <span class="title-ref">dummy.DummyRegressor</span> is deprecated and will be removed in 1.2. `20960` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.ensemble`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingRegressor</span> take cgroups quotas into account when deciding the number of threads used by OpenMP. This avoids performance problems caused by over-subscription when using those classes in a docker container for instance. `20477` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">\~sklearn.ensemble.HistGradientBoostingRegressor</span> are no longer experimental. They are now considered stable and are subject to the same deprecation cycles as all other estimators. `19799` by [Nicolas Hug](https://github.com/NicolasHug).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Improve the HTML rendering of the <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span>. `19564` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added Poisson criterion to <span class="title-ref">ensemble.RandomForestRegressor</span>. `19836` by `Brian Sun
    <bsun94>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Do not allow to compute out-of-bag (OOB) score in <span class="title-ref">ensemble.RandomForestClassifier</span> and <span class="title-ref">ensemble.ExtraTreesClassifier</span> with multiclass-multioutput target since scikit-learn does not provide any metric supporting this type of target. Additional private refactoring was performed. `19162` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improve numerical precision for weights boosting in <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> to avoid underflows. `10096` by `Fenil Suchak <fenilsuchak>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the range of the argument `max_samples` to be `(0.0, 1.0]` in <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, where <span class="title-ref">max\_samples=1.0</span> is interpreted as using all <span class="title-ref">n\_samples</span> for bootstrapping. `20159` by `murata-yu`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> where the <span class="title-ref">sample\_weight</span> parameter got overwritten during <span class="title-ref">fit</span>. `20534` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Removes <span class="title-ref">tol=None</span> option in <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>. Please use <span class="title-ref">tol=0</span> for the same behavior. `19296` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.text.HashingVectorizer</span> where some input strings would result in negative indices in the transformed data. `19035` by `Liu Yu <ly648499246>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.DictVectorizer</span> by raising an error with unsupported value type. `19520` by `Jeff Zhao <kamiyaa>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">feature\_extraction.image.img\_to\_graph</span> and <span class="title-ref">feature\_extraction.image.grid\_to\_graph</span> where singleton connected components were not handled properly, resulting in a wrong vertex indexing. `18964` by [Bertrand Thirion](https://team.inria.fr/parietal/bertrand-thirions-page).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise a warning in <span class="title-ref">feature\_extraction.text.CountVectorizer</span> with <span class="title-ref">lowercase=True</span> when there are vocabulary entries with uppercase characters to avoid silent misses in the resulting feature vectors. `19401` by `Zito Relova <zitorelova>`

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">feature\_selection.r\_regression</span> computes Pearson's R correlation coefficients between the features and the target. `17169` by `Dmytro Lituiev <DSLituiev>` and `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_selection.RFE.fit</span> accepts additional estimator parameters that are passed directly to the estimator's <span class="title-ref">fit</span> method. `20380` by `Iván Pulido <ijpulidos>`, `Felipe Bidu <fbidu>`, `Gil Rutter <g-rutter>`, and `Adrin Jalali <adrinjalali>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">isotonic.isotonic\_regression</span> where the <span class="title-ref">sample\_weight</span> passed by a user were overwritten during `fit`. `20515` by `Carsten Allefeld <allefeld>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Change <span class="title-ref">feature\_selection.SequentialFeatureSelector</span> to allow for unsupervised modelling so that the <span class="title-ref">fit</span> signature need not do any <span class="title-ref">y</span> validation and allow for <span class="title-ref">y=None</span>. `19568` by `Shyam Desai <ShyamDesai>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Raises an error in <span class="title-ref">feature\_selection.VarianceThreshold</span> when the variance threshold is negative. `20207` by `Tomohiro Endo <europeanplaice>`
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">grid\_scores\_</span> in favor of split scores in <span class="title-ref">cv\_results\_</span> in <span class="title-ref">feature\_selection.RFECV</span>. <span class="title-ref">grid\_scores\_</span> will be removed in version 1.2. `20161` by `Shuhei Kayawari <wowry>` and `arka204`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">max\_samples</span> parameter in <span class="title-ref">inspection.permutation\_importance</span>. It enables to draw a subset of the samples to compute the permutation importance. This is useful to keep the method tractable when evaluating feature importance on large datasets. `20431` by `Oliver Pfaffel <o1iv3r>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add kwargs to format ICE and PD lines separately in partial dependence plots <span class="title-ref">inspection.plot\_partial\_dependence</span> and <span class="title-ref">inspection.PartialDependenceDisplay.plot</span>. `19428` by `Mehdi
    Hamoumi <mhham>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Allow multiple scorers input to <span class="title-ref">inspection.permutation\_importance</span>. `19411` by `Simona
    Maggio <simonamaggio>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">inspection.PartialDependenceDisplay</span> exposes a class method: <span class="title-ref">\~inspection.PartialDependenceDisplay.from\_estimator</span>. <span class="title-ref">inspection.plot\_partial\_dependence</span> is deprecated in favor of the class method and will be removed in 1.2. `20959` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.kernel_approximation`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">kernel\_approximation.Nystroem</span> where the attribute <span class="title-ref">component\_indices\_</span> did not correspond to the subset of sample indices used to generate the approximated kernel. `20554` by `Xiangyin Kong <kxytim>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">linear\_model.QuantileRegressor</span> which implements linear quantile regression with L1 penalty. `9978` by `David Dale <avidale>` and `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The new <span class="title-ref">linear\_model.SGDOneClassSVM</span> provides an SGD implementation of the linear One-Class SVM. Combined with kernel approximation techniques, this implementation approximates the solution of a kernelized One Class SVM while benefitting from a linear complexity in the number of samples. `10027` by `Albert Thomas <albertcthomas>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">sample\_weight</span> parameter to <span class="title-ref">linear\_model.LassoCV</span> and <span class="title-ref">linear\_model.ElasticNetCV</span>. `16449` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added new solver <span class="title-ref">lbfgs</span> (available with <span class="title-ref">solver="lbfgs"</span>) and <span class="title-ref">positive</span> argument to <span class="title-ref">linear\_model.Ridge</span>. When <span class="title-ref">positive</span> is set to <span class="title-ref">True</span>, forces the coefficients to be positive (only supported by <span class="title-ref">lbfgs</span>). `20231` by `Toshihiro Nakae <tnakae>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The implementation of <span class="title-ref">linear\_model.LogisticRegression</span> has been optimised for dense matrices when using <span class="title-ref">solver='newton-cg'</span> and <span class="title-ref">multi\_class\!='multinomial'</span>. `19571` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">fit</span> method preserves dtype for numpy.float32 in <span class="title-ref">linear\_model.Lars</span>, <span class="title-ref">linear\_model.LassoLars</span>, <span class="title-ref">linear\_model.LassoLars</span>, <span class="title-ref">linear\_model.LarsCV</span> and <span class="title-ref">linear\_model.LassoLarsCV</span>. `20155` by `Takeshi Oura
    <takoika>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Validate user-supplied gram matrix passed to linear models via the <span class="title-ref">precompute</span> argument. `19004` by `Adam Midvidy <amidvidy>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.ElasticNet.fit</span> no longer modifies <span class="title-ref">sample\_weight</span> in place. `19055` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.Lasso</span> and <span class="title-ref">linear\_model.ElasticNet</span> no longer have a <span class="title-ref">dual\_gap\_</span> not corresponding to their objective. `19172` by `Mathurin Massias <mathurinm>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">sample\_weight</span> are now fully taken into account in linear models when <span class="title-ref">normalize=True</span> for both feature centering and feature scaling. `19426` by `Alexandre Gramfort <agramfort>` and `Maria Telenczuk <maikia>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Points with residuals equal to `residual_threshold` are now considered as inliers for <span class="title-ref">linear\_model.RANSACRegressor</span>. This allows fitting a model perfectly on some datasets when <span class="title-ref">residual\_threshold=0</span>. `19499` by `Gregory Strubel <gregorystrubel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Sample weight invariance for <span class="title-ref">linear\_model.Ridge</span> was fixed in `19616` by `Oliver Grisel <ogrisel>` and `Christian Lorentzen
    <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The dictionary <span class="title-ref">params</span> in <span class="title-ref">linear\_model.enet\_path</span> and <span class="title-ref">linear\_model.lasso\_path</span> should only contain parameter of the coordinate descent solver. Otherwise, an error will be raised. `19391` by `Shao Yang Hong <hongshaoyang>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Raise a warning in <span class="title-ref">linear\_model.RANSACRegressor</span> that from version 1.2, <span class="title-ref">min\_samples</span> need to be set explicitly for models other than <span class="title-ref">linear\_model.LinearRegression</span>. `19390` by `Shao Yang Hong
    <hongshaoyang>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}`: The parameter `normalize` of <span class="title-ref">linear\_model.LinearRegression</span> is deprecated and will be removed in 1.2. Motivation for this deprecation: `normalize` parameter did not take any effect if `fit_intercept` was set to False and therefore was deemed confusing. The behavior of the deprecated `LinearModel(normalize=True)` can be reproduced with a <span class="title-ref">\~sklearn.pipeline.Pipeline</span> with `LinearModel` (where `LinearModel` is <span class="title-ref">\~linear\_model.LinearRegression</span>, <span class="title-ref">\~linear\_model.Ridge</span>, <span class="title-ref">\~linear\_model.RidgeClassifier</span>, <span class="title-ref">\~linear\_model.RidgeCV</span> or <span class="title-ref">\~linear\_model.RidgeClassifierCV</span>) as follows: `make_pipeline(StandardScaler(with_mean=False), LinearModel())`. The `normalize` parameter in <span class="title-ref">\~linear\_model.LinearRegression</span> was deprecated in `17743` by `Maria Telenczuk <maikia>` and `Alexandre Gramfort <agramfort>`. Same for <span class="title-ref">\~linear\_model.Ridge</span>, <span class="title-ref">\~linear\_model.RidgeClassifier</span>, <span class="title-ref">\~linear\_model.RidgeCV</span>, and <span class="title-ref">\~linear\_model.RidgeClassifierCV</span>, in: `17772` by `Maria
    Telenczuk <maikia>` and `Alexandre Gramfort <agramfort>`. Same for <span class="title-ref">\~linear\_model.BayesianRidge</span>, <span class="title-ref">\~linear\_model.ARDRegression</span> in: `17746` by `Maria Telenczuk <maikia>`. Same for <span class="title-ref">\~linear\_model.Lasso</span>, <span class="title-ref">\~linear\_model.LassoCV</span>, <span class="title-ref">\~linear\_model.ElasticNet</span>, <span class="title-ref">\~linear\_model.ElasticNetCV</span>, <span class="title-ref">\~linear\_model.MultiTaskLasso</span>, <span class="title-ref">\~linear\_model.MultiTaskLassoCV</span>, <span class="title-ref">\~linear\_model.MultiTaskElasticNet</span>, <span class="title-ref">\~linear\_model.MultiTaskElasticNetCV</span>, in: `17785` by `Maria
    Telenczuk <maikia>` and `Alexandre Gramfort <agramfort>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The `normalize` parameter of <span class="title-ref">\~linear\_model.OrthogonalMatchingPursuit</span> and <span class="title-ref">\~linear\_model.OrthogonalMatchingPursuitCV</span> will default to False in 1.2 and will be removed in 1.4. `17750` by `Maria Telenczuk
    <maikia>` and `Alexandre Gramfort <agramfort>`. Same for <span class="title-ref">\~linear\_model.Lars</span> <span class="title-ref">\~linear\_model.LarsCV</span> <span class="title-ref">\~linear\_model.LassoLars</span> <span class="title-ref">\~linear\_model.LassoLarsCV</span> <span class="title-ref">\~linear\_model.LassoLarsIC</span>, in `17769` by `Maria Telenczuk
    <maikia>` and `Alexandre Gramfort <agramfort>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Keyword validation has moved from <span class="title-ref">\_\_init\_\_</span> and <span class="title-ref">set\_params</span> to <span class="title-ref">fit</span> for the following estimators conforming to scikit-learn's conventions: <span class="title-ref">\~linear\_model.SGDClassifier</span>, <span class="title-ref">\~linear\_model.SGDRegressor</span>, <span class="title-ref">\~linear\_model.SGDOneClassSVM</span>, <span class="title-ref">\~linear\_model.PassiveAggressiveClassifier</span>, and <span class="title-ref">\~linear\_model.PassiveAggressiveRegressor</span>. `20683` by [Guillaume Lemaitre](https://github.com/glemaitre).

#### `sklearn.manifold`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Implement <span class="title-ref">'auto'</span> heuristic for the <span class="title-ref">learning\_rate</span> in <span class="title-ref">manifold.TSNE</span>. It will become default in 1.2. The default initialization will change to <span class="title-ref">pca</span> in 1.2. PCA initialization will be scaled to have standard deviation 1e-4 in 1.2. `19491` by `Dmitry Kobak <dkobak>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Change numerical precision to prevent underflow issues during affinity matrix computation for <span class="title-ref">manifold.TSNE</span>. `19472` by `Dmitry Kobak <dkobak>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.Isomap</span> now uses <span class="title-ref">scipy.sparse.csgraph.shortest\_path</span> to compute the graph shortest path. It also connects disconnected components of the neighbors graph along some minimum distance pairs, instead of changing every infinite distances to zero. `20531` by [Roman Yurchak](https://github.com/rth) and [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Decrease the numerical default tolerance in the lobpcg call in <span class="title-ref">manifold.spectral\_embedding</span> to prevent numerical instability. `21194` by `Andrew Knyazev <lobpcg>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.mean\_pinball\_loss</span> exposes the pinball loss for quantile regression. `19415` by `Xavier Dupré <sdpython>` and `Oliver Grisel <ogrisel>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.d2\_tweedie\_score</span> calculates the D^2 regression score for Tweedie deviances with power parameter `power`. This is a generalization of the <span class="title-ref">r2\_score</span> and can be interpreted as percentage of Tweedie deviance explained. `17036` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.mean\_squared\_log\_error</span> now supports <span class="title-ref">squared=False</span>. `20326` by `Uttam kumar <helper-uttam>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improved speed of <span class="title-ref">metrics.confusion\_matrix</span> when labels are integral. `9843` by `Jon Crall <Erotemic>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A fix to raise an error in <span class="title-ref">metrics.hinge\_loss</span> when `pred_decision` is 1d whereas it is a multiclass classification or when `pred_decision` parameter is not consistent with the `labels` parameter. `19643` by `Pierre Attard <PierreAttard>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.ConfusionMatrixDisplay.plot</span> uses the correct max for colormap. `19784` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Samples with zero <span class="title-ref">sample\_weight</span> values do not affect the results from <span class="title-ref">metrics.det\_curve</span>, <span class="title-ref">metrics.precision\_recall\_curve</span> and <span class="title-ref">metrics.roc\_curve</span>. `18328` by `Albert Villanova del Moral <albertvillanova>` and `Alonso Silva Allende <alonsosilvaallende>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` avoid overflow in <span class="title-ref">metrics.adjusted\_rand\_score</span> with large amount of data. `20312` by `Divyanshu Deoli
    <divyanshudeoli>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.ConfusionMatrixDisplay</span> exposes two class methods <span class="title-ref">\~metrics.ConfusionMatrixDisplay.from\_estimator</span> and <span class="title-ref">\~metrics.ConfusionMatrixDisplay.from\_predictions</span> allowing to create a confusion matrix plot using an estimator or the predictions. <span class="title-ref">metrics.plot\_confusion\_matrix</span> is deprecated in favor of these two class methods and will be removed in 1.2. `18543` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.PrecisionRecallDisplay</span> exposes two class methods <span class="title-ref">\~metrics.PrecisionRecallDisplay.from\_estimator</span> and <span class="title-ref">\~metrics.PrecisionRecallDisplay.from\_predictions</span> allowing to create a precision-recall curve using an estimator or the predictions. <span class="title-ref">metrics.plot\_precision\_recall\_curve</span> is deprecated in favor of these two class methods and will be removed in 1.2. `20552` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.DetCurveDisplay</span> exposes two class methods <span class="title-ref">\~metrics.DetCurveDisplay.from\_estimator</span> and <span class="title-ref">\~metrics.DetCurveDisplay.from\_predictions</span> allowing to create a confusion matrix plot using an estimator or the predictions. <span class="title-ref">metrics.plot\_det\_curve</span> is deprecated in favor of these two class methods and will be removed in 1.2. `19278` by [Guillaume Lemaitre](https://github.com/glemaitre).

#### `sklearn.mixture`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Ensure that the best parameters are set appropriately in the case of divergency for <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span>. `20030` by `Tingshan Liu <tliu68>` and `Benjamin Pedigo <bdpedigo>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` added <span class="title-ref">model\_selection.StratifiedGroupKFold</span>, that combines <span class="title-ref">model\_selection.StratifiedKFold</span> and <span class="title-ref">model\_selection.GroupKFold</span>, providing an ability to split data preserving the distribution of classes in each split while keeping each group within a single split. `18649` by `Leandro Hermida <hermidalc>` and `Rodion Martynov <marrodion>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` warn only once in the main process for per-split fit failures in cross-validation. `20619` by `Loïc Estève <lesteve>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">model\_selection.BaseShuffleSplit</span> base class is now public. `20056` by `pabloduque0`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid premature overflow in <span class="title-ref">model\_selection.train\_test\_split</span>. `20904` by `Tomasz Jakubek <t-jakubek>`.

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">fit</span> and <span class="title-ref">partial\_fit</span> methods of the discrete naive Bayes classifiers (<span class="title-ref">naive\_bayes.BernoulliNB</span>, <span class="title-ref">naive\_bayes.CategoricalNB</span>, <span class="title-ref">naive\_bayes.ComplementNB</span>, and <span class="title-ref">naive\_bayes.MultinomialNB</span>) now correctly handle the degenerate case of a single class in the training set. `18925` by `David Poznik <dpoznik>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The attribute `sigma_` is now deprecated in <span class="title-ref">naive\_bayes.GaussianNB</span> and will be removed in 1.2. Use `var_` instead. `18842` by `Hong Shao Yang <hongshaoyang>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The creation of <span class="title-ref">neighbors.KDTree</span> and <span class="title-ref">neighbors.BallTree</span> has been improved for their worst-cases time complexity from \(\mathcal{O}(n^2)\) to \(\mathcal{O}(n)\). `19473` by `jiefangxuanyan <jiefangxuanyan>` and `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.DistanceMetric</span> subclasses now support readonly memory-mapped datasets. `19883` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.NearestNeighbors</span>, <span class="title-ref">neighbors.KNeighborsClassifier</span>, <span class="title-ref">neighbors.RadiusNeighborsClassifier</span>, <span class="title-ref">neighbors.KNeighborsRegressor</span> and <span class="title-ref">neighbors.RadiusNeighborsRegressor</span> do not validate <span class="title-ref">weights</span> in <span class="title-ref">\_\_init\_\_</span> and validates <span class="title-ref">weights</span> in <span class="title-ref">fit</span> instead. `20072` by `Juan Carlos Alfaro Jiménez <alfaro96>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter <span class="title-ref">kwargs</span> of <span class="title-ref">neighbors.RadiusNeighborsClassifier</span> is deprecated and will be removed in 1.2. `20842` by `Juan Martín Loyola <jmloyola>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span> now correctly support continued training when loading from a pickled file. `19631` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.pipeline`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">predict\_proba</span> and <span class="title-ref">predict\_log\_proba</span> methods of the <span class="title-ref">pipeline.Pipeline</span> now support passing prediction kwargs to the final estimator. `19790` by `Christopher Flynn <crflynn>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` The new <span class="title-ref">preprocessing.SplineTransformer</span> is a feature preprocessing tool for the generation of B-splines, parametrized by the polynomial `degree` of the splines, number of knots `n_knots` and knot positioning strategy `knots`. `18368` by `Christian Lorentzen <lorentzenchr>`. <span class="title-ref">preprocessing.SplineTransformer</span> also supports periodic splines via the `extrapolation` argument. `19483` by `Malte Londschien <mlondschien>`. <span class="title-ref">preprocessing.SplineTransformer</span> supports sample weights for knot position strategy `"quantile"`. `20526` by `Malte Londschien <mlondschien>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OrdinalEncoder</span> supports passing through missing values by default. `19069` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OneHotEncoder</span> now supports <span class="title-ref">handle\_unknown='ignore'</span> and dropping categories. `19041` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.PolynomialFeatures</span> now supports passing a tuple to <span class="title-ref">degree</span>, i.e. <span class="title-ref">degree=(min\_degree, max\_degree)</span>. `20250` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">preprocessing.StandardScaler</span> is faster and more memory efficient. `20652` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Changed `algorithm` argument for <span class="title-ref">cluster.KMeans</span> in <span class="title-ref">preprocessing.KBinsDiscretizer</span> from `auto` to `full`. `19934` by `Gleb Levitskiy <GLevV>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The implementation of <span class="title-ref">fit</span> for <span class="title-ref">preprocessing.PolynomialFeatures</span> transformer is now faster. This is especially noticeable on large sparse input. `19734` by `Fred
    Robinson <frrad>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">preprocessing.StandardScaler.inverse\_transform</span> method now raises error when the input data is 1D. `19752` by `Zhehao Liu
    <Max1993Liu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.scale</span>, <span class="title-ref">preprocessing.StandardScaler</span> and similar scalers detect near-constant features to avoid scaling them to very large values. This problem happens in particular when using a scaler on sparse data with a constant column with sample weights, in which case centering is typically disabled. `19527` by `Oliver Grisel
    <ogrisel>` and `Maria Telenczuk <maikia>` and `19788` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.StandardScaler.inverse\_transform</span> now correctly handles integer dtypes. `19356` by `makoeppel`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OrdinalEncoder.inverse\_transform</span> is not supporting sparse matrix and raises the appropriate error message. `19879` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">fit</span> method of <span class="title-ref">preprocessing.OrdinalEncoder</span> will not raise error when <span class="title-ref">handle\_unknown='ignore'</span> and unknown categories are given to <span class="title-ref">fit</span>. `19906` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">preprocessing.OrdinalEncoder</span> where large Python numeric would raise an error due to overflow when casted to C type (<span class="title-ref">np.float64</span> or <span class="title-ref">np.int64</span>). `20727` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.FunctionTransformer</span> does not set <span class="title-ref">n\_features\_in\_</span> based on the input to <span class="title-ref">inverse\_transform</span>. `20961` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">n\_input\_features\_</span> attribute of <span class="title-ref">preprocessing.PolynomialFeatures</span> is deprecated in favor of <span class="title-ref">n\_features\_in\_</span> and will be removed in 1.2. `20240` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter <span class="title-ref">\*\*params</span> of <span class="title-ref">svm.OneClassSVM.fit</span> is deprecated and will be removed in 1.2. `20843` by `Juan Martín Loyola <jmloyola>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add <span class="title-ref">fontname</span> argument in <span class="title-ref">tree.export\_graphviz</span> for non-English characters. `18959` by `Zero <Zeroto521>` and `wstates <wstates>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improves compatibility of <span class="title-ref">tree.plot\_tree</span> with high DPI screens. `20023` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span> where a node could be split whereas it should not have been due to incorrect handling of rounding errors. `19336` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">n\_features\_</span> attribute of <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeRegressor</span> is deprecated in favor of <span class="title-ref">n\_features\_in\_</span> and will be removed in 1.2. `20272` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Deprecated the default value of the <span class="title-ref">random\_state=0</span> in <span class="title-ref">\~sklearn.utils.extmath.randomized\_svd</span>. Starting in 1.2, the default value of <span class="title-ref">random\_state</span> will be set to <span class="title-ref">None</span>. `19459` by `Cindy Bezuidenhout <cinbez>` and `Clifford Akai-Nettey<cliffordEmmanuel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added helper decorator <span class="title-ref">utils.metaestimators.available\_if</span> to provide flexibility in metaestimators making methods available or unavailable on the basis of state, in a more readable way. `19948` by [Joel Nothman](https://joelnothman.com/).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.validation.check\_is\_fitted</span> now uses `__sklearn_is_fitted__` if available, instead of checking for attributes ending with an underscore. This also makes <span class="title-ref">pipeline.Pipeline</span> and <span class="title-ref">preprocessing.FunctionTransformer</span> pass `check_is_fitted(estimator)`. `20657` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">utils.sparsefuncs.mean\_variance\_axis</span> where the precision of the computed variance was very poor when the real variance is exactly zero. `19766` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The docstrings of properties that are decorated with <span class="title-ref">utils.deprecated</span> are now properly wrapped. `20385` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.stats.\_weighted\_percentile</span> now correctly ignores zero-weighted observations smaller than the smallest observation with positive weight for `percentile=0`. Affected classes are <span class="title-ref">dummy.DummyRegressor</span> for `quantile=0` and <span class="title-ref">ensemble.HuberLossFunction</span> and <span class="title-ref">ensemble.HuberLossFunction</span> for `alpha=0`. `20528` by `Malte Londschien <mlondschien>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.\_safe\_indexing</span> explicitly takes a dataframe copy when integer indices are provided avoiding to raise a warning from Pandas. This warning was previously raised in resampling utilities and functions using those utilities (e.g. <span class="title-ref">model\_selection.train\_test\_split</span>, <span class="title-ref">model\_selection.cross\_validate</span>, <span class="title-ref">model\_selection.cross\_val\_score</span>, <span class="title-ref">model\_selection.cross\_val\_predict</span>). `20673` by `Joris Van den Bossche  <jorisvandenbossche>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">utils.is\_scalar\_nan</span> where large Python numbers would raise an error due to overflow in C types (<span class="title-ref">np.float64</span> or <span class="title-ref">np.int64</span>). `20727` by [Guillaume Lemaitre](https://github.com/glemaitre).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Support for <span class="title-ref">np.matrix</span> is deprecated in <span class="title-ref">\~sklearn.utils.check\_array</span> in 1.0 and will raise a <span class="title-ref">TypeError</span> in 1.2. `20165` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.\_testing.assert\_warns</span> and <span class="title-ref">utils.\_testing.assert\_warns\_message</span> are deprecated in 1.0 and will be removed in 1.2. Used <span class="title-ref">pytest.warns</span> context manager instead. Note that these functions were not documented and part from the public API. `20521` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Fixed several bugs in <span class="title-ref">utils.graph.graph\_shortest\_path</span>, which is now deprecated. Use <span class="title-ref">scipy.sparse.csgraph.shortest\_path</span> instead. `20531` by [Tom Dupre la Tour](https://github.com/TomDLT).

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 0.24, including:

Abdulelah S. Al Mesfer, Abhinav Gupta, Adam J. Stewart, Adam Li, Adam Midvidy, Adrian Garcia Badaracco, Adrian Sadłocha, Adrin Jalali, Agamemnon Krasoulis, Alberto Rubiales, Albert Thomas, Albert Villanova del Moral, Alek Lefebvre, Alessia Marcolini, Alexandr Fonari, Alihan Zihna, Aline Ribeiro de Almeida, Amanda, Amanda Dsouza, Amol Deshmukh, Ana Pessoa, Anavelyz, Andreas Mueller, Andrew Delong, Ashish, Ashvith Shetty, Atsushi Nukariya, Aurélien Geron, Avi Gupta, Ayush Singh, baam, BaptBillard, Benjamin Pedigo, Bertrand Thirion, Bharat Raghunathan, bmalezieux, Brian Rice, Brian Sun, Bruno Charron, Bryan Chen, bumblebee, caherrera-meli, Carsten Allefeld, CeeThinwa, Chiara Marmo, chrissobel, Christian Lorentzen, Christopher Yeh, Chuliang Xiao, Clément Fauchereau, cliffordEmmanuel, Conner Shen, Connor Tann, David Dale, David Katz, David Poznik, Dimitri Papadopoulos Orfanos, Divyanshu Deoli, dmallia17, Dmitry Kobak, DS\_anas, Eduardo Jardim, EdwinWenink, EL-ATEIF Sara, Eleni Markou, EricEllwanger, Eric Fiegel, Erich Schubert, Ezri-Mudde, Fatos Morina, Felipe Rodrigues, Felix Hafner, Fenil Suchak, flyingdutchman23, Flynn, Fortune Uwha, Francois Berenger, Frankie Robertson, Frans Larsson, Frederick Robinson, frellwan, Gabriel S Vicente, Gael Varoquaux, genvalen, Geoffrey Thomas, geroldcsendes, Gleb Levitskiy, Glen, Glòria Macià Muñoz, gregorystrubel, groceryheist, Guillaume Lemaitre, guiweber, Haidar Almubarak, Hans Moritz Günther, Haoyin Xu, Harris Mirza, Harry Wei, Harutaka Kawamura, Hassan Alsawadi, Helder Geovane Gomes de Lima, Hugo DEFOIS, Igor Ilic, Ikko Ashimine, Isaack Mungui, Ishaan Bhat, Ishan Mishra, Iván Pulido, iwhalvic, J Alexander, Jack Liu, James Alan Preiss, James Budarz, James Lamb, Jannik, Jeff Zhao, Jennifer Maldonado, Jérémie du Boisberranger, Jesse Lima, Jianzhu Guo, jnboehm, Joel Nothman, JohanWork, John Paton, Jonathan Schneider, Jon Crall, Jon Haitz Legarreta Gorroño, Joris Van den Bossche, José Manuel Nápoles Duarte, Juan Carlos Alfaro Jiménez, Juan Martin Loyola, Julien Jerphanion, Julio Batista Silva, julyrashchenko, JVM, Kadatatlu Kishore, Karen Palacio, Kei Ishikawa, kmatt10, kobaski, Kot271828, Kunj, KurumeYuta, kxytim, lacrosse91, LalliAcqua, Laveen Bagai, Leonardo Rocco, Leonardo Uieda, Leopoldo Corona, Loic Esteve, LSturtew, Luca Bittarello, Luccas Quadros, Lucy Jiménez, Lucy Liu, ly648499246, Mabu Manaileng, Manimaran, makoeppel, Marco Gorelli, Maren Westermann, Mariangela, Maria Telenczuk, marielaraj, Martin Hirzel, Mateo Noreña, Mathieu Blondel, Mathis Batoul, mathurinm, Matthew Calcote, Maxime Prieur, Maxwell, Mehdi Hamoumi, Mehmet Ali Özer, Miao Cai, Michal Karbownik, michalkrawczyk, Mitzi, mlondschien, Mohamed Haseeb, Mohamed Khoualed, Muhammad Jarir Kanji, murata-yu, Nadim Kawwa, Nanshan Li, naozin555, Nate Parsons, Neal Fultz, Nic Annau, Nicolas Hug, Nicolas Miller, Nico Stefani, Nigel Bosch, Nikita Titov, Nodar Okroshiashvili, Norbert Preining, novaya, Ogbonna Chibuike Stephen, OGordon100, Oliver Pfaffel, Olivier Grisel, Oras Phongpanangam, Pablo Duque, Pablo Ibieta-Jimenez, Patric Lacouth, Paulo S. Costa, Paweł Olszewski, Peter Dye, PierreAttard, Pierre-Yves Le Borgne, PranayAnchuri, Prince Canuma, putschblos, qdeffense, RamyaNP, ranjanikrishnan, Ray Bell, Rene Jean Corneille, Reshama Shaikh, ricardojnf, RichardScottOZ, Rodion Martynov, Rohan Paul, Roman Lutz, Roman Yurchak, Samuel Brice, Sandy Khosasi, Sean Benhur J, Sebastian Flores, Sebastian Pölsterl, Shao Yang Hong, shinehide, shinnar, shivamgargsya, Shooter23, Shuhei Kayawari, Shyam Desai, simonamaggio, Sina Tootoonian, solosilence, Steven Kolawole, Steve Stagg, Surya Prakash, swpease, Sylvain Marié, Takeshi Oura, Terence Honles, TFiFiE, Thomas A Caswell, Thomas J. Fan, Tim Gates, TimotheeMathieu, Timothy Wolodzko, Tim Vink, t-jakubek, t-kusanagi, tliu68, Tobias Uhmann, tom1092, Tomás Moreyra, Tomás Ronald Hughes, Tom Dupré la Tour, Tommaso Di Noto, Tomohiro Endo, TONY GEORGE, Toshihiro NAKAE, tsuga, Uttam kumar, vadim-ushtanit, Vangelis Gkiastas, Venkatachalam N, Vilém Zouhar, Vinicius Rios Fuck, Vlasovets, waijean, Whidou, xavier dupré, xiaoyuchai, Yasmeen Alsaedy, yoch, Yosuke KOBAYASHI, Yu Feng, YusukeNagasaka, yzhenman, Zero, ZeyuSun, ZhaoweiWang, Zito, Zito Relova

---

v1.1.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.1

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_1\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_1\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 1.1.3

**October 2022**

This bugfix release only includes fixes for compatibility with the latest SciPy release \>= 1.9.2. Notable changes include:

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Include <span class="title-ref">msvcp140.dll</span> in the scikit-learn wheels since it has been removed in the latest SciPy wheels. `24631` by `Chiara Marmo <cmarmo>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Create wheels for Python 3.11. `24446` by `Chiara Marmo <cmarmo>`.

Other bug fixes will be available in the next 1.2 release, which will be released in the coming weeks.

Note that support for 32-bit Python on Windows has been dropped in this release. This is due to the fact that SciPy 1.9.2 also dropped the support for that platform. Windows users are advised to install the 64-bit version of Python instead.

## Version 1.1.2

**August 2022**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.TSNE</span> now throws a <span class="title-ref">ValueError</span> when fit with <span class="title-ref">perplexity\>=n\_samples</span> to ensure mathematical correctness of the algorithm. `10805` by `Mathias Andersen <MrMathias>` and `23471` by `Meekail Zain <micky774>`.

### Changelog

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A default HTML representation is shown for meta-estimators with invalid parameters. `24015` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Add support for F-contiguous arrays for estimators and functions whose back-end have been changed in 1.1. `23990` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Wheels are now available for MacOS 10.9 and greater. `23833` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.base`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">get\_params</span> method of the <span class="title-ref">base.BaseEstimator</span> class now supports estimators with <span class="title-ref">type</span>-type params that have the <span class="title-ref">get\_params</span> method. `24017` by `Henry Sorsky <hsorsky>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.Birch</span> that could trigger an error when splitting a node if there are duplicates in the dataset. `23395` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_selection.SelectFromModel</span> defaults to selection threshold 1e-5 when the estimator is either <span class="title-ref">linear\_model.ElasticNet</span> or <span class="title-ref">linear\_model.ElasticNetCV</span> with <span class="title-ref">l1\_ratio</span> equals 1 or <span class="title-ref">linear\_model.LassoCV</span>. `23636` by `Hao Chun Chang <haochunchang>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">impute.SimpleImputer</span> uses the dtype seen in <span class="title-ref">fit</span> for <span class="title-ref">transform</span> when the dtype is object. `22063` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Use dtype-aware tolerances for the validation of gram matrices (passed by users or precomputed). `22059` by `Malte S. Kurz <MalteKurz>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an error in <span class="title-ref">linear\_model.LogisticRegression</span> with <span class="title-ref">solver="newton-cg"</span>, <span class="title-ref">fit\_intercept=True</span>, and a single feature. `23608` by [Tom Dupre la Tour](https://github.com/TomDLT).

#### `sklearn.manifold`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.TSNE</span> now throws a <span class="title-ref">ValueError</span> when fit with <span class="title-ref">perplexity\>=n\_samples</span> to ensure mathematical correctness of the algorithm. `10805` by `Mathias Andersen <MrMathias>` and `23471` by `Meekail Zain <micky774>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed error message of <span class="title-ref">metrics.coverage\_error</span> for 1D array input. `23548` by `Hao Chun Chang <haochunchang>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OrdinalEncoder.inverse\_transform</span> correctly handles use cases where <span class="title-ref">unknown\_value</span> or <span class="title-ref">encoded\_missing\_value</span> is <span class="title-ref">nan</span>. `24087` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed invalid memory access bug during fit in <span class="title-ref">tree.DecisionTreeRegressor</span> and <span class="title-ref">tree.DecisionTreeClassifier</span>. `23273` by [Thomas Fan](https://github.com/thomasjpfan).

## Version 1.1.1

**May 2022**

### Changelog

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The error message is improved when importing <span class="title-ref">model\_selection.HalvingGridSearchCV</span>, <span class="title-ref">model\_selection.HalvingRandomSearchCV</span>, or <span class="title-ref">impute.IterativeImputer</span> without importing the experimental flag. `23194` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added an extension in doc/conf.py to automatically generate the list of estimators that handle NaN values. `23198` by `Lise Kleiber <lisekleiber>`, `Zhehao Liu <MaxwellLZH>` and `Chiara Marmo <cmarmo>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid timeouts in <span class="title-ref">datasets.fetch\_openml</span> by not passing a <span class="title-ref">timeout</span> argument, `23358` by `Loïc Estève <lesteve>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid spurious warning in <span class="title-ref">decomposition.IncrementalPCA</span> when <span class="title-ref">n\_samples == n\_components</span>. `23264` by `Lucy Liu <lucyleeow>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">partial\_fit</span> method of <span class="title-ref">feature\_selection.SelectFromModel</span> now conducts validation for <span class="title-ref">max\_features</span> and <span class="title-ref">feature\_names\_in</span> parameters. `23299` by `Long Bao <lorentzbao>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes <span class="title-ref">metrics.precision\_recall\_curve</span> to compute precision-recall at 100% recall. The Precision-Recall curve now displays the last point corresponding to a classifier that always predicts the positive class: recall=100% and precision=class balance. `23214` by `Stéphane Collot <stephanecollot>` and `Max Baak <mbaak>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.PolynomialFeatures</span> with `degree` equal to 0 will raise error when `include_bias` is set to False, and outputs a single constant array when `include_bias` is set to True. `23370` by `Zhehao Liu <MaxwellLZH>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes performance regression with low cardinality features for <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.GradientBoostingClassifier</span>, and <span class="title-ref">ensemble.GradientBoostingRegressor</span>. `23410` by `Loïc Estève <lesteve>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.class\_weight.compute\_sample\_weight</span> now works with sparse <span class="title-ref">y</span>. `23115` by `kernc <kernc>`.

## Version 1.1.0

**May 2022**

### Minimal dependencies

Version 1.1.0 of scikit-learn requires python 3.8+, numpy 1.17.3+ and scipy 1.3.2+. Optional minimal dependency is matplotlib 3.1.2+.

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.KMeans</span> now defaults to `algorithm="lloyd"` instead of `algorithm="auto"`, which was equivalent to `algorithm="elkan"`. Lloyd's algorithm and Elkan's algorithm converge to the same solution, up to numerical rounding errors, but in general Lloyd's algorithm uses much less memory, and it is often faster.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Fitting <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.GradientBoostingClassifier</span>, and <span class="title-ref">ensemble.GradientBoostingRegressor</span> is on average 15% faster than in previous versions thanks to a new sort algorithm to find the best split. Models might be different because of a different handling of splits with tied criterion values: both the old and the new sorting algorithm are unstable sorting algorithms. `22868` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The eigenvectors initialization for <span class="title-ref">cluster.SpectralClustering</span> and <span class="title-ref">manifold.SpectralEmbedding</span> now samples from a Gaussian when using the <span class="title-ref">'amg'</span> or <span class="title-ref">'lobpcg'</span> solver. This change improves numerical stability of the solver, but may result in a different model.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_selection.f\_regression</span> and <span class="title-ref">feature\_selection.r\_regression</span> will now returned finite score by default instead of <span class="title-ref">np.nan</span> and <span class="title-ref">np.inf</span> for some corner case. You can use <span class="title-ref">force\_finite=False</span> if you really want to get non-finite values and keep the old behavior.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Panda's DataFrames with all non-string columns such as a MultiIndex no longer warns when passed into an Estimator. Estimators will continue to ignore the column names in DataFrames with non-string columns. For <span class="title-ref">feature\_names\_in\_</span> to be defined, columns must be all strings. `22410` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.KBinsDiscretizer</span> changed handling of bin edges slightly, which might result in a different encoding with the same data.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">calibration.calibration\_curve</span> changed handling of bin edges slightly, which might result in a different output curve given the same data.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now uses the correct variance-scaling coefficient which may result in different model behavior.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_selection.SelectFromModel.fit</span> and <span class="title-ref">feature\_selection.SelectFromModel.partial\_fit</span> can now be called with <span class="title-ref">prefit=True</span>. <span class="title-ref">estimators\_</span> will be a deep copy of <span class="title-ref">estimator</span> when <span class="title-ref">prefit=True</span>. `23271` by `Guillaume Lemaitre <glemaitre>`.

### Changelog

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Low-level routines for reductions on pairwise distances for dense float64 datasets have been refactored. The following functions and estimators now benefit from improved performances in terms of hardware scalability and speed-ups:
    
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin</span>
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin\_min</span>
      - <span class="title-ref">sklearn.cluster.AffinityPropagation</span>
      - <span class="title-ref">sklearn.cluster.Birch</span>
      - <span class="title-ref">sklearn.cluster.MeanShift</span>
      - <span class="title-ref">sklearn.cluster.OPTICS</span>
      - <span class="title-ref">sklearn.cluster.SpectralClustering</span>
      - <span class="title-ref">sklearn.feature\_selection.mutual\_info\_regression</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.LocalOutlierFactor</span>
      - <span class="title-ref">sklearn.neighbors.NearestNeighbors</span>
      - <span class="title-ref">sklearn.manifold.Isomap</span>
      - <span class="title-ref">sklearn.manifold.LocallyLinearEmbedding</span>
      - <span class="title-ref">sklearn.manifold.TSNE</span>
      - <span class="title-ref">sklearn.manifold.trustworthiness</span>
      - <span class="title-ref">sklearn.semi\_supervised.LabelPropagation</span>
      - <span class="title-ref">sklearn.semi\_supervised.LabelSpreading</span>
    
    For instance <span class="title-ref">sklearn.neighbors.NearestNeighbors.kneighbors</span> and <span class="title-ref">sklearn.neighbors.NearestNeighbors.radius\_neighbors</span> can respectively be up to ×20 and ×5 faster than previously on a laptop.
    
    Moreover, implementations of those two algorithms are now suitable for machine with many cores, making them usable for datasets consisting of millions of samples.
    
    `21987`, `22064`, `22065`, `22288` and `22320` by `Julien Jerphanion <jjerphan>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` All scikit-learn models now generate a more informative error message when some input contains unexpected <span class="title-ref">NaN</span> or infinite values. In particular the message contains the input name ("X", "y" or "sample\_weight") and if an unexpected <span class="title-ref">NaN</span> value is found in <span class="title-ref">X</span>, the error message suggests potential solutions. `21219` by `Olivier Grisel <ogrisel>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` All scikit-learn models now generate a more informative error message when setting invalid hyper-parameters with <span class="title-ref">set\_params</span>. `21542` by `Olivier Grisel <ogrisel>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Removes random unique identifiers in the HTML representation. With this change, jupyter notebooks are reproducible as long as the cells are run in the same order. `23098` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Estimators with <span class="title-ref">non\_deterministic</span> tag set to <span class="title-ref">True</span> will skip both <span class="title-ref">check\_methods\_sample\_order\_invariance</span> and <span class="title-ref">check\_methods\_subset\_invariance</span> tests. `22318` by `Zhehao Liu <MaxwellLZH>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The option for using the log loss, aka binomial or multinomial deviance, via the <span class="title-ref">loss</span> parameters was made more consistent. The preferred way is by setting the value to <span class="title-ref">"log\_loss"</span>. Old option names are still valid and produce the same models, but are deprecated and will be removed in version 1.3.
    
      - For <span class="title-ref">ensemble.GradientBoostingClassifier</span>, the <span class="title-ref">loss</span> parameter name "deviance" is deprecated in favor of the new name "log\_loss", which is now the default. `23036` by `Christian Lorentzen <lorentzenchr>`.
      - For <span class="title-ref">ensemble.HistGradientBoostingClassifier</span>, the <span class="title-ref">loss</span> parameter names "auto", "binary\_crossentropy" and "categorical\_crossentropy" are deprecated in favor of the new name "log\_loss", which is now the default. `23040` by `Christian Lorentzen <lorentzenchr>`.
      - For <span class="title-ref">linear\_model.SGDClassifier</span>, the <span class="title-ref">loss</span> parameter name "log" is deprecated in favor of the new name "log\_loss". `23046` by `Christian Lorentzen <lorentzenchr>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Rich html representation of estimators is now enabled by default in Jupyter notebooks. It can be deactivated by setting <span class="title-ref">display='text'</span> in <span class="title-ref">sklearn.set\_config</span>. `22856` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.calibration`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">calibration.calibration\_curve</span> accepts a parameter <span class="title-ref">pos\_label</span> to specify the positive class label. `21032` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">calibration.CalibratedClassifierCV.fit</span> now supports passing <span class="title-ref">fit\_params</span>, which are routed to the <span class="title-ref">base\_estimator</span>. `18170` by `Benjamin Bossan <BenjaminBossan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">calibration.CalibrationDisplay</span> accepts a parameter <span class="title-ref">pos\_label</span> to add this information to the plot. `21038` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">calibration.calibration\_curve</span> handles bin edges more consistently now. `14975` by [Andreas Müller](https://amueller.github.io/) and `22526` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">calibration.calibration\_curve</span>'s <span class="title-ref">normalize</span> parameter is now deprecated and will be removed in version 1.3. It is recommended that a proper probability (i.e. a classifier's `predict_proba` positive class) is used for <span class="title-ref">y\_prob</span>. `23095` by `Jordan Silke <jsilke>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">cluster.BisectingKMeans</span> introducing Bisecting K-Means algorithm `20031` by `Michal Krawczyk <michalkrawczyk>`, `Tom Dupre la Tour <TomDLT>` and `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.SpectralClustering</span> and <span class="title-ref">cluster.spectral\_clustering</span> now include the new <span class="title-ref">'cluster\_qr'</span> method that clusters samples in the embedding space as an alternative to the existing <span class="title-ref">'kmeans'</span> and <span class="title-ref">'discrete'</span> methods. See <span class="title-ref">cluster.spectral\_clustering</span> for more details. `21148` by `Andrew Knyazev <lobpcg>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">cluster.Birch</span>, <span class="title-ref">cluster.FeatureAgglomeration</span>, <span class="title-ref">cluster.KMeans</span>, <span class="title-ref">cluster.MiniBatchKMeans</span>. `22255` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.SpectralClustering</span> now raises consistent error messages when passed invalid values for <span class="title-ref">n\_clusters</span>, <span class="title-ref">n\_init</span>, <span class="title-ref">gamma</span>, <span class="title-ref">n\_neighbors</span>, <span class="title-ref">eigen\_tol</span> or <span class="title-ref">degree</span>. `21881` by `Hugo Vassard <hvassard>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.AffinityPropagation</span> now returns cluster centers and labels if they exist, even if the model has not fully converged. When returning these potentially-degenerate cluster centers and labels, a new warning message is shown. If no cluster centers were constructed, then the cluster centers remain an empty list with labels set to <span class="title-ref">-1</span> and the original warning message is shown. `22217` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` In <span class="title-ref">cluster.KMeans</span>, the default `algorithm` is now `"lloyd"` which is the full classical EM-style algorithm. Both `"auto"` and `"full"` are deprecated and will be removed in version 1.3. They are now aliases for `"lloyd"`. The previous default was `"auto"`, which relied on Elkan's algorithm. Lloyd's algorithm uses less memory than Elkan's, it is faster on many datasets, and its results are identical, hence the change. `21735` by `Aurélien Geron <ageron>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.KMeans</span>'s <span class="title-ref">init</span> parameter now properly supports array-like input and NumPy string scalars. `22154` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> now removes validation errors from <span class="title-ref">\_\_init\_\_</span> and <span class="title-ref">set\_params</span> methods. `22537` by `iofall <iofall>` and `Arisa Y. <arisayosh>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` `get_feature_names_out` functionality in <span class="title-ref">compose.ColumnTransformer</span> was broken when columns were specified using <span class="title-ref">slice</span>. This is fixed in `22775` and `22913` by `randomgeek78 <randomgeek78>`.

#### `sklearn.covariance`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">covariance.GraphicalLassoCV</span> now accepts NumPy array for the parameter <span class="title-ref">alphas</span>. `22493` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.cross_decomposition`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` the <span class="title-ref">inverse\_transform</span> method of <span class="title-ref">cross\_decomposition.PLSRegression</span>, <span class="title-ref">cross\_decomposition.PLSCanonical</span> and <span class="title-ref">cross\_decomposition.CCA</span> now allows reconstruction of a <span class="title-ref">X</span> target when a <span class="title-ref">Y</span> parameter is given. `19680` by `Robin Thibaut <robinthibaut>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to all transformers in the `~sklearn.cross_decomposition` module: <span class="title-ref">cross\_decomposition.CCA</span>, <span class="title-ref">cross\_decomposition.PLSSVD</span>, <span class="title-ref">cross\_decomposition.PLSRegression</span>, and <span class="title-ref">cross\_decomposition.PLSCanonical</span>. `22119` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The shape of the `coef_` attribute of <span class="title-ref">cross\_decomposition.CCA</span>, <span class="title-ref">cross\_decomposition.PLSCanonical</span> and <span class="title-ref">cross\_decomposition.PLSRegression</span> will change in version 1.3, from <span class="title-ref">(n\_features, n\_targets)</span> to <span class="title-ref">(n\_targets, n\_features)</span>, to be consistent with other linear models and to make it work with interface expecting a specific shape for <span class="title-ref">coef\_</span> (e.g. <span class="title-ref">feature\_selection.RFE</span>). `22016` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` add the fitted attribute <span class="title-ref">intercept\_</span> to <span class="title-ref">cross\_decomposition.PLSCanonical</span>, <span class="title-ref">cross\_decomposition.PLSRegression</span>, and <span class="title-ref">cross\_decomposition.CCA</span>. The method <span class="title-ref">predict</span> is indeed equivalent to <span class="title-ref">Y = X @ coef\_ + intercept\_</span>. `22015` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">datasets.load\_files</span> now accepts a ignore list and an allow list based on file extensions. `19747` by `Tony Attalla <tonyattalla>` and `22498` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.make\_swiss\_roll</span> now supports the optional argument hole; when set to True, it returns the swiss-hole dataset. `21482` by `Sebastian Pujalte <pujaltes>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.make\_blobs</span> no longer copies data during the generation process, therefore uses less memory. `22412` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.load\_diabetes</span> now accepts the parameter `scaled`, to allow loading unscaled data. The scaled version of this dataset is now computed from the unscaled data, and can produce slightly different results that in previous version (within a 1e-4 absolute tolerance). `16605` by `Mandy Gu <happilyeverafter95>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.fetch\_openml</span> now has two optional arguments <span class="title-ref">n\_retries</span> and <span class="title-ref">delay</span>. By default, <span class="title-ref">datasets.fetch\_openml</span> will retry 3 times in case of a network failure with a delay between each try. `21901` by `Rileran <rileran>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_covtype</span> is now concurrent-safe: data is downloaded to a temporary directory before being moved to the data directory. `23113` by `Ilion Beyst <iasoon>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">datasets.make\_sparse\_coded\_signal</span> now accepts a parameter <span class="title-ref">data\_transposed</span> to explicitly specify the shape of matrix <span class="title-ref">X</span>. The default behavior <span class="title-ref">True</span> is to return a transposed matrix <span class="title-ref">X</span> corresponding to a <span class="title-ref">(n\_features, n\_samples)</span> shape. The default value will change to <span class="title-ref">False</span> in version 1.3. `21425` by `Gabriel Stefanini Vicente <g4brielvs>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added a new estimator <span class="title-ref">decomposition.MiniBatchNMF</span>. It is a faster but less accurate version of non-negative matrix factorization, better suited for large datasets. `16948` by `Chiara Marmo <cmarmo>`, `Patricio Cerda <pcerda>` and `Jérémie du Boisberranger <jeremiedbb>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.dict\_learning</span>, <span class="title-ref">decomposition.dict\_learning\_online</span> and <span class="title-ref">decomposition.sparse\_encode</span> preserve dtype for <span class="title-ref">numpy.float32</span>. <span class="title-ref">decomposition.DictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> and <span class="title-ref">decomposition.SparseCoder</span> preserve dtype for <span class="title-ref">numpy.float32</span>. `22002` by `Takeshi Oura <takoika>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.PCA</span> exposes a parameter <span class="title-ref">n\_oversamples</span> to tune <span class="title-ref">utils.extmath.randomized\_svd</span> and get accurate results when the number of features is large. `21109` by `Smile <x-shadow-man>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> and <span class="title-ref">decomposition.dict\_learning\_online</span> have been refactored and now have a stopping criterion based on a small change of the dictionary or objective function, controlled by the new <span class="title-ref">max\_iter</span>, <span class="title-ref">tol</span> and <span class="title-ref">max\_no\_improvement</span> parameters. In addition, some of their parameters and attributes are deprecated.
    
      - the <span class="title-ref">n\_iter</span> parameter of both is deprecated. Use <span class="title-ref">max\_iter</span> instead.
      - the <span class="title-ref">iter\_offset</span>, <span class="title-ref">return\_inner\_stats</span>, <span class="title-ref">inner\_stats</span> and <span class="title-ref">return\_n\_iter</span> parameters of <span class="title-ref">decomposition.dict\_learning\_online</span> serve internal purpose and are deprecated.
      - the <span class="title-ref">inner\_stats\_</span>, <span class="title-ref">iter\_offset\_</span> and <span class="title-ref">random\_state\_</span> attributes of <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> serve internal purpose and are deprecated.
      - the default value of the <span class="title-ref">batch\_size</span> parameter of both will change from 3 to 256 in version 1.3.
    
    `18975` by `Jérémie du Boisberranger <jeremiedbb>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.SparsePCA</span> and <span class="title-ref">decomposition.MiniBatchSparsePCA</span> preserve dtype for <span class="title-ref">numpy.float32</span>. `22111` by `Takeshi Oura <takoika>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.TruncatedSVD</span> now allows <span class="title-ref">n\_components == n\_features</span>, if <span class="title-ref">algorithm='randomized'</span>. `22181` by `Zach Deane-Mayer <zachmayer>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to all transformers in the `~sklearn.decomposition` module: <span class="title-ref">decomposition.DictionaryLearning</span>, <span class="title-ref">decomposition.FactorAnalysis</span>, <span class="title-ref">decomposition.FastICA</span>, <span class="title-ref">decomposition.IncrementalPCA</span>, <span class="title-ref">decomposition.KernelPCA</span>, <span class="title-ref">decomposition.LatentDirichletAllocation</span>, <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchSparsePCA</span>, <span class="title-ref">decomposition.NMF</span>, <span class="title-ref">decomposition.PCA</span>, <span class="title-ref">decomposition.SparsePCA</span>, and <span class="title-ref">decomposition.TruncatedSVD</span>. `21334` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.TruncatedSVD</span> exposes the parameter <span class="title-ref">n\_oversamples</span> and <span class="title-ref">power\_iteration\_normalizer</span> to tune <span class="title-ref">utils.extmath.randomized\_svd</span> and get accurate results when the number of features is large, the rank of the matrix is high, or other features of the matrix make low rank approximation difficult. `21705` by `Jay S. Stanley III <stanleyjs>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.PCA</span> exposes the parameter <span class="title-ref">power\_iteration\_normalizer</span> to tune <span class="title-ref">utils.extmath.randomized\_svd</span> and get more accurate results when low rank approximation is difficult. `21705` by `Jay S. Stanley III <stanleyjs>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.FastICA</span> now validates input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21432` by `Hannah Bohle <hhnnhh>` and `Maren Westermann <marenwestermann>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.FastICA</span> now accepts <span class="title-ref">np.float32</span> data without silent upcasting. The dtype is preserved by <span class="title-ref">fit</span> and <span class="title-ref">fit\_transform</span> and the main fitted attributes use a dtype of the same precision as the training data. `22806` by `Jihane Bennis <JihaneBennis>` and `Olivier Grisel <ogrisel>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.FactorAnalysis</span> now validates input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21713` by `Haya <HayaAlmutairi>` and `Krum Arnaudov <krumeto>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.KernelPCA</span> now validates input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21567` by `Maggie Chege <MaggieChege>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.PCA</span> and <span class="title-ref">decomposition.IncrementalPCA</span> more safely calculate precision using the inverse of the covariance matrix if <span class="title-ref">self.noise\_variance\_</span> is zero. `22300` by `Meekail Zain <micky774>` and `15948` by `sysuresh`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Greatly reduced peak memory usage in <span class="title-ref">decomposition.PCA</span> when calling <span class="title-ref">fit</span> or <span class="title-ref">fit\_transform</span>. `22553` by `Meekail Zain <micky774>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">decomposition.FastICA</span> now supports unit variance for whitening. The default value of its <span class="title-ref">whiten</span> argument will change from <span class="title-ref">True</span> (which behaves like <span class="title-ref">'arbitrary-variance'</span>) to <span class="title-ref">'unit-variance'</span> in version 1.3. `19490` by `Facundo Ferrin <fferrin>` and `Julien Jerphanion <jjerphan>`.

#### `sklearn.discriminant_analysis`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span>. `22120` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now uses the correct variance-scaling coefficient which may result in different model behavior. `15984` by `Okon Samuel <OkonSamuel>` and `22696` by `Meekail Zain <micky774>`.

#### `sklearn.dummy`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">dummy.DummyRegressor</span> no longer overrides the <span class="title-ref">constant</span> parameter during <span class="title-ref">fit</span>. `22486` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added additional option <span class="title-ref">loss="quantile"</span> to <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> for modelling quantiles. The quantile level can be specified with the new parameter <span class="title-ref">quantile</span>. `21800` and `20567` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">fit</span> of <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> now calls <span class="title-ref">utils.check\_array</span> with parameter <span class="title-ref">force\_all\_finite=False</span> for non initial warm-start runs as it has already been checked before. `22159` by `Geoffrey Paris <Geoffrey-Paris>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> is faster, for binary and in particular for multiclass problems thanks to the new private loss function module. `20811`, `20567` and `21814` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds support to use pre-fit models with <span class="title-ref">cv="prefit"</span> in <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span>. `16748` by `Siqi He <siqi-he>` and `22215` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.RandomForestClassifier</span> and <span class="title-ref">ensemble.ExtraTreesClassifier</span> have the new <span class="title-ref">criterion="log\_loss"</span>, which is equivalent to <span class="title-ref">criterion="entropy"</span>. `23047` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">ensemble.VotingClassifier</span>, <span class="title-ref">ensemble.VotingRegressor</span>, <span class="title-ref">ensemble.StackingClassifier</span>, and <span class="title-ref">ensemble.StackingRegressor</span>. `22695` and `22697` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.RandomTreesEmbedding</span> now has an informative `get_feature_names_out` function that includes both tree index and leaf index in the output feature names. `21762` by `Zhehao Liu <MaxwellLZH>` and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Fitting a <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>, and <span class="title-ref">ensemble.RandomTreesEmbedding</span> is now faster in a multiprocessing setting, especially for subsequent fits with <span class="title-ref">warm\_start</span> enabled. `22106` by `Pieter Gijsbers <PGijsbers>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Change the parameter <span class="title-ref">validation\_fraction</span> in <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> so that an error is raised if anything other than a float is passed in as an argument. `21632` by `Genesis Valencia <genvalen>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Removed a potential source of CPU oversubscription in <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> when CPU resource usage is limited, for instance using cgroups quota in a docker container. `22566` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> no longer warns when fitting on a pandas DataFrame with a non-default <span class="title-ref">scoring</span> parameter and early\_stopping enabled. `22908` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes HTML repr for <span class="title-ref">ensemble.StackingClassifier</span> and <span class="title-ref">ensemble.StackingRegressor</span>. `23097` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The attribute <span class="title-ref">loss\_</span> of <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> has been deprecated and will be removed in version 1.3. `23079` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Changed the default of <span class="title-ref">max\_features</span> to 1.0 for <span class="title-ref">ensemble.RandomForestRegressor</span> and to <span class="title-ref">"sqrt"</span> for <span class="title-ref">ensemble.RandomForestClassifier</span>. Note that these give the same fit results as before, but are much easier to understand. The old default value <span class="title-ref">"auto"</span> has been deprecated and will be removed in version 1.3. The same changes are also applied for <span class="title-ref">ensemble.ExtraTreesRegressor</span> and <span class="title-ref">ensemble.ExtraTreesClassifier</span>. `20803` by `Brian Sun <bsun94>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improve runtime performance of <span class="title-ref">ensemble.IsolationForest</span> by skipping repetitive input checks. `23149` by `Zhehao Liu <MaxwellLZH>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">feature\_extraction.FeatureHasher</span> now supports PyPy. `23023` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_extraction.FeatureHasher</span> now validates input parameters in <span class="title-ref">transform</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21573` by `Hannah Bohle <hhnnhh>` and `Maren Westermann <marenwestermann>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span> now does not create a <span class="title-ref">feature\_extraction.text.TfidfTransformer</span> at <span class="title-ref">\_\_init\_\_</span> as required by our API. `21832` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added auto mode to <span class="title-ref">feature\_selection.SequentialFeatureSelector</span>. If the argument <span class="title-ref">n\_features\_to\_select</span> is <span class="title-ref">'auto'</span>, select features until the score improvement does not exceed the argument <span class="title-ref">tol</span>. The default value of <span class="title-ref">n\_features\_to\_select</span> changed from <span class="title-ref">None</span> to <span class="title-ref">'warn'</span> in 1.1 and will become <span class="title-ref">'auto'</span> in 1.3. <span class="title-ref">None</span> and <span class="title-ref">'warn'</span> will be removed in 1.3. `20145` by `murata-yu <murata-yu>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added the ability to pass callables to the <span class="title-ref">max\_features</span> parameter of <span class="title-ref">feature\_selection.SelectFromModel</span>. Also introduced new attribute <span class="title-ref">max\_features\_</span> which is inferred from <span class="title-ref">max\_features</span> and the data during <span class="title-ref">fit</span>. If <span class="title-ref">max\_features</span> is an integer, then <span class="title-ref">max\_features\_ = max\_features</span>. If <span class="title-ref">max\_features</span> is a callable, then <span class="title-ref">max\_features\_ = max\_features(X)</span>. `22356` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_selection.GenericUnivariateSelect</span> preserves float32 dtype. `18482` by `Thierry Gameiro <titigmr>` and `Daniel Kharsa <aflatoune>` and `22370` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add a parameter <span class="title-ref">force\_finite</span> to <span class="title-ref">feature\_selection.f\_regression</span> and <span class="title-ref">feature\_selection.r\_regression</span>. This parameter allows to force the output to be finite in the case where a feature or a the target is constant or that the feature and target are perfectly correlated (only for the F-statistic). `17819` by `Juan Carlos Alfaro Jiménez <alfaro96>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improve runtime performance of <span class="title-ref">feature\_selection.chi2</span> with boolean arrays. `22235` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Reduced memory usage of <span class="title-ref">feature\_selection.chi2</span>. `21837` by `Louis Wagner <lrwagner>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">predict</span> and <span class="title-ref">sample\_y</span> methods of <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> now return arrays of the correct shape in single-target and multi-target cases, and for both <span class="title-ref">normalize\_y=False</span> and <span class="title-ref">normalize\_y=True</span>. `22199` by `Guillaume Lemaitre <glemaitre>`, `Aidar Shakerimoff <AidarShakerimoff>` and `Tenavi Nakamura-Zimmerer <Tenavi>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">gaussian\_process.GaussianProcessClassifier</span> raises a more informative error if <span class="title-ref">CompoundKernel</span> is passed via <span class="title-ref">kernel</span>. `22223` by `MarcoM <marcozzxx810>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">impute.SimpleImputer</span> now warns with feature names when features which are skipped due to the lack of any observed values in the training set. `21617` by `Christian Ritter <chritter>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added support for <span class="title-ref">pd.NA</span> in <span class="title-ref">impute.SimpleImputer</span>. `21114` by `Ying Xiong <yxiong>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">impute.SimpleImputer</span>, <span class="title-ref">impute.KNNImputer</span>, <span class="title-ref">impute.IterativeImputer</span>, and <span class="title-ref">impute.MissingIndicator</span>. `21078` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">verbose</span> parameter was deprecated for <span class="title-ref">impute.SimpleImputer</span>. A warning will always be raised upon the removal of empty columns. `21448` by `Oleh Kozynets <OlehKSS>` and `Christian Ritter <chritter>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add a display to plot the boundary decision of a classifier by using the method <span class="title-ref">inspection.DecisionBoundaryDisplay.from\_estimator</span>. `16061` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` In <span class="title-ref">inspection.PartialDependenceDisplay.from\_estimator</span>, allow <span class="title-ref">kind</span> to accept a list of strings to specify which type of plot to draw for each feature interaction. `19438` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">inspection.PartialDependenceDisplay.from\_estimator</span>, <span class="title-ref">inspection.PartialDependenceDisplay.plot</span>, and <span class="title-ref">inspection.plot\_partial\_dependence</span> now support plotting centered Individual Conditional Expectation (cICE) and centered PDP curves controlled by setting the parameter <span class="title-ref">centered</span>. `18310` by `Johannes Elfner <JoElfner>` and `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.isotonic`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">isotonic.IsotonicRegression</span>. `22249` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.kernel_approximation`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">kernel\_approximation.AdditiveChi2Sampler</span>. <span class="title-ref">kernel\_approximation.Nystroem</span>, <span class="title-ref">kernel\_approximation.PolynomialCountSketch</span>, <span class="title-ref">kernel\_approximation.RBFSampler</span>, and <span class="title-ref">kernel\_approximation.SkewedChi2Sampler</span>. `22137` and `22694` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.linear_model`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.ElasticNet</span>, <span class="title-ref">linear\_model.ElasticNetCV</span>, <span class="title-ref">linear\_model.Lasso</span> and <span class="title-ref">linear\_model.LassoCV</span> support <span class="title-ref">sample\_weight</span> for sparse input <span class="title-ref">X</span>. `22808` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.Ridge</span> with <span class="title-ref">solver="lsqr"</span> now supports to fit sparse input with <span class="title-ref">fit\_intercept=True</span>. `22950` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.QuantileRegressor</span> support sparse input for the highs based solvers. `21086` by `Venkatachalam Natchiappan <venkyyuvy>`. In addition, those solvers now use the CSC matrix right from the beginning which speeds up fitting. `22206` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.LogisticRegression</span> is faster for `solvers="lbfgs"` and `solver="newton-cg"`, for binary and in particular for multiclass problems thanks to the new private loss function module. In the multiclass case, the memory consumption has also been reduced for these solvers as the target is now label encoded (mapped to integers) instead of label binarized (one-hot encoded). The more classes, the larger the benefit. `21808`, `20567` and `21814` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.GammaRegressor</span>, <span class="title-ref">linear\_model.PoissonRegressor</span> and <span class="title-ref">linear\_model.TweedieRegressor</span> are faster for `solvers="lbfgs"`. `22548`, `21808` and `20567` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Rename parameter <span class="title-ref">base\_estimator</span> to <span class="title-ref">estimator</span> in <span class="title-ref">linear\_model.RANSACRegressor</span> to improve readability and consistency. <span class="title-ref">base\_estimator</span> is deprecated and will be removed in 1.3. `22062` by `Adrian Trujillo <trujillo9616>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.ElasticNet</span> and and other linear model classes using coordinate descent show error messages when non-finite parameter weights are produced. `22148` by `Christian Ritter <chritter>` and `Norbert Preining <norbusan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.ElasticNet</span> and <span class="title-ref">linear\_model.Lasso</span> now raise consistent error messages when passed invalid values for <span class="title-ref">l1\_ratio</span>, <span class="title-ref">alpha</span>, <span class="title-ref">max\_iter</span> and <span class="title-ref">tol</span>. `22240` by `Arturo Amor <ArturoAmorQ>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.BayesianRidge</span> and <span class="title-ref">linear\_model.ARDRegression</span> now preserve float32 dtype. `9087` by `Arthur Imbert <Henley13>` and `22525` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.RidgeClassifier</span> is now supporting multilabel classification. `19689` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span> now raise consistent error message when passed invalid values for <span class="title-ref">alphas</span>. `21606` by `Arturo Amor <ArturoAmorQ>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.Ridge</span> and <span class="title-ref">linear\_model.RidgeClassifier</span> now raise consistent error message when passed invalid values for <span class="title-ref">alpha</span>, <span class="title-ref">max\_iter</span> and <span class="title-ref">tol</span>. `21341` by `Arturo Amor <ArturoAmorQ>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.orthogonal\_mp\_gram</span> preservse dtype for <span class="title-ref">numpy.float32</span>. `22002` by `Takeshi Oura <takoika>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.LassoLarsIC</span> now correctly computes AIC and BIC. An error is now raised when <span class="title-ref">n\_features \> n\_samples</span> and when the noise variance is not provided. `21481` by `Guillaume Lemaitre <glemaitre>` and `Andrés Babino <ababino>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.TheilSenRegressor</span> now validates input parameter `max_subpopulation` in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21767` by `Maren Westermann <marenwestermann>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.ElasticNetCV</span> now produces correct warning when <span class="title-ref">l1\_ratio=0</span>. `21724` by `Yar Khine Phyo <yarkhinephyo>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> now set the <span class="title-ref">n\_iter\_</span> attribute with a shape that respects the docstring and that is consistent with the shape obtained when using the other solvers in the one-vs-rest setting. Previously, it would record only the maximum of the number of iterations for each binary sub-problem while now all of them are recorded. `21998` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The property <span class="title-ref">family</span> of <span class="title-ref">linear\_model.TweedieRegressor</span> is not validated in <span class="title-ref">\_\_init\_\_</span> anymore. Instead, this (private) property is deprecated in <span class="title-ref">linear\_model.GammaRegressor</span>, <span class="title-ref">linear\_model.PoissonRegressor</span> and <span class="title-ref">linear\_model.TweedieRegressor</span>, and will be removed in 1.3. `22548` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">coef\_</span> and <span class="title-ref">intercept\_</span> attributes of <span class="title-ref">linear\_model.LinearRegression</span> are now correctly computed in the presence of sample weights when the input is sparse. `22891` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">coef\_</span> and <span class="title-ref">intercept\_</span> attributes of <span class="title-ref">linear\_model.Ridge</span> with <span class="title-ref">solver="sparse\_cg"</span> and <span class="title-ref">solver="lbfgs"</span> are now correctly computed in the presence of sample weights when the input is sparse. `22899` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.SGDRegressor</span> and <span class="title-ref">linear\_model.SGDClassifier</span> now computes the validation error correctly when early stopping is enabled. `23256` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">linear\_model.LassoLarsIC</span> now exposes <span class="title-ref">noise\_variance</span> as a parameter in order to provide an estimate of the noise variance. This is particularly relevant when <span class="title-ref">n\_features \> n\_samples</span> and the estimator of the noise variance cannot be computed. `21481` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">manifold.Isomap</span> now supports radius-based neighbors via the <span class="title-ref">radius</span> argument. `19794` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">manifold.spectral\_embedding</span> and <span class="title-ref">manifold.SpectralEmbedding</span> supports <span class="title-ref">np.float32</span> dtype and will preserve this dtype. `21534` by `Andrew Knyazev <lobpcg>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">manifold.Isomap</span> and <span class="title-ref">manifold.LocallyLinearEmbedding</span>. `22254` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` added <span class="title-ref">metric\_params</span> to <span class="title-ref">manifold.TSNE</span> constructor for additional parameters of distance metric to use in optimization. `21805` by `Jeanne Dionisi <jeannedionisi>` and `22685` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">manifold.trustworthiness</span> raises an error if <span class="title-ref">n\_neighbours \>= n\_samples / 2</span> to ensure a correct support for the function. `18832` by `Hong Shao Yang <hongshaoyang>` and `23033` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.spectral\_embedding</span> now uses Gaussian instead of the previous uniform on \[0, 1\] random initial approximations to eigenvectors in eigen\_solvers <span class="title-ref">lobpcg</span> and <span class="title-ref">amg</span> to improve their numerical stability. `21565` by `Andrew Knyazev <lobpcg>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.r2\_score</span> and <span class="title-ref">metrics.explained\_variance\_score</span> have a new <span class="title-ref">force\_finite</span> parameter. Setting this parameter to <span class="title-ref">False</span> will return the actual non-finite score in case of perfect predictions or constant <span class="title-ref">y\_true</span>, instead of the finite approximation (<span class="title-ref">1.0</span> and <span class="title-ref">0.0</span> respectively) currently returned by default. `17266` by `Sylvain Marié <smarie>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.d2\_pinball\_score</span> and <span class="title-ref">metrics.d2\_absolute\_error\_score</span> calculate the \(D^2\) regression score for the pinball loss and the absolute error respectively. <span class="title-ref">metrics.d2\_absolute\_error\_score</span> is a special case of <span class="title-ref">metrics.d2\_pinball\_score</span> with a fixed quantile parameter <span class="title-ref">alpha=0.5</span> for ease of use and discovery. The \(D^2\) scores are generalizations of the <span class="title-ref">r2\_score</span> and can be interpreted as the fraction of deviance explained. `22118` by `Ohad Michel <ohadmich>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.top\_k\_accuracy\_score</span> raises an improved error message when <span class="title-ref">y\_true</span> is binary and <span class="title-ref">y\_score</span> is 2d. `22284` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.roc\_auc\_score</span> now supports `average=None` in the multiclass case when `multiclass='ovr'` which will return the score per class. `19158` by `Nicki Skafte <SkafteNicki>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds <span class="title-ref">im\_kw</span> parameter to <span class="title-ref">metrics.ConfusionMatrixDisplay.from\_estimator</span> <span class="title-ref">metrics.ConfusionMatrixDisplay.from\_predictions</span>, and <span class="title-ref">metrics.ConfusionMatrixDisplay.plot</span>. The <span class="title-ref">im\_kw</span> parameter is passed to the <span class="title-ref">matplotlib.pyplot.imshow</span> call when plotting the confusion matrix. `20753` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.silhouette\_score</span> now supports integer input for precomputed distances. `22108` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">metrics.normalized\_mutual\_info\_score</span> which could return unbounded values. `22635` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes <span class="title-ref">metrics.precision\_recall\_curve</span> and <span class="title-ref">metrics.average\_precision\_score</span> when true labels are all negative. `19085` by `Varun Agrawal <varunagrawal>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.SCORERS</span> is now deprecated and will be removed in 1.3. Please use <span class="title-ref">metrics.get\_scorer\_names</span> to retrieve the names of all available scorers. `22866` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Parameters `sample_weight` and `multioutput` of <span class="title-ref">metrics.mean\_absolute\_percentage\_error</span> are now keyword-only, in accordance with [SLEP009](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep009/proposal.html). A deprecation cycle was introduced. `21576` by `Paul-Emile Dugnat <pedugnat>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">"wminkowski"</span> metric of <span class="title-ref">metrics.DistanceMetric</span> is deprecated and will be removed in version 1.3. Instead the existing <span class="title-ref">"minkowski"</span> metric now takes in an optional <span class="title-ref">w</span> parameter for weights. This deprecation aims at remaining consistent with SciPy 1.8 convention. `21873` by `Yar Khine Phyo <yarkhinephyo>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.DistanceMetric</span> has been moved from `sklearn.neighbors` to `sklearn.metrics`. Using <span class="title-ref">neighbors.DistanceMetric</span> for imports is still valid for backward compatibility, but this alias will be removed in 1.3. `21177` by `Julien Jerphanion <jjerphan>`.

#### `sklearn.mixture`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span> can now be initialized using k-means++ and random data points. `20408` by `Gordon Walsh <g-walsh>`, `Alberto Ceballos<alceballosa>` and `Andres Rios<ariosramirez>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug that correctly initialize <span class="title-ref">precisions\_cholesky\_</span> in <span class="title-ref">mixture.GaussianMixture</span> when providing <span class="title-ref">precisions\_init</span> by taking its square root. `22058` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">mixture.GaussianMixture</span> now normalizes <span class="title-ref">weights\_</span> more safely, preventing rounding errors when calling <span class="title-ref">mixture.GaussianMixture.sample</span> with <span class="title-ref">n\_components=1</span>. `23034` by `Meekail Zain <micky774>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` it is now possible to pass <span class="title-ref">scoring="matthews\_corrcoef"</span> to all model selection tools with a <span class="title-ref">scoring</span> argument to use the Matthews correlation coefficient (MCC). `22203` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` raise an error during cross-validation when the fits for all the splits failed. Similarly raise an error during grid-search when the fits for all the models and all the splits failed. `21026` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.HalvingGridSearchCV</span> now validate input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21880` by `Mrinal Tyagi <MrinalTyagi>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.learning\_curve</span> now supports <span class="title-ref">partial\_fit</span> with regressors. `22982` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.multiclass`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">multiclass.OneVsRestClassifier</span> now supports a <span class="title-ref">verbose</span> parameter so progress on fitting can be seen. `22508` by `Chris Combs <combscCode>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">multiclass.OneVsOneClassifier.predict</span> returns correct predictions when the inner classifier only has a `predict_proba`. `22604` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.neighbors`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">neighbors.RadiusNeighborsTransformer</span>, <span class="title-ref">neighbors.KNeighborsTransformer</span> and <span class="title-ref">neighbors.NeighborhoodComponentsAnalysis</span>. `22212` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KernelDensity</span> now validates input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21430` by `Desislava Vasileva <DessyVV>` and `Lucy Jimenez <LucyJimenez>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KNeighborsRegressor.predict</span> now works properly when given an array-like input if <span class="title-ref">KNeighborsRegressor</span> is first constructed with a callable passed to the <span class="title-ref">weights</span> parameter. `22687` by `Meekail Zain <micky774>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span> show error messages when optimizers produce non-finite parameter weights. `22150` by `Christian Ritter <chritter>` and `Norbert Preining <norbusan>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">neural\_network.BernoulliRBM</span>. `22248` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.pipeline`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added support for "passthrough" in <span class="title-ref">pipeline.FeatureUnion</span>. Setting a transformer to "passthrough" will pass the features unchanged. `20860` by `Shubhraneel Pal <shubhraneel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">pipeline.Pipeline</span> now does not validate hyper-parameters in <span class="title-ref">\_\_init\_\_</span> but in <span class="title-ref">.fit()</span>. `21888` by `iofall <iofall>` and `Arisa Y. <arisayosh>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">pipeline.FeatureUnion</span> does not validate hyper-parameters in <span class="title-ref">\_\_init\_\_</span>. Validation is now handled in <span class="title-ref">.fit()</span> and <span class="title-ref">.fit\_transform()</span>. `21954` by `iofall <iofall>` and `Arisa Y. <arisayosh>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Defines <span class="title-ref">\_\_sklearn\_is\_fitted\_\_</span> in <span class="title-ref">pipeline.FeatureUnion</span> to return correct result with <span class="title-ref">utils.validation.check\_is\_fitted</span>. `22953` by `randomgeek78 <randomgeek78>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OneHotEncoder</span> now supports grouping infrequent categories into a single feature. Grouping infrequent categories is enabled by specifying how to select infrequent categories with <span class="title-ref">min\_frequency</span> or <span class="title-ref">max\_categories</span>. `16018` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds a <span class="title-ref">subsample</span> parameter to <span class="title-ref">preprocessing.KBinsDiscretizer</span>. This allows specifying a maximum number of samples to be used while fitting the model. The option is only available when <span class="title-ref">strategy</span> is set to <span class="title-ref">quantile</span>. `21445` by `Felipe Bidu <fbidu>` and `Amanda Dsouza <amy12xx>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds <span class="title-ref">encoded\_missing\_value</span> to <span class="title-ref">preprocessing.OrdinalEncoder</span> to configure the encoded value for missing data. `21988` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added the <span class="title-ref">get\_feature\_names\_out</span> method and a new parameter <span class="title-ref">feature\_names\_out</span> to <span class="title-ref">preprocessing.FunctionTransformer</span>. You can set <span class="title-ref">feature\_names\_out</span> to 'one-to-one' to use the input features names as the output feature names, or you can set it to a callable that returns the output feature names. This is especially useful when the transformer changes the number of features. If <span class="title-ref">feature\_names\_out</span> is None (which is the default), then <span class="title-ref">get\_output\_feature\_names</span> is not defined. `21569` by `Aurélien Geron <ageron>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to <span class="title-ref">preprocessing.Normalizer</span>, <span class="title-ref">preprocessing.KernelCenterer</span>, <span class="title-ref">preprocessing.OrdinalEncoder</span>, and <span class="title-ref">preprocessing.Binarizer</span>. `21079` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.PowerTransformer</span> with <span class="title-ref">method='yeo-johnson'</span> better supports significantly non-Gaussian data when searching for an optimal lambda. `20653` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.LabelBinarizer</span> now validates input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21434` by `Krum Arnaudov <krumeto>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.FunctionTransformer</span> with <span class="title-ref">check\_inverse=True</span> now provides informative error message when input has mixed dtypes. `19916` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.KBinsDiscretizer</span> handles bin edges more consistently now. `14975` by [Andreas Müller](https://amueller.github.io/) and `22526` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Adds <span class="title-ref">preprocessing.KBinsDiscretizer.get\_feature\_names\_out</span> support when <span class="title-ref">encode="ordinal"</span>. `22735` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.random_projection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds an <span class="title-ref">inverse\_transform</span> method and a <span class="title-ref">compute\_inverse\_transform</span> parameter to <span class="title-ref">random\_projection.GaussianRandomProjection</span> and <span class="title-ref">random\_projection.SparseRandomProjection</span>. When the parameter is set to True, the pseudo-inverse of the components is computed during <span class="title-ref">fit</span> and stored as <span class="title-ref">inverse\_components\_</span>. `21701` by `Aurélien Geron <ageron>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">random\_projection.SparseRandomProjection</span> and <span class="title-ref">random\_projection.GaussianRandomProjection</span> preserves dtype for <span class="title-ref">numpy.float32</span>. `22114` by `Takeshi Oura <takoika>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds `get_feature_names_out` to all transformers in the `sklearn.random_projection` module: <span class="title-ref">random\_projection.GaussianRandomProjection</span> and <span class="title-ref">random\_projection.SparseRandomProjection</span>. `21330` by `Loïc Estève <lesteve>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">svm.OneClassSVM</span>, <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.SVR</span> now expose <span class="title-ref">n\_iter\_</span>, the number of iterations of the libsvm optimization routine. `21408` by `Juan Martín Loyola <jmloyola>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.OneClassSVM</span>, <span class="title-ref">svm.NuSVC</span> now raise an error when the dual-gap estimation produce non-finite parameter weights. `22149` by `Christian Ritter <chritter>` and `Norbert Preining <norbusan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">svm.NuSVC</span>, <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.SVC</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.OneClassSVM</span> now validate input parameters in <span class="title-ref">fit</span> instead of <span class="title-ref">\_\_init\_\_</span>. `21436` by `Haidar Almubarak <Haidar13 >`.

#### `sklearn.tree`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">tree.DecisionTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeClassifier</span> have the new <span class="title-ref">criterion="log\_loss"</span>, which is equivalent to <span class="title-ref">criterion="entropy"</span>. `23047` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in the Poisson splitting criterion for <span class="title-ref">tree.DecisionTreeRegressor</span>. `22191` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Changed the default value of <span class="title-ref">max\_features</span> to 1.0 for <span class="title-ref">tree.ExtraTreeRegressor</span> and to <span class="title-ref">"sqrt"</span> for <span class="title-ref">tree.ExtraTreeClassifier</span>, which will not change the fit result. The original default value <span class="title-ref">"auto"</span> has been deprecated and will be removed in version 1.3. Setting <span class="title-ref">max\_features</span> to <span class="title-ref">"auto"</span> is also deprecated for <span class="title-ref">tree.DecisionTreeClassifier</span> and <span class="title-ref">tree.DecisionTreeRegressor</span>. `22476` by `Zhehao Liu <MaxwellLZH>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.check\_array</span> and <span class="title-ref">utils.multiclass.type\_of\_target</span> now accept an <span class="title-ref">input\_name</span> parameter to make the error message more informative when passed invalid input data (e.g. with NaN or infinite values). `21219` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.check\_array</span> returns a float ndarray with <span class="title-ref">np.nan</span> when passed a <span class="title-ref">Float32</span> or <span class="title-ref">Float64</span> pandas extension array with <span class="title-ref">pd.NA</span>. `21278` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.estimator\_html\_repr</span> shows a more helpful error message when running in a jupyter notebook that is not trusted. `21316` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.estimator\_html\_repr</span> displays an arrow on the top left corner of the HTML representation to show how the elements are clickable. `21298` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.check\_array</span> with <span class="title-ref">dtype=None</span> returns numeric arrays when passed in a pandas DataFrame with mixed dtypes. <span class="title-ref">dtype="numeric"</span> will also make better infer the dtype when the DataFrame has mixed dtypes. `22237` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.check\_scalar</span> now has better messages when displaying the type. `22218` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Changes the error message of the <span class="title-ref">ValidationError</span> raised by <span class="title-ref">utils.check\_X\_y</span> when y is None so that it is compatible with the <span class="title-ref">check\_requires\_y\_none</span> estimator check. `22578` by `Claudio Salvatore Arcidiacono <ClaudioSalvatoreArcidiacono>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.class\_weight.compute\_class\_weight</span> now only requires that all classes in <span class="title-ref">y</span> have a weight in <span class="title-ref">class\_weight</span>. An error is still raised when a class is present in <span class="title-ref">y</span> but not in <span class="title-ref">class\_weight</span>. `22595` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.estimator\_html\_repr</span> has an improved visualization for nested meta-estimators. `21310` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.check\_scalar</span> raises an error when <span class="title-ref">include\_boundaries={"left", "right"}</span> and the boundaries are not set. `22027` by `Marie Lanternier <mlant>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.metaestimators.available\_if</span> correctly returns a bounded method that can be pickled. `23077` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.estimator\_checks.check\_estimator</span>'s argument is now called <span class="title-ref">estimator</span> (previous name was <span class="title-ref">Estimator</span>). `22188` by `Mathurin Massias <mathurinm>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `utils.metaestimators.if_delegate_has_method` is deprecated and will be removed in version 1.3. Use <span class="title-ref">utils.metaestimators.available\_if</span> instead. `22830` by `Jérémie du Boisberranger <jeremiedbb>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.0, including:

2357juan, Abhishek Gupta, adamgonzo, Adam Li, adijohar, Aditya Kumawat, Aditya Raghuwanshi, Aditya Singh, Adrian Trujillo Duron, Adrin Jalali, ahmadjubair33, AJ Druck, aj-white, Alan Peixinho, Alberto Mario Ceballos-Arroyo, Alek Lefebvre, Alex, Alexandr, Alexandre Gramfort, alexanmv, almeidayoel, Amanda Dsouza, Aman Sharma, Amar pratap singh, Amit, amrcode, András Simon, Andreas Grivas, Andreas Mueller, Andrew Knyazev, Andriy, Angus L'Herrou, Ankit Sharma, Anne Ducout, Arisa, Arth, arthurmello, Arturo Amor, ArturoAmor, Atharva Patil, aufarkari, Aurélien Geron, avm19, Ayan Bag, baam, Bardiya Ak, Behrouz B, Ben3940, Benjamin Bossan, Bharat Raghunathan, Bijil Subhash, bmreiniger, Brandon Truth, Brenden Kadota, Brian Sun, cdrig, Chalmer Lowe, Chiara Marmo, Chitteti Srinath Reddy, Chloe-Agathe Azencott, Christian Lorentzen, Christian Ritter, christopherlim98, Christoph T. Weidemann, Christos Aridas, Claudio Salvatore Arcidiacono, combscCode, Daniela Fernandes, darioka, Darren Nguyen, Dave Eargle, David Gilbertson, David Poznik, Dea María Léon, Dennis Osei, DessyVV, Dev514, Dimitri Papadopoulos Orfanos, Diwakar Gupta, Dr. Felix M. Riese, drskd, Emiko Sano, Emmanouil Gionanidis, EricEllwanger, Erich Schubert, Eric Larson, Eric Ndirangu, ErmolaevPA, Estefania Barreto-Ojeda, eyast, Fatima GASMI, Federico Luna, Felix Glushchenkov, fkaren27, Fortune Uwha, FPGAwesome, francoisgoupil, Frans Larsson, ftorres16, Gabor Berei, Gabor Kertesz, Gabriel Stefanini Vicente, Gabriel S Vicente, Gael Varoquaux, GAURAV CHOUDHARY, Gauthier I, genvalen, Geoffrey-Paris, Giancarlo Pablo, glennfrutiz, gpapadok, Guillaume Lemaitre, Guillermo Tomás Fernández Martín, Gustavo Oliveira, Haidar Almubarak, Hannah Bohle, Hansin Ahuja, Haoyin Xu, Haya, Helder Geovane Gomes de Lima, henrymooresc, Hideaki Imamura, Himanshu Kumar, Hind-M, hmasdev, hvassard, i-aki-y, iasoon, Inclusive Coding Bot, Ingela, iofall, Ishan Kumar, Jack Liu, Jake Cowton, jalexand3r, J Alexander, Jauhar, Jaya Surya Kommireddy, Jay Stanley, Jeff Hale, je-kr, JElfner, Jenny Vo, Jérémie du Boisberranger, Jihane, Jirka Borovec, Joel Nothman, Jon Haitz Legarreta Gorroño, Jordan Silke, Jorge Ciprián, Jorge Loayza, Joseph Chazalon, Joseph Schwartz-Messing, Jovan Stojanovic, JSchuerz, Juan Carlos Alfaro Jiménez, Juan Martin Loyola, Julien Jerphanion, katotten, Kaushik Roy Chowdhury, Ken4git, Kenneth Prabakaran, kernc, Kevin Doucet, KimAYoung, Koushik Joshi, Kranthi Sedamaki, krishna kumar, krumetoft, lesnee, Lisa Casino, Logan Thomas, Loic Esteve, Louis Wagner, LucieClair, Lucy Liu, Luiz Eduardo Amaral, Magali, MaggieChege, Mai, mandjevant, Mandy Gu, Manimaran, MarcoM, Marco Wurps, Maren Westermann, Maria Boerner, MarieS-WiMLDS, Martel Corentin, martin-kokos, mathurinm, Matías, matjansen, Matteo Francia, Maxwell, Meekail Zain, Megabyte, Mehrdad Moradizadeh, melemo2, Michael I Chen, michalkrawczyk, Micky774, milana2, millawell, Ming-Yang Ho, Mitzi, miwojc, Mizuki, mlant, Mohamed Haseeb, Mohit Sharma, Moonkyung94, mpoemsl, MrinalTyagi, Mr. Leu, msabatier, murata-yu, N, Nadirhan Şahin, Naipawat Poolsawat, NartayXD, nastegiano, nathansquan, nat-salt, Nicki Skafte Detlefsen, Nicolas Hug, Niket Jain, Nikhil Suresh, Nikita Titov, Nikolay Kondratyev, Ohad Michel, Oleksandr Husak, Olivier Grisel, partev, Patrick Ferreira, Paul, pelennor, PierreAttard, Piet Brömmel, Pieter Gijsbers, Pinky, poloso, Pramod Anantharam, puhuk, Purna Chandra Mansingh, QuadV, Rahil Parikh, Randall Boyes, randomgeek78, Raz Hoshia, Reshama Shaikh, Ricardo Ferreira, Richard Taylor, Rileran, Rishabh, Robin Thibaut, Rocco Meli, Roman Feldbauer, Roman Yurchak, Ross Barnowski, rsnegrin, Sachin Yadav, sakinaOuisrani, Sam Adam Day, Sanjay Marreddi, Sebastian Pujalte, SEELE, SELEE, Seyedsaman (Sam) Emami, ShanDeng123, Shao Yang Hong, sharmadharmpal, shaymerNaturalint, Shuangchi He, Shubhraneel Pal, siavrez, slishak, Smile, spikebh, sply88, Srinath Kailasa, Stéphane Collot, Sultan Orazbayev, Sumit Saha, Sven Eschlbeck, Sven Stehle, Swapnil Jha, Sylvain Marié, Takeshi Oura, Tamires Santana, Tenavi, teunpe, Theis Ferré Hjortkjær, Thiruvenkadam, Thomas J. Fan, t-jakubek, toastedyeast, Tom Dupré la Tour, Tom McTiernan, TONY GEORGE, Tyler Martin, Tyler Reddy, Udit Gupta, Ugo Marchand, Varun Agrawal, Venkatachalam N, Vera Komeyer, victoirelouis, Vikas Vishwakarma, Vikrant khedkar, Vladimir Chernyy, Vladimir Kim, WeijiaDu, Xiao Yuan, Yar Khine Phyo, Ying Xiong, yiyangq, Yosshi999, Yuki Koyama, Zach Deane-Mayer, Zeel B Patel, zempleni, zhenfisher, 赵丰 (Zhao Feng)

---

v1.2.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.2

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_2\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_2\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 1.2.2

**March 2023**

### Changelog

#### `sklearn.base`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` When <span class="title-ref">set\_output(transform="pandas")</span>, <span class="title-ref">base.TransformerMixin</span> maintains the index if the `transform` output is already a DataFrame. `25747` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A deprecation warning is raised when using the <span class="title-ref">base\_estimator\_\_</span> prefix to set parameters of the estimator used in <span class="title-ref">calibration.CalibratedClassifierCV</span>. `25477` by `Tim Head <betatim>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">cluster.BisectingKMeans</span>, preventing <span class="title-ref">fit</span> to randomly fail due to a permutation of the labels when running multiple inits. `25563` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">compose.ColumnTransformer</span> which now supports empty selection of columns when <span class="title-ref">set\_output(transform="pandas")</span>. `25570` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` A deprecation warning is raised when using the <span class="title-ref">base\_estimator\_\_</span> prefix to set parameters of the estimator used in <span class="title-ref">ensemble.AdaBoostClassifier</span>, <span class="title-ref">ensemble.AdaBoostRegressor</span>, <span class="title-ref">ensemble.BaggingClassifier</span>, and <span class="title-ref">ensemble.BaggingRegressor</span>. `25477` by `Tim Head <betatim>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression where a negative <span class="title-ref">tol</span> would not be accepted any more by <span class="title-ref">feature\_selection.SequentialFeatureSelector</span>. `25664` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise a more informative error message in <span class="title-ref">inspection.partial\_dependence</span> when dealing with mixed data type categories that cannot be sorted by <span class="title-ref">numpy.unique</span>. This problem usually happen when categories are <span class="title-ref">str</span> and missing values are present using <span class="title-ref">np.nan</span>. `25774` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.isotonic`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">isotonic.IsotonicRegression</span> where <span class="title-ref">isotonic.IsotonicRegression.predict</span> would return a pandas DataFrame when the global configuration sets <span class="title-ref">transform\_output="pandas"</span>. `25500` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OneHotEncoder.drop\_idx\_</span> now properly references the dropped category in the <span class="title-ref">categories\_</span> attribute when there are infrequent categories. `25589` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OrdinalEncoder</span> now correctly supports <span class="title-ref">encoded\_missing\_value</span> or <span class="title-ref">unknown\_value</span> set to a categories' cardinality when there is missing values in the training data. `25704` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeRegressor</span> where an error was no longer raised in version 1.2 when <span class="title-ref">min\_sample\_split=1</span>. `25744` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">utils.check\_array</span> which now correctly performs non-finite validation with the Array API specification. `25619` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.multiclass.type\_of\_target</span> can identify pandas nullable data types as classification targets. `25638` by [Thomas Fan](https://github.com/thomasjpfan).

## Version 1.2.1

**January 2023**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The fitted components in <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> might differ. The online updates of the sufficient statistics now properly take the sizes of the batches into account. `25354` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">categories\_</span> attribute of <span class="title-ref">preprocessing.OneHotEncoder</span> now always contains an array of <span class="title-ref">object\`s when using predefined categories that are strings. Predefined categories encoded as bytes will no longer work with \`X</span> encoded as strings. `25174` by `Tim Head <betatim>`.

### Changes impacting all modules

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Support <span class="title-ref">pandas.Int64</span> dtyped <span class="title-ref">y</span> for classifiers and regressors. `25089` by `Tim Head <betatim>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Remove spurious warnings for estimators internally using neighbors search methods. `25129` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug where the current configuration was ignored in estimators using <span class="title-ref">n\_jobs \> 1</span>. This bug was triggered for tasks dispatched by the auxiliary thread of <span class="title-ref">joblib</span> as <span class="title-ref">sklearn.get\_config</span> used to access an empty thread local configuration instead of the configuration visible from the thread where <span class="title-ref">joblib.Parallel</span> was first called. `25363` by `Guillaume Lemaitre <glemaitre>`.

### Changelog

#### `sklearn.base`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">BaseEstimator.\_\_getstate\_\_</span> that would prevent certain estimators to be pickled when using Python 3.11. `25188` by `Benjamin Bossan <BenjaminBossan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Inheriting from <span class="title-ref">base.TransformerMixin</span> will only wrap the <span class="title-ref">transform</span> method if the class defines <span class="title-ref">transform</span> itself. `25295` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes an inconsistency in <span class="title-ref">datasets.fetch\_openml</span> between liac-arff and pandas parser when a leading space is introduced after the delimiter. The ARFF specs requires to ignore the leading space. `25312` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">datasets.fetch\_openml</span> when using <span class="title-ref">parser="pandas"</span> where single quote and backslash escape characters were not properly handled. `25511` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> where the online updates of the sufficient statistics where not correct when calling <span class="title-ref">partial\_fit</span> on batches of different sizes. `25354` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.DictionaryLearning</span> better supports readonly NumPy arrays. In particular, it better supports large datasets which are memory-mapped when it is used with coordinate descent algorithms (i.e. when <span class="title-ref">fit\_algorithm='cd'</span>). `25172` by `Julien Jerphanion <jjerphan>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span> <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span> now support sparse readonly datasets. `25341` by `Julien Jerphanion <jjerphan>`

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_extraction.FeatureHasher</span> raises an informative error when the input is a list of strings. `25094` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">linear\_model.SGDClassifier</span> and <span class="title-ref">linear\_model.SGDRegressor</span> that makes them unusable with the <span class="title-ref">verbose</span> parameter set to a value greater than 0. `25250` by `Jérémie Du Boisberranger <jeremiedbb>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.TSNE</span> now works correctly when output type is set to pandas `25370` by `Tim Head <betatim>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.cross\_validate</span> with multimetric scoring in case of some failing scorers the non-failing scorers now returns proper scores instead of <span class="title-ref">error\_score</span> values. `23101` by `András Simon <simonandras>` and [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.neural_network`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span> no longer raise warnings when fitting data with feature names. `24873` by `Tim Head <betatim>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improves error message in <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span>, when <span class="title-ref">early\_stopping=True</span> and <span class="title-ref">partial\_fit</span> is called. `25694` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.FunctionTransformer.inverse\_transform</span> correctly supports DataFrames that are all numerical when <span class="title-ref">check\_inverse=True</span>. `25274` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.SplineTransformer.get\_feature\_names\_out</span> correctly returns feature names when <span class="title-ref">extrapolations="periodic"</span>. `25296` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span> <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeRegressor</span> now support sparse readonly datasets. `25341` by `Julien Jerphanion <jjerphan>`

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Restore <span class="title-ref">utils.check\_array</span>'s behaviour for pandas Series of type boolean. The type is maintained, instead of converting to <span class="title-ref">float64.</span> `25147` by `Tim Head <betatim>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.fixes.delayed</span> is deprecated in 1.2.1 and will be removed in 1.5. Instead, import <span class="title-ref">utils.parallel.delayed</span> and use it in conjunction with the newly introduced <span class="title-ref">utils.parallel.Parallel</span> to ensure proper propagation of the scikit-learn configuration to the workers. `25363` by `Guillaume Lemaitre <glemaitre>`.

## Version 1.2.0

**December 2022**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The default <span class="title-ref">eigen\_tol</span> for <span class="title-ref">cluster.SpectralClustering</span>, <span class="title-ref">manifold.SpectralEmbedding</span>, <span class="title-ref">cluster.spectral\_clustering</span>, and <span class="title-ref">manifold.spectral\_embedding</span> is now <span class="title-ref">None</span> when using the <span class="title-ref">'amg'</span> or <span class="title-ref">'lobpcg'</span> solvers. This change improves numerical stability of the solver, but may result in a different model.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.GammaRegressor</span>, <span class="title-ref">linear\_model.PoissonRegressor</span> and <span class="title-ref">linear\_model.TweedieRegressor</span> can reach higher precision with the lbfgs solver, in particular when <span class="title-ref">tol</span> is set to a tiny value. Moreover, <span class="title-ref">verbose</span> is now properly propagated to L-BFGS-B. `23619` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The default value for <span class="title-ref">eps</span> <span class="title-ref">metrics.log\_loss</span> has changed from <span class="title-ref">1e-15</span> to <span class="title-ref">"auto"</span>. <span class="title-ref">"auto"</span> sets <span class="title-ref">eps</span> to <span class="title-ref">np.finfo(y\_pred.dtype).eps</span>. `24354` by `Safiuddin Khaja <Safikh>` and `gsiisg <gsiisg>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Make sign of <span class="title-ref">components\_</span> deterministic in <span class="title-ref">decomposition.SparsePCA</span>. `23935` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">components\_</span> signs in <span class="title-ref">decomposition.FastICA</span> might differ. It is now consistent and deterministic with all SVD solvers. `22527` by `Meekail Zain <micky774>` and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The condition for early stopping has now been changed in <span class="title-ref">linear\_model.\_sgd\_fast.\_plain\_sgd</span> which is used by <span class="title-ref">linear\_model.SGDRegressor</span> and <span class="title-ref">linear\_model.SGDClassifier</span>. The old condition did not disambiguate between training and validation set and had an effect of overscaling the error tolerance. This has been fixed in `23798` by `Harsh Agrawal <Harsh14901>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` For <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> ranks corresponding to nan scores will all be set to the maximum possible rank. `24543` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of <span class="title-ref">tol</span> was changed from <span class="title-ref">1e-3</span> to <span class="title-ref">1e-4</span> for <span class="title-ref">linear\_model.ridge\_regression</span>, <span class="title-ref">linear\_model.Ridge</span> and <span class="title-ref">linear\_model.RidgeClassifier</span>. `24465` by `Christian Lorentzen <lorentzenchr>`.

### Changes impacting all modules

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` The <span class="title-ref">set\_output</span> API has been adopted by all transformers. Meta-estimators that contain transformers such as <span class="title-ref">pipeline.Pipeline</span> or <span class="title-ref">compose.ColumnTransformer</span> also define a <span class="title-ref">set\_output</span>. For details, see [SLEP018](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep018/proposal.html). `23734` and `24699` by [Thomas Fan](https://github.com/thomasjpfan).

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Low-level routines for reductions on pairwise distances for dense float32 datasets have been refactored. The following functions and estimators now benefit from improved performances in terms of hardware scalability and speed-ups:
    
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin</span>
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin\_min</span>
      - <span class="title-ref">sklearn.cluster.AffinityPropagation</span>
      - <span class="title-ref">sklearn.cluster.Birch</span>
      - <span class="title-ref">sklearn.cluster.MeanShift</span>
      - <span class="title-ref">sklearn.cluster.OPTICS</span>
      - <span class="title-ref">sklearn.cluster.SpectralClustering</span>
      - <span class="title-ref">sklearn.feature\_selection.mutual\_info\_regression</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.LocalOutlierFactor</span>
      - <span class="title-ref">sklearn.neighbors.NearestNeighbors</span>
      - <span class="title-ref">sklearn.manifold.Isomap</span>
      - <span class="title-ref">sklearn.manifold.LocallyLinearEmbedding</span>
      - <span class="title-ref">sklearn.manifold.TSNE</span>
      - <span class="title-ref">sklearn.manifold.trustworthiness</span>
      - <span class="title-ref">sklearn.semi\_supervised.LabelPropagation</span>
      - <span class="title-ref">sklearn.semi\_supervised.LabelSpreading</span>
    
    For instance <span class="title-ref">sklearn.neighbors.NearestNeighbors.kneighbors</span> and <span class="title-ref">sklearn.neighbors.NearestNeighbors.radius\_neighbors</span> can respectively be up to ×20 and ×5 faster than previously on a laptop.
    
    Moreover, implementations of those two algorithms are now suitable for machine with many cores, making them usable for datasets consisting of millions of samples.
    
    `23865` by `Julien Jerphanion <jjerphan>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Finiteness checks (detection of NaN and infinite values) in all estimators are now significantly more efficient for float32 data by leveraging NumPy's SIMD optimized primitives. `23446` by `Meekail Zain <micky774>`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Finiteness checks (detection of NaN and infinite values) in all estimators are now faster by utilizing a more efficient stop-on-first second-pass algorithm. `23197` by `Meekail Zain <micky774>`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Support for combinations of dense and sparse datasets pairs for all distance metrics and for float32 and float64 datasets has been added or has seen its performance improved for the following estimators:
    
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin</span>
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin\_min</span>
      - <span class="title-ref">sklearn.cluster.AffinityPropagation</span>
      - <span class="title-ref">sklearn.cluster.Birch</span>
      - <span class="title-ref">sklearn.cluster.SpectralClustering</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.LocalOutlierFactor</span>
      - <span class="title-ref">sklearn.neighbors.NearestNeighbors</span>
      - <span class="title-ref">sklearn.manifold.Isomap</span>
      - <span class="title-ref">sklearn.manifold.TSNE</span>
      - <span class="title-ref">sklearn.manifold.trustworthiness</span>
    
    `23604` and `23585` by `Julien Jerphanion <jjerphan>`, `Olivier Grisel <ogrisel>`, and [Thomas Fan](https://github.com/thomasjpfan), `24556` by `Vincent Maladière <Vincent-Maladiere>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Systematically check the sha256 digest of dataset tarballs used in code examples in the documentation. `24617` by `Olivier Grisel <ogrisel>` and [Thomas Fan](https://github.com/thomasjpfan). Thanks to [Sim4n6](https://huntr.dev/users/sim4n6) for the report.

### Changelog

#### `sklearn.base`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Introduces <span class="title-ref">base.ClassNamePrefixFeaturesOutMixin</span> and <span class="title-ref">base.ClassNamePrefixFeaturesOutMixin</span> mixins that defines `get_feature_names_out` for common transformer uses cases. `24688` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.calibration`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Rename <span class="title-ref">base\_estimator</span> to <span class="title-ref">estimator</span> in <span class="title-ref">calibration.CalibratedClassifierCV</span> to improve readability and consistency. The parameter <span class="title-ref">base\_estimator</span> is deprecated and will be removed in 1.4. `22054` by `Kevin Roice <kevroi>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">cluster.KMeans</span> with <span class="title-ref">algorithm="lloyd"</span> is now faster and uses less memory. `24264` by `Vincent Maladiere <Vincent-Maladiere>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">predict</span> and <span class="title-ref">fit\_predict</span> methods of <span class="title-ref">cluster.OPTICS</span> now accept sparse data type for input data. `14736` by `Hunt Zhan <huntzhan>`, `20802` by `Brandon Pokorny <Clickedbigfoot>`, and `22965` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.Birch</span> now preserves dtype for <span class="title-ref">numpy.float32</span> inputs. `22968` by <span class="title-ref">Meekail Zain \<micky774\></span>.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.KMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span> now accept a new <span class="title-ref">'auto'</span> option for <span class="title-ref">n\_init</span> which changes the number of random initializations to one when using <span class="title-ref">init='k-means++'</span> for efficiency. This begins deprecation for the default values of <span class="title-ref">n\_init</span> in the two classes and both will have their defaults changed to <span class="title-ref">n\_init='auto'</span> in 1.4. `23038` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">cluster.SpectralClustering</span> and <span class="title-ref">cluster.spectral\_clustering</span> now propagates the <span class="title-ref">eigen\_tol</span> parameter to all choices of <span class="title-ref">eigen\_solver</span>. Includes a new option <span class="title-ref">eigen\_tol="auto"</span> and begins deprecation to change the default from <span class="title-ref">eigen\_tol=0</span> to <span class="title-ref">eigen\_tol="auto"</span> in version 1.3. `23210` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.KMeans</span> now supports readonly attributes when predicting. `24258` by [Thomas Fan](https://github.com/thomasjpfan)
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">affinity</span> attribute is now deprecated for <span class="title-ref">cluster.AgglomerativeClustering</span> and will be renamed to <span class="title-ref">metric</span> in v1.4. `23470` by `Meekail Zain <micky774>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Introduce the new parameter <span class="title-ref">parser</span> in <span class="title-ref">datasets.fetch\_openml</span>. <span class="title-ref">parser="pandas"</span> allows to use the very CPU and memory efficient <span class="title-ref">pandas.read\_csv</span> parser to load dense ARFF formatted dataset files. It is possible to pass <span class="title-ref">parser="liac-arff"</span> to use the old LIAC parser. When <span class="title-ref">parser="auto"</span>, dense datasets are loaded with "pandas" and sparse datasets are loaded with "liac-arff". Currently, <span class="title-ref">parser="liac-arff"</span> by default and will change to <span class="title-ref">parser="auto"</span> in version 1.4 `21938` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.dump\_svmlight\_file</span> is now accelerated with a Cython implementation, providing 2-4x speedups. `23127` by `Meekail Zain <micky774>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Path-like objects, such as those created with pathlib are now allowed as paths in <span class="title-ref">datasets.load\_svmlight\_file</span> and <span class="title-ref">datasets.load\_svmlight\_files</span>. `19075` by `Carlos Ramos Carreño <vnmabus>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Make sure that <span class="title-ref">datasets.fetch\_lfw\_people</span> and <span class="title-ref">datasets.fetch\_lfw\_pairs</span> internally crops images based on the <span class="title-ref">slice\_</span> parameter. `24951` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">decomposition.FastICA.fit</span> has been optimised w.r.t its memory footprint and runtime. `22268` by `MohamedBsh <Bsh>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.SparsePCA</span> and <span class="title-ref">decomposition.MiniBatchSparsePCA</span> now implements an <span class="title-ref">inverse\_transform</span> function. `23905` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.FastICA</span> now allows the user to select how whitening is performed through the new <span class="title-ref">whiten\_solver</span> parameter, which supports <span class="title-ref">svd</span> and <span class="title-ref">eigh</span>. <span class="title-ref">whiten\_solver</span> defaults to <span class="title-ref">svd</span> although <span class="title-ref">eigh</span> may be faster and more memory efficient in cases where <span class="title-ref">num\_features \> num\_samples</span>. `11860` by `Pierre Ablin <pierreablin>`, `22527` by `Meekail Zain <micky774>` and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.LatentDirichletAllocation</span> now preserves dtype for <span class="title-ref">numpy.float32</span> input. `24528` by `Takeshi Oura <takoika>` and `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Make sign of <span class="title-ref">components\_</span> deterministic in <span class="title-ref">decomposition.SparsePCA</span>. `23935` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">n\_iter</span> parameter of <span class="title-ref">decomposition.MiniBatchSparsePCA</span> is deprecated and replaced by the parameters <span class="title-ref">max\_iter</span>, <span class="title-ref">tol</span>, and <span class="title-ref">max\_no\_improvement</span> to be consistent with <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>. <span class="title-ref">n\_iter</span> will be removed in version 1.3. `23726` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">n\_features\_</span> attribute of <span class="title-ref">decomposition.PCA</span> is deprecated in favor of <span class="title-ref">n\_features\_in\_</span> and will be removed in 1.4. `24421` by `Kshitij Mathur <Kshitij68>`.

#### `sklearn.discriminant_analysis`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now supports the [Array API](https://data-apis.org/array-api/latest/) for <span class="title-ref">solver="svd"</span>. Array API support is considered experimental and might evolve without being subjected to our usual rolling deprecation cycle policy. See \[array\_api\](\#array\_api) for more details. `22554` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Validate parameters only in <span class="title-ref">fit</span> and not in <span class="title-ref">\_\_init\_\_</span> for <span class="title-ref">discriminant\_analysis.QuadraticDiscriminantAnalysis</span>. `24218` by `Stefanie Molin <stefmolin>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> now support interaction constraints via the argument <span class="title-ref">interaction\_cst</span> of their constructors. `21020` by `Christian Lorentzen <lorentzenchr>`. Using interaction constraints also makes fitting faster. `24856` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Adds <span class="title-ref">class\_weight</span> to <span class="title-ref">ensemble.HistGradientBoostingClassifier</span>. `22014` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improve runtime performance of <span class="title-ref">ensemble.IsolationForest</span> by avoiding data copies. `23252` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.StackingClassifier</span> now accepts any kind of base estimator. `24538` by `Guillem G Subies <GuillemGSubies>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Make it possible to pass the <span class="title-ref">categorical\_features</span> parameter of <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> as feature names. `24889` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.StackingClassifier</span> now supports multilabel-indicator target `24146` by `Nicolas Peretti <nicoperetti>`, `Nestor Navarro <nestornav>`, `Nati Tomattis <natitomattis>`, and `Vincent Maladiere <Vincent-Maladiere>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> now accept their <span class="title-ref">monotonic\_cst</span> parameter to be passed as a dictionary in addition to the previously supported array-like format. Such dictionary have feature names as keys and one of <span class="title-ref">-1</span>, <span class="title-ref">0</span>, <span class="title-ref">1</span> as value to specify monotonicity constraints for each feature. `24855` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Interaction constraints for <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> can now be specified as strings for two common cases: "no\_interactions" and "pairwise" interactions. `24849` by `Tim Head <betatim>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed the issue where <span class="title-ref">ensemble.AdaBoostClassifier</span> outputs NaN in feature importance when fitted with very small sample weight. `20415` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> no longer error when predicting on categories encoded as negative values and instead consider them a member of the "missing category". `24283` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span>, with <span class="title-ref">verbose\>=1</span>, print detailed timing information on computing histograms and finding best splits. The time spent in the root node was previously missing and is now included in the printed information. `24894` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Rename the constructor parameter <span class="title-ref">base\_estimator</span> to <span class="title-ref">estimator</span> in the following classes: <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span>, <span class="title-ref">ensemble.AdaBoostClassifier</span>, <span class="title-ref">ensemble.AdaBoostRegressor</span>. <span class="title-ref">base\_estimator</span> is deprecated in 1.2 and will be removed in 1.4. `23819` by `Adrian Trujillo <trujillo9616>` and `Edoardo Abati <EdAbati>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Rename the fitted attribute <span class="title-ref">base\_estimator\_</span> to <span class="title-ref">estimator\_</span> in the following classes: <span class="title-ref">ensemble.BaggingClassifier</span>, <span class="title-ref">ensemble.BaggingRegressor</span>, <span class="title-ref">ensemble.AdaBoostClassifier</span>, <span class="title-ref">ensemble.AdaBoostRegressor</span>, <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span>, <span class="title-ref">ensemble.ExtraTreesRegressor</span>, <span class="title-ref">ensemble.RandomTreesEmbedding</span>, <span class="title-ref">ensemble.IsolationForest</span>. <span class="title-ref">base\_estimator\_</span> is deprecated in 1.2 and will be removed in 1.4. `23819` by `Adrian Trujillo <trujillo9616>` and `Edoardo Abati <EdAbati>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">feature\_selection.mutual\_info\_regression</span> and <span class="title-ref">feature\_selection.mutual\_info\_classif</span>, where the continuous features in <span class="title-ref">X</span> should be scaled to a unit variance independently if the target <span class="title-ref">y</span> is continuous or discrete. `24747` by `Guillaume Lemaitre <glemaitre>`

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix <span class="title-ref">gaussian\_process.kernels.Matern</span> gradient computation with <span class="title-ref">nu=0.5</span> for PyPy (and possibly other non CPython interpreters). `24245` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">fit</span> method of <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> will not modify the input X in case a custom kernel is used, with a <span class="title-ref">diag</span> method that returns part of the input X. `24405` by `Omar Salman <OmarManzoor>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added <span class="title-ref">keep\_empty\_features</span> parameter to <span class="title-ref">impute.SimpleImputer</span>, <span class="title-ref">impute.KNNImputer</span> and <span class="title-ref">impute.IterativeImputer</span>, preventing removal of features containing only missing values when transforming. `16695` by `Vitor Santa Rosa <vitorsrg>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Extended <span class="title-ref">inspection.partial\_dependence</span> and <span class="title-ref">inspection.PartialDependenceDisplay</span> to handle categorical features. `18298` by `Madhura Jayaratne <madhuracj>` and `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">inspection.DecisionBoundaryDisplay</span> now raises error if input data is not 2-dimensional. `25077` by `Arturo Amor <ArturoAmorQ>`.

#### `sklearn.kernel_approximation`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">kernel\_approximation.RBFSampler</span> now preserves dtype for <span class="title-ref">numpy.float32</span> inputs. `24317` by <span class="title-ref">Tim Head \<betatim\></span>.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">kernel\_approximation.SkewedChi2Sampler</span> now preserves dtype for <span class="title-ref">numpy.float32</span> inputs. `24350` by `Rahil Parikh <rprkh>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">kernel\_approximation.RBFSampler</span> now accepts <span class="title-ref">'scale'</span> option for parameter <span class="title-ref">gamma</span>. `24755` by `Gleb Levitski <GLevV>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.LogisticRegression</span>, <span class="title-ref">linear\_model.LogisticRegressionCV</span>, <span class="title-ref">linear\_model.GammaRegressor</span>, <span class="title-ref">linear\_model.PoissonRegressor</span> and <span class="title-ref">linear\_model.TweedieRegressor</span> got a new solver <span class="title-ref">solver="newton-cholesky"</span>. This is a 2nd order (Newton) optimisation routine that uses a Cholesky decomposition of the hessian matrix. When <span class="title-ref">n\_samples \>\> n\_features</span>, the <span class="title-ref">"newton-cholesky"</span> solver has been observed to converge both faster and to a higher precision solution than the <span class="title-ref">"lbfgs"</span> solver on problems with one-hot encoded categorical variables with some rare categorical levels. `24637` and `24767` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.GammaRegressor</span>, <span class="title-ref">linear\_model.PoissonRegressor</span> and <span class="title-ref">linear\_model.TweedieRegressor</span> can reach higher precision with the lbfgs solver, in particular when <span class="title-ref">tol</span> is set to a tiny value. Moreover, <span class="title-ref">verbose</span> is now properly propagated to L-BFGS-B. `23619` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.SGDClassifier</span> and <span class="title-ref">linear\_model.SGDRegressor</span> will raise an error when all the validation samples have zero sample weight. `23275` by <span class="title-ref">Zhehao Liu \<MaxwellLZH\></span>.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.SGDOneClassSVM</span> no longer performs parameter validation in the constructor. All validation is now handled in <span class="title-ref">fit()</span> and <span class="title-ref">partial\_fit()</span>. `24433` by `Yogendrasingh <iofall>`, `Arisa Y. <arisayosh>` and `Tim Head <betatim>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix average loss calculation when early stopping is enabled in <span class="title-ref">linear\_model.SGDRegressor</span> and <span class="title-ref">linear\_model.SGDClassifier</span>. Also updated the condition for early stopping accordingly. `23798` by `Harsh Agrawal <Harsh14901>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value for the <span class="title-ref">solver</span> parameter in <span class="title-ref">linear\_model.QuantileRegressor</span> will change from <span class="title-ref">"interior-point"</span> to <span class="title-ref">"highs"</span> in version 1.4. `23637` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` String option <span class="title-ref">"none"</span> is deprecated for <span class="title-ref">penalty</span> argument in <span class="title-ref">linear\_model.LogisticRegression</span>, and will be removed in version 1.4. Use <span class="title-ref">None</span> instead. `23877` by `Zhehao Liu <MaxwellLZH>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of <span class="title-ref">tol</span> was changed from <span class="title-ref">1e-3</span> to <span class="title-ref">1e-4</span> for <span class="title-ref">linear\_model.ridge\_regression</span>, <span class="title-ref">linear\_model.Ridge</span> and <span class="title-ref">linear\_model.RidgeClassifier</span>. `24465` by `Christian Lorentzen <lorentzenchr>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Adds option to use the normalized stress in <span class="title-ref">manifold.MDS</span>. This is enabled by setting the new <span class="title-ref">normalize</span> parameter to <span class="title-ref">True</span>. `10168` by `Łukasz Borchmann <Borchmann>`, `12285` by `Matthias Miltenberger <mattmilten>`, `13042` by `Matthieu Parizy <matthieu-pa>`, `18094` by `Roth E Conrad <rotheconrad>` and `22562` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds <span class="title-ref">eigen\_tol</span> parameter to <span class="title-ref">manifold.SpectralEmbedding</span>. Both <span class="title-ref">manifold.spectral\_embedding</span> and <span class="title-ref">manifold.SpectralEmbedding</span> now propagate <span class="title-ref">eigen\_tol</span> to all choices of <span class="title-ref">eigen\_solver</span>. Includes a new option <span class="title-ref">eigen\_tol="auto"</span> and begins deprecation to change the default from <span class="title-ref">eigen\_tol=0</span> to <span class="title-ref">eigen\_tol="auto"</span> in version 1.3. `23210` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">manifold.Isomap</span> now preserves dtype for <span class="title-ref">np.float32</span> inputs. `24714` by `Rahil Parikh <rprkh>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Added an <span class="title-ref">"auto"</span> option to the <span class="title-ref">normalized\_stress</span> argument in <span class="title-ref">manifold.MDS</span> and <span class="title-ref">manifold.smacof</span>. Note that <span class="title-ref">normalized\_stress</span> is only valid for non-metric MDS, therefore the <span class="title-ref">"auto"</span> option enables <span class="title-ref">normalized\_stress</span> when <span class="title-ref">metric=False</span> and disables it when <span class="title-ref">metric=True</span>. <span class="title-ref">"auto"</span> will become the default value for <span class="title-ref">normalized\_stress</span> in version 1.4. `23834` by `Meekail Zain <micky774>`

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.ConfusionMatrixDisplay.from\_estimator</span>, <span class="title-ref">metrics.ConfusionMatrixDisplay.from\_predictions</span>, and <span class="title-ref">metrics.ConfusionMatrixDisplay.plot</span> accepts a <span class="title-ref">text\_kw</span> parameter which is passed to matplotlib's <span class="title-ref">text</span> function. `24051` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.class\_likelihood\_ratios</span> is added to compute the positive and negative likelihood ratios derived from the confusion matrix of a binary classification problem. `22518` by `Arturo Amor <ArturoAmorQ>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add <span class="title-ref">metrics.PredictionErrorDisplay</span> to plot residuals vs predicted and actual vs predicted to qualitatively assess the behavior of a regressor. The display can be created with the class methods <span class="title-ref">metrics.PredictionErrorDisplay.from\_estimator</span> and <span class="title-ref">metrics.PredictionErrorDisplay.from\_predictions</span>. `18020` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.roc\_auc\_score</span> now supports micro-averaging (<span class="title-ref">average="micro"</span>) for the One-vs-Rest multiclass case (<span class="title-ref">multi\_class="ovr"</span>). `24338` by `Arturo Amor <ArturoAmorQ>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds an <span class="title-ref">"auto"</span> option to <span class="title-ref">eps</span> in <span class="title-ref">metrics.log\_loss</span>. This option will automatically set the <span class="title-ref">eps</span> value depending on the data type of <span class="title-ref">y\_pred</span>. In addition, the default value of <span class="title-ref">eps</span> is changed from <span class="title-ref">1e-15</span> to the new <span class="title-ref">"auto"</span> option. `24354` by `Safiuddin Khaja <Safikh>` and `gsiisg <gsiisg>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Allows <span class="title-ref">csr\_matrix</span> as input for parameter: <span class="title-ref">y\_true</span> of the <span class="title-ref">metrics.label\_ranking\_average\_precision\_score</span> metric. `23442` by `Sean Atukorala <ShehanAT>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.ndcg\_score</span> will now trigger a warning when the <span class="title-ref">y\_true</span> value contains a negative value. Users may still use negative values, but the result may not be between 0 and 1. Starting in v1.4, passing in negative values for <span class="title-ref">y\_true</span> will raise an error. `22710` by `Conroy Trinh <trinhcon>` and `23461` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.log\_loss</span> with <span class="title-ref">eps=0</span> now returns a correct value of 0 or <span class="title-ref">np.inf</span> instead of <span class="title-ref">nan</span> for predictions at the boundaries (0 or 1). It also accepts integer input. `24365` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter <span class="title-ref">sum\_over\_features</span> of <span class="title-ref">metrics.pairwise.manhattan\_distances</span> is deprecated and will be removed in 1.4. `24630` by `Rushil Desai <rusdes>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added the class <span class="title-ref">model\_selection.LearningCurveDisplay</span> that allows to make easy plotting of learning curves obtained by the function <span class="title-ref">model\_selection.learning\_curve</span>. `24084` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` For all <span class="title-ref">SearchCV</span> classes and scipy \>= 1.10, rank corresponding to a nan score is correctly set to the maximum possible rank, rather than <span class="title-ref">np.iinfo(np.int32).min</span>. `24141` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` In both <span class="title-ref">model\_selection.HalvingGridSearchCV</span> and <span class="title-ref">model\_selection.HalvingRandomSearchCV</span> parameter combinations with a NaN score now share the lowest rank. `24539` by `Tim Head <betatim>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` For <span class="title-ref">model\_selection.GridSearchCV</span> and <span class="title-ref">model\_selection.RandomizedSearchCV</span> ranks corresponding to nan scores will all be set to the maximum possible rank. `24543` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added boolean <span class="title-ref">verbose</span> flag to classes: <span class="title-ref">multioutput.ClassifierChain</span> and <span class="title-ref">multioutput.RegressorChain</span>. `23977` by `Eric Fiegel <efiegel>`, `Chiara Marmo <cmarmo>`, `Lucy Liu <lucyleeow>`, and `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Add methods <span class="title-ref">predict\_joint\_log\_proba</span> to all naive Bayes classifiers. `23683` by `Andrey Melnik <avm19>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A new parameter <span class="title-ref">force\_alpha</span> was added to <span class="title-ref">naive\_bayes.BernoulliNB</span>, <span class="title-ref">naive\_bayes.ComplementNB</span>, <span class="title-ref">naive\_bayes.CategoricalNB</span>, and <span class="title-ref">naive\_bayes.MultinomialNB</span>, allowing user to set parameter alpha to a very small number, greater or equal 0, which was earlier automatically changed to <span class="title-ref">1e-10</span> instead. `16747` by `arka204`, `18805` by `hongshaoyang`, `22269` by `Meekail Zain <micky774>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Adds new function <span class="title-ref">neighbors.sort\_graph\_by\_row\_values</span> to sort a CSR sparse graph such that each row is stored with increasing values. This is useful to improve efficiency when using precomputed sparse distance matrices in a variety of estimators and avoid an <span class="title-ref">EfficiencyWarning</span>. `23139` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">neighbors.NearestCentroid</span> is faster and requires less memory as it better leverages CPUs' caches to compute predictions. `24645` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">neighbors.KernelDensity</span> bandwidth parameter now accepts definition using Scott's and Silverman's estimation methods. `10468` by `Ruben <icfly2>` and `22993` by `Jovan Stojanovic <jovan-stojanovic>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">neighbors.NeighborsBase</span> now accepts Minkowski semi-metric (i.e. when \(0 < p < 1\) for <span class="title-ref">metric="minkowski"</span>) for <span class="title-ref">algorithm="auto"</span> or <span class="title-ref">algorithm="brute"</span>. `24750` by `Rudresh Veerkhare <RudreshVeerkhare>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.NearestCentroid</span> now raises an informative error message at fit-time instead of failing with a low-level error message at predict-time. `23874` by `Juan Gomez <2357juan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Set <span class="title-ref">n\_jobs=None</span> by default (instead of <span class="title-ref">1</span>) for <span class="title-ref">neighbors.KNeighborsTransformer</span> and <span class="title-ref">neighbors.RadiusNeighborsTransformer</span>. `24075` by `Valentin Laurent <Valentin-Laurent>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">neighbors.LocalOutlierFactor</span> now preserves dtype for <span class="title-ref">numpy.float32</span> inputs. `22665` by `Julien Jerphanion <jjerphan>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neural\_network.MLPClassifier</span> and <span class="title-ref">neural\_network.MLPRegressor</span> always expose the parameters <span class="title-ref">best\_loss\_</span>, <span class="title-ref">validation\_scores\_</span>, and <span class="title-ref">best\_validation\_score\_</span>. <span class="title-ref">best\_loss\_</span> is set to <span class="title-ref">None</span> when <span class="title-ref">early\_stopping=True</span>, while <span class="title-ref">validation\_scores\_</span> and <span class="title-ref">best\_validation\_score\_</span> are set to <span class="title-ref">None</span> when <span class="title-ref">early\_stopping=False</span>. `24683` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">pipeline.FeatureUnion.get\_feature\_names\_out</span> can now be used when one of the transformers in the <span class="title-ref">pipeline.FeatureUnion</span> is <span class="title-ref">"passthrough"</span>. `24058` by `Diederik Perdok <diederikwp>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">pipeline.FeatureUnion</span> class now has a <span class="title-ref">named\_transformers</span> attribute for accessing transformers by name. `20331` by `Christopher Flynn <crflynn>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">preprocessing.FunctionTransformer</span> will always try to set <span class="title-ref">n\_features\_in\_</span> and <span class="title-ref">feature\_names\_in\_</span> regardless of the <span class="title-ref">validate</span> parameter. `23993` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.LabelEncoder</span> correctly encodes NaNs in <span class="title-ref">transform</span>. `22629` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">sparse</span> parameter of <span class="title-ref">preprocessing.OneHotEncoder</span> is now deprecated and will be removed in version 1.4. Use <span class="title-ref">sparse\_output</span> instead. `24412` by `Rushil Desai <rusdes>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">class\_weight\_</span> attribute is now deprecated for <span class="title-ref">svm.NuSVR</span>, <span class="title-ref">svm.SVR</span>, <span class="title-ref">svm.OneClassSVM</span>. `22898` by `Meekail Zain <micky774>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">tree.plot\_tree</span>, <span class="title-ref">tree.export\_graphviz</span> now uses a lower case <span class="title-ref">x\[i\]</span> to represent feature <span class="title-ref">i</span>. `23480` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.utils`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A new module exposes development tools to discover estimators (i.e. <span class="title-ref">utils.discovery.all\_estimators</span>), displays (i.e. <span class="title-ref">utils.discovery.all\_displays</span>) and functions (i.e. <span class="title-ref">utils.discovery.all\_functions</span>) in scikit-learn. `21469` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.extmath.randomized\_svd</span> now accepts an argument, <span class="title-ref">lapack\_svd\_driver</span>, to specify the lapack driver used in the internal deterministic SVD used by the randomized SVD algorithm. `20617` by `Srinath Kailasa <skailasa>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.validation.column\_or\_1d</span> now accepts a <span class="title-ref">dtype</span> parameter to specific <span class="title-ref">y</span>'s dtype. `22629` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">utils.extmath.cartesian</span> now accepts arrays with different <span class="title-ref">dtype</span> and will cast the output to the most permissive <span class="title-ref">dtype</span>. `25067` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.multiclass.type\_of\_target</span> now properly handles sparse matrices. `14862` by `Léonard Binet <leonardbinet>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` HTML representation no longer errors when an estimator class is a value in <span class="title-ref">get\_params</span>. `24512` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.estimator\_checks.check\_estimator</span> now takes into account the <span class="title-ref">requires\_positive\_X</span> tag correctly. `24667` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.check\_array</span> now supports Pandas Series with <span class="title-ref">pd.NA</span> by raising a better error message or returning a compatible <span class="title-ref">ndarray</span>. `25080` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The extra keyword parameters of <span class="title-ref">utils.extmath.density</span> are deprecated and will be removed in 1.4. `24523` by `Mia Bajic <clytaemnestra>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.1, including:

2357juan, 3lLobo, Adam J. Stewart, Adam Kania, Adam Li, Aditya Anulekh, Admir Demiraj, adoublet, Adrin Jalali, Ahmedbgh, Aiko, Akshita Prasanth, Ala-Na, Alessandro Miola, Alex, Alexandr, Alexandre Perez-Lebel, Alex Buzenet, Ali H. El-Kassas, aman kumar, Amit Bera, András Simon, Andreas Grivas, Andreas Mueller, Andrew Wang, angela-maennel, Aniket Shirsat, Anthony22-dev, Antony Lee, anupam, Apostolos Tsetoglou, Aravindh R, Artur Hermano, Arturo Amor, as-90, ashah002, Ashwin Mathur, avm19, Azaria Gebremichael, b0rxington, Badr MOUFAD, Bardiya Ak, Bartłomiej Gońda, BdeGraaff, Benjamin Bossan, Benjamin Carter, berkecanrizai, Bernd Fritzke, Bhoomika, Biswaroop Mitra, Brandon TH Chen, Brett Cannon, Bsh, cache-missing, carlo, Carlos Ramos Carreño, ceh, chalulu, Changyao Chen, Charles Zablit, Chiara Marmo, Christian Lorentzen, Christian Ritter, Christian Veenhuis, christianwaldmann, Christine P. Chai, Claudio Salvatore Arcidiacono, Clément Verrier, crispinlogan, Da-Lan, DanGonite57, Daniela Fernandes, DanielGaerber, darioka, Darren Nguyen, davidblnc, david-cortes, David Gilbertson, David Poznik, Dayne, Dea María Léon, Denis, Dev Khant, Dhanshree Arora, Diadochokinetic, diederikwp, Dimitri Papadopoulos Orfanos, Dimitris Litsidis, drewhogg, Duarte OC, Dwight Lindquist, Eden Brekke, Edern, Edoardo Abati, Eleanore Denies, EliaSchiavon, Emir, ErmolaevPA, Fabrizio Damicelli, fcharras, Felipe Siola, Flynn, francesco-tuveri, Franck Charras, ftorres16, Gael Varoquaux, Geevarghese George, genvalen, GeorgiaMayDay, Gianr Lazz, Gleb Levitski, Glòria Macià Muñoz, Guillaume Lemaitre, Guillem García Subies, Guitared, gunesbayir, Haesun Park, Hansin Ahuja, Hao Chun Chang, Harsh Agrawal, harshit5674, hasan-yaman, henrymooresc, Henry Sorsky, Hristo Vrigazov, htsedebenham, humahn, i-aki-y, Ian Thompson, Ido M, Iglesys, Iliya Zhechev, Irene, ivanllt, Ivan Sedykh, Jack McIvor, jakirkham, JanFidor, Jason G, Jérémie du Boisberranger, Jiten Sidhpura, jkarolczak, João David, JohnathanPi, John Koumentis, John P, John Pangas, johnthagen, Jordan Fleming, Joshua Choo Yun Keat, Jovan Stojanovic, Juan Carlos Alfaro Jiménez, juanfe88, Juan Felipe Arias, JuliaSchoepp, Julien Jerphanion, jygerardy, ka00ri, Kanishk Sachdev, Kanissh, Kaushik Amar Das, Kendall, Kenneth Prabakaran, Kento Nozawa, kernc, Kevin Roice, Kian Eliasi, Kilian Kluge, Kilian Lieret, Kirandevraj, Kraig, krishna kumar, krishna vamsi, Kshitij Kapadni, Kshitij Mathur, Lauren Burke, Léonard Binet, lingyi1110, Lisa Casino, Logan Thomas, Loic Esteve, Luciano Mantovani, Lucy Liu, Maascha, Madhura Jayaratne, madinak, Maksym, Malte S. Kurz, Mansi Agrawal, Marco Edward Gorelli, Marco Wurps, Maren Westermann, Maria Telenczuk, Mario Kostelac, martin-kokos, Marvin Krawutschke, Masanori Kanazu, mathurinm, Matt Haberland, mauroantonioserrano, Max Halford, Maxi Marufo, maximeSaur, Maxim Smolskiy, Maxwell, m. bou, Meekail Zain, Mehgarg, mehmetcanakbay, Mia Bajić, Michael Flaks, Michael Hornstein, Michel de Ruiter, Michelle Paradis, Mikhail Iljin, Misa Ogura, Moritz Wilksch, mrastgoo, Naipawat Poolsawat, Naoise Holohan, Nass, Nathan Jacobi, Nawazish Alam, Nguyễn Văn Diễn, Nicola Fanelli, Nihal Thukarama Rao, Nikita Jare, nima10khodaveisi, Nima Sarajpoor, nitinramvelraj, NNLNR, npache, Nwanna-Joseph, Nymark Kho, o-holman, Olivier Grisel, Olle Lukowski, Omar Hassoun, Omar Salman, osman tamer, ouss1508, Oyindamola Olatunji, PAB, Pandata, partev, Paulo Sergio Soares, Petar Mlinarić, Peter Jansson, Peter Steinbach, Philipp Jung, Piet Brömmel, Pooja M, Pooja Subramaniam, priyam kakati, puhuk, Rachel Freeland, Rachit Keerti Das, Rafal Wojdyla, Raghuveer Bhat, Rahil Parikh, Ralf Gommers, ram vikram singh, Ravi Makhija, Rehan Guha, Reshama Shaikh, Richard Klima, Rob Crockett, Robert Hommes, Robert Juergens, Robin Lenz, Rocco Meli, Roman4oo, Ross Barnowski, Rowan Mankoo, Rudresh Veerkhare, Rushil Desai, Sabri Monaf Sabri, Safikh, Safiuddin Khaja, Salahuddin, Sam Adam Day, Sandra Yojana Meneses, Sandro Ephrem, Sangam, SangamSwadik, SANJAI\_3, SarahRemus, Sashka Warner, SavkoMax, Scott Gigante, Scott Gustafson, Sean Atukorala, sec65, SELEE, seljaks, Shady el Gewily, Shane, shellyfung, Shinsuke Mori, Shiva chauhan, Shoaib Khan, Shogo Hida, Shrankhla Srivastava, Shuangchi He, Simon, sonnivs, Sortofamudkip, Srinath Kailasa, Stanislav (Stanley) Modrak, Stefanie Molin, stellalin7, Stéphane Collot, Steven Van Vaerenbergh, Steve Schmerler, Sven Stehle, Tabea Kossen, TheDevPanda, the-syd-sre, Thijs van Weezel, Thomas Bonald, Thomas Germer, Thomas J. Fan, Ti-Ion, Tim Head, Timofei Kornev, toastedyeast, Tobias Pitters, Tom Dupré la Tour, tomiock, Tom Mathews, Tom McTiernan, tspeng, Tyler Egashira, Valentin Laurent, Varun Jain, Vera Komeyer, Vicente Reyes-Puerta, Vinayak Mehta, Vincent M, Vishal, Vyom Pathak, wattai, wchathura, WEN Hao, William M, x110, Xiao Yuan, Xunius, yanhong-zhao-ef, Yusuf Raji, Z Adil Khwaja, zeeshan lone

---

v1.3.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.3

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_3\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_3\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 1.3.2

**October 2023**

### Changelog

#### `sklearn.datasets`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` All dataset fetchers now accept <span class="title-ref">data\_home</span> as any object that implements the <span class="title-ref">os.PathLike</span> interface, for instance, <span class="title-ref">pathlib.Path</span>. `27468` by `Yao Xiao <Charlie-XIAO>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">decomposition.KernelPCA</span> by forcing the output of the internal <span class="title-ref">preprocessing.KernelCenterer</span> to be a default array. When the arpack solver is used, it expects an array with a <span class="title-ref">dtype</span> attribute. `27583` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug for metrics using <span class="title-ref">zero\_division=np.nan</span> (e.g. <span class="title-ref">\~metrics.precision\_score</span>) within a paralell loop (e.g. <span class="title-ref">\~model\_selection.cross\_val\_score</span>) where the singleton for <span class="title-ref">np.nan</span> will be different in the sub-processes. `27573` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Do not leak data via non-initialized memory in decision tree pickle files and make the generation of those files deterministic. `27580` by `Loïc Estève <lesteve>`.

## Version 1.3.1

**September 2023**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Ridge models with <span class="title-ref">solver='sparse\_cg'</span> may have slightly different results with scipy\>=1.12, because of an underlying change in the scipy solver (see [scipy\#18488](https://github.com/scipy/scipy/pull/18488) for more details) `26814` by `Loïc Estève <lesteve>`

### Changes impacting all modules

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">set\_output</span> API correctly works with list input. `27044` by [Thomas Fan](https://github.com/thomasjpfan).

### Changelog

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">calibration.CalibratedClassifierCV</span> can now handle models that produce large prediction scores. Before it was numerically unstable. `26913` by `Omar Salman <OmarManzoor>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.BisectingKMeans</span> could crash when predicting on data with a different scale than the data used to fit the model. `27167` by [Olivier Grisel](https://twitter.com/ogrisel).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.BisectingKMeans</span> now works with data that has a single feature. `27243` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.cross_decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cross\_decomposition.PLSRegression</span> now automatically ravels the output of <span class="title-ref">predict</span> if fitted with one dimensional <span class="title-ref">y</span>. `26602` by `Yao Xiao <Charlie-XIAO>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a bug in <span class="title-ref">ensemble.AdaBoostClassifier</span> with <span class="title-ref">algorithm="SAMME"</span> where the decision function of each weak learner should be symmetric (i.e. the sum of the scores should sum to zero for a sample). `26521` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_selection.mutual\_info\_regression</span> now correctly computes the result when <span class="title-ref">X</span> is of integer dtype. `26748` by `Yao Xiao <Charlie-XIAO>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">impute.KNNImputer</span> now correctly adds a missing indicator column in `transform` when `add_indicator` is set to `True` and missing values are observed during `fit`. `26600` by `Shreesha Kumar Bhat <Shreesha3112>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Scorers used with <span class="title-ref">metrics.get\_scorer</span> handle properly multilabel-indicator matrix. `27002` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.mixture`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The initialization of <span class="title-ref">mixture.GaussianMixture</span> from user-provided <span class="title-ref">precisions\_init</span> for <span class="title-ref">covariance\_type</span> of <span class="title-ref">full</span> or <span class="title-ref">tied</span> was not correct, and has been fixed. `26416` by `Yang Tao <mchikyt3>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KNeighborsClassifier.predict</span> no longer raises an exception for <span class="title-ref">pandas.DataFrames</span> input. `26772` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Reintroduce <span class="title-ref">sklearn.neighbors.BallTree.valid\_metrics</span> and <span class="title-ref">sklearn.neighbors.KDTree.valid\_metrics</span> as public class attributes. `26754` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">sklearn.model\_selection.HalvingRandomSearchCV</span> no longer raises when the input to the <span class="title-ref">param\_distributions</span> parameter is a list of dicts. `26893` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Neighbors based estimators now correctly work when <span class="title-ref">metric="minkowski"</span> and the metric parameter <span class="title-ref">p</span> is in the range <span class="title-ref">0 \< p \< 1</span>, regardless of the <span class="title-ref">dtype</span> of <span class="title-ref">X</span>. `26760` by `Shreesha Kumar Bhat <Shreesha3112>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.LabelEncoder</span> correctly accepts <span class="title-ref">y</span> as a keyword argument. `26940` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OneHotEncoder</span> shows a more informative error message when <span class="title-ref">sparse\_output=True</span> and the output is configured to be pandas. `26931` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">tree.plot\_tree</span> now accepts <span class="title-ref">class\_names=True</span> as documented. `26903` by `Thomas Roehr <2maz>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">feature\_names</span> parameter of <span class="title-ref">tree.plot\_tree</span> now accepts any kind of array-like instead of just a list. `27292` by `Rahil Parikh <rprkh>`.

## Version 1.3.0

**June 2023**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">multiclass.OutputCodeClassifier.predict</span> now uses a more efficient pairwise distance reduction. As a consequence, the tie-breaking strategy is different and thus the predicted labels may be different. `25196` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">fit\_transform</span> method of <span class="title-ref">decomposition.DictionaryLearning</span> is more efficient but may produce different results as in previous versions when <span class="title-ref">transform\_algorithm</span> is not the same as <span class="title-ref">fit\_algorithm</span> and the number of iterations is small. `24871` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">sample\_weight</span> parameter now will be used in centroids initialization for <span class="title-ref">cluster.KMeans</span>, <span class="title-ref">cluster.BisectingKMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span>. This change will break backward compatibility, since numbers generated from same random seeds will be different. `25752` by `Gleb Levitski <glevv>`, `Jérémie du Boisberranger <jeremiedbb>`, `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Treat more consistently small values in the <span class="title-ref">W</span> and <span class="title-ref">H</span> matrices during the <span class="title-ref">fit</span> and <span class="title-ref">transform</span> steps of <span class="title-ref">decomposition.NMF</span> and <span class="title-ref">decomposition.MiniBatchNMF</span> which can produce different results than previous versions. `25438` by `Yotam Avidar-Constantini <yotamcons>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.KernelPCA</span> may produce different results through <span class="title-ref">inverse\_transform</span> if <span class="title-ref">gamma</span> is <span class="title-ref">None</span>. Now it will be chosen correctly as <span class="title-ref">1/n\_features</span> of the data that it is fitted on, while previously it might be incorrectly chosen as <span class="title-ref">1/n\_features</span> of the data passed to <span class="title-ref">inverse\_transform</span>. A new attribute <span class="title-ref">gamma\_</span> is provided for revealing the actual value of <span class="title-ref">gamma</span> used each time the kernel is called. `26337` by `Yao Xiao <Charlie-XIAO>`.

### Changed displays

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.LearningCurveDisplay</span> displays both the train and test curves by default. You can set <span class="title-ref">score\_type="test"</span> to keep the past behaviour. `25120` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.ValidationCurveDisplay</span> now accepts passing a list to the <span class="title-ref">param\_range</span> parameter. `27311` by `Arturo Amor <ArturoAmorQ>`.

### Changes impacting all modules

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">get\_feature\_names\_out</span> method of the following classes now raises a <span class="title-ref">NotFittedError</span> if the instance is not fitted. This ensures the error is consistent in all estimators with the <span class="title-ref">get\_feature\_names\_out</span> method.
    
      - <span class="title-ref">impute.MissingIndicator</span>
      - <span class="title-ref">feature\_extraction.DictVectorizer</span>
      - <span class="title-ref">feature\_extraction.text.TfidfTransformer</span>
      - <span class="title-ref">feature\_selection.GenericUnivariateSelect</span>
      - <span class="title-ref">feature\_selection.RFE</span>
      - <span class="title-ref">feature\_selection.RFECV</span>
      - <span class="title-ref">feature\_selection.SelectFdr</span>
      - <span class="title-ref">feature\_selection.SelectFpr</span>
      - <span class="title-ref">feature\_selection.SelectFromModel</span>
      - <span class="title-ref">feature\_selection.SelectFwe</span>
      - <span class="title-ref">feature\_selection.SelectKBest</span>
      - <span class="title-ref">feature\_selection.SelectPercentile</span>
      - <span class="title-ref">feature\_selection.SequentialFeatureSelector</span>
      - <span class="title-ref">feature\_selection.VarianceThreshold</span>
      - <span class="title-ref">kernel\_approximation.AdditiveChi2Sampler</span>
      - <span class="title-ref">impute.IterativeImputer</span>
      - <span class="title-ref">impute.KNNImputer</span>
      - <span class="title-ref">impute.SimpleImputer</span>
      - <span class="title-ref">isotonic.IsotonicRegression</span>
      - <span class="title-ref">preprocessing.Binarizer</span>
      - <span class="title-ref">preprocessing.KBinsDiscretizer</span>
      - <span class="title-ref">preprocessing.MaxAbsScaler</span>
      - <span class="title-ref">preprocessing.MinMaxScaler</span>
      - <span class="title-ref">preprocessing.Normalizer</span>
      - <span class="title-ref">preprocessing.OrdinalEncoder</span>
      - <span class="title-ref">preprocessing.PowerTransformer</span>
      - <span class="title-ref">preprocessing.QuantileTransformer</span>
      - <span class="title-ref">preprocessing.RobustScaler</span>
      - <span class="title-ref">preprocessing.SplineTransformer</span>
      - <span class="title-ref">preprocessing.StandardScaler</span>
      - <span class="title-ref">random\_projection.GaussianRandomProjection</span>
      - <span class="title-ref">random\_projection.SparseRandomProjection</span>
    
    The <span class="title-ref">NotFittedError</span> displays an informative message asking to fit the instance with the appropriate arguments.
    
    `25294`, `25308`, `25291`, `25367`, `25402`, by `John Pangas <jpangas>`, `Rahil Parikh <rprkh>` , and `Alex Buzenet <albuzenet>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added a multi-threaded Cython routine to the compute squared Euclidean distances (sometimes followed by a fused reduction operation) for a pair of datasets consisting of a sparse CSR matrix and a dense NumPy.
    
    This can improve the performance of following functions and estimators:
    
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin</span>
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin\_min</span>
      - <span class="title-ref">sklearn.cluster.AffinityPropagation</span>
      - <span class="title-ref">sklearn.cluster.Birch</span>
      - <span class="title-ref">sklearn.cluster.MeanShift</span>
      - <span class="title-ref">sklearn.cluster.OPTICS</span>
      - <span class="title-ref">sklearn.cluster.SpectralClustering</span>
      - <span class="title-ref">sklearn.feature\_selection.mutual\_info\_regression</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.LocalOutlierFactor</span>
      - <span class="title-ref">sklearn.neighbors.NearestNeighbors</span>
      - <span class="title-ref">sklearn.manifold.Isomap</span>
      - <span class="title-ref">sklearn.manifold.LocallyLinearEmbedding</span>
      - <span class="title-ref">sklearn.manifold.TSNE</span>
      - <span class="title-ref">sklearn.manifold.trustworthiness</span>
      - <span class="title-ref">sklearn.semi\_supervised.LabelPropagation</span>
      - <span class="title-ref">sklearn.semi\_supervised.LabelSpreading</span>
    
    A typical example of this performance improvement happens when passing a sparse CSR matrix to the <span class="title-ref">predict</span> or <span class="title-ref">transform</span> method of estimators that rely on a dense NumPy representation to store their fitted parameters (or the reverse).
    
    For instance, <span class="title-ref">sklearn.neighbors.NearestNeighbors.kneighbors</span> is now up to 2 times faster for this case on commonly available laptops.
    
    `25044` by `Julien Jerphanion <jjerphan>`.

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` All estimators that internally rely on OpenMP multi-threading (via Cython) now use a number of threads equal to the number of physical (instead of logical) cores by default. In the past, we observed that using as many threads as logical cores on SMT hosts could sometimes cause severe performance problems depending on the algorithms and the shape of the data. Note that it is still possible to manually adjust the number of threads used by OpenMP as documented in \[parallelism\](\#parallelism).
    
    `26082` by `Jérémie du Boisberranger <jeremiedbb>` and `Olivier Grisel <ogrisel>`.

### Experimental / Under Development

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` \[Metadata routing \<metadata\_routing\>\](\#metadata-routing-\<metadata\_routing\>)'s related base methods are included in this release. This feature is only available via the <span class="title-ref">enable\_metadata\_routing</span> feature flag which can be enabled using <span class="title-ref">sklearn.set\_config</span> and <span class="title-ref">sklearn.config\_context</span>. For now this feature is mostly useful for third party developers to prepare their code base for metadata routing, and we strongly recommend that they also hide it behind the same feature flag, rather than having it enabled by default. `24027` by [Adrin Jalali](https://github.com/adrinjalali), `Benjamin Bossan <BenjaminBossan>`, and `Omar Salman <OmarManzoor>`.

### Changelog

#### <span class="title-ref">sklearn</span>

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added a new option <span class="title-ref">skip\_parameter\_validation</span>, to the function <span class="title-ref">sklearn.set\_config</span> and context manager <span class="title-ref">sklearn.config\_context</span>, that allows to skip the validation of the parameters passed to the estimators and public functions. This can be useful to speed up the code but should be used with care because it can lead to unexpected behaviors or raise obscure error messages when setting invalid parameters. `25815` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.base`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A <span class="title-ref">\_\_sklearn\_clone\_\_</span> protocol is now available to override the default behavior of <span class="title-ref">base.clone</span>. `24568` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">base.TransformerMixin</span> now currently keeps a namedtuple's class if <span class="title-ref">transform</span> returns a namedtuple. `26121` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">calibration.CalibratedClassifierCV</span> now does not enforce sample alignment on <span class="title-ref">fit\_params</span>. `25805` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.cluster`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added <span class="title-ref">cluster.HDBSCAN</span>, a modern hierarchical density-based clustering algorithm. Similarly to <span class="title-ref">cluster.OPTICS</span>, it can be seen as a generalization of <span class="title-ref">cluster.DBSCAN</span> by allowing for hierarchical instead of flat clustering, however it varies in its approach from <span class="title-ref">cluster.OPTICS</span>. This algorithm is very robust with respect to its hyperparameters' values and can be used on a wide variety of data without much, if any, tuning.
    
    This implementation is an adaptation from the original implementation of HDBSCAN in [scikit-learn-contrib/hdbscan](https://github.com/scikit-learn-contrib/hdbscan), by `Leland McInnes <lmcinnes>` et al.
    
    `26385` by `Meekail Zain <micky774>`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">sample\_weight</span> parameter now will be used in centroids initialization for <span class="title-ref">cluster.KMeans</span>, <span class="title-ref">cluster.BisectingKMeans</span> and <span class="title-ref">cluster.MiniBatchKMeans</span>. This change will break backward compatibility, since numbers generated from same random seeds will be different. `25752` by `Gleb Levitski <glevv>`, `Jérémie du Boisberranger <jeremiedbb>`, `Guillaume Lemaitre <glemaitre>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.KMeans</span>, <span class="title-ref">cluster.MiniBatchKMeans</span> and <span class="title-ref">cluster.k\_means</span> now correctly handle the combination of <span class="title-ref">n\_init="auto"</span> and <span class="title-ref">init</span> being an array-like, running one initialization in that case. `26657` by `Binesh Bannerjee <bnsh>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">sample\_weight</span> parameter in <span class="title-ref">predict</span> for <span class="title-ref">cluster.KMeans.predict</span> and <span class="title-ref">cluster.MiniBatchKMeans.predict</span> is now deprecated and will be removed in v1.5. `25251` by `Gleb Levitski <glevv>`.

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">Xred</span> argument in <span class="title-ref">cluster.FeatureAgglomeration.inverse\_transform</span> is renamed to <span class="title-ref">Xt</span> and will be removed in v1.5. `26503` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> raises an informative error when the individual transformers of <span class="title-ref">ColumnTransformer</span> output pandas dataframes with indexes that are not consistent with each other and the output is configured to be pandas. `26286` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> correctly sets the output of the remainder when <span class="title-ref">set\_output</span> is called. `26323` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.covariance`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Allows <span class="title-ref">alpha=0</span> in <span class="title-ref">covariance.GraphicalLasso</span> to be consistent with <span class="title-ref">covariance.graphical\_lasso</span>. `26033` by `Genesis Valencia <genvalen>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">covariance.empirical\_covariance</span> now gives an informative error message when input is not appropriate. `26108` by `Quentin Barthélemy <qbarthelemy>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">cov\_init</span> in <span class="title-ref">covariance.graphical\_lasso</span> in 1.3 since the parameter has no effect. It will be removed in 1.5. `26033` by `Genesis Valencia <genvalen>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Adds <span class="title-ref">costs\_</span> fitted attribute in <span class="title-ref">covariance.GraphicalLasso</span> and <span class="title-ref">covariance.GraphicalLassoCV</span>. `26033` by `Genesis Valencia <genvalen>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Adds <span class="title-ref">covariance</span> parameter in <span class="title-ref">covariance.GraphicalLasso</span>. `26033` by `Genesis Valencia <genvalen>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Adds <span class="title-ref">eps</span> parameter in <span class="title-ref">covariance.GraphicalLasso</span>, <span class="title-ref">covariance.graphical\_lasso</span>, and <span class="title-ref">covariance.GraphicalLassoCV</span>. `26033` by `Genesis Valencia <genvalen>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Allows to overwrite the parameters used to open the ARFF file using the parameter <span class="title-ref">read\_csv\_kwargs</span> in <span class="title-ref">datasets.fetch\_openml</span> when using the pandas parser. `26433` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_openml</span> returns improved data types when <span class="title-ref">as\_frame=True</span> and <span class="title-ref">parser="liac-arff"</span>. `26386` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Following the ARFF specs, only the marker <span class="title-ref">"?"</span> is now considered as a missing values when opening ARFF files fetched using <span class="title-ref">datasets.fetch\_openml</span> when using the pandas parser. The parameter <span class="title-ref">read\_csv\_kwargs</span> allows to overwrite this behaviour. `26551` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.fetch\_openml</span> will consistently use <span class="title-ref">np.nan</span> as missing marker with both parsers <span class="title-ref">"pandas"</span> and <span class="title-ref">"liac-arff"</span>. `26579` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">data\_transposed</span> argument of <span class="title-ref">datasets.make\_sparse\_coded\_signal</span> is deprecated and will be removed in v1.5. `25784` by `Jérémie du Boisberranger`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span> and <span class="title-ref">decomposition.MiniBatchSparsePCA</span> are now faster for small batch sizes by avoiding duplicate validations. `25490` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.DictionaryLearning</span> now accepts the parameter <span class="title-ref">callback</span> for consistency with the function <span class="title-ref">decomposition.dict\_learning</span>. `24871` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Treat more consistently small values in the <span class="title-ref">W</span> and <span class="title-ref">H</span> matrices during the <span class="title-ref">fit</span> and <span class="title-ref">transform</span> steps of <span class="title-ref">decomposition.NMF</span> and <span class="title-ref">decomposition.MiniBatchNMF</span> which can produce different results than previous versions. `25438` by `Yotam Avidar-Constantini <yotamcons>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">W</span> argument in <span class="title-ref">decomposition.NMF.inverse\_transform</span> and <span class="title-ref">decomposition.MiniBatchNMF.inverse\_transform</span> is renamed to <span class="title-ref">Xt</span> and will be removed in v1.5. `26503` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.discriminant_analysis`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">discriminant\_analysis.LinearDiscriminantAnalysis</span> now supports the [PyTorch](https://pytorch.org/). See \[array\_api\](\#array\_api) for more details. `25956` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> now supports the Gamma deviance loss via <span class="title-ref">loss="gamma"</span>. Using the Gamma deviance as loss function comes in handy for modelling skewed distributed, strictly positive valued targets. `22409` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Compute a custom out-of-bag score by passing a callable to <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span>. `25177` by [Tim Head](https://betatim.github.io/).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.GradientBoostingClassifier</span> now exposes out-of-bag scores via the <span class="title-ref">oob\_scores\_</span> or <span class="title-ref">oob\_score\_</span> attributes. `24882` by `Ashwin Mathur <awinml>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.IsolationForest</span> predict time is now faster (typically by a factor of 8 or more). Internally, the estimator now precomputes decision path lengths per tree at <span class="title-ref">fit</span> time. It is therefore not possible to load an estimator trained with scikit-learn 1.2 to make it predict with scikit-learn 1.3: retraining with scikit-learn 1.3 is required. `25186` by `Felipe Breve Siola <fsiola>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.RandomForestClassifier</span> and <span class="title-ref">ensemble.RandomForestRegressor</span> with <span class="title-ref">warm\_start=True</span> now only recomputes out-of-bag scores when there are actually more <span class="title-ref">n\_estimators</span> in subsequent <span class="title-ref">fit</span> calls. `26318` by `Joshua Choo Yun Keat <choo8>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">ensemble.BaggingClassifier</span> and <span class="title-ref">ensemble.BaggingRegressor</span> expose the <span class="title-ref">allow\_nan</span> tag from the underlying estimator. `25506` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.RandomForestClassifier.fit</span> sets <span class="title-ref">max\_samples = 1</span> when <span class="title-ref">max\_samples</span> is a float and <span class="title-ref">round(n\_samples \* max\_samples) \< 1</span>. `25601` by `Jan Fidor <JanFidor>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.IsolationForest.fit</span> no longer warns about missing feature names when called with <span class="title-ref">contamination</span> not <span class="title-ref">"auto"</span> on a pandas dataframe. `25931` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> and <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> treats negative values for categorical features consistently as missing values, following LightGBM's and pandas' conventions. `25629` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix deprecation of <span class="title-ref">base\_estimator</span> in <span class="title-ref">ensemble.AdaBoostClassifier</span> and <span class="title-ref">ensemble.AdaBoostRegressor</span> that was introduced in `23819`. `26242` by `Marko Toplak <markotoplak>`.

#### `sklearn.exceptions`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Added <span class="title-ref">exceptions.InconsistentVersionWarning</span> which is raised when a scikit-learn estimator is unpickled with a scikit-learn version that is inconsistent with the sckit-learn version the estimator was pickled with. `25297` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">feature\_extraction.image.PatchExtractor</span> now follows the transformer API of scikit-learn. This class is defined as a stateless transformer meaning that it is note required to call <span class="title-ref">fit</span> before calling <span class="title-ref">transform</span>. Parameter validation only happens at <span class="title-ref">fit</span> time. `24230` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` All selectors in `sklearn.feature_selection` will preserve a DataFrame's dtype when transformed. `25102` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_selection.SequentialFeatureSelector</span>'s <span class="title-ref">cv</span> parameter now supports generators. `25973` by <span class="title-ref">Yao Xiao \<Charlie-XIAO\></span>.

#### `sklearn.impute`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added the parameter <span class="title-ref">fill\_value</span> to <span class="title-ref">impute.IterativeImputer</span>. `25232` by `Thijs van Weezel <ValueInvestorThijs>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">impute.IterativeImputer</span> now correctly preserves the Pandas Index when the <span class="title-ref">set\_config(transform\_output="pandas")</span>. `26454` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.inspection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added support for <span class="title-ref">sample\_weight</span> in <span class="title-ref">inspection.partial\_dependence</span> and <span class="title-ref">inspection.PartialDependenceDisplay.from\_estimator</span>. This allows for weighted averaging when aggregating for each value of the grid we are making the inspection on. The option is only available when <span class="title-ref">method</span> is set to <span class="title-ref">brute</span>. `25209` and `26644` by `Carlo Lemos <vitaliset>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">inspection.partial\_dependence</span> returns a <span class="title-ref">utils.Bunch</span> with new key: <span class="title-ref">grid\_values</span>. The <span class="title-ref">values</span> key is deprecated in favor of <span class="title-ref">grid\_values</span> and the <span class="title-ref">values</span> key will be removed in 1.5. `21809` and `25732` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.kernel_approximation`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">kernel\_approximation.AdditiveChi2Sampler</span> is now stateless. The <span class="title-ref">sample\_interval\_</span> attribute is deprecated and will be removed in 1.5. `25190` by `Vincent Maladière <Vincent-Maladiere>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Avoid data scaling when <span class="title-ref">sample\_weight=None</span> and other unnecessary data copies and unexpected dense to sparse data conversion in <span class="title-ref">linear\_model.LinearRegression</span>. `26207` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span> and <span class="title-ref">linear\_model.SGDOneClassSVM</span> now preserve dtype for <span class="title-ref">numpy.float32</span>. `25587` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">n\_iter\_</span> attribute has been included in <span class="title-ref">linear\_model.ARDRegression</span> to expose the actual number of iterations required to reach the stopping criterion. `25697` by `John Pangas <jpangas>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Use a more robust criterion to detect convergence of <span class="title-ref">linear\_model.LogisticRegression</span> with <span class="title-ref">penalty="l1"</span> and <span class="title-ref">solver="liblinear"</span> on linearly separable problems. `25214` by [Tom Dupre la Tour](https://github.com/TomDLT).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a crash when calling <span class="title-ref">fit</span> on <span class="title-ref">linear\_model.LogisticRegression</span> with <span class="title-ref">solver="newton-cholesky"</span> and <span class="title-ref">max\_iter=0</span> which failed to inspect the state of the model prior to the first parameter update. `26653` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">n\_iter</span> in favor of <span class="title-ref">max\_iter</span> in <span class="title-ref">linear\_model.BayesianRidge</span> and <span class="title-ref">linear\_model.ARDRegression</span>. <span class="title-ref">n\_iter</span> will be removed in scikit-learn 1.5. This change makes those estimators consistent with the rest of estimators. `25697` by `John Pangas <jpangas>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">manifold.Isomap</span> now correctly preserves the Pandas Index when the <span class="title-ref">set\_config(transform\_output="pandas")</span>. `26454` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` Adds <span class="title-ref">zero\_division=np.nan</span> to multiple classification metrics: <span class="title-ref">metrics.precision\_score</span>, <span class="title-ref">metrics.recall\_score</span>, <span class="title-ref">metrics.f1\_score</span>, <span class="title-ref">metrics.fbeta\_score</span>, <span class="title-ref">metrics.precision\_recall\_fscore\_support</span>, <span class="title-ref">metrics.classification\_report</span>. When <span class="title-ref">zero\_division=np.nan</span> and there is a zero division, the metric is undefined and is excluded from averaging. When not used for averages, the value returned is <span class="title-ref">np.nan</span>. `25531` by `Marc Torrellas Socastro <marctorsoc>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.average\_precision\_score</span> now supports the multiclass case. `17388` by `Geoffrey Bolmier <gbolmier>` and `24769` by `Ashwin Mathur <awinml>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The computation of the expected mutual information in <span class="title-ref">metrics.adjusted\_mutual\_info\_score</span> is now faster when the number of unique labels is large and its memory usage is reduced in general. `25713` by `Kshitij Mathur <Kshitij68>`, `Guillaume Lemaitre <glemaitre>`, `Omar Salman <OmarManzoor>` and `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.silhouette\_samples</span> nows accepts a sparse matrix of pairwise distances between samples, or a feature array. `18723` by `Sahil Gupta <sahilgupta2105>` and `24677` by `Ashwin Mathur <awinml>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A new parameter <span class="title-ref">drop\_intermediate</span> was added to <span class="title-ref">metrics.precision\_recall\_curve</span>, <span class="title-ref">metrics.PrecisionRecallDisplay.from\_estimator</span>, <span class="title-ref">metrics.PrecisionRecallDisplay.from\_predictions</span>, which drops some suboptimal thresholds to create lighter precision-recall curves. `24668` by `dberenbaum`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.RocCurveDisplay.from\_estimator</span> and <span class="title-ref">metrics.RocCurveDisplay.from\_predictions</span> now accept two new keywords, <span class="title-ref">plot\_chance\_level</span> and <span class="title-ref">chance\_level\_kw</span> to plot the baseline chance level. This line is exposed in the <span class="title-ref">chance\_level\_</span> attribute. `25987` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.PrecisionRecallDisplay.from\_estimator</span> and <span class="title-ref">metrics.PrecisionRecallDisplay.from\_predictions</span> now accept two new keywords, <span class="title-ref">plot\_chance\_level</span> and <span class="title-ref">chance\_level\_kw</span> to plot the baseline chance level. This line is exposed in the <span class="title-ref">chance\_level\_</span> attribute. `26019` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.pairwise.manhattan\_distances</span> now supports readonly sparse datasets. `25432` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed <span class="title-ref">metrics.classification\_report</span> so that empty input will return <span class="title-ref">np.nan</span>. Previously, "macro avg" and <span class="title-ref">weighted avg</span> would return e.g. <span class="title-ref">f1-score=np.nan</span> and <span class="title-ref">f1-score=0.0</span>, being inconsistent. Now, they both return <span class="title-ref">np.nan</span>. `25531` by `Marc Torrellas Socastro <marctorsoc>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.ndcg\_score</span> now gives a meaningful error message for input of length 1. `25672` by `Lene Preuss <lene>` and `Wei-Chun Chu <wcchu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.log\_loss</span> raises a warning if the values of the parameter <span class="title-ref">y\_pred</span> are not normalized, instead of actually normalizing them in the metric. Starting from 1.5 this will raise an error. `25299` by `Omar Salman <OmarManzoor`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` In <span class="title-ref">metrics.roc\_curve</span>, use the threshold value <span class="title-ref">np.inf</span> instead of arbitrary <span class="title-ref">max(y\_score) + 1</span>. This threshold is associated with the ROC curve point <span class="title-ref">tpr=0</span> and <span class="title-ref">fpr=0</span>. `26194` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">'matching'</span> metric has been removed when using SciPy\>=1.9 to be consistent with <span class="title-ref">scipy.spatial.distance</span> which does not support <span class="title-ref">'matching'</span> anymore. `26264` by `Barata T. Onggo <magnusbarata>`
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">eps</span> parameter of the <span class="title-ref">metrics.log\_loss</span> has been deprecated and will be removed in 1.5. `25299` by `Omar Salman <OmarManzoor>`.

#### `sklearn.gaussian_process`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">gaussian\_process.GaussianProcessRegressor</span> has a new argument <span class="title-ref">n\_targets</span>, which is used to decide the number of outputs when sampling from the prior distributions. `23099` by `Zhehao Liu <MaxwellLZH>`.

#### `sklearn.mixture`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">mixture.GaussianMixture</span> is more efficient now and will bypass unnecessary initialization if the weights, means, and precisions are given by users. `26021` by `Jiawei Zhang <jiawei-zhang-a>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Added the class <span class="title-ref">model\_selection.ValidationCurveDisplay</span> that allows easy plotting of validation curves obtained by the function <span class="title-ref">model\_selection.validation\_curve</span>. `25120` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The parameter <span class="title-ref">log\_scale</span> in the class <span class="title-ref">model\_selection.LearningCurveDisplay</span> has been deprecated in 1.3 and will be removed in 1.5. The default scale can be overridden by setting it directly on the <span class="title-ref">ax</span> object and will be set automatically from the spacing of the data points otherwise. `25120` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.cross\_validate</span> accepts a new parameter <span class="title-ref">return\_indices</span> to return the train-test indices of each cv split. `25659` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">getattr</span> on <span class="title-ref">multioutput.MultiOutputRegressor.partial\_fit</span> and <span class="title-ref">multioutput.MultiOutputClassifier.partial\_fit</span> now correctly raise an <span class="title-ref">AttributeError</span> if done before calling <span class="title-ref">fit</span>. `26333` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.naive_bayes`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">naive\_bayes.GaussianNB</span> does not raise anymore a <span class="title-ref">ZeroDivisionError</span> when the provided <span class="title-ref">sample\_weight</span> reduces the problem to a single class in <span class="title-ref">fit</span>. `24140` by `Jonathan Ohayon <Johayon>` and `Chiara Marmo <cmarmo>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The performance of <span class="title-ref">neighbors.KNeighborsClassifier.predict</span> and of <span class="title-ref">neighbors.KNeighborsClassifier.predict\_proba</span> has been improved when <span class="title-ref">n\_neighbors</span> is large and <span class="title-ref">algorithm="brute"</span> with non Euclidean metrics. `24076` by `Meekail Zain <micky774>`, `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Remove support for <span class="title-ref">KulsinskiDistance</span> in <span class="title-ref">neighbors.BallTree</span>. This dissimilarity is not a metric and cannot be supported by the BallTree. `25417` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The support for metrics other than <span class="title-ref">euclidean</span> and <span class="title-ref">manhattan</span> and for callables in <span class="title-ref">neighbors.NearestNeighbors</span> is deprecated and will be removed in version 1.5. `24083` by `Valentin Laurent <Valentin-Laurent>`.

#### `sklearn.neural_network`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neural\_network.MLPRegressor</span> and <span class="title-ref">neural\_network.MLPClassifier</span> reports the right <span class="title-ref">n\_iter\_</span> when <span class="title-ref">warm\_start=True</span>. It corresponds to the number of iterations performed on the current call to <span class="title-ref">fit</span> instead of the total number of iterations performed since the initialization of the estimator. `25443` by `Marvin Krawutschke <Marvvxi>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">pipeline.FeatureUnion</span> can now use indexing notation (e.g. <span class="title-ref">feature\_union\["scalar"\]</span>) to access transformers by name. `25093` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">pipeline.FeatureUnion</span> can now access the <span class="title-ref">feature\_names\_in\_</span> attribute if the <span class="title-ref">X</span> value seen during <span class="title-ref">.fit</span> has a <span class="title-ref">columns</span> attribute and all columns are strings. e.g. when <span class="title-ref">X</span> is a <span class="title-ref">pandas.DataFrame</span> `25220` by `Ian Thompson <it176131>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">pipeline.Pipeline.fit\_transform</span> now raises an <span class="title-ref">AttributeError</span> if the last step of the pipeline does not support <span class="title-ref">fit\_transform</span>. `26325` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Introduces <span class="title-ref">preprocessing.TargetEncoder</span> which is a categorical encoding based on target mean conditioned on the value of the category. `25334` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">preprocessing.OrdinalEncoder</span> now supports grouping infrequent categories into a single feature. Grouping infrequent categories is enabled by specifying how to select infrequent categories with <span class="title-ref">min\_frequency</span> or <span class="title-ref">max\_categories</span>. `25677` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">preprocessing.PolynomialFeatures</span> now calculates the number of expanded terms a-priori when dealing with sparse <span class="title-ref">csr</span> matrices in order to optimize the choice of <span class="title-ref">dtype</span> for <span class="title-ref">indices</span> and <span class="title-ref">indptr</span>. It can now output <span class="title-ref">csr</span> matrices with <span class="title-ref">np.int32</span> <span class="title-ref">indices/indptr</span> components when there are few enough elements, and will automatically use <span class="title-ref">np.int64</span> for sufficiently large matrices. `20524` by `niuk-a <niuk-a>` and `23731` by `Meekail Zain <micky774>`
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A new parameter <span class="title-ref">sparse\_output</span> was added to <span class="title-ref">preprocessing.SplineTransformer</span>, available as of SciPy 1.8. If <span class="title-ref">sparse\_output=True</span>, <span class="title-ref">preprocessing.SplineTransformer</span> returns a sparse CSR matrix. `24145` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds a <span class="title-ref">feature\_name\_combiner</span> parameter to <span class="title-ref">preprocessing.OneHotEncoder</span>. This specifies a custom callable to create feature names to be returned by <span class="title-ref">preprocessing.OneHotEncoder.get\_feature\_names\_out</span>. The callable combines input arguments <span class="title-ref">(input\_feature, category)</span> to a string. `22506` by `Mario Kostelac <mariokostelac>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added support for <span class="title-ref">sample\_weight</span> in <span class="title-ref">preprocessing.KBinsDiscretizer</span>. This allows specifying the parameter <span class="title-ref">sample\_weight</span> for each sample to be used while fitting. The option is only available when <span class="title-ref">strategy</span> is set to <span class="title-ref">quantile</span> and <span class="title-ref">kmeans</span>. `24935` by `Seladus <seladus>`, `Guillaume Lemaitre <glemaitre>`, and `Dea María Léon <deamarialeon>`, `25257` by `Gleb Levitski <glevv>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Subsampling through the <span class="title-ref">subsample</span> parameter can now be used in <span class="title-ref">preprocessing.KBinsDiscretizer</span> regardless of the strategy used. `26424` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.PowerTransformer</span> now correctly preserves the Pandas Index when the <span class="title-ref">set\_config(transform\_output="pandas")</span>. `26454` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.PowerTransformer</span> now correctly raises error when using <span class="title-ref">method="box-cox"</span> on data with a constant <span class="title-ref">np.nan</span> column. `26400` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.PowerTransformer</span> with <span class="title-ref">method="yeo-johnson"</span> now leaves constant features unchanged instead of transforming with an arbitrary value for the <span class="title-ref">lambdas\_</span> fitted parameter. `26566` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The default value of the <span class="title-ref">subsample</span> parameter of <span class="title-ref">preprocessing.KBinsDiscretizer</span> will change from <span class="title-ref">None</span> to <span class="title-ref">200\_000</span> in version 1.5 when <span class="title-ref">strategy="kmeans"</span> or <span class="title-ref">strategy="uniform"</span>. `26424` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">dual</span> parameter now accepts <span class="title-ref">auto</span> option for <span class="title-ref">svm.LinearSVC</span> and <span class="title-ref">svm.LinearSVR</span>. `26093` by `Gleb Levitski <glevv>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">tree.DecisionTreeRegressor</span> and <span class="title-ref">tree.DecisionTreeClassifier</span> support missing values when <span class="title-ref">splitter='best'</span> and criterion is <span class="title-ref">gini</span>, <span class="title-ref">entropy</span>, or <span class="title-ref">log\_loss</span>, for classification or <span class="title-ref">squared\_error</span>, <span class="title-ref">friedman\_mse</span>, or <span class="title-ref">poisson</span> for regression. `23595`, `26376` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds a <span class="title-ref">class\_names</span> parameter to <span class="title-ref">tree.export\_text</span>. This allows specifying the parameter <span class="title-ref">class\_names</span> for each target class in ascending numerical order. `25387` by `William M <Akbeeh>` and `crispinlogan <crispinlogan>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">tree.export\_graphviz</span> and <span class="title-ref">tree.export\_text</span> now accepts <span class="title-ref">feature\_names</span> and <span class="title-ref">class\_names</span> as array-like rather than lists. `26289` by `Yao Xiao <Charlie-XIAO>`

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes <span class="title-ref">utils.check\_array</span> to properly convert pandas extension arrays. `25813` and `26106` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.check\_array</span> now supports pandas DataFrames with extension arrays and object dtypes by return an ndarray with object dtype. `25814` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.estimator\_checks.check\_transformers\_unfitted\_stateless</span> has been introduced to ensure stateless transformers don't raise <span class="title-ref">NotFittedError</span> during <span class="title-ref">transform</span> with no prior call to <span class="title-ref">fit</span> or <span class="title-ref">fit\_transform</span>. `25190` by `Vincent Maladière <Vincent-Maladiere>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` A <span class="title-ref">FutureWarning</span> is now raised when instantiating a class which inherits from a deprecated base class (i.e. decorated by <span class="title-ref">utils.deprecated</span>) and which overrides the <span class="title-ref">\_\_init\_\_</span> method. `25733` by `Brigitta Sipőcz <bsipocz>` and `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.semi_supervised`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">semi\_supervised.LabelSpreading.fit</span> and <span class="title-ref">semi\_supervised.LabelPropagation.fit</span> now accepts sparse metrics. `19664` by `Kaushik Amar Das <cozek>`.

#### Miscellaneous

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Replace obsolete exceptions <span class="title-ref">EnvironmentError</span>, <span class="title-ref">IOError</span> and <span class="title-ref">WindowsError</span>. `26466` by `Dimitri Papadopoulos ORfanos <DimitriPapadopoulos>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.2, including:

2357juan, Abhishek Singh Kushwah, Adam Handke, Adam Kania, Adam Li, adienes, Admir Demiraj, adoublet, Adrin Jalali, A.H.Mansouri, Ahmedbgh, Ala-Na, Alex Buzenet, AlexL, Ali H. El-Kassas, amay, András Simon, André Pedersen, Andrew Wang, Ankur Singh, annegnx, Ansam Zedan, Anthony22-dev, Artur Hermano, Arturo Amor, as-90, ashah002, Ashish Dutt, Ashwin Mathur, AymericBasset, Azaria Gebremichael, Barata Tripramudya Onggo, Benedek Harsanyi, Benjamin Bossan, Bharat Raghunathan, Binesh Bannerjee, Boris Feld, Brendan Lu, Brevin Kunde, cache-missing, Camille Troillard, Carla J, carlo, Carlo Lemos, c-git, Changyao Chen, Chiara Marmo, Christian Lorentzen, Christian Veenhuis, Christine P. Chai, crispinlogan, Da-Lan, DanGonite57, Dave Berenbaum, davidblnc, david-cortes, Dayne, Dea María Léon, Denis, Dimitri Papadopoulos Orfanos, Dimitris Litsidis, Dmitry Nesterov, Dominic Fox, Dominik Prodinger, Edern, Ekaterina Butyugina, Elabonga Atuo, Emir, farhan khan, Felipe Siola, futurewarning, Gael Varoquaux, genvalen, Gleb Levitski, Guillaume Lemaitre, gunesbayir, Haesun Park, hujiahong726, i-aki-y, Ian Thompson, Ido M, Ily, Irene, Jack McIvor, jakirkham, James Dean, JanFidor, Jarrod Millman, JB Mountford, Jérémie du Boisberranger, Jessicakk0711, Jiawei Zhang, Joey Ortiz, JohnathanPi, John Pangas, Joshua Choo Yun Keat, Joshua Hedlund, JuliaSchoepp, Julien Jerphanion, jygerardy, ka00ri, Kaushik Amar Das, Kento Nozawa, Kian Eliasi, Kilian Kluge, Lene Preuss, Linus, Logan Thomas, Loic Esteve, Louis Fouquet, Lucy Liu, Madhura Jayaratne, Marc Torrellas Socastro, Maren Westermann, Mario Kostelac, Mark Harfouche, Marko Toplak, Marvin Krawutschke, Masanori Kanazu, mathurinm, Matt Haberland, Max Halford, maximeSaur, Maxwell Liu, m. bou, mdarii, Meekail Zain, Mikhail Iljin, murezzda, Nawazish Alam, Nicola Fanelli, Nightwalkx, Nikolay Petrov, Nishu Choudhary, NNLNR, npache, Olivier Grisel, Omar Salman, ouss1508, PAB, Pandata, partev, Peter Piontek, Phil, pnucci, Pooja M, Pooja Subramaniam, precondition, Quentin Barthélemy, Rafal Wojdyla, Raghuveer Bhat, Rahil Parikh, Ralf Gommers, ram vikram singh, Rushil Desai, Sadra Barikbin, SANJAI\_3, Sashka Warner, Scott Gigante, Scott Gustafson, searchforpassion, Seoeun Hong, Shady el Gewily, Shiva chauhan, Shogo Hida, Shreesha Kumar Bhat, sonnivs, Sortofamudkip, Stanislav (Stanley) Modrak, Stefanie Senger, Steven Van Vaerenbergh, Tabea Kossen, Théophile Baranger, Thijs van Weezel, Thomas A Caswell, Thomas Germer, Thomas J. Fan, Tim Head, Tim P, Tom Dupré la Tour, tomiock, tspeng, Valentin Laurent, Veghit, VIGNESH D, Vijeth Moudgalya, Vinayak Mehta, Vincent M, Vincent-violet, Vyom Pathak, William M, windiana42, Xiao Yuan, Yao Xiao, Yaroslav Halchenko, Yotam Avidar-Constantini, Yuchen Zhou, Yusuf Raji, zeeshan lone

---

v1.4.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.4

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_4\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_4\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 1.4.2

**April 2024**

This release only includes support for numpy 2.

## Version 1.4.1

**February 2024**

### Changed models

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">tree\_.value</span> attribute in <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeRegressor</span> changed from an weighted absolute count of number of samples to a weighted fraction of the total number of samples. `27639` by `Samuel Ronsin <samronsin>`.

### Metadata Routing

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix routing issue with <span class="title-ref">\~compose.ColumnTransformer</span> when used inside another meta-estimator. `28188` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` No error is raised when no metadata is passed to a metaestimator that includes a sub-estimator which doesn't support metadata routing. `28256` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix <span class="title-ref">multioutput.MultiOutputRegressor</span> and <span class="title-ref">multioutput.MultiOutputClassifier</span> to work with estimators that don't consume any metadata when metadata routing is enabled. `28240` by [Adrin Jalali](https://github.com/adrinjalali).

### DataFrame Support

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Pandas and Polars dataframe are validated directly without ducktyping checks. `28195` by [Thomas Fan](https://github.com/thomasjpfan).

### Changes impacting many modules

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Partial revert of `28191` to avoid a performance regression for estimators relying on euclidean pairwise computation with sparse matrices. The impacted estimators are:
    
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin</span>
      - <span class="title-ref">sklearn.metrics.pairwise\_distances\_argmin\_min</span>
      - <span class="title-ref">sklearn.cluster.AffinityPropagation</span>
      - <span class="title-ref">sklearn.cluster.Birch</span>
      - <span class="title-ref">sklearn.cluster.SpectralClustering</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.KNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsClassifier</span>
      - <span class="title-ref">sklearn.neighbors.RadiusNeighborsRegressor</span>
      - <span class="title-ref">sklearn.neighbors.LocalOutlierFactor</span>
      - <span class="title-ref">sklearn.neighbors.NearestNeighbors</span>
      - <span class="title-ref">sklearn.manifold.Isomap</span>
      - <span class="title-ref">sklearn.manifold.TSNE</span>
      - <span class="title-ref">sklearn.manifold.trustworthiness</span>
    
    `28235` by `Julien Jerphanion <jjerphan>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug for all scikit-learn transformers when using <span class="title-ref">set\_output</span> with <span class="title-ref">transform</span> set to <span class="title-ref">pandas</span> or <span class="title-ref">polars</span>. The bug could lead to wrong naming of the columns of the returned dataframe. `28262` by `Guillaume Lemaitre <glemaitre>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` When users try to use a method in <span class="title-ref">\~ensemble.StackingClassifier</span>, <span class="title-ref">\~ensemble.StackingClassifier</span>, <span class="title-ref">\~ensemble.StackingClassifier</span>, <span class="title-ref">\~feature\_selection.SelectFromModel</span>, <span class="title-ref">\~feature\_selection.RFE</span>, <span class="title-ref">\~semi\_supervised.SelfTrainingClassifier</span>, <span class="title-ref">\~multiclass.OneVsOneClassifier</span>, <span class="title-ref">\~multiclass.OutputCodeClassifier</span> or <span class="title-ref">\~multiclass.OneVsRestClassifier</span> that their sub-estimators don't implement, the <span class="title-ref">AttributeError</span> now reraises in the traceback. `28167` by `Stefanie Senger <StefanieSenger>`.

### Changelog

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">calibration.CalibratedClassifierCV</span> supports `predict_proba` with float32 output from the inner estimator. `28247` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.AffinityPropagation</span> now avoids assigning multiple different clusters for equal points. `28121` by `Pietro Peterlongo <pietroppeter>` and `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Avoid infinite loop in <span class="title-ref">cluster.KMeans</span> when the number of clusters is larger than the number of non-duplicate samples. `28165` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">compose.ColumnTransformer</span> now transform into a polars dataframe when <span class="title-ref">verbose\_feature\_names\_out=True</span> and the transformers internally used several times the same columns. Previously, it would raise a due to duplicated column names. `28262` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">HistGradientBoostingClassifier</span> and <span class="title-ref">HistGradientBoostingRegressor</span> when fitted on <span class="title-ref">pandas</span> <span class="title-ref">DataFrame</span> with extension dtypes, for example <span class="title-ref">pd.Int64Dtype</span> `28385` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes error message raised by <span class="title-ref">ensemble.VotingClassifier</span> when the target is multilabel or multiclass-multioutput in a DataFrame format. `27702` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`: <span class="title-ref">impute.SimpleImputer</span> now raises an error in <span class="title-ref">.fit</span> and <span class="title-ref">.transform</span> if <span class="title-ref">fill\_value</span> can not be cast to input value dtype with <span class="title-ref">casting='same\_kind'</span>. `28365` by `Leo Grinsztajn <LeoGrin>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">inspection.permutation\_importance</span> now handles properly <span class="title-ref">sample\_weight</span> together with subsampling (i.e. <span class="title-ref">max\_features</span> \< 1.0). `28184` by `Michael Mayer <mayer79>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.ARDRegression</span> now handles pandas input types for <span class="title-ref">predict(X, return\_std=True)</span>. `28377` by `Eddie Bergman <eddiebergman>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` make <span class="title-ref">preprocessing.FunctionTransformer</span> more lenient and overwrite output column names with the <span class="title-ref">get\_feature\_names\_out</span> in the following cases:
    
    (i) the input and output column names remain the same (happen when using NumPy <span class="title-ref">ufunc</span>); (ii) the input column names are numbers; (iii) the output will be set to Pandas or Polars dataframe. `28241` by `Guillaume Lemaitre <glemaitre>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.FunctionTransformer</span> now also warns when <span class="title-ref">set\_output</span> is called with <span class="title-ref">transform="polars"</span> and <span class="title-ref">func</span> does not return a Polars dataframe or <span class="title-ref">feature\_names\_out</span> is not specified. `28263` by `Guillaume Lemaitre <glemaitre>`.

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.TargetEncoder</span> no longer fails when <span class="title-ref">target\_type="continuous"</span> and the input is read-only. In particular, it now works with pandas copy-on-write mode enabled. `28233` by `John Hopfensperger <s-banach>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">tree.DecisionTreeClassifier</span> and <span class="title-ref">tree.DecisionTreeRegressor</span> are handling missing values properly. The internal criterion was not initialized when no missing values were present in the data, leading to potentially wrong criterion values. `28295` by `Guillaume Lemaitre <glemaitre>` and `28327` by `Adam Li <adam2392>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.metaestimators.available\_if</span> now reraises the error from the <span class="title-ref">check</span> function as the cause of the <span class="title-ref">AttributeError</span>. `28198` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">utils.\_safe\_indexing</span> now raises a <span class="title-ref">ValueError</span> when <span class="title-ref">X</span> is a Python list and <span class="title-ref">axis=1</span>, as documented in the docstring. `28222` by `Guillaume Lemaitre <glemaitre>`.

## Version 1.4.0

**January 2024**

### Changed models

The following estimators and functions, when fit with the same data and parameters, may produce different models from the previous version. This often occurs due to changes in the modelling logic (bug fixes or enhancements), or in random sampling procedures.

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> now have much better convergence for solvers <span class="title-ref">"lbfgs"</span> and <span class="title-ref">"newton-cg"</span>. Both solvers can now reach much higher precision for the coefficients depending on the specified <span class="title-ref">tol</span>. Additionally, lbfgs can make better use of <span class="title-ref">tol</span>, i.e., stop sooner or reach higher precision. Note: The lbfgs is the default solver, so this change might effect many models. This change also means that with this new version of scikit-learn, the resulting coefficients <span class="title-ref">coef\_</span> and <span class="title-ref">intercept\_</span> of your models will change for these two solvers (when fit on the same data again). The amount of change depends on the specified <span class="title-ref">tol</span>, for small values you will get more precise results. `26721` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` fixes a memory leak seen in PyPy for estimators using the Cython loss functions. `27670` by `Guillaume Lemaitre <glemaitre>`.

### Changes impacting all modules

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Transformers now support polars output with <span class="title-ref">set\_output(transform="polars")</span>. `27315` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` All estimators now recognizes the column names from any dataframe that adopts the [DataFrame Interchange Protocol](https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html). Dataframes that return a correct representation through <span class="title-ref">np.asarray(df)</span> is expected to work with our estimators and functions. `26464` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The HTML representation of estimators now includes a link to the documentation and is color-coded to denote whether the estimator is fitted or not (unfitted estimators are orange, fitted estimators are blue). `26616` by `Riccardo Cappuzzo <rcap107>`, `Ines Ibnukhsein <Ines1999>`, `Gael Varoquaux <GaelVaroquaux>`, [Joel Nothman](https://joelnothman.com/) and `Lilian Boulard <LilianBoulard>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a bug in most estimators and functions where setting a parameter to a large integer would cause a <span class="title-ref">TypeError</span>. `26648` by `Naoise Holohan <naoise-h>`.

### Metadata Routing

The following models now support metadata routing in one or more or their methods. Refer to the \[Metadata Routing User Guide \<metadata\_routing\>\](\#metadata-routing-user-guide-\<metadata\_routing\>) for more details.

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">LarsCV</span> and <span class="title-ref">LassoLarsCV</span> now support metadata routing in their <span class="title-ref">fit</span> method and route metadata to the CV splitter. `27538` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">multiclass.OneVsRestClassifier</span>, <span class="title-ref">multiclass.OneVsOneClassifier</span> and <span class="title-ref">multiclass.OutputCodeClassifier</span> now support metadata routing in their `fit` and `partial_fit`, and route metadata to the underlying estimator's `fit` and `partial_fit`. `27308` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">pipeline.Pipeline</span> now supports metadata routing according to \[metadata routing user guide \<metadata\_routing\>\](\#metadata-routing-user-guide-\<metadata\_routing\>). `26789` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">\~model\_selection.cross\_validate</span>, <span class="title-ref">\~model\_selection.cross\_val\_score</span>, and <span class="title-ref">\~model\_selection.cross\_val\_predict</span> now support metadata routing. The metadata are routed to the estimator's <span class="title-ref">fit</span>, the scorer, and the CV splitter's <span class="title-ref">split</span>. The metadata is accepted via the new <span class="title-ref">params</span> parameter. <span class="title-ref">fit\_params</span> is deprecated and will be removed in version 1.6. <span class="title-ref">groups</span> parameter is also not accepted as a separate argument when metadata routing is enabled and should be passed via the <span class="title-ref">params</span> parameter. `26896` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">\~model\_selection.GridSearchCV</span>, <span class="title-ref">\~model\_selection.RandomizedSearchCV</span>, <span class="title-ref">\~model\_selection.HalvingGridSearchCV</span>, and <span class="title-ref">\~model\_selection.HalvingRandomSearchCV</span> now support metadata routing in their `fit` and `score`, and route metadata to the underlying estimator's `fit`, the CV splitter, and the scorer. `27058` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">\~compose.ColumnTransformer</span> now supports metadata routing according to \[metadata routing user guide \<metadata\_routing\>\](\#metadata-routing-user-guide-\<metadata\_routing\>). `27005` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.LogisticRegressionCV</span> now supports metadata routing. <span class="title-ref">linear\_model.LogisticRegressionCV.fit</span> now accepts `**params` which are passed to the underlying splitter and scorer. <span class="title-ref">linear\_model.LogisticRegressionCV.score</span> now accepts `**score_params` which are passed to the underlying scorer. `26525` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">feature\_selection.SelectFromModel</span> now supports metadata routing in <span class="title-ref">fit</span> and <span class="title-ref">partial\_fit</span>. `27490` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.OrthogonalMatchingPursuitCV</span> now supports metadata routing. Its <span class="title-ref">fit</span> now accepts `**fit_params`, which are passed to the underlying splitter. `27500` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ElasticNetCV</span>, <span class="title-ref">LassoCV</span>, <span class="title-ref">MultiTaskElasticNetCV</span> and <span class="title-ref">MultiTaskLassoCV</span> now support metadata routing and route metadata to the CV splitter. `27478` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` All meta-estimators for which metadata routing is not yet implemented now raise a <span class="title-ref">NotImplementedError</span> on <span class="title-ref">get\_metadata\_routing</span> and on <span class="title-ref">fit</span> if metadata routing is enabled and any metadata is passed to them. `27389` by [Adrin Jalali](https://github.com/adrinjalali).

### Support for SciPy sparse arrays

Several estimators are now supporting SciPy sparse arrays. The following functions and classes are impacted:

**Functions:**

  - <span class="title-ref">cluster.compute\_optics\_graph</span> in `27104` by `Maren Westermann <marenwestermann>` and in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">cluster.kmeans\_plusplus</span> in `27179` by `Nurseit Kamchyev <Bncer>`;
  - <span class="title-ref">decomposition.non\_negative\_factorization</span> in `27100` by `Isaac Virshup <ivirshup>`;
  - <span class="title-ref">feature\_selection.f\_regression</span> in `27239` by `Yaroslav Korobko <Tialo>`;
  - <span class="title-ref">feature\_selection.r\_regression</span> in `27239` by `Yaroslav Korobko <Tialo>`;
  - <span class="title-ref">manifold.trustworthiness</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">manifold.spectral\_embedding</span> in `27240` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">metrics.pairwise\_distances</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">metrics.pairwise\_distances\_chunked</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">metrics.pairwise.pairwise\_kernels</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">utils.multiclass.type\_of\_target</span> in `27274` by `Yao Xiao <Charlie-XIAO>`.

**Classes:**

  - <span class="title-ref">cluster.HDBSCAN</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">cluster.KMeans</span> in `27179` by `Nurseit Kamchyev <Bncer>`;
  - <span class="title-ref">cluster.MiniBatchKMeans</span> in `27179` by `Nurseit Kamchyev <Bncer>`;
  - <span class="title-ref">cluster.OPTICS</span> in `27104` by `Maren Westermann <marenwestermann>` and in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">cluster.SpectralClustering</span> in `27161` by `Bharat Raghunathan <bharatr21>`;
  - <span class="title-ref">decomposition.MiniBatchNMF</span> in `27100` by `Isaac Virshup <ivirshup>`;
  - <span class="title-ref">decomposition.NMF</span> in `27100` by `Isaac Virshup <ivirshup>`;
  - <span class="title-ref">feature\_extraction.text.TfidfTransformer</span> in `27219` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">manifold.Isomap</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">manifold.SpectralEmbedding</span> in `27240` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">manifold.TSNE</span> in `27250` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">impute.SimpleImputer</span> in `27277` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">impute.IterativeImputer</span> in `27277` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">impute.KNNImputer</span> in `27277` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">kernel\_approximation.PolynomialCountSketch</span> in `27301` by `Lohit SundaramahaLingam <lohitslohit>`;
  - <span class="title-ref">neural\_network.BernoulliRBM</span> in `27252` by `Yao Xiao <Charlie-XIAO>`;
  - <span class="title-ref">preprocessing.PolynomialFeatures</span> in `27166` by `Mohit Joshi <work-mohit>`;
  - <span class="title-ref">random\_projection.GaussianRandomProjection</span> in `27314` by `Stefanie Senger <StefanieSenger>`;
  - <span class="title-ref">random\_projection.SparseRandomProjection</span> in `27314` by `Stefanie Senger <StefanieSenger>`.

### Support for Array API

Several estimators and functions support the [Array API](https://data-apis.org/array-api/latest/). Such changes allows for using the estimators and functions with other libraries such as JAX, CuPy, and PyTorch. This therefore enables some GPU-accelerated computations.

See \[array\_api\](\#array\_api) for more details.

**Functions:**

  - <span class="title-ref">sklearn.metrics.accuracy\_score</span> and <span class="title-ref">sklearn.metrics.zero\_one\_loss</span> in `27137` by `Edoardo Abati <EdAbati>`;
  - <span class="title-ref">sklearn.model\_selection.train\_test\_split</span> in `26855` by [Tim Head](https://betatim.github.io/);
  - <span class="title-ref">\~utils.multiclass.is\_multilabel</span> in `27601` by `Yaroslav Korobko <Tialo>`.

**Classes:**

  - <span class="title-ref">decomposition.PCA</span> for the <span class="title-ref">full</span> and <span class="title-ref">randomized</span> solvers (with QR power iterations) in `26315`, `27098` and `27431` by `Mateusz Sokół <mtsokol>`, `Olivier Grisel <ogrisel>` and `Edoardo Abati <EdAbati>`;
  - <span class="title-ref">preprocessing.KernelCenterer</span> in `27556` by `Edoardo Abati <EdAbati>`;
  - <span class="title-ref">preprocessing.MaxAbsScaler</span> in `27110` by `Edoardo Abati <EdAbati>`;
  - <span class="title-ref">preprocessing.MinMaxScaler</span> in `26243` by [Tim Head](https://betatim.github.io/);
  - <span class="title-ref">preprocessing.Normalizer</span> in `27558` by `Edoardo Abati <EdAbati>`.

### Private Loss Function Module

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The gradient computation of the binomial log loss is now numerically more stable for very large, in absolute value, input (raw predictions). Before, it could result in <span class="title-ref">np.nan</span>. Among the models that profit from this change are <span class="title-ref">ensemble.GradientBoostingClassifier</span>, <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">linear\_model.LogisticRegression</span>. `28048` by `Christian Lorentzen <lorentzenchr>`.

### Changelog

#### `sklearn.base`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">base.ClusterMixin.fit\_predict</span> and <span class="title-ref">base.OutlierMixin.fit\_predict</span> now accept `**kwargs` which are passed to the `fit` method of the estimator. `26506` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">base.TransformerMixin.fit\_transform</span> and <span class="title-ref">base.OutlierMixin.fit\_predict</span> now raise a warning if `transform` / `predict` consume metadata, but no custom `fit_transform` / `fit_predict` is defined in the class inheriting from them correspondingly. `26831` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">base.clone</span> now supports <span class="title-ref">dict</span> as input and creates a copy. `26786` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}`<span class="title-ref">\~utils.metadata\_routing.process\_routing</span> now has a different signature. The first two (the object and the method) are positional only, and all metadata are passed as keyword arguments. `26909` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.calibration`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The internal objective and gradient of the <span class="title-ref">sigmoid</span> method of <span class="title-ref">calibration.CalibratedClassifierCV</span> have been replaced by the private loss module. `27185` by `Omar Salman <OmarManzoor>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">degree</span> parameter in the <span class="title-ref">cluster.SpectralClustering</span> constructor now accepts real values instead of only integral values in accordance with the <span class="title-ref">degree</span> parameter of the <span class="title-ref">sklearn.metrics.pairwise.polynomial\_kernel</span>. `27668` by `Nolan McMahon <NolantheNerd>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">cluster.OPTICS</span> where the cluster correction based on predecessor was not using the right indexing. It would lead to inconsistent results depedendent on the order of the data. `26459` by `Haoying Zhang <stevezhang1999>` and `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improve error message when checking the number of connected components in the <span class="title-ref">fit</span> method of <span class="title-ref">cluster.HDBSCAN</span>. `27678` by `Ganesh Tata <tataganesh>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Create copy of precomputed sparse matrix within the <span class="title-ref">fit</span> method of <span class="title-ref">cluster.DBSCAN</span> to avoid in-place modification of the sparse matrix. `27651` by `Ganesh Tata <tataganesh>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raises a proper <span class="title-ref">ValueError</span> when <span class="title-ref">metric="precomputed"</span> and requested storing centers via the parameter <span class="title-ref">store\_centers</span>. `27898` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">kdtree</span> and <span class="title-ref">balltree</span> values are now deprecated and are renamed as <span class="title-ref">kd\_tree</span> and <span class="title-ref">ball\_tree</span> respectively for the <span class="title-ref">algorithm</span> parameter of <span class="title-ref">cluster.HDBSCAN</span> ensuring consistency in naming convention. <span class="title-ref">kdtree</span> and <span class="title-ref">balltree</span> values will be removed in 1.6. `26744` by `Shreesha Kumar Bhat <Shreesha3112>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The option <span class="title-ref">metric=None</span> in <span class="title-ref">cluster.AgglomerativeClustering</span> and <span class="title-ref">cluster.FeatureAgglomeration</span> is deprecated in version 1.4 and will be removed in version 1.6. Use the default value instead. `27828` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` Adds [polars](https://www.pola.rs) input support to <span class="title-ref">compose.ColumnTransformer</span> through the [DataFrame Interchange Protocol](https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html). The minimum supported version for polars is <span class="title-ref">0.19.12</span>. `26683` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.spectral\_clustering</span> and <span class="title-ref">cluster.SpectralClustering</span> now raise an explicit error message indicating that sparse matrices and arrays with <span class="title-ref">np.int64</span> indices are not supported. `27240` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` outputs that use pandas extension dtypes and contain <span class="title-ref">pd.NA</span> in <span class="title-ref">\~compose.ColumnTransformer</span> now result in a <span class="title-ref">FutureWarning</span> and will cause a <span class="title-ref">ValueError</span> in version 1.6, unless the output container has been configured as "pandas" with <span class="title-ref">set\_output(transform="pandas")</span>. Before, such outputs resulted in numpy arrays of dtype <span class="title-ref">object</span> containing <span class="title-ref">pd.NA</span> which could not be converted to numpy floats and caused errors when passed to other scikit-learn estimators. `27734` by `Jérôme Dockès <jeromedockes>`.

#### `sklearn.covariance`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Allow <span class="title-ref">covariance.shrunk\_covariance</span> to process multiple covariance matrices at once by handling nd-arrays. `25275` by `Quentin Barthélemy <qbarthelemy>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">\~compose.ColumnTransformer</span> now replaces <span class="title-ref">"passthrough"</span> with a corresponding <span class="title-ref">\~preprocessing.FunctionTransformer</span> in the fitted `transformers_` attribute. `27204` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">datasets.make\_sparse\_spd\_matrix</span> now uses a more memory-efficient sparse layout. It also accepts a new keyword <span class="title-ref">sparse\_format</span> that allows specifying the output format of the sparse matrix. By default <span class="title-ref">sparse\_format=None</span>, which returns a dense numpy ndarray as before. `27438` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">datasets.dump\_svmlight\_file</span> now does not raise <span class="title-ref">ValueError</span> when <span class="title-ref">X</span> is read-only, e.g., a <span class="title-ref">numpy.memmap</span> instance. `28111` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">datasets.make\_sparse\_spd\_matrix</span> deprecated the keyword argument `dim` in favor of `n_dim`. `dim` will be removed in version 1.6. `27718` by `Adam Li <adam2392>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">decomposition.PCA</span> now supports <span class="title-ref">scipy.sparse.sparray</span> and <span class="title-ref">scipy.sparse.spmatrix</span> inputs when using the <span class="title-ref">arpack</span> solver. When used on sparse data like <span class="title-ref">datasets.fetch\_20newsgroups\_vectorized</span> this can lead to speed-ups of 100x (single threaded) and 70x lower memory usage. Based on `Alexander Tarashansky <atarashansky>`'s implementation in [scanpy](https://github.com/scverse/scanpy). `18689` by `Isaac Virshup <ivirshup>` and `Andrey Portnoy <andportnoy>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` An "auto" option was added to the <span class="title-ref">n\_components</span> parameter of <span class="title-ref">decomposition.non\_negative\_factorization</span>, <span class="title-ref">decomposition.NMF</span> and <span class="title-ref">decomposition.MiniBatchNMF</span> to automatically infer the number of components from W or H shapes when using a custom initialization. The default value of this parameter will change from <span class="title-ref">None</span> to <span class="title-ref">auto</span> in version 1.6. `26634` by `Alexandre Landeau <AlexL>` and `Alexandre Vigny <avigny>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.dict\_learning\_online</span> does not ignore anymore the parameter <span class="title-ref">max\_iter</span>. `27834` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">degree</span> parameter in the <span class="title-ref">decomposition.KernelPCA</span> constructor now accepts real values instead of only integral values in accordance with the <span class="title-ref">degree</span> parameter of the <span class="title-ref">sklearn.metrics.pairwise.polynomial\_kernel</span>. `27668` by `Nolan McMahon <NolantheNerd>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The option <span class="title-ref">max\_iter=None</span> in <span class="title-ref">decomposition.MiniBatchDictionaryLearning</span>, <span class="title-ref">decomposition.MiniBatchSparsePCA</span>, and <span class="title-ref">decomposition.dict\_learning\_online</span> is deprecated and will be removed in version 1.6. Use the default value instead. `27834` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.RandomForestClassifier</span> and <span class="title-ref">ensemble.RandomForestRegressor</span> support missing values when the criterion is <span class="title-ref">gini</span>, <span class="title-ref">entropy</span>, or <span class="title-ref">log\_loss</span>, for classification or <span class="title-ref">squared\_error</span>, <span class="title-ref">friedman\_mse</span>, or <span class="title-ref">poisson</span> for regression. `26391` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> supports <span class="title-ref">categorical\_features="from\_dtype"</span>, which treats columns with Pandas or Polars Categorical dtype as categories in the algorithm. <span class="title-ref">categorical\_features="from\_dtype"</span> will become the default in v1.6. Categorical features no longer need to be encoded with numbers. When categorical features are numbers, the maximum value no longer needs to be smaller than <span class="title-ref">max\_bins</span>; only the number of (unique) categories must be smaller than <span class="title-ref">max\_bins</span>. `26411` by [Thomas Fan](https://github.com/thomasjpfan) and `27835` by `Jérôme Dockès <jeromedockes>`.
  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> got the new parameter <span class="title-ref">max\_features</span> to specify the proportion of randomly chosen features considered in each split. `27139` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span> now support monotonic constraints, useful when features are supposed to have a positive/negative effect on the target. Missing values in the train data and multi-output targets are not supported. `13649` by `Samuel Ronsin <samronsin>`, initiated by `Patrick O'Reilly <pat-oreilly>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> are now a bit faster by reusing the parent node's histogram as children node's histogram in the subtraction trick. In effect, less memory has to be allocated and deallocated. `27865` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.GradientBoostingClassifier</span> is faster, for binary and in particular for multiclass problems thanks to the private loss function module. `26278` and `28095` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improves runtime and memory usage for <span class="title-ref">ensemble.GradientBoostingClassifier</span> and <span class="title-ref">ensemble.GradientBoostingRegressor</span> when trained on sparse data. `26957` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> is now faster when <span class="title-ref">scoring</span> is a predefined metric listed in <span class="title-ref">metrics.get\_scorer\_names</span> and early stopping is enabled. `26163` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` A fitted property, `estimators_samples_`, was added to all Forest methods, including <span class="title-ref">ensemble.RandomForestClassifier</span>, <span class="title-ref">ensemble.RandomForestRegressor</span>, <span class="title-ref">ensemble.ExtraTreesClassifier</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span>, which allows to retrieve the training sample indices used for each tree estimator. `26736` by `Adam Li <adam2392>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes <span class="title-ref">ensemble.IsolationForest</span> when the input is a sparse matrix and <span class="title-ref">contamination</span> is set to a float value. `27645` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raises a <span class="title-ref">ValueError</span> in <span class="title-ref">ensemble.RandomForestRegressor</span> and <span class="title-ref">ensemble.ExtraTreesRegressor</span> when requesting OOB score with multioutput model for the targets being all rounded to integer. It was recognized as a multiclass problem. `27817` by `Daniele Ongari <danieleongari>`
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Changes estimator tags to acknowledge that <span class="title-ref">ensemble.VotingClassifier</span>, <span class="title-ref">ensemble.VotingRegressor</span>, <span class="title-ref">ensemble.StackingClassifier</span>, <span class="title-ref">ensemble.StackingRegressor</span>, support missing values if all <span class="title-ref">estimators</span> support missing values. `27710` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Support loading pickles of <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> when the pickle has been generated on a platform with a different bitness. A typical example is to train and pickle the model on 64 bit machine and load the model on a 32 bit machine for prediction. `28074` by `Christian Lorentzen <lorentzenchr>` and `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` In <span class="title-ref">ensemble.AdaBoostClassifier</span>, the <span class="title-ref">algorithm</span> argument <span class="title-ref">SAMME.R</span> was deprecated and will be removed in 1.6. `26830` by `Stefanie Senger <StefanieSenger>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Changed error type from <span class="title-ref">AttributeError</span> to <span class="title-ref">exceptions.NotFittedError</span> in unfitted instances of <span class="title-ref">feature\_extraction.DictVectorizer</span> for the following methods: <span class="title-ref">feature\_extraction.DictVectorizer.inverse\_transform</span>, <span class="title-ref">feature\_extraction.DictVectorizer.restrict</span>, <span class="title-ref">feature\_extraction.DictVectorizer.transform</span>. `24838` by `Lorenz Hertel <LoHertel>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_selection.SelectKBest</span>, <span class="title-ref">feature\_selection.SelectPercentile</span>, and <span class="title-ref">feature\_selection.GenericUnivariateSelect</span> now support unsupervised feature selection by providing a <span class="title-ref">score\_func</span> taking <span class="title-ref">X</span> and <span class="title-ref">y=None</span>. `27721` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_selection.SelectKBest</span> and <span class="title-ref">feature\_selection.GenericUnivariateSelect</span> with <span class="title-ref">mode='k\_best'</span> now shows a warning when <span class="title-ref">k</span> is greater than the number of features. `27841` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_selection.RFE</span> and <span class="title-ref">feature\_selection.RFECV</span> do not check for nans during input validation. `21807` by [Thomas Fan](https://github.com/thomasjpfan).

#### `sklearn.inspection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">inspection.DecisionBoundaryDisplay</span> now accepts a parameter <span class="title-ref">class\_of\_interest</span> to select the class of interest when plotting the response provided by <span class="title-ref">response\_method="predict\_proba"</span> or <span class="title-ref">response\_method="decision\_function"</span>. It allows to plot the decision boundary for both binary and multiclass classifiers. `27291` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">inspection.DecisionBoundaryDisplay.from\_estimator</span> and <span class="title-ref">inspection.PartialDependenceDisplay.from\_estimator</span> now return the correct type for subclasses. `27675` by `John Cant <johncant>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">inspection.DecisionBoundaryDisplay</span> raise an <span class="title-ref">AttributeError</span> instead of a <span class="title-ref">ValueError</span> when an estimator does not implement the requested response method. `27291` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.kernel_ridge`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">degree</span> parameter in the <span class="title-ref">kernel\_ridge.KernelRidge</span> constructor now accepts real values instead of only integral values in accordance with the <span class="title-ref">degree</span> parameter of the <span class="title-ref">sklearn.metrics.pairwise.polynomial\_kernel</span>. `27668` by `Nolan McMahon <NolantheNerd>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> now have much better convergence for solvers <span class="title-ref">"lbfgs"</span> and <span class="title-ref">"newton-cg"</span>. Both solvers can now reach much higher precision for the coefficients depending on the specified <span class="title-ref">tol</span>. Additionally, lbfgs can make better use of <span class="title-ref">tol</span>, i.e., stop sooner or reach higher precision. This is accomplished by better scaling of the objective function, i.e., using average per sample losses instead of sum of per sample losses. `26721` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> with solver <span class="title-ref">"newton-cg"</span> can now be considerably faster for some data and parameter settings. This is accomplished by a better line search convergence check for negligible loss improvements that takes into account gradient information. `26721` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Solver <span class="title-ref">"newton-cg"</span> in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> uses a little less memory. The effect is proportional to the number of coefficients (<span class="title-ref">n\_features \* n\_classes</span>). `27417` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Ensure that the <span class="title-ref">sigma\_</span> attribute of <span class="title-ref">linear\_model.ARDRegression</span> and <span class="title-ref">linear\_model.BayesianRidge</span> always has a <span class="title-ref">float32</span> dtype when fitted on <span class="title-ref">float32</span> data, even with the type promotion rules of NumPy 2. `27899` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The attribute <span class="title-ref">loss\_function\_</span> of <span class="title-ref">linear\_model.SGDClassifier</span> and <span class="title-ref">linear\_model.SGDOneClassSVM</span> has been deprecated and will be removed in version 1.6. `27979` by `Christian Lorentzen <lorentzenchr>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Computing pairwise distances via <span class="title-ref">metrics.DistanceMetric</span> for CSR x CSR, Dense x CSR, and CSR x Dense datasets is now 1.5x faster. `26765` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Computing distances via <span class="title-ref">metrics.DistanceMetric</span> for CSR x CSR, Dense x CSR, and CSR x Dense now uses \~50% less memory, and outputs distances in the same dtype as the provided data. `27006` by `Meekail Zain <micky774>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Improve the rendering of the plot obtained with the <span class="title-ref">metrics.PrecisionRecallDisplay</span> and <span class="title-ref">metrics.RocCurveDisplay</span> classes. the x- and y-axis limits are set to \[0, 1\] and the aspect ratio between both axis is set to be 1 to get a square plot. `26366` by `Mojdeh Rastgoo <mrastgoo>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Added <span class="title-ref">neg\_root\_mean\_squared\_log\_error\_scorer</span> as scorer `26734` by `Alejandro Martin Gil <101AlexMartin>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">metrics.confusion\_matrix</span> now warns when only one label was found in <span class="title-ref">y\_true</span> and <span class="title-ref">y\_pred</span>. `27650` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` computing pairwise distances with <span class="title-ref">metrics.pairwise.euclidean\_distances</span> no longer raises an exception when <span class="title-ref">X</span> is provided as a <span class="title-ref">float64</span> array and <span class="title-ref">X\_norm\_squared</span> as a <span class="title-ref">float32</span> array. `27624` by `Jérôme Dockès <jeromedockes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">f1\_score</span> now provides correct values when handling various cases in which division by zero occurs by using a formulation that does not depend on the precision and recall values. `27577` by `Omar Salman <OmarManzoor>` and `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.make\_scorer</span> now raises an error when using a regressor on a scorer requesting a non-thresholded decision function (from <span class="title-ref">decision\_function</span> or <span class="title-ref">predict\_proba</span>). Such scorer are specific to classification. `26840` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">metrics.DetCurveDisplay.from\_predictions</span>, <span class="title-ref">metrics.PrecisionRecallDisplay.from\_predictions</span>, <span class="title-ref">metrics.PredictionErrorDisplay.from\_predictions</span>, and <span class="title-ref">metrics.RocCurveDisplay.from\_predictions</span> now return the correct type for subclasses. `27675` by `John Cant <johncant>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecated <span class="title-ref">needs\_threshold</span> and <span class="title-ref">needs\_proba</span> from <span class="title-ref">metrics.make\_scorer</span>. These parameters will be removed in version 1.6. Instead, use <span class="title-ref">response\_method</span> that accepts <span class="title-ref">"predict"</span>, <span class="title-ref">"predict\_proba"</span> or <span class="title-ref">"decision\_function"</span> or a list of such values. <span class="title-ref">needs\_proba=True</span> is equivalent to <span class="title-ref">response\_method="predict\_proba"</span> and <span class="title-ref">needs\_threshold=True</span> is equivalent to <span class="title-ref">response\_method=("decision\_function", "predict\_proba")</span>. `26840` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The <span class="title-ref">squared</span> parameter of <span class="title-ref">metrics.mean\_squared\_error</span> and <span class="title-ref">metrics.mean\_squared\_log\_error</span> is deprecated and will be removed in 1.6. Use the new functions <span class="title-ref">metrics.root\_mean\_squared\_error</span> and <span class="title-ref">metrics.root\_mean\_squared\_log\_error</span> instead. `26734` by `Alejandro Martin Gil <101AlexMartin>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">model\_selection.learning\_curve</span> raises a warning when every cross validation fold fails. `26299` by `Rahil Parikh <rprkh>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span>, and <span class="title-ref">model\_selection.HalvingGridSearchCV</span> now don't change the given object in the parameter grid if it's an estimator. `26786` by [Adrin Jalali](https://github.com/adrinjalali).

#### `sklearn.multioutput`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Add method <span class="title-ref">predict\_log\_proba</span> to <span class="title-ref">multioutput.ClassifierChain</span>. `27720` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">sklearn.neighbors.KNeighborsRegressor.predict</span> and <span class="title-ref">sklearn.neighbors.KNeighborsClassifier.predict\_proba</span> now efficiently support pairs of dense and sparse datasets. `27018` by `Julien Jerphanion <jjerphan>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The performance of <span class="title-ref">neighbors.RadiusNeighborsClassifier.predict</span> and of <span class="title-ref">neighbors.RadiusNeighborsClassifier.predict\_proba</span> has been improved when <span class="title-ref">radius</span> is large and <span class="title-ref">algorithm="brute"</span> with non-Euclidean metrics. `26828` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Improve error message for <span class="title-ref">neighbors.LocalOutlierFactor</span> when it is invoked with <span class="title-ref">n\_samples=n\_neighbors</span>. `23317` by `Bharat Raghunathan <bharatr21>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">neighbors.KNeighborsClassifier.predict</span> and <span class="title-ref">neighbors.KNeighborsClassifier.predict\_proba</span> now raises an error when the weights of all neighbors of some sample are zero. This can happen when <span class="title-ref">weights</span> is a user-defined function. `26410` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">neighbors.KNeighborsRegressor</span> now accepts <span class="title-ref">metrics.DistanceMetric</span> objects directly via the <span class="title-ref">metric</span> keyword argument allowing for the use of accelerated third-party <span class="title-ref">metrics.DistanceMetric</span> objects. `26267` by `Meekail Zain <micky774>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">preprocessing.OrdinalEncoder</span> avoids calculating missing indices twice to improve efficiency. `27017` by `Xuefeng Xu <xuefeng-xu>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improves efficiency in <span class="title-ref">preprocessing.OneHotEncoder</span> and <span class="title-ref">preprocessing.OrdinalEncoder</span> in checking <span class="title-ref">nan</span>. `27760` by `Xuefeng Xu <xuefeng-xu>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Improves warnings in <span class="title-ref">preprocessing.FunctionTransformer</span> when <span class="title-ref">func</span> returns a pandas dataframe and the output is configured to be pandas. `26944` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">preprocessing.TargetEncoder</span> now supports <span class="title-ref">target\_type</span> 'multiclass'. `26674` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OneHotEncoder</span> and <span class="title-ref">preprocessing.OrdinalEncoder</span> raise an exception when <span class="title-ref">nan</span> is a category and is not the last in the user's provided categories. `27309` by `Xuefeng Xu <xuefeng-xu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.OneHotEncoder</span> and <span class="title-ref">preprocessing.OrdinalEncoder</span> raise an exception if the user provided categories contain duplicates. `27328` by `Xuefeng Xu <xuefeng-xu>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">preprocessing.FunctionTransformer</span> raises an error at <span class="title-ref">transform</span> if the output of <span class="title-ref">get\_feature\_names\_out</span> is not consistent with the column names of the output container if those are defined. `27801` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise a <span class="title-ref">NotFittedError</span> in <span class="title-ref">preprocessing.OrdinalEncoder</span> when calling <span class="title-ref">transform</span> without calling <span class="title-ref">fit</span> since <span class="title-ref">categories</span> always requires to be checked. `27821` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">tree.DecisionTreeClassifier</span>, <span class="title-ref">tree.DecisionTreeRegressor</span>, <span class="title-ref">tree.ExtraTreeClassifier</span> and <span class="title-ref">tree.ExtraTreeRegressor</span> now support monotonic constraints, useful when features are supposed to have a positive/negative effect on the target. Missing values in the train data and multi-output targets are not supported. `13649` by `Samuel Ronsin <samronsin>`, initiated by `Patrick O'Reilly <pat-oreilly>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">sklearn.utils.estimator\_html\_repr</span> dynamically adapts diagram colors based on the browser's <span class="title-ref">prefers-color-scheme</span>, providing improved adaptability to dark mode environments. `26862` by `Andrew Goh Yisheng <9y5>`, [Thomas Fan](https://github.com/thomasjpfan), [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">\~utils.metadata\_routing.MetadataRequest</span> and <span class="title-ref">\~utils.metadata\_routing.MetadataRouter</span> now have a `consumes` method which can be used to check whether a given set of parameters would be consumed. `26831` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Make <span class="title-ref">sklearn.utils.check\_array</span> attempt to output <span class="title-ref">int32</span>-indexed CSR and COO arrays when converting from DIA arrays if the number of non-zero entries is small enough. This ensures that estimators implemented in Cython and that do not accept <span class="title-ref">int64</span>-indexed sparse datastucture, now consistently accept the same sparse input formats for SciPy sparse matrices and arrays. `27372` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">sklearn.utils.check\_array</span> should accept both matrix and array from the sparse SciPy module. The previous implementation would fail if <span class="title-ref">copy=True</span> by calling specific NumPy <span class="title-ref">np.may\_share\_memory</span> that does not work with SciPy sparse array and does not return the correct result for SciPy sparse matrix. `27336` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">\~utils.estimator\_checks.check\_estimators\_pickle</span> with <span class="title-ref">readonly\_memmap=True</span> now relies on joblib's own capability to allocate aligned memory mapped arrays when loading a serialized estimator instead of calling a dedicated private function that would crash when OpenBLAS misdetects the CPU architecture. `27614` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Error message in <span class="title-ref">\~utils.check\_array</span> when a sparse matrix was passed but <span class="title-ref">accept\_sparse</span> is <span class="title-ref">False</span> now suggests to use <span class="title-ref">.toarray()</span> and not <span class="title-ref">X.toarray()</span>. `27757` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix the function <span class="title-ref">\~utils.check\_array</span> to output the right error message when the input is a Series instead of a DataFrame. `28090` by `Stan Furrer <stanFurrer>` and `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">sklearn.extmath.log\_logistic</span> is deprecated and will be removed in 1.6. Use <span class="title-ref">-np.logaddexp(0, -x)</span> instead. `27544` by `Christian Lorentzen <lorentzenchr>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.3, including:

101AlexMartin, Abhishek Singh Kushwah, Adam Li, Adarsh Wase, Adrin Jalali, Advik Sinha, Alex, Alexander Al-Feghali, Alexis IMBERT, AlexL, Alex Molas, Anam Fatima, Andrew Goh, andyscanzio, Aniket Patil, Artem Kislovskiy, Arturo Amor, ashah002, avm19, Ben Holmes, Ben Mares, Benoit Chevallier-Mames, Bharat Raghunathan, Binesh Bannerjee, Brendan Lu, Brevin Kunde, Camille Troillard, Carlo Lemos, Chad Parmet, Christian Clauss, Christian Lorentzen, Christian Veenhuis, Christos Aridas, Cindy Liang, Claudio Salvatore Arcidiacono, Connor Boyle, cynthias13w, DaminK, Daniele Ongari, Daniel Schmitz, Daniel Tinoco, David Brochart, Deborah L. Haar, DevanshKyada27, Dimitri Papadopoulos Orfanos, Dmitry Nesterov, DUONG, Edoardo Abati, Eitan Hemed, Elabonga Atuo, Elisabeth Günther, Emma Carballal, Emmanuel Ferdman, epimorphic, Erwan Le Floch, Fabian Egli, Filip Karlo Došilović, Florian Idelberger, Franck Charras, Gael Varoquaux, Ganesh Tata, Gleb Levitski, Guillaume Lemaitre, Haoying Zhang, Harmanan Kohli, Ily, ioangatop, IsaacTrost, Isaac Virshup, Iwona Zdzieblo, Jakub Kaczmarzyk, James McDermott, Jarrod Millman, JB Mountford, Jérémie du Boisberranger, Jérôme Dockès, Jiawei Zhang, Joel Nothman, John Cant, John Hopfensperger, Jona Sassenhagen, Jon Nordby, Julien Jerphanion, Kennedy Waweru, kevin moore, Kian Eliasi, Kishan Ved, Konstantinos Pitas, Koustav Ghosh, Kushan Sharma, ldwy4, Linus, Lohit SundaramahaLingam, Loic Esteve, Lorenz, Louis Fouquet, Lucy Liu, Luis Silvestrin, Lukáš Folwarczný, Lukas Geiger, Malte Londschien, Marcus Fraaß, Marek Hanuš, Maren Westermann, Mark Elliot, Martin Larralde, Mateusz Sokół, mathurinm, mecopur, Meekail Zain, Michael Higgins, Miki Watanabe, Milton Gomez, MN193, Mohammed Hamdy, Mohit Joshi, mrastgoo, Naman Dhingra, Naoise Holohan, Narendra Singh dangi, Noa Malem-Shinitski, Nolan, Nurseit Kamchyev, Oleksii Kachaiev, Olivier Grisel, Omar Salman, partev, Peter Hull, Peter Steinbach, Pierre de Fréminville, Pooja Subramaniam, Puneeth K, qmarcou, Quentin Barthélemy, Rahil Parikh, Rahul Mahajan, Raj Pulapakura, Raphael, Ricardo Peres, Riccardo Cappuzzo, Roman Lutz, Salim Dohri, Samuel O. Ronsin, Sandip Dutta, Sayed Qaiser Ali, scaja, scikit-learn-bot, Sebastian Berg, Shreesha Kumar Bhat, Shubhal Gupta, Søren Fuglede Jørgensen, Stefanie Senger, Tamara, Tanjina Afroj, THARAK HEGDE, thebabush, Thomas J. Fan, Thomas Roehr, Tialo, Tim Head, tongyu, Venkatachalam N, Vijeth Moudgalya, Vincent M, Vivek Reddy P, Vladimir Fokow, Xiao Yuan, Xuefeng Xu, Yang Tao, Yao Xiao, Yuchen Zhou, Yuusuke Hiramatsu

---

v1.5.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.5

For a short description of the main highlights of the release, please refer to \[sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_5\_0.py\](\#sphx\_glr\_auto\_examples\_release\_highlights\_plot\_release\_highlights\_1\_5\_0.py).

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

## Version 1.5.2

**September 2024**

### Changes impacting many modules

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed performance regression in a few Cython modules in <span class="title-ref">sklearn.\_loss</span>, <span class="title-ref">sklearn.manifold</span>, <span class="title-ref">sklearn.metrics</span> and <span class="title-ref">sklearn.utils</span>, which were built without OpenMP support. `29694` by `Loïc Estèvce <lesteve>`.

### Changelog

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise error when <span class="title-ref">\~sklearn.model\_selection.LeaveOneOut</span> used in <span class="title-ref">cv</span>, matching what would happen if <span class="title-ref">KFold(n\_splits=n\_samples)</span> was used. `29545` by `Lucy Liu <lucyleeow>`

#### `sklearn.compose`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed <span class="title-ref">compose.TransformedTargetRegressor</span> not to raise <span class="title-ref">UserWarning</span> if transform output is set to <span class="title-ref">pandas</span> or <span class="title-ref">polars</span>, since it isn't a transformer. `29401` by `Stefanie Senger <StefanieSenger>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Increase rank defficiency threshold in the whitening step of <span class="title-ref">decomposition.FastICA</span> with <span class="title-ref">whiten\_solver="eigh"</span> to improve the platform-agnosticity of the estimator. `29612` by `Olivier Grisel <ogrisel>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">metrics.accuracy\_score</span> and in <span class="title-ref">metrics.zero\_one\_loss</span> causing an error for Array API dispatch with multilabel inputs. `29336` by `Edoardo Abati <EdAbati>`.

#### `sklearn.svm`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in <span class="title-ref">svm.SVC</span> and <span class="title-ref">svm.SVR</span> such that we accept <span class="title-ref">C=float("inf")</span>. `29780` by `Guillaume Lemaitre <glemaitre>`.

## Version 1.5.1

**July 2024**

### Changes impacting many modules

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in the validation of the input data of all estimators where an unexpected error was raised when passing a DataFrame backed by a read-only buffer. `29018` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression causing a dead-lock at import time in some settings. `29235` by `Jérémie du Boisberranger <jeremiedbb>`.

### Changelog

#### `sklearn.compose`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Fix a performance regression in <span class="title-ref">compose.ColumnTransformer</span> where the full input data was copied for each transformer when <span class="title-ref">n\_jobs \> 1</span>. `29330` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.metrics`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">metrics.r2\_score</span>. Passing torch CPU tensors with array API dispatched disabled would complain about non-CPU devices instead of implicitly converting those inputs as regular NumPy arrays. `29119` by `Olivier Grisel`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">metrics.zero\_one\_loss</span> causing an error for Array API dispatch with multilabel inputs. `29269` by `Yaroslav Korobko <Tialo>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">model\_selection.GridSearchCV</span> for parameter grids that have heterogeneous parameter values. `29078` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">model\_selection.GridSearchCV</span> for parameter grids that have estimators as parameter values. `29179` by `Marco Gorelli<MarcoGorelli>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix a regression in <span class="title-ref">model\_selection.GridSearchCV</span> for parameter grids that have arrays of different sizes as parameter values. `29314` by `Marco Gorelli<MarcoGorelli>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix an issue in <span class="title-ref">tree.export\_graphviz</span> and <span class="title-ref">tree.plot\_tree</span> that could potentially result in exception or wrong results on 32bit OSes. `29327` by `Loïc Estève<lesteve>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.validation.check\_array</span> has a new parameter, <span class="title-ref">force\_writeable</span>, to control the writeability of the output array. If set to <span class="title-ref">True</span>, the output array will be guaranteed to be writeable and a copy will be made if the input array is read-only. If set to <span class="title-ref">False</span>, no guarantee is made about the writeability of the output array. `29018` by `Jérémie du Boisberranger <jeremiedbb>`.

## Version 1.5.0

**May 2024**

### Security

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">feature\_extraction.text.CountVectorizer</span> and <span class="title-ref">feature\_extraction.text.TfidfVectorizer</span> no longer store discarded tokens from the training set in their <span class="title-ref">stop\_words\_</span> attribute. This attribute would hold too frequent (above <span class="title-ref">max\_df</span>) but also too rare tokens (below <span class="title-ref">min\_df</span>). This fixes a potential security issue (data leak) if the discarded rare tokens hold sensitive information from the training set without the model developer's knowledge.
    
    Note: users of those classes are encouraged to either retrain their pipelines with the new scikit-learn version or to manually clear the <span class="title-ref">stop\_words\_</span> attribute from previously trained instances of those transformers. This attribute was designed only for model inspection purposes and has no impact on the behavior of the transformers. `28823` by `Olivier Grisel <ogrisel>`.

### Changed models

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` The subsampling in <span class="title-ref">preprocessing.QuantileTransformer</span> is now more efficient for dense arrays but the fitted quantiles and the results of <span class="title-ref">transform</span> may be slightly different than before (keeping the same statistical properties). `27344` by `Xuefeng Xu <xuefeng-xu>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.PCA</span>, <span class="title-ref">decomposition.SparsePCA</span> and <span class="title-ref">decomposition.TruncatedSVD</span> now set the sign of the <span class="title-ref">components\_</span> attribute based on the component values instead of using the transformed data as reference. This change is needed to be able to offer consistent component signs across all <span class="title-ref">PCA</span> solvers, including the new <span class="title-ref">svd\_solver="covariance\_eigh"</span> option introduced in this release.

### Changes impacting many modules

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Raise <span class="title-ref">ValueError</span> with an informative error message when passing 1D sparse arrays to methods that expect 2D sparse inputs. `28988` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` The name of the input of the <span class="title-ref">inverse\_transform</span> method of estimators has been standardized to <span class="title-ref">X</span>. As a consequence, <span class="title-ref">Xt</span> is deprecated and will be removed in version 1.7 in the following estimators: <span class="title-ref">cluster.FeatureAgglomeration</span>, <span class="title-ref">decomposition.MiniBatchNMF</span>, <span class="title-ref">decomposition.NMF</span>, <span class="title-ref">model\_selection.GridSearchCV</span>, <span class="title-ref">model\_selection.RandomizedSearchCV</span>, <span class="title-ref">pipeline.Pipeline</span> and <span class="title-ref">preprocessing.KBinsDiscretizer</span>. `28756` by `Will Dean <wd60622>`.

### Support for Array API

Additional estimators and functions have been updated to include support for all [Array API](https://data-apis.org/array-api/latest/) compliant inputs.

See \[array\_api\](\#array\_api) for more details.

**Functions:**

  - <span class="title-ref">sklearn.metrics.r2\_score</span> now supports Array API compliant inputs. `27904` by `Eric Lindgren <elindgren>`, `Franck Charras <fcharras>`, `Olivier Grisel <ogrisel>` and `Tim Head <betatim>`.

**Classes:**

  - <span class="title-ref">linear\_model.Ridge</span> now supports the Array API for the <span class="title-ref">svd</span> solver. See \[array\_api\](\#array\_api) for more details. `27800` by `Franck Charras <fcharras>`, `Olivier Grisel <ogrisel>` and `Tim Head <betatim>`.

### Support for building with Meson

From scikit-learn 1.5 onwards, Meson is the main supported way to build scikit-learn, see \[Building from source \<install\_bleeding\_edge\>\](\#building-from-source-\<install\_bleeding\_edge\>) for more details.

Unless we discover a major blocker, setuptools support will be dropped in scikit-learn 1.6. The 1.5.x releases will support building scikit-learn with setuptools.

Meson support for building scikit-learn was added in `28040` by `Loïc Estève <lesteve>`

### Metadata Routing

The following models now support metadata routing in one or more or their methods. Refer to the \[Metadata Routing User Guide \<metadata\_routing\>\](\#metadata-routing-user-guide-\<metadata\_routing\>) for more details.

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">impute.IterativeImputer</span> now supports metadata routing in its <span class="title-ref">fit</span> method. `28187` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.BaggingClassifier</span> and <span class="title-ref">ensemble.BaggingRegressor</span> now support metadata routing. The fit methods now accept `**fit_params` which are passed to the underlying estimators via their <span class="title-ref">fit</span> methods. `28432` by `Adam Li <adam2392>` and `Benjamin Bossan <BenjaminBossan>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span> now support metadata routing in their <span class="title-ref">fit</span> method and route metadata to the underlying <span class="title-ref">model\_selection.GridSearchCV</span> object or the underlying scorer. `27560` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">GraphicalLassoCV</span> now supports metadata routing in it's <span class="title-ref">fit</span> method and routes metadata to the CV splitter. `27566` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">linear\_model.RANSACRegressor</span> now supports metadata routing in its `fit`, `score` and `predict` methods and route metadata to its underlying estimator's' `fit`, `score` and `predict` methods. `28261` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">ensemble.VotingClassifier</span> and <span class="title-ref">ensemble.VotingRegressor</span> now support metadata routing and pass `**fit_params` to the underlying estimators via their <span class="title-ref">fit</span> methods. `27584` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">pipeline.FeatureUnion</span> now supports metadata routing in its `fit` and `fit_transform` methods and route metadata to the underlying transformers' `fit` and `fit_transform`. `28205` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix an issue when resolving default routing requests set via class attributes. `28435` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix an issue when <span class="title-ref">set\_{method}\_request</span> methods are used as unbound methods, which can happen if one tries to decorate them. `28651` by [Adrin Jalali](https://github.com/adrinjalali).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Prevent a <span class="title-ref">RecursionError</span> when estimators with the default <span class="title-ref">scoring</span> param (<span class="title-ref">None</span>) route metadata. `28712` by `Stefanie Senger <StefanieSenger>`.

### Changelog

#### `sklearn.calibration`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed a regression in <span class="title-ref">calibration.CalibratedClassifierCV</span> where an error was wrongly raised with string targets. `28843` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.cluster`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">cluster.MeanShift</span> class now properly converges for constant data. `28951` by `Akihiro Kuno <akikuno>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Create copy of precomputed sparse matrix within the <span class="title-ref">fit</span> method of <span class="title-ref">\~cluster.OPTICS</span> to avoid in-place modification of the sparse matrix. `28491` by `Thanh Lam Dang <lamdang2k>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">cluster.HDBSCAN</span> now supports all metrics supported by <span class="title-ref">sklearn.metrics.pairwise\_distances</span> when <span class="title-ref">algorithm="brute"</span> or <span class="title-ref">"auto"</span>. `28664` by `Manideep Yenugula <myenugula>`.

#### `sklearn.compose`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` A fitted <span class="title-ref">compose.ColumnTransformer</span> now implements <span class="title-ref">\_\_getitem\_\_</span> which returns the fitted transformers by name. `27990` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">compose.TransformedTargetRegressor</span> now raises an error in <span class="title-ref">fit</span> if only <span class="title-ref">inverse\_func</span> is provided without <span class="title-ref">func</span> (that would default to identity) being explicitly set as well. `28483` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">compose.ColumnTransformer</span> can now expose the "remainder" columns in the fitted <span class="title-ref">transformers\_</span> attribute as column names or boolean masks, rather than column indices. `27657` by `Jérôme Dockès <jeromedockes>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixed an bug in <span class="title-ref">compose.ColumnTransformer</span> with <span class="title-ref">n\_jobs \> 1</span>, where the intermediate selected columns were passed to the transformers as read-only arrays. `28822` by `Jérémie du Boisberranger <jeremiedbb>`.

#### `sklearn.cross_decomposition`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">coef\_</span> fitted attribute of <span class="title-ref">cross\_decomposition.PLSRegression</span> now takes into account both the scale of <span class="title-ref">X</span> and <span class="title-ref">Y</span> when <span class="title-ref">scale=True</span>. Note that the previous predicted values were not affected by this bug. `28612` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">Y</span> in favor of <span class="title-ref">y</span> in the methods fit, transform and inverse\_transform of: <span class="title-ref">cross\_decomposition.PLSRegression</span>. <span class="title-ref">cross\_decomposition.PLSCanonical</span>, <span class="title-ref">cross\_decomposition.CCA</span>, and <span class="title-ref">cross\_decomposition.PLSSVD</span>. <span class="title-ref">Y</span> will be removed in version 1.7. `28604` by `David Leon <davidleon123>`.

#### `sklearn.datasets`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Adds optional arguments <span class="title-ref">n\_retries</span> and <span class="title-ref">delay</span> to functions <span class="title-ref">datasets.fetch\_20newsgroups</span>, <span class="title-ref">datasets.fetch\_20newsgroups\_vectorized</span>, <span class="title-ref">datasets.fetch\_california\_housing</span>, <span class="title-ref">datasets.fetch\_covtype</span>, <span class="title-ref">datasets.fetch\_kddcup99</span>, <span class="title-ref">datasets.fetch\_lfw\_pairs</span>, <span class="title-ref">datasets.fetch\_lfw\_people</span>, <span class="title-ref">datasets.fetch\_olivetti\_faces</span>, <span class="title-ref">datasets.fetch\_rcv1</span>, and <span class="title-ref">datasets.fetch\_species\_distributions</span>. By default, the functions will retry up to 3 times in case of network failures. `28160` by `Zhehao Liu <MaxwellLZH>` and `Filip Karlo Došilović <fkdosilovic>`.

#### `sklearn.decomposition`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">decomposition.PCA</span> with <span class="title-ref">svd\_solver="full"</span> now assigns a contiguous <span class="title-ref">components\_</span> attribute instead of an non-contiguous slice of the singular vectors. When <span class="title-ref">n\_components \<\< n\_features</span>, this can save some memory and, more importantly, help speed-up subsequent calls to the <span class="title-ref">transform</span> method by more than an order of magnitude by leveraging cache locality of BLAS GEMM on contiguous arrays. `27491` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">\~decomposition.PCA</span> now automatically selects the ARPACK solver for sparse inputs when <span class="title-ref">svd\_solver="auto"</span> instead of raising an error. `28498` by `Thanh Lam Dang <lamdang2k>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">decomposition.PCA</span> now supports a new solver option named <span class="title-ref">svd\_solver="covariance\_eigh"</span> which offers an order of magnitude speed-up and reduced memory usage for datasets with a large number of data points and a small number of features (say, <span class="title-ref">n\_samples \>\> 1000 \> n\_features</span>). The <span class="title-ref">svd\_solver="auto"</span> option has been updated to use the new solver automatically for such datasets. This solver also accepts sparse input data. `27491` by `Olivier Grisel <ogrisel>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">decomposition.PCA</span> fit with <span class="title-ref">svd\_solver="arpack"</span>, <span class="title-ref">whiten=True</span> and a value for <span class="title-ref">n\_components</span> that is larger than the rank of the training set, no longer returns infinite values when transforming hold-out data. `27491` by `Olivier Grisel <ogrisel>`.

#### `sklearn.dummy`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">dummy.DummyClassifier</span> and <span class="title-ref">dummy.DummyRegressor</span> now have the <span class="title-ref">n\_features\_in\_</span> and <span class="title-ref">feature\_names\_in\_</span> attributes after <span class="title-ref">fit</span>. `27937` by `Marco vd Boom <tvdboom>`.

#### `sklearn.ensemble`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improves runtime of <span class="title-ref">predict</span> of <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> by avoiding to call <span class="title-ref">predict\_proba</span>. `27844` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> are now a tiny bit faster by pre-sorting the data before finding the thresholds for binning. `28102` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes a bug in <span class="title-ref">ensemble.HistGradientBoostingClassifier</span> and <span class="title-ref">ensemble.HistGradientBoostingRegressor</span> when <span class="title-ref">monotonic\_cst</span> is specified for non-categorical features. `28925` by `Xiao Yuan <yuanx749>`.

#### `sklearn.feature_extraction`

  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` <span class="title-ref">feature\_extraction.text.TfidfTransformer</span> is now faster and more memory-efficient by using a NumPy vector instead of a sparse matrix for storing the inverse document frequency. `18843` by `Paolo Montesel <thebabush>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_extraction.text.TfidfTransformer</span> now preserves the data type of the input matrix if it is <span class="title-ref">np.float64</span> or <span class="title-ref">np.float32</span>. `28136` by `Guillaume Lemaitre <glemaitre>`.

#### `sklearn.feature_selection`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">feature\_selection.mutual\_info\_regression</span> and <span class="title-ref">feature\_selection.mutual\_info\_classif</span> now support <span class="title-ref">n\_jobs</span> parameter. `28085` by `Neto Menoci <netomenoci>` and `Florin Andrei <FlorinAndrei>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The <span class="title-ref">cv\_results\_</span> attribute of <span class="title-ref">feature\_selection.RFECV</span> has a new key, <span class="title-ref">n\_features</span>, containing an array with the number of features selected at each step. `28670` by `Miguel Silva <miguelcsilva>`.

#### `sklearn.impute`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">impute.SimpleImputer</span> now supports custom strategies by passing a function in place of a strategy name. `28053` by `Mark Elliot <mark-thm>`.

#### `sklearn.inspection`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">inspection.DecisionBoundaryDisplay.from\_estimator</span> no longer warns about missing feature names when provided a <span class="title-ref">polars.DataFrame</span>. `28718` by `Patrick Wang <patrickkwang>`.

#### `sklearn.linear_model`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Solver <span class="title-ref">"newton-cg"</span> in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span> now emits information when <span class="title-ref">verbose</span> is set to positive values. `27526` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.ElasticNet</span>, <span class="title-ref">linear\_model.ElasticNetCV</span>, <span class="title-ref">linear\_model.Lasso</span> and <span class="title-ref">linear\_model.LassoCV</span> now explicitly don't accept large sparse data formats. `27576` by `Stefanie Senger <StefanieSenger>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">RidgeClassifierCV</span> correctly pass <span class="title-ref">sample\_weight</span> to the underlying scorer when <span class="title-ref">cv</span> is None. `27560` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">n\_nonzero\_coefs\_</span> attribute in <span class="title-ref">linear\_model.OrthogonalMatchingPursuit</span> will now always be <span class="title-ref">None</span> when <span class="title-ref">tol</span> is set, as <span class="title-ref">n\_nonzero\_coefs</span> is ignored in this case. `28557` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">linear\_model.RidgeCV</span> and <span class="title-ref">linear\_model.RidgeClassifierCV</span> will now allow <span class="title-ref">alpha=0</span> when <span class="title-ref">cv \!= None</span>, which is consistent with <span class="title-ref">linear\_model.Ridge</span> and <span class="title-ref">linear\_model.RidgeClassifier</span>. `28425` by `Lucy Liu <lucyleeow>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Passing <span class="title-ref">average=0</span> to disable averaging is deprecated in <span class="title-ref">linear\_model.PassiveAggressiveClassifier</span>, <span class="title-ref">linear\_model.PassiveAggressiveRegressor</span>, <span class="title-ref">linear\_model.SGDClassifier</span>, <span class="title-ref">linear\_model.SGDRegressor</span> and <span class="title-ref">linear\_model.SGDOneClassSVM</span>. Pass <span class="title-ref">average=False</span> instead. `28582` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Parameter <span class="title-ref">multi\_class</span> was deprecated in <span class="title-ref">linear\_model.LogisticRegression</span> and <span class="title-ref">linear\_model.LogisticRegressionCV</span>. <span class="title-ref">multi\_class</span> will be removed in 1.7, and internally, for 3 and more classes, it will always use multinomial. If you still want to use the one-vs-rest scheme, you can use <span class="title-ref">OneVsRestClassifier(LogisticRegression(..))</span>. `28703` by `Christian Lorentzen <lorentzenchr>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">store\_cv\_values</span> and <span class="title-ref">cv\_values\_</span> are deprecated in favor of <span class="title-ref">store\_cv\_results</span> and <span class="title-ref">cv\_results\_</span> in <span class="title-ref">\~linear\_model.RidgeCV</span> and <span class="title-ref">\~linear\_model.RidgeClassifierCV</span>. `28915` by `Lucy Liu <lucyleeow>`.

#### `sklearn.manifold`

  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Deprecates <span class="title-ref">n\_iter</span> in favor of <span class="title-ref">max\_iter</span> in <span class="title-ref">manifold.TSNE</span>. <span class="title-ref">n\_iter</span> will be removed in version 1.7. This makes <span class="title-ref">manifold.TSNE</span> consistent with the rest of the estimators. `28471` by `Lucy Liu <lucyleeow>`

#### `sklearn.metrics`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.pairwise\_distances</span> accepts calculating pairwise distances for non-numeric arrays as well. This is supported through custom metrics only. `27456` by `Venkatachalam N <venkyyuvy>`, `Kshitij Mathur <Kshitij68>` and `Julian Libiseller-Egger <julibeg>`.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">sklearn.metrics.check\_scoring</span> now returns a multi-metric scorer when <span class="title-ref">scoring</span> as a <span class="title-ref">dict</span>, <span class="title-ref">set</span>, <span class="title-ref">tuple</span>, or <span class="title-ref">list</span>. `28360` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">metrics.d2\_log\_loss\_score</span> has been added which calculates the D^2 score for the log loss. `28351` by `Omar Salman <OmarManzoor>`.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` Improve efficiency of functions <span class="title-ref">\~metrics.brier\_score\_loss</span>, <span class="title-ref">\~calibration.calibration\_curve</span>, <span class="title-ref">\~metrics.det\_curve</span>, <span class="title-ref">\~metrics.precision\_recall\_curve</span>, <span class="title-ref">\~metrics.roc\_curve</span> when <span class="title-ref">pos\_label</span> argument is specified. Also improve efficiency of methods <span class="title-ref">from\_estimator</span> and <span class="title-ref">from\_predictions</span> in <span class="title-ref">\~metrics.RocCurveDisplay</span>, <span class="title-ref">\~metrics.PrecisionRecallDisplay</span>, <span class="title-ref">\~metrics.DetCurveDisplay</span>, <span class="title-ref">\~calibration.CalibrationDisplay</span>. `28051` by `Pierre de Fréminville <pidefrem>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}`<span class="title-ref">metrics.classification\_report</span> now shows only accuracy and not micro-average when input is a subset of labels. `28399` by `Vineet Joshi <vjoshi253>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fix OpenBLAS 0.3.26 dead-lock on Windows in pairwise distances computation. This is likely to affect neighbor-based algorithms. `28692` by `Loïc Estève <lesteve>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.precision\_recall\_curve</span> deprecated the keyword argument <span class="title-ref">probas\_pred</span> in favor of <span class="title-ref">y\_score</span>. <span class="title-ref">probas\_pred</span> will be removed in version 1.7. `28092` by `Adam Li <adam2392>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">metrics.brier\_score\_loss</span> deprecated the keyword argument <span class="title-ref">y\_prob</span> in favor of <span class="title-ref">y\_proba</span>. <span class="title-ref">y\_prob</span> will be removed in version 1.7. `28092` by `Adam Li <adam2392>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` For classifiers and classification metrics, labels encoded as bytes is deprecated and will raise an error in v1.7. `18555` by `Kaushik Amar Das <cozek>`.

#### `sklearn.mixture`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` The <span class="title-ref">converged\_</span> attribute of <span class="title-ref">mixture.GaussianMixture</span> and <span class="title-ref">mixture.BayesianGaussianMixture</span> now reflects the convergence status of the best fit whereas it was previously <span class="title-ref">True</span> if any of the fits converged. `26837` by `Krsto Proroković <krstopro>`.

#### `sklearn.model_selection`

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` <span class="title-ref">model\_selection.TunedThresholdClassifierCV</span> finds the decision threshold of a binary classifier that maximizes a classification metric through cross-validation. <span class="title-ref">model\_selection.FixedThresholdClassifier</span> is an alternative when one wants to use a fixed decision threshold without any tuning scheme. `26120` by `Guillaume Lemaitre <glemaitre>`.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` `CV splitters <CV splitter>` that ignores the group parameter now raises a warning when groups are passed in to `split`. `28210` by [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` The HTML diagram representation of <span class="title-ref">\~model\_selection.GridSearchCV</span>, <span class="title-ref">\~model\_selection.RandomizedSearchCV</span>, <span class="title-ref">\~model\_selection.HalvingGridSearchCV</span>, and <span class="title-ref">\~model\_selection.HalvingRandomSearchCV</span> will show the best estimator when <span class="title-ref">refit=True</span>. `28722` by `Yao Xiao <Charlie-XIAO>` and [Thomas Fan](https://github.com/thomasjpfan).
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` the `cv_results_` attribute (of <span class="title-ref">model\_selection.GridSearchCV</span>) now returns masked arrays of the appropriate NumPy dtype, as opposed to always returning dtype `object`. `28352` by `Marco Gorelli<MarcoGorelli>`.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">model\_selection.train\_test\_split</span> works with Array API inputs. Previously indexing was not handled correctly leading to exceptions when using strict implementations of the Array API like CuPY. `28407` by `Tim Head <betatim>`.

#### `sklearn.multioutput`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">chain\_method</span> parameter added to <span class="title-ref">multioutput.ClassifierChain</span>. `27700` by `Lucy Liu <lucyleeow>`.

#### `sklearn.neighbors`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` Fixes <span class="title-ref">neighbors.NeighborhoodComponentsAnalysis</span> such that <span class="title-ref">get\_feature\_names\_out</span> returns the correct number of feature names. `28306` by `Brendan Lu <brendanlu>`.

#### `sklearn.pipeline`

  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` <span class="title-ref">pipeline.FeatureUnion</span> can now use the <span class="title-ref">verbose\_feature\_names\_out</span> attribute. If <span class="title-ref">True</span>, <span class="title-ref">get\_feature\_names\_out</span> will prefix all feature names with the name of the transformer that generated that feature. If <span class="title-ref">False</span>, <span class="title-ref">get\_feature\_names\_out</span> will not prefix any feature names and will error if feature names are not unique. `25991` by `Jiawei Zhang <jiawei-zhang-a>`.

#### `sklearn.preprocessing`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` <span class="title-ref">preprocessing.QuantileTransformer</span> and <span class="title-ref">preprocessing.quantile\_transform</span> now supports disabling subsampling explicitly. `27636` by `Ralph Urlus <rurlus>`.

#### `sklearn.tree`

  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` Plotting trees in matplotlib via <span class="title-ref">tree.plot\_tree</span> now show a "True/False" label to indicate the directionality the samples traverse given the split condition. `28552` by `Adam Li <adam2392>`.

#### `sklearn.utils`

  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` <span class="title-ref">\~utils.\_safe\_indexing</span> now works correctly for polars DataFrame when <span class="title-ref">axis=0</span> and supports indexing polars Series. `28521` by `Yao Xiao <Charlie-XIAO>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.IS\_PYPY</span> is deprecated and will be removed in version 1.7. `28768` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.tosequence</span> is deprecated and will be removed in version 1.7. `28763` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.parallel\_backend</span> and <span class="title-ref">utils.register\_parallel\_backend</span> are deprecated and will be removed in version 1.7. Use <span class="title-ref">joblib.parallel\_backend</span> and <span class="title-ref">joblib.register\_parallel\_backend</span> instead. `28847` by `Jérémie du Boisberranger <jeremiedbb>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` Raise informative warning message in <span class="title-ref">\~utils.multiclass.type\_of\_target</span> when represented as bytes. For classifiers and classification metrics, labels encoded as bytes is deprecated and will raise an error in v1.7. `18555` by `Kaushik Amar Das <cozek>`.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` <span class="title-ref">utils.estimator\_checks.check\_estimator\_sparse\_data</span> was split into two functions: <span class="title-ref">utils.estimator\_checks.check\_estimator\_sparse\_matrix</span> and <span class="title-ref">utils.estimator\_checks.check\_estimator\_sparse\_array</span>. `27576` by `Stefanie Senger <StefanieSenger>`.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.4, including:

101AlexMartin, Abdulaziz Aloqeely, Adam J. Stewart, Adam Li, Adarsh Wase, Adeyemi Biola, Aditi Juneja, Adrin Jalali, Advik Sinha, Aisha, Akash Srivastava, Akihiro Kuno, Alan Guedes, Alberto Torres, Alexis IMBERT, alexqiao, Ana Paula Gomes, Anderson Nelson, Andrei Dzis, Arif Qodari, Arnaud Capitaine, Arturo Amor, Aswathavicky, Audrey Flanders, awwwyan, baggiponte, Bharat Raghunathan, bme-git, brdav, Brendan Lu, Brigitta Sipőcz, Bruno, Cailean Carter, Cemlyn, Christian Lorentzen, Christian Veenhuis, Cindy Liang, Claudio Salvatore Arcidiacono, Connor Boyle, Conrad Stevens, crispinlogan, David Matthew Cherney, Davide Chicco, davidleon123, dependabot\[bot\], DerWeh, dinga92, Dipan Banik, Drew Craeton, Duarte São José, DUONG, Eddie Bergman, Edoardo Abati, Egehan Gunduz, Emad Izadifar, EmilyXinyi, Erich Schubert, Evelyn, Filip Karlo Došilović, Franck Charras, Gael Varoquaux, Gönül Aycı, Guillaume Lemaitre, Gyeongjae Choi, Harmanan Kohli, Hong Xiang Yue, Ian Faust, Ilya Komarov, itsaphel, Ivan Wiryadi, Jack Bowyer, Javier Marin Tur, Jérémie du Boisberranger, Jérôme Dockès, Jiawei Zhang, João Morais, Joe Cainey, Joel Nothman, Johanna Bayer, John Cant, John Enblom, John Hopfensperger, jpcars, jpienaar-tuks, Julian Chan, Julian Libiseller-Egger, Julien Jerphanion, KanchiMoe, Kaushik Amar Das, keyber, Koustav Ghosh, kraktus, Krsto Proroković, Lars, ldwy4, LeoGrin, lihaitao, Linus Sommer, Loic Esteve, Lucy Liu, Lukas Geiger, m-maggi, manasimj, Manuel Labbé, Manuel Morales, Marco Edward Gorelli, Marco Wolsza, Maren Westermann, Marija Vlajic, Mark Elliot, Martin Helm, Mateusz Sokół, mathurinm, Mavs, Michael Dawson, Michael Higgins, Michael Mayer, miguelcsilva, Miki Watanabe, Mohammed Hamdy, myenugula, Nathan Goldbaum, Naziya Mahimkar, nbrown-ScottLogic, Neto, Nithish Bolleddula, notPlancha, Olivier Grisel, Omar Salman, ParsifalXu, Patrick Wang, Pierre de Fréminville, Piotr, Priyank Shroff, Priyansh Gupta, Priyash Shah, Puneeth K, Rahil Parikh, raisadz, Raj Pulapakura, Ralf Gommers, Ralph Urlus, Randolf Scholz, renaissance0ne, Reshama Shaikh, Richard Barnes, Robert Pollak, Roberto Rosati, Rodrigo Romero, rwelsch427, Saad Mahmood, Salim Dohri, Sandip Dutta, SarahRemus, scikit-learn-bot, Shaharyar Choudhry, Shubham, sperret6, Stefanie Senger, Steffen Schneider, Suha Siddiqui, Thanh Lam DANG, thebabush, Thomas, Thomas J. Fan, Thomas Lazarus, Tialo, Tim Head, Tuhin Sharma, Tushar Parimi, VarunChaduvula, Vineet Joshi, virchan, Waël Boukhobza, Weyb, Will Dean, Xavier Beltran, Xiao Yuan, Xuefeng Xu, Yao Xiao, yareyaredesuyo, Ziad Amerr, Štěpán Sršeň

---

v1.6.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.6

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.5, including:

TODO: update at the time of the release.

---

v1.7.md

---

<div class="currentmodule">

sklearn

</div>

# Version 1.7

**Legend for changelogs**

  - `<span class="badge text-bg-success">Major Feature</span>` `{\small\sc [Major Feature]}` something big that you couldn't do before.
  - `<span class="badge text-bg-success">Feature</span>` `{\small\sc [Feature]}` something that you couldn't do before.
  - `<span class="badge text-bg-info">Efficiency</span>` `{\small\sc [Efficiency]}` an existing feature now may not require as much computation or memory.
  - `<span class="badge text-bg-info">Enhancement</span>` `{\small\sc [Enhancement]}` a miscellaneous minor improvement.
  - `<span class="badge text-bg-danger">Fix</span>` `{\small\sc [Fix]}` something that previously didn't work as documented -- or according to reasonable expectations -- should now work.
  - `<span class="badge text-bg-warning">API Change</span>` `{\small\sc [API Change]}` you will need to change your code to have the same effect in the future; or a feature will be removed in the future.

**Code and documentation contributors**

Thanks to everyone who has contributed to the maintenance and improvement of the project since version 1.7, including:

TODO: update at the time of the release.

---

whats_new.md

---

<div class="currentmodule">

sklearn

</div>

# Release History

Changelogs and release notes for all scikit-learn releases are linked in this page.

\> **Tip** \> [Subscribe to scikit-learn releases](https://libraries.io/pypi/scikit-learn) on libraries.io to be notified when new versions are released.

<div class="toctree" data-maxdepth="2">

whats\_new/v1.7.rst whats\_new/v1.6.rst whats\_new/v1.5.rst whats\_new/v1.4.rst whats\_new/v1.3.rst whats\_new/v1.2.rst whats\_new/v1.1.rst whats\_new/v1.0.rst whats\_new/v0.24.rst whats\_new/v0.23.rst whats\_new/v0.22.rst whats\_new/v0.21.rst whats\_new/v0.20.rst whats\_new/v0.19.rst whats\_new/v0.18.rst whats\_new/v0.17.rst whats\_new/v0.16.rst whats\_new/v0.15.rst whats\_new/v0.14.rst whats\_new/v0.13.rst whats\_new/older\_versions.rst

</div>

---

CODE_OF_CONDUCT.md

---

# Code of Conduct

We are a community based on openness, as well as friendly and didactic discussions.

We aspire to treat everybody equally, and value their contributions.

Decisions are made based on technical merit and consensus.

Code is not the only way to help the project. Reviewing pull requests,
answering questions to help others on mailing lists or issues, organizing and
teaching tutorials, working on the website, improving the documentation, are
all priceless contributions.

We abide by the principles of openness, respect, and consideration of others of
the Python Software Foundation: https://www.python.org/psf/codeofconduct/



---

CONTRIBUTING.md

---


Contributing to scikit-learn
============================

The latest contributing guide is available in the repository at
`doc/developers/contributing.rst`, or online at:

https://scikit-learn.org/dev/developers/contributing.html

There are many ways to contribute to scikit-learn, with the most common ones
being contribution of code or documentation to the project. Improving the
documentation is no less important than improving the library itself. If you
find a typo in the documentation, or have made improvements, do not hesitate to
send an email to the mailing list or preferably submit a GitHub pull request.
Documentation can be found under the
[doc/](https://github.com/scikit-learn/scikit-learn/tree/main/doc) directory.

But there are many other ways to help. In particular answering queries on the
[issue tracker](https://github.com/scikit-learn/scikit-learn/issues),
investigating bugs, and [reviewing other developers' pull
requests](https://scikit-learn.org/dev/developers/contributing.html#code-review-guidelines)
are very valuable contributions that decrease the burden on the project
maintainers.

Another way to contribute is to report issues you're facing, and give a "thumbs
up" on issues that others reported and that are relevant to you. It also helps
us if you spread the word: reference the project from your blog and articles,
link to it from your website, or simply star it in GitHub to say "I use it".

Quick links
-----------

* [Submitting a bug report or feature request](https://scikit-learn.org/dev/developers/contributing.html#submitting-a-bug-report-or-a-feature-request)
* [Contributing code](https://scikit-learn.org/dev/developers/contributing.html#contributing-code)
* [Coding guidelines](https://scikit-learn.org/dev/developers/develop.html#coding-guidelines)
* [Tips to read current code](https://scikit-learn.org/dev/developers/contributing.html#reading-the-existing-code-base)

Code of Conduct
---------------

We abide by the principles of openness, respect, and consideration of others
of the Python Software Foundation: https://www.python.org/psf/codeofconduct/.


---

README.rst

---

[![Azure](https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main)](https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main) [![CirrusCI](https://img.shields.io/cirrus/github/scikit-learn/scikit-learn/main?label=Cirrus%20CI)](https://cirrus-ci.com/github/scikit-learn/scikit-learn/main) [![Codecov](https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9)](https://codecov.io/gh/scikit-learn/scikit-learn) [![CircleCI](https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield)](https://circleci.com/gh/scikit-learn/scikit-learn) [![Nightly wheels](https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule)](https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule) [![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![PythonVersion](https://img.shields.io/pypi/pyversions/scikit-learn.svg)](https://pypi.org/project/scikit-learn/) [![PyPi](https://img.shields.io/pypi/v/scikit-learn)](https://pypi.org/project/scikit-learn) [![DOI](https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg)](https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn) [![Benchmark](https://img.shields.io/badge/Benchmarked%20by-asv-blue)](https://scikit-learn.org/scikit-learn-benchmarks)

[![image](https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png)](https://scikit-learn.org/)

**scikit-learn** is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.

The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the [About us](https://scikit-learn.org/dev/about.html#authors) page for a list of core contributors.

It is currently maintained by a team of volunteers.

Website: <https://scikit-learn.org>

Installation
============

Dependencies
------------

scikit-learn requires:

-   Python (\>= 3.9)
-   NumPy (\>= 1.19.5)
-   SciPy (\>= 1.6.0)
-   joblib (\>= 1.2.0)
-   threadpoolctl (\>= 3.1.0)

------------------------------------------------------------------------

**Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.** scikit-learn 1.0 and later require Python 3.7 or newer. scikit-learn 1.1 and later require Python 3.8 or newer.

Scikit-learn plotting capabilities (i.e., functions start with `plot_` and classes end with `Display`) require Matplotlib (\>= 3.3.4). For running the examples Matplotlib \>= 3.3.4 is required. A few examples require scikit-image \>= 0.17.2, a few examples require pandas \>= 1.1.5, some examples require seaborn \>= 0.9.0 and plotly \>= 5.14.0.

User installation
-----------------

If you already have a working installation of NumPy and SciPy, the easiest way to install scikit-learn is using `pip`:

    pip install -U scikit-learn

or `conda`:

    conda install -c conda-forge scikit-learn

The documentation includes more detailed [installation instructions](https://scikit-learn.org/stable/install.html).

Changelog
=========

See the [changelog](https://scikit-learn.org/dev/whats_new.html) for a history of notable changes to scikit-learn.

Development
===========

We welcome new contributors of all experience levels. The scikit-learn community goals are to be helpful, welcoming, and effective. The [Development Guide](https://scikit-learn.org/stable/developers/index.html) has detailed information about contributing code, documentation, tests, and more. We\'ve included some basic information in this README.

Important links
---------------

-   Official source code repo: <https://github.com/scikit-learn/scikit-learn>
-   Download releases: <https://pypi.org/project/scikit-learn/>
-   Issue tracker: <https://github.com/scikit-learn/scikit-learn/issues>

Source code
-----------

You can check the latest sources with the command:

    git clone https://github.com/scikit-learn/scikit-learn.git

Contributing
------------

To learn more about making a contribution to scikit-learn, please see our [Contributing guide](https://scikit-learn.org/dev/developers/contributing.html).

Testing
-------

After installation, you can launch the test suite from outside the source directory (you will need to have `pytest` \>= 7.1.2 installed):

    pytest sklearn

See the web page <https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage> for more information.

> Random number generation can be controlled during testing by setting the `SKLEARN_SEED` environment variable.

Submitting a Pull Request
-------------------------

Before opening a Pull Request, have a look at the full Contributing page to make sure your code complies with our guidelines: <https://scikit-learn.org/stable/developers/index.html>

Project History
===============

The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the [About us](https://scikit-learn.org/dev/about.html#authors) page for a list of core contributors.

The project is currently maintained by a team of volunteers.

**Note**: [scikit-learn]{.title-ref} was previously referred to as [scikits.learn]{.title-ref}.

Help and Support
================

Documentation
-------------

-   HTML documentation (stable release): <https://scikit-learn.org>
-   HTML documentation (development version): <https://scikit-learn.org/dev/>
-   FAQ: <https://scikit-learn.org/stable/faq.html>

Communication
-------------

-   Mailing list: <https://mail.python.org/mailman/listinfo/scikit-learn>
-   Logos & Branding: <https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos>
-   Blog: <https://blog.scikit-learn.org>
-   Calendar: <https://blog.scikit-learn.org/calendar/>
-   Twitter: <https://twitter.com/scikit_learn>
-   Stack Overflow: <https://stackoverflow.com/questions/tagged/scikit-learn>
-   GitHub Discussions: <https://github.com/scikit-learn/scikit-learn/discussions>
-   Website: <https://scikit-learn.org>
-   LinkedIn: <https://www.linkedin.com/company/scikit-learn>
-   YouTube: <https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists>
-   Facebook: <https://www.facebook.com/scikitlearnofficial/>
-   Instagram: <https://www.instagram.com/scikitlearnofficial/>
-   TikTok: <https://www.tiktok.com/@scikit.learn>
-   Mastodon: <https://mastodon.social/@sklearn@fosstodon.org>
-   Discord: <https://discord.gg/h9qyrK8Jc8>

Citation
--------

If you use scikit-learn in a scientific publication, we would appreciate citations: <https://scikit-learn.org/stable/about.html#citing-scikit-learn>


---

SECURITY.md

---

# Security Policy

## Supported Versions

| Version       | Supported          |
| ------------- | ------------------ |
| 1.5.2         | :white_check_mark: |
| < 1.5.2       | :x:                |

## Reporting a Vulnerability

Please report security vulnerabilities by email to `security@scikit-learn.org`.
This email is an alias to a subset of the scikit-learn maintainers' team.

If the security vulnerability is accepted, a patch will be crafted privately
in order to prepare a dedicated bugfix release as timely as possible (depending
on the complexity of the fix).

In addition to sending the report by email, you can also report security
vulnerabilities to [tidelift](https://tidelift.com/security).
