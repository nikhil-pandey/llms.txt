community.md

---

# Contributor community

pandas is a community-driven open source project developed by a large group of [contributors](https://github.com/pandas-dev/pandas/graphs/contributors) and a smaller group of [maintainers](https://pandas.pydata.org/about/team.html). The pandas leadership has made a strong commitment to creating an open, inclusive, and positive community. Please read the pandas [Code of Conduct](https://pandas.pydata.org/community/coc.html) for guidance on how to interact with others in a way that makes the community thrive.

We offer several meetings and communication channels to share knowledge and connect with others within the pandas community.

## Community meeting

The pandas Community Meeting is a regular sync meeting for the project's maintainers which is open to the community. Everyone is welcome to attend and contribute to conversations.

The meetings take place on the second and fourth Wednesdays of each month at 18:00 UTC.

The minutes of past meetings are available in [this Google Document](https://docs.google.com/document/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?usp=sharing).

## New contributor meeting

On the third Wednesday of the month, we hold meetings to welcome and support new contributors in our community.

ðŸ‘‹ you all are invited  
ðŸ’¬ everyone can present (add yourself to the hackMD agenda)  
ðŸ‘€ anyone can sit in and listen

Attendees are new and experienced contributors, as well as a few maintainers. We aim to answer questions about getting started, or help with work in progress when possible, as well as get to know each other and share our learnings and experiences.

The agenda for the next meeting and minutes of past meetings are available in [this HackMD](https://hackmd.io/@pandas-dev/HJgQt1Tei).

## Calendar

This calendar shows all the community meetings. Our community meetings are ideal for anyone wanting to contribute to pandas, or just curious to know how current development is going.

<iframe src="https://calendar.google.com/calendar/embed?src=pgbn14p6poja8a1cf2dv2jhrmg%40group.calendar.google.com" style="border: 0" width="800" height="600" frameborder="0" scrolling="no"></iframe>

You can subscribe to this calendar with the following links:

  - [iCal](https://calendar.google.com/calendar/ical/pgbn14p6poja8a1cf2dv2jhrmg%40group.calendar.google.com/public/basic.ics)
  - [Google calendar](https://calendar.google.com/calendar/r?cid=pgbn14p6poja8a1cf2dv2jhrmg@group.calendar.google.com)

Additionally, we'll sometimes have one-off meetings on specific topics. These will be published on the same calendar.

## [GitHub issue tracker](https://github.com/pandas-dev/pandas/issues)

The pandas contributor community conducts conversations mainly via this channel. Any community member can open issues to:

  - Report bugs, e.g. "I noticed the behavior of a certain function is incorrect"

  - Request features, e.g. "I would like this error message to be more readable"

  - Request documentation improvements, e.g. "I found this section unclear"

  - Ask questions, e.g. "I noticed the behavior of a certain function changed between versions. Is this expected?".
    
    > Ideally, your questions should be related to how pandas works rather than how you use pandas. [StackOverflow](https://stackoverflow.com/) is better suited for answering usage questions, and we ask that all usage questions are first asked on StackOverflow. Thank you for respecting our time and wishes. ðŸ™‡

Maintainers and frequent contributors might also open issues to discuss the ongoing development of the project. For example:

  - Report issues with the CI, GitHub Actions, or the performance of pandas
  - Open issues relating to the internals
  - Start roadmap discussion aligning on proposals for what to do in future releases or changes to the API.
  - Open issues relating to the project's website, logo, or governance

## The developer mailing list

The pandas mailing list [pandas-dev@python.org \<mailto://pandas-dev@python .org\>]() is used for long form conversations and to engage people in the wider community who might not be active on the issue tracker but we would like to include in discussions.

Join the mailing list and view the archives [here](https://mail.python.org/mailman/listinfo/pandas-dev).

## Community slack

We have a chat platform for contributors, maintainers and potential contributors. This is not a space for user questions, rather for questions about contributing to pandas. The slack is a private space, specifically meant for people who are hesitant to bring up their questions or ideas on a large public mailing list or GitHub.

If this sounds like the right place for you, you are welcome to join using [this link](https://join.slack.com/t/pandas-dev-community/shared_invite/zt-2blg6u9k3-K6_XvMRDZWeH7Id274UeIg)\! Please remember to follow our [Code of Conduct](https://pandas.pydata.org/community/coc.html), and be aware that our admins are monitoring for irrelevant messages and will remove folks who use our slack for spam, advertisements and messages not related to the pandas contributing community. And please remember that slack is not meant to replace the mailing list or issue tracker - all important announcements and conversations should still happen there.

---

contributing.md

---

<div id="contributing">

{{ header }}

</div>

# Contributing to pandas

<div class="contents" data-local="">

Table of contents:

</div>

All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.

## Bug reports and enhancement requests

Bug reports and enhancement requests are an important part of making pandas more stable and are curated though Github issues. When reporting an issue or request, please select the [appropriate category and fill out the issue form fully](https://github.com/pandas-dev/pandas/issues/new/choose) to ensure others and the core development team can fully understand the scope of the issue.

The issue will then show up to the pandas community and be open to comments/ideas from others.

## Finding an issue to contribute to

If you are brand new to pandas or open-source development, we recommend searching the [GitHub "issues" tab](https://github.com/pandas-dev/pandas/issues) to find issues that interest you. Unassigned issues labeled [Docs](https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+sort%3Aupdated-desc+label%3ADocs+no%3Aassignee) and [good first issue](https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+sort%3Aupdated-desc+label%3A%22good+first+issue%22+no%3Aassignee) are typically good for newer contributors.

Once you've found an interesting issue, it's a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text `take` to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).

If for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it's available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued).

We have several \[contributor community \<community\>\](\#contributor-community-\<community\>) communication channels, which you are welcome to join, and ask questions as you figure things out. Among them are regular meetings for new contributors, dev meetings, a dev mailing list, and a Slack for the contributor community. All pandas contributors are welcome to these spaces, where they can connect with each other. Even maintainers who have been with us for a long time felt just like you when they started out, and are happy to welcome you and support you as you get to know how we work, and where things are. Take a look at the next sections to learn more.

## Submitting a pull request

### Version control, Git, and GitHub

pandas is hosted on [GitHub](https://www.github.com/pandas-dev/pandas), and to contribute, you will need to sign up for a [free GitHub account](https://github.com/signup/free). We use [Git](https://git-scm.com/) for version control to allow many people to work together on the project.

If you are new to Git, you can reference some of these resources for learning Git. Feel free to reach out to the \[contributor community \<community\>\](\#contributor-community-\<community\>) for help if needed:

  - [Git documentation](https://git-scm.com/doc).

Also, the project follows a forking workflow further described on this page whereby contributors fork the repository, make changes and then create a pull request. So please be sure to read and follow all the instructions in this guide.

If you are new to contributing to projects through forking on GitHub, take a look at the [GitHub documentation for contributing to projects](https://docs.github.com/en/get-started/quickstart/contributing-to-projects). GitHub provides a quick tutorial using a test repository that may help you become more familiar with forking a repository, cloning a fork, creating a feature branch, pushing changes and making pull requests.

Below are some useful resources for learning more about forking and pull requests on GitHub:

  - the [GitHub documentation for forking a repo](https://docs.github.com/en/get-started/quickstart/fork-a-repo).
  - the [GitHub documentation for collaborating with pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests).
  - the [GitHub documentation for working with forks](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks).

### Getting started with Git

[GitHub has instructions](https://docs.github.com/en/get-started/quickstart/set-up-git) for installing git, setting up your SSH key, and configuring git. All these steps need to be completed before you can work seamlessly between your local repository and GitHub.

### Create a fork of pandas

You will need your own copy of pandas (aka fork) to work on the code. Go to the [pandas project page](https://github.com/pandas-dev/pandas) and hit the `Fork` button. Please uncheck the box to copy only the main branch before selecting `Create Fork`. You will want to clone your fork to your machine

`` `shell     git clone https://github.com/your-user-name/pandas.git pandas-yourname     cd pandas-yourname     git remote add upstream https://github.com/pandas-dev/pandas.git     git fetch upstream  This creates the directory ``pandas-yourname`and connects your repository to`\` the upstream (main project) *pandas* repository.

\> **Note** \> Performing a shallow clone (with `--depth==N`, for some `N` greater or equal to 1) might break some tests and features as `pd.show_versions()` as the version number cannot be computed anymore.

### Creating a feature branch

Your local `main` branch should always reflect the current state of pandas repository. First ensure it's up-to-date with the main pandas repository.

`` `shell     git checkout main     git pull upstream main --ff-only  Then, create a feature branch for making your changes. For example  .. code-block:: shell      git checkout -b shiny-new-feature  This changes your working branch from ``main`to the`shiny-new-feature`branch.  Keep any`<span class="title-ref"> changes in this branch specific to one bug or feature so it is clear what the branch brings to pandas. You can have many feature branches and switch in between them using the </span><span class="title-ref">git checkout</span>\` command.

When you want to update the feature branch with changes in main after you created the branch, check the section on \[updating a PR \<contributing.update-pr\>\](\#updating-a-pr-\<contributing.update-pr\>).

### Making code changes

Before modifying any code, ensure you follow the \[contributing environment \<contributing\_environment\>\](\#contributing-environment-\<contributing\_environment\>) guidelines to set up an appropriate development environment.

Then once you have made code changes, you can see all the changes you've currently made by running.

`` `shell     git status  For files you intended to modify or add, run.  .. code-block:: shell      git add path/to/file-to-be-added-or-changed.py  Running ``git status`again should display  .. code-block:: shell      On branch shiny-new-feature           modified:   /relative/path/to/file-to-be-added-or-changed.py   Finally, commit your changes to your local repository with an explanatory commit`\` message

`` `shell     git commit -m "your commit message goes here"  .. _contributing.push-code:  Pushing your changes ``\` --------------------

When you want your changes to appear publicly on your GitHub page, push your forked feature branch's commits

`` `shell     git push origin shiny-new-feature  Here ``origin`is the default name given to your remote repository on GitHub.`\` You can see the remote repositories

`` `shell     git remote -v  If you added the upstream repository as described above you will see something ``\` like

`` `shell     origin  git@github.com:yourname/pandas.git (fetch)     origin  git@github.com:yourname/pandas.git (push)     upstream        git://github.com/pandas-dev/pandas.git (fetch)     upstream        git://github.com/pandas-dev/pandas.git (push)  Now your code is on GitHub, but it is not yet a part of the pandas project. For that to ``\` happen, a pull request needs to be submitted on GitHub.

### Making a pull request

One you have finished your code changes, your code change will need to follow the \[pandas contribution guidelines \<contributing\_codebase\>\](\#pandas-contribution-guidelines-\<contributing\_codebase\>) to be successfully accepted.

If everything looks good, you are ready to make a pull request. A pull request is how code from your local repository becomes available to the GitHub community to review and merged into project to appear the in the next release. To submit a pull request:

1.  Navigate to your repository on GitHub

2.  Click on the `Compare & pull request` button

3.  You can then click on `Commits` and `Files Changed` to make sure everything looks okay one last time

4.  Write a descriptive title that includes prefixes. pandas uses a convention for title prefixes. Here are some common ones along with general guidelines for when to use them:
    
    >   - ENH: Enhancement, new functionality
    >   - BUG: Bug fix
    >   - DOC: Additions/updates to documentation
    >   - TST: Additions/updates to tests
    >   - BLD: Updates to the build process/scripts
    >   - PERF: Performance improvement
    >   - TYP: Type annotations
    >   - CLN: Code cleanup

5.  Write a description of your changes in the `Preview Discussion` tab

6.  Click `Send Pull Request`.

This request then goes to the repository maintainers, and they will review the code.

### Updating your pull request

Based on the review you get on your pull request, you will probably need to make some changes to the code. You can follow the \[code committing steps \<contributing.commit-code\>\](\#code-committing-steps-\<contributing.commit-code\>) again to address any feedback and update your pull request.

It is also important that updates in the pandas `main` branch are reflected in your pull request. To update your feature branch with changes in the pandas `main` branch, run:

`` `shell     git checkout shiny-new-feature     git fetch upstream     git merge upstream/main  If there are no conflicts (or they could be fixed automatically), a file with a ``\` default commit message will open, and you can simply save and quit this file.

If there are merge conflicts, you need to solve those conflicts. See for example at <https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/> for an explanation on how to do this.

Once the conflicts are resolved, run:

1.  `git add -u` to stage any files you've updated;
2.  `git commit` to finish the merge.

\> **Note** \> If you have uncommitted changes at the moment you want to update the branch with `main`, you will need to `stash` them prior to updating (see the [stash docs](https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning)). This will effectively store your changes and they can be reapplied after updating.

After the feature branch has been update locally, you can now update your pull request by pushing to the branch on GitHub:

`` `shell     git push origin shiny-new-feature  Any ``git push`will automatically update your pull request with your branch's changes`\` and restart the \[Continuous Integration \<contributing.ci\>\](\#continuous-integration-\<contributing.ci\>) checks.

### Updating the development environment

It is important to periodically update your local `main` branch with updates from the pandas `main` branch and update your development environment to reflect any changes to the various packages that are used during development.

If using \[conda \<contributing.conda\>\](\#conda-\<contributing.conda\>), run:

`` `shell     git checkout main     git fetch upstream     git merge upstream/main     conda activate pandas-dev     conda env update -f environment.yml --prune  If using [pip <contributing.pip>](#pip-<contributing.pip>) , do:  .. code-block:: shell      git checkout main     git fetch upstream     git merge upstream/main     # activate the virtual environment based on your platform     python -m pip install --upgrade -r requirements-dev.txt  Tips for a successful pull request ``\` ==================================

If you have made it to the [Making a pull request](#making-a-pull-request) phase, one of the core contributors may take a look. Please note however that a handful of people are responsible for reviewing all of the contributions, which can often lead to bottlenecks.

To improve the chances of your pull request being reviewed, you should:

  - **Reference an open issue** for non-trivial changes to clarify the PR's purpose
  - **Ensure you have appropriate tests**. These should be the first part of any PR
  - **Keep your pull requests as simple as possible**. Larger PRs take longer to review
  - **Ensure that CI is in a green state**. Reviewers may not even look otherwise
  - **Keep** [Updating your pull request](#updating-your-pull-request), either by request or every few days

---

contributing_codebase.md

---

<div id="contributing_codebase">

{{ header }}

</div>

# Contributing to the code base

<div class="contents" data-local="">

Table of Contents:

</div>

## Code standards

Writing good code is not just about what you write. It is also about *how* you write it. During \[Continuous Integration \<contributing.ci\>\](\#continuous-integration-\<contributing.ci\>) testing, several tools will be run to check your code for stylistic errors. Generating any warnings will cause the test to fail. Thus, good style is a requirement for submitting code to pandas.

There are a couple of tools in pandas to help contributors verify their changes before contributing to the project

  - `./ci/code_checks.sh`: a script validates the doctests, formatting in docstrings, and imported modules. It is possible to run the checks independently by using the parameters `docstrings`, `code`, and `doctests` (e.g. `./ci/code_checks.sh doctests`);
  - `pre-commit`, which we go into detail on in the next section.

In addition, because a lot of people use our library, it is important that we do not make sudden changes to the code that could have the potential to break a lot of user code as a result, that is, we need it to be as *backwards compatible* as possible to avoid mass breakages.

## Pre-commit

Additionally, \[Continuous Integration \<contributing.ci\>\](\#continuous-integration-\<contributing.ci\>) will run code formatting checks like `ruff`, `isort`, and `clang-format` and more using [pre-commit hooks](https://pre-commit.com/). Any warnings from these checks will cause the \[Continuous Integration \<contributing.ci\>\](\#continuous-integration-\<contributing.ci\>) to fail; therefore, it is helpful to run the check yourself before submitting code. This can be done by installing `pre-commit` (which should already have happened if you followed the instructions in \[Setting up your development environment \<contributing\_environment\>\](\#setting-up-your-development-environment-\<contributing\_environment\>)) and then running:

    pre-commit install

from the root of the pandas repository. Now all of the styling checks will be run each time you commit changes without your needing to run each one manually. In addition, using `pre-commit` will also allow you to more easily remain up-to-date with our code checks as they change.

Note that if needed, you can skip these checks with `git commit --no-verify`.

If you don't want to use `pre-commit` as part of your workflow, you can still use it to run its checks with one of the following:

    pre-commit run --files <files you have modified>
    pre-commit run --from-ref=upstream/main --to-ref=HEAD --all-files

without needing to have done `pre-commit install` beforehand.

Finally, we also have some slow pre-commit checks, which don't run on each commit but which do run during continuous integration. You can trigger them manually with:

    pre-commit run --hook-stage manual --all-files

\> **Note** \> You may want to periodically run `pre-commit gc`, to clean up repos which are no longer used.

<div class="note">

<div class="title">

Note

</div>

If you have conflicting installations of `virtualenv`, then you may get an error - see [here](https://github.com/pypa/virtualenv/issues/1875).

Also, due to a [bug in virtualenv](https://github.com/pypa/virtualenv/issues/1986), you may run into issues if you're using conda. To solve this, you can downgrade `virtualenv` to version `20.0.33`.

</div>

<div class="note">

<div class="title">

Note

</div>

If you have recently merged in main from the upstream branch, some of the dependencies used by `pre-commit` may have changed. Make sure to \[update your development environment \<contributing.update-dev\>\](\#update-your-development-environment-\<contributing.update-dev\>).

</div>

## Optional dependencies

Optional dependencies (e.g. matplotlib) should be imported with the private helper `pandas.compat._optional.import_optional_dependency`. This ensures a consistent error message when the dependency is not met.

All methods using an optional dependency should include a test asserting that an `ImportError` is raised when the optional dependency is not found. This test should be skipped if the library is present.

All optional dependencies should be documented in \[install.optional\_dependencies\](\#install.optional\_dependencies) and the minimum required version should be set in the `pandas.compat._optional.VERSIONS` dict.

## Backwards compatibility

Please try to maintain backward compatibility. pandas has lots of users with lots of existing code, so don't break it if at all possible. If you think breakage is required, clearly state why as part of the pull request. Also, be careful when changing method signatures and add deprecation warnings where needed. Also, add the deprecated sphinx directive to the deprecated functions or methods.

If a function with the same arguments as the one being deprecated exist, you can use the `pandas.util._decorators.deprecate`:

`` `python     from pandas.util._decorators import deprecate      deprecate('old_func', 'new_func', '1.1.0')  Otherwise, you need to do it manually:  .. code-block:: python      import warnings     from pandas.util._exceptions import find_stack_level       def old_func():         """Summary of the function.          .. deprecated:: 1.1.0            Use new_func instead.         """         warnings.warn(             'Use new_func instead.',             FutureWarning,             stacklevel=find_stack_level(),         )         new_func()       def new_func():         pass  You'll also need to  1. Write a new test that asserts a warning is issued when calling with the deprecated argument ``\` 2. Update all of pandas existing tests and code to use the new argument

See \[contributing.warnings\](\#contributing.warnings) for more.

## Type hints

pandas strongly encourages the use of `484` style type hints. New development should contain type hints and pull requests to annotate existing code are accepted as well\!

### Style guidelines

Type imports should follow the `from typing import ...` convention. Your code may be automatically re-written to use some modern constructs (e.g. using the built-in `list` instead of `typing.List`) by the \[pre-commit checks \<contributing.pre-commit\>\](\#pre-commit-checks-\<contributing.pre-commit\>).

In some cases in the code base classes may define class variables that shadow builtins. This causes an issue as described in [Mypy 1775](https://github.com/python/mypy/issues/1775#issuecomment-310969854). The defensive solution here is to create an unambiguous alias of the builtin and use that without your annotation. For example, if you come across a definition like

`` `python    class SomeClass1:        str = None  The appropriate way to annotate this would be as follows  .. code-block:: python     str_type = str     class SomeClass2:        str: str_type = None  In some cases you may be tempted to use ``cast`from the typing module when you know better than the analyzer. This occurs particularly when using custom inference functions. For example  .. code-block:: python     from typing import cast     from pandas.core.dtypes.common import is_number     def cannot_infer_bad(obj: Union[str, int, float]):         if is_number(obj):            ...        else:  # Reasonably only str objects would reach this but...            obj = cast(str, obj)  # Mypy complains without this!            return obj.upper()  The limitation here is that while a human can reasonably understand that`is\_number`would catch the`int`and`float``types mypy cannot make that same inference just yet (see `mypy #5206 <https://github.com/python/mypy/issues/5206>`_. While the above works, the use of``cast`is **strongly discouraged**. Where applicable a refactor of the code to appease static analysis is preferable  .. code-block:: python     def cannot_infer_good(obj: Union[str, int, float]):         if isinstance(obj, str):            return obj.upper()        else:            ...  With custom types and inference this is not always possible so exceptions are made, but every effort should be exhausted to avoid`cast`before going down such paths.  pandas-specific types`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Commonly used types specific to pandas will appear in [pandas.\_typing](https://github.com/pandas-dev/pandas/blob/main/pandas/_typing.py) and you should use these where applicable. This module is private for now but ultimately this should be exposed to third party libraries who want to implement type checking against pandas.

For example, quite a few functions in pandas accept a `dtype` argument. This can be expressed as a string like `"object"`, a `numpy.dtype` like `np.int64` or even a pandas `ExtensionDtype` like `pd.CategoricalDtype`. Rather than burden the user with having to constantly annotate all of those options, this can simply be imported and reused from the pandas.\_typing module

`` `python    from pandas._typing import Dtype     def as_type(dtype: Dtype) -> ...:        ...  This module will ultimately house types for repeatedly used concepts like "path-like", "array-like", "numeric", etc... and can also hold aliases for commonly appearing parameters like ``axis`. Development of this module is active so be sure to refer to the source for the most up to date list of available types.  Validating type hints`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

pandas uses [mypy](http://mypy-lang.org) and [pyright](https://github.com/microsoft/pyright) to statically analyze the code base and type hints. After making any change you can ensure your type hints are consistent by running

`` `shell     pre-commit run --hook-stage manual --all-files mypy     pre-commit run --hook-stage manual --all-files pyright     pre-commit run --hook-stage manual --all-files pyright_reportGeneralTypeIssues     # the following might fail if the installed pandas version does not correspond to your local git version     pre-commit run --hook-stage manual --all-files stubtest  in your python environment.  > **Warning** >      * Please be aware that the above commands will use the current python environment. If your python packages are older/newer than those installed by the pandas CI, the above commands might fail. This is often the case when the ``mypy`or`numpy``versions do not match. Please see [how to setup the python environment <contributing.conda>](#how-to-setup-the-python-environment-<contributing.conda>) or select a `recently succeeded workflow <https://github.com/pandas-dev/pandas/actions/workflows/code-checks.yml?query=branch%3Amain+is%3Asuccess>`_, select the "Docstring validation, typing, and other manual pre-commit hooks" job, then click on "Set up Conda" and "Environment info" to see which versions the pandas CI installs.  .. _contributing.ci:  Testing type hints in code using pandas``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

\> **Warning** \> \* pandas is not yet a py.typed library (`561`)\! The primary purpose of locally declaring pandas as a py.typed library is to test and improve the pandas-builtin type annotations.

Until pandas becomes a py.typed library, it is possible to easily experiment with the type annotations shipped with pandas by creating an empty file named "py.typed" in the pandas installation folder:

`` `none    python -c "import pandas; import pathlib; (pathlib.Path(pandas.__path__[0]) / 'py.typed').touch()"  The existence of the py.typed file signals to type checkers that pandas is already a py.typed ``\` library. This makes type checkers aware of the type annotations shipped with pandas.

## Testing with continuous integration

The pandas test suite will run automatically on [GitHub Actions](https://github.com/features/actions/) continuous integration services, once your pull request is submitted. However, if you wish to run the test suite on a branch prior to submitting the pull request, then the continuous integration services need to be hooked to your GitHub repository. Instructions are here for [GitHub Actions](https://docs.github.com/en/actions/).

A pull-request will be considered for merging when you have an all 'green' build. If any tests are failing, then you will get a red 'X', where you can click through to see the individual failed tests. This is an example of a green build.

![image](../_static/ci.png)

## Test-driven development

pandas is serious about testing and strongly encourages contributors to embrace [test-driven development (TDD)](https://en.wikipedia.org/wiki/Test-driven_development). This development process "relies on the repetition of a very short development cycle: first the developer writes an (initially failing) automated test case that defines a desired improvement or new function, then produces the minimum amount of code to pass that test." So, before actually writing any code, you should write your tests. Often the test can be taken from the original GitHub issue. However, it is always worth considering additional use cases and writing corresponding tests.

We use [code coverage](https://en.wikipedia.org/wiki/Code_coverage) to help understand the amount of code which is covered by a test. We recommend striving to ensure code you add or change within Pandas is covered by a test. Please see our [code coverage dashboard through Codecov](https://app.codecov.io/github/pandas-dev/pandas) for more information.

Adding tests is one of the most common requests after code is pushed to pandas. Therefore, it is worth getting in the habit of writing tests ahead of time so this is never an issue.

### Writing tests

All tests should go into the `tests` subdirectory of the specific package. This folder contains many current examples of tests, and we suggest looking to these for inspiration.

As a general tip, you can use the search functionality in your integrated development environment (IDE) or the git grep command in a terminal to find test files in which the method is called. If you are unsure of the best location to put your test, take your best guess, but note that reviewers may request that you move the test to a different location.

To use git grep, you can run the following command in a terminal:

`git grep "function_name("`

This will search through all files in your repository for the text `function_name(`. This can be a useful way to quickly locate the function in the codebase and determine the best location to add a test for it.

Ideally, there should be one, and only one, obvious place for a test to reside. Until we reach that ideal, these are some rules of thumb for where a test should be located.

  - 1\. Does your test depend only on code in `pd._libs.tslibs`?  
    This test likely belongs in one of:
    
      - tests.tslibs
        
        \> **Note**

  - \>  
    No file in `tests.tslibs` should import from any pandas modules outside of `pd._libs.tslibs`
    
      - tests.scalar
      - tests.tseries.offsets

  - 2\. Does your test depend only on code in pd.\_libs?  
    This test likely belongs in one of:
    
      - tests.libs
      - tests.groupby.test\_libgroupby

  - 3\. Is your test for an arithmetic or comparison method?  
    This test likely belongs in one of:
    
      - tests.arithmetic
        
        <div class="note">
        
        <div class="title">
        
        Note
        
        </div>
        
        These are intended for tests that can be shared to test the behavior of DataFrame/Series/Index/ExtensionArray using the `box_with_array` fixture.
        
        </div>
    
      - tests.frame.test\_arithmetic
    
      - tests.series.test\_arithmetic

  - 4\. Is your test for a reduction method (min, max, sum, prod, ...)?  
    This test likely belongs in one of:
    
      - tests.reductions
        
        <div class="note">
        
        <div class="title">
        
        Note
        
        </div>
        
        These are intended for tests that can be shared to test the behavior of DataFrame/Series/Index/ExtensionArray.
        
        </div>
    
      - tests.frame.test\_reductions
    
      - tests.series.test\_reductions
    
      - tests.test\_nanops

  - 5\. Is your test for an indexing method?  
    This is the most difficult case for deciding where a test belongs, because there are many of these tests, and many of them test more than one method (e.g. both `Series.__getitem__` and `Series.loc.__getitem__`)
    
    1)  Is the test specifically testing an Index method (e.g. `Index.get_loc`, `Index.get_indexer`)? This test likely belongs in one of:
        
          - tests.indexes.test\_indexing
          - tests.indexes.fooindex.test\_indexing
        
        Within that files there should be a method-specific test class e.g. `TestGetLoc`.
        
        In most cases, neither `Series` nor `DataFrame` objects should be needed in these tests.
    
    2)  Is the test for a Series or DataFrame indexing method *other* than `__getitem__` or `__setitem__`, e.g. `xs`, `where`, `take`, `mask`, `lookup`, or `insert`? This test likely belongs in one of:
        
          - tests.frame.indexing.test\_methodname
          - tests.series.indexing.test\_methodname
    
    3)  Is the test for any of `loc`, `iloc`, `at`, or `iat`? This test likely belongs in one of:
        
          - tests.indexing.test\_loc
          - tests.indexing.test\_iloc
          - tests.indexing.test\_at
          - tests.indexing.test\_iat
        
        Within the appropriate file, test classes correspond to either types of indexers (e.g. `TestLocBooleanMask`) or major use cases (e.g. `TestLocSetitemWithExpansion`).
        
        See the note in section D) about tests that test multiple indexing methods.
    
    4)  Is the test for `Series.__getitem__`, `Series.__setitem__`, `DataFrame.__getitem__`, or `DataFrame.__setitem__`? This test likely belongs in one of:
        
          - tests.series.test\_getitem
          - tests.series.test\_setitem
          - tests.frame.test\_getitem
          - tests.frame.test\_setitem
        
        If many cases such a test may test multiple similar methods, e.g.
        
          - \`\`\`python  
            import pandas as pd import pandas.\_testing as tm
            
              - def test\_getitem\_listlike\_of\_ints():  
                ser = pd.Series(range(5))
                
                result = ser\[\[3, 4\]\] expected = pd.Series(\[2, 3\]) tm.assert\_series\_equal(result, expected)
                
                result = ser.loc\[\[3, 4\]\] tm.assert\_series\_equal(result, expected)
    
    > In cases like this, the test location should be based on the *underlying* method being tested. Or in the case of a test for a bugfix, the location of the actual bug. So in this example, we know that `Series.__getitem__` calls `Series.loc.__getitem__`, so this is *really* a test for `loc.__getitem__`. So this test belongs in `tests.indexing.test_loc`.

<!-- end list -->

6.  Is your test for a DataFrame or Series method?
    1)  Is the method a plotting method? This test likely belongs in one of:
          - tests.plotting
    2)  Is the method an IO method? This test likely belongs in one of:
          - tests.io
            
            <div class="note">
            
            <div class="title">
            
            Note
            
            </div>
            
            This includes `to_string` but excludes `__repr__`, which is tested in `tests.frame.test_repr` and `tests.series.test_repr`. Other classes often have a `test_formats` file.
            
            </div>
    3)  Otherwise This test likely belongs in one of:
          - tests.series.methods.test\_mymethod
        
          - tests.frame.methods.test\_mymethod
            
            <div class="note">
            
            <div class="title">
            
            Note
            
            </div>
            
            If a test can be shared between DataFrame/Series using the `frame_or_series` fixture, by convention it goes in the `tests.frame` file.
            
            </div>
7.  Is your test for an Index method, not depending on Series/DataFrame? This test likely belongs in one of:
      - tests.indexes

<!-- end list -->

8)  Is your test for one of the pandas-provided ExtensionArrays (`Categorical`, `DatetimeArray`, `TimedeltaArray`, `PeriodArray`, `IntervalArray`, `NumpyExtensionArray`, `FloatArray`, `BoolArray`, `StringArray`)? This test likely belongs in one of:
      - tests.arrays
9)  Is your test for *all* ExtensionArray subclasses (the "EA Interface")? This test likely belongs in one of:
      - tests.extension

Using `pytest` `` ` ~~~~~~~~~~~~~~~~  Test structure ^^^^^^^^^^^^^^  pandas existing test structure is *mostly* class-based, meaning that you will typically find tests wrapped in a class. ``\`python class TestReallyCoolFeature: def test\_cool\_feature\_aspect(self): pass

We prefer a more *functional* style using the [pytest](https://docs.pytest.org/en/latest/) framework, which offers a richer testing `` ` framework that will facilitate testing and developing. Thus, instead of writing test classes, we will write test functions like this: ``\`python def test\_really\_cool\_feature(): pass

Preferred `pytest` idioms `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^  * Functional tests named ``def [test]()*\`\` and*only\* take arguments that are either fixtures or parameters. \* Use a bare `assert` for testing scalars and truth-testing \* Use `tm.assert_series_equal(result, expected)` and `tm.assert_frame_equal(result, expected)` for comparing <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> results respectively. \* Use [@pytest.mark.parameterize](https://docs.pytest.org/en/latest/how-to/parametrize.html) when testing multiple cases. \* Use [pytest.mark.xfail](https://docs.pytest.org/en/latest/reference/reference.html?#pytest.mark.xfail) when a test case is expected to fail. \* Use [pytest.mark.skip](https://docs.pytest.org/en/latest/reference/reference.html?#pytest.mark.skip) when a test case is never expected to pass. \* Use [pytest.param](https://docs.pytest.org/en/latest/reference/reference.html?#pytest-param) when a test case needs a particular mark. \* Use [@pytest.fixture](https://docs.pytest.org/en/latest/reference/reference.html?#pytest-fixture) if multiple tests can share a setup object.

\> **Warning** \> Do not use `pytest.xfail` (which is different than `pytest.mark.xfail`) since it immediately stops the test and does not check if the test will fail. If this is the behavior you desire, use `pytest.skip` instead.

If a test is known to fail but the manner in which it fails is not meant to be captured, use `pytest.mark.xfail` It is common to use this method for a test that exhibits buggy behavior or a non-implemented feature. If the failing test has flaky behavior, use the argument `strict=False`. This will make it so pytest does not fail if the test happens to pass. Using `strict=False` is highly undesirable, please use it only as a last resort.

Prefer the decorator `@pytest.mark.xfail` and the argument `pytest.param` over usage within a test so that the test is appropriately marked during the collection phase of pytest. For xfailing a test that involves multiple parameters, a fixture, or a combination of these, it is only possible to xfail during the testing phase. To do so, use the `request` fixture:

`` `python     def test_xfail(request):         mark = pytest.mark.xfail(raises=TypeError, reason="Indicate why here")         request.applymarker(mark)  xfail is not to be used for tests involving failure due to invalid user arguments. ``<span class="title-ref"> For these tests, we need to verify the correct exception type and error message is being raised, using </span><span class="title-ref">pytest.raises</span>\` instead.

#### Testing a warning

Use `tm.assert_produces_warning` as a context manager to check that a block of code raises a warning and specify the warning message using the `match` argument.

`` `python     with tm.assert_produces_warning(DeprecationWarning, match="the warning message"):         pd.deprecated_function()  If a warning should specifically not happen in a block of code, pass ``False`into the context manager.  .. code-block:: python      with tm.assert_produces_warning(False):         pd.no_warning_function()  If you have a test that would emit a warning, but you aren't actually testing the`<span class="title-ref"> warning itself (say because it's going to be removed in the future, or because we're matching a 3rd-party library's behavior), then use </span><span class="title-ref">pytest.mark.filterwarnings</span>\` to ignore the error.

`` `python     @pytest.mark.filterwarnings("ignore:msg:category")     def test_thing(self):         pass  Testing an exception ``\` ^^^^^^^^^^^^^^^^^^^^

Use [pytest.raises](https://docs.pytest.org/en/latest/reference/reference.html#pytest-raises) as a context manager with the specific exception subclass (i.e. never use :py\`Exception\`) and the exception message in `match`.

`` `python     with pytest.raises(ValueError, match="an error"):         raise ValueError("an error")  Testing involving files ``\` ^^^^^^^^^^^^^^^^^^^^^^^

The `temp_file` pytest fixture creates a temporary file :py\`Pathlib\` object for testing:

`` `python     def test_something(temp_file):         pd.DataFrame([1]).to_csv(str(temp_file))  Please reference `pytest's documentation <https://docs.pytest.org/en/latest/how-to/tmp_path.html#the-default-base-temporary-directory>`_ ``\` for the file retention policy.

#### Testing involving network connectivity

A unit test should not access a public data set over the internet due to flakiness of network connections and lack of ownership of the server that is being connected to. To mock this interaction, use the `httpserver` fixture from the [pytest-localserver plugin.](https://github.com/pytest-dev/pytest-localserver) with synthetic data.

`` `python     @pytest.mark.network     @pytest.mark.single_cpu     def test_network(httpserver):         httpserver.serve_content(content="content")         result = pd.read_html(httpserver.url)  Example ``\` ^^^^^^^

Here is an example of a self-contained set of tests in a file `pandas/tests/test_cool_feature.py` that illustrate multiple features that we like to use. Please remember to add the GitHub Issue Number as a comment to a new test.

`` `python    import pytest    import numpy as np    import pandas as pd      @pytest.mark.parametrize('dtype', ['int8', 'int16', 'int32', 'int64'])    def test_dtypes(dtype):        assert str(np.dtype(dtype)) == dtype      @pytest.mark.parametrize(        'dtype', ['float32', pytest.param('int16', marks=pytest.mark.skip),                  pytest.param('int32', marks=pytest.mark.xfail(                      reason='to show how it works'))])    def test_mark(dtype):        assert str(np.dtype(dtype)) == 'float32'      @pytest.fixture    def series():        return pd.Series([1, 2, 3])      @pytest.fixture(params=['int8', 'int16', 'int32', 'int64'])    def dtype(request):        return request.param      def test_series(series, dtype):        # GH <issue_number>        result = series.astype(dtype)        assert result.dtype == dtype         expected = pd.Series([1, 2, 3], dtype=dtype)        tm.assert_series_equal(result, expected)   A test run of this yields  .. code-block:: shell     ((pandas) bash-3.2$ pytest  test_cool_feature.py  -v    =========================== test session starts ===========================    platform darwin -- Python 3.6.2, pytest-3.6.0, py-1.4.31, pluggy-0.4.0    collected 11 items     tester.py::test_dtypes[int8] PASSED    tester.py::test_dtypes[int16] PASSED    tester.py::test_dtypes[int32] PASSED    tester.py::test_dtypes[int64] PASSED    tester.py::test_mark[float32] PASSED    tester.py::test_mark[int16] SKIPPED    tester.py::test_mark[int32] xfail    tester.py::test_series[int8] PASSED    tester.py::test_series[int16] PASSED    tester.py::test_series[int32] PASSED    tester.py::test_series[int64] PASSED  Tests that we have ``parametrized`are now accessible via the test name, for example we could run these with`-k int8`to sub-select *only* those tests which match`int8`.   .. code-block:: shell     ((pandas) bash-3.2$ pytest  test_cool_feature.py  -v -k int8    =========================== test session starts ===========================    platform darwin -- Python 3.6.2, pytest-3.6.0, py-1.4.31, pluggy-0.4.0    collected 11 items     test_cool_feature.py::test_dtypes[int8] PASSED    test_cool_feature.py::test_series[int8] PASSED   .. _using-hypothesis:  Using`hypothesis`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Hypothesis is a library for property-based testing. Instead of explicitly parametrizing a test, you can describe *all* valid inputs and let Hypothesis try to find a failing input. Even better, no matter how many random examples it tries, Hypothesis always reports a single minimal counterexample to your assertions - often an example that you would never have thought to test.

See [Getting Started with Hypothesis](https://hypothesis.works/articles/getting-started-with-hypothesis/) for more of an introduction, then [refer to the Hypothesis documentation for details](https://hypothesis.readthedocs.io/en/latest/index.html).

`` `python     import json     from hypothesis import given, strategies as st      any_json_value = st.deferred(lambda: st.one_of(         st.none(), st.booleans(), st.floats(allow_nan=False), st.text(),         st.lists(any_json_value), st.dictionaries(st.text(), any_json_value)     ))       @given(value=any_json_value)     def test_json_roundtrip(value):         result = json.loads(json.dumps(value))         assert value == result  This test shows off several useful features of Hypothesis, as well as ``\` demonstrating a good use-case: checking properties that should hold over a large or complicated domain of inputs.

To keep the pandas test suite running quickly, parametrized tests are preferred if the inputs or logic are simple, with Hypothesis tests reserved for cases with complex logic or where there are too many combinations of options or subtle interactions to test (or think of\!) all of them.

## Running the test suite

The tests can then be run directly inside your Git clone (without having to install pandas) by typing:

    pytest pandas

\> **Note** \> If a handful of tests don't pass, it may not be an issue with your pandas installation. Some tests (e.g. some SQLAlchemy ones) require additional setup, others might start failing because a non-pinned library released a new version, and others might be flaky if run in parallel. As long as you can import pandas from your locally built version, your installation is probably fine and you can start contributing\!

Often it is worth running only a subset of tests first around your changes before running the entire suite.

The easiest way to do this is with:

    pytest pandas/path/to/test.py -k regex_matching_test_name

Or with one of the following constructs:

    pytest pandas/tests/[test-module].py
    pytest pandas/tests/[test-module].py::[TestClass]
    pytest pandas/tests/[test-module].py::[TestClass]::[test_method]

Using [pytest-xdist](https://pypi.org/project/pytest-xdist), which is included in our 'pandas-dev' environment, one can speed up local testing on multicore machines. The `-n` number flag then can be specified when running pytest to parallelize a test run across the number of specified cores or auto to utilize all the available cores on your machine.

`` `bash    # Utilize 4 cores    pytest -n 4 pandas     # Utilizes all available cores    pytest -n auto pandas  If you'd like to speed things along further a more advanced use of this ``\` command would look like this

`` `bash     pytest pandas -n 4 -m "not slow and not network and not db and not single_cpu" -r sxX  In addition to the multithreaded performance increase this improves test ``<span class="title-ref"> speed by skipping some tests using the </span><span class="title-ref">-m</span>\` mark flag:

  - slow: any test taking long (think seconds rather than milliseconds)
  - network: tests requiring network connectivity
  - db: tests requiring a database (mysql or postgres)
  - single\_cpu: tests that should run on a single cpu only

You might want to enable the following option if it's relevant for you:

  - arm\_slow: any test taking long on arm64 architecture

These markers are defined [in this toml file](https://github.com/pandas-dev/pandas/blob/main/pyproject.toml) , under `[tool.pytest.ini_options]` in a list called `markers`, in case you want to check if new ones have been created which are of interest to you.

The `-r` report flag will display a short summary info (see [pytest documentation](https://docs.pytest.org/en/4.6.x/usage.html#detailed-summary-report)) . Here we are displaying the number of:

  - s: skipped tests
  - x: xfailed tests
  - X: xpassed tests

The summary is optional and can be removed if you don't need the added information. Using the parallelization option can significantly reduce the time it takes to locally run tests before submitting a pull request.

If you require assistance with the results, which has happened in the past, please set a seed before running the command and opening a bug report, that way we can reproduce it. Here's an example for setting a seed on windows

`` `bash     set PYTHONHASHSEED=314159265     pytest pandas -n 4 -m "not slow and not network and not db and not single_cpu" -r sxX  On Unix use  .. code-block:: bash      export PYTHONHASHSEED=314159265     pytest pandas -n 4 -m "not slow and not network and not db and not single_cpu" -r sxX  For more, see the `pytest <https://docs.pytest.org/en/latest/>`_ documentation.  Furthermore one can run  .. code-block:: python     pd.test()  with an imported pandas to run tests similarly.  Running the performance test suite ``\` ----------------------------------

Performance matters and it is worth considering whether your code has introduced performance regressions. pandas is in the process of migrating to [asv benchmarks](https://github.com/airspeed-velocity/asv) to enable easy monitoring of the performance of critical pandas operations. These benchmarks are all found in the `pandas/asv_bench` directory, and the test results can be found [here](https://asv-runner.github.io/asv-collection/pandas).

To use all features of asv, you will need either `conda` or `virtualenv`. For more details please check the [asv installation webpage](https://asv.readthedocs.io/en/latest/installing.html).

To install asv:

    pip install git+https://github.com/airspeed-velocity/asv

If you need to run a benchmark, change your directory to `asv_bench/` and run:

    asv continuous -f 1.1 upstream/main HEAD

You can replace `HEAD` with the name of the branch you are working on, and report benchmarks that changed by more than 10%. The command uses `conda` by default for creating the benchmark environments. If you want to use virtualenv instead, write:

    asv continuous -f 1.1 -E virtualenv upstream/main HEAD

The `-E virtualenv` option should be added to all `asv` commands that run benchmarks. The default value is defined in `asv.conf.json`.

Running the full benchmark suite can be an all-day process, depending on your hardware and its resource utilization. However, usually it is sufficient to paste only a subset of the results into the pull request to show that the committed changes do not cause unexpected performance regressions. You can run specific benchmarks using the `-b` flag, which takes a regular expression. For example, this will only run benchmarks from a `pandas/asv_bench/benchmarks/groupby.py` file:

    asv continuous -f 1.1 upstream/main HEAD -b ^groupby

If you want to only run a specific group of benchmarks from a file, you can do it using `.` as a separator. For example:

    asv continuous -f 1.1 upstream/main HEAD -b groupby.GroupByMethods

will only run the `GroupByMethods` benchmark defined in `groupby.py`.

You can also run the benchmark suite using the version of `pandas` already installed in your current Python environment. This can be useful if you do not have virtualenv or conda, or are using the `setup.py develop` approach discussed above; for the in-place build you need to set `PYTHONPATH`, e.g. `PYTHONPATH="$PWD/.." asv [remaining arguments]`. You can run benchmarks using an existing Python environment by:

    asv run -e -E existing

or, to use a specific Python interpreter,:

    asv run -e -E existing:python3.6

This will display stderr from the benchmarks, and use your local `python` that comes from your `$PATH`.

Information on how to write a benchmark and how to use asv can be found in the [asv documentation](https://asv.readthedocs.io/en/latest/writing_benchmarks.html).

## Documenting your code

Changes should be reflected in the release notes located in `doc/source/whatsnew/vx.y.z.rst`. This file contains an ongoing change log for each release. Add an entry to this file to document your fix, enhancement or (unavoidable) breaking change. Make sure to include the GitHub issue number when adding your entry (using ``:issue:`1234``<span class="title-ref"> where </span><span class="title-ref">1234</span>\` is the issue/pull request number). Your entry should be written using full sentences and proper grammar.

When mentioning parts of the API, use a Sphinx `:func:`, `:meth:`, or `:class:` directive as appropriate. Not all public API functions and methods have a documentation page; ideally links would only be added if they resolve. You can usually find similar examples by checking the release notes for one of the previous versions.

If your code is a bugfix, add your entry to the relevant bugfix section. Avoid adding to the `Other` section; only in rare cases should entries go there. Being as concise as possible, the description of the bug should include how the user may encounter it and an indication of the bug itself, e.g. "produces incorrect results" or "incorrectly raises". It may be necessary to also indicate the new behavior.

If your code is an enhancement, it is most likely necessary to add usage examples to the existing documentation. This can be done following the section regarding \[documentation \<contributing\_documentation\>\](\#documentation-\<contributing\_documentation\>). Further, to let users know when this feature was added, the `versionadded` directive is used. The sphinx syntax for that is:

`` `rst     .. versionadded:: 2.1.0  This will put the text *New in version 2.1.0* wherever you put the sphinx ``<span class="title-ref"> directive. This should also be put in the docstring when adding a new function or method (\`example \<https://github.com/pandas-dev/pandas/blob/v0.20.2/pandas/core/frame.py\#L1495\></span>\_\_) or a new keyword argument ([example](https://github.com/pandas-dev/pandas/blob/v0.20.2/pandas/core/generic.py#L568)).

---

contributing_docstring.md

---

<div id="docstring">

{{ header }}

</div>

# pandas docstring guide

## About docstrings and standards

A Python docstring is a string used to document a Python module, class, function or method, so programmers can understand what it does without having to read the details of the implementation.

Also, it is a common practice to generate online (html) documentation automatically from docstrings. [Sphinx](https://www.sphinx-doc.org) serves this purpose.

The next example gives an idea of what a docstring looks like:

`` `python     def add(num1, num2):         """         Add up two integer numbers.          This function simply wraps the ``+`operator, and does not         do anything interesting, except for illustrating what         the docstring of a very simple function looks like.          Parameters         ----------         num1 : int             First number to add.         num2 : int             Second number to add.          Returns         -------         int             The sum of`num1`and`num2`.          See Also         --------         subtract : Subtract one integer from another.          Examples         --------         >>> add(2, 2)         4         >>> add(25, 0)         25         >>> add(10, -10)         0         """         return num1 + num2  Some standards regarding docstrings exist, which make them easier to read, and allow them`\` be easily exported to other formats such as html or pdf.

The first conventions every Python docstring should follow are defined in [PEP-257](https://www.python.org/dev/peps/pep-0257/).

As PEP-257 is quite broad, other more specific standards also exist. In the case of pandas, the NumPy docstring convention is followed. These conventions are explained in this document:

  - [numpydoc docstring guide](https://numpydoc.readthedocs.io/en/latest/format.html)

numpydoc is a Sphinx extension to support the NumPy docstring convention.

The standard uses reStructuredText (reST). reStructuredText is a markup language that allows encoding styles in plain text files. Documentation about reStructuredText can be found in:

  - [Sphinx reStructuredText primer](https://www.sphinx-doc.org/en/stable/rest.html)
  - [Quick reStructuredText reference](https://docutils.sourceforge.io/docs/user/rst/quickref.html)
  - [Full reStructuredText specification](https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html)

pandas has some helpers for sharing docstrings between related classes, see \[docstring.sharing\](\#docstring.sharing).

The rest of this document will summarize all the above guidelines, and will provide additional conventions specific to the pandas project.

## Writing a docstring

### General rules

Docstrings must be defined with three double-quotes. No blank lines should be left before or after the docstring. The text starts in the next line after the opening quotes. The closing quotes have their own line (meaning that they are not at the end of the last sentence).

On rare occasions reST styles like bold text or italics will be used in docstrings, but is it common to have inline code, which is presented between backticks. The following are considered inline code:

  - The name of a parameter
  - Python code, a module, function, built-in, type, literal... (e.g. `os`, `list`, `numpy.abs`, `datetime.date`, `True`)
  - A pandas class (in the form `` `pandas.Series ``\`)
  - A pandas method (in the form `` `pandas.Series.sum ``\`)
  - A pandas function (in the form `` `pandas.to_datetime ``\`)

<div class="note">

<div class="title">

Note

</div>

To display only the last component of the linked class, method or function, prefix it with `~`. For example, `` `~pandas.Series ``<span class="title-ref"> will link to </span><span class="title-ref">pandas.Series</span><span class="title-ref"> but only display the last part, </span><span class="title-ref">Series</span><span class="title-ref"> as the link text. See \`Sphinx cross-referencing syntax \<https://www.sphinx-doc.org/en/stable/domains.html\#cross-referencing-syntax\></span>\_ for details.

</div>

**Good:**

`` `python     def add_values(arr):         """         Add the values in ``arr`.          This is equivalent to Python`sum``of `pandas.Series.sum`.          Some sections are omitted here for simplicity.         """         return sum(arr)  **Bad:**  .. code-block:: python      def func():          """Some function.          With several mistakes in the docstring.          It has a blank line after the signature``def func():`.          The text 'Some function' should go in the line after the         opening quotes of the docstring, not in the same line.          There is a blank line between the docstring and the first line         of code`foo = 1`.          The closing quotes should be in the next line, not in this one."""          foo = 1         bar = 2         return foo + bar  .. _docstring.short_summary:  Section 1: short summary`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

The short summary is a single sentence that expresses what the function does in a concise way.

The short summary must start with a capital letter, end with a dot, and fit in a single line. It needs to express what the object does without providing details. For functions and methods, the short summary must start with an infinitive verb.

**Good:**

`` `python     def astype(dtype):         """         Cast Series type.          This section will provide further details.         """         pass  **Bad:**  .. code-block:: python      def astype(dtype):         """         Casts Series type.          Verb in third-person of the present simple, should be infinitive.         """         pass  .. code-block:: python      def astype(dtype):         """         Method to cast Series type.          Does not start with verb.         """         pass  .. code-block:: python      def astype(dtype):         """         Cast Series type          Missing dot at the end.         """         pass  .. code-block:: python      def astype(dtype):         """         Cast Series type from its current type to the new type defined in         the parameter dtype.          Summary is too verbose and doesn't fit in a single line.         """         pass  .. _docstring.extended_summary:  Section 2: extended summary ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

The extended summary provides details on what the function does. It should not go into the details of the parameters, or discuss implementation notes, which go in other sections.

A blank line is left between the short summary and the extended summary. Every paragraph in the extended summary ends with a dot.

The extended summary should provide details on why the function is useful and their use cases, if it is not too generic.

`` `python     def unstack():         """         Pivot a row index to columns.          When using a MultiIndex, a level can be pivoted so each value in         the index becomes a column. This is especially useful when a subindex         is repeated for the main index, and data is easier to visualize as a         pivot table.          The index level will be automatically removed from the index when added         as columns.         """         pass  .. _docstring.parameters:  Section 3: parameters ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

The details of the parameters will be added in this section. This section has the title "Parameters", followed by a line with a hyphen under each letter of the word "Parameters". A blank line is left before the section title, but not after, and not between the line with the word "Parameters" and the one with the hyphens.

After the title, each parameter in the signature must be documented, including `*args` and `**kwargs`, but not `self`.

The parameters are defined by their name, followed by a space, a colon, another space, and the type (or types). Note that the space between the name and the colon is important. Types are not defined for `*args` and `**kwargs`, but must be defined for all other parameters. After the parameter definition, it is required to have a line with the parameter description, which is indented, and can have multiple lines. The description must start with a capital letter, and finish with a dot.

For keyword arguments with a default value, the default will be listed after a comma at the end of the type. The exact form of the type in this case will be "int, default 0". In some cases it may be useful to explain what the default argument means, which can be added after a comma "int, default -1, meaning all cpus".

In cases where the default value is `None`, meaning that the value will not be used. Instead of `"str, default None"`, it is preferred to write `"str, optional"`. When `None` is a value being used, we will keep the form "str, default None". For example, in `df.to_csv(compression=None)`, `None` is not a value being used, but means that compression is optional, and no compression is being used if not provided. In this case we will use `"str, optional"`. Only in cases like `func(value=None)` and `None` is being used in the same way as `0` or `foo` would be used, then we will specify "str, int or None, default None".

**Good:**

`` `python     class Series:         def plot(self, kind, color='blue', **kwargs):             """             Generate a plot.              Render the data in the Series as a matplotlib plot of the             specified kind.              Parameters             ----------             kind : str                 Kind of matplotlib plot.             color : str, default 'blue'                 Color name or rgb code.             **kwargs                 These parameters will be passed to the matplotlib plotting                 function.             """             pass  **Bad:**  .. code-block:: python      class Series:         def plot(self, kind, **kwargs):             """             Generate a plot.              Render the data in the Series as a matplotlib plot of the             specified kind.              Note the blank line between the parameters title and the first             parameter. Also, note that after the name of the parameter ``kind`and before the colon, a space is missing.              Also, note that the parameter descriptions do not start with a             capital letter, and do not finish with a dot.              Finally, the`\*\*kwargs`parameter is missing.              Parameters             ----------              kind: str                 kind of matplotlib plot             """             pass  .. _docstring.parameter_types:  Parameter types`\` ^^^^^^^^^^^^^^^

When specifying the parameter types, Python built-in data types can be used directly (the Python type is preferred to the more verbose string, integer, boolean, etc):

  - int
  - float
  - str
  - bool

For complex types, define the subtypes. For `dict` and `tuple`, as more than one type is present, we use the brackets to help read the type (curly brackets for `dict` and normal brackets for `tuple`):

  - list of int
  - dict of {str : int}
  - tuple of (str, int, int)
  - tuple of (str,)
  - set of str

In case where there are just a set of values allowed, list them in curly brackets and separated by commas (followed by a space). If the values are ordinal and they have an order, list them in this order. Otherwise, list the default value first, if there is one:

  - {0, 10, 25}
  - {'simple', 'advanced'}
  - {'low', 'medium', 'high'}
  - {'cat', 'dog', 'bird'}

If the type is defined in a Python module, the module must be specified:

  - datetime.date
  - datetime.datetime
  - decimal.Decimal

If the type is in a package, the module must be also specified:

  - numpy.ndarray
  - scipy.sparse.coo\_matrix

If the type is a pandas type, also specify pandas except for Series and DataFrame:

  - Series
  - DataFrame
  - pandas.Index
  - pandas.Categorical
  - pandas.arrays.SparseArray

If the exact type is not relevant, but must be compatible with a NumPy array, array-like can be specified. If Any type that can be iterated is accepted, iterable can be used:

  - array-like
  - iterable

If more than one type is accepted, separate them by commas, except the last two types, that need to be separated by the word 'or':

  - int or float
  - float, decimal.Decimal or None
  - str or list of str

If `None` is one of the accepted values, it always needs to be the last in the list.

For axis, the convention is to use something like:

  - axis : {0 or 'index', 1 or 'columns', None}, default None

### Section 4: returns or yields

If the method returns a value, it will be documented in this section. Also if the method yields its output.

The title of the section will be defined in the same way as the "Parameters". With the names "Returns" or "Yields" followed by a line with as many hyphens as the letters in the preceding word.

The documentation of the return is also similar to the parameters. But in this case, no name will be provided, unless the method returns or yields more than one value (a tuple of values).

The types for "Returns" and "Yields" are the same as the ones for the "Parameters". Also, the description must finish with a dot.

For example, with a single value:

`` `python     def sample():         """         Generate and return a random number.          The value is sampled from a continuous uniform distribution between         0 and 1.          Returns         -------         float             Random number generated.         """         return np.random.random()  With more than one value:  .. code-block:: python      import string      def random_letters():         """         Generate and return a sequence of random letters.          The length of the returned string is also random, and is also         returned.          Returns         -------         length : int             Length of the returned string.         letters : str             String of random letters.         """         length = np.random.randint(1, 10)         letters = ''.join(np.random.choice(string.ascii_lowercase)                           for i in range(length))         return length, letters  If the method yields its value:  .. code-block:: python      def sample_values():         """         Generate an infinite sequence of random numbers.          The values are sampled from a continuous uniform distribution between         0 and 1.          Yields         ------         float             Random number generated.         """         while True:             yield np.random.random()  .. _docstring.see_also:  Section 5: see also ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This section is used to let users know about pandas functionality related to the one being documented. In rare cases, if no related methods or functions can be found at all, this section can be skipped.

An obvious example would be the `head()` and `tail()` methods. As `tail()` does the equivalent as `head()` but at the end of the `Series` or `DataFrame` instead of at the beginning, it is good to let the users know about it.

To give an intuition on what can be considered related, here there are some examples:

  - `loc` and `iloc`, as they do the same, but in one case providing indices and in the other positions
  - `max` and `min`, as they do the opposite
  - `iterrows`, `itertuples` and `items`, as it is easy that a user looking for the method to iterate over columns ends up in the method to iterate over rows, and vice-versa
  - `fillna` and `dropna`, as both methods are used to handle missing values
  - `read_csv` and `to_csv`, as they are complementary
  - `merge` and `join`, as one is a generalization of the other
  - `astype` and `pandas.to_datetime`, as users may be reading the documentation of `astype` to know how to cast as a date, and the way to do it is with `pandas.to_datetime`
  - `where` is related to `numpy.where`, as its functionality is based on it

When deciding what is related, you should mainly use your common sense and think about what can be useful for the users reading the documentation, especially the less experienced ones.

When relating to other libraries (mainly `numpy`), use the name of the module first (not an alias like `np`). If the function is in a module which is not the main one, like `scipy.sparse`, list the full module (e.g. `scipy.sparse.coo_matrix`).

This section has a header, "See Also" (note the capital S and A), followed by the line with hyphens and preceded by a blank line.

After the header, we will add a line for each related method or function, followed by a space, a colon, another space, and a short description that illustrates what this method or function does, why is it relevant in this context, and what the key differences are between the documented function and the one being referenced. The description must also end with a dot.

Note that in "Returns" and "Yields", the description is located on the line after the type. In this section, however, it is located on the same line, with a colon in between. If the description does not fit on the same line, it can continue onto other lines which must be further indented.

For example:

`` `python     class Series:         def head(self):             """             Return the first 5 elements of the Series.              This function is mainly useful to preview the values of the             Series without displaying the whole of it.              Returns             -------             Series                 Subset of the original series with the 5 first values.              See Also             --------             Series.tail : Return the last 5 elements of the Series.             Series.iloc : Return a slice of the elements in the Series,                 which can also be used to return the first or last n.             """             return self.iloc[:5]  .. _docstring.notes:  Section 6: notes ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This is an optional section used for notes about the implementation of the algorithm, or to document technical aspects of the function behavior.

Feel free to skip it, unless you are familiar with the implementation of the algorithm, or you discover some counter-intuitive behavior while writing the examples for the function.

This section follows the same format as the extended summary section.

### Section 7: examples

This is one of the most important sections of a docstring, despite being placed in the last position, as often people understand concepts better by example than through accurate explanations.

Examples in docstrings, besides illustrating the usage of the function or method, must be valid Python code, that returns the given output in a deterministic way, and that can be copied and run by users.

Examples are presented as a session in the Python terminal. `>>>` is used to present code. `...` is used for code continuing from the previous line. Output is presented immediately after the last line of code generating the output (no blank lines in between). Comments describing the examples can be added with blank lines before and after them.

The way to present examples is as follows:

1.  Import required libraries (except `numpy` and `pandas`)
2.  Create the data required for the example
3.  Show a very basic example that gives an idea of the most common use case
4.  Add examples with explanations that illustrate how the parameters can be used for extended functionality

A simple example could be:

`` `python     class Series:          def head(self, n=5):             """             Return the first elements of the Series.              This function is mainly useful to preview the values of the             Series without displaying all of it.              Parameters             ----------             n : int                 Number of values to return.              Return             ------             pandas.Series                 Subset of the original series with the n first values.              See Also             --------             tail : Return the last n elements of the Series.              Examples             --------             >>> ser = pd.Series(['Ant', 'Bear', 'Cow', 'Dog', 'Falcon',             ...                'Lion', 'Monkey', 'Rabbit', 'Zebra'])             >>> ser.head()             0   Ant             1   Bear             2   Cow             3   Dog             4   Falcon             dtype: object              With the ``n`parameter, we can change the number of returned rows:              >>> ser.head(n=3)             0   Ant             1   Bear             2   Cow             dtype: object             """             return self.iloc[:n]  The examples should be as concise as possible. In cases where the complexity of`<span class="title-ref"> the function requires long examples, is recommended to use blocks with headers in bold. Use double star </span><span class="title-ref">\*\*</span><span class="title-ref"> to make a text bold, like in </span><span class="title-ref">\*\*this example\*\*</span>\`.

#### Conventions for the examples

Code in examples is assumed to always start with these two lines which are not shown:

`` `python     import numpy as np     import pandas as pd  Any other module used in the examples must be explicitly imported, one per line (as ``<span class="title-ref"> recommended in :pep:\`8\#imports</span>) and avoiding aliases. Avoid excessive imports, but if needed, imports from the standard library go first, followed by third-party libraries (like matplotlib).

When illustrating examples with a single `Series` use the name `ser`, and if illustrating with a single `DataFrame` use the name `df`. For indices, `idx` is the preferred name. If a set of homogeneous `Series` or `DataFrame` is used, name them `ser1`, `ser2`, `ser3`... or `df1`, `df2`, `df3`... If the data is not homogeneous, and more than one structure is needed, name them with something meaningful, for example `df_main` and `df_to_join`.

Data used in the example should be as compact as possible. The number of rows is recommended to be around 4, but make it a number that makes sense for the specific example. For example in the `head` method, it requires to be higher than 5, to show the example with the default values. If doing the `mean`, we could use something like `[1, 2, 3]`, so it is easy to see that the value returned is the mean.

For more complex examples (grouping for example), avoid using data without interpretation, like a matrix of random numbers with columns A, B, C, D... And instead use a meaningful example, which makes it easier to understand the concept. Unless required by the example, use names of animals, to keep examples consistent. And numerical properties of them.

When calling the method, keywords arguments `head(n=3)` are preferred to positional arguments `head(3)`.

**Good:**

`` `python     class Series:          def mean(self):             """             Compute the mean of the input.              Examples             --------             >>> ser = pd.Series([1, 2, 3])             >>> ser.mean()             2             """             pass           def fillna(self, value):             """             Replace missing values by ``value`.              Examples             --------             >>> ser = pd.Series([1, np.nan, 3])             >>> ser.fillna(0)             [1, 0, 3]             """             pass          def groupby_mean(self):             """             Group by index and return mean.              Examples             --------             >>> ser = pd.Series([380., 370., 24., 26],             ...               name='max_speed',             ...               index=['falcon', 'falcon', 'parrot', 'parrot'])             >>> ser.groupby_mean()             index             falcon    375.0             parrot     25.0             Name: max_speed, dtype: float64             """             pass          def contains(self, pattern, case_sensitive=True, na=numpy.nan):             """             Return whether each value contains`pattern`.              In this case, we are illustrating how to use sections, even             if the example is simple enough and does not require them.              Examples             --------             >>> ser = pd.Series('Antelope', 'Lion', 'Zebra', np.nan)             >>> ser.contains(pattern='a')             0    False             1    False             2     True             3      NaN             dtype: bool              **Case sensitivity**              With`case\_sensitive`set to`False`we can match`a`with both`a`and`A`:              >>> s.contains(pattern='a', case_sensitive=False)             0     True             1    False             2     True             3      NaN             dtype: bool              **Missing values**              We can fill missing values in the output using the`na`parameter:              >>> ser.contains(pattern='a', na=False)             0    False             1    False             2     True             3    False             dtype: bool             """             pass  **Bad:**  .. code-block:: python      def method(foo=None, bar=None):         """         A sample DataFrame method.          Do not import NumPy and pandas.          Try to use meaningful data, when it makes the example easier         to understand.          Try to avoid positional arguments like in`df.method(1)`. They         can be all right if previously defined with a meaningful name,         like in`present\_value(interest\_rate)`, but avoid them otherwise.          When presenting the behavior with different parameters, do not place         all the calls one next to the other. Instead, add a short sentence         explaining what the example shows.          Examples         --------         >>> import numpy as np         >>> import pandas as pd         >>> df = pd.DataFrame(np.random.randn(3, 3),         ...                   columns=('a', 'b', 'c'))         >>> df.method(1)         21         >>> df.method(bar=14)         123         """         pass   .. _docstring.doctest_tips:  Tips for getting your examples pass the doctests`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Getting the examples pass the doctests in the validation script can sometimes be tricky. Here are some attention points:

  - Import all needed libraries (except for pandas and NumPy, those are already imported as `import pandas as pd` and `import numpy as np`) and define all variables you use in the example.

  - Try to avoid using random data. However random data might be OK in some cases, like if the function you are documenting deals with probability distributions, or if the amount of data needed to make the function result meaningful is too much, such that creating it manually is very cumbersome. In those cases, always use a fixed random seed to make the generated examples predictable. Example:
    
        >>> np.random.seed(42)
        >>> df = pd.DataFrame({'normal': np.random.normal(100, 5, 20)})

  - If you have a code snippet that wraps multiple lines, you need to use '...' on the continued lines: :
    
        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['a', 'b', 'c'],
        ...                   columns=['A', 'B'])

  - If you want to show a case where an exception is raised, you can do:
    
        >>> pd.to_datetime(["712-01-01"])
        Traceback (most recent call last):
        OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 712-01-01 00:00:00
    
    It is essential to include the "Traceback (most recent call last):", but for the actual error only the error name is sufficient.

  - If there is a small part of the result that can vary (e.g. a hash in an object representation), you can use `...` to represent this part.
    
    If you want to show that `s.plot()` returns a matplotlib AxesSubplot object, this will fail the doctest :
    
        >>> s.plot()
        <matplotlib.axes._subplots.AxesSubplot at 0x7efd0c0b0690>
    
    However, you can do (notice the comment that needs to be added) :
    
        >>> s.plot()  # doctest: +ELLIPSIS
        <matplotlib.axes._subplots.AxesSubplot at ...>

#### Plots in examples

There are some methods in pandas returning plots. To render the plots generated by the examples in the documentation, the `.. plot::` directive exists.

To use it, place the next code after the "Examples" header as shown below. The plot will be generated automatically when building the documentation.

`` `python     class Series:         def plot(self):             """             Generate a plot with the ``Series`data.              Examples             --------              .. plot::                 :context: close-figs                  >>> ser = pd.Series([1, 2, 3])                 >>> ser.plot()             """             pass  .. _docstring.sharing:  Sharing docstrings`\` ------------------

pandas has a system for sharing docstrings, with slight variations, between classes. This helps us keep docstrings consistent, while keeping things clear for the user reading. It comes at the cost of some complexity when writing.

Each shared docstring will have a base template with variables, like `{klass}`. The variables filled in later on using the `doc` decorator. Finally, docstrings can also be appended to with the `doc` decorator.

In this example, we'll create a parent docstring normally (this is like `pandas.core.generic.NDFrame`). Then we'll have two children (like `pandas.Series` and `pandas.DataFrame`). We'll substitute the class names in this docstring.

`` `python    class Parent:        @doc(klass="Parent")        def my_function(self):            """Apply my function to {klass}."""            ...      class ChildA(Parent):        @doc(Parent.my_function, klass="ChildA")        def my_function(self):            ...      class ChildB(Parent):        @doc(Parent.my_function, klass="ChildB")        def my_function(self):            ...  The resulting docstrings are  .. code-block:: python     >>> print(Parent.my_function.__doc__)    Apply my function to Parent.    >>> print(ChildA.my_function.__doc__)    Apply my function to ChildA.    >>> print(ChildB.my_function.__doc__)    Apply my function to ChildB.  Notice:  1. We "append" the parent docstring to the children docstrings, which are    initially empty.  Our files will often contain a module-level ``\_shared\_doc\_kwargs`with some`<span class="title-ref"> common substitution values (things like </span><span class="title-ref">klass</span><span class="title-ref">, </span><span class="title-ref">axes</span>\`, etc).

You can substitute and append in one shot with something like

`` `python    @doc(template, **_shared_doc_kwargs)    def my_function(self):        ...  where ``template`may come from a module-level`\_shared\_docs`dictionary`<span class="title-ref"> mapping function names to docstrings. Wherever possible, we prefer using </span><span class="title-ref">doc</span>\`, since the docstring-writing processes is slightly closer to normal.

See `pandas.core.generic.NDFrame.fillna` for an example template, and `pandas.Series.fillna` and `pandas.core.generic.frame.fillna` for the filled versions.

---

contributing_documentation.md

---

<div id="contributing_documentation">

{{ header }}

</div>

# Contributing to the documentation

Contributing to the documentation benefits everyone who uses pandas. We encourage you to help us improve the documentation, and you don't have to be an expert on pandas to do so\! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn't make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person. Please visit the [issues page](https://github.com/pandas-dev/pandas/issues?page=1&q=is%3Aopen+sort%3Aupdated-desc+label%3ADocs) for a full list of issues that are currently open regarding the pandas documentation.

<div class="contents" data-local="">

Documentation:

</div>

## About the pandas documentation

The documentation is written in **reStructuredText**, which is almost like writing in plain English, and built using [Sphinx](https://www.sphinx-doc.org/en/master/). The Sphinx Documentation has an excellent [introduction to reST](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html). Review the Sphinx docs to perform more complex changes to the documentation as well.

Some other important things to know about the docs:

  - The pandas documentation consists of two parts: the docstrings in the code itself and the docs in this folder `doc/`.
    
    The docstrings provide a clear explanation of the usage of the individual functions, while the documentation in this folder consists of tutorial-like overviews per topic together with some other information (what's new, installation, etc).

  - The docstrings follow a pandas convention, based on the **Numpy Docstring Standard**. Follow the \[pandas docstring guide \<docstring\>\](\#pandas-docstring-guide-\<docstring\>) for detailed instructions on how to write a correct docstring.
    
    <div class="toctree" data-maxdepth="2">
    
    contributing\_docstring.rst
    
    </div>

  - The tutorials make heavy use of the [IPython directive](https://matplotlib.org/sampledoc/ipython_directive.html) sphinx extension. This directive lets you put code in the documentation which will be run during the doc build. For example:
    
        .. ipython:: python
        
            x = 2
            x**3
    
    will be rendered as:
    
        In [1]: x = 2
        
        In [2]: x**3
        Out[2]: 8
    
    Almost all code examples in the docs are run (and the output saved) during the doc build. This approach means that code examples will always be up to date, but it does make the doc building a bit more complex.

  - Our API documentation files in `doc/source/reference` house the auto-generated documentation from the docstrings. For classes, there are a few subtleties around controlling which methods and attributes have pages auto-generated.
    
    We have two autosummary templates for classes.
    
    1.  `_templates/autosummary/class.rst`. Use this when you want to automatically generate a page for every public method and attribute on the class. The `Attributes` and `Methods` sections will be automatically added to the class' rendered documentation by numpydoc. See `DataFrame` for an example.
    2.  `_templates/autosummary/class_without_autosummary`. Use this when you want to pick a subset of methods / attributes to auto-generate pages for. When using this template, you should include an `Attributes` and `Methods` section in the class docstring. See `CategoricalIndex` for an example.
    
    Every method should be included in a `toctree` in one of the documentation files in `doc/source/reference`, else Sphinx will emit a warning.

The utility script `scripts/validate_docstrings.py` can be used to get a csv summary of the API documentation. And also validate common errors in the docstring of a specific class, function or method. The summary also compares the list of methods documented in the files in `doc/source/reference` (which is used to generate the [API Reference](https://pandas.pydata.org/pandas-docs/stable/api.html) page) and the actual public methods. This will identify methods documented in `doc/source/reference` that are not actually class methods, and existing methods that are not documented in `doc/source/reference`.

## Updating a pandas docstring

When improving a single function or method's docstring, it is not necessarily needed to build the full documentation (see next section). However, there is a script that checks a docstring (for example for the `DataFrame.mean` method):

    python scripts/validate_docstrings.py pandas.DataFrame.mean

This script will indicate some formatting errors if present, and will also run and test the examples included in the docstring. Check the \[pandas docstring guide \<docstring\>\](\#pandas-docstring-guide-\<docstring\>) for a detailed guide on how to format the docstring.

The examples in the docstring ('doctests') must be valid Python code, that in a deterministic way returns the presented output, and that can be copied and run by users. This can be checked with the script above, and is also tested on Travis. A failing doctest will be a blocker for merging a PR. Check the \[examples \<docstring.examples\>\](\#examples-\<docstring.examples\>) section in the docstring guide for some tips and tricks to get the doctests passing.

When doing a PR with a docstring update, it is good to post the output of the validation script in a comment on github.

## How to build the pandas documentation

### Requirements

First, you need to have a development environment to be able to build pandas (see the docs on \[creating a development environment \<contributing\_environment\>\](\#creating-a-development-environment-\<contributing\_environment\>)).

### Building the documentation

So how do you build the docs? Navigate to your local `doc/` directory in the console and run:

    python make.py html

Then you can find the HTML output in the folder `doc/build/html/`.

The first time you build the docs, it will take quite a while because it has to run all the code examples and build all the generated docstring pages. In subsequent evocations, sphinx will try to only build the pages that have been modified.

If you want to do a full clean build, do:

    python make.py clean
    python make.py html

You can tell `make.py` to compile only a single section of the docs, greatly reducing the turn-around time for checking your changes.

    # omit autosummary and API section
    python make.py clean
    python make.py --no-api
    
    # compile the docs with only a single section, relative to the "source" folder.
    # For example, compiling only this guide (doc/source/development/contributing.rst)
    python make.py clean
    python make.py --single development/contributing.rst
    
    # compile the reference docs for a single function
    python make.py clean
    python make.py --single pandas.DataFrame.join
    
    # compile whatsnew and API section (to resolve links in the whatsnew)
    python make.py clean
    python make.py --whatsnew

For comparison, a full documentation build may take 15 minutes, but a single section may take 15 seconds. Subsequent builds, which only process portions you have changed, will be faster.

The build will automatically use the number of cores available on your machine to speed up the documentation build. You can override this:

    python make.py html --num-jobs 4

Open the following file in a web browser to see the full documentation you just built `doc/build/html/index.html`.

And you'll have the satisfaction of seeing your new and improved documentation\!

### Building main branch documentation

When pull requests are merged into the pandas `main` branch, the main parts of the documentation are also built by Travis-CI. These docs are then hosted [here](https://pandas.pydata.org/docs/dev/), see also the `Continuous Integration <contributing.ci>` section.

## Previewing changes

Once, the pull request is submitted, GitHub Actions will automatically build the documentation. To view the built site:

1.  Wait for the `CI / Web and docs` check to complete.
2.  Click `Details` next to it.
3.  From the `Artifacts` drop-down, click `docs` or `website` to download the site as a ZIP file.

---

contributing_environment.md

---

<div id="contributing_environment">

{{ header }}

</div>

# Creating a development environment

To test out code changes, you'll need to build pandas from source, which requires a C/C++ compiler and Python environment. If you're making documentation changes, you can skip to \[contributing to the documentation \<contributing\_documentation\>\](\#contributing-to-the-documentation-\<contributing\_documentation\>) but if you skip creating the development environment you won't be able to build the documentation locally before pushing your changes. It's recommended to also install the \[pre-commit hooks \<contributing.pre-commit\>\](\#pre-commit-hooks-\<contributing.pre-commit\>).

<div class="toctree" data-maxdepth="2" hidden="">

contributing\_gitpod.rst

</div>

## Step 1: install a C compiler

How to do this will depend on your platform. If you choose to use `Docker` or `GitPod` in the next step, then you can skip this step.

**Windows**

You will need [Build Tools for Visual Studio 2022](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022).

<div class="note">

<div class="title">

Note

</div>

You DO NOT need to install Visual Studio 2022. You only need "Build Tools for Visual Studio 2022" found by scrolling down to "All downloads" -\> "Tools for Visual Studio". In the installer, select the "Desktop development with C++" Workloads.

If you encounter an error indicating `cl.exe` is not found when building with Meson, reopen the installer and also select the optional component **MSVC v142 - VS 2019 C++ x64/x86 build tools** in the right pane for installation.

</div>

Alternatively, you can install the necessary components on the commandline using [vs\_BuildTools.exe](https://learn.microsoft.com/en-us/visualstudio/install/use-command-line-parameters-to-install-visual-studio?source=recommendations&view=vs-2022)

Alternatively, you could use the [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) and consult the `Linux` instructions below.

**macOS**

To use the \[conda \<contributing.conda\>\](\#conda-\<contributing.conda\>)-based compilers, you will need to install the Developer Tools using `xcode-select --install`.

If you prefer to use a different compiler, general information can be found here: <https://devguide.python.org/setup/#macos>

**Linux**

For Linux-based \[conda \<contributing.conda\>\](\#conda-\<contributing.conda\>) installations, you won't have to install any additional components outside of the conda environment. The instructions below are only needed if your setup isn't based on conda environments.

Some Linux distributions will come with a pre-installed C compiler. To find out which compilers (and versions) are installed on your system:

    # for Debian/Ubuntu:
    dpkg --list | grep compiler
    # for Red Hat/RHEL/CentOS/Fedora:
    yum list installed | grep -i --color compiler

[GCC (GNU Compiler Collection)](https://gcc.gnu.org/), is a widely used compiler, which supports C and a number of other languages. If GCC is listed as an installed compiler nothing more is required.

If no C compiler is installed, or you wish to upgrade, or you're using a different Linux distribution, consult your favorite search engine for compiler installation/update instructions.

Let us know if you have any difficulties by opening an issue or reaching out on our contributor community \[Slack \<community.slack\>\](\#slack-\<community.slack\>).

## Step 2: create an isolated environment

Before we begin, please:

  - Make sure that you have `cloned the repository <contributing.forking>`
  - `cd` to the pandas source directory you just created with the clone command

### Option 1: using conda (recommended)

  - Install miniforge to get [conda](https://github.com/conda-forge/miniforge?tab=readme-ov-file#download)
  - Create and activate the `pandas-dev` conda environment using the following commands:

`` `bash    conda env create --file environment.yml    conda activate pandas-dev  .. _contributing.pip:  Option 2: using pip ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

You'll need to have at least the \[minimum Python version \<install.version\>\](\#minimum-python-version-\<install.version\>) that pandas supports. You also need to have `setuptools` 51.0.0 or later to build pandas.

**Unix**/**macOS with virtualenv**

`` `bash    # Create a virtual environment    # Use an ENV_DIR of your choice. We'll use ~/virtualenvs/pandas-dev    # Any parent directories should already exist    python3 -m venv ~/virtualenvs/pandas-dev     # Activate the virtualenv    . ~/virtualenvs/pandas-dev/bin/activate     # Install the build dependencies    python -m pip install -r requirements-dev.txt  **Unix**/**macOS with pyenv**  Consult the docs for setting up pyenv `here <https://github.com/pyenv/pyenv>`__.  .. code-block:: bash     # Create a virtual environment    # Use an ENV_DIR of your choice. We'll use ~/Users/<yourname>/.pyenv/versions/pandas-dev    pyenv virtualenv <version> <name-to-give-it>     # For instance:    pyenv virtualenv 3.10 pandas-dev     # Activate the virtualenv    pyenv activate pandas-dev     # Now install the build dependencies in the cloned pandas repo    python -m pip install -r requirements-dev.txt  **Windows**  Below is a brief overview on how to set-up a virtual environment with Powershell ``<span class="title-ref"> under Windows. For details please refer to the \`official virtualenv user guide \<https://virtualenv.pypa.io/en/latest/user\_guide.html\#activators\></span>\_\_.

Use an ENV\_DIR of your choice. We'll use `~\\virtualenvs\\pandas-dev` where `~` is the folder pointed to by either `$env:USERPROFILE` (Powershell) or `%USERPROFILE%` (cmd.exe) environment variable. Any parent directories should already exist.

`` `powershell    # Create a virtual environment    python -m venv $env:USERPROFILE\virtualenvs\pandas-dev     # Activate the virtualenv. Use activate.bat for cmd.exe    ~\virtualenvs\pandas-dev\Scripts\Activate.ps1     # Install the build dependencies    python -m pip install -r requirements-dev.txt  Option 3: using Docker ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

pandas provides a `DockerFile` in the root directory to build a Docker image with a full pandas development environment.

**Docker Commands**

Build the Docker image:

    # Build the image
    docker build -t pandas-dev .

Run Container:

    # Run a container and bind your local repo to the container
    # This command assumes you are running from your local repo
    # but if not alter ${PWD} to match your local repo path
    docker run -it --rm -v ${PWD}:/home/pandas pandas-dev

*Even easier, you can integrate Docker with the following IDEs:*

**Visual Studio Code**

You can use the DockerFile to launch a remote session with Visual Studio Code, a popular free IDE, using the `.devcontainer.json` file. See <https://code.visualstudio.com/docs/remote/containers> for details.

**PyCharm (Professional)**

Enable Docker support and use the Services tool window to build and manage images as well as run and interact with containers. See <https://www.jetbrains.com/help/pycharm/docker.html> for details.

### Option 4: using Gitpod

Gitpod is an open-source platform that automatically creates the correct development environment right in your browser, reducing the need to install local development environments and deal with incompatible dependencies.

If you are a Windows user, unfamiliar with using the command line or building pandas for the first time, it is often faster to build with Gitpod. Here are the in-depth instructions for \[building pandas with GitPod \<contributing-gitpod\>\](\#building-pandas-with-gitpod-\<contributing-gitpod\>).

## Step 3: build and install pandas

There are currently two supported ways of building pandas, pip/meson and setuptools(setup.py). Historically, pandas has only supported using setuptools to build pandas. However, this method requires a lot of convoluted code in setup.py and also has many issues in compiling pandas in parallel due to limitations in setuptools.

The newer build system, invokes the meson backend through pip (via a [PEP 517](https://peps.python.org/pep-0517/) build). It automatically uses all available cores on your CPU, and also avoids the need for manual rebuilds by rebuilding automatically whenever pandas is imported (with an editable install).

For these reasons, you should compile pandas with meson. Because the meson build system is newer, you may find bugs/minor issues as it matures. You can report these bugs [here](https://github.com/pandas-dev/pandas/issues/49683).

To compile pandas with meson, run:

    # Build and install pandas
    # By default, this will print verbose output
    # showing the "rebuild" taking place on import (see section below for explanation)
    # If you do not want to see this, omit everything after --no-build-isolation
    python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true

<div class="note">

<div class="title">

Note

</div>

The version number is pulled from the latest repository tag. Be sure to fetch the latest tags from upstream before building:

    # set the upstream repository, if not done already, and fetch the latest tags

git remote add upstream <https://github.com/pandas-dev/pandas.git> git fetch upstream --tags

</div>

**Build options**

It is possible to pass options from the pip frontend to the meson backend if you would like to configure your install. Occasionally, you'll want to use this to adjust the build directory, and/or toggle debug/optimization levels.

You can pass a build directory to pandas by appending `-Cbuilddir="your builddir here"` to your pip command. This option allows you to configure where meson stores your built C extensions, and allows for fast rebuilds.

Sometimes, it might be useful to compile pandas with debugging symbols, when debugging C extensions. Appending `-Csetup-args="-Ddebug=true"` will do the trick.

With pip, it is possible to chain together multiple config settings (for example specifying both a build directory and building with debug symbols would look like `-Cbuilddir="your builddir here" -Csetup-args="-Dbuildtype=debug"`.

**Compiling pandas with setup.py**

<div class="note">

<div class="title">

Note

</div>

This method of compiling pandas will be deprecated and removed very soon, as the meson backend matures.

</div>

To compile pandas with setuptools, run:

    python setup.py develop

<div class="note">

<div class="title">

Note

</div>

If pandas is already installed (via meson), you have to uninstall it first:

python -m pip uninstall pandas

</div>

This is because python setup.py develop will not uninstall the loader script that `meson-python` uses to import the extension from the build folder, which may cause errors such as an `FileNotFoundError` to be raised.

<div class="note">

<div class="title">

Note

</div>

You will need to repeat this step each time the C extensions change, for example if you modified any file in `pandas/_libs` or if you did a fetch and merge from `upstream/main`.

</div>

**Checking the build**

At this point you should be able to import pandas from your locally built version:

    $ python
    >>> import pandas
    >>> print(pandas.__version__)  # note: the exact output may differ
    2.0.0.dev0+880.g2b9e661fbb.dirty

At this point you may want to try [running the test suite](https://pandas.pydata.org/docs/dev/development/contributing_codebase.html#running-the-test-suite).

**Keeping up to date with the latest build**

When building pandas with meson, importing pandas will automatically trigger a rebuild, even when C/Cython files are modified. By default, no output will be produced by this rebuild (the import will just take longer). If you would like to see meson's output when importing pandas, you can set the environment variable `MESONPY_EDITABLE_VERBOSE`. For example, this would be:

    # On Linux/macOS
    MESONPY_EDITABLE_VERBOSE=1 python
    
    # Windows
    set MESONPY_EDITABLE_VERBOSE=1 # Only need to set this once per session
    python

If you would like to see this verbose output every time, you can set the `editable-verbose` config setting to `true` like so:

    python -m pip install -ve . -Ceditable-verbose=true

<div class="tip">

<div class="title">

Tip

</div>

If you ever find yourself wondering whether setuptools or meson was used to build your pandas, you can check the value of `pandas._built_with_meson`, which will be true if meson was used to compile pandas.

</div>

---

contributing_gitpod.md

---

# Using Gitpod for pandas development

This section of the documentation will guide you through:

  - using Gitpod for your pandas development environment
  - creating a personal fork of the pandas repository on GitHub
  - a quick tour of pandas and VSCode
  - working on the pandas documentation in Gitpod

## Gitpod

[Gitpod](https://www.gitpod.io/) is an open-source platform for automated and ready-to-code development environments. It enables developers to describe their dev environment as code and start instant and fresh development environments for each new task directly from your browser. This reduces the need to install local development environments and deal with incompatible dependencies.

## Gitpod GitHub integration

To be able to use Gitpod, you will need to have the Gitpod app installed on your GitHub account, so if you do not have an account yet, you will need to create one first.

To get started just login at [Gitpod](https://www.gitpod.io/), and grant the appropriate permissions to GitHub.

We have built a python 3.10 environment and all development dependencies will install when the environment starts.

## Forking the pandas repository

The best way to work on pandas as a contributor is by making a fork of the repository first.

1.  Browse to the [pandas repository on GitHub](https://github.com/pandas-dev/pandas) and [create your own fork](https://help.github.com/en/articles/fork-a-repo).
2.  Browse to your fork. Your fork will have a URL like <https://github.com/noatamir/pandas-dev>, except with your GitHub username in place of `noatamir`.

## Starting Gitpod

Once you have authenticated to Gitpod through GitHub, you can install the [Gitpod Chromium or Firefox browser extension](https://www.gitpod.io/docs/browser-extension) which will add a **Gitpod** button next to the **Code** button in the repository:

![pandas repository with Gitpod button screenshot](./gitpod-imgs/pandas-github.png)

1.  If you install the extension - you can click the **Gitpod** button to start a new workspace.

2.  Alternatively, if you do not want to install the browser extension, you can visit <https://gitpod.io/#https://github.com/USERNAME/pandas> replacing `USERNAME` with your GitHub username.

3.  In both cases, this will open a new tab on your web browser and start building your development environment. Please note this can take a few minutes.

4.  Once the build is complete, you will be directed to your workspace, including the VSCode editor and all the dependencies you need to work on pandas. The first time you start your workspace, you will notice that there might be some actions running. This will ensure that you have a development version of pandas installed.

5.  When your workspace is ready, you can \[test the build\<contributing.running\_tests\>\](\#test-the-build\<contributing.running\_tests\>) by entering:
    
        $ python -m pytest pandas
    
    Note that this command takes a while to run, so once you've confirmed it's running you may want to cancel it using ctrl-c.

## Quick workspace tour

Gitpod uses VSCode as the editor. If you have not used this editor before, you can check the Getting started [VSCode docs](https://code.visualstudio.com/docs/getstarted/tips-and-tricks) to familiarize yourself with it.

Your workspace will look similar to the image below:

![Gitpod workspace screenshot](./gitpod-imgs/gitpod-workspace.png)

We have marked some important sections in the editor:

1.  Your current Python interpreter - by default, this is `pandas-dev` and should be displayed in the status bar and on your terminal. You do not need to activate the conda environment as this will always be activated for you.
2.  Your current branch is always displayed in the status bar. You can also use this button to change or create branches.
3.  GitHub Pull Requests extension - you can use this to work with Pull Requests from your workspace.
4.  Marketplace extensions - we have added some essential extensions to the pandas Gitpod. Still, you can also install other extensions or syntax highlighting themes for your user, and these will be preserved for you.
5.  Your workspace directory - by default, it is `/workspace/pandas-dev`. **Do not change this** as this is the only directory preserved in Gitpod.

We have also pre-installed a few tools and VSCode extensions to help with the development experience:

  - [VSCode rst extension](https://marketplace.visualstudio.com/items?itemName=lextudio.restructuredtext)
  - [Markdown All in One](https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one)
  - [VSCode Gitlens extension](https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens)
  - [VSCode Git Graph extension](https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph)

## Development workflow with Gitpod

The \[contributing\](\#contributing) section of this documentation contains information regarding the pandas development workflow. Make sure to check this before working on your contributions.

When using Gitpod, git is pre configured for you:

1.  You do not need to configure your git username, and email as this should be done for you as you authenticated through GitHub. Unless you are using GitHub feature to keep email address private. You can check the git configuration with the command `git config --list` in your terminal. Use `git config --global user.email â€œyour-secret-email@users.noreply.github.comâ€` to set your email address to the one you use to make commits with your github profile.

2.  As you started your workspace from your own pandas fork, you will by default have both `upstream` and `origin` added as remotes. You can verify this by typing `git remote` on your terminal or by clicking on the **branch name** on the status bar (see image below).
    
    ![Gitpod workspace branches plugin screenshot](./gitpod-imgs/pandas-gitpod-branches.png)

## Rendering the pandas documentation

You can find the detailed documentation on how rendering the documentation with Sphinx works in the \[contributing.howto-build-docs\](\#contributing.howto-build-docs) section. To build the full docs you need to run the following command in the `/doc` directory:

    $ cd doc
    $ python make.py html

Alternatively you can build a single page with:

    python make.py --single development/contributing_gitpod.rst

You have two main options to render the documentation in Gitpod.

### Option 1: using Liveserve

1.  View the documentation in `pandas/doc/build/html`.

2.  To see the rendered version of a page, you can right-click on the `.html` file and click on **Open with Live Serve**. Alternatively, you can open the file in the editor and click on the **Go live** button on the status bar.
    
    > ![Gitpod workspace VSCode start live serve screenshot](./gitpod-imgs/vscode-statusbar.png)

3.  A simple browser will open to the right-hand side of the editor. We recommend closing it and click on the **Open in browser** button in the pop-up.

4.  To stop the server click on the **Port: 5500** button on the status bar.

### Option 2: using the rst extension

A quick and easy way to see live changes in a `.rst` file as you work on it uses the rst extension with docutils.

<div class="note">

<div class="title">

Note

</div>

This will generate a simple live preview of the document without the `html` theme, and some backlinks might not be added correctly. But it is an easy and lightweight way to get instant feedback on your work, without building the html files.

</div>

1.  Open any of the source documentation files located in `doc/source` in the editor.

2.  Open VSCode Command Palette with `Cmd-Shift-P` in Mac or `Ctrl-Shift-P` in Linux and Windows. Start typing "restructured" and choose either "Open preview" or "Open preview to the Side".
    
    > ![Gitpod workspace VSCode open rst screenshot](./gitpod-imgs/vscode-rst.png)

3.  As you work on the document, you will see a live rendering of it on the editor.
    
    > ![Gitpod workspace VSCode rst rendering screenshot](./gitpod-imgs/rst-rendering.png)

If you want to see the final output with the `html` theme you will need to rebuild the docs with `make html` and use Live Serve as described in option 1.

## FAQ's and troubleshooting

### How long is my Gitpod workspace kept for?

Your stopped workspace will be kept for 14 days and deleted afterwards if you do not use them.

### Can I come back to a previous workspace?

Yes, let's say you stepped away for a while and you want to carry on working on your pandas contributions. You need to visit <https://gitpod.io/workspaces> and click on the workspace you want to spin up again. All your changes will be there as you last left them.

### Can I install additional VSCode extensions?

Absolutely\! Any extensions you installed will be installed in your own workspace and preserved.

### I registered on Gitpod but I still cannot see a `Gitpod` button in my repositories.

Head to <https://gitpod.io/integrations> and make sure you are logged in. Hover over GitHub and click on the three buttons that appear on the right. Click on edit permissions and make sure you have `user:email`, `read:user`, and `public_repo` checked. Click on **Update Permissions** and confirm the changes in the GitHub application page.

![Gitpod integrations - edit GH permissions screenshot](./gitpod-imgs/gitpod-edit-permissions-gh.png)

### How long does my workspace stay active if I'm not using it?

If you keep your workspace open in a browser tab but don't interact with it, it will shut down after 30 minutes. If you close the browser tab, it will shut down after 3 minutes.

### My terminal is blank - there is no cursor and it's completely unresponsive

Unfortunately this is a known-issue on Gitpod's side. You can sort this issue in two ways:

1.  Create a new Gitpod workspace altogether.
2.  Head to your [Gitpod dashboard](https://gitpod.io/workspaces) and locate the running workspace. Hover on it and click on the **three dots menu** and then click on **Stop**. When the workspace is completely stopped you can click on its name to restart it again.

![Gitpod dashboard and workspace menu screenshot](./gitpod-imgs/gitpod-dashboard-stop.png)

### I authenticated through GitHub but I still cannot commit to the repository through Gitpod.

Head to <https://gitpod.io/integrations> and make sure you are logged in. Hover over GitHub and click on the three buttons that appear on the right. Click on edit permissions and make sure you have `public_repo` checked. Click on **Update Permissions** and confirm the changes in the GitHub application page.

![Gitpod integrations - edit GH repository permissions screenshot](./gitpod-imgs/gitpod-edit-permissions-repo.png)

## Acknowledgments

This page is lightly adapted from the [NumPy](https://www.numpy.org/) project .

---

copy_on_write.md

---

<div id="copy_on_write_dev">

{{ header }}

</div>

# Copy on write

Copy on Write is a mechanism to simplify the indexing API and improve performance through avoiding copies if possible. CoW means that any DataFrame or Series derived from another in any way always behaves as a copy. An explanation on how to use Copy on Write efficiently can be found \[here \<copy\_on\_write\>\](\#here-\<copy\_on\_write\>).

## Reference tracking

To be able to determine if we have to make a copy when writing into a DataFrame, we have to be aware if the values are shared with another DataFrame. pandas keeps track of all `Blocks` that share values with another block internally to be able to tell when a copy needs to be triggered. The reference tracking mechanism is implemented on the Block level.

We use a custom reference tracker object, `BlockValuesRefs`, that keeps track of every block, whose values share memory with each other. The reference is held through a weak-reference. Every pair of blocks that share some memory should point to the same `BlockValuesRefs` object. If one block goes out of scope, the reference to this block dies. As a consequence, the reference tracker object always knows how many blocks are alive and share memory.

Whenever a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> object is sharing data with another object, it is required that each of those objects have its own BlockManager and Block objects. Thus, in other words, one Block instance (that is held by a DataFrame, not necessarily for intermediate objects) should always be uniquely used for only a single DataFrame/Series object. For example, when you want to use the same Block for another object, you can create a shallow copy of the Block instance with `block.copy(deep=False)` (which will create a new Block instance with the same underlying values and which will correctly set up the references).

We can ask the reference tracking object if there is another block alive that shares data with us before writing into the values. We can trigger a copy before writing if there is in fact another block alive.

---

debugging_extensions.md

---

<div id="debugging_c_extensions">

{{ header }}

</div>

# Debugging C extensions

pandas uses Cython and C/C++ [extension modules](https://docs.python.org/3/extending/extending.html) to optimize performance. Unfortunately, the standard Python debugger does not allow you to step into these extensions. Cython extensions can be debugged with the [Cython debugger](https://docs.cython.org/en/latest/src/userguide/debugging.html) and C/C++ extensions can be debugged using the tools shipped with your platform's compiler.

For Python developers with limited or no C/C++ experience this can seem a daunting task. Core developer Will Ayd has written a 3 part blog series to help guide you from the standard Python debugger into these other tools:

> 1.  [Fundamental Python Debugging Part 1 - Python](https://willayd.com/fundamental-python-debugging-part-1-python.html)
> 2.  [Fundamental Python Debugging Part 2 - Python Extensions](https://willayd.com/fundamental-python-debugging-part-2-python-extensions.html)
> 3.  [Fundamental Python Debugging Part 3 - Cython Extensions](https://willayd.com/fundamental-python-debugging-part-3-cython-extensions.html)

## Debugging locally

By default building pandas from source will generate a release build. To generate a development build you can type:

    pip install -ve . --no-build-isolation -Cbuilddir="debug" -Csetup-args="-Dbuildtype=debug"

\> **Note** \> conda environments update CFLAGS/CPPFLAGS with flags that are geared towards generating releases. If using conda, you may need to set `CFLAGS="$CFLAGS -O0"` and `CPPFLAGS="$CPPFLAGS -O0"` to ensure optimizations are turned off for debugging

By specifying `builddir="debug"` all of the targets will be built and placed in the debug directory relative to the project root. This helps to keep your debug and release artifacts separate; you are of course able to choose a different directory name or omit altogether if you do not care to separate build types.

## Using Docker

To simplify the debugging process, pandas has created a Docker image with a debug build of Python and the gdb/Cython debuggers pre-installed. You may either `docker pull pandas/pandas-debug` to get access to this image or build it from the `tooling/debug` folder locally.

You can then mount your pandas repository into this image via:

`` `sh    docker run --rm -it -w /data -v ${PWD}:/data pandas/pandas-debug  Inside the image, you can use meson to build/install pandas and place the build artifacts into a ``debug`folder using a command as follows:  .. code-block:: sh      python -m pip install -ve . --no-build-isolation -Cbuilddir="debug" -Csetup-args="-Dbuildtype=debug"  If planning to use cygdb, the files required by that application are placed within the build folder. So you have to first`cd``to the build folder, then start that application.  .. code-block:: sh     cd debug    cygdb  Within the debugger you can use `cygdb commands <https://docs.cython.org/en/latest/src/userguide/debugging.html#using-the-debugger>`_ to navigate cython extensions.  Editor support``\` --------------

The meson build system generates a [compilation database](https://clang.llvm.org/docs/JSONCompilationDatabase.html) automatically and places it in the build directory. Many language servers and IDEs can use this information to provide code-completion, go-to-definition and error checking support as you type.

How each language server / IDE chooses to look for the compilation database may vary. When in doubt you may want to create a symlink at the root of the project that points to the compilation database in your build directory. Assuming you used *debug* as your directory name, you can run:

    ln -s debug/compile_commands.json .

---

developer.md

---

<div id="developer">

{{ header }}

</div>

<div class="currentmodule">

pandas

</div>

# Developer

This section will focus on downstream applications of pandas.

## Storing pandas DataFrame objects in Apache Parquet format

The [Apache Parquet](https://github.com/apache/parquet-format) format provides key-value metadata at the file and column level, stored in the footer of the Parquet file:

`` `shell   5: optional list<KeyValue> key_value_metadata  where ``KeyValue`is  .. code-block:: shell     struct KeyValue {      1: required string key      2: optional string value    }  So that a`pandas.DataFrame`can be faithfully reconstructed, we store a`<span class="title-ref"> </span><span class="title-ref">pandas</span><span class="title-ref"> metadata key in the </span><span class="title-ref">FileMetaData</span>\` with the value stored as :

`` `text    {'index_columns': [<descr0>, <descr1>, ...],     'column_indexes': [<ci0>, <ci1>, ..., <ciN>],     'columns': [<c0>, <c1>, ...],     'pandas_version': $VERSION,     'creator': {       'library': $LIBRARY,       'version': $LIBRARY_VERSION     }}  The "descriptor" values ``\<descr0\>`in the`'index\_columns'`field are`\` strings (referring to a column) or dictionaries with values as described below.

The `<c0>`/`<ci0>` and so forth are dictionaries containing the metadata for each column, *including the index columns*. This has JSON form:

`` `text    {'name': column_name,     'field_name': parquet_column_name,     'pandas_type': pandas_type,     'numpy_type': numpy_type,     'metadata': metadata}  See below for the detailed specification for these.  Index metadata descriptors ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

`RangeIndex` can be stored as metadata only, not requiring serialization. The descriptor format for these as is follows:

`` `python    index = pd.RangeIndex(0, 10, 2)    {        "kind": "range",        "name": index.name,        "start": index.start,        "stop": index.stop,        "step": index.step,    }  Other index types must be serialized as data columns along with the other ``<span class="title-ref"> DataFrame columns. The metadata for these is a string indicating the name of the field in the data columns, for example </span><span class="title-ref">'\_\_index\_level\_0\_\_'</span>\`.

If an index has a non-None `name` attribute, and there is no other column with a name matching that value, then the `index.name` value can be used as the descriptor. Otherwise (for unnamed indexes and ones with names colliding with other column names) a disambiguating name with pattern matching `__index_level_\d+__` should be used. In cases of named indexes as data columns, `name` attribute is always stored in the column descriptors as above.

### Column metadata

`pandas_type` is the logical type of the column, and is one of:

  - Boolean: `'bool'`
  - Integers: `'int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64'`
  - Floats: `'float16', 'float32', 'float64'`
  - Date and Time Types: `'datetime', 'datetimetz'`, `'timedelta'`
  - String: `'unicode', 'bytes'`
  - Categorical: `'categorical'`
  - Other Python objects: `'object'`

The `numpy_type` is the physical storage type of the column, which is the result of `str(dtype)` for the underlying NumPy array that holds the data. So for `datetimetz` this is `datetime64[ns]` and for categorical, it may be any of the supported integer categorical types.

The `metadata` field is `None` except for:

  - `datetimetz`: `{'timezone': zone, 'unit': 'ns'}`, e.g. `{'timezone', 'America/New_York', 'unit': 'ns'}`. The `'unit'` is optional, and if omitted it is assumed to be nanoseconds.

  - `categorical`: `{'num_categories': K, 'ordered': is_ordered, 'type': $TYPE}`
    
    >   - Here `'type'` is optional, and can be a nested pandas type specification here (but not categorical)

  - `unicode`: `{'encoding': encoding}`
    
    >   - The encoding is optional, and if not present is UTF-8

  - `object`: `{'encoding': encoding}`. Objects can be serialized and stored in `BYTE_ARRAY` Parquet columns. The encoding can be one of:
    
    >   - `'pickle'`
    >   - `'bson'`
    >   - `'json'`

  - `timedelta`: `{'unit': 'ns'}`. The `'unit'` is optional, and if omitted it is assumed to be nanoseconds. This metadata is optional altogether

For types other than these, the `'metadata'` key can be omitted. Implementations can assume `None` if the key is not present.

As an example of fully-formed metadata:

`` `text {'index_columns': ['__index_level_0__'],  'column_indexes': [      {'name': None,       'field_name': 'None',       'pandas_type': 'unicode',       'numpy_type': 'object',       'metadata': {'encoding': 'UTF-8'}}  ],  'columns': [      {'name': 'c0',       'field_name': 'c0',       'pandas_type': 'int8',       'numpy_type': 'int8',       'metadata': None},      {'name': 'c1',       'field_name': 'c1',       'pandas_type': 'bytes',       'numpy_type': 'object',       'metadata': None},      {'name': 'c2',       'field_name': 'c2',       'pandas_type': 'categorical',       'numpy_type': 'int16',       'metadata': {'num_categories': 1000, 'ordered': False}},      {'name': 'c3',       'field_name': 'c3',       'pandas_type': 'datetimetz',       'numpy_type': 'datetime64[ns]',       'metadata': {'timezone': 'America/Los_Angeles'}},      {'name': 'c4',       'field_name': 'c4',       'pandas_type': 'object',       'numpy_type': 'object',       'metadata': {'encoding': 'pickle'}},      {'name': None,       'field_name': '__index_level_0__',       'pandas_type': 'int64',       'numpy_type': 'int64',       'metadata': None}  ],  'pandas_version': '1.4.0',  'creator': {    'library': 'pyarrow',    'version': '0.13.0'  }} ``\`

---

extending.md

---

<div id="extending">

{{ header }}

</div>

# Extending pandas

While pandas provides a rich set of methods, containers, and data types, your needs may not be fully satisfied. pandas offers a few options for extending pandas.

## Registering custom accessors

Libraries can use the decorators <span class="title-ref">pandas.api.extensions.register\_dataframe\_accessor</span>, <span class="title-ref">pandas.api.extensions.register\_series\_accessor</span>, and <span class="title-ref">pandas.api.extensions.register\_index\_accessor</span>, to add additional "namespaces" to pandas objects. All of these follow a similar convention: you decorate a class, providing the name of attribute to add. The class's `__init__` method gets the object being decorated. For example:

`` `python    @pd.api.extensions.register_dataframe_accessor("geo")    class GeoAccessor:        def __init__(self, pandas_obj):            self._validate(pandas_obj)            self._obj = pandas_obj         @staticmethod        def _validate(obj):            # verify there is a column latitude and a column longitude            if "latitude" not in obj.columns or "longitude" not in obj.columns:                raise AttributeError("Must have 'latitude' and 'longitude'.")         @property        def center(self):            # return the geographic center point of this DataFrame            lat = self._obj.latitude            lon = self._obj.longitude            return (float(lon.mean()), float(lat.mean()))         def plot(self):            # plot this array's data on a map, e.g., using Cartopy            pass  Now users can access your methods using the ``geo`namespace:        >>> ds = pd.DataFrame(       ...     {"longitude": np.linspace(0, 10), "latitude": np.linspace(0, 20)}       ... )       >>> ds.geo.center       (5.0, 10.0)       >>> ds.geo.plot()       # plots data on a map  This can be a convenient way to extend pandas objects without subclassing them.`<span class="title-ref"> If you write a custom accessor, make a pull request adding it to our \`ecosystem \<https://pandas.pydata.org/community/ecosystem.html\></span>\_ page.

We highly recommend validating the data in your accessor's `__init__`. In our `GeoAccessor`, we validate that the data contains the expected columns, raising an `AttributeError` when the validation fails. For a `Series` accessor, you should validate the `dtype` if the accessor applies only to certain dtypes.

## Extension types

\> **Note** \> The <span class="title-ref">pandas.api.extensions.ExtensionDtype</span> and <span class="title-ref">pandas.api.extensions.ExtensionArray</span> APIs were experimental prior to pandas 1.5. Starting with version 1.5, future changes will follow the \[pandas deprecation policy \<policies.version\>\](\#pandas-deprecation-policy-\<policies.version\>).

pandas defines an interface for implementing data types and arrays that *extend* NumPy's type system. pandas itself uses the extension system for some types that aren't built into NumPy (categorical, period, interval, datetime with timezone).

Libraries can define a custom array and data type. When pandas encounters these objects, they will be handled properly (i.e. not converted to an ndarray of objects). Many methods like <span class="title-ref">pandas.isna</span> will dispatch to the extension type's implementation.

If you're building a library that implements the interface, please publicize it on [the ecosystem page](https://pandas.pydata.org/community/ecosystem.html).

The interface consists of two classes.

### <span class="title-ref">\~pandas.api.extensions.ExtensionDtype</span>

A <span class="title-ref">pandas.api.extensions.ExtensionDtype</span> is similar to a `numpy.dtype` object. It describes the data type. Implementers are responsible for a few unique items like the name.

One particularly important item is the `type` property. This should be the class that is the scalar type for your data. For example, if you were writing an extension array for IP Address data, this might be `ipaddress.IPv4Address`.

See the [extension dtype source](https://github.com/pandas-dev/pandas/blob/main/pandas/core/dtypes/base.py) for interface definition.

<span class="title-ref">pandas.api.extensions.ExtensionDtype</span> can be registered to pandas to allow creation via a string dtype name. This allows one to instantiate `Series` and `.astype()` with a registered string name, for example `'category'` is a registered string accessor for the `CategoricalDtype`.

See the [extension dtype dtypes](https://github.com/pandas-dev/pandas/blob/main/pandas/core/dtypes/dtypes.py) for more on how to register dtypes.

### <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>

This class provides all the array-like functionality. ExtensionArrays are limited to 1 dimension. An ExtensionArray is linked to an ExtensionDtype via the `dtype` attribute.

pandas makes no restrictions on how an extension array is created via its `__new__` or `__init__`, and puts no restrictions on how you store your data. We do require that your array be convertible to a NumPy array, even if this is relatively expensive (as it is for `Categorical`).

They may be backed by none, one, or many NumPy arrays. For example, <span class="title-ref">pandas.Categorical</span> is an extension array backed by two arrays, one for codes and one for categories. An array of IPv6 addresses may be backed by a NumPy structured array with two fields, one for the lower 64 bits and one for the upper 64 bits. Or they may be backed by some other storage type, like Python lists.

See the [extension array source](https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/base.py) for the interface definition. The docstrings and comments contain guidance for properly implementing the interface.

### <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span> operator support

By default, there are no operators defined for the class <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>. There are two approaches for providing operator support for your ExtensionArray:

1.  Define each of the operators on your `ExtensionArray` subclass.
2.  Use an operator implementation from pandas that depends on operators that are already defined on the underlying elements (scalars) of the ExtensionArray.

\> **Note** \> Regardless of the approach, you may want to set `__array_priority__` if you want your implementation to be called when involved in binary operations with NumPy arrays.

For the first approach, you define selected operators, e.g., `__add__`, `__le__`, etc. that you want your `ExtensionArray` subclass to support.

The second approach assumes that the underlying elements (i.e., scalar type) of the `ExtensionArray` have the individual operators already defined. In other words, if your `ExtensionArray` named `MyExtensionArray` is implemented so that each element is an instance of the class `MyExtensionElement`, then if the operators are defined for `MyExtensionElement`, the second approach will automatically define the operators for `MyExtensionArray`.

A mixin class, <span class="title-ref">\~pandas.api.extensions.ExtensionScalarOpsMixin</span> supports this second approach. If developing an `ExtensionArray` subclass, for example `MyExtensionArray`, can simply include `ExtensionScalarOpsMixin` as a parent class of `MyExtensionArray`, and then call the methods <span class="title-ref">\~MyExtensionArray.\_add\_arithmetic\_ops</span> and/or <span class="title-ref">\~MyExtensionArray.\_add\_comparison\_ops</span> to hook the operators into your `MyExtensionArray` class, as follows:

`` `python     from pandas.api.extensions import ExtensionArray, ExtensionScalarOpsMixin       class MyExtensionArray(ExtensionArray, ExtensionScalarOpsMixin):         pass       MyExtensionArray._add_arithmetic_ops()     MyExtensionArray._add_comparison_ops()   > **Note** >     Since ``pandas`automatically calls the underlying operator on each    element one-by-one, this might not be as performant as implementing your own    version of the associated operators directly on the`ExtensionArray`.  For arithmetic operations, this implementation will try to reconstruct a new`<span class="title-ref"> </span><span class="title-ref">ExtensionArray</span><span class="title-ref"> with the result of the element-wise operation. Whether or not that succeeds depends on whether the operation returns a result that's valid for the </span><span class="title-ref">ExtensionArray</span><span class="title-ref">. If an </span><span class="title-ref">ExtensionArray</span>\` cannot be reconstructed, an ndarray containing the scalars returned instead.

For ease of implementation and consistency with operations between pandas and NumPy ndarrays, we recommend *not* handling Series and Indexes in your binary ops. Instead, you should detect these cases and return `NotImplemented`. When pandas encounters an operation like `op(Series, ExtensionArray)`, pandas will

1.  unbox the array from the `Series` (`Series.array`)
2.  call `result = op(values, ExtensionArray)`
3.  re-box the result in a `Series`

### NumPy universal functions

<span class="title-ref">Series</span> implements `__array_ufunc__`. As part of the implementation, pandas unboxes the `ExtensionArray` from the <span class="title-ref">Series</span>, applies the ufunc, and re-boxes it if necessary.

If applicable, we highly recommend that you implement `__array_ufunc__` in your extension array to avoid coercion to an ndarray. See [the NumPy documentation](https://numpy.org/doc/stable/reference/generated/numpy.lib.mixins.NDArrayOperatorsMixin.html) for an example.

As part of your implementation, we require that you defer to pandas when a pandas container (<span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span>, <span class="title-ref">Index</span>) is detected in `inputs`. If any of those is present, you should return `NotImplemented`. pandas will take care of unboxing the array from the container and re-calling the ufunc with the unwrapped input.

### Testing extension arrays

We provide a test suite for ensuring that your extension arrays satisfy the expected behavior. To use the test suite, you must provide several pytest fixtures and inherit from the base test class. The required fixtures are found in <https://github.com/pandas-dev/pandas/blob/main/pandas/tests/extension/conftest.py>.

To use a test, subclass it:

`` `python    from pandas.tests.extension import base      class TestConstructors(base.BaseConstructorsTests):        pass   See https://github.com/pandas-dev/pandas/blob/main/pandas/tests/extension/base/__init__.py ``\` for a list of all the tests available.

### Compatibility with Apache Arrow

An `ExtensionArray` can support conversion to / from `pyarrow` arrays (and thus support for example serialization to the Parquet file format) by implementing two methods: `ExtensionArray.__arrow_array__` and `ExtensionDtype.__from_arrow__`.

The `ExtensionArray.__arrow_array__` ensures that `pyarrow` knowns how to convert the specific extension array into a `pyarrow.Array` (also when included as a column in a pandas DataFrame):

`` `python     class MyExtensionArray(ExtensionArray):         ...          def __arrow_array__(self, type=None):             # convert the underlying array values to a pyarrow Array             import pyarrow              return pyarrow.array(..., type=type)  The ``ExtensionDtype.\_\_from\_arrow\_\_`method then controls the conversion`<span class="title-ref"> back from pyarrow to a pandas ExtensionArray. This method receives a pyarrow </span><span class="title-ref">Array</span><span class="title-ref"> or </span><span class="title-ref">ChunkedArray</span><span class="title-ref"> as only argument and is expected to return the appropriate pandas </span><span class="title-ref">ExtensionArray</span>\` for this dtype and the passed values:

`` `none     class ExtensionDtype:         ...          def __from_arrow__(self, array: pyarrow.Array/ChunkedArray) -> ExtensionArray:             ...  See more in the `Arrow documentation <https://arrow.apache.org/docs/python/extending_types.html>`__.  Those methods have been implemented for the nullable integer and string extension ``\` dtypes included in pandas, and ensure roundtrip to pyarrow and the Parquet file format.

## Subclassing pandas data structures

<div class="warning">

<div class="title">

Warning

</div>

There are some easier alternatives before considering subclassing `pandas` data structures.

1.  Extensible method chains with \[pipe \<basics.pipe\>\](\#pipe-\<basics.pipe\>)
2.  Use *composition*. See [here](https://en.wikipedia.org/wiki/Composition_over_inheritance).
3.  Extending by \[registering an accessor \<extending.register-accessors\>\](\#registering-an-accessor-\<extending.register-accessors\>)
4.  Extending by \[extension type \<extending.extension-types\>\](\#extension-type-\<extending.extension-types\>)

</div>

This section describes how to subclass `pandas` data structures to meet more specific needs. There are two points that need attention:

1.  Override constructor properties.
2.  Define original properties

\> **Note** \> You can find a nice example in [geopandas](https://github.com/geopandas/geopandas) project.

### Override constructor properties

Each data structure has several *constructor properties* for returning a new data structure as the result of an operation. By overriding these properties, you can retain subclasses through `pandas` data manipulations.

There are 3 possible constructor properties to be defined on a subclass:

  - `DataFrame/Series._constructor`: Used when a manipulation result has the same dimension as the original.
  - `DataFrame._constructor_sliced`: Used when a `DataFrame` (sub-)class manipulation result should be a `Series` (sub-)class.
  - `Series._constructor_expanddim`: Used when a `Series` (sub-)class manipulation result should be a `DataFrame` (sub-)class, e.g. `Series.to_frame()`.

Below example shows how to define `SubclassedSeries` and `SubclassedDataFrame` overriding constructor properties.

`` `python    class SubclassedSeries(pd.Series):        @property        def _constructor(self):            return SubclassedSeries         @property        def _constructor_expanddim(self):            return SubclassedDataFrame      class SubclassedDataFrame(pd.DataFrame):        @property        def _constructor(self):            return SubclassedDataFrame         @property        def _constructor_sliced(self):            return SubclassedSeries  .. code-block:: python     >>> s = SubclassedSeries([1, 2, 3])    >>> type(s)    <class '__main__.SubclassedSeries'>     >>> to_framed = s.to_frame()    >>> type(to_framed)    <class '__main__.SubclassedDataFrame'>     >>> df = SubclassedDataFrame({"A": [1, 2, 3], "B": [4, 5, 6], "C": [7, 8, 9]})    >>> df       A  B  C    0  1  4  7    1  2  5  8    2  3  6  9     >>> type(df)    <class '__main__.SubclassedDataFrame'>     >>> sliced1 = df[["A", "B"]]    >>> sliced1       A  B    0  1  4    1  2  5    2  3  6     >>> type(sliced1)    <class '__main__.SubclassedDataFrame'>     >>> sliced2 = df["A"]    >>> sliced2    0    1    1    2    2    3    Name: A, dtype: int64     >>> type(sliced2)    <class '__main__.SubclassedSeries'>  Define original properties ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^

To let original data structures have additional properties, you should let `pandas` know what properties are added. `pandas` maps unknown properties to data names overriding `__getattribute__`. Defining original properties can be done in one of 2 ways:

1.  Define `_internal_names` and `_internal_names_set` for temporary properties which WILL NOT be passed to manipulation results.
2.  Define `_metadata` for normal properties which will be passed to manipulation results.

Below is an example to define two original properties, "internal\_cache" as a temporary property and "added\_property" as a normal property

`` `python    class SubclassedDataFrame2(pd.DataFrame):         # temporary properties        _internal_names = pd.DataFrame._internal_names + ["internal_cache"]        _internal_names_set = set(_internal_names)         # normal properties        _metadata = ["added_property"]         @property        def _constructor(self):            return SubclassedDataFrame2  .. code-block:: python     >>> df = SubclassedDataFrame2({"A": [1, 2, 3], "B": [4, 5, 6], "C": [7, 8, 9]})    >>> df       A  B  C    0  1  4  7    1  2  5  8    2  3  6  9     >>> df.internal_cache = "cached"    >>> df.added_property = "property"     >>> df.internal_cache    cached    >>> df.added_property    property     # properties defined in _internal_names is reset after manipulation    >>> df[["A", "B"]].internal_cache    AttributeError: 'SubclassedDataFrame2' object has no attribute 'internal_cache'     # properties defined in _metadata are retained    >>> df[["A", "B"]].added_property    property  .. _extending.plotting-backends:  Plotting backends ``\` -----------------

pandas can be extended with third-party plotting backends. The main idea is letting users select a plotting backend different than the provided one based on Matplotlib. For example:

`` `python     >>> pd.set_option("plotting.backend", "backend.module")     >>> pd.Series([1, 2, 3]).plot()  This would be more or less equivalent to:  .. code-block:: python      >>> import backend.module     >>> backend.module.plot(pd.Series([1, 2, 3]))  The backend module can then use other visualization tools (Bokeh, Altair,...) ``\` to generate the plots.

Libraries implementing the plotting backend should use [entry points](https://setuptools.pypa.io/en/latest/userguide/entry_point.html) to make their backend discoverable to pandas. The key is `"pandas_plotting_backends"`. For example, pandas registers the default "matplotlib" backend as follows.

`` `python    # in setup.py    setup(  # noqa: F821        ...,        entry_points={            "pandas_plotting_backends": [                "matplotlib = pandas:plotting._matplotlib",            ],        },    )   More information on how to implement a third-party plotting backend can be found at ``\` <https://github.com/pandas-dev/pandas/blob/main/pandas/plotting/__init__.py#L1>.

## Arithmetic with 3rd party types

In order to control how arithmetic works between a custom type and a pandas type, implement `__pandas_priority__`. Similar to numpy's `__array_priority__` semantics, arithmetic methods on <span class="title-ref">DataFrame</span>, <span class="title-ref">Series</span>, and <span class="title-ref">Index</span> objects will delegate to `other`, if it has an attribute `__pandas_priority__` with a higher value.

By default, pandas objects try to operate with other objects, even if they are not types known to pandas:

`` `python     >>> pd.Series([1, 2]) + [10, 20]     0    11     1    22     dtype: int64  In the example above, if ``\[10, 20\]`was a custom type that can be understood as a list, pandas objects will still operate with it in the same way.  In some cases, it is useful to delegate to the other type the operation. For example, consider I implement a`<span class="title-ref"> custom list object, and I want the result of adding my custom list with a pandas \`Series</span> to be an instance of my list and not a <span class="title-ref">Series</span> as seen in the previous example. This is now possible by defining the `__pandas_priority__` attribute of my custom list, and setting it to a higher value, than the priority of the pandas objects I want to operate with.

The `__pandas_priority__` of <span class="title-ref">DataFrame</span>, <span class="title-ref">Series</span>, and <span class="title-ref">Index</span> are `4000`, `3000`, and `2000` respectively. The base `ExtensionArray.__pandas_priority__` is `1000`.

`` `python class CustomList(list):     __pandas_priority__ = 5000      def __radd__(self, other):         # return `self` and not the addition for simplicity         return self  custom = CustomList() series = pd.Series([1, 2, 3])  # Series refuses to add custom, since it's an unknown type with higher priority assert series.__add__(custom) is NotImplemented  # This will cause the custom class `__radd__` being used instead assert series + custom is custom ``\`

---

index.md

---

{{ header }}

# Development

<div class="toctree" data-maxdepth="2">

contributing contributing\_environment contributing\_documentation contributing\_codebase maintaining internals copy\_on\_write debugging\_extensions extending developer policies community

</div>

---

internals.md

---

<div id="internals">

{{ header }}

</div>

# Internals

This section will provide a look into some of pandas internals. It's primarily intended for developers of pandas itself.

## Indexing

In pandas there are a few objects implemented which can serve as valid containers for the axis labels:

  - \`Index\`: the generic "ordered set" object, an ndarray of object dtype assuming nothing about its contents. The labels must be hashable (and likely immutable) and unique. Populates a dict of label to location in Cython to do `O(1)` lookups.
  - \`MultiIndex\`: the standard hierarchical index object
  - \`DatetimeIndex\`: An Index object with <span class="title-ref">Timestamp</span> boxed elements (impl are the int64 values)
  - \`TimedeltaIndex\`: An Index object with <span class="title-ref">Timedelta</span> boxed elements (impl are the in64 values)
  - \`PeriodIndex\`: An Index object with Period elements

There are functions that make the creation of a regular index easy:

  - \`date\_range\`: fixed frequency date range generated from a time rule or DateOffset. An ndarray of Python datetime objects
  - \`period\_range\`: fixed frequency date range generated from a time rule or DateOffset. An ndarray of <span class="title-ref">Period</span> objects, representing timespans

\> **Warning** \> Custom <span class="title-ref">Index</span> subclasses are not supported, custom behavior should be implemented using the <span class="title-ref">ExtensionArray</span> interface instead.

### MultiIndex

Internally, the <span class="title-ref">MultiIndex</span> consists of a few things: the **levels**, the integer **codes**, and the level **names**:

<div class="ipython">

python

  - index = pd.MultiIndex.from\_product(  
    \[range(3), \["one", "two"\]\], names=\["first", "second"\]

) index index.levels index.codes index.names

</div>

You can probably guess that the codes determine which unique element is identified with that location at each layer of the index. It's important to note that sortedness is determined **solely** from the integer codes and does not check (or care) whether the levels themselves are sorted. Fortunately, the constructors <span class="title-ref">\~MultiIndex.from\_tuples</span> and <span class="title-ref">\~MultiIndex.from\_arrays</span> ensure that this is true, but if you compute the levels and codes yourself, please be careful.

### Values

pandas extends NumPy's type system with custom types, like <span class="title-ref">Categorical</span> or datetimes with a timezone, so we have multiple notions of "values". For 1-D containers (`Index` classes and `Series`) we have the following convention:

  - `cls._values` refers is the "best possible" array. This could be an `ndarray` or `ExtensionArray`.

So, for example, `Series[category]._values` is a `Categorical`.

## Subclassing pandas data structures

This section has been moved to \[extending.subclassing-pandas\](\#extending.subclassing-pandas).

---

maintaining.md

---

# pandas maintenance

This guide is for pandas' maintainers. It may also be interesting to contributors looking to understand the pandas development process and what steps are necessary to become a maintainer.

The main contributing guide is available at \[contributing\](\#contributing).

## Roles

pandas uses two levels of permissions: **triage** and **core** team members.

Triage members can label and close issues and pull requests.

Core team members can label and close issues and pull request, and can merge pull requests.

GitHub publishes the full [list of permissions](https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/repository-roles-for-an-organization).

## Tasks

pandas is largely a volunteer project, so these tasks shouldn't be read as "expectations" of triage and maintainers. Rather, they're general descriptions of what it means to be a maintainer.

  - Triage newly filed issues (see \[maintaining.triage\](\#maintaining.triage))
  - Review newly opened pull requests
  - Respond to updates on existing issues and pull requests
  - Drive discussion and decisions on stalled issues and pull requests
  - Provide experience / wisdom on API design questions to ensure consistency and maintainability
  - Project organization (run / attend developer meetings, represent pandas)

<https://matthewrocklin.com/blog/2019/05/18/maintainer> may be interesting background reading.

## Issue triage

Triage is an important first step in addressing issues reported by the community, and even partial contributions are a great way to help maintain pandas. Only remove the "Needs Triage" tag once all of the steps below have been completed.

Here's a typical workflow for triaging a newly opened issue.

1.  **Thank the reporter for opening an issue**
    
    The issue tracker is many people's first interaction with the pandas project itself, beyond just using the library. As such, we want it to be a welcoming, pleasant experience.

2.  **Is the necessary information provided?**
    
    Ideally reporters would fill out the issue template, but many don't. If crucial information (like the version of pandas they used), is missing feel free to ask for that and label the issue with "Needs info". The report should follow the guidelines in \[contributing.bug\_reports\](\#contributing.bug\_reports). You may want to link to that if they didn't follow the template.
    
    Make sure that the title accurately reflects the issue. Edit it yourself if it's not clear.

3.  **Is this a duplicate issue?**
    
    We have many open issues. If a new issue is clearly a duplicate, label the new issue as "Duplicate" and close the issue with a link to the original issue. Make sure to still thank the reporter, and encourage them to chime in on the original issue, and perhaps try to fix it.
    
    If the new issue provides relevant information, such as a better or slightly different example, add it to the original issue as a comment or an edit to the original post.

4.  **Is the issue minimal and reproducible**?
    
    For bug reports, we ask that the reporter provide a minimal reproducible example. See <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports> for a good explanation. If the example is not reproducible, or if it's *clearly* not minimal, feel free to ask the reporter if they can provide an example or simplify the provided one. Do acknowledge that writing minimal reproducible examples is hard work. If the reporter is struggling, you can try to write one yourself and we'll edit the original post to include it.
    
    If a reproducible example can't be provided, add the "Needs info" label.
    
    If a reproducible example is provided, but you see a simplification, edit the original post with your simpler reproducible example.
    
    If this is a regression report, post the result of a `git bisect` run. More info on this can be found in the \[maintaining.regressions\](\#maintaining.regressions) section.
    
    Ensure the issue exists on the main branch and that it has the "Needs Triage" tag until all steps have been completed. Add a comment to the issue once you have verified it exists on the main branch, so others know it has been confirmed.

5.  **Is this a clearly defined feature request?**
    
    Generally, pandas prefers to discuss and design new features in issues, before a pull request is made. Encourage the submitter to include a proposed API for the new feature. Having them write a full docstring is a good way to pin down specifics.
    
    Tag new feature requests with "Needs Discussion", as we'll need a discussion from several pandas maintainers before deciding whether the proposal is in scope for pandas.

6.  **Is this a usage question?**
    
    We prefer that usage questions are asked on StackOverflow with the pandas tag. <https://stackoverflow.com/questions/tagged/pandas>
    
    If it's easy to answer, feel free to link to the relevant documentation section, let them know that in the future this kind of question should be on StackOverflow, and close the issue.

7.  **What labels and milestones should I add?**
    
    Apply the relevant labels. This is a bit of an art, and comes with experience. Look at similar issues to get a feel for how things are labeled.
    
    If the issue is clearly defined and the fix seems relatively straightforward, label the issue as "Good first issue".
    
    If the issue is a regression report, add the "Regression" label and the next patch release milestone.
    
    Once you have completed the above, make sure to remove the "Needs Triage" label.

## Investigating regressions

Regressions are bugs that unintentionally break previously working code. The common way to investigate regressions is by using [git bisect](https://git-scm.com/docs/git-bisect), which finds the first commit that introduced the bug.

For example: a user reports that `pd.Series([1, 1]).sum()` returns `3` in pandas version `1.5.0` while in version `1.4.0` it returned `2`. To begin, create a file `t.py` in your pandas directory, which contains

`` `python     import pandas as pd     assert pd.Series([1, 1]).sum() == 2  and then run::      git bisect start     git bisect good v1.4.0     git bisect bad v1.5.0     git bisect run bash -c "python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true; python t.py"  This finds the first commit that changed the behavior. The C extensions have to be ``\` rebuilt at every step, so the search can take a while.

Exit bisect and rebuild the current version:

    git bisect reset
    python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true

Report your findings under the corresponding issue and ping the commit author to get their input.

<div class="note">

<div class="title">

Note

</div>

In the `bisect run` command above, commits are considered good if `t.py` exits with `0` and bad otherwise. When raising an exception is the desired behavior, wrap the code in an appropriate `try/except` statement. See `35685` for more examples.

</div>

## Closing issues

Be delicate here: many people interpret closing an issue as us saying that the conversation is over. It's typically best to give the reporter some time to respond or self-close their issue if it's determined that the behavior is not a bug, or the feature is out of scope. Sometimes reporters just go away though, and we'll close the issue after the conversation has died. If you think an issue should be closed but are not completely sure, please apply the "closing candidate" label and wait for other maintainers to take a look.

## Reviewing pull requests

Anybody can review a pull request: regular contributors, triagers, or core-team members. But only core-team members can merge pull requests when they're ready.

Here are some things to check when reviewing a pull request.

  - Tests should be in a sensible location: in the same file as closely related tests.
  - New public APIs should be included somewhere in `doc/source/reference/`.
  - New / changed API should use the `versionadded` or `versionchanged` directives in the docstring.
  - User-facing changes should have a whatsnew in the appropriate file.
  - Regression tests should reference the original GitHub issue number like `# GH-1234`.
  - The pull request should be labeled and assigned the appropriate milestone (the next patch release for regression fixes and small bug fixes, the next minor milestone otherwise)
  - Changes should comply with our \[policies.version\](\#policies.version).

## Backporting

pandas supports point releases (e.g. `1.4.3`) that aim to:

1.  Fix bugs in new features introduced in the first minor version release.

>   - e.g. If a new feature was added in `1.4` and contains a bug, a fix can be applied in `1.4.3`

2.  Fix bugs that used to work in a few minor releases prior. There should be agreement between core team members that a backport is appropriate.

>   - e.g. If a feature worked in `1.2` and stopped working since `1.3`, a fix can be applied in `1.4.3`.

Since pandas minor releases are based on GitHub branches (e.g. point release of `1.4` are based off the `1.4.x` branch), "backporting" means merging a pull request fix to the `main` branch and correct minor branch associated with the next point release.

By default, if a pull request is assigned to the next point release milestone within the GitHub interface, the backporting process should happen automatically by the `@meeseeksdev` bot once the pull request is merged. A new pull request will be made backporting the pull request to the correct version branch. Sometimes due to merge conflicts, a manual pull request will need to be made addressing the code conflict.

If the bot does not automatically start the backporting process, you can also write a GitHub comment in the merged pull request to trigger the backport:

    @meeseeksdev backport version-branch

This will trigger a workflow which will backport a given change to a branch (e.g. @meeseeksdev backport 1.4.x)

## Cleaning up old issues

Every open issue in pandas has a cost. Open issues make finding duplicates harder, and can make it harder to know what needs to be done in pandas. That said, closing issues isn't a goal on its own. Our goal is to make pandas the best it can be, and that's best done by ensuring that the quality of our open issues is high.

Occasionally, bugs are fixed but the issue isn't linked to in the Pull Request. In these cases, comment that "This has been fixed, but could use a test." and label the issue as "Good First Issue" and "Needs Test".

If an older issue doesn't follow our issue template, edit the original post to include a minimal example, the actual output, and the expected output. Uniformity in issue reports is valuable.

If an older issue lacks a reproducible example, label it as "Needs Info" and ask them to provide one (or write one yourself if possible). If one isn't provide reasonably soon, close it according to the policies in \[maintaining.closing\](\#maintaining.closing).

## Cleaning up old pull requests

Occasionally, contributors are unable to finish off a pull request. If some time has passed (two weeks, say) since the last review requesting changes, gently ask if they're still interested in working on this. If another two weeks or so passes with no response, thank them for their work and then either:

  - close the pull request;
  - push to the contributor's branch to push their work over the finish line (if you're part of `pandas-core`). This can be helpful for pushing an important PR across the line, or for fixing a small merge conflict.

If closing the pull request, then please comment on the original issue that "There's a stalled PR at \#1234 that may be helpful.", and perhaps label the issue as "Good first issue" if the PR was relatively close to being accepted.

## Becoming a pandas maintainer

The full process is outlined in our [governance documents](https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/governance.md). In summary, we're happy to give triage permissions to anyone who shows interest by being helpful on the issue tracker.

The required steps for adding a maintainer are:

1.  Contact the contributor and ask their interest to join.
2.  Add the contributor to the appropriate [GitHub Team](https://github.com/orgs/pandas-dev/teams) if accepted the invitation.

>   - `pandas-core` is for core team members
>   - `pandas-triage` is for pandas triage members

If adding to `pandas-core`, there are two additional steps:

3.  Add the contributor to the pandas Google group.
4.  Create a pull request to add the contributor's GitHub handle to `pandas-dev/pandas/web/pandas/config.yml`.

The current list of core-team members is at <https://github.com/pandas-dev/pandas/blob/main/web/pandas/config.yml>

## Merging pull requests

Only core team members can merge pull requests. We have a few guidelines.

1.  You should typically not self-merge your own pull requests without approval. Exceptions include things like small changes to fix CI (e.g. pinning a package version). Self-merging with approval from other core team members is fine if the change is something you're very confident about.
2.  You should not merge pull requests that have an active discussion, or pull requests that has any `-1` votes from a core maintainer. pandas operates by consensus.
3.  For larger changes, it's good to have a +1 from at least two core team members.

In addition to the items listed in \[maintaining.closing\](\#maintaining.closing), you should verify that the pull request is assigned the correct milestone.

Pull requests merged with a patch-release milestone will typically be backported by our bot. Verify that the bot noticed the merge (it will leave a comment within a minute typically). If a manual backport is needed please do that, and remove the "Needs backport" label once you've done it manually. If you forget to assign a milestone before tagging, you can request the bot to backport it with:

`` `console    @Meeseeksdev backport <branch>   .. _maintaining.release:  Release process ``\` ---------------

The release process makes a snapshot of pandas (a git commit) available to users with a particular version number. After the release the new pandas version will be available in the next places:

  - Git repo with a [new tag](https://github.com/pandas-dev/pandas/tags)
  - Source distribution in a [GitHub release](https://github.com/pandas-dev/pandas/releases)
  - Pip packages in the [PyPI](https://pypi.org/project/pandas/)
  - Conda packages in [conda-forge](https://anaconda.org/conda-forge/pandas)

The process for releasing a new version of pandas is detailed next section.

The instructions contain `<version>` which needs to be replaced with the version to be released (e.g. `1.5.2`). Also the branch to be released `<branch>`, which depends on whether the version being released is the release candidate of a new version, or any other version. Release candidates are released from `main`, while other versions are released from their branch (e.g. `1.5.x`).

### Prerequisites

In order to be able to release a new pandas version, the next permissions are needed:

  - Merge rights to the [pandas](https://github.com/pandas-dev/pandas/) and [pandas-feedstock](https://github.com/conda-forge/pandas-feedstock/) repositories. For the latter, open a PR adding your GitHub username to the conda-forge recipe.
  - Permissions to push to `main` in the pandas repository, to push the new tags.
  - [Write permissions to PyPI](https://github.com/conda-forge/pandas-feedstock/pulls).
  - Access to our website / documentation server. Share your public key with the infrastructure committee to be added to the `authorized_keys` file of the main server user.
  - Access to the social media accounts, to publish the announcements.

### Pre-release

1.  Agree with the core team on the next topics:
      - Release date (major/minor releases happen usually every 6 months, and patch releases monthly until x.x.5, just before the next major/minor)
      - Blockers (issues and PRs that must be part of the release)
      - Next version after the one being released
2.  Update and clean release notes for the version to be released, including:
      - Set the final date of the release
      - Remove any unused bullet point
      - Make sure there are no formatting issues, typos, etc.
3.  Make sure the CI is green for the last commit of the branch being released.
4.  If not a release candidate, make sure all backporting pull requests to the branch being released are merged.
5.  Create a new issue and milestone for the version after the one being released. If the release was a release candidate, we would usually want to create issues and milestones for both the next major/minor, and the next patch release. In the milestone of a patch release, we add the description `on-merge: backport to <branch>`, so tagged PRs are automatically backported to the release branch by our bot.
6.  Change the milestone of all issues and PRs in the milestone being released to the next milestone.

### Release

1.  Create an empty commit and a tag in the last commit of the branch to be released:
    
        git checkout <branch>
        git pull --ff-only upstream <branch>
        git clean -xdf
        git commit --allow-empty --author="pandas Development Team <pandas-dev@python.org>" -m "RLS: <version>"
        git tag -a v<version> -m "Version <version>"  # NOTE that the tag is v1.5.2 with "v" not 1.5.2
        git push upstream <branch> --follow-tags

The docs for the new version will be built and published automatically with the docs job in the CI, which will be triggered when the tag is pushed.

2.  Only if the release is a release candidate, we want to create a new branch for it, immediately after creating the tag. For example, if we are releasing pandas 1.4.0rc0, we would like to create the branch 1.4.x to backport commits to the 1.4 versions. As well as create a tag to mark the start of the development of 1.5.0 (assuming it is the next version):
    
        git checkout -b 1.4.x
        git push upstream 1.4.x
        git checkout main
        git commit --allow-empty -m "Start 1.5.0"
        git tag -a v1.5.0.dev0 -m "DEV: Start 1.5.0"
        git push upstream main --follow-tags

3.  Download the source distribution and wheels from the [wheel staging area](https://anaconda.org/scientific-python-nightly-wheels/pandas). Be careful to make sure that no wheels are missing (e.g. due to failed builds).
    
    Running scripts/download\_wheels.sh with the version that you want to download wheels/the sdist for should do the trick. This script will make a `dist` folder inside your clone of pandas and put the downloaded wheels and sdist there:
    
        scripts/download_wheels.sh <VERSION>

4.  Create a [new GitHub release](https://github.com/pandas-dev/pandas/releases/new):
    
      - Tag: `<version>`
      - Title: `pandas <version>`
      - Description: Copy the description of the last release of the same kind (release candidate, major/minor or patch release)
      - Files: `pandas-<version>.tar.gz` source distribution just generated
      - Set as a pre-release: Only check for a release candidate
      - Set as the latest release: Leave checked, unless releasing a patch release for an older version (e.g. releasing 1.4.5 after 1.5 has been released)

5.  Upload wheels to PyPI:
    
        twine upload pandas/dist/pandas-<version>*.{whl,tar.gz} --skip-existing

6.  The GitHub release will after some hours trigger an [automated conda-forge PR](https://github.com/conda-forge/pandas-feedstock/pulls). (If you don't want to wait, you can open an issue titled `@conda-forge-admin, please update version` to trigger the bot.) Merge it once the CI is green, and it will generate the conda-forge packages.
    
    In case a manual PR needs to be done, the version, sha256 and build fields are the ones that usually need to be changed. If anything else in the recipe has changed since the last release, those changes should be available in `ci/meta.yaml`.

### Post-Release

1.  Update symlinks to stable documentation by logging in to our web server, and editing `/var/www/html/pandas-docs/stable` to point to `version/<latest-version>` for major and minor releases, or `version/<minor>` to `version/<patch>` for patch releases. The exact instructions are (replace the example version numbers by the appropriate ones for the version you are releasing):
    
    >   - Log in to the server and use the correct user.
    >   - `cd /var/www/html/pandas-docs/`
    >   - `ln -sfn version/2.1 stable` (for a major or minor release)
    >   - `ln -sfn version/2.0.3 version/2.0` (for a patch release)

2.  If releasing a major or minor release, open a PR in our source code to update `web/pandas/versions.json`, to have the desired versions in the documentation dropdown menu.

3.  Close the milestone and the issue for the released version.

4.  Create a new issue for the next release, with the estimated date of release.

5.  Open a PR with the placeholder for the release notes of the next version. See for example [the PR for 1.5.3](https://github.com/pandas-dev/pandas/pull/49843/files). Note that the template to use depends on whether it is a major, minor or patch release.

6.  Announce the new release in the official channels (use previous announcements for reference):
    
    >   - The pandas-dev and pydata mailing lists
    >   - Twitter, Mastodon, Telegram and LinkedIn

7.  Update this release instructions to fix anything incorrect and to update about any change since the last release.

---

policies.md

---

# Policies

## Version policy

pandas uses a loose variant of semantic versioning ([SemVer](https://semver.org)) to govern deprecations, API compatibility, and version numbering.

A pandas release number is made up of `MAJOR.MINOR.PATCH`.

API breaking changes should only occur in **major** releases. These changes will be documented, with clear guidance on what is changing, why it's changing, and how to migrate existing code to the new behavior.

Whenever possible, a deprecation path will be provided rather than an outright breaking change.

pandas will introduce deprecations in **minor** releases. These deprecations will preserve the existing behavior while emitting a warning that provide guidance on:

  - How to achieve similar behavior if an alternative is available
  - The pandas version in which the deprecation will be enforced.

We will not introduce new deprecations in patch releases.

Deprecations will only be enforced in **major** releases. For example, if a behavior is deprecated in pandas 1.2.0, it will continue to work, with a warning, for all releases in the 1.x series. The behavior will change and the deprecation removed in the next major release (2.0.0).

\> **Note** \> pandas will sometimes make *behavior changing* bug fixes, as part of minor or patch releases. Whether or not a change is a bug fix or an API-breaking change is a judgement call. We'll do our best, and we invite you to participate in development discussion on the issue tracker or mailing list.

These policies do not apply to features marked as **experimental** in the documentation. pandas may change the behavior of experimental features at any time.

## Python support

pandas mirrors the [SPEC 0 guideline for Python support](https://scientific-python.org/specs/spec-0000).

## Security policy

To report a security vulnerability to pandas, please go to <https://github.com/pandas-dev/pandas/security/policy> and see the instructions there.

---

comparison_with_r.md

---

<div id="compare_with_r">

{{ header }}

</div>

# Comparison with R / R libraries

Since pandas aims to provide a lot of the data manipulation and analysis functionality that people use [R](https://www.r-project.org/) for, this page was started to provide a more detailed look at the [R language](https://en.wikipedia.org/wiki/R_\(programming_language\)) and its many third party libraries as they relate to pandas. In comparisons with R and CRAN libraries, we care about the following things:

  - **Functionality / flexibility**: what can/cannot be done with each tool
  - **Performance**: how fast are operations. Hard numbers/benchmarks are preferable
  - **Ease-of-use**: Is one tool easier/harder to use (you may have to be the judge of this, given side-by-side code comparisons)

This page is also here to offer a bit of a translation guide for users of these R packages.

## Quick reference

We'll start off with a quick reference guide pairing some common R operations using [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html) with pandas equivalents.

### Querying, filtering, sampling

| R                                  | pandas                                       |
| ---------------------------------- | -------------------------------------------- |
| `dim(df)`                          | `df.shape`                                   |
| `head(df)`                         | `df.head()`                                  |
| `slice(df, 1:10)`                  | `df.iloc[:9]`                                |
| `filter(df, col1 == 1, col2 == 1)` | `df.query('col1 == 1 & col2 == 1')`          |
| `df[df$col1 == 1 & df$col2 == 1,]` | `df[(df.col1 == 1) & (df.col2 == 1)]`        |
| `select(df, col1, col2)`           | `df[['col1', 'col2']]`                       |
| `select(df, col1:col3)`            | `df.loc[:, 'col1':'col3']`                   |
| `select(df, -(col1:col3))`         | `df.drop(cols_to_drop, axis=1)` but see\[1\] |
| `distinct(select(df, col1))`       | `df[['col1']].drop_duplicates()`             |
| `distinct(select(df, col1, col2))` | `df[['col1', 'col2']].drop_duplicates()`     |
| `sample_n(df, 10)`                 | `df.sample(n=10)`                            |
| `sample_frac(df, 0.01)`            | `df.sample(frac=0.01)`                       |

### Sorting

| R                         | pandas                                    |
| ------------------------- | ----------------------------------------- |
| `arrange(df, col1, col2)` | `df.sort_values(['col1', 'col2'])`        |
| `arrange(df, desc(col1))` | `df.sort_values('col1', ascending=False)` |

### Transforming

| R                            | pandas                                              |
| ---------------------------- | --------------------------------------------------- |
| `select(df, col_one = col1)` | `df.rename(columns={'col1': 'col_one'})['col_one']` |
| `rename(df, col_one = col1)` | `df.rename(columns={'col1': 'col_one'})`            |
| `mutate(df, c=a-b)`          | `df.assign(c=df['a']-df['b'])`                      |

### Grouping and summarizing

| R                                            | pandas                                     |
| -------------------------------------------- | ------------------------------------------ |
| `summary(df)`                                | `df.describe()`                            |
| `gdf <- group_by(df, col1)`                  | `gdf = df.groupby('col1')`                 |
| `summarise(gdf, avg=mean(col1, na.rm=TRUE))` | `df.groupby('col1').agg({'col1': 'mean'})` |
| `summarise(gdf, total=sum(col1))`            | `df.groupby('col1').sum()`                 |

## Base R

### Slicing with R's `c`\_

R makes it easy to access `data.frame` columns by name

`` `r    df <- data.frame(a=rnorm(5), b=rnorm(5), c=rnorm(5), d=rnorm(5), e=rnorm(5))    df[, c("a", "c", "e")]  or by integer location  .. code-block:: r     df <- data.frame(matrix(rnorm(1000), ncol=100))    df[, c(1:10, 25:30, 40, 50:100)]  Selecting multiple columns by name in pandas is straightforward  .. ipython:: python     df = pd.DataFrame(np.random.randn(10, 3), columns=list("abc"))    df[["a", "c"]]    df.loc[:, ["a", "c"]]  Selecting multiple noncontiguous columns by integer location can be achieved ``<span class="title-ref"> with a combination of the </span><span class="title-ref">iloc</span><span class="title-ref"> indexer attribute and </span><span class="title-ref">numpy.r\_</span>\`.

<div class="ipython">

python

named = list("abcdefg") n = 30 columns = named + np.arange(len(named), n).tolist() df = pd.DataFrame(np.random.randn(n, n), columns=columns)

df.iloc\[:, [np.r]()\[:10, 24:30\]\]

</div>

### `aggregate`\_

In R you may want to split data into subsets and compute the mean for each. Using a data.frame called `df` and splitting it into groups `by1` and `by2`:

`` `r    df <- data.frame(      v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),      v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99),      by1 = c("red", "blue", 1, 2, NA, "big", 1, 2, "red", 1, NA, 12),      by2 = c("wet", "dry", 99, 95, NA, "damp", 95, 99, "red", 99, NA, NA))    aggregate(x=df[, c("v1", "v2")], by=list(mydf2$by1, mydf2$by2), FUN = mean)  The `~pandas.DataFrame.groupby` method is similar to base R ``aggregate`  `\` function.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "v1": \[1, 3, 5, 7, 8, 3, 5, np.nan, 4, 5, 7, 9\], "v2": \[11, 33, 55, 77, 88, 33, 55, np.nan, 44, 55, 77, 99\], "by1": \["red", "blue", 1, 2, np.nan, "big", 1, 2, "red", 1, np.nan, 12\], "by2": \[ "wet", "dry", 99, 95, np.nan, "damp", 95, 99, "red", 99, np.nan, np.nan, \],
    
    }

)

g = df.groupby(\["by1", "by2"\]) g\[\["v1", "v2"\]\].mean()

</div>

For more details and examples see \[the groupby documentation \<groupby.split\>\](\#the-groupby-documentation \<groupby.split\>).

### `match` / `%in%`\_

A common way to select data in R is using `%in%` which is defined using the function `match`. The operator `%in%` is used to return a logical vector indicating if there is a match or not:

`` `r    s <- 0:4    s %in% c(2,4)  The `~pandas.DataFrame.isin` method is similar to R ``%in%`operator:  .. ipython:: python     s = pd.Series(np.arange(5), dtype=np.float32)    s.isin([2, 4])  The`match`function returns a vector of the positions of matches`\` of its first argument in its second:

`` `r    s <- 0:4    match(s, c(2,4))  For more details and examples see [the reshaping documentation ](#the-reshaping-documentation ) `` \<indexing.basics.indexing\_isin\>\`.

### `tapply`\_

`tapply` is similar to `aggregate`, but data can be in a ragged array, since the subclass sizes are possibly irregular. Using a data.frame called `baseball`, and retrieving information based on the array `team`:

`` `r    baseball <-      data.frame(team = gl(5, 5,                 labels = paste("Team", LETTERS[1:5])),                 player = sample(letters, 25),                 batting.average = runif(25, .200, .400))     tapply(baseball$batting.average, baseball.example$team,           max)  In pandas we may use `~pandas.pivot_table` method to handle this:  .. ipython:: python     import random    import string     baseball = pd.DataFrame(        {            "team": ["team %d" % (x + 1) for x in range(5)] * 5,            "player": random.sample(list(string.ascii_lowercase), 25),            "batting avg": np.random.uniform(0.200, 0.400, 25),        }    )     baseball.pivot_table(values="batting avg", columns="team", aggfunc="max")  For more details and examples see [the reshaping documentation ](#the-reshaping-documentation ) `` \<reshaping.pivot\>\`.

### `subset`\_

The <span class="title-ref">\~pandas.DataFrame.query</span> method is similar to the base R `subset` function. In R you might want to get the rows of a `data.frame` where one column's values are less than another column's values:

`` `r    df <- data.frame(a=rnorm(10), b=rnorm(10))    subset(df, a <= b)    df[df$a <= df$b,]  # note the comma  In pandas, there are a few ways to perform subsetting. You can use ``<span class="title-ref"> </span>\~pandas.DataFrame.query\` or pass an expression as if it were an index/slice as well as standard boolean indexing:

<div class="ipython">

python

df = pd.DataFrame({"a": np.random.randn(10), "b": np.random.randn(10)}) df.query("a \<= b") df\[df\["a"\] \<= df\["b"\]\] df.loc\[df\["a"\] \<= df\["b"\]\]

</div>

For more details and examples see \[the query documentation \<indexing.query\>\](\#the-query-documentation \<indexing.query\>).

### `with`\_

An expression using a data.frame called `df` in R with the columns `a` and `b` would be evaluated using `with` like so:

`` `r    df <- data.frame(a=rnorm(10), b=rnorm(10))    with(df, a + b)    df$a + df$b  # same as the previous expression  In pandas the equivalent expression, using the ``<span class="title-ref"> </span>\~pandas.DataFrame.eval\` method, would be:

<div class="ipython">

python

df = pd.DataFrame({"a": np.random.randn(10), "b": np.random.randn(10)}) df.eval("a + b") df\["a"\] + df\["b"\] \# same as the previous expression

</div>

In certain cases <span class="title-ref">\~pandas.DataFrame.eval</span> will be much faster than evaluation in pure Python. For more details and examples see \[the eval documentation \<enhancingperf.eval\>\](\#the-eval documentation-\<enhancingperf.eval\>).

## plyr

`plyr` is an R library for the split-apply-combine strategy for data analysis. The functions revolve around three data structures in R, `a` for `arrays`, `l` for `lists`, and `d` for `data.frame`. The table below shows how these data structures could be mapped in Python.

| R          | Python                        |
| ---------- | ----------------------------- |
| array      | list                          |
| lists      | dictionary or list of objects |
| data.frame | dataframe                     |

### ddply

An expression using a data.frame called `df` in R where you want to summarize `x` by `month`:

`` `r    require(plyr)    df <- data.frame(      x = runif(120, 1, 168),      y = runif(120, 7, 334),      z = runif(120, 1.7, 20.7),      month = rep(c(5,6,7,8),30),      week = sample(1:4, 120, TRUE)    )     ddply(df, .(month, week), summarize,          mean = round(mean(x), 2),          sd = round(sd(x), 2))  In pandas the equivalent expression, using the ``<span class="title-ref"> </span>\~pandas.DataFrame.groupby\` method, would be:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "x": np.random.uniform(1.0, 168.0, 120), "y": np.random.uniform(7.0, 334.0, 120), "z": np.random.uniform(1.7, 20.7, 120), "month": \[5, 6, 7, 8\] \* 30, "week": np.random.randint(1, 4, 120),
    
    }

)

grouped = df.groupby(\["month", "week"\]) grouped\["x"\].agg(\["mean", "std"\])

</div>

For more details and examples see \[the groupby documentation \<groupby.aggregate\>\](\#the-groupby-documentation \<groupby.aggregate\>).

## reshape / reshape2

### meltarray

An expression using a 3 dimensional array called `a` in R where you want to melt it into a data.frame:

`` `r    a <- array(c(1:23, NA), c(2,3,4))    data.frame(melt(a))  In Python, since ``a`is a list, you can simply use list comprehension.  .. ipython:: python     a = np.array(list(range(1, 24)) + [np.NAN]).reshape(2, 3, 4)    pd.DataFrame([tuple(list(x) + [val]) for x, val in np.ndenumerate(a)])  meltlist`\` \~\~\~\~\~\~\~\~

An expression using a list called `a` in R where you want to melt it into a data.frame:

`` `r    a <- as.list(c(1:4, NA))    data.frame(melt(a))  In Python, this list would be a list of tuples, so ``<span class="title-ref"> </span>\~pandas.DataFrame\` method would convert it to a dataframe as required.

<div class="ipython">

python

a = list(enumerate(list(range(1, 5)) + \[np.NAN\])) pd.DataFrame(a)

</div>

For more details and examples see \[the Intro to Data Structures documentation \<dsintro\>\](\#the-intro-to-data-structures documentation-\<dsintro\>).

### meltdf

An expression using a data.frame called `cheese` in R where you want to reshape the data.frame:

`` `r    cheese <- data.frame(      first = c('John', 'Mary'),      last = c('Doe', 'Bo'),      height = c(5.5, 6.0),      weight = c(130, 150)    )    melt(cheese, id=c("first", "last"))  In Python, the `~pandas.melt` method is the R equivalent:  .. ipython:: python     cheese = pd.DataFrame(        {            "first": ["John", "Mary"],            "last": ["Doe", "Bo"],            "height": [5.5, 6.0],            "weight": [130, 150],        }    )     pd.melt(cheese, id_vars=["first", "last"])    cheese.set_index(["first", "last"]).stack()  # alternative way  For more details and examples see [the reshaping documentation ](#the-reshaping-documentation ) `` \<reshaping.melt\>\`.

### cast

In R `acast` is an expression using a data.frame called `df` in R to cast into a higher dimensional array:

`` `r    df <- data.frame(      x = runif(12, 1, 168),      y = runif(12, 7, 334),      z = runif(12, 1.7, 20.7),      month = rep(c(5,6,7),4),      week = rep(c(1,2), 6)    )     mdf <- melt(df, id=c("month", "week"))    acast(mdf, week ~ month ~ variable, mean)  In Python the best way is to make use of `~pandas.pivot_table`:  .. ipython:: python     df = pd.DataFrame(        {            "x": np.random.uniform(1.0, 168.0, 12),            "y": np.random.uniform(7.0, 334.0, 12),            "z": np.random.uniform(1.7, 20.7, 12),            "month": [5, 6, 7] * 4,            "week": [1, 2] * 6,        }    )     mdf = pd.melt(df, id_vars=["month", "week"])    pd.pivot_table(        mdf,        values="value",        index=["variable", "week"],        columns=["month"],        aggfunc="mean",    )  Similarly for ``dcast`which uses a data.frame called`df`in R to`<span class="title-ref"> aggregate information based on </span><span class="title-ref">Animal</span><span class="title-ref"> and </span><span class="title-ref">FeedType</span>\`:

`` `r    df <- data.frame(      Animal = c('Animal1', 'Animal2', 'Animal3', 'Animal2', 'Animal1',                 'Animal2', 'Animal3'),      FeedType = c('A', 'B', 'A', 'A', 'B', 'B', 'A'),      Amount = c(10, 7, 4, 2, 5, 6, 2)    )     dcast(df, Animal ~ FeedType, sum, fill=NaN)    # Alternative method using base R    with(df, tapply(Amount, list(Animal, FeedType), sum))  Python can approach this in two different ways. Firstly, similar to above ``<span class="title-ref"> using </span>\~pandas.pivot\_table\`:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {
        
          - "Animal": \[  
            "Animal1", "Animal2", "Animal3", "Animal2", "Animal1", "Animal2", "Animal3",
        
        \], "FeedType": \["A", "B", "A", "A", "B", "B", "A"\], "Amount": \[10, 7, 4, 2, 5, 6, 2\],
    
    }

)

df.pivot\_table(values="Amount", index="Animal", columns="FeedType", aggfunc="sum")

</div>

The second approach is to use the <span class="title-ref">\~pandas.DataFrame.groupby</span> method:

<div class="ipython">

python

df.groupby(\["Animal", "FeedType"\])\["Amount"\].sum()

</div>

For more details and examples see \[the reshaping documentation \<reshaping.pivot\>\](\#the-reshaping-documentation \<reshaping.pivot\>) or \[the groupby documentation\<groupby.split\>\](\#the-groupby-documentation\<groupby.split\>).

### `factor`\_

pandas has a data type for categorical data.

`` `r    cut(c(1,2,3,4,5,6), 3)    factor(c(1,2,3,2,2,3))  In pandas this is accomplished with ``pd.cut`and`astype("category")`:  .. ipython:: python     pd.cut(pd.Series([1, 2, 3, 4, 5, 6]), 3)    pd.Series([1, 2, 3, 2, 2, 3]).astype("category")  For more details and examples see [categorical introduction <categorical>](#categorical-introduction-<categorical>) and the`\` \[API documentation \<api.arrays.categorical\>\](\#api-documentation-\<api.arrays.categorical\>). There is also a documentation regarding the \[differences to R's factor \<categorical.rfactor\>\](\#differences-to-r's-factor-\<categorical.rfactor\>).

1.  R's shorthand for a subrange of columns (`select(df, col1:col3)`) can be approached cleanly in pandas, if you have the list of columns, for example `df[cols[1:3]]` or `df.drop(cols[1:3])`, but doing this by column name is a bit messy.

---

comparison_with_sas.md

---

<div id="compare_with_sas">

{{ header }}

</div>

# Comparison with SAS

For potential users coming from [SAS](https://en.wikipedia.org/wiki/SAS_\(software\)) this page is meant to demonstrate how different SAS operations would be performed in pandas.

If you're new to pandas, you might want to first read through \[10 Minutes to pandas\<10min\>\](\#10-minutes-to-pandas\<10min\>) to familiarize yourself with the library.

As is customary, we import pandas and NumPy as follows:

<div class="ipython">

python

import pandas as pd import numpy as np

</div>

## Data structures

### General terminology translation

| pandas      | SAS         |
| ----------- | ----------- |
| `DataFrame` | data set    |
| column      | variable    |
| row         | observation |
| groupby     | BY-group    |
| `NaN`       | `.`         |

### `DataFrame`

A `DataFrame` in pandas is analogous to a SAS data set - a two-dimensional data source with labeled columns that can be of different types. As will be shown in this document, almost any operation that can be applied to a data set using SAS's `DATA` step, can also be accomplished in pandas.

### `Series`

A `Series` is the data structure that represents one column of a `DataFrame`. SAS doesn't have a separate data structure for a single column, but in general, working with a `Series` is analogous to referencing a column in the `DATA` step.

### `Index`

Every `DataFrame` and `Series` has an `Index` - which are labels on the *rows* of the data. SAS does not have an exactly analogous concept. A data set's rows are essentially unlabeled, other than an implicit integer index that can be accessed during the `DATA` step (`_N_`).

In pandas, if no index is specified, an integer index is also used by default (first row = 0, second row = 1, and so on). While using a labeled `Index` or `MultiIndex` can enable sophisticated analyses and is ultimately an important part of pandas to understand, for this comparison we will essentially ignore the `Index` and just treat the `DataFrame` as a collection of columns. Please see the \[indexing documentation\<indexing\>\](\#indexing-documentation\<indexing\>) for much more on how to use an `Index` effectively.

### Copies vs. in place operations

Most pandas operations return copies of the `Series`/`DataFrame`. To make the changes "stick", you'll need to either assign to a new variable:

>   - \`\`\`python  
>     sorted\_df = df.sort\_values("col1")

or overwrite the original one:

> 
> 
> ``` python
> df = df.sort_values("col1")
> ```

\> **Note** \> You will see an `inplace=True` or `copy=False` keyword argument available for some methods:

> 
> 
> ``` python
> df.replace(5, inplace=True)
> ```
> 
> There is an active discussion about deprecating and removing `inplace` and `copy` for most methods (e.g. `dropna`) except for a very small subset of methods (including `replace`). Both keywords won't be necessary anymore in the context of Copy-on-Write. The proposal can be found [here](https://github.com/pandas-dev/pandas/pull/51466).

Data input / output `` ` -------------------  Constructing a DataFrame from values ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  A SAS data set can be built from specified values by placing the data after a ``datalines`statement and specifying the column names.`\`sas data df; input x y; datalines; 1 2 3 4 5 6 ; run;

A pandas `DataFrame` can be constructed in many different ways, `` ` but for a small number of values, it is often convenient to specify it as a Python dictionary, where the keys are the column names and the values are the data.  .. ipython:: python     df = pd.DataFrame({"x": [1, 3, 5], "y": [2, 4, 6]})    df   Reading external data ~~~~~~~~~~~~~~~~~~~~~  Like SAS, pandas provides utilities for reading in data from many formats.  The ``tips``dataset, found within the pandas tests (`csv <https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/tips.csv>`_) will be used in many of the following examples.  SAS provides``PROC IMPORT`to read csv data into a data set.`\`sas proc import datafile='tips.csv' dbms=csv out=tips replace; getnames=yes; run;

The pandas method is <span class="title-ref">read\_csv</span>, which works similarly.

<div class="ipython">

python

  - url = (  
    "<https://raw.githubusercontent.com/pandas-dev/>" "pandas/main/pandas/tests/io/data/csv/tips.csv"

) tips = pd.read\_csv(url) tips

</div>

Like `PROC IMPORT`, `read_csv` can take a number of parameters to specify `` ` how the data should be parsed.  For example, if the data was instead tab delimited, and did not have column names, the pandas command would be: ``\`python tips = pd.read\_csv("tips.csv", sep="t", header=None)

> \# alternatively, read\_table is an alias to read\_csv with tab delimiter tips = pd.read\_table("tips.csv", header=None)

In addition to text/csv, pandas supports a variety of other data formats `` ` such as Excel, HDF5, and SQL databases.  These are all read via a ``[pd.read]()\*`function.  See the [IO documentation<io>](#io-documentation<io>) for more details.  Limiting output ~~~~~~~~~~~~~~~  By default, pandas will truncate output of large`DataFrame``\s to show the first and last rows. This can be overridden by [changing the pandas options <options>](#changing-the-pandas-options-<options>), or using `DataFrame.head` or `DataFrame.tail`.  .. ipython:: python     tips.head(5)   The equivalent in SAS would be:``\`sas proc print data=df(obs=5); run;

Exporting data `` ` ~~~~~~~~~~~~~~  The inverse of ``PROC IMPORT`in SAS is`PROC EXPORT`  `\`sas proc export data=tips outfile='tips2.csv' dbms=csv; run;

Similarly in pandas, the opposite of `read_csv` is <span class="title-ref">\~DataFrame.to\_csv</span>, `` ` and other data formats follow a similar api. ``\`python tips.to\_csv("tips2.csv")

Data operations `` ` ---------------  Operations on columns ~~~~~~~~~~~~~~~~~~~~~  In the ``DATA`step, arbitrary math expressions can be used on new or existing columns.`\`sas data tips; set tips; total\_bill = total\_bill - 2; new\_bill = total\_bill / 2; run;

pandas provides vectorized operations by specifying the individual `Series` in the `` ` ``DataFrame``. New columns can be assigned in the same way. The `DataFrame.drop` method drops a column from the``DataFrame`.  .. ipython:: python     tips["total_bill"] = tips["total_bill"] - 2    tips["new_bill"] = tips["total_bill"] / 2    tips     tips = tips.drop("new_bill", axis=1)    Filtering ~~~~~~~~~  Filtering in SAS is done with an`if`or`where`statement, on one or more columns.`\`sas data tips; set tips; if total\_bill \> 10; run;

>   - data tips;  
>     set tips; where total\_bill \> 10; /\* equivalent in this case - where happens before the DATA step begins and can also be used in PROC statements \*/
> 
> run;

DataFrames can be filtered in multiple ways; the most intuitive of which is using `` ` [boolean indexing <indexing.boolean>](#boolean-indexing-<indexing.boolean>).  .. ipython:: python     tips[tips["total_bill"] > 10]  The above statement is simply passing a ``Series`of`True`/`False`objects to the DataFrame, returning all rows with`True`.  .. ipython:: python      is_dinner = tips["time"] == "Dinner"     is_dinner     is_dinner.value_counts()     tips[is_dinner]   If/then logic ~~~~~~~~~~~~~  In SAS, if/then logic can be used to create new columns.`\`sas data tips; set tips; format bucket $4.;

> if total\_bill \< 10 then bucket = 'low'; else bucket = 'high'; run;

The same operation in pandas can be accomplished using `` ` the ``where`method from`numpy`.  .. ipython:: python     tips["bucket"] = np.where(tips["total_bill"] < 10, "low", "high")    tips  .. ipython:: python    :suppress:     tips = tips.drop("bucket", axis=1)   Date functionality ~~~~~~~~~~~~~~~~~~  SAS provides a variety of functions to do operations on date/datetime columns.`\`sas data tips; set tips; format date1 date2 date1\_plusmonth mmddyy10.; date1 = mdy(1, 15, 2013); date2 = mdy(2, 15, 2015); date1\_year = year(date1); date2\_month = month(date2); \* shift date to beginning of next interval; date1\_next = intnx('MONTH', date1, 1); \* count intervals between dates; months\_between = intck('MONTH', date1, date2); run;

The equivalent pandas operations are shown below. In addition to these `` ` functions pandas supports other Time Series features not available in Base SAS (such as resampling and custom offsets) - see the [timeseries documentation<timeseries>](#timeseries-documentation<timeseries>) for more details.  .. ipython:: python     tips["date1"] = pd.Timestamp("2013-01-15")    tips["date2"] = pd.Timestamp("2015-02-15")    tips["date1_year"] = tips["date1"].dt.year    tips["date2_month"] = tips["date2"].dt.month    tips["date1_next"] = tips["date1"] + pd.offsets.MonthBegin()    tips["months_between"] = tips["date2"].dt.to_period("M") - tips[        "date1"    ].dt.to_period("M")     tips[        ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"]    ]  .. ipython:: python    :suppress:     tips = tips.drop(        ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"],        axis=1,    )   Selection of columns ~~~~~~~~~~~~~~~~~~~~  SAS provides keywords in the ``DATA`step to select, drop, and rename columns.`\`sas data tips; set tips; keep sex total\_bill tip; run;

>   - data tips;  
>     set tips; drop sex;
> 
> run;
> 
>   - data tips;  
>     set tips; rename total\_bill=total\_bill\_2;
> 
> run;

The same operations are expressed in pandas below.

Keep certain columns `` ` ''''''''''''''''''''  .. ipython:: python     tips[["sex", "total_bill", "tip"]]  Drop a column '''''''''''''  .. ipython:: python     tips.drop("sex", axis=1)  Rename a column '''''''''''''''  .. ipython:: python     tips.rename(columns={"total_bill": "total_bill_2"})    Sorting by values ~~~~~~~~~~~~~~~~~  Sorting in SAS is accomplished via ``PROC SORT`  `\`sas proc sort data=tips; by sex total\_bill; run;

pandas has a <span class="title-ref">DataFrame.sort\_values</span> method, which takes a list of columns to sort by.

<div class="ipython">

python

tips = tips.sort\_values(\["sex", "total\_bill"\]) tips

</div>

String processing `` ` -----------------  Finding length of string ~~~~~~~~~~~~~~~~~~~~~~~~  SAS determines the length of a character string with the `LENGTHN <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002284668.htm>`__ and `LENGTHC <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002283942.htm>`__ functions. ``LENGTHN`excludes trailing blanks and`LENGTHC`includes trailing blanks.`\`sas data \_[null](); set tips; put(LENGTHN(time)); put(LENGTHC(time)); run;

You can find the length of a character string with <span class="title-ref">Series.str.len</span>. `` ` In Python 3, all strings are Unicode strings. ``len`includes trailing blanks. Use`len`and`rstrip``to exclude trailing blanks.  .. ipython:: python     tips["time"].str.len()    tips["time"].str.rstrip().str.len()    Finding position of substring ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  SAS determines the position of a character in a string with the `FINDW <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002978282.htm>`__ function.``FINDW`takes the string defined by the first argument and searches for the first position of the substring you supply as the second argument.`\`sas data \_[null](); set tips; put(FINDW(sex,'ale')); run;

You can find the position of a character in a column of strings with the <span class="title-ref">Series.str.find</span> `` ` method. ``find`searches for the first position of the substring. If the substring is found, the method returns its position. If not found, it returns`-1``. Keep in mind that Python indexes are zero-based.  .. ipython:: python     tips["sex"].str.find("ale")    Extracting substring by position ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  SAS extracts a substring from a string based on its position with the `SUBSTR <https://support.sas.com/documentation/cdl/en/imlug/66845/HTML/default/viewer.htm#imlug_langref_sect455.htm>`__ function.``\`sas data \_[null](); set tips; put(substr(sex,1,1)); run;

With pandas you can use `[]` notation to extract a substring `` ` from a string by position locations. Keep in mind that Python indexes are zero-based.  .. ipython:: python     tips["sex"].str[0:1]    Extracting nth word ~~~~~~~~~~~~~~~~~~~  The SAS `SCAN <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000214639.htm>`__ function returns the nth word from a string. The first argument is the string you want to parse and the second argument specifies which word you want to extract. ``\`sas data firstlast; input String $60.; First\_Name = scan(string, 1); Last\_Name = scan(string, -1); datalines2; John Smith; Jane Cook; ;;; run;

The simplest way to extract words in pandas is to split the strings by spaces, then reference the `` ` word by index. Note there are more powerful approaches should you need them.  .. ipython:: python     firstlast = pd.DataFrame({"String": ["John Smith", "Jane Cook"]})    firstlast["First_Name"] = firstlast["String"].str.split(" ", expand=True)[0]    firstlast["Last_Name"] = firstlast["String"].str.rsplit(" ", expand=True)[1]    firstlast    Changing case ~~~~~~~~~~~~~  The SAS `UPCASE <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000245965.htm>`__ `LOWCASE <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000245912.htm>`__ and `PROPCASE <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/a002598106.htm>`__ functions change the case of the argument. ``\`sas data firstlast; input String $60.; string\_up = UPCASE(string); string\_low = LOWCASE(string); string\_prop = PROPCASE(string); datalines2; John Smith; Jane Cook; ;;; run;

The equivalent pandas methods are <span class="title-ref">Series.str.upper</span>, <span class="title-ref">Series.str.lower</span>, and `` ` `Series.str.title`.  .. ipython:: python     firstlast = pd.DataFrame({"string": ["John Smith", "Jane Cook"]})    firstlast["upper"] = firstlast["string"].str.upper()    firstlast["lower"] = firstlast["string"].str.lower()    firstlast["title"] = firstlast["string"].str.title()    firstlast    Merging -------  The following tables will be used in the merge examples:  .. ipython:: python     df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})    df1    df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})    df2   In SAS, data must be explicitly sorted before merging.  Different types of joins are accomplished using the ``in=`dummy variables to track whether a match was found in one or both input frames.`\`sas proc sort data=df1; by key; run;

>   - proc sort data=df2;  
>     by key;
> 
> run;
> 
>   - data left\_join inner\_join right\_join outer\_join;  
>     merge df1(in=a) df2(in=b);
>     
>     if a and b then output inner\_join; if a then output left\_join; if b then output right\_join; if a or b then output outer\_join;
> 
> run;

pandas DataFrames have a <span class="title-ref">\~DataFrame.merge</span> method, which provides similar functionality. The `` ` data does not have to be sorted ahead of time, and different join types are accomplished via the ``how`keyword.  .. ipython:: python     inner_join = df1.merge(df2, on=["key"], how="inner")    inner_join     left_join = df1.merge(df2, on=["key"], how="left")    left_join     right_join = df1.merge(df2, on=["key"], how="right")    right_join     outer_join = df1.merge(df2, on=["key"], how="outer")    outer_join    Missing data ------------  Both pandas and SAS have a representation for missing data.  pandas represents missing data with the special float value`NaN`(not a number).  Many of the semantics are the same; for example missing data propagates through numeric operations, and is ignored by default for aggregations.  .. ipython:: python     outer_join    outer_join["value_x"] + outer_join["value_y"]    outer_join["value_x"].sum()   One difference is that missing data cannot be compared to its sentinel value. For example, in SAS you could do this to filter missing values.`\`sas data outer\_join\_nulls; set outer\_join; if value\_x = .; run;

>   - data outer\_join\_no\_nulls;  
>     set outer\_join; if value\_x ^= .;
> 
> run;

In pandas, <span class="title-ref">Series.isna</span> and <span class="title-ref">Series.notna</span> can be used to filter the rows.

<div class="ipython">

python

outer\_join\[outer\_join\["value\_x"\].isna()\] outer\_join\[outer\_join\["value\_x"\].notna()\]

</div>

pandas provides \[a variety of methods to work with missing data \<missing\_data\>\](\#a-variety-of-methods-to-work-with-missing-data-\<missing\_data\>). Here are some examples:

Drop rows with missing values `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. ipython:: python     outer_join.dropna()  Forward fill from previous rows ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. ipython:: python     outer_join.ffill()  Replace missing values with a specified value ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Using the mean:  .. ipython:: python     outer_join["value_x"].fillna(outer_join["value_x"].mean())    GroupBy -------  Aggregation ~~~~~~~~~~~  SAS's ``PROC SUMMARY`can be used to group by one or more key variables and compute aggregations on numeric columns.`\`sas proc summary data=tips nway; class sex smoker; var total\_bill tip; output out=tips\_summed sum=; run;

pandas provides a flexible `groupby` mechanism that allows similar aggregations. See the `` ` [groupby documentation<groupby>](#groupby-documentation<groupby>) for more details and examples.  .. ipython:: python     tips_summed = tips.groupby(["sex", "smoker"])[["total_bill", "tip"]].sum()    tips_summed    Transformation ~~~~~~~~~~~~~~  In SAS, if the group aggregations need to be used with the original frame, it must be merged back together.  For example, to subtract the mean for each observation by smoker group. ``\`sas proc summary data=tips missing nway; class smoker; var total\_bill; output out=smoker\_means mean(total\_bill)=group\_bill; run;

>   - proc sort data=tips;  
>     by smoker;
> 
> run;
> 
>   - data tips;  
>     merge tips(in=a) smoker\_means(in=b); by smoker; adj\_total\_bill = total\_bill - group\_bill; if a and b;
> 
> run;

pandas provides a \[groupby.transform\](\#groupby.transform) mechanism that allows these type of operations to be `` ` succinctly expressed in one operation.  .. ipython:: python     gb = tips.groupby("smoker")["total_bill"]    tips["adj_total_bill"] = tips["total_bill"] - gb.transform("mean")    tips    By group processing ~~~~~~~~~~~~~~~~~~~  In addition to aggregation, pandas ``groupby`can be used to replicate most other by group processing from SAS. For example, this`DATA`step reads the data by sex/smoker group and filters to the first entry for each.`\`sas proc sort data=tips; by sex smoker; run;

>   - data tips\_first;  
>     set tips; by sex smoker; if FIRST.sex or FIRST.smoker then output;
> 
> run;

In pandas this would be written as:

<div class="ipython">

python

tips.groupby(\["sex", "smoker"\]).first()

</div>

Other considerations `` ` --------------------  Disk vs memory ~~~~~~~~~~~~~~  pandas operates exclusively in memory, where a SAS data set exists on disk. This means that the size of data able to be loaded in pandas is limited by your machine's memory, but also that the operations on that data may be faster.  If out of core processing is needed, one possibility is the `dask.dataframe <https://docs.dask.org/en/latest/dataframe.html>`_ library (currently in development) which provides a subset of pandas functionality for an on-disk ``DataFrame``Data interop ~~~~~~~~~~~~  pandas provides a `read_sas` method that can read SAS data saved in the XPORT or SAS7BDAT binary format.``\`sas libname xportout xport 'transport-file.xpt'; data xportout.tips; set tips(rename=(total\_bill=tbill)); \* xport variable names limited to 6 characters; run;

``` python
df = pd.read_sas("transport-file.xpt")
df = pd.read_sas("binary-file.sas7bdat")
```

You can also specify the file format directly. By default, pandas will try `` ` to infer the file format based on its extension. ``\`python df = pd.read\_sas("transport-file.xpt", format="xport") df = pd.read\_sas("binary-file.sas7bdat", format="sas7bdat")

XPORT is a relatively limited format and the parsing of it is not as `` ` optimized as some of the other pandas readers. An alternative way to interop data between SAS and pandas is to serialize to csv. ``\`ipython \# version 0.17, 10M rows

In \[8\]: %time df = pd.read\_sas('big.xpt') Wall time: 14.6 s

In \[9\]: %time df = pd.read\_csv('big.csv') Wall time: 4.86 s \`\`\`

---

comparison_with_spreadsheets.md

---

<div id="compare_with_spreadsheets">

{{ header }}

</div>

# Comparison with spreadsheets

Since many potential pandas users have some familiarity with spreadsheet programs like [Excel](https://support.microsoft.com/en-us/excel), this page is meant to provide some examples of how various spreadsheet operations would be performed using pandas. This page will use terminology and link to documentation for Excel, but much will be the same/similar in [Google Sheets](https://support.google.com/a/users/answer/9282959), [LibreOffice Calc](https://help.libreoffice.org/latest/en-US/text/scalc/main0000.html?DbPAR=CALC), [Apple Numbers](https://www.apple.com/numbers/compatibility/), and other Excel-compatible spreadsheet software.

If you're new to pandas, you might want to first read through \[10 Minutes to pandas\<10min\>\](\#10-minutes-to-pandas\<10min\>) to familiarize yourself with the library.

As is customary, we import pandas and NumPy as follows:

<div class="ipython">

python

import pandas as pd import numpy as np

</div>

## Data structures

### General terminology translation

| pandas      | Excel        |
| ----------- | ------------ |
| `DataFrame` | worksheet    |
| `Series`    | column       |
| `Index`     | row headings |
| row         | row          |
| `NaN`       | empty cell   |

### `DataFrame`

A `DataFrame` in pandas is analogous to an Excel worksheet. While an Excel workbook can contain multiple worksheets, pandas `DataFrame`s exist independently.

### `Series`

A `Series` is the data structure that represents one column of a `DataFrame`. Working with a `Series` is analogous to referencing a column of a spreadsheet.

### `Index`

Every `DataFrame` and `Series` has an `Index`, which are labels on the *rows* of the data. In pandas, if no index is specified, a <span class="title-ref">\~pandas.RangeIndex</span> is used by default (first row = 0, second row = 1, and so on), analogous to row headings/numbers in spreadsheets.

In pandas, indexes can be set to one (or multiple) unique values, which is like having a column that is used as the row identifier in a worksheet. Unlike most spreadsheets, these `Index` values can actually be used to reference the rows. (Note that [this can be done in Excel with structured references](https://support.microsoft.com/en-us/office/using-structured-references-with-excel-tables-f5ed2452-2337-4f71-bed3-c8ae6d2b276e).) For example, in spreadsheets, you would reference the first row as `A1:Z1`, while in pandas you could use `populations.loc['Chicago']`.

Index values are also persistent, so if you re-order the rows in a `DataFrame`, the label for a particular row don't change.

See the \[indexing documentation\<indexing\>\](\#indexing-documentation\<indexing\>) for much more on how to use an `Index` effectively.

### Copies vs. in place operations

Most pandas operations return copies of the `Series`/`DataFrame`. To make the changes "stick", you'll need to either assign to a new variable:

>   - \`\`\`python  
>     sorted\_df = df.sort\_values("col1")

or overwrite the original one:

> 
> 
> ``` python
> df = df.sort_values("col1")
> ```

\> **Note** \> You will see an `inplace=True` or `copy=False` keyword argument available for some methods:

> 
> 
> ``` python
> df.replace(5, inplace=True)
> ```
> 
> There is an active discussion about deprecating and removing `inplace` and `copy` for most methods (e.g. `dropna`) except for a very small subset of methods (including `replace`). Both keywords won't be necessary anymore in the context of Copy-on-Write. The proposal can be found [here](https://github.com/pandas-dev/pandas/pull/51466).

Data input / output `` ` -------------------  Constructing a DataFrame from values ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  In a spreadsheet, `values can be typed directly into cells <https://support.microsoft.com/en-us/office/enter-data-manually-in-worksheet-cells-c798181d-d75a-41b1-92ad-6c0800f80038>`_.  A pandas ``DataFrame``can be constructed in many different ways, but for a small number of values, it is often convenient to specify it as a Python dictionary, where the keys are the column names and the values are the data.  .. ipython:: python     df = pd.DataFrame({"x": [1, 3, 5], "y": [2, 4, 6]})    df   Reading external data ~~~~~~~~~~~~~~~~~~~~~  Both `Excel <https://support.microsoft.com/en-us/office/import-data-from-data-sources-power-query-be4330b3-5356-486c-a168-b68e9e616f5a>`__ and [pandas <10min_tut_02_read_write>](#pandas-<10min_tut_02_read_write>) can import data from various sources in various formats.  CSV '''  Let's load and display the `tips <https://github.com/pandas-dev/pandas/blob/main/pandas/tests/io/data/csv/tips.csv>`_ dataset from the pandas tests, which is a CSV file. In Excel, you would download and then `open the CSV <https://support.microsoft.com/en-us/office/import-or-export-text-txt-or-csv-files-5250ac4c-663c-47ce-937b-339e391393ba>`_. In pandas, you pass the URL or local path of the CSV file to `~pandas.read_csv`:  .. ipython:: python     url = (        "https://raw.githubusercontent.com/pandas-dev"        "/pandas/main/pandas/tests/io/data/csv/tips.csv"    )    tips = pd.read_csv(url)    tips  Like `Excel's Text Import Wizard <https://support.microsoft.com/en-us/office/text-import-wizard-c5b02af6-fda1-4440-899f-f78bafe41857>`_,``read\_csv`can take a number of parameters to specify how the data should be parsed. For example, if the data was instead tab delimited, and did not have column names, the pandas command would be:`\`python tips = pd.read\_csv("tips.csv", sep="t", header=None)

> \# alternatively, read\_table is an alias to read\_csv with tab delimiter tips = pd.read\_table("tips.csv", header=None)

Excel files `` ` '''''''''''  Excel opens `various Excel file formats <https://support.microsoft.com/en-us/office/file-formats-that-are-supported-in-excel-0943ff2c-6014-4e8d-aaea-b83d51d46247>`_ by double-clicking them, or using `the Open menu <https://support.microsoft.com/en-us/office/open-files-from-the-file-menu-97f087d8-3136-4485-8e86-c5b12a8c4176>`_. In pandas, you use [special methods for reading and writing from/to Excel files <io.excel>](#special-methods-for-reading-and-writing-from/to-excel-files-<io.excel>).  Let's first [create a new Excel file <io.excel_writer>](#create-a-new-excel-file-<io.excel_writer>) based on the ``tips`dataframe in the above example:`\`python tips.to\_excel("./tips.xlsx")

Should you wish to subsequently access the data in the `tips.xlsx` file, you can read it into your module using

``` python
tips_df = pd.read_excel("./tips.xlsx", index_col=0)
```

You have just read in an Excel file using pandas\!

Limiting output `` ` ~~~~~~~~~~~~~~~  Spreadsheet programs will only show one screenful of data at a time and then allow you to scroll, so there isn't really a need to limit output. In pandas, you'll need to put a little more thought into controlling how your ``DataFrame`\s are displayed.  By default, pandas will truncate output of large`DataFrame``\s to show the first and last rows. This can be overridden by [changing the pandas options <options>](#changing-the-pandas-options-<options>), or using `DataFrame.head` or `DataFrame.tail`.  .. ipython:: python     tips.head(5)    Exporting data ~~~~~~~~~~~~~~  By default, desktop spreadsheet software will save to its respective file format (``.xlsx`,`.ods``, etc). You can, however, `save to other file formats <https://support.microsoft.com/en-us/office/save-a-workbook-in-another-file-format-6a16c862-4a36-48f9-a300-c2ca0065286e>`_.  [pandas can create Excel files <io.excel_writer>](#pandas-can-create-excel-files-<io.excel_writer>), [CSV <io.store_in_csv>](#csv-<io.store_in_csv>), or [a number of other formats <io>](#a-number-of-other-formats-<io>).  Data operations ---------------  Operations on columns ~~~~~~~~~~~~~~~~~~~~~  In spreadsheets, `formulas <https://support.microsoft.com/en-us/office/overview-of-formulas-in-excel-ecfdc708-9162-49e8-b993-c311f47ca173>`_ are often created in individual cells and then `dragged <https://support.microsoft.com/en-us/office/copy-a-formula-by-dragging-the-fill-handle-in-excel-for-mac-dd928259-622b-473f-9a33-83aa1a63e218>`_ into other cells to compute them for other columns. In pandas, you're able to do operations on whole columns directly.  pandas provides vectorized operations by specifying the individual``Series`in the`DataFrame``. New columns can be assigned in the same way. The `DataFrame.drop` method drops a column from the``DataFrame``.  .. ipython:: python     tips["total_bill"] = tips["total_bill"] - 2    tips["new_bill"] = tips["total_bill"] / 2    tips     tips = tips.drop("new_bill", axis=1)   Note that we aren't having to tell it to do that subtraction cell-by-cell â€” pandas handles that for us. See [how to create new columns derived from existing columns <10min_tut_05_columns>](#how-to-create-new-columns-derived-from-existing-columns-<10min_tut_05_columns>).   Filtering ~~~~~~~~~  `In Excel, filtering is done through a graphical menu. <https://support.microsoft.com/en-us/office/filter-data-in-a-range-or-table-01832226-31b5-4568-8806-38c37dcc180e>`_  .. image:: ../../_static/spreadsheets/filter.png    :alt: Screenshot showing filtering of the total_bill column to values greater than 10    :align: center  DataFrames can be filtered in multiple ways; the most intuitive of which is using [boolean indexing <indexing.boolean>](#boolean-indexing-<indexing.boolean>).  .. ipython:: python     tips[tips["total_bill"] > 10]  The above statement is simply passing a``Series`of`True`/`False`objects to the DataFrame, returning all rows with`True`.  .. ipython:: python      is_dinner = tips["time"] == "Dinner"     is_dinner     is_dinner.value_counts()     tips[is_dinner]   If/then logic ~~~~~~~~~~~~~  Let's say we want to make a`bucket`column with values of`low`and`high`, based on whether the`total\_bill``is less or more than $10.  In spreadsheets, logical comparison can be done with `conditional formulas <https://support.microsoft.com/en-us/office/create-conditional-formulas-ca916c57-abd8-4b44-997c-c309b7307831>`_. We'd use a formula of``=IF(A2 \< 10, "low", "high")`, dragged to all cells in a new`bucket`column.  .. image:: ../../_static/spreadsheets/conditional.png    :alt: Screenshot showing the formula from above in a bucket column of the tips spreadsheet    :align: center  The same operation in pandas can be accomplished using the`where`method from`numpy``.  .. ipython:: python     tips["bucket"] = np.where(tips["total_bill"] < 10, "low", "high")    tips  .. ipython:: python    :suppress:     tips = tips.drop("bucket", axis=1)   Date functionality ~~~~~~~~~~~~~~~~~~  *This section will refer to "dates", but timestamps are handled similarly.*  We can think of date functionality in two parts: parsing, and output. In spreadsheets, date values are generally parsed automatically, though there is a `DATEVALUE <https://support.microsoft.com/en-us/office/datevalue-function-df8b07d4-7761-4a93-bc33-b7471bbff252>`_ function if you need it. In pandas, you need to explicitly convert plain text to datetime objects, either [while reading from a CSV <io.read_csv_table.datetime>](#while-reading-from-a-csv-<io.read_csv_table.datetime>) or [once in a DataFrame <10min_tut_09_timeseries.properties>](#once-in-a-dataframe <10min_tut_09_timeseries.properties>).  Once parsed, spreadsheets display the dates in a default format, though `the format can be changed <https://support.microsoft.com/en-us/office/format-a-date-the-way-you-want-8e10019e-d5d8-47a1-ba95-db95123d273e>`_. In pandas, you'll generally want to keep dates as``datetime``objects while you're doing calculations with them. Outputting *parts* of dates (such as the year) is done through `date functions <https://support.microsoft.com/en-us/office/date-and-time-functions-reference-fd1b5961-c1ae-4677-be58-074152f97b81>`_ in spreadsheets, and [datetime properties <10min_tut_09_timeseries.properties>](#datetime-properties-<10min_tut_09_timeseries.properties>) in pandas.  Given``date1`and`date2`in columns`A`and`B`of a spreadsheet, you might have these formulas:  .. list-table::     :header-rows: 1     :widths: auto      * - column       - formula     * -`date1\_year`-`=YEAR(A2)`* -`date2\_month`-`=MONTH(B2)`* -`date1\_next`-`=DATE(YEAR(A2),MONTH(A2)+1,1)`* -`months\_between`-`=DATEDIF(A2,B2,"M")``The equivalent pandas operations are shown below.  .. ipython:: python     tips["date1"] = pd.Timestamp("2013-01-15")    tips["date2"] = pd.Timestamp("2015-02-15")    tips["date1_year"] = tips["date1"].dt.year    tips["date2_month"] = tips["date2"].dt.month    tips["date1_next"] = tips["date1"] + pd.offsets.MonthBegin()    tips["months_between"] = tips["date2"].dt.to_period("M") - tips[        "date1"    ].dt.to_period("M")     tips[        ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"]    ]  .. ipython:: python    :suppress:     tips = tips.drop(        ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"],        axis=1,    )   See [timeseries](#timeseries) for more details.   Selection of columns ~~~~~~~~~~~~~~~~~~~~  In spreadsheets, you can select columns you want by:  - `Hiding columns <https://support.microsoft.com/en-us/office/hide-or-show-rows-or-columns-659c2cad-802e-44ee-a614-dde8443579f8>`_ - `Deleting columns <https://support.microsoft.com/en-us/office/insert-or-delete-rows-and-columns-6f40e6e4-85af-45e0-b39d-65dd504a3246>`_ - `Referencing a range <https://support.microsoft.com/en-us/office/create-or-change-a-cell-reference-c7b8b95d-c594-4488-947e-c835903cebaa>`_ from one worksheet into another  Since spreadsheet columns are typically `named in a header row <https://support.microsoft.com/en-us/office/turn-excel-table-headers-on-or-off-c91d1742-312c-4480-820f-cf4b534c8b3b>`_, renaming a column is simply a matter of changing the text in that first cell.  The same operations are expressed in pandas below.  Keep certain columns ''''''''''''''''''''  .. ipython:: python     tips[["sex", "total_bill", "tip"]]  Drop a column '''''''''''''  .. ipython:: python     tips.drop("sex", axis=1)  Rename a column '''''''''''''''  .. ipython:: python     tips.rename(columns={"total_bill": "total_bill_2"})    Sorting by values ~~~~~~~~~~~~~~~~~  Sorting in spreadsheets is accomplished via `the sort dialog <https://support.microsoft.com/en-us/office/sort-data-in-a-range-or-table-62d0b95d-2a90-4610-a6ae-2e545c4a4654>`_.  .. image:: ../../_static/spreadsheets/sort.png    :alt: Screenshot of dialog from Excel showing sorting by the sex then total_bill columns    :align: center  pandas has a `DataFrame.sort_values` method, which takes a list of columns to sort by.  .. ipython:: python     tips = tips.sort_values(["sex", "total_bill"])    tips   String processing -----------------  Finding length of string ~~~~~~~~~~~~~~~~~~~~~~~~  In spreadsheets, the number of characters in text can be found with the `LEN <https://support.microsoft.com/en-us/office/len-lenb-functions-29236f94-cedc-429d-affd-b5e33d2c67cb>`_ function. This can be used with the `TRIM <https://support.microsoft.com/en-us/office/trim-function-410388fa-c5df-49c6-b16c-9e5630b479f9>`_ function to remove extra whitespace.  ::     =LEN(TRIM(A2))  You can find the length of a character string with `Series.str.len`. In Python 3, all strings are Unicode strings.``len`includes trailing blanks. Use`len`and`rstrip``to exclude trailing blanks.  .. ipython:: python     tips["time"].str.len()    tips["time"].str.rstrip().str.len()   Note this will still include multiple spaces within the string, so isn't 100% equivalent.   Finding position of substring ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The `FIND <https://support.microsoft.com/en-us/office/find-findb-functions-c7912941-af2a-4bdf-a553-d0d89b0a0628>`_ spreadsheet function returns the position of a substring, with the first character being``1``.  .. image:: ../../_static/spreadsheets/sort.png    :alt: Screenshot of FIND formula being used in Excel    :align: center  You can find the position of a character in a column of strings with the `Series.str.find` method.``find`searches for the first position of the substring. If the substring is found, the method returns its position. If not found, it returns`-1``. Keep in mind that Python indexes are zero-based.  .. ipython:: python     tips["sex"].str.find("ale")    Extracting substring by position ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Spreadsheets have a `MID <https://support.microsoft.com/en-us/office/mid-midb-functions-d5f9e25c-d7d6-472e-b568-4ecb12433028>`_ formula for extracting a substring from a given position. To get the first character::     =MID(A2,1,1)  With pandas you can use``\[\]``notation to extract a substring from a string by position locations. Keep in mind that Python indexes are zero-based.  .. ipython:: python     tips["sex"].str[0:1]    Extracting nth word ~~~~~~~~~~~~~~~~~~~  In Excel, you might use the `Text to Columns Wizard <https://support.microsoft.com/en-us/office/split-text-into-different-columns-with-the-convert-text-to-columns-wizard-30b14928-5550-41f5-97ca-7a3e9c363ed7>`_ for splitting text and retrieving a specific column. (Note `it's possible to do so through a formula as well <https://exceljet.net/formula/extract-nth-word-from-text-string>`_.)  The simplest way to extract words in pandas is to split the strings by spaces, then reference the word by index. Note there are more powerful approaches should you need them.  .. ipython:: python     firstlast = pd.DataFrame({"String": ["John Smith", "Jane Cook"]})    firstlast["First_Name"] = firstlast["String"].str.split(" ", expand=True)[0]    firstlast["Last_Name"] = firstlast["String"].str.rsplit(" ", expand=True)[1]    firstlast    Changing case ~~~~~~~~~~~~~  Spreadsheets provide `UPPER, LOWER, and PROPER functions <https://support.microsoft.com/en-us/office/change-the-case-of-text-01481046-0fa7-4f3b-a693-496795a7a44d>`_ for converting text to upper, lower, and title case, respectively.  The equivalent pandas methods are `Series.str.upper`, `Series.str.lower`, and `Series.str.title`.  .. ipython:: python     firstlast = pd.DataFrame({"string": ["John Smith", "Jane Cook"]})    firstlast["upper"] = firstlast["string"].str.upper()    firstlast["lower"] = firstlast["string"].str.lower()    firstlast["title"] = firstlast["string"].str.title()    firstlast    Merging -------  The following tables will be used in the merge examples:  .. ipython:: python     df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})    df1    df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})    df2   In Excel, there are `merging of tables can be done through a VLOOKUP <https://support.microsoft.com/en-us/office/how-can-i-merge-two-or-more-tables-c80a9fce-c1ab-4425-bb96-497dd906d656>`_.  .. image:: ../../_static/spreadsheets/vlookup.png    :alt: Screenshot showing a VLOOKUP formula between two tables in Excel, with some values being filled in and others with "#N/A"    :align: center  pandas DataFrames have a `~DataFrame.merge` method, which provides similar functionality. The data does not have to be sorted ahead of time, and different join types are accomplished via the``how`keyword.  .. ipython:: python     inner_join = df1.merge(df2, on=["key"], how="inner")    inner_join     left_join = df1.merge(df2, on=["key"], how="left")    left_join     right_join = df1.merge(df2, on=["key"], how="right")    right_join     outer_join = df1.merge(df2, on=["key"], how="outer")    outer_join`merge`has a number of advantages over`VLOOKUP``:  * The lookup value doesn't need to be the first column of the lookup table * If multiple rows are matched, there will be one row for each match, instead of just the first * It will include all columns from the lookup table, instead of just a single specified column * It supports [more complex join operations <merging.join>](#more-complex-join-operations-<merging.join>)   Other considerations --------------------  Fill Handle ~~~~~~~~~~~  Create a series of numbers following a set pattern in a certain set of cells. In a spreadsheet, this would be done by shift+drag after entering the first number or by entering the first two or three values and then dragging.  This can be achieved by creating a series and assigning it to the desired cells.  .. ipython:: python      df = pd.DataFrame({"AAA": [1] * 8, "BBB": list(range(0, 8))})     df      series = list(range(1, 5))     series      df.loc[2:5, "AAA"] = series      df  Drop Duplicates ~~~~~~~~~~~~~~~  Excel has built-in functionality for `removing duplicate values <https://support.microsoft.com/en-us/office/find-and-remove-duplicates-00e35bea-b46a-4d5d-b28e-66a552dc138d>`_. This is supported in pandas via `~DataFrame.drop_duplicates`.  .. ipython:: python      df = pd.DataFrame(         {             "class": ["A", "A", "A", "B", "C", "D"],             "student_count": [42, 35, 42, 50, 47, 45],             "all_pass": ["Yes", "Yes", "Yes", "No", "No", "Yes"],         }     )      df.drop_duplicates()      df.drop_duplicates(["class", "student_count"])  Pivot Tables ~~~~~~~~~~~~  `PivotTables <https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576>`_ from spreadsheets can be replicated in pandas through [reshaping](#reshaping). Using the``tips``dataset again, let's find the average gratuity by size of the party and sex of the server.  In Excel, we use the following configuration for the PivotTable:  .. image:: ../../_static/spreadsheets/pivot.png    :alt: Screenshot showing a PivotTable in Excel, using sex as the column, size as the rows, then average tip as the values    :align: center  The equivalent in pandas:  .. ipython:: python      pd.pivot_table(         tips, values="tip", index=["size"], columns=["sex"], aggfunc=np.average     )   Adding a row ~~~~~~~~~~~~  Assuming we are using a `~pandas.RangeIndex` (numbered``0`,`1``, etc.), we can use `concat` to add a row to the bottom of a``DataFrame``.  .. ipython:: python      df     new_row = pd.DataFrame([["E", 51, True]],                            columns=["class", "student_count", "all_pass"])     pd.concat([df, new_row])   Find and Replace ~~~~~~~~~~~~~~~~  `Excel's Find dialog <https://support.microsoft.com/en-us/office/find-or-replace-text-and-numbers-on-a-worksheet-0e304ca5-ecef-4808-b90f-fdb42f892e90>`_ takes you to cells that match, one by one. In pandas, this operation is generally done for an entire column or``DataFrame``at once through [conditional expressions <10min_tut_03_subset.rows_and_columns>](#conditional-expressions-<10min_tut_03_subset.rows_and_columns>).  .. ipython:: python      tips     tips == "Sun"     tips["day"].str.contains("S")  pandas' `~DataFrame.replace` is comparable to Excel's``Replace All\`\`.

<div class="ipython">

python

tips.replace("Thu", "Thursday")

</div>

---

comparison_with_sql.md

---

<div id="compare_with_sql">

{{ header }}

</div>

# Comparison with SQL

Since many potential pandas users have some familiarity with [SQL](https://en.wikipedia.org/wiki/SQL), this page is meant to provide some examples of how various SQL operations would be performed using pandas.

If you're new to pandas, you might want to first read through \[10 Minutes to pandas\<10min\>\](\#10-minutes-to-pandas\<10min\>) to familiarize yourself with the library.

As is customary, we import pandas and NumPy as follows:

<div class="ipython">

python

import pandas as pd import numpy as np

</div>

Most of the examples will utilize the `tips` dataset found within pandas tests. We'll read the data into a DataFrame called `tips` and assume we have a database table of the same name and structure.

<div class="ipython">

python

  - url = (  
    "<https://raw.githubusercontent.com/pandas-dev>" "/pandas/main/pandas/tests/io/data/csv/tips.csv"

) tips = pd.read\_csv(url) tips

</div>

## Copies vs. in place operations

Most pandas operations return copies of the `Series`/`DataFrame`. To make the changes "stick", you'll need to either assign to a new variable:

>   - \`\`\`python  
>     sorted\_df = df.sort\_values("col1")

or overwrite the original one:

> 
> 
> ``` python
> df = df.sort_values("col1")
> ```

\> **Note** \> You will see an `inplace=True` or `copy=False` keyword argument available for some methods:

> 
> 
> ``` python
> df.replace(5, inplace=True)
> ```
> 
> There is an active discussion about deprecating and removing `inplace` and `copy` for most methods (e.g. `dropna`) except for a very small subset of methods (including `replace`). Both keywords won't be necessary anymore in the context of Copy-on-Write. The proposal can be found [here](https://github.com/pandas-dev/pandas/pull/51466).

SELECT `` ` ------ In SQL, selection is done using a comma-separated list of columns you'd like to select (or a ``\*`to select all columns):`\`sql SELECT total\_bill, tip, smoker, time FROM tips;

With pandas, column selection is done by passing a list of column names to your DataFrame:

<div class="ipython">

python

tips\[\["total\_bill", "tip", "smoker", "time"\]\]

</div>

Calling the DataFrame without the list of column names would display all columns (akin to SQL's `` ` ``\*`).  In SQL, you can add a calculated column:`\`sql SELECT \*, tip/total\_bill as tip\_rate FROM tips;

With pandas, you can use the <span class="title-ref">DataFrame.assign</span> method of a DataFrame to append a new column:

<div class="ipython">

python

tips.assign(tip\_rate=tips\["tip"\] / tips\["total\_bill"\])

</div>

WHERE `` ` ----- Filtering in SQL is done via a WHERE clause. ``\`sql SELECT \* FROM tips WHERE time = 'Dinner';

DataFrames can be filtered in multiple ways; the most intuitive of which is using `` ` [boolean indexing <indexing.boolean>](#boolean-indexing-<indexing.boolean>).  .. ipython:: python     tips[tips["total_bill"] > 10]  The above statement is simply passing a ``Series`of`True`/`False`objects to the DataFrame, returning all rows with`True`.  .. ipython:: python      is_dinner = tips["time"] == "Dinner"     is_dinner     is_dinner.value_counts()     tips[is_dinner]   Just like SQL's`OR`and`AND`, multiple conditions can be passed to a DataFrame using`|`(`OR`) and`&`(`AND`).  Tips of more than $5 at Dinner meals:`\`sql SELECT \* FROM tips WHERE time = 'Dinner' AND tip \> 5.00;

<div class="ipython">

python

tips\[(tips\["time"\] == "Dinner") & (tips\["tip"\] \> 5.00)\]

</div>

Tips by parties of at least 5 diners OR bill total was more than $45:

``` sql
SELECT *
FROM tips
WHERE size >= 5 OR total_bill > 45;
```

<div class="ipython">

python

tips\[(tips\["size"\] \>= 5) | (tips\["total\_bill"\] \> 45)\]

</div>

NULL checking is done using the <span class="title-ref">\~pandas.Series.notna</span> and <span class="title-ref">\~pandas.Series.isna</span> `` ` methods.  .. ipython:: python      frame = pd.DataFrame(         {"col1": ["A", "B", np.nan, "C", "D"], "col2": ["F", np.nan, "G", "H", "I"]}     )     frame  Assume we have a table of the same structure as our DataFrame above. We can see only the records where ``col2`IS NULL with the following query:`\`sql SELECT \* FROM frame WHERE col2 IS NULL;

<div class="ipython">

python

frame\[frame\["col2"\].isna()\]

</div>

Getting items where `col1` IS NOT NULL can be done with <span class="title-ref">\~pandas.Series.notna</span>.

``` sql
SELECT *
FROM frame
WHERE col1 IS NOT NULL;
```

<div class="ipython">

python

frame\[frame\["col1"\].notna()\]

</div>

GROUP BY `` ` -------- In pandas, SQL's ``GROUP BY``operations are performed using the similarly named `~pandas.DataFrame.groupby` method. `~pandas.DataFrame.groupby` typically refers to a process where we'd like to split a dataset into groups, apply some function (typically aggregation) , and then combine the groups together.  A common SQL operation would be getting the count of records in each group throughout a dataset. For instance, a query getting us the number of tips left by sex:``\`sql SELECT sex, count(*) FROM tips GROUP BY sex; /* Female 87 Male 157 \*/

The pandas equivalent would be:

<div class="ipython">

python

tips.groupby("sex").size()

</div>

Notice that in the pandas code we used <span class="title-ref">.DataFrameGroupBy.size</span> and not `` ` `.DataFrameGroupBy.count`. This is because `.DataFrameGroupBy.count` applies the function to each column, returning the number of ``NOT NULL``records within each.  .. ipython:: python      tips.groupby("sex").count()  Alternatively, we could have applied the `.DataFrameGroupBy.count` method to an individual column:  .. ipython:: python      tips.groupby("sex")["total_bill"].count()  Multiple functions can also be applied at once. For instance, say we'd like to see how tip amount differs by day of the week - `.DataFrameGroupBy.agg` allows you to pass a dictionary to your grouped DataFrame, indicating which functions to apply to specific columns.``\`sql SELECT day, AVG(tip), COUNT(*) FROM tips GROUP BY day; /* Fri 2.734737 19 Sat 2.993103 87 Sun 3.255132 76 Thu 2.771452 62 \*/

<div class="ipython">

python

tips.groupby("day").agg({"tip": "mean", "day": "size"})

</div>

Grouping by more than one column is done by passing a list of columns to the `` ` `~pandas.DataFrame.groupby` method. ``\`sql SELECT smoker, day, COUNT(*), AVG(tip) FROM tips GROUP BY smoker, day; /* smoker day No Fri 4 2.812500 Sat 45 3.102889 Sun 57 3.167895 Thu 45 2.673778 Yes Fri 15 2.714000 Sat 42 2.875476 Sun 19 3.516842 Thu 17 3.030000 \*/

<div class="ipython">

python

tips.groupby(\["smoker", "day"\]).agg({"tip": \["size", "mean"\]})

</div>

<div id="compare_with_sql.join">

JOIN `` ` ---- ``JOIN``\s can be performed with `~pandas.DataFrame.join` or `~pandas.merge`. By default, `~pandas.DataFrame.join` will join the DataFrames on their indices. Each method has parameters allowing you to specify the type of join to perform (``LEFT`,`RIGHT`,`INNER`,`FULL`) or the columns to join on (column names or indices).  > **Warning** >      If both key columns contain rows where the key is a null value, those     rows will be matched against each other. This is different from usual SQL     join behaviour and can lead to unexpected results.  .. ipython:: python      df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})     df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})  Assume we have two database tables of the same name and structure as our DataFrames.  Now let's go over the various types of`JOIN`\s.  INNER JOIN ~~~~~~~~~~`\`sql SELECT \* FROM df1 INNER JOIN df2 ON df1.key = df2.key;

</div>

<div class="ipython">

python

\# merge performs an INNER JOIN by default pd.merge(df1, df2, on="key")

</div>

<span class="title-ref">\~pandas.merge</span> also offers parameters for cases when you'd like to join one DataFrame's `` ` column with another DataFrame's index.  .. ipython:: python      indexed_df2 = df2.set_index("key")     pd.merge(df1, indexed_df2, left_on="key", right_index=True)  LEFT OUTER JOIN ~~~~~~~~~~~~~~~  Show all records from ``df1`.`\`sql SELECT \* FROM df1 LEFT OUTER JOIN df2 ON df1.key = df2.key;

<div class="ipython">

python

pd.merge(df1, df2, on="key", how="left")

</div>

RIGHT JOIN `` ` ~~~~~~~~~~  Show all records from ``df2`.`\`sql SELECT \* FROM df1 RIGHT OUTER JOIN df2 ON df1.key = df2.key;

<div class="ipython">

python

pd.merge(df1, df2, on="key", how="right")

</div>

FULL JOIN `` ` ~~~~~~~~~ pandas also allows for ``FULL JOIN`\s, which display both sides of the dataset, whether or not the joined columns find a match. As of writing,`FULL JOIN`\s are not supported in all RDBMS (MySQL).  Show all records from both tables.`\`sql SELECT \* FROM df1 FULL OUTER JOIN df2 ON df1.key = df2.key;

<div class="ipython">

python

pd.merge(df1, df2, on="key", how="outer")

</div>

UNION `` ` ----- ``UNION ALL``can be performed using `~pandas.concat`.  .. ipython:: python      df1 = pd.DataFrame(         {"city": ["Chicago", "San Francisco", "New York City"], "rank": range(1, 4)}     )     df2 = pd.DataFrame(         {"city": ["Chicago", "Boston", "Los Angeles"], "rank": [1, 4, 5]}     )``\`sql SELECT city, rank FROM df1 UNION ALL SELECT city, rank FROM df2; /\* city rank Chicago 1 San Francisco 2 New York City 3 Chicago 1 Boston 4 Los Angeles 5 \*/

<div class="ipython">

python

pd.concat(\[df1, df2\])

</div>

SQL's `UNION` is similar to `UNION ALL`, however `UNION` will remove duplicate rows.

``` sql
SELECT city, rank
FROM df1
UNION
SELECT city, rank
FROM df2;
-- notice that there is only one Chicago record this time
/*
         city  rank
      Chicago     1
San Francisco     2
New York City     3
       Boston     4
  Los Angeles     5
*/
```

In pandas, you can use <span class="title-ref">\~pandas.concat</span> in conjunction with `` ` `~pandas.DataFrame.drop_duplicates`.  .. ipython:: python      pd.concat([df1, df2]).drop_duplicates()   LIMIT ----- ``\`sql SELECT \* FROM tips LIMIT 10;

<div class="ipython">

python

tips.head(10)

</div>

pandas equivalents for some SQL analytic and aggregate functions `` ` ----------------------------------------------------------------  Top n rows with offset ~~~~~~~~~~~~~~~~~~~~~~ ``\`sql -- MySQL SELECT \* FROM tips ORDER BY tip DESC LIMIT 10 OFFSET 5;

<div class="ipython">

python

tips.nlargest(10 + 5, columns="tip").tail(10)

</div>

Top n rows per group `` ` ~~~~~~~~~~~~~~~~~~~~ ``\`sql -- Oracle's ROW\_NUMBER() analytic function SELECT \* FROM ( SELECT t.\*, ROW\_NUMBER() OVER(PARTITION BY day ORDER BY total\_bill DESC) AS rn FROM tips t ) WHERE rn \< 3 ORDER BY day, rn;

<div class="ipython">

python

  - (
    
      - tips.assign(  
        rn=tips.sort\_values(\["total\_bill"\], ascending=False) .groupby(\["day"\]) .cumcount() + 1
    
    ) .query("rn \< 3") .sort\_values(\["day", "rn"\])

)

</div>

the same using `rank(method='first')` function

<div class="ipython">

python

  - (
    
      - tips.assign(
        
          - rnk=tips.groupby(\["day"\])\["total\_bill"\].rank(  
            method="first", ascending=False
        
        )
    
    ) .query("rnk \< 3") .sort\_values(\["day", "rnk"\])

)

</div>

``` sql
-- Oracle's RANK() analytic function
SELECT * FROM (
  SELECT
    t.*,
    RANK() OVER(PARTITION BY sex ORDER BY tip) AS rnk
  FROM tips t
  WHERE tip < 2
)
WHERE rnk < 3
ORDER BY sex, rnk;
```

Let's find tips with (rank \< 3) per gender group for (tips \< 2). `` ` Notice that when using ``rank(method='min')`function`rnk\_min`remains the same for the same`tip`(as Oracle's`RANK()`function)  .. ipython:: python      (         tips[tips["tip"] < 2]         .assign(rnk_min=tips.groupby(["sex"])["tip"].rank(method="min"))         .query("rnk_min < 3")         .sort_values(["sex", "rnk_min"])     )   UPDATE ------`\`sql UPDATE tips SET tip = tip\*2 WHERE tip \< 2;

<div class="ipython">

python

tips.loc\[tips\["tip"\] \< 2, "tip"\] \*= 2

</div>

DELETE `` ` ------ ``\`sql DELETE FROM tips WHERE tip \> 9;

In pandas we select the rows that should remain instead of deleting the rows that should be removed:

<div class="ipython">

python

tips = tips.loc\[tips\["tip"\] \<= 9\]

</div>

\`\`\`

---

comparison_with_stata.md

---

<div id="compare_with_stata">

{{ header }}

</div>

# Comparison with Stata

For potential users coming from [Stata](https://en.wikipedia.org/wiki/Stata) this page is meant to demonstrate how different Stata operations would be performed in pandas.

If you're new to pandas, you might want to first read through \[10 Minutes to pandas\<10min\>\](\#10-minutes-to-pandas\<10min\>) to familiarize yourself with the library.

As is customary, we import pandas and NumPy as follows:

<div class="ipython">

python

import pandas as pd import numpy as np

</div>

## Data structures

### General terminology translation

| pandas      | Stata       |
| ----------- | ----------- |
| `DataFrame` | data set    |
| column      | variable    |
| row         | observation |
| groupby     | bysort      |
| `NaN`       | `.`         |

### `DataFrame`

A `DataFrame` in pandas is analogous to a Stata data set -- a two-dimensional data source with labeled columns that can be of different types. As will be shown in this document, almost any operation that can be applied to a data set in Stata can also be accomplished in pandas.

### `Series`

A `Series` is the data structure that represents one column of a `DataFrame`. Stata doesn't have a separate data structure for a single column, but in general, working with a `Series` is analogous to referencing a column of a data set in Stata.

### `Index`

Every `DataFrame` and `Series` has an `Index` -- labels on the *rows* of the data. Stata does not have an exactly analogous concept. In Stata, a data set's rows are essentially unlabeled, other than an implicit integer index that can be accessed with `_n`.

In pandas, if no index is specified, an integer index is also used by default (first row = 0, second row = 1, and so on). While using a labeled `Index` or `MultiIndex` can enable sophisticated analyses and is ultimately an important part of pandas to understand, for this comparison we will essentially ignore the `Index` and just treat the `DataFrame` as a collection of columns. Please see the \[indexing documentation\<indexing\>\](\#indexing-documentation\<indexing\>) for much more on how to use an `Index` effectively.

### Copies vs. in place operations

Most pandas operations return copies of the `Series`/`DataFrame`. To make the changes "stick", you'll need to either assign to a new variable:

>   - \`\`\`python  
>     sorted\_df = df.sort\_values("col1")

or overwrite the original one:

> 
> 
> ``` python
> df = df.sort_values("col1")
> ```

\> **Note** \> You will see an `inplace=True` or `copy=False` keyword argument available for some methods:

> 
> 
> ``` python
> df.replace(5, inplace=True)
> ```
> 
> There is an active discussion about deprecating and removing `inplace` and `copy` for most methods (e.g. `dropna`) except for a very small subset of methods (including `replace`). Both keywords won't be necessary anymore in the context of Copy-on-Write. The proposal can be found [here](https://github.com/pandas-dev/pandas/pull/51466).

Data input / output `` ` -------------------  Constructing a DataFrame from values ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  A Stata data set can be built from specified values by placing the data after an ``input`statement and specifying the column names.`\`stata input x y 1 2 3 4 5 6 end

A pandas `DataFrame` can be constructed in many different ways, `` ` but for a small number of values, it is often convenient to specify it as a Python dictionary, where the keys are the column names and the values are the data.  .. ipython:: python     df = pd.DataFrame({"x": [1, 3, 5], "y": [2, 4, 6]})    df   Reading external data ~~~~~~~~~~~~~~~~~~~~~  Like Stata, pandas provides utilities for reading in data from many formats.  The ``tips``data set, found within the pandas tests (`csv <https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/tips.csv>`_) will be used in many of the following examples.  Stata provides``import delimited`to read csv data into a data set in memory. If the`tips.csv`file is in the current working directory, we can import it as follows.`\`stata import delimited tips.csv

The pandas method is <span class="title-ref">read\_csv</span>, which works similarly. Additionally, it will automatically download `` ` the data set if presented with a url.  .. ipython:: python     url = (        "https://raw.githubusercontent.com/pandas-dev"        "/pandas/main/pandas/tests/io/data/csv/tips.csv"    )    tips = pd.read_csv(url)    tips  Like ``import delimited``, `read_csv` can take a number of parameters to specify how the data should be parsed.  For example, if the data were instead tab delimited, did not have column names, and existed in the current working directory, the pandas command would be:``\`python tips = pd.read\_csv("tips.csv", sep="t", header=None)

> \# alternatively, read\_table is an alias to read\_csv with tab delimiter tips = pd.read\_table("tips.csv", header=None)

pandas can also read Stata data sets in `.dta` format with the <span class="title-ref">read\_stata</span> function.

``` python
df = pd.read_stata("data.dta")
```

In addition to text/csv and Stata files, pandas supports a variety of other data formats `` ` such as Excel, SAS, HDF5, Parquet, and SQL databases.  These are all read via a ``[pd.read]()\*`function.  See the [IO documentation<io>](#io-documentation<io>) for more details.   Limiting output ~~~~~~~~~~~~~~~  By default, pandas will truncate output of large`DataFrame``\s to show the first and last rows. This can be overridden by [changing the pandas options <options>](#changing-the-pandas-options-<options>), or using `DataFrame.head` or `DataFrame.tail`.  .. ipython:: python     tips.head(5)   The equivalent in Stata would be:``\`stata list in 1/5

Exporting data `` ` ~~~~~~~~~~~~~~  The inverse of ``import delimited`in Stata is`export delimited`  `\`stata export delimited tips2.csv

Similarly in pandas, the opposite of `read_csv` is <span class="title-ref">DataFrame.to\_csv</span>.

``` python
tips.to_csv("tips2.csv")
```

pandas can also export to Stata file format with the <span class="title-ref">DataFrame.to\_stata</span> method.

``` python
tips.to_stata("tips2.dta")
```

Data operations `` ` ---------------  Operations on columns ~~~~~~~~~~~~~~~~~~~~~  In Stata, arbitrary math expressions can be used with the ``generate`and`replace`commands on new or existing columns. The`drop`command drops the column from the data set.`\`stata replace total\_bill = total\_bill - 2 generate new\_bill = total\_bill / 2 drop new\_bill

pandas provides vectorized operations by specifying the individual `Series` in the `` ` ``DataFrame``. New columns can be assigned in the same way. The `DataFrame.drop` method drops a column from the``DataFrame`.  .. ipython:: python     tips["total_bill"] = tips["total_bill"] - 2    tips["new_bill"] = tips["total_bill"] / 2    tips     tips = tips.drop("new_bill", axis=1)    Filtering ~~~~~~~~~  Filtering in Stata is done with an`if`clause on one or more columns.`\`stata list if total\_bill \> 10

DataFrames can be filtered in multiple ways; the most intuitive of which is using `` ` [boolean indexing <indexing.boolean>](#boolean-indexing-<indexing.boolean>).  .. ipython:: python     tips[tips["total_bill"] > 10]  The above statement is simply passing a ``Series`of`True`/`False`objects to the DataFrame, returning all rows with`True`.  .. ipython:: python      is_dinner = tips["time"] == "Dinner"     is_dinner     is_dinner.value_counts()     tips[is_dinner]   If/then logic ~~~~~~~~~~~~~  In Stata, an`if`clause can also be used to create new columns.`\`stata generate bucket = "low" if total\_bill \< 10 replace bucket = "high" if total\_bill \>= 10

The same operation in pandas can be accomplished using `` ` the ``where`method from`numpy`.  .. ipython:: python     tips["bucket"] = np.where(tips["total_bill"] < 10, "low", "high")    tips  .. ipython:: python    :suppress:     tips = tips.drop("bucket", axis=1)   Date functionality ~~~~~~~~~~~~~~~~~~  Stata provides a variety of functions to do operations on date/datetime columns.`\`stata generate date1 = mdy(1, 15, 2013) generate date2 = date("Feb152015", "MDY")

> generate date1\_year = year(date1) generate date2\_month = month(date2)
> 
> \* shift date to beginning of next month generate date1\_next = mdy(month(date1) + 1, 1, year(date1)) if month(date1) \!= 12 replace date1\_next = mdy(1, 1, year(date1) + 1) if month(date1) == 12 generate months\_between = mofd(date2) - mofd(date1)
> 
> list date1 date2 date1\_year date2\_month date1\_next months\_between

The equivalent pandas operations are shown below. In addition to these `` ` functions, pandas supports other Time Series features not available in Stata (such as time zone handling and custom offsets) -- see the [timeseries documentation<timeseries>](#timeseries-documentation<timeseries>) for more details.  .. ipython:: python     tips["date1"] = pd.Timestamp("2013-01-15")    tips["date2"] = pd.Timestamp("2015-02-15")    tips["date1_year"] = tips["date1"].dt.year    tips["date2_month"] = tips["date2"].dt.month    tips["date1_next"] = tips["date1"] + pd.offsets.MonthBegin()    tips["months_between"] = tips["date2"].dt.to_period("M") - tips[        "date1"    ].dt.to_period("M")     tips[        ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"]    ]  .. ipython:: python    :suppress:     tips = tips.drop(        ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"],        axis=1,    )   Selection of columns ~~~~~~~~~~~~~~~~~~~~  Stata provides keywords to select, drop, and rename columns. ``\`stata keep sex total\_bill tip

> drop sex
> 
> rename total\_bill total\_bill\_2

The same operations are expressed in pandas below.

Keep certain columns `` ` ''''''''''''''''''''  .. ipython:: python     tips[["sex", "total_bill", "tip"]]  Drop a column '''''''''''''  .. ipython:: python     tips.drop("sex", axis=1)  Rename a column '''''''''''''''  .. ipython:: python     tips.rename(columns={"total_bill": "total_bill_2"})    Sorting by values ~~~~~~~~~~~~~~~~~  Sorting in Stata is accomplished via ``sort`  `\`stata sort sex total\_bill

pandas has a <span class="title-ref">DataFrame.sort\_values</span> method, which takes a list of columns to sort by.

<div class="ipython">

python

tips = tips.sort\_values(\["sex", "total\_bill"\]) tips

</div>

String processing `` ` -----------------  Finding length of string ~~~~~~~~~~~~~~~~~~~~~~~~  Stata determines the length of a character string with the `strlen` and `ustrlen` functions for ASCII and Unicode strings, respectively. ``\`stata generate strlen\_time = strlen(time) generate ustrlen\_time = ustrlen(time)

You can find the length of a character string with <span class="title-ref">Series.str.len</span>. `` ` In Python 3, all strings are Unicode strings. ``len`includes trailing blanks. Use`len`and`rstrip``to exclude trailing blanks.  .. ipython:: python     tips["time"].str.len()    tips["time"].str.rstrip().str.len()    Finding position of substring ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Stata determines the position of a character in a string with the `strpos` function. This takes the string defined by the first argument and searches for the first position of the substring you supply as the second argument.``\`stata generate str\_position = strpos(sex, "ale")

You can find the position of a character in a column of strings with the <span class="title-ref">Series.str.find</span> `` ` method. ``find`searches for the first position of the substring. If the substring is found, the method returns its position. If not found, it returns`-1``. Keep in mind that Python indexes are zero-based.  .. ipython:: python     tips["sex"].str.find("ale")    Extracting substring by position ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Stata extracts a substring from a string based on its position with the `substr` function.``\`stata generate short\_sex = substr(sex, 1, 1)

With pandas you can use `[]` notation to extract a substring `` ` from a string by position locations. Keep in mind that Python indexes are zero-based.  .. ipython:: python     tips["sex"].str[0:1]    Extracting nth word ~~~~~~~~~~~~~~~~~~~  The Stata `word` function returns the nth word from a string. The first argument is the string you want to parse and the second argument specifies which word you want to extract. ``\`stata clear input str20 string "John Smith" "Jane Cook" end

> generate first\_name = word(name, 1) generate last\_name = word(name, -1)

The simplest way to extract words in pandas is to split the strings by spaces, then reference the `` ` word by index. Note there are more powerful approaches should you need them.  .. ipython:: python     firstlast = pd.DataFrame({"String": ["John Smith", "Jane Cook"]})    firstlast["First_Name"] = firstlast["String"].str.split(" ", expand=True)[0]    firstlast["Last_Name"] = firstlast["String"].str.rsplit(" ", expand=True)[1]    firstlast    Changing case ~~~~~~~~~~~~~  The Stata `strupper`, `strlower`, `strproper`, `ustrupper`, `ustrlower`, and `ustrtitle` functions change the case of ASCII and Unicode strings, respectively. ``\`stata clear input str20 string "John Smith" "Jane Cook" end

> generate upper = strupper(string) generate lower = strlower(string) generate title = strproper(string) list

The equivalent pandas methods are <span class="title-ref">Series.str.upper</span>, <span class="title-ref">Series.str.lower</span>, and `` ` `Series.str.title`.  .. ipython:: python     firstlast = pd.DataFrame({"string": ["John Smith", "Jane Cook"]})    firstlast["upper"] = firstlast["string"].str.upper()    firstlast["lower"] = firstlast["string"].str.lower()    firstlast["title"] = firstlast["string"].str.title()    firstlast    Merging -------  The following tables will be used in the merge examples:  .. ipython:: python     df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})    df1    df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})    df2   In Stata, to perform a merge, one data set must be in memory and the other must be referenced as a file name on disk. In contrast, Python must have both ``DataFrames`already in memory.  By default, Stata performs an outer join, where all observations from both data sets are left in memory after the merge. One can keep only observations from the initial data set, the merged data set, or the intersection of the two by using the values created in the`\_merge`variable.`\`stata \* First create df2 and save to disk clear input str1 key B D D E end generate value = rnormal() save df2.dta

> \* Now create df1 in memory clear input str1 key A B C D end generate value = rnormal()
> 
> preserve
> 
> \* Left join merge 1:n key using df2.dta keep if \_merge == 1
> 
> \* Right join restore, preserve merge 1:n key using df2.dta keep if \_merge == 2
> 
> \* Inner join restore, preserve merge 1:n key using df2.dta keep if \_merge == 3
> 
> \* Outer join restore merge 1:n key using df2.dta

pandas DataFrames have a <span class="title-ref">\~DataFrame.merge</span> method, which provides similar functionality. The `` ` data does not have to be sorted ahead of time, and different join types are accomplished via the ``how`keyword.  .. ipython:: python     inner_join = df1.merge(df2, on=["key"], how="inner")    inner_join     left_join = df1.merge(df2, on=["key"], how="left")    left_join     right_join = df1.merge(df2, on=["key"], how="right")    right_join     outer_join = df1.merge(df2, on=["key"], how="outer")    outer_join    Missing data ------------  Both pandas and Stata have a representation for missing data.  pandas represents missing data with the special float value`NaN`(not a number).  Many of the semantics are the same; for example missing data propagates through numeric operations, and is ignored by default for aggregations.  .. ipython:: python     outer_join    outer_join["value_x"] + outer_join["value_y"]    outer_join["value_x"].sum()   One difference is that missing data cannot be compared to its sentinel value. For example, in Stata you could do this to filter missing values.`\`stata \* Keep missing values list if value\_x == . \* Keep non-missing values list if value\_x \!= .

In pandas, <span class="title-ref">Series.isna</span> and <span class="title-ref">Series.notna</span> can be used to filter the rows.

<div class="ipython">

python

outer\_join\[outer\_join\["value\_x"\].isna()\] outer\_join\[outer\_join\["value\_x"\].notna()\]

</div>

pandas provides \[a variety of methods to work with missing data \<missing\_data\>\](\#a-variety-of-methods-to-work-with-missing-data-\<missing\_data\>). Here are some examples:

Drop rows with missing values `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. ipython:: python     outer_join.dropna()  Forward fill from previous rows ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. ipython:: python     outer_join.ffill()  Replace missing values with a specified value ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Using the mean:  .. ipython:: python     outer_join["value_x"].fillna(outer_join["value_x"].mean())    GroupBy -------  Aggregation ~~~~~~~~~~~  Stata's ``collapse`can be used to group by one or more key variables and compute aggregations on numeric columns.`\`stata collapse (sum) total\_bill tip, by(sex smoker)

pandas provides a flexible `groupby` mechanism that allows similar aggregations. See the `` ` [groupby documentation<groupby>](#groupby-documentation<groupby>) for more details and examples.  .. ipython:: python     tips_summed = tips.groupby(["sex", "smoker"])[["total_bill", "tip"]].sum()    tips_summed    Transformation ~~~~~~~~~~~~~~  In Stata, if the group aggregations need to be used with the original data set, one would usually use ``bysort``with `egen`. For example, to subtract the mean for each observation by smoker group.``\`stata bysort sex smoker: egen group\_bill = mean(total\_bill) generate adj\_total\_bill = total\_bill - group\_bill

pandas provides a \[groupby.transform\](\#groupby.transform) mechanism that allows these type of operations to be `` ` succinctly expressed in one operation.  .. ipython:: python     gb = tips.groupby("smoker")["total_bill"]    tips["adj_total_bill"] = tips["total_bill"] - gb.transform("mean")    tips    By group processing ~~~~~~~~~~~~~~~~~~~  In addition to aggregation, pandas ``groupby`can be used to replicate most other`bysort`processing from Stata. For example, the following example lists the first observation in the current sort order by sex/smoker group.`\`stata bysort sex smoker: list if \_n == 1

In pandas this would be written as:

<div class="ipython">

python

tips.groupby(\["sex", "smoker"\]).first()

</div>

Other considerations `` ` --------------------  Disk vs memory ~~~~~~~~~~~~~~  pandas and Stata both operate exclusively in memory. This means that the size of data able to be loaded in pandas is limited by your machine's memory. If out of core processing is needed, one possibility is the `dask.dataframe <https://docs.dask.org/en/latest/dataframe.html>`_ library, which provides a subset of pandas functionality for an on-disk ``DataFrame\`\`.

---

case.md

---

The equivalent pandas methods are <span class="title-ref">Series.str.upper</span>, <span class="title-ref">Series.str.lower</span>, and <span class="title-ref">Series.str.title</span>.

<div class="ipython">

python

firstlast = pd.DataFrame({"string": \["John Smith", "Jane Cook"\]}) firstlast\["upper"\] = firstlast\["string"\].str.upper() firstlast\["lower"\] = firstlast\["string"\].str.lower() firstlast\["title"\] = firstlast\["string"\].str.title() firstlast

</div>

---

column_operations.md

---

pandas provides vectorized operations by specifying the individual `Series` in the `DataFrame`. New columns can be assigned in the same way. The <span class="title-ref">DataFrame.drop</span> method drops a column from the `DataFrame`.

<div class="ipython">

python

tips\["total\_bill"\] = tips\["total\_bill"\] - 2 tips\["new\_bill"\] = tips\["total\_bill"\] / 2 tips

tips = tips.drop("new\_bill", axis=1)

</div>

---

column_selection.md

---

The same operations are expressed in pandas below.

# Keep certain columns

<div class="ipython">

python

tips\[\["sex", "total\_bill", "tip"\]\]

</div>

# Drop a column

<div class="ipython">

python

tips.drop("sex", axis=1)

</div>

# Rename a column

<div class="ipython">

python

tips.rename(columns={"total\_bill": "total\_bill\_2"})

</div>

---

construct_dataframe.md

---

A pandas `DataFrame` can be constructed in many different ways, but for a small number of values, it is often convenient to specify it as a Python dictionary, where the keys are the column names and the values are the data.

<div class="ipython">

python

df = pd.DataFrame({"x": \[1, 3, 5\], "y": \[2, 4, 6\]}) df

</div>

---

copies.md

---

Most pandas operations return copies of the `Series`/`DataFrame`. To make the changes "stick", you'll need to either assign to a new variable:

>   - \`\`\`python  
>     sorted\_df = df.sort\_values("col1")

or overwrite the original one:

> 
> 
> ``` python
> df = df.sort_values("col1")
> ```

\> **Note** \> You will see an `inplace=True` or `copy=False` keyword argument available for some methods:

``` python
df.replace(5, inplace=True)
```

There is an active discussion about deprecating and removing `inplace` and `copy` for most methods (e.g. `dropna`) except for a very small subset of methods (including `replace`). Both keywords won't be necessary anymore in the context of Copy-on-Write. The proposal can be found [here](https://github.com/pandas-dev/pandas/pull/51466). \`\`\`

---

extract_substring.md

---

With pandas you can use `[]` notation to extract a substring from a string by position locations. Keep in mind that Python indexes are zero-based.

<div class="ipython">

python

tips\["sex"\].str\[0:1\]

</div>

---

filtering.md

---

DataFrames can be filtered in multiple ways; the most intuitive of which is using \[boolean indexing \<indexing.boolean\>\](\#boolean-indexing-\<indexing.boolean\>).

<div class="ipython">

python

tips\[tips\["total\_bill"\] \> 10\]

</div>

The above statement is simply passing a `Series` of `True`/`False` objects to the DataFrame, returning all rows with `True`.

<div class="ipython">

python

is\_dinner = tips\["time"\] == "Dinner" is\_dinner is\_dinner.value\_counts() tips\[is\_dinner\]

</div>

---

find_substring.md

---

You can find the position of a character in a column of strings with the <span class="title-ref">Series.str.find</span> method. `find` searches for the first position of the substring. If the substring is found, the method returns its position. If not found, it returns `-1`. Keep in mind that Python indexes are zero-based.

<div class="ipython">

python

tips\["sex"\].str.find("ale")

</div>

---

groupby.md

---

pandas provides a flexible `groupby` mechanism that allows similar aggregations. See the \[groupby documentation\<groupby\>\](\#groupby-documentation\<groupby\>) for more details and examples.

<div class="ipython">

python

tips\_summed = tips.groupby(\["sex", "smoker"\])\[\["total\_bill", "tip"\]\].sum() tips\_summed

</div>

---

if_then.md

---

The same operation in pandas can be accomplished using the `where` method from `numpy`.

<div class="ipython">

python

tips\["bucket"\] = np.where(tips\["total\_bill"\] \< 10, "low", "high") tips

</div>

<div class="ipython" data-suppress="">

python

tips = tips.drop("bucket", axis=1)

</div>

---

introduction.md

---

If you're new to pandas, you might want to first read through \[10 Minutes to pandas\<10min\>\](\#10-minutes-to-pandas\<10min\>) to familiarize yourself with the library.

As is customary, we import pandas and NumPy as follows:

<div class="ipython">

python

import pandas as pd import numpy as np

</div>

---

length.md

---

You can find the length of a character string with <span class="title-ref">Series.str.len</span>. In Python 3, all strings are Unicode strings. `len` includes trailing blanks. Use `len` and `rstrip` to exclude trailing blanks.

<div class="ipython">

python

tips\["time"\].str.len() tips\["time"\].str.rstrip().str.len()

</div>

---

limit.md

---

By default, pandas will truncate output of large `DataFrame`s to show the first and last rows. This can be overridden by \[changing the pandas options \<options\>\](\#changing-the-pandas-options-\<options\>), or using <span class="title-ref">DataFrame.head</span> or <span class="title-ref">DataFrame.tail</span>.

<div class="ipython">

python

tips.head(5)

</div>

---

merge.md

---

pandas DataFrames have a <span class="title-ref">\~DataFrame.merge</span> method, which provides similar functionality. The data does not have to be sorted ahead of time, and different join types are accomplished via the `how` keyword.

<div class="ipython">

python

inner\_join = df1.merge(df2, on=\["key"\], how="inner") inner\_join

left\_join = df1.merge(df2, on=\["key"\], how="left") left\_join

right\_join = df1.merge(df2, on=\["key"\], how="right") right\_join

outer\_join = df1.merge(df2, on=\["key"\], how="outer") outer\_join

</div>

---

merge_setup.md

---

The following tables will be used in the merge examples:

<div class="ipython">

python

df1 = pd.DataFrame({"key": \["A", "B", "C", "D"\], "value": np.random.randn(4)}) df1 df2 = pd.DataFrame({"key": \["B", "D", "D", "E"\], "value": np.random.randn(4)}) df2

</div>

---

missing.md

---

In pandas, <span class="title-ref">Series.isna</span> and <span class="title-ref">Series.notna</span> can be used to filter the rows.

<div class="ipython">

python

outer\_join\[outer\_join\["value\_x"\].isna()\] outer\_join\[outer\_join\["value\_x"\].notna()\]

</div>

pandas provides \[a variety of methods to work with missing data \<missing\_data\>\](\#a-variety-of-methods-to-work-with-missing-data-\<missing\_data\>). Here are some examples:

# Drop rows with missing values

<div class="ipython">

python

outer\_join.dropna()

</div>

# Forward fill from previous rows

<div class="ipython">

python

outer\_join.ffill()

</div>

# Replace missing values with a specified value

Using the mean:

<div class="ipython">

python

outer\_join\["value\_x"\].fillna(outer\_join\["value\_x"\].mean())

</div>

---

missing_intro.md

---

pandas represents missing data with the special float value `NaN` (not a number). Many of the semantics are the same; for example missing data propagates through numeric operations, and is ignored by default for aggregations.

<div class="ipython">

python

outer\_join outer\_join\["value\_x"\] + outer\_join\["value\_y"\] outer\_join\["value\_x"\].sum()

</div>

---

nth_word.md

---

The simplest way to extract words in pandas is to split the strings by spaces, then reference the word by index. Note there are more powerful approaches should you need them.

<div class="ipython">

python

firstlast = pd.DataFrame({"String": \["John Smith", "Jane Cook"\]}) firstlast\["First\_Name"\] = firstlast\["String"\].str.split(" ", expand=True)\[0\] firstlast\["Last\_Name"\] = firstlast\["String"\].str.rsplit(" ", expand=True)\[1\] firstlast

</div>

---

sorting.md

---

pandas has a <span class="title-ref">DataFrame.sort\_values</span> method, which takes a list of columns to sort by.

<div class="ipython">

python

tips = tips.sort\_values(\["sex", "total\_bill"\]) tips

</div>

---

time_date.md

---

<div class="ipython">

python

tips\["date1"\] = pd.Timestamp("2013-01-15") tips\["date2"\] = pd.Timestamp("2015-02-15") tips\["date1\_year"\] = tips\["date1"\].dt.year tips\["date2\_month"\] = tips\["date2"\].dt.month tips\["date1\_next"\] = tips\["date1"\] + pd.offsets.MonthBegin() tips\["months\_between"\] = tips\["date2"\].dt.to\_period("M") - tips\[ "date1" \].dt.to\_period("M")

  - tips\[  
    \["date1", "date2", "date1\_year", "date2\_month", "date1\_next", "months\_between"\]

\]

</div>

<div class="ipython" data-suppress="">

python

  - tips = tips.drop(  
    \["date1", "date2", "date1\_year", "date2\_month", "date1\_next", "months\_between"\], axis=1,

)

</div>

---

transform.md

---

pandas provides a \[groupby.transform\](\#groupby.transform) mechanism that allows these type of operations to be succinctly expressed in one operation.

<div class="ipython">

python

gb = tips.groupby("smoker")\["total\_bill"\] tips\["adj\_total\_bill"\] = tips\["total\_bill"\] - gb.transform("mean") tips

</div>

---

index.md

---

{{ header }}

# Comparison with other tools

<div class="toctree" data-maxdepth="2">

comparison\_with\_r comparison\_with\_sql comparison\_with\_spreadsheets comparison\_with\_sas comparison\_with\_stata

</div>

---

index.md

---

{{ header }}

# Getting started

## Installation

<div class="grid" data-gutter="4">

1 2 2 2

<div class="grid-item-card" data-class-card="install-card" data-columns="12 12 6 6" data-padding="3">

Working with conda?

pandas can be installed via conda from [conda-forge](https://anaconda.org/conda-forge/pandas).

-----

  - \`\`\`bash  
    conda install -c conda-forge pandas

</div>

<div class="grid-item-card" data-class-card="install-card" data-columns="12 12 6 6" data-padding="3">

Prefer pip?

pandas can be installed via pip from [PyPI](https://pypi.org/project/pandas).

-----

``` bash
pip install pandas
```

</div>

<div class="grid-item-card" data-class-card="install-card" data-columns="12" data-padding="3">

In-depth instructions?

Installing a specific version? Installing from source? Check the advanced installation page.

\+++

<div class="button-ref" data-ref-type="ref" data-click-parent="" color="secondary" data-expand="">

install

Learn more

</div>

</div>

</div>

<div id="gentle_intro">

Intro to pandas `` ` ---------------  .. raw:: html      <div class="container">     <div id="accordion" class="shadow tutorial-accordion">          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseOne">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         What kind of data does pandas handle?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_01_tableoriented>](#straight-to-tutorial...<10min_tut_01_tableoriented>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseOne" class="collapse" data-parent="#accordion">                 <div class="card-body">  When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a `DataFrame`.  .. image:: ../_static/schemas/01_table_dataframe.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_01_tableoriented>](#to-introduction-tutorial-<10min_tut_01_tableoriented>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <dsintro>](#to-user-guide-<dsintro>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseTwo">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How do I read and write tabular data?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_02_read_write>](#straight-to-tutorial...<10min_tut_02_read_write>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseTwo" class="collapse" data-parent="#accordion">                 <div class="card-body">  pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,â€¦). The ability to import data from each of these data sources is provided by functions with the prefix, ``[read]()*\`\`. Similarly, the \`\`to\_*``methods are used to store data.  .. image:: ../_static/schemas/02_io_readwrite.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_02_read_write>](#to-introduction-tutorial-<10min_tut_02_read_write>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <io>](#to-user-guide-<io>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseThree">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How do I select a subset of a table?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_03_subset>](#straight-to-tutorial...<10min_tut_03_subset>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseThree" class="collapse" data-parent="#accordion">                 <div class="card-body">  Selecting or filtering specific rows and/or columns? Filtering the data on a particular condition? Methods for slicing, selecting, and extracting the data you need are available in pandas.  .. image:: ../_static/schemas/03_subset_columns_rows.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_03_subset>](#to-introduction-tutorial-<10min_tut_03_subset>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <indexing>](#to-user-guide-<indexing>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseFour">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to create plots in pandas?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_04_plotting>](#straight-to-tutorial...<10min_tut_04_plotting>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseFour" class="collapse" data-parent="#accordion">                 <div class="card-body">  pandas provides plotting for your data right out of the box with the power of Matplotlib. Simply pick the plot type (scatter, bar, boxplot,...) corresponding to your data.  .. image:: ../_static/schemas/04_plot_overview.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_04_plotting>](#to-introduction-tutorial-<10min_tut_04_plotting>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <visualization>](#to-user-guide-<visualization>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseFive">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to create new columns derived from existing columns?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_05_columns>](#straight-to-tutorial...<10min_tut_05_columns>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseFive" class="collapse" data-parent="#accordion">                 <div class="card-body">  There's no need to loop over all rows of your data table to do calculations. Column data manipulations work elementwise in pandas. Adding a column to a `DataFrame` based on existing data in other columns is straightforward.  .. image:: ../_static/schemas/05_newcolumn_2.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_05_columns>](#to-introduction-tutorial-<10min_tut_05_columns>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <basics.dataframe.sel_add_del>](#to-user-guide-<basics.dataframe.sel_add_del>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseSix">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to calculate summary statistics?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_06_stats>](#straight-to-tutorial...<10min_tut_06_stats>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseSix" class="collapse" data-parent="#accordion">                 <div class="card-body">  Basic statistics (mean, median, min, max, counts...) are easily calculable across data frames. These, or even custom aggregations, can be applied on the entire data set, a sliding window of the data, or grouped by categories. The latter is also known as the split-apply-combine approach.  .. image:: ../_static/schemas/06_groupby.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_06_stats>](#to-introduction-tutorial-<10min_tut_06_stats>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <groupby>](#to-user-guide-<groupby>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseSeven">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to reshape the layout of tables?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_07_reshape>](#straight-to-tutorial...<10min_tut_07_reshape>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseSeven" class="collapse" data-parent="#accordion">                 <div class="card-body">  Change the structure of your data table in a variety of ways. You can use `~pandas.melt` to reshape your data from a wide format to a long and tidy one. Use `~pandas.pivot`  to go from long to wide format. With aggregations built-in, a pivot table can be created with a single command.  .. image:: ../_static/schemas/07_melt.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_07_reshape>](#to-introduction-tutorial-<10min_tut_07_reshape>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <reshaping>](#to-user-guide-<reshaping>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseEight">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to combine data from multiple tables?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_08_combine>](#straight-to-tutorial...<10min_tut_08_combine>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseEight" class="collapse" data-parent="#accordion">                 <div class="card-body">  Multiple tables can be concatenated column wise or row wise with pandas' database-like join and merge operations.  .. image:: ../_static/schemas/08_concat_row.svg    :align: center  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_08_combine>](#to-introduction-tutorial-<10min_tut_08_combine>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <merging>](#to-user-guide-<merging>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseNine">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to handle time series data?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_09_timeseries>](#straight-to-tutorial...<10min_tut_09_timeseries>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseNine" class="collapse" data-parent="#accordion">                 <div class="card-body">  pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_09_timeseries>](#to-introduction-tutorial-<10min_tut_09_timeseries>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <timeseries>](#to-user-guide-<timeseries>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>          <div class="card tutorial-card">             <div class="card-header collapsed card-link" data-bs-toggle="collapse" data-bs-target="#collapseTen">                 <div class="d-flex flex-row tutorial-card-header-1">                     <div class="d-flex flex-row tutorial-card-header-2">                         <button class="btn btn-dark btn-sm"></button>                         How to manipulate textual data?                     </div>                     <span class="badge gs-badge-link">  [Straight to tutorial...<10min_tut_10_text>](#straight-to-tutorial...<10min_tut_10_text>)  .. raw:: html                      </span>                 </div>             </div>             <div id="collapseTen" class="collapse" data-parent="#accordion">                 <div class="card-body">  Data sets often contain more than just numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.  .. raw:: html                      <div class="d-flex flex-row">                         <span class="badge gs-badge-link">  [To introduction tutorial <10min_tut_10_text>](#to-introduction-tutorial-<10min_tut_10_text>)  .. raw:: html                          </span>                         <span class="badge gs-badge-link">  [To user guide <text>](#to-user-guide-<text>)  .. raw:: html                          </span>                     </div>                 </div>             </div>         </div>      </div>     </div>   .. _comingfrom:  Coming from... --------------  Are you familiar with other software for manipulating tabular data? Learn the pandas-equivalent operations compared to software you already know:  .. grid:: 1 2 2 2     :gutter: 4     :class-container: sd-text-center sd-d-inline-flex      .. grid-item-card::         :img-top: ../_static/logo_r.svg         :columns: 12 6 6 6         :class-card: comparison-card         :shadow: md          The `R programming language <https://www.r-project.org/>`__ provides a``data.frame``data structure as well as packages like         `tidyverse <https://www.tidyverse.org>`__ which use and extend``data.frame`for convenient data handling functionalities similar to pandas.          +++          .. button-ref:: compare_with_r             :ref-type: ref             :click-parent:             :color: secondary             :expand:              Learn more      .. grid-item-card::         :img-top: ../_static/logo_sql.svg         :columns: 12 6 6 6         :class-card: comparison-card         :shadow: md          Already familiar with`SELECT`,`GROUP BY`,`JOIN`, etc.?         Many SQL manipulations have equivalents in pandas.          +++          .. button-ref:: compare_with_sql             :ref-type: ref             :click-parent:             :color: secondary             :expand:              Learn more      .. grid-item-card::         :img-top: ../_static/logo_stata.svg         :columns: 12 6 6 6         :class-card: comparison-card         :shadow: md          The`data set``included in the `STATA <https://en.wikipedia.org/wiki/Stata>`__         statistical software suite corresponds to the pandas``DataFrame``.         Many of the operations known from STATA have an equivalent in pandas.          +++          .. button-ref:: compare_with_stata             :ref-type: ref             :click-parent:             :color: secondary             :expand:              Learn more      .. grid-item-card::         :img-top: ../_static/spreadsheets/logo_excel.svg         :columns: 12 6 6 6         :class-card: comparison-card         :shadow: md          Users of `Excel <https://en.wikipedia.org/wiki/Microsoft_Excel>`__         or other spreadsheet programs will find that many of the concepts are         transferable to pandas.          +++          .. button-ref:: compare_with_spreadsheets             :ref-type: ref             :click-parent:             :color: secondary             :expand:              Learn more      .. grid-item-card::         :img-top: ../_static/logo_sas.svg         :columns: 12 6 6 6         :class-card: comparison-card         :shadow: md          `SAS <https://en.wikipedia.org/wiki/SAS_(software)>`__, the statistical software suite,         uses the``data set`structure, which closely corresponds pandas'`DataFrame\`\`. Also SAS vectorized operations such as filtering or string processing operations have similar functions in pandas.

</div>

> \+++
> 
> <div class="button-ref" data-ref-type="ref" data-click-parent="" color="secondary" data-expand="">
> 
> compare\_with\_sas
> 
> Learn more
> 
> </div>

## Tutorials

For a quick overview of pandas functionality, see \[10 Minutes to pandas\<10min\>\](\#10-minutes-to-pandas\<10min\>).

You can also reference the pandas [cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) for a succinct guide for manipulating data with pandas.

The community produces a wide variety of tutorials available online. Some of the material is enlisted in the community contributed \[communitytutorials\](\#communitytutorials).

<div class="toctree" data-maxdepth="2" hidden="">

install overview intro\_tutorials/index comparison/index tutorials

</div>

---

install.md

---

<div id="install">

{{ header }}

</div>

# Installation

The pandas development team officially distributes pandas for installation through the following methods:

  - Available on [conda-forge](https://anaconda.org/conda-forge/pandas) for installation with the conda package manager.
  - Available on [PyPI](https://pypi.org/project/pandas/) for installation with pip.
  - Available on [Github](https://github.com/pandas-dev/pandas) for installation from source.

<div class="note">

<div class="title">

Note

</div>

pandas may be installable from other sources besides the ones listed above, but they are **not** managed by the pandas development team.

</div>

## Python version support

See \[Python support policy \<policies.python\_support\>\](\#python-support-policy-\<policies.python\_support\>).

## Installing pandas

### Installing with Conda

For users working with the [Conda](https://conda.io/en/latest/) package manager, pandas can be installed from the `conda-forge` channel.

`` `shell     conda install -c conda-forge pandas  To install the Conda package manager on your system, the ``<span class="title-ref"> \`Miniforge distribution \<https://github.com/conda-forge/miniforge?tab=readme-ov-file\#install\></span>\_\_ is recommended.

Additionally, it is recommended to install and run pandas from a virtual environment.

`` `shell     conda create -c conda-forge -n name_of_my_env python pandas     # On Linux or MacOS     source activate name_of_my_env     # On Windows     activate name_of_my_env  .. tip::     For users that are new to Python, the easiest way to install Python, pandas, and the     packages that make up the `PyData <https://pydata.org/>`__ stack such as     `SciPy <https://scipy.org/>`__, `NumPy <https://numpy.org/>`__ and     `Matplotlib <https://matplotlib.org/>`__     is with `Anaconda <https://docs.anaconda.com/anaconda/install/>`__, a cross-platform     (Linux, macOS, Windows) Python distribution for data analytics and     scientific computing.      However, pandas from Anaconda is **not** officially managed by the pandas development team.  .. _install.pip:  Installing with pip ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

For users working with the [pip](https://pip.pypa.io/en/stable/) package manager, pandas can be installed from [PyPI](https://pypi.org/project/pandas/).

`` `shell     pip install pandas  pandas can also be installed with sets of optional dependencies to enable certain functionality. For example, ``\` to install pandas with the optional dependencies to read Excel files.

`` `shell     pip install "pandas[excel]"  The full list of extras that can be installed can be found in the [dependency section.<install.optional_dependencies>](#dependency-section.<install.optional_dependencies>)  Additionally, it is recommended to install and run pandas from a virtual environment, for example, ``<span class="title-ref"> using the Python standard library's \`venv \<https://docs.python.org/3/library/venv.html\></span>\_\_

### Installing from source

See the \[contributing guide \<contributing\>\](\#contributing-guide-\<contributing\>) for complete instructions on building from the git source tree. Further, see \[creating a development environment \<contributing\_environment\>\](\#creating-a-development-environment-\<contributing\_environment\>) if you wish to create a pandas development environment.

### Installing the development version of pandas

Installing the development version is the quickest way to:

  - Try a new feature that will be shipped in the next release (that is, a feature from a pull-request that was recently merged to the main branch).
  - Check whether a bug you encountered has been fixed since the last release.

The development version is usually uploaded daily to the scientific-python-nightly-wheels index from the PyPI registry of anaconda.org. You can install it by running.

`` `shell     pip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas  .. note::     You might be required to uninstall an existing version of pandas to install the development version.      .. code-block:: shell          pip uninstall pandas -y  Running the test suite ``\` ----------------------

If pandas has been installed \[from source \<install.source\>\](\#from-source-\<install.source\>), running `pytest pandas` will run all of pandas unit tests.

The unit tests can also be run from the pandas module itself with the <span class="title-ref">test</span> function. The packages required to run the tests can be installed with `pip install "pandas[test]"`.

\> **Note** \> Test failures are not necessarily indicative of a broken pandas installation.

## Dependencies

### Required dependencies

pandas requires the following dependencies.

| Package                                                       | Minimum supported version |
| ------------------------------------------------------------- | ------------------------- |
| [NumPy](https://numpy.org)                                    | 1.23.5                    |
| [python-dateutil](https://dateutil.readthedocs.io/en/stable/) | 2.8.2                     |
| [tzdata](https://pypi.org/project/tzdata/)                    | 2022.7                    |

### Optional dependencies

pandas has many optional dependencies that are only used for specific methods. For example, <span class="title-ref">pandas.read\_hdf</span> requires the `pytables` package, while <span class="title-ref">DataFrame.to\_markdown</span> requires the `tabulate` package. If the optional dependency is not installed, pandas will raise an `ImportError` when the method requiring that dependency is called.

With pip, optional pandas dependencies can be installed or managed in a file (e.g. requirements.txt or pyproject.toml) as optional extras (e.g. `pandas[performance, aws]`). All optional dependencies can be installed with `pandas[all]`, and specific sets of dependencies are listed in the sections below.

#### Performance dependencies (recommended)

\> **Note** \> You are highly encouraged to install these libraries, as they provide speed improvements, especially when working with large data sets.

Installable with `pip install "pandas[performance]"`

| Dependency                                         | Minimum Version | pip extra   | Notes                                                                                                                                                                             |
| -------------------------------------------------- | --------------- | ----------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [numexpr](https://github.com/pydata/numexpr)       | 2.8.4           | performance | Accelerates certain numerical operations by using multiple cores as well as smart chunking and caching to achieve large speedups                                                  |
| [bottleneck](https://github.com/pydata/bottleneck) | 1.3.6           | performance | Accelerates certain types of `nan` by using specialized cython routines to achieve large speedup.                                                                                 |
| [numba](https://github.com/numba/numba)            | 0.56.4          | performance | Alternative execution engine for operations that accept `engine="numba"` using a JIT compiler that translates Python functions to optimized machine code using the LLVM compiler. |

#### Visualization

Installable with `pip install "pandas[plot, output-formatting]"`.

| Dependency | Minimum Version | pip extra         | Notes                                                                                             |
| ---------- | --------------- | ----------------- | ------------------------------------------------------------------------------------------------- |
| matplotlib | 3.6.3           | plot              | Plotting library                                                                                  |
| Jinja2     | 3.1.2           | output-formatting | Conditional formatting with DataFrame.style                                                       |
| tabulate   | 0.9.0           | output-formatting | Printing in Markdown-friendly format (see [tabulate](https://github.com/astanin/python-tabulate)) |

#### Computation

Installable with `pip install "pandas[computation]"`.

| Dependency | Minimum Version | pip extra   | Notes                                  |
| ---------- | --------------- | ----------- | -------------------------------------- |
| SciPy      | 1.10.0          | computation | Miscellaneous statistical functions    |
| xarray     | 2022.12.0       | computation | pandas-like API for N-dimensional data |

#### Excel files

Installable with `pip install "pandas[excel]"`.

| Dependency      | Minimum Version | pip extra | Notes                                                      |
| --------------- | --------------- | --------- | ---------------------------------------------------------- |
| xlrd            | 2.0.1           | excel     | Reading for xls files                                      |
| xlsxwriter      | 3.0.5           | excel     | Writing for xlsx files                                     |
| openpyxl        | 3.1.0           | excel     | Reading / writing for Excel 2010 xlsx/xlsm/xltx/xltm files |
| pyxlsb          | 1.0.10          | excel     | Reading for xlsb files                                     |
| python-calamine | 0.1.7           | excel     | Reading for xls/xlsx/xlsm/xlsb/xla/xlam/ods files          |
| odfpy           | 1.4.1           | excel     | Reading / writing for OpenDocument 1.2 files               |

#### HTML

Installable with `pip install "pandas[html]"`.

| Dependency     | Minimum Version | pip extra | Notes                      |
| -------------- | --------------- | --------- | -------------------------- |
| BeautifulSoup4 | 4.11.2          | html      | HTML parser for read\_html |
| html5lib       | 1.1             | html      | HTML parser for read\_html |
| lxml           | 4.9.2           | html      | HTML parser for read\_html |

One of the following combinations of libraries is needed to use the top-level <span class="title-ref">\~pandas.read\_html</span> function:

  - [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup) and [html5lib](https://github.com/html5lib/html5lib-python)
  - [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup) and [lxml](https://lxml.de)
  - [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup) and [html5lib](https://github.com/html5lib/html5lib-python) and [lxml](https://lxml.de)
  - Only [lxml](https://lxml.de), although see \[HTML Table Parsing \<io.html.gotchas\>\](\#html-table-parsing-\<io.html.gotchas\>) for reasons as to why you should probably **not** take this approach.

\> **Warning** \> \* if you install [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup) you must install either [lxml](https://lxml.de) or [html5lib](https://github.com/html5lib/html5lib-python) or both. <span class="title-ref">\~pandas.read\_html</span> will **not** work with *only* [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup) installed. \* You are highly encouraged to read \[HTML Table Parsing gotchas \<io.html.gotchas\>\](\#html-table-parsing-gotchas-\<io.html.gotchas\>). It explains issues surrounding the installation and usage of the above three libraries.

#### XML

Installable with `pip install "pandas[xml]"`.

| Dependency | Minimum Version | pip extra | Notes                                                 |
| ---------- | --------------- | --------- | ----------------------------------------------------- |
| lxml       | 4.9.2           | xml       | XML parser for read\_xml and tree builder for to\_xml |

#### SQL databases

Traditional drivers are installable with `pip install "pandas[postgresql, mysql, sql-other]"`

| Dependency             | Minimum Version | pip extra                    | Notes                                       |
| ---------------------- | --------------- | ---------------------------- | ------------------------------------------- |
| SQLAlchemy             | 2.0.0           | postgresql, mysql, sql-other | SQL support for databases other than sqlite |
| psycopg2               | 2.9.6           | postgresql                   | PostgreSQL engine for sqlalchemy            |
| pymysql                | 1.0.2           | mysql                        | MySQL engine for sqlalchemy                 |
| adbc-driver-postgresql | 0.10.0          | postgresql                   | ADBC Driver for PostgreSQL                  |
| adbc-driver-sqlite     | 0.8.0           | sql-other                    | ADBC Driver for SQLite                      |

#### Other data sources

Installable with `pip install "pandas[hdf5, parquet, feather, spss, excel]"`

| Dependency  | Minimum Version | pip extra        | Notes                                                                |
| ----------- | --------------- | ---------------- | -------------------------------------------------------------------- |
| PyTables    | 3.8.0           | hdf5             | HDF5-based reading / writing                                         |
| blosc zlib  | 1.21.3          | hdf5 hdf5        | Compression for HDF5; only available on `conda` Compression for HDF5 |
| fastparquet | 2023.10.0       | \-               | Parquet reading / writing (pyarrow is default)                       |
| pyarrow     | 10.0.1          | parquet, feather | Parquet, ORC, and feather reading / writing                          |
| pyreadstat  | 1.2.0           | spss             | SPSS files (.sav) reading                                            |
| odfpy       | 1.4.1           | excel            | Open document format (.odf, .ods, .odt) reading / writing            |

<div id="install.warn_orc">

\> **Warning** \> \* If you want to use <span class="title-ref">\~pandas.read\_orc</span>, it is highly recommended to install pyarrow using conda. <span class="title-ref">\~pandas.read\_orc</span> may fail if pyarrow was installed from pypi, and <span class="title-ref">\~pandas.read\_orc</span> is not compatible with Windows OS.

</div>

#### Access data in the cloud

Installable with `pip install "pandas[fss, aws, gcp]"`

| Dependency | Minimum Version | pip extra     | Notes                                                                                 |
| ---------- | --------------- | ------------- | ------------------------------------------------------------------------------------- |
| fsspec     | 2022.11.0       | fss, gcp, aws | Handling files aside from simple local and HTTP (required dependency of s3fs, gcsfs). |
| gcsfs      | 2022.11.0       | gcp           | Google Cloud Storage access                                                           |
| s3fs       | 2022.11.0       | aws           | Amazon S3 access                                                                      |

#### Clipboard

Installable with `pip install "pandas[clipboard]"`.

| Dependency  | Minimum Version | pip extra | Notes         |
| ----------- | --------------- | --------- | ------------- |
| PyQt4/PyQt5 | 5.15.9          | clipboard | Clipboard I/O |
| qtpy        | 2.3.0           | clipboard | Clipboard I/O |

\> **Note** \> Depending on operating system, system-level packages may need to installed. For clipboard to operate on Linux one of the CLI tools `xclip` or `xsel` must be installed on your system.

#### Compression

Installable with `pip install "pandas[compression]"`

| Dependency | Minimum Version | pip extra   | Notes                 |
| ---------- | --------------- | ----------- | --------------------- |
| Zstandard  | 0.19.0          | compression | Zstandard compression |

#### Timezone

Installable with `pip install "pandas[timezone]"`

| Dependency | Minimum Version | pip extra | Notes                                       |
| ---------- | --------------- | --------- | ------------------------------------------- |
| pytz       | 2023.4          | timezone  | Alternative timezone library to `zoneinfo`. |

---

01_table_oriented.md

---

<div id="10min_tut_01_tableoriented">

{{ header }}

</div>

# What kind of data does pandas handle?

<ul class="task-bullet">
    <li>

I want to start using pandas

<div class="ipython">

python

import pandas as pd

</div>

To load the pandas package and start working with it, import the package. The community agreed alias for pandas is `pd`, so loading pandas as `pd` is assumed standard practice for all of the pandas documentation.

</li>
</ul>

## pandas data table representation

![image](../../_static/schemas/01_table_dataframe.svg)

<ul class="task-bullet">
    <li>

I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {
        
          - "Name": \[  
            "Braund, Mr. Owen Harris", "Allen, Mr. William Henry", "Bonnell, Miss Elizabeth",
        
        \], "Age": \[22, 35, 58\], "Sex": \["male", "male", "female"\],
    
    }

) df

</div>

To manually store data in a table, create a `DataFrame`. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the `DataFrame`.

</li>
</ul>

A <span class="title-ref">DataFrame</span> is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table or the `data.frame` in R.

  - The table has 3 columns, each of them with a column label. The column labels are respectively `Name`, `Age` and `Sex`.
  - The column `Name` consists of textual data with each value a string, the column `Age` are numbers and the column `Sex` is textual data.

In spreadsheet software, the table representation of our data would look very similar:

![image](../../_static/schemas/01_table_spreadsheet.png)

## Each column in a `DataFrame` is a `Series`

![image](../../_static/schemas/01_table_series.svg)

<ul class="task-bullet">
    <li>

Iâ€™m just interested in working with the data in the column `Age`

<div class="ipython">

python

df\["Age"\]

</div>

When selecting a single column of a pandas <span class="title-ref">DataFrame</span>, the result is a pandas <span class="title-ref">Series</span>. To select the column, use the column label in between square brackets `[]`.

</li>
</ul>

<div class="note">

<div class="title">

Note

</div>

If you are familiar with Python \[dictionaries \<python:tut-dictionaries\>\](\#dictionaries-\<python:tut-dictionaries\>), the selection of a single column is very similar to the selection of dictionary values based on the key.

</div>

You can create a `Series` from scratch as well:

<div class="ipython">

python

ages = pd.Series(\[22, 35, 58\], name="Age") ages

</div>

A pandas `Series` has no column labels, as it is just a single column of a `DataFrame`. A Series does have row labels.

## Do something with a DataFrame or Series

<ul class="task-bullet">
    <li>

I want to know the maximum Age of the passengers

We can do this on the `DataFrame` by selecting the `Age` column and applying `max()`:

<div class="ipython">

python

df\["Age"\].max()

</div>

Or to the `Series`:

<div class="ipython">

python

ages.max()

</div>

</li>
</ul>

As illustrated by the `max()` method, you can *do* things with a `DataFrame` or `Series`. pandas provides a lot of functionalities, each of them a *method* you can apply to a `DataFrame` or `Series`. As methods are functions, do not forget to use parentheses `()`.

<ul class="task-bullet">
    <li>

Iâ€™m interested in some basic statistics of the numerical data of my data table

<div class="ipython">

python

df.describe()

</div>

The <span class="title-ref">\~DataFrame.describe</span> method provides a quick overview of the numerical data in a `DataFrame`. As the `Name` and `Sex` columns are textual data, these are by default not taken into account by the <span class="title-ref">\~DataFrame.describe</span> method.

</li>
</ul>

Many pandas operations return a `DataFrame` or a `Series`. The <span class="title-ref">\~DataFrame.describe</span> method is an example of a pandas operation returning a pandas `Series` or a pandas `DataFrame`.

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

Check more options on `describe` in the user guide section about \[aggregations with describe \<basics.describe\>\](\#aggregations-with-describe-\<basics.describe\>)

</div>

<div class="note">

<div class="title">

Note

</div>

This is just a starting point. Similar to spreadsheet software, pandas represents data as a table with columns and rows. Apart from the representation, the data manipulations and calculations you would do in spreadsheet software are also supported by pandas. Continue reading the next tutorials to get started\!

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Import the package, aka `import pandas as pd`
  - A table of data is stored as a pandas `DataFrame`
  - Each column in a `DataFrame` is a `Series`
  - You can do things by applying a method on a `DataFrame` or `Series`

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A more extended explanation of `DataFrame` and `Series` is provided in the \[introduction to data structures \<dsintro\>\](\#introduction-to-data-structures-\<dsintro\>) page.

</div>

---

02_read_write.md

---

<div id="10min_tut_02_read_write">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Titanic data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:

  - PassengerId: Id of every passenger.
  - Survived: Indication whether passenger survived. `0` for yes and `1` for no.
  - Pclass: One out of the 3 ticket classes: Class `1`, Class `2` and Class `3`.
  - Name: Name of passenger.
  - Sex: Gender of passenger.
  - Age: Age of passenger in years.
  - SibSp: Number of siblings or spouses aboard.
  - Parch: Number of parents or children aboard.
  - Ticket: Ticket number of passenger.
  - Fare: Indicating the fare.
  - Cabin: Cabin number of passenger.
  - Embarked: Port of embarkation.

</p>
<a href="https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

</li>
</ul>
</div>

# How do I read and write tabular data?

![image](../../_static/schemas/02_io_readwrite.svg)

<ul class="task-bullet">
    <li>

I want to analyze the Titanic passenger data, available as a CSV file.

<div class="ipython">

python

titanic = pd.read\_csv("data/titanic.csv")

</div>

pandas provides the <span class="title-ref">read\_csv</span> function to read data stored as a csv file into a pandas `DataFrame`. pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, â€¦), each of them with the prefix `read_*`.

</li>
</ul>

Make sure to always have a check on the data after reading in the data. When displaying a `DataFrame`, the first and last 5 rows will be shown by default:

<div class="ipython">

python

titanic

</div>

<ul class="task-bullet">
    <li>

I want to see the first 8 rows of a pandas DataFrame.

<div class="ipython">

python

titanic.head(8)

</div>

To see the first N rows of a `DataFrame`, use the <span class="title-ref">\~DataFrame.head</span> method with the required number of rows (in this case 8) as argument.

</li>
</ul>

\> **Note** \> Interested in the last N rows instead? pandas also provides a <span class="title-ref">\~DataFrame.tail</span> method. For example, `titanic.tail(10)` will return the last 10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be done by requesting the pandas `dtypes` attribute:

<div class="ipython">

python

titanic.dtypes

</div>

For each of the columns, the used data type is enlisted. The data types in this `DataFrame` are integers (`int64`), floats (`float64`) and strings (`object`).

<div class="note">

<div class="title">

Note

</div>

When asking for the `dtypes`, no parentheses `()` are used\! `dtypes` is an attribute of a `DataFrame` and `Series`. Attributes of a `DataFrame` or `Series` do not need `()`. Attributes represent a characteristic of a `DataFrame`/`Series`, whereas methods (which require parentheses `()`) *do* something with the `DataFrame`/`Series` as introduced in the \[first tutorial \<10min\_tut\_01\_tableoriented\>\](\#first-tutorial-\<10min\_tut\_01\_tableoriented\>).

</div>

<ul class="task-bullet">
    <li>

My colleague requested the Titanic data as a spreadsheet.

<div class="note">

<div class="title">

Note

</div>

If you want to use <span class="title-ref">\~pandas.to\_excel</span> and <span class="title-ref">\~pandas.read\_excel</span>, you need to install an Excel reader as outlined in the \[Excel files \<install.excel\_dependencies\>\](\#excel-files-\<install.excel\_dependencies\>) section of the installation documentation.

</div>

<div class="ipython">

python

titanic.to\_excel("titanic.xlsx", sheet\_name="passengers", index=False)

</div>

Whereas `read_*` functions are used to read data to pandas, the `to_*` methods are used to store data. The <span class="title-ref">\~DataFrame.to\_excel</span> method stores the data as an excel file. In the example here, the `sheet_name` is named *passengers* instead of the default *Sheet1*. By setting `index=False` the row index labels are not saved in the spreadsheet.

</li>
</ul>

The equivalent read function <span class="title-ref">\~DataFrame.read\_excel</span> will reload the data to a `DataFrame`:

<div class="ipython">

python

titanic = pd.read\_excel("titanic.xlsx", sheet\_name="passengers")

</div>

<div class="ipython">

python

titanic.head()

</div>

<div class="ipython" data-suppress="">

python

import os

os.remove("titanic.xlsx")

</div>

<ul class="task-bullet">
    <li>

Iâ€™m interested in a technical summary of a `DataFrame`

<div class="ipython">

python

titanic.info()

</div>

The method <span class="title-ref">\~DataFrame.info</span> provides technical information about a `DataFrame`, so letâ€™s explain the output in more detail:

  - It is indeed a <span class="title-ref">DataFrame</span>.
  - There are 891 entries, i.e.Â 891 rows.
  - Each row has a row label (aka the `index`) with values ranging from 0 to 890.
  - The table has 12 columns. Most columns have a value for each of the rows (all 891 values are `non-null`). Some columns do have missing values and less than 891 `non-null` values.
  - The columns `Name`, `Sex`, `Cabin` and `Embarked` consist of textual data (strings, aka `object`). The other columns are numerical data, some of them are whole numbers (`integer`) and others are real numbers (`float`).
  - The kind of data (characters, integers, â€¦) in the different columns are summarized by listing the `dtypes`.
  - The approximate amount of RAM used to hold the DataFrame is provided as well.

</li>
</ul>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Getting data in to pandas from many different file formats or data sources is supported by `read_*` functions.
  - Exporting data out of pandas is provided by different `to_*` methods.
  - The `head`/`tail`/`info` methods and the `dtypes` attribute are convenient for a first check.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

For a complete overview of the input and output possibilities from and to pandas, see the user guide section about \[reader and writer functions \<io\>\](\#reader-and-writer-functions-\<io\>).

</div>

---

03_subset_data.md

---

<div id="10min_tut_03_subset">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Titanic data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:

  - PassengerId: Id of every passenger.
  - Survived: Indication whether passenger survived. `0` for yes and `1` for no.
  - Pclass: One out of the 3 ticket classes: Class `1`, Class `2` and Class `3`.
  - Name: Name of passenger.
  - Sex: Gender of passenger.
  - Age: Age of passenger in years.
  - SibSp: Number of siblings or spouses aboard.
  - Parch: Number of parents or children aboard.
  - Ticket: Ticket number of passenger.
  - Fare: Indicating the fare.
  - Cabin: Cabin number of passenger.
  - Embarked: Port of embarkation.

</p>
<a href="https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

titanic = pd.read\_csv("data/titanic.csv") titanic.head()

</div>

</li>
</ul>
</div>

# How do I select a subset of a `DataFrame`?

## How do I select specific columns from a `DataFrame`?

![image](../../_static/schemas/03_subset_columns.svg)

<ul class="task-bullet">
    <li>

Iâ€™m interested in the age of the Titanic passengers.

<div class="ipython">

python

ages = titanic\["Age"\] ages.head()

</div>

To select a single column, use square brackets `[]` with the column name of the column of interest.

</li>
</ul>

Each column in a <span class="title-ref">DataFrame</span> is a <span class="title-ref">Series</span>. As a single column is selected, the returned object is a pandas <span class="title-ref">Series</span>. We can verify this by checking the type of the output:

<div class="ipython">

python

type(titanic\["Age"\])

</div>

And have a look at the `shape` of the output:

<div class="ipython">

python

titanic\["Age"\].shape

</div>

<span class="title-ref">DataFrame.shape</span> is an attribute (remember \[tutorial on reading and writing \<10min\_tut\_02\_read\_write\>\](\#tutorial-on-reading-and-writing-\<10min\_tut\_02\_read\_write\>), do not use parentheses for attributes) of a pandas `Series` and `DataFrame` containing the number of rows and columns: *(nrows, ncolumns)*. A pandas Series is 1-dimensional and only the number of rows is returned.

<ul class="task-bullet">
    <li>

Iâ€™m interested in the age and sex of the Titanic passengers.

<div class="ipython">

python

age\_sex = titanic\[\["Age", "Sex"\]\] age\_sex.head()

</div>

To select multiple columns, use a list of column names within the selection brackets `[]`.

</li>
</ul>

<div class="note">

<div class="title">

Note

</div>

The inner square brackets define a \[Python list \<python:tut-morelists\>\](\#python-list-\<python:tut-morelists\>) with column names, whereas the outer square brackets are used to select the data from a pandas `DataFrame` as seen in the previous example.

</div>

The returned data type is a pandas DataFrame:

<div class="ipython">

python

type(titanic\[\["Age", "Sex"\]\])

</div>

<div class="ipython">

python

titanic\[\["Age", "Sex"\]\].shape

</div>

The selection returned a `DataFrame` with 891 rows and 2 columns. Remember, a `DataFrame` is 2-dimensional with both a row and column dimension.

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

For basic information on indexing, see the user guide section on \[indexing and selecting data \<indexing.basics\>\](\#indexing-and-selecting-data-\<indexing.basics\>).

</div>

## How do I filter specific rows from a `DataFrame`?

![image](../../_static/schemas/03_subset_rows.svg)

<ul class="task-bullet">
    <li>

Iâ€™m interested in the passengers older than 35 years.

<div class="ipython">

python

above\_35 = titanic\[titanic\["Age"\] \> 35\] above\_35.head()

</div>

To select rows based on a conditional expression, use a condition inside the selection brackets `[]`.

</li>
</ul>

The condition inside the selection brackets `titanic["Age"] > 35` checks for which rows the `Age` column has a value larger than 35:

<div class="ipython">

python

titanic\["Age"\] \> 35

</div>

The output of the conditional expression (`>`, but also `==`, `!=`, `<`, `<=`,â€¦ would work) is actually a pandas `Series` of boolean values (either `True` or `False`) with the same number of rows as the original `DataFrame`. Such a `Series` of boolean values can be used to filter the `DataFrame` by putting it in between the selection brackets `[]`. Only rows for which the value is `True` will be selected.

We know from before that the original Titanic `DataFrame` consists of 891 rows. Letâ€™s have a look at the number of rows which satisfy the condition by checking the `shape` attribute of the resulting `DataFrame` `above_35`:

<div class="ipython">

python

above\_35.shape

</div>

<ul class="task-bullet">
    <li>

Iâ€™m interested in the Titanic passengers from cabin class 2 and 3.

<div class="ipython">

python

class\_23 = titanic\[titanic\["Pclass"\].isin(\[2, 3\])\] class\_23.head()

</div>

Similar to the conditional expression, the <span class="title-ref">\~Series.isin</span> conditional function returns a `True` for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets `[]`. In this case, the condition inside the selection brackets `titanic["Pclass"].isin([2, 3])` checks for which rows the `Pclass` column is either 2 or 3.

</li>
</ul>

The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an `|` (or) operator:

<div class="ipython">

python

class\_23 = titanic\[(titanic\["Pclass"\] == 2) | (titanic\["Pclass"\] == 3)\] class\_23.head()

</div>

<div class="note">

<div class="title">

Note

</div>

When combining multiple conditional statements, each condition must be surrounded by parentheses `()`. Moreover, you can not use `or`/`and` but need to use the `or` operator `|` and the `and` operator `&`.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

See the dedicated section in the user guide about \[boolean indexing \<indexing.boolean\>\](\#boolean-indexing-\<indexing.boolean\>) or about the \[isin function \<indexing.basics.indexing\_isin\>\](\#isin-function-\<indexing.basics.indexing\_isin\>).

</div>

<ul class="task-bullet">
    <li>

I want to work with passenger data for which the age is known.

<div class="ipython">

python

age\_no\_na = titanic\[titanic\["Age"\].notna()\] age\_no\_na.head()

</div>

The <span class="title-ref">\~Series.notna</span> conditional function returns a `True` for each row the values are not a `Null` value. As such, this can be combined with the selection brackets `[]` to filter the data table.

</li>
</ul>

You might wonder what actually changed, as the first 5 lines are still the same values. One way to verify is to check if the shape has changed:

<div class="ipython">

python

age\_no\_na.shape

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

For more dedicated functions on missing values, see the user guide section about \[handling missing data \<missing\_data\>\](\#handling-missing-data-\<missing\_data\>).

</div>

## How do I select specific rows and columns from a `DataFrame`?

![image](../../_static/schemas/03_subset_columns_rows.svg)

<ul class="task-bullet">
    <li>

Iâ€™m interested in the names of the passengers older than 35 years.

<div class="ipython">

python

adult\_names = titanic.loc\[titanic\["Age"\] \> 35, "Name"\] adult\_names.head()

</div>

In this case, a subset of both rows and columns is made in one go and just using selection brackets `[]` is not sufficient anymore. The `loc`/`iloc` operators are required in front of the selection brackets `[]`. When using `loc`/`iloc`, the part before the comma is the rows you want, and the part after the comma is the columns you want to select.

</li>
</ul>

When using column names, row labels or a condition expression, use the `loc` operator in front of the selection brackets `[]`. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. Using a colon specifies you want to select all rows or columns.

<ul class="task-bullet">
    <li>

Iâ€™m interested in rows 10 till 25 and columns 3 to 5.

<div class="ipython">

python

titanic.iloc\[9:25, 2:5\]

</div>

Again, a subset of both rows and columns is made in one go and just using selection brackets `[]` is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the `iloc` operator in front of the selection brackets `[]`.

</li>
</ul>

When selecting specific rows and/or columns with `loc` or `iloc`, new values can be assigned to the selected data. For example, to assign the name `anonymous` to the first 3 elements of the fourth column:

<div class="ipython">

python

titanic.iloc\[0:3, 3\] = "anonymous" titanic.head()

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

See the user guide section on \[different choices for indexing \<indexing.choice\>\](\#different-choices-for-indexing-\<indexing.choice\>) to get more insight into the usage of `loc` and `iloc`.

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - When selecting subsets of data, square brackets `[]` are used.
  - Inside these square brackets, you can use a single column/row label, a list of column/row labels, a slice of labels, a conditional expression or a colon.
  - Use `loc` for label-based selection (using row/column names).
  - Use `iloc` for position-based selection (using table positions).
  - You can assign new values to a selection based on `loc`/`iloc`.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full overview of indexing is provided in the user guide pages on \[indexing and selecting data \<indexing\>\](\#indexing-and-selecting-data-\<indexing\>).

</div>

---

04_plotting.md

---

<div id="10min_tut_04_plotting">

{{ header }}

</div>

# How do I create plots in pandas?

![image](../../_static/schemas/04_plot_overview.svg)

<div class="ipython">

python

import pandas as pd import matplotlib.pyplot as plt

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Air quality data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

For this tutorial, air quality data about \(NO_2\) is used, made available by [OpenAQ](https://openaq.org) and using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package. The `air_quality_no2.csv` data set provides \(NO_2\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

air\_quality = pd.read\_csv("data/air\_quality\_no2.csv", index\_col=0, parse\_dates=True) air\_quality.head()

</div>

<div class="note">

<div class="title">

Note

</div>

The `index_col=0` and `parse_dates=True` parameters passed to the `read_csv` function define the first (0th) column as index of the resulting `DataFrame` and convert the dates in the column to <span class="title-ref">Timestamp</span> objects, respectively.

</div>

</li>
</ul>
</div>

<ul class="task-bullet">
    <li>

I want a quick visual check of the data.

<div class="ipython" data-okwarning="">

python

@savefig 04\_airqual\_quick.png air\_quality.plot() plt.show()

</div>

With a `DataFrame`, pandas creates by default one line plot for each of the columns with numeric data.

</li>
</ul>

<ul class="task-bullet">
    <li>

I want to plot only the columns of the data table with the data from Paris.

<div class="ipython" data-suppress="">

python

\# We need to clear the figure here as, within doc generation, the plot \# accumulates data on each plot(). This is not needed when running \# in a notebook, so is suppressed from output. plt.clf()

</div>

<div class="ipython" data-okwarning="">

python

@savefig 04\_airqual\_paris.png air\_quality\["station\_paris"\].plot() plt.show()

</div>

To plot a specific column, use a selection method from the \[subset data tutorial \<10min\_tut\_03\_subset\>\](\#subset-data-tutorial-\<10min\_tut\_03\_subset\>) in combination with the <span class="title-ref">\~DataFrame.plot</span> method. Hence, the <span class="title-ref">\~DataFrame.plot</span> method works on both `Series` and `DataFrame`.

</li>
</ul>

<ul class="task-bullet">
    <li>

I want to visually compare the \(NO_2\) values measured in London versus Paris.

<div class="ipython" data-okwarning="">

python

@savefig 04\_airqual\_scatter.png air\_quality.plot.scatter(x="station\_london", y="station\_paris", alpha=0.5) plt.show()

</div>

</li>
</ul>

Apart from the default `line` plot when using the `plot` function, a number of alternatives are available to plot data. Letâ€™s use some standard Python to get an overview of the available plot methods:

<div class="ipython">

python

  - \[  
    method\_name for method\_name in dir(air\_quality.plot) if not method\_name.startswith("\_")

\]

</div>

<div class="note">

<div class="title">

Note

</div>

In many development environments such as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example `air_quality.plot.` + TAB.

</div>

One of the options is <span class="title-ref">DataFrame.plot.box</span>, which refers to a [boxplot](https://en.wikipedia.org/wiki/Box_plot). The `box` method is applicable on the air quality example data:

<div class="ipython" data-okwarning="">

python

@savefig 04\_airqual\_boxplot.png air\_quality.plot.box() plt.show()

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

For an introduction to plots other than the default line plot, see the user guide section about \[supported plot styles \<visualization.other\>\](\#supported-plot-styles-\<visualization.other\>).

</div>

<ul class="task-bullet">
    <li>

I want each of the columns in a separate subplot.

<div class="ipython" data-okwarning="">

python

@savefig 04\_airqual\_area\_subplot.png axs = air\_quality.plot.area(figsize=(12, 4), subplots=True) plt.show()

</div>

Separate subplots for each of the data columns are supported by the `subplots` argument of the `plot` functions. The builtin options available in each of the pandas plot functions are worth reviewing.

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

Some more formatting options are explained in the user guide section on \[plot formatting \<visualization.formatting\>\](\#plot-formatting-\<visualization.formatting\>).

</div>

<ul class="task-bullet">
    <li>

I want to further customize, extend or save the resulting plot.

<div class="ipython" data-okwarning="">

python

fig, axs = plt.subplots(figsize=(12, 4)) air\_quality.plot.area(ax=axs) axs.set\_ylabel("NO$\_2$ concentration") @savefig 04\_airqual\_customized.png fig.savefig("no2\_concentrations.png") plt.show()

</div>

<div class="ipython" data-suppress="" data-okwarning="">

python

import os

os.remove("no2\_concentrations.png")

</div>

</li>
</ul>

Each of the plot objects created by pandas is a [Matplotlib](https://matplotlib.org/) object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of Matplotlib to the plot. This strategy is applied in the previous example:

    fig, axs = plt.subplots(figsize=(12, 4))        # Create an empty Matplotlib Figure and Axes
    air_quality.plot.area(ax=axs)                   # Use pandas to put the area plot on the prepared Figure/Axes
    axs.set_ylabel("NO$_2$ concentration")          # Do any Matplotlib customization you like
    fig.savefig("no2_concentrations.png")           # Save the Figure/Axes using the existing Matplotlib method.
    plt.show()                                      # Display the plot

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - The `.plot.*` methods are applicable on both Series and DataFrames.
  - By default, each of the columns is plotted as a different element (line, boxplot, â€¦).
  - Any plot created by pandas is a Matplotlib object.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full overview of plotting in pandas is provided in the \[visualization pages \<visualization\>\](\#visualization-pages-\<visualization\>).

</div>

---

05_add_columns.md

---

<div id="10min_tut_05_columns">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Air quality data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

For this tutorial, air quality data about \(NO_2\) is used, made available by [OpenAQ](https://openaq.org) and using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package. The `air_quality_no2.csv` data set provides \(NO_2\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

air\_quality = pd.read\_csv("data/air\_quality\_no2.csv", index\_col=0, parse\_dates=True) air\_quality.head()

</div>

</li>
</ul>
</div>

# How to create new columns derived from existing columns

![image](../../_static/schemas/05_newcolumn_1.svg)

<ul class="task-bullet">
    <li>

I want to express the \(NO_2\) concentration of the station in London in mg/m\(^3\).

(*If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882*)

<div class="ipython">

python

air\_quality\["london\_mg\_per\_cubic"\] = air\_quality\["station\_london"\] \* 1.882 air\_quality.head()

</div>

To create a new column, use the square brackets `[]` with the new column name at the left side of the assignment.

</li>
</ul>

<div class="note">

<div class="title">

Note

</div>

The calculation of the values is done **element-wise**. This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows\!

</div>

![image](../../_static/schemas/05_newcolumn_2.svg)

<ul class="task-bullet">
    <li>

I want to check the ratio of the values in Paris versus Antwerp and save the result in a new column.

<div class="ipython">

python

  - air\_quality\["ratio\_paris\_antwerp"\] = (  
    air\_quality\["station\_paris"\] / air\_quality\["station\_antwerp"\]

) air\_quality.head()

</div>

The calculation is again element-wise, so the `/` is applied *for the values in each row*.

</li>
</ul>

Other mathematical operators (`+`, `-`, `*`, `/`, â€¦) and logical operators (`<`, `>`, `==`, â€¦) also work element-wise. The latter was already used in the \[subset data tutorial \<10min\_tut\_03\_subset\>\](\#subset-data-tutorial-\<10min\_tut\_03\_subset\>) to filter rows of a table using a conditional expression.

If you need more advanced logic, you can use arbitrary Python code via <span class="title-ref">\~DataFrame.apply</span>.

<ul class="task-bullet">
    <li>

I want to rename the data columns to the corresponding station identifiers used by [OpenAQ](https://openaq.org/).

<div class="ipython">

python

  - air\_quality\_renamed = air\_quality.rename(
    
      - columns={  
        "station\_antwerp": "BETR801", "station\_paris": "FR04014", "station\_london": "London Westminster",
    
    }

)

</div>

<div class="ipython">

python

air\_quality\_renamed.head()

</div>

The <span class="title-ref">\~DataFrame.rename</span> function can be used for both row labels and column labels. Provide a dictionary with the keys the current names and the values the new names to update the corresponding names.

</li>
</ul>

The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:

<div class="ipython">

python

air\_quality\_renamed = air\_quality\_renamed.rename(columns=str.lower) air\_quality\_renamed.head()

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

Details about column or row label renaming is provided in the user guide section on \[renaming labels \<basics.rename\>\](\#renaming-labels-\<basics.rename\>).

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Create a new column by assigning the output to the DataFrame with a new column name in between the `[]`.
  - Operations are element-wise, no need to loop over rows.
  - Use `rename` with a dictionary or function to rename row labels or column names.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

The user guide contains a separate section on \[column addition and deletion \<basics.dataframe.sel\_add\_del\>\](\#column-addition-and-deletion-\<basics.dataframe.sel\_add\_del\>).

</div>

---

06_calculate_statistics.md

---

<div id="10min_tut_06_stats">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Titanic data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:

  - PassengerId: Id of every passenger.
  - Survived: Indication whether passenger survived. `0` for yes and `1` for no.
  - Pclass: One out of the 3 ticket classes: Class `1`, Class `2` and Class `3`.
  - Name: Name of passenger.
  - Sex: Gender of passenger.
  - Age: Age of passenger in years.
  - SibSp: Number of siblings or spouses aboard.
  - Parch: Number of parents or children aboard.
  - Ticket: Ticket number of passenger.
  - Fare: Indicating the fare.
  - Cabin: Cabin number of passenger.
  - Embarked: Port of embarkation.

</p>
<a href="https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

titanic = pd.read\_csv("data/titanic.csv") titanic.head()

</div>

</li>
</ul>
</div>

# How to calculate summary statistics

## Aggregating statistics

![image](../../_static/schemas/06_aggregate.svg)

<ul class="task-bullet">
    <li>

What is the average age of the Titanic passengers?

<div class="ipython">

python

titanic\["Age"\].mean()

</div>

</li>
</ul>

Different statistics are available and can be applied to columns with numerical data. Operations in general exclude missing data and operate across rows by default.

![image](../../_static/schemas/06_reduction.svg)

<ul class="task-bullet">
    <li>

What is the median age and ticket fare price of the Titanic passengers?

<div class="ipython">

python

titanic\[\["Age", "Fare"\]\].median()

</div>

The statistic applied to multiple columns of a `DataFrame` (the selection of two columns returns a `DataFrame`, see the \[subset data tutorial \<10min\_tut\_03\_subset\>\](\#subset-data-tutorial-\<10min\_tut\_03\_subset\>)) is calculated for each numeric column.

</li>
</ul>

The aggregating statistic can be calculated for multiple columns at the same time. Remember the `describe` function from the \[first tutorial \<10min\_tut\_01\_tableoriented\>\](\#first-tutorial-\<10min\_tut\_01\_tableoriented\>)?

<div class="ipython">

python

titanic\[\["Age", "Fare"\]\].describe()

</div>

Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the <span class="title-ref">DataFrame.agg</span> method:

<div class="ipython">

python

  - titanic.agg(
    
      - {  
        "Age": \["min", "max", "median", "skew"\], "Fare": \["min", "max", "median", "mean"\],
    
    }

)

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

Details about descriptive statistics are provided in the user guide section on \[descriptive statistics \<basics.stats\>\](\#descriptive-statistics-\<basics.stats\>).

</div>

## Aggregating statistics grouped by category

![image](../../_static/schemas/06_groupby.svg)

<ul class="task-bullet">
    <li>

What is the average age for male versus female Titanic passengers?

<div class="ipython">

python

titanic\[\["Sex", "Age"\]\].groupby("Sex").mean()

</div>

As our interest is the average age for each gender, a subselection on these two columns is made first: `titanic[["Sex", "Age"]]`. Next, the <span class="title-ref">\~DataFrame.groupby</span> method is applied on the `Sex` column to make a group per category. The average age *for each gender* is calculated and returned.

</li>
</ul>

Calculating a given statistic (e.g. `mean` age) *for each category in a column* (e.g.Â male/female in the `Sex` column) is a common pattern. The `groupby` method is used to support this type of operations. This fits in the more general `split-apply-combine` pattern:

  - **Split** the data into groups
  - **Apply** a function to each group independently
  - **Combine** the results into a data structure

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If not, the `mean` method is applied to each column containing numerical columns by passing `numeric_only=True`:

<div class="ipython">

python

titanic.groupby("Sex").mean(numeric\_only=True)

</div>

It does not make much sense to get the average value of the `Pclass`. If we are only interested in the average age for each gender, the selection of columns (square brackets `[]` as usual) is supported on the grouped data as well:

<div class="ipython">

python

titanic.groupby("Sex")\["Age"\].mean()

</div>

![image](../../_static/schemas/06_groupby_select_detail.svg)

<div class="note">

<div class="title">

Note

</div>

The `Pclass` column contains numerical data but actually represents 3 categories (or factors) with respectively the labels â€˜1â€™, â€˜2â€™ and â€˜3â€™. Calculating statistics on these does not make much sense. Therefore, pandas provides a `Categorical` data type to handle this type of data. More information is provided in the user guide \[categorical\](\#categorical) section.

</div>

<ul class="task-bullet">
    <li>

What is the mean ticket fare price for each of the sex and cabin class combinations?

<div class="ipython">

python

titanic.groupby(\["Sex", "Pclass"\])\["Fare"\].mean()

</div>

Grouping can be done by multiple columns at the same time. Provide the column names as a list to the <span class="title-ref">\~DataFrame.groupby</span> method.

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full description on the split-apply-combine approach is provided in the user guide section on \[groupby operations \<groupby\>\](\#groupby-operations-\<groupby\>).

</div>

## Count number of records by category

![image](../../_static/schemas/06_valuecounts.svg)

<ul class="task-bullet">
    <li>

What is the number of passengers in each of the cabin classes?

<div class="ipython">

python

titanic\["Pclass"\].value\_counts()

</div>

The <span class="title-ref">\~Series.value\_counts</span> method counts the number of records for each category in a column.

</li>
</ul>

The function is a shortcut, it is actually a groupby operation in combination with counting the number of records within each group:

<div class="ipython">

python

titanic.groupby("Pclass")\["Pclass"\].count()

</div>

<div class="note">

<div class="title">

Note

</div>

Both `size` and `count` can be used in combination with `groupby`. Whereas `size` includes `NaN` values and just provides the number of rows (size of the table), `count` excludes the missing values. In the `value_counts` method, use the `dropna` argument to include or exclude the `NaN` values.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

The user guide has a dedicated section on `value_counts` , see the page on \[discretization \<basics.discretization\>\](\#discretization-\<basics.discretization\>).

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Aggregation statistics can be calculated on entire columns or rows.
  - `groupby` provides the power of the *split-apply-combine* pattern.
  - `value_counts` is a convenient shortcut to count the number of entries in each category of a variable.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full description on the split-apply-combine approach is provided in the user guide pages about \[groupby operations \<groupby\>\](\#groupby-operations-\<groupby\>).

</div>

---

07_reshape_table_layout.md

---

<div id="10min_tut_07_reshape">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Titanic data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:

  - PassengerId: Id of every passenger.
  - Survived: Indication whether passenger survived. `0` for yes and `1` for no.
  - Pclass: One out of the 3 ticket classes: Class `1`, Class `2` and Class `3`.
  - Name: Name of passenger.
  - Sex: Gender of passenger.
  - Age: Age of passenger in years.
  - SibSp: Number of siblings or spouses aboard.
  - Parch: Number of parents or children aboard.
  - Ticket: Ticket number of passenger.
  - Fare: Indicating the fare.
  - Cabin: Cabin number of passenger.
  - Embarked: Port of embarkation.

</p>
<a href="https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

titanic = pd.read\_csv("data/titanic.csv") titanic.head()

</div>

</li>
<li class="list-group-item gs-data-list">
    <div data-bs-toggle="collapse" href="#collapsedata2" role="button" aria-expanded="false" aria-controls="collapsedata2">
        <span class="badge bg-secondary">Air quality data</span>
    </div>
    <div class="collapse" id="collapsedata2">
        <div class="card-body">
            <p class="card-text">

This tutorial uses air quality data about \(NO_2\) and Particulate matter less than 2.5 micrometers, made available by [OpenAQ](https://openaq.org) and using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package. The `air_quality_long.csv` data set provides \(NO_2\) and \(PM_{25}\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

The air-quality data set has the following columns:

  - city: city where the sensor is used, either Paris, Antwerp or London
  - country: country where the sensor is used, either FR, BE or GB
  - location: the id of the sensor, either *FR04014*, *BETR801* or *London Westminster*
  - parameter: the parameter measured by the sensor, either \(NO_2\) or Particulate matter
  - value: the measured value
  - unit: the unit of the measured parameter, in this case â€˜Âµg/mÂ³â€™

and the index of the `DataFrame` is `datetime`, the datetime of the measurement.

<div class="note">

<div class="title">

Note

</div>

The air-quality data is provided in a so-called *long format* data representation with each observation on a separate row and each variable a separate column of the data table. The long/narrow format is also known as the [tidy data format](https://www.jstatsoft.org/article/view/v059i10).

</div>

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_long.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

  - air\_quality = pd.read\_csv(  
    "data/air\_quality\_long.csv", index\_col="date.utc", parse\_dates=True

) air\_quality.head()

</div>

</li>
</ul>
</div>

# How to reshape the layout of tables

## Sort table rows

<ul class="task-bullet">
    <li>

I want to sort the Titanic data according to the age of the passengers.

<div class="ipython">

python

titanic.sort\_values(by="Age").head()

</div>

</li>
</ul>

<ul class="task-bullet">
    <li>

I want to sort the Titanic data according to the cabin class and age in descending order.

<div class="ipython">

python

titanic.sort\_values(by=\['Pclass', 'Age'\], ascending=False).head()

</div>

With <span class="title-ref">DataFrame.sort\_values</span>, the rows in the table are sorted according to the defined column(s). The index will follow the row order.

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

More details about sorting of tables is provided in the user guide section on \[sorting data \<basics.sorting\>\](\#sorting-data-\<basics.sorting\>).

</div>

## Long to wide table format

Letâ€™s use a small subset of the air quality data set. We focus on \(NO_2\) data and only use the first two measurements of each location (i.e.Â the head of each group). The subset of data will be called `no2_subset`.

<div class="ipython">

python

\# filter for no2 data only no2 = air\_quality\[air\_quality\["parameter"\] == "no2"\]

</div>

<div class="ipython">

python

\# use 2 measurements (head) for each location (groupby) no2\_subset = no2.sort\_index().groupby(\["location"\]).head(2) no2\_subset

</div>

![image](../../_static/schemas/07_pivot.svg)

<ul class="task-bullet">
    <li>

I want the values for the three stations as separate columns next to each other.

<div class="ipython">

python

no2\_subset.pivot(columns="location", values="value")

</div>

The <span class="title-ref">\~pandas.pivot</span> function is purely reshaping of the data: a single value for each index/column combination is required.

</li>
</ul>

As pandas supports plotting of multiple columns (see \[plotting tutorial \<10min\_tut\_04\_plotting\>\](\#plotting-tutorial-\<10min\_tut\_04\_plotting\>)) out of the box, the conversion from *long* to *wide* table format enables the plotting of the different time series at the same time:

<div class="ipython">

python

no2.head()

</div>

<div class="ipython">

python

@savefig 7\_reshape\_columns.png no2.pivot(columns="location", values="value").plot()

</div>

<div class="note">

<div class="title">

Note

</div>

When the `index` parameter is not defined, the existing index (row labels) is used.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

For more information about <span class="title-ref">\~DataFrame.pivot</span>, see the user guide section on \[pivoting DataFrame objects \<reshaping.reshaping\>\](\#pivoting-dataframe-objects-\<reshaping.reshaping\>).

</div>

## Pivot table

![image](../../_static/schemas/07_pivot_table.svg)

<ul class="task-bullet">
    <li>

I want the mean concentrations for \(NO_2\) and \(PM_{2.5}\) in each of the stations in table form.

<div class="ipython">

python

  - air\_quality.pivot\_table(  
    values="value", index="location", columns="parameter", aggfunc="mean"

)

</div>

In the case of <span class="title-ref">\~DataFrame.pivot</span>, the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps), <span class="title-ref">\~DataFrame.pivot\_table</span> can be used, providing an aggregation function (e.g.Â mean) on how to combine these values.

</li>
</ul>

Pivot table is a well known concept in spreadsheet software. When interested in the row/column margins (subtotals) for each variable, set the `margins` parameter to `True`:

<div class="ipython">

python

  - air\_quality.pivot\_table(  
    values="value", index="location", columns="parameter", aggfunc="mean", margins=True,

)

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

For more information about <span class="title-ref">\~DataFrame.pivot\_table</span>, see the user guide section on \[pivot tables \<reshaping.pivot\>\](\#pivot-tables-\<reshaping.pivot\>).

</div>

<div class="note">

<div class="title">

Note

</div>

In case you are wondering, <span class="title-ref">\~DataFrame.pivot\_table</span> is indeed directly linked to <span class="title-ref">\~DataFrame.groupby</span>. The same result can be derived by grouping on both `parameter` and `location`:

    air_quality.groupby(["parameter", "location"])[["value"]].mean()

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

</div>

## Wide to long format

Starting again from the wide format table created in the previous section, we add a new index to the `DataFrame` with <span class="title-ref">\~DataFrame.reset\_index</span>.

<div class="ipython">

python

no2\_pivoted = no2.pivot(columns="location", values="value").reset\_index() no2\_pivoted.head()

</div>

![image](../../_static/schemas/07_melt.svg)

<ul class="task-bullet">
    <li>

I want to collect all air quality \(NO_2\) measurements in a single column (long format).

<div class="ipython">

python

no\_2 = no2\_pivoted.melt(id\_vars="date.utc") no\_2.head()

</div>

The <span class="title-ref">pandas.melt</span> method on a `DataFrame` converts the data table from wide format to long format. The column headers become the variable names in a newly created column.

</li>
</ul>

The solution is the short version on how to apply <span class="title-ref">pandas.melt</span>. The method will *melt* all columns NOT mentioned in `id_vars` together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name `value`.

The parameters passed to <span class="title-ref">pandas.melt</span> can be defined in more detail:

<div class="ipython">

python

  - no\_2 = no2\_pivoted.melt(  
    id\_vars="date.utc", value\_vars=\["BETR801", "FR04014", "London Westminster"\], value\_name="NO\_2", var\_name="id\_location",

) no\_2.head()

</div>

The additional parameters have the following effects:

  - `value_vars` defines which columns to *melt* together
  - `value_name` provides a custom column name for the values column instead of the default column name `value`
  - `var_name` provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default `variable`

Hence, the arguments `value_name` and `var_name` are just user-defined names for the two generated columns. The columns to melt are defined by `id_vars` and `value_vars`.

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

Conversion from wide to long format with <span class="title-ref">pandas.melt</span> is explained in the user guide section on \[reshaping by melt \<reshaping.melt\>\](\#reshaping-by-melt-\<reshaping.melt\>).

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Sorting by one or more columns is supported by `sort_values`.
  - The `pivot` function is purely restructuring of the data, `pivot_table` supports aggregations.
  - The reverse of `pivot` (long to wide format) is `melt` (wide to long format).

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full overview is available in the user guide on the pages about \[reshaping and pivoting \<reshaping\>\](\#reshaping-and-pivoting-\<reshaping\>).

</div>

---

08_combine_dataframes.md

---

<div id="10min_tut_08_combine">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">
            <div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
                <span class="badge bg-secondary">Air quality Nitrate data</span>
            </div>
            <div class="collapse" id="collapsedata">
                <div class="card-body">
                    <p class="card-text">

For this tutorial, air quality data about \(NO_2\) is used, made available by [OpenAQ](https://openaq.org) and downloaded using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package.

The `air_quality_no2_long.csv` data set provides \(NO_2\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2_long.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

  - air\_quality\_no2 = pd.read\_csv("data/air\_quality\_no2\_long.csv",  
    parse\_dates=True)

  - air\_quality\_no2 = air\_quality\_no2\[\["date.utc", "location",  
    "parameter", "value"\]\]

air\_quality\_no2.head()

</div>

</li>
<li class="list-group-item gs-data-list">
    <div data-bs-toggle="collapse" href="#collapsedata2" role="button" aria-expanded="false" aria-controls="collapsedata2">
        <span class="badge bg-secondary">Air quality Particulate matter data</span>
    </div>
    <div class="collapse" id="collapsedata2">
        <div class="card-body">
            <p class="card-text">

For this tutorial, air quality data about Particulate matter less than 2.5 micrometers is used, made available by [OpenAQ](https://openaq.org) and downloaded using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package.

The `air_quality_pm25_long.csv` data set provides \(PM_{25}\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_pm25_long.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

  - air\_quality\_pm25 = pd.read\_csv("data/air\_quality\_pm25\_long.csv",  
    parse\_dates=True)

  - air\_quality\_pm25 = air\_quality\_pm25\[\["date.utc", "location",  
    "parameter", "value"\]\]

air\_quality\_pm25.head()

</div>

</li>
</ul>
</div>

# How to combine data from multiple tables

## Concatenating objects

![image](../../_static/schemas/08_concat_row.svg)

<ul class="task-bullet">
    <li>

I want to combine the measurements of \(NO_2\) and \(PM_{25}\), two tables with a similar structure, in a single table.

<div class="ipython">

python

air\_quality = pd.concat(\[air\_quality\_pm25, air\_quality\_no2\], axis=0) air\_quality.head()

</div>

The <span class="title-ref">\~pandas.concat</span> function performs concatenation operations of multiple tables along one of the axes (row-wise or column-wise).

</li>
</ul>

By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Letâ€™s check the shape of the original and the concatenated tables to verify the operation:

<div class="ipython">

python

print('Shape of the `air_quality_pm25` table: ', air\_quality\_pm25.shape) print('Shape of the `air_quality_no2` table: ', air\_quality\_no2.shape) print('Shape of the resulting `air_quality` table: ', air\_quality.shape)

</div>

Hence, the resulting table has 3178 = 1110 + 2068 rows.

<div class="note">

<div class="title">

Note

</div>

The **axis** argument will return in a number of pandas methods that can be applied **along an axis**. A `DataFrame` has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well.

</div>

Sorting the table on the datetime information also illustrates the combination of both tables, with the `parameter` column defining the origin of the table (either `no2` from table `air_quality_no2` or `pm25` from table `air_quality_pm25`):

<div class="ipython">

python

air\_quality = air\_quality.sort\_values("date.utc") air\_quality.head()

</div>

In this specific example, the `parameter` column provided by the data ensures that each of the original tables can be identified. This is not always the case. The `concat` function provides a convenient solution with the `keys` argument, adding an additional (hierarchical) row index. For example:

<div class="ipython">

python

[air\_quality]() = pd.concat(\[air\_quality\_pm25, air\_quality\_no2\], keys=\["PM25", "NO2"\]) [air\_quality]().head()

</div>

<div class="note">

<div class="title">

Note

</div>

The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. *Hierarchical indexing* or *MultiIndex* is an advanced and powerful pandas feature to analyze higher dimensional data.

Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the function `reset_index` can be used to convert any level of an index to a column, e.g. `air_quality.reset_index(level=0)`

<div class="d-flex flex-row  gs-torefguide">
    <span class="badge badge-info">To user guide</span>

Feel free to dive into the world of multi-indexing at the user guide section on \[advanced indexing \<advanced\>\](\#advanced-indexing-\<advanced\>).

</div>

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

More options on table concatenation (row and column wise) and how `concat` can be used to define the logic (union or intersection) of the indexes on the other axes is provided at the section on \[object concatenation \<merging.concat\>\](\#object-concatenation-\<merging.concat\>).

</div>

## Join tables using a common identifier

![image](../../_static/schemas/08_merge_left.svg)

<ul class="task-bullet">
    <li>

Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

<div class="warning">

<div class="title">

Warning

</div>

The air quality measurement station coordinates are stored in a data file `air_quality_stations.csv`, downloaded using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package.

</div>

<div class="ipython">

python

stations\_coord = pd.read\_csv("data/air\_quality\_stations.csv") stations\_coord.head()

</div>

<div class="note">

<div class="title">

Note

</div>

The stations used in this example (FR04014, BETR801 and London Westminster) are just three entries enlisted in the metadata table. We only want to add the coordinates of these three to the measurements table, each on the corresponding rows of the `air_quality` table.

</div>

<div class="ipython">

python

air\_quality.head()

</div>

<div class="ipython">

python

air\_quality = pd.merge(air\_quality, stations\_coord, how="left", on="location") air\_quality.head()

</div>

Using the <span class="title-ref">\~pandas.merge</span> function, for each of the rows in the `air_quality` table, the corresponding coordinates are added from the `air_quality_stations_coord` table. Both tables have the column `location` in common which is used as a key to combine the information. By choosing the `left` join, only the locations available in the `air_quality` (left) table, i.e.Â FR04014, BETR801 and London Westminster, end up in the resulting table. The `merge` function supports multiple join options similar to database-style operations.

</li>
</ul>

<ul class="task-bullet">
    <li>

Add the parameters' full description and name, provided by the parameters metadata table, to the measurements table.

<div class="warning">

<div class="title">

Warning

</div>

The air quality parameters metadata are stored in a data file `air_quality_parameters.csv`, downloaded using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package.

</div>

<div class="ipython">

python

air\_quality\_parameters = pd.read\_csv("data/air\_quality\_parameters.csv") air\_quality\_parameters.head()

</div>

<div class="ipython">

python

  - air\_quality = pd.merge(air\_quality, air\_quality\_parameters,  
    how='left', left\_on='parameter', right\_on='id')

air\_quality.head()

</div>

Compared to the previous example, there is no common column name. However, the `parameter` column in the `air_quality` table and the `id` column in the `air_quality_parameters` table both provide the measured variable in a common format. The `left_on` and `right_on` arguments are used here (instead of just `on`) to make the link between the two tables.

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

pandas also supports inner, outer, and right joins. More information on join/merge of tables is provided in the user guide section on \[database style merging of tables \<merging.join\>\](\#database-style-merging-of-tables-\<merging.join\>). Or have a look at the \[comparison with SQL\<compare\_with\_sql.join\>\](\#comparison-with-sql\<compare\_with\_sql.join\>) page.

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Multiple tables can be concatenated column-wise or row-wise using the `concat` function.
  - For database-like merging/joining of tables, use the `merge` function.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

See the user guide for a full description of the various \[facilities to combine data tables \<merging\>\](\#facilities-to-combine-data-tables-\<merging\>).

</div>

---

09_timeseries.md

---

<div id="10min_tut_09_timeseries">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd import matplotlib.pyplot as plt

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">
            <div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
                <span class="badge bg-secondary">Air quality data</span>
            </div>
            <div class="collapse" id="collapsedata">
                <div class="card-body">
                    <p class="card-text">

For this tutorial, air quality data about \(NO_2\) and Particulate matter less than 2.5 micrometers is used, made available by [OpenAQ](https://openaq.org) and downloaded using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package. The `air_quality_no2_long.csv"` data set provides \(NO_2\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2_long.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

air\_quality = pd.read\_csv("data/air\_quality\_no2\_long.csv") air\_quality = air\_quality.rename(columns={"date.utc": "datetime"}) air\_quality.head()

</div>

<div class="ipython">

python

air\_quality.city.unique()

</div>

</li>
</ul>
</div>

# How to handle time series data with ease

## Using pandas datetime properties

<ul class="task-bullet">
    <li>

I want to work with the dates in the column `datetime` as datetime objects instead of plain text

<div class="ipython">

python

air\_quality\["datetime"\] = pd.to\_datetime(air\_quality\["datetime"\]) air\_quality\["datetime"\]

</div>

Initially, the values in `datetime` are character strings and do not provide any datetime operations (e.g.Â extract the year, day of the week, â€¦). By applying the `to_datetime` function, pandas interprets the strings and convert these to datetime (i.e. `datetime64[ns, UTC]`) objects. In pandas we call these datetime objects that are similar to `datetime.datetime` from the standard library as <span class="title-ref">pandas.Timestamp</span>.

</li>
</ul>

<div class="note">

<div class="title">

Note

</div>

As many data sets do contain datetime information in one of the columns, pandas input function like <span class="title-ref">pandas.read\_csv</span> and <span class="title-ref">pandas.read\_json</span> can do the transformation to dates when reading the data using the `parse_dates` parameter with a list of the columns to read as Timestamp:

    pd.read_csv("../data/air_quality_no2_long.csv", parse_dates=["datetime"])

</div>

Why are these <span class="title-ref">pandas.Timestamp</span> objects useful? Letâ€™s illustrate the added value with some example cases.

> What is the start and end date of the time series data set we are working with?

<div class="ipython">

python

air\_quality\["datetime"\].min(), air\_quality\["datetime"\].max()

</div>

Using <span class="title-ref">pandas.Timestamp</span> for datetimes enables us to calculate with date information and make them comparable. Hence, we can use this to get the length of our time series:

<div class="ipython">

python

air\_quality\["datetime"\].max() - air\_quality\["datetime"\].min()

</div>

The result is a <span class="title-ref">pandas.Timedelta</span> object, similar to `datetime.timedelta` from the standard Python library which defines a time duration.

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

The various time concepts supported by pandas are explained in the user guide section on \[time related concepts \<timeseries.overview\>\](\#time-related-concepts-\<timeseries.overview\>).

</div>

<ul class="task-bullet">
    <li>

I want to add a new column to the `DataFrame` containing only the month of the measurement

<div class="ipython">

python

air\_quality\["month"\] = air\_quality\["datetime"\].dt.month air\_quality.head()

</div>

By using `Timestamp` objects for dates, a lot of time-related properties are provided by pandas. For example the `month`, but also `year`, `quarter`,â€¦ All of these properties are accessible by the `dt` accessor.

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

An overview of the existing date properties is given in the \[time and date components overview table \<timeseries.components\>\](\#time-and-date-components-overview-table-\<timeseries.components\>). More details about the `dt` accessor to return datetime like properties are explained in a dedicated section on the \[dt accessor \<basics.dt\_accessors\>\](\#dt-accessor-\<basics.dt\_accessors\>).

</div>

<ul class="task-bullet">
    <li>

What is the average \(NO_2\) concentration for each day of the week for each of the measurement locations?

<div class="ipython">

python

  - air\_quality.groupby(  
    \[air\_quality\["datetime"\].dt.weekday, "location"\])\["value"\].mean()

</div>

Remember the split-apply-combine pattern provided by `groupby` from the \[tutorial on statistics calculation \<10min\_tut\_06\_stats\>\](\#tutorial-on-statistics-calculation-\<10min\_tut\_06\_stats\>)? Here, we want to calculate a given statistic (e.g.Â mean \(NO_2\)) **for each weekday** and **for each measurement location**. To group on weekdays, we use the datetime property `weekday` (with Monday=0 and Sunday=6) of pandas `Timestamp`, which is also accessible by the `dt` accessor. The grouping on both locations and weekdays can be done to split the calculation of the mean on each of these combinations.

<div class="danger">

<div class="title">

Danger

</div>

As we are working with a very short time series in these examples, the analysis does not provide a long-term representative result\!

</div>

</li>
</ul>

<ul class="task-bullet">
    <li>

Plot the typical \(NO_2\) pattern during the day of our time series of all stations together. In other words, what is the average value for each hour of the day?

<div class="ipython">

python

fig, axs = plt.subplots(figsize=(12, 4)) air\_quality.groupby(air\_quality\["datetime"\].dt.hour)\["value"\].mean().plot( kind='bar', rot=0, ax=axs ) plt.xlabel("Hour of the day"); \# custom x label using Matplotlib @savefig 09\_bar\_chart.png plt.ylabel("$NO\_2 (Âµg/m^3)$");

</div>

Similar to the previous case, we want to calculate a given statistic (e.g.Â mean \(NO_2\)) **for each hour of the day** and we can use the split-apply-combine approach again. For this case, we use the datetime property `hour` of pandas `Timestamp`, which is also accessible by the `dt` accessor.

</li>
</ul>

## Datetime as index

In the \[tutorial on reshaping \<10min\_tut\_07\_reshape\>\](\#tutorial-on-reshaping-\<10min\_tut\_07\_reshape\>), <span class="title-ref">\~pandas.pivot</span> was introduced to reshape the data table with each of the measurements locations as a separate column:

<div class="ipython">

python

no\_2 = air\_quality.pivot(index="datetime", columns="location", values="value") no\_2.head()

</div>

<div class="note">

<div class="title">

Note

</div>

By pivoting the data, the datetime information became the index of the table. In general, setting a column as an index can be achieved by the `set_index` function.

</div>

Working with a datetime index (i.e. `DatetimeIndex`) provides powerful functionalities. For example, we do not need the `dt` accessor to get the time series properties, but have these properties available on the index directly:

<div class="ipython">

python

no\_2.index.year, no\_2.index.weekday

</div>

Some other advantages are the convenient subsetting of time period or the adapted time scale on plots. Letâ€™s apply this on our data.

<ul class="task-bullet">
    <li>

Create a plot of the \(NO_2\) values in the different stations from May 20th till the end of May 21st.

<div class="ipython" data-okwarning="">

python

@savefig 09\_time\_section.png no\_2\["2019-05-20":"2019-05-21"\].plot();

</div>

By providing a **string that parses to a datetime**, a specific subset of the data can be selected on a `DatetimeIndex`.

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

More information on the `DatetimeIndex` and the slicing by using strings is provided in the section on \[time series indexing \<timeseries.datetimeindex\>\](\#time-series-indexing-\<timeseries.datetimeindex\>).

</div>

## Resample a time series to another frequency

<ul class="task-bullet">
    <li>

Aggregate the current hourly time series values to the monthly maximum value in each of the stations.

<div class="ipython">

python

monthly\_max = no\_2.resample("MS").max() monthly\_max

</div>

A very powerful method on time series data with a datetime index, is the ability to <span class="title-ref">\~Series.resample</span> time series to another frequency (e.g., converting secondly data into 5-minutely data).

</li>
</ul>

The <span class="title-ref">\~Series.resample</span> method is similar to a groupby operation:

  - it provides a time-based grouping, by using a string (e.g. `M`, `5H`, â€¦) that defines the target frequency
  - it requires an aggregation function such as `mean`, `max`,â€¦

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

An overview of the aliases used to define time series frequencies is given in the \[offset aliases overview table \<timeseries.offset\_aliases\>\](\#offset-aliases-overview-table-\<timeseries.offset\_aliases\>).

</div>

When defined, the frequency of the time series is provided by the `freq` attribute:

<div class="ipython">

python

monthly\_max.index.freq

</div>

<ul class="task-bullet">
    <li>

Make a plot of the daily mean \(NO_2\) value in each of the stations.

<div class="ipython" data-okwarning="">

python

@savefig 09\_resample\_mean.png no\_2.resample("D").mean().plot(style="-o", figsize=(10, 5));

</div>

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

More details on the power of time series `resampling` is provided in the user guide section on \[resampling \<timeseries.resampling\>\](\#resampling-\<timeseries.resampling\>).

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - Valid date strings can be converted to datetime objects using `to_datetime` function or as part of read functions.
  - Datetime objects in pandas support calculations, logical operations and convenient date-related properties using the `dt` accessor.
  - A `DatetimeIndex` contains these date-related properties and supports convenient slicing.
  - `Resample` is a powerful method to change the frequency of a time series.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full overview on time series is given on the pages on \[time series and date functionality \<timeseries\>\](\#time-series-and-date-functionality-\<timeseries\>).

</div>

---

10_text_data.md

---

<div id="10min_tut_10_text">

{{ header }}

</div>

<div class="ipython">

python

import pandas as pd

</div>

<div class="card gs-data">
    <div class="card-header gs-data-header">
        <div class="gs-data-title">
            Data used for this tutorial:
        </div>
    </div>
    <ul class="list-group list-group-flush">
        <li class="list-group-item gs-data-list">

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Titanic data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:

  - PassengerId: Id of every passenger.
  - Survived: Indication whether passenger survived. `0` for yes and `1` for no.
  - Pclass: One out of the 3 ticket classes: Class `1`, Class `2` and Class `3`.
  - Name: Name of passenger.
  - Sex: Gender of passenger.
  - Age: Age of passenger in years.
  - SibSp: Number of siblings or spouses aboard.
  - Parch: Number of parents or children aboard.
  - Ticket: Ticket number of passenger.
  - Fare: Indicating the fare.
  - Cabin: Cabin number of passenger.
  - Embarked: Port of embarkation.

</p>
<a href="https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

<div class="ipython">

python

titanic = pd.read\_csv("data/titanic.csv") titanic.head()

</div>

</li>
</ul>
</div>

# How to manipulate textual data

<ul class="task-bullet">
    <li>

Make all name characters lowercase.

<div class="ipython">

python

titanic\["Name"\].str.lower()

</div>

To make each of the strings in the `Name` column lowercase, select the `Name` column (see the \[tutorial on selection of data \<10min\_tut\_03\_subset\>\](\#tutorial-on-selection-of-data-\<10min\_tut\_03\_subset\>)), add the `str` accessor and apply the `lower` method. As such, each of the strings is converted element-wise.

</li>
</ul>

Similar to datetime objects in the \[time series tutorial \<10min\_tut\_09\_timeseries\>\](\#time-series-tutorial-\<10min\_tut\_09\_timeseries\>) having a `dt` accessor, a number of specialized string methods are available when using the `str` accessor. These methods have in general matching names with the equivalent built-in string methods for single elements, but are applied element-wise (remember \[element-wise calculations \<10min\_tut\_05\_columns\>\](\#element-wise-calculations-\<10min\_tut\_05\_columns\>)?) on each of the values of the columns.

<ul class="task-bullet">
    <li>

Create a new column `Surname` that contains the surname of the passengers by extracting the part before the comma.

<div class="ipython">

python

titanic\["Name"\].str.split(",")

</div>

Using the <span class="title-ref">Series.str.split</span> method, each of the values is returned as a list of 2 elements. The first element is the part before the comma and the second element is the part after the comma.

<div class="ipython">

python

titanic\["Surname"\] = titanic\["Name"\].str.split(",").str.get(0) titanic\["Surname"\]

</div>

As we are only interested in the first part representing the surname (element 0), we can again use the `str` accessor and apply <span class="title-ref">Series.str.get</span> to extract the relevant part. Indeed, these string functions can be concatenated to combine multiple functions at once\!

</li>
</ul>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

More information on extracting parts of strings is available in the user guide section on \[splitting and replacing strings \<text.split\>\](\#splitting-and-replacing-strings-\<text.split\>).

</div>

<ul class="task-bullet">
    <li>

Extract the passenger data about the countesses on board of the Titanic.

<div class="ipython">

python

titanic\["Name"\].str.contains("Countess")

</div>

<div class="ipython">

python

titanic\[titanic\["Name"\].str.contains("Countess")\]

</div>

(*Interested in her story? See* [Wikipedia](https://en.wikipedia.org/wiki/No%C3%ABl_Leslie,_Countess_of_Rothes)*\!*)

The string method <span class="title-ref">Series.str.contains</span> checks for each of the values in the column `Name` if the string contains the word `Countess` and returns for each of the values `True` (`Countess` is part of the name) or `False` (`Countess` is not part of the name). This output can be used to subselect the data using conditional (boolean) indexing introduced in the \[subsetting of data tutorial \<10min\_tut\_03\_subset\>\](\#subsetting-of-data-tutorial-\<10min\_tut\_03\_subset\>). As there was only one countess on the Titanic, we get one row as a result.

</li>
</ul>

<div class="note">

<div class="title">

Note

</div>

More powerful extractions on strings are supported, as the <span class="title-ref">Series.str.contains</span> and <span class="title-ref">Series.str.extract</span> methods accept [regular expressions](https://docs.python.org/3/library/re.html), but are out of the scope of this tutorial.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

More information on extracting parts of strings is available in the user guide section on \[string matching and extracting \<text.extract\>\](\#string-matching-and-extracting-\<text.extract\>).

</div>

<ul class="task-bullet">
    <li>

Which passenger of the Titanic has the longest name?

<div class="ipython">

python

titanic\["Name"\].str.len()

</div>

To get the longest name we first have to get the lengths of each of the names in the `Name` column. By using pandas string methods, the <span class="title-ref">Series.str.len</span> function is applied to each of the names individually (element-wise).

<div class="ipython">

python

titanic\["Name"\].str.len().idxmax()

</div>

Next, we need to get the corresponding location, preferably the index label, in the table for which the name length is the largest. The <span class="title-ref">\~Series.idxmax</span> method does exactly that. It is not a string method and is applied to integers, so no `str` is used.

<div class="ipython">

python

titanic.loc\[titanic\["Name"\].str.len().idxmax(), "Name"\]

</div>

Based on the index name of the row (`307`) and the column (`Name`), we can do a selection using the `loc` operator, introduced in the \[tutorial on subsetting \<10min\_tut\_03\_subset\>\](\#tutorial-on-subsetting-\<10min\_tut\_03\_subset\>).

</li>
</ul>

<ul class="task-bullet">
    <li>

In the "Sex" column, replace values of "male" by "M" and values of "female" by "F".

<div class="ipython">

python

titanic\["Sex\_short"\] = titanic\["Sex"\].replace({"male": "M", "female": "F"}) titanic\["Sex\_short"\]

</div>

Whereas <span class="title-ref">\~Series.replace</span> is not a string method, it provides a convenient way to use mappings or vocabularies to translate certain values. It requires a `dictionary` to define the mapping `{from: to}`.

</li>
</ul>

<div class="warning">

<div class="title">

Warning

</div>

There is also a <span class="title-ref">\~Series.str.replace</span> method available to replace a specific set of characters. However, when having a mapping of multiple values, this would become:

    titanic["Sex_short"] = titanic["Sex"].str.replace("female", "F")
    titanic["Sex_short"] = titanic["Sex_short"].str.replace("male", "M")

This would become cumbersome and easily lead to mistakes. Just think (or try out yourself) what would happen if those two statements are applied in the opposite orderâ€¦

</div>

<div class="shadow gs-callout gs-callout-remember">
    <h4>REMEMBER</h4>

  - String methods are available using the `str` accessor.
  - String methods work element-wise and can be used for conditional indexing.
  - The `replace` method is a convenient method to convert values according to a given dictionary.

</div>

<div class="d-flex flex-row gs-torefguide">
    <span class="badge badge-info">To user guide</span>

A full overview is provided in the user guide pages on \[working with text data \<text\>\](\#working-with-text-data-\<text\>).

</div>

---

air_quality_no2.md

---

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Air quality data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

For this tutorial, air quality data about \(NO_2\) is used, made available by [OpenAQ](https://openaq.org) and using the [py-openaq](http://dhhagan.github.io/py-openaq/index.html) package. The `air_quality_no2.csv` data set provides \(NO_2\) values for the measurement stations *FR04014*, *BETR801* and *London Westminster* in respectively Paris, Antwerp and London.

</p>
<a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

---

titanic.md

---

<div data-bs-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
    <span class="badge bg-secondary">Titanic data</span>
</div>
<div class="collapse" id="collapsedata">
    <div class="card-body">
        <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:

  - PassengerId: Id of every passenger.
  - Survived: Indication whether passenger survived. `0` for yes and `1` for no.
  - Pclass: One out of the 3 ticket classes: Class `1`, Class `2` and Class `3`.
  - Name: Name of passenger.
  - Sex: Gender of passenger.
  - Age: Age of passenger in years.
  - SibSp: Number of siblings or spouses aboard.
  - Parch: Number of parents or children aboard.
  - Ticket: Ticket number of passenger.
  - Fare: Indicating the fare.
  - Cabin: Cabin number of passenger.
  - Embarked: Port of embarkation.

</p>
<a href="https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
</div>
</div>

---

index.md

---

{{ header }}

# Getting started tutorials

<div class="toctree" data-maxdepth="1">

01\_table\_oriented 02\_read\_write 03\_subset\_data 04\_plotting 05\_add\_columns 06\_calculate\_statistics 07\_reshape\_table\_layout 08\_combine\_dataframes 09\_timeseries 10\_text\_data

</div>

---

overview.md

---

<div id="overview">

{{ header }}

</div>

# Package overview

pandas is a [Python](https://www.python.org) package that provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive. It aims to be the fundamental high-level building block for Python's practical, **real-world** data analysis. Additionally, it seeks to become **the most powerful and flexible open source data analysis/manipulation tool available in any language**. It is already well on its way toward this goal.

pandas is well suited for many different kinds of data:

>   - Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet
>   - Ordered and unordered (not necessarily fixed-frequency) time series data.
>   - Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
>   - Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure

The two primary data structures of pandas, <span class="title-ref">Series</span> (1-dimensional) and <span class="title-ref">DataFrame</span> (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. For R users, <span class="title-ref">DataFrame</span> provides everything that R's `data.frame` provides and much more. pandas is built on top of [NumPy](https://numpy.org) and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:

>   - Easy handling of **missing data** (represented as NaN) in floating point as well as non-floating point data
>   - Size mutability: columns can be **inserted and deleted** from DataFrame and higher dimensional objects
>   - Automatic and explicit **data alignment**: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let `Series`, `DataFrame`, etc. automatically align the data for you in computations
>   - Powerful, flexible **group by** functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
>   - Make it **easy to convert** ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
>   - Intelligent label-based **slicing**, **fancy indexing**, and **subsetting** of large data sets
>   - Intuitive **merging** and **joining** data sets
>   - Flexible **reshaping** and pivoting of data sets
>   - **Hierarchical** labeling of axes (possible to have multiple labels per tick)
>   - Robust IO tools for loading data from **flat files** (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast **HDF5 format**
>   - **Time series**-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.

Some other notes

>   - pandas is **fast**. Many of the low-level algorithmic bits have been extensively tweaked in [Cython](https://cython.org) code. However, as with anything else generalization usually sacrifices performance. So if you focus on one feature for your application you may be able to create a faster specialized tool.
>   - pandas is a dependency of [statsmodels](https://www.statsmodels.org/), making it an important part of the statistical computing ecosystem in Python.
>   - pandas has been used extensively in production in financial applications.

## Data structures

| Dimensions | Name      | Description                                                                                      |
| ---------- | --------- | ------------------------------------------------------------------------------------------------ |
| 1          | Series    | 1D labeled homogeneously-typed array                                                             |
| 2          | DataFrame | General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column |

### Why more than one data structure?

The best way to think about the pandas data structures is as flexible containers for lower dimensional data. For example, DataFrame is a container for Series, and Series is a container for scalars. We would like to be able to insert and remove objects from these containers in a dictionary-like fashion.

Also, we would like sensible default behaviors for the common API functions which take into account the typical orientation of time series and cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional data, a burden is placed on the user to consider the orientation of the data set when writing functions; axes are considered more or less equivalent (except when C- or Fortran-contiguousness matters for performance). In pandas, the axes are intended to lend more semantic meaning to the data; i.e., for a particular data set, there is likely to be a "right" way to orient the data. The goal, then, is to reduce the amount of mental effort required to code up data transformations in downstream functions.

For example, with tabular data (DataFrame) it is more semantically helpful to think of the **index** (the rows) and the **columns** rather than axis 0 and axis 1. Iterating through the columns of the DataFrame thus results in more readable code:

    for col in df.columns:
        series = df[col]
        # do something with series

## Mutability and copying of data

All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects and leave the input data untouched. In general we like to **favor immutability** where sensible.

## Getting support

The first stop for pandas issues and ideas is the [GitHub Issue Tracker](https://github.com/pandas-dev/pandas/issues). If you have a general question, pandas community experts can answer through [Stack Overflow](https://stackoverflow.com/questions/tagged/pandas).

## Community

pandas is actively supported today by a community of like-minded individuals around the world who contribute their valuable time and energy to help make open source pandas possible. Thanks to [all of our contributors](https://github.com/pandas-dev/pandas/graphs/contributors).

If you're interested in contributing, please visit the \[contributing guide \<contributing\>\](\#contributing-guide-\<contributing\>).

pandas is a [NumFOCUS](https://numfocus.org/sponsored-projects) sponsored project. This will help ensure the success of the development of pandas as a world-class open-source project and makes it possible to [donate](https://pandas.pydata.org/donate.html) to the project.

## Project governance

The governance process that pandas project has used informally since its inception in 2008 is formalized in [Project Governance documents](https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/governance.md). The documents clarify how decisions are made and how the various elements of our community interact, including the relationship between open source collaborative development and work that may be funded by for-profit or non-profit entities.

Wes McKinney is the Benevolent Dictator for Life (BDFL).

## Development team

The list of the Core Team members and more detailed information can be found on the [pandas website](https://pandas.pydata.org/about/team.html).

## Institutional partners

The information about current institutional partners can be found on [pandas website page](https://pandas.pydata.org/about/sponsors.html).

## License

<div class="literalinclude">

../../../LICENSE

</div>

---

tutorials.md

---

<div id="communitytutorials">

{{ header }}

</div>

# Community tutorials

This is a guide to many pandas tutorials by the community, geared mainly for new users.

## pandas cookbook by Julia Evans

The goal of this 2015 cookbook (by [Julia Evans](https://jvns.ca)) is to give you some concrete examples for getting started with pandas. These are examples with real-world data, and all the bugs and weirdness that entails. For the table of contents, see the [pandas-cookbook GitHub repository](https://github.com/jvns/pandas-cookbook).

## pandas workshop by Stefanie Molin

An introductory workshop by [Stefanie Molin](https://github.com/stefmolin) designed to quickly get you up to speed with pandas using real-world datasets. It covers getting started with pandas, data wrangling, and data visualization (with some exposure to matplotlib and seaborn). The [pandas-workshop GitHub repository](https://github.com/stefmolin/pandas-workshop) features detailed environment setup instructions (including a Binder environment), slides and notebooks for following along, and exercises to practice the concepts. There is also a lab with new exercises on a dataset not covered in the workshop for additional practice.

## Learn pandas by Hernan Rojas

A set of lesson for new pandas users: <https://bitbucket.org/hrojas/learn-pandas>

## Practical data analysis with Python

This [guide](https://wavedatalab.github.io/datawithpython) is an introduction to the data analysis process using the Python data ecosystem and an interesting open dataset. There are four sections covering selected topics as [munging data](https://wavedatalab.github.io/datawithpython/munge.html), [aggregating data](https://wavedatalab.github.io/datawithpython/aggregate.html), [visualizing data](https://wavedatalab.github.io/datawithpython/visualize.html) and [time series](https://wavedatalab.github.io/datawithpython/timeseries.html).

## Exercises for new users

Practice your skills with real data sets and exercises. For more resources, please visit the main [repository](https://github.com/guipsamora/pandas_exercises).

## Modern pandas

Tutorial series written in 2016 by [Tom Augspurger](https://github.com/TomAugspurger). The source may be found in the GitHub repository [TomAugspurger/effective-pandas](https://github.com/TomAugspurger/effective-pandas).

  - [Modern Pandas](https://tomaugspurger.github.io/modern-1-intro.html)
  - [Method Chaining](https://tomaugspurger.github.io/method-chaining.html)
  - [Indexes](https://tomaugspurger.github.io/modern-3-indexes.html)
  - [Performance](https://tomaugspurger.github.io/modern-4-performance.html)
  - [Tidy Data](https://tomaugspurger.github.io/modern-5-tidy.html)
  - [Visualization](https://tomaugspurger.github.io/modern-6-visualization.html)
  - [Timeseries](https://tomaugspurger.github.io/modern-7-timeseries.html)

## Excel charts with pandas, vincent and xlsxwriter

  - [Using Pandas and XlsxWriter to create Excel charts](https://pandas-xlsxwriter-charts.readthedocs.io/)

## Joyful pandas

A tutorial written in Chinese by Yuanhao Geng. It covers the basic operations for NumPy and pandas, 4 main data manipulation methods (including indexing, groupby, reshaping and concatenation) and 4 main data types (including missing data, string data, categorical data and time series data). At the end of each chapter, corresponding exercises are posted. All the datasets and related materials can be found in the GitHub repository [datawhalechina/joyful-pandas](https://github.com/datawhalechina/joyful-pandas).

## Video tutorials

  - [Pandas From The Ground Up](https://www.youtube.com/watch?v=5JnMutdy6Fw)
    
    (2015) (2:24) [GitHub repo](https://github.com/brandon-rhodes/pycon-pandas-tutorial)

  - [Introduction Into Pandas](https://www.youtube.com/watch?v=-NR-ynQg0YM)
    
    (2016) (1:28) [GitHub repo](https://github.com/chendaniely/2016-pydata-carolinas-pandas)

  - [Pandas: .head() to .tail()](https://www.youtube.com/watch?v=7vuO9QXDN50)
    
    (2016) (1:26) [GitHub repo](https://github.com/TomAugspurger/pydata-chi-h2t)

  - [Data analysis in Python with pandas](https://www.youtube.com/playlist?list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y) (2016-2018) [GitHub repo](https://github.com/justmarkham/pandas-videos) and [Jupyter Notebook](https://nbviewer.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb)

  - [Best practices with pandas](https://www.youtube.com/playlist?list=PL5-da3qGB5IBITZj_dYSFqnd_15JgqwA6) (2018) [GitHub repo](https://github.com/justmarkham/pycon-2018-tutorial) and [Jupyter Notebook](https://nbviewer.org/github/justmarkham/pycon-2018-tutorial/blob/master/tutorial.ipynb)

## Various tutorials

  - [Wes McKinney's (pandas BDFL) blog](https://wesmckinney.com/archives.html)
  - [Statistical analysis made easy in Python with SciPy and pandas DataFrames, by Randal Olson](http://www.randalolson.com/2012/08/06/statistical-analysis-made-easy-in-python/)
  - [Statistical Data Analysis in Python, tutorial videos, by Christopher Fonnesbeck from SciPy 2013](https://conference.scipy.org/scipy2013/tutorial_detail.php?id=109)
  - [Financial analysis in Python, by Thomas Wiecki](https://nbviewer.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynb)
  - [Intro to pandas data structures, by Greg Reda](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/)
  - [Pandas DataFrames Tutorial, by Karlijn Willems](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)
  - [A concise tutorial with real life examples](https://tutswiki.com/pandas-cookbook/chapter1/)
  - [430+ Searchable Pandas recipes by Isshin Inada](https://skytowner.com/explore/pandas_recipes_reference)

---

arrays.md

---

{{ header }}

# pandas arrays, scalars, and data types

## Objects

<div class="currentmodule">

pandas

</div>

For most data types, pandas uses NumPy arrays as the concrete objects contained with a <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span>.

For some data types, pandas extends NumPy's type system. String aliases for these types can be found at \[basics.dtypes\](\#basics.dtypes).

<table>
<thead>
<tr class="header">
<th>Kind of Data</th>
<th>pandas Data Type</th>
<th>Scalar</th>
<th>Array</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TZ-aware datetime</td>
<td><span class="title-ref">DatetimeTZDtype</span> `Timest</td>
<td>amp` [api.arrays.da</td>
<td>tetime](#api.arrays.datetime)</td>
</tr>
<tr class="even">
<td>Timedeltas</td>
<td>(none)</td>
<td><span class="title-ref">Timedelta</span> [api.ar</td>
<td>rays.timedelta](#api.arrays.timedelta)</td>
</tr>
<tr class="odd">
<td>Period (time spans)</td>
<td><span class="title-ref">PeriodDtype</span> `Period</td>
<td>` [api.arrays.pe</td>
<td>riod](#api.arrays.period)</td>
</tr>
<tr class="even">
<td>Intervals</td>
<td><span class="title-ref">IntervalDtype</span> `Interv</td>
<td>al` [api.arrays.in</td>
<td>terval](#api.arrays.interval)</td>
</tr>
<tr class="odd">
<td>Nullable Integer</td>
<td><span class="title-ref">Int64Dtype</span>, ... (none)</td>
<td><blockquote>
<p>[api.ar</p>
</blockquote></td>
<td>rays.integer_na](#api.arrays.integer_na)</td>
</tr>
<tr class="even">
<td>Nullable Float</td>
<td><span class="title-ref">Float64Dtype</span>, ... (none)</td>
<td><blockquote>
<p>[api.ar</p>
</blockquote></td>
<td>rays.float_na](#api.arrays.float_na)</td>
</tr>
<tr class="odd">
<td>Categorical</td>
<td><span class="title-ref">CategoricalDtype</span> (none)</td>
<td><blockquote>
<p>[api.ar</p>
</blockquote></td>
<td>rays.categorical](#api.arrays.categorical)</td>
</tr>
<tr class="even">
<td>Sparse</td>
<td><span class="title-ref">SparseDtype</span> (none)</td>
<td><blockquote>
<p>[api.ar</p>
</blockquote></td>
<td>rays.sparse](#api.arrays.sparse)</td>
</tr>
<tr class="odd">
<td>Strings</td>
<td><span class="title-ref">StringDtype</span> <span class="title-ref">str</span></td>
<td><blockquote>
<p>[api.arrays.st</p>
</blockquote></td>
<td>ring](#api.arrays.string)</td>
</tr>
<tr class="even">
<td>Nullable Boolean</td>
<td><span class="title-ref">BooleanDtype</span> <span class="title-ref">bool</span></td>
<td><blockquote>
<p>[api.arrays.bo</p>
</blockquote></td>
<td>ol](#api.arrays.bool)</td>
</tr>
<tr class="odd">
<td>PyArrow</td>
<td><span class="title-ref">ArrowDtype</span> Python</td>
<td>Scalars or <span class="title-ref">NA</span> [api.arrays.ar</td>
<td>row](#api.arrays.arrow)</td>
</tr>
</tbody>
</table>

pandas and third-party libraries can extend NumPy's type system (see \[extending.extension-types\](\#extending.extension-types)). The top-level <span class="title-ref">array</span> method can be used to create a new array, which may be stored in a <span class="title-ref">Series</span>, <span class="title-ref">Index</span>, or as a column in a <span class="title-ref">DataFrame</span>.

<div class="autosummary" data-toctree="api/">

array

</div>

### PyArrow

\> **Warning** \> This feature is experimental, and the API can change in a future release without warning.

The <span class="title-ref">arrays.ArrowExtensionArray</span> is backed by a :external+pyarrow:py\`pyarrow.ChunkedArray\` with a :external+pyarrow:py\`pyarrow.DataType\` instead of a NumPy array and data type. The `.dtype` of a <span class="title-ref">arrays.ArrowExtensionArray</span> is an <span class="title-ref">ArrowDtype</span>.

[Pyarrow](https://arrow.apache.org/docs/python/index.html) provides similar array and [data type](https://arrow.apache.org/docs/python/api/datatypes.html) support as NumPy including first-class nullability support for all data types, immutability and more.

The table below shows the equivalent pyarrow-backed (`pa`), pandas extension, and numpy (`np`) types that are recognized by pandas. Pyarrow-backed types below need to be passed into <span class="title-ref">ArrowDtype</span> to be recognized by pandas e.g. `pd.ArrowDtype(pa.bool_())`

<table>
<thead>
<tr class="header">
<th>PyArrow type</th>
<th>pandas extension type</th>
<th>NumPy type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p>:external+pyarrow:py`<a href="">pyarrow.bool</a><span class="title-ref"> `Boole :external+pyarrow:py`pyarrow.int8</span> <span class="title-ref">Int8D :external+pyarrow:py`pyarrow.int16</span> <span class="title-ref">Int16 :external+pyarrow:py`pyarrow.int32</span> <span class="title-ref">Int32 :external+pyarrow:py`pyarrow.int64</span> <span class="title-ref">Int64 :external+pyarrow:py`pyarrow.uint8</span> <span class="title-ref">UInt8 :external+pyarrow:py`pyarrow.uint16</span> <span class="title-ref">UInt1 :external+pyarrow:py`pyarrow.uint32</span> <span class="title-ref">UInt3 :external+pyarrow:py`pyarrow.uint64</span> `UInt6</p></td>
<td><p>anDtype` <code>np.bool_</code> type` <code>np.int8</code> Dtype` <code>np.int16</code> Dtype` <code>np.int32</code> Dtype` <code>np.int64</code> Dtype` <code>np.uint8</code> 6Dtype` <code>np.uint16</code> 2Dtype` <code>np.uint32</code> 4Dtype` <code>np.uint64</code></p></td>
<td></td>
</tr>
<tr class="even">
<td>:external+pyarrow:py`pyarrow.float32` `Float</td>
<td>32Dtype` <span class="title-ref">`np.float32</span></td>
<td>`</td>
</tr>
<tr class="odd">
<td><p>:external+pyarrow:py`pyarrow.float64` <span class="title-ref">Float :external+pyarrow:py`pyarrow.time32</span> (none) :external+pyarrow:py`pyarrow.time64` (none)</p></td>
<td><dl>
<dt>64Dtype` <span class="title-ref">`np.float64</span></dt>
<dd><p>(none) (none)</p>
</dd>
</dl></td>
<td><p>`</p></td>
</tr>
<tr class="even">
<td><p>:external+pyarrow:py`pyarrow.timestamp` <span class="title-ref">Datet :external+pyarrow:py`pyarrow.date32</span> (none) :external+pyarrow:py`pyarrow.date64` (none)</p></td>
<td><dl>
<dt>imeTZDtype` ``np.datetime</dt>
<dd><p>(none) (none)</p>
</dd>
</dl></td>
<td><p>64``</p></td>
</tr>
<tr class="odd">
<td><p>:external+pyarrow:py`pyarrow.duration` (none) :external+pyarrow:py`pyarrow.binary` (none) :external+pyarrow:py`pyarrow.string` <span class="title-ref">Strin :external+pyarrow:py`pyarrow.decimal128</span> (none) :external+pyarrow:py`<a href="">pyarrow.list</a><span class="title-ref"> (none) :external+pyarrow:py`pyarrow.map_</span> (none) :external+pyarrow:py`pyarrow.dictionary` `Categ</p></td>
<td><blockquote>
<p>``np.t (none)</p>
</blockquote>
<dl>
<dt>gDtype` <code>np.str_</code></dt>
<dd><p>(none) (none) (none)</p>
</dd>
</dl>
<p>oricalDtype` (none)</p></td>
<td><p>imedelta64``</p></td>
</tr>
</tbody>
</table>

\> **Note** \> Pyarrow-backed string support is provided by both `pd.StringDtype("pyarrow")` and `pd.ArrowDtype(pa.string())`. `pd.StringDtype("pyarrow")` is described below in the \[string section \<api.arrays.string\>\](\#string-section-\<api.arrays.string\>) and will be returned if the string alias `"string[pyarrow]"` is specified. `pd.ArrowDtype(pa.string())` generally has better interoperability with <span class="title-ref">ArrowDtype</span> of different types.

While individual values in an <span class="title-ref">arrays.ArrowExtensionArray</span> are stored as a PyArrow objects, scalars are **returned** as Python scalars corresponding to the data type, e.g. a PyArrow int64 will be returned as Python int, or <span class="title-ref">NA</span> for missing values.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.ArrowExtensionArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

ArrowDtype

</div>

For more information, please see the \[PyArrow user guide \<pyarrow\>\](\#pyarrow-user-guide-\<pyarrow\>)

### Datetimes

NumPy cannot natively represent timezone-aware datetimes. pandas supports this with the <span class="title-ref">arrays.DatetimeArray</span> extension array, which can hold timezone-naive or timezone-aware values.

<span class="title-ref">Timestamp</span>, a subclass of <span class="title-ref">datetime.datetime</span>, is pandas' scalar type for timezone-naive or timezone-aware datetime data. <span class="title-ref">NaT</span> is the missing value for datetime data.

<div class="autosummary" data-toctree="api/">

Timestamp

</div>

#### Properties

<div class="autosummary" data-toctree="api/">

Timestamp.asm8 Timestamp.day Timestamp.dayofweek Timestamp.day\_of\_week Timestamp.dayofyear Timestamp.day\_of\_year Timestamp.days\_in\_month Timestamp.daysinmonth Timestamp.fold Timestamp.hour Timestamp.is\_leap\_year Timestamp.is\_month\_end Timestamp.is\_month\_start Timestamp.is\_quarter\_end Timestamp.is\_quarter\_start Timestamp.is\_year\_end Timestamp.is\_year\_start Timestamp.max Timestamp.microsecond Timestamp.min Timestamp.minute Timestamp.month Timestamp.nanosecond Timestamp.quarter Timestamp.resolution Timestamp.second Timestamp.tz Timestamp.tzinfo Timestamp.unit Timestamp.value Timestamp.week Timestamp.weekofyear Timestamp.year

</div>

#### Methods

<div class="autosummary" data-toctree="api/">

Timestamp.as\_unit Timestamp.astimezone Timestamp.ceil Timestamp.combine Timestamp.ctime Timestamp.date Timestamp.day\_name Timestamp.dst Timestamp.floor Timestamp.fromordinal Timestamp.fromtimestamp Timestamp.isocalendar Timestamp.isoformat Timestamp.isoweekday Timestamp.month\_name Timestamp.normalize Timestamp.now Timestamp.replace Timestamp.round Timestamp.strftime Timestamp.strptime Timestamp.time Timestamp.timestamp Timestamp.timetuple Timestamp.timetz Timestamp.to\_datetime64 Timestamp.to\_numpy Timestamp.to\_julian\_date Timestamp.to\_period Timestamp.to\_pydatetime Timestamp.today Timestamp.toordinal Timestamp.tz\_convert Timestamp.tz\_localize Timestamp.tzname Timestamp.utcfromtimestamp Timestamp.utcnow Timestamp.utcoffset Timestamp.utctimetuple Timestamp.weekday

</div>

A collection of timestamps may be stored in a <span class="title-ref">arrays.DatetimeArray</span>. For timezone-aware data, the `.dtype` of a <span class="title-ref">arrays.DatetimeArray</span> is a <span class="title-ref">DatetimeTZDtype</span>. For timezone-naive data, `np.dtype("datetime64[ns]")` is used.

If the data are timezone-aware, then every value in the array must have the same timezone.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.DatetimeArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

DatetimeTZDtype

</div>

### Timedeltas

NumPy can natively represent timedeltas. pandas provides <span class="title-ref">Timedelta</span> for symmetry with <span class="title-ref">Timestamp</span>. <span class="title-ref">NaT</span> is the missing value for timedelta data.

<div class="autosummary" data-toctree="api/">

Timedelta

</div>

#### Properties

<div class="autosummary" data-toctree="api/">

Timedelta.asm8 Timedelta.components Timedelta.days Timedelta.max Timedelta.microseconds Timedelta.min Timedelta.nanoseconds Timedelta.resolution Timedelta.seconds Timedelta.unit Timedelta.value Timedelta.view

</div>

#### Methods

<div class="autosummary" data-toctree="api/">

Timedelta.as\_unit Timedelta.ceil Timedelta.floor Timedelta.isoformat Timedelta.round Timedelta.to\_pytimedelta Timedelta.to\_timedelta64 Timedelta.to\_numpy Timedelta.total\_seconds

</div>

A collection of <span class="title-ref">Timedelta</span> may be stored in a <span class="title-ref">TimedeltaArray</span>.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.TimedeltaArray

</div>

### Periods

pandas represents spans of times as <span class="title-ref">Period</span> objects.

### Period

<div class="autosummary" data-toctree="api/">

Period

</div>

#### Properties

<div class="autosummary" data-toctree="api/">

Period.day Period.dayofweek Period.day\_of\_week Period.dayofyear Period.day\_of\_year Period.days\_in\_month Period.daysinmonth Period.end\_time Period.freq Period.freqstr Period.hour Period.is\_leap\_year Period.minute Period.month Period.ordinal Period.quarter Period.qyear Period.second Period.start\_time Period.week Period.weekday Period.weekofyear Period.year

</div>

#### Methods

<div class="autosummary" data-toctree="api/">

Period.asfreq Period.now Period.strftime Period.to\_timestamp

</div>

A collection of <span class="title-ref">Period</span> may be stored in a <span class="title-ref">arrays.PeriodArray</span>. Every period in a <span class="title-ref">arrays.PeriodArray</span> must have the same `freq`.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.PeriodArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

PeriodDtype

</div>

### Intervals

Arbitrary intervals can be represented as <span class="title-ref">Interval</span> objects.

<div class="autosummary" data-toctree="api/">

Interval

</div>

#### Properties

<div class="autosummary" data-toctree="api/">

Interval.closed Interval.closed\_left Interval.closed\_right Interval.is\_empty Interval.left Interval.length Interval.mid Interval.open\_left Interval.open\_right Interval.overlaps Interval.right

</div>

A collection of intervals may be stored in an <span class="title-ref">arrays.IntervalArray</span>.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.IntervalArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

IntervalDtype

</div>

### Nullable integer

<span class="title-ref">numpy.ndarray</span> cannot natively represent integer-data with missing values. pandas provides this through <span class="title-ref">arrays.IntegerArray</span>.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.IntegerArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

Int8Dtype Int16Dtype Int32Dtype Int64Dtype UInt8Dtype UInt16Dtype UInt32Dtype UInt64Dtype

</div>

### Nullable float

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.FloatingArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

Float32Dtype Float64Dtype

</div>

### Categoricals

pandas defines a custom data type for representing data that can take only a limited, fixed set of values. The dtype of a <span class="title-ref">Categorical</span> can be described by a <span class="title-ref">CategoricalDtype</span>.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

CategoricalDtype

</div>

<div class="autosummary" data-toctree="api/">

CategoricalDtype.categories CategoricalDtype.ordered

</div>

Categorical data can be stored in a <span class="title-ref">pandas.Categorical</span>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

Categorical

</div>

The alternative <span class="title-ref">Categorical.from\_codes</span> constructor can be used when you have the categories and integer codes already:

<div class="autosummary" data-toctree="api/">

Categorical.from\_codes

</div>

The dtype information is available on the <span class="title-ref">Categorical</span>

<div class="autosummary" data-toctree="api/">

Categorical.dtype Categorical.categories Categorical.ordered Categorical.codes

</div>

`np.asarray(categorical)` works by implementing the array interface. Be aware, that this converts the <span class="title-ref">Categorical</span> back to a NumPy array, so categories and order information is not preserved\!

<div class="autosummary" data-toctree="api/">

Categorical.\_\_array\_\_

</div>

A <span class="title-ref">Categorical</span> can be stored in a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span>. To create a Series of dtype `category`, use `cat = s.astype(dtype)` or `Series(..., dtype=dtype)` where `dtype` is either

  - the string `'category'`
  - an instance of <span class="title-ref">CategoricalDtype</span>.

If the <span class="title-ref">Series</span> is of dtype <span class="title-ref">CategoricalDtype</span>, `Series.cat` can be used to change the categorical data. See \[api.series.cat\](\#api.series.cat) for more.

More methods are available on \`Categorical\`:

<div class="autosummary" data-toctree="api/">

Categorical.as\_ordered Categorical.as\_unordered Categorical.set\_categories Categorical.rename\_categories Categorical.reorder\_categories Categorical.add\_categories Categorical.remove\_categories Categorical.remove\_unused\_categories Categorical.map

</div>

### Sparse

Data where a single value is repeated many times (e.g. `0` or `NaN`) may be stored efficiently as a <span class="title-ref">arrays.SparseArray</span>.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.SparseArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

SparseDtype

</div>

The `Series.sparse` accessor may be used to access sparse-specific attributes and methods if the <span class="title-ref">Series</span> contains sparse values. See \[api.series.sparse\](\#api.series.sparse) and \[the user guide \<sparse\>\](\#the-user-guide-\<sparse\>) for more.

### Strings

When working with text data, where each valid element is a string or missing, we recommend using <span class="title-ref">StringDtype</span> (with the alias `"string"`).

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.StringArray arrays.ArrowStringArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

StringDtype

</div>

The `Series.str` accessor is available for <span class="title-ref">Series</span> backed by a <span class="title-ref">arrays.StringArray</span>. See \[api.series.str\](\#api.series.str) for more.

### Nullable Boolean

The boolean dtype (with the alias `"boolean"`) provides support for storing boolean data (`True`, `False`) with missing values, which is not possible with a bool <span class="title-ref">numpy.ndarray</span>.

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

arrays.BooleanArray

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

BooleanDtype

</div>

## Utilities

### Constructors

<div class="autosummary" data-toctree="api/">

api.types.union\_categoricals api.types.infer\_dtype api.types.pandas\_dtype

</div>

#### Data type introspection

<div class="autosummary" data-toctree="api/">

api.types.is\_any\_real\_numeric\_dtype api.types.is\_bool\_dtype api.types.is\_categorical\_dtype api.types.is\_complex\_dtype api.types.is\_datetime64\_any\_dtype api.types.is\_datetime64\_dtype api.types.is\_datetime64\_ns\_dtype api.types.is\_datetime64tz\_dtype api.types.is\_extension\_array\_dtype api.types.is\_float\_dtype api.types.is\_int64\_dtype api.types.is\_integer\_dtype api.types.is\_interval\_dtype api.types.is\_numeric\_dtype api.types.is\_object\_dtype api.types.is\_period\_dtype api.types.is\_signed\_integer\_dtype api.types.is\_string\_dtype api.types.is\_timedelta64\_dtype api.types.is\_timedelta64\_ns\_dtype api.types.is\_unsigned\_integer\_dtype api.types.is\_sparse

</div>

#### Iterable introspection

<div class="autosummary" data-toctree="api/">

api.types.is\_dict\_like api.types.is\_file\_like api.types.is\_list\_like api.types.is\_named\_tuple api.types.is\_iterator

</div>

#### Scalar introspection

<div class="autosummary" data-toctree="api/">

api.types.is\_bool api.types.is\_complex api.types.is\_float api.types.is\_hashable api.types.is\_integer api.types.is\_number api.types.is\_re api.types.is\_re\_compilable api.types.is\_scalar

</div>

---

extensions.md

---

{{ header }}

# Extensions

<div class="currentmodule">

pandas

</div>

These are primarily intended for library authors looking to extend pandas objects.

<div class="autosummary" data-toctree="api/">

api.extensions.register\_extension\_dtype api.extensions.register\_dataframe\_accessor api.extensions.register\_series\_accessor api.extensions.register\_index\_accessor api.extensions.ExtensionDtype

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

api.extensions.ExtensionArray arrays.NumpyExtensionArray

</div>

Additionally, we have some utility methods for ensuring your object behaves correctly.

<div class="autosummary" data-toctree="api/">

api.indexers.check\_array\_indexer

</div>

The sentinel `pandas.api.extensions.no_default` is used as the default value in some methods. Use an `is` comparison to check if the user provides a non-default value.

---

frame.md

---

{{ header }}

# DataFrame

<div class="currentmodule">

pandas

</div>

## Constructor

<div class="autosummary" data-toctree="api/">

DataFrame

</div>

## Attributes and underlying data

**Axes**

<div class="autosummary" data-toctree="api/">

DataFrame.index DataFrame.columns

</div>

<div class="autosummary" data-toctree="api/">

DataFrame.dtypes DataFrame.info DataFrame.select\_dtypes DataFrame.values DataFrame.axes DataFrame.ndim DataFrame.size DataFrame.shape DataFrame.memory\_usage DataFrame.empty DataFrame.set\_flags

</div>

## Conversion

<div class="autosummary" data-toctree="api/">

DataFrame.astype DataFrame.convert\_dtypes DataFrame.infer\_objects DataFrame.copy DataFrame.to\_numpy

</div>

## Indexing, iteration

<div class="autosummary" data-toctree="api/">

DataFrame.head DataFrame.at DataFrame.iat DataFrame.loc DataFrame.iloc DataFrame.insert DataFrame.\_\_iter\_\_ DataFrame.items DataFrame.keys DataFrame.iterrows DataFrame.itertuples DataFrame.pop DataFrame.tail DataFrame.xs DataFrame.get DataFrame.isin DataFrame.where DataFrame.mask DataFrame.query DataFrame.isetitem

</div>

For more information on `.at`, `.iat`, `.loc`, and `.iloc`, see the \[indexing documentation \<indexing\>\](\#indexing-documentation-\<indexing\>).

## Binary operator functions

<div class="autosummary" data-toctree="api/">

DataFrame.\_\_add\_\_ DataFrame.add DataFrame.sub DataFrame.mul DataFrame.div DataFrame.truediv DataFrame.floordiv DataFrame.mod DataFrame.pow DataFrame.dot DataFrame.radd DataFrame.rsub DataFrame.rmul DataFrame.rdiv DataFrame.rtruediv DataFrame.rfloordiv DataFrame.rmod DataFrame.rpow DataFrame.lt DataFrame.gt DataFrame.le DataFrame.ge DataFrame.ne DataFrame.eq DataFrame.combine DataFrame.combine\_first

</div>

## Function application, GroupBy & window

<div class="autosummary" data-toctree="api/">

DataFrame.apply DataFrame.map DataFrame.pipe DataFrame.agg DataFrame.aggregate DataFrame.transform DataFrame.groupby DataFrame.rolling DataFrame.expanding DataFrame.ewm

</div>

## Computations / descriptive stats

<div class="autosummary" data-toctree="api/">

DataFrame.abs DataFrame.all DataFrame.any DataFrame.clip DataFrame.corr DataFrame.corrwith DataFrame.count DataFrame.cov DataFrame.cummax DataFrame.cummin DataFrame.cumprod DataFrame.cumsum DataFrame.describe DataFrame.diff DataFrame.eval DataFrame.kurt DataFrame.kurtosis DataFrame.max DataFrame.mean DataFrame.median DataFrame.min DataFrame.mode DataFrame.pct\_change DataFrame.prod DataFrame.product DataFrame.quantile DataFrame.rank DataFrame.round DataFrame.sem DataFrame.skew DataFrame.sum DataFrame.std DataFrame.var DataFrame.nunique DataFrame.value\_counts

</div>

## Reindexing / selection / label manipulation

<div class="autosummary" data-toctree="api/">

DataFrame.add\_prefix DataFrame.add\_suffix DataFrame.align DataFrame.at\_time DataFrame.between\_time DataFrame.drop DataFrame.drop\_duplicates DataFrame.duplicated DataFrame.equals DataFrame.filter DataFrame.head DataFrame.idxmax DataFrame.idxmin DataFrame.reindex DataFrame.reindex\_like DataFrame.rename DataFrame.rename\_axis DataFrame.reset\_index DataFrame.sample DataFrame.set\_axis DataFrame.set\_index DataFrame.tail DataFrame.take DataFrame.truncate

</div>

## Missing data handling

<div class="autosummary" data-toctree="api/">

DataFrame.bfill DataFrame.dropna DataFrame.ffill DataFrame.fillna DataFrame.interpolate DataFrame.isna DataFrame.isnull DataFrame.notna DataFrame.notnull DataFrame.replace

</div>

## Reshaping, sorting, transposing

<div class="autosummary" data-toctree="api/">

DataFrame.droplevel DataFrame.pivot DataFrame.pivot\_table DataFrame.reorder\_levels DataFrame.sort\_values DataFrame.sort\_index DataFrame.nlargest DataFrame.nsmallest DataFrame.swaplevel DataFrame.stack DataFrame.unstack DataFrame.melt DataFrame.explode DataFrame.squeeze DataFrame.to\_xarray DataFrame.T DataFrame.transpose

</div>

## Combining / comparing / joining / merging

<div class="autosummary" data-toctree="api/">

DataFrame.assign DataFrame.compare DataFrame.join DataFrame.merge DataFrame.update

</div>

## Time Series-related

<div class="autosummary" data-toctree="api/">

DataFrame.asfreq DataFrame.asof DataFrame.shift DataFrame.first\_valid\_index DataFrame.last\_valid\_index DataFrame.resample DataFrame.to\_period DataFrame.to\_timestamp DataFrame.tz\_convert DataFrame.tz\_localize

</div>

## Flags

Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in <span class="title-ref">DataFrame.attrs</span>.

<div class="autosummary" data-toctree="api/">

Flags

</div>

## Metadata

<span class="title-ref">DataFrame.attrs</span> is a dictionary for storing global metadata for this DataFrame.

<div class="warning">

<div class="title">

Warning

</div>

`DataFrame.attrs` is considered experimental and may change without warning.

</div>

<div class="autosummary" data-toctree="api/">

DataFrame.attrs

</div>

## Plotting

`DataFrame.plot` is both a callable method and a namespace attribute for specific plotting methods of the form `DataFrame.plot.<kind>`.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_callable.rst">

DataFrame.plot

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

DataFrame.plot.area DataFrame.plot.bar DataFrame.plot.barh DataFrame.plot.box DataFrame.plot.density DataFrame.plot.hexbin DataFrame.plot.hist DataFrame.plot.kde DataFrame.plot.line DataFrame.plot.pie DataFrame.plot.scatter

</div>

<div class="autosummary" data-toctree="api/">

DataFrame.boxplot DataFrame.hist

</div>

## Sparse accessor

Sparse-dtype specific methods and attributes are provided under the `DataFrame.sparse` accessor.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

DataFrame.sparse.density

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

DataFrame.sparse.from\_spmatrix DataFrame.sparse.to\_coo DataFrame.sparse.to\_dense

</div>

## Serialization / IO / conversion

<div class="autosummary" data-toctree="api/">

DataFrame.from\_dict DataFrame.from\_records DataFrame.to\_orc DataFrame.to\_parquet DataFrame.to\_pickle DataFrame.to\_csv DataFrame.to\_hdf DataFrame.to\_sql DataFrame.to\_dict DataFrame.to\_excel DataFrame.to\_json DataFrame.to\_html DataFrame.to\_feather DataFrame.to\_latex DataFrame.to\_stata DataFrame.to\_records DataFrame.to\_string DataFrame.to\_clipboard DataFrame.to\_markdown DataFrame.style DataFrame.\_\_dataframe\_\_

</div>

---

general_functions.md

---

{{ header }}

# General functions

<div class="currentmodule">

pandas

</div>

## Data manipulations

<div class="autosummary" data-toctree="api/">

melt pivot pivot\_table crosstab cut qcut merge merge\_ordered merge\_asof concat get\_dummies from\_dummies factorize unique lreshape wide\_to\_long

</div>

## Top-level missing data

<div class="autosummary" data-toctree="api/">

isna isnull notna notnull

</div>

## Top-level dealing with numeric data

<div class="autosummary" data-toctree="api/">

to\_numeric

</div>

## Top-level dealing with datetimelike data

<div class="autosummary" data-toctree="api/">

to\_datetime to\_timedelta date\_range bdate\_range period\_range timedelta\_range infer\_freq

</div>

## Top-level dealing with Interval data

<div class="autosummary" data-toctree="api/">

interval\_range

</div>

## Top-level evaluation

<div class="autosummary" data-toctree="api/">

eval

</div>

## Datetime formats

<div class="autosummary" data-toctree="api/">

tseries.api.guess\_datetime\_format

</div>

## Hashing

<div class="autosummary" data-toctree="api/">

util.hash\_array util.hash\_pandas\_object

</div>

## Importing from other DataFrame libraries

<div class="autosummary" data-toctree="api/">

api.interchange.from\_dataframe

</div>

---

groupby.md

---

{{ header }}

# GroupBy

<div class="currentmodule">

pandas.core.groupby

</div>

<span class="title-ref">pandas.api.typing.DataFrameGroupBy</span> and <span class="title-ref">pandas.api.typing.SeriesGroupBy</span> instances are returned by groupby calls <span class="title-ref">pandas.DataFrame.groupby</span> and <span class="title-ref">pandas.Series.groupby</span> respectively.

## Indexing, iteration

<div class="autosummary" data-toctree="api/">

DataFrameGroupBy.\_\_iter\_\_ SeriesGroupBy.\_\_iter\_\_ DataFrameGroupBy.groups SeriesGroupBy.groups DataFrameGroupBy.indices SeriesGroupBy.indices DataFrameGroupBy.get\_group SeriesGroupBy.get\_group

</div>

<div class="currentmodule">

pandas

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

Grouper

</div>

## Function application helper

<div class="autosummary" data-toctree="api/">

NamedAgg

</div>

<div class="currentmodule">

pandas.core.groupby

</div>

## Function application

<div class="autosummary" data-toctree="api/">

SeriesGroupBy.apply DataFrameGroupBy.apply SeriesGroupBy.agg DataFrameGroupBy.agg SeriesGroupBy.aggregate DataFrameGroupBy.aggregate SeriesGroupBy.transform DataFrameGroupBy.transform SeriesGroupBy.pipe DataFrameGroupBy.pipe DataFrameGroupBy.filter SeriesGroupBy.filter

</div>

## `DataFrameGroupBy` computations / descriptive stats

<div class="autosummary" data-toctree="api/">

DataFrameGroupBy.all DataFrameGroupBy.any DataFrameGroupBy.bfill DataFrameGroupBy.corr DataFrameGroupBy.corrwith DataFrameGroupBy.count DataFrameGroupBy.cov DataFrameGroupBy.cumcount DataFrameGroupBy.cummax DataFrameGroupBy.cummin DataFrameGroupBy.cumprod DataFrameGroupBy.cumsum DataFrameGroupBy.describe DataFrameGroupBy.diff DataFrameGroupBy.ffill DataFrameGroupBy.first DataFrameGroupBy.head DataFrameGroupBy.idxmax DataFrameGroupBy.idxmin DataFrameGroupBy.last DataFrameGroupBy.max DataFrameGroupBy.mean DataFrameGroupBy.median DataFrameGroupBy.min DataFrameGroupBy.ngroup DataFrameGroupBy.nth DataFrameGroupBy.nunique DataFrameGroupBy.ohlc DataFrameGroupBy.pct\_change DataFrameGroupBy.prod DataFrameGroupBy.quantile DataFrameGroupBy.rank DataFrameGroupBy.resample DataFrameGroupBy.rolling DataFrameGroupBy.sample DataFrameGroupBy.sem DataFrameGroupBy.shift DataFrameGroupBy.size DataFrameGroupBy.skew DataFrameGroupBy.std DataFrameGroupBy.sum DataFrameGroupBy.var DataFrameGroupBy.tail DataFrameGroupBy.take DataFrameGroupBy.value\_counts

</div>

## `SeriesGroupBy` computations / descriptive stats

<div class="autosummary" data-toctree="api/">

SeriesGroupBy.all SeriesGroupBy.any SeriesGroupBy.bfill SeriesGroupBy.corr SeriesGroupBy.count SeriesGroupBy.cov SeriesGroupBy.cumcount SeriesGroupBy.cummax SeriesGroupBy.cummin SeriesGroupBy.cumprod SeriesGroupBy.cumsum SeriesGroupBy.describe SeriesGroupBy.diff SeriesGroupBy.ffill SeriesGroupBy.first SeriesGroupBy.head SeriesGroupBy.last SeriesGroupBy.idxmax SeriesGroupBy.idxmin SeriesGroupBy.is\_monotonic\_increasing SeriesGroupBy.is\_monotonic\_decreasing SeriesGroupBy.max SeriesGroupBy.mean SeriesGroupBy.median SeriesGroupBy.min SeriesGroupBy.ngroup SeriesGroupBy.nlargest SeriesGroupBy.nsmallest SeriesGroupBy.nth SeriesGroupBy.nunique SeriesGroupBy.unique SeriesGroupBy.ohlc SeriesGroupBy.pct\_change SeriesGroupBy.prod SeriesGroupBy.quantile SeriesGroupBy.rank SeriesGroupBy.resample SeriesGroupBy.rolling SeriesGroupBy.sample SeriesGroupBy.sem SeriesGroupBy.shift SeriesGroupBy.size SeriesGroupBy.skew SeriesGroupBy.std SeriesGroupBy.sum SeriesGroupBy.var SeriesGroupBy.tail SeriesGroupBy.take SeriesGroupBy.value\_counts

</div>

## Plotting and visualization

<div class="autosummary" data-toctree="api/">

DataFrameGroupBy.boxplot DataFrameGroupBy.hist SeriesGroupBy.hist DataFrameGroupBy.plot SeriesGroupBy.plot

</div>

---

index.md

---

{{ header }}

# API reference

This page gives an overview of all public pandas objects, functions and methods. All classes and functions exposed in `pandas.*` namespace are public.

The following subpackages are public.

  - `pandas.errors`: Custom exception and warnings classes that are raised by pandas.
  - `pandas.plotting`: Plotting public API.
  - `pandas.testing`: Functions that are useful for writing tests involving pandas objects.
  - `pandas.api.extensions`: Functions and classes for extending pandas objects.
  - `pandas.api.indexers`: Functions and classes for rolling window indexers.
  - `pandas.api.interchange`: DataFrame interchange protocol.
  - `pandas.api.types`: Datatype classes and functions.
  - `pandas.api.typing`: Classes that may be necessary for type-hinting. These are classes that are encountered as intermediate results but should not be instantiated directly by users. These classes are not to be confused with classes from the [pandas-stubs](https://github.com/pandas-dev/pandas-stubs) package which has classes in addition to those that occur in pandas for type-hinting.

In addition, public functions in `pandas.io`, `pandas.tseries`, `pandas.util` submodules are explicitly mentioned in the documentation. Further APIs in these modules are not guaranteed to be stable.

\> **Warning** \> The `pandas.core`, `pandas.compat` top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed.

<div class="toctree" data-maxdepth="2">

io general\_functions series frame arrays indexing offset\_frequency window groupby resampling style plotting options extensions testing missing\_value

</div>

---

indexing.md

---

{{ header }}

# Index objects

## Index

<div class="currentmodule">

pandas

</div>

**Many of these methods or variants thereof are available on the objects that contain an index (Series/DataFrame) and those should most likely be used before calling these methods directly.**

<div class="autosummary" data-toctree="api/">

Index

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Index.values Index.is\_monotonic\_increasing Index.is\_monotonic\_decreasing Index.is\_unique Index.has\_duplicates Index.hasnans Index.dtype Index.inferred\_type Index.shape Index.name Index.names Index.nbytes Index.ndim Index.size Index.empty Index.T Index.memory\_usage Index.array

</div>

### Modifying and computations

<div class="autosummary" data-toctree="api/">

Index.all Index.any Index.argmin Index.argmax Index.copy Index.delete Index.drop Index.drop\_duplicates Index.duplicated Index.equals Index.factorize Index.identical Index.insert [Index.is]() Index.min Index.max Index.reindex Index.rename Index.repeat Index.where Index.take Index.putmask Index.unique Index.nunique Index.value\_counts

</div>

### Compatibility with MultiIndex

<div class="autosummary" data-toctree="api/">

Index.set\_names Index.droplevel

</div>

### Missing values

<div class="autosummary" data-toctree="api/">

Index.fillna Index.dropna Index.isna Index.notna

</div>

### Conversion

<div class="autosummary" data-toctree="api/">

Index.astype Index.item Index.map Index.ravel Index.to\_list Index.to\_series Index.to\_frame Index.to\_numpy Index.view

</div>

### Sorting

<div class="autosummary" data-toctree="api/">

Index.argsort Index.searchsorted Index.sort\_values

</div>

### Time-specific operations

<div class="autosummary" data-toctree="api/">

Index.shift

</div>

### Combining / joining / set operations

<div class="autosummary" data-toctree="api/">

Index.append Index.join Index.intersection Index.union Index.difference Index.symmetric\_difference

</div>

### Selecting

<div class="autosummary" data-toctree="api/">

Index.asof Index.asof\_locs Index.get\_indexer Index.get\_indexer\_for Index.get\_indexer\_non\_unique Index.get\_level\_values Index.get\_loc Index.get\_slice\_bound Index.isin Index.slice\_indexer Index.slice\_locs

</div>

## Numeric Index

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

RangeIndex

</div>

<div class="autosummary" data-toctree="api/">

RangeIndex.start RangeIndex.stop RangeIndex.step RangeIndex.from\_range

</div>

## CategoricalIndex

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

CategoricalIndex

</div>

### Categorical components

<div class="autosummary" data-toctree="api/">

CategoricalIndex.codes CategoricalIndex.categories CategoricalIndex.ordered CategoricalIndex.rename\_categories CategoricalIndex.reorder\_categories CategoricalIndex.add\_categories CategoricalIndex.remove\_categories CategoricalIndex.remove\_unused\_categories CategoricalIndex.set\_categories CategoricalIndex.as\_ordered CategoricalIndex.as\_unordered

</div>

### Modifying and computations

<div class="autosummary" data-toctree="api/">

CategoricalIndex.map CategoricalIndex.equals

</div>

## IntervalIndex

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

IntervalIndex

</div>

### IntervalIndex components

<div class="autosummary" data-toctree="api/">

IntervalIndex.from\_arrays IntervalIndex.from\_tuples IntervalIndex.from\_breaks IntervalIndex.left IntervalIndex.right IntervalIndex.mid IntervalIndex.closed IntervalIndex.length IntervalIndex.values IntervalIndex.is\_empty IntervalIndex.is\_non\_overlapping\_monotonic IntervalIndex.is\_overlapping IntervalIndex.get\_loc IntervalIndex.get\_indexer IntervalIndex.set\_closed IntervalIndex.contains IntervalIndex.overlaps IntervalIndex.to\_tuples

</div>

## MultiIndex

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

MultiIndex

</div>

### MultiIndex constructors

<div class="autosummary" data-toctree="api/">

MultiIndex.from\_arrays MultiIndex.from\_tuples MultiIndex.from\_product MultiIndex.from\_frame

</div>

### MultiIndex properties

<div class="autosummary" data-toctree="api/">

MultiIndex.names MultiIndex.levels MultiIndex.codes MultiIndex.nlevels MultiIndex.levshape MultiIndex.dtypes

</div>

### MultiIndex components

<div class="autosummary" data-toctree="api/">

MultiIndex.set\_levels MultiIndex.set\_codes MultiIndex.to\_flat\_index MultiIndex.to\_frame MultiIndex.sortlevel MultiIndex.droplevel MultiIndex.swaplevel MultiIndex.reorder\_levels MultiIndex.remove\_unused\_levels MultiIndex.drop MultiIndex.copy MultiIndex.append MultiIndex.truncate

</div>

### MultiIndex selecting

<div class="autosummary" data-toctree="api/">

MultiIndex.get\_loc MultiIndex.get\_locs MultiIndex.get\_loc\_level MultiIndex.get\_indexer MultiIndex.get\_level\_values

</div>

<div class="autosummary" data-toctree="api/">

IndexSlice

</div>

## DatetimeIndex

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

DatetimeIndex

</div>

### Time/date components

<div class="autosummary" data-toctree="api/">

DatetimeIndex.year DatetimeIndex.month DatetimeIndex.day DatetimeIndex.hour DatetimeIndex.minute DatetimeIndex.second DatetimeIndex.microsecond DatetimeIndex.nanosecond DatetimeIndex.date DatetimeIndex.time DatetimeIndex.timetz DatetimeIndex.dayofyear DatetimeIndex.day\_of\_year DatetimeIndex.dayofweek DatetimeIndex.day\_of\_week DatetimeIndex.weekday DatetimeIndex.quarter DatetimeIndex.tz DatetimeIndex.freq DatetimeIndex.freqstr DatetimeIndex.is\_month\_start DatetimeIndex.is\_month\_end DatetimeIndex.is\_quarter\_start DatetimeIndex.is\_quarter\_end DatetimeIndex.is\_year\_start DatetimeIndex.is\_year\_end DatetimeIndex.is\_leap\_year DatetimeIndex.inferred\_freq

</div>

### Selecting

<div class="autosummary" data-toctree="api/">

DatetimeIndex.indexer\_at\_time DatetimeIndex.indexer\_between\_time

</div>

### Time-specific operations

<div class="autosummary" data-toctree="api/">

DatetimeIndex.normalize DatetimeIndex.strftime DatetimeIndex.snap DatetimeIndex.tz\_convert DatetimeIndex.tz\_localize DatetimeIndex.round DatetimeIndex.floor DatetimeIndex.ceil DatetimeIndex.month\_name DatetimeIndex.day\_name

</div>

### Conversion

<div class="autosummary" data-toctree="api/">

DatetimeIndex.as\_unit DatetimeIndex.to\_period DatetimeIndex.to\_pydatetime DatetimeIndex.to\_series DatetimeIndex.to\_frame

</div>

### Methods

<div class="autosummary" data-toctree="api/">

DatetimeIndex.mean DatetimeIndex.std

</div>

## TimedeltaIndex

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

TimedeltaIndex

</div>

### Components

<div class="autosummary" data-toctree="api/">

TimedeltaIndex.days TimedeltaIndex.seconds TimedeltaIndex.microseconds TimedeltaIndex.nanoseconds TimedeltaIndex.components TimedeltaIndex.inferred\_freq

</div>

### Conversion

<div class="autosummary" data-toctree="api/">

TimedeltaIndex.as\_unit TimedeltaIndex.to\_pytimedelta TimedeltaIndex.to\_series TimedeltaIndex.round TimedeltaIndex.floor TimedeltaIndex.ceil TimedeltaIndex.to\_frame

</div>

### Methods

<div class="autosummary" data-toctree="api/">

TimedeltaIndex.mean

</div>

<div class="currentmodule">

pandas

</div>

## PeriodIndex

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

PeriodIndex

</div>

### Properties

<div class="autosummary" data-toctree="api/">

PeriodIndex.day PeriodIndex.dayofweek PeriodIndex.day\_of\_week PeriodIndex.dayofyear PeriodIndex.day\_of\_year PeriodIndex.days\_in\_month PeriodIndex.daysinmonth PeriodIndex.end\_time PeriodIndex.freq PeriodIndex.freqstr PeriodIndex.hour PeriodIndex.is\_leap\_year PeriodIndex.minute PeriodIndex.month PeriodIndex.quarter PeriodIndex.qyear PeriodIndex.second PeriodIndex.start\_time PeriodIndex.week PeriodIndex.weekday PeriodIndex.weekofyear PeriodIndex.year

</div>

### Methods

<div class="autosummary" data-toctree="api/">

PeriodIndex.asfreq PeriodIndex.strftime PeriodIndex.to\_timestamp PeriodIndex.from\_fields PeriodIndex.from\_ordinals

</div>

---

io.md

---

{{ header }}

# Input/output

<div class="currentmodule">

pandas

</div>

## Pickling

<div class="autosummary" data-toctree="api/">

read\_pickle DataFrame.to\_pickle

</div>

## Flat file

<div class="autosummary" data-toctree="api/">

read\_table read\_csv DataFrame.to\_csv read\_fwf

</div>

## Clipboard

<div class="autosummary" data-toctree="api/">

read\_clipboard DataFrame.to\_clipboard

</div>

## Excel

<div class="autosummary" data-toctree="api/">

read\_excel DataFrame.to\_excel ExcelFile ExcelFile.book ExcelFile.sheet\_names ExcelFile.parse

</div>

<div class="currentmodule">

pandas.io.formats.style

</div>

<div class="autosummary" data-toctree="api/">

Styler.to\_excel

</div>

<div class="currentmodule">

pandas

</div>

<div class="autosummary" data-toctree="api/">

ExcelWriter

</div>

<div class="currentmodule">

pandas

</div>

## JSON

<div class="autosummary" data-toctree="api/">

read\_json json\_normalize DataFrame.to\_json

</div>

<div class="currentmodule">

pandas.io.json

</div>

<div class="autosummary" data-toctree="api/">

build\_table\_schema

</div>

<div class="currentmodule">

pandas

</div>

## HTML

<div class="autosummary" data-toctree="api/">

read\_html DataFrame.to\_html

</div>

<div class="currentmodule">

pandas.io.formats.style

</div>

<div class="autosummary" data-toctree="api/">

Styler.to\_html

</div>

<div class="currentmodule">

pandas

</div>

## XML

<div class="autosummary" data-toctree="api/">

read\_xml DataFrame.to\_xml

</div>

## Latex

<div class="autosummary" data-toctree="api/">

DataFrame.to\_latex

</div>

<div class="currentmodule">

pandas.io.formats.style

</div>

<div class="autosummary" data-toctree="api/">

Styler.to\_latex

</div>

<div class="currentmodule">

pandas

</div>

## HDFStore: PyTables (HDF5)

<div class="autosummary" data-toctree="api/">

read\_hdf HDFStore.put HDFStore.append HDFStore.get HDFStore.select HDFStore.info HDFStore.keys HDFStore.groups HDFStore.walk

</div>

\> **Warning** \> One can store a subclass of <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> to HDF5, but the type of the subclass is lost upon storing.

## Feather

<div class="autosummary" data-toctree="api/">

read\_feather DataFrame.to\_feather

</div>

## Parquet

<div class="autosummary" data-toctree="api/">

read\_parquet DataFrame.to\_parquet

</div>

## ORC

<div class="autosummary" data-toctree="api/">

read\_orc DataFrame.to\_orc

</div>

## SAS

<div class="autosummary" data-toctree="api/">

read\_sas

</div>

## SPSS

<div class="autosummary" data-toctree="api/">

read\_spss

</div>

## SQL

<div class="autosummary" data-toctree="api/">

read\_sql\_table read\_sql\_query read\_sql DataFrame.to\_sql

</div>

## STATA

<div class="autosummary" data-toctree="api/">

read\_stata DataFrame.to\_stata

</div>

<div class="currentmodule">

pandas.io.stata

</div>

<div class="autosummary" data-toctree="api/">

StataReader.data\_label StataReader.value\_labels StataReader.variable\_labels StataWriter.write\_file

</div>

---

missing_value.md

---

{{ header }}

# Missing values

<div class="currentmodule">

pandas

</div>

NA is the way to represent missing values for nullable dtypes (see below):

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

NA

</div>

NaT is the missing value for timedelta and datetime data (see below):

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

NaT

</div>

---

offset_frequency.md

---

{{ header }}

# Date offsets

<div class="currentmodule">

pandas.tseries.offsets

</div>

## DateOffset

<div class="autosummary" data-toctree="api/">

DateOffset

</div>

### Properties

<div class="autosummary" data-toctree="api/">

DateOffset.freqstr DateOffset.kwds DateOffset.name DateOffset.nanos DateOffset.normalize DateOffset.rule\_code DateOffset.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

DateOffset.copy DateOffset.is\_on\_offset DateOffset.is\_month\_start DateOffset.is\_month\_end DateOffset.is\_quarter\_start DateOffset.is\_quarter\_end DateOffset.is\_year\_start DateOffset.is\_year\_end

</div>

## BusinessDay

<div class="autosummary" data-toctree="api/">

BusinessDay

</div>

Alias:

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

BDay

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BusinessDay.freqstr BusinessDay.kwds BusinessDay.name BusinessDay.nanos BusinessDay.normalize BusinessDay.rule\_code BusinessDay.n BusinessDay.weekmask BusinessDay.holidays BusinessDay.calendar

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BusinessDay.copy BusinessDay.is\_on\_offset BusinessDay.is\_month\_start BusinessDay.is\_month\_end BusinessDay.is\_quarter\_start BusinessDay.is\_quarter\_end BusinessDay.is\_year\_start BusinessDay.is\_year\_end

</div>

## BusinessHour

<div class="autosummary" data-toctree="api/">

BusinessHour

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BusinessHour.freqstr BusinessHour.kwds BusinessHour.name BusinessHour.nanos BusinessHour.normalize BusinessHour.rule\_code BusinessHour.n BusinessHour.start BusinessHour.end BusinessHour.weekmask BusinessHour.holidays BusinessHour.calendar

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BusinessHour.copy BusinessHour.is\_on\_offset BusinessHour.is\_month\_start BusinessHour.is\_month\_end BusinessHour.is\_quarter\_start BusinessHour.is\_quarter\_end BusinessHour.is\_year\_start BusinessHour.is\_year\_end

</div>

## CustomBusinessDay

<div class="autosummary" data-toctree="api/">

CustomBusinessDay

</div>

Alias:

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

CDay

</div>

### Properties

<div class="autosummary" data-toctree="api/">

CustomBusinessDay.freqstr CustomBusinessDay.kwds CustomBusinessDay.name CustomBusinessDay.nanos CustomBusinessDay.normalize CustomBusinessDay.rule\_code CustomBusinessDay.n CustomBusinessDay.weekmask CustomBusinessDay.calendar CustomBusinessDay.holidays

</div>

### Methods

<div class="autosummary" data-toctree="api/">

CustomBusinessDay.copy CustomBusinessDay.is\_on\_offset CustomBusinessDay.is\_month\_start CustomBusinessDay.is\_month\_end CustomBusinessDay.is\_quarter\_start CustomBusinessDay.is\_quarter\_end CustomBusinessDay.is\_year\_start CustomBusinessDay.is\_year\_end

</div>

## CustomBusinessHour

<div class="autosummary" data-toctree="api/">

CustomBusinessHour

</div>

### Properties

<div class="autosummary" data-toctree="api/">

CustomBusinessHour.freqstr CustomBusinessHour.kwds CustomBusinessHour.name CustomBusinessHour.nanos CustomBusinessHour.normalize CustomBusinessHour.rule\_code CustomBusinessHour.n CustomBusinessHour.weekmask CustomBusinessHour.calendar CustomBusinessHour.holidays CustomBusinessHour.start CustomBusinessHour.end

</div>

### Methods

<div class="autosummary" data-toctree="api/">

CustomBusinessHour.copy CustomBusinessHour.is\_on\_offset CustomBusinessHour.is\_month\_start CustomBusinessHour.is\_month\_end CustomBusinessHour.is\_quarter\_start CustomBusinessHour.is\_quarter\_end CustomBusinessHour.is\_year\_start CustomBusinessHour.is\_year\_end

</div>

## MonthEnd

<div class="autosummary" data-toctree="api/">

MonthEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

MonthEnd.freqstr MonthEnd.kwds MonthEnd.name MonthEnd.nanos MonthEnd.normalize MonthEnd.rule\_code MonthEnd.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

MonthEnd.copy MonthEnd.is\_on\_offset MonthEnd.is\_month\_start MonthEnd.is\_month\_end MonthEnd.is\_quarter\_start MonthEnd.is\_quarter\_end MonthEnd.is\_year\_start MonthEnd.is\_year\_end

</div>

## MonthBegin

<div class="autosummary" data-toctree="api/">

MonthBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

MonthBegin.freqstr MonthBegin.kwds MonthBegin.name MonthBegin.nanos MonthBegin.normalize MonthBegin.rule\_code MonthBegin.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

MonthBegin.copy MonthBegin.is\_on\_offset MonthBegin.is\_month\_start MonthBegin.is\_month\_end MonthBegin.is\_quarter\_start MonthBegin.is\_quarter\_end MonthBegin.is\_year\_start MonthBegin.is\_year\_end

</div>

## BusinessMonthEnd

<div class="autosummary" data-toctree="api/">

BusinessMonthEnd

</div>

Alias:

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

BMonthEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BusinessMonthEnd.freqstr BusinessMonthEnd.kwds BusinessMonthEnd.name BusinessMonthEnd.nanos BusinessMonthEnd.normalize BusinessMonthEnd.rule\_code BusinessMonthEnd.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BusinessMonthEnd.copy BusinessMonthEnd.is\_on\_offset BusinessMonthEnd.is\_month\_start BusinessMonthEnd.is\_month\_end BusinessMonthEnd.is\_quarter\_start BusinessMonthEnd.is\_quarter\_end BusinessMonthEnd.is\_year\_start BusinessMonthEnd.is\_year\_end

</div>

## BusinessMonthBegin

<div class="autosummary" data-toctree="api/">

BusinessMonthBegin

</div>

Alias:

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

BMonthBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BusinessMonthBegin.freqstr BusinessMonthBegin.kwds BusinessMonthBegin.name BusinessMonthBegin.nanos BusinessMonthBegin.normalize BusinessMonthBegin.rule\_code BusinessMonthBegin.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BusinessMonthBegin.copy BusinessMonthBegin.is\_on\_offset BusinessMonthBegin.is\_month\_start BusinessMonthBegin.is\_month\_end BusinessMonthBegin.is\_quarter\_start BusinessMonthBegin.is\_quarter\_end BusinessMonthBegin.is\_year\_start BusinessMonthBegin.is\_year\_end

</div>

## CustomBusinessMonthEnd

<div class="autosummary" data-toctree="api/">

CustomBusinessMonthEnd

</div>

Alias:

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

CBMonthEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

CustomBusinessMonthEnd.freqstr CustomBusinessMonthEnd.kwds CustomBusinessMonthEnd.m\_offset CustomBusinessMonthEnd.name CustomBusinessMonthEnd.nanos CustomBusinessMonthEnd.normalize CustomBusinessMonthEnd.rule\_code CustomBusinessMonthEnd.n CustomBusinessMonthEnd.weekmask CustomBusinessMonthEnd.calendar CustomBusinessMonthEnd.holidays

</div>

### Methods

<div class="autosummary" data-toctree="api/">

CustomBusinessMonthEnd.copy CustomBusinessMonthEnd.is\_on\_offset CustomBusinessMonthEnd.is\_month\_start CustomBusinessMonthEnd.is\_month\_end CustomBusinessMonthEnd.is\_quarter\_start CustomBusinessMonthEnd.is\_quarter\_end CustomBusinessMonthEnd.is\_year\_start CustomBusinessMonthEnd.is\_year\_end

</div>

## CustomBusinessMonthBegin

<div class="autosummary" data-toctree="api/">

CustomBusinessMonthBegin

</div>

Alias:

<div class="autosummary" data-toctree="api/" data-template="autosummary/class_without_autosummary.rst">

CBMonthBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

CustomBusinessMonthBegin.freqstr CustomBusinessMonthBegin.kwds CustomBusinessMonthBegin.m\_offset CustomBusinessMonthBegin.name CustomBusinessMonthBegin.nanos CustomBusinessMonthBegin.normalize CustomBusinessMonthBegin.rule\_code CustomBusinessMonthBegin.n CustomBusinessMonthBegin.weekmask CustomBusinessMonthBegin.calendar CustomBusinessMonthBegin.holidays

</div>

### Methods

<div class="autosummary" data-toctree="api/">

CustomBusinessMonthBegin.copy CustomBusinessMonthBegin.is\_on\_offset CustomBusinessMonthBegin.is\_month\_start CustomBusinessMonthBegin.is\_month\_end CustomBusinessMonthBegin.is\_quarter\_start CustomBusinessMonthBegin.is\_quarter\_end CustomBusinessMonthBegin.is\_year\_start CustomBusinessMonthBegin.is\_year\_end

</div>

## SemiMonthEnd

<div class="autosummary" data-toctree="api/">

SemiMonthEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

SemiMonthEnd.freqstr SemiMonthEnd.kwds SemiMonthEnd.name SemiMonthEnd.nanos SemiMonthEnd.normalize SemiMonthEnd.rule\_code SemiMonthEnd.n SemiMonthEnd.day\_of\_month

</div>

### Methods

<div class="autosummary" data-toctree="api/">

SemiMonthEnd.copy SemiMonthEnd.is\_on\_offset SemiMonthEnd.is\_month\_start SemiMonthEnd.is\_month\_end SemiMonthEnd.is\_quarter\_start SemiMonthEnd.is\_quarter\_end SemiMonthEnd.is\_year\_start SemiMonthEnd.is\_year\_end

</div>

## SemiMonthBegin

<div class="autosummary" data-toctree="api/">

SemiMonthBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

SemiMonthBegin.freqstr SemiMonthBegin.kwds SemiMonthBegin.name SemiMonthBegin.nanos SemiMonthBegin.normalize SemiMonthBegin.rule\_code SemiMonthBegin.n SemiMonthBegin.day\_of\_month

</div>

### Methods

<div class="autosummary" data-toctree="api/">

SemiMonthBegin.copy SemiMonthBegin.is\_on\_offset SemiMonthBegin.is\_month\_start SemiMonthBegin.is\_month\_end SemiMonthBegin.is\_quarter\_start SemiMonthBegin.is\_quarter\_end SemiMonthBegin.is\_year\_start SemiMonthBegin.is\_year\_end

</div>

## Week

<div class="autosummary" data-toctree="api/">

Week

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Week.freqstr Week.kwds Week.name Week.nanos Week.normalize Week.rule\_code Week.n Week.weekday

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Week.copy Week.is\_on\_offset Week.is\_month\_start Week.is\_month\_end Week.is\_quarter\_start Week.is\_quarter\_end Week.is\_year\_start Week.is\_year\_end

</div>

## WeekOfMonth

<div class="autosummary" data-toctree="api/">

WeekOfMonth

</div>

### Properties

<div class="autosummary" data-toctree="api/">

WeekOfMonth.freqstr WeekOfMonth.kwds WeekOfMonth.name WeekOfMonth.nanos WeekOfMonth.normalize WeekOfMonth.rule\_code WeekOfMonth.n WeekOfMonth.week

</div>

### Methods

<div class="autosummary" data-toctree="api/">

WeekOfMonth.copy WeekOfMonth.is\_on\_offset WeekOfMonth.weekday WeekOfMonth.is\_month\_start WeekOfMonth.is\_month\_end WeekOfMonth.is\_quarter\_start WeekOfMonth.is\_quarter\_end WeekOfMonth.is\_year\_start WeekOfMonth.is\_year\_end

</div>

## LastWeekOfMonth

<div class="autosummary" data-toctree="api/">

LastWeekOfMonth

</div>

### Properties

<div class="autosummary" data-toctree="api/">

LastWeekOfMonth.freqstr LastWeekOfMonth.kwds LastWeekOfMonth.name LastWeekOfMonth.nanos LastWeekOfMonth.normalize LastWeekOfMonth.rule\_code LastWeekOfMonth.n LastWeekOfMonth.weekday LastWeekOfMonth.week

</div>

### Methods

<div class="autosummary" data-toctree="api/">

LastWeekOfMonth.copy LastWeekOfMonth.is\_on\_offset LastWeekOfMonth.is\_month\_start LastWeekOfMonth.is\_month\_end LastWeekOfMonth.is\_quarter\_start LastWeekOfMonth.is\_quarter\_end LastWeekOfMonth.is\_year\_start LastWeekOfMonth.is\_year\_end

</div>

## BQuarterEnd

<div class="autosummary" data-toctree="api/">

BQuarterEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BQuarterEnd.freqstr BQuarterEnd.kwds BQuarterEnd.name BQuarterEnd.nanos BQuarterEnd.normalize BQuarterEnd.rule\_code BQuarterEnd.n BQuarterEnd.startingMonth

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BQuarterEnd.copy BQuarterEnd.is\_on\_offset BQuarterEnd.is\_month\_start BQuarterEnd.is\_month\_end BQuarterEnd.is\_quarter\_start BQuarterEnd.is\_quarter\_end BQuarterEnd.is\_year\_start BQuarterEnd.is\_year\_end

</div>

## BQuarterBegin

<div class="autosummary" data-toctree="api/">

BQuarterBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BQuarterBegin.freqstr BQuarterBegin.kwds BQuarterBegin.name BQuarterBegin.nanos BQuarterBegin.normalize BQuarterBegin.rule\_code BQuarterBegin.n BQuarterBegin.startingMonth

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BQuarterBegin.copy BQuarterBegin.is\_on\_offset BQuarterBegin.is\_month\_start BQuarterBegin.is\_month\_end BQuarterBegin.is\_quarter\_start BQuarterBegin.is\_quarter\_end BQuarterBegin.is\_year\_start BQuarterBegin.is\_year\_end

</div>

## QuarterEnd

<div class="autosummary" data-toctree="api/">

QuarterEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

QuarterEnd.freqstr QuarterEnd.kwds QuarterEnd.name QuarterEnd.nanos QuarterEnd.normalize QuarterEnd.rule\_code QuarterEnd.n QuarterEnd.startingMonth

</div>

### Methods

<div class="autosummary" data-toctree="api/">

QuarterEnd.copy QuarterEnd.is\_on\_offset QuarterEnd.is\_month\_start QuarterEnd.is\_month\_end QuarterEnd.is\_quarter\_start QuarterEnd.is\_quarter\_end QuarterEnd.is\_year\_start QuarterEnd.is\_year\_end

</div>

## QuarterBegin

<div class="autosummary" data-toctree="api/">

QuarterBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

QuarterBegin.freqstr QuarterBegin.kwds QuarterBegin.name QuarterBegin.nanos QuarterBegin.normalize QuarterBegin.rule\_code QuarterBegin.n QuarterBegin.startingMonth

</div>

### Methods

<div class="autosummary" data-toctree="api/">

QuarterBegin.copy QuarterBegin.is\_on\_offset QuarterBegin.is\_month\_start QuarterBegin.is\_month\_end QuarterBegin.is\_quarter\_start QuarterBegin.is\_quarter\_end QuarterBegin.is\_year\_start QuarterBegin.is\_year\_end

</div>

## BYearEnd

<div class="autosummary" data-toctree="api/">

BYearEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BYearEnd.freqstr BYearEnd.kwds BYearEnd.name BYearEnd.nanos BYearEnd.normalize BYearEnd.rule\_code BYearEnd.n BYearEnd.month

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BYearEnd.copy BYearEnd.is\_on\_offset BYearEnd.is\_month\_start BYearEnd.is\_month\_end BYearEnd.is\_quarter\_start BYearEnd.is\_quarter\_end BYearEnd.is\_year\_start BYearEnd.is\_year\_end

</div>

## BYearBegin

<div class="autosummary" data-toctree="api/">

BYearBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

BYearBegin.freqstr BYearBegin.kwds BYearBegin.name BYearBegin.nanos BYearBegin.normalize BYearBegin.rule\_code BYearBegin.n BYearBegin.month

</div>

### Methods

<div class="autosummary" data-toctree="api/">

BYearBegin.copy BYearBegin.is\_on\_offset BYearBegin.is\_month\_start BYearBegin.is\_month\_end BYearBegin.is\_quarter\_start BYearBegin.is\_quarter\_end BYearBegin.is\_year\_start BYearBegin.is\_year\_end

</div>

## YearEnd

<div class="autosummary" data-toctree="api/">

YearEnd

</div>

### Properties

<div class="autosummary" data-toctree="api/">

YearEnd.freqstr YearEnd.kwds YearEnd.name YearEnd.nanos YearEnd.normalize YearEnd.rule\_code YearEnd.n YearEnd.month

</div>

### Methods

<div class="autosummary" data-toctree="api/">

YearEnd.copy YearEnd.is\_on\_offset YearEnd.is\_month\_start YearEnd.is\_month\_end YearEnd.is\_quarter\_start YearEnd.is\_quarter\_end YearEnd.is\_year\_start YearEnd.is\_year\_end

</div>

## YearBegin

<div class="autosummary" data-toctree="api/">

YearBegin

</div>

### Properties

<div class="autosummary" data-toctree="api/">

YearBegin.freqstr YearBegin.kwds YearBegin.name YearBegin.nanos YearBegin.normalize YearBegin.rule\_code YearBegin.n YearBegin.month

</div>

### Methods

<div class="autosummary" data-toctree="api/">

YearBegin.copy YearBegin.is\_on\_offset YearBegin.is\_month\_start YearBegin.is\_month\_end YearBegin.is\_quarter\_start YearBegin.is\_quarter\_end YearBegin.is\_year\_start YearBegin.is\_year\_end

</div>

## FY5253

<div class="autosummary" data-toctree="api/">

FY5253

</div>

### Properties

<div class="autosummary" data-toctree="api/">

FY5253.freqstr FY5253.kwds FY5253.name FY5253.nanos FY5253.normalize FY5253.rule\_code FY5253.n FY5253.startingMonth FY5253.variation FY5253.weekday

</div>

### Methods

<div class="autosummary" data-toctree="api/">

FY5253.copy FY5253.get\_rule\_code\_suffix FY5253.get\_year\_end FY5253.is\_on\_offset FY5253.is\_month\_start FY5253.is\_month\_end FY5253.is\_quarter\_start FY5253.is\_quarter\_end FY5253.is\_year\_start FY5253.is\_year\_end

</div>

## FY5253Quarter

<div class="autosummary" data-toctree="api/">

FY5253Quarter

</div>

### Properties

<div class="autosummary" data-toctree="api/">

FY5253Quarter.freqstr FY5253Quarter.kwds FY5253Quarter.name FY5253Quarter.nanos FY5253Quarter.normalize FY5253Quarter.rule\_code FY5253Quarter.n FY5253Quarter.qtr\_with\_extra\_week FY5253Quarter.startingMonth FY5253Quarter.variation FY5253Quarter.weekday

</div>

### Methods

<div class="autosummary" data-toctree="api/">

FY5253Quarter.copy FY5253Quarter.get\_rule\_code\_suffix FY5253Quarter.get\_weeks FY5253Quarter.is\_on\_offset FY5253Quarter.year\_has\_extra\_week FY5253Quarter.is\_month\_start FY5253Quarter.is\_month\_end FY5253Quarter.is\_quarter\_start FY5253Quarter.is\_quarter\_end FY5253Quarter.is\_year\_start FY5253Quarter.is\_year\_end

</div>

## Easter

<div class="autosummary" data-toctree="api/">

Easter

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Easter.freqstr Easter.kwds Easter.name Easter.nanos Easter.normalize Easter.rule\_code Easter.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Easter.copy Easter.is\_on\_offset Easter.is\_month\_start Easter.is\_month\_end Easter.is\_quarter\_start Easter.is\_quarter\_end Easter.is\_year\_start Easter.is\_year\_end

</div>

## Tick

<div class="autosummary" data-toctree="api/">

Tick

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Tick.freqstr Tick.kwds Tick.name Tick.nanos Tick.normalize Tick.rule\_code Tick.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Tick.copy Tick.is\_on\_offset Tick.is\_month\_start Tick.is\_month\_end Tick.is\_quarter\_start Tick.is\_quarter\_end Tick.is\_year\_start Tick.is\_year\_end

</div>

## Day

<div class="autosummary" data-toctree="api/">

Day

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Day.freqstr Day.kwds Day.name Day.nanos Day.normalize Day.rule\_code Day.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Day.copy Day.is\_on\_offset Day.is\_month\_start Day.is\_month\_end Day.is\_quarter\_start Day.is\_quarter\_end Day.is\_year\_start Day.is\_year\_end

</div>

## Hour

<div class="autosummary" data-toctree="api/">

Hour

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Hour.freqstr Hour.kwds Hour.name Hour.nanos Hour.normalize Hour.rule\_code Hour.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Hour.copy Hour.is\_on\_offset Hour.is\_month\_start Hour.is\_month\_end Hour.is\_quarter\_start Hour.is\_quarter\_end Hour.is\_year\_start Hour.is\_year\_end

</div>

## Minute

<div class="autosummary" data-toctree="api/">

Minute

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Minute.freqstr Minute.kwds Minute.name Minute.nanos Minute.normalize Minute.rule\_code Minute.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Minute.copy Minute.is\_on\_offset Minute.is\_month\_start Minute.is\_month\_end Minute.is\_quarter\_start Minute.is\_quarter\_end Minute.is\_year\_start Minute.is\_year\_end

</div>

## Second

<div class="autosummary" data-toctree="api/">

Second

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Second.freqstr Second.kwds Second.name Second.nanos Second.normalize Second.rule\_code Second.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Second.copy Second.is\_on\_offset Second.is\_month\_start Second.is\_month\_end Second.is\_quarter\_start Second.is\_quarter\_end Second.is\_year\_start Second.is\_year\_end

</div>

## Milli

<div class="autosummary" data-toctree="api/">

Milli

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Milli.freqstr Milli.kwds Milli.name Milli.nanos Milli.normalize Milli.rule\_code Milli.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Milli.copy Milli.is\_on\_offset Milli.is\_month\_start Milli.is\_month\_end Milli.is\_quarter\_start Milli.is\_quarter\_end Milli.is\_year\_start Milli.is\_year\_end

</div>

## Micro

<div class="autosummary" data-toctree="api/">

Micro

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Micro.freqstr Micro.kwds Micro.name Micro.nanos Micro.normalize Micro.rule\_code Micro.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Micro.copy Micro.is\_on\_offset Micro.is\_month\_start Micro.is\_month\_end Micro.is\_quarter\_start Micro.is\_quarter\_end Micro.is\_year\_start Micro.is\_year\_end

</div>

## Nano

<div class="autosummary" data-toctree="api/">

Nano

</div>

### Properties

<div class="autosummary" data-toctree="api/">

Nano.freqstr Nano.kwds Nano.name Nano.nanos Nano.normalize Nano.rule\_code Nano.n

</div>

### Methods

<div class="autosummary" data-toctree="api/">

Nano.copy Nano.is\_on\_offset Nano.is\_month\_start Nano.is\_month\_end Nano.is\_quarter\_start Nano.is\_quarter\_end Nano.is\_year\_start Nano.is\_year\_end

</div>

# Frequencies

<div class="currentmodule">

pandas.tseries.frequencies

</div>

<div id="api.offsets">

<div class="autosummary" data-toctree="api/">

to\_offset

</div>

</div>

---

options.md

---

{{ header }}

# Options and settings

<div class="currentmodule">

pandas

</div>

API for configuring global behavior. See \[the User Guide \<options\>\](\#the-user-guide-\<options\>) for more.

## Working with options

<div class="autosummary" data-toctree="api/">

describe\_option reset\_option get\_option set\_option option\_context

</div>

## Numeric formatting

<div class="autosummary" data-toctree="api/">

set\_eng\_float\_format

</div>

---

plotting.md

---

{{ header }}

# Plotting

<div class="currentmodule">

pandas.plotting

</div>

The following functions are contained in the `pandas.plotting` module.

<div class="autosummary" data-toctree="api/">

andrews\_curves autocorrelation\_plot bootstrap\_plot boxplot deregister\_matplotlib\_converters lag\_plot parallel\_coordinates plot\_params radviz register\_matplotlib\_converters scatter\_matrix table

</div>

---

resampling.md

---

{{ header }}

# Resampling

<div class="currentmodule">

pandas.core.resample

</div>

<span class="title-ref">pandas.api.typing.Resampler</span> instances are returned by resample calls: <span class="title-ref">pandas.DataFrame.resample</span>, <span class="title-ref">pandas.Series.resample</span>.

## Indexing, iteration

<div class="autosummary" data-toctree="api/">

Resampler.\_\_iter\_\_ Resampler.groups Resampler.indices Resampler.get\_group

</div>

## Function application

<div class="autosummary" data-toctree="api/">

Resampler.apply Resampler.aggregate Resampler.transform Resampler.pipe

</div>

## Upsampling

<div class="autosummary" data-toctree="api/">

Resampler.ffill Resampler.bfill Resampler.nearest Resampler.asfreq Resampler.interpolate

</div>

## Computations / descriptive stats

<div class="autosummary" data-toctree="api/">

Resampler.count Resampler.nunique Resampler.first Resampler.last Resampler.max Resampler.mean Resampler.median Resampler.min Resampler.ohlc Resampler.prod Resampler.size Resampler.sem Resampler.std Resampler.sum Resampler.var Resampler.quantile

</div>

---

series.md

---

{{ header }}

# Series

<div class="currentmodule">

pandas

</div>

## Constructor

<div class="autosummary" data-toctree="api/">

Series

</div>

## Attributes

**Axes**

<div class="autosummary" data-toctree="api/">

Series.index Series.array Series.values Series.dtype Series.shape Series.nbytes Series.ndim Series.size Series.T Series.memory\_usage Series.hasnans Series.empty Series.dtypes Series.name Series.flags Series.set\_flags

</div>

## Conversion

<div class="autosummary" data-toctree="api/">

Series.astype Series.convert\_dtypes Series.infer\_objects Series.copy Series.to\_numpy Series.to\_period Series.to\_timestamp Series.to\_list Series.\_\_array\_\_

</div>

## Indexing, iteration

<div class="autosummary" data-toctree="api/">

Series.get Series.at Series.iat Series.loc Series.iloc Series.\_\_iter\_\_ Series.items Series.keys Series.pop Series.item Series.xs

</div>

For more information on `.at`, `.iat`, `.loc`, and `.iloc`, see the \[indexing documentation \<indexing\>\](\#indexing-documentation-\<indexing\>).

## Binary operator functions

<div class="autosummary" data-toctree="api/">

Series.add Series.sub Series.mul Series.div Series.truediv Series.floordiv Series.mod Series.pow Series.radd Series.rsub Series.rmul Series.rdiv Series.rtruediv Series.rfloordiv Series.rmod Series.rpow Series.combine Series.combine\_first Series.round Series.lt Series.gt Series.le Series.ge Series.ne Series.eq Series.product Series.dot

</div>

## Function application, GroupBy & window

<div class="autosummary" data-toctree="api/">

Series.apply Series.agg Series.aggregate Series.transform Series.map Series.groupby Series.rolling Series.expanding Series.ewm Series.pipe

</div>

## Computations / descriptive stats

<div class="autosummary" data-toctree="api/">

Series.abs Series.all Series.any Series.autocorr Series.between Series.clip Series.corr Series.count Series.cov Series.cummax Series.cummin Series.cumprod Series.cumsum Series.describe Series.diff Series.factorize Series.kurt Series.max Series.mean Series.median Series.min Series.mode Series.nlargest Series.nsmallest Series.pct\_change Series.prod Series.quantile Series.rank Series.sem Series.skew Series.std Series.sum Series.var Series.kurtosis Series.unique Series.nunique Series.is\_unique Series.is\_monotonic\_increasing Series.is\_monotonic\_decreasing Series.value\_counts

</div>

## Reindexing / selection / label manipulation

<div class="autosummary" data-toctree="api/">

Series.align Series.case\_when Series.drop Series.droplevel Series.drop\_duplicates Series.duplicated Series.equals Series.head Series.idxmax Series.idxmin Series.isin Series.reindex Series.reindex\_like Series.rename Series.rename\_axis Series.reset\_index Series.sample Series.set\_axis Series.take Series.tail Series.truncate Series.where Series.mask Series.add\_prefix Series.add\_suffix Series.filter

</div>

## Missing data handling

<div class="autosummary" data-toctree="api/">

Series.bfill Series.dropna Series.ffill Series.fillna Series.interpolate Series.isna Series.isnull Series.notna Series.notnull Series.replace

</div>

## Reshaping, sorting

<div class="autosummary" data-toctree="api/">

Series.argsort Series.argmin Series.argmax Series.reorder\_levels Series.sort\_values Series.sort\_index Series.swaplevel Series.unstack Series.explode Series.searchsorted Series.repeat Series.squeeze

</div>

## Combining / comparing / joining / merging

<div class="autosummary" data-toctree="api/">

Series.compare Series.update

</div>

## Time Series-related

<div class="autosummary" data-toctree="api/">

Series.asfreq Series.asof Series.shift Series.first\_valid\_index Series.last\_valid\_index Series.resample Series.tz\_convert Series.tz\_localize Series.at\_time Series.between\_time

</div>

## Accessors

pandas provides dtype-specific methods under various accessors. These are separate namespaces within <span class="title-ref">Series</span> that only apply to specific data types.

<div class="autosummary" data-toctree="api/" data-nosignatures="" data-template="autosummary/accessor.rst">

Series.str Series.cat Series.dt Series.sparse DataFrame.sparse Index.str

</div>

| Data Type                   | Accessor                                                         |
| --------------------------- | ---------------------------------------------------------------- |
| Datetime, Timedelta, Period | \[dt \<api.series.dt\>\](\#dt-\<api.series.dt\>)                 |
| String                      | \[str \<api.series.str\>\](\#str-\<api.series.str\>)             |
| Categorical                 | \[cat \<api.series.cat\>\](\#cat-\<api.series.cat\>)             |
| Sparse                      | \[sparse \<api.series.sparse\>\](\#sparse-\<api.series.sparse\>) |

### Datetimelike properties

`Series.dt` can be used to access the values of the series as datetimelike and return several properties. These can be accessed like `Series.dt.<property>`.

#### Datetime properties

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

Series.dt.date Series.dt.time Series.dt.timetz Series.dt.year Series.dt.month Series.dt.day Series.dt.hour Series.dt.minute Series.dt.second Series.dt.microsecond Series.dt.nanosecond Series.dt.dayofweek Series.dt.day\_of\_week Series.dt.weekday Series.dt.dayofyear Series.dt.day\_of\_year Series.dt.days\_in\_month Series.dt.quarter Series.dt.is\_month\_start Series.dt.is\_month\_end Series.dt.is\_quarter\_start Series.dt.is\_quarter\_end Series.dt.is\_year\_start Series.dt.is\_year\_end Series.dt.is\_leap\_year Series.dt.daysinmonth Series.dt.days\_in\_month Series.dt.tz Series.dt.freq Series.dt.unit

</div>

#### Datetime methods

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.dt.isocalendar Series.dt.to\_period Series.dt.to\_pydatetime Series.dt.tz\_localize Series.dt.tz\_convert Series.dt.normalize Series.dt.strftime Series.dt.round Series.dt.floor Series.dt.ceil Series.dt.month\_name Series.dt.day\_name Series.dt.as\_unit

</div>

#### Period properties

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

Series.dt.qyear Series.dt.start\_time Series.dt.end\_time

</div>

#### Timedelta properties

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

Series.dt.days Series.dt.seconds Series.dt.microseconds Series.dt.nanoseconds Series.dt.components Series.dt.unit

</div>

#### Timedelta methods

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.dt.to\_pytimedelta Series.dt.total\_seconds Series.dt.as\_unit

</div>

### String handling

`Series.str` can be used to access the values of the series as strings and apply several methods to it. These can be accessed like `Series.str.<function/property>`.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.str.capitalize Series.str.casefold Series.str.cat Series.str.center Series.str.contains Series.str.count Series.str.decode Series.str.encode Series.str.endswith Series.str.extract Series.str.extractall Series.str.find Series.str.findall Series.str.fullmatch Series.str.get Series.str.index Series.str.join Series.str.len Series.str.ljust Series.str.lower Series.str.lstrip Series.str.match Series.str.normalize Series.str.pad Series.str.partition Series.str.removeprefix Series.str.removesuffix Series.str.repeat Series.str.replace Series.str.rfind Series.str.rindex Series.str.rjust Series.str.rpartition Series.str.rstrip Series.str.slice Series.str.slice\_replace Series.str.split Series.str.rsplit Series.str.startswith Series.str.strip Series.str.swapcase Series.str.title Series.str.translate Series.str.upper Series.str.wrap Series.str.zfill Series.str.isalnum Series.str.isalpha Series.str.isdigit Series.str.isspace Series.str.islower Series.str.isupper Series.str.istitle Series.str.isnumeric Series.str.isdecimal Series.str.get\_dummies

</div>

### Categorical accessor

Categorical-dtype specific methods and attributes are available under the `Series.cat` accessor.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

Series.cat.categories Series.cat.ordered Series.cat.codes

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.cat.rename\_categories Series.cat.reorder\_categories Series.cat.add\_categories Series.cat.remove\_categories Series.cat.remove\_unused\_categories Series.cat.set\_categories Series.cat.as\_ordered Series.cat.as\_unordered

</div>

### Sparse accessor

Sparse-dtype specific methods and attributes are provided under the `Series.sparse` accessor.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

Series.sparse.npoints Series.sparse.density Series.sparse.fill\_value Series.sparse.sp\_values

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.sparse.from\_coo Series.sparse.to\_coo

</div>

### List accessor

Arrow list-dtype specific methods and attributes are provided under the `Series.list` accessor.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.list.flatten Series.list.len Series.list.\_\_getitem\_\_

</div>

### Struct accessor

Arrow struct-dtype specific methods and attributes are provided under the `Series.struct` accessor.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_attribute.rst">

Series.struct.dtypes

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.struct.field Series.struct.explode

</div>

### Flags

Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in <span class="title-ref">Series.attrs</span>.

<div class="autosummary" data-toctree="api/">

Flags

</div>

### Metadata

<span class="title-ref">Series.attrs</span> is a dictionary for storing global metadata for this Series.

<div class="warning">

<div class="title">

Warning

</div>

`Series.attrs` is considered experimental and may change without warning.

</div>

<div class="autosummary" data-toctree="api/">

Series.attrs

</div>

## Plotting

`Series.plot` is both a callable method and a namespace attribute for specific plotting methods of the form `Series.plot.<kind>`.

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_callable.rst">

Series.plot

</div>

<div class="autosummary" data-toctree="api/" data-template="autosummary/accessor_method.rst">

Series.plot.area Series.plot.bar Series.plot.barh Series.plot.box Series.plot.density Series.plot.hist Series.plot.kde Series.plot.line Series.plot.pie

</div>

<div class="autosummary" data-toctree="api/">

Series.hist

</div>

## Serialization / IO / conversion

<div class="autosummary" data-toctree="api/">

Series.to\_pickle Series.to\_csv Series.to\_dict Series.to\_excel Series.to\_frame Series.to\_xarray Series.to\_hdf Series.to\_sql Series.to\_json Series.to\_string Series.to\_clipboard Series.to\_latex Series.to\_markdown

</div>

---

style.md

---

{{ header }}

# Style

<div class="currentmodule">

pandas.io.formats.style

</div>

`Styler` objects are returned by <span class="title-ref">pandas.DataFrame.style</span>.

## Styler constructor

<div class="autosummary" data-toctree="api/">

Styler Styler.from\_custom\_template

</div>

## Styler properties

<div class="autosummary" data-toctree="api/">

Styler.env Styler.template\_html Styler.template\_html\_style Styler.template\_html\_table Styler.template\_latex Styler.template\_string Styler.loader

</div>

## Style application

<div class="autosummary" data-toctree="api/">

Styler.apply Styler.map Styler.apply\_index Styler.map\_index Styler.format Styler.format\_index Styler.format\_index\_names Styler.relabel\_index Styler.hide Styler.concat Styler.set\_td\_classes Styler.set\_table\_styles Styler.set\_table\_attributes Styler.set\_tooltips Styler.set\_caption Styler.set\_sticky Styler.set\_properties Styler.set\_uuid Styler.clear Styler.pipe

</div>

## Builtin styles

<div class="autosummary" data-toctree="api/">

Styler.highlight\_null Styler.highlight\_max Styler.highlight\_min Styler.highlight\_between Styler.highlight\_quantile Styler.background\_gradient Styler.text\_gradient Styler.bar

</div>

## Style export and import

<div class="autosummary" data-toctree="api/">

Styler.to\_html Styler.to\_latex Styler.to\_excel Styler.to\_string Styler.export Styler.use

</div>

---

testing.md

---

{{ header }}

# Testing

<div class="currentmodule">

pandas

</div>

## Assertion functions

<div class="autosummary" data-toctree="api/">

testing.assert\_frame\_equal testing.assert\_series\_equal testing.assert\_index\_equal testing.assert\_extension\_array\_equal

</div>

## Exceptions and warnings

<div class="autosummary" data-toctree="api/">

errors.AbstractMethodError errors.AttributeConflictWarning errors.CategoricalConversionWarning errors.ChainedAssignmentError errors.ClosedFileError errors.CSSWarning errors.DatabaseError errors.DataError errors.DtypeWarning errors.DuplicateLabelError errors.EmptyDataError errors.IncompatibilityWarning errors.IndexingError errors.InvalidColumnName errors.InvalidComparison errors.InvalidIndexError errors.InvalidVersion errors.IntCastingNaNError errors.LossySetitemError errors.MergeError errors.NoBufferPresent errors.NullFrequencyError errors.NumbaUtilError errors.NumExprClobberingError errors.OptionError errors.OutOfBoundsDatetime errors.OutOfBoundsTimedelta errors.ParserError errors.ParserWarning errors.PerformanceWarning errors.PossibleDataLossError errors.PossiblePrecisionLoss errors.PyperclipException errors.PyperclipWindowsException errors.SpecificationError errors.UndefinedVariableError errors.UnsortedIndexError errors.UnsupportedFunctionCall errors.ValueLabelTypeMismatch

</div>

## Bug report function

<div class="autosummary" data-toctree="api/">

show\_versions

</div>

## Test suite runner

<div class="autosummary" data-toctree="api/">

test

</div>

---

window.md

---

{{ header }}

# Window

<span class="title-ref">pandas.api.typing.Rolling</span> instances are returned by `.rolling` calls: <span class="title-ref">pandas.DataFrame.rolling</span> and <span class="title-ref">pandas.Series.rolling</span>. <span class="title-ref">pandas.api.typing.Expanding</span> instances are returned by `.expanding` calls: <span class="title-ref">pandas.DataFrame.expanding</span> and <span class="title-ref">pandas.Series.expanding</span>. <span class="title-ref">pandas.api.typing.ExponentialMovingWindow</span> instances are returned by `.ewm` calls: <span class="title-ref">pandas.DataFrame.ewm</span> and <span class="title-ref">pandas.Series.ewm</span>.

## Rolling window functions

<div class="currentmodule">

pandas.core.window.rolling

</div>

<div class="autosummary" data-toctree="api/">

Rolling.count Rolling.sum Rolling.mean Rolling.median Rolling.var Rolling.std Rolling.min Rolling.max Rolling.corr Rolling.cov Rolling.skew Rolling.kurt Rolling.apply Rolling.aggregate Rolling.quantile Rolling.sem Rolling.rank

</div>

## Weighted window functions

<div class="currentmodule">

pandas.core.window.rolling

</div>

<div class="autosummary" data-toctree="api/">

Window.mean Window.sum Window.var Window.std

</div>

## Expanding window functions

<div class="currentmodule">

pandas.core.window.expanding

</div>

<div class="autosummary" data-toctree="api/">

Expanding.count Expanding.sum Expanding.mean Expanding.median Expanding.var Expanding.std Expanding.min Expanding.max Expanding.corr Expanding.cov Expanding.skew Expanding.kurt Expanding.apply Expanding.aggregate Expanding.quantile Expanding.sem Expanding.rank

</div>

## Exponentially-weighted window functions

<div class="currentmodule">

pandas.core.window.ewm

</div>

<div class="autosummary" data-toctree="api/">

ExponentialMovingWindow.mean ExponentialMovingWindow.sum ExponentialMovingWindow.std ExponentialMovingWindow.var ExponentialMovingWindow.corr ExponentialMovingWindow.cov

</div>

## Window indexer

<div class="currentmodule">

pandas

</div>

Base class for defining custom window boundaries.

<div class="autosummary" data-toctree="api/">

api.indexers.BaseIndexer api.indexers.FixedForwardWindowIndexer api.indexers.VariableOffsetWindowIndexer

</div>

---

10min.md

---

<div id="10min">

{{ header }}

</div>

# 10 minutes to pandas

This is a short introduction to pandas, geared mainly for new users. You can see more complex recipes in the \[Cookbook\<cookbook\>\](\#cookbook\<cookbook\>).

Customarily, we import as follows:

<div class="ipython">

python

import numpy as np import pandas as pd

</div>

## Basic data structures in pandas

pandas provides two types of classes for handling data:

1.    - \`Series\`: a one-dimensional labeled array holding data of any type  
        such as integers, strings, Python objects etc.

2.  \`DataFrame\`: a two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns.

## Object creation

See the \[Intro to data structures section \<dsintro\>\](\#intro-to-data-structures-section-\<dsintro\>).

Creating a <span class="title-ref">Series</span> by passing a list of values, letting pandas create a default <span class="title-ref">RangeIndex</span>.

<div class="ipython">

python

s = pd.Series(\[1, 3, 5, np.nan, 6, 8\]) s

</div>

Creating a <span class="title-ref">DataFrame</span> by passing a NumPy array with a datetime index using <span class="title-ref">date\_range</span> and labeled columns:

<div class="ipython">

python

dates = pd.date\_range("20130101", periods=6) dates df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list("ABCD")) df

</div>

Creating a <span class="title-ref">DataFrame</span> by passing a dictionary of objects where the keys are the column labels and the values are the column values.

<div class="ipython">

python

  - df2 = pd.DataFrame(
    
      - {  
        "A": 1.0, "B": pd.Timestamp("20130102"), "C": pd.Series(1, index=list(range(4)), dtype="float32"), "D": np.array(\[3\] \* 4, dtype="int32"), "E": pd.Categorical(\["test", "train", "test", "train"\]), "F": "foo",
    
    }

) df2

</div>

The columns of the resulting <span class="title-ref">DataFrame</span> have different \[dtypes \<basics.dtypes\>\](\#dtypes-\<basics.dtypes\>):

<div class="ipython">

python

df2.dtypes

</div>

If you're using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here's a subset of the attributes that will be completed:

<div class="ipython">

@verbatim In \[1\]: df2.\<TAB\> \# noqa: E225, E999 df2.A df2.bool df2.abs df2.boxplot df2.add df2.C df2.add\_prefix df2.clip df2.add\_suffix df2.columns df2.align df2.copy df2.all df2.count df2.any df2.combine df2.append df2.D df2.apply df2.describe df2.B df2.duplicated df2.diff

</div>

As you can see, the columns `A`, `B`, `C`, and `D` are automatically tab completed. `E` and `F` are there as well; the rest of the attributes have been truncated for brevity.

## Viewing data

See the \[Essential basic functionality section \<basics\>\](\#essential-basic-functionality-section-\<basics\>).

Use <span class="title-ref">DataFrame.head</span> and <span class="title-ref">DataFrame.tail</span> to view the top and bottom rows of the frame respectively:

<div class="ipython">

python

df.head() df.tail(3)

</div>

Display the <span class="title-ref">DataFrame.index</span> or \`DataFrame.columns\`:

<div class="ipython">

python

df.index df.columns

</div>

Return a NumPy representation of the underlying data with <span class="title-ref">DataFrame.to\_numpy</span> without the index or column labels:

<div class="ipython">

python

df.to\_numpy()

</div>

\> **Note** \> **NumPy arrays have one dtype for the entire array while pandas DataFrames have one dtype per column**. When you call <span class="title-ref">DataFrame.to\_numpy</span>, pandas will find the NumPy dtype that can hold *all* of the dtypes in the DataFrame. If the common data type is `object`, <span class="title-ref">DataFrame.to\_numpy</span> will require copying data.

> 
> 
> <div class="ipython">
> 
> python
> 
> df2.dtypes df2.to\_numpy()
> 
> </div>

<span class="title-ref">\~DataFrame.describe</span> shows a quick statistic summary of your data:

<div class="ipython">

python

df.describe()

</div>

Transposing your data:

<div class="ipython">

python

df.T

</div>

<span class="title-ref">DataFrame.sort\_index</span> sorts by an axis:

<div class="ipython">

python

df.sort\_index(axis=1, ascending=False)

</div>

<span class="title-ref">DataFrame.sort\_values</span> sorts by values:

<div class="ipython">

python

df.sort\_values(by="B")

</div>

## Selection

\> **Note** \> While standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, <span class="title-ref">DataFrame.at</span>, <span class="title-ref">DataFrame.iat</span>, <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">DataFrame.iloc</span>.

See the indexing documentation \[Indexing and Selecting Data \<indexing\>\](\#indexing-and-selecting-data-\<indexing\>) and \[MultiIndex / Advanced Indexing \<advanced\>\](\#multiindex-/-advanced-indexing-\<advanced\>).

### Getitem (`[]`)

For a <span class="title-ref">DataFrame</span>, passing a single label selects a column and yields a <span class="title-ref">Series</span> equivalent to `df.A`:

<div class="ipython">

python

df\["A"\]

</div>

For a <span class="title-ref">DataFrame</span>, passing a slice `:` selects matching rows:

<div class="ipython">

python

df\[0:3\] df\["20130102":"20130104"\]

</div>

### Selection by label

See more in \[Selection by Label \<indexing.label\>\](\#selection-by-label-\<indexing.label\>) using <span class="title-ref">DataFrame.loc</span> or <span class="title-ref">DataFrame.at</span>.

Selecting a row matching a label:

<div class="ipython">

python

df.loc\[dates\[0\]\]

</div>

Selecting all rows (`:`) with a select column labels:

<div class="ipython">

python

df.loc\[:, \["A", "B"\]\]

</div>

For label slicing, both endpoints are *included*:

<div class="ipython">

python

df.loc\["20130102":"20130104", \["A", "B"\]\]

</div>

Selecting a single row and column label returns a scalar:

<div class="ipython">

python

df.loc\[dates\[0\], "A"\]

</div>

For getting fast access to a scalar (equivalent to the prior method):

<div class="ipython">

python

df.at\[dates\[0\], "A"\]

</div>

### Selection by position

See more in \[Selection by Position \<indexing.integer\>\](\#selection-by-position-\<indexing.integer\>) using <span class="title-ref">DataFrame.iloc</span> or <span class="title-ref">DataFrame.iat</span>.

Select via the position of the passed integers:

<div class="ipython">

python

df.iloc\[3\]

</div>

Integer slices acts similar to NumPy/Python:

<div class="ipython">

python

df.iloc\[3:5, 0:2\]

</div>

Lists of integer position locations:

<div class="ipython">

python

df.iloc\[\[1, 2, 4\], \[0, 2\]\]

</div>

For slicing rows explicitly:

<div class="ipython">

python

df.iloc\[1:3, :\]

</div>

For slicing columns explicitly:

<div class="ipython">

python

df.iloc\[:, 1:3\]

</div>

For getting a value explicitly:

<div class="ipython">

python

df.iloc\[1, 1\]

</div>

For getting fast access to a scalar (equivalent to the prior method):

<div class="ipython">

python

df.iat\[1, 1\]

</div>

### Boolean indexing

Select rows where `df.A` is greater than `0`.

<div class="ipython">

python

df\[df\["A"\] \> 0\]

</div>

Selecting values from a <span class="title-ref">DataFrame</span> where a boolean condition is met:

<div class="ipython">

python

df\[df \> 0\]

</div>

Using <span class="title-ref">\~Series.isin</span> method for filtering:

<div class="ipython">

python

df2 = df.copy() df2\["E"\] = \["one", "one", "two", "three", "four", "three"\] df2 df2\[df2\["E"\].isin(\["two", "four"\])\]

</div>

### Setting

Setting a new column automatically aligns the data by the indexes:

<div class="ipython">

python

s1 = pd.Series(\[1, 2, 3, 4, 5, 6\], index=pd.date\_range("20130102", periods=6)) s1 df\["F"\] = s1

</div>

Setting values by label:

<div class="ipython">

python

df.at\[dates\[0\], "A"\] = 0

</div>

Setting values by position:

<div class="ipython">

python

df.iat\[0, 1\] = 0

</div>

Setting by assigning with a NumPy array:

<div class="ipython" data-okwarning="">

python

df.loc\[:, "D"\] = np.array(\[5\] \* len(df))

</div>

The result of the prior setting operations:

<div class="ipython">

python

df

</div>

A `where` operation with setting:

<div class="ipython">

python

df2 = df.copy() df2\[df2 \> 0\] = -df2 df2

</div>

## Missing data

For NumPy data types, `np.nan` represents missing data. It is by default not included in computations. See the \[Missing Data section \<missing\_data\>\](\#missing-data-section \<missing\_data\>).

Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data:

<div class="ipython">

python

df1 = df.reindex(index=dates\[0:4\], columns=list(df.columns) + \["E"\]) df1.loc\[dates\[0\] : dates\[1\], "E"\] = 1 df1

</div>

<span class="title-ref">DataFrame.dropna</span> drops any rows that have missing data:

<div class="ipython">

python

df1.dropna(how="any")

</div>

<span class="title-ref">DataFrame.fillna</span> fills missing data:

<div class="ipython">

python

df1.fillna(value=5)

</div>

<span class="title-ref">isna</span> gets the boolean mask where values are `nan`:

<div class="ipython">

python

pd.isna(df1)

</div>

## Operations

See the \[Basic section on Binary Ops \<basics.binop\>\](\#basic-section-on-binary-ops-\<basics.binop\>).

### Stats

Operations in general *exclude* missing data.

Calculate the mean value for each column:

<div class="ipython">

python

df.mean()

</div>

Calculate the mean value for each row:

<div class="ipython">

python

df.mean(axis=1)

</div>

Operating with another <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a different index or column will align the result with the union of the index or column labels. In addition, pandas automatically broadcasts along the specified dimension and will fill unaligned labels with `np.nan`.

<div class="ipython">

python

s = pd.Series(\[1, 3, 5, np.nan, 6, 8\], index=dates).shift(2) s df.sub(s, axis="index")

</div>

### User defined functions

<span class="title-ref">DataFrame.agg</span> and <span class="title-ref">DataFrame.transform</span> applies a user defined function that reduces or broadcasts its result respectively.

<div class="ipython">

python

df.agg(lambda x: np.mean(x) \* 5.6) df.transform(lambda x: x \* 101.2)

</div>

### Value Counts

See more at \[Histogramming and Discretization \<basics.discretization\>\](\#histogramming-and-discretization-\<basics.discretization\>).

<div class="ipython">

python

s = pd.Series(np.random.randint(0, 7, size=10)) s s.value\_counts()

</div>

### String Methods

<span class="title-ref">Series</span> is equipped with a set of string processing methods in the `str` attribute that make it easy to operate on each element of the array, as in the code snippet below. See more at \[Vectorized String Methods \<text.string\_methods\>\](\#vectorized-string-methods \<text.string\_methods\>).

<div class="ipython">

python

s = pd.Series(\["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"\]) s.str.lower()

</div>

## Merge

### Concat

pandas provides various facilities for easily combining together <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.

See the \[Merging section \<merging\>\](\#merging-section-\<merging\>).

Concatenating pandas objects together row-wise with \`concat\`:

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 4)) df

\# break it into pieces pieces = \[df\[:3\], df\[3:7\], df\[7:\]\]

pd.concat(pieces)

</div>

\> **Note** \> Adding a column to a <span class="title-ref">DataFrame</span> is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the <span class="title-ref">DataFrame</span> constructor instead of building a <span class="title-ref">DataFrame</span> by iteratively appending records to it.

### Join

<span class="title-ref">merge</span> enables SQL style join types along specific columns. See the \[Database style joining \<merging.join\>\](\#database-style-joining-\<merging.join\>) section.

<div class="ipython">

python

left = pd.DataFrame({"key": \["foo", "foo"\], "lval": \[1, 2\]}) right = pd.DataFrame({"key": \["foo", "foo"\], "rval": \[4, 5\]}) left right pd.merge(left, right, on="key")

</div>

<span class="title-ref">merge</span> on unique keys:

<div class="ipython">

python

left = pd.DataFrame({"key": \["foo", "bar"\], "lval": \[1, 2\]}) right = pd.DataFrame({"key": \["foo", "bar"\], "rval": \[4, 5\]}) left right pd.merge(left, right, on="key")

</div>

## Grouping

By "group by" we are referring to a process involving one or more of the following steps:

  - **Splitting** the data into groups based on some criteria
  - **Applying** a function to each group independently
  - **Combining** the results into a data structure

See the \[Grouping section \<groupby\>\](\#grouping-section-\<groupby\>).

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": \["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"\], "B": \["one", "one", "two", "three", "two", "two", "one", "three"\], "C": np.random.randn(8), "D": np.random.randn(8),
    
    }

) df

</div>

Grouping by a column label, selecting column labels, and then applying the <span class="title-ref">.DataFrameGroupBy.sum</span> function to the resulting groups:

<div class="ipython">

python

df.groupby("A")\[\["C", "D"\]\].sum()

</div>

Grouping by multiple columns label forms <span class="title-ref">MultiIndex</span>.

<div class="ipython">

python

df.groupby(\["A", "B"\]).sum()

</div>

## Reshaping

See the sections on \[Hierarchical Indexing \<advanced.hierarchical\>\](\#hierarchical-indexing-\<advanced.hierarchical\>) and \[Reshaping \<reshaping.stacking\>\](\#reshaping-\<reshaping.stacking\>).

### Stack

<div class="ipython">

python

  - arrays = \[  
    \["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\], \["one", "two", "one", "two", "one", "two", "one", "two"\],

\] index = pd.MultiIndex.from\_arrays(arrays, names=\["first", "second"\]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=\["A", "B"\]) df2 = df\[:4\] df2

</div>

The <span class="title-ref">\~DataFrame.stack</span> method "compresses" a level in the DataFrame's columns:

<div class="ipython">

python

stacked = df2.stack() stacked

</div>

With a "stacked" DataFrame or Series (having a <span class="title-ref">MultiIndex</span> as the `index`), the inverse operation of <span class="title-ref">\~DataFrame.stack</span> is <span class="title-ref">\~DataFrame.unstack</span>, which by default unstacks the **last level**:

<div class="ipython">

python

stacked.unstack() stacked.unstack(1) stacked.unstack(0)

</div>

### Pivot tables

See the section on \[Pivot Tables \<reshaping.pivot\>\](\#pivot-tables-\<reshaping.pivot\>).

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": \["one", "one", "two", "three"\] \* 3, "B": \["A", "B", "C"\] \* 4, "C": \["foo", "foo", "foo", "bar", "bar", "bar"\] \* 2, "D": np.random.randn(12), "E": np.random.randn(12),
    
    }

) df

</div>

<span class="title-ref">pivot\_table</span> pivots a <span class="title-ref">DataFrame</span> specifying the `values`, `index` and `columns`

<div class="ipython">

python

pd.pivot\_table(df, values="D", index=\["A", "B"\], columns=\["C"\])

</div>

## Time series

pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. See the \[Time Series section \<timeseries\>\](\#time-series-section-\<timeseries\>).

<div class="ipython">

python

rng = pd.date\_range("1/1/2012", periods=100, freq="s") ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) ts.resample("5Min").sum()

</div>

<span class="title-ref">Series.tz\_localize</span> localizes a time series to a time zone:

<div class="ipython">

python

rng = pd.date\_range("3/6/2012 00:00", periods=5, freq="D") ts = pd.Series(np.random.randn(len(rng)), rng) ts ts\_utc = ts.tz\_localize("UTC") ts\_utc

</div>

<span class="title-ref">Series.tz\_convert</span> converts a timezones aware time series to another time zone:

<div class="ipython">

python

ts\_utc.tz\_convert("US/Eastern")

</div>

Adding a non-fixed duration (<span class="title-ref">\~pandas.tseries.offsets.BusinessDay</span>) to a time series:

<div class="ipython">

python

rng rng + pd.offsets.BusinessDay(5)

</div>

## Categoricals

pandas can include categorical data in a <span class="title-ref">DataFrame</span>. For full docs, see the \[categorical introduction \<categorical\>\](\#categorical-introduction-\<categorical\>) and the \[API documentation \<api.arrays.categorical\>\](\#api-documentation-\<api.arrays.categorical\>).

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"id": \[1, 2, 3, 4, 5, 6\], "raw\_grade": \["a", "b", "b", "a", "a", "e"\]}

)

</div>

Converting the raw grades to a categorical data type:

<div class="ipython">

python

df\["grade"\] = df\["raw\_grade"\].astype("category") df\["grade"\]

</div>

Rename the categories to more meaningful names:

<div class="ipython">

python

new\_categories = \["very good", "good", "very bad"\] df\["grade"\] = df\["grade"\].cat.rename\_categories(new\_categories)

</div>

Reorder the categories and simultaneously add the missing categories (methods under <span class="title-ref">Series.cat</span> return a new <span class="title-ref">Series</span> by default):

<div class="ipython">

python

  - df\["grade"\] = df\["grade"\].cat.set\_categories(  
    \["very bad", "bad", "medium", "good", "very good"\]

) df\["grade"\]

</div>

Sorting is per order in the categories, not lexical order:

<div class="ipython">

python

df.sort\_values(by="grade")

</div>

Grouping by a categorical column with `observed=False` also shows empty categories:

<div class="ipython">

python

df.groupby("grade", observed=False).size()

</div>

## Plotting

See the \[Plotting \<visualization\>\](\#plotting-\<visualization\>) docs.

We use the standard convention for referencing the matplotlib API:

<div class="ipython">

python

import matplotlib.pyplot as plt

plt.close("all")

</div>

The `plt.close` method is used to [close](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.close.html) a figure window:

<div class="ipython">

python

ts = pd.Series(np.random.randn(1000), index=pd.date\_range("1/1/2000", periods=1000)) ts = ts.cumsum()

@savefig series\_plot\_basic.png ts.plot();

</div>

\> **Note** \> When using Jupyter, the plot will appear using <span class="title-ref">\~Series.plot</span>. Otherwise use [matplotlib.pyplot.show](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) to show it or [matplotlib.pyplot.savefig](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) to write it to a file.

<span class="title-ref">\~DataFrame.plot</span> plots all columns:

<div class="ipython">

python

  - df = pd.DataFrame(  
    np.random.randn(1000, 4), index=ts.index, columns=\["A", "B", "C", "D"\]

)

df = df.cumsum()

plt.figure(); df.plot(); @savefig frame\_plot\_basic.png plt.legend(loc='best');

</div>

## Importing and exporting data

See the \[IO Tools \<io\>\](\#io-tools-\<io\>) section.

### CSV

\[Writing to a csv file: \<io.store\_in\_csv\>\](\#writing-to-a-csv-<file:->\<io.store\_in\_csv\>) using <span class="title-ref">DataFrame.to\_csv</span>

<div class="ipython">

python

df = pd.DataFrame(np.random.randint(0, 5, (10, 5))) df.to\_csv("foo.csv")

</div>

\[Reading from a csv file: \<io.read\_csv\_table\>\](\#reading-from-a-csv-<file:->\<io.read\_csv\_table\>) using <span class="title-ref">read\_csv</span>

<div class="ipython">

python

pd.read\_csv("foo.csv")

</div>

<div class="ipython" data-suppress="">

python

import os

os.remove("foo.csv")

</div>

### Parquet

Writing to a Parquet file:

<div class="ipython">

python

df.to\_parquet("foo.parquet")

</div>

Reading from a Parquet file Store using \`read\_parquet\`:

<div class="ipython">

python

pd.read\_parquet("foo.parquet")

</div>

<div class="ipython" data-suppress="">

python

os.remove("foo.parquet")

</div>

### Excel

Reading and writing to \[Excel \<io.excel\>\](\#excel-\<io.excel\>).

Writing to an excel file using \`DataFrame.to\_excel\`:

<div class="ipython">

python

df.to\_excel("foo.xlsx", sheet\_name="Sheet1")

</div>

Reading from an excel file using \`read\_excel\`:

<div class="ipython">

python

pd.read\_excel("foo.xlsx", "Sheet1", index\_col=None, na\_values=\["NA"\])

</div>

<div class="ipython" data-suppress="">

python

os.remove("foo.xlsx")

</div>

## Gotchas

If you are attempting to perform a boolean operation on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> you might see an exception like:

<div class="ipython" data-okexcept="">

python

  - if pd.Series(\[False, True, False\]):  
    print("I was true")

</div>

See \[Comparisons\<basics.compare\>\](\#comparisons\<basics.compare\>) and \[Gotchas\<gotchas\>\](\#gotchas\<gotchas\>) for an explanation and what to do.

---

advanced.md

---

<div id="advanced">

{{ header }}

</div>

# MultiIndex / advanced indexing

This section covers \[indexing with a MultiIndex \<advanced.hierarchical\>\](\#indexing-with-a-multiindex-\<advanced.hierarchical\>) and \[other advanced indexing features \<advanced.index\_types\>\](\#other-advanced-indexing-features-\<advanced.index\_types\>).

See the \[Indexing and Selecting Data \<indexing\>\](\#indexing-and-selecting-data-\<indexing\>) for general indexing documentation.

See the \[cookbook\<cookbook.selection\>\](\#cookbook\<cookbook.selection\>) for some advanced strategies.

## Hierarchical indexing (MultiIndex)

Hierarchical / Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like `Series` (1d) and `DataFrame` (2d).

In this section, we will show what exactly we mean by "hierarchical" indexing and how it integrates with all of the pandas indexing functionality described above and in prior sections. Later, when discussing \[group by \<groupby\>\](\#group-by \<groupby\>) and \[pivoting and reshaping data \<reshaping\>\](\#pivoting-and-reshaping-data-\<reshaping\>), we'll show non-trivial applications to illustrate how it aids in structuring data for analysis.

See the \[cookbook\<cookbook.multi\_index\>\](\#cookbook\<cookbook.multi\_index\>) for some advanced strategies.

### Creating a MultiIndex (hierarchical index) object

The <span class="title-ref">MultiIndex</span> object is the hierarchical analogue of the standard <span class="title-ref">Index</span> object which typically stores the axis labels in pandas objects. You can think of `MultiIndex` as an array of tuples where each tuple is unique. A `MultiIndex` can be created from a list of arrays (using <span class="title-ref">MultiIndex.from\_arrays</span>), an array of tuples (using <span class="title-ref">MultiIndex.from\_tuples</span>), a crossed set of iterables (using <span class="title-ref">MultiIndex.from\_product</span>), or a <span class="title-ref">DataFrame</span> (using <span class="title-ref">MultiIndex.from\_frame</span>). The `Index` constructor will attempt to return a `MultiIndex` when it is passed a list of tuples. The following examples demonstrate different ways to initialize MultiIndexes.

<div class="ipython">

python

  - arrays = \[  
    \["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\], \["one", "two", "one", "two", "one", "two", "one", "two"\],

\] tuples = list(zip(\*arrays)) tuples

index = pd.MultiIndex.from\_tuples(tuples, names=\["first", "second"\]) index

s = pd.Series(np.random.randn(8), index=index) s

</div>

When you want every pairing of the elements in two iterables, it can be easier to use the <span class="title-ref">MultiIndex.from\_product</span> method:

<div class="ipython">

python

iterables = \[\["bar", "baz", "foo", "qux"\], \["one", "two"\]\] pd.MultiIndex.from\_product(iterables, names=\["first", "second"\])

</div>

You can also construct a `MultiIndex` from a `DataFrame` directly, using the method <span class="title-ref">MultiIndex.from\_frame</span>. This is a complementary method to <span class="title-ref">MultiIndex.to\_frame</span>.

<div class="ipython">

python

  - df = pd.DataFrame(  
    \[\["bar", "one"\], \["bar", "two"\], \["foo", "one"\], \["foo", "two"\]\], columns=\["first", "second"\],

) pd.MultiIndex.from\_frame(df)

</div>

As a convenience, you can pass a list of arrays directly into `Series` or `DataFrame` to construct a `MultiIndex` automatically:

<div class="ipython">

python

  - arrays = \[  
    np.array(\["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\]), np.array(\["one", "two", "one", "two", "one", "two", "one", "two"\]),

\] s = pd.Series(np.random.randn(8), index=arrays) s df = pd.DataFrame(np.random.randn(8, 4), index=arrays) df

</div>

All of the `MultiIndex` constructors accept a `names` argument which stores string names for the levels themselves. If no names are provided, `None` will be assigned:

<div class="ipython">

python

df.index.names

</div>

This index can back any axis of a pandas object, and the number of **levels** of the index is up to you:

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(3, 8), index=\["A", "B", "C"\], columns=index) df pd.DataFrame(np.random.randn(6, 6), index=index\[:6\], columns=index\[:6\])

</div>

We've "sparsified" the higher levels of the indexes to make the console output a bit easier on the eyes. Note that how the index is displayed can be controlled using the `multi_sparse` option in `pandas.set_options()`:

<div class="ipython">

python

  - with pd.option\_context("display.multi\_sparse", False):  
    df

</div>

It's worth keeping in mind that there's nothing preventing you from using tuples as atomic labels on an axis:

<div class="ipython">

python

pd.Series(np.random.randn(8), index=tuples)

</div>

The reason that the `MultiIndex` matters is that it can allow you to do grouping, selection, and reshaping operations as we will describe below and in subsequent areas of the documentation. As you will see in later sections, you can find yourself working with hierarchically-indexed data without creating a `MultiIndex` explicitly yourself. However, when loading data from a file, you may wish to generate your own `MultiIndex` when preparing the data set.

### Reconstructing the level labels

The method <span class="title-ref">\~MultiIndex.get\_level\_values</span> will return a vector of the labels for each location at a particular level:

<div class="ipython">

python

index.get\_level\_values(0) index.get\_level\_values("second")

</div>

### Basic indexing on axis with MultiIndex

One of the important features of hierarchical indexing is that you can select data by a "partial" label identifying a subgroup in the data. **Partial** selection "drops" levels of the hierarchical index in the result in a completely analogous way to selecting a column in a regular DataFrame:

<div class="ipython">

python

df\["bar"\] df\["bar", "one"\] df\["bar"\]\["one"\] s\["qux"\]

</div>

See \[Cross-section with hierarchical index \<advanced.xs\>\](\#cross-section-with-hierarchical-index-\<advanced.xs\>) for how to select on a deeper level.

### Defined levels

The <span class="title-ref">MultiIndex</span> keeps all the defined levels of an index, even if they are not actually used. When slicing an index, you may notice this. For example:

<div class="ipython">

python

Â  df.columns.levels \# original MultiIndex

> df\[\["foo","qux"\]\].columns.levels \# sliced

</div>

This is done to avoid a recomputation of the levels in order to make slicing highly performant. If you want to see only the used levels, you can use the <span class="title-ref">\~MultiIndex.get\_level\_values</span> method.

<div class="ipython">

python

df\[\["foo", "qux"\]\].columns.to\_numpy()

\# for a specific level df\[\["foo", "qux"\]\].columns.get\_level\_values(0)

</div>

To reconstruct the `MultiIndex` with only the used levels, the <span class="title-ref">\~MultiIndex.remove\_unused\_levels</span> method may be used.

<div class="ipython">

python

new\_mi = df\[\["foo", "qux"\]\].columns.remove\_unused\_levels() new\_mi.levels

</div>

### Data alignment and using `reindex`

Operations between differently-indexed objects having `MultiIndex` on the axes will work as you expect; data alignment will work the same as an Index of tuples:

<div class="ipython">

python

s + s\[:-2\] s + s\[::2\]

</div>

The <span class="title-ref">\~DataFrame.reindex</span> method of `Series`/`DataFrames` can be called with another `MultiIndex`, or even a list or array of tuples:

<div class="ipython">

python

s.reindex(index\[:3\]) s.reindex(\[("foo", "two"), ("bar", "one"), ("qux", "one"), ("baz", "one")\])

</div>

## Advanced indexing with hierarchical index

Syntactically integrating `MultiIndex` in advanced indexing with `.loc` is a bit challenging, but we've made every effort to do so. In general, MultiIndex keys take the form of tuples. For example, the following works as you would expect:

<div class="ipython">

python

df = df.T df df.loc\[("bar", "two")\]

</div>

Note that `df.loc['bar', 'two']` would also work in this example, but this shorthand notation can lead to ambiguity in general.

If you also want to index a specific column with `.loc`, you must use a tuple like this:

<div class="ipython">

python

df.loc\[("bar", "two"), "A"\]

</div>

You don't have to specify all levels of the `MultiIndex` by passing only the first elements of the tuple. For example, you can use "partial" indexing to get all elements with `bar` in the first level as follows:

<div class="ipython">

python

df.loc\["bar"\]

</div>

This is a shortcut for the slightly more verbose notation `df.loc[('bar',),]` (equivalent to `df.loc['bar',]` in this example).

"Partial" slicing also works quite nicely.

<div class="ipython">

python

df.loc\["baz":"foo"\]

</div>

You can slice with a 'range' of values, by providing a slice of tuples.

<div class="ipython">

python

df.loc\[("baz", "two"):("qux", "one")\] df.loc\[("baz", "two"):"foo"\]

</div>

Passing a list of labels or tuples works similar to reindexing:

<div class="ipython">

python

df.loc\[\[("bar", "two"), ("qux", "one")\]\]

</div>

\> **Note** \> It is important to note that tuples and lists are not treated identically in pandas when it comes to indexing. Whereas a tuple is interpreted as one multi-level key, a list is used to specify several keys. Or in other words, tuples go horizontally (traversing levels), lists go vertically (scanning levels).

Importantly, a list of tuples indexes several complete `MultiIndex` keys, whereas a tuple of lists refer to several values within a level:

<div class="ipython">

python

  - s = pd.Series(  
    \[1, 2, 3, 4, 5, 6\], index=pd.MultiIndex.from\_product(\[\["A", "B"\], \["c", "d", "e"\]\]),

) s.loc\[\[("A", "c"), ("B", "d")\]\] \# list of tuples s.loc\[(\["A", "B"\], \["c", "d"\])\] \# tuple of lists

</div>

### Using slicers

You can slice a `MultiIndex` by providing multiple indexers.

You can provide any of the selectors as if you are indexing by label, see \[Selection by Label \<indexing.label\>\](\#selection-by-label-\<indexing.label\>), including slices, lists of labels, labels, and boolean indexers.

You can use `slice(None)` to select all the contents of *that* level. You do not need to specify all the *deeper* levels, they will be implied as `slice(None)`.

As usual, **both sides** of the slicers are included as this is label indexing.

\> **Warning** \> You should specify all axes in the `.loc` specifier, meaning the indexer for the **index** and for the **columns**. There are some ambiguous cases where the passed indexer could be misinterpreted Â  as indexing *both* axes, rather than into say the `MultiIndex` for the rows.

> You should do this:
> 
>   - \`\`\`python  
>     df.loc\[(slice("A1", "A3"), ...), :\] \# noqa: E999
> 
> Â  You should **not** do this:

  - Â 
    
    ``` python
    df.loc[(slice("A1", "A3"), ...)]  # noqa: E999
    ```

<div class="ipython">

python

  - def mklbl(prefix, n):  
    return \["%s%s" % (prefix, i) for i in range(n)\]

  - miindex = pd.MultiIndex.from\_product(  
    \[mklbl("A", 4), mklbl("B", 2), mklbl("C", 4), mklbl("D", 2)\]

) micolumns = pd.MultiIndex.from\_tuples( \[("a", "foo"), ("a", "bar"), ("b", "foo"), ("b", "bah")\], names=\["lvl0", "lvl1"\] ) dfmi = ( pd.DataFrame( np.arange(len(miindex) \* len(micolumns)).reshape( (len(miindex), len(micolumns)) ), index=miindex, columns=micolumns, ) .sort\_index() .sort\_index(axis=1) ) dfmi

</div>

Basic MultiIndex slicing using slices, lists, and labels.

<div class="ipython">

python

dfmi.loc\[(slice("A1", "A3"), slice(None), \["C1", "C3"\]), :\]

</div>

You can use <span class="title-ref">pandas.IndexSlice</span> to facilitate a more natural syntax `` ` using ``:`, rather than using`slice(None)`.  .. ipython:: python     idx = pd.IndexSlice    dfmi.loc[idx[:, :, ["C1", "C3"]], idx[:, "foo"]]  It is possible to perform quite complicated selections using this method on multiple axes at the same time.  .. ipython:: python     dfmi.loc["A1", (slice(None), "foo")]    dfmi.loc[idx[:, :, ["C1", "C3"]], idx[:, "foo"]]  Using a boolean indexer you can provide selection related to the *values*.  .. ipython:: python     mask = dfmi[("a", "foo")] > 200    dfmi.loc[idx[mask, :, ["C1", "C3"]], idx[:, "foo"]]  You can also specify the`axis`argument to`.loc``to interpret the passed slicers on a single axis.  .. ipython:: python     dfmi.loc(axis=0)[:, :, ["C1", "C3"]]  Furthermore, you can *set* the values using the following methods.  .. ipython:: python    :okwarning:     df2 = dfmi.copy()    df2.loc(axis=0)[:, :, ["C1", "C3"]] = -10    df2  You can use a right-hand-side of an alignable object as well.  .. ipython:: python     df2 = dfmi.copy()    df2.loc[idx[:, :, ["C1", "C3"]], :] = df2 * 1000    df2  .. _advanced.xs:  Cross-section ~~~~~~~~~~~~~  The `~DataFrame.xs` method of``DataFrame`additionally takes a level argument to make selecting data at a particular level of a`MultiIndex`easier.  .. ipython:: python     df    df.xs("one", level="second")  .. ipython:: python     # using the slicers    df.loc[(slice(None), "one"), :]  You can also select on the columns with`xs`, by providing the axis argument.  .. ipython:: python     df = df.T    df.xs("one", level="second", axis=1)  .. ipython:: python     # using the slicers    df.loc[:, (slice(None), "one")]`xs`also allows selection with multiple keys.  .. ipython:: python     df.xs(("one", "bar"), level=("second", "first"), axis=1)  .. ipython:: python     # using the slicers    df.loc[:, ("bar", "one")]  You can pass`drop\_level=False`to`xs`to retain the level that was selected.  .. ipython:: python     df.xs("one", level="second", axis=1, drop_level=False)  Compare the above with the result using`drop\_level=True`(the default value).  .. ipython:: python     df.xs("one", level="second", axis=1, drop_level=True)  .. _advanced.advanced_reindex:  Advanced reindexing and alignment ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Using the parameter`level``in the `~DataFrame.reindex` and `~DataFrame.align` methods of pandas objects is useful to broadcast values across a level. For instance:  .. ipython:: python     midx = pd.MultiIndex(        levels=[["zero", "one"], ["x", "y"]], codes=[[1, 1, 0, 0], [1, 0, 1, 0]]    )    df = pd.DataFrame(np.random.randn(4, 2), index=midx)    df    df2 = df.groupby(level=0).mean()    df2    df2.reindex(df.index, level=0)     # aligning    df_aligned, df2_aligned = df.align(df2, level=0)    df_aligned    df2_aligned   Swapping levels with``swaplevel``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The `~MultiIndex.swaplevel` method can switch the order of two levels:  .. ipython:: python     df[:5]    df[:5].swaplevel(0, 1, axis=0)  .. _advanced.reorderlevels:  Reordering levels with``reorder\_levels``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The `~MultiIndex.reorder_levels` method generalizes the``swaplevel`method, allowing you to permute the hierarchical index levels in one step:  .. ipython:: python     df[:5].reorder_levels([1, 0], axis=0)  .. _advanced.index_names:  Renaming names of an`Index`or`MultiIndex``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The `~DataFrame.rename` method is used to rename the labels of a``MultiIndex`, and is typically used to rename the columns of a`DataFrame`. The`columns`argument of`rename`allows a dictionary to be specified that includes only the columns you wish to rename.  .. ipython:: python     df.rename(columns={0: "col0", 1: "col1"})  This method can also be used to rename specific labels of the main index of the`DataFrame``.  .. ipython:: python     df.rename(index={"one": "two", "y": "z"})  The `~DataFrame.rename_axis` method is used to rename the name of a``Index`or`MultiIndex`. In particular, the names of the levels of a`MultiIndex`can be specified, which is useful if`reset\_index()`is later used to move the values from the`MultiIndex`to a column.  .. ipython:: python     df.rename_axis(index=["abc", "def"])  Note that the columns of a`DataFrame`are an index, so that using`rename\_axis`with the`columns`argument will change the name of that index.  .. ipython:: python     df.rename_axis(columns="Cols").columns  Both`rename`and`rename\_axis`support specifying a dictionary,`Series`or a mapping function to map labels/names to new values.  When working with an`Index`object directly, rather than via a`DataFrame``, `Index.set_names` can be used to change the names.  .. ipython:: python     mi = pd.MultiIndex.from_product([[1, 2], ["a", "b"]], names=["x", "y"])    mi.names     mi2 = mi.rename("new name", level=0)    mi2   You cannot set the names of the MultiIndex via a level.  .. ipython:: python    :okexcept:     mi.levels[0].name = "name via level"  Use `Index.set_names` instead.  Sorting a``MultiIndex``------------------------  For `MultiIndex`-ed objects to be indexed and sliced effectively, they need to be sorted. As with any index, you can use `~DataFrame.sort_index`.  .. ipython:: python     import random     random.shuffle(tuples)    s = pd.Series(np.random.randn(8), index=pd.MultiIndex.from_tuples(tuples))    s    s.sort_index()    s.sort_index(level=0)    s.sort_index(level=1)  .. _advanced.sortlevel_byname:  You may also pass a level name to``sort\_index`if the`MultiIndex`levels are named.  .. ipython:: python     s.index = s.index.set_names(["L1", "L2"])    s.sort_index(level="L1")    s.sort_index(level="L2")  On higher dimensional objects, you can sort any of the other axes by level if they have a`MultiIndex`:  .. ipython:: python     df.T.sort_index(level=1, axis=1)  Indexing will work even if the data are not sorted, but will be rather inefficient (and show a`PerformanceWarning``). It will also return a copy of the data rather than a view:  .. ipython:: python    :okwarning:     dfm = pd.DataFrame(        {"jim": [0, 0, 1, 1], "joe": ["x", "x", "z", "y"], "jolie": np.random.rand(4)}    )    dfm = dfm.set_index(["jim", "joe"])    dfm    dfm.loc[(1, 'z')]  .. _advanced.unsorted:  Furthermore, if you try to index something that is not fully lexsorted, this can raise:  .. ipython:: python    :okexcept:     dfm.loc[(0, 'y'):(1, 'z')]  The `~MultiIndex.is_monotonic_increasing` method on a``MultiIndex`shows if the index is sorted:  .. ipython:: python     dfm.index.is_monotonic_increasing  .. ipython:: python     dfm = dfm.sort_index()    dfm    dfm.index.is_monotonic_increasing  And now selection works as expected.  .. ipython:: python     dfm.loc[(0, "y"):(1, "z")]  Take methods ------------  .. _advanced.take:  Similar to NumPy ndarrays, pandas`Index`,`Series`, and`DataFrame``also provides the `~DataFrame.take` method that retrieves elements along a given axis at the given indices. The given indices must be either a list or an ndarray of integer index positions.``take`will also accept negative integers as relative positions to the end of the object.  .. ipython:: python     index = pd.Index(np.random.randint(0, 1000, 10))    index     positions = [0, 9, 3]     index[positions]    index.take(positions)     ser = pd.Series(np.random.randn(10))     ser.iloc[positions]    ser.take(positions)  For DataFrames, the given indices should be a 1d list or ndarray that specifies row or column positions.  .. ipython:: python     frm = pd.DataFrame(np.random.randn(5, 3))     frm.take([1, 4, 3])     frm.take([0, 2], axis=1)  It is important to note that the`take`method on pandas objects are not intended to work on boolean indices and may return unexpected results.  .. ipython:: python     arr = np.random.randn(10)    arr.take([False, False, True, True])    arr[[0, 1]]     ser = pd.Series(np.random.randn(10))    ser.take([False, False, True, True])    ser.iloc[[0, 1]]  Finally, as a small note on performance, because the`take`method handles a narrower range of inputs, it can offer performance that is a good deal faster than fancy indexing.  .. ipython:: python     arr = np.random.randn(10000, 5)    indexer = np.arange(10000)    random.shuffle(indexer)     %timeit arr[indexer]    %timeit arr.take(indexer, axis=0)  .. ipython:: python     ser = pd.Series(arr[:, 0])    %timeit ser.iloc[indexer]    %timeit ser.take(indexer)  .. _advanced.index_types:  Index types -----------  We have discussed`MultiIndex`in the previous sections pretty extensively. Documentation about`DatetimeIndex`and`PeriodIndex`are shown [here <timeseries.overview>](#here-<timeseries.overview>), and documentation about`TimedeltaIndex``is found [here <timedeltas.index>](#here-<timedeltas.index>).  In the following sub-sections we will highlight some other index types.  .. _advanced.categoricalindex:  CategoricalIndex ~~~~~~~~~~~~~~~~  `CategoricalIndex` is a type of index that is useful for supporting indexing with duplicates. This is a container around a `Categorical` and allows efficient indexing and storage of an index with a large number of duplicated elements.  .. ipython:: python     from pandas.api.types import CategoricalDtype     df = pd.DataFrame({"A": np.arange(6), "B": list("aabbca")})    df["B"] = df["B"].astype(CategoricalDtype(list("cab")))    df    df.dtypes    df["B"].cat.categories  Setting the index will create a``CategoricalIndex`.  .. ipython:: python     df2 = df.set_index("B")    df2.index  Indexing with`\_\_getitem\_\_/.iloc/.loc`works similarly to an`Index`with duplicates. The indexers **must** be in the category or the operation will raise a`KeyError`.  .. ipython:: python     df2.loc["a"]  The`CategoricalIndex`is **preserved** after indexing:  .. ipython:: python     df2.loc["a"].index  Sorting the index will sort by the order of the categories (recall that we created the index with`CategoricalDtype(list('cab'))`, so the sorted order is`cab`).  .. ipython:: python     df2.sort_index()  Groupby operations on the index will preserve the index nature as well.  .. ipython:: python     df2.groupby(level=0, observed=True).sum()    df2.groupby(level=0, observed=True).sum().index  Reindexing operations will return a resulting index based on the type of the passed indexer. Passing a list will return a plain-old`Index`; indexing with a`Categorical`will return a`CategoricalIndex`, indexed according to the categories of the **passed**`Categorical`dtype. This allows one to arbitrarily index these even with values **not** in the categories, similarly to how you can reindex **any** pandas index.  .. ipython:: python     df3 = pd.DataFrame(        {"A": np.arange(3), "B": pd.Series(list("abc")).astype("category")}    )    df3 = df3.set_index("B")    df3  .. ipython:: python     df3.reindex(["a", "e"])    df3.reindex(["a", "e"]).index    df3.reindex(pd.Categorical(["a", "e"], categories=list("abe")))    df3.reindex(pd.Categorical(["a", "e"], categories=list("abe"))).index  > **Warning** >     Reshaping and Comparison operations on a`CategoricalIndex`must have the same categories    or a`TypeError``will be raised.     .. ipython:: python        df4 = pd.DataFrame({"A": np.arange(2), "B": list("ba")})       df4["B"] = df4["B"].astype(CategoricalDtype(list("ab")))       df4 = df4.set_index("B")       df4.index        df5 = pd.DataFrame({"A": np.arange(2), "B": list("bc")})       df5["B"] = df5["B"].astype(CategoricalDtype(list("bc")))       df5 = df5.set_index("B")       df5.index     .. ipython:: python       :okexcept:        pd.concat([df4, df5])  .. _advanced.rangeindex:  RangeIndex ~~~~~~~~~~  `RangeIndex` is a sub-class of `Index`  that provides the default index for all `DataFrame` and `Series` objects.``RangeIndex`is an optimized version of`Index``that can represent a monotonic ordered set. These are analogous to Python `range types <https://docs.python.org/3/library/stdtypes.html#typesseq-range>`__. A``RangeIndex`will always have an`int64`dtype.  .. ipython:: python     idx = pd.RangeIndex(5)    idx`RangeIndex``is the default index for all `DataFrame` and `Series` objects:  .. ipython:: python     ser = pd.Series([1, 2, 3])    ser.index    df = pd.DataFrame([[1, 2], [3, 4]])    df.index    df.columns  A``RangeIndex``will behave similarly to a `Index` with an``int64`dtype and operations on a`RangeIndex`, whose result cannot be represented by a`RangeIndex`, but should have an integer dtype, will be converted to an`Index`with`int64``. For example:  .. ipython:: python     idx[[0, 2]]   .. _advanced.intervalindex:  IntervalIndex ~~~~~~~~~~~~~  `IntervalIndex` together with its own dtype, `~pandas.api.types.IntervalDtype` as well as the `Interval` scalar type,  allow first-class support in pandas for interval notation.  The``IntervalIndex``allows some unique indexing and is also used as a return type for the categories in `cut` and `qcut`.  Indexing with an``IntervalIndex`^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  An`IntervalIndex`can be used in`Series`and in`DataFrame`as the index.  .. ipython:: python     df = pd.DataFrame(        {"A": [1, 2, 3, 4]}, index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4])    )    df  Label based indexing via`.loc`along the edges of an interval works as you would expect, selecting that particular interval.  .. ipython:: python     df.loc[2]    df.loc[[2, 3]]  If you select a label *contained* within an interval, this will also select the interval.  .. ipython:: python     df.loc[2.5]    df.loc[[2.5, 3.5]]  Selecting using an`Interval`will only return exact matches.  .. ipython:: python     df.loc[pd.Interval(1, 2)]  Trying to select an`Interval`that is not exactly contained in the`IntervalIndex`will raise a`KeyError`.  .. ipython:: python    :okexcept:     df.loc[pd.Interval(0.5, 2.5)]  Selecting all`Intervals`that overlap a given`Interval``can be performed using the `~IntervalIndex.overlaps` method to create a boolean indexer.  .. ipython:: python     idxr = df.index.overlaps(pd.Interval(0.5, 2.5))    idxr    df[idxr]  Binning data with``cut`and`qcut``^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `cut` and `qcut` both return a``Categorical`object, and the bins they create are stored as an`IntervalIndex`in its`.categories``attribute.  .. ipython:: python     c = pd.cut(range(4), bins=2)    c    c.categories  `cut` also accepts an``IntervalIndex`for its`bins``argument, which enables a useful pandas idiom. First, We call `cut` with some data and``bins`set to a fixed number, to generate the bins. Then, we pass the values of`.categories`as the`bins``argument in subsequent calls to `cut`, supplying new data which will be binned into the same bins.  .. ipython:: python     pd.cut([0, 3, 5, 1], bins=c.categories)  Any value which falls outside all bins will be assigned a``NaN``value.  Generating ranges of intervals ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  If we need intervals on a regular frequency, we can use the `interval_range` function to create an``IntervalIndex`using various combinations of`start`,`end`, and`periods`. The default frequency for`interval\_range`is a 1 for numeric intervals, and calendar day for datetime-like intervals:  .. ipython:: python     pd.interval_range(start=0, end=5)     pd.interval_range(start=pd.Timestamp("2017-01-01"), periods=4)     pd.interval_range(end=pd.Timedelta("3 days"), periods=3)  The`freq`parameter can used to specify non-default frequencies, and can utilize a variety of [frequency aliases <timeseries.offset_aliases>](#frequency-aliases-<timeseries.offset_aliases>) with datetime-like intervals:  .. ipython:: python     pd.interval_range(start=0, periods=5, freq=1.5)     pd.interval_range(start=pd.Timestamp("2017-01-01"), periods=4, freq="W")     pd.interval_range(start=pd.Timedelta("0 days"), periods=3, freq="9h")  Additionally, the`closed`parameter can be used to specify which side(s) the intervals are closed on.  Intervals are closed on the right side by default.  .. ipython:: python     pd.interval_range(start=0, end=4, closed="both")     pd.interval_range(start=0, end=4, closed="neither")  Specifying`start`,`end`, and`periods`will generate a range of evenly spaced intervals from`start`to`end`inclusively, with`periods`number of elements in the resulting`IntervalIndex`:  .. ipython:: python     pd.interval_range(start=0, end=6, periods=4)     pd.interval_range(pd.Timestamp("2018-01-01"), pd.Timestamp("2018-02-28"), periods=3)  Miscellaneous indexing FAQ --------------------------  Integer indexing ~~~~~~~~~~~~~~~~  Label-based indexing with integer axis labels is a thorny topic. It has been discussed heavily on mailing lists and among various members of the scientific Python community. In pandas, our general viewpoint is that labels matter more than integer locations. Therefore, with an integer axis index *only* label-based indexing is possible with the standard tools like`.loc`. The following code will generate exceptions:  .. ipython:: python    :okexcept:     s = pd.Series(range(5))    s[-1]    df = pd.DataFrame(np.random.randn(5, 4))    df    df.loc[-2:]  This deliberate decision was made to prevent ambiguities and subtle bugs (many users reported finding bugs when the API change was made to stop "falling back" on position-based indexing).  Non-monotonic indexes require exact matches ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  If the index of a`Series`or`DataFrame`is monotonically increasing or decreasing, then the bounds of a label-based slice can be outside the range of the index, much like slice indexing a normal Python`list``. Monotonicity of an index can be tested with the `~Index.is_monotonic_increasing` and `~Index.is_monotonic_decreasing` attributes.  .. ipython:: python      df = pd.DataFrame(index=[2, 3, 3, 4, 5], columns=["data"], data=list(range(5)))     df.index.is_monotonic_increasing      # no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4:     df.loc[0:4, :]      # slice is are outside the index, so empty DataFrame is returned     df.loc[13:15, :]  On the other hand, if the index is not monotonic, then both slice bounds must be *unique* members of the index.  .. ipython:: python      df = pd.DataFrame(index=[2, 3, 1, 4, 3, 5], columns=["data"], data=list(range(6)))     df.index.is_monotonic_increasing      # OK because 2 and 4 are in the index     df.loc[2:4, :]  .. ipython:: python    :okexcept:      # 0 is not in the index     df.loc[0:4, :]      # 3 is not a unique label     df.loc[2:3, :]``Index.is\_monotonic\_increasing`and`Index.is\_monotonic\_decreasing``only check that an index is weakly monotonic. To check for strict monotonicity, you can combine one of those with the `~Index.is_unique` attribute.  .. ipython:: python     weakly_monotonic = pd.Index(["a", "b", "c", "c"])    weakly_monotonic    weakly_monotonic.is_monotonic_increasing    weakly_monotonic.is_monotonic_increasing & weakly_monotonic.is_unique  .. _advanced.endpoints_are_inclusive:  Endpoints are inclusive ~~~~~~~~~~~~~~~~~~~~~~~  Compared with standard Python sequence slicing in which the slice endpoint is not inclusive, label-based slicing in pandas **is inclusive**. The primary reason for this is that it is often not possible to easily determine the "successor" or next element after a particular label in an index. For example, consider the following``Series`:  .. ipython:: python     s = pd.Series(np.random.randn(6), index=list("abcdef"))    s  Suppose we wished to slice from`c`to`e`, using integers this would be accomplished as such:  .. ipython:: python     s[2:5]  However, if you only had`c`and`e`, determining the next element in the index can be somewhat complicated. For example, the following does not work:  .. ipython:: python    :okexcept:      s.loc['c':'e' + 1]  A very common use case is to limit a time series to start and end at two specific dates. To enable this, we made the design choice to make label-based slicing include both endpoints:  .. ipython:: python      s.loc["c":"e"]  This is most definitely a "practicality beats purity" sort of thing, but it is something to watch out for if you expect label-based slicing to behave exactly in the way that standard Python integer slicing works.   Indexing potentially changes underlying Series dtype ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The different indexing operation can potentially change the dtype of a`Series`.  .. ipython:: python     series1 = pd.Series([1, 2, 3])    series1.dtype    res = series1.reindex([0, 4])    res.dtype    res  .. ipython:: python     series2 = pd.Series([True])    series2.dtype    res = series2.reindex_like(series1)    res.dtype    res  This is because the (re)indexing operations above silently inserts`NaNs`and the`dtype`changes accordingly.  This can cause some issues when using`numpy`  `ufuncs`such as`numpy.logical\_and\`\`.

See the `2388` for a more detailed discussion.

---

basics.md

---

<div id="basics">

{{ header }}

</div>

# Essential basic functionality

Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let's create some example objects like we did in the \[10 minutes to pandas \<10min\>\](\#10-minutes-to-pandas-\<10min\>) section:

<div class="ipython">

python

index = pd.date\_range("1/1/2000", periods=8) s = pd.Series(np.random.randn(5), index=\["a", "b", "c", "d", "e"\]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=\["A", "B", "C"\])

</div>

## Head and tail

To view a small sample of a Series or DataFrame object, use the <span class="title-ref">\~DataFrame.head</span> and <span class="title-ref">\~DataFrame.tail</span> methods. The default number of elements to display is five, but you may pass a custom number.

<div class="ipython">

python

long\_series = pd.Series(np.random.randn(1000)) long\_series.head() long\_series.tail(3)

</div>

## Attributes and underlying data

pandas objects have a number of attributes enabling you to access the metadata

  - **shape**: gives the axis dimensions of the object, consistent with ndarray

  -   - Axis labels
        
          - **Series**: *index* (only axis)
          - **DataFrame**: *index* (rows) and *columns*

Note, **these attributes can be safely assigned to**\!

<div class="ipython">

python

df\[:2\] df.columns = \[x.lower() for x in df.columns\] df

</div>

pandas objects (<span class="title-ref">Index</span>, <span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span>) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a <span class="title-ref">numpy.ndarray</span>. However, pandas and 3rd party libraries may *extend* NumPy's type system to add support for custom arrays (see \[basics.dtypes\](\#basics.dtypes)).

To get the actual data inside a <span class="title-ref">Index</span> or <span class="title-ref">Series</span>, use the `.array` property

<div class="ipython">

python

s.array s.index.array

</div>

<span class="title-ref">\~Series.array</span> will always be an <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>. The exact details of what an <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span> is and why pandas uses them are a bit beyond the scope of this introduction. See \[basics.dtypes\](\#basics.dtypes) for more.

If you know you need a NumPy array, use <span class="title-ref">\~Series.to\_numpy</span> or <span class="title-ref">numpy.asarray</span>.

<div class="ipython">

python

s.to\_numpy() np.asarray(s)

</div>

When the Series or Index is backed by an <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>, <span class="title-ref">\~Series.to\_numpy</span> may involve copying data and coercing values. See \[basics.dtypes\](\#basics.dtypes) for more.

<span class="title-ref">\~Series.to\_numpy</span> gives some control over the `dtype` of the resulting <span class="title-ref">numpy.ndarray</span>. For example, consider datetimes with timezones. NumPy doesn't have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:

1.  An object-dtype <span class="title-ref">numpy.ndarray</span> with <span class="title-ref">Timestamp</span> objects, each with the correct `tz`
2.  A `datetime64[ns]` -dtype <span class="title-ref">numpy.ndarray</span>, where the values have been converted to UTC and the timezone discarded

Timezones may be preserved with `dtype=object`

<div class="ipython">

python

ser = pd.Series(pd.date\_range("2000", periods=2, tz="CET")) ser.to\_numpy(dtype=object)

</div>

Or thrown away with `dtype='datetime64[ns]'`

<div class="ipython">

python

ser.to\_numpy(dtype="datetime64\[ns\]")

</div>

Getting the "raw data" inside a <span class="title-ref">DataFrame</span> is possibly a bit more complex. When your `DataFrame` only has a single data type for all the columns, <span class="title-ref">DataFrame.to\_numpy</span> will return the underlying data:

<div class="ipython">

python

df.to\_numpy()

</div>

If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame's columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to.

\> **Note** \> When working with heterogeneous data, the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype.

In the past, pandas recommended <span class="title-ref">Series.values</span> or <span class="title-ref">DataFrame.values</span> for extracting the data from a Series or DataFrame. You'll still find references to these in old code bases and online. Going forward, we recommend avoiding `.values` and using `.array` or `.to_numpy()`. `.values` has the following drawbacks:

1.  When your Series contains an \[extension type \<extending.extension-types\>\](\#extension-type-\<extending.extension-types\>), it's unclear whether <span class="title-ref">Series.values</span> returns a NumPy array or the extension array. <span class="title-ref">Series.array</span> will always return an <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>, and will never copy data. <span class="title-ref">Series.to\_numpy</span> will always return a NumPy array, potentially at the cost of copying / coercing values.
2.  When your DataFrame contains a mixture of data types, <span class="title-ref">DataFrame.values</span> may involve copying data and coercing values to a common dtype, a relatively expensive operation. <span class="title-ref">DataFrame.to\_numpy</span>, being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.

## Accelerated operations

pandas has support for accelerating certain types of binary numerical and boolean operations using the `numexpr` library and the `bottleneck` libraries.

These libraries are especially useful when dealing with large data sets, and provide large speedups. `numexpr` uses smart chunking, caching, and multiple cores. `bottleneck` is a set of specialized cython routines that are especially fast when dealing with arrays that have `nans`.

You are highly encouraged to install both libraries. See the section \[Recommended Dependencies \<install.recommended\_dependencies\>\](\#recommended-dependencies-\<install.recommended\_dependencies\>) for more installation info.

These are both enabled to be used by default, you can control this by setting the options:

`` `python    pd.set_option("compute.use_bottleneck", False)    pd.set_option("compute.use_numexpr", False)  .. _basics.binop:  Flexible binary operations ``\` --------------------------

With binary operations between pandas data structures, there are two key points of interest:

  - Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects.
  - Missing data in computations.

We will demonstrate how to manage these issues independently, though they can be handled simultaneously.

### Matching / broadcasting behavior

DataFrame has the methods <span class="title-ref">\~DataFrame.add</span>, <span class="title-ref">\~DataFrame.sub</span>, <span class="title-ref">\~DataFrame.mul</span>, <span class="title-ref">\~DataFrame.div</span> and related functions <span class="title-ref">\~DataFrame.radd</span>, <span class="title-ref">\~DataFrame.rsub</span>, ... for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the *index* or *columns* via the **axis** keyword:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "one": pd.Series(np.random.randn(3), index=\["a", "b", "c"\]), "two": pd.Series(np.random.randn(4), index=\["a", "b", "c", "d"\]), "three": pd.Series(np.random.randn(3), index=\["b", "c", "d"\]),
    
    }

) df row = df.iloc\[1\] column = df\["two"\]

df.sub(row, axis="columns") df.sub(row, axis=1)

df.sub(column, axis="index") df.sub(column, axis=0)

</div>

Furthermore you can align a level of a MultiIndexed DataFrame with a Series.

<div class="ipython">

python

dfmi = df.copy() dfmi.index = pd.MultiIndex.from\_tuples( \[(1, "a"), (1, "b"), (1, "c"), (2, "a")\], names=\["first", "second"\] ) dfmi.sub(column, axis=0, level="second")

</div>

Series and Index also support the <span class="title-ref">divmod</span> builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example:

<div class="ipython">

python

s = pd.Series(np.arange(10)) s div, rem = divmod(s, 3) div rem

idx = pd.Index(np.arange(10)) idx div, rem = divmod(idx, 3) div rem

</div>

We can also do elementwise \`divmod\`:

<div class="ipython">

python

div, rem = divmod(s, \[2, 2, 3, 3, 4, 4, 5, 5, 6, 6\]) div rem

</div>

### Missing data / operations with fill values

In Series and DataFrame, the arithmetic functions have the option of inputting a *fill\_value*, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using `fillna` if you wish).

<div class="ipython">

python

df2 = df.copy() df2.loc\["a", "three"\] = 1.0 df df2 df + df2 df.add(df2, fill\_value=0)

</div>

### Flexible comparisons

Series and DataFrame have the binary comparison methods `eq`, `ne`, `lt`, `gt`, `le`, and `ge` whose behavior is analogous to the binary arithmetic operations described above:

<div class="ipython">

python

df.gt(df2) df2.ne(df)

</div>

These operations produce a pandas object of the same type as the left-hand-side input that is of dtype `bool`. These `boolean` objects can be used in indexing operations, see the section on \[Boolean indexing\<indexing.boolean\>\](\#boolean-indexing\<indexing.boolean\>).

### Boolean reductions

You can apply the reductions: <span class="title-ref">\~DataFrame.empty</span>, <span class="title-ref">\~DataFrame.any</span>, <span class="title-ref">\~DataFrame.all</span>.

<div class="ipython">

python

(df \> 0).all() (df \> 0).any()

</div>

You can reduce to a final boolean value.

<div class="ipython">

python

(df \> 0).any().any()

</div>

You can test if a pandas object is empty, via the <span class="title-ref">\~DataFrame.empty</span> property.

<div class="ipython">

python

df.empty pd.DataFrame(columns=list("ABC")).empty

</div>

\> **Warning** \> Asserting the truthiness of a pandas object will raise an error, as the testing of the emptiness or values is ambiguous.

> 
> 
> <div class="ipython" data-okexcept="">
> 
> python
> 
>   - if df:  
>     print(True)
> 
> </div>
> 
> <div class="ipython" data-okexcept="">
> 
> python
> 
> df and df2
> 
> </div>
> 
> See \[gotchas\<gotchas.truth\>\](\#gotchas\<gotchas.truth\>) for a more detailed discussion.

### Comparing if objects are equivalent

Often you may find that there is more than one way to compute the same result. As a simple example, consider `df + df` and `df * 2`. To test that these two computations produce the same result, given the tools shown above, you might imagine using `(df + df == df * 2).all()`. But in fact, this expression is False:

<div class="ipython">

python

df + df == df \* 2 (df + df == df \* 2).all()

</div>

Notice that the boolean DataFrame `df + df == df * 2` contains some False values\! This is because NaNs do not compare as equals:

<div class="ipython">

python

np.nan == np.nan

</div>

So, NDFrames (such as Series and DataFrames) have an <span class="title-ref">\~DataFrame.equals</span> method for testing equality, with NaNs in corresponding locations treated as equal.

<div class="ipython">

python

(df + df).equals(df \* 2)

</div>

Note that the Series or DataFrame index needs to be in the same order for equality to be True:

<div class="ipython">

python

df1 = pd.DataFrame({"col": \["foo", 0, np.nan\]}) df2 = pd.DataFrame({"col": \[np.nan, 0, "foo"\]}, index=\[2, 1, 0\]) df1.equals(df2) df1.equals(df2.sort\_index())

</div>

### Comparing array-like objects

You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value:

<div class="ipython">

python

pd.Series(\["foo", "bar", "baz"\]) == "foo" pd.Index(\["foo", "bar", "baz"\]) == "foo"

</div>

pandas also handles element-wise comparisons between different array-like objects of the same length:

<div class="ipython">

python

pd.Series(\["foo", "bar", "baz"\]) == pd.Index(\["foo", "bar", "qux"\]) pd.Series(\["foo", "bar", "baz"\]) == np.array(\["foo", "bar", "qux"\])

</div>

Trying to compare `Index` or `Series` objects of different lengths will raise a ValueError:

<div class="ipython" data-okexcept="">

python

pd.Series(\['foo', 'bar', 'baz'\]) == pd.Series(\['foo', 'bar'\])

pd.Series(\['foo', 'bar', 'baz'\]) == pd.Series(\['foo'\])

</div>

### Combining overlapping data sets

A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of "higher quality". However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is <span class="title-ref">\~DataFrame.combine\_first</span>, which we illustrate:

<div class="ipython">

python

  - df1 = pd.DataFrame(  
    {"A": \[1.0, np.nan, 3.0, 5.0, np.nan\], "B": \[np.nan, 2.0, 3.0, np.nan, 6.0\]}

) df2 = pd.DataFrame( { "A": \[5.0, 2.0, 4.0, np.nan, 3.0, 7.0\], "B": \[np.nan, np.nan, 3.0, 4.0, 6.0, 8.0\], } ) df1 df2 df1.combine\_first(df2)

</div>

### General DataFrame combine

The <span class="title-ref">\~DataFrame.combine\_first</span> method above calls the more general <span class="title-ref">DataFrame.combine</span>. This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same).

So, for instance, to reproduce <span class="title-ref">\~DataFrame.combine\_first</span> as above:

<div class="ipython">

python

  - def combiner(x, y):  
    return np.where(pd.isna(x), y, x)

df1.combine(df2, combiner)

</div>

## Descriptive statistics

There exists a large number of methods for computing descriptive statistics and other related operations on \[Series \<api.series.stats\>\](\#series-\<api.series.stats\>), \[DataFrame \<api.dataframe.stats\>\](\#dataframe \<api.dataframe.stats\>). Most of these are aggregations (hence producing a lower-dimensional result) like <span class="title-ref">\~DataFrame.sum</span>, <span class="title-ref">\~DataFrame.mean</span>, and <span class="title-ref">\~DataFrame.quantile</span>, but some of them, like <span class="title-ref">\~DataFrame.cumsum</span> and <span class="title-ref">\~DataFrame.cumprod</span>, produce an object of the same size. Generally speaking, these methods take an **axis** argument, just like *ndarray.{sum, std, ...}*, but the axis can be specified by name or integer:

  - **Series**: no axis argument needed
  - **DataFrame**: "index" (axis=0, default), "columns" (axis=1)

For example:

<div class="ipython">

python

df df.mean(axis=0) df.mean(axis=1)

</div>

All such methods have a `skipna` option signaling whether to exclude missing data (`True` by default):

<div class="ipython">

python

df.sum(axis=0, skipna=False) df.sum(axis=1, skipna=True)

</div>

Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standardization (rendering data zero mean and standard deviation of 1), very concisely:

<div class="ipython">

python

ts\_stand = (df - df.mean()) / df.std() ts\_stand.std() xs\_stand = df.sub(df.mean(axis=1), axis=0).div(df.std(axis=1), axis=0) xs\_stand.std(axis=1)

</div>

Note that methods like <span class="title-ref">\~DataFrame.cumsum</span> and <span class="title-ref">\~DataFrame.cumprod</span> preserve the location of `NaN` values. This is somewhat different from <span class="title-ref">\~DataFrame.expanding</span> and <span class="title-ref">\~DataFrame.rolling</span> since `NaN` behavior is furthermore dictated by a `min_periods` parameter.

<div class="ipython">

python

df.cumsum()

</div>

Here is a quick reference summary table of common functions. Each also takes an optional `level` parameter which applies only if the object has a \[hierarchical index\<advanced.hierarchical\>\](\#hierarchical-index\<advanced.hierarchical\>).

| Function   | Description                                |
| ---------- | ------------------------------------------ |
| `count`    | Number of non-NA observations              |
| `sum`      | Sum of values                              |
| `mean`     | Mean of values                             |
| `median`   | Arithmetic median of values                |
| `min`      | Minimum                                    |
| `max`      | Maximum                                    |
| `mode`     | Mode                                       |
| `abs`      | Absolute Value                             |
| `prod`     | Product of values                          |
| `std`      | Bessel-corrected sample standard deviation |
| `var`      | Unbiased variance                          |
| `sem`      | Standard error of the mean                 |
| `skew`     | Sample skewness (3rd moment)               |
| `kurt`     | Sample kurtosis (4th moment)               |
| `quantile` | Sample quantile (value at %)               |
| `cumsum`   | Cumulative sum                             |
| `cumprod`  | Cumulative product                         |
| `cummax`   | Cumulative maximum                         |
| `cummin`   | Cumulative minimum                         |

Note that by chance some NumPy methods, like `mean`, `std`, and `sum`, will exclude NAs on Series input by default:

<div class="ipython">

python

np.mean(df\["one"\]) np.mean(df\["one"\].to\_numpy())

</div>

<span class="title-ref">Series.nunique</span> will return the number of unique non-NA values in a Series:

<div class="ipython">

python

series = pd.Series(np.random.randn(500)) series\[20:500\] = np.nan series\[10:20\] = 5 series.nunique()

</div>

### Summarizing data: describe

There is a convenient <span class="title-ref">\~DataFrame.describe</span> function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course):

<div class="ipython">

python

series = pd.Series(np.random.randn(1000)) series\[::2\] = np.nan series.describe() frame = pd.DataFrame(np.random.randn(1000, 5), columns=\["a", "b", "c", "d", "e"\]) frame.iloc\[::2\] = np.nan frame.describe()

</div>

You can select specific percentiles to include in the output:

<div class="ipython">

python

series.describe(percentiles=\[0.05, 0.25, 0.75, 0.95\])

</div>

By default, the median is always included.

For a non-numerical Series object, <span class="title-ref">\~Series.describe</span> will give a simple summary of the number of unique values and most frequently occurring values:

<div class="ipython">

python

s = pd.Series(\["a", "a", "b", "b", "a", "a", np.nan, "c", "d", "a"\]) s.describe()

</div>

Note that on a mixed-type DataFrame object, <span class="title-ref">\~DataFrame.describe</span> will restrict the summary to include only numerical columns or, if none are, only categorical columns:

<div class="ipython">

python

frame = pd.DataFrame({"a": \["Yes", "Yes", "No", "No"\], "b": range(4)}) frame.describe()

</div>

This behavior can be controlled by providing a list of types as `include`/`exclude` arguments. The special value `all` can also be used:

<div class="ipython">

python

frame.describe(include=\["object"\]) frame.describe(include=\["number"\]) frame.describe(include="all")

</div>

That feature relies on \[select\_dtypes \<basics.selectdtypes\>\](\#select\_dtypes-\<basics.selectdtypes\>). Refer to there for details about accepted inputs.

### Index of min/max values

The <span class="title-ref">\~DataFrame.idxmin</span> and <span class="title-ref">\~DataFrame.idxmax</span> functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values:

<div class="ipython">

python

s1 = pd.Series(np.random.randn(5)) s1 s1.idxmin(), s1.idxmax()

df1 = pd.DataFrame(np.random.randn(5, 3), columns=\["A", "B", "C"\]) df1 df1.idxmin(axis=0) df1.idxmax(axis=1)

</div>

When there are multiple rows (or columns) matching the minimum or maximum value, <span class="title-ref">\~DataFrame.idxmin</span> and <span class="title-ref">\~DataFrame.idxmax</span> return the first matching index:

<div class="ipython">

python

df3 = pd.DataFrame(\[2, 1, 1, 3, np.nan\], columns=\["A"\], index=list("edcba")) df3 df3\["A"\].idxmin()

</div>

\> **Note** \> `idxmin` and `idxmax` are called `argmin` and `argmax` in NumPy.

### Value counts (histogramming) / mode

The <span class="title-ref">\~Series.value\_counts</span> Series method computes a histogram of a 1D array of values. It can also be used as a function on regular arrays:

<div class="ipython">

python

data = np.random.randint(0, 7, size=50) data s = pd.Series(data) s.value\_counts()

</div>

The <span class="title-ref">\~DataFrame.value\_counts</span> method can be used to count combinations across multiple columns. By default all columns are used but a subset can be selected using the `subset` argument.

<div class="ipython">

python

data = {"a": \[1, 2, 3, 4\], "b": \["x", "x", "y", "y"\]} frame = pd.DataFrame(data) frame.value\_counts()

</div>

Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame:

<div class="ipython">

python

s5 = pd.Series(\[1, 1, 3, 3, 3, 5, 5, 7, 7, 7\]) s5.mode() df5 = pd.DataFrame( { "A": np.random.randint(0, 7, size=50), "B": np.random.randint(-10, 15, size=50), } ) df5.mode()

</div>

### Discretization and quantiling

Continuous values can be discretized using the <span class="title-ref">cut</span> (bins based on values) and <span class="title-ref">qcut</span> (bins based on sample quantiles) functions:

<div class="ipython">

python

arr = np.random.randn(20) factor = pd.cut(arr, 4) factor

factor = pd.cut(arr, \[-5, -1, 0, 1, 5\]) factor

</div>

<span class="title-ref">qcut</span> computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so:

<div class="ipython">

python

arr = np.random.randn(30) factor = pd.qcut(arr, \[0, 0.25, 0.5, 0.75, 1\]) factor

</div>

We can also pass infinite values to define the bins:

<div class="ipython">

python

arr = np.random.randn(20) factor = pd.cut(arr, \[-np.inf, 0, np.inf\]) factor

</div>

## Function application

To apply your own or another library's functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire `DataFrame` or `Series`, row- or column-wise, or elementwise.

1.  [Tablewise Function Application](#tablewise-function-application): <span class="title-ref">\~DataFrame.pipe</span>
2.  [Row or Column-wise Function Application](#row-or-column-wise-function-application): <span class="title-ref">\~DataFrame.apply</span>
3.  [Aggregation API](#aggregation-api): <span class="title-ref">\~DataFrame.agg</span> and <span class="title-ref">\~DataFrame.transform</span>
4.  [Applying Elementwise Functions](#applying-elementwise-functions): <span class="title-ref">\~DataFrame.map</span>

### Tablewise function application

`DataFrames` and `Series` can be passed into functions. However, if the function needs to be called in a chain, consider using the <span class="title-ref">\~DataFrame.pipe</span> method.

First some setup:

<div class="ipython">

python

  - def extract\_city\_name(df):  
    """ Chicago, IL -\> Chicago for city\_name column """ df\["city\_name"\] = df\["city\_and\_code"\].str.split(",").str.get(0) return df

  - def add\_country\_name(df, country\_name=None):  
    """ Chicago -\> Chicago-US for city\_name column """ col = "city\_name" df\["city\_and\_country"\] = df\[col\] + country\_name return df

df\_p = pd.DataFrame({"city\_and\_code": \["Chicago, IL"\]})

</div>

`extract_city_name` and `add_country_name` are functions taking and returning `DataFrames`.

Now compare the following:

<div class="ipython">

python

add\_country\_name(extract\_city\_name(df\_p), country\_name="US")

</div>

Is equivalent to:

<div class="ipython">

python

df\_p.pipe(extract\_city\_name).pipe(add\_country\_name, country\_name="US")

</div>

pandas encourages the second style, which is known as method chaining. `pipe` makes it easy to use your own or another library's functions in method chains, alongside pandas' methods.

In the example above, the functions `extract_city_name` and `add_country_name` each expected a `DataFrame` as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide `pipe` with a tuple of `(callable, data_keyword)`. `.pipe` will route the `DataFrame` to the argument specified in the tuple.

For example, we can fit a regression using statsmodels. Their API expects a formula first and a `DataFrame` as the second argument, `data`. We pass in the function, keyword pair `(sm.ols, 'data')` to `pipe`:

`` `ipython    In [147]: import statsmodels.formula.api as sm     In [148]: bb = pd.read_csv("data/baseball.csv", index_col="id")     In [149]: (       .....:     bb.query("h > 0")       .....:     .assign(ln_h=lambda df: np.log(df.h))       .....:     .pipe((sm.ols, "data"), "hr ~ ln_h + year + g + C(lg)")       .....:     .fit()       .....:     .summary()       .....: )       .....:    Out[149]:    <class 'statsmodels.iolib.summary.Summary'>    """                               OLS Regression Results    ==============================================================================    Dep. Variable:                     hr   R-squared:                       0.685    Model:                            OLS   Adj. R-squared:                  0.665    Method:                 Least Squares   F-statistic:                     34.28    Date:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15    Time:                        05:34:17   Log-Likelihood:                -205.92    No. Observations:                  68   AIC:                             421.8    Df Residuals:                      63   BIC:                             432.9    Df Model:                           4    Covariance Type:            nonrobust    ===============================================================================                      coef    std err          t      P>|t|      [0.025      0.975]    -------------------------------------------------------------------------------    Intercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780    C(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375    ln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395    year            4.2277      2.324      1.819      0.074      -0.417       8.872    g               0.1841      0.029      6.258      0.000       0.125       0.243    ==============================================================================    Omnibus:                       10.875   Durbin-Watson:                   1.999    Prob(Omnibus):                  0.004   Jarque-Bera (JB):               17.298    Skew:                           0.537   Prob(JB):                     0.000175    Kurtosis:                       5.225   Cond. No.                     1.49e+07    ==============================================================================     Notes:    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.    [2] The condition number is large, 1.49e+07. This might indicate that there are    strong multicollinearity or other numerical problems.    """  The pipe method is inspired by unix pipes and more recently dplyr_ and magrittr_, which ``<span class="title-ref"> have introduced the popular </span><span class="title-ref">(%\>%)</span><span class="title-ref"> (read pipe) operator for R\_. The implementation of </span><span class="title-ref">pipe</span><span class="title-ref"> here is quite clean and feels right at home in Python. We encourage you to view the source code of </span>\~DataFrame.pipe\`.

### Row or column-wise function application

Arbitrary functions can be applied along the axes of a DataFrame using the <span class="title-ref">\~DataFrame.apply</span> method, which, like the descriptive statistics methods, takes an optional `axis` argument:

<div class="ipython">

python

df.apply(lambda x: np.mean(x)) df.apply(lambda x: np.mean(x), axis=1) df.apply(lambda x: x.max() - x.min()) df.apply(np.cumsum) df.apply(np.exp)

</div>

The <span class="title-ref">\~DataFrame.apply</span> method will also dispatch on a string method name.

<div class="ipython">

python

df.apply("mean") df.apply("mean", axis=1)

</div>

The return type of the function passed to <span class="title-ref">\~DataFrame.apply</span> affects the type of the final output from `DataFrame.apply` for the default behaviour:

  - If the applied function returns a `Series`, the final output is a `DataFrame`. The columns match the index of the `Series` returned by the applied function.
  - If the applied function returns any other type, the final output is a `Series`.

This default behaviour can be overridden using the `result_type`, which accepts three options: `reduce`, `broadcast`, and `expand`. These will determine how list-likes return values expand (or not) to a `DataFrame`.

<span class="title-ref">\~DataFrame.apply</span> combined with some cleverness can be used to answer many questions about a data set. For example, suppose we wanted to extract the date where the maximum value for each column occurred:

<div class="ipython">

python

  - tsdf = pd.DataFrame(  
    np.random.randn(1000, 3), columns=\["A", "B", "C"\], index=pd.date\_range("1/1/2000", periods=1000),

) tsdf.apply(lambda x: x.idxmax())

</div>

You may also pass additional arguments and keyword arguments to the <span class="title-ref">\~DataFrame.apply</span> method.

<div class="ipython">

python

  - def subtract\_and\_divide(x, sub, divide=1):  
    return (x - sub) / divide

df\_udf = pd.DataFrame(np.ones((2, 2))) df\_udf.apply(subtract\_and\_divide, args=(5,), divide=3)

</div>

Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row:

<div class="ipython">

python

  - tsdf = pd.DataFrame(  
    np.random.randn(10, 3), columns=\["A", "B", "C"\], index=pd.date\_range("1/1/2000", periods=10),

) tsdf.iloc\[3:7\] = np.nan tsdf tsdf.apply(pd.Series.interpolate)

</div>

Finally, <span class="title-ref">\~DataFrame.apply</span> takes an argument `raw` which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.

### Aggregation API

The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see \[groupby API \<groupby.aggregate\>\](\#groupby-api-\<groupby.aggregate\>), the \[window API \<window.overview\>\](\#window-api-\<window.overview\>), and the \[resample API \<timeseries.aggregate\>\](\#resample-api-\<timeseries.aggregate\>). The entry point for aggregation is <span class="title-ref">DataFrame.aggregate</span>, or the alias <span class="title-ref">DataFrame.agg</span>.

We will use a similar starting frame from above:

<div class="ipython">

python

  - tsdf = pd.DataFrame(  
    np.random.randn(10, 3), columns=\["A", "B", "C"\], index=pd.date\_range("1/1/2000", periods=10),

) tsdf.iloc\[3:7\] = np.nan tsdf

</div>

Using a single function is equivalent to <span class="title-ref">\~DataFrame.apply</span>. You can also pass named methods as strings. These will return a `Series` of the aggregated output:

<div class="ipython">

python

tsdf.agg(lambda x: np.sum(x))

tsdf.agg("sum")

\# these are equivalent to a `.sum()` because we are aggregating \# on a single function tsdf.sum()

</div>

Single aggregations on a `Series` this will return a scalar value:

<div class="ipython">

python

tsdf\["A"\].agg("sum")

</div>

#### Aggregating with multiple functions

You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting `DataFrame`. These are naturally named from the aggregation function.

<div class="ipython">

python

tsdf.agg(\["sum"\])

</div>

Multiple functions yield multiple rows:

<div class="ipython">

python

tsdf.agg(\["sum", "mean"\])

</div>

On a `Series`, multiple functions return a `Series`, indexed by the function names:

<div class="ipython">

python

tsdf\["A"\].agg(\["sum", "mean"\])

</div>

Passing a `lambda` function will yield a `<lambda>` named row:

<div class="ipython">

python

tsdf\["A"\].agg(\["sum", lambda x: x.mean()\])

</div>

Passing a named function will yield that name for the row:

<div class="ipython">

python

  - def mymean(x):  
    return x.mean()

tsdf\["A"\].agg(\["sum", mymean\])

</div>

#### Aggregating with a dict

Passing a dictionary of column names to a scalar or a list of scalars, to `DataFrame.agg` allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an `OrderedDict` instead to guarantee ordering.

<div class="ipython">

python

tsdf.agg({"A": "mean", "B": "sum"})

</div>

Passing a list-like will generate a `DataFrame` output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be `NaN`:

<div class="ipython">

python

tsdf.agg({"A": \["mean", "min"\], "B": "sum"})

</div>

#### Custom describe

With `.agg()` it is possible to easily create a custom describe function, similar to the built in \[describe function \<basics.describe\>\](\#describe-function-\<basics.describe\>).

<div class="ipython">

python

from functools import partial

q\_25 = partial(pd.Series.quantile, q=0.25) q\_25.\_\_name\_\_ = "25%" q\_75 = partial(pd.Series.quantile, q=0.75) q\_75.\_\_name\_\_ = "75%"

tsdf.agg(\["count", "mean", "std", "min", q\_25, "median", q\_75, "max"\])

</div>

### Transform API

The <span class="title-ref">\~DataFrame.transform</span> method returns an object that is indexed the same (same size) as the original. This API allows you to provide *multiple* operations at the same time rather than one-by-one. Its API is quite similar to the `.agg` API.

We create a frame similar to the one used in the above sections.

<div class="ipython">

python

  - tsdf = pd.DataFrame(  
    np.random.randn(10, 3), columns=\["A", "B", "C"\], index=pd.date\_range("1/1/2000", periods=10),

) tsdf.iloc\[3:7\] = np.nan tsdf

</div>

Transform the entire frame. `.transform()` allows input functions as: a NumPy function, a string function name or a user defined function.

<div class="ipython" data-okwarning="">

python

tsdf.transform(np.abs) tsdf.transform("abs") tsdf.transform(lambda x: x.abs())

</div>

Here <span class="title-ref">\~DataFrame.transform</span> received a single function; this is equivalent to a [ufunc](https://numpy.org/doc/stable/reference/ufuncs.html) application.

<div class="ipython">

python

np.abs(tsdf)

</div>

Passing a single function to `.transform()` with a `Series` will yield a single `Series` in return.

<div class="ipython">

python

tsdf\["A"\].transform(np.abs)

</div>

#### Transform with multiple functions

Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions.

<div class="ipython">

python

tsdf.transform(\[np.abs, lambda x: x + 1\])

</div>

Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions.

<div class="ipython">

python

tsdf\["A"\].transform(\[np.abs, lambda x: x + 1\])

</div>

#### Transforming with a dict

Passing a dict of functions will allow selective transforming per column.

<div class="ipython">

python

tsdf.transform({"A": np.abs, "B": lambda x: x + 1})

</div>

Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms.

<div class="ipython" data-okwarning="">

python

tsdf.transform({"A": np.abs, "B": \[lambda x: x + 1, "sqrt"\]})

</div>

### Applying elementwise functions

Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods <span class="title-ref">\~DataFrame.map</span> on DataFrame and analogously <span class="title-ref">\~Series.map</span> on Series accept any Python function taking a single value and returning a single value. For example:

<div class="ipython">

python

df4 = df.copy() df4

  - def f(x):  
    return len(str(x))

df4\["one"\].map(f) df4.map(f)

</div>

<span class="title-ref">Series.map</span> has an additional feature; it can be used to easily "link" or "map" values defined by a secondary series. This is closely related to \[merging/joining functionality \<merging\>\](\#merging/joining-functionality-\<merging\>):

<div class="ipython">

python

  - s = pd.Series(  
    \["six", "seven", "six", "seven", "six"\], index=\["a", "b", "c", "d", "e"\]

) t = pd.Series({"six": 6.0, "seven": 7.0}) s s.map(t)

</div>

## Reindexing and altering labels

<span class="title-ref">\~Series.reindex</span> is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To *reindex* means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:

  - Reorders the existing data to match a new set of labels
  - Inserts missing value (NA) markers in label locations where no data for that label existed
  - If specified, **fill** data for missing labels using logic (highly relevant to working with time series data)

Here is a simple example:

<div class="ipython">

python

s = pd.Series(np.random.randn(5), index=\["a", "b", "c", "d", "e"\]) s s.reindex(\["e", "b", "f", "d"\])

</div>

Here, the `f` label was not contained in the Series and hence appears as `NaN` in the result.

With a DataFrame, you can simultaneously reindex the index and columns:

<div class="ipython">

python

df df.reindex(index=\["c", "f", "b"\], columns=\["three", "two", "one"\])

</div>

Note that the `Index` objects containing the actual axis labels can be **shared** between objects. So if we have a Series and a DataFrame, the following can be done:

<div class="ipython">

python

rs = s.reindex(df.index) rs rs.index is df.index

</div>

This means that the reindexed Series's index is the same Python object as the DataFrame's index.

<span class="title-ref">DataFrame.reindex</span> also supports an "axis-style" calling convention, where you specify a single `labels` argument and the `axis` it applies to.

<div class="ipython">

python

df.reindex(\["c", "f", "b"\], axis="index") df.reindex(\["three", "two", "one"\], axis="columns")

</div>

<div class="seealso">

\[MultiIndex / Advanced Indexing \<advanced\>\](\#multiindex-/-advanced-indexing-\<advanced\>) is an even more concise way of doing reindexing.

</div>

\> **Note** \> When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: **many operations are faster on pre-aligned data**. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because `reindex` has been heavily optimized), but when CPU cycles matter sprinkling a few explicit `reindex` calls here and there can have an impact.

### Reindexing to align with another object

You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the <span class="title-ref">\~DataFrame.reindex\_like</span> method is available to make this simpler:

<div class="ipython">

python

df2 = df.reindex(\["a", "b", "c"\], columns=\["one", "two"\]) df3 = df2 - df2.mean() df2 df3 df.reindex\_like(df2)

</div>

### Aligning objects with each other with `align`

The <span class="title-ref">\~Series.align</span> method is the fastest way to simultaneously align two objects. It supports a `join` argument (related to \[joining and merging \<merging\>\](\#joining-and-merging-\<merging\>)):

>   - `join='outer'`: take the union of the indexes (default)
>   - `join='left'`: use the calling object's index
>   - `join='right'`: use the passed object's index
>   - `join='inner'`: intersect the indexes

It returns a tuple with both of the reindexed Series:

<div class="ipython">

python

s = pd.Series(np.random.randn(5), index=\["a", "b", "c", "d", "e"\]) s1 = s\[:4\] s2 = s\[1:\] s1.align(s2) s1.align(s2, join="inner") s1.align(s2, join="left")

</div>

<div id="basics.df_join">

For DataFrames, the join method will be applied to both the index and the columns by default:

</div>

<div class="ipython">

python

df.align(df2, join="inner")

</div>

You can also pass an `axis` option to only align on the specified axis:

<div class="ipython">

python

df.align(df2, join="inner", axis=0)

</div>

<div id="basics.align.frame.series">

If you pass a Series to <span class="title-ref">DataFrame.align</span>, you can choose to align both objects either on the DataFrame's index or columns using the `axis` argument:

</div>

<div class="ipython">

python

df.align(df2.iloc\[0\], axis=1)

</div>

### Filling while reindexing

<span class="title-ref">\~Series.reindex</span> takes an optional parameter `method` which is a filling method chosen from the following table:

| Method  | Action                            |
| ------- | --------------------------------- |
| ffill   | Fill values forward               |
| bfill   | Fill values backward              |
| nearest | Fill from the nearest index value |

We illustrate these fill methods on a simple Series:

<div class="ipython">

python

rng = pd.date\_range("1/3/2000", periods=8) ts = pd.Series(np.random.randn(8), index=rng) ts2 = ts.iloc\[\[0, 3, 6\]\] ts ts2

ts2.reindex(ts.index) ts2.reindex(ts.index, method="ffill") ts2.reindex(ts.index, method="bfill") ts2.reindex(ts.index, method="nearest")

</div>

These methods require that the indexes are **ordered** increasing or decreasing.

Note that the same result could have been achieved using \[ffill \<missing\_data.fillna\>\](\#ffill-\<missing\_data.fillna\>) (except for `method='nearest'`) or \[interpolate \<missing\_data.interpolate\>\](\#interpolate-\<missing\_data.interpolate\>):

<div class="ipython">

python

ts2.reindex(ts.index).ffill()

</div>

<span class="title-ref">\~Series.reindex</span> will raise a ValueError if the index is not monotonically increasing or decreasing. <span class="title-ref">\~Series.fillna</span> and <span class="title-ref">\~Series.interpolate</span> will not perform any checks on the order of the index.

### Limits on filling while reindexing

The `limit` and `tolerance` arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches:

<div class="ipython">

python

ts2.reindex(ts.index, method="ffill", limit=1)

</div>

In contrast, tolerance specifies the maximum distance between the index and indexer values:

<div class="ipython">

python

ts2.reindex(ts.index, method="ffill", tolerance="1 day")

</div>

Notice that when used on a `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`, `tolerance` will coerced into a `Timedelta` if possible. This allows you to specify tolerance with appropriate strings.

### Dropping labels from an axis

A method closely related to `reindex` is the <span class="title-ref">\~DataFrame.drop</span> function. It removes a set of labels from an axis:

<div class="ipython">

python

df df.drop(\["a", "d"\], axis=0) df.drop(\["one"\], axis=1)

</div>

Note that the following also works, but is a bit less obvious / clean:

<div class="ipython">

python

df.reindex(df.index.difference(\["a", "d"\]))

</div>

### Renaming / mapping labels

The <span class="title-ref">\~DataFrame.rename</span> method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function.

<div class="ipython">

python

s s.rename(str.upper)

</div>

If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used:

<div class="ipython">

python

  - df.rename(  
    columns={"one": "foo", "two": "bar"}, index={"a": "apple", "b": "banana", "d": "durian"},

)

</div>

If the mapping doesn't include a column/index label, it isn't renamed. Note that extra labels in the mapping don't throw an error.

<span class="title-ref">DataFrame.rename</span> also supports an "axis-style" calling convention, where you specify a single `mapper` and the `axis` to apply that mapping to.

<div class="ipython">

python

df.rename({"one": "foo", "two": "bar"}, axis="columns") df.rename({"a": "apple", "b": "banana", "d": "durian"}, axis="index")

</div>

Finally, <span class="title-ref">\~Series.rename</span> also accepts a scalar or list-like for altering the `Series.name` attribute.

<div class="ipython">

python

s.rename("scalar-name")

</div>

<div id="basics.rename_axis">

The methods <span class="title-ref">DataFrame.rename\_axis</span> and <span class="title-ref">Series.rename\_axis</span> allow specific names of a `MultiIndex` to be changed (as opposed to the labels).

</div>

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"x": \[1, 2, 3, 4, 5, 6\], "y": \[10, 20, 30, 40, 50, 60\]}, index=pd.MultiIndex.from\_product( \[\["a", "b", "c"\], \[1, 2\]\], names=\["let", "num"\] ),

) df df.rename\_axis(index={"let": "abc"}) df.rename\_axis(index=str.upper)

</div>

## Iteration

The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the "keys" of the objects.

In short, basic iteration (`for i in object`) produces:

  - **Series**: values
  - **DataFrame**: column labels

Thus, for example, iterating over a DataFrame gives you the column names:

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"col1": np.random.randn(3), "col2": np.random.randn(3)}, index=\["a", "b", "c"\]

)

  - for col in df:  
    print(col)

</div>

pandas objects also have the dict-like <span class="title-ref">\~DataFrame.items</span> method to iterate over the (key, value) pairs.

To iterate over the rows of a DataFrame, you can use the following methods:

  - \`\~DataFrame.iterrows\`: Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications.
  - \`\~DataFrame.itertuples\`: Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than <span class="title-ref">\~DataFrame.iterrows</span>, and is in most cases preferable to use to iterate over the values of a DataFrame.

\> **Warning** \> Iterating through pandas objects is generally **slow**. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:

>   - Look for a *vectorized* solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, ...
>   - When you have a function that cannot work on the full DataFrame/Series at once, it is better to use <span class="title-ref">\~DataFrame.apply</span> instead of iterating over the values. See the docs on \[function application \<basics.apply\>\](\#function-application-\<basics.apply\>).
>   - If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the \[enhancing performance \<enhancingperf\>\](\#enhancing-performance-\<enhancingperf\>) section for some examples of this approach.

<div class="warning">

<div class="title">

Warning

</div>

You should **never modify** something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect\!

For example, in the following case setting the value has no effect:

<div class="ipython">

python

df = pd.DataFrame({"a": \[1, 2, 3\], "b": \["a", "b", "c"\]})

  - for index, row in df.iterrows():  
    row\["a"\] = 10

df

</div>

</div>

### items

Consistent with the dict-like interface, <span class="title-ref">\~DataFrame.items</span> iterates through key-value pairs:

  - **Series**: (index, scalar value) pairs
  - **DataFrame**: (column, Series) pairs

For example:

<div class="ipython">

python

  - for label, ser in df.items():  
    print(label) print(ser)

</div>

### iterrows

<span class="title-ref">\~DataFrame.iterrows</span> allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row:

<div class="ipython">

python

  - for row\_index, row in df.iterrows():  
    print(row\_index, row, sep="n")

</div>

\> **Note** \> Because <span class="title-ref">\~DataFrame.iterrows</span> returns a Series for each row, it does **not** preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example,

> 
> 
> <div class="ipython">
> 
> python
> 
> df\_orig = pd.DataFrame(\[\[1, 1.5\]\], columns=\["int", "float"\]) df\_orig.dtypes row = next(df\_orig.iterrows())\[1\] row
> 
> </div>
> 
> All values in `row`, returned as a Series, are now upcasted to floats, also the original integer value in column `x`:
> 
> <div class="ipython">
> 
> python
> 
> row\["int"\].dtype df\_orig\["int"\].dtype
> 
> </div>
> 
> To preserve dtypes while iterating over the rows, it is better to use <span class="title-ref">\~DataFrame.itertuples</span> which returns namedtuples of the values and which is generally much faster than <span class="title-ref">\~DataFrame.iterrows</span>.

For instance, a contrived way to transpose the DataFrame would be:

<div class="ipython">

python

df2 = pd.DataFrame({"x": \[1, 2, 3\], "y": \[4, 5, 6\]}) print(df2) print(df2.T)

df2\_t = pd.DataFrame({idx: values for idx, values in df2.iterrows()}) print(df2\_t)

</div>

### itertuples

The <span class="title-ref">\~DataFrame.itertuples</span> method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row's corresponding index value, while the remaining values are the row values.

For instance:

<div class="ipython">

python

  - for row in df.itertuples():  
    print(row)

</div>

This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, <span class="title-ref">\~DataFrame.itertuples</span> preserves the data type of the values and is generally faster than <span class="title-ref">\~DataFrame.iterrows</span>.

\> **Note** \> The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (\>255), regular tuples are returned.

## .dt accessor

`Series` has an accessor to succinctly return datetime like properties for the *values* of the Series, if it is a datetime/period like Series. This will return a Series, indexed like the existing Series.

<div class="ipython">

python

\# datetime s = pd.Series(pd.date\_range("20130101 09:10:12", periods=4)) s s.dt.hour s.dt.second s.dt.day

</div>

This enables nice expressions like this:

<div class="ipython">

python

s\[s.dt.day == 2\]

</div>

You can easily produces tz aware transformations:

<div class="ipython">

python

stz = s.dt.tz\_localize("US/Eastern") stz stz.dt.tz

</div>

You can also chain these types of operations:

<div class="ipython">

python

s.dt.tz\_localize("UTC").dt.tz\_convert("US/Eastern")

</div>

You can also format datetime values as strings with <span class="title-ref">Series.dt.strftime</span> which supports the same format as the standard <span class="title-ref">\~datetime.datetime.strftime</span>.

<div class="ipython">

python

\# DatetimeIndex s = pd.Series(pd.date\_range("20130101", periods=4)) s s.dt.strftime("%Y/%m/%d")

</div>

<div class="ipython">

python

\# PeriodIndex s = pd.Series(pd.period\_range("20130101", periods=4)) s s.dt.strftime("%Y/%m/%d")

</div>

The `.dt` accessor works for period and timedelta dtypes.

<div class="ipython">

python

\# period s = pd.Series(pd.period\_range("20130101", periods=4, freq="D")) s s.dt.year s.dt.day

</div>

<div class="ipython">

python

\# timedelta s = pd.Series(pd.timedelta\_range("1 day 00:00:05", periods=4, freq="s")) s s.dt.days s.dt.seconds s.dt.components

</div>

\> **Note** \> `Series.dt` will raise a `TypeError` if you access with a non-datetime-like values.

## Vectorized string methods

Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Series's `str` attribute and generally have names matching the equivalent (scalar) built-in string methods. For example:

> 
> 
> <div class="ipython">
> 
> python
> 
>   - s = pd.Series(  
>     \["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"\], dtype="string"
> 
> ) s.str.lower()
> 
> </div>

Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses [regular expressions](https://docs.python.org/3/library/re.html) by default (and in some cases always uses them).

\> **Note** \> Prior to pandas 1.0, string methods were only available on `object` -dtype `Series`. pandas 1.0 added the <span class="title-ref">StringDtype</span> which is dedicated to strings. See \[text.types\](\#text.types) for more.

Please see \[Vectorized String Methods \<text.string\_methods\>\](\#vectorized-string-methods-\<text.string\_methods\>) for a complete description.

## Sorting

pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both.

### By index

The <span class="title-ref">Series.sort\_index</span> and <span class="title-ref">DataFrame.sort\_index</span> methods are used to sort a pandas object by its index levels.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "one": pd.Series(np.random.randn(3), index=\["a", "b", "c"\]), "two": pd.Series(np.random.randn(4), index=\["a", "b", "c", "d"\]), "three": pd.Series(np.random.randn(3), index=\["b", "c", "d"\]),
    
    }

)

  - unsorted\_df = df.reindex(  
    index=\["a", "d", "c", "b"\], columns=\["three", "two", "one"\]

) unsorted\_df

\# DataFrame unsorted\_df.sort\_index() unsorted\_df.sort\_index(ascending=False) unsorted\_df.sort\_index(axis=1)

\# Series unsorted\_df\["three"\].sort\_index()

</div>

<div id="basics.sort_index_key">

Sorting by index also supports a `key` parameter that takes a callable function to apply to the index being sorted. For `MultiIndex` objects, the key is applied per-level to the levels specified by `level`.

</div>

<div class="ipython">

python

  - s1 = pd.DataFrame({"a": \["B", "a", "C"\], "b": \[1, 2, 3\], "c": \[2, 3, 4\]}).set\_index(  
    list("ab")

) s1

</div>

<div class="ipython">

python

s1.sort\_index(level="a") s1.sort\_index(level="a", key=lambda idx: idx.str.lower())

</div>

For information on key sorting by value, see \[value sorting \<basics.sort\_value\_key\>\](\#value-sorting \<basics.sort\_value\_key\>).

### By values

The <span class="title-ref">Series.sort\_values</span> method is used to sort a `Series` by its values. The <span class="title-ref">DataFrame.sort\_values</span> method is used to sort a `DataFrame` by its column or row values. The optional `by` parameter to <span class="title-ref">DataFrame.sort\_values</span> may used to specify one or more columns to use to determine the sorted order.

<div class="ipython">

python

  - df1 = pd.DataFrame(  
    {"one": \[2, 1, 1, 1\], "two": \[1, 3, 2, 4\], "three": \[5, 4, 3, 2\]}

) df1.sort\_values(by="two")

</div>

The `by` parameter can take a list of column names, e.g.:

<div class="ipython">

python

df1\[\["one", "two", "three"\]\].sort\_values(by=\["one", "two"\])

</div>

These methods have special treatment of NA values via the `na_position` argument:

<div class="ipython">

python

s\[2\] = np.nan s.sort\_values() s.sort\_values(na\_position="first")

</div>

<div id="basics.sort_value_key">

Sorting also supports a `key` parameter that takes a callable function to apply to the values being sorted.

</div>

<div class="ipython">

python

s1 = pd.Series(\["B", "a", "C"\])

</div>

<div class="ipython">

python

s1.sort\_values() s1.sort\_values(key=lambda x: x.str.lower())

</div>

`key` will be given the <span class="title-ref">Series</span> of values and should return a `Series` or array of the same shape with the transformed values. For `DataFrame` objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g.

<div class="ipython">

python

df = pd.DataFrame({"a": \["B", "a", "C"\], "b": \[1, 2, 3\]})

</div>

<div class="ipython">

python

df.sort\_values(by="a") df.sort\_values(by="a", key=lambda col: col.str.lower())

</div>

The name or type of each column can be used to apply different functions to different columns.

### By indexes and values

Strings passed as the `by` parameter to <span class="title-ref">DataFrame.sort\_values</span> may refer to either columns or index level names.

<div class="ipython">

python

\# Build MultiIndex idx = pd.MultiIndex.from\_tuples( \[("a", 1), ("a", 2), ("a", 2), ("b", 2), ("b", 1), ("b", 1)\] ) idx.names = \["first", "second"\]

\# Build DataFrame df\_multi = pd.DataFrame({"A": np.arange(6, 0, -1)}, index=idx) df\_multi

</div>

Sort by 'second' (index) and 'A' (column)

<div class="ipython">

python

df\_multi.sort\_values(by=\["second", "A"\])

</div>

\> **Note** \> If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.

### searchsorted

Series has the <span class="title-ref">\~Series.searchsorted</span> method, which works similarly to <span class="title-ref">numpy.ndarray.searchsorted</span>.

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3\]) ser.searchsorted(\[0, 3\]) ser.searchsorted(\[0, 4\]) ser.searchsorted(\[1, 3\], side="right") ser.searchsorted(\[1, 3\], side="left") ser = pd.Series(\[3, 1, 2\]) ser.searchsorted(\[0, 3\], sorter=np.argsort(ser))

</div>

### smallest / largest values

`Series` has the <span class="title-ref">\~Series.nsmallest</span> and <span class="title-ref">\~Series.nlargest</span> methods which return the smallest or largest \(n\) values. For a large `Series` this can be much faster than sorting the entire Series and calling `head(n)` on the result.

<div class="ipython">

python

s = pd.Series(np.random.permutation(10)) s s.sort\_values() s.nsmallest(3) s.nlargest(3)

</div>

`DataFrame` also has the `nlargest` and `nsmallest` methods.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "a": \[-2, -1, 1, 10, 8, 11, -1\], "b": list("abdceff"), "c": \[1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0\],
    
    }

) df.nlargest(3, "a") df.nlargest(5, \["a", "c"\]) df.nsmallest(3, "a") df.nsmallest(5, \["a", "c"\])

</div>

### Sorting by a MultiIndex column

You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to `by`.

<div class="ipython">

python

  - df1.columns = pd.MultiIndex.from\_tuples(  
    \[("a", "one"), ("a", "two"), ("b", "three")\]

) df1.sort\_values(by=("a", "two"))

</div>

## Copying

The <span class="title-ref">\~DataFrame.copy</span> method on pandas objects copies the underlying data (though not the axis indexes, since they are immutable) and returns a new object. Note that **it is seldom necessary to copy objects**. For example, there are only a handful of ways to alter a DataFrame *in-place*:

  - Inserting, deleting, or modifying a column.
  - Assigning to the `index` or `columns` attributes.
  - For homogeneous data, directly modifying the values via the `values` attribute or advanced indexing.

To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.

## dtypes

For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for `float`, `int`, `bool`, `timedelta64[ns]` and `datetime64[ns]` (note that NumPy does not support timezone-aware datetimes).

pandas and third-party libraries *extend* NumPy's type system in a few places. This section describes the extensions pandas has made internally. See \[extending.extension-types\](\#extending.extension-types) for how to write your own extension that works with pandas. See [the ecosystem page](https://pandas.pydata.org/community/ecosystem.html) for a list of third-party libraries that have implemented an extension.

The following table lists all of pandas extension types. For methods requiring `dtype` arguments, strings can be specified as indicated. See the respective documentation sections for more on each type.

\+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | Kind of Data | Data Type | Scalar | Array | String Aliases | +=================================================+===============+===========+========+===========+===============================+========================================+ | \[tz-aware datetime \<timeseries.timezone\>\](\#tz-aware-datetime-\<timeseries.timezone\>) | <span class="title-ref">DatetimeTZDtype</span> | <span class="title-ref">Timestamp</span> | <span class="title-ref">arrays.DatetimeArray</span> | `'datetime64[ns, <tz>]'` | | | | | | | +-------------------------------------------------+---------------+-----------+--------------------+-------------------------------+----------------------------------------+ | \[Categorical \<categorical\>\](\#categorical-\<categorical\>) | <span class="title-ref">CategoricalDtype</span> | (none) | <span class="title-ref">Categorical</span> | `'category'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[period (time spans) \<timeseries.periods\>\](\#period-(time-spans)-\<timeseries.periods\>) | <span class="title-ref">PeriodDtype</span> | <span class="title-ref">Period</span> | <span class="title-ref">arrays.PeriodArray</span> | `'period[<freq>]'`, | | | | | `'Period[<freq>]'` | | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[sparse \<sparse\>\](\#sparse-\<sparse\>) | <span class="title-ref">SparseDtype</span> | (none) | <span class="title-ref">arrays.SparseArray</span> | `'Sparse'`, `'Sparse[int]'`, | | | | | | `'Sparse[float]'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[intervals \<advanced.intervalindex\>\](\#intervals-\<advanced.intervalindex\>) | <span class="title-ref">IntervalDtype</span> | <span class="title-ref">Interval</span> | <span class="title-ref">arrays.IntervalArray</span> | `'interval'`, `'Interval'`, | | | | | | `'Interval[<numpy_dtype>]'`, | | | | | | `'Interval[datetime64[ns, <tz>]]'`, | | | | | | `'Interval[timedelta64[<freq>]]'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[nullable integer \<integer\_na\>\](\#nullable-integer-\<integer\_na\>) | <span class="title-ref">Int64Dtype</span>, ... | (none) | <span class="title-ref">arrays.IntegerArray</span> | `'Int8'`, `'Int16'`, `'Int32'`, | | | | | | `'Int64'`, `'UInt8'`, `'UInt16'`,| | | | | | `'UInt32'`, `'UInt64'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[nullable float \<api.arrays.float\_na\>\](\#nullable-float-\<api.arrays.float\_na\>) | <span class="title-ref">Float64Dtype</span>, ...| (none) | <span class="title-ref">arrays.FloatingArray</span> | `'Float32'`, `'Float64'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[Strings \<text\>\](\#strings-\<text\>) | <span class="title-ref">StringDtype</span> | <span class="title-ref">str</span> | <span class="title-ref">arrays.StringArray</span> | `'string'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+ | \[Boolean (with NA) \<api.arrays.bool\>\](\#boolean-(with-na)-\<api.arrays.bool\>) | <span class="title-ref">BooleanDtype</span> | <span class="title-ref">bool</span> | <span class="title-ref">arrays.BooleanArray</span> | `'boolean'` | +-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+

pandas has two ways to store strings.

1.  `object` dtype, which can hold any Python object, including strings.
2.  <span class="title-ref">StringDtype</span>, which is dedicated to strings.

Generally, we recommend using <span class="title-ref">StringDtype</span>. See \[text.types\](\#text.types) for more.

Finally, arbitrary objects may be stored using the `object` dtype, but should be avoided to the extent possible (for performance and interoperability with other libraries and methods. See \[basics.object\_conversion\](\#basics.object\_conversion)).

A convenient <span class="title-ref">\~DataFrame.dtypes</span> attribute for DataFrame returns a Series with the data type of each column.

<div class="ipython">

python

  - dft = pd.DataFrame(
    
      - {  
        "A": np.random.rand(3), "B": 1, "C": "foo", "D": pd.Timestamp("20010102"), "E": pd.Series(\[1.0\] \* 3).astype("float32"), "F": False, "G": pd.Series(\[1\] \* 3, dtype="int8"),
    
    }

) dft dft.dtypes

</div>

On a `Series` object, use the <span class="title-ref">\~Series.dtype</span> attribute.

<div class="ipython">

python

dft\["A"\].dtype

</div>

If a pandas object contains data with multiple dtypes *in a single column*, the dtype of the column will be chosen to accommodate all of the data types (`object` is the most general).

<div class="ipython">

python

\# these ints are coerced to floats pd.Series(\[1, 2, 3, 4, 5, 6.0\])

\# string data forces an `object` dtype pd.Series(\[1, 2, 3, 6.0, "foo"\])

</div>

The number of columns of each type in a `DataFrame` can be found by calling `DataFrame.dtypes.value_counts()`.

<div class="ipython">

python

dft.dtypes.value\_counts()

</div>

Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the `dtype` keyword, a passed `ndarray`, or a passed `Series`), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will **NOT** be combined. The following example will give you a taste.

<div class="ipython">

python

df1 = pd.DataFrame(np.random.randn(8, 1), columns=\["A"\], dtype="float32") df1 df1.dtypes df2 = pd.DataFrame( { "A": pd.Series(np.random.randn(8), dtype="float16"), "B": pd.Series(np.random.randn(8)), "C": pd.Series(np.random.randint(0, 255, size=8), dtype="uint8"), \# \[0,255\] (range of uint8) } ) df2 df2.dtypes

</div>

### defaults

By default integer types are `int64` and float types are `float64`, *regardless* of platform (32-bit or 64-bit). The following will all result in `int64` dtypes.

<div class="ipython">

python

pd.DataFrame(\[1, 2\], columns=\["a"\]).dtypes pd.DataFrame({"a": \[1, 2\]}).dtypes pd.DataFrame({"a": 1}, index=list(range(2))).dtypes

</div>

Note that Numpy will choose *platform-dependent* types when creating arrays. The following **WILL** result in `int32` on 32-bit platform.

<div class="ipython">

python

frame = pd.DataFrame(np.array(\[1, 2\]))

</div>

### upcasting

Types can potentially be *upcasted* when combined with other types, meaning they are promoted from the current type (e.g. `int` to `float`).

<div class="ipython">

python

df3 = df1.reindex\_like(df2).fillna(value=0.0) + df2 df3 df3.dtypes

</div>

<span class="title-ref">DataFrame.to\_numpy</span> will return the *lower-common-denominator* of the dtypes, meaning the dtype that can accommodate **ALL** of the types in the resulting homogeneous dtyped NumPy array. This can force some *upcasting*.

<div class="ipython">

python

df3.to\_numpy().dtype

</div>

### astype

<div id="basics.cast">

You can use the <span class="title-ref">\~DataFrame.astype</span> method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass `copy=False` to change this behavior). In addition, they will raise an exception if the astype operation is invalid.

</div>

Upcasting is always according to the **NumPy** rules. If two different dtypes are involved in an operation, then the more *general* one will be used as the result of the operation.

<div class="ipython">

python

df3 df3.dtypes

\# conversion of dtypes df3.astype("float32").dtypes

</div>

Convert a subset of columns to a specified type using <span class="title-ref">\~DataFrame.astype</span>.

<div class="ipython">

python

dft = pd.DataFrame({"a": \[1, 2, 3\], "b": \[4, 5, 6\], "c": \[7, 8, 9\]}) dft\[\["a", "b"\]\] = dft\[\["a", "b"\]\].astype(np.uint8) dft dft.dtypes

</div>

Convert certain columns to a specific dtype by passing a dict to <span class="title-ref">\~DataFrame.astype</span>.

<div class="ipython">

python

dft1 = pd.DataFrame({"a": \[1, 0, 1\], "b": \[4, 5, 6\], "c": \[7, 8, 9\]}) dft1 = dft1.astype({"a": [np.bool](), "c": np.float64}) dft1 dft1.dtypes

</div>

\> **Note** \> When trying to convert a subset of columns to a specified type using <span class="title-ref">\~DataFrame.astype</span> and <span class="title-ref">\~DataFrame.loc</span>, upcasting occurs.

> <span class="title-ref">\~DataFrame.loc</span> tries to fit in what we are assigning to the current dtypes, while `[]` will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result.
> 
> <div class="ipython">
> 
> python
> 
> dft = pd.DataFrame({"a": \[1, 2, 3\], "b": \[4, 5, 6\], "c": \[7, 8, 9\]}) dft.loc\[:, \["a", "b"\]\].astype(np.uint8).dtypes dft.loc\[:, \["a", "b"\]\] = dft.loc\[:, \["a", "b"\]\].astype(np.uint8) dft.dtypes
> 
> </div>

### object conversion

pandas offers various functions to try to force conversion of types from the `object` dtype to other types. In cases where the data is already of the correct type, but stored in an `object` array, the <span class="title-ref">DataFrame.infer\_objects</span> and <span class="title-ref">Series.infer\_objects</span> methods can be used to soft convert to the correct type.

> 
> 
> <div class="ipython">
> 
> python
> 
> import datetime
> 
>   - df = pd.DataFrame(
>     
>       - \[  
>         \[1, 2\], \["a", "b"\], \[datetime.datetime(2016, 3, 2), datetime.datetime(2016, 3, 2)\],
>     
>     \]
> 
> ) df = df.T df df.dtypes
> 
> </div>

Because the data was transposed the original inference stored all columns as object, which `infer_objects` will correct.

> 
> 
> <div class="ipython">
> 
> python
> 
> df.infer\_objects().dtypes
> 
> </div>

The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type:

  - <span class="title-ref">\~pandas.to\_numeric</span> (conversion to numeric dtypes)
    
    <div class="ipython">
    
    python
    
    m = \["1.1", 2, 3\] pd.to\_numeric(m)
    
    </div>

  - <span class="title-ref">\~pandas.to\_datetime</span> (conversion to datetime objects)
    
    <div class="ipython">
    
    python
    
    import datetime
    
    m = \["2016-07-09", datetime.datetime(2016, 3, 2)\] pd.to\_datetime(m)
    
    </div>

  - <span class="title-ref">\~pandas.to\_timedelta</span> (conversion to timedelta objects)
    
    <div class="ipython">
    
    python
    
    m = \["5us", pd.Timedelta("1day")\] pd.to\_timedelta(m)
    
    </div>

To force a conversion, we can pass in an `errors` argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, `errors='raise'`, meaning that any errors encountered will be raised during the conversion process. However, if `errors='coerce'`, these errors will be ignored and pandas will convert problematic elements to `pd.NaT` (for datetime and timedelta) or `np.nan` (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing:

<div class="ipython" data-okwarning="">

python

import datetime

m = \["apple", datetime.datetime(2016, 3, 2)\] pd.to\_datetime(m, errors="coerce")

m = \["apple", 2, 3\] pd.to\_numeric(m, errors="coerce")

m = \["apple", pd.Timedelta("1day")\] pd.to\_timedelta(m, errors="coerce")

</div>

In addition to object conversion, <span class="title-ref">\~pandas.to\_numeric</span> provides another argument `downcast`, which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory:

<div class="ipython">

python

m = \["1", 2, 3\] pd.to\_numeric(m, downcast="integer") \# smallest signed int dtype pd.to\_numeric(m, downcast="signed") \# same as 'integer' pd.to\_numeric(m, downcast="unsigned") \# smallest unsigned int dtype pd.to\_numeric(m, downcast="float") \# smallest float dtype

</div>

As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such as DataFrames. However, with <span class="title-ref">\~pandas.DataFrame.apply</span>, we can "apply" the function over each column efficiently:

<div class="ipython">

python

import datetime

df = pd.DataFrame(\[\["2016-07-09", datetime.datetime(2016, 3, 2)\]\] \* 2, dtype="O") df df.apply(pd.to\_datetime)

df = pd.DataFrame(\[\["1.1", 2, 3\]\] \* 2, dtype="O") df df.apply(pd.to\_numeric)

df = pd.DataFrame(\[\["5us", pd.Timedelta("1day")\]\] \* 2, dtype="O") df df.apply(pd.to\_timedelta)

</div>

### gotchas

Performing selection operations on `integer` type data can easily upcast the data to `floating`. The dtype of the input data will be preserved in cases where `nans` are not introduced. See also \[Support for integer NA \<gotchas.intna\>\](\#support-for-integer-na-\<gotchas.intna\>).

<div class="ipython">

python

dfi = df3.astype("int32") dfi\["E"\] = 1 dfi dfi.dtypes

casted = dfi\[dfi \> 0\] casted casted.dtypes

</div>

While float dtypes are unchanged.

<div class="ipython">

python

dfa = df3.copy() dfa\["A"\] = dfa\["A"\].astype("float32") dfa.dtypes

casted = dfa\[df2 \> 0\] casted casted.dtypes

</div>

## Selecting columns based on `dtype`

<div id="basics.selectdtypes">

The <span class="title-ref">\~DataFrame.select\_dtypes</span> method implements subsetting of columns based on their `dtype`.

</div>

First, let's create a <span class="title-ref">DataFrame</span> with a slew of different dtypes:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "string": list("abc"), "int64": list(range(1, 4)), "uint8": np.arange(3, 6).astype("u1"), "float64": np.arange(4.0, 7.0), "bool1": \[True, False, True\], "bool2": \[False, True, False\], "dates": pd.date\_range("now", periods=3), "category": pd.Series(list("ABC")).astype("category"),
    
    }

) df\["tdeltas"\] = df.dates.diff() df\["uint64"\] = np.arange(3, 6).astype("u8") df\["other\_dates"\] = pd.date\_range("20130101", periods=3) df\["tz\_aware\_dates"\] = pd.date\_range("20130101", periods=3, tz="US/Eastern") df

</div>

And the dtypes:

<div class="ipython">

python

df.dtypes

</div>

<span class="title-ref">\~DataFrame.select\_dtypes</span> has two parameters `include` and `exclude` that allow you to say "give me the columns *with* these dtypes" (`include`) and/or "give the columns *without* these dtypes" (`exclude`).

For example, to select `bool` columns:

<div class="ipython">

python

df.select\_dtypes(include=\[bool\])

</div>

You can also pass the name of a dtype in the [NumPy dtype hierarchy](https://numpy.org/doc/stable/reference/arrays.scalars.html):

<div class="ipython">

python

df.select\_dtypes(include=\["bool"\])

</div>

<span class="title-ref">\~pandas.DataFrame.select\_dtypes</span> also works with generic dtypes as well.

For example, to select all numeric and boolean columns while excluding unsigned integers:

<div class="ipython">

python

df.select\_dtypes(include=\["number", "bool"\], exclude=\["unsignedinteger"\])

</div>

To select string columns you must use the `object` dtype:

<div class="ipython">

python

df.select\_dtypes(include=\["object"\])

</div>

To see all the child dtypes of a generic `dtype` like `numpy.number` you can define a function that returns a tree of child dtypes:

<div class="ipython">

python

  - def subdtypes(dtype):  
    subs = dtype.\_\_subclasses\_\_() if not subs: return dtype return \[dtype, \[subdtypes(dt) for dt in subs\]\]

</div>

All NumPy dtypes are subclasses of `numpy.generic`:

<div class="ipython">

python

subdtypes(np.generic)

</div>

\> **Note** \> pandas also defines the types `category`, and `datetime64[ns, tz]`, which are not integrated into the normal NumPy hierarchy and won't show up with the above function.

---

boolean.md

---

<div class="currentmodule">

pandas

</div>

<div class="ipython" data-suppress="">

python

import pandas as pd import numpy as np

</div>

# Nullable Boolean data type

\> **Note** \> BooleanArray is currently experimental. Its API or implementation may change without warning.

## Indexing with NA values

pandas allows indexing with `NA` values in a boolean array, which are treated as `False`.

<div class="ipython" data-okexcept="">

python

s = pd.Series(\[1, 2, 3\]) mask = pd.array(\[True, False, pd.NA\], dtype="boolean") s\[mask\]

</div>

If you would prefer to keep the `NA` values you can manually fill them with `fillna(True)`.

<div class="ipython">

python

s\[mask.fillna(True)\]

</div>

If you create a column of `NA` values (for example to fill them later) with `df['new_col'] = pd.NA`, the `dtype` would be set to `object` in the new column. The performance on this column will be worse than with the appropriate type. It's better to use `df['new_col'] = pd.Series(pd.NA, dtype="boolean")` (or another `dtype` that supports `NA`).

<div class="ipython">

python

df = pd.DataFrame() df\['objects'\] = pd.NA df.dtypes

</div>

## Kleene logical operations

<span class="title-ref">arrays.BooleanArray</span> implements [Kleene Logic](https://en.wikipedia.org/wiki/Three-valued_logic#Kleene_and_Priest_logics) (sometimes called three-value logic) for logical operations like `&` (and), `|` (or) and `^` (exclusive-or).

This table demonstrates the results for every combination. These operations are symmetrical, so flipping the left- and right-hand side makes no difference in the result.

| Expression       | Result  |
| ---------------- | ------- |
| `True & True`    | `True`  |
| `True & False`   | `False` |
| `True & NA`      | `NA`    |
| `False & False`  | `False` |
| `False & NA`     | `False` |
| `NA & NA`        | `NA`    |
| `True \| True`   | `True`  |
| `True \| False`  | `True`  |
| `True \| NA`     | `True`  |
| `False \| False` | `False` |
| `False \| NA`    | `NA`    |
| `NA \| NA`       | `NA`    |
| `True ^ True`    | `False` |
| `True ^ False`   | `True`  |
| `True ^ NA`      | `NA`    |
| `False ^ False`  | `False` |
| `False ^ NA`     | `NA`    |
| `NA ^ NA`        | `NA`    |

When an `NA` is present in an operation, the output value is `NA` only if the result cannot be determined solely based on the other input. For example, `True | NA` is `True`, because both `True | True` and `True | False` are `True`. In that case, we don't actually need to consider the value of the `NA`.

On the other hand, `True & NA` is `NA`. The result depends on whether the `NA` really is `True` or `False`, since `True & True` is `True`, but `True & False` is `False`, so we can't determine the output.

This differs from how `np.nan` behaves in logical operations. pandas treated `np.nan` is *always false in the output*.

In `or`

<div class="ipython">

python

pd.Series(\[True, False, np.nan\], dtype="object") | True pd.Series(\[True, False, np.nan\], dtype="boolean") | True

</div>

In `and`

<div class="ipython">

python

pd.Series(\[True, False, np.nan\], dtype="object") & True pd.Series(\[True, False, np.nan\], dtype="boolean") & True

</div>

---

categorical.md

---

<div id="categorical">

{{ header }}

</div>

# Categorical data

This is an introduction to pandas categorical data type, including a short comparison with R's `factor`.

`Categoricals` are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (`categories`; `levels` in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales.

In contrast to statistical categorical variables, categorical data might have an order (e.g. 'strongly agree' vs 'agree' or 'first observation' vs. 'second observation'), but numerical operations (additions, divisions, ...) are not possible.

All values of categorical data are either in `categories` or `np.nan`. Order is defined by the order of `categories`, not lexical order of the values. Internally, the data structure consists of a `categories` array and an integer array of `codes` which point to the real value in the `categories` array.

The categorical data type is useful in the following cases:

  - A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, see \[here \<categorical.memory\>\](\#here-\<categorical.memory\>).
  - The lexical order of a variable is not the same as the logical order ("one", "two", "three"). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, see \[here \<categorical.sort\>\](\#here-\<categorical.sort\>).
  - As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).

See also the \[API docs on categoricals\<api.arrays.categorical\>\](\#api-docs-on-categoricals\<api.arrays.categorical\>).

## Object creation

### Series creation

Categorical `Series` or columns in a `DataFrame` can be created in several ways:

By specifying `dtype="category"` when constructing a `Series`:

<div class="ipython">

python

s = pd.Series(\["a", "b", "c", "a"\], dtype="category") s

</div>

By converting an existing `Series` or column to a `category` dtype:

<div class="ipython">

python

df = pd.DataFrame({"A": \["a", "b", "c", "a"\]}) df\["B"\] = df\["A"\].astype("category") df

</div>

By using special functions, such as <span class="title-ref">\~pandas.cut</span>, which groups data into discrete bins. See the \[example on tiling \<reshaping.tile.cut\>\](\#example-on-tiling-\<reshaping.tile.cut\>) in the docs.

<div class="ipython">

python

df = pd.DataFrame({"value": np.random.randint(0, 100, 20)}) labels = \["{0} - {1}".format(i, i + 9) for i in range(0, 100, 10)\]

df\["group"\] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels) df.head(10)

</div>

By passing a <span class="title-ref">pandas.Categorical</span> object to a `Series` or assigning it to a `DataFrame`.

<div class="ipython">

python

  - raw\_cat = pd.Categorical(  
    \["a", "b", "c", "a"\], categories=\["b", "c", "d"\], ordered=False

) s = pd.Series(raw\_cat) s df = pd.DataFrame({"A": \["a", "b", "c", "a"\]}) df\["B"\] = raw\_cat df

</div>

Categorical data has a specific `category` \[dtype \<basics.dtypes\>\](\#dtype-\<basics.dtypes\>):

<div class="ipython">

python

df.dtypes

</div>

### DataFrame creation

Similar to the previous section where a single column was converted to categorical, all columns in a `DataFrame` can be batch converted to categorical either during or after construction.

This can be done during construction by specifying `dtype="category"` in the `DataFrame` constructor:

<div class="ipython">

python

df = pd.DataFrame({"A": list("abca"), "B": list("bccd")}, dtype="category") df.dtypes

</div>

Note that the categories present in each column differ; the conversion is done column by column, so only labels present in a given column are categories:

<div class="ipython">

python

df\["A"\] df\["B"\]

</div>

Analogously, all columns in an existing `DataFrame` can be batch converted using \`DataFrame.astype\`:

<div class="ipython">

python

df = pd.DataFrame({"A": list("abca"), "B": list("bccd")}) df\_cat = df.astype("category") df\_cat.dtypes

</div>

This conversion is likewise done column by column:

<div class="ipython">

python

df\_cat\["A"\] df\_cat\["B"\]

</div>

### Controlling behavior

In the examples above where we passed `dtype='category'`, we used the default behavior:

1.  Categories are inferred from the data.
2.  Categories are unordered.

To control those behaviors, instead of passing `'category'`, use an instance of <span class="title-ref">\~pandas.api.types.CategoricalDtype</span>.

<div class="ipython">

python

from pandas.api.types import CategoricalDtype

s = pd.Series(\["a", "b", "c", "a"\]) cat\_type = CategoricalDtype(categories=\["b", "c", "d"\], ordered=True) s\_cat = s.astype(cat\_type) s\_cat

</div>

Similarly, a `CategoricalDtype` can be used with a `DataFrame` to ensure that categories are consistent among all columns.

<div class="ipython">

python

from pandas.api.types import CategoricalDtype

df = pd.DataFrame({"A": list("abca"), "B": list("bccd")}) cat\_type = CategoricalDtype(categories=list("abcd"), ordered=True) df\_cat = df.astype(cat\_type) df\_cat\["A"\] df\_cat\["B"\]

</div>

\> **Note** \> To perform table-wise conversion, where all labels in the entire `DataFrame` are used as categories for each column, the `categories` parameter can be determined programmatically by `categories = pd.unique(df.to_numpy().ravel())`.

If you already have `codes` and `categories`, you can use the <span class="title-ref">\~pandas.Categorical.from\_codes</span> constructor to save the factorize step during normal constructor mode:

<div class="ipython">

python

splitter = np.random.choice(\[0, 1\], 5, p=\[0.5, 0.5\]) s = pd.Series(pd.Categorical.from\_codes(splitter, categories=\["train", "test"\]))

</div>

### Regaining original data

To get back to the original `Series` or NumPy array, use `Series.astype(original_dtype)` or `np.asarray(categorical)`:

<div class="ipython">

python

s = pd.Series(\["a", "b", "c", "a"\]) s s2 = s.astype("category") s2 s2.astype(str) np.asarray(s2)

</div>

\> **Note** \> In contrast to R's `factor` function, categorical data is not converting input values to strings; categories will end up the same data type as the original values.

<div class="note">

<div class="title">

Note

</div>

In contrast to R's `factor` function, there is currently no way to assign/change labels at creation time. Use `categories` to change the categories after creation time.

</div>

## CategoricalDtype

A categorical's type is fully described by

1.  `categories`: a sequence of unique values and no missing values
2.  `ordered`: a boolean

This information can be stored in a <span class="title-ref">\~pandas.api.types.CategoricalDtype</span>. The `categories` argument is optional, which implies that the actual categories should be inferred from whatever is present in the data when the <span class="title-ref">pandas.Categorical</span> is created. The categories are assumed to be unordered by default.

<div class="ipython">

python

from pandas.api.types import CategoricalDtype

CategoricalDtype(\["a", "b", "c"\]) CategoricalDtype(\["a", "b", "c"\], ordered=True) CategoricalDtype()

</div>

A <span class="title-ref">\~pandas.api.types.CategoricalDtype</span> can be used in any place pandas expects a `dtype`. For example <span class="title-ref">pandas.read\_csv</span>, <span class="title-ref">pandas.DataFrame.astype</span>, or in the `Series` constructor.

\> **Note** \> As a convenience, you can use the string `'category'` in place of a <span class="title-ref">\~pandas.api.types.CategoricalDtype</span> when you want the default behavior of the categories being unordered, and equal to the set values present in the array. In other words, `dtype='category'` is equivalent to `dtype=CategoricalDtype()`.

### Equality semantics

Two instances of <span class="title-ref">\~pandas.api.types.CategoricalDtype</span> compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the `categories` is not considered. Note that categories with different dtypes are not the same.

<div class="ipython">

python

c1 = CategoricalDtype(\["a", "b", "c"\], ordered=False)

\# Equal, since order is not considered when ordered=False c1 == CategoricalDtype(\["b", "c", "a"\], ordered=False)

\# Unequal, since the second CategoricalDtype is ordered c1 == CategoricalDtype(\["a", "b", "c"\], ordered=True)

</div>

All instances of `CategoricalDtype` compare equal to the string `'category'`.

<div class="ipython">

python

c1 == "category"

</div>

Notice that the `categories_dtype` should be considered, especially when comparing with two empty `CategoricalDtype` instances.

<div class="ipython">

python

c2 = pd.Categorical(np.array(\[\], dtype=object)) c3 = pd.Categorical(np.array(\[\], dtype=float))

c2.dtype == c3.dtype

</div>

## Description

Using <span class="title-ref">\~DataFrame.describe</span> on categorical data will produce similar output to a `Series` or `DataFrame` of type `string`.

<div class="ipython">

python

cat = pd.Categorical(\["a", "c", "c", np.nan\], categories=\["b", "a", "c"\]) df = pd.DataFrame({"cat": cat, "s": \["a", "c", "c", np.nan\]}) df.describe() df\["cat"\].describe()

</div>

## Working with categories

Categorical data has a `categories` and a `ordered` property, which list their possible values and whether the ordering matters or not. These properties are exposed as `s.cat.categories` and `s.cat.ordered`. If you don't manually specify categories and ordering, they are inferred from the passed arguments.

<div class="ipython">

python

s = pd.Series(\["a", "b", "c", "a"\], dtype="category") s.cat.categories s.cat.ordered

</div>

It's also possible to pass in the categories in a specific order:

<div class="ipython">

python

s = pd.Series(pd.Categorical(\["a", "b", "c", "a"\], categories=\["c", "b", "a"\])) s.cat.categories s.cat.ordered

</div>

\> **Note** \> New categorical data are **not** automatically ordered. You must explicitly pass `ordered=True` to indicate an ordered `Categorical`.

<div class="note">

<div class="title">

Note

</div>

The result of <span class="title-ref">\~Series.unique</span> is not always the same as `Series.cat.categories`, because `Series.unique()` has a couple of guarantees, namely that it returns categories in the order of appearance, and it only includes values that are actually present.

<div class="ipython">

python

s = pd.Series(list("babc")).astype(CategoricalDtype(list("abcd"))) s

\# categories s.cat.categories

\# uniques s.unique()

</div>

</div>

### Renaming categories

Renaming categories is done by using the <span class="title-ref">\~pandas.Categorical.rename\_categories</span> method:

<div class="ipython">

python

s = pd.Series(\["a", "b", "c", "a"\], dtype="category") s new\_categories = \["Group %s" % g for g in s.cat.categories\] s = s.cat.rename\_categories(new\_categories) s \# You can also pass a dict-like object to map the renaming s = s.cat.rename\_categories({1: "x", 2: "y", 3: "z"}) s

</div>

\> **Note** \> In contrast to R's `factor`, categorical data can have categories of other types than string.

Categories must be unique or a `ValueError` is raised:

<div class="ipython">

python

  - try:  
    s = s.cat.rename\_categories(\[1, 1, 1\])

  - except ValueError as e:  
    print("ValueError:", str(e))

</div>

Categories must also not be `NaN` or a `ValueError` is raised:

<div class="ipython">

python

  - try:  
    s = s.cat.rename\_categories(\[1, 2, np.nan\])

  - except ValueError as e:  
    print("ValueError:", str(e))

</div>

### Appending new categories

Appending categories can be done by using the <span class="title-ref">\~pandas.Categorical.add\_categories</span> method:

<div class="ipython">

python

s = s.cat.add\_categories(\[4\]) s.cat.categories s

</div>

### Removing categories

Removing categories can be done by using the <span class="title-ref">\~pandas.Categorical.remove\_categories</span> method. Values which are removed are replaced by `np.nan`.:

<div class="ipython">

python

s = s.cat.remove\_categories(\[4\]) s

</div>

### Removing unused categories

Removing unused categories can also be done:

<div class="ipython">

python

s = pd.Series(pd.Categorical(\["a", "b", "a"\], categories=\["a", "b", "c", "d"\])) s s.cat.remove\_unused\_categories()

</div>

### Setting categories

If you want to do remove and add new categories in one step (which has some speed advantage), or simply set the categories to a predefined scale, use <span class="title-ref">\~pandas.Categorical.set\_categories</span>.

<div class="ipython">

python

s = pd.Series(\["one", "two", "four", "-"\], dtype="category") s s = s.cat.set\_categories(\["one", "two", "three", "four"\]) s

</div>

<div class="note">

<div class="title">

Note

</div>

Be aware that <span class="title-ref">Categorical.set\_categories</span> cannot know whether some category is omitted intentionally or because it is misspelled or (under Python3) due to a type difference (e.g., NumPy S1 dtype and Python strings). This can result in surprising behaviour\!

</div>

## Sorting and order

<div id="categorical.sort">

If categorical data is ordered (`s.cat.ordered == True`), then the order of the categories has a meaning and certain operations are possible. If the categorical is unordered, `.min()/.max()` will raise a `TypeError`.

</div>

<div class="ipython">

python

s = pd.Series(pd.Categorical(\["a", "b", "c", "a"\], ordered=False)) s = s.sort\_values() s = pd.Series(\["a", "b", "c", "a"\]).astype(CategoricalDtype(ordered=True)) s = s.sort\_values() s s.min(), s.max()

</div>

You can set categorical data to be ordered by using `as_ordered()` or unordered by using `as_unordered()`. These will by default return a *new* object.

<div class="ipython">

python

s.cat.as\_ordered() s.cat.as\_unordered()

</div>

Sorting will use the order defined by categories, not any lexical order present on the data type. This is even true for strings and numeric data:

<div class="ipython">

python

s = pd.Series(\[1, 2, 3, 1\], dtype="category") s = s.cat.set\_categories(\[2, 3, 1\], ordered=True) s s = s.sort\_values() s s.min(), s.max()

</div>

### Reordering

Reordering the categories is possible via the <span class="title-ref">Categorical.reorder\_categories</span> and the <span class="title-ref">Categorical.set\_categories</span> methods. For <span class="title-ref">Categorical.reorder\_categories</span>, all old categories must be included in the new categories and no new categories are allowed. This will necessarily make the sort order the same as the categories order.

<div class="ipython">

python

s = pd.Series(\[1, 2, 3, 1\], dtype="category") s = s.cat.reorder\_categories(\[2, 3, 1\], ordered=True) s s = s.sort\_values() s s.min(), s.max()

</div>

\> **Note** \> Note the difference between assigning new categories and reordering the categories: the first renames categories and therefore the individual values in the `Series`, but if the first position was sorted last, the renamed value will still be sorted last. Reordering means that the way values are sorted is different afterwards, but not that individual values in the `Series` are changed.

<div class="note">

<div class="title">

Note

</div>

If the `Categorical` is not ordered, <span class="title-ref">Series.min</span> and <span class="title-ref">Series.max</span> will raise `TypeError`. Numeric operations like `+`, `-`, `*`, `/` and operations based on them (e.g. <span class="title-ref">Series.median</span>, which would need to compute the mean between two values if the length of an array is even) do not work and raise a `TypeError`.

</div>

### Multi column sorting

A categorical dtyped column will participate in a multi-column sort in a similar manner to other columns. The ordering of the categorical is determined by the `categories` of that column.

<div class="ipython">

python

  - dfs = pd.DataFrame(
    
      - {
        
          - "A": pd.Categorical(  
            list("bbeebbaa"), categories=\["e", "a", "b"\], ordered=True,
        
        ), "B": \[1, 2, 1, 2, 2, 1, 2, 1\],
    
    }

) dfs.sort\_values(by=\["A", "B"\])

</div>

Reordering the `categories` changes a future sort.

<div class="ipython">

python

dfs\["A"\] = dfs\["A"\].cat.reorder\_categories(\["a", "b", "e"\]) dfs.sort\_values(by=\["A", "B"\])

</div>

## Comparisons

Comparing categorical data with other objects is possible in three cases:

  - Comparing equality (`==` and `!=`) to a list-like object (list, Series, array, ...) of the same length as the categorical data.
  - All comparisons (`==`, `!=`, `>`, `>=`, `<`, and `<=`) of categorical data to another categorical Series, when `ordered==True` and the `categories` are the same.
  - All comparisons of a categorical data to a scalar.

All other comparisons, especially "non-equality" comparisons of two categoricals with different categories or a categorical with any list-like object, will raise a `TypeError`.

\> **Note** \> Any "non-equality" comparisons of categorical data with a `Series`, `np.array`, `list` or categorical data with different categories or ordering will raise a `TypeError` because custom categories ordering could be interpreted in two ways: one with taking into account the ordering and one without.

<div class="ipython">

python

cat = pd.Series(\[1, 2, 3\]).astype(CategoricalDtype(\[3, 2, 1\], ordered=True)) cat\_base = pd.Series(\[2, 2, 2\]).astype(CategoricalDtype(\[3, 2, 1\], ordered=True)) cat\_base2 = pd.Series(\[2, 2, 2\]).astype(CategoricalDtype(ordered=True))

cat cat\_base cat\_base2

</div>

Comparing to a categorical with the same categories and ordering or to a scalar works:

<div class="ipython">

python

cat \> cat\_base cat \> 2

</div>

Equality comparisons work with any list-like object of same length and scalars:

<div class="ipython">

python

cat == cat\_base cat == np.array(\[1, 2, 3\]) cat == 2

</div>

This doesn't work because the categories are not the same:

<div class="ipython">

python

  - try:  
    cat \> cat\_base2

  - except TypeError as e:  
    print("TypeError:", str(e))

</div>

If you want to do a "non-equality" comparison of a categorical series with a list-like object which is not categorical data, you need to be explicit and convert the categorical data back to the original values:

<div class="ipython">

python

base = np.array(\[1, 2, 3\])

  - try:  
    cat \> base

  - except TypeError as e:  
    print("TypeError:", str(e))

np.asarray(cat) \> base

</div>

When you compare two unordered categoricals with the same categories, the order is not considered:

<div class="ipython">

python

c1 = pd.Categorical(\["a", "b"\], categories=\["a", "b"\], ordered=False) c2 = pd.Categorical(\["a", "b"\], categories=\["b", "a"\], ordered=False) c1 == c2

</div>

## Operations

Apart from <span class="title-ref">Series.min</span>, <span class="title-ref">Series.max</span> and <span class="title-ref">Series.mode</span>, the following operations are possible with categorical data:

`Series` methods like <span class="title-ref">Series.value\_counts</span> will use all categories, even if some categories are not present in the data:

<div class="ipython">

python

s = pd.Series(pd.Categorical(\["a", "b", "c", "c"\], categories=\["c", "a", "b", "d"\])) s.value\_counts()

</div>

`DataFrame` methods like <span class="title-ref">DataFrame.sum</span> also show "unused" categories when `observed=False`.

<div class="ipython">

python

  - columns = pd.Categorical(  
    \["One", "One", "Two"\], categories=\["One", "Two", "Three"\], ordered=True

) df = pd.DataFrame( data=\[\[1, 2, 3\], \[4, 5, 6\]\], columns=pd.MultiIndex.from\_arrays(\[\["A", "B", "B"\], columns\]), ).T df.groupby(level=1, observed=False).sum()

</div>

Groupby will also show "unused" categories when `observed=False`:

<div class="ipython">

python

  - cats = pd.Categorical(  
    \["a", "b", "b", "b", "c", "c", "c"\], categories=\["a", "b", "c", "d"\]

) df = pd.DataFrame({"cats": cats, "values": \[1, 2, 2, 2, 3, 4, 5\]}) df.groupby("cats", observed=False).mean()

cats2 = pd.Categorical(\["a", "a", "b", "b"\], categories=\["a", "b", "c"\]) df2 = pd.DataFrame( { "cats": cats2, "B": \["c", "d", "c", "d"\], "values": \[1, 2, 3, 4\], } ) df2.groupby(\["cats", "B"\], observed=False).mean()

</div>

Pivot tables:

<div class="ipython">

python

raw\_cat = pd.Categorical(\["a", "a", "b", "b"\], categories=\["a", "b", "c"\]) df = pd.DataFrame({"A": raw\_cat, "B": \["c", "d", "c", "d"\], "values": \[1, 2, 3, 4\]}) pd.pivot\_table(df, values="values", index=\["A", "B"\], observed=False)

</div>

## Data munging

The optimized pandas data access methods `.loc`, `.iloc`, `.at`, and `.iat`, work as normal. The only difference is the return type (for getting) and that only values already in `categories` can be assigned.

### Getting

If the slicing operation returns either a `DataFrame` or a column of type `Series`, the `category` dtype is preserved.

<div class="ipython">

python

idx = pd.Index(\["h", "i", "j", "k", "l", "m", "n"\]) cats = pd.Series(\["a", "b", "b", "b", "c", "c", "c"\], dtype="category", index=idx) values = \[1, 2, 2, 2, 3, 4, 5\] df = pd.DataFrame({"cats": cats, "values": values}, index=idx) df.iloc\[2:4, :\] df.iloc\[2:4, :\].dtypes df.loc\["h":"j", "cats"\] df\[df\["cats"\] == "b"\]

</div>

An example where the category type is not preserved is if you take one single row: the resulting `Series` is of dtype `object`:

<div class="ipython">

python

\# get the complete "h" row as a Series df.loc\["h", :\]

</div>

Returning a single item from categorical data will also return the value, not a categorical of length "1".

<div class="ipython">

python

df.iat\[0, 0\] df\["cats"\] = df\["cats"\].cat.rename\_categories(\["x", "y", "z"\]) df.at\["h", "cats"\] \# returns a string

</div>

<div class="note">

<div class="title">

Note

</div>

The is in contrast to R's `factor` function, where `factor(c(1,2,3))[1]` returns a single value `factor`.

</div>

To get a single value `Series` of type `category`, you pass in a list with a single value:

<div class="ipython">

python

df.loc\[\["h"\], "cats"\]

</div>

### String and datetime accessors

The accessors `.dt` and `.str` will work if the `s.cat.categories` are of an appropriate type:

<div class="ipython">

python

str\_s = pd.Series(list("aabb")) str\_cat = str\_s.astype("category") str\_cat str\_cat.str.contains("a")

date\_s = pd.Series(pd.date\_range("1/1/2015", periods=5)) date\_cat = date\_s.astype("category") date\_cat date\_cat.dt.day

</div>

\> **Note** \> The returned `Series` (or `DataFrame`) is of the same type as if you used the `.str.<method>` / `.dt.<method>` on a `Series` of that type (and not of type `category`\!).

That means, that the returned values from methods and properties on the accessors of a `Series` and the returned values from methods and properties on the accessors of this `Series` transformed to one of type `category` will be equal:

<div class="ipython">

python

ret\_s = str\_s.str.contains("a") ret\_cat = str\_cat.str.contains("a") ret\_s.dtype == ret\_cat.dtype ret\_s == ret\_cat

</div>

\> **Note** \> The work is done on the `categories` and then a new `Series` is constructed. This has some performance implication if you have a `Series` of type string, where lots of elements are repeated (i.e. the number of unique elements in the `Series` is a lot smaller than the length of the `Series`). In this case it can be faster to convert the original `Series` to one of type `category` and use `.str.<method>` or `.dt.<property>` on that.

### Setting

Setting values in a categorical column (or `Series`) works as long as the value is included in the `categories`:

<div class="ipython">

python

idx = pd.Index(\["h", "i", "j", "k", "l", "m", "n"\]) cats = pd.Categorical(\["a", "a", "a", "a", "a", "a", "a"\], categories=\["a", "b"\]) values = \[1, 1, 1, 1, 1, 1, 1\] df = pd.DataFrame({"cats": cats, "values": values}, index=idx)

df.iloc\[2:4, :\] = \[\["b", 2\], \["b", 2\]\] df try: df.iloc\[2:4, :\] = \[\["c", 3\], \["c", 3\]\] except TypeError as e: print("TypeError:", str(e))

</div>

Setting values by assigning categorical data will also check that the `categories` match:

<div class="ipython">

python

df.loc\["j":"k", "cats"\] = pd.Categorical(\["a", "a"\], categories=\["a", "b"\]) df try: df.loc\["j":"k", "cats"\] = pd.Categorical(\["b", "b"\], categories=\["a", "b", "c"\]) except TypeError as e: print("TypeError:", str(e))

</div>

Assigning a `Categorical` to parts of a column of other types will use the values:

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame({"a": \[1, 1, 1, 1, 1\], "b": \["a", "a", "a", "a", "a"\]}) df.loc\[1:2, "a"\] = pd.Categorical(\[2, 2\], categories=\[2, 3\]) df.loc\[2:3, "b"\] = pd.Categorical(\["b", "b"\], categories=\["a", "b"\]) df df.dtypes

</div>

### Merging / concatenation<span id="categorical.merge"></span>

By default, combining `Series` or `DataFrames` which contain the same categories results in `category` dtype, otherwise results will depend on the dtype of the underlying categories. Merges that result in non-categorical dtypes will likely have higher memory usage. Use `.astype` or `union_categoricals` to ensure `category` results.

<div class="ipython">

python

from pandas.api.types import union\_categoricals

\# same categories s1 = pd.Series(\["a", "b"\], dtype="category") s2 = pd.Series(\["a", "b", "a"\], dtype="category") pd.concat(\[s1, s2\])

\# different categories s3 = pd.Series(\["b", "c"\], dtype="category") pd.concat(\[s1, s3\])

\# Output dtype is inferred based on categories values int\_cats = pd.Series(\[1, 2\], dtype="category") float\_cats = pd.Series(\[3.0, 4.0\], dtype="category") pd.concat(\[int\_cats, float\_cats\])

pd.concat(\[s1, s3\]).astype("category") union\_categoricals(\[s1.array, s3.array\])

</div>

The following table summarizes the results of merging `Categoricals`:

| arg1              | arg2              | identical | result                     |
| ----------------- | ----------------- | --------- | -------------------------- |
| category          | category          | True      | category                   |
| category (object) | category (object) | False     | object (dtype is inferred) |
| category (int)    | category (float)  | False     | float (dtype is inferred)  |

### Unioning

If you want to combine categoricals that do not necessarily have the same categories, the <span class="title-ref">\~pandas.api.types.union\_categoricals</span> function will combine a list-like of categoricals. The new categories will be the union of the categories being combined.

<div class="ipython">

python

from pandas.api.types import union\_categoricals

a = pd.Categorical(\["b", "c"\]) b = pd.Categorical(\["a", "b"\]) union\_categoricals(\[a, b\])

</div>

By default, the resulting categories will be ordered as they appear in the data. If you want the categories to be lexsorted, use `sort_categories=True` argument.

<div class="ipython">

python

union\_categoricals(\[a, b\], sort\_categories=True)

</div>

`union_categoricals` also works with the "easy" case of combining two categoricals of the same categories and order information (e.g. what you could also `append` for).

<div class="ipython">

python

a = pd.Categorical(\["a", "b"\], ordered=True) b = pd.Categorical(\["a", "b", "a"\], ordered=True) union\_categoricals(\[a, b\])

</div>

The below raises `TypeError` because the categories are ordered and not identical.

<div class="ipython" data-okexcept="">

python

a = pd.Categorical(\["a", "b"\], ordered=True) b = pd.Categorical(\["a", "b", "c"\], ordered=True) union\_categoricals(\[a, b\])

</div>

Ordered categoricals with different categories or orderings can be combined by using the `ignore_ordered=True` argument.

<div class="ipython">

python

a = pd.Categorical(\["a", "b", "c"\], ordered=True) b = pd.Categorical(\["c", "b", "a"\], ordered=True) union\_categoricals(\[a, b\], ignore\_order=True)

</div>

<span class="title-ref">\~pandas.api.types.union\_categoricals</span> also works with a `CategoricalIndex`, or `Series` containing categorical data, but note that the resulting array will always be a plain `Categorical`:

<div class="ipython">

python

a = pd.Series(\["b", "c"\], dtype="category") b = pd.Series(\["a", "b"\], dtype="category") union\_categoricals(\[a, b\])

</div>

\> **Note** \> `union_categoricals` may recode the integer codes for categories when combining categoricals. This is likely what you want, but if you are relying on the exact numbering of the categories, be aware.

> 
> 
> <div class="ipython">
> 
> python
> 
> c1 = pd.Categorical(\["b", "c"\]) c2 = pd.Categorical(\["a", "b"\])
> 
> c1 \# "b" is coded to 0 c1.codes
> 
> c2 \# "b" is coded to 1 c2.codes
> 
> c = union\_categoricals(\[c1, c2\]) c \# "b" is coded to 0 throughout, same as c1, different from c2 c.codes
> 
> </div>

## Getting data in/out

You can write data that contains `category` dtypes to a `HDFStore`. See \[here \<io.hdf5-categorical\>\](\#here-\<io.hdf5-categorical\>) for an example and caveats.

It is also possible to write data to and reading data from *Stata* format files. See \[here \<io.stata-categorical\>\](\#here-\<io.stata-categorical\>) for an example and caveats.

Writing to a CSV file will convert the data, effectively removing any information about the categorical (categories and ordering). So if you read back the CSV file you have to convert the relevant columns back to `category` and assign the right categories and categories ordering.

<div class="ipython">

python

import io

s = pd.Series(pd.Categorical(\["a", "b", "b", "a", "a", "d"\])) \# rename the categories s = s.cat.rename\_categories(\["very good", "good", "bad"\]) \# reorder the categories and add missing categories s = s.cat.set\_categories(\["very bad", "bad", "medium", "good", "very good"\]) df = pd.DataFrame({"cats": s, "vals": \[1, 2, 3, 4, 5, 6\]}) csv = io.StringIO() df.to\_csv(csv) df2 = pd.read\_csv(io.StringIO(csv.getvalue())) df2.dtypes df2\["cats"\] \# Redo the category df2\["cats"\] = df2\["cats"\].astype("category") df2\["cats"\] = df2\["cats"\].cat.set\_categories( \["very bad", "bad", "medium", "good", "very good"\] ) df2.dtypes df2\["cats"\]

</div>

The same holds for writing to a SQL database with `to_sql`.

## Missing data

pandas primarily uses the value `np.nan` to represent missing data. It is by default not included in computations. See the \[Missing Data section \<missing\_data\>\](\#missing-data-section \<missing\_data\>).

Missing values should **not** be included in the Categorical's `categories`, only in the `values`. Instead, it is understood that NaN is different, and is always a possibility. When working with the Categorical's `codes`, missing values will always have a code of `-1`.

<div class="ipython">

python

s = pd.Series(\["a", "b", np.nan, "a"\], dtype="category") \# only two categories s s.cat.codes

</div>

Methods for working with missing data, e.g. <span class="title-ref">\~Series.isna</span>, <span class="title-ref">\~Series.fillna</span>, <span class="title-ref">\~Series.dropna</span>, all work normally:

<div class="ipython">

python

s = pd.Series(\["a", "b", np.nan\], dtype="category") s pd.isna(s) s.fillna("a")

</div>

## Differences to R's `factor`

The following differences to R's factor functions can be observed:

  - R's `levels` are named `categories`.
  - R's `levels` are always of type string, while `categories` in pandas can be of any dtype.
  - It's not possible to specify labels at creation time. Use `s.cat.rename_categories(new_labels)` afterwards.
  - In contrast to R's `factor` function, using categorical data as the sole input to create a new categorical series will *not* remove unused categories but create a new categorical series which is equal to the passed in one\!
  - R allows for missing values to be included in its `levels` (pandas' `categories`). pandas does not allow `NaN` categories, but missing values can still be in the `values`.

## Gotchas

### Memory usage

<div id="categorical.memory">

The memory usage of a `Categorical` is proportional to the number of categories plus the length of the data. In contrast, an `object` dtype is a constant times the length of the data.

</div>

<div class="ipython">

python

s = pd.Series(\["foo", "bar"\] \* 1000)

\# object dtype s.nbytes

\# category dtype s.astype("category").nbytes

</div>

\> **Note** \> If the number of categories approaches the length of the data, the `Categorical` will use nearly the same or more memory than an equivalent `object` dtype representation.

> 
> 
> <div class="ipython">
> 
> python
> 
> s = pd.Series(\["foo%04d" % i for i in range(2000)\])
> 
> \# object dtype s.nbytes
> 
> \# category dtype s.astype("category").nbytes
> 
> </div>

### `Categorical` is not a `numpy` array

Currently, categorical data and the underlying `Categorical` is implemented as a Python object and not as a low-level NumPy array dtype. This leads to some problems.

NumPy itself doesn't know about the new `dtype`:

<div class="ipython">

python

  - try:  
    np.dtype("category")

  - except TypeError as e:  
    print("TypeError:", str(e))

dtype = pd.Categorical(\["a"\]).dtype try: np.dtype(dtype) except TypeError as e: print("TypeError:", str(e))

</div>

Dtype comparisons work:

<div class="ipython">

python

dtype == [np.str]() [np.str]() == dtype

</div>

To check if a Series contains Categorical data, use `hasattr(s, 'cat')`:

<div class="ipython">

python

hasattr(pd.Series(\["a"\], dtype="category"), "cat") hasattr(pd.Series(\["a"\]), "cat")

</div>

Using NumPy functions on a `Series` of type `category` should not work as `Categoricals` are not numeric data (even in the case that `.categories` is numeric).

<div class="ipython">

python

s = pd.Series(pd.Categorical(\[1, 2, 3, 4\])) try: np.sum(s) \# same with np.log(s),... except TypeError as e: print("TypeError:", str(e))

</div>

<div class="note">

<div class="title">

Note

</div>

If such a function works, please file a bug at <https://github.com/pandas-dev/pandas>\!

</div>

### dtype in apply

pandas currently does not preserve the dtype in apply functions: If you apply along rows you get a `Series` of `object` `dtype` (same as getting a row -\> getting one element will return a basic type) and applying along columns will also convert to object. `NaN` values are unaffected. You can use `fillna` to handle missing values before applying a function.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "a": \[1, 2, 3, 4\], "b": \["a", "b", "c", "d"\], "cats": pd.Categorical(\[1, 2, 3, 2\]),
    
    }

) df.apply(lambda row: type(row\["cats"\]), axis=1) df.apply(lambda col: col.dtype, axis=0)

</div>

### Categorical index

`CategoricalIndex` is a type of index that is useful for supporting indexing with duplicates. This is a container around a `Categorical` and allows efficient indexing and storage of an index with a large number of duplicated elements. See the \[advanced indexing docs \<advanced.categoricalindex\>\](\#advanced-indexing-docs-\<advanced.categoricalindex\>) for a more detailed explanation.

Setting the index will create a `CategoricalIndex`:

<div class="ipython">

python

cats = pd.Categorical(\[1, 2, 3, 4\], categories=\[4, 2, 3, 1\]) strings = \["a", "b", "c", "d"\] values = \[4, 2, 3, 1\] df = pd.DataFrame({"strings": strings, "values": values}, index=cats) df.index \# This now sorts by the categories order df.sort\_index()

</div>

### Side effects

Constructing a `Series` from a `Categorical` will not copy the input `Categorical`. This means that changes to the `Series` will in most cases change the original `Categorical`:

<div class="ipython">

python

cat = pd.Categorical(\[1, 2, 3, 10\], categories=\[1, 2, 3, 4, 10\]) s = pd.Series(cat, name="cat") cat s.iloc\[0:2\] = 10 cat

</div>

Use `copy=True` to prevent such a behaviour or simply don't reuse `Categoricals`:

<div class="ipython">

python

cat = pd.Categorical(\[1, 2, 3, 10\], categories=\[1, 2, 3, 4, 10\]) s = pd.Series(cat, name="cat", copy=True) cat s.iloc\[0:2\] = 10 cat

</div>

\> **Note** \> This also happens in some cases when you supply a NumPy array instead of a `Categorical`: using an int array (e.g. `np.array([1,2,3,4])`) will exhibit the same behavior, while using a string array (e.g. `np.array(["a","b","c","a"])`) will not.

---

cookbook.md

---

<div id="cookbook">

{{ header }}

</div>

# Cookbook

This is a repository for *short and sweet* examples and links for useful pandas recipes. We encourage users to add to this documentation.

Adding interesting links and/or inline examples to this section is a great *First Pull Request*.

Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer.

pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.

## Idioms

<div id="cookbook.idioms">

These are some neat pandas `idioms`

</div>

[if-then/if-then-else on one column, and assignment to another one or more columns:](https://stackoverflow.com/questions/17128302/python-pandas-idiom-for-if-then-else)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df

</div>

### If-then...

An if-then on one column

<div class="ipython">

python

df.loc\[df.AAA \>= 5, "BBB"\] = -1 df

</div>

An if-then with assignment to 2 columns:

<div class="ipython">

python

df.loc\[df.AAA \>= 5, \["BBB", "CCC"\]\] = 555 df

</div>

Add another line with different logic, to do the -else

<div class="ipython">

python

df.loc\[df.AAA \< 5, \["BBB", "CCC"\]\] = 2000 df

</div>

Or use pandas where after you've set up a mask

<div class="ipython">

python

  - df\_mask = pd.DataFrame(  
    {"AAA": \[True\] \* 4, "BBB": \[False\] \* 4, "CCC": \[True, False\] \* 2}

) df.where(df\_mask, -1000)

</div>

[if-then-else using NumPy's where()](https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df df\["logic"\] = np.where(df\["AAA"\] \> 5, "high", "low") df

</div>

### Splitting

[Split a frame with a boolean criterion](https://stackoverflow.com/questions/14957116/how-to-split-a-dataframe-according-to-a-boolean-criterion)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df

df\[df.AAA \<= 5\] df\[df.AAA \> 5\]

</div>

### Building criteria

[Select with multi-column criteria](https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df

</div>

...and (without assignment returns a Series)

<div class="ipython">

python

df.loc\[(df\["BBB"\] \< 25) & (df\["CCC"\] \>= -40), "AAA"\]

</div>

...or (without assignment returns a Series)

<div class="ipython">

python

df.loc\[(df\["BBB"\] \> 25) | (df\["CCC"\] \>= -40), "AAA"\]

</div>

...or (with assignment modifies the DataFrame.)

<div class="ipython">

python

df.loc\[(df\["BBB"\] \> 25) | (df\["CCC"\] \>= 75), "AAA"\] = 999 df

</div>

[Select rows with data closest to certain value using argsort](https://stackoverflow.com/questions/17758023/return-rows-in-a-dataframe-closest-to-a-user-defined-number)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df aValue = 43.0 df.loc\[(df.CCC - aValue).abs().argsort()\]

</div>

[Dynamically reduce a list of criteria using a binary operators](https://stackoverflow.com/questions/21058254/pandas-boolean-operation-in-a-python-list/21058331)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df

Crit1 = df.AAA \<= 5.5 Crit2 = df.BBB == 10.0 Crit3 = df.CCC \> -40.0

</div>

One could hard code:

<div class="ipython">

python

AllCrit = Crit1 & Crit2 & Crit3

</div>

...Or it can be done with a list of dynamically built criteria

<div class="ipython">

python

import functools

CritList = \[Crit1, Crit2, Crit3\] AllCrit = functools.reduce(lambda x, y: x & y, CritList)

df\[AllCrit\]

</div>

## Selection

### DataFrames

The \[indexing \<indexing\>\](\#indexing-\<indexing\>) docs.

[Using both row labels and value conditionals](https://stackoverflow.com/questions/14725068/pandas-using-row-labels-in-boolean-indexing)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df

df\[(df.AAA \<= 6) & (df.index.isin(\[0, 2, 4\]))\]

</div>

Use loc for label-oriented slicing and iloc positional slicing `2904`

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}, index=\["foo", "bar", "boo", "kar"\],

)

</div>

There are 2 explicit slicing methods, with a third general case

1.  Positional-oriented (Python slicing style : exclusive of end)
2.  Label-oriented (Non-Python slicing style : inclusive of end)
3.  General (Either slicing style : depends on if the slice contains labels or positions)

<div class="ipython">

python df.iloc\[0:3\] \# Positional

df.loc\["bar":"kar"\] \# Label

\# Generic df\[0:3\] df\["bar":"kar"\]

</div>

Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment.

<div class="ipython">

python

data = {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]} df2 = pd.DataFrame(data=data, index=\[1, 2, 3, 4\]) \# Note index starts at 1. df2.iloc\[1:3\] \# Position-oriented df2.loc\[1:3\] \# Label-oriented

</div>

[Using inverse operator (\~) to take the complement of a mask](https://stackoverflow.com/q/14986510)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[4, 5, 6, 7\], "BBB": \[10, 20, 30, 40\], "CCC": \[100, 50, -30, -50\]}

) df

df\[\~((df.AAA \<= 6) & (df.index.isin(\[0, 2, 4\])))\]

</div>

### New columns

[Efficiently and dynamically creating new columns using DataFrame.map (previously named applymap)](https://stackoverflow.com/questions/16575868/efficiently-creating-additional-columns-in-a-pandas-dataframe-using-map)

<div class="ipython">

python

df = pd.DataFrame({"AAA": \[1, 2, 1, 3\], "BBB": \[1, 1, 2, 2\], "CCC": \[2, 1, 3, 1\]}) df

source\_cols = df.columns \# Or some subset would work too new\_cols = \[str(x) + "\_cat" for x in source\_cols\] categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

df\[new\_cols\] = df\[source\_cols\].map(categories.get) df

</div>

[Keep other columns when using min() with groupby](https://stackoverflow.com/q/23394476)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"AAA": \[1, 1, 1, 2, 2, 2, 3, 3\], "BBB": \[2, 1, 3, 4, 5, 1, 2, 3\]}

) df

</div>

Method 1 : idxmin() to get the index of the minimums

<div class="ipython">

python

df.loc\[df.groupby("AAA")\["BBB"\].idxmin()\]

</div>

Method 2 : sort then take first of each

<div class="ipython">

python

df.sort\_values(by="BBB").groupby("AAA", as\_index=False).first()

</div>

Notice the same results, with the exception of the index.

## Multiindexing

The \[multindexing \<advanced.hierarchical\>\](\#multindexing-\<advanced.hierarchical\>) docs.

[Creating a MultiIndex from a labeled frame](https://stackoverflow.com/questions/14916358/reshaping-dataframes-in-pandas-based-on-column-labels)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "row": \[0, 1, 2\], "One\_X": \[1.1, 1.1, 1.1\], "One\_Y": \[1.2, 1.2, 1.2\], "Two\_X": \[1.11, 1.11, 1.11\], "Two\_Y": \[1.22, 1.22, 1.22\],
    
    }

) df

\# As Labelled Index df = df.set\_index("row") df \# With Hierarchical Columns df.columns = pd.MultiIndex.from\_tuples(\[tuple(c.split("\_")) for c in df.columns\]) df \# Now stack & Reset df = df.stack(0).reset\_index(1) df \# And fix the labels (Notice the label 'level\_1' got added automatically) df.columns = \["Sample", "All\_X", "All\_Y"\] df

</div>

### Arithmetic

[Performing arithmetic with a MultiIndex that needs broadcasting](https://stackoverflow.com/questions/19501510/divide-entire-pandas-multiindex-dataframe-by-dataframe-variable/19502176#19502176)

<div class="ipython">

python

  - cols = pd.MultiIndex.from\_tuples(  
    \[(x, y) for x in \["A", "B", "C"\] for y in \["O", "I"\]\]

) df = pd.DataFrame(np.random.randn(2, 6), index=\["n", "m"\], columns=cols) df df = df.div(df\["C"\], level=1) df

</div>

### Slicing

[Slicing a MultiIndex with xs](https://stackoverflow.com/questions/12590131/how-to-slice-multindex-columns-in-pandas-dataframes)

<div class="ipython">

python

coords = \[("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")\] index = pd.MultiIndex.from\_tuples(coords) df = pd.DataFrame(\[11, 22, 33, 44, 55\], index, \["MyData"\]) df

</div>

To take the cross section of the 1st level and 1st axis the index:

<div class="ipython">

python

\# Note : level and axis are optional, and default to zero df.xs("BB", level=0, axis=0)

</div>

...and now the 2nd level of the 1st axis.

<div class="ipython">

python

df.xs("six", level=1, axis=0)

</div>

[Slicing a MultiIndex with xs, method \#2](https://stackoverflow.com/questions/14964493/multiindex-based-indexing-in-pandas)

<div class="ipython">

python

import itertools

index = list(itertools.product(\["Ada", "Quinn", "Violet"\], \["Comp", "Math", "Sci"\])) headr = list(itertools.product(\["Exams", "Labs"\], \["I", "II"\])) indx = pd.MultiIndex.from\_tuples(index, names=\["Student", "Course"\]) cols = pd.MultiIndex.from\_tuples(headr) \# Notice these are un-named data = \[\[70 + x + y + (x \* y) % 3 for x in range(4)\] for y in range(9)\] df = pd.DataFrame(data, indx, cols) df

All = slice(None) df.loc\["Violet"\] df.loc\[(All, "Math"), All\] df.loc\[(slice("Ada", "Quinn"), "Math"), All\] df.loc\[(All, "Math"), ("Exams")\] df.loc\[(All, "Math"), (All, "II")\]

</div>

[Setting portions of a MultiIndex with xs](https://stackoverflow.com/questions/19319432/pandas-selecting-a-lower-level-in-a-dataframe-to-do-a-ffill)

### Sorting

[Sort by specific column or an ordered list of columns, with a MultiIndex](https://stackoverflow.com/q/14733871)

<div class="ipython">

python

df.sort\_values(by=("Labs", "II"), ascending=False)

</div>

Partial selection, the need for sortedness `2995`

### Levels

[Prepending a level to a multiindex](https://stackoverflow.com/questions/14744068/prepend-a-level-to-a-pandas-multiindex)

[Flatten Hierarchical columns](https://stackoverflow.com/q/14507794)

## Missing data

The \[missing data\<missing\_data\>\](\#missing-data\<missing\_data\>) docs.

Fill forward a reversed timeseries

<div class="ipython">

python

  - df = pd.DataFrame(  
    np.random.randn(6, 1), index=pd.date\_range("2013-08-01", periods=6, freq="B"), columns=list("A"),

) df.loc\[df.index\[3\], "A"\] = np.nan df df.bfill()

</div>

[cumsum reset at NaN values](https://stackoverflow.com/questions/18196811/cumsum-reset-at-nan)

### Replace

[Using replace with backrefs](https://stackoverflow.com/questions/16818871/extracting-value-and-creating-new-column-out-of-it)

## Grouping

The \[grouping \<groupby\>\](\#grouping-\<groupby\>) docs.

[Basic grouping with apply](https://stackoverflow.com/questions/15322632/python-pandas-df-groupy-agg-column-reference-in-agg)

Unlike agg, apply's callable is passed a sub-DataFrame which gives you access to all the columns

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "animal": "cat dog cat fish dog cat cat".split(), "size": list("SSMMMLL"), "weight": \[8, 10, 11, 1, 20, 12, 12\], "adult": \[False\] \* 5 + \[True\] \* 2,
    
    }

) df

\# List the size of the animals with the highest weight. df.groupby("animal").apply(lambda subf: subf\["size"\]\[subf\["weight"\].idxmax()\], include\_groups=False)

</div>

[Using get\_group](https://stackoverflow.com/questions/14734533/how-to-access-pandas-groupby-dataframe-by-key)

<div class="ipython">

python

gb = df.groupby("animal") gb.get\_group("cat")

</div>

[Apply to different items in a group](https://stackoverflow.com/questions/15262134/apply-different-functions-to-different-items-in-group-object-python-pandas)

<div class="ipython">

python

  - def GrowUp(x):  
    avg\_weight = sum(x\[x\["size"\] == "S"\].weight \* 1.5) avg\_weight += sum(x\[x\["size"\] == "M"\].weight \* 1.25) avg\_weight += sum(x\[x\["size"\] == "L"\].weight) avg\_weight /= len(x) return pd.Series(\["L", avg\_weight, True\], index=\["size", "weight", "adult"\])

expected\_df = gb.apply(GrowUp, include\_groups=False) expected\_df

</div>

[Expanding apply](https://stackoverflow.com/questions/14542145/reductions-down-a-column-in-pandas)

<div class="ipython">

python

S = pd.Series(\[i / 100.0 for i in range(1, 11)\])

  - def cum\_ret(x, y):  
    return x \* (1 + y)

  - def red(x):  
    return functools.reduce(cum\_ret, x, 1.0)

S.expanding().apply(red, raw=True)

</div>

[Replacing some values with mean of the rest of a group](https://stackoverflow.com/questions/14760757/replacing-values-with-groupby-means)

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 1, 2, 2\], "B": \[1, -1, 1, 2\]}) gb = df.groupby("A")

  - def replace(g):  
    mask = g \< 0 return g.where(\~mask, g\[\~mask\].mean())

gb.transform(replace)

</div>

[Sort groups by aggregated data](https://stackoverflow.com/questions/14941366/pandas-sort-by-group-aggregate-and-column)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "code": \["foo", "bar", "baz"\] \* 2, "data": \[0.16, -0.21, 0.33, 0.45, -0.59, 0.62\], "flag": \[False, True\] \* 3,
    
    }

)

code\_groups = df.groupby("code")

agg\_n\_sort\_order = code\_groups\[\["data"\]\].transform("sum").sort\_values(by="data")

sorted\_df = df.loc\[agg\_n\_sort\_order.index\]

sorted\_df

</div>

[Create multiple aggregated columns](https://stackoverflow.com/questions/14897100/create-multiple-columns-in-pandas-aggregation-function)

<div class="ipython">

python

rng = pd.date\_range(start="2014-10-07", periods=10, freq="2min") ts = pd.Series(data=list(range(10)), index=rng)

  - def MyCust(x):
    
      - if len(x) \> 2:  
        return x.iloc\[1\] \* 1.234
    
    return pd.NaT

mhc = {"Mean": "mean", "Max": "max", "Custom": MyCust} ts.resample("5min").apply(mhc) ts

</div>

[Create a value counts column and reassign back to the DataFrame](https://stackoverflow.com/q/17709270)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"Color": "Red Red Red Blue".split(), "Value": \[100, 150, 50, 50\]}

) df df\["Counts"\] = df.groupby(\["Color"\]).transform(len) df

</div>

[Shift groups of the values in a column based on the index](https://stackoverflow.com/q/23198053/190597)

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"line\_race": \[10, 10, 8, 10, 10, 8\], "beyer": \[99, 102, 103, 103, 88, 100\]}, index=\[ "Last Gunfighter", "Last Gunfighter", "Last Gunfighter", "Paynter", "Paynter", "Paynter", \],

) df df\["beyer\_shifted"\] = df.groupby(level=0)\["beyer"\].shift(1) df

</div>

[Select row with maximum value from each group](https://stackoverflow.com/q/26701849/190597)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "host": \["other", "other", "that", "this", "this"\], "service": \["mail", "web", "mail", "mail", "web"\], "no": \[1, 2, 1, 2, 1\],
    
    }

).set\_index(\["host", "service"\]) mask = df.groupby(level=0).agg("idxmax") df\_count = df.loc\[mask\["no"\]\].reset\_index() df\_count

</div>

[Grouping like Python's itertools.groupby](https://stackoverflow.com/q/29142487/846892)

<div class="ipython">

python

df = pd.DataFrame(\[0, 1, 0, 1, 1, 1, 0, 1, 1\], columns=\["A"\]) df\["A"\].groupby((df\["A"\] \!= df\["A"\].shift()).cumsum()).groups df\["A"\].groupby((df\["A"\] \!= df\["A"\].shift()).cumsum()).cumsum()

</div>

### Expanding data

[Alignment and to-date](https://stackoverflow.com/questions/15489011/python-time-series-alignment-and-to-date-functions)

[Rolling Computation window based on values instead of counts](https://stackoverflow.com/questions/14300768/pandas-rolling-computation-with-window-based-on-values-instead-of-counts)

[Rolling Mean by Time Interval](https://stackoverflow.com/questions/15771472/pandas-rolling-mean-by-time-interval)

### Splitting

[Splitting a frame](https://stackoverflow.com/questions/13353233/best-way-to-split-a-dataframe-given-an-edge/15449992#15449992)

Create a list of dataframes, split using a delineation based on logic included in rows.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - data={  
        "Case": \["A", "A", "A", "B", "A", "A", "B", "A", "A"\], "Data": np.random.randn(9),
    
    }

)

  - dfs = list(
    
      - zip(
        
          - *df.groupby( (1* (df\["Case"\] == "B"))  
            .cumsum() .rolling(window=3, min\_periods=1) .median()
        
        )
    
    )

)\[-1\]

dfs\[0\] dfs\[1\] dfs\[2\]

</div>

### Pivot

The \[Pivot \<reshaping.pivot\>\](\#pivot-\<reshaping.pivot\>) docs.

[Partial sums and subtotals](https://stackoverflow.com/a/15574875)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - data={  
        "Province": \["ON", "QC", "BC", "AL", "AL", "MN", "ON"\], "City": \[ "Toronto", "Montreal", "Vancouver", "Calgary", "Edmonton", "Winnipeg", "Windsor", \], "Sales": \[13, 6, 16, 8, 4, 3, 1\],
    
    }

) table = pd.pivot\_table( df, values=\["Sales"\], index=\["Province"\], columns=\["City"\], aggfunc="sum", margins=True, ) table.stack("City")

</div>

[Frequency table like plyr in R](https://stackoverflow.com/questions/15589354/frequency-tables-in-pandas-like-plyr-in-r)

<div class="ipython">

python

grades = \[48, 99, 75, 80, 42, 80, 72, 68, 36, 78\] df = pd.DataFrame( { "ID": \["x%d" % r for r in range(10)\], "Gender": \["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"\], "ExamYear": \[ "2007", "2007", "2007", "2008", "2008", "2008", "2008", "2009", "2009", "2009", \], "Class": \[ "algebra", "stats", "bio", "algebra", "algebra", "stats", "stats", "algebra", "bio", "bio", \], "Participated": \[ "yes", "yes", "yes", "yes", "no", "yes", "yes", "yes", "yes", "yes", \], "Passed": \["yes" if x \> 50 else "no" for x in grades\], "Employed": \[ True, True, True, False, False, False, False, True, True, False, \], "Grade": grades, } )

  - df.groupby("ExamYear").agg(
    
      - {  
        "Participated": lambda x: x.value\_counts()\["yes"\], "Passed": lambda x: sum(x == "yes"), "Employed": lambda x: sum(x), "Grade": lambda x: sum(x) / len(x),
    
    }

)

</div>

[Plot pandas DataFrame with year over year data](https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data)

To create year and month cross tabulation:

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"value": np.random.randn(36)}, index=pd.date\_range("2011-01-01", freq="ME", periods=36),

)

  - pd.pivot\_table(  
    df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"

)

</div>

### Apply

[Rolling apply to organize - Turning embedded lists into a MultiIndex frame](https://stackoverflow.com/questions/17349981/converting-pandas-dataframe-with-categorical-values-into-binary-values)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - data={  
        "A": \[\[2, 4, 8, 16\], \[100, 200\], \[10, 20, 30\]\], "B": \[\["a", "b", "c"\], \["jj", "kk"\], \["ccc"\]\],
    
    }, index=\["I", "II", "III"\],

)

  - def SeriesFromSubList(aList):  
    return pd.Series(aList)

  - df\_orgz = pd.concat(  
    {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}

) df\_orgz

</div>

[Rolling apply with a DataFrame returning a Series](https://stackoverflow.com/questions/19121854/using-rolling-apply-on-a-dataframe-object)

Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned

<div class="ipython">

python

  - df = pd.DataFrame(  
    data=np.random.randn(2000, 2) / 10000, index=pd.date\_range("2001-01-01", periods=2000), columns=\["A", "B"\],

) df

  - def gm(df, const):  
    v = ((((df\["A"\] + df\["B"\]) + 1).cumprod()) - 1) \* const return v.iloc\[-1\]

  - s = pd.Series(
    
      - {  
        df.index\[i\]: gm(df.iloc\[i: min(i + 51, len(df) - 1)\], 5) for i in range(len(df) - 50)
    
    }

) s

</div>

[Rolling apply with a DataFrame returning a Scalar](https://stackoverflow.com/questions/21040766/python-pandas-rolling-apply-two-column-input-into-function/21045831#21045831)

Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price)

<div class="ipython">

python

rng = pd.date\_range(start="2014-01-01", periods=100) df = pd.DataFrame( { "Open": np.random.randn(len(rng)), "Close": np.random.randn(len(rng)), "Volume": np.random.randint(100, 2000, len(rng)), }, index=rng, ) df

  - def vwap(bars):  
    return (bars.Close \* bars.Volume).sum() / bars.Volume.sum()

window = 5 s = pd.concat( \[ (pd.Series(vwap(df.iloc\[i: i + window\]), index=\[df.index\[i + window\]\])) for i in range(len(df) - window) \] ) s.round(2)

</div>

## Timeseries

[Between times](https://stackoverflow.com/questions/14539992/pandas-drop-rows-outside-of-time-range)

[Using indexer between time](https://stackoverflow.com/questions/17559885/pandas-dataframe-mask-based-on-index)

[Constructing a datetime range that excludes weekends and includes only certain times](https://stackoverflow.com/a/24014440)

[Vectorized Lookup](https://stackoverflow.com/questions/13893227/vectorized-look-up-of-values-in-pandas-dataframe)

[Aggregation and plotting time series](https://nipunbatra.github.io/blog/visualisation/2013/05/01/aggregation-timeseries.html)

Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. [How to rearrange a Python pandas DataFrame?](https://stackoverflow.com/questions/15432659/how-to-rearrange-a-python-pandas-dataframe)

[Dealing with duplicates when reindexing a timeseries to a specified frequency](https://stackoverflow.com/questions/22244383/pandas-df-refill-adding-two-columns-of-different-shape)

Calculate the first day of the month for each entry in a DatetimeIndex

<div class="ipython">

python

dates = pd.date\_range("2000-01-01", periods=5) dates.to\_period(freq="M").to\_timestamp()

</div>

### Resampling

The \[Resample \<timeseries.resampling\>\](\#resample-\<timeseries.resampling\>) docs.

[Using Grouper instead of TimeGrouper for time grouping of values](https://stackoverflow.com/questions/15297053/how-can-i-divide-single-values-of-a-dataframe-by-monthly-averages)

[Time grouping with some missing values](https://stackoverflow.com/questions/33637312/pandas-grouper-by-frequency-with-completeness-requirement)

Valid frequency arguments to Grouper \[Timeseries \<timeseries.offset\_aliases\>\](\#timeseries-\<timeseries.offset\_aliases\>)

[Grouping using a MultiIndex](https://stackoverflow.com/questions/41483763/pandas-timegrouper-on-multiindex)

Using TimeGrouper and another grouping to create subgroups, then apply a custom function `3791`

[Resampling with custom periods](https://stackoverflow.com/questions/15408156/resampling-with-custom-periods)

[Resample intraday frame without adding new days](https://stackoverflow.com/questions/14898574/resample-intraday-pandas-dataframe-without-add-new-days)

[Resample minute data](https://stackoverflow.com/questions/14861023/resampling-minute-data)

[Resample with groupby](https://stackoverflow.com/q/18677271/564538)

## Merge

The \[Join \<merging.join\>\](\#join-\<merging.join\>) docs.

[Concatenate two dataframes with overlapping index (emulate R rbind)](https://stackoverflow.com/questions/14988480/pandas-version-of-rbind)

<div class="ipython">

python

rng = pd.date\_range("2000-01-01", periods=6) df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=\["A", "B", "C"\]) df2 = df1.copy()

</div>

Depending on df construction, `ignore_index` may be needed

<div class="ipython">

python

df = pd.concat(\[df1, df2\], ignore\_index=True) df

</div>

Self Join of a DataFrame `2996`

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - data={  
        "Area": \["A"\] \* 5 + \["C"\] \* 2, "Bins": \[110\] \* 2 + \[160\] \* 3 + \[40\] \* 2, "Test\_0": \[0, 1, 0, 1, 2, 0, 1\], "Data": np.random.randn(7),
    
    }

) df

df\["Test\_1"\] = df\["Test\_0"\] - 1

  - pd.merge(  
    df, df, left\_on=\["Bins", "Area", "Test\_0"\], right\_on=\["Bins", "Area", "Test\_1"\], suffixes=("\_L", "\_R"),

)

</div>

[How to set the index and join](https://stackoverflow.com/questions/14341805/pandas-merge-pd-merge-how-to-set-the-index-and-join)

[KDB like asof join](https://stackoverflow.com/questions/12322289/kdb-like-asof-join-for-timeseries-data-in-pandas/12336039#12336039)

[Join with a criteria based on the values](https://stackoverflow.com/questions/15581829/how-to-perform-an-inner-or-outer-join-of-dataframes-with-pandas-on-non-simplisti)

[Using searchsorted to merge based on values inside a range](https://stackoverflow.com/questions/25125626/pandas-merge-with-logic/2512764)

## Plotting

The \[Plotting \<visualization\>\](\#plotting-\<visualization\>) docs.

[Make Matplotlib look like R](https://stackoverflow.com/questions/14349055/making-matplotlib-graphs-look-like-r-by-default)

[Setting x-axis major and minor labels](https://stackoverflow.com/questions/12945971/pandas-timeseries-plot-setting-x-axis-major-and-minor-ticks-and-labels)

[Plotting multiple charts in an IPython Jupyter notebook](https://stackoverflow.com/questions/16392921/make-more-than-one-chart-in-same-ipython-notebook-cell)

[Creating a multi-line plot](https://stackoverflow.com/questions/16568964/make-a-multiline-plot-from-csv-file-in-matplotlib)

[Plotting a heatmap](https://stackoverflow.com/questions/17050202/plot-timeseries-of-histograms-in-python)

[Annotate a time-series plot](https://stackoverflow.com/questions/11067368/annotate-time-series-plot-in-matplotlib)

[Annotate a time-series plot \#2](https://stackoverflow.com/questions/17891493/annotating-points-from-a-pandas-dataframe-in-matplotlib-plot)

[Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter](https://pandas-xlsxwriter-charts.readthedocs.io/)

[Boxplot for each quartile of a stratifying variable](https://stackoverflow.com/questions/23232989/boxplot-stratified-by-column-in-python-pandas)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "stratifying\_var": np.random.uniform(0, 100, 20), "price": np.random.normal(100, 5, 20),
    
    }

)

  - df\["quartiles"\] = pd.qcut(  
    df\["stratifying\_var"\], 4, labels=\["0-25%", "25-50%", "50-75%", "75-100%"\]

)

@savefig quartile\_boxplot.png df.boxplot(column="price", by="quartiles")

</div>

## Data in/out

[Performance comparison of SQL vs HDF5](https://stackoverflow.com/q/16628329)

### CSV

The \[CSV \<io.read\_csv\_table\>\](\#csv-\<io.read\_csv\_table\>) docs

[read\_csv in action](https://wesmckinney.com/blog/update-on-upcoming-pandas-v0-10-new-file-parser-other-performance-wins/)

[appending to a csv](https://stackoverflow.com/questions/17134942/pandas-dataframe-output-end-of-csv)

[Reading a csv chunk-by-chunk](https://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas/12193309#12193309)

[Reading only certain rows of a csv chunk-by-chunk](https://stackoverflow.com/questions/19674212/pandas-data-frame-select-rows-and-clear-memory)

[Reading the first few lines of a frame](https://stackoverflow.com/questions/15008970/way-to-read-first-few-lines-for-pandas-dataframe)

Reading a file that is compressed but not by `gzip/bz2` (the native compressed formats which `read_csv` understands). This example shows a `WinZipped` file, but is a general application of opening the file within a context manager and using that handle to read. [See here](https://stackoverflow.com/questions/17789907/pandas-convert-winzipped-csv-file-to-data-frame)

[Inferring dtypes from a file](https://stackoverflow.com/questions/15555005/get-inferred-dataframe-types-iteratively-using-chunksize)

Dealing with bad lines `2886`

[Write a multi-row index CSV without writing duplicates](https://stackoverflow.com/questions/17349574/pandas-write-multiindex-rows-with-to-csv)

#### Reading multiple files to create a single DataFrame

The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using \`pd.concat\`:

<div class="ipython">

python

  - for i in range(3):  
    data = pd.DataFrame(np.random.randn(10, 4)) data.to\_csv("[file](){}.csv".format(i))

files = \["file\_0.csv", "file\_1.csv", "file\_2.csv"\] result = pd.concat(\[pd.read\_csv(f) for f in files\], ignore\_index=True)

</div>

You can use the same approach to read all files matching a pattern. Here is an example using `glob`:

<div class="ipython">

python

import glob import os

files = glob.glob("[file]()\*.csv") result = pd.concat(\[pd.read\_csv(f) for f in files\], ignore\_index=True)

</div>

Finally, this strategy will work with the other `pd.read_*(...)` functions described in the \[io docs\<io\>\](\#io-docs\<io\>).

<div class="ipython" data-suppress="">

python

  - for i in range(3):  
    os.remove("[file](){}.csv".format(i))

</div>

#### Parsing date components in multi-columns

Parsing date components in multi-columns is faster with a format

<div class="ipython">

python

i = pd.date\_range("20000101", periods=10000) df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day}) df.head()

%timeit pd.to\_datetime(df.year \* 10000 + df.month \* 100 + df.day, format='%Y%m%d') ds = df.apply(lambda x: "%04d%02d%02d" % (x\["year"\], x\["month"\], x\["day"\]), axis=1) ds.head() %timeit pd.to\_datetime(ds)

</div>

#### Skip row between header and data

<div class="ipython">

python

  - data = """;;;;
    
    ##### ;;;;
    
    ###### ;;;;

  - ;;;;
    
    ###### ;;;;

;;;; date;Param1;Param2;Param4;Param5 ;mÂ²;Â°C;mÂ²;m ;;;; 01.01.1990 00:00;1;1;2;3 01.01.1990 01:00;5;3;4;5 01.01.1990 02:00;9;5;6;7 01.01.1990 03:00;13;7;8;9 01.01.1990 04:00;17;9;10;11 01.01.1990 05:00;21;11;12;13 """

</div>

# Option 1: pass rows explicitly to skip rows

<div class="ipython">

python

from io import StringIO

  - pd.read\_csv(  
    StringIO(data), sep=";", skiprows=\[11, 12\], index\_col=0, parse\_dates=True, header=10,

)

</div>

# Option 2: read column names and then data

<div class="ipython">

python

pd.read\_csv(StringIO(data), sep=";", header=10, nrows=10).columns columns = pd.read\_csv(StringIO(data), sep=";", header=10, nrows=10).columns pd.read\_csv( StringIO(data), sep=";", index\_col=0, header=12, parse\_dates=True, names=columns )

</div>

### SQL

The \[SQL \<io.sql\>\](\#sql-\<io.sql\>) docs

[Reading from databases with SQL](https://stackoverflow.com/questions/10065051/python-pandas-and-databases-like-mysql)

### Excel

The \[Excel \<io.excel\>\](\#excel-\<io.excel\>) docs

[Reading from a filelike handle](https://stackoverflow.com/questions/15588713/sheets-of-excel-workbook-from-a-url-into-a-pandas-dataframe)

[Modifying formatting in XlsxWriter output](https://pbpython.com/improve-pandas-excel-output.html)

Loading only visible sheets `19842#issuecomment-892150745`

### HTML

[Reading HTML tables from a server that cannot handle the default request header](https://stackoverflow.com/a/18939272/564538)

### HDFStore

The \[HDFStores \<io.hdf5\>\](\#hdfstores-\<io.hdf5\>) docs

[Simple queries with a Timestamp Index](https://stackoverflow.com/questions/13926089/selecting-columns-from-pandas-hdfstore-table)

Managing heterogeneous data using a linked multiple table hierarchy `3032`

[Merging on-disk tables with millions of rows](https://stackoverflow.com/questions/14614512/merging-two-tables-with-millions-of-rows-in-python/14617925#14617925)

[Avoiding inconsistencies when writing to a store from multiple processes/threads](https://stackoverflow.com/a/29014295/2858145)

De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. [See here](https://stackoverflow.com/questions/16110252/need-to-compare-very-large-files-around-1-5gb-in-python/16110391#16110391)

[Creating a store chunk-by-chunk from a csv file](https://stackoverflow.com/questions/20428355/appending-column-to-frame-of-hdf-file-in-pandas/20428786#20428786)

[Appending to a store, while creating a unique index](https://stackoverflow.com/questions/16997048/how-does-one-append-large-amounts-of-data-to-a-pandas-hdfstore-and-get-a-natural/16999397#16999397)

[Large Data work flows](https://stackoverflow.com/q/14262433)

[Reading in a sequence of files, then providing a global unique index to a store while appending](https://stackoverflow.com/questions/16997048/how-does-one-append-large-amounts-of-data-to-a-pandas-hdfstore-and-get-a-natural)

[Groupby on a HDFStore with low group density](https://stackoverflow.com/questions/15798209/pandas-group-by-query-on-large-data-in-hdfstore)

[Groupby on a HDFStore with high group density](https://stackoverflow.com/questions/25459982/trouble-with-grouby-on-millions-of-keys-on-a-chunked-file-in-python-pandas/25471765#25471765)

[Hierarchical queries on a HDFStore](https://stackoverflow.com/questions/22777284/improve-query-performance-from-a-large-hdfstore-table-with-pandas/22820780#22820780)

[Counting with a HDFStore](https://stackoverflow.com/questions/20497897/converting-dict-of-dicts-into-pandas-dataframe-memory-issues)

[Troubleshoot HDFStore exceptions](https://stackoverflow.com/questions/15488809/how-to-trouble-shoot-hdfstore-exception-cannot-find-the-correct-atom-type)

[Setting min\_itemsize with strings](https://stackoverflow.com/questions/15988871/hdfstore-appendstring-dataframe-fails-when-string-column-contents-are-longer)

[Using ptrepack to create a completely-sorted-index on a store](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)

Storing Attributes to a group node

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(8, 3)) store = pd.HDFStore("test.h5") store.put("df", df)

\# you can store an arbitrary Python object via pickle store.get\_storer("df").attrs.my\_attribute = {"A": 10} store.get\_storer("df").attrs.my\_attribute

</div>

<div class="ipython" data-suppress="">

python

store.close() os.remove("test.h5")

</div>

You can create or load a HDFStore in-memory by passing the `driver` parameter to PyTables. Changes are only written to disk when the HDFStore is closed.

<div class="ipython">

python

store = pd.HDFStore("test.h5", "w", driver="H5FD\_CORE")

df = pd.DataFrame(np.random.randn(8, 3)) store\["test"\] = df

\# only after closing the store, data is written to disk: store.close()

</div>

<div class="ipython" data-suppress="">

python

os.remove("test.h5")

</div>

### Binary files

pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called `main.c` compiled with `gcc main.c -std=gnu99` on a 64-bit machine,

`` `c    #include <stdio.h>    #include <stdint.h>     typedef struct _Data    {        int32_t count;        double avg;        float scale;    } Data;     int main(int argc, const char *argv[])    {        size_t n = 10;        Data d[n];         for (int i = 0; i < n; ++i)        {            d[i].count = i;            d[i].avg = i + 1.0;            d[i].scale = (float) i + 2.0f;        }         FILE *file = fopen("binary.dat", "wb");        fwrite(&d, sizeof(Data), n, file);        fclose(file);         return 0;    }  the following Python code will read the binary file ``'binary.dat'`into a`<span class="title-ref"> pandas </span><span class="title-ref">DataFrame</span>\`, where each element of the struct corresponds to a column in the frame:

`` `python    names = "count", "avg", "scale"     # note that the offsets are larger than the size of the type because of    # struct padding    offsets = 0, 8, 16    formats = "i4", "f8", "f4"    dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)    df = pd.DataFrame(np.fromfile("binary.dat", dt))  > **Note** >     The offsets of the structure elements may be different depending on the    architecture of the machine on which the file was created. Using a raw    binary file format like this for general data storage is not recommended, as    it is not cross platform. We recommended either HDF5 or parquet, both of    which are supported by pandas' IO facilities.  Computation ``\` -----------

[Numerical integration (sample-based) of a time series](https://nbviewer.ipython.org/gist/metakermit/5720498)

### Correlation

Often it's useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from <span class="title-ref">DataFrame.corr</span>. This can be achieved by passing a boolean mask to `where` as follows:

<div class="ipython">

python

df = pd.DataFrame(np.random.random(size=(100, 5)))

corr\_mat = df.corr() mask = np.tril(np.ones\_like(corr\_mat, dtype=np.[bool]()), k=-1)

corr\_mat.where(mask)

</div>

The `method` argument within `DataFrame.corr` can accept a callable in addition to the named correlation types. Here we compute the [distance correlation](https://en.wikipedia.org/wiki/Distance_correlation) matrix for a `DataFrame` object.

<div class="ipython">

python

  - def distcorr(x, y):  
    n = len(x) a = np.zeros(shape=(n, n)) b = np.zeros(shape=(n, n)) for i in range(n): for j in range(i + 1, n): a\[i, j\] = abs(x\[i\] - x\[j\]) b\[i, j\] = abs(y\[i\] - y\[j\]) a += a.T b += b.T a\_bar = np.vstack(\[np.nanmean(a, axis=0)\] \* n) b\_bar = np.vstack(\[np.nanmean(b, axis=0)\] \* n) A = a - a\_bar - a\_bar.T + np.full(shape=(n, n), fill\_value=a\_bar.mean()) B = b - b\_bar - b\_bar.T + np.full(shape=(n, n), fill\_value=b\_bar.mean()) cov\_ab = np.sqrt(np.nansum(A \* B)) / n std\_a = np.sqrt(np.sqrt(np.nansum(A \*\* 2)) / n) std\_b = np.sqrt(np.sqrt(np.nansum(B \*\* 2)) / n) return cov\_ab / std\_a / std\_b

df = pd.DataFrame(np.random.normal(size=(100, 3))) df.corr(method=distcorr)

</div>

## Timedeltas

The \[Timedeltas \<timedeltas.timedeltas\>\](\#timedeltas-\<timedeltas.timedeltas\>) docs.

[Using timedeltas](https://github.com/pandas-dev/pandas/pull/2899)

<div class="ipython">

python

import datetime

s = pd.Series(pd.date\_range("2012-1-1", periods=3, freq="D"))

s - s.max()

s.max() - s

s - datetime.datetime(2011, 1, 1, 3, 5)

s + datetime.timedelta(minutes=5)

datetime.datetime(2011, 1, 1, 3, 5) - s

datetime.timedelta(minutes=5) + s

</div>

[Adding and subtracting deltas and dates](https://stackoverflow.com/questions/16385785/add-days-to-dates-in-dataframe)

<div class="ipython">

python

deltas = pd.Series(\[datetime.timedelta(days=i) for i in range(3)\])

df = pd.DataFrame({"A": s, "B": deltas}) df

df\["New Dates"\] = df\["A"\] + df\["B"\]

df\["Delta"\] = df\["A"\] - df\["New Dates"\] df

df.dtypes

</div>

[Another example](https://stackoverflow.com/questions/15683588/iterating-through-a-pandas-dataframe)

Values can be set to NaT using np.nan, similar to datetime

<div class="ipython">

python

y = s - s.shift() y

y\[1\] = np.nan y

</div>

## Creating example data

To create a dataframe from every combination of some given values, like R's `expand.grid()` function, we can create a dict where the keys are column names and the values are lists of the data values:

<div class="ipython">

python

  - def expand\_grid(data\_dict):  
    rows = itertools.product(\*data\_dict.values()) return pd.DataFrame.from\_records(rows, columns=data\_dict.keys())

  - df = expand\_grid(  
    {"height": \[60, 70\], "weight": \[100, 140, 180\], "sex": \["Male", "Female"\]}

) df

</div>

## Constant Series

To assess if a series has a constant value, we can check if `series.nunique() <= 1`. However, a more performant approach, that does not count all unique values first, is:

<div class="ipython">

python

v = s.to\_numpy() is\_constant = v.shape\[0\] == 0 or (s\[0\] == s).all()

</div>

This approach assumes that the series does not contain missing values. For the case that we would drop NA values, we can simply remove those values first:

<div class="ipython">

python

v = s.dropna().to\_numpy() is\_constant = v.shape\[0\] == 0 or (s\[0\] == s).all()

</div>

If missing values are considered distinct from any other value, then one could use:

<div class="ipython">

python

v = s.to\_numpy() is\_constant = v.shape\[0\] == 0 or (s\[0\] == s).all() or not pd.notna(v).any()

</div>

(Note that this example does not disambiguate between `np.nan`, `pd.NA` and `None`)

---

copy_on_write.md

---

<div id="copy_on_write">

{{ header }}

</div>

# Copy-on-Write (CoW)

\> **Note** \> Copy-on-Write is now the default with pandas 3.0.

Copy-on-Write was first introduced in version 1.5.0. Starting from version 2.0 most of the optimizations that become possible through CoW are implemented and supported. All possible optimizations are supported starting from pandas 2.1.

CoW will lead to more predictable behavior since it is not possible to update more than one object with one statement, e.g. indexing operations or methods won't have side-effects. Additionally, through delaying copies as long as possible, the average performance and memory usage will improve.

## Previous behavior

pandas indexing behavior is tricky to understand. Some operations return views while other return copies. Depending on the result of the operation, mutating one object might accidentally mutate another:

`` `ipython     In [1]: df = pd.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})     In [2]: subset = df["foo"]     In [3]: subset.iloc[0] = 100     In [4]: df     Out[4]:        foo  bar     0  100    4     1    2    5     2    3    6   Mutating ``subset`, e.g. updating its values, also updated`df`. The exact behavior was`<span class="title-ref"> hard to predict. Copy-on-Write solves accidentally modifying more than one object, it explicitly disallows this. </span><span class="title-ref">df</span>\` is unchanged:

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) subset = df\["foo"\] subset.iloc\[0\] = 100 df

</div>

The following sections will explain what this means and how it impacts existing applications.

## Migrating to Copy-on-Write

Copy-on-Write is the default and only mode in pandas 3.0. This means that users need to migrate their code to be compliant with CoW rules.

The default mode in pandas \< 3.0 raises warnings for certain cases that will actively change behavior and thus change user intended behavior.

pandas 2.2 has a warning mode

`` `python     pd.options.mode.copy_on_write = "warn"  that will warn for every operation that will change behavior with CoW. We expect this mode ``\` to be very noisy, since many cases that we don't expect that they will influence users will also emit a warning. We recommend checking this mode and analyzing the warnings, but it is not necessary to address all of these warning. The first two items of the following lists are the only cases that need to be addressed to make existing code work with CoW.

The following few items describe the user visible changes:

**Chained assignment will never work**

`loc` should be used as an alternative. Check the \[chained assignment section \<copy\_on\_write\_chained\_assignment\>\](\#chained-assignment-section-\<copy\_on\_write\_chained\_assignment\>) for more details.

**Accessing the underlying array of a pandas object will return a read-only view**

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3\]) ser.to\_numpy()

</div>

This example returns a NumPy array that is a view of the Series object. This view can be modified and thus also modify the pandas object. This is not compliant with CoW rules. The returned array is set to non-writeable to protect against this behavior. Creating a copy of this array allows modification. You can also make the array writeable again if you don't care about the pandas object anymore.

See the section about \[read-only NumPy arrays \<copy\_on\_write\_read\_only\_na\>\](\#read-only-numpy-arrays-\<copy\_on\_write\_read\_only\_na\>) for more details.

**Only one pandas object is updated at once**

The following code snippet updated both `df` and `subset` without CoW:

`` `ipython     In [1]: df = pd.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})     In [2]: subset = df["foo"]     In [3]: subset.iloc[0] = 100     In [4]: df     Out[4]:        foo  bar     0  100    4     1    2    5     2    3    6  This is not possible anymore with CoW, since the CoW rules explicitly forbid this. ``<span class="title-ref"> This includes updating a single column as a \`Series</span> and relying on the change propagating back to the parent <span class="title-ref">DataFrame</span>. This statement can be rewritten into a single statement with `loc` or `iloc` if this behavior is necessary. <span class="title-ref">DataFrame.where</span> is another suitable alternative for this case.

Updating a column selected from a <span class="title-ref">DataFrame</span> with an inplace method will also not work anymore.

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df\["foo"\].replace(1, 5, inplace=True) df

</div>

This is another form of chained assignment. This can generally be rewritten in 2 different forms:

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df.replace({"foo": {1: 5}}, inplace=True) df

</div>

A different alternative would be to not use `inplace`:

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df\["foo"\] = df\["foo"\].replace(1, 5) df

</div>

**Constructors now copy NumPy arrays by default**

The Series and DataFrame constructors now copies a NumPy array by default when not otherwise specified. This was changed to avoid mutating a pandas object when the NumPy array is changed inplace outside of pandas. You can set `copy=False` to avoid this copy.

## Description

CoW means that any DataFrame or Series derived from another in any way always behaves as a copy. As a consequence, we can only change the values of an object through modifying the object itself. CoW disallows updating a DataFrame or a Series that shares data with another DataFrame or Series object inplace.

This avoids side-effects when modifying values and hence, most methods can avoid actually copying the data and only trigger a copy when necessary.

The following example will operate inplace:

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df.iloc\[0, 0\] = 100 df

</div>

The object `df` does not share any data with any other object and hence no copy is triggered when updating the values. In contrast, the following operation triggers a copy of the data under CoW:

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df2 = df.reset\_index(drop=True) df2.iloc\[0, 0\] = 100

df df2

</div>

`reset_index` returns a lazy copy with CoW while it copies the data without CoW. Since both objects, `df` and `df2` share the same data, a copy is triggered when modifying `df2`. The object `df` still has the same values as initially while `df2` was modified.

If the object `df` isn't needed anymore after performing the `reset_index` operation, you can emulate an inplace-like operation through assigning the output of `reset_index` to the same variable:

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df = df.reset\_index(drop=True) df.iloc\[0, 0\] = 100 df

</div>

The initial object gets out of scope as soon as the result of `reset_index` is reassigned and hence `df` does not share data with any other object. No copy is necessary when modifying the object. This is generally true for all methods listed in \[Copy-on-Write optimizations \<copy\_on\_write.optimizations\>\](\#copy-on-write-optimizations-\<copy\_on\_write.optimizations\>).

Previously, when operating on views, the view and the parent object was modified:

`` `ipython     In [1]: df = pd.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})     In [2]: subset = df["foo"]     In [3]: subset.iloc[0] = 100     In [4]: df     Out[4]:        foo  bar     0  100    4     1    2    5     2    3    6  CoW triggers a copy when ``df`is changed to avoid mutating`view`as well:  .. ipython:: python      df = pd.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})     view = df[:]     df.iloc[0, 0] = 100      df     view  .. _copy_on_write_chained_assignment:  Chained Assignment`\` ------------------

Chained assignment references a technique where an object is updated through two subsequent indexing operations, e.g.

`` `ipython     In [1]: df = pd.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})     In [2]: df["foo"][df["bar"] > 5] = 100     In [3]: df     Out[3]:        foo  bar     0  100    4     1    2    5     2    3    6  The column ``foo`was updated where the column`bar`is greater than 5.`<span class="title-ref"> This violated the CoW principles though, because it would have to modify the view </span><span class="title-ref">df\["foo"\]</span><span class="title-ref"> and </span><span class="title-ref">df</span><span class="title-ref"> in one step. Hence, chained assignment will consistently never work and raise a </span><span class="title-ref">ChainedAssignmentError</span>\` warning with CoW enabled:

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": \[4, 5, 6\]}) df\["foo"\]\[df\["bar"\] \> 5\] = 100

</div>

With copy on write this can be done by using `loc`.

<div class="ipython">

python

df.loc\[df\["bar"\] \> 5, "foo"\] = 100

</div>

## Read-only NumPy arrays

Accessing the underlying NumPy array of a DataFrame will return a read-only array if the array shares data with the initial DataFrame:

The array is a copy if the initial DataFrame consists of more than one array:

<div class="ipython">

python

df = pd.DataFrame({"a": \[1, 2\], "b": \[1.5, 2.5\]}) df.to\_numpy()

</div>

The array shares data with the DataFrame if the DataFrame consists of only one NumPy array:

<div class="ipython">

python

df = pd.DataFrame({"a": \[1, 2\], "b": \[3, 4\]}) df.to\_numpy()

</div>

This array is read-only, which means that it can't be modified inplace:

<div class="ipython" data-okexcept="">

python

arr = df.to\_numpy() arr\[0, 0\] = 100

</div>

The same holds true for a Series, since a Series always consists of a single array.

There are two potential solutions to this:

  - Trigger a copy manually if you want to avoid updating DataFrames that share memory with your array.
  - Make the array writeable. This is a more performant solution but circumvents Copy-on-Write rules, so it should be used with caution.

<div class="ipython">

python

arr = df.to\_numpy() arr.flags.writeable = True arr\[0, 0\] = 100 arr

</div>

## Patterns to avoid

No defensive copy will be performed if two objects share the same data while you are modifying one object inplace.

<div class="ipython">

python

df = pd.DataFrame({"a": \[1, 2, 3\], "b": \[4, 5, 6\]}) df2 = df.reset\_index(drop=True) df2.iloc\[0, 0\] = 100

</div>

This creates two objects that share data and thus the setitem operation will trigger a copy. This is not necessary if the initial object `df` isn't needed anymore. Simply reassigning to the same variable will invalidate the reference that is held by the object.

<div class="ipython">

python

df = pd.DataFrame({"a": \[1, 2, 3\], "b": \[4, 5, 6\]}) df = df.reset\_index(drop=True) df.iloc\[0, 0\] = 100

</div>

No copy is necessary in this example. Creating multiple references keeps unnecessary references alive and thus will hurt performance with Copy-on-Write.

## Copy-on-Write optimizations

A new lazy copy mechanism that defers the copy until the object in question is modified and only if this object shares data with another object. This mechanism was added to methods that don't require a copy of the underlying data. Popular examples are <span class="title-ref">DataFrame.drop</span> for `axis=1` and <span class="title-ref">DataFrame.rename</span>.

These methods return views when Copy-on-Write is enabled, which provides a significant performance improvement compared to the regular execution.

---

dsintro.md

---

<div id="dsintro">

{{ header }}

</div>

# Intro to data structures

We'll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, axis labeling, and alignment apply across all of the objects. To get started, import NumPy and load pandas into your namespace:

<div class="ipython">

python

import numpy as np import pandas as pd

</div>

Fundamentally, **data alignment is intrinsic**. The link between labels and data will not be broken unless done so explicitly by you.

We'll give a brief intro to the data structures, then consider all of the broad categories of functionality and methods in separate sections.

## Series

<span class="title-ref">Series</span> is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the **index**. The basic method to create a <span class="title-ref">Series</span> is to call:

`` `python    s = pd.Series(data, index=index)  Here, ``data`can be many different things:  * a Python dict`\` \* an ndarray \* a scalar value (like 5)

The passed **index** is a list of axis labels. The constructor's behavior depends on **data**'s type:

**From ndarray**

If `data` is an ndarray, **index** must be the same length as **data**. If no index is passed, one will be created having values `[0, ..., len(data) - 1]`.

<div class="ipython">

python

s = pd.Series(np.random.randn(5), index=\["a", "b", "c", "d", "e"\]) s s.index

pd.Series(np.random.randn(5))

</div>

\> **Note** \> pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time.

**From dict**

<span class="title-ref">Series</span> can be instantiated from dicts:

<div class="ipython">

python

d = {"b": 1, "a": 0, "c": 2} pd.Series(d)

</div>

If an index is passed, the values in data corresponding to the labels in the index will be pulled out.

<div class="ipython">

python

d = {"a": 0.0, "b": 1.0, "c": 2.0} pd.Series(d) pd.Series(d, index=\["b", "c", "d", "a"\])

</div>

\> **Note** \> NaN (not a number) is the standard missing data marker used in pandas.

**From scalar value**

If `data` is a scalar value, the value will be repeated to match the length of **index**. If the **index** is not provided, it defaults to `RangeIndex(1)`.

<div class="ipython">

python

pd.Series(5.0, index=\["a", "b", "c", "d", "e"\])

</div>

### Series is ndarray-like

<span class="title-ref">Series</span> acts very similarly to a <span class="title-ref">numpy.ndarray</span> and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index.

<div class="ipython">

python

s.iloc\[0\] s.iloc\[:3\] s\[s \> s.median()\] s.iloc\[\[4, 3, 1\]\] np.exp(s)

</div>

\> **Note** \> We will address array-based indexing like `s.iloc[[4, 3, 1]]` in the \[section on indexing \<indexing\>\](\#section-on-indexing-\<indexing\>).

Like a NumPy array, a pandas <span class="title-ref">Series</span> has a single <span class="title-ref">\~Series.dtype</span>.

<div class="ipython">

python

s.dtype

</div>

This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy's type system in a few places, in which case the dtype would be an <span class="title-ref">\~pandas.api.extensions.ExtensionDtype</span>. Some examples within pandas are \[categorical\](\#categorical) and \[integer\_na\](\#integer\_na). See \[basics.dtypes\](\#basics.dtypes) for more.

If you need the actual array backing a <span class="title-ref">Series</span>, use <span class="title-ref">Series.array</span>.

<div class="ipython">

python

s.array

</div>

Accessing the array can be useful when you need to do some operation without the index (to disable \[automatic alignment \<dsintro.alignment\>\](\#automatic-alignment-\<dsintro.alignment\>), for example).

<span class="title-ref">Series.array</span> will always be an <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>. Briefly, an ExtensionArray is a thin wrapper around one or more *concrete* arrays like a <span class="title-ref">numpy.ndarray</span>. pandas knows how to take an <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span> and store it in a <span class="title-ref">Series</span> or a column of a <span class="title-ref">DataFrame</span>. See \[basics.dtypes\](\#basics.dtypes) for more.

While <span class="title-ref">Series</span> is ndarray-like, if you need an *actual* ndarray, then use <span class="title-ref">Series.to\_numpy</span>.

<div class="ipython">

python

s.to\_numpy()

</div>

Even if the <span class="title-ref">Series</span> is backed by a <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>, <span class="title-ref">Series.to\_numpy</span> will return a NumPy ndarray.

### Series is dict-like

A <span class="title-ref">Series</span> is also like a fixed-size dict in that you can get and set values by index label:

<div class="ipython">

python

s\["a"\] s\["e"\] = 12.0 s "e" in s "f" in s

</div>

If a label is not contained in the index, an exception is raised:

<div class="ipython" data-okexcept="">

python

s\["f"\]

</div>

Using the <span class="title-ref">Series.get</span> method, a missing label will return None or specified default:

<div class="ipython">

python

s.get("f")

s.get("f", np.nan)

</div>

These labels can also be accessed by \[attribute\<indexing.attribute\_access\>\](\#attribute\<indexing.attribute\_access\>).

### Vectorized operations and label alignment with Series

When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with <span class="title-ref">Series</span> in pandas. <span class="title-ref">Series</span> can also be passed into most NumPy methods expecting an ndarray.

<div class="ipython">

python

s + s s \* 2 np.exp(s)

</div>

A key difference between <span class="title-ref">Series</span> and ndarray is that operations between <span class="title-ref">Series</span> automatically align the data based on label. Thus, you can write computations without giving consideration to whether the <span class="title-ref">Series</span> involved have the same labels.

<div class="ipython">

python

s.iloc\[1:\] + s.iloc\[:-1\]

</div>

The result of an operation between unaligned <span class="title-ref">Series</span> will have the **union** of the indexes involved. If a label is not found in one <span class="title-ref">Series</span> or the other, the result will be marked as missing `NaN`. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.

\> **Note** \> In general, we chose to make the default result of operations between differently indexed objects yield the **union** of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the **dropna** function.

### Name attribute

<div id="dsintro.name_attribute">

<span class="title-ref">Series</span> also has a `name` attribute:

</div>

<div class="ipython">

python

s = pd.Series(np.random.randn(5), name="something") s s.name

</div>

The <span class="title-ref">Series</span> `name` can be assigned automatically in many cases, in particular, when selecting a single column from a <span class="title-ref">DataFrame</span>, the `name` will be assigned the column label.

You can rename a <span class="title-ref">Series</span> with the <span class="title-ref">pandas.Series.rename</span> method.

<div class="ipython">

python

s2 = s.rename("different") s2.name

</div>

Note that `s` and `s2` refer to different objects.

## DataFrame

<span class="title-ref">DataFrame</span> is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input:

  - Dict of 1D ndarrays, lists, dicts, or <span class="title-ref">Series</span>
  - 2-D numpy.ndarray
  - [Structured or record](https://numpy.org/doc/stable/user/basics.rec.html) ndarray
  - A <span class="title-ref">Series</span>
  - Another <span class="title-ref">DataFrame</span>

Along with the data, you can optionally pass **index** (row labels) and **columns** (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index.

If axis labels are not passed, they will be constructed from the input data based on common sense rules.

### From dict of Series or dicts

The resulting **index** will be the **union** of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys.

<div class="ipython">

python

  - d = {  
    "one": pd.Series(\[1.0, 2.0, 3.0\], index=\["a", "b", "c"\]), "two": pd.Series(\[1.0, 2.0, 3.0, 4.0\], index=\["a", "b", "c", "d"\]),

} df = pd.DataFrame(d) df

pd.DataFrame(d, index=\["d", "b", "a"\]) pd.DataFrame(d, index=\["d", "b", "a"\], columns=\["two", "three"\])

</div>

The row and column labels can be accessed respectively by accessing the **index** and **columns** attributes:

\> **Note** \> When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict.

<div class="ipython">

python

df.index df.columns

</div>

### From dict of ndarrays / lists

All ndarrays must share the same length. If an index is passed, it must also be the same length as the arrays. If no index is passed, the result will be `range(n)`, where `n` is the array length.

<div class="ipython">

python

d = {"one": \[1.0, 2.0, 3.0, 4.0\], "two": \[4.0, 3.0, 2.0, 1.0\]} pd.DataFrame(d) pd.DataFrame(d, index=\["a", "b", "c", "d"\])

</div>

### From structured or record array

This case is handled identically to a dict of arrays.

<div class="ipython">

python

data = np.zeros((2,), dtype=\[("A", "i4"), ("B", "f4"), ("C", "a10")\]) data\[:\] = \[(1, 2.0, "Hello"), (2, 3.0, "World")\]

pd.DataFrame(data) pd.DataFrame(data, index=\["first", "second"\]) pd.DataFrame(data, columns=\["C", "A", "B"\])

</div>

\> **Note** \> DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.

### From a list of dicts

<div class="ipython">

python

data2 = \[{"a": 1, "b": 2}, {"a": 5, "b": 10, "c": 20}\] pd.DataFrame(data2) pd.DataFrame(data2, index=\["first", "second"\]) pd.DataFrame(data2, columns=\["a", "b"\])

</div>

### From a dict of tuples

You can automatically create a MultiIndexed frame by passing a tuples dictionary.

<div class="ipython">

python

  - pd.DataFrame(
    
      - {  
        ("a", "b"): {("A", "B"): 1, ("A", "C"): 2}, ("a", "a"): {("A", "C"): 3, ("A", "B"): 4}, ("a", "c"): {("A", "B"): 5, ("A", "C"): 6}, ("b", "a"): {("A", "C"): 7, ("A", "B"): 8}, ("b", "b"): {("A", "D"): 9, ("A", "B"): 10},
    
    }

)

</div>

### From a Series

The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided).

<div class="ipython">

python

ser = pd.Series(range(3), index=list("abc"), name="ser") pd.DataFrame(ser)

</div>

### From a list of namedtuples

The field names of the first `namedtuple` in the list determine the columns of the <span class="title-ref">DataFrame</span>. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the <span class="title-ref">DataFrame</span>. If any of those tuples is shorter than the first `namedtuple` then the later columns in the corresponding row are marked as missing values. If any are longer than the first `namedtuple`, a `ValueError` is raised.

<div class="ipython">

python

from collections import namedtuple

Point = namedtuple("Point", "x y")

pd.DataFrame(\[Point(0, 0), Point(0, 3), (2, 3)\])

Point3D = namedtuple("Point3D", "x y z")

pd.DataFrame(\[Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)\])

</div>

### From a list of dataclasses

Data Classes as introduced in [PEP557](https://www.python.org/dev/peps/pep-0557), can be passed into the DataFrame constructor. Passing a list of dataclasses is equivalent to passing a list of dictionaries.

Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a `TypeError`.

<div class="ipython">

python

from dataclasses import make\_dataclass

Point = make\_dataclass("Point", \[("x", int), ("y", int)\])

pd.DataFrame(\[Point(0, 0), Point(0, 3), Point(2, 3)\])

</div>

**Missing data**

To construct a DataFrame with missing data, we use `np.nan` to represent missing values. Alternatively, you may pass a `numpy.MaskedArray` as the data argument to the DataFrame constructor, and its masked entries will be considered missing. See \[Missing data \<missing\_data\>\](\#missing-data-\<missing\_data\>) for more.

### Alternate constructors

<div id="basics.dataframe.from_dict">

**DataFrame.from\_dict**

</div>

<span class="title-ref">DataFrame.from\_dict</span> takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the <span class="title-ref">DataFrame</span> constructor except for the `orient` parameter which is `'columns'` by default, but which can be set to `'index'` in order to use the dict keys as row labels.

<div class="ipython">

python

pd.DataFrame.from\_dict(dict(\[("A", \[1, 2, 3\]), ("B", \[4, 5, 6\])\]))

</div>

If you pass `orient='index'`, the keys will be the row labels. In this case, you can also pass the desired column names:

<div class="ipython">

python

  - pd.DataFrame.from\_dict(  
    dict(\[("A", \[1, 2, 3\]), ("B", \[4, 5, 6\])\]), orient="index", columns=\["one", "two", "three"\],

)

</div>

<div id="basics.dataframe.from_records">

**DataFrame.from\_records**

</div>

<span class="title-ref">DataFrame.from\_records</span> takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal <span class="title-ref">DataFrame</span> constructor, except that the resulting DataFrame index may be a specific field of the structured dtype.

<div class="ipython">

python

data pd.DataFrame.from\_records(data, index="C")

</div>

### Column selection, addition, deletion

You can treat a <span class="title-ref">DataFrame</span> semantically like a dict of like-indexed <span class="title-ref">Series</span> objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations:

<div class="ipython">

python

df\["one"\] df\["three"\] = df\["one"\] \* df\["two"\] df\["flag"\] = df\["one"\] \> 2 df

</div>

Columns can be deleted or popped like with a dict:

<div class="ipython">

python

del df\["two"\] three = df.pop("three") df

</div>

When inserting a scalar value, it will naturally be propagated to fill the column:

<div class="ipython">

python

df\["foo"\] = "bar" df

</div>

When inserting a <span class="title-ref">Series</span> that does not have the same index as the <span class="title-ref">DataFrame</span>, it will be conformed to the DataFrame's index:

<div class="ipython">

python

df\["one\_trunc"\] = df\["one"\]\[:2\] df

</div>

You can insert raw ndarrays but their length must match the length of the DataFrame's index.

By default, columns get inserted at the end. <span class="title-ref">DataFrame.insert</span> inserts at a particular location in the columns:

<div class="ipython">

python

df.insert(1, "bar", df\["one"\]) df

</div>

### Assigning new columns in method chains

Inspired by [dplyr's](https://dplyr.tidyverse.org/reference/mutate.html) `mutate` verb, DataFrame has an <span class="title-ref">\~pandas.DataFrame.assign</span> method that allows you to easily create new columns that are potentially derived from existing columns.

<div class="ipython">

python

iris = pd.read\_csv("data/iris.data") iris.head() iris.assign(sepal\_ratio=iris\["SepalWidth"\] / iris\["SepalLength"\]).head()

</div>

In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to.

<div class="ipython">

python

iris.assign(sepal\_ratio=lambda x: (x\["SepalWidth"\] / x\["SepalLength"\])).head()

</div>

<span class="title-ref">\~pandas.DataFrame.assign</span> **always** returns a copy of the data, leaving the original DataFrame untouched.

Passing a callable, as opposed to an actual value to be inserted, is useful when you don't have a reference to the DataFrame at hand. This is common when using <span class="title-ref">\~pandas.DataFrame.assign</span> in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot:

<div class="ipython">

python

@savefig basics\_assign.png ( iris.query("SepalLength \> 5") .assign( SepalRatio=lambda x: x.SepalWidth / x.SepalLength, PetalRatio=lambda x: x.PetalWidth / x.PetalLength, ) .plot(kind="scatter", x="SepalRatio", y="PetalRatio") )

</div>

Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that's been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn't have a reference to the *filtered* DataFrame available.

The function signature for <span class="title-ref">\~pandas.DataFrame.assign</span> is simply `**kwargs`. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a <span class="title-ref">Series</span> or NumPy array), or a function of one argument to be called on the <span class="title-ref">DataFrame</span>. A *copy* of the original <span class="title-ref">DataFrame</span> is returned, with the new values inserted.

The order of `**kwargs` is preserved. This allows for *dependent* assignment, where an expression later in `**kwargs` can refer to a column created earlier in the same <span class="title-ref">\~DataFrame.assign</span>.

<div class="ipython">

python

dfa = pd.DataFrame({"A": \[1, 2, 3\], "B": \[4, 5, 6\]}) dfa.assign(C=lambda x: x\["A"\] + x\["B"\], D=lambda x: x\["A"\] + x\["C"\])

</div>

In the second expression, `x['C']` will refer to the newly created column, that's equal to `dfa['A'] + dfa['B']`.

### Indexing / selection

The basics of indexing are as follows:

| Operation                      | Syntax          | Result    |
| ------------------------------ | --------------- | --------- |
| Select column                  | `df[col]`       | Series    |
| Select row by label            | `df.loc[label]` | Series    |
| Select row by integer location | `df.iloc[loc]`  | Series    |
| Slice rows                     | `df[5:10]`      | DataFrame |
| Select rows by boolean vector  | `df[bool_vec]`  | DataFrame |

Row selection, for example, returns a <span class="title-ref">Series</span> whose index is the columns of the \`DataFrame\`:

<div class="ipython">

python

df.loc\["b"\] df.iloc\[2\]

</div>

For a more exhaustive treatment of sophisticated label-based indexing and slicing, see the \[section on indexing \<indexing\>\](\#section-on-indexing-\<indexing\>). We will address the fundamentals of reindexing / conforming to new sets of labels in the \[section on reindexing \<basics.reindexing\>\](\#section-on-reindexing-\<basics.reindexing\>).

### Data alignment and arithmetic

Data alignment between <span class="title-ref">DataFrame</span> objects automatically align on **both the columns and the index (row labels)**. Again, the resulting object will have the union of the column and row labels.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 4), columns=\["A", "B", "C", "D"\]) df2 = pd.DataFrame(np.random.randn(7, 3), columns=\["A", "B", "C"\]) df + df2

</div>

When doing an operation between <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span>, the default behavior is to align the <span class="title-ref">Series</span> **index** on the <span class="title-ref">DataFrame</span> **columns**, thus [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) row-wise. For example:

<div class="ipython">

python

df - df.iloc\[0\]

</div>

For explicit control over the matching and broadcasting behavior, see the section on \[flexible binary operations \<basics.binop\>\](\#flexible-binary-operations-\<basics.binop\>).

Arithmetic operations with scalars operate element-wise:

<div class="ipython">

python

df \* 5 + 2 1 / df df \*\* 4

</div>

<div id="dsintro.boolean">

Boolean operators operate element-wise as well:

</div>

<div class="ipython">

python

df1 = pd.DataFrame({"a": \[1, 0, 1\], "b": \[0, 1, 1\]}, dtype=bool) df2 = pd.DataFrame({"a": \[0, 1, 1\], "b": \[1, 1, 0\]}, dtype=bool) df1 & df2 df1 | df2 df1 ^ df2 -df1

</div>

### Transposing

To transpose, access the `T` attribute or <span class="title-ref">DataFrame.transpose</span>, similar to an ndarray:

<div class="ipython">

python

\# only show the first 5 rows df\[:5\].T

</div>

### DataFrame interoperability with NumPy functions

Most NumPy functions can be called directly on <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span>.

<div class="ipython">

python

np.exp(df) np.asarray(df)

</div>

<span class="title-ref">DataFrame</span> is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array.

<span class="title-ref">Series</span> implements `__array_ufunc__`, which allows it to work with NumPy's [universal functions](https://numpy.org/doc/stable/reference/ufuncs.html).

The ufunc is applied to the underlying array in a <span class="title-ref">Series</span>.

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3, 4\]) np.exp(ser)

</div>

When multiple <span class="title-ref">Series</span> are passed to a ufunc, they are aligned before performing the operation.

Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using <span class="title-ref">numpy.remainder</span> on two <span class="title-ref">Series</span> with differently ordered labels will align before the operation.

<div class="ipython">

python

ser1 = pd.Series(\[1, 2, 3\], index=\["a", "b", "c"\]) ser2 = pd.Series(\[1, 3, 5\], index=\["b", "a", "c"\]) ser1 ser2 np.remainder(ser1, ser2)

</div>

As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values.

<div class="ipython">

python

ser3 = pd.Series(\[2, 4, 6\], index=\["b", "c", "d"\]) ser3 np.remainder(ser1, ser3)

</div>

When a binary ufunc is applied to a <span class="title-ref">Series</span> and <span class="title-ref">Index</span>, the <span class="title-ref">Series</span> implementation takes precedence and a <span class="title-ref">Series</span> is returned.

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3\]) idx = pd.Index(\[4, 5, 6\])

np.maximum(ser, idx)

</div>

NumPy ufuncs are safe to apply to <span class="title-ref">Series</span> backed by non-ndarray arrays, for example <span class="title-ref">arrays.SparseArray</span> (see \[sparse.calculation\](\#sparse.calculation)). If possible, the ufunc is applied without converting the underlying data to an ndarray.

### Console display

A very large <span class="title-ref">DataFrame</span> will be truncated to display them in the console. You can also get a summary using <span class="title-ref">\~pandas.DataFrame.info</span>. (The **baseball** dataset is from the **plyr** R package):

<div class="ipython" data-suppress="">

python

\# force a summary to be printed pd.set\_option("display.max\_rows", 5)

</div>

<div class="ipython">

python

baseball = pd.read\_csv("data/baseball.csv") print(baseball) baseball.info()

</div>

<div class="ipython" data-suppress="" data-okwarning="">

python

\# restore GlobalPrintConfig pd.reset\_option(r"^display.")

</div>

However, using <span class="title-ref">DataFrame.to\_string</span> will return a string representation of the <span class="title-ref">DataFrame</span> in tabular form, though it won't always fit the console width:

<div class="ipython">

python

print(baseball.iloc\[-20:, :12\].to\_string())

</div>

Wide DataFrames will be printed across multiple rows by default:

<div class="ipython">

python

pd.DataFrame(np.random.randn(3, 12))

</div>

You can change how much to print on a single row by setting the `display.width` option:

<div class="ipython">

python

pd.set\_option("display.width", 40) \# default is 80

pd.DataFrame(np.random.randn(3, 12))

</div>

You can adjust the max width of the individual columns by setting `display.max_colwidth`

<div class="ipython">

python

  - datafile = {  
    "filename": \["filename\_01", "filename\_02"\], "path": \[ "media/user\_name/storage/folder\_01/filename\_01", "media/user\_name/storage/folder\_02/filename\_02", \],

}

pd.set\_option("display.max\_colwidth", 30) pd.DataFrame(datafile)

pd.set\_option("display.max\_colwidth", 100) pd.DataFrame(datafile)

</div>

<div class="ipython" data-suppress="">

python

pd.reset\_option("display.width") pd.reset\_option("display.max\_colwidth")

</div>

You can also disable this feature via the `expand_frame_repr` option. This will print the table in one block.

### DataFrame column attribute access and IPython completion

If a <span class="title-ref">DataFrame</span> column label is a valid Python variable name, the column can be accessed like an attribute:

<div class="ipython">

python

df = pd.DataFrame({"foo1": np.random.randn(5), "foo2": np.random.randn(5)}) df df.foo1

</div>

The columns are also connected to the [IPython](https://ipython.org) completion mechanism so they can be tab-completed:

`` `ipython In [5]: df.foo<TAB>  # noqa: E225, E999 df.foo1  df.foo2 ``\`

---

duplicates.md

---

# Duplicate Labels

<span class="title-ref">Index</span> objects are not required to be unique; you can have duplicate row or column labels. This may be a bit confusing at first. If you're familiar with SQL, you know that row labels are similar to a primary key on a table, and you would never want duplicates in a SQL table. But one of pandas' roles is to clean messy, real-world data before it goes to some downstream system. And real-world data has duplicates, even in fields that are supposed to be unique.

This section describes how duplicate labels change the behavior of certain operations, and how prevent duplicates from arising during operations, or to detect them if they do.

<div class="ipython">

python

import pandas as pd import numpy as np

</div>

## Consequences of Duplicate Labels

Some pandas methods (<span class="title-ref">Series.reindex</span> for example) just don't work with duplicates present. The output can't be determined, and so pandas raises.

<div class="ipython" data-okexcept="" data-okwarning="">

python

s1 = pd.Series(\[0, 1, 2\], index=\["a", "b", "b"\]) s1.reindex(\["a", "b", "c"\])

</div>

Other methods, like indexing, can give very surprising results. Typically indexing with a scalar will *reduce dimensionality*. Slicing a `DataFrame` with a scalar will return a `Series`. Slicing a `Series` with a scalar will return a scalar. But with duplicates, this isn't the case.

<div class="ipython">

python

df1 = pd.DataFrame(\[\[0, 1, 2\], \[3, 4, 5\]\], columns=\["A", "A", "B"\]) df1

</div>

We have duplicates in the columns. If we slice `'B'`, we get back a `Series`

<div class="ipython">

python

df1\["B"\] \# a series

</div>

But slicing `'A'` returns a `DataFrame`

<div class="ipython">

python

df1\["A"\] \# a DataFrame

</div>

This applies to row labels as well

<div class="ipython">

python

df2 = pd.DataFrame({"A": \[0, 1, 2\]}, index=\["a", "a", "b"\]) df2 df2.loc\["b", "A"\] \# a scalar df2.loc\["a", "A"\] \# a Series

</div>

## Duplicate Label Detection

You can check whether an <span class="title-ref">Index</span> (storing the row or column labels) is unique with \`Index.is\_unique\`:

<div class="ipython">

python

df2 df2.index.is\_unique df2.columns.is\_unique

</div>

\> **Note** \> Checking whether an index is unique is somewhat expensive for large datasets. pandas does cache this result, so re-checking on the same index is very fast.

<span class="title-ref">Index.duplicated</span> will return a boolean ndarray indicating whether a label is repeated.

<div class="ipython">

python

df2.index.duplicated()

</div>

Which can be used as a boolean filter to drop duplicate rows.

<div class="ipython">

python

df2.loc\[\~df2.index.duplicated(), :\]

</div>

If you need additional logic to handle duplicate labels, rather than just dropping the repeats, using <span class="title-ref">\~DataFrame.groupby</span> on the index is a common trick. For example, we'll resolve duplicates by taking the average of all rows with the same label.

<div class="ipython">

python

df2.groupby(level=0).mean()

</div>

## Disallowing Duplicate Labels

<div class="versionadded">

1.2.0

</div>

As noted above, handling duplicates is an important feature when reading in raw data. That said, you may want to avoid introducing duplicates as part of a data processing pipeline (from methods like <span class="title-ref">pandas.concat</span>, <span class="title-ref">\~DataFrame.rename</span>, etc.). Both <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> *disallow* duplicate labels by calling `.set_flags(allows_duplicate_labels=False)`. (the default is to allow them). If there are duplicate labels, an exception will be raised.

<div class="ipython" data-okexcept="">

python

pd.Series(\[0, 1, 2\], index=\["a", "b", "b"\]).set\_flags(allows\_duplicate\_labels=False)

</div>

This applies to both row and column labels for a <span class="title-ref">DataFrame</span>

<div class="ipython" data-okexcept="">

python

  - pd.DataFrame(\[\[0, 1, 2\], \[3, 4, 5\]\], columns=\["A", "B", "C"\],).set\_flags(  
    allows\_duplicate\_labels=False

)

</div>

This attribute can be checked or set with <span class="title-ref">\~DataFrame.flags.allows\_duplicate\_labels</span>, which indicates whether that object can have duplicate labels.

<div class="ipython">

python

  - df = pd.DataFrame({"A": \[0, 1, 2, 3\]}, index=\["x", "y", "X", "Y"\]).set\_flags(  
    allows\_duplicate\_labels=False

) df df.flags.allows\_duplicate\_labels

</div>

<span class="title-ref">DataFrame.set\_flags</span> can be used to return a new `DataFrame` with attributes like `allows_duplicate_labels` set to some value

<div class="ipython">

python

df2 = df.set\_flags(allows\_duplicate\_labels=True) df2.flags.allows\_duplicate\_labels

</div>

The new `DataFrame` returned is a view on the same data as the old `DataFrame`. Or the property can just be set directly on the same object

<div class="ipython">

python

df2.flags.allows\_duplicate\_labels = False df2.flags.allows\_duplicate\_labels

</div>

When processing raw, messy data you might initially read in the messy data (which potentially has duplicate labels), deduplicate, and then disallow duplicates going forward, to ensure that your data pipeline doesn't introduce duplicates.

`` `python    >>> raw = pd.read_csv("...")    >>> deduplicated = raw.groupby(level=0).first()  # remove duplicates    >>> deduplicated.flags.allows_duplicate_labels = False  # disallow going forward  Setting ``allows\_duplicate\_labels=False`on a`Series`or`DataFrame`with duplicate`<span class="title-ref"> labels or performing an operation that introduces duplicate labels on a </span><span class="title-ref">Series</span><span class="title-ref"> or </span><span class="title-ref">DataFrame</span><span class="title-ref"> that disallows duplicates will raise an \`errors.DuplicateLabelError</span>.

<div class="ipython" data-okexcept="">

python

df.rename(str.upper)

</div>

This error message contains the labels that are duplicated, and the numeric positions of all the duplicates (including the "original") in the `Series` or `DataFrame`

### Duplicate Label Propagation

In general, disallowing duplicates is "sticky". It's preserved through operations.

<div class="ipython" data-okexcept="">

python

s1 = pd.Series(0, index=\["a", "b"\]).set\_flags(allows\_duplicate\_labels=False) s1 s1.head().rename({"a": "b"})

</div>

\> **Warning** \> This is an experimental feature. Currently, many methods fail to propagate the `allows_duplicate_labels` value. In future versions it is expected that every method taking or returning one or more DataFrame or Series objects will propagate `allows_duplicate_labels`.

---

enhancingperf.md

---

<div id="enhancingperf">

{{ header }}

</div>

# Enhancing performance

In this part of the tutorial, we will investigate how to speed up certain functions operating on pandas <span class="title-ref">DataFrame</span> using Cython, Numba and <span class="title-ref">pandas.eval</span>. Generally, using Cython and Numba can offer a larger speedup than using <span class="title-ref">pandas.eval</span> but will require a lot more code.

\> **Note** \> In addition to following the steps in this tutorial, users interested in enhancing performance are highly encouraged to install the \[recommended dependencies\<install.recommended\_dependencies\>\](\#recommended-dependencies\<install.recommended\_dependencies\>) for pandas. These dependencies are often not installed by default, but will offer speed improvements if present.

## Cython (writing C extensions for pandas)

For many use cases writing pandas in pure Python and NumPy is sufficient. In some computationally heavy applications however, it can be possible to achieve sizable speed-ups by offloading work to [cython](https://cython.org/).

This tutorial assumes you have refactored as much as possible in Python, for example by trying to remove for-loops and making use of NumPy vectorization. It's always worth optimising in Python first.

This tutorial walks through a "typical" process of cythonizing a slow computation. We use an [example from the Cython documentation](https://docs.cython.org/en/latest/src/quickstart/cythonize.html) but in the context of pandas. Our final cythonized solution is around 100 times faster than the pure Python solution.

### Pure Python

We have a <span class="title-ref">DataFrame</span> to which we want to apply a function row-wise.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "a": np.random.randn(1000), "b": np.random.randn(1000), "N": np.random.randint(100, 1000, (1000)), "x": "x",
    
    }

) df

</div>

Here's the function in pure Python:

<div class="ipython">

python

  - def f(x):  
    return x \* (x - 1)

  - def integrate\_f(a, b, N):  
    s = 0 dx = (b - a) / N for i in range(N): s += f(a + i \* dx) return s \* dx

</div>

We achieve our result by using <span class="title-ref">DataFrame.apply</span> (row-wise):

<div class="ipython">

python

%timeit df.apply(lambda x: integrate\_f(x\["a"\], x\["b"\], x\["N"\]), axis=1)

</div>

Let's take a look and see where the time is spent during this operation using the [prun ipython magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-prun):

<div class="ipython">

python

\# most time consuming 4 calls %prun -l 4 df.apply(lambda x: integrate\_f(x\["a"\], x\["b"\], x\["N"\]), axis=1) \# noqa E999

</div>

By far the majority of time is spend inside either `integrate_f` or `f`, hence we'll concentrate our efforts cythonizing these two functions.

### Plain Cython

First we're going to need to import the Cython magic function to IPython:

<div class="ipython" data-okwarning="">

python

%load\_ext Cython

</div>

Now, let's simply copy our functions over to Cython:

<div class="ipython">

  - In \[2\]: %%cython  
    ...: def f\_plain(x): ...: return x \* (x - 1) ...: def integrate\_f\_plain(a, b, N): ...: s = 0 ...: dx = (b - a) / N ...: for i in range(N): ...: s += f\_plain(a + i \* dx) ...: return s \* dx ...:

</div>

<div class="ipython">

python

%timeit df.apply(lambda x: integrate\_f\_plain(x\["a"\], x\["b"\], x\["N"\]), axis=1)

</div>

This has improved the performance compared to the pure Python approach by one-third.

### Declaring C types

We can annotate the function variables and return types as well as use `cdef` and `cpdef` to improve performance:

<div class="ipython">

  - In \[3\]: %%cython  
    ...: cdef double f\_typed(double x) except? -2: ...: return x \* (x - 1) ...: cpdef double integrate\_f\_typed(double a, double b, int N): ...: cdef int i ...: cdef double s, dx ...: s = 0 ...: dx = (b - a) / N ...: for i in range(N): ...: s += f\_typed(a + i \* dx) ...: return s \* dx ...:

</div>

<div class="ipython">

python

%timeit df.apply(lambda x: integrate\_f\_typed(x\["a"\], x\["b"\], x\["N"\]), axis=1)

</div>

Annotating the functions with C types yields an over ten times performance improvement compared to the original Python implementation.

### Using ndarray

When re-profiling, time is spent creating a <span class="title-ref">Series</span> from each row, and calling `__getitem__` from both the index and the series (three times for each row). These Python function calls are expensive and can be improved by passing an `np.ndarray`.

<div class="ipython">

python

%prun -l 4 df.apply(lambda x: integrate\_f\_typed(x\["a"\], x\["b"\], x\["N"\]), axis=1)

</div>

<div class="ipython">

  - In \[4\]: %%cython  
    ...: cimport numpy as np ...: import numpy as np ...: cdef double f\_typed(double x) except? -2: ...: return x \* (x - 1) ...: cpdef double integrate\_f\_typed(double a, double b, int N): ...: cdef int i ...: cdef double s, dx ...: s = 0 ...: dx = (b - a) / N ...: for i in range(N): ...: s += f\_typed(a + i \* dx) ...: return s \* dx ...: cpdef np.ndarray\[double\] apply\_integrate\_f(np.ndarray col\_a, np.ndarray col\_b, ...: np.ndarray col\_N): ...: assert (col\_a.dtype == np.float64 ...: and col\_b.dtype == np.float64 and col\_N.dtype == np.dtype(int)) ...: cdef Py\_ssize\_t i, n = len(col\_N) ...: assert (len(col\_a) == len(col\_b) == n) ...: cdef np.ndarray\[double\] res = np.empty(n) ...: for i in range(len(col\_a)): ...: res\[i\] = integrate\_f\_typed(col\_a\[i\], col\_b\[i\], col\_N\[i\]) ...: return res ...:

</div>

This implementation creates an array of zeros and inserts the result of `integrate_f_typed` applied over each row. Looping over an `ndarray` is faster in Cython than looping over a <span class="title-ref">Series</span> object.

Since `apply_integrate_f` is typed to accept an `np.ndarray`, <span class="title-ref">Series.to\_numpy</span> calls are needed to utilize this function.

<div class="ipython">

python

%timeit apply\_integrate\_f(df\["a"\].to\_numpy(), df\["b"\].to\_numpy(), df\["N"\].to\_numpy())

</div>

Performance has improved from the prior implementation by almost ten times.

### Disabling compiler directives

The majority of the time is now spent in `apply_integrate_f`. Disabling Cython's `boundscheck` and `wraparound` checks can yield more performance.

<div class="ipython">

python

%prun -l 4 apply\_integrate\_f(df\["a"\].to\_numpy(), df\["b"\].to\_numpy(), df\["N"\].to\_numpy())

</div>

<div class="ipython">

  - In \[5\]: %%cython  
    ...: cimport cython ...: cimport numpy as np ...: import numpy as np ...: cdef np.float64\_t f\_typed(np.float64\_t x) except? -2: ...: return x \* (x - 1) ...: cpdef np.float64\_t integrate\_f\_typed(np.float64\_t a, np.float64\_t b, np.int64\_t N): ...: cdef np.int64\_t i ...: cdef np.float64\_t s = 0.0, dx ...: dx = (b - a) / N ...: for i in range(N): ...: s += f\_typed(a + i \* dx) ...: return s \* dx ...: @cython.boundscheck(False) ...: @cython.wraparound(False) ...: cpdef np.ndarray\[np.float64\_t\] apply\_integrate\_f\_wrap( ...: np.ndarray\[np.float64\_t\] col\_a, ...: np.ndarray\[np.float64\_t\] col\_b, ...: np.ndarray\[np.int64\_t\] col\_N ...: ): ...: cdef np.int64\_t i, n = len(col\_N) ...: assert len(col\_a) == len(col\_b) == n ...: cdef np.ndarray\[np.float64\_t\] res = np.empty(n, dtype=np.float64) ...: for i in range(n): ...: res\[i\] = integrate\_f\_typed(col\_a\[i\], col\_b\[i\], col\_N\[i\]) ...: return res ...:

</div>

<div class="ipython">

python

%timeit apply\_integrate\_f\_wrap(df\["a"\].to\_numpy(), df\["b"\].to\_numpy(), df\["N"\].to\_numpy())

</div>

However, a loop indexer `i` accessing an invalid location in an array would cause a segfault because memory access isn't checked. For more about `boundscheck` and `wraparound`, see the Cython docs on [compiler directives](https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html#compiler-directives).

## Numba (JIT compilation)

An alternative to statically compiling Cython code is to use a dynamic just-in-time (JIT) compiler with [Numba](https://numba.pydata.org/).

Numba allows you to write a pure Python function which can be JIT compiled to native machine instructions, similar in performance to C, C++ and Fortran, by decorating your function with `@jit`.

Numba works by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool). Numba supports compilation of Python to run on either CPU or GPU hardware and is designed to integrate with the Python scientific software stack.

\> **Note** \> The `@jit` compilation will add overhead to the runtime of the function, so performance benefits may not be realized especially when using small data sets. Consider [caching](https://numba.readthedocs.io/en/stable/developer/caching.html) your function to avoid compilation overhead each time your function is run.

Numba can be used in 2 ways with pandas:

1.  Specify the `engine="numba"` keyword in select pandas methods
2.  Define your own Python function decorated with `@jit` and pass the underlying NumPy array of <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> (using <span class="title-ref">Series.to\_numpy</span>) into the function

### pandas Numba Engine

If Numba is installed, one can specify `engine="numba"` in select pandas methods to execute the method using Numba. Methods that support `engine="numba"` will also have an `engine_kwargs` keyword that accepts a dictionary that allows one to specify `"nogil"`, `"nopython"` and `"parallel"` keys with boolean values to pass into the `@jit` decorator. If `engine_kwargs` is not specified, it defaults to `{"nogil": False, "nopython": True, "parallel": False}` unless otherwise specified.

\> **Note** \> In terms of performance, **the first time a function is run using the Numba engine will be slow** as Numba will have some function compilation overhead. However, the JIT compiled functions are cached, and subsequent calls will be fast. In general, the Numba engine is performant with a larger amount of data points (e.g. 1+ million).

>   - \`\`\`ipython  
>     In \[1\]: data = pd.Series(range(1\_000\_000)) \# noqa: E225
>     
>     In \[2\]: roll = data.rolling(10)
>     
>       - In \[3\]: def f(x):  
>         ...: return np.sum(x) + 5
>     
>     \# Run the first time, compilation time will affect performance In \[4\]: %timeit -r 1 -n 1 roll.apply(f, engine='numba', raw=True) 1.23 s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each) \# Function is cached and performance will improve In \[5\]: %timeit roll.apply(f, engine='numba', raw=True) 188 ms Â± 1.93 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
>     
>     In \[6\]: %timeit roll.apply(f, engine='cython', raw=True) 3.92 s Â± 59 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

If your compute hardware contains multiple CPUs, the largest performance gain can be realized by setting `parallel` to `True` `` ` to leverage more than 1 CPU. Internally, pandas leverages numba to parallelize computations over the columns of a `DataFrame`; therefore, this performance benefit is only beneficial for a `DataFrame` with a large number of columns. ``\`ipython In \[1\]: import numba

> In \[2\]: numba.set\_num\_threads(1)
> 
> In \[3\]: df = pd.DataFrame(np.random.randn(10\_000, 100))
> 
> In \[4\]: roll = df.rolling(100)
> 
> In \[5\]: %timeit roll.mean(engine="numba", engine\_kwargs={"parallel": True}) 347 ms Â± 26 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
> 
> In \[6\]: numba.set\_num\_threads(2)
> 
> In \[7\]: %timeit roll.mean(engine="numba", engine\_kwargs={"parallel": True}) 201 ms Â± 2.97 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

Custom Function Examples `` ` ~~~~~~~~~~~~~~~~~~~~~~~~  A custom Python function decorated with ``@jit``can be used with pandas objects by passing their NumPy array representations with `Series.to_numpy`.``\`python import numba

> @numba.jit def f\_plain(x): return x \* (x - 1)
> 
> @numba.jit def integrate\_f\_numba(a, b, N): s = 0 dx = (b - a) / N for i in range(N): s += f\_plain(a + i \* dx) return s \* dx
> 
> @numba.jit def apply\_integrate\_f\_numba(col\_a, col\_b, col\_N): n = len(col\_N) result = np.empty(n, dtype="float64") assert len(col\_a) == len(col\_b) == n for i in range(n): result\[i\] = integrate\_f\_numba(col\_a\[i\], col\_b\[i\], col\_N\[i\]) return result
> 
>   - def compute\_numba(df):
>     
>       - result = apply\_integrate\_f\_numba(  
>         df\["a"\].to\_numpy(), df\["b"\].to\_numpy(), df\["N"\].to\_numpy()
>     
>     ) return pd.Series(result, index=df.index, name="result")

``` ipython
In [4]: %timeit compute_numba(df)
1000 loops, best of 3: 798 us per loop
```

In this example, using Numba was faster than Cython.

Numba can also be used to write vectorized functions that do not require the user to explicitly `` ` loop over the observations of a vector; a vectorized function will be applied to each row automatically. Consider the following example of doubling each observation: ``\`python import numba

>   - def double\_every\_value\_nonumba(x):  
>     return x \* 2
> 
> @numba.vectorize def double\_every\_value\_withnumba(x): \# noqa E501 return x \* 2

``` ipython
# Custom function without numba
In [5]: %timeit df["col1_doubled"] = df["a"].apply(double_every_value_nonumba)  # noqa E501
1000 loops, best of 3: 797 us per loop

# Standard implementation (faster than a custom function)
In [6]: %timeit df["col1_doubled"] = df["a"] * 2
1000 loops, best of 3: 233 us per loop

# Custom function with numba
In [7]: %timeit df["col1_doubled"] = double_every_value_withnumba(df["a"].to_numpy())
1000 loops, best of 3: 145 us per loop
```

Caveats `` ` ~~~~~~~  Numba is best at accelerating functions that apply numerical functions to NumPy arrays. If you try to ``@jit``a function that contains unsupported `Python <https://numba.readthedocs.io/en/stable/reference/pysupported.html>`__ or `NumPy <https://numba.readthedocs.io/en/stable/reference/numpysupported.html>`__ code, compilation will revert `object mode <https://numba.readthedocs.io/en/stable/glossary.html#term-object-mode>`__ which will mostly likely not speed up your function. If you would prefer that Numba throw an error if it cannot compile a function in a way that speeds up your code, pass Numba the argument``nopython=True`(e.g.`@jit(nopython=True)``). For more on troubleshooting Numba modes, see the `Numba troubleshooting page <https://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#the-compiled-code-is-too-slow>`__.  Using``parallel=True`(e.g.`@jit(parallel=True)`) may result in a`SIGABRT``if the threading layer leads to unsafe behavior. You can first `specify a safe threading layer <https://numba.readthedocs.io/en/stable/user/threading-layer.html#selecting-a-threading-layer-for-safe-parallel-execution>`__ before running a JIT function with``parallel=True`.  Generally if the you encounter a segfault (`SIGSEGV``) while using Numba, please report the issue to the `Numba issue tracker. <https://github.com/numba/numba/issues/new/choose>`__  .. _enhancingperf.eval:  Expression evaluation via `~pandas.eval` ----------------------------------------------  The top-level function `pandas.eval` implements performant expression evaluation of `~pandas.Series` and `~pandas.DataFrame`. Expression evaluation allows operations to be expressed as strings and can potentially provide a performance improvement by evaluate arithmetic and boolean expression all at once for large `~pandas.DataFrame`.  > **Note** >     You should not use `~pandas.eval` for simple    expressions or for expressions involving small DataFrames. In fact,    `~pandas.eval` is many orders of magnitude slower for    smaller expressions or objects than plain Python. A good rule of thumb is    to only use `~pandas.eval` when you have a    `~pandas.core.frame.DataFrame` with more than 10,000 rows.  Supported syntax ~~~~~~~~~~~~~~~~  These operations are supported by `pandas.eval`:  * Arithmetic operations except for the left shift (``\<\<`) and right shift   (`\>\>`) operators, e.g.,`df + 2 \* pi / s \*\* 4 % 42 - the\_golden\_ratio`* Comparison operations, including chained comparisons, e.g.,`2 \< df \< df2`* Boolean operations, e.g.,`df \< df2 and df3 \< df4 or not df\_bool`*`list`and`tuple`literals, e.g.,`\[1, 2\]`or`(1, 2)`* Attribute access, e.g.,`df.a`* Subscript expressions, e.g.,`df\[0\]`* Simple variable evaluation, e.g.,`pd.eval("df")`(this is not very useful) * Math functions:`sin`,`cos`,`exp`,`log`,`expm1`,`log1p`,`sqrt`,`sinh`,`cosh`,`tanh`,`arcsin`,`arccos`,`arctan`,`arccosh`,`arcsinh`,`arctanh`,`abs`,`arctan2`and`log10`.  The following Python syntax is **not** allowed:  * Expressions      * Function calls other than math functions.     *`is`/`is not`operations     *`if`expressions     *`lambda`expressions     *`list`/`set`/`dict`comprehensions     * Literal`dict`and`set`expressions     *`yield``expressions     * Generator expressions     * Boolean expressions consisting of only scalar values  * Statements      * Neither `simple <https://docs.python.org/3/reference/simple_stmts.html>`__       or `compound <https://docs.python.org/3/reference/compound_stmts.html>`__       statements are allowed. This includes``for`,`while`, and`if`.  Local variables ~~~~~~~~~~~~~~~  You must *explicitly reference* any local variable that you want to use in an expression by placing the`@``character in front of the name. This mechanism is the same for both `DataFrame.query` and `DataFrame.eval`. For example,  .. ipython:: python     df = pd.DataFrame(np.random.randn(5, 2), columns=list("ab"))    newcol = np.random.randn(len(df))    df.eval("b + @newcol")    df.query("b < @newcol")  If you don't prefix the local variable with``@``, pandas will raise an exception telling you the variable is undefined.  When using `DataFrame.eval` and `DataFrame.query`, this allows you to have a local variable and a `~pandas.DataFrame` column with the same name in an expression.   .. ipython:: python     a = np.random.randn()    df.query("@a < a")    df.loc[a < df["a"]]  # same as the previous expression  > **Warning** >     `pandas.eval` will raise an exception if you cannot use the``@``prefix because it    isn't defined in that context.     .. ipython:: python       :okexcept:        a, b = 1, 2       pd.eval("@a + b")     In this case, you should simply refer to the variables like you would in    standard Python.     .. ipython:: python        pd.eval("a + b")   `pandas.eval` parsers ~~~~~~~~~~~~~~~~~~~~~~~~~~~  There are two different expression syntax parsers.  The default``'pandas'`parser allows a more intuitive syntax for expressing query-like operations (comparisons, conjunctions and disjunctions). In particular, the precedence of the`&`and` 4, but should evaluate to 3 \~1 \# this is okay, but slower when using eval

> should be performed in Python. An exception will be raised if you try to perform any boolean/bitwise operations with scalar operands that are not of type `bool` or `np.bool_`.

Here is a plot showing the running time of `` ` `pandas.eval` as function of the size of the frame involved in the computation. The two lines are two different engines.  ..     The eval-perf.png figure below was generated with /doc/scripts/eval_performance.py  .. image:: ../_static/eval-perf.png  You will only see the performance benefits of using the ``numexpr``engine with `pandas.eval` if your `~pandas.DataFrame` has more than approximately 100,000 rows.  This plot was created using a `DataFrame` with 3 columns each containing floating point values generated using``numpy.random.randn()`.  Expression evaluation limitations with`numexpr`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Expressions that would result in an object dtype or involve datetime operations because of`NaT`must be evaluated in Python space, but part of an expression can still be evaluated with`numexpr`. For example:  .. ipython:: python     df = pd.DataFrame(        {"strings": np.repeat(list("cba"), 3), "nums": np.repeat(range(3), 3)}    )    df    df.query("strings == 'a' and nums == 1")  The numeric part of the comparison (`nums == 1`) will be evaluated by`numexpr`and the object part of the comparison (`"strings == 'a'\`\`) will be evaluated by Python.

---

gotchas.md

---

<div id="gotchas">

{{ header }}

</div>

# Frequently Asked Questions (FAQ)

## DataFrame memory usage

The memory usage of a <span class="title-ref">DataFrame</span> (including the index) is shown when calling the <span class="title-ref">\~DataFrame.info</span>. A configuration option, `display.memory_usage` (see \[the list of options \<options.available\>\](\#the-list-of-options-\<options.available\>)), specifies if the <span class="title-ref">DataFrame</span> memory usage will be displayed when invoking the <span class="title-ref">\~DataFrame.info</span> method.

For example, the memory usage of the <span class="title-ref">DataFrame</span> below is shown when calling \`\~DataFrame.info\`:

<div class="ipython">

python

  - dtypes = \[  
    "int64", "float64", "datetime64\[ns\]", "timedelta64\[ns\]", "complex128", "object", "bool",

\] n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df\["categorical"\] = df\["object"\].astype("category")

df.info()

</div>

The `+` symbol indicates that the true memory usage could be higher, because pandas does not count the memory used by values in columns with `dtype=object`.

Passing `memory_usage='deep'` will enable a more accurate memory usage report, accounting for the full usage of the contained objects. This is optional as it can be expensive to do this deeper introspection.

<div class="ipython">

python

df.info(memory\_usage="deep")

</div>

By default the display option is set to `True` but can be explicitly overridden by passing the `memory_usage` argument when invoking <span class="title-ref">\~DataFrame.info</span>.

The memory usage of each column can be found by calling the <span class="title-ref">\~DataFrame.memory\_usage</span> method. This returns a <span class="title-ref">Series</span> with an index represented by column names and memory usage of each column shown in bytes. For the <span class="title-ref">DataFrame</span> above, the memory usage of each column and the total memory usage can be found with the <span class="title-ref">\~DataFrame.memory\_usage</span> method:

<div class="ipython">

python

df.memory\_usage()

\# total memory usage of dataframe df.memory\_usage().sum()

</div>

By default the memory usage of the <span class="title-ref">DataFrame</span> index is shown in the returned <span class="title-ref">Series</span>, the memory usage of the index can be suppressed by passing the `index=False` argument:

<div class="ipython">

python

df.memory\_usage(index=False)

</div>

The memory usage displayed by the <span class="title-ref">\~DataFrame.info</span> method utilizes the <span class="title-ref">\~DataFrame.memory\_usage</span> method to determine the memory usage of a <span class="title-ref">DataFrame</span> while also formatting the output in human-readable units (base-2 representation; i.e. 1KB = 1024 bytes).

See also \[Categorical Memory Usage \<categorical.memory\>\](\#categorical-memory-usage-\<categorical.memory\>).

## Using if/truth statements with pandas

pandas follows the NumPy convention of raising an error when you try to convert something to a `bool`. This happens in an `if`-statement or when using the boolean operations: `and`, `or`, and `not`. It is not clear what the result of the following code should be:

`` `python     >>> if pd.Series([False, True, False]):     ...     pass  Should it be ``True`because it's not zero-length, or`False`because there`<span class="title-ref"> are </span><span class="title-ref">False</span><span class="title-ref"> values? It is unclear, so instead, pandas raises a </span><span class="title-ref">ValueError</span>\`:

<div class="ipython" data-okexcept="">

python

  - if pd.Series(\[False, True, False\]):  
    print("I was true")

</div>

You need to explicitly choose what you want to do with the <span class="title-ref">DataFrame</span>, e.g. use <span class="title-ref">\~DataFrame.any</span>, <span class="title-ref">\~DataFrame.all</span> or <span class="title-ref">\~DataFrame.empty</span>. Alternatively, you might want to compare if the pandas object is `None`:

<div class="ipython">

python

  - if pd.Series(\[False, True, False\]) is not None:  
    print("I was not None")

</div>

Below is how to check if any of the values are `True`:

<div class="ipython">

python

  - if pd.Series(\[False, True, False\]).any():  
    print("I am any")

</div>

### Bitwise Boolean

Bitwise boolean operators like `==` and `!=` return a boolean <span class="title-ref">Series</span> which performs an element-wise comparison when compared to a scalar.

<div class="ipython">

python

s = pd.Series(range(5)) s == 4

</div>

See \[boolean comparisons\<basics.compare\>\](\#boolean-comparisons\<basics.compare\>) for more examples.

### Using the `in` operator

Using the Python `in` operator on a <span class="title-ref">Series</span> tests for membership in the **index**, not membership among the values.

<div class="ipython">

python

s = pd.Series(range(5), index=list("abcde")) 2 in s 'b' in s

</div>

If this behavior is surprising, keep in mind that using `in` on a Python dictionary tests keys, not values, and <span class="title-ref">Series</span> are dict-like. To test for membership in the values, use the method \`\~pandas.Series.isin\`:

<div class="ipython">

python

s.isin(\[2\]) s.isin(\[2\]).any()

</div>

For <span class="title-ref">DataFrame</span>, likewise, `in` applies to the column axis, testing for membership in the list of column names.

## Mutating with User Defined Function (UDF) methods

This section applies to pandas methods that take a UDF. In particular, the methods <span class="title-ref">DataFrame.apply</span>, <span class="title-ref">DataFrame.aggregate</span>, <span class="title-ref">DataFrame.transform</span>, and <span class="title-ref">DataFrame.filter</span>.

It is a general rule in programming that one should not mutate a container while it is being iterated over. Mutation will invalidate the iterator, causing unexpected behavior. Consider the example:

<div class="ipython">

python

values = \[0, 1, 2, 3, 4, 5\] n\_removed = 0 for k, value in enumerate(values): idx = k - n\_removed if value % 2 == 1: del values\[idx\] n\_removed += 1 else: values\[idx\] = value + 1 values

</div>

One probably would have expected that the result would be `[1, 3, 5]`. When using a pandas method that takes a UDF, internally pandas is often iterating over the <span class="title-ref">DataFrame</span> or other pandas object. Therefore, if the UDF mutates (changes) the <span class="title-ref">DataFrame</span>, unexpected behavior can arise.

Here is a similar example with \`DataFrame.apply\`:

<div class="ipython" data-okexcept="">

python

  - def f(s):  
    s.pop("a") return s

df = pd.DataFrame({"a": \[1, 2, 3\], "b": \[4, 5, 6\]}) df.apply(f, axis="columns")

</div>

To resolve this issue, one can make a copy so that the mutation does not apply to the container being iterated over.

<div class="ipython">

python

values = \[0, 1, 2, 3, 4, 5\] n\_removed = 0 for k, value in enumerate(values.copy()): idx = k - n\_removed if value % 2 == 1: del values\[idx\] n\_removed += 1 else: values\[idx\] = value + 1 values

</div>

<div class="ipython">

python

  - def f(s):  
    s = s.copy() s.pop("a") return s

df = pd.DataFrame({"a": \[1, 2, 3\], 'b': \[4, 5, 6\]}) df.apply(f, axis="columns")

</div>

## Missing value representation for NumPy types

### `np.nan` as the `NA` representation for NumPy types

For lack of `NA` (missing) support from the ground up in NumPy and Python in general, `NA` could have been represented with:

  - A *masked array* solution: an array of data and an array of boolean values indicating whether a value is there or is missing.
  - Using a special sentinel value, bit pattern, or set of sentinel values to denote `NA` across the dtypes.

The special value `np.nan` (Not-A-Number) was chosen as the `NA` value for NumPy types, and there are API functions like <span class="title-ref">DataFrame.isna</span> and <span class="title-ref">DataFrame.notna</span> which can be used across the dtypes to detect NA values. However, this choice has a downside of coercing missing integer data as float types as shown in \[gotchas.intna\](\#gotchas.intna).

### `NA` type promotions for NumPy types

When introducing NAs into an existing <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> via <span class="title-ref">\~Series.reindex</span> or some other means, boolean and integer types will be promoted to a different dtype in order to store the NAs. The promotions are summarized in this table:

| Typeclass  | Promotion dtype for storing NAs |
| ---------- | ------------------------------- |
| `floating` | no change                       |
| `object`   | no change                       |
| `integer`  | cast to `float64`               |
| `boolean`  | cast to `object`                |

### Support for integer `NA`

In the absence of high performance `NA` support being built into NumPy from the ground up, the primary casualty is the ability to represent NAs in integer arrays. For example:

<div class="ipython">

python

s = pd.Series(\[1, 2, 3, 4, 5\], index=list("abcde")) s s.dtype

s2 = s.reindex(\["a", "b", "c", "f", "u"\]) s2 s2.dtype

</div>

This trade-off is made largely for memory and performance reasons, and also so that the resulting <span class="title-ref">Series</span> continues to be "numeric".

If you need to represent integers with possibly missing values, use one of the nullable-integer extension dtypes provided by pandas or pyarrow

  - <span class="title-ref">Int8Dtype</span>
  - <span class="title-ref">Int16Dtype</span>
  - <span class="title-ref">Int32Dtype</span>
  - <span class="title-ref">Int64Dtype</span>
  - <span class="title-ref">ArrowDtype</span>

<div class="ipython">

python

s\_int = pd.Series(\[1, 2, 3, 4, 5\], index=list("abcde"), dtype=pd.Int64Dtype()) s\_int s\_int.dtype

s2\_int = s\_int.reindex(\["a", "b", "c", "f", "u"\]) s2\_int s2\_int.dtype

s\_int\_pa = pd.Series(\[1, 2, None\], dtype="int64\[pyarrow\]") s\_int\_pa

</div>

See \[integer\_na\](\#integer\_na) and \[pyarrow\](\#pyarrow) for more.

### Why not make NumPy like R?

Many people have suggested that NumPy should simply emulate the `NA` support present in the more domain-specific statistical programming language [R](https://www.r-project.org/). Part of the reason is the [NumPy type hierarchy](https://numpy.org/doc/stable/user/basics.types.html).

The R language, by contrast, only has a handful of built-in data types: `integer`, `numeric` (floating-point), `character`, and `boolean`. `NA` types are implemented by reserving special bit patterns for each type to be used as the missing value. While doing this with the full NumPy type hierarchy would be possible, it would be a more substantial trade-off (especially for the 8- and 16-bit data types) and implementation undertaking.

However, R `NA` semantics are now available by using masked NumPy types such as <span class="title-ref">Int64Dtype</span> or PyArrow types (<span class="title-ref">ArrowDtype</span>).

## Differences with NumPy

For <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> objects, <span class="title-ref">\~DataFrame.var</span> normalizes by `N-1` to produce [unbiased estimates of the population variance](https://en.wikipedia.org/wiki/Bias_of_an_estimator), while NumPy's <span class="title-ref">numpy.var</span> normalizes by N, which measures the variance of the sample. Note that <span class="title-ref">\~DataFrame.cov</span> normalizes by `N-1` in both pandas and NumPy.

## Thread-safety

pandas is not 100% thread safe. The known issues relate to the <span class="title-ref">\~DataFrame.copy</span> method. If you are doing a lot of copying of <span class="title-ref">DataFrame</span> objects shared among threads, we recommend holding locks inside the threads where the data copying occurs.

See [this link](https://stackoverflow.com/questions/13592618/python-pandas-dataframe-thread-safe) for more information.

## Byte-ordering issues

Occasionally you may have to deal with data that were created on a machine with a different byte order than the one on which you are running Python. A common symptom of this issue is an error like:

    Traceback
        ...
    ValueError: Big-endian buffer not supported on little-endian compiler

To deal with this issue you should convert the underlying NumPy array to the native system byte order *before* passing it to <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> constructors using something similar to the following:

<div class="ipython">

python

x = np.array(list(range(10)), "\>i4") \# big endian newx = x.byteswap().view(x.dtype.newbyteorder()) \# force native byteorder s = pd.Series(newx)

</div>

See [the NumPy documentation on byte order](https://numpy.org/doc/stable/user/basics.byteswapping.html) for more details.

---

groupby.md

---

<div id="groupby">

{{ header }}

</div>

# Group by: split-apply-combine

By "group by" we are referring to a process involving one or more of the following steps:

  - **Splitting** the data into groups based on some criteria.
  - **Applying** a function to each group independently.
  - **Combining** the results into a data structure.

Out of these, the split step is the most straightforward. In the apply step, we might wish to do one of the following:

  - **Aggregation**: compute a summary statistic (or statistics) for each group. Some examples:
    
    >   - Compute group sums or means.
    >   - Compute group sizes / counts.

  - **Transformation**: perform some group-specific computations and return a like-indexed object. Some examples:
    
    >   - Standardize data (zscore) within a group.
    >   - Filling NAs within groups with a value derived from each group.

  - **Filtration**: discard some groups, according to a group-wise computation that evaluates to True or False. Some examples:
    
    >   - Discard data that belong to groups with only a few members.
    >   - Filter out data based on the group sum or mean.

Many of these operations are defined on GroupBy objects. These operations are similar to those of the \[aggregating API \<basics.aggregate\>\](\#aggregating-api-\<basics.aggregate\>), \[window API \<window.overview\>\](\#window-api-\<window.overview\>), and \[resample API \<timeseries.aggregate\>\](\#resample-api-\<timeseries.aggregate\>).

It is possible that a given operation does not fall into one of these categories or is some combination of them. In such a case, it may be possible to compute the operation using GroupBy's `apply` method. This method will examine the results of the apply step and try to sensibly combine them into a single result if it doesn't fit into either of the above three categories.

\> **Note** \> An operation that is split into multiple steps using built-in GroupBy operations will be more efficient than using the `apply` method with a user-defined Python function.

The name GroupBy should be quite familiar to those who have used a SQL-based tool (or `itertools`), in which you can write code like:

`` `sql    SELECT Column1, Column2, mean(Column3), sum(Column4)    FROM SomeTable    GROUP BY Column1, Column2  We aim to make operations like this natural and easy to express using ``\` pandas. We'll address each area of GroupBy functionality, then provide some non-trivial examples / use cases.

See the \[cookbook\<cookbook.grouping\>\](\#cookbook\<cookbook.grouping\>) for some advanced strategies.

## Splitting an object into groups

The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object (more on what the GroupBy object is later), you may do the following:

<div class="ipython">

python

  - speeds = pd.DataFrame(
    
      - \[  
        ("bird", "Falconiformes", 389.0), ("bird", "Psittaciformes", 24.0), ("mammal", "Carnivora", 80.2), ("mammal", "Primates", np.nan), ("mammal", "Carnivora", 58),
    
    \], index=\["falcon", "parrot", "lion", "monkey", "leopard"\], columns=("class", "order", "max\_speed"),

) speeds

grouped = speeds.groupby("class") grouped = speeds.groupby(\["class", "order"\])

</div>

The mapping can be specified many different ways:

  - A Python function, to be called on each of the index labels.
  - A list or NumPy array of the same length as the index.
  - A dict or `Series`, providing a `label -> group name` mapping.
  - For `DataFrame` objects, a string indicating either a column name or an index level name to be used to group.
  - A list of any of the above things.

Collectively we refer to the grouping objects as the **keys**. For example, consider the following `DataFrame`:

\> **Note** \> A string passed to `groupby` may refer to either a column or an index level. If a string matches both a column name and an index level name, a `ValueError` will be raised.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": \["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"\], "B": \["one", "one", "two", "three", "two", "two", "one", "three"\], "C": np.random.randn(8), "D": np.random.randn(8),
    
    }

) df

</div>

On a DataFrame, we obtain a GroupBy object by calling <span class="title-ref">\~DataFrame.groupby</span>. This method returns a `pandas.api.typing.DataFrameGroupBy` instance. We could naturally group by either the `A` or `B` columns, or both:

<div class="ipython">

python

grouped = df.groupby("A") grouped = df.groupby("B") grouped = df.groupby(\["A", "B"\])

</div>

\> **Note** \> `df.groupby('A')` is just syntactic sugar for `df.groupby(df['A'])`.

The above GroupBy will split the DataFrame on its index (rows). To split by columns, first do a transpose:

<div class="ipython">

  - In \[4\]: def get\_letter\_type(letter):  
    ...: if letter.lower() in 'aeiou': ...: return 'vowel' ...: else: ...: return 'consonant' ...:

In \[5\]: grouped = df.T.groupby(get\_letter\_type)

</div>

pandas <span class="title-ref">\~pandas.Index</span> objects support duplicate values. If a non-unique index is used as the group key in a groupby operation, all values for the same index value will be considered to be in one group and thus the output of aggregation functions will only contain unique index values:

<div class="ipython">

python

index = \[1, 2, 3, 1, 2, 3\]

s = pd.Series(\[1, 2, 3, 10, 20, 30\], index=index)

s

grouped = s.groupby(level=0)

grouped.first()

grouped.last()

grouped.sum()

</div>

Note that **no splitting occurs** until it's needed. Creating the GroupBy object only verifies that you've passed a valid mapping.

\> **Note** \> Many kinds of complicated data manipulations can be expressed in terms of GroupBy operations (though it can't be guaranteed to be the most efficient implementation). You can get quite creative with the label mapping functions.

### GroupBy sorting

By default the group keys are sorted during the `groupby` operation. You may however pass `sort=False` for potential speedups. With `sort=False` the order among group-keys follows the order of appearance of the keys in the original dataframe:

<div class="ipython">

python

df2 = pd.DataFrame({"X": \["B", "B", "A", "A"\], "Y": \[1, 2, 3, 4\]}) df2.groupby(\["X"\]).sum() df2.groupby(\["X"\], sort=False).sum()

</div>

Note that `groupby` will preserve the order in which *observations* are sorted *within* each group. For example, the groups created by `groupby()` below are in the order they appeared in the original `DataFrame`:

<div class="ipython">

python

df3 = pd.DataFrame({"X": \["A", "B", "A", "B"\], "Y": \[1, 4, 3, 2\]}) df3.groupby("X").get\_group("A")

df3.groupby(\["X"\]).get\_group(("B",))

</div>

#### GroupBy dropna

By default `NA` values are excluded from group keys during the `groupby` operation. However, in case you want to include `NA` values in group keys, you could pass `dropna=False` to achieve it.

<div class="ipython">

python

df\_list = \[\[1, 2, 3\], \[1, None, 4\], \[2, 1, 3\], \[1, 2, 2\]\] df\_dropna = pd.DataFrame(df\_list, columns=\["a", "b", "c"\])

df\_dropna

</div>

<div class="ipython">

python

\# Default `dropna` is set to True, which will exclude NaNs in keys df\_dropna.groupby(by=\["b"\], dropna=True).sum()

\# In order to allow NaN in keys, set `dropna` to False df\_dropna.groupby(by=\["b"\], dropna=False).sum()

</div>

The default setting of `dropna` argument is `True` which means `NA` are not included in group keys.

### GroupBy object attributes

The `groups` attribute is a dictionary whose keys are the computed unique groups and corresponding values are the index labels belonging to each group. In the above example we have:

<div class="ipython">

python

df.groupby("A").groups df.T.groupby(get\_letter\_type).groups

</div>

Calling the standard Python `len` function on the GroupBy object returns the number of groups, which is the same as the length of the `groups` dictionary:

<div class="ipython">

python

grouped = df.groupby(\["A", "B"\]) grouped.groups len(grouped)

</div>

<div id="groupby.tabcompletion">

`GroupBy` will tab complete column names, GroupBy operations, and other attributes:

</div>

<div class="ipython">

python

n = 10 weight = np.random.normal(166, 20, size=n) height = np.random.normal(60, 10, size=n) time = pd.date\_range("1/1/2000", periods=n) gender = np.random.choice(\["male", "female"\], size=n) df = pd.DataFrame( {"height": height, "weight": weight, "gender": gender}, index=time ) df gb = df.groupby("gender")

</div>

<div class="ipython">

@verbatim In \[1\]: gb.\<TAB\> \# noqa: E225, E999 gb.agg gb.boxplot gb.cummin gb.describe gb.filter gb.get\_group gb.height gb.last gb.median gb.ngroups gb.plot gb.rank gb.std gb.transform gb.aggregate gb.count gb.cumprod gb.dtype gb.first gb.groups gb.hist gb.max gb.min gb.nth gb.prod gb.resample gb.sum gb.var gb.apply gb.cummax gb.cumsum gb.gender gb.head gb.indices gb.mean gb.name gb.ohlc gb.quantile gb.size gb.tail gb.weight

</div>

### GroupBy with MultiIndex

With \[hierarchically-indexed data \<advanced.hierarchical\>\](\#hierarchically-indexed-data-\<advanced.hierarchical\>), it's quite natural to group by one of the levels of the hierarchy.

Let's create a Series with a two-level `MultiIndex`.

<div class="ipython">

python

  - arrays = \[  
    \["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\], \["one", "two", "one", "two", "one", "two", "one", "two"\],

\] index = pd.MultiIndex.from\_arrays(arrays, names=\["first", "second"\]) s = pd.Series(np.random.randn(8), index=index) s

</div>

We can then group by one of the levels in `s`.

<div class="ipython">

python

grouped = s.groupby(level=0) grouped.sum()

</div>

If the MultiIndex has names specified, these can be passed instead of the level number:

<div class="ipython">

python

s.groupby(level="second").sum()

</div>

Grouping with multiple levels is supported.

<div class="ipython">

python

  - arrays = \[  
    \["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\], \["doo", "doo", "bee", "bee", "bop", "bop", "bop", "bop"\], \["one", "two", "one", "two", "one", "two", "one", "two"\],

\] index = pd.MultiIndex.from\_arrays(arrays, names=\["first", "second", "third"\]) s = pd.Series(np.random.randn(8), index=index) s s.groupby(level=\["first", "second"\]).sum()

</div>

Index level names may be supplied as keys.

<div class="ipython">

python

s.groupby(\["first", "second"\]).sum()

</div>

More on the `sum` function and aggregation later.

### Grouping DataFrame with Index levels and columns

A DataFrame may be grouped by a combination of columns and index levels. You can specify both column and index names, or use a <span class="title-ref">Grouper</span>.

Let's first create a DataFrame with a MultiIndex:

<div class="ipython">

python

  - arrays = \[  
    \["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\], \["one", "two", "one", "two", "one", "two", "one", "two"\],

\]

index = pd.MultiIndex.from\_arrays(arrays, names=\["first", "second"\])

df = pd.DataFrame({"A": \[1, 1, 1, 1, 2, 2, 3, 3\], "B": np.arange(8)}, index=index)

df

</div>

Then we group `df` by the `second` index level and the `A` column.

<div class="ipython">

python

df.groupby(\[pd.Grouper(level=1), "A"\]).sum()

</div>

Index levels may also be specified by name.

<div class="ipython">

python

df.groupby(\[pd.Grouper(level="second"), "A"\]).sum()

</div>

Index level names may be specified as keys directly to `groupby`.

<div class="ipython">

python

df.groupby(\["second", "A"\]).sum()

</div>

### DataFrame column selection in GroupBy

Once you have created the GroupBy object from a DataFrame, you might want to do something different for each of the columns. Thus, by using `[]` on the GroupBy object in a similar way as the one used to get a column from a DataFrame, you can do:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": \["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"\], "B": \["one", "one", "two", "three", "two", "two", "one", "three"\], "C": np.random.randn(8), "D": np.random.randn(8),
    
    }

)

df

grouped = df.groupby(\["A"\]) grouped\_C = grouped\["C"\] grouped\_D = grouped\["D"\]

</div>

This is mainly syntactic sugar for the alternative, which is much more verbose:

<div class="ipython">

python

df\["C"\].groupby(df\["A"\])

</div>

Additionally, this method avoids recomputing the internal grouping information derived from the passed key.

You can also include the grouping columns if you want to operate on them.

<div class="ipython">

python

grouped\[\["A", "B"\]\].sum()

</div>

\> **Note** \> The `groupby` operation in Pandas drops the `name` field of the columns Index object after the operation. This change ensures consistency in syntax between different column selection methods within groupby operations.

## Iterating through groups

With the GroupBy object in hand, iterating through the grouped data is very natural and functions similarly to :py\`itertools.groupby\`:

<div class="ipython">

In \[4\]: grouped = df.groupby('A')

  - In \[5\]: for name, group in grouped:  
    ...: print(name) ...: print(group) ...:

</div>

In the case of grouping by multiple keys, the group name will be a tuple:

<div class="ipython">

  - In \[5\]: for name, group in df.groupby(\['A', 'B'\]):  
    ...: print(name) ...: print(group) ...:

</div>

See \[timeseries.iterating-label\](\#timeseries.iterating-label).

## Selecting a group

A single group can be selected using \`.DataFrameGroupBy.get\_group\`:

<div class="ipython">

python

grouped.get\_group("bar")

</div>

Or for an object grouped on multiple columns:

<div class="ipython">

python

df.groupby(\["A", "B"\]).get\_group(("bar", "one"))

</div>

## Aggregation

An aggregation is a GroupBy operation that reduces the dimension of the grouping object. The result of an aggregation is, or at least is treated as, a scalar value for each column in a group. For example, producing the sum of each column in a group of values.

<div class="ipython">

python

  - animals = pd.DataFrame(
    
      - {  
        "kind": \["cat", "dog", "cat", "dog"\], "height": \[9.1, 6.0, 9.5, 34.0\], "weight": \[7.9, 7.5, 9.9, 198.0\],
    
    }

) animals animals.groupby("kind").sum()

</div>

In the result, the keys of the groups appear in the index by default. They can be instead included in the columns by passing `as_index=False`.

<div class="ipython">

python

animals.groupby("kind", as\_index=False).sum()

</div>

### Built-in aggregation methods

Many common aggregations are built-in to GroupBy objects as methods. Of the methods listed below, those with a `*` do *not* have an efficient, GroupBy-specific, implementation.

| Method                                                      | Description                                                        |
| ----------------------------------------------------------- | ------------------------------------------------------------------ |
| <span class="title-ref">\~.DataFrameGroupBy.any</span>      | Compute whether any of the values in the groups are truthy         |
| <span class="title-ref">\~.DataFrameGroupBy.all</span>      | Compute whether all of the values in the groups are truthy         |
| <span class="title-ref">\~.DataFrameGroupBy.count</span>    | Compute the number of non-NA values in the groups                  |
| <span class="title-ref">\~.DataFrameGroupBy.cov</span> \*   | Compute the covariance of the groups                               |
| <span class="title-ref">\~.DataFrameGroupBy.first</span>    | Compute the first occurring value in each group                    |
| <span class="title-ref">\~.DataFrameGroupBy.idxmax</span>   | Compute the index of the maximum value in each group               |
| <span class="title-ref">\~.DataFrameGroupBy.idxmin</span>   | Compute the index of the minimum value in each group               |
| <span class="title-ref">\~.DataFrameGroupBy.last</span>     | Compute the last occurring value in each group                     |
| <span class="title-ref">\~.DataFrameGroupBy.max</span>      | Compute the maximum value in each group                            |
| <span class="title-ref">\~.DataFrameGroupBy.mean</span>     | Compute the mean of each group                                     |
| <span class="title-ref">\~.DataFrameGroupBy.median</span>   | Compute the median of each group                                   |
| <span class="title-ref">\~.DataFrameGroupBy.min</span>      | Compute the minimum value in each group                            |
| <span class="title-ref">\~.DataFrameGroupBy.nunique</span>  | Compute the number of unique values in each group                  |
| <span class="title-ref">\~.DataFrameGroupBy.prod</span>     | Compute the product of the values in each group                    |
| <span class="title-ref">\~.DataFrameGroupBy.quantile</span> | Compute a given quantile of the values in each group               |
| <span class="title-ref">\~.DataFrameGroupBy.sem</span>      | Compute the standard error of the mean of the values in each group |
| <span class="title-ref">\~.DataFrameGroupBy.size</span>     | Compute the number of values in each group                         |
| <span class="title-ref">\~.DataFrameGroupBy.skew</span> \*  | Compute the skew of the values in each group                       |
| <span class="title-ref">\~.DataFrameGroupBy.std</span>      | Compute the standard deviation of the values in each group         |
| <span class="title-ref">\~.DataFrameGroupBy.sum</span>      | Compute the sum of the values in each group                        |
| <span class="title-ref">\~.DataFrameGroupBy.var</span>      | Compute the variance of the values in each group                   |

Some examples:

<div class="ipython">

python

df.groupby("A")\[\["C", "D"\]\].max() df.groupby(\["A", "B"\]).mean()

</div>

Another aggregation example is to compute the size of each group. This is included in GroupBy as the `size` method. It returns a Series whose index consists of the group names and the values are the sizes of each group.

<div class="ipython">

python

grouped = df.groupby(\["A", "B"\]) grouped.size()

</div>

While the <span class="title-ref">.DataFrameGroupBy.describe</span> method is not itself a reducer, it can be used to conveniently produce a collection of summary statistics about each of the groups.

<div class="ipython">

python

grouped.describe()

</div>

Another aggregation example is to compute the number of unique values of each group. This is similar to the <span class="title-ref">.DataFrameGroupBy.value\_counts</span> function, except that it only counts the number of unique values.

<div class="ipython">

python

ll = \[\['foo', 1\], \['foo', 2\], \['foo', 2\], \['bar', 1\], \['bar', 1\]\] df4 = pd.DataFrame(ll, columns=\["A", "B"\]) df4 df4.groupby("A")\["B"\].nunique()

</div>

\> **Note** \> Aggregation functions **will not** return the groups that you are aggregating over as named *columns* when `as_index=True`, the default. The grouped columns will be the **indices** of the returned object.

> Passing `as_index=False` **will** return the groups that you are aggregating over as named columns, regardless if they are named **indices** or *columns* in the inputs.

### The <span class="title-ref">\~.DataFrameGroupBy.aggregate</span> method

<div class="note">

<div class="title">

Note

</div>

The <span class="title-ref">\~.DataFrameGroupBy.aggregate</span> method can accept many different types of inputs. This section details using string aliases for various GroupBy methods; other inputs are detailed in the sections below.

</div>

Any reduction method that pandas implements can be passed as a string to <span class="title-ref">\~.DataFrameGroupBy.aggregate</span>. Users are encouraged to use the shorthand, `agg`. It will operate as if the corresponding method was called.

<div class="ipython">

python

grouped = df.groupby("A") grouped\[\["C", "D"\]\].aggregate("sum")

grouped = df.groupby(\["A", "B"\]) grouped.agg("sum")

</div>

The result of the aggregation will have the group names as the new index. In the case of multiple keys, the result is a \[MultiIndex \<advanced.hierarchical\>\](\#multiindex-\<advanced.hierarchical\>) by default. As mentioned above, this can be changed by using the `as_index` option:

<div class="ipython">

python

grouped = df.groupby(\["A", "B"\], as\_index=False) grouped.agg("sum")

df.groupby("A", as\_index=False)\[\["C", "D"\]\].agg("sum")

</div>

Note that you could use the <span class="title-ref">DataFrame.reset\_index</span> DataFrame function to achieve the same result as the column names are stored in the resulting `MultiIndex`, although this will make an extra copy.

<div class="ipython">

python

df.groupby(\["A", "B"\]).agg("sum").reset\_index()

</div>

### Aggregation with user-defined functions

Users can also provide their own User-Defined Functions (UDFs) for custom aggregations.

\> **Warning** \> When aggregating with a UDF, the UDF should not mutate the provided `Series`. See \[gotchas.udf-mutation\](\#gotchas.udf-mutation) for more information.

\> **Note** \> Aggregating with a UDF is often less performant than using the pandas built-in methods on GroupBy. Consider breaking up a complex operation into a chain of operations that utilize the built-in methods.

<div class="ipython">

python

animals animals.groupby("kind")\[\["height"\]\].agg(lambda x: set(x))

</div>

The resulting dtype will reflect that of the aggregating function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as `DataFrame` construction.

<div class="ipython">

python

animals.groupby("kind")\[\["height"\]\].agg(lambda x: x.astype(int).sum())

</div>

### Applying multiple functions at once

On a grouped `Series`, you can pass a list or dict of functions to <span class="title-ref">SeriesGroupBy.agg</span>, outputting a DataFrame:

<div class="ipython">

python

grouped = df.groupby("A") grouped\["C"\].agg(\["sum", "mean", "std"\])

</div>

On a grouped `DataFrame`, you can pass a list of functions to <span class="title-ref">DataFrameGroupBy.agg</span> to aggregate each column, which produces an aggregated result with a hierarchical column index:

<div class="ipython">

python

grouped\[\["C", "D"\]\].agg(\["sum", "mean", "std"\])

</div>

The resulting aggregations are named after the functions themselves.

For a `Series`, if you need to rename, you can add in a chained operation like this:

<div class="ipython">

python

  - (  
    grouped\["C"\] .agg(\["sum", "mean", "std"\]) .rename(columns={"sum": "foo", "mean": "bar", "std": "baz"})

)

</div>

Or, you can simply pass a list of tuples each with the name of the new column and the aggregate function:

<div class="ipython">

python

  - (  
    grouped\["C"\] .agg(\[("foo", "sum"), ("bar", "mean"), ("baz", "std")\])

)

</div>

For a grouped `DataFrame`, you can rename in a similar manner:

By chaining `rename` operation,

<div class="ipython">

python

  - (
    
      - grouped\[\["C", "D"\]\].agg(\["sum", "mean", "std"\]).rename(  
        columns={"sum": "foo", "mean": "bar", "std": "baz"}
    
    )

)

</div>

Or, passing a list of tuples,

<div class="ipython">

python

  - (
    
      - grouped\[\["C", "D"\]\].agg(  
        \[("foo", "sum"), ("bar", "mean"), ("baz", "std")\]
    
    )

)

</div>

\> **Note** \> In general, the output column names should be unique, but pandas will allow you apply to the same function (or two functions with the same name) to the same column.

> 
> 
> <div class="ipython">
> 
> python
> 
> grouped\["C"\].agg(\["sum", "sum"\])
> 
> </div>
> 
> pandas also allows you to provide multiple lambdas. In this case, pandas will mangle the name of the (nameless) lambda functions, appending `_<i>` to each subsequent lambda.
> 
> <div class="ipython">
> 
> python
> 
> grouped\["C"\].agg(\[lambda x: x.max() - x.min(), lambda x: x.median() - x.mean()\])
> 
> </div>

### Named aggregation

To support column-specific aggregation *with control over the output column names*, pandas accepts the special syntax in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span>, known as "named aggregation", where

  - The keywords are the *output* column names
  - The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. pandas provides the <span class="title-ref">NamedAgg</span> namedtuple with the fields `['column', 'aggfunc']` to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias.

<div class="ipython">

python

animals

  - animals.groupby("kind").agg(  
    min\_height=pd.NamedAgg(column="height", aggfunc="min"), max\_height=pd.NamedAgg(column="height", aggfunc="max"), average\_weight=pd.NamedAgg(column="weight", aggfunc="mean"),

)

</div>

<span class="title-ref">NamedAgg</span> is just a `namedtuple`. Plain tuples are allowed as well.

<div class="ipython">

python

  - animals.groupby("kind").agg(  
    min\_height=("height", "min"), max\_height=("height", "max"), average\_weight=("weight", "mean"),

)

</div>

If the column names you want are not valid Python keywords, construct a dictionary and unpack the keyword arguments

<div class="ipython">

python

  - animals.groupby("kind").agg(
    
      - \*\*{  
        "total weight": pd.NamedAgg(column="weight", aggfunc="sum")
    
    }

)

</div>

When using named aggregation, additional keyword arguments are not passed through to the aggregation functions; only pairs of `(column, aggfunc)` should be passed as `**kwargs`. If your aggregation functions require additional arguments, apply them partially with <span class="title-ref">functools.partial</span>.

Named aggregation is also valid for Series groupby aggregations. In this case there's no column selection, so the values are just the functions.

<div class="ipython">

python

  - animals.groupby("kind").height.agg(  
    min\_height="min", max\_height="max",

)

</div>

### Applying different functions to DataFrame columns

By passing a dict to `aggregate` you can apply a different aggregation to the columns of a DataFrame:

<div class="ipython">

python

grouped.agg({"C": "sum", "D": lambda x: np.std(x, ddof=1)})

</div>

The function names can also be strings. In order for a string to be valid it must be implemented on GroupBy:

<div class="ipython">

python

grouped.agg({"C": "sum", "D": "std"})

</div>

## Transformation

A transformation is a GroupBy operation whose result is indexed the same as the one being grouped. Common examples include <span class="title-ref">\~.DataFrameGroupBy.cumsum</span> and <span class="title-ref">\~.DataFrameGroupBy.diff</span>.

<div class="ipython">

python

speeds grouped = speeds.groupby("class")\["max\_speed"\] grouped.cumsum() grouped.diff()

</div>

Unlike aggregations, the groupings that are used to split the original object are not included in the result.

\> **Note** \> Since transformations do not include the groupings that are used to split the result, the arguments `as_index` and `sort` in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> have no effect.

A common use of a transformation is to add the result back into the original DataFrame.

<div class="ipython">

python

result = speeds.copy() result\["cumsum"\] = grouped.cumsum() result\["diff"\] = grouped.diff() result

</div>

### Built-in transformation methods

The following methods on GroupBy act as transformations.

| Method                                                         | Description                                                          |
| -------------------------------------------------------------- | -------------------------------------------------------------------- |
| <span class="title-ref">\~.DataFrameGroupBy.bfill</span>       | Back fill NA values within each group                                |
| <span class="title-ref">\~.DataFrameGroupBy.cumcount</span>    | Compute the cumulative count within each group                       |
| <span class="title-ref">\~.DataFrameGroupBy.cummax</span>      | Compute the cumulative max within each group                         |
| <span class="title-ref">\~.DataFrameGroupBy.cummin</span>      | Compute the cumulative min within each group                         |
| <span class="title-ref">\~.DataFrameGroupBy.cumprod</span>     | Compute the cumulative product within each group                     |
| <span class="title-ref">\~.DataFrameGroupBy.cumsum</span>      | Compute the cumulative sum within each group                         |
| <span class="title-ref">\~.DataFrameGroupBy.diff</span>        | Compute the difference between adjacent values within each group     |
| <span class="title-ref">\~.DataFrameGroupBy.ffill</span>       | Forward fill NA values within each group                             |
| <span class="title-ref">\~.DataFrameGroupBy.pct\_change</span> | Compute the percent change between adjacent values within each group |
| <span class="title-ref">\~.DataFrameGroupBy.rank</span>        | Compute the rank of each value within each group                     |
| <span class="title-ref">\~.DataFrameGroupBy.shift</span>       | Shift values up or down within each group                            |

In addition, passing any built-in aggregation method as a string to <span class="title-ref">\~.DataFrameGroupBy.transform</span> (see the next section) will broadcast the result across the group, producing a transformed result. If the aggregation method has an efficient implementation, this will be performant as well.

### The <span class="title-ref">\~.DataFrameGroupBy.transform</span> method

Similar to the \[aggregation method \<groupby.aggregate.agg\>\](\#aggregation-method-\<groupby.aggregate.agg\>), the <span class="title-ref">\~.DataFrameGroupBy.transform</span> method can accept string aliases to the built-in transformation methods in the previous section. It can *also* accept string aliases to the built-in aggregation methods. When an aggregation method is provided, the result will be broadcast across the group.

<div class="ipython">

python

speeds grouped = speeds.groupby("class")\[\["max\_speed"\]\] grouped.transform("cumsum") grouped.transform("sum")

</div>

In addition to string aliases, the <span class="title-ref">\~.DataFrameGroupBy.transform</span> method can also accept User-Defined Functions (UDFs). The UDF must:

  - Return a result that is either the same size as the group chunk or broadcastable to the size of the group chunk (e.g., a scalar, `grouped.transform(lambda x: x.iloc[-1])`).
  - Operate column-by-column on the group chunk. The transform is applied to the first group chunk using chunk.apply.
  - Not perform in-place operations on the group chunk. Group chunks should be treated as immutable, and changes to a group chunk may produce unexpected results. See \[gotchas.udf-mutation\](\#gotchas.udf-mutation) for more information.
  - (Optionally) operates on all columns of the entire group chunk at once. If this is supported, a fast path is used starting from the *second* chunk.

\> **Note** \> Transforming by supplying `transform` with a UDF is often less performant than using the built-in methods on GroupBy. Consider breaking up a complex operation into a chain of operations that utilize the built-in methods.

> All of the examples in this section can be made more performant by calling built-in methods instead of using UDFs. See \[below for examples \<groupby\_efficient\_transforms\>\](\#below-for-examples-\<groupby\_efficient\_transforms\>).

<div class="versionchanged">

2.0.0

When using `.transform` on a grouped DataFrame and the transformation function returns a DataFrame, pandas now aligns the result's index with the input's index. You can call `.to_numpy()` within the transformation function to avoid alignment.

</div>

Similar to \[groupby.aggregate.agg\](\#groupby.aggregate.agg), the resulting dtype will reflect that of the transformation function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as `DataFrame` construction.

Suppose we wish to standardize the data within each group:

<div class="ipython">

python

index = pd.date\_range("10/1/1999", periods=1100) ts = pd.Series(np.random.normal(0.5, 2, 1100), index) ts = ts.rolling(window=100, min\_periods=100).mean().dropna()

ts.head() ts.tail()

  - transformed = ts.groupby(lambda x: x.year).transform(  
    lambda x: (x - x.mean()) / x.std()

)

</div>

We would expect the result to now have mean 0 and standard deviation 1 within each group (up to floating-point error), which we can easily check:

<div class="ipython">

python

\# Original Data grouped = ts.groupby(lambda x: x.year) grouped.mean() grouped.std()

\# Transformed Data grouped\_trans = transformed.groupby(lambda x: x.year) grouped\_trans.mean() grouped\_trans.std()

</div>

We can also visually compare the original and transformed data sets.

<div class="ipython">

python

compare = pd.DataFrame({"Original": ts, "Transformed": transformed})

@savefig groupby\_transform\_plot.png compare.plot()

</div>

Transformation functions that have lower dimension outputs are broadcast to match the shape of the input array.

<div class="ipython">

python

ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min())

</div>

Another common data transform is to replace missing data with the group mean.

<div class="ipython">

python

cols = \["A", "B", "C"\] values = np.random.randn(1000, 3) values\[np.random.randint(0, 1000, 100), 0\] = np.nan values\[np.random.randint(0, 1000, 50), 1\] = np.nan values\[np.random.randint(0, 1000, 200), 2\] = np.nan data\_df = pd.DataFrame(values, columns=cols) data\_df

countries = np.array(\["US", "UK", "GR", "JP"\]) key = countries\[np.random.randint(0, 4, 1000)\]

grouped = data\_df.groupby(key)

\# Non-NA count in each group grouped.count()

transformed = grouped.transform(lambda x: x.fillna(x.mean()))

</div>

We can verify that the group means have not changed in the transformed data, and that the transformed data contains no NAs.

<div class="ipython">

python

grouped\_trans = transformed.groupby(key)

grouped.mean() \# original group means grouped\_trans.mean() \# transformation did not change group means

grouped.count() \# original has some missing data points grouped\_trans.count() \# counts after transformation grouped\_trans.size() \# Verify non-NA count equals group size

</div>

<div id="groupby_efficient_transforms">

As mentioned in the note above, each of the examples in this section can be computed more efficiently using built-in methods. In the code below, the inefficient way using a UDF is commented out and the faster alternative appears below.

</div>

<div class="ipython">

python

\# result = ts.groupby(lambda x: x.year).transform( \# lambda x: (x - x.mean()) / x.std() \# ) grouped = ts.groupby(lambda x: x.year) result = (ts - grouped.transform("mean")) / grouped.transform("std")

\# result = ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min()) grouped = ts.groupby(lambda x: x.year) result = grouped.transform("max") - grouped.transform("min")

\# grouped = data\_df.groupby(key) \# result = grouped.transform(lambda x: x.fillna(x.mean())) grouped = data\_df.groupby(key) result = data\_df.fillna(grouped.transform("mean"))

</div>

### Window and resample operations

It is possible to use `resample()`, `expanding()` and `rolling()` as methods on groupbys.

The example below will apply the `rolling()` method on the samples of the column B, based on the groups of column A.

<div class="ipython">

python

df\_re = pd.DataFrame({"A": \[1\] \* 10 + \[5\] \* 10, "B": np.arange(20)}) df\_re

df\_re.groupby("A").rolling(4).B.mean()

</div>

The `expanding()` method will accumulate a given operation (`sum()` in the example) for all the members of each particular group.

<div class="ipython">

python

df\_re.groupby("A").expanding().sum()

</div>

Suppose you want to use the `resample()` method to get a daily frequency in each group of your dataframe, and wish to complete the missing values with the `ffill()` method.

<div class="ipython">

python

  - df\_re = pd.DataFrame(
    
      - {  
        "date": pd.date\_range(start="2016-01-01", periods=4, freq="W"), "group": \[1, 1, 2, 2\], "val": \[5, 6, 7, 8\],
    
    }

).set\_index("date") df\_re

df\_re.groupby("group").resample("1D", include\_groups=False).ffill()

</div>

## Filtration

A filtration is a GroupBy operation that subsets the original grouping object. It may either filter out entire groups, part of groups, or both. Filtrations return a filtered version of the calling object, including the grouping columns when provided. In the following example, `class` is included in the result.

<div class="ipython">

python

speeds speeds.groupby("class").nth(1)

</div>

\> **Note** \> Unlike aggregations, filtrations do not add the group keys to the index of the result. Because of this, passing `as_index=False` or `sort=True` will not affect these methods.

Filtrations will respect subsetting the columns of the GroupBy object.

<div class="ipython">

python

speeds.groupby("class")\[\["order", "max\_speed"\]\].nth(1)

</div>

### Built-in filtrations

The following methods on GroupBy act as filtrations. All these methods have an efficient, GroupBy-specific, implementation.

| Method                                                  | Description                            |
| ------------------------------------------------------- | -------------------------------------- |
| <span class="title-ref">\~.DataFrameGroupBy.head</span> | Select the top row(s) of each group    |
| <span class="title-ref">\~.DataFrameGroupBy.nth</span>  | Select the nth row(s) of each group    |
| <span class="title-ref">\~.DataFrameGroupBy.tail</span> | Select the bottom row(s) of each group |

Users can also use transformations along with Boolean indexing to construct complex filtrations within groups. For example, suppose we are given groups of products and their volumes, and we wish to subset the data to only the largest products capturing no more than 90% of the total volume within each group.

<div class="ipython">

python

  - product\_volumes = pd.DataFrame(
    
      - {  
        "group": list("xxxxyyy"), "product": list("abcdefg"), "volume": \[10, 30, 20, 15, 40, 10, 20\],
    
    }

) product\_volumes

\# Sort by volume to select the largest products first product\_volumes = product\_volumes.sort\_values("volume", ascending=False) grouped = product\_volumes.groupby("group")\["volume"\] cumpct = grouped.cumsum() / grouped.transform("sum") cumpct significant\_products = product\_volumes\[cumpct \<= 0.9\] significant\_products.sort\_values(\["group", "product"\])

</div>

### The <span class="title-ref">\~DataFrameGroupBy.filter</span> method

\> **Note** \> Filtering by supplying `filter` with a User-Defined Function (UDF) is often less performant than using the built-in methods on GroupBy. Consider breaking up a complex operation into a chain of operations that utilize the built-in methods.

The `filter` method takes a User-Defined Function (UDF) that, when applied to an entire group, returns either `True` or `False`. The result of the `filter` method is then the subset of groups for which the UDF returned `True`.

Suppose we want to take only elements that belong to groups with a group sum greater than 2.

<div class="ipython">

python

sf = pd.Series(\[1, 1, 2, 3, 3, 3\]) sf.groupby(sf).filter(lambda x: x.sum() \> 2)

</div>

Another useful operation is filtering out elements that belong to groups with only a couple members.

<div class="ipython">

python

dff = pd.DataFrame({"A": np.arange(8), "B": list("aabbbbcc")}) dff.groupby("B").filter(lambda x: len(x) \> 2)

</div>

Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs.

<div class="ipython">

python

dff.groupby("B").filter(lambda x: len(x) \> 2, dropna=False)

</div>

For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion.

<div class="ipython">

python

dff\["C"\] = np.arange(8) dff.groupby("B").filter(lambda x: len(x\["C"\]) \> 2)

</div>

## Flexible `apply`

Some operations on the grouped data might not fit into the aggregation, transformation, or filtration categories. For these, you can use the `apply` function.

\> **Warning** \> `apply` has to try to infer from the result whether it should act as a reducer, transformer, *or* filter, depending on exactly what is passed to it. Thus the grouped column(s) may be included in the output or not. While it tries to intelligently guess how to behave, it can sometimes guess wrong.

\> **Note** \> All of the examples in this section can be more reliably, and more efficiently, computed using other pandas functionality.

<div class="ipython">

python

df grouped = df.groupby("A")

\# could also just call .describe() grouped\["C"\].apply(lambda x: x.describe())

</div>

The dimension of the returned result can also change:

<div class="ipython">

python

grouped = df.groupby('A')\['C'\]

  - def f(group):
    
      - return pd.DataFrame({'original': group,  
        'demeaned': group - group.mean()})

grouped.apply(f)

</div>

`apply` on a Series can operate on a returned value from the applied function that is itself a series, and possibly upcast the result to a DataFrame:

<div class="ipython">

python

  - def f(x):  
    return pd.Series(\[x, x \*\* 2\], index=\["x", "x^2"\])

s = pd.Series(np.random.rand(5)) s s.apply(f)

</div>

Similar to \[groupby.aggregate.agg\](\#groupby.aggregate.agg), the resulting dtype will reflect that of the apply function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as `DataFrame` construction.

### Control grouped column(s) placement with `group_keys`

To control whether the grouped column(s) are included in the indices, you can use the argument `group_keys` which defaults to `True`. Compare

<div class="ipython">

python

df.groupby("A", group\_keys=True).apply(lambda x: x, include\_groups=False)

</div>

with

<div class="ipython">

python

df.groupby("A", group\_keys=False).apply(lambda x: x, include\_groups=False)

</div>

## Numba accelerated routines

<div class="versionadded">

1.1

</div>

If [Numba](https://numba.pydata.org/) is installed as an optional dependency, the `transform` and `aggregate` methods support `engine='numba'` and `engine_kwargs` arguments. See \[enhancing performance with Numba \<enhancingperf.numba\>\](\#enhancing-performance-with-numba-\<enhancingperf.numba\>) for general usage of the arguments and performance considerations.

The function signature must start with `values, index` **exactly** as the data belonging to each group will be passed into `values`, and the group index will be passed into `index`.

\> **Warning** \> When using `engine='numba'`, there will be no "fall back" behavior internally. The group data and group index will be passed as NumPy arrays to the JITed user defined function, and no alternative execution attempts will be tried.

## Other useful features

### Exclusion of non-numeric columns

Again consider the example DataFrame we've been looking at:

<div class="ipython">

python

df

</div>

Suppose we wish to compute the standard deviation grouped by the `A` column. There is a slight problem, namely that we don't care about the data in column `B` because it is not numeric. You can avoid non-numeric columns by specifying `numeric_only=True`:

<div class="ipython">

python

df.groupby("A").std(numeric\_only=True)

</div>

Note that `df.groupby('A').colname.std().` is more efficient than `df.groupby('A').std().colname`. So if the result of an aggregation function is only needed over one column (here `colname`), it may be filtered *before* applying the aggregation function.

<div class="ipython">

python

from decimal import Decimal

  - df\_dec = pd.DataFrame(
    
      - {  
        "id": \[1, 2, 1, 2\], "int\_column": \[1, 2, 3, 4\], "dec\_column": \[ Decimal("0.50"), Decimal("0.15"), Decimal("0.25"), Decimal("0.40"), \],
    
    }

) df\_dec.groupby(\["id"\])\[\["dec\_column"\]\].sum()

</div>

### Handling of (un)observed Categorical values

When using a `Categorical` grouper (as a single grouper, or as part of multiple groupers), the `observed` keyword controls whether to return a cartesian product of all possible groupers values (`observed=False`) or only those that are observed groupers (`observed=True`).

Show all values:

<div class="ipython">

python

  - pd.Series(\[1, 1, 1\]).groupby(  
    pd.Categorical(\["a", "a", "a"\], categories=\["a", "b"\]), observed=False

).count()

</div>

Show only the observed values:

<div class="ipython">

python

  - pd.Series(\[1, 1, 1\]).groupby(  
    pd.Categorical(\["a", "a", "a"\], categories=\["a", "b"\]), observed=True

).count()

</div>

The returned dtype of the grouped will *always* include *all* of the categories that were grouped.

<div class="ipython">

python

  - s = (  
    pd.Series(\[1, 1, 1\]) .groupby(pd.Categorical(\["a", "a", "a"\], categories=\["a", "b"\]), observed=True) .count()

) s.index.dtype

</div>

### NA group handling

By `NA`, we are referring to any `NA` values, including <span class="title-ref">NA</span>, `NaN`, `NaT`, and `None`. If there are any `NA` values in the grouping key, by default these will be excluded. In other words, any "`NA` group" will be dropped. You can include NA groups by specifying `dropna=False`.

<div class="ipython">

python

df = pd.DataFrame({"key": \[1.0, 1.0, np.nan, 2.0, np.nan\], "A": \[1, 2, 3, 4, 5\]}) df

df.groupby("key", dropna=True).sum()

df.groupby("key", dropna=False).sum()

</div>

### Grouping with ordered factors

Categorical variables represented as instances of pandas's `Categorical` class can be used as group keys. If so, the order of the levels will be preserved. When `observed=False` and `sort=False`, any unobserved categories will be at the end of the result in order.

<div class="ipython">

python

  - days = pd.Categorical(  
    values=\["Wed", "Mon", "Thu", "Mon", "Wed", "Sat"\], categories=\["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"\],

) data = pd.DataFrame( { "day": days, "workers": \[3, 4, 1, 4, 2, 2\], } ) data

data.groupby("day", observed=False, sort=True).sum()

data.groupby("day", observed=False, sort=False).sum()

</div>

### Grouping with a grouper specification

You may need to specify a bit more data to properly group. You can use the `pd.Grouper` to provide this local control.

<div class="ipython">

python

import datetime

  - df = pd.DataFrame(
    
      - {  
        "Branch": "A A A A A A A B".split(), "Buyer": "Carl Mark Carl Carl Joe Joe Joe Carl".split(), "Quantity": \[1, 3, 5, 1, 8, 1, 9, 3\], "Date": \[ datetime.datetime(2013, 1, 1, 13, 0), datetime.datetime(2013, 1, 1, 13, 5), datetime.datetime(2013, 10, 1, 20, 0), datetime.datetime(2013, 10, 2, 10, 0), datetime.datetime(2013, 10, 1, 20, 0), datetime.datetime(2013, 10, 2, 10, 0), datetime.datetime(2013, 12, 2, 12, 0), datetime.datetime(2013, 12, 2, 14, 0), \],
    
    }

)

df

</div>

Groupby a specific column with the desired frequency. This is like resampling.

<div class="ipython">

python

df.groupby(\[pd.Grouper(freq="1ME", key="Date"), "Buyer"\])\[\["Quantity"\]\].sum()

</div>

When `freq` is specified, the object returned by `pd.Grouper` will be an instance of `pandas.api.typing.TimeGrouper`. When there is a column and index with the same name, you can use `key` to group by the column and `level` to group by the index.

<div class="ipython">

python

df = df.set\_index("Date") df\["Date"\] = df.index + pd.offsets.MonthEnd(2) df.groupby(\[pd.Grouper(freq="6ME", key="Date"), "Buyer"\])\[\["Quantity"\]\].sum()

df.groupby(\[pd.Grouper(freq="6ME", level="Date"), "Buyer"\])\[\["Quantity"\]\].sum()

</div>

### Taking the first rows of each group

Just like for a DataFrame or Series you can call head and tail on a groupby:

<div class="ipython">

python

df = pd.DataFrame(\[\[1, 2\], \[1, 4\], \[5, 6\]\], columns=\["A", "B"\]) df

g = df.groupby("A") g.head(1)

g.tail(1)

</div>

This shows the first or last n rows from each group.

### Taking the nth row of each group

To select the nth item from each group, use <span class="title-ref">.DataFrameGroupBy.nth</span> or <span class="title-ref">.SeriesGroupBy.nth</span>. Arguments supplied can be any integer, lists of integers, slices, or lists of slices; see below for examples. When the nth element of a group does not exist an error is *not* raised; instead no corresponding rows are returned.

In general this operation acts as a filtration. In certain cases it will also return one row per group, making it also a reduction. However because in general it can return zero or multiple rows per group, pandas treats it as a filtration in all cases.

<div class="ipython">

python

df = pd.DataFrame(\[\[1, np.nan\], \[1, 4\], \[5, 6\]\], columns=\["A", "B"\]) g = df.groupby("A")

g.nth(0) g.nth(-1) g.nth(1)

</div>

If the nth element of a group does not exist, then no corresponding row is included in the result. In particular, if the specified `n` is larger than any group, the result will be an empty DataFrame.

<div class="ipython">

python

g.nth(5)

</div>

If you want to select the nth not-null item, use the `dropna` kwarg. For a DataFrame this should be either `'any'` or `'all'` just like you would pass to dropna:

<div class="ipython">

python

\# nth(0) is the same as g.first() g.nth(0, dropna="any") g.first()

\# nth(-1) is the same as g.last() g.nth(-1, dropna="any") g.last()

g.B.nth(0, dropna="all")

</div>

You can also select multiple rows from each group by specifying multiple nth values as a list of ints.

<div class="ipython">

python

business\_dates = pd.date\_range(start="4/1/2014", end="6/30/2014", freq="B") df = pd.DataFrame(1, index=business\_dates, columns=\["a", "b"\]) \# get the first, 4th, and last date index for each month df.groupby(\[df.index.year, df.index.month\]).nth(\[0, 3, -1\])

</div>

You may also use slices or lists of slices.

<div class="ipython">

python

df.groupby(\[df.index.year, df.index.month\]).nth\[1:\] df.groupby(\[df.index.year, df.index.month\]).nth\[1:, :-1\]

</div>

### Enumerate group items

To see the order in which each row appears within its group, use the `cumcount` method:

<div class="ipython">

python

dfg = pd.DataFrame(list("aaabba"), columns=\["A"\]) dfg

dfg.groupby("A").cumcount()

dfg.groupby("A").cumcount(ascending=False)

</div>

### Enumerate groups

To see the ordering of the groups (as opposed to the order of rows within a group given by `cumcount`) you can use <span class="title-ref">.DataFrameGroupBy.ngroup</span>.

Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed.

<div class="ipython">

python

dfg = pd.DataFrame(list("aaabba"), columns=\["A"\]) dfg

dfg.groupby("A").ngroup()

dfg.groupby("A").ngroup(ascending=False)

</div>

### Plotting

Groupby also works with some plotting methods. In this case, suppose we suspect that the values in column 1 are 3 times higher on average in group "B".

<div class="ipython">

python

np.random.seed(1234) df = pd.DataFrame(np.random.randn(50, 2)) df\["g"\] = np.random.choice(\["A", "B"\], size=50) df.loc\[df\["g"\] == "B", 1\] += 3

</div>

We can easily visualize this with a boxplot:

<div class="ipython" data-okwarning="">

python

@savefig groupby\_boxplot.png df.groupby("g").boxplot()

</div>

The result of calling `boxplot` is a dictionary whose keys are the values of our grouping column `g` ("A" and "B"). The values of the resulting dictionary can be controlled by the `return_type` keyword of `boxplot`. See the \[visualization documentation\<visualization.box\>\](\#visualization-documentation\<visualization.box\>) for more.

\> **Warning** \> For historical reasons, `df.groupby("g").boxplot()` is not equivalent to `df.boxplot(by="g")`. See \[here\<visualization.box.return\>\](\#here\<visualization.box.return\>) for an explanation.

### Piping function calls

Similar to the functionality provided by `DataFrame` and `Series`, functions that take `GroupBy` objects can be chained together using a `pipe` method to allow for a cleaner, more readable syntax. To read about `.pipe` in general terms, see \[here \<basics.pipe\>\](\#here-\<basics.pipe\>).

Combining `.groupby` and `.pipe` is often useful when you need to reuse GroupBy objects.

As an example, imagine having a DataFrame with columns for stores, products, revenue and quantity sold. We'd like to do a groupwise calculation of *prices* (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable. First we set the data:

<div class="ipython">

python

n = 1000 df = pd.DataFrame( { "Store": np.random.choice(\["Store\_1", "Store\_2"\], n), "Product": np.random.choice(\["Product\_1", "Product\_2"\], n), "Revenue": (np.random.random(n) \* 50 + 10).round(2), "Quantity": np.random.randint(1, 10, size=n), } ) df.head(2)

</div>

We now find the prices per store/product.

<div class="ipython">

python

  - (  
    df.groupby(\["Store", "Product"\]) .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum()) .unstack() .round(2)

)

</div>

Piping can also be expressive when you want to deliver a grouped object to some arbitrary function, for example:

<div class="ipython">

python

  - def mean(groupby):  
    return groupby.mean()

df.groupby(\["Store", "Product"\]).pipe(mean)

</div>

Here `mean` takes a GroupBy object and finds the mean of the Revenue and Quantity columns respectively for each Store-Product combination. The `mean` function can be any function that takes in a GroupBy object; the `.pipe` will pass the GroupBy object as a parameter into the function you specify.

## Examples

### Multi-column factorization

By using <span class="title-ref">.DataFrameGroupBy.ngroup</span>, we can extract information about the groups in a way similar to <span class="title-ref">factorize</span> (as described further in the \[reshaping API \<reshaping.factorize\>\](\#reshaping-api-\<reshaping.factorize\>)) but which applies naturally to multiple columns of mixed type and different sources. This can be useful as an intermediate categorical-like step in processing, when the relationships between the group rows are more important than their content, or as input to an algorithm which only accepts the integer encoding. (For more information about support in pandas for full categorical data, see the \[Categorical introduction \<categorical\>\](\#categorical introduction-\<categorical\>) and the \[API documentation \<api.arrays.categorical\>\](\#api-documentation-\<api.arrays.categorical\>).)

<div class="ipython">

python

dfg = pd.DataFrame({"A": \[1, 1, 2, 3, 2\], "B": list("aaaba")})

dfg

dfg.groupby(\["A", "B"\]).ngroup()

dfg.groupby(\["A", \[0, 0, 0, 1, 1\]\]).ngroup()

</div>

### GroupBy by indexer to 'resample' data

Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples.

In order for resample to work on indices that are non-datetimelike, the following procedure can be utilized.

In the following examples, **df.index // 5** returns an integer array which is used to determine what gets selected for the groupby operation.

\> **Note** \> The example below shows how we can downsample by consolidation of samples into fewer ones. Here by using **df.index // 5**, we are aggregating the samples in bins. By applying **std()** function, we aggregate the information contained in many samples into a small subset of values which is their standard deviation thereby reducing the number of samples.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 2)) df df.index // 5 df.groupby(df.index // 5).std()

</div>

### Returning a Series to propagate names

Group DataFrame columns, compute a set of metrics and return a named Series. The Series name is used as the name for the column index. This is especially useful in conjunction with reshaping operations such as stacking, in which the column index name will be used as the name of the inserted column:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "a": \[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2\], "b": \[0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1\], "c": \[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0\], "d": \[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1\],
    
    }

)

  - def compute\_metrics(x):  
    result = {"b\_sum": x\["b"\].sum(), "c\_mean": x\["c"\].mean()} return pd.Series(result, name="metrics")

result = df.groupby("a").apply(compute\_metrics, include\_groups=False)

result

result.stack()

</div>

---

index.md

---

{{ header }}

# User Guide

The User Guide covers all of pandas by topic area. Each of the subsections introduces a topic (such as "working with missing data"), and discusses how pandas approaches the problem, with many examples throughout.

Users brand-new to pandas should start with \[10min\](\#10min).

For a high level summary of the pandas fundamentals, see \[dsintro\](\#dsintro) and \[basics\](\#basics).

Further information on any specific method can be obtained in the \[api\](\#api).

## How to read these guides

In these guides you will see input code inside code blocks such as:

    import pandas as pd
    pd.DataFrame({'A': [1, 2, 3]})

or:

<div class="ipython">

python

import pandas as pd pd.DataFrame({'A': \[1, 2, 3\]})

</div>

The first block is a standard python input, while in the second the `In [1]:` indicates the input is inside a [notebook](https://jupyter.org). In Jupyter Notebooks the last line is printed and plots are shown inline.

For example:

<div class="ipython">

python

a = 1 a

</div>

is equivalent to:

    a = 1
    print(a)

## Guides

<div class="toctree" data-maxdepth="2">

10min dsintro basics io pyarrow indexing advanced copy\_on\_write merging reshaping text missing\_data duplicates categorical integer\_na boolean visualization style groupby window timeseries timedeltas options enhancingperf scale sparse gotchas cookbook

</div>

---

indexing.md

---

<div id="indexing">

{{ header }}

</div>

# Indexing and selecting data

The axis labeling information in pandas objects serves many purposes:

  - Identifies data (i.e. provides *metadata*) using known indicators, important for analysis, visualization, and interactive console display.
  - Enables automatic and explicit data alignment.
  - Allows intuitive getting and setting of subsets of the data set.

In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.

\> **Note** \> The Python and NumPy indexing operators `[]` and attribute operator `.` provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as there's little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isn't known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the optimized pandas data access methods exposed in this chapter.

See the \[MultiIndex / Advanced Indexing \<advanced\>\](\#multiindex-/-advanced-indexing-\<advanced\>) for `MultiIndex` and more advanced indexing documentation.

See the \[cookbook\<cookbook.selection\>\](\#cookbook\<cookbook.selection\>) for some advanced strategies.

## Different choices for indexing

Object selection has had a number of user-requested additions in order to support more explicit location based indexing. pandas now supports three types of multi-axis indexing.

  - `.loc` is primarily label based, but may also be used with a boolean array. `.loc` will raise `KeyError` when the items are not found. Allowed inputs are:
    
    >   - A single label, e.g. `5` or `'a'` (Note that `5` is interpreted as a *label* of the index. This use is **not** an integer position along the index.).
    >   - A list or array of labels `['a', 'b', 'c']`.
    >   - A slice object with labels `'a':'f'` (Note that contrary to usual Python slices, **both** the start and the stop are included, when present in the index\! See \[Slicing with labels \<indexing.slicing\_with\_labels\>\](\#slicing-with-labels-\<indexing.slicing\_with\_labels\>) and \[Endpoints are inclusive \<advanced.endpoints\_are\_inclusive\>\](\#endpoints-are-inclusive-\<advanced.endpoints\_are\_inclusive\>).)
    >   - A boolean array (any `NA` values will be treated as `False`).
    >   - A `callable` function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).
    >   - A tuple of row (and column) indices whose elements are one of the above inputs.
    
    See more at \[Selection by Label \<indexing.label\>\](\#selection-by-label-\<indexing.label\>).

<!-- end list -->

  - \* `.iloc` is primarily integer position based (from `0` to  
    `length-1` of the axis), but may also be used with a boolean array. `.iloc` will raise `IndexError` if a requested indexer is out-of-bounds, except *slice* indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy *slice* semantics). Allowed inputs are:
    
    >   - An integer e.g. `5`.
    >   - A list or array of integers `[4, 3, 0]`.
    >   - A slice object with ints `1:7`.
    >   - A boolean array (any `NA` values will be treated as `False`).
    >   - A `callable` function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).
    >   - A tuple of row (and column) indices whose elements are one of the above inputs.
    
    See more at \[Selection by Position \<indexing.integer\>\](\#selection-by-position-\<indexing.integer\>), \[Advanced Indexing \<advanced\>\](\#advanced-indexing-\<advanced\>) and \[Advanced Hierarchical \<advanced.advanced\_hierarchical\>\](\#advanced

\--hierarchical-\<advanced.advanced\_hierarchical\>).

\* `.loc`, `.iloc`, and also `[]` indexing can accept a `callable` as indexer. See more at \[Selection By Callable \<indexing.callable\>\](\#selection-by-callable-\<indexing.callable\>).

> \> **Note**

  - \>  
    Destructuring tuple keys into row (and column) indexes occurs *before* callables are applied, so you cannot return a tuple from a callable to index both rows and columns.

Getting values from an object with multi-axes selection uses the following notation (using `.loc` as an example, but the following applies to `.iloc` as well). Any of the axes accessors may be the null slice `:`. Axes left out of the specification are assumed to be `:`, e.g. `p.loc['a']` is equivalent to `p.loc['a', :]`.

<div class="ipython">

python

ser = pd.Series(range(5), index=list("abcde")) ser.loc\[\["a", "c", "e"\]\]

df = pd.DataFrame(np.arange(25).reshape(5, 5), index=list("abcde"), columns=list("abcde")) df.loc\[\["a", "c", "e"\], \["b", "d"\]\]

</div>

## Basics

As mentioned when introducing the data structures in the \[last section \<basics\>\](\#last-section \<basics\>), the primary function of indexing with `[]` (a.k.a. `__getitem__` for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with `[]`:

| Object Type | Selection        | Return Value Type                 |
| ----------- | ---------------- | --------------------------------- |
| Series      | `series[label]`  | scalar value                      |
| DataFrame   | `frame[colname]` | `Series` corresponding to colname |

Here we construct a simple time series data set to use for illustrating the indexing functionality:

<div class="ipython">

python

dates = pd.date\_range('1/1/2000', periods=8) df = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=\['A', 'B', 'C', 'D'\]) df

</div>

\> **Note** \> None of the indexing functionality is time series specific unless specifically stated.

Thus, as per above, we have the most basic indexing using `[]`:

<div class="ipython">

python

s = df\['A'\] s\[dates\[5\]\]

</div>

You can pass a list of columns to `[]` to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner:

<div class="ipython">

python

df df\[\['B', 'A'\]\] = df\[\['A', 'B'\]\] df

</div>

You may find this useful for applying a transform (in-place) to a subset of the columns.

\> **Warning** \> pandas aligns all AXES when setting `Series` and `DataFrame` from `.loc`.

> This will **not** modify `df` because the column alignment is before value assignment.
> 
> <div class="ipython">
> 
> python
> 
> df\[\['A', 'B'\]\] df.loc\[:, \['B', 'A'\]\] = df\[\['A', 'B'\]\] df\[\['A', 'B'\]\]
> 
> </div>
> 
> The correct way to swap column values is by using raw values:
> 
> <div class="ipython">
> 
> python
> 
> df.loc\[:, \['B', 'A'\]\] = df\[\['A', 'B'\]\].to\_numpy() df\[\['A', 'B'\]\]
> 
> </div>
> 
> However, pandas does not align AXES when setting `Series` and `DataFrame` from `.iloc` because `.iloc` operates by position.
> 
> This will modify `df` because the column alignment is not done before value assignment.
> 
> <div class="ipython">
> 
> python
> 
> df\[\['A', 'B'\]\] df.iloc\[:, \[1, 0\]\] = df\[\['A', 'B'\]\] df\[\['A','B'\]\]
> 
> </div>

## Attribute access

<div id="indexing.columns.multiple">

<div id="indexing.df_cols">

<div id="indexing.attribute_access">

You may access an index on a `Series` or column on a `DataFrame` directly as an attribute:

</div>

</div>

</div>

<div class="ipython">

python

sa = pd.Series(\[1, 2, 3\], index=list('abc')) dfa = df.copy()

</div>

<div class="ipython">

python

sa.b dfa.A

</div>

<div class="ipython">

python

sa.a = 5 sa dfa.A = list(range(len(dfa.index))) \# ok if A already exists dfa dfa\['A'\] = list(range(len(dfa.index))) \# use this form to create a new column dfa

</div>

\> **Warning** \> - You can use this access only if the index element is a valid Python identifier, e.g. `s.1` is not allowed. See [here for an explanation of valid identifiers](https://docs.python.org/3/reference/lexical_analysis.html#identifiers).

>   - The attribute will not be available if it conflicts with an existing method name, e.g. `s.min` is not allowed, but `s['min']` is possible.
>   - Similarly, the attribute will not be available if it conflicts with any of the following list: `index`, `major_axis`, `minor_axis`, `items`.
>   - In any of these cases, standard indexing will still work, e.g. `s['1']`, `s['min']`, and `s['index']` will access the corresponding element or column.

If you are using the IPython environment, you may also use tab-completion to see these accessible attributes.

You can also assign a `dict` to a row of a `DataFrame`:

<div class="ipython">

python

x = pd.DataFrame({'x': \[1, 2, 3\], 'y': \[3, 4, 5\]}) x.iloc\[1\] = {'x': 9, 'y': 99} x

</div>

You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column and will this raise a `UserWarning`:

<div class="ipython" data-okwarning="">

python

df\_new = pd.DataFrame({'one': \[1., 2., 3.\]}) df\_new.two = \[4, 5, 6\] df\_new

</div>

## Slicing ranges

The most robust and consistent way of slicing ranges along arbitrary axes is described in the \[Selection by Position \<indexing.integer\>\](\#selection-by-position-\<indexing.integer\>) section detailing the `.iloc` method. For now, we explain the semantics of slicing using the `[]` operator.

> \> **Note**

  - \>  
    When the <span class="title-ref">Series</span> has float indices, slicing will select by position.

With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels:

<div class="ipython">

python

s\[:5\] s\[::2\] s\[::-1\]

</div>

Note that setting works as well:

<div class="ipython">

python

s2 = s.copy() s2\[:5\] = 0 s2

</div>

With DataFrame, slicing inside of `[]` **slices the rows**. This is provided largely as a convenience since it is such a common operation.

<div class="ipython">

python

df\[:3\] df\[::-1\]

</div>

## Selection by label

\> **Warning** \> `.loc` is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a `DatetimeIndex`. These will raise a `TypeError`.

> 
> 
> <div class="ipython" data-okexcept="">
> 
> python
> 
>   - dfl = pd.DataFrame(np.random.randn(5, 4),  
>     columns=list('ABCD'), index=pd.date\_range('20130101', periods=5))
> 
> dfl dfl.loc\[2:3\]
> 
> </div>
> 
> String likes in slicing *can* be convertible to the type of the index and lead to natural slicing.
> 
> <div class="ipython">
> 
> python
> 
> dfl.loc\['20130102':'20130104'\]
> 
> </div>

pandas provides a suite of methods in order to have **purely label based indexing**. This is a strict inclusion based protocol. Every label asked for must be in the index, or a `KeyError` will be raised. When slicing, both the start bound **AND** the stop bound are *included*, if present in the index. Integers are valid labels, but they refer to the label **and not the position**.

The `.loc` attribute is the primary access method. The following are valid inputs:

  - A single label, e.g. `5` or `'a'` (Note that `5` is interpreted as a *label* of the index. This use is **not** an integer position along the index.).
  - A list or array of labels `['a', 'b', 'c']`.
  - A slice object with labels `'a':'f'` (Note that contrary to usual Python slices, **both** the start and the stop are included, when present in the index\! See \[Slicing with labels \<indexing.slicing\_with\_labels\>\](\#slicing-with-labels-\<indexing.slicing\_with\_labels\>).
  - A boolean array.
  - A `callable`, see \[Selection By Callable \<indexing.callable\>\](\#selection-by-callable-\<indexing.callable\>).

<div class="ipython">

python

s1 = pd.Series(np.random.randn(6), index=list('abcdef')) s1 s1.loc\['c':\] s1.loc\['b'\]

</div>

Note that setting works as well:

<div class="ipython">

python

s1.loc\['c':\] = 0 s1

</div>

With a DataFrame:

<div class="ipython">

python

  - df1 = pd.DataFrame(np.random.randn(6, 4),  
    index=list('abcdef'), columns=list('ABCD'))

df1 df1.loc\[\['a', 'b', 'd'\], :\]

</div>

Accessing via label slices:

<div class="ipython">

python

df1.loc\['d':, 'A':'C'\]

</div>

For getting a cross section using a label (equivalent to `df.xs('a')`):

<div class="ipython">

python

df1.loc\['a'\]

</div>

For getting values with a boolean array:

<div class="ipython">

python

df1.loc\['a'\] \> 0 df1.loc\[:, df1.loc\['a'\] \> 0\]

</div>

NA values in a boolean array propagate as `False`:

<div class="ipython">

python

mask = pd.array(\[True, False, True, False, pd.NA, False\], dtype="boolean") mask df1\[mask\]

</div>

For getting a value explicitly:

<div class="ipython">

python

\# this is also equivalent to `df1.at['a','A']` df1.loc\['a', 'A'\]

</div>

### Slicing with labels

When using `.loc` with slices, if both the start and the stop labels are present in the index, then elements *located* between the two (including them) are returned:

<div class="ipython">

python

s = pd.Series(list('abcde'), index=\[0, 3, 2, 5, 4\]) s.loc\[3:5\]

</div>

If the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which *rank* between the two:

<div class="ipython">

python

s.sort\_index() s.sort\_index().loc\[1:6\]

</div>

However, if at least one of the two is absent *and* the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, `s.loc[1:6]` would raise `KeyError`.

For the rationale behind this behavior, see \[Endpoints are inclusive \<advanced.endpoints\_are\_inclusive\>\](\#endpoints-are-inclusive-\<advanced.endpoints\_are\_inclusive\>).

<div class="ipython">

python

s = pd.Series(list('abcdef'), index=\[0, 3, 2, 5, 4, 2\]) s.loc\[3:5\]

</div>

Also, if the index has duplicate labels *and* either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, `s.loc[2:5]` would raise a `KeyError`.

For more information about duplicate labels, see \[Duplicate Labels \<duplicates\>\](\#duplicate-labels-\<duplicates\>).

## Selection by position

pandas provides a suite of methods in order to get **purely integer based indexing**. The semantics follow closely Python and NumPy slicing. These are `0-based` indexing. When slicing, the start bound is *included*, while the upper bound is *excluded*. Trying to use a non-integer, even a **valid** label will raise an `IndexError`.

The `.iloc` attribute is the primary access method. The following are valid inputs:

  - An integer e.g. `5`.
  - A list or array of integers `[4, 3, 0]`.
  - A slice object with ints `1:7`.
  - A boolean array.
  - A `callable`, see \[Selection By Callable \<indexing.callable\>\](\#selection-by-callable-\<indexing.callable\>).
  - A tuple of row (and column) indexes, whose elements are one of the above types.

<div class="ipython">

python

s1 = pd.Series(np.random.randn(5), index=list(range(0, 10, 2))) s1 s1.iloc\[:3\] s1.iloc\[3\]

</div>

Note that setting works as well:

<div class="ipython">

python

s1.iloc\[:3\] = 0 s1

</div>

With a DataFrame:

<div class="ipython">

python

  - df1 = pd.DataFrame(np.random.randn(6, 4),  
    index=list(range(0, 12, 2)), columns=list(range(0, 8, 2)))

df1

</div>

Select via integer slicing:

<div class="ipython">

python

df1.iloc\[:3\] df1.iloc\[1:5, 2:4\]

</div>

Select via integer list:

<div class="ipython">

python

df1.iloc\[\[1, 3, 5\], \[1, 3\]\]

</div>

<div class="ipython">

python

df1.iloc\[1:3, :\]

</div>

<div class="ipython">

python

df1.iloc\[:, 1:3\]

</div>

<div class="ipython">

python

\# this is also equivalent to `df1.iat[1,1]` df1.iloc\[1, 1\]

</div>

For getting a cross section using an integer position (equiv to `df.xs(1)`):

<div class="ipython">

python

df1.iloc\[1\]

</div>

Out of range slice indexes are handled gracefully just as in Python/NumPy.

<div class="ipython">

python

\# these are allowed in Python/NumPy. x = list('abcdef') x x\[4:10\] x\[8:10\] s = pd.Series(x) s s.iloc\[4:10\] s.iloc\[8:10\]

</div>

Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned).

<div class="ipython">

python

dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB')) dfl dfl.iloc\[:, 2:3\] dfl.iloc\[:, 1:3\] dfl.iloc\[4:6\]

</div>

A single indexer that is out of bounds will raise an `IndexError`. A list of indexers where any element is out of bounds will raise an `IndexError`.

<div class="ipython" data-okexcept="">

python

dfl.iloc\[\[4, 5, 6\]\]

</div>

<div class="ipython" data-okexcept="">

python

dfl.iloc\[:, 4\]

</div>

## Selection by callable

`.loc`, `.iloc`, and also `[]` indexing can accept a `callable` as indexer. The `callable` must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing.

\> **Note** \> For `.iloc` indexing, returning a tuple from the callable is not supported, since tuple destructuring for row and column indexes occurs *before* applying callables.

<div class="ipython">

python

  - df1 = pd.DataFrame(np.random.randn(6, 4),  
    index=list('abcdef'), columns=list('ABCD'))

df1

df1.loc\[lambda df: df\['A'\] \> 0, :\] df1.loc\[:, lambda df: \['A', 'B'\]\]

df1.iloc\[:, lambda df: \[0, 1\]\]

df1\[lambda df: df.columns\[0\]\]

</div>

You can use callable indexing in `Series`.

<div class="ipython">

python

df1\['A'\].loc\[lambda s: s \> 0\]

</div>

Using these methods / indexers, you can chain data selection operations without using a temporary variable.

<div class="ipython">

python

bb = pd.read\_csv('data/baseball.csv', index\_col='id') (bb.groupby(\['year', 'team'\]).sum(numeric\_only=True) .loc\[lambda df: df\['r'\] \> 100\])

</div>

## Combining positional and label-based indexing

If you wish to get the 0th and the 2nd elements from the index in the 'A' column, you can do:

<div class="ipython">

python

  - dfd = pd.DataFrame({'A': \[1, 2, 3\],  
    'B': \[4, 5, 6\]}, index=list('abc'))

dfd dfd.loc\[dfd.index\[\[0, 2\]\], 'A'\]

</div>

This can also be expressed using `.iloc`, by explicitly getting locations on the indexers, and using *positional* indexing to select things.

<div class="ipython">

python

dfd.iloc\[\[0, 2\], dfd.columns.get\_loc('A')\]

</div>

For getting *multiple* indexers, using `.get_indexer`:

<div class="ipython">

python

dfd.iloc\[\[0, 2\], dfd.columns.get\_indexer(\['A', 'B'\])\]

</div>

### Reindexing

The idiomatic way to achieve selecting potentially not-found elements is via `.reindex()`. See also the section on \[reindexing \<basics.reindexing\>\](\#reindexing-\<basics.reindexing\>).

<div class="ipython">

python

s = pd.Series(\[1, 2, 3\]) s.reindex(\[1, 2, 3\])

</div>

Alternatively, if you want to select only *valid* keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection.

<div class="ipython">

python

labels = \[1, 2, 3\] s.loc\[s.index.intersection(labels)\]

</div>

Having a duplicated index will raise for a `.reindex()`:

<div class="ipython" data-okexcept="">

python

s = pd.Series(np.arange(4), index=\['a', 'a', 'b', 'c'\]) labels = \['c', 'd'\] s.reindex(labels)

</div>

Generally, you can intersect the desired labels with the current axis, and then reindex.

<div class="ipython">

python

s.loc\[s.index.intersection(labels)\].reindex(labels)

</div>

However, this would *still* raise if your resulting index is duplicated.

<div class="ipython" data-okexcept="">

python

labels = \['a', 'd'\] s.loc\[s.index.intersection(labels)\].reindex(labels)

</div>

## Selecting random samples

A random selection of rows or columns from a Series or DataFrame with the <span class="title-ref">\~DataFrame.sample</span> method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows.

<div class="ipython">

python

s = pd.Series(\[0, 1, 2, 3, 4, 5\])

\# When no arguments are passed, returns 1 row. s.sample()

\# One may specify either a number of rows: s.sample(n=3)

\# Or a fraction of the rows: s.sample(frac=0.5)

</div>

By default, `sample` will return each row at most once, but one can also sample with replacement using the `replace` option:

<div class="ipython">

python

s = pd.Series(\[0, 1, 2, 3, 4, 5\])

\# Without replacement (default): s.sample(n=6, replace=False)

\# With replacement: s.sample(n=6, replace=True)

</div>

By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the `sample` function sampling weights as `weights`. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example:

<div class="ipython">

python

s = pd.Series(\[0, 1, 2, 3, 4, 5\]) example\_weights = \[0, 0, 0.2, 0.2, 0.2, 0.4\] s.sample(n=3, weights=example\_weights)

\# Weights will be re-normalized automatically example\_weights2 = \[0.5, 0, 0, 0, 0, 0\] s.sample(n=1, weights=example\_weights2)

</div>

When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string.

<div class="ipython">

python

  - df2 = pd.DataFrame({'col1': \[9, 8, 7, 6\],  
    'weight\_column': \[0.5, 0.4, 0.1, 0\]})

df2.sample(n=3, weights='weight\_column')

</div>

`sample` also allows users to sample columns instead of rows using the `axis` argument.

<div class="ipython">

python

df3 = pd.DataFrame({'col1': \[1, 2, 3\], 'col2': \[2, 3, 4\]}) df3.sample(n=1, axis=1)

</div>

Finally, one can also set a seed for `sample`'s random number generator using the `random_state` argument, which will accept either an integer (as a seed) or a NumPy RandomState object.

<div class="ipython">

python

df4 = pd.DataFrame({'col1': \[1, 2, 3\], 'col2': \[2, 3, 4\]})

\# With a given seed, the sample will always draw the same rows. df4.sample(n=2, random\_state=2) df4.sample(n=2, random\_state=2)

</div>

## Setting with enlargement

The `.loc/[]` operations can perform enlargement when setting a non-existent key for that axis.

In the `Series` case this is effectively an appending operation.

<div class="ipython">

python

se = pd.Series(\[1, 2, 3\]) se se\[5\] = 5. se

</div>

A `DataFrame` can be enlarged on either axis via `.loc`.

<div class="ipython">

python

  - dfi = pd.DataFrame(np.arange(6).reshape(3, 2),  
    columns=\['A', 'B'\])

dfi dfi.loc\[:, 'C'\] = dfi.loc\[:, 'A'\] dfi

</div>

This is like an `append` operation on the `DataFrame`.

<div class="ipython">

python

dfi.loc\[3\] = 5 dfi

</div>

## Fast scalar value getting and setting

Since indexing with `[]` must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what you're asking for. If you only want to access a scalar value, the fastest way is to use the `at` and `iat` methods, which are implemented on all of the data structures.

Similarly to `loc`, `at` provides **label** based scalar lookups, while, `iat` provides **integer** based lookups analogously to `iloc`

<div class="ipython">

python

s.iat\[5\] df.at\[dates\[5\], 'A'\] df.iat\[3, 0\]

</div>

You can also set using these same indexers.

<div class="ipython">

python

df.at\[dates\[5\], 'E'\] = 7 df.iat\[3, 0\] = 7

</div>

`at` may enlarge the object in-place as above if the indexer is missing.

<div class="ipython">

python

df.at\[dates\[-1\] + pd.Timedelta('1 day'), 0\] = 7 df

</div>

## Boolean indexing

<div id="indexing.boolean">

Another common operation is the use of boolean vectors to filter the data. The operators are: `|` for `or`, `&` for `and`, and `~` for `not`. These **must** be grouped by using parentheses, since by default Python will evaluate an expression such as `df['A'] > 2 & df['B'] < 3` as `df['A'] > (2 & df['B']) < 3`, while the desired evaluation order is `(df['A'] > 2) & (df['B'] < 3)`.

</div>

Using a boolean vector to index a Series works exactly as in a NumPy ndarray:

<div class="ipython">

python

s = pd.Series(range(-3, 4)) s s\[s \> 0\] s\[(s \< -1) | (s \> 0.5)\] s\[\~(s \< 0)\]

</div>

You may select rows from a DataFrame using a boolean vector the same length as the DataFrame's index (for example, something derived from one of the columns of the DataFrame):

<div class="ipython">

python

df\[df\['A'\] \> 0\]

</div>

List comprehensions and the `map` method of Series can also be used to produce more complex criteria:

<div class="ipython">

python

  - df2 = pd.DataFrame({'a': \['one', 'one', 'two', 'three', 'two', 'one', 'six'\],  
    'b': \['x', 'y', 'y', 'x', 'y', 'x', 'x'\], 'c': np.random.randn(7)})

\# only want 'two' or 'three' criterion = df2\['a'\].map(lambda x: x.startswith('t'))

df2\[criterion\]

\# equivalent but slower df2\[\[x.startswith('t') for x in df2\['a'\]\]\]

\# Multiple criteria df2\[criterion & (df2\['b'\] == 'x')\]

</div>

With the choice methods \[Selection by Label \<indexing.label\>\](\#selection-by-label-\<indexing.label\>), \[Selection by Position \<indexing.integer\>\](\#selection-by-position-\<indexing.integer\>), and \[Advanced Indexing \<advanced\>\](\#advanced-indexing-\<advanced\>) you may select along more than one axis using boolean vectors combined with other indexing expressions.

<div class="ipython">

python

df2.loc\[criterion & (df2\['b'\] == 'x'), 'b':'c'\]

</div>

\> **Warning** \> `iloc` supports two kinds of boolean indexing. If the indexer is a boolean `Series`, an error will be raised. For instance, in the following example, `df.iloc[s.values, 1]` is ok. The boolean indexer is an array. But `df.iloc[s, 1]` would raise `ValueError`.

> 
> 
> <div class="ipython">
> 
> python
> 
>   - df = pd.DataFrame(\[\[1, 2\], \[3, 4\], \[5, 6\]\],  
>     index=list('abc'), columns=\['A', 'B'\])
> 
> s = (df\['A'\] \> 2) s
> 
> df.loc\[s, 'B'\]
> 
> df.iloc\[s.values, 1\]
> 
> </div>

## Indexing with isin

Consider the <span class="title-ref">\~Series.isin</span> method of `Series`, which returns a boolean vector that is true wherever the `Series` elements exist in the passed list. This allows you to select rows where one or more columns have values you want:

<div class="ipython">

python

s = pd.Series(np.arange(5), index=np.arange(5)\[::-1\], dtype='int64') s s.isin(\[2, 4, 6\]) s\[s.isin(\[2, 4, 6\])\]

</div>

The same method is available for `Index` objects and is useful for the cases when you don't know which of the sought labels are in fact present:

<div class="ipython">

python

s\[s.index.isin(\[2, 4, 6\])\]

\# compare it to the following s.reindex(\[2, 4, 6\])

</div>

In addition to that, `MultiIndex` allows selecting a separate level to use in the membership check:

<div class="ipython">

python

  - s\_mi = pd.Series(np.arange(6),  
    index=pd.MultiIndex.from\_product(\[\[0, 1\], \['a', 'b', 'c'\]\]))

s\_mi s\_mi.iloc\[s\_mi.index.isin(\[(1, 'a'), (2, 'b'), (0, 'c')\])\] s\_mi.iloc\[s\_mi.index.isin(\['a', 'c', 'e'\], level=1)\]

</div>

DataFrame also has an <span class="title-ref">\~DataFrame.isin</span> method. When calling `isin`, pass a set of values as either an array or dict. If values is an array, `isin` returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values.

<div class="ipython">

python

  - df = pd.DataFrame({'vals': \[1, 2, 3, 4\], 'ids': \['a', 'b', 'f', 'n'\],  
    'ids2': \['a', 'n', 'c', 'n'\]})

values = \['a', 'b', 1, 3\]

df.isin(values)

</div>

Oftentimes you'll want to match certain values with certain columns. Just make values a `dict` where the key is the column, and the value is a list of items you want to check for.

<div class="ipython">

python

values = {'ids': \['a', 'b'\], 'vals': \[1, 3\]}

df.isin(values)

</div>

To return the DataFrame of booleans where the values are *not* in the original DataFrame, use the `~` operator:

<div class="ipython">

python

values = {'ids': \['a', 'b'\], 'vals': \[1, 3\]}

\~df.isin(values)

</div>

Combine DataFrame's `isin` with the `any()` and `all()` methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion:

<div class="ipython">

python

values = {'ids': \['a', 'b'\], 'ids2': \['a', 'c'\], 'vals': \[1, 3\]}

row\_mask = df.isin(values).all(axis=1)

df\[row\_mask\]

</div>

## The <span class="title-ref">\~pandas.DataFrame.where</span> Method and Masking

Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the `where` method in `Series` and `DataFrame`.

To return only the selected rows:

<div class="ipython">

python

s\[s \> 0\]

</div>

To return a Series of the same shape as the original:

<div class="ipython">

python

s.where(s \> 0)

</div>

Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. `where` is used under the hood as the implementation. The code below is equivalent to `df.where(df < 0)`.

<div class="ipython">

python

dates = pd.date\_range('1/1/2000', periods=8) df = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=\['A', 'B', 'C', 'D'\]) df\[df \< 0\]

</div>

In addition, `where` takes an optional `other` argument for replacement of values where the condition is False, in the returned copy.

<div class="ipython">

python

df.where(df \< 0, -df)

</div>

You may wish to set values based on some boolean criteria. This can be done intuitively like so:

<div class="ipython">

python

s2 = s.copy() s2\[s2 \< 0\] = 0 s2

df2 = df.copy() df2\[df2 \< 0\] = 0 df2

</div>

`where` returns a modified copy of the data.

\> **Note** \> The signature for <span class="title-ref">DataFrame.where</span> differs from <span class="title-ref">numpy.where</span>. Roughly `df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.

> 
> 
> <div class="ipython">
> 
> python
> 
> df.where(df \< 0, -df) == np.where(df \< 0, df, -df)
> 
> </div>

**Alignment**

Furthermore, `where` aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via `.loc` (but on the contents rather than the axis labels).

<div class="ipython">

python

df2 = df.copy() df2\[df2\[1:4\] \> 0\] = 3 df2

</div>

Where can also accept `axis` and `level` parameters to align the input when performing the `where`.

<div class="ipython">

python

df2 = df.copy() df2.where(df2 \> 0, df2\['A'\], axis='index')

</div>

This is equivalent to (but faster than) the following.

<div class="ipython">

python

df2 = df.copy() df.apply(lambda x, y: x.where(x \> 0, y), y=df\['A'\])

</div>

`where` can accept a callable as condition and `other` arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and `other` argument.

<div class="ipython">

python

  - df3 = pd.DataFrame({'A': \[1, 2, 3\],  
    'B': \[4, 5, 6\], 'C': \[7, 8, 9\]})

df3.where(lambda x: x \> 4, lambda x: x + 10)

</div>

### Mask

<span class="title-ref">\~pandas.DataFrame.mask</span> is the inverse boolean operation of `where`.

<div class="ipython">

python

s.mask(s \>= 0) df.mask(df \>= 0)

</div>

## Setting with enlargement conditionally using <span class="title-ref">numpy</span>

An alternative to <span class="title-ref">\~pandas.DataFrame.where</span> is to use <span class="title-ref">numpy.where</span>. Combined with setting a new column, you can use it to enlarge a DataFrame where the values are determined conditionally.

Consider you have two choices to choose from in the following DataFrame. And you want to set a new column color to 'green' when the second column has 'Z'. You can do the following:

<div class="ipython">

python

df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')}) df\['color'\] = np.where(df\['col2'\] == 'Z', 'green', 'red') df

</div>

If you have multiple conditions, you can use <span class="title-ref">numpy.select</span> to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following.

<div class="ipython">

python

  - conditions = \[  
    (df\['col2'\] == 'Z') & (df\['col1'\] == 'A'), (df\['col2'\] == 'Z') & (df\['col1'\] == 'B'), (df\['col1'\] == 'B')

\] choices = \['yellow', 'blue', 'purple'\] df\['color'\] = np.select(conditions, choices, default='black') df

</div>

## The <span class="title-ref">\~pandas.DataFrame.query</span> Method

<span class="title-ref">\~pandas.DataFrame</span> objects have a <span class="title-ref">\~pandas.DataFrame.query</span> method that allows selection using an expression.

You can get the value of the frame where column `b` has values between the values of columns `a` and `c`. For example:

<div class="ipython">

python

n = 10 df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc')) df

\# pure python df\[(df\['a'\] \< df\['b'\]) & (df\['b'\] \< df\['c'\])\]

\# query df.query('(a \< b) & (b \< c)')

</div>

Do the same thing but fall back on a named index if there is no column with the name `a`.

<div class="ipython">

python

df = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc')) df.index.name = 'a' df df.query('a \< b and b \< c')

</div>

If instead you don't want to or cannot name your index, you can use the name `index` in your query expression:

<div class="ipython">

python

df = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc')) df df.query('index \< b \< c')

</div>

\> **Note** \> If the name of your index overlaps with a column name, the column name is given precedence. For example,

> 
> 
> <div class="ipython">
> 
> python
> 
> df = pd.DataFrame({'a': np.random.randint(5, size=5)}) df.index.name = 'a' df.query('a \> 2') \# uses the column 'a', not the index
> 
> </div>
> 
> You can still use the index in a query expression by using the special identifier 'index':
> 
> <div class="ipython">
> 
> python
> 
> df.query('index \> 2')
> 
> </div>
> 
> If for some reason you have a column named `index`, then you can refer to the index as `ilevel_0` as well, but at this point you should consider renaming your columns to something less ambiguous.

### <span class="title-ref">\~pandas.MultiIndex</span> <span class="title-ref">\~pandas.DataFrame.query</span> Syntax

You can also use the levels of a `DataFrame` with a <span class="title-ref">\~pandas.MultiIndex</span> as if they were columns in the frame:

<div class="ipython">

python

n = 10 colors = np.random.choice(\['red', 'green'\], size=n) foods = np.random.choice(\['eggs', 'ham'\], size=n) colors foods

index = pd.MultiIndex.from\_arrays(\[colors, foods\], names=\['color', 'food'\]) df = pd.DataFrame(np.random.randn(n, 2), index=index) df df.query('color == "red"')

</div>

If the levels of the `MultiIndex` are unnamed, you can refer to them using special names:

<div class="ipython">

python

df.index.names = \[None, None\] df df.query('ilevel\_0 == "red"')

</div>

The convention is `ilevel_0`, which means "index level 0" for the 0th level of the `index`.

### <span class="title-ref">\~pandas.DataFrame.query</span> Use Cases

A use case for <span class="title-ref">\~pandas.DataFrame.query</span> is when you have a collection of <span class="title-ref">\~pandas.DataFrame</span> objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames *without* having to specify which frame you're interested in querying

<div class="ipython">

python

df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc')) df df2 = pd.DataFrame(np.random.rand(n + 2, 3), columns=df.columns) df2 expr = '0.0 \<= a \<= c \<= 0.5' map(lambda frame: frame.query(expr), \[df, df2\])

</div>

### <span class="title-ref">\~pandas.DataFrame.query</span> Python versus pandas Syntax Comparison

Full numpy-like syntax:

<div class="ipython">

python

df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc')) df df.query('(a \< b) & (b \< c)') df\[(df\['a'\] \< df\['b'\]) & (df\['b'\] \< df\['c'\])\]

</div>

Slightly nicer by removing the parentheses (comparison operators bind tighter than `&` and `|`):

<div class="ipython">

python

df.query('a \< b & b \< c')

</div>

Use English instead of symbols:

<div class="ipython">

python

df.query('a \< b and b \< c')

</div>

Pretty close to how you might write it on paper:

<div class="ipython">

python

df.query('a \< b \< c')

</div>

### The `in` and `not in` operators

<span class="title-ref">\~pandas.DataFrame.query</span> also supports special use of Python's `in` and `not in` comparison operators, providing a succinct syntax for calling the `isin` method of a `Series` or `DataFrame`.

<div class="ipython">

python

\# get all rows where columns "a" and "b" have overlapping values df = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'), 'c': np.random.randint(5, size=12), 'd': np.random.randint(9, size=12)}) df df.query('a in b')

\# How you'd do it in pure Python df\[df\['a'\].isin(df\['b'\])\]

df.query('a not in b')

\# pure Python df\[\~df\['a'\].isin(df\['b'\])\]

</div>

You can combine this with other expressions for very succinct queries:

<div class="ipython">

python

\# rows where cols a and b have overlapping values \# and col c's values are less than col d's df.query('a in b and c \< d')

\# pure Python df\[df\['b'\].isin(df\['a'\]) & (df\['c'\] \< df\['d'\])\]

</div>

\> **Note** \> Note that `in` and `not in` are evaluated in Python, since `numexpr` has no equivalent of this operation. However, **only the** `in`/`not in` **expression itself** is evaluated in vanilla Python. For example, in the expression

> `` `python    df.query('a in b + c + d') ``(b + c + d)`is evaluated by`numexpr`and *then* the`in`operation is evaluated in plain Python. In general, any operations that can be evaluated using`numexpr\`\` will be.

Special use of the `==` operator with `list` objects `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Comparing a ``list`of values to a column using`==`/`\!=`works similarly to`in`/`not in`.  .. ipython:: python     df.query('b == ["a", "b", "c"]')     # pure Python    df[df['b'].isin(["a", "b", "c"])]     df.query('c == [1, 2]')     df.query('c != [1, 2]')     # using in/not in    df.query('[1, 2] in c')     df.query('[1, 2] not in c')     # pure Python    df[df['c'].isin([1, 2])]   Boolean operators ~~~~~~~~~~~~~~~~~  You can negate boolean expressions with the word`not`or the`\~``operator.  .. ipython:: python     df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))    df['bools'] = np.random.rand(len(df)) > 0.5    df.query('~bools')    df.query('not bools')    df.query('not bools') == df[~df['bools']]  Of course, expressions can be arbitrarily complex too:  .. ipython:: python     # short query syntax    shorter = df.query('a < b < c and (not bools) or bools > 2')     # equivalent in pure Python    longer = df[(df['a'] < df['b'])                & (df['b'] < df['c'])                & (~df['bools'])                | (df['bools'] > 2)]     shorter    longer     shorter == longer   Performance of `~pandas.DataFrame.query` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``DataFrame.query()`using`numexpr`is slightly faster than Python for large frames.  ..     The eval-perf.png figure below was generated with /doc/scripts/eval_performance.py  .. image:: ../_static/query-perf.png    You will only see the performance benefits of using the`numexpr`engine with`DataFrame.query()`if your frame has more than approximately 100,000 rows.    This plot was created using a`DataFrame`with 3 columns each containing floating point values generated using`numpy.random.randn()`.  .. ipython:: python     df = pd.DataFrame(np.random.randn(8, 4),                      index=dates, columns=['A', 'B', 'C', 'D'])    df2 = df.copy()   Duplicate data --------------  .. _indexing.duplicate:  If you want to identify and remove duplicate rows in a DataFrame,  there are two methods that will help:`duplicated`and`drop\_duplicates`. Each takes as an argument the columns to use to identify duplicated rows.  *`duplicated`returns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated. *`drop\_duplicates`removes duplicate rows.  By default, the first observed row of a duplicate set is considered unique, but each method has a`keep`parameter to specify targets to be kept.  *`keep='first'`(default): mark / drop duplicates except for the first occurrence. *`keep='last'`: mark / drop duplicates except for the last occurrence. *`keep=False`: mark  / drop all duplicates.  .. ipython:: python     df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],                        'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],                        'c': np.random.randn(7)})    df2    df2.duplicated('a')    df2.duplicated('a', keep='last')    df2.duplicated('a', keep=False)    df2.drop_duplicates('a')    df2.drop_duplicates('a', keep='last')    df2.drop_duplicates('a', keep=False)  Also, you can pass a list of columns to identify duplications.  .. ipython:: python     df2.duplicated(['a', 'b'])    df2.drop_duplicates(['a', 'b'])  To drop duplicates by index value, use`Index.duplicated`then perform slicing. The same set of options are available for the`keep``parameter.  .. ipython:: python     df3 = pd.DataFrame({'a': np.arange(6),                        'b': np.random.randn(6)},                       index=['a', 'a', 'b', 'c', 'b', 'a'])    df3    df3.index.duplicated()    df3[~df3.index.duplicated()]    df3[~df3.index.duplicated(keep='last')]    df3[~df3.index.duplicated(keep=False)]  .. _indexing.dictionarylike:  Dictionary-like `~pandas.DataFrame.get` method ----------------------------------------------------  Each of Series or DataFrame have a``get`method which can return a default value.  .. ipython:: python     s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])    s.get('a')  # equivalent to s['a']    s.get('x', default=-1)  .. _indexing.lookup:  Looking up values by index/column labels ----------------------------------------  Sometimes you want to extract a set of values given a sequence of row labels and column labels, this can be achieved by`pandas.factorize`and NumPy indexing. For instance:  .. ipython:: python      df = pd.DataFrame({'col': ["A", "A", "B", "B"],                        'A': [80, 23, np.nan, 22],                        'B': [80, 55, 76, 67]})     df     idx, cols = pd.factorize(df['col'])     df.reindex(cols, axis=1).to_numpy()[np.arange(len(df)), idx]  Formerly this could be achieved with the dedicated`DataFrame.lookup``method which was deprecated in version 1.2.0 and removed in version 2.0.0.  .. _indexing.class:  Index objects -------------  The pandas `~pandas.Index` class and its subclasses can be viewed as implementing an *ordered multiset*. Duplicates are allowed.  `~pandas.Index` also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an `~pandas.Index` directly is to pass a``list``or other sequence to `~pandas.Index`:  .. ipython:: python     index = pd.Index(['e', 'd', 'a', 'b'])    index    'd' in index  or using numbers:  .. ipython:: python     index = pd.Index([1, 5, 12])    index    5 in index  If no dtype is given,``Index``tries to infer the dtype from the data. It is also possible to give an explicit dtype when instantiating an `Index`:  .. ipython:: python     index = pd.Index(['e', 'd', 'a', 'b'], dtype="string")    index    index = pd.Index([1, 5, 12], dtype="int8")    index    index = pd.Index([1, 5, 12], dtype="float32")    index  You can also pass a``name`to be stored in the index:  .. ipython:: python     index = pd.Index(['e', 'd', 'a', 'b'], name='something')    index.name  The name, if set, will be shown in the console display:  .. ipython:: python     index = pd.Index(list(range(5)), name='rows')    columns = pd.Index(['A', 'B', 'C'], name='cols')    df = pd.DataFrame(np.random.randn(5, 3), index=index, columns=columns)    df    df['A']  .. _indexing.set_metadata:  Setting metadata ~~~~~~~~~~~~~~~~  Indexes are "mostly immutable", but it is possible to set and change their`name`attribute. You can use the`rename`,`set\_names`to set these attributes directly, and they default to returning a copy.  See [Advanced Indexing <advanced>](#advanced-indexing-<advanced>) for usage of MultiIndexes.  .. ipython:: python    ind = pd.Index([1, 2, 3])   ind.rename("apple")   ind   ind = ind.set_names(["apple"])   ind.name = "bob"   ind`set\_names`,`set\_levels`, and`set\_codes`also take an optional`level`argument  .. ipython:: python    index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])   index   index.levels[1]   index.set_levels(["a", "b"], level=1)  .. _indexing.set_ops:  Set operations on Index objects ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The two main operations are`union`and`intersection`. Difference is provided via the`.difference()`method.  .. ipython:: python     a = pd.Index(['c', 'b', 'a'])    b = pd.Index(['c', 'e', 'd'])    a.difference(b)  Also available is the`symmetric\_difference`operation, which returns elements that appear in either`idx1`or`idx2`, but not in both. This is equivalent to the Index created by`idx1.difference(idx2).union(idx2.difference(idx1))``, with duplicates dropped.  .. ipython:: python     idx1 = pd.Index([1, 2, 3, 4])    idx2 = pd.Index([2, 3, 4, 5])    idx1.symmetric_difference(idx2)  > **Note** >     The resulting index from a set operation will be sorted in ascending order.  When performing `Index.union` between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float  .. ipython:: python     idx1 = pd.Index([0, 1, 2])    idx2 = pd.Index([0.5, 1.5])    idx1.union(idx2)  .. _indexing.missing:  Missing values ~~~~~~~~~~~~~~  > **Important** >     Even though``Index`can hold missing values (`NaN`), it should be avoided    if you do not want any unexpected results. For example, some operations    exclude missing values implicitly.`Index.fillna``fills missing values with specified scalar value.  .. ipython:: python     idx1 = pd.Index([1, np.nan, 3, 4])    idx1    idx1.fillna(2)     idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'),                             pd.NaT,                             pd.Timestamp('2011-01-03')])    idx2    idx2.fillna(pd.Timestamp('2011-01-02'))  Set / reset index -----------------  Occasionally you will load or create a data set into a DataFrame and want to add an index after you've already done so. There are a couple of different ways.  .. _indexing.set_index:  Set an index ~~~~~~~~~~~~  DataFrame has a `~DataFrame.set_index` method which takes a column name (for a regular``Index`) or a list of column names (for a`MultiIndex`). To create a new, re-indexed DataFrame:  .. ipython:: python     data = pd.DataFrame({'a': ['bar', 'bar', 'foo', 'foo'],                         'b': ['one', 'two', 'one', 'two'],                         'c': ['z', 'y', 'x', 'w'],                         'd': [1., 2., 3, 4]})    data    indexed1 = data.set_index('c')    indexed1    indexed2 = data.set_index(['a', 'b'])    indexed2  The`append`keyword option allow you to keep the existing index and append the given columns to a MultiIndex:  .. ipython:: python     frame = data.set_index('c', drop=False)    frame = frame.set_index(['a', 'b'], append=True)    frame  Other options in`set\_index``allow you not drop the index columns.  .. ipython:: python     data.set_index('c', drop=False)  Reset the index ~~~~~~~~~~~~~~~  As a convenience, there is a new function on DataFrame called `~DataFrame.reset_index` which transfers the index values into the DataFrame's columns and sets a simple integer index. This is the inverse operation of `~DataFrame.set_index`.   .. ipython:: python     data    data.reset_index()  The output is more similar to a SQL table or a record array. The names for the columns derived from the index are the ones stored in the``names`attribute.  You can use the`level`keyword to remove only a portion of the index:  .. ipython:: python     frame    frame.reset_index(level=1)`reset\_index`takes an optional parameter`drop`which if true simply discards the index, instead of putting index values in the DataFrame's columns.  Adding an ad hoc index ~~~~~~~~~~~~~~~~~~~~~~  You can assign a custom index to the`index\`\` attribute:

<div class="ipython">

python

df\_idx = pd.DataFrame(range(4)) df\_idx.index = pd.Index(\[10, 20, 30, 40\], name="a") df\_idx

</div>

### Why does assignment fail when using chained indexing?

\[Copy-on-Write \<copy\_on\_write\>\](\#copy-on-write-\<copy\_on\_write\>) is the new default with pandas 3.0. This means that chained indexing will never work. See \[this section \<copy\_on\_write\_chained\_assignment\>\](\#this-section-\<copy\_on\_write\_chained\_assignment\>) for more context.

---

integer_na.md

---

<div class="currentmodule">

pandas

</div>

{{ header }}

# Nullable integer data type

\> **Note** \> IntegerArray is currently experimental. Its API or implementation may change without warning. Uses <span class="title-ref">pandas.NA</span> as the missing value.

In \[missing\_data\](\#missing\_data), we saw that pandas primarily uses `NaN` to represent missing data. Because `NaN` is a float, this forces an array of integers with any missing values to become floating point. In some cases, this may not matter much. But if your integer column is, say, an identifier, casting to float can be problematic. Some integers cannot even be represented as floating point numbers.

## Construction

pandas can represent integer data with possibly missing values using <span class="title-ref">arrays.IntegerArray</span>. This is an \[extension type \<extending.extension-types\>\](\#extension-type-\<extending.extension-types\>) implemented within pandas.

<div class="ipython">

python

arr = pd.array(\[1, 2, None\], dtype=pd.Int64Dtype()) arr

</div>

Or the string alias `"Int64"` (note the capital `"I"`) to differentiate from NumPy's `'int64'` dtype:

<div class="ipython">

python

pd.array(\[1, 2, np.nan\], dtype="Int64")

</div>

All NA-like values are replaced with <span class="title-ref">pandas.NA</span>.

<div class="ipython">

python

pd.array(\[1, 2, np.nan, None, pd.NA\], dtype="Int64")

</div>

This array can be stored in a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> like any NumPy array.

<div class="ipython">

python

pd.Series(arr)

</div>

You can also pass the list-like object to the <span class="title-ref">Series</span> constructor with the dtype.

\> **Warning** \> Currently <span class="title-ref">pandas.array</span> and <span class="title-ref">pandas.Series</span> use different rules for dtype inference. <span class="title-ref">pandas.array</span> will infer a nullable-integer dtype

> 
> 
> <div class="ipython">
> 
> python
> 
> pd.array(\[1, None\]) pd.array(\[1, 2\])
> 
> </div>
> 
> For backwards-compatibility, <span class="title-ref">Series</span> infers these as either integer or float dtype.
> 
> <div class="ipython">
> 
> python
> 
> pd.Series(\[1, None\]) pd.Series(\[1, 2\])
> 
> </div>
> 
> We recommend explicitly providing the dtype to avoid confusion.
> 
> <div class="ipython">
> 
> python
> 
> pd.array(\[1, None\], dtype="Int64") pd.Series(\[1, None\], dtype="Int64")
> 
> </div>
> 
> In the future, we may provide an option for <span class="title-ref">Series</span> to infer a nullable-integer dtype.

If you create a column of `NA` values (for example to fill them later) with `df['new_col'] = pd.NA`, the `dtype` would be set to `object` in the new column. The performance on this column will be worse than with the appropriate type. It's better to use `df['new_col'] = pd.Series(pd.NA, dtype="Int64")` (or another `dtype` that supports `NA`).

<div class="ipython">

python

df = pd.DataFrame() df\['objects'\] = pd.NA df.dtypes

</div>

## Operations

Operations involving an integer array will behave similar to NumPy arrays. Missing values will be propagated, and the data will be coerced to another dtype if needed.

<div class="ipython">

python

s = pd.Series(\[1, 2, None\], dtype="Int64")

\# arithmetic s + 1

\# comparison s == 1

\# slicing operation s.iloc\[1:3\]

\# operate with other dtypes s + s.iloc\[1:3\].astype("Int8")

\# coerce when needed s + 0.01

</div>

These dtypes can operate as part of a `DataFrame`.

<div class="ipython">

python

df = pd.DataFrame({"A": s, "B": \[1, 1, 3\], "C": list("aab")}) df df.dtypes

</div>

These dtypes can be merged, reshaped & casted.

<div class="ipython">

python

pd.concat(\[df\[\["A"\]\], df\[\["B", "C"\]\]\], axis=1).dtypes df\["A"\].astype(float)

</div>

Reduction and groupby operations such as <span class="title-ref">\~DataFrame.sum</span> work as well.

<div class="ipython">

python

df.sum(numeric\_only=True) df.sum() df.groupby("B").A.sum()

</div>

## Scalar NA value

<span class="title-ref">arrays.IntegerArray</span> uses <span class="title-ref">pandas.NA</span> as its scalar missing value. Slicing a single element that's missing will return <span class="title-ref">pandas.NA</span>

<div class="ipython">

python

a = pd.array(\[1, None\], dtype="Int64") a\[1\]

</div>

---

io.md

---

<div id="io">

<div class="currentmodule">

pandas

</div>

</div>

# IO tools (text, CSV, HDF5, ...)

The pandas I/O API is a set of top level `reader` functions accessed like <span class="title-ref">pandas.read\_csv</span> that generally return a pandas object. The corresponding `writer` functions are object methods that are accessed like <span class="title-ref">DataFrame.to\_csv</span>. Below is a table containing available `readers` and `writers`.

| Format Type | Data Description                                                      | Reader                                                                   | Writer                                                             |
| ----------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------ |
| text        | [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)           | \[read\_csv\<io.read\_csv\_table\>\](\#read\_csv\<io.read\_csv\_table\>) | \[to\_csv\<io.store\_in\_csv\>\](\#to\_csv\<io.store\_in\_csv\>)   |
| text        | Fixed-Width Text File                                                 | \[read\_fwf\<io.fwf\_reader\>\](\#read\_fwf\<io.fwf\_reader\>)           | NA                                                                 |
| text        | [JSON](https://www.json.org/)                                         | \[read\_json\<io.json\_reader\>\](\#read\_json\<io.json\_reader\>)       | \[to\_json\<io.json\_writer\>\](\#to\_json\<io.json\_writer\>)     |
| text        | [HTML](https://en.wikipedia.org/wiki/HTML)                            | \[read\_html\<io.read\_html\>\](\#read\_html\<io.read\_html\>)           | \[to\_html\<io.html\>\](\#to\_html\<io.html\>)                     |
| text        | [LaTeX](https://en.wikipedia.org/wiki/LaTeX)                          | \[Styler.to\_latex\<io.latex\>\](\#styler.to\_latex\<io.latex\>)         | NA                                                                 |
| text        | [XML](https://www.w3.org/standards/xml/core)                          | \[read\_xml\<io.read\_xml\>\](\#read\_xml\<io.read\_xml\>)               | \[to\_xml\<io.xml\>\](\#to\_xml\<io.xml\>)                         |
| text        | Local clipboard                                                       | \[read\_clipboard\<io.clipboard\>\](\#read\_clipboard\<io.clipboard\>)   | \[to\_clipboard\<io.clipboard\>\](\#to\_clipboard\<io.clipboard\>) |
| binary      | [MS Excel](https://en.wikipedia.org/wiki/Microsoft_Excel)             | \[read\_excel\<io.excel\_reader\>\](\#read\_excel\<io.excel\_reader\>)   | \[to\_excel\<io.excel\_writer\>\](\#to\_excel\<io.excel\_writer\>) |
| binary      | [OpenDocument](http://opendocumentformat.org)                         | \[read\_excel\<io.ods\>\](\#read\_excel\<io.ods\>)                       | NA                                                                 |
| binary      | [HDF5 Format](https://support.hdfgroup.org/HDF5/whatishdf5.html)      | \[read\_hdf\<io.hdf5\>\](\#read\_hdf\<io.hdf5\>)                         | \[to\_hdf\<io.hdf5\>\](\#to\_hdf\<io.hdf5\>)                       |
| binary      | [Feather Format](https://github.com/wesm/feather)                     | \[read\_feather\<io.feather\>\](\#read\_feather\<io.feather\>)           | \[to\_feather\<io.feather\>\](\#to\_feather\<io.feather\>)         |
| binary      | [Parquet Format](https://parquet.apache.org/)                         | \[read\_parquet\<io.parquet\>\](\#read\_parquet\<io.parquet\>)           | \[to\_parquet\<io.parquet\>\](\#to\_parquet\<io.parquet\>)         |
| binary      | [ORC Format](https://orc.apache.org/)                                 | \[read\_orc\<io.orc\>\](\#read\_orc\<io.orc\>)                           | \[to\_orc\<io.orc\>\](\#to\_orc\<io.orc\>)                         |
| binary      | [Stata](https://en.wikipedia.org/wiki/Stata)                          | \[read\_stata\<io.stata\_reader\>\](\#read\_stata\<io.stata\_reader\>)   | \[to\_stata\<io.stata\_writer\>\](\#to\_stata\<io.stata\_writer\>) |
| binary      | [SAS](https://en.wikipedia.org/wiki/SAS_\(software\))                 | \[read\_sas\<io.sas\_reader\>\](\#read\_sas\<io.sas\_reader\>)           | NA                                                                 |
| binary      | [SPSS](https://en.wikipedia.org/wiki/SPSS)                            | \[read\_spss\<io.spss\_reader\>\](\#read\_spss\<io.spss\_reader\>)       | NA                                                                 |
| binary      | [Python Pickle Format](https://docs.python.org/3/library/pickle.html) | \[read\_pickle\<io.pickle\>\](\#read\_pickle\<io.pickle\>)               | \[to\_pickle\<io.pickle\>\](\#to\_pickle\<io.pickle\>)             |
| SQL         | [SQL](https://en.wikipedia.org/wiki/SQL)                              | \[read\_sql\<io.sql\>\](\#read\_sql\<io.sql\>)                           | \[to\_sql\<io.sql\>\](\#to\_sql\<io.sql\>)                         |

\[Here \<io.perf\>\](\#here-\<io.perf\>) is an informal performance comparison for some of these IO methods.

<div class="note">

<div class="title">

Note

</div>

For examples that use the `StringIO` class, make sure you import it with `from io import StringIO` for Python 3.

</div>

## CSV & text files

The workhorse function for reading text files (a.k.a. flat files) is <span class="title-ref">read\_csv</span>. See the \[cookbook\<cookbook.csv\>\](\#cookbook\<cookbook.csv\>) for some advanced strategies.

### Parsing options

<span class="title-ref">read\_csv</span> accepts the following common arguments:

#### Basic

  - filepath\_or\_buffer : various  
    Either a path to a file (a <span class="title-ref">python:str</span>, <span class="title-ref">python:pathlib.Path</span>) URL (including http, ftp, and S3 locations), or any object with a `read()` method (such as an open file or <span class="title-ref">\~python:io.StringIO</span>).

  - sep : str, defaults to `','` for <span class="title-ref">read\_csv</span>, `\t` for <span class="title-ref">read\_table</span>  
    Delimiter to use. If sep is `None`, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, <span class="title-ref">python:csv.Sniffer</span>. In addition, separators longer than 1 character and different from `'\s+'` will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: `'\\r\\t'`.

  - delimiter : str, default `None`  
    Alternative argument name for sep.

#### Column and index locations and names

  - header : int or list of ints, default `'infer'`  
    Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to `header=0` and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to `header=None`. Explicitly pass `header=0` to be able to replace existing names.
    
    The header can be a list of ints that specify row locations for a MultiIndex on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes the first line of data rather than the first line of the file.

  - names : array-like, default `None`  
    List of column names to use. If file contains no header row, then you should explicitly pass `header=None`. Duplicates in this list are not allowed.

  - index\_col : int, str, sequence of int / str, or False, optional, default `None`  
    Column(s) to use as the row labels of the `DataFrame`, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    `index_col=False` can be used to force pandas to *not* use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.
    
    </div>
    
    The default value of `None` instructs pandas to guess. If the number of fields in the column header row is equal to the number of fields in the body of the data file, then a default index is used. If it is larger, then the first columns are used as index so that the remaining number of fields in the body are equal to the number of fields in the header.
    
    The first row after the header is used to determine the number of columns, which will go into the index. If the subsequent rows contain less columns than the first row, they are filled with `NaN`.
    
    This can be avoided through `usecols`. This ensures that the columns are taken as is and the trailing data are ignored.

  - usecols : list-like or callable, default `None`  
    Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in `names` or inferred from the document header row(s). If `names` are given, the document header row(s) are not taken into account. For example, a valid list-like `usecols` parameter would be `[0, 1, 2]` or `['foo', 'bar', 'baz']`.
    
    Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
    
    If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True:
    
    <div class="ipython">
    
    python
    
    import pandas as pd from io import StringIO
    
    data = "col1,col2,col3na,b,1na,b,2nc,d,3" pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data), usecols=lambda x: x.upper() in \["COL1", "COL3"\])
    
    </div>
    
    Using this parameter results in much faster parsing time and lower memory usage when using the c engine. The Python engine loads the data first before deciding which columns to drop.

#### General parsing configuration

  - dtype : Type name or dict of column -\> type, default `None`  
    Data type for data or columns. E.g. `{'a': np.float64, 'b': np.int32, 'c': 'Int64'}` Use `str` or `object` together with suitable `na_values` settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.
    
    <div class="versionadded">
    
    1.5.0
    
    Support for defaultdict was added. Specify a defaultdict as input where the default determines the dtype of the columns which are not explicitly listed.
    
    </div>

  - dtype\_backend : {"numpy\_nullable", "pyarrow"}, defaults to NumPy backed DataFrames  
    Which dtype\_backend to use, e.g. whether a DataFrame should have NumPy arrays, nullable dtypes are used for all dtypes that have a nullable implementation when "numpy\_nullable" is set, pyarrow is used for all dtypes if "pyarrow" is set.
    
    The dtype\_backends are still experimental.
    
    <div class="versionadded">
    
    2.0
    
    </div>

  - engine : {`'c'`, `'python'`, `'pyarrow'`}  
    Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.
    
    <div class="versionadded">
    
    1.4.0
    
    The "pyarrow" engine was added as an *experimental* engine, and some features are unsupported, or may not work correctly, with this engine.
    
    </div>

  - converters : dict, default `None`  
    Dict of functions for converting values in certain columns. Keys can either be integers or column labels.

  - true\_values : list, default `None`  
    Values to consider as `True`.

  - false\_values : list, default `None`  
    Values to consider as `False`.

  - skipinitialspace : boolean, default `False`  
    Skip spaces after delimiter.

  - skiprows : list-like or integer, default `None`  
    Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.
    
    If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise:
    
    <div class="ipython">
    
    python
    
    data = "col1,col2,col3na,b,1na,b,2nc,d,3" pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data), skiprows=lambda x: x % 2 \!= 0)
    
    </div>

  - skipfooter : int, default `0`  
    Number of lines at bottom of file to skip (unsupported with engine='c').

  - nrows : int, default `None`  
    Number of rows of file to read. Useful for reading pieces of large files.

  - low\_memory : boolean, default `True`  
    Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set `False`, or specify the type with the `dtype` parameter. Note that the entire file is read into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter to return the data in chunks. (Only valid with C parser)

  - memory\_map : boolean, default False  
    If a filepath is provided for `filepath_or_buffer`, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.

#### NA and missing data handling

  - na\_values : scalar, str, list-like, or dict, default `None`  
    Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See \[na values const \<io.navaluesconst\>\](\#na-values-const-\<io.navaluesconst\>) below for a list of the values interpreted as NaN by default.

  - keep\_default\_na : boolean, default `True`  
    Whether or not to include the default NaN values when parsing the data. Depending on whether `na_values` is passed in, the behavior is as follows:
    
      - If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is appended to the default NaN values used for parsing.
      - If `keep_default_na` is `True`, and `na_values` are not specified, only the default NaN values are used for parsing.
      - If `keep_default_na` is `False`, and `na_values` are specified, only the NaN values specified `na_values` are used for parsing.
      - If `keep_default_na` is `False`, and `na_values` are not specified, no strings will be parsed as NaN.
    
    Note that if `na_filter` is passed in as `False`, the `keep_default_na` and `na_values` parameters will be ignored.

  - na\_filter : boolean, default `True`  
    Detect missing value markers (empty strings and the value of na\_values). In data without any NAs, passing `na_filter=False` can improve the performance of reading a large file.

  - verbose : boolean, default `False`  
    Indicate number of NA values placed in non-numeric columns.

  - skip\_blank\_lines : boolean, default `True`  
    If `True`, skip over blank lines rather than interpreting as NaN values.

#### Datetime handling

  - parse\_dates : boolean or list of ints or names or list of lists or dict, default `False`.
    
      - If `True` -\> try parsing the index.
      - If `[1, 2, 3]` -\> try parsing columns 1, 2, 3 each as a separate date column.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    A fast-path exists for iso8601-formatted dates.
    
    </div>

  - date\_format : str or dict of column -\> format, default `None`  
    If used in conjunction with `parse_dates`, will parse dates according to this format. For anything more complex, please read in as `object` and then apply <span class="title-ref">to\_datetime</span> as-needed.
    
    <div class="versionadded">
    
    2.0.0
    
    </div>

  - dayfirst : boolean, default `False`  
    DD/MM format dates, international and European format.

  - cache\_dates : boolean, default True  
    If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.

#### Iteration

  - iterator : boolean, default `False`  
    Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.

  - chunksize : int, default `None`  
    Return `TextFileReader` object for iteration. See \[iterating and chunking \<io.chunking\>\](\#iterating-and-chunking

\--\<io.chunking\>) below.

#### Quoting, compression, and file format

  - compression : {`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`, `dict`}, default `'infer'`  
    For on-the-fly decompression of on-disk data. If 'infer', then use gzip, bz2, zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in '.gz', '.bz2', '.zip', '.xz', '.zst', respectively, and no decompression otherwise. If using 'zip', the ZIP file must contain only one data file to be read in. Set to `None` for no decompression. Can also be a dict with key `'method'` set to one of {`'zip'`, `'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: `compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.
    
    <div class="versionchanged">
    
    1.2.0 Previous versions forwarded dict entries for 'gzip' to `gzip.open`.
    
    </div>

  - thousands : str, default `None`  
    Thousands separator.

  - decimal : str, default `'.'`  
    Character to recognize as decimal point. E.g. use `','` for European data.

  - float\_precision : string, default None  
    Specifies which converter the C engine should use for floating-point values. The options are `None` for the ordinary converter, `high` for the high-precision converter, and `round_trip` for the round-trip converter.

  - lineterminator : str (length 1), default `None`  
    Character to break file into lines. Only valid with C parser.

  - quotechar : str (length 1)  
    The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.

  - quoting : int or `csv.QUOTE_*` instance, default `0`  
    Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL` (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).

  - doublequote : boolean, default `True`  
    When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether or not to interpret two consecutive `quotechar` elements **inside** a field as a single `quotechar` element.

  - escapechar : str (length 1), default `None`  
    One-character string used to escape delimiter when quoting is `QUOTE_NONE`.

  - comment : str, default `None`  
    Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as `skip_blank_lines=True`), fully commented lines are ignored by the parameter `header` but not by `skiprows`. For example, if `comment='#'`, parsing '\#empty\\na,b,c\\n1,2,3' with `header=0` will result in 'a,b,c' being treated as the header.

  - encoding : str, default `None`  
    Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).

  - dialect : str or <span class="title-ref">python:csv.Dialect</span> instance, default `None`  
    If provided, this parameter will override values (default or not) for the following parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to override values, a ParserWarning will be issued. See <span class="title-ref">python:csv.Dialect</span> documentation for more details.

#### Error handling

  - on\_bad\_lines : {{'error', 'warn', 'skip'}}, default 'error'  
    Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :
    
      - 'error', raise an ParserError when a bad line is encountered.
      - 'warn', print a warning when a bad line is encountered and skip that line.
      - 'skip', skip bad lines without raising or warning when they are encountered.
    
    <div class="versionadded">
    
    1.3.0
    
    </div>

### Specifying column data types

You can indicate the data type for the whole `DataFrame` or individual columns:

<div class="ipython">

python

import numpy as np

data = "a,b,c,dn1,2,3,4n5,6,7,8n9,10,11" print(data)

df = pd.read\_csv(StringIO(data), dtype=object) df df\["a"\]\[0\] df = pd.read\_csv(StringIO(data), dtype={"b": object, "c": np.float64, "d": "Int64"}) df.dtypes

</div>

Fortunately, pandas offers more than one way to ensure that your column(s) contain only one `dtype`. If you're unfamiliar with these concepts, you can see \[here\<basics.dtypes\>\](\#here\<basics.dtypes\>) to learn more about dtypes, and \[here\<basics.object\_conversion\>\](\#here\<basics.object\_conversion\>) to learn more about `object` conversion in pandas.

For instance, you can use the `converters` argument of \`\~pandas.read\_csv\`:

<div class="ipython">

python

data = "col\_1n1n2n'A'n4.22" df = pd.read\_csv(StringIO(data), converters={"col\_1": str}) df df\["col\_1"\].apply(type).value\_counts()

</div>

Or you can use the <span class="title-ref">\~pandas.to\_numeric</span> function to coerce the dtypes after reading in the data,

<div class="ipython">

python

df2 = pd.read\_csv(StringIO(data)) df2\["col\_1"\] = pd.to\_numeric(df2\["col\_1"\], errors="coerce") df2 df2\["col\_1"\].apply(type).value\_counts()

</div>

which will convert all valid parsing to floats, leaving the invalid parsing as `NaN`.

Ultimately, how you deal with reading in columns containing mixed dtypes depends on your specific needs. In the case above, if you wanted to `NaN` out the data anomalies, then <span class="title-ref">\~pandas.to\_numeric</span> is probably your best option. However, if you wanted for all the data to be coerced, no matter the type, then using the `converters` argument of <span class="title-ref">\~pandas.read\_csv</span> would certainly be worth trying.

<div class="note">

<div class="title">

Note

</div>

In some cases, reading in abnormal data with columns containing mixed dtypes will result in an inconsistent dataset. If you rely on pandas to infer the dtypes of your columns, the parsing engine will go and infer the dtypes for different chunks of the data, rather than the whole dataset at once. Consequently, you can end up with column(s) with mixed dtypes. For example,

<div class="ipython" data-okwarning="">

python

col\_1 = list(range(500000)) + \["a", "b"\] + list(range(500000)) df = pd.DataFrame({"col\_1": col\_1}) df.to\_csv("foo.csv") mixed\_df = pd.read\_csv("foo.csv") mixed\_df\["col\_1"\].apply(type).value\_counts() mixed\_df\["col\_1"\].dtype

</div>

will result with `mixed_df` containing an `int` dtype for certain chunks of the column, and `str` for others due to the mixed dtypes from the data that was read in. It is important to note that the overall column will be marked with a `dtype` of `object`, which is used for columns with mixed dtypes.

</div>

<div class="ipython" data-suppress="">

python

import os

os.remove("foo.csv")

</div>

Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for every column.

<div class="ipython">

python

data = """a,b,c,d,e,f,g,h,i,j 1,2.5,True,a,,,,,12-31-2019, 3,4.5,False,b,6,7.5,True,a,12-31-2019, """

df = pd.read\_csv(StringIO(data), dtype\_backend="numpy\_nullable", parse\_dates=\["i"\]) df df.dtypes

</div>

### Specifying categorical dtype

`Categorical` columns can be parsed directly by specifying `dtype='category'` or `dtype=CategoricalDtype(categories, ordered)`.

<div class="ipython">

python

data = "col1,col2,col3na,b,1na,b,2nc,d,3"

pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data)).dtypes pd.read\_csv(StringIO(data), dtype="category").dtypes

</div>

Individual columns can be parsed as a `Categorical` using a dict specification:

<div class="ipython">

python

pd.read\_csv(StringIO(data), dtype={"col1": "category"}).dtypes

</div>

Specifying `dtype='category'` will result in an unordered `Categorical` whose `categories` are the unique values observed in the data. For more control on the categories and order, create a <span class="title-ref">\~pandas.api.types.CategoricalDtype</span> ahead of time, and pass that for that column's `dtype`.

<div class="ipython">

python

from pandas.api.types import CategoricalDtype

dtype = CategoricalDtype(\["d", "c", "b", "a"\], ordered=True) pd.read\_csv(StringIO(data), dtype={"col1": dtype}).dtypes

</div>

When using `dtype=CategoricalDtype`, "unexpected" values outside of `dtype.categories` are treated as missing values.

<div class="ipython">

python

dtype = CategoricalDtype(\["a", "b", "d"\]) \# No 'c' pd.read\_csv(StringIO(data), dtype={"col1": dtype}).col1

</div>

This matches the behavior of <span class="title-ref">Categorical.set\_categories</span>.

\> **Note** \> With `dtype='category'`, the resulting categories will always be parsed as strings (object dtype). If the categories are numeric they can be converted using the <span class="title-ref">to\_numeric</span> function, or as appropriate, another converter such as <span class="title-ref">to\_datetime</span>.

> When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric, all datetimes, etc.), the conversion is done automatically.
> 
> <div class="ipython">
> 
> python
> 
> df = pd.read\_csv(StringIO(data), dtype="category") df.dtypes df\["col3"\] new\_categories = pd.to\_numeric(df\["col3"\].cat.categories) df\["col3"\] = df\["col3"\].cat.rename\_categories(new\_categories) df\["col3"\]
> 
> </div>

### Naming and using columns

#### Handling column names

A file may or may not have a header row. pandas assumes the first row should be used as the column names:

<div class="ipython">

python

data = "a,b,cn1,2,3n4,5,6n7,8,9" print(data) pd.read\_csv(StringIO(data))

</div>

By specifying the `names` argument in conjunction with `header` you can indicate other names to use and whether or not to throw away the header row (if any):

<div class="ipython">

python

print(data) pd.read\_csv(StringIO(data), names=\["foo", "bar", "baz"\], header=0) pd.read\_csv(StringIO(data), names=\["foo", "bar", "baz"\], header=None)

</div>

If the header is in a row other than the first, pass the row number to `header`. This will skip the preceding rows:

<div class="ipython">

python

data = "skip this skip itna,b,cn1,2,3n4,5,6n7,8,9" pd.read\_csv(StringIO(data), header=1)

</div>

\> **Note** \> Default behavior is to infer the column names: if no names are passed the behavior is identical to `header=0` and column names are inferred from the first non-blank line of the file, if column names are passed explicitly then the behavior is identical to `header=None`.

### Duplicate names parsing

If the file or header contains duplicate names, pandas will by default distinguish between them so as to prevent overwriting data:

<div class="ipython">

python

data = "a,b,an0,1,2n3,4,5" pd.read\_csv(StringIO(data))

</div>

There is no more duplicate data because duplicate columns 'X', ..., 'X' become 'X', 'X.1', ..., 'X.N'.

#### Filtering columns (`usecols`)

The `usecols` argument allows you to select any subset of the columns in a file, either using the column names, position numbers or a callable:

<div class="ipython">

python

data = "a,b,c,dn1,2,3,foon4,5,6,barn7,8,9,baz" pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data), usecols=\["b", "d"\]) pd.read\_csv(StringIO(data), usecols=\[0, 2, 3\]) pd.read\_csv(StringIO(data), usecols=lambda x: x.upper() in \["A", "C"\])

</div>

The `usecols` argument can also be used to specify which columns not to use in the final result:

<div class="ipython">

python

pd.read\_csv(StringIO(data), usecols=lambda x: x not in \["a", "c"\])

</div>

In this case, the callable is specifying that we exclude the "a" and "c" columns from the output.

### Comments and empty lines

#### Ignoring line comments and empty lines

If the `comment` parameter is specified, then completely commented lines will be ignored. By default, completely blank lines will be ignored as well.

<div class="ipython">

python

data = "na,b,cn n\# commented linen1,2,3nn4,5,6" print(data) pd.read\_csv(StringIO(data), comment="\#")

</div>

If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:

<div class="ipython">

python

data = "a,b,cnn1,2,3nnn4,5,6" pd.read\_csv(StringIO(data), skip\_blank\_lines=False)

</div>

\> **Warning** \> The presence of ignored lines might create ambiguities involving line numbers; the parameter `header` uses row numbers (ignoring commented/empty lines), while `skiprows` uses line numbers (including commented/empty lines):

> 
> 
> <div class="ipython">
> 
> python
> 
> data = "\#commentna,b,cnA,B,Cn1,2,3" pd.read\_csv(StringIO(data), comment="\#", header=1) data = "A,B,Cn\#commentna,b,cn1,2,3" pd.read\_csv(StringIO(data), comment="\#", skiprows=2)
> 
> </div>
> 
> If both `header` and `skiprows` are specified, `header` will be relative to the end of `skiprows`. For example:

<div class="ipython">

python

  - data = (  
    "\# emptyn" "\# second empty linen" "\# third emptylinen" "X,Y,Zn" "1,2,3n" "A,B,Cn" "1,2.,4.n" "5.,NaN,10.0n"

) print(data) pd.read\_csv(StringIO(data), comment="\#", skiprows=4, header=1)

</div>

#### Comments

Sometimes comments or meta data may be included in a file:

<div class="ipython">

python

  - data = (  
    "ID,level,categoryn" "Patient1,123000,x \# really unpleasantn" "Patient2,23000,y \# wouldn't take his medicinen" "Patient3,1234018,z \# awesome"

) with open("tmp.csv", "w") as fh: fh.write(data)

print(open("tmp.csv").read())

</div>

By default, the parser includes the comments in the output:

<div class="ipython">

python

df = pd.read\_csv("tmp.csv") df

</div>

We can suppress the comments using the `comment` keyword:

<div class="ipython">

python

df = pd.read\_csv("tmp.csv", comment="\#") df

</div>

<div class="ipython" data-suppress="">

python

os.remove("tmp.csv")

</div>

### Dealing with Unicode data

The `encoding` argument should be used for encoded unicode data, which will result in byte strings being decoded to unicode in the result:

<div class="ipython">

python

from io import BytesIO

data = b"word,lengthn" b"Trxc3xa4umen,7n" b"Grxc3xbcxc3x9fe,5" data = data.decode("utf8").encode("latin-1") df = pd.read\_csv(BytesIO(data), encoding="latin-1") df df\["word"\]\[1\]

</div>

Some formats which encode all characters as multiple bytes, like UTF-16, won't parse correctly at all without specifying the encoding. [Full list of Python standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).

### Index columns and trailing delimiters

If a file has one more column of data than the number of column names, the first column will be used as the `DataFrame`'s row names:

<div class="ipython">

python

data = "a,b,cn4,apple,bat,5.7n8,orange,cow,10" pd.read\_csv(StringIO(data))

</div>

<div class="ipython">

python

data = "index,a,b,cn4,apple,bat,5.7n8,orange,cow,10" pd.read\_csv(StringIO(data), index\_col=0)

</div>

Ordinarily, you can achieve this behavior using the `index_col` option.

There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing the parser. To explicitly disable the index column inference and discard the last column, pass `index_col=False`:

<div class="ipython">

python

data = "a,b,cn4,apple,bat,n8,orange,cow," print(data) pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data), index\_col=False)

</div>

If a subset of data is being parsed using the `usecols` option, the `index_col` specification is based on that subset, not the original data.

<div class="ipython">

python

data = "a,b,cn4,apple,bat,n8,orange,cow," print(data) pd.read\_csv(StringIO(data), usecols=\["b", "c"\]) pd.read\_csv(StringIO(data), usecols=\["b", "c"\], index\_col=0)

</div>

### Date Handling

#### Specifying date columns

To better facilitate working with datetime data, <span class="title-ref">read\_csv</span> uses the keyword arguments `parse_dates` and `date_format` to allow users to specify a variety of columns and date/time formats to turn the input text data into `datetime` objects.

The simplest case is to just pass in `parse_dates=True`:

<div class="ipython">

python

  - with open("foo.csv", mode="w") as f:  
    f.write("date,A,B,Cn20090101,a,1,2n20090102,b,3,4n20090103,c,4,5")

\# Use a column as an index, and parse it as dates. df = pd.read\_csv("foo.csv", index\_col=0, parse\_dates=True) df

\# These are Python datetime objects df.index

</div>

It is often the case that we may want to store date and time data separately, or store various date fields separately. the `parse_dates` keyword can be used to specify columns to parse the dates and/or times.

<div class="note">

<div class="title">

Note

</div>

If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use <span class="title-ref">to\_datetime</span> after `pd.read_csv`.

</div>

<div class="note">

<div class="title">

Note

</div>

read\_csv has a fast\_path for parsing datetime strings in iso8601 format, e.g "2000-01-01T00:01:02+00:00" and similar variations. If you can arrange for your data to store datetimes in this format, load times will be significantly faster, \~20x has been observed.

</div>

#### Date parsing functions

Finally, the parser allows you to specify a custom `date_format`. Performance-wise, you should try these methods of parsing dates in order:

1.  If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or `date_format={column_name: "%d/%m/%Y"}`.
2.  If you different formats for different columns, or want to pass any extra options (such as `utc`) to `to_datetime`, then you should read in your data as `object` dtype, and then use `to_datetime`.

#### Parsing a CSV with mixed timezones

pandas cannot natively represent a column or index with mixed timezones. If your CSV file contains columns with a mixture of timezones, the default result will be an object-dtype column with strings, even with `parse_dates`. To parse the mixed-timezone values as a datetime column, read in as `object` dtype and then call <span class="title-ref">to\_datetime</span> with `utc=True`.

<div class="ipython">

python

content = """a 2000-01-01T00:00:00+05:00 2000-01-01T00:00:00+06:00""" df = pd.read\_csv(StringIO(content)) df\["a"\] = pd.to\_datetime(df\["a"\], utc=True) df\["a"\]

</div>

#### Inferring datetime format

Here are some examples of datetime strings that can be guessed (all representing December 30th, 2011 at 00:00:00):

  - "20111230"
  - "2011/12/30"
  - "20111230 00:00:00"
  - "12/30/2011 00:00:00"
  - "30/Dec/2011 00:00:00"
  - "30/December/2011 00:00:00"

Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`, it will guess "01/12/2011" to be December 1st. With `dayfirst=False` (default) it will guess "01/12/2011" to be January 12th.

If you try to parse a column of date strings, pandas will attempt to guess the format from the first non-NaN element, and will then parse the rest of the column with that format. If pandas fails to guess the format (for example if your first string is `'01 December US/Pacific 2000'`), then a warning will be raised and each row will be parsed individually by `dateutil.parser.parse`. The safest way to parse dates is to explicitly set `format=`.

<div class="ipython">

python

  - df = pd.read\_csv(  
    "foo.csv", index\_col=0, parse\_dates=True,

) df

</div>

In the case that you have mixed datetime formats within the same column, you can pass `format='mixed'`

<div class="ipython">

python

data = StringIO("daten12 Jan 2000n2000-01-13n") df = pd.read\_csv(data) df\['date'\] = pd.to\_datetime(df\['date'\], format='mixed') df

</div>

or, if your datetime formats are all ISO8601 (possibly not identically-formatted):

<div class="ipython">

python

data = StringIO("daten2020-01-01n2020-01-01 03:00n") df = pd.read\_csv(data) df\['date'\] = pd.to\_datetime(df\['date'\], format='ISO8601') df

</div>

<div class="ipython" data-suppress="">

python

os.remove("foo.csv")

</div>

#### International date formats

While US date formats tend to be MM/DD/YYYY, many international formats use DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:

<div class="ipython">

python

data = "date,value,catn1/6/2000,5,an2/6/2000,10,bn3/6/2000,15,c" print(data) with open("tmp.csv", "w") as fh: fh.write(data)

pd.read\_csv("tmp.csv", parse\_dates=\[0\]) pd.read\_csv("tmp.csv", dayfirst=True, parse\_dates=\[0\])

</div>

<div class="ipython" data-suppress="">

python

os.remove("tmp.csv")

</div>

#### Writing CSVs to binary file objects

<div class="versionadded">

1.2.0

</div>

`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary mode. In most cases, it is not necessary to specify `mode` as pandas will auto-detect whether the file object is opened in text or binary mode.

<div class="ipython">

python

import io

data = pd.DataFrame(\[0, 1, 2\]) buffer = io.BytesIO() data.to\_csv(buffer, encoding="utf-8", compression="gzip")

</div>

### Specifying method for floating-point conversion

The parameter `float_precision` can be specified in order to use a specific floating-point converter during parsing with the C engine. The options are the ordinary converter, the high-precision converter, and the round-trip converter (which is guaranteed to round-trip values after writing to a file). For example:

<div class="ipython">

python

val = "0.3066101993807095471566981359501369297504425048828125" data = "a,b,cn1,2,{0}".format(val) abs( pd.read\_csv( StringIO(data), engine="c", float\_precision=None, )\["c"\]\[0\] - float(val) ) abs( pd.read\_csv( StringIO(data), engine="c", float\_precision="high", )\["c"\]\[0\] - float(val) ) abs( pd.read\_csv(StringIO(data), engine="c", float\_precision="round\_trip")\["c"\]\[0\] - float(val) )

</div>

### Thousand separators

For large numbers that have been written with a thousands separator, you can set the `thousands` keyword to a string of length 1 so that integers will be parsed correctly:

By default, numbers with a thousands separator will be parsed as strings:

<div class="ipython">

python

  - data = (  
    "IDcategoryn" "Patient1xn" "Patient2yn" "Patient3z"

)

  - with open("tmp.csv", "w") as fh:  
    fh.write(data)

df = pd.read\_csv("tmp.csv", sep="|") df

df.level.dtype

</div>

The `thousands` keyword allows integers to be parsed correctly:

<div class="ipython">

python

df = pd.read\_csv("tmp.csv", sep="|", thousands=",") df

df.level.dtype

</div>

<div class="ipython" data-suppress="">

python

os.remove("tmp.csv")

</div>

### NA values

To control which values are parsed as missing values (which are signified by `NaN`), specify a string in `na_values`. If you specify a list of strings, then all values in it are considered to be missing values. If you specify a number (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent values will also imply a missing value (in this case effectively `[5.0, 5]` are recognized as `NaN`).

To completely override the default values that are recognized as missing, specify `keep_default_na=False`.

<div id="io.navaluesconst">

The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', 'None', '']`.

</div>

Let us consider some examples:

`` `python    pd.read_csv("path_to_file.csv", na_values=[5])  In the example above ``5`and`5.0`will be recognized as`NaN`, in`<span class="title-ref"> addition to the defaults. A string will first be interpreted as a numerical </span><span class="title-ref">5</span><span class="title-ref">, then as a </span><span class="title-ref">NaN</span>\`.

`` `python    pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=[""])  Above, only an empty field will be recognized as ``NaN`.  .. code-block:: python     pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=["NA", "0"])  Above, both`NA`and`0`as strings are`NaN`.  .. code-block:: python     pd.read_csv("path_to_file.csv", na_values=["Nope"])  The default values, in addition to the string`"Nope"`are recognized as`<span class="title-ref"> </span><span class="title-ref">NaN</span>\`.

### Infinity

`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf` as `-np.inf` (negative infinity). These will ignore the case of the value, meaning `Inf`, will also be parsed as `np.inf`.

### Boolean values

The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as boolean. Occasionally you might want to recognize other values as being boolean. To do this, use the `true_values` and `false_values` options as follows:

<div class="ipython">

python

data = "a,b,cn1,Yes,2n3,No,4" print(data) pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data), true\_values=\["Yes"\], false\_values=\["No"\])

</div>

### Handling "bad" lines

Some files may have malformed lines with too few fields or too many. Lines with too few fields will have NA values filled in the trailing fields. Lines with too many fields will raise an error by default:

<div class="ipython" data-okexcept="">

python

data = "a,b,cn1,2,3n4,5,6,7n8,9,10" pd.read\_csv(StringIO(data))

</div>

You can elect to skip bad lines:

<div class="ipython">

python

data = "a,b,cn1,2,3n4,5,6,7n8,9,10" pd.read\_csv(StringIO(data), on\_bad\_lines="skip")

</div>

<div class="versionadded">

1.4.0

</div>

Or pass a callable function to handle the bad line if `engine="python"`. The bad line will be a list of strings that was split by the `sep`:

<div class="ipython">

python

external\_list = \[\] def bad\_lines\_func(line): external\_list.append(line) return line\[-3:\] pd.read\_csv(StringIO(data), on\_bad\_lines=bad\_lines\_func, engine="python") external\_list

</div>

\> **Note** \> The callable function will handle only a line with too many fields. Bad lines caused by other errors will be silently skipped.

> 
> 
> <div class="ipython">
> 
> python
> 
> bad\_lines\_func = lambda line: print(line)
> 
> data = 'name,typenname a,a is of type anname b,"b" is of type b"' data pd.read\_csv(StringIO(data), on\_bad\_lines=bad\_lines\_func, engine="python")
> 
> </div>
> 
> The line was not processed in this case, as a "bad line" here is caused by an escape character.

You can also use the `usecols` parameter to eliminate extraneous column data that appear in some lines but not others:

<div class="ipython" data-okexcept="">

python

pd.read\_csv(StringIO(data), usecols=\[0, 1, 2\])

</div>

In case you want to keep all data including the lines with too many fields, you can specify a sufficient number of `names`. This ensures that lines with not enough fields are filled with `NaN`.

<div class="ipython">

python

pd.read\_csv(StringIO(data), names=\['a', 'b', 'c', 'd'\])

</div>

### Dialect

The `dialect` keyword gives greater flexibility in specifying the file format. By default it uses the Excel dialect but you can specify either the dialect name or a <span class="title-ref">python:csv.Dialect</span> instance.

Suppose you had data with unenclosed quotes:

<div class="ipython">

python

data = "label1,label2,label3n" 'index1,"a,c,en' "index2,b,d,f" print(data)

</div>

By default, `read_csv` uses the Excel dialect and treats the double quote as the quote character, which causes it to fail when it finds a newline before it finds the closing double quote.

We can get around this using `dialect`:

<div class="ipython" data-okwarning="">

python

import csv

dia = csv.excel() dia.quoting = csv.QUOTE\_NONE pd.read\_csv(StringIO(data), dialect=dia)

</div>

All of the dialect options can be specified separately by keyword arguments:

<div class="ipython">

python

data = "a,b,c\~1,2,3\~4,5,6" pd.read\_csv(StringIO(data), lineterminator="\~")

</div>

Another common dialect option is `skipinitialspace`, to skip any whitespace after a delimiter:

<div class="ipython">

python

data = "a, b, cn1, 2, 3n4, 5, 6" print(data) pd.read\_csv(StringIO(data), skipinitialspace=True)

</div>

The parsers make every attempt to "do the right thing" and not be fragile. Type inference is a pretty big deal. If a column can be coerced to integer dtype without altering the contents, the parser will do so. Any non-numeric columns will come through as object dtype as with the rest of pandas objects.

### Quoting and Escape Characters

Quotes (and other escape characters) in embedded fields can be handled in any number of ways. One way is to use backslashes; to properly parse this data, you should pass the `escapechar` option:

<div class="ipython">

python

data = 'a,bn"hello, \\"Bob\\", nice to see you",5' print(data) pd.read\_csv(StringIO(data), escapechar="\\")

</div>

### Files with fixed width columns<span id="io.fwf_reader"></span>

While <span class="title-ref">read\_csv</span> reads delimited data, the <span class="title-ref">read\_fwf</span> function works with data files that have known and fixed column widths. The function parameters to `read_fwf` are largely the same as `read_csv` with two extra parameters, and a different usage of the `delimiter` parameter:

  - `colspecs`: A list of pairs (tuples) giving the extents of the fixed-width fields of each line as half-open intervals (i.e., \[from, to\[ ). String value 'infer' can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data. Default behavior, if not specified, is to infer.
  - `widths`: A list of field widths which can be used instead of 'colspecs' if the intervals are contiguous.
  - `delimiter`: Characters to consider as filler characters in the fixed-width file. Can be used to specify the filler character of the fields if it is not spaces (e.g., '\~').

Consider a typical fixed-width data file:

<div class="ipython">

python

  - data1 = (  
    "id8141 360.242940 149.910199 11950.7n" "id1594 444.953632 166.985655 11788.4n" "id1849 364.136849 183.628767 11806.2n" "id1230 413.836124 184.375703 11916.8n" "id1948 502.953953 173.237159 12468.3"

) with open("bar.csv", "w") as f: f.write(data1)

</div>

In order to parse this file into a `DataFrame`, we simply need to supply the column specifications to the `read_fwf` function along with the file name:

<div class="ipython">

python

\# Column specifications are a list of half-intervals colspecs = \[(0, 6), (8, 20), (21, 33), (34, 43)\] df = pd.read\_fwf("bar.csv", colspecs=colspecs, header=None, index\_col=0) df

</div>

Note how the parser automatically picks column names X.\<column number\> when `header=None` argument is specified. Alternatively, you can supply just the column widths for contiguous columns:

<div class="ipython">

python

\# Widths are a list of integers widths = \[6, 14, 13, 10\] df = pd.read\_fwf("bar.csv", widths=widths, header=None) df

</div>

The parser will take care of extra white spaces around the columns so it's ok to have extra separation between the columns in the file.

By default, `read_fwf` will try to infer the file's `colspecs` by using the first 100 rows of the file. It can do it only in cases when the columns are aligned and correctly separated by the provided `delimiter` (default delimiter is whitespace).

<div class="ipython">

python

df = pd.read\_fwf("bar.csv", header=None, index\_col=0) df

</div>

`read_fwf` supports the `dtype` parameter for specifying the types of parsed columns to be different from the inferred type.

<div class="ipython">

python

pd.read\_fwf("bar.csv", header=None, index\_col=0).dtypes pd.read\_fwf("bar.csv", header=None, dtype={2: "object"}).dtypes

</div>

<div class="ipython" data-suppress="">

python

os.remove("bar.csv")

</div>

### Indexes

#### Files with an "implicit" index column

Consider a file with one less entry in the header than the number of data column:

<div class="ipython">

python

data = "A,B,Cn20090101,a,1,2n20090102,b,3,4n20090103,c,4,5" print(data) with open("foo.csv", "w") as f: f.write(data)

</div>

In this special case, `read_csv` assumes that the first column is to be used as the index of the `DataFrame`:

<div class="ipython">

python

pd.read\_csv("foo.csv")

</div>

Note that the dates weren't automatically parsed. In that case you would need to do as before:

<div class="ipython">

python

df = pd.read\_csv("foo.csv", parse\_dates=True) df.index

</div>

<div class="ipython" data-suppress="">

python

os.remove("foo.csv")

</div>

#### Reading an index with a `MultiIndex`

<div id="io.csv_multiindex">

Suppose you have data indexed by two columns:

</div>

<div class="ipython">

python

data = 'year,indiv,zit,xitn1977,"A",1.2,.6n1977,"B",1.5,.5' print(data) with open("mindex\_ex.csv", mode="w") as f: f.write(data)

</div>

The `index_col` argument to `read_csv` can take a list of column numbers to turn multiple columns into a `MultiIndex` for the index of the returned object:

<div class="ipython">

python

df = pd.read\_csv("mindex\_ex.csv", index\_col=\[0, 1\]) df df.loc\[1977\]

</div>

<div class="ipython" data-suppress="">

python

os.remove("mindex\_ex.csv")

</div>

#### Reading columns with a `MultiIndex`

By specifying list of row locations for the `header` argument, you can read in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the intervening rows.

<div class="ipython">

python

mi\_idx = pd.MultiIndex.from\_arrays(\[\[1, 2, 3, 4\], list("abcd")\], names=list("ab")) mi\_col = pd.MultiIndex.from\_arrays(\[\[1, 2\], list("ab")\], names=list("cd")) df = pd.DataFrame(np.ones((4, 2)), index=mi\_idx, columns=mi\_col) df.to\_csv("mi.csv") print(open("mi.csv").read()) pd.read\_csv("mi.csv", header=\[0, 1, 2, 3\], index\_col=\[0, 1\])

</div>

`read_csv` is also able to interpret a more common format of multi-columns indices.

<div class="ipython">

python

data = ",a,a,a,b,c,cn,q,r,s,t,u,vnone,1,2,3,4,5,6ntwo,7,8,9,10,11,12" print(data) with open("mi2.csv", "w") as fh: fh.write(data)

pd.read\_csv("mi2.csv", header=\[0, 1\], index\_col=0)

</div>

<div class="note">

<div class="title">

Note

</div>

If an `index_col` is not specified (e.g. you don't have an index, or wrote it with `df.to_csv(..., index=False)`, then any `names` on the columns index will be *lost*.

</div>

<div class="ipython" data-suppress="">

python

os.remove("mi.csv") os.remove("mi2.csv")

</div>

### Automatically "sniffing" the delimiter

`read_csv` is capable of inferring delimited (not necessarily comma-separated) files, as pandas uses the <span class="title-ref">python:csv.Sniffer</span> class of the csv module. For this, you have to specify `sep=None`.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 4)) df.to\_csv("tmp2.csv", sep=":", index=False) pd.read\_csv("tmp2.csv", sep=None, engine="python")

</div>

<div class="ipython" data-suppress="">

python

os.remove("tmp2.csv")

</div>

### Reading multiple files to create a single DataFrame

It's best to use <span class="title-ref">\~pandas.concat</span> to combine multiple files. See the \[cookbook\<cookbook.csv.multiple\_files\>\](\#cookbook\<cookbook.csv.multiple\_files\>) for an example.

### Iterating through files chunk by chunk

Suppose you wish to iterate through a (potentially very large) file lazily rather than reading the entire file into memory, such as the following:

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 4)) df.to\_csv("tmp.csv", index=False) table = pd.read\_csv("tmp.csv") table

</div>

By specifying a `chunksize` to `read_csv`, the return value will be an iterable object of type `TextFileReader`:

<div class="ipython">

python

  - with pd.read\_csv("tmp.csv", chunksize=4) as reader:  
    print(reader) for chunk in reader: print(chunk)

</div>

<div class="versionchanged">

1.2

`read_csv/json/sas` return a context-manager when iterating through a file.

</div>

Specifying `iterator=True` will also return the `TextFileReader` object:

<div class="ipython">

python

  - with pd.read\_csv("tmp.csv", iterator=True) as reader:  
    print(reader.get\_chunk(5))

</div>

<div class="ipython" data-suppress="">

python

os.remove("tmp.csv")

</div>

### Specifying the parser engine

pandas currently supports three engines, the C engine, the python engine, and an experimental pyarrow engine (requires the `pyarrow` package). In general, the pyarrow engine is fastest on larger workloads and is equivalent in speed to the C engine on most other workloads. The python engine tends to be slower than the pyarrow and C engines on most workloads. However, the pyarrow engine is much less robust than the C engine, which lacks a few features compared to the Python engine.

Where possible, pandas uses the C parser (specified as `engine='c'`), but it may fall back to Python if C-unsupported options are specified.

Currently, options unsupported by the C and pyarrow engines include:

  - `sep` other than a single character (e.g. regex separators)
  - `skipfooter`

Specifying any of the above options will produce a `ParserWarning` unless the python engine is selected explicitly using `engine='python'`.

Options that are unsupported by the pyarrow engine which are not covered by the list above include:

  - `float_precision`
  - `chunksize`
  - `comment`
  - `nrows`
  - `thousands`
  - `memory_map`
  - `dialect`
  - `on_bad_lines`
  - `quoting`
  - `lineterminator`
  - `converters`
  - `decimal`
  - `iterator`
  - `dayfirst`
  - `verbose`
  - `skipinitialspace`
  - `low_memory`

Specifying these options with `engine='pyarrow'` will raise a `ValueError`.

### Reading/writing remote files

You can pass in a URL to read or write remote files to many of pandas' IO functions - the following example shows reading a CSV file:

`` `python    df = pd.read_csv("https://download.bls.gov/pub/time.series/cu/cu.item", sep="\t")  .. versionadded:: 1.3.0  A custom header can be sent alongside HTTP(s) requests by passing a dictionary ``<span class="title-ref"> of header key value mappings to the </span><span class="title-ref">storage\_options</span>\` keyword argument as shown below:

`` `python    headers = {"User-Agent": "pandas"}    df = pd.read_csv(        "https://download.bls.gov/pub/time.series/cu/cu.item",        sep="\t",        storage_options=headers    )  All URLs which are not local files or HTTP(s) are handled by ``<span class="title-ref"> \`fsspec</span>\_, if installed, and its various filesystem implementations (including Amazon S3, Google Cloud, SSH, FTP, webHDFS...). Some of these implementations will require additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/) library:

`` `python    df = pd.read_json("s3://pandas-test/adatafile.json")  When dealing with remote storage systems, you might need ``<span class="title-ref"> extra configuration with environment variables or config files in special locations. For example, to access data in your S3 bucket, you will need to define credentials in one of the several ways listed in the \`S3Fs documentation \<https://s3fs.readthedocs.io/en/latest/\#credentials\></span>\_. The same is true for several of the storage backends, and you should follow the links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations) for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations) for those not included in the main `fsspec` distribution.

You can also pass parameters directly to the backend driver. Since `fsspec` does not utilize the `AWS_S3_HOST` environment variable, we can directly define a dictionary containing the endpoint\_url and pass the object into the storage option parameter:

`` `python    storage_options = {"client_kwargs": {"endpoint_url": "http://127.0.0.1:5555"}}    df = pd.read_json("s3://pandas-test/test-1", storage_options=storage_options)  More sample configurations and documentation can be found at `S3Fs documentation ``<https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage>.

If you do *not* have S3 credentials, you can still access public data by specifying an anonymous connection, such as

<div class="versionadded">

1.2.0

</div>

`` `python    pd.read_csv(        "s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013"        "-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv",        storage_options={"anon": True},    ) ``fsspec`also allows complex URLs, for accessing data in compressed`\` archives, local caching of files, and more. To locally cache the above example, you would modify the call to

`` `python    pd.read_csv(        "simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/"        "SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv",        storage_options={"s3": {"anon": True}},    )  where we specify that the "anon" parameter is meant for the "s3" part of ``\` the implementation, not to the caching implementation. Note that this caches to a temporary directory for the duration of the session only, but you can also specify a permanent store.

### Writing out data

#### Writing to CSV format

The `Series` and `DataFrame` objects have an instance method `to_csv` which allows storing the contents of the object as a comma-separated-values file. The function takes a number of arguments. Only the first is required.

  - `path_or_buf`: A string path to the file to write or a file object. If a file object it must be opened with `newline=''`
  - `sep` : Field delimiter for the output file (default ",")
  - `na_rep`: A string representation of a missing value (default '')
  - `float_format`: Format string for floating point numbers
  - `columns`: Columns to write (default None)
  - `header`: Whether to write out the column names (default True)
  - `index`: whether to write row (index) names (default True)
  - `index_label`: Column label(s) for index column(s) if desired. If None (default), and `header` and `index` are True, then the index names are used. (A sequence should be given if the `DataFrame` uses MultiIndex).
  - `mode` : Python write mode, default 'w'
  - `encoding`: a string representing the encoding to use if the contents are non-ASCII, for Python versions prior to 3
  - `lineterminator`: Character sequence denoting line end (default `os.linesep`)
  - `quoting`: Set quoting rules as in csv module (default csv.QUOTE\_MINIMAL). Note that if you have set a `float_format` then floats are converted to strings and csv.QUOTE\_NONNUMERIC will treat them as non-numeric
  - `quotechar`: Character used to quote fields (default '"')
  - `doublequote`: Control quoting of `quotechar` in fields (default True)
  - `escapechar`: Character used to escape `sep` and `quotechar` when appropriate (default None)
  - `chunksize`: Number of rows to write at a time
  - `date_format`: Format string for datetime objects

#### Writing a formatted string

<div id="io.formatting">

The `DataFrame` object has an instance method `to_string` which allows control over the string representation of the object. All arguments are optional:

</div>

  - `buf` default None, for example a StringIO object
  - `columns` default None, which columns to write
  - `col_space` default None, minimum width of each column.
  - `na_rep` default `NaN`, representation of NA value
  - `formatters` default None, a dictionary (by column) of functions each of which takes a single argument and returns a formatted string
  - `float_format` default None, a function which takes a single (float) argument and returns a formatted string; to be applied to floats in the `DataFrame`.
  - `sparsify` default True, set to False for a `DataFrame` with a hierarchical index to print every MultiIndex key at each row.
  - `index_names` default True, will print the names of the indices
  - `index` default True, will print the index (ie, row labels)
  - `header` default True, will print the column labels
  - `justify` default `left`, will print column headers left- or right-justified

The `Series` object also has a `to_string` method, but with only the `buf`, `na_rep`, `float_format` arguments. There is also a `length` argument which, if set to `True`, will additionally output the length of the Series.

## JSON

Read and write `JSON` format files and strings.

### Writing JSON

A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json` with optional parameters:

  - `path_or_buf` : the pathname or buffer to write the output. This can be `None` in which case a JSON string is returned.

  - `orient` :
    
      - `Series`:
        
          - default is `index`
          - allowed values are {`split`, `records`, `index`}
    
      - `DataFrame`:
        
          - default is `columns`
          - allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
    
    The format of the JSON string
    
    |           |                                                                                      |
    | --------- | ------------------------------------------------------------------------------------ |
    | `split`   | dict like {index -\> \[index\]; columns -\> \[columns\]; data -\> \[values\]}        |
    | `records` | list like \[{column -\> value}; ... \]                                               |
    | `index`   | dict like {index -\> {column -\> value}}                                             |
    | `columns` | dict like {column -\> {index -\> value}}                                             |
    | `values`  | just the values array                                                                |
    | `table`   | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/) |

  - `date_format` : string, type of date conversion, 'epoch' for timestamp, 'iso' for ISO8601.

  - `double_precision` : The number of decimal places to use when encoding floating point values, default 10.

  - `force_ascii` : force encoded string to be ASCII, default True.

  - `date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us' or 'ns' for seconds, milliseconds, microseconds and nanoseconds respectively. Default 'ms'.

  - `default_handler` : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object.

  - `lines` : If `records` orient, then will write each record per line as json.

  - `mode` : string, writer mode when writing to path. 'w' for write, 'a' for append. Default 'w'

Note `NaN`'s, `NaT`'s and `None` will be converted to `null` and `datetime` objects will be converted based on the `date_format` and `date_unit` parameters.

<div class="ipython">

python

dfj = pd.DataFrame(np.random.randn(5, 2), columns=list("AB")) json = dfj.to\_json() json

</div>

#### Orient options

There are a number of different options for the format of the resulting JSON file / string. Consider the following `DataFrame` and `Series`:

<div class="ipython">

python

  - dfjo = pd.DataFrame(  
    dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)), columns=list("ABC"), index=list("xyz"),

) dfjo sjo = pd.Series(dict(x=15, y=16, z=17), name="D") sjo

</div>

**Column oriented** (the default for `DataFrame`) serializes the data as nested JSON objects with column labels acting as the primary index:

<div class="ipython">

python

dfjo.to\_json(orient="columns") \# Not available for Series

</div>

**Index oriented** (the default for `Series`) similar to column oriented but the index labels are now primary:

<div class="ipython">

python

dfjo.to\_json(orient="index") sjo.to\_json(orient="index")

</div>

**Record oriented** serializes the data to a JSON array of column -\> value records, index labels are not included. This is useful for passing `DataFrame` data to plotting libraries, for example the JavaScript library `d3.js`:

<div class="ipython">

python

dfjo.to\_json(orient="records") sjo.to\_json(orient="records")

</div>

**Value oriented** is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included:

<div class="ipython">

python

dfjo.to\_json(orient="values") \# Not available for Series

</div>

**Split oriented** serializes to a JSON object containing separate entries for values, index and columns. Name is also included for `Series`:

<div class="ipython">

python

dfjo.to\_json(orient="split") sjo.to\_json(orient="split")

</div>

**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/), allowing for the preservation of metadata including but not limited to dtypes and index names.

\> **Note** \> Any orient option that encodes to a JSON object will not preserve the ordering of index and column labels during round-trip serialization. If you wish to preserve label ordering use the `split` option as it uses ordered containers.

#### Date handling

Writing in ISO date format:

<div class="ipython">

python

dfd = pd.DataFrame(np.random.randn(5, 2), columns=list("AB")) dfd\["date"\] = pd.Timestamp("20130101") dfd = dfd.sort\_index(axis=1, ascending=False) json = dfd.to\_json(date\_format="iso") json

</div>

Writing in ISO date format, with microseconds:

<div class="ipython">

python

json = dfd.to\_json(date\_format="iso", date\_unit="us") json

</div>

Writing to a file, with a date index and a date column:

<div class="ipython">

python

dfj2 = dfj.copy() dfj2\["date"\] = pd.Timestamp("20130101") dfj2\["ints"\] = list(range(5)) dfj2\["bools"\] = True dfj2.index = pd.date\_range("20130101", periods=5) dfj2.to\_json("test.json", date\_format="iso")

  - with open("test.json") as fh:  
    print(fh.read())

</div>

#### Fallback behavior

If the JSON serializer cannot handle the container contents directly it will fall back in the following manner:

  - if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`, if provided, will be called for each value, otherwise an exception is raised.

  - if an object is unsupported it will attempt the following:
    
    >   - check if the object has defined a `toDict` method and call it. A `toDict` method should return a `dict` which will then be JSON serialized.
    >   - invoke the `default_handler` if one was provided.
    >   - convert the object to a `dict` by traversing its contents. However this will often fail with an `OverflowError` or give unexpected results.

In general the best approach for unsupported objects or dtypes is to provide a `default_handler`. For example:

`` `python   >>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises   RuntimeError: Unhandled numpy dtype 15  can be dealt with by specifying a simple ``default\_handler`:  .. ipython:: python     pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)  .. _io.json_reader:  Reading JSON`\` ''''''''''''

Reading a JSON string to pandas object can take a number of parameters. The parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`. To explicitly force `Series` parsing, pass `typ=series`

  - `filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The string could be a URL. Valid URL schemes include http, ftp, S3, and file. For file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json

  - `typ` : type of object to recover (series or frame), default 'frame'

  - `orient` :
    
      - Series :
        
          - default is `index`
          - allowed values are {`split`, `records`, `index`}
    
      - DataFrame
        
          - default is `columns`
          - allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
    
    The format of the JSON string
    
    |           |                                                                                      |
    | --------- | ------------------------------------------------------------------------------------ |
    | `split`   | dict like {index -\> \[index\]; columns -\> \[columns\]; data -\> \[values\]}        |
    | `records` | list like \[{column -\> value} ...\]                                                 |
    | `index`   | dict like {index -\> {column -\> value}}                                             |
    | `columns` | dict like {column -\> {index -\> value}}                                             |
    | `values`  | just the values array                                                                |
    | `table`   | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/) |

  - `dtype` : if True, infer dtypes, if a dict of column to dtype, then use those, if `False`, then don't infer dtypes at all, default is True, apply only to the data.

  - `convert_axes` : boolean, try to convert the axes to the proper dtypes, default is `True`

  - `convert_dates` : a list of columns to parse for dates; If `True`, then try to parse date-like columns, default is `True`.

  - `keep_default_dates` : boolean, default `True`. If parsing dates, then parse the default date-like columns.

  - `precise_float` : boolean, default `False`. Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (`False`) is to use fast but less precise builtin functionality.

  - `date_unit` : string, the timestamp unit to detect if converting dates. Default None. By default the timestamp precision will be detected, if this is not desired then pass one of 's', 'ms', 'us' or 'ns' to force timestamp precision to seconds, milliseconds, microseconds or nanoseconds respectively.

  - `lines` : reads file as one json object per line.

  - `encoding` : The encoding to use to decode py3 bytes.

  - `chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader` which reads in `chunksize` lines per iteration.

  - `engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which dispatches to pyarrow's `pyarrow.json.read_json`. The `"pyarrow"` is only available when `lines=True`

The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON is not parseable.

If a non-default `orient` was used when encoding to JSON be sure to pass the same option here so that decoding produces sensible results, see [Orient Options](#orient-options) for an overview.

#### Data conversion

The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will try to parse the axes, and all of the data into appropriate types, including dates. If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes` should only be set to `False` if you need to preserve string-like numbers (e.g. '1', '2') in an axes.

\> **Note** \> Large integer values may be converted to dates if `convert_dates=True` and the data and / or column labels appear 'date-like'. The exact threshold depends on the `date_unit` specified. 'date-like' means that the column label meets one of the following criteria:

>   - it ends with `'_at'`
>   - it ends with `'_time'`
>   - it begins with `'timestamp'`
>   - it is `'modified'`
>   - it is `'date'`

\> **Warning** \> When reading JSON data, automatic coercing into dtypes has some quirks:

>   - an index can be reconstructed in a different order from serialization, that is, the returned order is not guaranteed to be the same as before serialization
>   - a column that was `float` data will be converted to `integer` if it can be done safely, e.g. a column of `1.`
>   - bool columns will be converted to `integer` on reconstruction
> 
> Thus there are times where you may want to specify specific dtypes via the `dtype` keyword argument.

Reading from a JSON string:

<div class="ipython">

python

from io import StringIO pd.read\_json(StringIO(json))

</div>

Reading from a file:

<div class="ipython">

python

pd.read\_json("test.json")

</div>

Don't convert any data (but still convert axes and dates):

<div class="ipython">

python

pd.read\_json("test.json", dtype=object).dtypes

</div>

Specify dtypes for conversion:

<div class="ipython">

python

pd.read\_json("test.json", dtype={"A": "float32", "bools": "int8"}).dtypes

</div>

Preserve string indices:

<div class="ipython">

python

from io import StringIO si = pd.DataFrame( np.zeros((4, 4)), columns=list(range(4)), index=\[str(i) for i in range(4)\] ) si si.index si.columns json = si.to\_json()

sij = pd.read\_json(StringIO(json), convert\_axes=False) sij sij.index sij.columns

</div>

Dates written in nanoseconds need to be read back in nanoseconds:

<div class="ipython">

python

from io import StringIO json = dfj2.to\_json(date\_format="iso", date\_unit="ns")

\# Try to parse timestamps as milliseconds -\> Won't Work dfju = pd.read\_json(StringIO(json), date\_unit="ms") dfju

\# Let pandas detect the correct precision dfju = pd.read\_json(StringIO(json)) dfju

\# Or specify that all timestamps are in nanoseconds dfju = pd.read\_json(StringIO(json), date\_unit="ns") dfju

</div>

By setting the `dtype_backend` argument you can control the default dtypes used for the resulting DataFrame.

<div class="ipython">

python

  - data = (  
    '{"a":{"0":1,"1":3},"b":{"0":2.5,"1":4.5},"c":{"0":true,"1":false},"d":{"0":"a","1":"b"},' '"e":{"0":null,"1":6.0},"f":{"0":null,"1":7.5},"g":{"0":null,"1":true},"h":{"0":null,"1":"a"},' '"i":{"0":"12-31-2019","1":"12-31-2019"},"j":{"0":null,"1":null}}'

) df = pd.read\_json(StringIO(data), dtype\_backend="pyarrow") df df.dtypes

</div>

### Normalization

pandas provides a utility function to take a dict or list of dicts and *normalize* this semi-structured data into a flat table.

<div class="ipython">

python

  - data = \[  
    {"id": 1, "name": {"first": "Coleen", "last": "Volk"}}, {"name": {"given": "Mark", "family": "Regner"}}, {"id": 2, "name": "Faye Raker"},

\] pd.json\_normalize(data)

</div>

<div class="ipython">

python

  - data = \[
    
      - {  
        "state": "Florida", "shortname": "FL", "info": {"governor": "Rick Scott"}, "county": \[ {"name": "Dade", "population": 12345}, {"name": "Broward", "population": 40000}, {"name": "Palm Beach", "population": 60000}, \],
    
    }, { "state": "Ohio", "shortname": "OH", "info": {"governor": "John Kasich"}, "county": \[ {"name": "Summit", "population": 1234}, {"name": "Cuyahoga", "population": 1337}, \], },

\]

pd.json\_normalize(data, "county", \["state", "shortname", \["info", "governor"\]\])

</div>

The max\_level parameter provides more control over which level to end normalization. With max\_level=1 the following snippet normalizes until 1st nesting level of the provided dict.

<div class="ipython">

python

  - data = \[
    
      - {  
        "CreatedBy": {"Name": "User001"}, "Lookup": { "TextField": "Some text", "UserField": {"Id": "ID001", "Name": "Name001"}, }, "Image": {"a": "b"},
    
    }

\] pd.json\_normalize(data, max\_level=1)

</div>

### Line delimited json

pandas is able to read and write line-delimited json files that are common in data processing pipelines using Hadoop or Spark.

For line-delimited json files, pandas can also return an iterator which reads in `chunksize` lines at a time. This can be useful for large files or to read from a stream.

<div class="ipython">

python

from io import StringIO jsonl = """ {"a": 1, "b": 2} {"a": 3, "b": 4} """ df = pd.read\_json(StringIO(jsonl), lines=True) df df.to\_json(orient="records", lines=True)

\# reader is an iterator that returns `chunksize` lines each iteration with pd.read\_json(StringIO(jsonl), lines=True, chunksize=1) as reader: reader for chunk in reader: print(chunk)

</div>

Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.

<div class="ipython">

python

from io import BytesIO df = pd.read\_json(BytesIO(jsonl.encode()), lines=True, engine="pyarrow") df

</div>

<div class="versionadded">

2.0.0

</div>

### Table schema

[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for describing tabular datasets as a JSON object. The JSON includes information on the field names, types, and other attributes. You can use the orient `table` to build a JSON string with two fields, `schema` and `data`.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": \[1, 2, 3\], "B": \["a", "b", "c"\], "C": pd.date\_range("2016-01-01", freq="D", periods=3),
    
    }, index=pd.Index(range(3), name="idx"),

) df df.to\_json(orient="table", date\_format="iso")

</div>

The `schema` field contains the `fields` key, which itself contains a list of column name to type pairs, including the `Index` or `MultiIndex` (see below for a list of types). The `schema` field also contains a `primaryKey` field if the (Multi)index is unique.

The second field, `data`, contains the serialized data with the `records` orient. The index is included, and any datetimes are ISO 8601 formatted, as required by the Table Schema spec.

The full list of types supported are described in the Table Schema spec. This table shows the mapping from pandas types:

| pandas type       | Table Schema type |
| ----------------- | ----------------- |
| int64             | integer           |
| float64           | number            |
| bool              | boolean           |
| datetime64\[ns\]  | datetime          |
| timedelta64\[ns\] | duration          |
| categorical       | any               |
| object            | str               |

A few notes on the generated table schema:

  - The `schema` object contains a `pandas_version` field. This contains the version of pandas' dialect of the schema, and will be incremented with each revision.

  - All dates are converted to UTC when serializing. Even timezone naive values, which are treated as UTC with an offset of 0.
    
    <div class="ipython">
    
    python
    
    from pandas.io.json import build\_table\_schema
    
    s = pd.Series(pd.date\_range("2016", periods=4)) build\_table\_schema(s)
    
    </div>

  - datetimes with a timezone (before serializing), include an additional field `tz` with the time zone name (e.g. `'US/Central'`).
    
    <div class="ipython">
    
    python
    
    s\_tz = pd.Series(pd.date\_range("2016", periods=12, tz="US/Central")) build\_table\_schema(s\_tz)
    
    </div>

  - Periods are converted to timestamps before serialization, and so have the same behavior of being converted to UTC. In addition, periods will contain and additional field `freq` with the period's frequency, e.g. `'A-DEC'`.
    
    <div class="ipython">
    
    python
    
    s\_per = pd.Series(1, index=pd.period\_range("2016", freq="Y-DEC", periods=4)) build\_table\_schema(s\_per)
    
    </div>

  - Categoricals use the `any` type and an `enum` constraint listing the set of possible values. Additionally, an `ordered` field is included:
    
    <div class="ipython">
    
    python
    
    s\_cat = pd.Series(pd.Categorical(\["a", "b", "a"\])) build\_table\_schema(s\_cat)
    
    </div>

  - A `primaryKey` field, containing an array of labels, is included *if the index is unique*:
    
    <div class="ipython">
    
    python
    
    s\_dupe = pd.Series(\[1, 2\], index=\[1, 1\]) build\_table\_schema(s\_dupe)
    
    </div>

  - The `primaryKey` behavior is the same with MultiIndexes, but in this case the `primaryKey` is an array:
    
    <div class="ipython">
    
    python
    
    s\_multi = pd.Series(1, index=pd.MultiIndex.from\_product(\[("a", "b"), (0, 1)\])) build\_table\_schema(s\_multi)
    
    </div>

  - The default naming roughly follows these rules:
    
    >   - For series, the `object.name` is used. If that's none, then the name is `values`
    >   - For `DataFrames`, the stringified version of the column name is used
    >   - For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index` if that is None.
    >   - For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>` is used.

`read_json` also accepts `orient='table'` as an argument. This allows for the preservation of metadata such as dtypes and index names in a round-trippable manner.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "foo": \[1, 2, 3, 4\], "bar": \["a", "b", "c", "d"\], "baz": pd.date\_range("2018-01-01", freq="D", periods=4), "qux": pd.Categorical(\["a", "b", "c", "c"\]),
    
    }, index=pd.Index(range(4), name="idx"),

) df df.dtypes

df.to\_json("test.json", orient="table") new\_df = pd.read\_json("test.json", orient="table") new\_df new\_df.dtypes

</div>

Please note that the literal string 'index' as the name of an <span class="title-ref">Index</span> is not round-trippable, nor are any names beginning with `'level_'` within a <span class="title-ref">MultiIndex</span>. These are used by default in <span class="title-ref">DataFrame.to\_json</span> to indicate missing values and the subsequent read cannot distinguish the intent.

<div class="ipython" data-okwarning="">

python

df.index.name = "index" df.to\_json("test.json", orient="table") new\_df = pd.read\_json("test.json", orient="table") print(new\_df.index.name)

</div>

<div class="ipython" data-suppress="">

python

os.remove("test.json")

</div>

When using `orient='table'` along with user-defined `ExtensionArray`, the generated schema will contain an additional `extDtype` key in the respective `fields` element. This extra key is not standard but does enable JSON roundtrips for extension types (e.g. `read_json(df.to_json(orient="table"), orient="table")`).

The `extDtype` key carries the name of the extension, if you have properly registered the `ExtensionDtype`, pandas will use said name to perform a lookup into the registry and re-convert the serialized data into your custom dtype.

## HTML

### Reading HTML content

\> **Warning** \> We **highly encourage** you to read the \[HTML Table Parsing gotchas \<io.html.gotchas\>\](\#html-table-parsing-gotchas-\<io.html.gotchas\>) below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.

The top-level <span class="title-ref">\~pandas.io.html.read\_html</span> function can accept an HTML string/file/URL and will parse HTML tables into list of pandas `DataFrames`. Let's look at a few examples.

\> **Note** \> `read_html` returns a `list` of `DataFrame` objects, even if there is only a single table contained in the HTML content.

Read a URL with no options:

`` `ipython    In [320]: url = "https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list"    In [321]: pd.read_html(url)    Out[321]:    [                         Bank NameBank           CityCity StateSt  ...              Acquiring InstitutionAI Closing DateClosing FundFund     0                    Almena State Bank             Almena      KS  ...                          Equity Bank    October 23, 2020    10538     1           First City Bank of Florida  Fort Walton Beach      FL  ...            United Fidelity Bank, fsb    October 16, 2020    10537     2                 The First State Bank      Barboursville      WV  ...                       MVB Bank, Inc.       April 3, 2020    10536     3                   Ericson State Bank            Ericson      NE  ...           Farmers and Merchants Bank   February 14, 2020    10535     4     City National Bank of New Jersey             Newark      NJ  ...                      Industrial Bank    November 1, 2019    10534     ..                                 ...                ...     ...  ...                                  ...                 ...      ...     558                 Superior Bank, FSB           Hinsdale      IL  ...                Superior Federal, FSB       July 27, 2001     6004     559                Malta National Bank              Malta      OH  ...                    North Valley Bank         May 3, 2001     4648     560    First Alliance Bank & Trust Co.         Manchester      NH  ...  Southern New Hampshire Bank & Trust    February 2, 2001     4647     561  National State Bank of Metropolis         Metropolis      IL  ...              Banterra Bank of Marion   December 14, 2000     4646     562                   Bank of Honolulu           Honolulu      HI  ...                   Bank of the Orient    October 13, 2000     4645      [563 rows x 7 columns]]  .. note::     The data from the above URL changes every Monday so the resulting data above may be slightly different.  Read a URL while passing headers alongside the HTTP request:  .. code-block:: ipython     In [322]: url = 'https://www.sump.org/notes/request/' # HTTP request reflector    In [323]: pd.read_html(url)    Out[323]:    [                   0                    1     0     Remote Socket:  51.15.105.256:51760     1  Protocol Version:             HTTP/1.1     2    Request Method:                  GET     3       Request URI:      /notes/request/     4     Request Query:                  NaN,     0   Accept-Encoding:             identity     1              Host:         www.sump.org     2        User-Agent:    Python-urllib/3.8     3        Connection:                close]    In [324]: headers = {    In [325]:    'User-Agent':'Mozilla Firefox v14.0',    In [326]:    'Accept':'application/json',    In [327]:    'Connection':'keep-alive',    In [328]:    'Auth':'Bearer 2*/f3+fe68df*4'    In [329]: }    In [340]: pd.read_html(url, storage_options=headers)    Out[340]:    [                   0                    1     0     Remote Socket:  51.15.105.256:51760     1  Protocol Version:             HTTP/1.1     2    Request Method:                  GET     3       Request URI:      /notes/request/     4     Request Query:                  NaN,     0        User-Agent: Mozilla Firefox v14.0     1    AcceptEncoding:   gzip,  deflate,  br     2            Accept:      application/json     3        Connection:             keep-alive     4              Auth:  Bearer 2*/f3+fe68df*4]  .. note::     We see above that the headers we passed are reflected in the HTTP request.  Read in the content of the file from the above URL and pass it to ``read\_html`  `\` as a string:

<div class="ipython">

python

  - html\_str = """
    
      - \<table\>
        
          - \<tr\>  
            \<th\>A\</th\> \<th colspan="1"\>B\</th\> \<th rowspan="1"\>C\</th\>
        
        \</tr\> \<tr\> \<td\>a\</td\> \<td\>b\</td\> \<td\>c\</td\> \</tr\>
    
    \</table\> """

  - with open("tmp.html", "w") as f:  
    f.write(html\_str)

df = pd.read\_html("tmp.html") df\[0\]

</div>

<div class="ipython" data-suppress="">

python

os.remove("tmp.html")

</div>

You can even pass in an instance of `StringIO` if you so desire:

<div class="ipython">

python

dfs = pd.read\_html(StringIO(html\_str)) dfs\[0\]

</div>

\> **Note** \> The following examples are not run by the IPython evaluator due to the fact that having so many network-accessing functions slows down the documentation build. If you spot an error or an example that doesn't run, please do not hesitate to report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).

Read a URL and match a table that contains specific text:

`` `python    match = "Metcalf Bank"    df_list = pd.read_html(url, match=match)  Specify a header row (by default ``\<th\>`or`\<td\>`elements located within a`<span class="title-ref"> </span><span class="title-ref">\<thead\></span><span class="title-ref"> are used to form the column index, if multiple rows are contained within </span><span class="title-ref">\<thead\></span><span class="title-ref"> then a MultiIndex is created); if specified, the header row is taken from the data minus the parsed header elements (</span><span class="title-ref">\<th\></span>\` elements).

`` `python    dfs = pd.read_html(url, header=0)  Specify an index column:  .. code-block:: python     dfs = pd.read_html(url, index_col=0)  Specify a number of rows to skip:  .. code-block:: python     dfs = pd.read_html(url, skiprows=0)  Specify a number of rows to skip using a list ( ``range`works`\` as well):

`` `python    dfs = pd.read_html(url, skiprows=range(2))  Specify an HTML attribute:  .. code-block:: python     dfs1 = pd.read_html(url, attrs={"id": "table"})    dfs2 = pd.read_html(url, attrs={"class": "sortable"})    print(np.array_equal(dfs1[0], dfs2[0]))  # Should be True  Specify values that should be converted to NaN:  .. code-block:: python     dfs = pd.read_html(url, na_values=["No Acquirer"])  Specify whether to keep the default set of NaN values:  .. code-block:: python     dfs = pd.read_html(url, keep_default_na=False)  Specify converters for columns. This is useful for numerical text data that has ``\` leading zeros. By default columns that are numerical are cast to numeric types and the leading zeros are lost. To avoid this, we can convert these columns to strings.

`` `python    url_mcc = "https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761"    dfs = pd.read_html(        url_mcc,        match="Telekom Albania",        header=0,        converters={"MNC": str},    )  Use some combination of the above:  .. code-block:: python     dfs = pd.read_html(url, match="Metcalf Bank", index_col=0)  Read in pandas ``to\_html`output (with some loss of floating point precision):  .. code-block:: python     df = pd.DataFrame(np.random.randn(2, 2))    s = df.to_html(float_format="{0:.40g}".format)    dfin = pd.read_html(s, index_col=0)  The`lxml`backend will raise an error on a failed parse if that is the only`\` parser you provide. If you only have a single parser you can provide just a string, but it is considered good practice to pass a list with one string if, for example, the function expects a sequence of strings. You may use:

`` `python    dfs = pd.read_html(url, "Metcalf Bank", index_col=0, flavor=["lxml"])  Or you could pass ``flavor='lxml'`without a list:  .. code-block:: python     dfs = pd.read_html(url, "Metcalf Bank", index_col=0, flavor="lxml")  However, if you have bs4 and html5lib installed and pass`None`or`\['lxml', `` ` 'bs4'] `` then the parse will most likely succeed. Note that *as soon as a parse succeeds, the function will return*.

`` `python    dfs = pd.read_html(url, "Metcalf Bank", index_col=0, flavor=["lxml", "bs4"])  Links can be extracted from cells along with the text using ``extract\_links="all"`.  .. ipython:: python      html_table = """     <table>       <tr>         <th>GitHub</th>       </tr>       <tr>         <td><a href="https://github.com/pandas-dev/pandas">pandas</a></td>       </tr>     </table>     """      df = pd.read_html(         StringIO(html_table),         extract_links="all"     )[0]     df     df[("GitHub", None)]     df[("GitHub", None)].str[1]  .. versionadded:: 1.5.0  .. _io.html:  Writing to HTML files`\` ''''''''''''''''''''''

`DataFrame` objects have an instance method `to_html` which renders the contents of the `DataFrame` as an HTML table. The function arguments are as in the method `to_string` described above.

\> **Note** \> Not all of the possible options for `DataFrame.to_html` are shown here for brevity's sake. See <span class="title-ref">.DataFrame.to\_html</span> for the full set of options.

<div class="note">

<div class="title">

Note

</div>

In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))`\` will render the raw HTML into the environment.

</div>

<div class="ipython">

python

from IPython.display import display, HTML

df = pd.DataFrame(np.random.randn(2, 2)) df html = df.to\_html() print(html) \# raw html display(HTML(html))

</div>

The `columns` argument will limit the columns shown:

<div class="ipython">

python

html = df.to\_html(columns=\[0\]) print(html) display(HTML(html))

</div>

`float_format` takes a Python callable to control the precision of floating point values:

<div class="ipython">

python

html = df.to\_html(float\_format="{0:.10f}".format) print(html) display(HTML(html))

</div>

`bold_rows` will make the row labels bold by default, but you can turn that off:

<div class="ipython">

python

html = df.to\_html(bold\_rows=False) print(html) display(HTML(html))

</div>

The `classes` argument provides the ability to give the resulting HTML table CSS classes. Note that these classes are *appended* to the existing `'dataframe'` class.

<div class="ipython">

python

print(df.to\_html(classes=\["awesome\_table\_class", "even\_more\_awesome\_class"\]))

</div>

The `render_links` argument provides the ability to add hyperlinks to cells that contain URLs.

<div class="ipython">

python

  - url\_df = pd.DataFrame(
    
      - {  
        "name": \["Python", "pandas"\], "url": \["<https://www.python.org/>", "<https://pandas.pydata.org>"\],
    
    }

) html = url\_df.to\_html(render\_links=True) print(html) display(HTML(html))

</div>

Finally, the `escape` argument allows you to control whether the "\<", "\>" and "&" characters escaped in the resulting HTML (by default it is `True`). So to get the HTML without escaped characters pass `escape=False`

<div class="ipython">

python

df = pd.DataFrame({"a": list("&\<\>"), "b": np.random.randn(3)})

</div>

Escaped:

<div class="ipython">

python

html = df.to\_html() print(html) display(HTML(html))

</div>

Not escaped:

<div class="ipython">

python

html = df.to\_html(escape=False) print(html) display(HTML(html))

</div>

\> **Note** \> Some browsers may not show a difference in the rendering of the previous two HTML tables.

### HTML Table Parsing Gotchas

There are some versioning issues surrounding the libraries that are used to parse HTML tables in the top-level pandas io function `read_html`.

**Issues with** **lxml**\_

  - Benefits
    
    >   - **lxml**\_ is very fast.
    >   - **lxml**\_ requires Cython to install correctly.

  - Drawbacks
    
    >   - **lxml**\_ does *not* make any guarantees about the results of its parse *unless* it is given **strictly valid markup**\_.
    >   - In light of the above, we have chosen to allow you, the user, to use the **lxml**\_ backend, but **this backend will use** **html5lib**\_ if **lxml**\_ fails to parse
    >   - It is therefore *highly recommended* that you install both **BeautifulSoup4**\_ and **html5lib**\_, so that you will still get a valid result (provided everything else is valid) even if **lxml**\_ fails.

**Issues with** **BeautifulSoup4**\_ **using** **lxml**\_ **as a backend**

  - The above issues hold here as well since **BeautifulSoup4**\_ is essentially just a wrapper around a parser backend.

**Issues with** **BeautifulSoup4**\_ **using** **html5lib**\_ **as a backend**

  - Benefits
    
    >   - **html5lib**\_ is far more lenient than **lxml**\_ and consequently deals with *real-life markup* in a much saner way rather than just, e.g., dropping an element without notifying you.
    >   - **html5lib**\_ *generates valid HTML5 markup from invalid markup automatically*. This is extremely important for parsing HTML tables, since it guarantees a valid document. However, that does NOT mean that it is "correct", since the process of fixing markup does not have a single definition.
    >   - **html5lib**\_ is pure Python and requires no additional build steps beyond its own installation.

  - Drawbacks
    
    >   - The biggest drawback to using **html5lib**\_ is that it is slow as molasses. However consider the fact that many tables on the web are not big enough for the parsing algorithm runtime to matter. It is more likely that the bottleneck will be in the process of reading the raw text from the URL over the web, i.e., IO (input-output). For very large tables, this might not be true.

## LaTeX

<div class="versionadded">

1.3.0

</div>

Currently there are no methods to read from LaTeX, only output methods.

### Writing to LaTeX files

\> **Note** \> DataFrame *and* Styler objects currently have a `to_latex` method. We recommend using the [Styler.to\_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.md) method over [DataFrame.to\_latex()](../reference/api/pandas.DataFrame.to_latex.md) due to the former's greater flexibility with conditional styling, and the latter's possible future deprecation.

Review the documentation for [Styler.to\_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.md), which gives examples of conditional styling and explains the operation of its keyword arguments.

For simple application the following pattern is sufficient.

<div class="ipython">

python

df = pd.DataFrame(\[\[1, 2\], \[3, 4\]\], index=\["a", "b"\], columns=\["c", "d"\]) print(df.style.to\_latex())

</div>

To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.md) method.

<div class="ipython">

python

print(df.style.format("â‚¬ {}").to\_latex())

</div>

## XML

### Reading XML

<div class="versionadded">

1.3.0

</div>

The top-level <span class="title-ref">\~pandas.io.xml.read\_xml</span> function can accept an XML string/file/URL and will parse nodes and attributes into a pandas `DataFrame`.

\> **Note** \> Since there is no standard XML structure where design types can vary in many ways, `read_xml` works best with flatter, shallow versions. If an XML document is deeply nested, use the `stylesheet` feature to transform XML into a flatter version.

Let's look at a few examples.

Read an XML string:

<div class="ipython">

python

from io import StringIO xml = """\<?xml version="1.0" encoding="UTF-8"?\> \<bookstore\> \<book category="cooking"\> \<title lang="en"\>Everyday Italian\</title\> \<author\>Giada De Laurentiis\</author\> \<year\>2005\</year\> \<price\>30.00\</price\> \</book\> \<book category="children"\> \<title lang="en"\>Harry Potter\</title\> \<author\>J K. Rowling\</author\> \<year\>2005\</year\> \<price\>29.99\</price\> \</book\> \<book category="web"\> \<title lang="en"\>Learning XML\</title\> \<author\>Erik T. Ray\</author\> \<year\>2003\</year\> \<price\>39.95\</price\> \</book\> \</bookstore\>"""

df = pd.read\_xml(StringIO(xml)) df

</div>

Read a URL with no options:

<div class="ipython">

python

df = pd.read\_xml("<https://www.w3schools.com/xml/books.xml>") df

</div>

Read in the content of the "books.xml" file and pass it to `read_xml` as a string:

<div class="ipython">

python

file\_path = "books.xml" with open(file\_path, "w") as f: f.write(xml)

  - with open(file\_path, "r") as f:  
    df = pd.read\_xml(StringIO(f.read()))

df

</div>

Read in the content of the "books.xml" as instance of `StringIO` or `BytesIO` and pass it to `read_xml`:

<div class="ipython">

python

  - with open(file\_path, "r") as f:  
    sio = StringIO(f.read())

df = pd.read\_xml(sio) df

</div>

<div class="ipython">

python

  - with open(file\_path, "rb") as f:  
    bio = BytesIO(f.read())

df = pd.read\_xml(bio) df

</div>

Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing Biomedical and Life Science Journals:

`` `python    >>> df = pd.read_xml(    ...    "s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml",    ...    xpath=".//journal-meta",    ...)    >>> df          journal-id  journal-title  issn  publisher    0 Cardiovasc Ultrasound Cardiovascular Ultrasound 1476-7120 NaN  With `lxml`_ as default ``parser`, you access the full-featured XML library`\` that extends Python's ElementTree API. One powerful tool is ability to query nodes selectively or conditionally with more expressive XPath:

<div class="ipython">

python

df = pd.read\_xml(file\_path, xpath="//book\[year=2005\]") df

</div>

Specify only elements or only attributes to parse:

<div class="ipython">

python

df = pd.read\_xml(file\_path, elems\_only=True) df

</div>

<div class="ipython">

python

df = pd.read\_xml(file\_path, attrs\_only=True) df

</div>

<div class="ipython" data-suppress="">

python

os.remove("books.xml")

</div>

XML documents can have namespaces with prefixes and default namespaces without prefixes both of which are denoted with a special attribute `xmlns`. In order to parse by node under a namespace context, `xpath` must reference a prefix.

For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`. In order to parse `doc:row` nodes, `namespaces` must be used.

<div class="ipython">

python

xml = """\<?xml version='1.0' encoding='utf-8'?\> \<doc:data xmlns:doc="<https://example.com>"\> \<doc:row\> \<doc:shape\>square\</doc:shape\> \<doc:degrees\>360\</doc:degrees\> \<doc:sides\>4.0\</doc:sides\> \</doc:row\> \<doc:row\> \<doc:shape\>circle\</doc:shape\> \<doc:degrees\>360\</doc:degrees\> \<doc:sides/\> \</doc:row\> \<doc:row\> \<doc:shape\>triangle\</doc:shape\> \<doc:degrees\>180\</doc:degrees\> \<doc:sides\>3.0\</doc:sides\> \</doc:row\> \</doc:data\>"""

  - df = pd.read\_xml(StringIO(xml),  
    xpath="//doc:row", namespaces={"doc": "<https://example.com>"})

df

</div>

Similarly, an XML document can have a default namespace without prefix. Failing to assign a temporary prefix will return no nodes and raise a `ValueError`. But assigning *any* temporary name to correct URI allows parsing by nodes.

<div class="ipython">

python

xml = """\<?xml version='1.0' encoding='utf-8'?\> \<data xmlns="<https://example.com>"\> \<row\> \<shape\>square\</shape\> \<degrees\>360\</degrees\> \<sides\>4.0\</sides\> \</row\> \<row\> \<shape\>circle\</shape\> \<degrees\>360\</degrees\> \<sides/\> \</row\> \<row\> \<shape\>triangle\</shape\> \<degrees\>180\</degrees\> \<sides\>3.0\</sides\> \</row\> \</data\>"""

  - df = pd.read\_xml(StringIO(xml),  
    xpath="//pandas:row", namespaces={"pandas": "<https://example.com>"})

df

</div>

However, if XPath does not reference node names such as default, `/*`, then `namespaces` is not required.

\> **Note** \> Since `xpath` identifies the parent of content to be parsed, only immediate descendants which include child nodes or current attributes are parsed. Therefore, `read_xml` will not parse the text of grandchildren or other descendants and will not parse attributes of any descendant. To retrieve lower level content, adjust xpath to lower level. For example,

> 
> 
> <div class="ipython" data-okwarning="">
> 
> python
> 
> xml = """ \<data\> \<row\> \<shape sides="4"\>square\</shape\> \<degrees\>360\</degrees\> \</row\> \<row\> \<shape sides="0"\>circle\</shape\> \<degrees\>360\</degrees\> \</row\> \<row\> \<shape sides="3"\>triangle\</shape\> \<degrees\>180\</degrees\> \</row\> \</data\>"""
> 
> df = pd.read\_xml(StringIO(xml), xpath="./row") df
> 
> </div>
> 
> shows the attribute `sides` on `shape` element was not parsed as expected since this attribute resides on the child of `row` element and not `row` element itself. In other words, `sides` attribute is a grandchild level descendant of `row` element. However, the `xpath` targets `row` element which covers only its children and attributes.

With [lxml](https://lxml.de) as parser, you can flatten nested XML documents with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/) is a special-purpose language written in a special XML file that can transform original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using an XSLT processor.

For example, consider this somewhat nested structure of Chicago "L" Rides where station and rides elements encapsulate data in their own sections. With below XSLT, `lxml` can transform original nested document into a flatter output (as shown below for demonstration) for easier parse into `DataFrame`:

<div class="ipython">

python

  - xml = """\<?xml version='1.0' encoding='utf-8'?\>
    
      - \<response\>
        
          - \<row\>  
            \<station id="40850" name="Library"/\> \<month\>2020-09-01T00:00:00\</month\> \<rides\> \<avg\_weekday\_rides\>864.2\</avg\_weekday\_rides\> \<avg\_saturday\_rides\>534\</avg\_saturday\_rides\> \<avg\_sunday\_holiday\_rides\>417.2\</avg\_sunday\_holiday\_rides\> \</rides\>
        
        \</row\> \<row\> \<station id="41700" name="Washington/Wabash"/\> \<month\>2020-09-01T00:00:00\</month\> \<rides\> \<avg\_weekday\_rides\>2707.4\</avg\_weekday\_rides\> \<avg\_saturday\_rides\>1909.8\</avg\_saturday\_rides\> \<avg\_sunday\_holiday\_rides\>1438.6\</avg\_sunday\_holiday\_rides\> \</rides\> \</row\> \<row\> \<station id="40380" name="Clark/Lake"/\> \<month\>2020-09-01T00:00:00\</month\> \<rides\> \<avg\_weekday\_rides\>2949.6\</avg\_weekday\_rides\> \<avg\_saturday\_rides\>1657\</avg\_saturday\_rides\> \<avg\_sunday\_holiday\_rides\>1453.8\</avg\_sunday\_holiday\_rides\> \</rides\> \</row\>
    
    \</response\>"""

  - xsl = """\<xsl:stylesheet version="1.0" xmlns:xsl="<http://www.w3.org/1999/XSL/Transform>"\>  
    \<xsl:output method="xml" omit-xml-declaration="no" indent="yes"/\> \<xsl:strip-space elements="*"/\> \<xsl:template match="/response"\> \<xsl:copy\> \<xsl:apply-templates select="row"/\> \</xsl:copy\> \</xsl:template\> \<xsl:template match="row"\> \<xsl:copy\> \<station\_id\>\<xsl:value-of select="station/@id"/\>\</station\_id\> \<station\_name\>\<xsl:value-of select="station/@name"/\>\</station\_name\> \<xsl:copy-of select="month|rides/*"/\> \</xsl:copy\> \</xsl:template\> \</xsl:stylesheet\>"""

  - output = """\<?xml version='1.0' encoding='utf-8'?\>
    
      - \<response\>
        
          - \<row\>  
            \<station\_id\>40850\</station\_id\> \<station\_name\>Library\</station\_name\> \<month\>2020-09-01T00:00:00\</month\> \<avg\_weekday\_rides\>864.2\</avg\_weekday\_rides\> \<avg\_saturday\_rides\>534\</avg\_saturday\_rides\> \<avg\_sunday\_holiday\_rides\>417.2\</avg\_sunday\_holiday\_rides\>
        
        \</row\> \<row\> \<station\_id\>41700\</station\_id\> \<station\_name\>Washington/Wabash\</station\_name\> \<month\>2020-09-01T00:00:00\</month\> \<avg\_weekday\_rides\>2707.4\</avg\_weekday\_rides\> \<avg\_saturday\_rides\>1909.8\</avg\_saturday\_rides\> \<avg\_sunday\_holiday\_rides\>1438.6\</avg\_sunday\_holiday\_rides\> \</row\> \<row\> \<station\_id\>40380\</station\_id\> \<station\_name\>Clark/Lake\</station\_name\> \<month\>2020-09-01T00:00:00\</month\> \<avg\_weekday\_rides\>2949.6\</avg\_weekday\_rides\> \<avg\_saturday\_rides\>1657\</avg\_saturday\_rides\> \<avg\_sunday\_holiday\_rides\>1453.8\</avg\_sunday\_holiday\_rides\> \</row\>
    
    \</response\>"""

df = pd.read\_xml(StringIO(xml), stylesheet=StringIO(xsl)) df

</div>

For very large XML files that can range in hundreds of megabytes to gigabytes, <span class="title-ref">pandas.read\_xml</span> supports parsing such sizeable files using [lxml's iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk) and [etree's iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse) which are memory-efficient methods to iterate through an XML tree and extract specific elements and attributes. without holding entire tree in memory.

<div class="versionadded">

1.5.0

</div>

To use this feature, you must pass a physical XML file path into `read_xml` and use the `iterparse` argument. Files should not be compressed or point to online sources but stored on local disk. Also, `iterparse` should be a dictionary where the key is the repeating nodes in document (which become the rows) and the value is a list of any element or attribute that is a descendant (i.e., child, grandchild) of repeating node. Since XPath is not used in this method, descendants do not need to share same relationship with one another. Below shows example of reading in Wikipedia's very large (12 GB+) latest article data dump.

`` `ipython     In [1]: df = pd.read_xml(     ...         "/path/to/downloaded/enwikisource-latest-pages-articles.xml",     ...         iterparse = {"page": ["title", "ns", "id"]}     ...     )     ...     df     Out[2]:                                                          title   ns        id     0                                       Gettysburg Address    0     21450     1                                                Main Page    0     42950     2                            Declaration by United Nations    0      8435     3             Constitution of the United States of America    0      8435     4                     Declaration of Independence (Israel)    0     17858     ...                                                    ...  ...       ...     3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649     3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649     3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649     3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291     3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450      [3578765 rows x 3 columns]  .. _io.xml:  Writing XML ``\` '''''''''''

<div class="versionadded">

1.3.0

</div>

`DataFrame` objects have an instance method `to_xml` which renders the contents of the `DataFrame` as an XML document.

\> **Note** \> This method does not support special properties of XML including DTD, CData, XSD schemas, processing instructions, comments, and others. Only namespaces at the root level is supported. However, `stylesheet` allows design changes after initial output.

Let's look at a few examples.

Write an XML without options:

<div class="ipython">

python

  - geom\_df = pd.DataFrame(
    
      - {  
        "shape": \["square", "circle", "triangle"\], "degrees": \[360, 360, 180\], "sides": \[4, np.nan, 3\],
    
    }

)

print(geom\_df.to\_xml())

</div>

Write an XML with new root and row name:

<div class="ipython">

python

print(geom\_df.to\_xml(root\_name="geometry", row\_name="objects"))

</div>

Write an attribute-centric XML:

<div class="ipython">

python

print(geom\_df.to\_xml(attr\_cols=geom\_df.columns.tolist()))

</div>

Write a mix of elements and attributes:

<div class="ipython">

python

  - print(
    
      - geom\_df.to\_xml(  
        index=False, attr\_cols=\['shape'\], elem\_cols=\['degrees', 'sides'\])

)

</div>

Any `DataFrames` with hierarchical columns will be flattened for XML element names with levels delimited by underscores:

<div class="ipython">

python

  - ext\_geom\_df = pd.DataFrame(
    
      - {  
        "type": \["polygon", "other", "polygon"\], "shape": \["square", "circle", "triangle"\], "degrees": \[360, 360, 180\], "sides": \[4, np.nan, 3\],
    
    }

)

  - pvt\_df = ext\_geom\_df.pivot\_table(index='shape',  
    columns='type', values=\['degrees', 'sides'\], aggfunc='sum')

pvt\_df

print(pvt\_df.to\_xml())

</div>

Write an XML with default namespace:

<div class="ipython">

python

print(geom\_df.to\_xml(namespaces={"": "<https://example.com>"}))

</div>

Write an XML with namespace prefix:

<div class="ipython">

python

  - print(
    
      - geom\_df.to\_xml(namespaces={"doc": "<https://example.com>"},  
        prefix="doc")

)

</div>

Write an XML without declaration or pretty print:

<div class="ipython">

python

  - print(
    
      - geom\_df.to\_xml(xml\_declaration=False,  
        pretty\_print=False)

)

</div>

Write an XML and transform with stylesheet:

<div class="ipython">

python

  - xsl = """\<xsl:stylesheet version="1.0" xmlns:xsl="<http://www.w3.org/1999/XSL/Transform>"\>  
    \<xsl:output method="xml" omit-xml-declaration="no" indent="yes"/\> \<xsl:strip-space elements="\*"/\> \<xsl:template match="/data"\> \<geometry\> \<xsl:apply-templates select="row"/\> \</geometry\> \</xsl:template\> \<xsl:template match="row"\> \<object index="{index}"\> \<xsl:if test="shape\!='circle'"\> \<xsl:attribute name="type"\>polygon\</xsl:attribute\> \</xsl:if\> \<xsl:copy-of select="shape"/\> \<property\> \<xsl:copy-of select="degrees|sides"/\> \</property\> \</object\> \</xsl:template\> \</xsl:stylesheet\>"""

print(geom\_df.to\_xml(stylesheet=StringIO(xsl)))

</div>

### XML Final Notes

  - All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/). Both `etree` and `lxml` parsers will fail to parse any markup document that is not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document unless it follows XHTML specs. However, other popular markup types including KML, XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  - For above reason, if your application builds XML prior to pandas operations, use appropriate DOM libraries like `etree` and `lxml` to build the necessary document and not by string concatenation or regex adjustments. Always remember XML is a *special* text file with markup rules.
  - With very large XML files (several hundred MBs to GBs), XPath and XSLT can become memory-intensive operations. Be sure to have enough available RAM for reading and writing to large XML files (roughly about 5 times the size of text).
  - Because XSLT is a programming language, use it with caution since such scripts can pose a security risk in your environment and can run large or infinite recursive operations. Always test scripts on small fragments before full run.
  - The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser supports all functionality of both `read_xml` and `to_xml` except for complex XPath and any XSLT. Though limited in features, `etree` is still a reliable and capable parser and tree builder. Its performance may trail `lxml` to a certain degree for larger files but relatively unnoticeable on small to medium size files.

## Excel files

The <span class="title-ref">\~pandas.read\_excel</span> method can read Excel 2007+ (`.xlsx`) files using the `openpyxl` Python module. Excel 2003 (`.xls`) files can be read using `xlrd`. Binary Excel (`.xlsb`) files can be read using `pyxlsb`. All formats can be read using \[calamine\<io.calamine\>\](\#calamine\<io.calamine\>) engine. The <span class="title-ref">\~DataFrame.to\_excel</span> instance method is used for saving a `DataFrame` to Excel. Generally the semantics are similar to working with \[csv\<io.read\_csv\_table\>\](\#csv\<io.read\_csv\_table\>) data. See the \[cookbook\<cookbook.excel\>\](\#cookbook\<cookbook.excel\>) for some advanced strategies.

\> **Note** \> When `engine=None`, the following logic will be used to determine the engine:

>   - If `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then [odf](https://pypi.org/project/odfpy/) will be used.
>   - Otherwise if `path_or_buffer` is an xls format, `xlrd` will be used.
>   - Otherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.
>   - Otherwise `openpyxl` will be used.

### Reading Excel files

In the most basic use-case, `read_excel` takes a path to an Excel file, and the `sheet_name` indicating which sheet to parse.

When using the `engine_kwargs` parameter, pandas will pass these arguments to the engine. For this, it is important to know which function pandas is using internally.

  - For the engine openpyxl, pandas is using <span class="title-ref">openpyxl.load\_workbook</span> to read in (`.xlsx`) and (`.xlsm`) files.
  - For the engine xlrd, pandas is using <span class="title-ref">xlrd.open\_workbook</span> to read in (`.xls`) files.
  - For the engine pyxlsb, pandas is using <span class="title-ref">pyxlsb.open\_workbook</span> to read in (`.xlsb`) files.
  - For the engine odf, pandas is using <span class="title-ref">odf.opendocument.load</span> to read in (`.ods`) files.
  - For the engine calamine, pandas is using <span class="title-ref">python\_calamine.load\_workbook</span> to read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.

`` `python    # Returns a DataFrame    pd.read_excel("path_to_file.xls", sheet_name="Sheet1")   .. _io.excel.excelfile_class: ``ExcelFile`class`\` +++++++++++++++++++

To facilitate working with multiple sheets from the same file, the `ExcelFile` class can be used to wrap the file and can be passed into `read_excel` There will be a performance benefit for reading multiple sheets as the file is read into memory only once.

`` `python    xlsx = pd.ExcelFile("path_to_file.xls")    df = pd.read_excel(xlsx, "Sheet1")  The ``ExcelFile`class can also be used as a context manager.  .. code-block:: python     with pd.ExcelFile("path_to_file.xls") as xls:        df1 = pd.read_excel(xls, "Sheet1")        df2 = pd.read_excel(xls, "Sheet2")  The`sheet\_names`property will generate`\` a list of the sheet names in the file.

The primary use-case for an `ExcelFile` is parsing multiple sheets with different parameters:

`` `python     data = {}     # For when Sheet1's format differs from Sheet2     with pd.ExcelFile("path_to_file.xls") as xls:         data["Sheet1"] = pd.read_excel(xls, "Sheet1", index_col=None, na_values=["NA"])         data["Sheet2"] = pd.read_excel(xls, "Sheet2", index_col=1)  Note that if the same parsing parameters are used for all sheets, a list ``<span class="title-ref"> of sheet names can simply be passed to </span><span class="title-ref">read\_excel</span>\` with no loss in performance.

`` `python     # using the ExcelFile class     data = {}     with pd.ExcelFile("path_to_file.xls") as xls:         data["Sheet1"] = pd.read_excel(xls, "Sheet1", index_col=None, na_values=["NA"])         data["Sheet2"] = pd.read_excel(xls, "Sheet2", index_col=None, na_values=["NA"])      # equivalent using the read_excel function     data = pd.read_excel(         "path_to_file.xls", ["Sheet1", "Sheet2"], index_col=None, na_values=["NA"]     ) ``ExcelFile`can also be called with a`xlrd.book.Book`object`<span class="title-ref"> as a parameter. This allows the user to control how the excel file is read. For example, sheets can be loaded on demand by calling </span><span class="title-ref">xlrd.open\_workbook()</span><span class="title-ref"> with </span><span class="title-ref">on\_demand=True</span>\`.

`` `python     import xlrd      xlrd_book = xlrd.open_workbook("path_to_file.xls", on_demand=True)     with pd.ExcelFile(xlrd_book) as xls:         df1 = pd.read_excel(xls, "Sheet1")         df2 = pd.read_excel(xls, "Sheet2")  .. _io.excel.specifying_sheets:  Specifying sheets ``\` +++++++++++++++++

<div class="note">

<div class="title">

Note

</div>

The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.

</div>

<div class="note">

<div class="title">

Note

</div>

An ExcelFile's attribute `sheet_names` provides access to a list of sheets.

</div>

  - The arguments `sheet_name` allows specifying the sheet or sheets to read.
  - The default value for `sheet_name` is 0, indicating to read the first sheet
  - Pass a string to refer to the name of a particular sheet in the workbook.
  - Pass an integer to refer to the index of a sheet. Indices follow Python convention, beginning at 0.
  - Pass a list of either strings or integers, to return a dictionary of specified sheets.
  - Pass a `None` to return a dictionary of all available sheets.

`` `python    # Returns a DataFrame    pd.read_excel("path_to_file.xls", "Sheet1", index_col=None, na_values=["NA"])  Using the sheet index:  .. code-block:: python     # Returns a DataFrame    pd.read_excel("path_to_file.xls", 0, index_col=None, na_values=["NA"])  Using all default values:  .. code-block:: python     # Returns a DataFrame    pd.read_excel("path_to_file.xls")  Using None to get all sheets:  .. code-block:: python     # Returns a dictionary of DataFrames    pd.read_excel("path_to_file.xls", sheet_name=None)  Using a list to get multiple sheets:  .. code-block:: python     # Returns the 1st and 4th sheet, as a dictionary of DataFrames.    pd.read_excel("path_to_file.xls", sheet_name=["Sheet1", 3]) ``read\_excel`can read more than one sheet, by setting`sheet\_name`to either`<span class="title-ref"> a list of sheet names, a list of sheet positions, or </span><span class="title-ref">None</span>\` to read all sheets. Sheets can be specified by sheet index or sheet name, using an integer or string, respectively.

#### Reading a `MultiIndex`

`read_excel` can read a `MultiIndex` index, by passing a list of columns to `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If either the `index` or `columns` have serialized level names those will be read in as well by specifying the rows/columns that make up the levels.

For example, to read in a `MultiIndex` index without names:

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"a": \[1, 2, 3, 4\], "b": \[5, 6, 7, 8\]}, index=pd.MultiIndex.from\_product(\[\["a", "b"\], \["c", "d"\]\]),

) df.to\_excel("path\_to\_file.xlsx") df = pd.read\_excel("path\_to\_file.xlsx", index\_col=\[0, 1\]) df

</div>

If the index has level names, they will be parsed as well, using the same parameters.

<div class="ipython">

python

df.index = df.index.set\_names(\["lvl1", "lvl2"\]) df.to\_excel("path\_to\_file.xlsx") df = pd.read\_excel("path\_to\_file.xlsx", index\_col=\[0, 1\]) df

</div>

If the source file has both `MultiIndex` index and columns, lists specifying each should be passed to `index_col` and `header`:

<div class="ipython">

python

df.columns = pd.MultiIndex.from\_product(\[\["a"\], \["b", "d"\]\], names=\["c1", "c2"\]) df.to\_excel("path\_to\_file.xlsx") df = pd.read\_excel("path\_to\_file.xlsx", index\_col=\[0, 1\], header=\[0, 1\]) df

</div>

<div class="ipython" data-suppress="">

python

os.remove("path\_to\_file.xlsx")

</div>

Missing values in columns specified in `index_col` will be forward filled to allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward filling the missing values use `set_index` after reading the data instead of `index_col`.

#### Parsing specific columns

It is often the case that users will insert columns to do temporary computations in Excel and you may not want to read in those columns. `read_excel` takes a `usecols` keyword to allow you to specify a subset of columns to parse.

You can specify a comma-delimited set of Excel columns and ranges as a string:

`` `python    pd.read_excel("path_to_file.xls", "Sheet1", usecols="A,C:E")  If ``usecols`is a list of integers, then it is assumed to be the file column`\` indices to be parsed.

`` `python    pd.read_excel("path_to_file.xls", "Sheet1", usecols=[0, 2, 3])  Element order is ignored, so ``usecols=\[0, 1\]`is the same as`\[1, 0\]`.  If`usecols`is a list of strings, it is assumed that each string corresponds`<span class="title-ref"> to a column name provided either by the user in </span><span class="title-ref">names</span>\` or inferred from the document header row(s). Those strings define which columns will be parsed:

`` `python     pd.read_excel("path_to_file.xls", "Sheet1", usecols=["foo", "bar"])  Element order is ignored, so ``usecols=\['baz', 'joe'\]`is the same as`\['joe', 'baz'\]`.  If`usecols`is callable, the callable function will be evaluated against`<span class="title-ref"> the column names, returning names where the callable function evaluates to </span><span class="title-ref">True</span>\`.

`` `python     pd.read_excel("path_to_file.xls", "Sheet1", usecols=lambda x: x.isalpha())  Parsing dates ``\` +++++++++++++

Datetime-like values are normally automatically converted to the appropriate dtype when reading the excel file. But if you have a column of strings that *look* like dates (but are not actually formatted as dates in excel), you can use the `parse_dates` keyword to parse those strings to datetimes:

`` `python    pd.read_excel("path_to_file.xls", "Sheet1", parse_dates=["date_strings"])   Cell converters ``\` +++++++++++++++

It is possible to transform the contents of Excel cells via the `converters` option. For instance, to convert a column to boolean:

`` `python    pd.read_excel("path_to_file.xls", "Sheet1", converters={"MyBools": bool})  This options handles missing values and treats exceptions in the converters ``\` as missing data. Transformations are applied cell by cell rather than to the column as a whole, so the array dtype is not guaranteed. For instance, a column of integers with missing values cannot be transformed to an array with integer dtype, because NaN is strictly a float. You can manually mask missing data to recover integer dtype:

`` `python    def cfun(x):        return int(x) if x else -1      pd.read_excel("path_to_file.xls", "Sheet1", converters={"MyInts": cfun})  Dtype specifications ``\` ++++++++++++++++++++

As an alternative to converters, the type for an entire column can be specified using the `dtype` keyword, which takes a dictionary mapping column names to types. To interpret data with no type inference, use the type `str` or `object`.

`` `python    pd.read_excel("path_to_file.xls", dtype={"MyInts": "int64", "MyText": str})  .. _io.excel_writer:  Writing Excel files ``\` '''''''''''''''''''

#### Writing Excel files to disk

To write a `DataFrame` object to a sheet of an Excel file, you can use the `to_excel` instance method. The arguments are largely the same as `to_csv` described above, the first argument being the name of the excel file, and the optional second argument the name of the sheet to which the `DataFrame` should be written. For example:

`` `python    df.to_excel("path_to_file.xlsx", sheet_name="Sheet1")  Files with a ``<span class="title-ref"> </span><span class="title-ref">.xlsx</span><span class="title-ref"> extension will be written using </span><span class="title-ref">xlsxwriter</span><span class="title-ref"> (if available) or </span><span class="title-ref">openpyxl</span>\`.

The `DataFrame` will be written in a way that tries to mimic the REPL output. The `index_label` will be placed in the second row instead of the first. You can place it in the first row by setting the `merge_cells` option in `to_excel()` to `False`:

`` `python    df.to_excel("path_to_file.xlsx", index_label="label", merge_cells=False)  In order to write separate ``DataFrames`to separate sheets in a single Excel file,`<span class="title-ref"> one can pass an </span>\~pandas.io.excel.ExcelWriter\`.

`` `python    with pd.ExcelWriter("path_to_file.xlsx") as writer:        df1.to_excel(writer, sheet_name="Sheet1")        df2.to_excel(writer, sheet_name="Sheet2")  .. _io.excel_writing_buffer:  When using the ``engine\_kwargs`parameter, pandas will pass these arguments to the`\` engine. For this, it is important to know which function pandas is using internally.

  - For the engine openpyxl, pandas is using <span class="title-ref">openpyxl.Workbook</span> to create a new sheet and <span class="title-ref">openpyxl.load\_workbook</span> to append data to an existing sheet. The openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  - For the engine xlsxwriter, pandas is using <span class="title-ref">xlsxwriter.Workbook</span> to write to (`.xlsx`) files.
  - For the engine odf, pandas is using <span class="title-ref">odf.opendocument.OpenDocumentSpreadsheet</span> to write to (`.ods`) files.

#### Writing Excel files to memory

pandas supports writing Excel files to buffer-like objects such as `StringIO` or `BytesIO` using <span class="title-ref">\~pandas.io.excel.ExcelWriter</span>.

`` `python    from io import BytesIO     bio = BytesIO()     # By setting the 'engine' in the ExcelWriter constructor.    writer = pd.ExcelWriter(bio, engine="xlsxwriter")    df.to_excel(writer, sheet_name="Sheet1")     # Save the workbook    writer.save()     # Seek to the beginning and read to copy the workbook to a variable in memory    bio.seek(0)    workbook = bio.read()  > **Note** > ``engine`is optional but recommended.  Setting the engine determines     the version of workbook produced. Setting`engine='xlrd'`will produce an     Excel 2003-format workbook (xls).  Using either`'openpyxl'`or`'xlsxwriter'`will produce an Excel 2007-format workbook (xlsx). If     omitted, an Excel 2007-formatted workbook is produced.   .. _io.excel.writers:  Excel writer engines`\` ''''''''''''''''''''

pandas chooses an Excel writer via two methods:

1.  the `engine` keyword argument
2.  the filename extension (via the default specified in config options)

By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io) for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you have multiple engines installed, you can set the default engine through \[setting the config options \<options\>\](\#setting-the config-options-\<options\>) `io.excel.xlsx.writer` and `io.excel.xls.writer`. pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx` files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.

To specify which writer you want to use, you can pass an engine keyword argument to `to_excel` and to `ExcelWriter`. The built-in engines are:

  - `openpyxl`: version 2.4 or higher is required
  - `xlsxwriter`

`` `python    # By setting the 'engine' in the DataFrame 'to_excel()' methods.    df.to_excel("path_to_file.xlsx", sheet_name="Sheet1", engine="xlsxwriter")     # By setting the 'engine' in the ExcelWriter constructor.    writer = pd.ExcelWriter("path_to_file.xlsx", engine="xlsxwriter")     # Or via pandas configuration.    from pandas import options  # noqa: E402     options.io.excel.xlsx.writer = "xlsxwriter"     df.to_excel("path_to_file.xlsx", sheet_name="Sheet1")  .. _io.excel.style:  Style and formatting ``\` ''''''''''''''''''''

The look and feel of Excel worksheets created from pandas can be modified using the following parameters on the `DataFrame`'s `to_excel` method.

  - `float_format` : Format string for floating point numbers (default `None`).
  - `freeze_panes` : A tuple of two integers representing the bottommost row and rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will freeze the first row and first column (default `None`).

\> **Note** \> As of pandas 3.0, by default spreadsheets created with the `to_excel` method will not contain any styling. Users wishing to bold text, add bordered styles, etc in a worksheet output by `to_excel` can do so by using <span class="title-ref">Styler.to\_excel</span> to create styled excel files. For documentation on styling spreadsheets, see [here](https://pandas.pydata.org/docs/user_guide/style.html#Export-to-Excel).

`` `python     css = "border: 1px solid black; font-weight: bold;"     df.style.map_index(lambda x: css).map_index(lambda x: css, axis=1).to_excel("myfile.xlsx")  Using the `Xlsxwriter`_ engine provides many options for controlling the ``<span class="title-ref"> format of an Excel worksheet created with the </span><span class="title-ref">to\_excel</span><span class="title-ref"> method. Excellent examples can be found in the \`Xlsxwriter</span>\_ documentation here: <https://xlsxwriter.readthedocs.io/working_with_pandas.html>

## OpenDocument Spreadsheets

The io methods for [Excel files](#excel-files) also support reading and writing OpenDocument spreadsheets using the [odfpy](https://pypi.org/project/odfpy/) module. The semantics and features for reading and writing OpenDocument spreadsheets match what can be done for [Excel files](#excel-files) using `engine='odf'`. The optional dependency 'odfpy' needs to be installed.

The <span class="title-ref">\~pandas.read\_excel</span> method can read OpenDocument spreadsheets

`` `python    # Returns a DataFrame    pd.read_excel("path_to_file.ods", engine="odf")  Similarly, the `~pandas.to_excel` method can write OpenDocument spreadsheets  .. code-block:: python     # Writes DataFrame to a .ods file    df.to_excel("path_to_file.ods", engine="odf")  .. _io.xlsb:  Binary Excel (.xlsb) files ``\` --------------------------

The <span class="title-ref">\~pandas.read\_excel</span> method can also read binary Excel files using the `pyxlsb` module. The semantics and features for reading binary Excel files mostly match what can be done for [Excel files](#excel-files) using `engine='pyxlsb'`. `pyxlsb` does not recognize datetime types in files and will return floats instead (you can use \[calamine\<io.calamine\>\](\#calamine\<io.calamine\>) if you need recognize datetime types).

`` `python    # Returns a DataFrame    pd.read_excel("path_to_file.xlsb", engine="pyxlsb")  > **Note** >     Currently pandas only supports *reading* binary Excel files. Writing    is not implemented.  .. _io.calamine:  Calamine (Excel and ODS files) ``\` ------------------------------

The <span class="title-ref">\~pandas.read\_excel</span> method can read Excel file (`.xlsx`, `.xlsm`, `.xls`, `.xlsb`) and OpenDocument spreadsheets (`.ods`) using the `python-calamine` module. This module is a binding for Rust library [calamine](https://crates.io/crates/calamine) and is faster than other engines in most cases. The optional dependency 'python-calamine' needs to be installed.

`` `python    # Returns a DataFrame    pd.read_excel("path_to_file.xlsb", engine="calamine")  .. _io.clipboard:  Clipboard ``\` ---------

A handy way to grab data is to use the <span class="title-ref">\~DataFrame.read\_clipboard</span> method, which takes the contents of the clipboard buffer and passes them to the `read_csv` method. For instance, you can copy the following text to the clipboard (CTRL-C on many operating systems):

`` `console      A B C    x 1 4 p    y 2 5 q    z 3 6 r  And then import the data directly to a ``DataFrame`by calling:  .. code-block:: python      >>> clipdf = pd.read_clipboard()     >>> clipdf       A B C     x 1 4 p     y 2 5 q     z 3 6 r  The`to\_clipboard`method can be used to write the contents of a`DataFrame`to`<span class="title-ref"> the clipboard. Following which you can paste the clipboard contents into other applications (CTRL-V on many operating systems). Here we illustrate writing a </span><span class="title-ref">DataFrame</span>\` into clipboard and reading it back.

`` `python     >>> df = pd.DataFrame(     ...     {"A": [1, 2, 3], "B": [4, 5, 6], "C": ["p", "q", "r"]}, index=["x", "y", "z"]     ... )      >>> df       A B C     x 1 4 p     y 2 5 q     z 3 6 r     >>> df.to_clipboard()     >>> pd.read_clipboard()       A B C     x 1 4 p     y 2 5 q     z 3 6 r  We can see that we got the same content back, which we had earlier written to the clipboard.  > **Note** >     You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to use these methods.  .. _io.pickle:  Pickling ``\` --------

All pandas objects are equipped with `to_pickle` methods which use Python's `cPickle` module to save data structures to disk using the pickle format.

<div class="ipython">

python

df df.to\_pickle("foo.pkl")

</div>

The `read_pickle` function in the `pandas` namespace can be used to load any pickled pandas object (or any other pickled object) from file:

<div class="ipython">

python

pd.read\_pickle("foo.pkl")

</div>

<div class="ipython" data-suppress="">

python

os.remove("foo.pkl")

</div>

\> **Warning** \> Loading pickled data received from untrusted sources can be unsafe.

> See: <https://docs.python.org/3/library/pickle.html>

<div class="warning">

<div class="title">

Warning

</div>

<span class="title-ref">read\_pickle</span> is only guaranteed backwards compatible back to a few minor release.

</div>

### Compressed pickle files

<span class="title-ref">read\_pickle</span>, <span class="title-ref">DataFrame.to\_pickle</span> and <span class="title-ref">Series.to\_pickle</span> can read and write compressed pickle files. The compression types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The `zip` file format only supports reading and must contain only one data file to be read.

The compression type can be an explicit parameter or be inferred from the file extension. If 'infer', then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.

The compression parameter can also be a `dict` in order to pass options to the compression protocol. It must have a `'method'` key set to the name of the compression protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}. All other key-value pairs are passed to the underlying compression library.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": np.random.randn(1000), "B": "foo", "C": pd.date\_range("20130101", periods=1000, freq="s"),
    
    }

) df

</div>

Using an explicit compression type:

<div class="ipython">

python

df.to\_pickle("data.pkl.compress", compression="gzip") rt = pd.read\_pickle("data.pkl.compress", compression="gzip") rt

</div>

Inferring compression type from the extension:

<div class="ipython">

python

df.to\_pickle("data.pkl.xz", compression="infer") rt = pd.read\_pickle("data.pkl.xz", compression="infer") rt

</div>

The default is to 'infer':

<div class="ipython">

python

df.to\_pickle("data.pkl.gz") rt = pd.read\_pickle("data.pkl.gz") rt

df\["A"\].to\_pickle("s1.pkl.bz2") rt = pd.read\_pickle("s1.pkl.bz2") rt

</div>

Passing options to the compression protocol in order to speed up compression:

<div class="ipython">

python

df.to\_pickle("data.pkl.gz", compression={"method": "gzip", "compresslevel": 1})

</div>

<div class="ipython" data-suppress="">

python

os.remove("data.pkl.compress") os.remove("data.pkl.xz") os.remove("data.pkl.gz") os.remove("s1.pkl.bz2")

</div>

## msgpack

pandas support for `msgpack` has been removed in version 1.0.0. It is recommended to use \[pickle \<io.pickle\>\](\#pickle-\<io.pickle\>) instead.

Alternatively, you can also the Arrow IPC serialization format for on-the-wire transmission of pandas objects. For documentation on pyarrow, see [here](https://arrow.apache.org/docs/python/ipc.html).

## HDF5 (PyTables)

`HDFStore` is a dict-like object which reads and writes pandas using the high performance HDF5 format using the excellent [PyTables](https://www.pytables.org/) library. See the \[cookbook \<cookbook.hdf\>\](\#cookbook-\<cookbook.hdf\>) for some advanced strategies

\> **Warning** \> pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle. Loading pickled data received from untrusted sources can be unsafe.

> See: <https://docs.python.org/3/library/pickle.html> for more.

<div class="ipython" data-suppress="" data-okexcept="">

python

os.remove("store.h5")

</div>

<div class="ipython">

python

store = pd.HDFStore("store.h5") print(store)

</div>

Objects can be written to the file just like adding key-value pairs to a dict:

<div class="ipython">

python

index = pd.date\_range("1/1/2000", periods=8) s = pd.Series(np.random.randn(5), index=\["a", "b", "c", "d", "e"\]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=\["A", "B", "C"\])

\# store.put('s', s) is an equivalent method store\["s"\] = s

store\["df"\] = df

store

</div>

In a current or later Python session, you can retrieve stored objects:

<div class="ipython">

python

\# store.get('df') is an equivalent method store\["df"\]

\# dotted (attribute) access provides get as well store.df

</div>

Deletion of the object specified by the key:

<div class="ipython">

python

\# store.remove('df') is an equivalent method del store\["df"\]

store

</div>

Closing a Store and using a context manager:

<div class="ipython">

python

store.close() store store.is\_open

\# Working with, and automatically closing the store using a context manager with pd.HDFStore("store.h5") as store: store.keys()

</div>

<div class="ipython" data-suppress="">

python

store.close() os.remove("store.h5")

</div>

### Read/write API

`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf` for writing, similar to how `read_csv` and `to_csv` work.

<div class="ipython">

python

df\_tl = pd.DataFrame({"A": list(range(5)), "B": list(range(5))}) df\_tl.to\_hdf("store\_tl.h5", key="table", append=True) pd.read\_hdf("store\_tl.h5", "table", where=\["index\>2"\])

</div>

<div class="ipython" data-suppress="" data-okexcept="">

python

os.remove("store\_tl.h5")

</div>

HDFStore will by default not drop rows that are all missing. This behavior can be changed by setting `dropna=True`.

<div class="ipython">

python

  - df\_with\_missing = pd.DataFrame(
    
      - {  
        "col1": \[0, np.nan, 2\], "col2": \[1, np.nan, np.nan\],
    
    }

) df\_with\_missing

df\_with\_missing.to\_hdf("file.h5", key="df\_with\_missing", format="table", mode="w")

pd.read\_hdf("file.h5", "df\_with\_missing")

  - df\_with\_missing.to\_hdf(  
    "file.h5", key="df\_with\_missing", format="table", mode="w", dropna=True

) pd.read\_hdf("file.h5", "df\_with\_missing")

</div>

<div class="ipython" data-suppress="">

python

os.remove("file.h5")

</div>

### Fixed format

The examples above show storing using `put`, which write the HDF5 to `PyTables` in a fixed array format, called the `fixed` format. These types of stores are **not** appendable once written (though you can simply remove them and rewrite). Nor are they **queryable**; they must be retrieved in their entirety. They also do not support dataframes with non-unique column names. The `fixed` format stores offer very fast writing and slightly faster reading than `table` stores. This format is specified by default when using `put` or `to_hdf` or by `format='fixed'` or `format='f'`.

\> **Warning** \> A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:

> 
> 
> <div class="ipython" data-okexcept="">
> 
> python
> 
> pd.DataFrame(np.random.randn(10, 2)).to\_hdf("test\_fixed.h5", key="df") pd.read\_hdf("test\_fixed.h5", "df", where="index\>5")
> 
> </div>
> 
> <div class="ipython" data-suppress="">
> 
> python
> 
> os.remove("test\_fixed.h5")
> 
> </div>

### Table format

`HDFStore` supports another `PyTables` format on disk, the `table` format. Conceptually a `table` is shaped very much like a DataFrame, with rows and columns. A `table` may be appended to in the same or other sessions. In addition, delete and query type operations are supported. This format is specified by `format='table'` or `format='t'` to `append` or `put` or `to_hdf`.

This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')` to enable `put/append/to_hdf` to by default store in the `table` format.

<div class="ipython" data-suppress="" data-okexcept="">

python

os.remove("store.h5")

</div>

<div class="ipython">

python

store = pd.HDFStore("store.h5") df1 = df\[0:4\] df2 = df\[4:\]

\# append data (creates a table automatically) store.append("df", df1) store.append("df", df2) store

\# select the entire object store.select("df")

\# the type of stored data store.root.df.\_v\_attrs.pandas\_type

</div>

\> **Note** \> You can also create a `table` by passing `format='table'` or `format='t'` to a `put` operation.

### Hierarchical keys

Keys to a store can be specified as a string. These can be in a hierarchical path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the leading '/' and are **always** absolute (e.g. 'foo' refers to '/foo'). Removal operations can remove everything in the sub-store and **below**, so be *careful*.

<div class="ipython">

python

store.put("foo/bar/bah", df) store.append("food/orange", df) store.append("food/apple", df) store

\# a list of keys are returned store.keys()

\# remove all nodes under this level store.remove("food") store

</div>

You can walk through the group hierarchy using the `walk` method which will yield a tuple for each group key along with the relative keys of its contents.

<div class="ipython">

python

  - for (path, subgroups, subkeys) in store.walk():
    
      - for subgroup in subgroups:  
        print("GROUP: {}/{}".format(path, subgroup))
    
      - for subkey in subkeys:  
        key = "/".join(\[path, subkey\]) print("KEY: {}".format(key)) print(store.get(key))

</div>

\> **Warning** \> Hierarchical keys cannot be retrieved as dotted (attribute) access as described above for items stored under the root node.

> 
> 
> <div class="ipython" data-okexcept="">
> 
> python
> 
> store.foo.bar.bah
> 
> </div>
> 
> <div class="ipython">
> 
> python
> 
> \# you can directly access the actual PyTables node but using the root node store.root.foo.bar.bah
> 
> </div>
> 
> Instead, use explicit string based keys:
> 
> <div class="ipython">
> 
> python
> 
> store\["foo/bar/bah"\]
> 
> </div>

### Storing types

#### Storing mixed types in a table

Storing mixed-dtype data is supported. Strings are stored as a fixed-width using the maximum size of the appended column. Subsequent attempts at appending longer strings will raise a `ValueError`.

Passing ``min_itemsize={`values`: size}`` as a parameter to append will set a larger minimum for the string columns. Storing `floats, strings, ints, bools, datetime64` are currently supported. For string columns, passing `nan_rep = 'nan'` to append will change the default nan representation on disk (which converts to/from `np.nan`), this defaults to `nan`.

<div class="ipython">

python

  - df\_mixed = pd.DataFrame(
    
      - {  
        "A": np.random.randn(8), "B": np.random.randn(8), "C": np.array(np.random.randn(8), dtype="float32"), "string": "string", "int": 1, "bool": True, "datetime64": pd.Timestamp("20010102"),
    
    }, index=list(range(8)),

) df\_mixed.loc\[df\_mixed.index\[3:5\], \["A", "B", "string", "datetime64"\]\] = np.nan

store.append("df\_mixed", df\_mixed, min\_itemsize={"values": 50}) df\_mixed1 = store.select("df\_mixed") df\_mixed1 df\_mixed1.dtypes.value\_counts()

\# we have provided a minimum string column size store.root.df\_mixed.table

</div>

#### Storing MultiIndex DataFrames

Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting from homogeneous index `DataFrames`.

<div class="ipython">

python

  - index = pd.MultiIndex(  
    levels=\[\["foo", "bar", "baz", "qux"\], \["one", "two", "three"\]\], codes=\[\[0, 0, 0, 1, 1, 2, 2, 3, 3, 3\], \[0, 1, 2, 0, 1, 1, 2, 0, 1, 2\]\], names=\["foo", "bar"\],

) df\_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=\["A", "B", "C"\]) df\_mi

store.append("df\_mi", df\_mi) store.select("df\_mi")

\# the levels are automatically included as data columns store.select("df\_mi", "foo=bar")

</div>

<div class="note">

<div class="title">

Note

</div>

The `index` keyword is reserved and cannot be use as a level name.

</div>

### Querying

#### Querying a table

`select` and `delete` operations have an optional criterion that can be specified to select/delete only a subset of the data. This allows one to have a very large on-disk table and retrieve only a portion of the data.

A query is specified using the `Term` class under the hood, as a boolean expression.

  - `index` and `columns` are supported indexers of `DataFrames`.
  - if `data_columns` are specified, these can be used as additional indexers.
  - level name in a MultiIndex, with default name `level_0`, `level_1`, â€¦ if not provided.

Valid comparison operators are:

`=, ==, !=, >, >=, <, <=`

Valid boolean expressions are combined with:

  - `|` : or
  - `&` : and
  - `(` and `)` : for grouping

These rules are similar to how boolean expressions are used in pandas for indexing.

\> **Note** \> - `=` will be automatically expanded to the comparison operator `==` - `~` is the not operator, but can only be used in very limited circumstances - If a list/tuple of expressions is passed they will be combined via `&`

The following are valid expressions:

  - `'index >= date'`
  - `"columns = ['A', 'D']"`
  - `"columns in ['A', 'D']"`
  - `'columns = A'`
  - `'columns == A'`
  - `"~(columns = ['A', 'B'])"`
  - `'index > df.index[3] & string = "bar"'`
  - `'(index > df.index[3] & index <= df.index[6]) | string = "bar"'`
  - `"ts >= Timestamp('2012-02-01')"`
  - `"major_axis>=20130101"`

The `indexers` are on the left-hand side of the sub-expression:

`columns`, `major_axis`, `ts`

The right-hand side of the sub-expression (after a comparison operator) can be:

  - functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  - strings, e.g. `"bar"`
  - date-like, e.g. `20130101`, or `"20130101"`
  - lists, e.g. `"['A', 'B']"`
  - variables that are defined in the local names space, e.g. `date`

\> **Note** \> Passing a string to a query by interpolating it into the query expression is not recommended. Simply assign the string of interest to a variable and use that variable in an expression. For example, do this

> `` `python    string = "HolyMoly'"    store.select("df", "index == string")  instead of this  .. code-block:: python     string = "HolyMoly'"    store.select('df', f'index == {string}')  The latter will **not** work and will raise a ``SyntaxError`.Note that there's a single quote followed by a double quote in the`string`variable.  If you *must* interpolate, use the`'%r'`format specifier  .. code-block:: python     store.select("df", "index == %r" % string)  which will quote`string\`\`.

Here are some examples:

<div class="ipython">

python

  - dfq = pd.DataFrame(  
    np.random.randn(10, 4), columns=list("ABCD"), index=pd.date\_range("20130101", periods=10),

) store.append("dfq", dfq, format="table", data\_columns=True)

</div>

Use boolean expressions, with in-line function evaluation.

<div class="ipython">

python

store.select("dfq", "index\>pd.Timestamp('20130104') & columns=\['A', 'B'\]")

</div>

Use inline column reference.

<div class="ipython">

python

store.select("dfq", where="A\>0 or C\>0")

</div>

The `columns` keyword can be supplied to select a list of columns to be `` ` returned, this is equivalent to passing a ``'columns=list\_of\_columns\_to\_filter'`:  .. ipython:: python     store.select("df", "columns=['A', 'B']")`start`and`stop`parameters can be specified to limit the total search space. These are in terms of the total number of rows in a table.  > **Note** >`select`will raise a`ValueError`if the query expression has an unknown    variable reference. Usually this means that you are trying to select on a column    that is **not** a data_column.`select`will raise a`SyntaxError`if the query expression is not valid.   .. _io.hdf5-timedelta:  Query timedelta64[ns] +++++++++++++++++++++  You can store and query using the`timedelta64\[ns\]`type. Terms can be specified in the format:`\<float\>(\<unit\>)`, where float may be signed (and fractional), and unit can be`D,s,ms,us,ns`for the timedelta. Here's an example:  .. ipython:: python     from datetime import timedelta     dftd = pd.DataFrame(        {            "A": pd.Timestamp("20130101"),            "B": [                pd.Timestamp("20130101") + timedelta(days=i, seconds=10)                for i in range(10)            ],        }    )    dftd["C"] = dftd["A"] - dftd["B"]    dftd    store.append("dftd", dftd, data_columns=True)    store.select("dftd", "C<'-3.5D'")  .. _io.query_multi:  Query MultiIndex ++++++++++++++++  Selecting from a`MultiIndex`can be achieved by using the name of the level.  .. ipython:: python     df_mi.index.names    store.select("df_mi", "foo=baz and bar=two")  If the`MultiIndex`levels names are`None`, the levels are automatically made available via the`level\_n`keyword with`n`the level of the`MultiIndex`you want to select from.  .. ipython:: python     index = pd.MultiIndex(        levels=[["foo", "bar", "baz", "qux"], ["one", "two", "three"]],        codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],    )    df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=["A", "B", "C"])    df_mi_2     store.append("df_mi_2", df_mi_2)     # the levels are automatically included as data columns with keyword level_n    store.select("df_mi_2", "level_0=foo and level_1=two")   Indexing ++++++++  You can create/modify an index for a table with`create\_table\_index`after data is already in the table (after and`append/put`operation). Creating a table index is **highly** encouraged. This will speed your queries a great deal when you use a`select`with the indexed dimension as the`where`.  > **Note** >     Indexes are automagically created on the indexables    and any data columns you specify. This behavior can be turned off by passing`index=False`to`append``.  .. ipython:: python     # we have automagically already created an index (in the first section)    i = store.root.df.table.cols.index.index    i.optlevel, i.kind     # change an index by passing new parameters    store.create_table_index("df", optlevel=9, kind="full")    i = store.root.df.table.cols.index.index    i.optlevel, i.kind  Oftentimes when appending large amounts of data to a store, it is useful to turn off index creation for each append, then recreate at the end.  .. ipython:: python     df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list("AB"))    df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list("AB"))     st = pd.HDFStore("appends.h5", mode="w")    st.append("df", df_1, data_columns=["B"], index=False)    st.append("df", df_2, data_columns=["B"], index=False)    st.get_storer("df").table  Then create the index when finished appending.  .. ipython:: python     st.create_table_index("df", columns=["B"], optlevel=9, kind="full")    st.get_storer("df").table     st.close()  .. ipython:: python    :suppress:    :okexcept:     os.remove("appends.h5")  See `here <https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index>`__ for how to create a completely-sorted-index (CSI) on an existing store.  .. _io.hdf5-query-data-columns:  Query via data columns ++++++++++++++++++++++  You can designate (and index) certain columns that you want to be able to perform queries (other than the``indexable`columns, which you can always query). For instance say you want to perform this common operation, on-disk, and return just the frame that matches this query. You can specify`data\_columns = True`to force all columns to be`data\_columns`.  .. ipython:: python     df_dc = df.copy()    df_dc["string"] = "foo"    df_dc.loc[df_dc.index[4:6], "string"] = np.nan    df_dc.loc[df_dc.index[7:9], "string"] = "bar"    df_dc["string2"] = "cool"    df_dc.loc[df_dc.index[1:3], ["B", "C"]] = 1.0    df_dc     # on-disk operations    store.append("df_dc", df_dc, data_columns=["B", "C", "string", "string2"])    store.select("df_dc", where="B > 0")     # getting creative    store.select("df_dc", "B > 0 & C > 0 & string == foo")     # this is in-memory version of this type of selection    df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == "foo")]     # we have automagically created this index and the B/C/string/string2    # columns are stored separately as`PyTables`columns    store.root.df_dc.table  There is some performance degradation by making lots of columns into`data columns`, so it is up to the user to designate these. In addition, you cannot change data columns (nor indexables) after the first append/put operation (Of course you can simply read in the data and create a new table!).  Iterator ++++++++  You can pass`iterator=True`or`chunksize=number\_in\_a\_chunk`to`select`and`select\_as\_multiple`to return an iterator on the results. The default is 50,000 rows returned in a chunk.  .. ipython:: python     for df in store.select("df", chunksize=3):        print(df)  > **Note** >     You can also use the iterator with`read\_hdf`which will open, then    automatically close the store when finished iterating.`\`python for df in pd.read\_hdf("store.h5", "df", chunksize=3): print(df)

Note, that the chunksize keyword applies to the **source** rows. So if you `` ` are doing a query, then the chunksize will subdivide the total rows in the table and the query applied, returning an iterator on potentially unequal sized chunks.  Here is a recipe for generating a query and using it to create equal sized return chunks.  .. ipython:: python     dfeq = pd.DataFrame({"number": np.arange(1, 11)})    dfeq     store.append("dfeq", dfeq, data_columns=["number"])     def chunks(l, n):        return [l[i: i + n] for i in range(0, len(l), n)]     evens = [2, 4, 6, 8, 10]    coordinates = store.select_as_coordinates("dfeq", "number=evens")    for c in chunks(coordinates, 2):        print(store.select("dfeq", where=c))  Advanced queries ++++++++++++++++  Select a single column ^^^^^^^^^^^^^^^^^^^^^^  To retrieve a single indexable or data column, use the method ``select\_column`. This will, for example, enable you to get the index very quickly. These return a`Series`of the result, indexed by the row number. These do not currently accept the`where`selector.  .. ipython:: python     store.select_column("df_dc", "index")    store.select_column("df_dc", "string")  .. _io.hdf5-selecting_coordinates:  Selecting coordinates ^^^^^^^^^^^^^^^^^^^^^  Sometimes you want to get the coordinates (a.k.a the index locations) of your query. This returns an`Index`of the resulting locations. These coordinates can also be passed to subsequent`where`operations.  .. ipython:: python     df_coord = pd.DataFrame(        np.random.randn(1000, 2), index=pd.date_range("20000101", periods=1000)    )    store.append("df_coord", df_coord)    c = store.select_as_coordinates("df_coord", "index > 20020101")    c    store.select("df_coord", where=c)  .. _io.hdf5-where_mask:  Selecting using a where mask ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Sometime your query can involve creating a list of rows to select. Usually this`mask`would be a resulting`index`from an indexing operation. This example selects the months of a datetimeindex which are 5.  .. ipython:: python     df_mask = pd.DataFrame(        np.random.randn(1000, 2), index=pd.date_range("20000101", periods=1000)    )    store.append("df_mask", df_mask)    c = store.select_column("df_mask", "index")    where = c[pd.DatetimeIndex(c).month == 5].index    store.select("df_mask", where=where)  Storer object ^^^^^^^^^^^^^  If you want to inspect the stored object, retrieve via`get\_storer`. You could use this programmatically to say get the number of rows in an object.  .. ipython:: python     store.get_storer("df_dc").nrows   Multiple table queries ++++++++++++++++++++++  The methods`append\_to\_multiple`and`select\_as\_multiple`can perform appending/selecting from multiple tables at once. The idea is to have one table (call it the selector table) that you index most/all of the columns, and perform your queries. The other table(s) are data tables with an index matching the selector table's index. You can then perform a very fast query on the selector table, yet get lots of data back. This method is similar to having a very wide table, but enables more efficient queries.  The`append\_to\_multiple`method splits a given single DataFrame into multiple tables according to`d`, a dictionary that maps the table names to a list of 'columns' you want in that table. If`None`is used in place of a list, that table will have the remaining unspecified columns of the given DataFrame. The argument`selector`defines which table is the selector table (which you can make queries from). The argument`dropna`will drop rows from the input`DataFrame`to ensure tables are synchronized.  This means that if a row for one of the tables being written to is entirely`np.nan`, that row will be dropped from all tables.  If`dropna`is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**. Remember that entirely`np.Nan`rows are not written to the HDFStore, so if you choose to call`dropna=False`, some tables may have more rows than others, and therefore`select\_as\_multiple`may not work or it may return unexpected results.  .. ipython:: python     df_mt = pd.DataFrame(        np.random.randn(8, 6),        index=pd.date_range("1/1/2000", periods=8),        columns=["A", "B", "C", "D", "E", "F"],    )    df_mt["foo"] = "bar"    df_mt.loc[df_mt.index[1], ("A", "B")] = np.nan     # you can also create the tables individually    store.append_to_multiple(        {"df1_mt": ["A", "B"], "df2_mt": None}, df_mt, selector="df1_mt"    )    store     # individual tables were created    store.select("df1_mt")    store.select("df2_mt")     # as a multiple    store.select_as_multiple(        ["df1_mt", "df2_mt"],        where=["A>0", "B>0"],        selector="df1_mt",    )   Delete from a table '''''''''''''''''''  You can delete from a table selectively by specifying a`where`. In deleting rows, it is important to understand the`PyTables`deletes rows by erasing the rows, then **moving** the following data. Thus deleting can potentially be a very expensive operation depending on the orientation of your data. To get optimal performance, it's worthwhile to have the dimension you are deleting be the first of the`indexables`.  Data is ordered (on the disk) in terms of the`indexables`. Here's a simple use case. You store panel-type data, with dates in the`major\_axis`and ids in the`minor\_axis`. The data is then interleaved like this:  * date_1     * id_1     * id_2     *  .     * id_n * date_2     * id_1     *  .     * id_n  It should be clear that a delete operation on the`major\_axis`will be fairly quick, as one chunk is removed, then the following data moved. On the other hand a delete operation on the`minor\_axis`will be very expensive. In this case it would almost certainly be faster to rewrite the table using a`where`that selects all but the missing data.  > **Warning** >     Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files    automatically. Thus, repeatedly deleting (or removing nodes) and adding    again, **WILL TEND TO INCREASE THE FILE SIZE**.     To *repack and clean* the file, use [ptrepack <io.hdf5-ptrepack>](#ptrepack-<io.hdf5-ptrepack>).  .. _io.hdf5-notes:  Notes & caveats '''''''''''''''   Compression +++++++++++`PyTables`allows the stored data to be compressed. This applies to all kinds of stores, not just tables. Two parameters are used to control compression:`complevel`and`complib`.  *`complevel`specifies if and how hard data is to be compressed.`complevel=0`and`complevel=None`disables compression and`0\<complevel\<10`enables compression.  *`complib`specifies which compression library to use.   If nothing is  specified the default library`zlib``is used. A   compression library usually optimizes for either good compression rates   or speed and the results will depend on the type of data. Which type of   compression to choose depends on your specific needs and data. The list   of supported compression libraries:    - `zlib <https://zlib.net/>`_: The default compression library.     A classic in terms of compression, achieves good compression     rates but is somewhat slow.   - `lzo <https://www.oberhumer.com/opensource/lzo/>`_: Fast     compression and decompression.   - `bzip2 <https://sourceware.org/bzip2/>`_: Good compression rates.   - `blosc <https://www.blosc.org/>`_: Fast compression and     decompression.      Support for alternative blosc compressors:      - `blosc:blosclz <https://www.blosc.org/>`_ This is the       default compressor for``blosc``- `blosc:lz4       <https://fastcompression.blogspot.com/p/lz4.html>`_:       A compact, very popular and fast compressor.     - `blosc:lz4hc       <https://fastcompression.blogspot.com/p/lz4.html>`_:       A tweaked version of LZ4, produces better       compression ratios at the expense of speed.     - `blosc:snappy <https://google.github.io/snappy/>`_:       A popular compressor used in many places.     - `blosc:zlib <https://zlib.net/>`_: A classic;       somewhat slower than the previous ones, but       achieving better compression ratios.     - `blosc:zstd <https://facebook.github.io/zstd/>`_: An       extremely well balanced codec; it provides the best       compression ratios among the others above, and at       reasonably fast speed.    If``complib`is defined as something other than the listed libraries a`ValueError`exception is issued.  > **Note** >     If the library specified with the`complib`option is missing on your platform,    compression defaults to`zlib`without further ado.  Enable compression for all objects within the file:`\`python store\_compressed = pd.HDFStore( "store\_compressed.h5", complevel=9, complib="blosc:blosclz" )

Or on-the-fly compression (this only applies to tables) in stores where compression is not enabled:

``` python
store.append("df", df, complib="zlib", complevel=5)
```

<div id="io.hdf5-ptrepack">

ptrepack `` ` ++++++++ ``PyTables`offers better write performance when tables are compressed after they are written, as opposed to turning on compression at the very beginning. You can use the supplied`PyTables`utility`ptrepack`. In addition,`ptrepack`can change compression levels after the fact.`\`console ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5

</div>

Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow `` ` you to reuse previously deleted space. Alternatively, one can simply remove the file and write again, or use the ``copy`method.  .. _io.hdf5-caveats:  Caveats +++++++  > **Warning** >`HDFStore`is **not-threadsafe for writing**. The underlying`PyTables``only supports concurrent reads (via threading or    processes). If you need reading and writing *at the same time*, you    need to serialize these operations in a single thread in a single    process. You will corrupt your data otherwise. See the (:issue:`2397`) for more information.  * If you use locks to manage write access between multiple processes, you   may want to use :py`~os.fsync` before releasing write locks. For   convenience you can use``store.flush(fsync=True)`to do this for you. * Once a`table`is created columns (DataFrame)   are fixed; only exactly the same columns can be appended * Be aware that timezones (e.g.,`zoneinfo.ZoneInfo('US/Eastern')`)   are not necessarily equal across timezone versions.  So if data is   localized to a specific timezone in the HDFStore using one version   of a timezone library and that data is updated with another version, the data   will be converted to UTC since these timezones are not considered   equal.  Either use the same version of timezone library or use`tz\_convert`with   the updated timezone definition.  > **Warning** >`PyTables`will show a`NaturalNameWarning`if a column name    cannot be used as an attribute selector.    *Natural* identifiers contain only letters, numbers, and underscores,    and may not begin with a number.    Other identifiers cannot be used in a`where`clause    and are generally a bad idea.  .. _io.hdf5-data_types:  DataTypes '''''''''`HDFStore`will map an object dtype to the`PyTables`underlying dtype. This means the following types are known to work:  ======================================================  ========================= Type                                                    Represents missing values ======================================================  ========================= floating :`float64, float32, float16`  `np.nan`integer :`int64, int32, int8, uint64,uint32, uint8`boolean`datetime64\[ns\]`  `NaT`  `timedelta64\[ns\]`  `NaT`categorical : see the section below object :`strings`  `np.nan`======================================================  =========================`unicode`columns are not supported, and **WILL FAIL**.  .. _io.hdf5-categorical:  Categorical data ++++++++++++++++  You can write data that contains`category`dtypes to a`HDFStore`. Queries work the same as if it was an object array. However, the`category`dtyped data is stored in a more efficient manner.  .. ipython:: python     dfcat = pd.DataFrame(        {"A": pd.Series(list("aabbcdba")).astype("category"), "B": np.random.randn(8)}    )    dfcat    dfcat.dtypes    cstore = pd.HDFStore("cats.h5", mode="w")    cstore.append("dfcat", dfcat, format="table", data_columns=["A"])    result = cstore.select("dfcat", where="A in ['b', 'c']")    result    result.dtypes  .. ipython:: python    :suppress:    :okexcept:     cstore.close()    os.remove("cats.h5")   String columns ++++++++++++++  **min_itemsize**  The underlying implementation of`HDFStore`uses a fixed column width (itemsize) for string columns. A string column itemsize is calculated as the maximum of the length of data (for that column) that is passed to the`HDFStore`, **in the first append**. Subsequent appends, may introduce a string for a column **larger** than the column can hold, an Exception will be raised (otherwise you could have a silent truncation of these columns, leading to loss of information). In the future we may relax this and allow a user-specified truncation to occur.  Pass`min\_itemsize`on the first table creation to a-priori specify the minimum length of a particular string column.`min\_itemsize`can be an integer, or a dict mapping a column name to an integer. You can pass`values`as a key to allow all *indexables* or *data_columns* to have this min_itemsize.  Passing a`min\_itemsize`dict will cause all passed columns to be created as *data_columns* automatically.  > **Note** >     If you are not passing any`data\_columns`, then the`min\_itemsize`will be the maximum of the length of any string passed  .. ipython:: python     dfs = pd.DataFrame({"A": "foo", "B": "bar"}, index=list(range(5)))    dfs     # A and B have a size of 30    store.append("dfs", dfs, min_itemsize=30)    store.get_storer("dfs").table     # A is created as a data_column with a size of 30    # B is size is calculated    store.append("dfs2", dfs, min_itemsize={"A": 30})    store.get_storer("dfs2").table  **nan_rep**  String columns will serialize a`np.nan`(a missing value) with the`nan\_rep`string representation. This defaults to the string value`nan`. You could inadvertently turn an actual`nan`value into a missing value.  .. ipython:: python     dfss = pd.DataFrame({"A": ["foo", "bar", "nan"]})    dfss     store.append("dfss", dfss)    store.select("dfss")     # here you need to specify a different nan rep    store.append("dfss2", dfss, nan_rep="_nan_")    store.select("dfss2")   Performance '''''''''''  *`tables`format come with a writing performance penalty as compared to`fixed`stores. The benefit is the ability to append/delete and   query (potentially very large amounts of data).  Write times are   generally longer as compared with regular stores. Query times can   be quite fast, especially on an indexed axis. * You can pass`chunksize=\<int\>`to`append`, specifying the   write chunksize (default is 50000). This will significantly lower   your memory usage on writing. * You can pass`expectedrows=\<int\>`to the first`append`,   to set the TOTAL number of rows that`PyTables`will expect.   This will optimize read/write performance. * Duplicate rows can be written to tables, but are filtered out in   selection (with the last items being selected; thus a table is   unique on major, minor pairs) * A`PerformanceWarning``will be raised if you are attempting to   store types that will be pickled by PyTables (rather than stored as   endemic types). See   `Here <https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190>`__   for more information and some solutions.   .. ipython:: python    :suppress:     store.close()    os.remove("store.h5")   .. _io.feather:  Feather -------  Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.  Feather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as categorical and datetime with tz.  Several caveats:  * The format will NOT write an``Index`, or`MultiIndex`for the`DataFrame`and will raise an error if a non-default one is provided. You   can`.reset\_index()`to store the index or`.reset\_index(drop=True)``to   ignore it. * Duplicate column names and non-string columns names are not supported * Actual Python objects in object dtype columns are not supported. These will   raise a helpful error message on an attempt at serialization.  See the `Full Documentation <https://github.com/wesm/feather>`__.  .. ipython:: python     import pytz     df = pd.DataFrame(        {            "a": list("abc"),            "b": list(range(1, 4)),            "c": np.arange(3, 6).astype("u1"),            "d": np.arange(4.0, 7.0, dtype="float64"),            "e": [True, False, True],            "f": pd.Categorical(list("abc")),            "g": pd.date_range("20130101", periods=3),            "h": pd.date_range("20130101", periods=3, tz=pytz.timezone("US/Eastern")),            "i": pd.date_range("20130101", periods=3, freq="ns"),        }    )     df    df.dtypes  Write to a feather file.  .. ipython:: python    :okwarning:     df.to_feather("example.feather")  Read from a feather file.  .. ipython:: python    :okwarning:     result = pd.read_feather("example.feather")    result     # we preserve dtypes    result.dtypes  .. ipython:: python    :suppress:     os.remove("example.feather")   .. _io.parquet:  Parquet -------  `Apache Parquet <https://parquet.apache.org/>`__ provides a partitioned binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Parquet can use a variety of compression techniques to shrink the file size as much as possible while still maintaining good read performance.  Parquet is designed to faithfully serialize and de-serialize``DataFrame`s, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz.  Several caveats.  * Duplicate column names and non-string columns names are not supported. * The`pyarrow`engine always writes the index to the output, but`fastparquet`only writes non-default   indexes. This extra column can cause problems for non-pandas consumers that are not expecting it. You can   force including or omitting indexes with the`index`argument, regardless of the underlying engine. * Index level names, if specified, must be strings. * In the`pyarrow`engine, categorical dtypes for non-string types can be serialized to parquet, but will de-serialize as their primitive dtype. * The`pyarrow`engine preserves the`ordered`flag of categorical dtypes with string types.`fastparquet`does not preserve the`ordered`flag. * Non supported types include`Interval`and actual Python object types. These will raise a helpful error message   on an attempt at serialization.`Period`type is supported with pyarrow >= 0.16.0. * The`pyarrow`engine preserves extension data types such as the nullable integer and string data   type (requiring pyarrow >= 0.16.0, and requiring the extension type to implement the needed protocols,   see the [extension types documentation <extending.extension.arrow>](#extension-types-documentation-<extending.extension.arrow>)).  You can specify an`engine`to direct the serialization. This can be one of`pyarrow`, or`fastparquet`, or`auto`. If the engine is NOT specified, then the`pd.options.io.parquet.engine`option is checked; if this is also`auto`, then`pyarrow`is tried, and falling back to`fastparquet``.  See the documentation for `pyarrow <https://arrow.apache.org/docs/python/>`__ and `fastparquet <https://fastparquet.readthedocs.io/en/latest/>`__.  > **Note** >     These engines are very similar and should read/write nearly identical parquet format files.``pyarrow\>=8.0.0`supports timedelta data,`fastparquet\>=0.1.4`supports timezone aware datetimes.    These libraries differ by having different underlying dependencies (`fastparquet`by using`numba`, while`pyarrow`uses a c-library).  .. ipython:: python     df = pd.DataFrame(        {            "a": list("abc"),            "b": list(range(1, 4)),            "c": np.arange(3, 6).astype("u1"),            "d": np.arange(4.0, 7.0, dtype="float64"),            "e": [True, False, True],            "f": pd.date_range("20130101", periods=3),            "g": pd.date_range("20130101", periods=3, tz="US/Eastern"),            "h": pd.Categorical(list("abc")),            "i": pd.Categorical(list("abc"), ordered=True),        }    )     df    df.dtypes  Write to a parquet file.  .. ipython:: python     df.to_parquet("example_pa.parquet", engine="pyarrow")    df.to_parquet("example_fp.parquet", engine="fastparquet")  Read from a parquet file.  .. ipython:: python     result = pd.read_parquet("example_fp.parquet", engine="fastparquet")    result = pd.read_parquet("example_pa.parquet", engine="pyarrow")     result.dtypes  By setting the`dtype\_backend`argument you can control the default dtypes used for the resulting DataFrame.  .. ipython:: python     result = pd.read_parquet("example_pa.parquet", engine="pyarrow", dtype_backend="pyarrow")     result.dtypes  .. note::     Note that this is not supported for`fastparquet`.   Read only certain columns of a parquet file.  .. ipython:: python     result = pd.read_parquet(        "example_fp.parquet",        engine="fastparquet",        columns=["a", "b"],    )    result = pd.read_parquet(        "example_pa.parquet",        engine="pyarrow",        columns=["a", "b"],    )    result.dtypes   .. ipython:: python    :suppress:     os.remove("example_pa.parquet")    os.remove("example_fp.parquet")   Handling indexes ''''''''''''''''  Serializing a`DataFrame`to parquet may include the implicit index as one or more columns in the output file. Thus, this code:  .. ipython:: python      df = pd.DataFrame({"a": [1, 2], "b": [3, 4]})     df.to_parquet("test.parquet", engine="pyarrow")  creates a parquet file with *three* columns if you use`pyarrow`for serialization:`a`,`b`, and`\_\_index\_level\_0\_\_`. If you're using`fastparquet``, the index `may or may not <https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write>`_ be written to the file.  This unexpected extra column causes some databases like Amazon Redshift to reject the file, because that column doesn't exist in the target table.  If you want to omit a dataframe's indexes when writing, pass``index=False``to `~pandas.DataFrame.to_parquet`:  .. ipython:: python      df.to_parquet("test.parquet", index=False)  This creates a parquet file with just the two expected columns,``a`and`b`. If your`DataFrame`has a custom index, you won't get it back when you load this file into a`DataFrame`.  Passing`index=True`will *always* write the index, even if that's not the underlying engine's default behavior.  .. ipython:: python    :suppress:     os.remove("test.parquet")   Partitioning Parquet files ''''''''''''''''''''''''''  Parquet supports partitioning of data based on the values of one or more columns.  .. ipython:: python      df = pd.DataFrame({"a": [0, 0, 1, 1], "b": [0, 1, 0, 1]})     df.to_parquet(path="test", engine="pyarrow", partition_cols=["a"], compression=None)  The`path`specifies the parent directory to which data will be saved. The`partition\_cols`are the column names by which the dataset will be partitioned. Columns are partitioned in the order they are given. The partition splits are determined by the unique values in the partition columns. The above example creates a partitioned dataset that may look like:`\`text test â”œâ”€â”€ a=0 â”‚ â”œâ”€â”€ 0bac803e32dc42ae83fddfd029cbdebc.parquet â”‚ â””â”€â”€ ... â””â”€â”€ a=1 â”œâ”€â”€ e6ab24a4f45147b49b54a662f0c412a3.parquet â””â”€â”€ ...

<div class="ipython" data-suppress="">

python

from shutil import rmtree

  - try:  
    rmtree("test")

  - except OSError:  
    pass

</div>

##### ORC

\---

Similar to the \[parquet \<io.parquet\>\](\#parquet-\<io.parquet\>) format, the [ORC Format](https://orc.apache.org/) is a binary columnar serialization for data frames. It is designed to make reading data frames efficient. pandas provides both the reader and the writer for the ORC format, <span class="title-ref">\~pandas.read\_orc</span> and <span class="title-ref">\~pandas.DataFrame.to\_orc</span>. This requires the [pyarrow](https://arrow.apache.org/docs/python/) library.

\> **Warning** \> \* It is *highly recommended* to install pyarrow using conda due to some issues occurred by pyarrow. \* <span class="title-ref">\~pandas.DataFrame.to\_orc</span> requires pyarrow\>=7.0.0. \* <span class="title-ref">\~pandas.read\_orc</span> and <span class="title-ref">\~pandas.DataFrame.to\_orc</span> are not supported on Windows yet, you can find valid environments on \[install optional dependencies \<install.warn\_orc\>\](\#install-optional-dependencies-\<install.warn\_orc\>). \* For supported dtypes please refer to [supported ORC features in Arrow](https://arrow.apache.org/docs/cpp/orc.html#data-types). \* Currently timezones in datetime columns are not preserved when a dataframe is converted into ORC files.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "a": list("abc"), "b": list(range(1, 4)), "c": np.arange(4.0, 7.0, dtype="float64"), "d": \[True, False, True\], "e": pd.date\_range("20130101", periods=3),
    
    }

)

df df.dtypes

</div>

Write to an orc file.

<div class="ipython">

python

df.to\_orc("example\_pa.orc", engine="pyarrow")

</div>

Read from an orc file.

<div class="ipython">

python

result = pd.read\_orc("example\_pa.orc")

result.dtypes

</div>

Read only certain columns of an orc file.

<div class="ipython">

python

  - result = pd.read\_orc(  
    "example\_pa.orc", columns=\["a", "b"\],

) result.dtypes

</div>

<div class="ipython" data-suppress="">

python

os.remove("example\_pa.orc")

</div>

## SQL queries

The `pandas.io.sql` module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API.

Where available, users may first want to opt for [Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html) drivers. These drivers should provide the best performance, null handling, and type detection.

> 
> 
> <div class="versionadded">
> 
> 2.2.0
> 
> Added native support for ADBC drivers
> 
> </div>

For a full list of ADBC drivers and their development status, see the [ADBC Driver Implementation Status](https://arrow.apache.org/adbc/current/driver/status.html) documentation.

Where an ADBC driver is not available or may be missing functionality, users should opt for installing SQLAlchemy alongside their database driver library. Examples of such drivers are [psycopg2](https://www.psycopg.org/) for PostgreSQL or [pymysql](https://github.com/PyMySQL/PyMySQL) for MySQL. For [SQLite](https://docs.python.org/3/library/sqlite3.html) this is included in Python's standard library by default. You can find an overview of supported drivers for each SQL dialect in the [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/dialects/index.html).

If SQLAlchemy is not installed, you can use a <span class="title-ref">sqlite3.Connection</span> in place of a SQLAlchemy engine, connection, or URI string.

See also some \[cookbook examples \<cookbook.sql\>\](\#cookbook-examples-\<cookbook.sql\>) for some advanced strategies.

The key functions are:

<div class="autosummary">

read\_sql\_table read\_sql\_query read\_sql DataFrame.to\_sql

</div>

\> **Note** \> The function <span class="title-ref">\~pandas.read\_sql</span> is a convenience wrapper around <span class="title-ref">\~pandas.read\_sql\_table</span> and <span class="title-ref">\~pandas.read\_sql\_query</span> (and for backward compatibility) and will delegate to specific function depending on the provided input (database table name or sql query). Table names do not need to be quoted if they have special characters.

In the following example, we use the [SQlite](https://www.sqlite.org/index.html) SQL database engine. You can use a temporary SQLite database where data are stored in "memory".

To connect using an ADBC driver you will want to install the `adbc_driver_sqlite` using your package manager. Once installed, you can use the DBAPI interface provided by the ADBC driver to connect to your database.

`` `python    import adbc_driver_sqlite.dbapi as sqlite_dbapi     # Create the connection    with sqlite_dbapi.connect("sqlite:///:memory:") as conn:         df = pd.read_sql_table("data", conn)  To connect with SQLAlchemy you use the `create_engine` function to create an engine ``<span class="title-ref"> object from database URI. You only need to create the engine once per database you are connecting to. For more information on \`create\_engine</span> and the URI formatting, see the examples below and the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)

<div class="ipython">

python

from sqlalchemy import create\_engine

\# Create your engine. engine = create\_engine("sqlite:///:memory:")

</div>

If you want to manage your own connections you can pass one of those instead. The example below opens a connection to the database using a Python context manager that automatically closes the connection after the block has completed. See the [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage) for an explanation of how the database connection is handled.

`` `python    with engine.connect() as conn, conn.begin():        data = pd.read_sql_table("data", conn)  > **Warning** >          When you open a connection to a database you are also responsible for closing it.         Side effects of leaving a connection open may include locking the database or         other breaking behaviour.  Writing DataFrames ``\` ''''''''''''''''''

Assuming the following data is in a `DataFrame` `data`, we can insert it into the database using <span class="title-ref">\~pandas.DataFrame.to\_sql</span>.

<table style="width:60%;">
<colgroup>
<col style="width: 8%" />
<col style="width: 18%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="header">
<th>id</th>
<th>Date</th>
<th>Col_1</th>
<th>Col_2</th>
<th>Col_3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>26</td>
<td>2012-10-18</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>25.7</p>
</blockquote></td>
<td>True</td>
</tr>
<tr class="even">
<td>42</td>
<td>2012-10-19</td>
<td><blockquote>
<p>Y</p>
</blockquote></td>
<td>-12.4</td>
<td>False</td>
</tr>
<tr class="odd">
<td>63</td>
<td>2012-10-20</td>
<td><blockquote>
<p>Z</p>
</blockquote></td>
<td><blockquote>
<p>5.73</p>
</blockquote></td>
<td>True</td>
</tr>
</tbody>
</table>

<div class="ipython">

python

import datetime

c = \["id", "Date", "Col\_1", "Col\_2", "Col\_3"\] d = \[ (26, datetime.datetime(2010, 10, 18), "X", 27.5, True), (42, datetime.datetime(2010, 10, 19), "Y", -12.5, False), (63, datetime.datetime(2010, 10, 20), "Z", 5.73, True), \]

data = pd.DataFrame(d, columns=c)

data data.to\_sql("data", con=engine)

</div>

With some databases, writing large DataFrames can result in errors due to packet size limitations being exceeded. This can be avoided by setting the `chunksize` parameter when calling `to_sql`. For example, the following writes `data` to the database in batches of 1000 rows at a time:

<div class="ipython">

python

data.to\_sql("data\_chunked", con=engine, chunksize=1000)

</div>

#### SQL data types

Ensuring consistent data type management across SQL databases is challenging. Not every SQL database offers the same types, and even when they do the implementation of a given type can vary in ways that have subtle effects on how types can be preserved.

For the best odds at preserving database types users are advised to use ADBC drivers when available. The Arrow type system offers a wider array of types that more closely match database types than the historical pandas/NumPy type system. To illustrate, note this (non-exhaustive) listing of types available in different databases and pandas backends:

<table style="width:96%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 33%" />
<col style="width: 23%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>numpy/pandas</th>
<th>arrow</th>
<th>postgres</th>
<th>sqlite</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>int16/Int16</td>
<td>int16</td>
<td>SMALLINT</td>
<td>INTEGER</td>
</tr>
<tr class="even">
<td>int32/Int32</td>
<td>int32</td>
<td>INTEGER</td>
<td>INTEGER</td>
</tr>
<tr class="odd">
<td>int64/Int64</td>
<td>int64</td>
<td>BIGINT</td>
<td>INTEGER</td>
</tr>
<tr class="even">
<td>float32</td>
<td>float32</td>
<td>REAL</td>
<td>REAL</td>
</tr>
<tr class="odd">
<td>float64</td>
<td>float64</td>
<td>DOUBLE PRECISION</td>
<td>REAL</td>
</tr>
<tr class="even">
<td>object</td>
<td>string</td>
<td>TEXT</td>
<td>TEXT</td>
</tr>
<tr class="odd">
<td>bool</td>
<td><code>bool_</code></td>
<td>BOOLEAN</td>
<td></td>
</tr>
<tr class="even">
<td>datetime64[ns]</td>
<td>timestamp(us)</td>
<td>TIMESTAMP</td>
<td></td>
</tr>
<tr class="odd">
<td>datetime64[ns,tz]</td>
<td>timestamp(us,tz)</td>
<td>TIMESTAMPTZ</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>date32</td>
<td>DATE</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>month_day_nano_interval</td>
<td>INTERVAL</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>binary</td>
<td>BINARY</td>
<td>BLOB</td>
</tr>
<tr class="odd">
<td></td>
<td>decimal128</td>
<td>DECIMAL[1]</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>list</td>
<td>ARRAY[2]</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>struct</td>
<td><dl>
<dt>COMPOSITE TYPE</dt>
<dd><p>[3]</p>
</dd>
</dl></td>
<td></td>
</tr>
</tbody>
</table>

**Footnotes**

If you are interested in preserving database types as best as possible throughout the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"` argument of <span class="title-ref">\~pandas.read\_sql</span>

`` `ipython    # for roundtripping    with pg_dbapi.connect(uri) as conn:        df2 = pd.read_sql("pandas_table", conn, dtype_backend="pyarrow")  This will prevent your data from being converted to the traditional pandas/NumPy ``\` type system, which often converts SQL types in ways that make them impossible to round-trip.

In case an ADBC driver is not available, <span class="title-ref">\~pandas.DataFrame.to\_sql</span> will try to map your data to an appropriate SQL data type based on the dtype of the data. When you have columns of dtype `object`, pandas will try to infer the data type.

You can always override the default type by specifying the desired SQL type of any of the columns by using the `dtype` argument. This argument needs a dictionary mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback mode). For example, specifying to use the sqlalchemy `String` type instead of the default `Text` type for string columns:

<div class="ipython">

python

from sqlalchemy.types import String

data.to\_sql("data\_dtype", con=engine, dtype={"Col\_1": String})

</div>

\> **Note** \> Due to the limited support for timedelta's in the different database flavors, columns with type `timedelta64` will be written as integer values as nanoseconds to the database and a warning will be raised. The only exception to this is when using the ADBC PostgreSQL driver in which case a timedelta will be written to the database as an `INTERVAL`

<div class="note">

<div class="title">

Note

</div>

Columns of `category` dtype will be converted to the dense representation as you would get with `np.asarray(categorical)` (e.g. for string categories this gives an array of strings). Because of this, reading the database table back in does **not** generate a categorical.

</div>

### Datetime data types

Using ADBC or SQLAlchemy, <span class="title-ref">\~pandas.DataFrame.to\_sql</span> is capable of writing datetime data that is timezone naive or timezone aware. However, the resulting data stored in the database ultimately depends on the supported data type for datetime data of the database system being used.

The following table lists supported data types for datetime data for some common databases. Other database dialects may have different data types for datetime data.

| Database   | SQL Datetime Types                        | Timezone Support |
| ---------- | ----------------------------------------- | ---------------- |
| SQLite     | `TEXT`                                    | No               |
| MySQL      | `TIMESTAMP` or `DATETIME`                 | No               |
| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes              |

When writing timezone aware data to databases that do not support timezones, the data will be written as timezone naive timestamps that are in local time with respect to the timezone.

<span class="title-ref">\~pandas.read\_sql\_table</span> is also capable of reading datetime data that is timezone aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert the data to UTC.

#### Insertion method

The parameter `method` controls the SQL insertion clause used. Possible values are:

  - `None`: Uses standard SQL `INSERT` clause (one per row).
  - `'multi'`: Pass multiple values in a single `INSERT` clause. It uses a *special* SQL syntax not supported by all backends. This usually provides better performance for analytic databases like *Presto* and *Redshift*, but has worse performance for traditional SQL backend if the table contains many columns. For more information check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).
  - callable with signature `(pd_table, conn, keys, data_iter)`: This can be used to implement a more performant insertion method based on specific backend dialect features.

Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):

    # Alternative to_sql() *method* for DBs that support COPY FROM
    import csv
    from io import StringIO
    
    def psql_insert_copy(table, conn, keys, data_iter):
        """
        Execute SQL statement inserting data
    
        Parameters
        ----------
        table : pandas.io.sql.SQLTable
        conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection
        keys : list of str
            Column names
        data_iter : Iterable that iterates the values to be inserted
        """
        # gets a DBAPI connection that can provide a cursor
        dbapi_conn = conn.connection
        with dbapi_conn.cursor() as cur:
            s_buf = StringIO()
            writer = csv.writer(s_buf)
            writer.writerows(data_iter)
            s_buf.seek(0)
    
            columns = ', '.join(['"{}"'.format(k) for k in keys])
            if table.schema:
                table_name = '{}.{}'.format(table.schema, table.name)
            else:
                table_name = table.name
    
            sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(
                table_name, columns)
            cur.copy_expert(sql=sql, file=s_buf)

### Reading tables

<span class="title-ref">\~pandas.read\_sql\_table</span> will read a database table given the table name and optionally a subset of columns to read.

\> **Note** \> In order to use <span class="title-ref">\~pandas.read\_sql\_table</span>, you **must** have the ADBC driver or SQLAlchemy optional dependency installed.

<div class="ipython">

python

pd.read\_sql\_table("data", engine)

</div>

<div class="note">

<div class="title">

Note

</div>

ADBC drivers will map database types directly back to arrow types. For other drivers note that pandas infers column dtypes from query outputs, and not by looking up data types in the physical database schema. For example, assume `userid` is an integer column in a table. Then, intuitively, `select userid ...` will return integer-valued series, while `select cast(userid as text) ...` will return object-valued (str) series. Accordingly, if the query output is empty, then all resulting columns will be returned as object-valued (since they are most general). If you foresee that your query will sometimes generate an empty result, you may want to explicitly typecast afterwards to ensure dtype integrity.

</div>

You can also specify the name of the column as the `DataFrame` index, and specify a subset of columns to be read.

<div class="ipython">

python

pd.read\_sql\_table("data", engine, index\_col="id") pd.read\_sql\_table("data", engine, columns=\["Col\_1", "Col\_2"\])

</div>

And you can explicitly force columns to be parsed as dates:

<div class="ipython">

python

pd.read\_sql\_table("data", engine, parse\_dates=\["Date"\])

</div>

If needed you can explicitly specify a format string, or a dict of arguments to pass to \`pandas.to\_datetime\`:

`` `python    pd.read_sql_table("data", engine, parse_dates={"Date": "%Y-%m-%d"})    pd.read_sql_table(        "data",        engine,        parse_dates={"Date": {"format": "%Y-%m-%d %H:%M:%S"}},    )   You can check if a table exists using `~pandas.io.sql.has_table`  Schema support ``\` ''''''''''''''

Reading from and writing to different schemas is supported through the `schema` keyword in the <span class="title-ref">\~pandas.read\_sql\_table</span> and <span class="title-ref">\~pandas.DataFrame.to\_sql</span> functions. Note however that this depends on the database flavor (sqlite does not have schemas). For example:

`` `python    df.to_sql(name="table", con=engine, schema="other_schema")    pd.read_sql_table("table", engine, schema="other_schema")  Querying ``\` ''''''''

You can query using raw SQL in the <span class="title-ref">\~pandas.read\_sql\_query</span> function. In this case you must use the SQL variant appropriate for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression language constructs, which are database-agnostic.

<div class="ipython">

python

pd.read\_sql\_query("SELECT \* FROM data", engine)

</div>

Of course, you can specify a more "complex" query.

<div class="ipython">

python

pd.read\_sql\_query("SELECT id, Col\_1, Col\_2 FROM data WHERE id = 42;", engine)

</div>

The <span class="title-ref">\~pandas.read\_sql\_query</span> function supports a `chunksize` argument. Specifying this will return an iterator through chunks of the query result:

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(20, 3), columns=list("abc")) df.to\_sql(name="data\_chunks", con=engine, index=False)

</div>

<div class="ipython">

python

  - for chunk in pd.read\_sql\_query("SELECT \* FROM data\_chunks", engine, chunksize=5):  
    print(chunk)

</div>

### Engine connection examples

To connect with SQLAlchemy you use the <span class="title-ref">create\_engine</span> function to create an engine object from database URI. You only need to create the engine once per database you are connecting to.

`` `python    from sqlalchemy import create_engine     engine = create_engine("postgresql://scott:tiger@localhost:5432/mydatabase")     engine = create_engine("mysql+mysqldb://scott:tiger@localhost/foo")     engine = create_engine("oracle://scott:tiger@127.0.0.1:1521/sidname")     engine = create_engine("mssql+pyodbc://mydsn")     # sqlite://<nohostname>/<path>    # where <path> is relative:    engine = create_engine("sqlite:///foo.db")     # or absolute, starting with a slash:    engine = create_engine("sqlite:////absolute/path/to/foo.db")  For more information see the examples the SQLAlchemy `documentation <https://docs.sqlalchemy.org/en/latest/core/engines.html>`__   Advanced SQLAlchemy queries ``\` '''''''''''''''''''''''''''

You can use SQLAlchemy constructs to describe your query.

Use <span class="title-ref">sqlalchemy.text</span> to specify query parameters in a backend-neutral way

<div class="ipython">

python

import sqlalchemy as sa

  - pd.read\_sql(  
    sa.text("SELECT \* FROM data where Col\_1=:col1"), engine, params={"col1": "X"}

)

</div>

If you have an SQLAlchemy description of your database you can express where conditions using SQLAlchemy expressions

<div class="ipython">

python

metadata = sa.MetaData() data\_table = sa.Table( "data", metadata, sa.Column("index", sa.Integer), sa.Column("Date", sa.DateTime), sa.Column("Col\_1", sa.String), sa.Column("Col\_2", sa.Float), sa.Column("Col\_3", sa.Boolean), )

pd.read\_sql(sa.select(data\_table).where(data\_table.c.Col\_3 is True), engine)

</div>

You can combine SQLAlchemy expressions with parameters passed to <span class="title-ref">read\_sql</span> using <span class="title-ref">sqlalchemy.bindparam</span>

<div class="ipython">

python

import datetime as dt

expr = sa.select(data\_table).where(data\_table.c.Date \> sa.bindparam("date")) pd.read\_sql(expr, engine, params={"date": dt.datetime(2010, 10, 18)})

</div>

### Sqlite fallback

The use of sqlite is supported without using SQLAlchemy. This mode requires a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).

You can create connections like so:

`` `python    import sqlite3     con = sqlite3.connect(":memory:")  And then issue the following queries:  .. code-block:: python     data.to_sql("data", con)    pd.read_sql_query("SELECT * FROM data", con)   .. _io.bigquery:  Google BigQuery ``\` ---------------

The `pandas-gbq` package provides functionality to read/write from Google BigQuery.

Full documentation can be found [here](https://pandas-gbq.readthedocs.io/en/latest/).

## STATA format

### Writing to stata format

The method <span class="title-ref">.DataFrame.to\_stata</span> will write a DataFrame into a .dta file. The format version of this file is always 115 (Stata 12).

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 2), columns=list("AB")) df.to\_stata("stata.dta")

</div>

*Stata* data files have limited data type support; only strings with 244 or fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored in `.dta` files. Additionally, *Stata* reserves certain values to represent missing data. Exporting a non-missing value that is outside of the permitted range in Stata for a particular data type will retype the variable to the next larger size. For example, `int8` values are restricted to lie between -127 and 100 in Stata, and so variables with values above 100 will trigger a conversion to `int16`. `nan` values in floating points data types are stored as the basic missing data type (`.` in *Stata*).

\> **Note** \> It is not possible to export missing data values for integer data types.

The *Stata* writer gracefully handles other data types including `int64`, `bool`, `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can represent the data. For example, data with a type of `uint8` will be cast to `int8` if all values are less than 100 (the upper bound for non-missing `int8` data in *Stata*), or, if values are outside of this range, the variable is cast to `int16`.

\> **Warning** \> Conversion from `int64` to `float64` may result in a loss of precision if `int64` values are larger than 2\*\*53.

<div class="warning">

<div class="title">

Warning

</div>

<span class="title-ref">\~pandas.io.stata.StataWriter</span> and <span class="title-ref">.DataFrame.to\_stata</span> only support fixed width strings containing up to 244 characters, a limitation imposed by the version 115 dta file format. Attempting to write *Stata* dta files with strings longer than 244 characters raises a `ValueError`.

</div>

### Reading from Stata format

The top-level function `read_stata` will read a dta file and return either a `DataFrame` or a <span class="title-ref">pandas.api.typing.StataReader</span> that can be used to read the file incrementally.

<div class="ipython">

python

pd.read\_stata("stata.dta")

</div>

Specifying a `chunksize` yields a <span class="title-ref">pandas.api.typing.StataReader</span> instance that can be used to read `chunksize` lines from the file at a time. The `StataReader` object can be used as an iterator.

<div class="ipython">

python

  - with pd.read\_stata("stata.dta", chunksize=3) as reader:
    
      - for df in reader:  
        print(df.shape)

</div>

For more fine-grained control, use `iterator=True` and specify `chunksize` with each call to <span class="title-ref">\~pandas.io.stata.StataReader.read</span>.

<div class="ipython">

python

  - with pd.read\_stata("stata.dta", iterator=True) as reader:  
    chunk1 = reader.read(5) chunk2 = reader.read(5)

</div>

Currently the `index` is retrieved as a column.

The parameter `convert_categoricals` indicates whether value labels should be read and used to create a `Categorical` variable from them. Value labels can also be retrieved by the function `value_labels`, which requires <span class="title-ref">\~pandas.io.stata.StataReader.read</span> to be called before use.

The parameter `convert_missing` indicates whether missing value representations in Stata should be preserved. If `False` (the default), missing values are represented as `np.nan`. If `True`, missing values are represented using `StataMissingValue` objects, and columns containing missing values will have `object` data type.

\> **Note** \> <span class="title-ref">\~pandas.read\_stata</span> and <span class="title-ref">\~pandas.io.stata.StataReader</span> support .dta formats 113-115 (Stata 10-12), 117 (Stata 13), and 118 (Stata 14).

<div class="note">

<div class="title">

Note

</div>

Setting `preserve_dtypes=False` will upcast to the standard pandas data types: `int64` for all integer types and `float64` for floating point data. By default, the Stata data types are preserved when importing.

</div>

<div class="note">

<div class="title">

Note

</div>

All <span class="title-ref">\~pandas.io.stata.StataReader</span> objects, whether created by <span class="title-ref">\~pandas.read\_stata</span> (when using `iterator=True` or `chunksize`) or instantiated by hand, must be used as context managers (e.g. the `with` statement). While the <span class="title-ref">\~pandas.io.stata.StataReader.close</span> method is available, its use is unsupported. It is not part of the public API and will be removed in with future without warning.

</div>

<div class="ipython" data-suppress="">

python

os.remove("stata.dta")

</div>

#### Categorical data

`Categorical` data can be exported to *Stata* data files as value labeled data. The exported data consists of the underlying category codes as integer data values and the categories as value labels. *Stata* does not have an explicit equivalent to a `Categorical` and information about *whether* the variable is ordered is lost when exporting.

\> **Warning** \> *Stata* only supports string value labels, and so `str` is called on the categories when exporting data. Exporting `Categorical` variables with non-string categories produces a warning, and can result a loss of information if the `str` representations of the categories are not unique.

Labeled data can similarly be imported from *Stata* data files as `Categorical` variables using the keyword argument `convert_categoricals` (`True` by default). The keyword argument `order_categoricals` (`True` by default) determines whether imported `Categorical` variables are ordered.

\> **Note** \> When importing categorical data, the values of the variables in the *Stata* data file are not preserved since `Categorical` variables always use integer data types between `-1` and `n-1` where `n` is the number of categories. If the original values in the *Stata* data file are required, these can be imported by setting `convert_categoricals=False`, which will import original data (but not the variable labels). The original values can be matched to the imported categorical data since there is a simple mapping between the original *Stata* data values and the category codes of imported Categorical variables: missing values are assigned code `-1`, and the smallest original value is assigned `0`, the second smallest is assigned `1` and so on until the largest original value is assigned the code `n-1`.

<div class="note">

<div class="title">

Note

</div>

*Stata* supports partially labeled series. These series have value labels for some but not all data values. Importing a partially labeled series will produce a `Categorical` with string categories for the values that are labeled and numeric categories for values with no label.

</div>

## SAS formats<span id="io.sas"></span>

The top-level function <span class="title-ref">read\_sas</span> can read (but not write) SAS XPORT (.xpt) and SAS7BDAT (.sas7bdat) format files.

SAS files only contain two value types: ASCII text and floating point values (usually 8 bytes but sometimes truncated). For xport files, there is no automatic type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format codes may allow date variables to be automatically converted to dates. By default the whole file is read and returned as a `DataFrame`.

Specify a `chunksize` or use `iterator=True` to obtain reader objects (`XportReader` or `SAS7BDATReader`) for incrementally reading the file. The reader objects also have attributes that contain additional information about the file and its variables.

Read a SAS7BDAT file:

`` `python     df = pd.read_sas("sas_data.sas7bdat")  Obtain an iterator and read an XPORT file 100,000 lines at a time:  .. code-block:: python      def do_something(chunk):         pass       with pd.read_sas("sas_xport.xpt", chunk=100000) as rdr:         for chunk in rdr:             do_something(chunk)  The specification_ for the xport file format is available from the SAS ``\` web site.

No official documentation is available for the SAS7BDAT format.

## SPSS formats<span id="io.spss"></span>

The top-level function <span class="title-ref">read\_spss</span> can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav) format files.

SPSS files contain column names. By default the whole file is read, categorical columns are converted into `pd.Categorical`, and a `DataFrame` with all columns is returned.

Specify the `usecols` parameter to obtain a subset of columns. Specify `convert_categoricals=False` to avoid converting categorical columns into `pd.Categorical`.

Read an SPSS file:

`` `python     df = pd.read_spss("spss_data.sav")  Extract a subset of columns contained in ``usecols`from an SPSS file and`<span class="title-ref"> avoid converting categorical columns into </span><span class="title-ref">pd.Categorical</span>\`:

`` `python     df = pd.read_spss(         "spss_data.sav",         usecols=["foo", "bar"],         convert_categoricals=False,     )  More information about the SAV and ZSAV file formats is available here_.    .. _io.other:  Other file formats ``\` ------------------

pandas itself only supports IO with a limited set of file formats that map cleanly to its tabular data model. For reading and writing other file formats into and from pandas, we recommend these packages from the broader community.

### netCDF

[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired by the pandas `DataFrame` for working with multi-dimensional datasets, with a focus on the netCDF file format and easy conversion to and from pandas.

## Performance considerations

This is an informal comparison of various IO methods, using pandas 0.24.2. Timings are machine dependent and small differences should be ignored.

`` `ipython    In [1]: sz = 1000000    In [2]: df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})     In [3]: df.info()    <class 'pandas.DataFrame'>    RangeIndex: 1000000 entries, 0 to 999999    Data columns (total 2 columns):    A    1000000 non-null float64    B    1000000 non-null int64    dtypes: float64(1), int64(1)    memory usage: 15.3 MB  The following test functions will be used below to compare the performance of several IO methods:  .. code-block:: python       import numpy as np     import os     sz = 1000000    df = pd.DataFrame({"A": np.random.randn(sz), "B": [1] * sz})     sz = 1000000    np.random.seed(42)    df = pd.DataFrame({"A": np.random.randn(sz), "B": [1] * sz})      def test_sql_write(df):        if os.path.exists("test.sql"):            os.remove("test.sql")        sql_db = sqlite3.connect("test.sql")        df.to_sql(name="test_table", con=sql_db)        sql_db.close()      def test_sql_read():        sql_db = sqlite3.connect("test.sql")        pd.read_sql_query("select * from test_table", sql_db)        sql_db.close()      def test_hdf_fixed_write(df):        df.to_hdf("test_fixed.hdf", key="test", mode="w")      def test_hdf_fixed_read():        pd.read_hdf("test_fixed.hdf", "test")      def test_hdf_fixed_write_compress(df):        df.to_hdf("test_fixed_compress.hdf", key="test", mode="w", complib="blosc")      def test_hdf_fixed_read_compress():        pd.read_hdf("test_fixed_compress.hdf", "test")      def test_hdf_table_write(df):        df.to_hdf("test_table.hdf", key="test", mode="w", format="table")      def test_hdf_table_read():        pd.read_hdf("test_table.hdf", "test")      def test_hdf_table_write_compress(df):        df.to_hdf(            "test_table_compress.hdf", key="test", mode="w", complib="blosc", format="table"        )      def test_hdf_table_read_compress():        pd.read_hdf("test_table_compress.hdf", "test")      def test_csv_write(df):        df.to_csv("test.csv", mode="w")      def test_csv_read():        pd.read_csv("test.csv", index_col=0)      def test_feather_write(df):        df.to_feather("test.feather")      def test_feather_read():        pd.read_feather("test.feather")      def test_pickle_write(df):        df.to_pickle("test.pkl")      def test_pickle_read():        pd.read_pickle("test.pkl")      def test_pickle_write_compress(df):        df.to_pickle("test.pkl.compress", compression="xz")      def test_pickle_read_compress():        pd.read_pickle("test.pkl.compress", compression="xz")      def test_parquet_write(df):        df.to_parquet("test.parquet")      def test_parquet_read():        pd.read_parquet("test.parquet")  When writing, the top three functions in terms of speed are ``test\_feather\_write`,`test\_hdf\_fixed\_write`and`test\_hdf\_fixed\_write\_compress`.  .. code-block:: ipython     In [4]: %timeit test_sql_write(df)    3.29 s Â± 43.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [5]: %timeit test_hdf_fixed_write(df)    19.4 ms Â± 560 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [6]: %timeit test_hdf_fixed_write_compress(df)    19.6 ms Â± 308 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)     In [7]: %timeit test_hdf_table_write(df)    449 ms Â± 5.61 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [8]: %timeit test_hdf_table_write_compress(df)    448 ms Â± 11.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [9]: %timeit test_csv_write(df)    3.66 s Â± 26.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [10]: %timeit test_feather_write(df)    9.75 ms Â± 117 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)     In [11]: %timeit test_pickle_write(df)    30.1 ms Â± 229 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)     In [12]: %timeit test_pickle_write_compress(df)    4.29 s Â± 15.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [13]: %timeit test_parquet_write(df)    67.6 ms Â± 706 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)  When reading, the top three functions in terms of speed are`test\_feather\_read`,`test\_pickle\_read`and`<span class="title-ref"> </span><span class="title-ref">test\_hdf\_fixed\_read</span>\`.

`` `ipython    In [14]: %timeit test_sql_read()    1.77 s Â± 17.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [15]: %timeit test_hdf_fixed_read()    19.4 ms Â± 436 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)     In [16]: %timeit test_hdf_fixed_read_compress()    19.5 ms Â± 222 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)     In [17]: %timeit test_hdf_table_read()    38.6 ms Â± 857 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)     In [18]: %timeit test_hdf_table_read_compress()    38.8 ms Â± 1.49 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)     In [19]: %timeit test_csv_read()    452 ms Â± 9.04 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [20]: %timeit test_feather_read()    12.4 ms Â± 99.7 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)     In [21]: %timeit test_pickle_read()    18.4 ms Â± 191 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)     In [22]: %timeit test_pickle_read_compress()    915 ms Â± 7.48 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)     In [23]: %timeit test_parquet_read()    24.4 ms Â± 146 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)   The files ``test.pkl.compress`,`test.parquet`and`test.feather`took the least space on disk (in bytes).  .. code-block:: none      29519500 Oct 10 06:45 test.csv     16000248 Oct 10 06:45 test.feather     8281983  Oct 10 06:49 test.parquet     16000857 Oct 10 06:47 test.pkl     7552144  Oct 10 06:48 test.pkl.compress     34816000 Oct 10 06:42 test.sql     24009288 Oct 10 06:43 test_fixed.hdf     24009288 Oct 10 06:43 test_fixed_compress.hdf     24458940 Oct 10 06:44 test_table.hdf     24458940 Oct 10 06:44 test_table_compress.hdf`\`

1.  Not implemented as of writing, but theoretically possible

2.  Not implemented as of writing, but theoretically possible

3.  Not implemented as of writing, but theoretically possible

---

merging.md

---

<div id="merging">

{{ header }}

</div>

<div class="ipython" data-suppress="">

python

from matplotlib import pyplot as plt import pandas.util.\_doctools as doctools

p = doctools.TablePlotter()

</div>

# Merge, join, concatenate and compare

pandas provides various methods for combining and comparing <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span>.

  - \`\~pandas.concat\`: Merge multiple <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects along a shared index or column
  - \`DataFrame.join\`: Merge multiple <span class="title-ref">DataFrame</span> objects along the columns
  - \`DataFrame.combine\_first\`: Update missing values with non-missing values in the same location
  - \`\~pandas.merge\`: Combine two <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects with SQL-style joining
  - \`\~pandas.merge\_ordered\`: Combine two <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects along an ordered axis
  - \`\~pandas.merge\_asof\`: Combine two <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects by near instead of exact matching keys
  - <span class="title-ref">Series.compare</span> and \`DataFrame.compare\`: Show differences in values between two <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects

## <span class="title-ref">\~pandas.concat</span>

The <span class="title-ref">\~pandas.concat</span> function concatenates an arbitrary amount of <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects along an axis while performing optional set logic (union or intersection) of the indexes on the other axes. Like `numpy.concatenate`, <span class="title-ref">\~pandas.concat</span> takes a list or dict of homogeneously-typed objects and concatenates them.

<div class="ipython">

python

  - df1 = pd.DataFrame(
    
      - {  
        "A": \["A0", "A1", "A2", "A3"\], "B": \["B0", "B1", "B2", "B3"\], "C": \["C0", "C1", "C2", "C3"\], "D": \["D0", "D1", "D2", "D3"\],
    
    }, index=\[0, 1, 2, 3\],

)

  - df2 = pd.DataFrame(
    
      - {  
        "A": \["A4", "A5", "A6", "A7"\], "B": \["B4", "B5", "B6", "B7"\], "C": \["C4", "C5", "C6", "C7"\], "D": \["D4", "D5", "D6", "D7"\],
    
    }, index=\[4, 5, 6, 7\],

)

  - df3 = pd.DataFrame(
    
      - {  
        "A": \["A8", "A9", "A10", "A11"\], "B": \["B8", "B9", "B10", "B11"\], "C": \["C8", "C9", "C10", "C11"\], "D": \["D8", "D9", "D10", "D11"\],
    
    }, index=\[8, 9, 10, 11\],

)

frames = \[df1, df2, df3\] result = pd.concat(frames) result

</div>

<div class="ipython" data-suppress="">

python

@savefig merging\_concat\_basic.png p.plot(frames, result, labels=\["df1", "df2", "df3"\], vertical=True); plt.close("all");

</div>

\> **Note** \> <span class="title-ref">\~pandas.concat</span> makes a full copy of the data, and iteratively reusing <span class="title-ref">\~pandas.concat</span> can create unnecessary copies. Collect all <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> objects in a list before using <span class="title-ref">\~pandas.concat</span>.

>   - \`\`\`python  
>     frames = \[process\_your\_file(f) for f in files\] result = pd.concat(frames)

<div class="note">

<div class="title">

Note

</div>

When concatenating <span class="title-ref">DataFrame</span> with named axes, pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for <span class="title-ref">MultiIndex</span>, but the logic is applied separately on a level-by-level basis.

</div>

Joining logic of the resulting axis `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The ``join``keyword specifies how to handle axis values that don't exist in the first `DataFrame`.``join='outer'`takes the union of all axis values  .. ipython:: python     df4 = pd.DataFrame(        {            "B": ["B2", "B3", "B6", "B7"],            "D": ["D2", "D3", "D6", "D7"],            "F": ["F2", "F3", "F6", "F7"],        },        index=[2, 3, 6, 7],    )    result = pd.concat([df1, df4], axis=1)    result   .. ipython:: python    :suppress:     @savefig merging_concat_axis1.png    p.plot([df1, df4], result, labels=["df1", "df4"], vertical=False);    plt.close("all");`join='inner'``takes the intersection of the axis values  .. ipython:: python     result = pd.concat([df1, df4], axis=1, join="inner")    result  .. ipython:: python    :suppress:     @savefig merging_concat_axis1_inner.png    p.plot([df1, df4], result, labels=["df1", "df4"], vertical=False);    plt.close("all");  To perform an effective "left" join using the *exact index* from the original `DataFrame`, result can be reindexed.  .. ipython:: python     result = pd.concat([df1, df4], axis=1).reindex(df1.index)    result  .. ipython:: python    :suppress:     @savefig merging_concat_axis1_join_axes.png    p.plot([df1, df4], result, labels=["df1", "df4"], vertical=False);    plt.close("all");  .. _merging.ignore_index:  Ignoring indexes on the concatenation axis ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  For `DataFrame` objects which don't have a meaningful index, the``ignore\_index``ignores overlapping indexes.  .. ipython:: python     result = pd.concat([df1, df4], ignore_index=True, sort=False)    result  .. ipython:: python    :suppress:     @savefig merging_concat_ignore_index.png    p.plot([df1, df4], result, labels=["df1", "df4"], vertical=True);    plt.close("all");  .. _merging.mixed_ndims:  Concatenating `Series` and `DataFrame` together ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You can concatenate a mix of `Series` and `DataFrame` objects. The `Series` will be transformed to `DataFrame` with the column name as the name of the `Series`.  .. ipython:: python     s1 = pd.Series(["X0", "X1", "X2", "X3"], name="X")    result = pd.concat([df1, s1], axis=1)    result  .. ipython:: python    :suppress:     @savefig merging_concat_mixed_ndim.png    p.plot([df1, s1], result, labels=["df1", "s1"], vertical=False);    plt.close("all");  Unnamed `Series` will be numbered consecutively.  .. ipython:: python     s2 = pd.Series(["_0", "_1", "_2", "_3"])    result = pd.concat([df1, s2, s2, s2], axis=1)    result  .. ipython:: python    :suppress:     @savefig merging_concat_unnamed_series.png    p.plot([df1, s2], result, labels=["df1", "s2"], vertical=False);    plt.close("all");``ignore\_index=True`will drop all name references.  .. ipython:: python     result = pd.concat([df1, s1], axis=1, ignore_index=True)    result  .. ipython:: python    :suppress:     @savefig merging_concat_series_ignore_index.png    p.plot([df1, s1], result, labels=["df1", "s1"], vertical=False);    plt.close("all");  Resulting`keys`~~~~~~~~~~~~~~~~~~  The`keys``argument adds another axis level to the resulting index or column (creating a `MultiIndex`) associate specific keys with each original `DataFrame`.  .. ipython:: python     result = pd.concat(frames, keys=["x", "y", "z"])    result    result.loc["y"]  .. ipython:: python    :suppress:     @savefig merging_concat_keys.png    p.plot(frames, result, labels=["df1", "df2", "df3"], vertical=True)    plt.close("all");  The``keys``argument can override the column names when creating a new `DataFrame` based on existing `Series`.  .. ipython:: python     s3 = pd.Series([0, 1, 2, 3], name="foo")    s4 = pd.Series([0, 1, 2, 3])    s5 = pd.Series([0, 1, 4, 5])     pd.concat([s3, s4, s5], axis=1)    pd.concat([s3, s4, s5], axis=1, keys=["red", "blue", "yellow"])  You can also pass a dict to `concat` in which case the dict keys will be used for the``keys`argument unless other`keys``argument is specified:  .. ipython:: python     pieces = {"x": df1, "y": df2, "z": df3}    result = pd.concat(pieces)    result  .. ipython:: python    :suppress:     @savefig merging_concat_dict.png    p.plot([df1, df2, df3], result, labels=["df1", "df2", "df3"], vertical=True);    plt.close("all");  .. ipython:: python     result = pd.concat(pieces, keys=["z", "y"])    result  .. ipython:: python    :suppress:     @savefig merging_concat_dict_keys.png    p.plot([df1, df2, df3], result, labels=["df1", "df2", "df3"], vertical=True);    plt.close("all");  The `MultiIndex` created has levels that are constructed from the passed keys and the index of the `DataFrame` pieces:  .. ipython:: python     result.index.levels``levels`argument allows specifying resulting levels associated with the`keys``.. ipython:: python     result = pd.concat(        pieces, keys=["x", "y", "z"], levels=[["z", "y", "x", "w"]], names=["group_key"]    )    result  .. ipython:: python    :suppress:     @savefig merging_concat_dict_keys_names.png    p.plot([df1, df2, df3], result, labels=["df1", "df2", "df3"], vertical=True);    plt.close("all");  .. ipython:: python     result.index.levels  .. _merging.append.row:  Appending rows to a `DataFrame` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  If you have a `Series` that you want to append as a single row to a `DataFrame`, you can convert the row into a `DataFrame` and use `concat`  .. ipython:: python     s2 = pd.Series(["X0", "X1", "X2", "X3"], index=["A", "B", "C", "D"])    result = pd.concat([df1, s2.to_frame().T], ignore_index=True)    result  .. ipython:: python    :suppress:     @savefig merging_append_series_as_row.png    p.plot([df1, s2], result, labels=["df1", "s2"], vertical=True);    plt.close("all");  .. _merging.join:  `~pandas.merge` ---------------------  `~pandas.merge` performs join operations similar to relational databases like SQL. Users who are familiar with SQL but new to pandas can reference a [comparison with SQL<compare_with_sql.join>](#comparison-with-sql<compare_with_sql.join>).  Merge types ~~~~~~~~~~~  `~pandas.merge` implements common SQL style joining operations.  * **one-to-one**: joining two `DataFrame` objects on   their indexes which must contain unique values. * **many-to-one**: joining a unique index to one or   more columns in a different `DataFrame`. * **many-to-many** : joining columns on columns.  > **Note** >     When joining columns on columns, potentially a many-to-many join, any    indexes on the passed `DataFrame` objects **will be discarded**.   For a **many-to-many** join, if a key combination appears more than once in both tables, the `DataFrame` will have the **Cartesian product** of the associated data.  .. ipython:: python     left = pd.DataFrame(        {            "key": ["K0", "K1", "K2", "K3"],            "A": ["A0", "A1", "A2", "A3"],            "B": ["B0", "B1", "B2", "B3"],        }    )     right = pd.DataFrame(        {            "key": ["K0", "K1", "K2", "K3"],            "C": ["C0", "C1", "C2", "C3"],            "D": ["D0", "D1", "D2", "D3"],        }    )    result = pd.merge(left, right, on="key")    result  .. ipython:: python    :suppress:     @savefig merging_merge_on_key.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  The``how``argument to `~pandas.merge` specifies which keys are included in the resulting table. If a key combination **does not appear** in either the left or right tables, the values in the joined table will be``NA`. Here is a summary of the`how`options and their SQL equivalent names:  .. csv-table::     :header: "Merge method", "SQL Join Name", "Description"     :widths: 20, 20, 60`left`,`LEFT OUTER JOIN`, Use keys from left frame only`right`,`RIGHT OUTER JOIN`, Use keys from right frame only`outer`,`FULL OUTER JOIN`, Use union of keys from both frames`inner`,`INNER JOIN`, Use intersection of keys from both frames`cross`,`CROSS JOIN``, Create the cartesian product of rows of both frames  .. ipython:: python     left = pd.DataFrame(       {          "key1": ["K0", "K0", "K1", "K2"],          "key2": ["K0", "K1", "K0", "K1"],          "A": ["A0", "A1", "A2", "A3"],          "B": ["B0", "B1", "B2", "B3"],       }    )    right = pd.DataFrame(       {          "key1": ["K0", "K1", "K1", "K2"],          "key2": ["K0", "K0", "K0", "K0"],          "C": ["C0", "C1", "C2", "C3"],          "D": ["D0", "D1", "D2", "D3"],       }    )    result = pd.merge(left, right, how="left", on=["key1", "key2"])    result  .. ipython:: python    :suppress:     @savefig merging_merge_on_key_left.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = pd.merge(left, right, how="right", on=["key1", "key2"])    result  .. ipython:: python    :suppress:     @savefig merging_merge_on_key_right.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);  .. ipython:: python     result = pd.merge(left, right, how="outer", on=["key1", "key2"])    result  .. ipython:: python    :suppress:     @savefig merging_merge_on_key_outer.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = pd.merge(left, right, how="inner", on=["key1", "key2"])    result  .. ipython:: python    :suppress:     @savefig merging_merge_on_key_inner.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = pd.merge(left, right, how="cross")    result  .. ipython:: python    :suppress:     @savefig merging_merge_cross.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  You can merge `Series` and a `DataFrame` with a `MultiIndex` if the names of the `MultiIndex` correspond to the columns from the `DataFrame`. Transform the `Series` to a `DataFrame` using `Series.reset_index` before merging  .. ipython:: python     df = pd.DataFrame({"Let": ["A", "B", "C"], "Num": [1, 2, 3]})    df     ser = pd.Series(        ["a", "b", "c", "d", "e", "f"],        index=pd.MultiIndex.from_arrays(            [["A", "B", "C"] * 2, [1, 2, 3, 4, 5, 6]], names=["Let", "Num"]        ),    )    ser     pd.merge(df, ser.reset_index(), on=["Let", "Num"])   Performing an outer join with duplicate join keys in `DataFrame`  .. ipython:: python     left = pd.DataFrame({"A": [1, 2], "B": [2, 2]})     right = pd.DataFrame({"A": [4, 5, 6], "B": [2, 2, 2]})     result = pd.merge(left, right, on="B", how="outer")    result  .. ipython:: python    :suppress:     @savefig merging_merge_on_key_dup.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");   > **Warning** >    Merging on duplicate keys significantly increase the dimensions of the result   and can cause a memory overflow.  .. _merging.validation:  Merge key uniqueness ~~~~~~~~~~~~~~~~~~~~  The``validate``argument checks whether the uniqueness of merge keys. Key uniqueness is checked before merge operations and can protect against memory overflows and unexpected key duplication.  .. ipython:: python    :okexcept:     left = pd.DataFrame({"A": [1, 2], "B": [1, 2]})    right = pd.DataFrame({"A": [4, 5, 6], "B": [2, 2, 2]})    result = pd.merge(left, right, on="B", how="outer", validate="one_to_one")  If the user is aware of the duplicates in the right `DataFrame` but wants to ensure there are no duplicates in the left `DataFrame`, one can use the``validate='one\_to\_many'``argument instead, which will not raise an exception.  .. ipython:: python     pd.merge(left, right, on="B", how="outer", validate="one_to_many")   .. _merging.indicator:  Merge result indicator ~~~~~~~~~~~~~~~~~~~~~~  `~pandas.merge` accepts the argument``indicator`. If`True`, a Categorical-type column called`\_merge`will be added to the output object that takes on values:    ===================================   ================   Observation Origin`\_merge`value   ===================================   ================   Merge key only in`'left'`frame`left\_only`Merge key only in`'right'`frame`right\_only`Merge key in both frames`both`===================================   ================  .. ipython:: python     df1 = pd.DataFrame({"col1": [0, 1], "col_left": ["a", "b"]})    df2 = pd.DataFrame({"col1": [1, 2, 2], "col_right": [2, 2, 2]})    pd.merge(df1, df2, on="col1", how="outer", indicator=True)  A string argument to`indicator`will use the value as the name for the indicator column.  .. ipython:: python     pd.merge(df1, df2, on="col1", how="outer", indicator="indicator_column")   Overlapping value columns ~~~~~~~~~~~~~~~~~~~~~~~~~  The merge`suffixes``argument takes a tuple of list of strings to append to overlapping column names in the input `DataFrame` to disambiguate the result columns:  .. ipython:: python     left = pd.DataFrame({"k": ["K0", "K1", "K2"], "v": [1, 2, 3]})    right = pd.DataFrame({"k": ["K0", "K0", "K3"], "v": [4, 5, 6]})     result = pd.merge(left, right, on="k")    result  .. ipython:: python    :suppress:     @savefig merging_merge_overlapped.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = pd.merge(left, right, on="k", suffixes=("_l", "_r"))    result  .. ipython:: python    :suppress:     @savefig merging_merge_overlapped_suffix.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  `DataFrame.join` ----------------------  `DataFrame.join` combines the columns of multiple, potentially differently-indexed `DataFrame` into a single result `DataFrame`.  .. ipython:: python     left = pd.DataFrame(        {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]}, index=["K0", "K1", "K2"]    )     right = pd.DataFrame(        {"C": ["C0", "C2", "C3"], "D": ["D0", "D2", "D3"]}, index=["K0", "K2", "K3"]    )     result = left.join(right)    result  .. ipython:: python    :suppress:     @savefig merging_join.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = left.join(right, how="outer")    result  .. ipython:: python    :suppress:     @savefig merging_join_outer.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = left.join(right, how="inner")    result  .. ipython:: python    :suppress:     @savefig merging_join_inner.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  `DataFrame.join` takes an optional``on``argument which may be a column or multiple column names that the passed `DataFrame` is to be aligned.  .. ipython:: python     left = pd.DataFrame(        {            "A": ["A0", "A1", "A2", "A3"],            "B": ["B0", "B1", "B2", "B3"],            "key": ["K0", "K1", "K0", "K1"],        }    )     right = pd.DataFrame({"C": ["C0", "C1"], "D": ["D0", "D1"]}, index=["K0", "K1"])     result = left.join(right, on="key")    result  .. ipython:: python    :suppress:     @savefig merging_join_key_columns.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. ipython:: python     result = pd.merge(        left, right, left_on="key", right_index=True, how="left", sort=False    )    result  .. ipython:: python    :suppress:     @savefig merging_merge_key_columns.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. _merging.multikey_join:  To join on multiple keys, the passed `DataFrame` must have a `MultiIndex`:  .. ipython:: python     left = pd.DataFrame(        {            "A": ["A0", "A1", "A2", "A3"],            "B": ["B0", "B1", "B2", "B3"],            "key1": ["K0", "K0", "K1", "K2"],            "key2": ["K0", "K1", "K0", "K1"],        }    )     index = pd.MultiIndex.from_tuples(        [("K0", "K0"), ("K1", "K0"), ("K2", "K0"), ("K2", "K1")]    )    right = pd.DataFrame(        {"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]}, index=index    )    result = left.join(right, on=["key1", "key2"])    result  .. ipython:: python    :suppress:     @savefig merging_join_multikeys.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. _merging.df_inner_join:  The default for `DataFrame.join` is to perform a left join which uses only the keys found in the calling `DataFrame`. Other join types can be specified with``how``.  .. ipython:: python     result = left.join(right, on=["key1", "key2"], how="inner")    result  .. ipython:: python    :suppress:     @savefig merging_join_multikeys_inner.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. _merging.join_on_mi:  Joining a single Index to a MultiIndex ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You can join a `DataFrame` with a `Index` to a `DataFrame` with a `MultiIndex` on a level. The``name``of the `Index` will match the level name of the `MultiIndex`.  ..  ipython:: python      left = pd.DataFrame(         {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]},         index=pd.Index(["K0", "K1", "K2"], name="key"),     )      index = pd.MultiIndex.from_tuples(         [("K0", "Y0"), ("K1", "Y1"), ("K2", "Y2"), ("K2", "Y3")],         names=["key", "Y"],     )     right = pd.DataFrame(         {"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]},         index=index,     )      result = left.join(right, how="inner")     result   .. ipython:: python    :suppress:     @savefig merging_join_multiindex_inner.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. _merging.join_with_two_multi_indexes:  Joining with two `MultiIndex` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The `MultiIndex` of the input argument must be completely used in the join and is a subset of the indices in the left argument.  .. ipython:: python     leftindex = pd.MultiIndex.from_product(        [list("abc"), list("xy"), [1, 2]], names=["abc", "xy", "num"]    )    left = pd.DataFrame({"v1": range(12)}, index=leftindex)    left     rightindex = pd.MultiIndex.from_product(        [list("abc"), list("xy")], names=["abc", "xy"]    )    right = pd.DataFrame({"v2": [100 * i for i in range(1, 7)]}, index=rightindex)    right     left.join(right, on=["abc", "xy"], how="inner")  .. ipython:: python     leftindex = pd.MultiIndex.from_tuples(        [("K0", "X0"), ("K0", "X1"), ("K1", "X2")], names=["key", "X"]    )    left = pd.DataFrame(        {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]}, index=leftindex    )     rightindex = pd.MultiIndex.from_tuples(        [("K0", "Y0"), ("K1", "Y1"), ("K2", "Y2"), ("K2", "Y3")], names=["key", "Y"]    )    right = pd.DataFrame(        {"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]}, index=rightindex    )     result = pd.merge(        left.reset_index(), right.reset_index(), on=["key"], how="inner"    ).set_index(["key", "X", "Y"])    result  .. ipython:: python    :suppress:     @savefig merging_merge_two_multiindex.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  .. _merging.merge_on_columns_and_levels:  Merging on a combination of columns and index levels ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Strings passed as the``on`,`left\_on`, and`right\_on``parameters may refer to either column names or index level names.  This enables merging `DataFrame` instances on a combination of index levels and columns without resetting indexes.  .. ipython:: python     left_index = pd.Index(["K0", "K0", "K1", "K2"], name="key1")     left = pd.DataFrame(        {            "A": ["A0", "A1", "A2", "A3"],            "B": ["B0", "B1", "B2", "B3"],            "key2": ["K0", "K1", "K0", "K1"],        },        index=left_index,    )     right_index = pd.Index(["K0", "K1", "K2", "K2"], name="key1")     right = pd.DataFrame(        {            "C": ["C0", "C1", "C2", "C3"],            "D": ["D0", "D1", "D2", "D3"],            "key2": ["K0", "K0", "K0", "K1"],        },        index=right_index,    )     result = left.merge(right, on=["key1", "key2"])    result  .. ipython:: python    :suppress:     @savefig merge_on_index_and_column.png    p.plot([left, right], result, labels=["left", "right"], vertical=False);    plt.close("all");  > **Note** >     When `DataFrame` are joined on a string that matches an index level in both    arguments, the index level is preserved as an index level in the resulting    `DataFrame`.  .. note::     When `DataFrame` are joined using only some of the levels of a `MultiIndex`,    the extra levels will be dropped from the resulting join. To    preserve those levels, use `DataFrame.reset_index` on those level    names to move those levels to columns prior to the join.  .. _merging.multiple_join:  Joining multiple `DataFrame` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  A list or tuple of``<span class="title-ref">DataFrame</span>``can also be passed to `~DataFrame.join` to join them together on their indexes.  .. ipython:: python     right2 = pd.DataFrame({"v": [7, 8, 9]}, index=["K1", "K1", "K2"])    result = left.join([right, right2])  .. ipython:: python    :suppress:     @savefig merging_join_multi_df.png    p.plot(        [left, right, right2],        result,        labels=["left", "right", "right2"],        vertical=False,    );    plt.close("all");  .. _merging.combine_first.update:  `DataFrame.combine_first` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  `DataFrame.combine_first` update missing values from one `DataFrame` with the non-missing values in another `DataFrame` in the corresponding location.  .. ipython:: python     df1 = pd.DataFrame(        [[np.nan, 3.0, 5.0], [-4.6, np.nan, np.nan], [np.nan, 7.0, np.nan]]    )    df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5.0, 1.6, 4]], index=[1, 2])    result = df1.combine_first(df2)    result  .. ipython:: python    :suppress:     @savefig merging_combine_first.png    p.plot([df1, df2], result, labels=["df1", "df2"], vertical=False);    plt.close("all");  .. _merging.merge_ordered:  `merge_ordered` ---------------------  `merge_ordered` combines order data such as numeric or time series data with optional filling of missing data with``fill\_method``.  .. ipython:: python     left = pd.DataFrame(        {"k": ["K0", "K1", "K1", "K2"], "lv": [1, 2, 3, 4], "s": ["a", "b", "c", "d"]}    )     right = pd.DataFrame({"k": ["K1", "K2", "K4"], "rv": [1, 2, 3]})     pd.merge_ordered(left, right, fill_method="ffill", left_by="s")  .. _merging.merge_asof:  `merge_asof` ---------------------  `merge_asof` is similar to an ordered left-join except that matches are on the nearest key rather than equal keys. For each row in the``left`` `DataFrame`, the last row in the ``right`` `DataFrame` are selected where the ``on``key is less than the left's key. Both `DataFrame` must be sorted by the key.  Optionally an `merge_asof` can perform a group-wise merge by matching the``by`key in addition to the nearest match on the`on``key.  .. ipython:: python     trades = pd.DataFrame(        {            "time": pd.to_datetime(                [                    "20160525 13:30:00.023",                    "20160525 13:30:00.038",                    "20160525 13:30:00.048",                    "20160525 13:30:00.048",                    "20160525 13:30:00.048",                ]            ),            "ticker": ["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"],            "price": [51.95, 51.95, 720.77, 720.92, 98.00],            "quantity": [75, 155, 100, 100, 100],        },        columns=["time", "ticker", "price", "quantity"],    )     quotes = pd.DataFrame(        {            "time": pd.to_datetime(                [                    "20160525 13:30:00.023",                    "20160525 13:30:00.023",                    "20160525 13:30:00.030",                    "20160525 13:30:00.041",                    "20160525 13:30:00.048",                    "20160525 13:30:00.049",                    "20160525 13:30:00.072",                    "20160525 13:30:00.075",                ]            ),            "ticker": ["GOOG", "MSFT", "MSFT", "MSFT", "GOOG", "AAPL", "GOOG", "MSFT"],            "bid": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],            "ask": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],        },        columns=["time", "ticker", "bid", "ask"],    )    trades    quotes    pd.merge_asof(trades, quotes, on="time", by="ticker")  `merge_asof` within``2ms``between the quote time and the trade time.  .. ipython:: python     pd.merge_asof(trades, quotes, on="time", by="ticker", tolerance=pd.Timedelta("2ms"))  `merge_asof` within``10ms``between the quote time and the trade time and exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes **do** propagate to that point in time.  .. ipython:: python     pd.merge_asof(        trades,        quotes,        on="time",        by="ticker",        tolerance=pd.Timedelta("10ms"),        allow_exact_matches=False,    )  .. _merging.compare:  `~Series.compare` -----------------------  The `Series.compare` and `DataFrame.compare` methods allow you to compare two `DataFrame` or `Series`, respectively, and summarize their differences.  .. ipython:: python     df = pd.DataFrame(        {            "col1": ["a", "a", "b", "b", "a"],            "col2": [1.0, 2.0, 3.0, np.nan, 5.0],            "col3": [1.0, 2.0, 3.0, 4.0, 5.0],        },        columns=["col1", "col2", "col3"],    )    df    df2 = df.copy()    df2.loc[0, "col1"] = "c"    df2.loc[2, "col3"] = 4.0    df2    df.compare(df2)  By default, if two corresponding values are equal, they will be shown as``NaN`. Furthermore, if all values in an entire row / column are equal, that row / column will be omitted from the result. The remaining differences will be aligned on columns.  Stack the differences on rows.  .. ipython:: python     df.compare(df2, align_axis=0)  Keep all original rows and columns with`keep\_shape=True\`\`

<div class="ipython">

python

df.compare(df2, keep\_shape=True)

</div>

Keep all the original values even if they are equal.

<div class="ipython">

python

df.compare(df2, keep\_shape=True, keep\_equal=True)

</div>

---

missing_data.md

---

<div id="missing_data">

{{ header }}

</div>

# Working with missing data

## Values considered "missing"

pandas uses different sentinel values to represent a missing (also referred to as NA) depending on the data type.

`numpy.nan` for NumPy data types. The disadvantage of using NumPy data types is that the original data type will be coerced to `np.float64` or `object`.

<div class="ipython">

python

pd.Series(\[1, 2\], dtype=np.int64).reindex(\[0, 1, 2\]) pd.Series(\[True, False\], dtype=np.[bool]()).reindex(\[0, 1, 2\])

</div>

<span class="title-ref">NaT</span> for NumPy `np.datetime64`, `np.timedelta64`, and <span class="title-ref">PeriodDtype</span>. For typing applications, use <span class="title-ref">api.typing.NaTType</span>.

<div class="ipython">

python

pd.Series(\[1, 2\], dtype=np.dtype("timedelta64\[ns\]")).reindex(\[0, 1, 2\]) pd.Series(\[1, 2\], dtype=np.dtype("datetime64\[ns\]")).reindex(\[0, 1, 2\]) pd.Series(\["2020", "2020"\], dtype=pd.PeriodDtype("D")).reindex(\[0, 1, 2\])

</div>

<span class="title-ref">NA</span> for <span class="title-ref">StringDtype</span>, <span class="title-ref">Int64Dtype</span> (and other bit widths), <span class="title-ref">Float64Dtype</span> (and other bit widths), <span class="title-ref">BooleanDtype</span> and <span class="title-ref">ArrowDtype</span>. These types will maintain the original data type of the data. For typing applications, use <span class="title-ref">api.typing.NAType</span>.

<div class="ipython">

python

pd.Series(\[1, 2\], dtype="Int64").reindex(\[0, 1, 2\]) pd.Series(\[True, False\], dtype="boolean\[pyarrow\]").reindex(\[0, 1, 2\])

</div>

To detect these missing value, use the <span class="title-ref">isna</span> or <span class="title-ref">notna</span> methods.

<div class="ipython">

python

ser = pd.Series(\[pd.Timestamp("2020-01-01"), pd.NaT\]) ser pd.isna(ser)

</div>

\> **Note** \> <span class="title-ref">isna</span> or <span class="title-ref">notna</span> will also consider `None` a missing value.

> 
> 
> <div class="ipython">
> 
> python
> 
> ser = pd.Series(\[1, None\], dtype=object) ser pd.isna(ser)
> 
> </div>

\> **Warning** \> Equality comparisons between `np.nan`, <span class="title-ref">NaT</span>, and <span class="title-ref">NA</span> do not act like `None`

> 
> 
> <div class="ipython">
> 
> python
> 
> None == None \# noqa: E711 np.nan == np.nan pd.NaT == pd.NaT pd.NA == pd.NA
> 
> </div>
> 
> Therefore, an equality comparison between a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with one of these missing values does not provide the same information as <span class="title-ref">isna</span> or <span class="title-ref">notna</span>.
> 
> <div class="ipython">
> 
> python
> 
> ser = pd.Series(\[True, None\], dtype="boolean\[pyarrow\]") ser == pd.NA pd.isna(ser)
> 
> </div>

## <span class="title-ref">NA</span> semantics

\> **Warning** \> Experimental: the behaviour of <span class="title-ref">NA</span> can still change without warning.

Starting from pandas 1.0, an experimental <span class="title-ref">NA</span> value (singleton) is available to represent scalar missing values. The goal of <span class="title-ref">NA</span> is provide a "missing" indicator that can be used consistently across data types (instead of `np.nan`, `None` or `pd.NaT` depending on the data type).

For example, when having missing values in a <span class="title-ref">Series</span> with the nullable integer dtype, it will use \`NA\`:

<div class="ipython">

python

s = pd.Series(\[1, 2, None\], dtype="Int64") s s\[2\] s\[2\] is pd.NA

</div>

Currently, pandas does not use those data types using <span class="title-ref">NA</span> by default in a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span>, so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained in the \[conversion section \<missing\_data.NA.conversion\>\](\#conversion-section-\<missing\_data.na.conversion\>).

### Propagation in arithmetic and comparison operations

In general, missing values *propagate* in operations involving <span class="title-ref">NA</span>. When one of the operands is unknown, the outcome of the operation is also unknown.

For example, <span class="title-ref">NA</span> propagates in arithmetic operations, similarly to `np.nan`:

<div class="ipython">

python

pd.NA + 1 "a" \* pd.NA

</div>

There are a few special cases when the result is known, even when one of the operands is `NA`.

<div class="ipython">

python

pd.NA \*\* 0 1 \*\* pd.NA

</div>

In equality and comparison operations, <span class="title-ref">NA</span> also propagates. This deviates from the behaviour of `np.nan`, where comparisons with `np.nan` always return `False`.

<div class="ipython">

python

pd.NA == 1 pd.NA == pd.NA pd.NA \< 2.5

</div>

To check if a value is equal to <span class="title-ref">NA</span>, use <span class="title-ref">isna</span>

<div class="ipython">

python

pd.isna(pd.NA)

</div>

\> **Note** \> An exception on this basic propagation rule are *reductions* (such as the mean or the minimum), where pandas defaults to skipping missing values. See the \[calculation section \<missing\_data.calculations\>\](\#calculation-section-\<missing\_data.calculations\>) for more.

### Logical operations

For logical operations, <span class="title-ref">NA</span> follows the rules of the [three-valued logic](https://en.wikipedia.org/wiki/Three-valued_logic) (or *Kleene logic*, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required.

For example, for the logical "or" operation (`|`), if one of the operands is `True`, we already know the result will be `True`, regardless of the other value (so regardless the missing value would be `True` or `False`). In this case, <span class="title-ref">NA</span> does not propagate:

<div class="ipython">

python

True | False True | pd.NA pd.NA | True

</div>

On the other hand, if one of the operands is `False`, the result depends on the value of the other operand. Therefore, in this case <span class="title-ref">NA</span> propagates:

<div class="ipython">

python

False | True False | False False | pd.NA

</div>

The behaviour of the logical "and" operation (`&`) can be derived using similar logic (where now <span class="title-ref">NA</span> will not propagate if one of the operands is already `False`):

<div class="ipython">

python

False & True False & False False & pd.NA

</div>

<div class="ipython">

python

True & True True & False True & pd.NA

</div>

### `NA` in a boolean context

Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value.

<div class="ipython" data-okexcept="">

python

bool(pd.NA)

</div>

This also means that <span class="title-ref">NA</span> cannot be used in a context where it is evaluated to a boolean, such as `if condition: ...` where `condition` can potentially be <span class="title-ref">NA</span>. In such cases, <span class="title-ref">isna</span> can be used to check for <span class="title-ref">NA</span> or `condition` being <span class="title-ref">NA</span> can be avoided, for example by filling missing values beforehand.

A similar situation occurs when using <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> objects in `if` statements, see \[gotchas.truth\](\#gotchas.truth).

### NumPy ufuncs

<span class="title-ref">pandas.NA</span> implements NumPy's `__array_ufunc__` protocol. Most ufuncs work with `NA`, and generally return `NA`:

<div class="ipython">

python

np.log(pd.NA) np.add(pd.NA, 1)

</div>

\> **Warning** \> Currently, ufuncs involving an ndarray and `NA` will return an object-dtype filled with NA values.

> 
> 
> <div class="ipython">
> 
> python
> 
> a = np.array(\[1, 2, 3\]) np.greater(a, pd.NA)
> 
> </div>
> 
> The return type here may change to return a different array type in the future.

See \[dsintro.numpy\_interop\](\#dsintro.numpy\_interop) for more on ufuncs.

#### Conversion

If you have a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> using `np.nan`, <span class="title-ref">DataFrame.convert\_dtypes</span> and <span class="title-ref">Series.convert\_dtypes</span>, respectively, will convert your data to use the nullable data types supporting <span class="title-ref">NA</span>, such as <span class="title-ref">Int64Dtype</span> or <span class="title-ref">ArrowDtype</span>. This is especially helpful after reading in data sets from IO methods where data types were inferred.

In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns.

<div class="ipython">

python

import io data = io.StringIO("a,bn,Truen2,") df = pd.read\_csv(data) df.dtypes df\_conv = df.convert\_dtypes() df\_conv df\_conv.dtypes

</div>

## Inserting missing data

You can insert missing values by simply assigning to a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span>. The missing value sentinel used will be chosen based on the dtype.

<div class="ipython">

python

ser = pd.Series(\[1., 2., 3.\]) ser.loc\[0\] = None ser

ser = pd.Series(\[pd.Timestamp("2021"), pd.Timestamp("2021")\]) ser.iloc\[0\] = np.nan ser

ser = pd.Series(\[True, False\], dtype="boolean\[pyarrow\]") ser.iloc\[0\] = None ser

</div>

For `object` types, pandas will use the value given:

<div class="ipython">

python

s = pd.Series(\["a", "b", "c"\], dtype=object) s.loc\[0\] = None s.loc\[1\] = np.nan s

</div>

## Calculations with missing data

Missing values propagate through arithmetic operations between pandas objects.

<div class="ipython">

python

ser1 = pd.Series(\[np.nan, np.nan, 2, 3\]) ser2 = pd.Series(\[np.nan, 1, np.nan, 4\]) ser1 ser2 ser1 + ser2

</div>

The descriptive statistics and computational methods discussed in the \[data structure overview \<basics.stats\>\](\#data-structure-overview-\<basics.stats\>) (and listed \[here \<api.series.stats\>\](\#here \<api.series.stats\>) and \[here \<api.dataframe.stats\>\](\#here-\<api.dataframe.stats\>)) all account for missing data.

When summing data, NA values or empty data will be treated as zero.

<div class="ipython">

python

pd.Series(\[np.nan\]).sum() pd.Series(\[\], dtype="float64").sum()

</div>

When taking the product, NA values or empty data will be treated as 1.

<div class="ipython">

python

pd.Series(\[np.nan\]).prod() pd.Series(\[\], dtype="float64").prod()

</div>

Cumulative methods like <span class="title-ref">\~DataFrame.cumsum</span> and <span class="title-ref">\~DataFrame.cumprod</span> ignore NA values by default, but preserve them in the resulting array. To override this behaviour and include NA values in the calculation, use `skipna=False`.

<div class="ipython">

python

ser = pd.Series(\[1, np.nan, 3, np.nan\]) ser ser.cumsum() ser.cumsum(skipna=False)

</div>

## Dropping missing data

<span class="title-ref">\~DataFrame.dropna</span> drops rows or columns with missing data.

<div class="ipython">

python

df = pd.DataFrame(\[\[np.nan, 1, 2\], \[1, 2, np.nan\], \[1, 2, 3\]\]) df df.dropna() df.dropna(axis=1)

ser = pd.Series(\[1, pd.NA\], dtype="int64\[pyarrow\]") ser.dropna()

</div>

## Filling missing data

### Filling by value

<span class="title-ref">\~DataFrame.fillna</span> replaces NA values with non-NA data.

Replace NA with a scalar value

<div class="ipython">

python

data = {"np": \[1.0, np.nan, np.nan, 2\], "arrow": pd.array(\[1.0, pd.NA, pd.NA, 2\], dtype="float64\[pyarrow\]")} df = pd.DataFrame(data) df df.fillna(0)

</div>

When the data has object dtype, you can control what type of NA values are present.

<div class="ipython">

python

df = pd.DataFrame({"a": \[pd.NA, np.nan, None\]}, dtype=object) df df.fillna(None) df.fillna(np.nan) df.fillna(pd.NA)

</div>

However when the dtype is not object, these will all be replaced with the proper NA value for the dtype.

<div class="ipython">

python

data = {"np": \[1.0, np.nan, np.nan, 2\], "arrow": pd.array(\[1.0, pd.NA, pd.NA, 2\], dtype="float64\[pyarrow\]")} df = pd.DataFrame(data) df df.fillna(None) df.fillna(np.nan) df.fillna(pd.NA)

</div>

Fill gaps forward or backward

<div class="ipython">

python

df.ffill() df.bfill()

</div>

<div id="missing_data.fillna.limit">

Limit the number of NA values filled

</div>

<div class="ipython">

python

df.ffill(limit=1)

</div>

NA values can be replaced with corresponding value from a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> where the index and column aligns between the original object and the filled object.

<div class="ipython">

python

dff = pd.DataFrame(np.arange(30, dtype=np.float64).reshape(10, 3), columns=list("ABC")) dff.iloc\[3:5, 0\] = np.nan dff.iloc\[4:6, 1\] = np.nan dff.iloc\[5:8, 2\] = np.nan dff dff.fillna(dff.mean())

</div>

\> **Note** \> <span class="title-ref">DataFrame.where</span> can also be used to fill NA values.Same result as above.

> 
> 
> <div class="ipython">
> 
> python
> 
> dff.where(pd.notna(dff), dff.mean(), axis="columns")
> 
> </div>

### Interpolation

<span class="title-ref">DataFrame.interpolate</span> and <span class="title-ref">Series.interpolate</span> fills NA values using various interpolation methods.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "A": \[1, 2.1, np.nan, 4.7, 5.6, 6.8\], "B": \[0.25, np.nan, np.nan, 4, 12.2, 14.4\],
    
    }

) df df.interpolate()

idx = pd.date\_range("2020-01-01", periods=10, freq="D") data = np.random.default\_rng(2).integers(0, 10, 10).astype(np.float64) ts = pd.Series(data, index=idx) ts.iloc\[\[1, 2, 5, 6, 9\]\] = np.nan

ts @savefig series\_before\_interpolate.png ts.plot()

</div>

<div class="ipython">

python

ts.interpolate() @savefig series\_interpolate.png ts.interpolate().plot()

</div>

Interpolation relative to a <span class="title-ref">Timestamp</span> in the <span class="title-ref">DatetimeIndex</span> is available by setting `method="time"`

<div class="ipython">

python

ts2 = ts.iloc\[\[0, 1, 3, 7, 9\]\] ts2 ts2.interpolate() ts2.interpolate(method="time")

</div>

For a floating-point index, use `method='values'`:

<div class="ipython">

python

idx = \[0.0, 1.0, 10.0\] ser = pd.Series(\[0.0, np.nan, 10.0\], idx) ser ser.interpolate() ser.interpolate(method="values")

</div>

If you have [scipy](https://scipy.org/) installed, you can pass the name of a 1-d interpolation routine to `method`. as specified in the scipy interpolation [documentation](https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation) and reference [guide](https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). The appropriate interpolation method will depend on the data type.

\> **Tip** \> If you are dealing with a time series that is growing at an increasing rate, use `method='barycentric'`.

> If you have values approximating a cumulative distribution function, use `method='pchip'`.
> 
> To fill missing values with goal of smooth plotting use `method='akima'`.
> 
> <div class="ipython">
> 
> python
> 
>   - df = pd.DataFrame(
>     
>       - {  
>         "A": \[1, 2.1, np.nan, 4.7, 5.6, 6.8\], "B": \[0.25, np.nan, np.nan, 4, 12.2, 14.4\],
>     
>     }
> 
> ) df df.interpolate(method="barycentric") df.interpolate(method="pchip") df.interpolate(method="akima")
> 
> </div>

When interpolating via a polynomial or spline approximation, you must also specify the degree or order of the approximation:

<div class="ipython">

python

df.interpolate(method="spline", order=2) df.interpolate(method="polynomial", order=2)

</div>

Comparing several methods.

<div class="ipython">

python

np.random.seed(2)

ser = pd.Series(np.arange(1, 10.1, 0.25) \*\* 2 + np.random.randn(37)) missing = np.array(\[4, 13, 14, 15, 16, 17, 18, 20, 29\]) ser.iloc\[missing\] = np.nan methods = \["linear", "quadratic", "cubic"\]

df = pd.DataFrame({m: ser.interpolate(method=m) for m in methods}) @savefig compare\_interpolations.png df.plot()

</div>

Interpolating new observations from expanding data with <span class="title-ref">Series.reindex</span>.

<div class="ipython">

python

ser = pd.Series(np.sort(np.random.uniform(size=100)))

\# interpolate at new\_index new\_index = ser.index.union(pd.Index(\[49.25, 49.5, 49.75, 50.25, 50.5, 50.75\])) interp\_s = ser.reindex(new\_index).interpolate(method="pchip") interp\_s.loc\[49:51\]

</div>

#### Interpolation limits

<span class="title-ref">\~DataFrame.interpolate</span> accepts a `limit` keyword argument to limit the number of consecutive `NaN` values filled since the last valid observation

<div class="ipython">

python

ser = pd.Series(\[np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13, np.nan, np.nan\]) ser ser.interpolate() ser.interpolate(limit=1)

</div>

By default, `NaN` values are filled in a `forward` direction. Use `limit_direction` parameter to fill `backward` or from `both` directions.

<div class="ipython">

python

ser.interpolate(limit=1, limit\_direction="backward") ser.interpolate(limit=1, limit\_direction="both") ser.interpolate(limit\_direction="both")

</div>

By default, `NaN` values are filled whether they are surrounded by existing valid values or outside existing valid values. The `limit_area` parameter restricts filling to either inside or outside values.

<div class="ipython">

python

\# fill one consecutive inside value in both directions ser.interpolate(limit\_direction="both", limit\_area="inside", limit=1)

\# fill all consecutive outside values backward ser.interpolate(limit\_direction="backward", limit\_area="outside")

\# fill all consecutive outside values in both directions ser.interpolate(limit\_direction="both", limit\_area="outside")

</div>

### Replacing values

<span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> can be used similar to <span class="title-ref">Series.fillna</span> and <span class="title-ref">DataFrame.fillna</span> to replace or insert missing values.

<div class="ipython">

python

df = pd.DataFrame(np.eye(3)) df df\_missing = df.replace(0, np.nan) df\_missing df\_filled = df\_missing.replace(np.nan, 2) df\_filled

</div>

Replacing more than one value is possible by passing a list.

<div class="ipython">

python

df\_filled.replace(\[1, 44\], \[2, 28\])

</div>

Replacing using a mapping dict.

<div class="ipython">

python

df\_filled.replace({1: 44, 2: 28})

</div>

#### Regular expression replacement

\> **Note** \> Python strings prefixed with the `r` character such as `r'hello world'` are ["raw" strings](https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals). They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash, e.g., `r'\' == '\\'`.

Replace the '.' with `NaN`

<div class="ipython">

python

d = {"a": list(range(4)), "b": list("ab.."), "c": \["a", "b", np.nan, "d"\]} df = pd.DataFrame(d) df.replace(".", np.nan)

</div>

Replace the '.' with `NaN` with regular expression that removes surrounding whitespace

<div class="ipython">

python

df.replace(r"s*.s*", np.nan, regex=True)

</div>

Replace with a list of regexes.

<div class="ipython">

python

df.replace(\[r".", r"(a)"\], \["dot", r"1stuff"\], regex=True)

</div>

Replace with a regex in a mapping dict.

<div class="ipython">

python

df.replace({"b": r"s*.s*"}, {"b": np.nan}, regex=True)

</div>

Pass nested dictionaries of regular expressions that use the `regex` keyword.

<div class="ipython">

python

df.replace({"b": {"b": r""}}, regex=True) df.replace(regex={"b": {r"s*.s*": np.nan}}) df.replace({"b": r"s*(.)s*"}, {"b": r"1ty"}, regex=True)

</div>

Pass a list of regular expressions that will replace matches with a scalar.

<div class="ipython">

python

df.replace(\[r"s*.s*", r"a|b"\], "placeholder", regex=True)

</div>

All of the regular expression examples can also be passed with the `to_replace` argument as the `regex` argument. In this case the `value` argument must be passed explicitly by name or `regex` must be a nested dictionary.

<div class="ipython">

python

df.replace(regex=\[r"s*.s*", r"a|b"\], value="placeholder")

</div>

\> **Note** \> A regular expression object from `re.compile` is a valid input as well.

---

options.md

---

<div id="options">

{{ header }}

</div>

# Options and settings

## Overview

pandas has an options API to configure and customize global behavior related to <span class="title-ref">DataFrame</span> display, data behavior and more.

Options have a full "dotted-style", case-insensitive name (e.g. `display.max_rows`). You can get/set options directly as attributes of the top-level `options` attribute:

<div class="ipython">

python

import pandas as pd

pd.options.display.max\_rows pd.options.display.max\_rows = 999 pd.options.display.max\_rows

</div>

The API is composed of 5 relevant functions, available directly from the `pandas` namespace:

  - <span class="title-ref">\~pandas.get\_option</span> / <span class="title-ref">\~pandas.set\_option</span> - get/set the value of a single option.
  - <span class="title-ref">\~pandas.reset\_option</span> - reset one or more options to their default value.
  - <span class="title-ref">\~pandas.describe\_option</span> - print the descriptions of one or more options.
  - <span class="title-ref">\~pandas.option\_context</span> - execute a codeblock with a set of options that revert to prior settings after execution.

\> **Note** \> Developers can check out [pandas/core/config\_init.py](https://github.com/pandas-dev/pandas/blob/main/pandas/core/config_init.py) for more information.

All of the functions above accept a regexp pattern (`re.search` style) as an argument, to match an unambiguous substring:

<div class="ipython">

python

pd.get\_option("display.chop\_threshold") pd.set\_option("display.chop\_threshold", 2) pd.get\_option("display.chop\_threshold") pd.set\_option("chop", 4) pd.get\_option("display.chop\_threshold")

</div>

The following will **not work** because it matches multiple option names, e.g. `display.max_colwidth`, `display.max_rows`, `display.max_columns`:

<div class="ipython" data-okexcept="">

python

pd.get\_option("max")

</div>

\> **Warning** \> Using this form of shorthand may cause your code to break if new options with similar names are added in future versions.

<div class="ipython" data-suppress="" data-okwarning="">

python

pd.reset\_option("all")

</div>

## Available options

You can get a list of available options and their descriptions with <span class="title-ref">\~pandas.describe\_option</span>. When called with no argument <span class="title-ref">\~pandas.describe\_option</span> will print out the descriptions for all available options.

<div class="ipython">

python

pd.describe\_option()

</div>

## Getting and setting options

As described above, <span class="title-ref">\~pandas.get\_option</span> and <span class="title-ref">\~pandas.set\_option</span> are available from the pandas namespace. To change an option, call `set_option('option regex', new_value)`.

<div class="ipython">

python

pd.get\_option("mode.sim\_interactive") pd.set\_option("mode.sim\_interactive", True) pd.get\_option("mode.sim\_interactive")

</div>

\> **Note** \> The option `'mode.sim_interactive'` is mostly used for debugging purposes.

You can use <span class="title-ref">\~pandas.reset\_option</span> to revert to a setting's default value

<div class="ipython" data-suppress="">

python

pd.reset\_option("display.max\_rows")

</div>

<div class="ipython">

python

pd.get\_option("display.max\_rows") pd.set\_option("display.max\_rows", 999) pd.get\_option("display.max\_rows") pd.reset\_option("display.max\_rows") pd.get\_option("display.max\_rows")

</div>

It's also possible to reset multiple options at once (using a regex):

<div class="ipython" data-okwarning="">

python

pd.reset\_option("^display")

</div>

<span class="title-ref">\~pandas.option\_context</span> context manager has been exposed through the top-level API, allowing you to execute code with given option values. Option values are restored automatically when you exit the `with` block:

<div class="ipython">

python

  - with pd.option\_context("display.max\_rows", 10, "display.max\_columns", 5):  
    print(pd.get\_option("display.max\_rows")) print(pd.get\_option("display.max\_columns"))

print(pd.get\_option("display.max\_rows")) print(pd.get\_option("display.max\_columns"))

</div>

## Setting startup options in Python/IPython environment

Using startup scripts for the Python/IPython environment to import pandas and set options makes working with pandas more efficient. To do this, create a `.py` or `.ipy` script in the startup directory of the desired profile. An example where the startup folder is in a default IPython profile can be found at:

`` `none   $IPYTHONDIR/profile_default/startup  More information can be found in the `IPython documentation ``<https://ipython.org/ipython-doc/stable/interactive/tutorial.html#startup-files>. An example startup script for pandas is displayed below:

`` `python   import pandas as pd    pd.set_option("display.max_rows", 999)   pd.set_option("display.precision", 5)  .. _options.frequently_used:  Frequently used options ``\` -----------------------The following is a demonstrates the more frequently used display options.

`display.max_rows` and `display.max_columns` sets the maximum number of rows and columns displayed when a frame is pretty-printed. Truncated lines are replaced by an ellipsis.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(7, 2)) pd.set\_option("display.max\_rows", 7) df pd.set\_option("display.max\_rows", 5) df pd.reset\_option("display.max\_rows")

</div>

Once the `display.max_rows` is exceeded, the `display.min_rows` options determines how many rows are shown in the truncated repr.

<div class="ipython">

python

pd.set\_option("display.max\_rows", 8) pd.set\_option("display.min\_rows", 4) \# below max\_rows -\> all rows shown df = pd.DataFrame(np.random.randn(7, 2)) df \# above max\_rows -\> only min\_rows (4) rows shown df = pd.DataFrame(np.random.randn(9, 2)) df pd.reset\_option("display.max\_rows") pd.reset\_option("display.min\_rows")

</div>

`display.expand_frame_repr` allows for the representation of a <span class="title-ref">DataFrame</span> to stretch across pages, wrapped over the all the columns.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(5, 10)) pd.set\_option("expand\_frame\_repr", True) df pd.set\_option("expand\_frame\_repr", False) df pd.reset\_option("expand\_frame\_repr")

</div>

`display.large_repr` displays a <span class="title-ref">DataFrame</span> that exceed `max_columns` or `max_rows` as a truncated frame or summary.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 10)) pd.set\_option("display.max\_rows", 5) pd.set\_option("large\_repr", "truncate") df pd.set\_option("large\_repr", "info") df pd.reset\_option("large\_repr") pd.reset\_option("display.max\_rows")

</div>

`display.max_colwidth` sets the maximum width of columns. Cells of this length or longer will be truncated with an ellipsis.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - np.array(
        
          - \[  
            \["foo", "bar", "bim", "uncomfortably long string"\], \["horse", "cow", "banana", "apple"\],
        
        \]
    
    )

) pd.set\_option("max\_colwidth", 40) df pd.set\_option("max\_colwidth", 6) df pd.reset\_option("max\_colwidth")

</div>

`display.max_info_columns` sets a threshold for the number of columns displayed when calling <span class="title-ref">\~pandas.DataFrame.info</span>.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10, 10)) pd.set\_option("max\_info\_columns", 11) df.info() pd.set\_option("max\_info\_columns", 5) df.info() pd.reset\_option("max\_info\_columns")

</div>

`display.max_info_rows`: <span class="title-ref">\~pandas.DataFrame.info</span> will usually show null-counts for each column. For a large <span class="title-ref">DataFrame</span>, this can be quite slow. `max_info_rows` and `max_info_cols` limit this null check to the specified rows and columns respectively. The <span class="title-ref">\~pandas.DataFrame.info</span> keyword argument `show_counts=True` will override this.

<div class="ipython">

python

df = pd.DataFrame(np.random.choice(\[0, 1, np.nan\], size=(10, 10))) df pd.set\_option("max\_info\_rows", 11) df.info() pd.set\_option("max\_info\_rows", 5) df.info() pd.reset\_option("max\_info\_rows")

</div>

`display.precision` sets the output display precision in terms of decimal places.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(5, 5)) pd.set\_option("display.precision", 7) df pd.set\_option("display.precision", 4) df

</div>

`display.chop_threshold` sets the rounding threshold to zero when displaying a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span>. This setting does not change the precision at which the number is stored.

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(6, 6)) pd.set\_option("chop\_threshold", 0) df pd.set\_option("chop\_threshold", 0.5) df pd.reset\_option("chop\_threshold")

</div>

`display.colheader_justify` controls the justification of the headers. The options are `'right'`, and `'left'`.

<div class="ipython">

python

  - df = pd.DataFrame(  
    np.array(\[np.random.randn(6), np.random.randint(1, 9, 6) \* 0.1, np.zeros(6)\]).T, columns=\["A", "B", "C"\], dtype="float",

) pd.set\_option("colheader\_justify", "right") df pd.set\_option("colheader\_justify", "left") df pd.reset\_option("colheader\_justify")

</div>

## Number formatting

pandas also allows you to set how numbers are displayed in the console. This option is not set through the `set_options` API.

Use the `set_eng_float_format` function to alter the floating-point formatting of pandas objects to produce a particular format.

<div class="ipython">

python

import numpy as np

pd.set\_eng\_float\_format(accuracy=3, use\_eng\_prefix=True) s = pd.Series(np.random.randn(5), index=\["a", "b", "c", "d", "e"\]) s / 1.0e3 s / 1.0e6

</div>

<div class="ipython" data-suppress="" data-okwarning="">

python

pd.reset\_option("^display")

</div>

Use <span class="title-ref">\~pandas.DataFrame.round</span> to specifically control rounding of an individual <span class="title-ref">DataFrame</span>

## Unicode formatting

\> **Warning** \> Enabling this option will affect the performance for printing of DataFrame and Series (about 2 times slower). Use only when it is actually required.

Some East Asian countries use Unicode characters whose width corresponds to two Latin characters. If a DataFrame or Series contains these characters, the default output mode may not align them properly.

<div class="ipython">

python

df = pd.DataFrame({"å›½ç±": \["UK", "æ—¥æœ¬"\], "åå‰": \["Alice", "ã—ã®ã¶"\]}) df

</div>

Enabling `display.unicode.east_asian_width` allows pandas to check each character's "East Asian Width" property. These characters can be aligned properly by setting this option to `True`. However, this will result in longer render times than the standard `len` function.

<div class="ipython">

python

pd.set\_option("display.unicode.east\_asian\_width", True) df

</div>

In addition, Unicode characters whose width is "ambiguous" can either be 1 or 2 characters wide depending on the terminal setting or encoding. The option `display.unicode.ambiguous_as_wide` can be used to handle the ambiguity.

By default, an "ambiguous" character's width, such as "Â¡" (inverted exclamation) in the example below, is taken to be 1.

<div class="ipython">

python

df = pd.DataFrame({"a": \["xxx", "Â¡Â¡"\], "b": \["yyy", "Â¡Â¡"\]}) df

</div>

Enabling `display.unicode.ambiguous_as_wide` makes pandas interpret these characters' widths to be 2. (Note that this option will only be effective when `display.unicode.east_asian_width` is enabled.)

However, setting this option incorrectly for your terminal will cause these characters to be aligned incorrectly:

<div class="ipython">

python

pd.set\_option("display.unicode.ambiguous\_as\_wide", True) df

</div>

<div class="ipython" data-suppress="">

python

pd.set\_option("display.unicode.east\_asian\_width", False) pd.set\_option("display.unicode.ambiguous\_as\_wide", False)

</div>

## Table schema display

<span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> will publish a Table Schema representation by default. This can be enabled globally with the `display.html.table_schema` option:

<div class="ipython">

python

pd.set\_option("display.html.table\_schema", True)

</div>

Only `'display.max_rows'` are serialized and published.

<div class="ipython" data-suppress="">

python

pd.reset\_option("display.html.table\_schema")

</div>

---

pyarrow.md

---

<div id="pyarrow">

{{ header }}

</div>

# PyArrow Functionality

pandas can utilize [PyArrow](https://arrow.apache.org/docs/python/index.html) to extend functionality and improve the performance of various APIs. This includes:

  - More extensive [data types](https://arrow.apache.org/docs/python/api/datatypes.html) compared to NumPy
  - Missing data support (NA) for all data types
  - Performant IO reader integration
  - Facilitate interoperability with other dataframe libraries based on the Apache Arrow specification (e.g. polars, cuDF)

To use this functionality, please ensure you have \[installed the minimum supported PyArrow version. \<install.optional\_dependencies\>\](\#installed-the-minimum-supported-pyarrow-version.-\<install.optional\_dependencies\>)

## Data Structure Integration

A <span class="title-ref">Series</span>, <span class="title-ref">Index</span>, or the columns of a <span class="title-ref">DataFrame</span> can be directly backed by a :external+pyarrow:py\`pyarrow.ChunkedArray\` which is similar to a NumPy array. To construct these from the main pandas data structures, you can pass in a string of the type followed by `[pyarrow]`, e.g. `"int64[pyarrow]""` into the `dtype` parameter

<div class="ipython">

python

ser = pd.Series(\[-1.5, 0.2, None\], dtype="float32\[pyarrow\]") ser

idx = pd.Index(\[True, None\], dtype="bool\[pyarrow\]") idx

df = pd.DataFrame(\[\[1, 2\], \[3, 4\]\], dtype="uint64\[pyarrow\]") df

</div>

\> **Note** \> The string alias `"string[pyarrow]"` maps to `pd.StringDtype("pyarrow")` which is not equivalent to specifying `dtype=pd.ArrowDtype(pa.string())`. Generally, operations on the data will behave similarly except `pd.StringDtype("pyarrow")` can return NumPy-backed nullable types while `pd.ArrowDtype(pa.string())` will return <span class="title-ref">ArrowDtype</span>.

> 
> 
> <div class="ipython">
> 
> python
> 
> import pyarrow as pa data = list("abc") ser\_sd = pd.Series(data, dtype="string\[pyarrow\]") ser\_ad = pd.Series(data, dtype=pd.ArrowDtype(pa.string())) ser\_ad.dtype == ser\_sd.dtype ser\_sd.str.contains("a") ser\_ad.str.contains("a")
> 
> </div>

For PyArrow types that accept parameters, you can pass in a PyArrow type with those parameters into <span class="title-ref">ArrowDtype</span> to use in the `dtype` parameter.

<div class="ipython">

python

import pyarrow as pa list\_str\_type = [pa.list]()(pa.string()) ser = pd.Series(\[\["hello"\], \["there"\]\], dtype=pd.ArrowDtype(list\_str\_type)) ser

</div>

<div class="ipython">

python

from datetime import time idx = pd.Index(\[time(12, 30), None\], dtype=pd.ArrowDtype(pa.time64("us"))) idx

</div>

<div class="ipython">

python

from decimal import Decimal decimal\_type = pd.ArrowDtype(pa.decimal128(3, scale=2)) data = \[\[Decimal("3.19"), None\], \[None, Decimal("-1.23")\]\] df = pd.DataFrame(data, dtype=decimal\_type) df

</div>

If you already have an :external+pyarrow:py\`pyarrow.Array\` or :external+pyarrow:py\`pyarrow.ChunkedArray\`, you can pass it into <span class="title-ref">.arrays.ArrowExtensionArray</span> to construct the associated <span class="title-ref">Series</span>, <span class="title-ref">Index</span> or <span class="title-ref">DataFrame</span> object.

<div class="ipython">

python

  - pa\_array = pa.array(  
    \[{"1": "2"}, {"10": "20"}, None\], type=pa.[map]()(pa.string(), pa.string()),

) ser = pd.Series(pd.arrays.ArrowExtensionArray(pa\_array)) ser

</div>

To retrieve a pyarrow :external+pyarrow:py\`pyarrow.ChunkedArray\` from a <span class="title-ref">Series</span> or <span class="title-ref">Index</span>, you can call the pyarrow array constructor on the <span class="title-ref">Series</span> or <span class="title-ref">Index</span>.

<div class="ipython">

python

ser = pd.Series(\[1, 2, None\], dtype="uint8\[pyarrow\]") pa.array(ser)

idx = pd.Index(ser) pa.array(idx)

</div>

To convert a :external+pyarrow:py\`pyarrow.Table\` to a <span class="title-ref">DataFrame</span>, you can call the :external+pyarrow:py\`pyarrow.Table.to\_pandas\` method with `types_mapper=pd.ArrowDtype`.

<div class="ipython">

python

table = pa.table(\[pa.array(\[1, 2, 3\], type=pa.int64())\], names=\["a"\])

df = table.to\_pandas(types\_mapper=pd.ArrowDtype) df df.dtypes

</div>

## Operations

PyArrow data structure integration is implemented through pandas' <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span> \[interface \<extending.extension-types\>\](\#interface-\<extending.extension-types\>); therefore, supported functionality exists where this interface is integrated within the pandas API. Additionally, this functionality is accelerated with PyArrow [compute functions](https://arrow.apache.org/docs/python/api/compute.html) where available. This includes:

  - Numeric aggregations
  - Numeric arithmetic
  - Numeric rounding
  - Logical and comparison functions
  - String functionality
  - Datetime functionality

The following are just some examples of operations that are accelerated by native PyArrow compute functions.

<div class="ipython">

python

import pyarrow as pa ser = pd.Series(\[-1.545, 0.211, None\], dtype="float32\[pyarrow\]") ser.mean() ser + ser ser \> (ser + 1)

ser.dropna() ser.isna() ser.fillna(0)

</div>

<div class="ipython">

python

ser\_str = pd.Series(\["a", "b", None\], dtype=pd.ArrowDtype(pa.string())) ser\_str.str.startswith("a")

</div>

<div class="ipython">

python

from datetime import datetime pa\_type = pd.ArrowDtype(pa.timestamp("ns")) ser\_dt = pd.Series(\[datetime(2022, 1, 1), None\], dtype=pa\_type) ser\_dt.dt.strftime("%Y-%m")

</div>

## I/O Reading

PyArrow also provides IO reading functionality that has been integrated into several pandas IO readers. The following functions provide an `engine` keyword that can dispatch to PyArrow to accelerate reading from an IO source.

  - <span class="title-ref">read\_csv</span>
  - <span class="title-ref">read\_feather</span>
  - <span class="title-ref">read\_json</span>
  - <span class="title-ref">read\_orc</span>
  - <span class="title-ref">read\_parquet</span>
  - <span class="title-ref">read\_table</span> (experimental)

<div class="ipython">

python

import io data = io.StringIO("""a,b,c 1,2.5,True 3,4.5,False """) df = pd.read\_csv(data, engine="pyarrow") df

</div>

By default, these functions and all other IO reader functions return NumPy-backed data. These readers can return PyArrow-backed data by specifying the parameter `dtype_backend="pyarrow"`. A reader does not need to set `engine="pyarrow"` to necessarily return PyArrow-backed data.

<div class="ipython">

python

import io data = io.StringIO("""a,b,c,d,e,f,g,h,i 1,2.5,True,a,,,,, 3,4.5,False,b,6,7.5,True,a, """) df\_pyarrow = pd.read\_csv(data, dtype\_backend="pyarrow") df\_pyarrow.dtypes

</div>

Several non-IO reader functions can also use the `dtype_backend` argument to return PyArrow-backed data including:

  - <span class="title-ref">to\_numeric</span>
  - <span class="title-ref">DataFrame.convert\_dtypes</span>
  - <span class="title-ref">Series.convert\_dtypes</span>

---

reshaping.md

---

<div id="reshaping">

{{ header }}

</div>

# Reshaping and pivot tables

<div id="reshaping.reshaping">

pandas provides methods for manipulating a <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> to alter the representation of the data for further data processing or data summarization.

</div>

  - <span class="title-ref">\~pandas.pivot</span> and \`\~pandas.pivot\_table\`: Group unique values within one or more discrete categories.
  - <span class="title-ref">\~DataFrame.stack</span> and \`\~DataFrame.unstack\`: Pivot a column or row level to the opposite axis respectively.
  - <span class="title-ref">\~pandas.melt</span> and \`\~pandas.wide\_to\_long\`: Unpivot a wide <span class="title-ref">DataFrame</span> to a long format.
  - <span class="title-ref">\~pandas.get\_dummies</span> and \`\~pandas.from\_dummies\`: Conversions with indicator variables.
  - \`\~Series.explode\`: Convert a column of list-like values to individual rows.
  - \`\~pandas.crosstab\`: Calculate a cross-tabulation of multiple 1 dimensional factor arrays.
  - \`\~pandas.cut\`: Transform continuous variables to discrete, categorical values
  - \`\~pandas.factorize\`: Encode 1 dimensional variables into integer labels.

## <span class="title-ref">\~pandas.pivot</span> and <span class="title-ref">\~pandas.pivot\_table</span>

![image](../_static/reshaping_pivot.png)

### <span class="title-ref">\~pandas.pivot</span>

Data is often stored in so-called "stacked" or "record" format. In a "record" or "wide" format, typically there is one row for each subject. In the "stacked" or "long" format there are multiple rows for each subject where applicable.

<div class="ipython">

python

  - data = {  
    "value": range(12), "variable": \["A"\] \* 3 + \["B"\] \* 3 + \["C"\] \* 3 + \["D"\] \* 3, "date": pd.to\_datetime(\["2020-01-03", "2020-01-04", "2020-01-05"\] \* 4)

} df = pd.DataFrame(data)

</div>

To perform time series operations with each unique variable, a better representation would be where the `columns` are the unique variables and an `index` of dates identifies individual observations. To reshape the data into this form, we use the <span class="title-ref">DataFrame.pivot</span> method (also implemented as a top level function <span class="title-ref">\~pandas.pivot</span>):

<div class="ipython">

python

pivoted = df.pivot(index="date", columns="variable", values="value") pivoted

</div>

If the `values` argument is omitted, and the input <span class="title-ref">DataFrame</span> has more than one column of values which are not used as column or index inputs to <span class="title-ref">\~DataFrame.pivot</span>, then the resulting "pivoted" <span class="title-ref">DataFrame</span> will have \[hierarchical columns \<advanced.hierarchical\>\](\#hierarchical-columns \<advanced.hierarchical\>) whose topmost level indicates the respective value column:

<div class="ipython">

python

df\["value2"\] = df\["value"\] \* 2 pivoted = df.pivot(index="date", columns="variable") pivoted

</div>

You can then select subsets from the pivoted \`DataFrame\`:

<div class="ipython">

python

pivoted\["value2"\]

</div>

Note that this returns a view on the underlying data in the case where the data are homogeneously-typed.

\> **Note** \> <span class="title-ref">\~pandas.pivot</span> can only handle unique rows specified by `index` and `columns`. If you data contains duplicates, use <span class="title-ref">\~pandas.pivot\_table</span>.

### <span class="title-ref">\~pandas.pivot\_table</span>

While <span class="title-ref">\~DataFrame.pivot</span> provides general purpose pivoting with various data types, pandas also provides <span class="title-ref">\~pandas.pivot\_table</span> or <span class="title-ref">\~DataFrame.pivot\_table</span> for pivoting with aggregation of numeric data.

The function <span class="title-ref">\~pandas.pivot\_table</span> can be used to create spreadsheet-style pivot tables. See the \[cookbook\<cookbook.pivot\>\](\#cookbook\<cookbook.pivot\>) for some advanced strategies.

<div class="ipython">

python

import datetime

  - df = pd.DataFrame(
    
      - {  
        "A": \["one", "one", "two", "three"\] \* 6, "B": \["A", "B", "C"\] \* 8, "C": \["foo", "foo", "foo", "bar", "bar", "bar"\] \* 4, "D": np.random.randn(24), "E": np.random.randn(24), "F": \[datetime.datetime(2013, i, 1) for i in range(1, 13)\] + \[datetime.datetime(2013, i, 15) for i in range(1, 13)\],
    
    }

) df pd.pivot\_table(df, values="D", index=\["A", "B"\], columns=\["C"\]) pd.pivot\_table( df, values=\["D", "E"\], index=\["B"\], columns=\["A", "C"\], aggfunc="sum", ) pd.pivot\_table( df, values="E", index=\["B", "C"\], columns=\["A"\], aggfunc=\["sum", "mean"\], )

</div>

The result is a <span class="title-ref">DataFrame</span> potentially having a <span class="title-ref">MultiIndex</span> on the index or column. If the `values` column name is not given, the pivot table will include all of the data in an additional level of hierarchy in the columns:

<div class="ipython">

python

pd.pivot\_table(df\[\["A", "B", "C", "D", "E"\]\], index=\["A", "B"\], columns=\["C"\])

</div>

Also, you can use <span class="title-ref">Grouper</span> for `index` and `columns` keywords. For detail of <span class="title-ref">Grouper</span>, see \[Grouping with a Grouper specification \<groupby.specify\>\](\#grouping-with-a-grouper-specification-\<groupby.specify\>).

<div class="ipython">

python

pd.pivot\_table(df, values="D", index=pd.Grouper(freq="ME", key="F"), columns="C")

</div>

#### Adding margins

Passing `margins=True` to <span class="title-ref">\~DataFrame.pivot\_table</span> will add a row and column with an `All` label with partial group aggregates across the categories on the rows and columns:

<div class="ipython">

python

  - table = df.pivot\_table(  
    index=\["A", "B"\], columns="C", values=\["D", "E"\], margins=True, aggfunc="std"

) table

</div>

Additionally, you can call <span class="title-ref">DataFrame.stack</span> to display a pivoted DataFrame as having a multi-level index:

<div class="ipython">

python

table.stack()

</div>

## <span class="title-ref">\~DataFrame.stack</span> and <span class="title-ref">\~DataFrame.unstack</span>

![image](../_static/reshaping_stack.png)

Closely related to the <span class="title-ref">\~DataFrame.pivot</span> method are the related <span class="title-ref">\~DataFrame.stack</span> and <span class="title-ref">\~DataFrame.unstack</span> methods available on <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span>. These methods are designed to work together with <span class="title-ref">MultiIndex</span> objects (see the section on \[hierarchical indexing \<advanced.hierarchical\>\](\#hierarchical-indexing \<advanced.hierarchical\>)).

  - \`\~DataFrame.stack\`: "pivot" a level of the (possibly hierarchical) column labels, returning a <span class="title-ref">DataFrame</span> with an index with a new inner-most level of row labels.
  - \`\~DataFrame.unstack\`: (inverse operation of <span class="title-ref">\~DataFrame.stack</span>) "pivot" a level of the (possibly hierarchical) row index to the column axis, producing a reshaped <span class="title-ref">DataFrame</span> with a new inner-most level of column labels.

![image](../_static/reshaping_unstack.png)

<div class="ipython">

python

  - tuples = \[  
    \["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"\], \["one", "two", "one", "two", "one", "two", "one", "two"\],

\] index = pd.MultiIndex.from\_arrays(tuples, names=\["first", "second"\]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=\["A", "B"\]) df2 = df\[:4\] df2

</div>

The <span class="title-ref">\~DataFrame.stack</span> function "compresses" a level in the <span class="title-ref">DataFrame</span> columns to produce either:

  - A <span class="title-ref">Series</span>, in the case of a <span class="title-ref">Index</span> in the columns.
  - A <span class="title-ref">DataFrame</span>, in the case of a <span class="title-ref">MultiIndex</span> in the columns.

If the columns have a <span class="title-ref">MultiIndex</span>, you can choose which level to stack. The stacked level becomes the new lowest level in a <span class="title-ref">MultiIndex</span> on the columns:

<div class="ipython">

python

stacked = df2.stack() stacked

</div>

With a "stacked" <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> (having a <span class="title-ref">MultiIndex</span> as the `index`), the inverse operation of <span class="title-ref">\~DataFrame.stack</span> is <span class="title-ref">\~DataFrame.unstack</span>, which by default unstacks the **last level**:

<div class="ipython">

python

stacked.unstack() stacked.unstack(1) stacked.unstack(0)

</div>

<div id="reshaping.unstack_by_name">

![image](../_static/reshaping_unstack_1.png)

</div>

If the indexes have names, you can use the level names instead of specifying the level numbers:

<div class="ipython">

python

stacked.unstack("second")

</div>

![image](../_static/reshaping_unstack_0.png)

Notice that the <span class="title-ref">\~DataFrame.stack</span> and <span class="title-ref">\~DataFrame.unstack</span> methods implicitly sort the index levels involved. Hence a call to <span class="title-ref">\~DataFrame.stack</span> and then <span class="title-ref">\~DataFrame.unstack</span>, or vice versa, will result in a **sorted** copy of the original <span class="title-ref">DataFrame</span> or \`Series\`:

<div class="ipython">

python

index = pd.MultiIndex.from\_product(\[\[2, 1\], \["a", "b"\]\]) df = pd.DataFrame(np.random.randn(4), index=index, columns=\["A"\]) df all(df.unstack().stack() == df.sort\_index())

</div>

### Multiple levels

You may also stack or unstack more than one level at a time by passing a list of levels, in which case the end result is as if each level in the list were processed individually.

<div class="ipython">

python

  - columns = pd.MultiIndex.from\_tuples(
    
      - \[  
        ("A", "cat", "long"), ("B", "cat", "long"), ("A", "dog", "short"), ("B", "dog", "short"),
    
    \], names=\["exp", "animal", "hair\_length"\],

) df = pd.DataFrame(np.random.randn(4, 4), columns=columns) df

df.stack(level=\["animal", "hair\_length"\])

</div>

The list of levels can contain either level names or level numbers but not a mixture of the two.

<div class="ipython">

python

\# df.stack(level=\['animal', 'hair\_length'\]) \# from above is equivalent to: df.stack(level=\[1, 2\])

</div>

### Missing data

Unstacking can result in missing values if subgroups do not have the same set of labels. By default, missing values will be replaced with the default fill value for that data type.

<div class="ipython">

python

  - columns = pd.MultiIndex.from\_tuples(
    
      - \[  
        ("A", "cat"), ("B", "dog"), ("B", "cat"), ("A", "dog"),
    
    \], names=\["exp", "animal"\],

) index = pd.MultiIndex.from\_product( \[("bar", "baz", "foo", "qux"), ("one", "two")\], names=\["first", "second"\] ) df = pd.DataFrame(np.random.randn(8, 4), index=index, columns=columns) df3 = df.iloc\[\[0, 1, 4, 7\], \[1, 2\]\] df3 df3.unstack()

</div>

The missing value can be filled with a specific value with the `fill_value` argument.

<div class="ipython">

python

df3.unstack(fill\_value=-1e9)

</div>

## <span class="title-ref">\~pandas.melt</span> and <span class="title-ref">\~pandas.wide\_to\_long</span>

![image](../_static/reshaping_melt.png)

The top-level <span class="title-ref">\~pandas.melt</span> function and the corresponding <span class="title-ref">DataFrame.melt</span> are useful to massage a <span class="title-ref">DataFrame</span> into a format where one or more columns are *identifier variables*, while all other columns, considered *measured variables*, are "unpivoted" to the row axis, leaving just two non-identifier columns, "variable" and "value". The names of those columns can be customized by supplying the `var_name` and `value_name` parameters.

<div class="ipython">

python

  - cheese = pd.DataFrame(
    
      - {  
        "first": \["John", "Mary"\], "last": \["Doe", "Bo"\], "height": \[5.5, 6.0\], "weight": \[130, 150\],
    
    }

) cheese cheese.melt(id\_vars=\["first", "last"\]) cheese.melt(id\_vars=\["first", "last"\], var\_name="quantity")

</div>

When transforming a DataFrame using <span class="title-ref">\~pandas.melt</span>, the index will be ignored. The original index values can be kept by setting the `ignore_index=False` parameter to `False` (default is `True`). `ignore_index=False` will however duplicate index values.

<div class="ipython">

python

index = pd.MultiIndex.from\_tuples(\[("person", "A"), ("person", "B")\]) cheese = pd.DataFrame( { "first": \["John", "Mary"\], "last": \["Doe", "Bo"\], "height": \[5.5, 6.0\], "weight": \[130, 150\], }, index=index, ) cheese cheese.melt(id\_vars=\["first", "last"\]) cheese.melt(id\_vars=\["first", "last"\], ignore\_index=False)

</div>

<span class="title-ref">\~pandas.wide\_to\_long</span> is similar to <span class="title-ref">\~pandas.melt</span> with more customization for column matching.

<div class="ipython">

python

  - dft = pd.DataFrame(
    
      - {  
        "A1970": {0: "a", 1: "b", 2: "c"}, "A1980": {0: "d", 1: "e", 2: "f"}, "B1970": {0: 2.5, 1: 1.2, 2: 0.7}, "B1980": {0: 3.2, 1: 1.3, 2: 0.1}, "X": dict(zip(range(3), np.random.randn(3))),
    
    }

) dft\["id"\] = dft.index dft pd.wide\_to\_long(dft, \["A", "B"\], i="id", j="year")

</div>

## <span class="title-ref">\~pandas.get\_dummies</span> and <span class="title-ref">\~pandas.from\_dummies</span>

To convert categorical variables of a <span class="title-ref">Series</span> into a "dummy" or "indicator", <span class="title-ref">\~pandas.get\_dummies</span> creates a new <span class="title-ref">DataFrame</span> with columns of the unique variables and the values representing the presence of those variables per row.

<div class="ipython">

python

df = pd.DataFrame({"key": list("bbacab"), "data1": range(6)})

pd.get\_dummies(df\["key"\]) df\["key"\].str.get\_dummies()

</div>

`prefix` adds a prefix to the the column names which is useful for merging the result with the original \`DataFrame\`:

<div class="ipython">

python

dummies = pd.get\_dummies(df\["key"\], prefix="key") dummies

df\[\["data1"\]\].join(dummies)

</div>

This function is often used along with discretization functions like \`\~pandas.cut\`:

<div class="ipython">

python

values = np.random.randn(10) values

bins = \[0, 0.2, 0.4, 0.6, 0.8, 1\]

pd.get\_dummies(pd.cut(values, bins))

</div>

<span class="title-ref">get\_dummies</span> also accepts a <span class="title-ref">DataFrame</span>. By default, `object`, `string`, or `categorical` type columns are encoded as dummy variables with other columns unaltered.

<div class="ipython">

python

df = pd.DataFrame({"A": \["a", "b", "a"\], "B": \["c", "c", "b"\], "C": \[1, 2, 3\]}) pd.get\_dummies(df)

</div>

Specifying the `columns` keyword will encode a column of any type.

<div class="ipython">

python

pd.get\_dummies(df, columns=\["A"\])

</div>

As with the <span class="title-ref">Series</span> version, you can pass values for the `prefix` and `prefix_sep`. By default the column name is used as the prefix and `_` as the prefix separator. You can specify `prefix` and `prefix_sep` in 3 ways:

  - string: Use the same value for `prefix` or `prefix_sep` for each column to be encoded.
  - list: Must be the same length as the number of columns being encoded.
  - dict: Mapping column name to prefix.

<div class="ipython">

python

simple = pd.get\_dummies(df, prefix="new\_prefix") simple from\_list = pd.get\_dummies(df, prefix=\["from\_A", "from\_B"\]) from\_list from\_dict = pd.get\_dummies(df, prefix={"B": "from\_B", "A": "from\_A"}) from\_dict

</div>

To avoid collinearity when feeding the result to statistical models, specify `drop_first=True`.

<div class="ipython">

python

s = pd.Series(list("abcaa"))

pd.get\_dummies(s)

pd.get\_dummies(s, drop\_first=True)

</div>

When a column contains only one level, it will be omitted in the result.

<div class="ipython">

python

df = pd.DataFrame({"A": list("aaaaa"), "B": list("ababc")})

pd.get\_dummies(df)

pd.get\_dummies(df, drop\_first=True)

</div>

The values can be cast to a different type using the `dtype` argument.

<div class="ipython">

python

df = pd.DataFrame({"A": list("abc"), "B": \[1.1, 2.2, 3.3\]})

pd.get\_dummies(df, dtype=np.float32).dtypes

</div>

<div class="versionadded">

1.5.0

</div>

<span class="title-ref">\~pandas.from\_dummies</span> converts the output of <span class="title-ref">\~pandas.get\_dummies</span> back into a <span class="title-ref">Series</span> of categorical values from indicator values.

<div class="ipython">

python

df = pd.DataFrame({"prefix\_a": \[0, 1, 0\], "prefix\_b": \[1, 0, 1\]}) df

pd.from\_dummies(df, sep="\_")

</div>

Dummy coded data only requires `k - 1` categories to be included, in this case the last category is the default category. The default category can be modified with `default_category`.

<div class="ipython">

python

df = pd.DataFrame({"prefix\_a": \[0, 1, 0\]}) df

pd.from\_dummies(df, sep="\_", default\_category="b")

</div>

## <span class="title-ref">\~Series.explode</span>

For a <span class="title-ref">DataFrame</span> column with nested, list-like values, <span class="title-ref">\~Series.explode</span> will transform each list-like value to a separate row. The resulting <span class="title-ref">Index</span> will be duplicated corresponding to the index label from the original row:

<div class="ipython">

python

keys = \["panda1", "panda2", "panda3"\] values = \[\["eats", "shoots"\], \["shoots", "leaves"\], \["eats", "leaves"\]\] df = pd.DataFrame({"keys": keys, "values": values}) df df\["values"\].explode()

</div>

<span class="title-ref">DataFrame.explode</span> can also explode the column in the <span class="title-ref">DataFrame</span>.

<div class="ipython">

python

df.explode("values")

</div>

<span class="title-ref">Series.explode</span> will replace empty lists with a missing value indicator and preserve scalar entries.

<div class="ipython">

python

s = pd.Series(\[\[1, 2, 3\], "foo", \[\], \["a", "b"\]\]) s s.explode()

</div>

A comma-separated string value can be split into individual values in a list and then exploded to a new row.

<div class="ipython">

python

df = pd.DataFrame(\[{"var1": "a,b,c", "var2": 1}, {"var1": "d,e,f", "var2": 2}\]) df.assign(var1=df.var1.str.split(",")).explode("var1")

</div>

## <span class="title-ref">\~pandas.crosstab</span>

Use <span class="title-ref">\~pandas.crosstab</span> to compute a cross-tabulation of two (or more) factors. By default <span class="title-ref">\~pandas.crosstab</span> computes a frequency table of the factors unless an array of values and an aggregation function are passed.

Any <span class="title-ref">Series</span> passed will have their name attributes used unless row or column names for the cross-tabulation are specified

<div class="ipython">

python

a = np.array(\["foo", "foo", "bar", "bar", "foo", "foo"\], dtype=object) b = np.array(\["one", "one", "two", "one", "two", "one"\], dtype=object) c = np.array(\["dull", "dull", "shiny", "dull", "dull", "shiny"\], dtype=object) pd.crosstab(a, \[b, c\], rownames=\["a"\], colnames=\["b", "c"\])

</div>

If <span class="title-ref">\~pandas.crosstab</span> receives only two <span class="title-ref">Series</span>, it will provide a frequency table.

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"A": \[1, 2, 2, 2, 2\], "B": \[3, 3, 4, 4, 4\], "C": \[1, 1, np.nan, 1, 1\]}

) df

pd.crosstab(df\["A"\], df\["B"\])

</div>

<span class="title-ref">\~pandas.crosstab</span> can also summarize to <span class="title-ref">Categorical</span> data.

<div class="ipython">

python

foo = pd.Categorical(\["a", "b"\], categories=\["a", "b", "c"\]) bar = pd.Categorical(\["d", "e"\], categories=\["d", "e", "f"\]) pd.crosstab(foo, bar)

</div>

For <span class="title-ref">Categorical</span> data, to include **all** of data categories even if the actual data does not contain any instances of a particular category, use `dropna=False`.

<div class="ipython">

python

pd.crosstab(foo, bar, dropna=False)

</div>

### Normalization

Frequency tables can also be normalized to show percentages rather than counts using the `normalize` argument:

<div class="ipython">

python

pd.crosstab(df\["A"\], df\["B"\], normalize=True)

</div>

`normalize` can also normalize values within each row or within each column:

<div class="ipython">

python

pd.crosstab(df\["A"\], df\["B"\], normalize="columns")

</div>

<span class="title-ref">\~pandas.crosstab</span> can also accept a third <span class="title-ref">Series</span> and an aggregation function (`aggfunc`) that will be applied to the values of the third <span class="title-ref">Series</span> within each group defined by the first two \`Series\`:

<div class="ipython">

python

pd.crosstab(df\["A"\], df\["B"\], values=df\["C"\], aggfunc="sum")

</div>

### Adding margins

`margins=True` will add a row and column with an `All` label with partial group aggregates across the categories on the rows and columns:

<div class="ipython">

python

  - pd.crosstab(  
    df\["A"\], df\["B"\], values=df\["C"\], aggfunc="sum", normalize=True, margins=True

)

</div>

## <span class="title-ref">\~pandas.cut</span><span id="reshaping.tile"></span>

The <span class="title-ref">\~pandas.cut</span> function computes groupings for the values of the input array and is often used to transform continuous variables to discrete or categorical variables:

An integer `bins` will form equal-width bins.

<div class="ipython">

python

ages = np.array(\[10, 15, 13, 12, 23, 25, 28, 59, 60\])

pd.cut(ages, bins=3)

</div>

A list of ordered bin edges will assign an interval for each variable.

<div class="ipython">

python

pd.cut(ages, bins=\[0, 18, 35, 70\])

</div>

If the `bins` keyword is an <span class="title-ref">IntervalIndex</span>, then these will be used to bin the passed data.

<div class="ipython">

python

pd.cut(ages, bins=pd.IntervalIndex.from\_breaks(\[0, 40, 70\]))

</div>

## <span class="title-ref">\~pandas.factorize</span>

<span class="title-ref">\~pandas.factorize</span> encodes 1 dimensional values into integer labels. Missing values are encoded as `-1`.

<div class="ipython">

python

x = pd.Series(\["A", "A", np.nan, "B", 3.14, np.inf\]) x labels, uniques = pd.factorize(x) labels uniques

</div>

<span class="title-ref">Categorical</span> will similarly encode 1 dimensional values for further categorical operations

<div class="ipython">

python

pd.Categorical(x)

</div>

---

scale.md

---

# Scaling to large datasets

pandas provides data structures for in-memory analytics, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies.

This document provides a few recommendations for scaling your analysis to larger datasets. It's a complement to \[enhancingperf\](\#enhancingperf), which focuses on speeding up analysis for datasets that fit in memory.

## Load less data

Suppose our raw dataset on disk has many columns.

<div class="ipython" data-okwarning="">

python

import pandas as pd import numpy as np

  - def make\_timeseries(start="2000-01-01", end="2000-12-31", freq="1D", seed=None):  
    index = pd.date\_range(start=start, end=end, freq=freq, name="timestamp") n = len(index) state = np.random.RandomState(seed) columns = { "name": state.choice(\["Alice", "Bob", "Charlie"\], size=n), "id": state.poisson(1000, size=n), "x": state.rand(n) \* 2 - 1, "y": state.rand(n) \* 2 - 1, } df = pd.DataFrame(columns, index=index, columns=sorted(columns)) if df.index\[-1\] == end: df = df.iloc\[:-1\] return df

  - timeseries = \[  
    make\_timeseries(freq="1min", seed=i).rename(columns=lambda x: f"{x}\_{i}") for i in range(10)

\] ts\_wide = pd.concat(timeseries, axis=1) ts\_wide.head() ts\_wide.to\_parquet("timeseries\_wide.parquet")

</div>

To load the columns we want, we have two options. Option 1 loads in all the data and then filters to what we need.

<div class="ipython">

python

columns = \["id\_0", "name\_0", "x\_0", "y\_0"\]

pd.read\_parquet("timeseries\_wide.parquet")\[columns\]

</div>

Option 2 only loads the columns we request.

<div class="ipython">

python

pd.read\_parquet("timeseries\_wide.parquet", columns=columns)

</div>

<div class="ipython" data-suppress="">

python

import os

os.remove("timeseries\_wide.parquet")

</div>

If we were to measure the memory usage of the two calls, we'd see that specifying `columns` uses about 1/10th the memory in this case.

With <span class="title-ref">pandas.read\_csv</span>, you can specify `usecols` to limit the columns read into memory. Not all file formats that can be read by pandas provide an option to read a subset of columns.

## Use efficient datatypes

The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as "low-cardinality" data). By using more efficient data types, you can store larger datasets in memory.

<div class="ipython" data-okwarning="">

python

ts = make\_timeseries(freq="30s", seed=0) ts.to\_parquet("timeseries.parquet") ts = pd.read\_parquet("timeseries.parquet") ts

</div>

<div class="ipython" data-suppress="">

python

os.remove("timeseries.parquet")

</div>

Now, let's inspect the data types and memory usage to see where we should focus our attention.

<div class="ipython">

python

ts.dtypes

</div>

<div class="ipython">

python

ts.memory\_usage(deep=True) \# memory usage in bytes

</div>

The `name` column is taking up much more memory than any other. It has just a few unique values, so it's a good candidate for converting to a <span class="title-ref">pandas.Categorical</span>. With a <span class="title-ref">pandas.Categorical</span>, we store each unique name once and use space-efficient integers to know which specific name is used in each row.

<div class="ipython">

python

ts2 = ts.copy() ts2\["name"\] = ts2\["name"\].astype("category") ts2.memory\_usage(deep=True)

</div>

We can go a bit further and downcast the numeric columns to their smallest types using <span class="title-ref">pandas.to\_numeric</span>.

<div class="ipython">

python

ts2\["id"\] = pd.to\_numeric(ts2\["id"\], downcast="unsigned") ts2\[\["x", "y"\]\] = ts2\[\["x", "y"\]\].apply(pd.to\_numeric, downcast="float") ts2.dtypes

</div>

<div class="ipython">

python

ts2.memory\_usage(deep=True)

</div>

<div class="ipython">

python

reduction = ts2.memory\_usage(deep=True).sum() / ts.memory\_usage(deep=True).sum() print(f"{reduction:0.2f}")

</div>

In all, we've reduced the in-memory footprint of this dataset to 1/5 of its original size.

See \[categorical\](\#categorical) for more on <span class="title-ref">pandas.Categorical</span> and \[basics.dtypes\](\#basics.dtypes) for an overview of all of pandas' dtypes.

## Use chunking

Some workloads can be achieved with chunking by splitting a large problem into a bunch of small problems. For example, converting an individual CSV file into a Parquet file and repeating that for each file in a directory. As long as each chunk fits in memory, you can work with datasets that are much larger than memory.

\> **Note** \> Chunking works well when the operation you're performing requires zero or minimal coordination between chunks. For more complicated workflows, you're better off \[using other libraries \<scale.other\_libraries\>\](\#using-other-libraries-\<scale.other\_libraries\>).

Suppose we have an even larger "logical dataset" on disk that's a directory of parquet files. Each file in the directory represents a different year of the entire dataset.

<div class="ipython" data-okwarning="">

python

import pathlib

N = 12 starts = \[f"20{i:\>02d}-01-01" for i in range(N)\] ends = \[f"20{i:\>02d}-12-13" for i in range(N)\]

pathlib.Path("data/timeseries").mkdir(exist\_ok=True)

  - for i, (start, end) in enumerate(zip(starts, ends)):  
    ts = make\_timeseries(start=start, end=end, freq="1min", seed=i) ts.to\_parquet(f"data/timeseries/ts-{i:0\>2d}.parquet")

</div>

    data
    â””â”€â”€ timeseries
        â”œâ”€â”€ ts-00.parquet
        â”œâ”€â”€ ts-01.parquet
        â”œâ”€â”€ ts-02.parquet
        â”œâ”€â”€ ts-03.parquet
        â”œâ”€â”€ ts-04.parquet
        â”œâ”€â”€ ts-05.parquet
        â”œâ”€â”€ ts-06.parquet
        â”œâ”€â”€ ts-07.parquet
        â”œâ”€â”€ ts-08.parquet
        â”œâ”€â”€ ts-09.parquet
        â”œâ”€â”€ ts-10.parquet
        â””â”€â”€ ts-11.parquet

Now we'll implement an out-of-core <span class="title-ref">pandas.Series.value\_counts</span>. The peak memory usage of this workflow is the single largest chunk, plus a small series storing the unique value counts up to this point. As long as each individual file fits in memory, this will work for arbitrary-sized datasets.

<div class="ipython">

python

%%time files = pathlib.Path("data/timeseries/").glob("ts\*.parquet") counts = pd.Series(dtype=int) for path in files: df = pd.read\_parquet(path) counts = counts.add(df\["name"\].value\_counts(), fill\_value=0) counts.astype(int)

</div>

Some readers, like <span class="title-ref">pandas.read\_csv</span>, offer parameters to control the `chunksize` when reading a single file.

Manually chunking is an OK option for workflows that don't require too sophisticated of operations. Some operations, like <span class="title-ref">pandas.DataFrame.groupby</span>, are much harder to do chunkwise. In these cases, you may be better switching to a different library that implements these out-of-core algorithms for you.

## Use Other Libraries

There are other libraries which provide similar APIs to pandas and work nicely with pandas DataFrame, and can give you the ability to scale your large dataset processing and analytics by parallel runtime, distributed memory, clustering, etc. You can find more information in [the ecosystem page](https://pandas.pydata.org/community/ecosystem.html#out-of-core).

---

sparse.md

---

<div id="sparse">

{{ header }}

</div>

# Sparse data structures

pandas provides data structures for efficiently storing sparse data. These are not necessarily sparse in the typical "mostly 0". Rather, you can view these objects as being "compressed" where any data matching a specific value (`NaN` / missing value, though any value can be chosen, including 0) is omitted. The compressed values are not actually stored in the array.

<div class="ipython">

python

arr = np.random.randn(10) arr\[2:-2\] = np.nan ts = pd.Series(pd.arrays.SparseArray(arr)) ts

</div>

Notice the dtype, `Sparse[float64, nan]`. The `nan` means that elements in the array that are `nan` aren't actually stored, only the non-`nan` elements are. Those non-`nan` elements have a `float64` dtype.

The sparse objects exist for memory efficiency reasons. Suppose you had a large, mostly NA \`DataFrame\`:

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(10000, 4)) df.iloc\[:9998\] = np.nan sdf = df.astype(pd.SparseDtype("float", np.nan)) sdf.head() sdf.dtypes sdf.sparse.density

</div>

As you can see, the density (% of values that have not been "compressed") is extremely low. This sparse object takes up much less memory on disk (pickled) and in the Python interpreter.

<div class="ipython">

python

'dense : {:0.2f} bytes'.format(df.memory\_usage().sum() / 1e3) 'sparse: {:0.2f} bytes'.format(sdf.memory\_usage().sum() / 1e3)

</div>

Functionally, their behavior should be nearly identical to their dense counterparts.

## SparseArray

<span class="title-ref">arrays.SparseArray</span> is a <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span> for storing an array of sparse values (see \[basics.dtypes\](\#basics.dtypes) for more on extension arrays). It is a 1-dimensional ndarray-like object storing only values distinct from the `fill_value`:

<div class="ipython">

python

arr = np.random.randn(10) arr\[2:5\] = np.nan arr\[7:8\] = np.nan sparr = pd.arrays.SparseArray(arr) sparr

</div>

A sparse array can be converted to a regular (dense) ndarray with <span class="title-ref">numpy.asarray</span>

<div class="ipython">

python

np.asarray(sparr)

</div>

## SparseDtype

The <span class="title-ref">SparseArray.dtype</span> property stores two pieces of information

1.  The dtype of the non-sparse values
2.  The scalar fill value

<div class="ipython">

python

sparr.dtype

</div>

A <span class="title-ref">SparseDtype</span> may be constructed by passing only a dtype

<div class="ipython">

python

pd.SparseDtype(np.dtype('datetime64\[ns\]'))

</div>

in which case a default fill value will be used (for NumPy dtypes this is often the "missing" value for that dtype). To override this default an explicit fill value may be passed instead

<div class="ipython">

python

  - pd.SparseDtype(np.dtype('datetime64\[ns\]'),  
    fill\_value=pd.Timestamp('2017-01-01'))

</div>

Finally, the string alias `'Sparse[dtype]'` may be used to specify a sparse dtype in many places

<div class="ipython">

python

pd.array(\[1, 0, 0, 2\], dtype='Sparse\[int\]')

</div>

## Sparse accessor

pandas provides a `.sparse` accessor, similar to `.str` for string data, `.cat` for categorical data, and `.dt` for datetime-like data. This namespace provides attributes and methods that are specific to sparse data.

<div class="ipython">

python

s = pd.Series(\[0, 0, 1, 2\], dtype="Sparse\[int\]") s.sparse.density s.sparse.fill\_value

</div>

This accessor is available only on data with `SparseDtype`, and on the <span class="title-ref">Series</span> class itself for creating a Series with sparse data from a scipy COO matrix with.

A `.sparse` accessor has been added for <span class="title-ref">DataFrame</span> as well. See \[api.frame.sparse\](\#api.frame.sparse) for more.

## Sparse calculation

You can apply NumPy [ufuncs](https://numpy.org/doc/stable/reference/ufuncs.html) to <span class="title-ref">arrays.SparseArray</span> and get a <span class="title-ref">arrays.SparseArray</span> as a result.

<div class="ipython">

python

arr = pd.arrays.SparseArray(\[1., np.nan, np.nan, -2., np.nan\]) np.abs(arr)

</div>

The *ufunc* is also applied to `fill_value`. This is needed to get the correct dense result.

<div class="ipython">

python

arr = pd.arrays.SparseArray(\[1., -1, -1, -2., -1\], fill\_value=-1) np.abs(arr) np.abs(arr).to\_dense()

</div>

**Conversion**

To convert data from sparse to dense, use the `.sparse` accessors

<div class="ipython">

python

sdf.sparse.to\_dense()

</div>

From dense to sparse, use <span class="title-ref">DataFrame.astype</span> with a <span class="title-ref">SparseDtype</span>.

<div class="ipython">

python

dense = pd.DataFrame({"A": \[1, 0, 0, 1\]}) dtype = pd.SparseDtype(int, fill\_value=0) dense.astype(dtype)

</div>

## Interaction with *scipy.sparse*

Use <span class="title-ref">DataFrame.sparse.from\_spmatrix</span> to create a <span class="title-ref">DataFrame</span> with sparse values from a sparse matrix.

<div class="ipython">

python

from scipy.sparse import csr\_matrix

arr = np.random.random(size=(1000, 5)) arr\[arr \< .9\] = 0

sp\_arr = csr\_matrix(arr) sp\_arr

sdf = pd.DataFrame.sparse.from\_spmatrix(sp\_arr) sdf.head() sdf.dtypes

</div>

All sparse formats are supported, but matrices that are not in `COOrdinate <scipy.sparse>` format will be converted, copying data as needed. To convert back to sparse SciPy matrix in COO format, you can use the <span class="title-ref">DataFrame.sparse.to\_coo</span> method:

<div class="ipython">

python

sdf.sparse.to\_coo()

</div>

<span class="title-ref">Series.sparse.to\_coo</span> is implemented for transforming a <span class="title-ref">Series</span> with sparse values indexed by a <span class="title-ref">MultiIndex</span> to a <span class="title-ref">scipy.sparse.coo\_matrix</span>.

The method requires a <span class="title-ref">MultiIndex</span> with two or more levels.

<div class="ipython">

python

s = pd.Series(\[3.0, np.nan, 1.0, 3.0, np.nan, np.nan\]) s.index = pd.MultiIndex.from\_tuples( \[ (1, 2, "a", 0), (1, 2, "a", 1), (1, 1, "b", 0), (1, 1, "b", 1), (2, 1, "b", 0), (2, 1, "b", 1), \], names=\["A", "B", "C", "D"\], ) ss = s.astype('Sparse') ss

</div>

In the example below, we transform the <span class="title-ref">Series</span> to a sparse representation of a 2-d array by specifying that the first and second `MultiIndex` levels define labels for the rows and the third and fourth levels define labels for the columns. We also specify that the column and row labels should be sorted in the final sparse representation.

<div class="ipython">

python

  - A, rows, columns = ss.sparse.to\_coo(  
    row\_levels=\["A", "B"\], column\_levels=\["C", "D"\], sort\_labels=True

)

A A.todense() rows columns

</div>

Specifying different row and column labels (and not sorting them) yields a different sparse matrix:

<div class="ipython">

python

  - A, rows, columns = ss.sparse.to\_coo(  
    row\_levels=\["A", "B", "C"\], column\_levels=\["D"\], sort\_labels=False

)

A A.todense() rows columns

</div>

A convenience method <span class="title-ref">Series.sparse.from\_coo</span> is implemented for creating a <span class="title-ref">Series</span> with sparse values from a `scipy.sparse.coo_matrix`.

<div class="ipython">

python

from scipy import sparse A = sparse.coo\_matrix((\[3.0, 1.0, 2.0\], (\[1, 0, 0\], \[0, 2, 3\])), shape=(3, 4)) A A.todense()

</div>

The default behaviour (with `dense_index=False`) simply returns a <span class="title-ref">Series</span> containing only the non-null entries.

<div class="ipython">

python

ss = pd.Series.sparse.from\_coo(A) ss

</div>

Specifying `dense_index=True` will result in an index that is the Cartesian product of the row and columns coordinates of the matrix. Note that this will consume a significant amount of memory (relative to `dense_index=False`) if the sparse matrix is large (and sparse) enough.

<div class="ipython">

python

ss\_dense = pd.Series.sparse.from\_coo(A, dense\_index=True) ss\_dense

</div>

---

text.md

---

<div id="text">

{{ header }}

</div>

# Working with text data

## Text data types

There are two ways to store text data in pandas:

1.  `object` -dtype NumPy array.
2.  <span class="title-ref">StringDtype</span> extension type.

We recommend using <span class="title-ref">StringDtype</span> to store text data.

Prior to pandas 1.0, `object` dtype was the only option. This was unfortunate for many reasons:

1.  You can accidentally store a *mixture* of strings and non-strings in an `object` dtype array. It's better to have a dedicated dtype.
2.  `object` dtype breaks dtype-specific operations like <span class="title-ref">DataFrame.select\_dtypes</span>. There isn't a clear way to select *just* text while excluding non-text but still object-dtype columns.
3.  When reading code, the contents of an `object` dtype array is less clear than `'string'`.

Currently, the performance of `object` dtype arrays of strings and <span class="title-ref">arrays.StringArray</span> are about the same. We expect future enhancements to significantly increase the performance and lower the memory overhead of <span class="title-ref">\~arrays.StringArray</span>.

\> **Warning** \> `StringArray` is currently considered experimental. The implementation and parts of the API may change without warning.

For backwards-compatibility, `object` dtype remains the default type we infer a list of strings to

<div class="ipython">

python

pd.Series(\["a", "b", "c"\])

</div>

To explicitly request `string` dtype, specify the `dtype`

<div class="ipython">

python

pd.Series(\["a", "b", "c"\], dtype="string") pd.Series(\["a", "b", "c"\], dtype=pd.StringDtype())

</div>

Or `astype` after the `Series` or `DataFrame` is created

<div class="ipython">

python

s = pd.Series(\["a", "b", "c"\]) s s.astype("string")

</div>

You can also use <span class="title-ref">StringDtype</span>/`"string"` as the dtype on non-string data and it will be converted to `string` dtype:

<div class="ipython">

python

s = pd.Series(\["a", 2, np.nan\], dtype="string") s type(s\[1\])

</div>

or convert from existing pandas data:

<div class="ipython">

python

s1 = pd.Series(\[1, 2, np.nan\], dtype="Int64") s1 s2 = s1.astype("string") s2 type(s2\[0\])

</div>

### Behavior differences

These are places where the behavior of `StringDtype` objects differ from `object` dtype

12. For `StringDtype`, \[string accessor methods\<api.series.str\>\](\#string-accessor-methods\<api.series.str\>) that return **numeric** output will always return a nullable integer dtype, rather than either int or float dtype, depending on the presence of NA values. Methods returning **boolean** output will return a nullable boolean dtype.
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\["a", None, "b"\], dtype="string") s s.str.count("a") s.dropna().str.count("a")
    
    </div>
    
    Both outputs are `Int64` dtype. Compare that with object-dtype
    
    <div class="ipython">
    
    python
    
    s2 = pd.Series(\["a", None, "b"\], dtype="object") s2.str.count("a") s2.dropna().str.count("a")
    
    </div>
    
    When NA values are present, the output dtype is float64. Similarly for methods returning boolean values.
    
    <div class="ipython">
    
    python
    
    s.str.isdigit() s.str.match("a")
    
    </div>

<!-- end list -->

2.  Some string methods, like <span class="title-ref">Series.str.decode</span> are not available on `StringArray` because `StringArray` only holds strings, not bytes.
3.  In comparison operations, <span class="title-ref">arrays.StringArray</span> and `Series` backed by a `StringArray` will return an object with <span class="title-ref">BooleanDtype</span>, rather than a `bool` dtype object. Missing values in a `StringArray` will propagate in comparison operations, rather than always comparing unequal like <span class="title-ref">numpy.nan</span>.

Everything else that follows in the rest of this document applies equally to `string` and `object` dtype.

## String methods

Series and Index are equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the `str` attribute and generally have names matching the equivalent (scalar) built-in string methods:

<div class="ipython">

python

  - s = pd.Series(  
    \["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"\], dtype="string"

) s.str.lower() s.str.upper() s.str.len()

</div>

<div class="ipython">

python

idx = pd.Index(\[" jack", "jill ", " jesse ", "frank"\]) idx.str.strip() idx.str.lstrip() idx.str.rstrip()

</div>

The string methods on Index are especially useful for cleaning up or transforming DataFrame columns. For instance, you may have columns with leading or trailing whitespace:

<div class="ipython">

python

  - df = pd.DataFrame(  
    np.random.randn(3, 2), columns=\[" Column A ", " Column B "\], index=range(3)

) df

</div>

Since `df.columns` is an Index object, we can use the `.str` accessor

<div class="ipython">

python

df.columns.str.strip() df.columns.str.lower()

</div>

These string methods can then be used to clean up the columns as needed. Here we are removing leading and trailing whitespaces, lower casing all names, and replacing any remaining whitespaces with underscores:

<div class="ipython">

python

df.columns = df.columns.str.strip().str.lower().str.replace(" ", "\_") df

</div>

\> **Note** \> If you have a `Series` where lots of elements are repeated (i.e. the number of unique elements in the `Series` is a lot smaller than the length of the `Series`), it can be faster to convert the original `Series` to one of type `category` and then use `.str.<method>` or `.dt.<property>` on that. The performance difference comes from the fact that, for `Series` of type `category`, the string operations are done on the `.categories` and not on each element of the `Series`.

> Please note that a `Series` of type `category` with string `.categories` has some limitations in comparison to `Series` of type string (e.g. you can't add strings to each other: `s + " " + s` won't work if `s` is a `Series` of type `category`). Also, `.str` methods which operate on elements of type `list` are not available on such a `Series`.

<div id="text.warn_types">

\> **Warning** \> The type of the Series is inferred and is one among the allowed types (i.e. strings).

</div>

> Generally speaking, the `.str` accessor is intended to work only on strings. With very few exceptions, other uses are not supported, and may be disabled at a later point.

## Splitting and replacing strings

Methods like `split` return a Series of lists:

<div class="ipython">

python

s2 = pd.Series(\["a\_b\_c", "c\_d\_e", np.nan, "f\_g\_h"\], dtype="string") s2.str.split("\_")

</div>

Elements in the split lists can be accessed using `get` or `[]` notation:

<div class="ipython">

python

s2.str.split("\_").str.get(1) s2.str.split("\_").str\[1\]

</div>

It is easy to expand this to return a DataFrame using `expand`.

<div class="ipython">

python

s2.str.split("\_", expand=True)

</div>

When original `Series` has <span class="title-ref">StringDtype</span>, the output columns will all be <span class="title-ref">StringDtype</span> as well.

It is also possible to limit the number of splits:

<div class="ipython">

python

s2.str.split("\_", expand=True, n=1)

</div>

`rsplit` is similar to `split` except it works in the reverse direction, i.e., from the end of the string to the beginning of the string:

<div class="ipython">

python

s2.str.rsplit("\_", expand=True, n=1)

</div>

`replace` optionally uses [regular expressions](https://docs.python.org/3/library/re.html):

<div class="ipython">

python

  - s3 = pd.Series(  
    \["A", "B", "C", "Aaba", "Baca", "", np.nan, "CABA", "dog", "cat"\], dtype="string",

) s3 s3.str.replace("^.a|dog", "XX-XX ", case=False, regex=True)

</div>

<div class="versionchanged">

2.0

</div>

Single character pattern with `regex=True` will also be treated as regular expressions:

<div class="ipython">

python

s4 = pd.Series(\["a.b", ".", "b", np.nan, ""\], dtype="string") s4 s4.str.replace(".", "a", regex=True)

</div>

If you want literal replacement of a string (equivalent to <span class="title-ref">str.replace</span>), you can set the optional `regex` parameter to `False`, rather than escaping each character. In this case both `pat` and `repl` must be strings:

<div class="ipython">

python

dollars = pd.Series(\["12", "-$10", "$10,000"\], dtype="string")

\# These lines are equivalent dollars.str.replace(r"-$", "-", regex=True) dollars.str.replace("-$", "-", regex=False)

</div>

The `replace` method can also take a callable as replacement. It is called on every `pat` using <span class="title-ref">re.sub</span>. The callable should expect one positional argument (a regex object) and return a string.

<div class="ipython">

python

\# Reverse every lowercase alphabetic word pat = r"\[a-z\]+"

  - def repl(m):  
    return m.group(0)\[::-1\]

  - pd.Series(\["foo 123", "bar baz", np.nan\], dtype="string").str.replace(  
    pat, repl, regex=True

)

\# Using regex groups pat = r"(?P\<one\>w+) (?P\<two\>w+) (?P\<three\>w+)"

  - def repl(m):  
    return m.group("two").swapcase()

  - pd.Series(\["Foo Bar Baz", np.nan\], dtype="string").str.replace(  
    pat, repl, regex=True

)

</div>

The `replace` method also accepts a compiled regular expression object from <span class="title-ref">re.compile</span> as a pattern. All flags should be included in the compiled regular expression object.

<div class="ipython">

python

import re

regex\_pat = re.compile(r"^.a|dog", flags=re.IGNORECASE) s3.str.replace(regex\_pat, "XX-XX ", regex=True)

</div>

Including a `flags` argument when calling `replace` with a compiled regular expression object will raise a `ValueError`.

<div class="ipython">

@verbatim In \[1\]: s3.str.replace(regex\_pat, 'XX-XX ', flags=re.IGNORECASE) ---------------------------------------------------------------------------ValueError: case and flags cannot be set when pat is a compiled regex

</div>

`removeprefix` and `removesuffix` have the same effect as `str.removeprefix` and `str.removesuffix` added in Python 3.9 \<<https://docs.python.org/3/library/stdtypes.html#str.removeprefix>\>\`\_\_:

<div class="versionadded">

1.4.0

</div>

<div class="ipython">

python

s = pd.Series(\["str\_foo", "str\_bar", "no\_prefix"\]) s.str.removeprefix("[str]()")

s = pd.Series(\["foo\_str", "bar\_str", "no\_suffix"\]) s.str.removesuffix("\_str")

</div>

## Concatenation

There are several ways to concatenate a `Series` or `Index`, either with itself or others, all based on <span class="title-ref">\~Series.str.cat</span>, resp. `Index.str.cat`.

### Concatenating a single Series into a string

The content of a `Series` (or `Index`) can be concatenated:

<div class="ipython">

python

s = pd.Series(\["a", "b", "c", "d"\], dtype="string") s.str.cat(sep=",")

</div>

If not specified, the keyword `sep` for the separator defaults to the empty string, `sep=''`:

<div class="ipython">

python

s.str.cat()

</div>

By default, missing values are ignored. Using `na_rep`, they can be given a representation:

<div class="ipython">

python

t = pd.Series(\["a", "b", np.nan, "d"\], dtype="string") t.str.cat(sep=",") t.str.cat(sep=",", na\_rep="-")

</div>

### Concatenating a Series and something list-like into a Series

The first argument to <span class="title-ref">\~Series.str.cat</span> can be a list-like object, provided that it matches the length of the calling `Series` (or `Index`).

<div class="ipython">

python

s.str.cat(\["A", "B", "C", "D"\])

</div>

Missing values on either side will result in missing values in the result as well, *unless* `na_rep` is specified:

<div class="ipython">

python

s.str.cat(t) s.str.cat(t, na\_rep="-")

</div>

### Concatenating a Series and something array-like into a Series

The parameter `others` can also be two-dimensional. In this case, the number or rows must match the lengths of the calling `Series` (or `Index`).

<div class="ipython">

python

d = pd.concat(\[t, s\], axis=1) s d s.str.cat(d, na\_rep="-")

</div>

### Concatenating a Series and an indexed object into a Series, with alignment

For concatenation with a `Series` or `DataFrame`, it is possible to align the indexes before concatenation by setting the `join`-keyword.

<div class="ipython" data-okwarning="">

python

u = pd.Series(\["b", "d", "a", "c"\], index=\[1, 3, 0, 2\], dtype="string") s u s.str.cat(u) s.str.cat(u, join="left")

</div>

The usual options are available for `join` (one of `'left', 'outer', 'inner', 'right'`). In particular, alignment also means that the different lengths do not need to coincide anymore.

<div class="ipython">

python

v = pd.Series(\["z", "a", "b", "d", "e"\], index=\[-1, 0, 1, 3, 4\], dtype="string") s v s.str.cat(v, join="left", na\_rep="-") s.str.cat(v, join="outer", na\_rep="-")

</div>

The same alignment can be used when `others` is a `DataFrame`:

<div class="ipython">

python

f = d.loc\[\[3, 2, 1, 0\], :\] s f s.str.cat(f, join="left", na\_rep="-")

</div>

### Concatenating a Series and many objects into a Series

Several array-like items (specifically: `Series`, `Index`, and 1-dimensional variants of `np.ndarray`) can be combined in a list-like container (including iterators, `dict`-views, etc.).

<div class="ipython">

python

s u s.str.cat(\[u, u.to\_numpy()\], join="left")

</div>

All elements without an index (e.g. `np.ndarray`) within the passed list-like must match in length to the calling `Series` (or `Index`), but `Series` and `Index` may have arbitrary length (as long as alignment is not disabled with `join=None`):

<div class="ipython">

python

v s.str.cat(\[v, u, u.to\_numpy()\], join="outer", na\_rep="-")

</div>

If using `join='right'` on a list-like of `others` that contains different indexes, the union of these indexes will be used as the basis for the final concatenation:

<div class="ipython">

python

u.loc\[\[3\]\] v.loc\[\[-1, 0\]\] s.str.cat(\[u.loc\[\[3\]\], v.loc\[\[-1, 0\]\]\], join="right", na\_rep="-")

</div>

## Indexing with `.str`

<div id="text.indexing">

You can use `[]` notation to directly index by position locations. If you index past the end of the string, the result will be a `NaN`.

</div>

<div class="ipython">

python

  - s = pd.Series(  
    \["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"\], dtype="string"

)

s.str\[0\] s.str\[1\]

</div>

## Extracting substrings

### Extract first match in each subject (extract)

The `extract` method accepts a [regular expression](https://docs.python.org/3/library/re.html) with at least one capture group.

Extracting a regular expression with more than one group returns a DataFrame with one column per group.

<div class="ipython">

python

  - pd.Series(  
    \["a1", "b2", "c3"\], dtype="string",

).str.extract(r"(\[ab\])(d)", expand=False)

</div>

Elements that do not match return a row filled with `NaN`. Thus, a Series of messy strings can be "converted" into a like-indexed Series or DataFrame of cleaned-up or more useful strings, without necessitating `get()` to access tuples or `re.match` objects. The dtype of the result is always object, even if no match is found and the result only contains `NaN`.

Named groups like

<div class="ipython">

python

  - pd.Series(\["a1", "b2", "c3"\], dtype="string").str.extract(  
    r"(?P\<letter\>\[ab\])(?P\<digit\>d)", expand=False

)

</div>

and optional groups like

<div class="ipython">

python

  - pd.Series(  
    \["a1", "b2", "3"\], dtype="string",

).str.extract(r"(\[ab\])?(d)", expand=False)

</div>

can also be used. Note that any capture group names in the regular expression will be used for column names; otherwise capture group numbers will be used.

Extracting a regular expression with one group returns a `DataFrame` with one column if `expand=True`.

<div class="ipython">

python

pd.Series(\["a1", "b2", "c3"\], dtype="string").str.extract(r"\[ab\](d)", expand=True)

</div>

It returns a Series if `expand=False`.

<div class="ipython">

python

pd.Series(\["a1", "b2", "c3"\], dtype="string").str.extract(r"\[ab\](d)", expand=False)

</div>

Calling on an `Index` with a regex with exactly one capture group returns a `DataFrame` with one column if `expand=True`.

<div class="ipython">

python

s = pd.Series(\["a1", "b2", "c3"\], \["A11", "B22", "C33"\], dtype="string") s s.index.str.extract("(?P\<letter\>\[a-zA-Z\])", expand=True)

</div>

It returns an `Index` if `expand=False`.

<div class="ipython">

python

s.index.str.extract("(?P\<letter\>\[a-zA-Z\])", expand=False)

</div>

Calling on an `Index` with a regex with more than one capture group returns a `DataFrame` if `expand=True`.

<div class="ipython">

python

s.index.str.extract("(?P\<letter\>\[a-zA-Z\])(\[0-9\]+)", expand=True)

</div>

It raises `ValueError` if `expand=False`.

<div class="ipython" data-okexcept="">

python

s.index.str.extract("(?P\<letter\>\[a-zA-Z\])(\[0-9\]+)", expand=False)

</div>

The table below summarizes the behavior of `extract(expand=False)` (input subject in first column, number of groups in regex in first row)

|        |         |            |
| ------ | ------- | ---------- |
|        | 1 group | \>1 group  |
| Index  | Index   | ValueError |
| Series | Series  | DataFrame  |

### Extract all matches in each subject (extractall)

<div id="text.extractall">

Unlike `extract` (which returns only the first match),

</div>

<div class="ipython">

python

s = pd.Series(\["a1a2", "b1", "c1"\], index=\["A", "B", "C"\], dtype="string") s two\_groups = "(?P\<letter\>\[a-z\])(?P\<digit\>\[0-9\])" s.str.extract(two\_groups, expand=True)

</div>

the `extractall` method returns every match. The result of `extractall` is always a `DataFrame` with a `MultiIndex` on its rows. The last level of the `MultiIndex` is named `match` and indicates the order in the subject.

<div class="ipython">

python

s.str.extractall(two\_groups)

</div>

When each subject string in the Series has exactly one match,

<div class="ipython">

python

s = pd.Series(\["a3", "b3", "c2"\], dtype="string") s

</div>

then `extractall(pat).xs(0, level='match')` gives the same result as `extract(pat)`.

<div class="ipython">

python

extract\_result = s.str.extract(two\_groups, expand=True) extract\_result extractall\_result = s.str.extractall(two\_groups) extractall\_result extractall\_result.xs(0, level="match")

</div>

`Index` also supports `.str.extractall`. It returns a `DataFrame` which has the same result as a `Series.str.extractall` with a default index (starts from 0).

<div class="ipython">

python

pd.Index(\["a1a2", "b1", "c1"\]).str.extractall(two\_groups)

pd.Series(\["a1a2", "b1", "c1"\], dtype="string").str.extractall(two\_groups)

</div>

## Testing for strings that match or contain a pattern

You can check whether elements contain a pattern:

<div class="ipython">

python

pattern = r"\[0-9\]\[a-z\]" pd.Series( \["1", "2", "3a", "3b", "03c", "4dx"\], dtype="string", ).str.contains(pattern)

</div>

Or whether elements match a pattern:

<div class="ipython">

python

  - pd.Series(  
    \["1", "2", "3a", "3b", "03c", "4dx"\], dtype="string",

).str.match(pattern)

</div>

<div class="ipython">

python

  - pd.Series(  
    \["1", "2", "3a", "3b", "03c", "4dx"\], dtype="string",

).str.fullmatch(pattern)

</div>

\> **Note** \> The distinction between `match`, `fullmatch`, and `contains` is strictness: `fullmatch` tests whether the entire string matches the regular expression; `match` tests whether there is a match of the regular expression that begins at the first character of the string; and `contains` tests whether there is a match of the regular expression at any position within the string.

> The corresponding functions in the `re` package for these three match modes are [re.fullmatch](https://docs.python.org/3/library/re.html#re.fullmatch), [re.match](https://docs.python.org/3/library/re.html#re.match), and [re.search](https://docs.python.org/3/library/re.html#re.search), respectively.

Methods like `match`, `fullmatch`, `contains`, `startswith`, and `endswith` take an extra `na` argument so missing values can be considered True or False:

<div class="ipython">

python

  - s4 = pd.Series(  
    \["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"\], dtype="string"

) s4.str.contains("A", na=False)

</div>

## Creating indicator variables

You can extract dummy variables from string columns. For example if they are separated by a `'|'`:

<div class="ipython">

python

s = pd.Series(\["a", "ac"\], dtype="string") s.str.get\_dummies(sep="|")

</div>

String `Index` also supports `get_dummies` which returns a `MultiIndex`.

<div class="ipython">

python

idx = pd.Index(\["a", "ac"\]) idx.str.get\_dummies(sep="|")

</div>

See also <span class="title-ref">\~pandas.get\_dummies</span>.

## Method summary

<div id="text.summary">

| Method                                                     | Description                                                                                                                    |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| <span class="title-ref">\~Series.str.cat</span>            | Concatenate strings                                                                                                            |
| <span class="title-ref">\~Series.str.split</span>          | Split strings on delimiter                                                                                                     |
| <span class="title-ref">\~Series.str.rsplit</span>         | Split strings on delimiter working from the end of the string                                                                  |
| <span class="title-ref">\~Series.str.get</span>            | Index into each element (retrieve i-th element)                                                                                |
| <span class="title-ref">\~Series.str.join</span>           | Join strings in each element of the Series with passed separator                                                               |
| <span class="title-ref">\~Series.str.get\_dummies</span>   | Split strings on the delimiter returning DataFrame of dummy variables                                                          |
| <span class="title-ref">\~Series.str.contains</span>       | Return boolean array if each string contains pattern/regex                                                                     |
| <span class="title-ref">\~Series.str.replace</span>        | Replace occurrences of pattern/regex/string with some other string or the return value of a callable given the occurrence      |
| <span class="title-ref">\~Series.str.removeprefix</span>   | Remove prefix from string i.e. only remove if string starts with prefix.                                                       |
| <span class="title-ref">\~Series.str.removesuffix</span>   | Remove suffix from string i.e. only remove if string ends with suffix.                                                         |
| <span class="title-ref">\~Series.str.repeat</span>         | Duplicate values (`s.str.repeat(3)` equivalent to `x * 3`)                                                                     |
| <span class="title-ref">\~Series.str.pad</span>            | Add whitespace to the sides of strings                                                                                         |
| <span class="title-ref">\~Series.str.center</span>         | Equivalent to `str.center`                                                                                                     |
| <span class="title-ref">\~Series.str.ljust</span>          | Equivalent to `str.ljust`                                                                                                      |
| <span class="title-ref">\~Series.str.rjust</span>          | Equivalent to `str.rjust`                                                                                                      |
| <span class="title-ref">\~Series.str.zfill</span>          | Equivalent to `str.zfill`                                                                                                      |
| <span class="title-ref">\~Series.str.wrap</span>           | Split long strings into lines with length less than a given width                                                              |
| <span class="title-ref">\~Series.str.slice</span>          | Slice each string in the Series                                                                                                |
| <span class="title-ref">\~Series.str.slice\_replace</span> | Replace slice in each string with passed value                                                                                 |
| <span class="title-ref">\~Series.str.count</span>          | Count occurrences of pattern                                                                                                   |
| <span class="title-ref">\~Series.str.startswith</span>     | Equivalent to `str.startswith(pat)` for each element                                                                           |
| <span class="title-ref">\~Series.str.endswith</span>       | Equivalent to `str.endswith(pat)` for each element                                                                             |
| <span class="title-ref">\~Series.str.findall</span>        | Compute list of all occurrences of pattern/regex for each string                                                               |
| <span class="title-ref">\~Series.str.match</span>          | Call `re.match` on each element returning matched groups as list                                                               |
| <span class="title-ref">\~Series.str.extract</span>        | Call `re.search` on each element returning DataFrame with one row for each element and one column for each regex capture group |
| <span class="title-ref">\~Series.str.extractall</span>     | Call `re.findall` on each element returning DataFrame with one row for each match and one column for each regex capture group  |
| <span class="title-ref">\~Series.str.len</span>            | Compute string lengths                                                                                                         |
| <span class="title-ref">\~Series.str.strip</span>          | Equivalent to `str.strip`                                                                                                      |
| <span class="title-ref">\~Series.str.rstrip</span>         | Equivalent to `str.rstrip`                                                                                                     |
| <span class="title-ref">\~Series.str.lstrip</span>         | Equivalent to `str.lstrip`                                                                                                     |
| <span class="title-ref">\~Series.str.partition</span>      | Equivalent to `str.partition`                                                                                                  |
| <span class="title-ref">\~Series.str.rpartition</span>     | Equivalent to `str.rpartition`                                                                                                 |
| <span class="title-ref">\~Series.str.lower</span>          | Equivalent to `str.lower`                                                                                                      |
| <span class="title-ref">\~Series.str.casefold</span>       | Equivalent to `str.casefold`                                                                                                   |
| <span class="title-ref">\~Series.str.upper</span>          | Equivalent to `str.upper`                                                                                                      |
| <span class="title-ref">\~Series.str.find</span>           | Equivalent to `str.find`                                                                                                       |
| <span class="title-ref">\~Series.str.rfind</span>          | Equivalent to `str.rfind`                                                                                                      |
| <span class="title-ref">\~Series.str.index</span>          | Equivalent to `str.index`                                                                                                      |
| <span class="title-ref">\~Series.str.rindex</span>         | Equivalent to `str.rindex`                                                                                                     |
| <span class="title-ref">\~Series.str.capitalize</span>     | Equivalent to `str.capitalize`                                                                                                 |
| <span class="title-ref">\~Series.str.swapcase</span>       | Equivalent to `str.swapcase`                                                                                                   |
| <span class="title-ref">\~Series.str.normalize</span>      | Return Unicode normal form. Equivalent to `unicodedata.normalize`                                                              |
| <span class="title-ref">\~Series.str.translate</span>      | Equivalent to `str.translate`                                                                                                  |
| <span class="title-ref">\~Series.str.isalnum</span>        | Equivalent to `str.isalnum`                                                                                                    |
| <span class="title-ref">\~Series.str.isalpha</span>        | Equivalent to `str.isalpha`                                                                                                    |
| <span class="title-ref">\~Series.str.isdigit</span>        | Equivalent to `str.isdigit`                                                                                                    |
| <span class="title-ref">\~Series.str.isspace</span>        | Equivalent to `str.isspace`                                                                                                    |
| <span class="title-ref">\~Series.str.islower</span>        | Equivalent to `str.islower`                                                                                                    |
| <span class="title-ref">\~Series.str.isupper</span>        | Equivalent to `str.isupper`                                                                                                    |
| <span class="title-ref">\~Series.str.istitle</span>        | Equivalent to `str.istitle`                                                                                                    |
| <span class="title-ref">\~Series.str.isnumeric</span>      | Equivalent to `str.isnumeric`                                                                                                  |
| <span class="title-ref">\~Series.str.isdecimal</span>      | Equivalent to `str.isdecimal`                                                                                                  |

</div>

---

timedeltas.md

---

<div id="timedeltas">

{{ header }}

</div>

# Time deltas

Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative.

`Timedelta` is a subclass of `datetime.timedelta`, and behaves in a similar manner, but allows compatibility with `np.timedelta64` types as well as a host of custom representation, parsing, and attributes.

## Parsing

You can construct a `Timedelta` scalar through various arguments, including [ISO 8601 Duration](https://en.wikipedia.org/wiki/ISO_8601#Durations) strings.

<div class="ipython">

python

import datetime

\# strings pd.Timedelta("1 days") pd.Timedelta("1 days 00:00:00") pd.Timedelta("1 days 2 hours") pd.Timedelta("-1 days 2 min 3us")

\# like datetime.timedelta \# note: these MUST be specified as keyword arguments pd.Timedelta(days=1, seconds=1)

\# integers with a unit pd.Timedelta(1, unit="D")

\# from a datetime.timedelta/np.timedelta64 pd.Timedelta(datetime.timedelta(days=1, seconds=1)) pd.Timedelta(np.timedelta64(1, "ms"))

\# negative Timedeltas have this string repr \# to be more consistent with datetime.timedelta conventions pd.Timedelta("-1us")

\# a NaT pd.Timedelta("nan") pd.Timedelta("nat")

\# ISO 8601 Duration strings pd.Timedelta("P0DT0H1M0S") pd.Timedelta("P0DT0H0M0.000000123S")

</div>

\[DateOffsets\<timeseries.offsets\>\](\#dateoffsets\<timeseries.offsets\>) (`Day, Hour, Minute, Second, Milli, Micro, Nano`) can also be used in construction.

<div class="ipython">

python

pd.Timedelta(pd.offsets.Second(2))

</div>

Further, operations among the scalars yield another scalar `Timedelta`.

<div class="ipython">

python

  - pd.Timedelta(pd.offsets.Day(2)) + pd.Timedelta(pd.offsets.Second(2)) + pd.Timedelta(  
    "00:00:00.000123"

)

</div>

### to\_timedelta

Using the top-level `pd.to_timedelta`, you can convert a scalar, array, list, or Series from a recognized timedelta format / value into a `Timedelta` type. It will construct Series if the input is a Series, a scalar if the input is scalar-like, otherwise it will output a `TimedeltaIndex`.

You can parse a single string to a Timedelta:

<div class="ipython">

python

pd.to\_timedelta("1 days 06:05:01.00003") pd.to\_timedelta("15.5us")

</div>

or a list/array of strings:

<div class="ipython">

python

pd.to\_timedelta(\["1 days 06:05:01.00003", "15.5us", "nan"\])

</div>

The `unit` keyword argument specifies the unit of the Timedelta if the input is numeric:

<div class="ipython">

python

pd.to\_timedelta(np.arange(5), unit="s") pd.to\_timedelta(np.arange(5), unit="D")

</div>

<div class="warning">

<div class="title">

Warning

</div>

If a string or array of strings is passed as an input then the `unit` keyword argument will be ignored. If a string without units is passed then the default unit of nanoseconds is assumed.

</div>

### Timedelta limitations

pandas represents `Timedeltas` in nanosecond resolution using 64 bit integers. As such, the 64 bit integer limits determine the `Timedelta` limits.

<div class="ipython">

python

pd.Timedelta.min pd.Timedelta.max

</div>

## Operations

You can operate on Series/DataFrames and construct `timedelta64[ns]` Series through subtraction operations on `datetime64[ns]` Series, or `Timestamps`.

<div class="ipython">

python

s = pd.Series(pd.date\_range("2012-1-1", periods=3, freq="D")) td = pd.Series(\[pd.Timedelta(days=i) for i in range(3)\]) df = pd.DataFrame({"A": s, "B": td}) df df\["C"\] = df\["A"\] + df\["B"\] df df.dtypes

s - s.max() s - datetime.datetime(2011, 1, 1, 3, 5) s + datetime.timedelta(minutes=5) s + pd.offsets.Minute(5) s + pd.offsets.Minute(5) + pd.offsets.Milli(5)

</div>

Operations with scalars from a `timedelta64[ns]` series:

<div class="ipython">

python

y = s - s\[0\] y

</div>

Series of timedeltas with `NaT` values are supported:

<div class="ipython">

python

y = s - s.shift() y

</div>

Elements can be set to `NaT` using `np.nan` analogously to datetimes:

<div class="ipython">

python

y\[1\] = np.nan y

</div>

Operands can also appear in a reversed order (a singular object operated with a Series):

<div class="ipython">

python

s.max() - s datetime.datetime(2011, 1, 1, 3, 5) - s datetime.timedelta(minutes=5) + s

</div>

`min, max` and the corresponding `idxmin, idxmax` operations are supported on frames:

<div class="ipython">

python

A = s - pd.Timestamp("20120101") - pd.Timedelta("00:05:05") B = s - pd.Series(pd.date\_range("2012-1-2", periods=3, freq="D"))

df = pd.DataFrame({"A": A, "B": B}) df

df.min() df.min(axis=1)

df.idxmin() df.idxmax()

</div>

`min, max, idxmin, idxmax` operations are supported on Series as well. A scalar result will be a `Timedelta`.

<div class="ipython">

python

df.min().max() df.min(axis=1).min()

df.min().idxmax() df.min(axis=1).idxmin()

</div>

You can fillna on timedeltas, passing a timedelta to get a particular value.

<div class="ipython">

python

y.fillna(pd.Timedelta(0)) y.fillna(pd.Timedelta(10, unit="s")) y.fillna(pd.Timedelta("-1 days, 00:00:05"))

</div>

You can also negate, multiply and use `abs` on `Timedeltas`:

<div class="ipython">

python

td1 = pd.Timedelta("-1 days 2 hours 3 seconds") td1 -1 \* td1 -td1 abs(td1)

</div>

## Reductions

Numeric reduction operation for `timedelta64[ns]` will return `Timedelta` objects. As usual `NaT` are skipped during evaluation.

<div class="ipython">

python

  - y2 = pd.Series(  
    pd.to\_timedelta(\["-1 days +00:00:05", "nat", "-1 days +00:00:05", "1 days"\])

) y2 y2.mean() y2.median() y2.quantile(0.1) y2.sum()

</div>

## Frequency conversion

Timedelta Series and `TimedeltaIndex`, and `Timedelta` can be converted to other frequencies by astyping to a specific timedelta dtype.

<div class="ipython">

python

december = pd.Series(pd.date\_range("20121201", periods=4)) january = pd.Series(pd.date\_range("20130101", periods=4)) td = january - december

td\[2\] += datetime.timedelta(minutes=5, seconds=3) td\[3\] = np.nan td

\# to seconds td.astype("timedelta64\[s\]")

</div>

For timedelta64 resolutions other than the supported "s", "ms", "us", "ns", an alternative is to divide by another timedelta object. Note that division by the NumPy scalar is true division, while astyping is equivalent of floor division.

<div class="ipython">

python

\# to days td / np.timedelta64(1, "D")

</div>

Dividing or multiplying a `timedelta64[ns]` Series by an integer or integer Series yields another `timedelta64[ns]` dtypes Series.

<div class="ipython">

python

td \* -1 td \* pd.Series(\[1, 2, 3, 4\])

</div>

Rounded division (floor-division) of a `timedelta64[ns]` Series by a scalar `Timedelta` gives a series of integers.

<div class="ipython">

python

td // pd.Timedelta(days=3, hours=4) pd.Timedelta(days=3, hours=4) // td

</div>

<div id="timedeltas.mod_divmod">

The mod (%) and divmod operations are defined for `Timedelta` when operating with another timedelta-like or with a numeric argument.

</div>

<div class="ipython">

python

pd.Timedelta(hours=37) % datetime.timedelta(hours=2)

\# divmod against a timedelta-like returns a pair (int, Timedelta) divmod(datetime.timedelta(hours=2), pd.Timedelta(minutes=11))

\# divmod against a numeric returns a pair (Timedelta, Timedelta) divmod(pd.Timedelta(hours=25), 86400000000000)

</div>

## Attributes

You can access various components of the `Timedelta` or `TimedeltaIndex` directly using the attributes `days,seconds,microseconds,nanoseconds`. These are identical to the values returned by `datetime.timedelta`, in that, for example, the `.seconds` attribute represents the number of seconds \>= 0 and \< 1 day. These are signed according to whether the `Timedelta` is signed.

These operations can also be directly accessed via the `.dt` property of the `Series` as well.

\> **Note** \> Note that the attributes are NOT the displayed values of the `Timedelta`. Use `.components` to retrieve the displayed values.

For a `Series`:

<div class="ipython">

python

td.dt.days td.dt.seconds

</div>

You can access the value of the fields for a scalar `Timedelta` directly.

<div class="ipython">

python

tds = pd.Timedelta("31 days 5 min 3 sec") tds.days tds.seconds (-tds).seconds

</div>

You can use the `.components` property to access a reduced form of the timedelta. This returns a `DataFrame` indexed similarly to the `Series`. These are the *displayed* values of the `Timedelta`.

<div class="ipython">

python

td.dt.components td.dt.components.seconds

</div>

<div id="timedeltas.isoformat">

You can convert a `Timedelta` to an [ISO 8601 Duration](https://en.wikipedia.org/wiki/ISO_8601#Durations) string with the `.isoformat` method

</div>

<div class="ipython">

python

  - pd.Timedelta(  
    days=6, minutes=50, seconds=3, milliseconds=10, microseconds=10, nanoseconds=12

).isoformat()

</div>

## TimedeltaIndex

To generate an index with time delta, you can use either the <span class="title-ref">TimedeltaIndex</span> or the <span class="title-ref">timedelta\_range</span> constructor.

Using `TimedeltaIndex` you can pass string-like, `Timedelta`, `timedelta`, or `np.timedelta64` objects. Passing `np.nan/pd.NaT/nat` will represent missing values.

<div class="ipython">

python

  - pd.TimedeltaIndex(
    
      - \[  
        "1 days", "1 days, 00:00:05", np.timedelta64(2, "D"), datetime.timedelta(days=2, seconds=2),
    
    \]

)

</div>

The string 'infer' can be passed in order to set the frequency of the index as the inferred frequency upon creation:

<div class="ipython">

python

pd.TimedeltaIndex(\["0 days", "10 days", "20 days"\], freq="infer")

</div>

### Generating ranges of time deltas

Similar to <span class="title-ref">date\_range</span>, you can construct regular ranges of a `TimedeltaIndex` using <span class="title-ref">timedelta\_range</span>. The default frequency for `timedelta_range` is calendar day:

<div class="ipython">

python

pd.timedelta\_range(start="1 days", periods=5)

</div>

Various combinations of `start`, `end`, and `periods` can be used with `timedelta_range`:

<div class="ipython">

python

pd.timedelta\_range(start="1 days", end="5 days")

pd.timedelta\_range(end="10 days", periods=4)

</div>

The `freq` parameter can passed a variety of \[frequency aliases \<timeseries.offset\_aliases\>\](\#frequency-aliases-\<timeseries.offset\_aliases\>):

<div class="ipython">

python

pd.timedelta\_range(start="1 days", end="2 days", freq="30min")

pd.timedelta\_range(start="1 days", periods=5, freq="2D5h")

</div>

Specifying `start`, `end`, and `periods` will generate a range of evenly spaced timedeltas from `start` to `end` inclusively, with `periods` number of elements in the resulting `TimedeltaIndex`:

<div class="ipython">

python

pd.timedelta\_range("0 days", "4 days", periods=5)

pd.timedelta\_range("0 days", "4 days", periods=10)

</div>

### Using the TimedeltaIndex

Similarly to other of the datetime-like indices, `DatetimeIndex` and `PeriodIndex`, you can use `TimedeltaIndex` as the index of pandas objects.

<div class="ipython">

python

  - s = pd.Series(  
    np.arange(100), index=pd.timedelta\_range("1 days", periods=100, freq="h"),

) s

</div>

Selections work similarly, with coercion on string-likes and slices:

<div class="ipython">

python

s\["1 day":"2 day"\] s\["1 day 01:00:00"\] s\[pd.Timedelta("1 day 1h")\]

</div>

Furthermore you can use partial string selection and the range will be inferred:

<div class="ipython">

python

s\["1 day":"1 day 5 hours"\]

</div>

### Operations

Finally, the combination of `TimedeltaIndex` with `DatetimeIndex` allow certain combination operations that are NaT preserving:

<div class="ipython">

python

tdi = pd.TimedeltaIndex(\["1 days", pd.NaT, "2 days"\]) tdi.to\_list() dti = pd.date\_range("20130101", periods=3) dti.to\_list() (dti + tdi).to\_list() (dti - tdi).to\_list()

</div>

### Conversions

Similarly to frequency conversion on a `Series` above, you can convert these indices to yield another Index.

<div class="ipython">

python

tdi / np.timedelta64(1, "s") tdi.astype("timedelta64\[s\]")

</div>

Scalars type ops work as well. These can potentially return a *different* type of index.

<div class="ipython">

python

\# adding or timedelta and date -\> datelike tdi + pd.Timestamp("20130101")

\# subtraction of a date and a timedelta -\> datelike \# note that trying to subtract a date from a Timedelta will raise an exception (pd.Timestamp("20130101") - tdi).to\_list()

\# timedelta + timedelta -\> timedelta tdi + pd.Timedelta("10 days")

\# division can result in a Timedelta if the divisor is an integer tdi / 2

\# or a float64 Index if the divisor is a Timedelta tdi / tdi\[0\]

</div>

## Resampling

Similar to \[timeseries resampling \<timeseries.resampling\>\](\#timeseries-resampling-\<timeseries.resampling\>), we can resample with a `TimedeltaIndex`.

<div class="ipython">

python

s.resample("D").mean()

</div>

---

timeseries.md

---

<div id="timeseries">

{{ header }}

</div>

# Time series / date functionality

pandas contains extensive capabilities and features for working with time series data for all domains. Using the NumPy `datetime64` and `timedelta64` dtypes, pandas has consolidated a large number of features from other Python libraries like `scikits.timeseries` as well as created a tremendous amount of new functionality for manipulating time series data.

For example, pandas supports:

Parsing time series information from various sources and formats

<div class="ipython">

python

import datetime

  - dti = pd.to\_datetime(  
    \["1/1/2018", np.datetime64("2018-01-01"), datetime.datetime(2018, 1, 1)\]

) dti

</div>

Generate sequences of fixed-frequency dates and time spans

<div class="ipython">

python

dti = pd.date\_range("2018-01-01", periods=3, freq="h") dti

</div>

Manipulating and converting date times with timezone information

<div class="ipython">

python

dti = dti.tz\_localize("UTC") dti dti.tz\_convert("US/Pacific")

</div>

Resampling or converting a time series to a particular frequency

<div class="ipython">

python

idx = pd.date\_range("2018-01-01", periods=5, freq="h") ts = pd.Series(range(len(idx)), index=idx) ts ts.resample("2h").mean()

</div>

Performing date and time arithmetic with absolute or relative time increments

<div class="ipython">

python

friday = pd.Timestamp("2018-01-05") friday.day\_name() \# Add 1 day saturday = friday + pd.Timedelta("1 day") saturday.day\_name() \# Add 1 business day (Friday --\> Monday) monday = friday + pd.offsets.BDay() monday.day\_name()

</div>

pandas provides a relatively compact and self-contained set of tools for performing the above tasks and more.

## Overview

pandas captures 4 general time related concepts:

1.  Date times: A specific date and time with timezone support. Similar to `datetime.datetime` from the standard library.
2.  Time deltas: An absolute time duration. Similar to `datetime.timedelta` from the standard library.
3.  Time spans: A span of time defined by a point in time and its associated frequency.
4.  Date offsets: A relative time duration that respects calendar arithmetic. Similar to `dateutil.relativedelta.relativedelta` from the `dateutil` package.

| Concept      | Scalar Class | Array Class      | pandas Data Type                         | Primary Creation Method             |
| ------------ | ------------ | ---------------- | ---------------------------------------- | ----------------------------------- |
| Date times   | `Timestamp`  | `DatetimeIndex`  | `datetime64[ns]` or `datetime64[ns, tz]` | `to_datetime` or `date_range`       |
| Time deltas  | `Timedelta`  | `TimedeltaIndex` | `timedelta64[ns]`                        | `to_timedelta` or `timedelta_range` |
| Time spans   | `Period`     | `PeriodIndex`    | `period[freq]`                           | `Period` or `period_range`          |
| Date offsets | `DateOffset` | `None`           | `None`                                   | `DateOffset`                        |

For time series data, it's conventional to represent the time component in the index of a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> so manipulations can be performed with respect to the time element.

<div class="ipython">

python

pd.Series(range(3), index=pd.date\_range("2000", freq="D", periods=3))

</div>

However, <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> can directly also support the time component as data itself.

<div class="ipython">

python

pd.Series(pd.date\_range("2000", freq="D", periods=3))

</div>

<span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> have extended data type support and functionality for `datetime`, `timedelta` and `Period` data when passed into those constructors. `DateOffset` data however will be stored as `object` data.

<div class="ipython">

python

pd.Series(pd.period\_range("1/1/2011", freq="M", periods=3)) pd.Series(\[pd.DateOffset(1), pd.DateOffset(2)\]) pd.Series(pd.date\_range("1/1/2011", freq="ME", periods=3))

</div>

Lastly, pandas represents null date times, time deltas, and time spans as `NaT` which is useful for representing missing or null date like values and behaves similar as `np.nan` does for float data.

<div class="ipython">

python

pd.Timestamp(pd.NaT) pd.Timedelta(pd.NaT) pd.Period(pd.NaT) \# Equality acts as np.nan would pd.NaT == pd.NaT

</div>

## Timestamps vs. time spans

Timestamped data is the most basic type of time series data that associates values with points in time. For pandas objects it means using the points in time.

<div class="ipython">

python

import datetime

pd.Timestamp(datetime.datetime(2012, 5, 1)) pd.Timestamp("2012-05-01") pd.Timestamp(2012, 5, 1)

</div>

However, in many cases it is more natural to associate things like change variables with a time span instead. The span represented by `Period` can be specified explicitly, or inferred from datetime string format.

For example:

<div class="ipython">

python

pd.Period("2011-01")

pd.Period("2012-05", freq="D")

</div>

<span class="title-ref">Timestamp</span> and <span class="title-ref">Period</span> can serve as an index. Lists of `Timestamp` and `Period` are automatically coerced to <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">PeriodIndex</span> respectively.

<div class="ipython">

python

  - dates = \[  
    pd.Timestamp("2012-05-01"), pd.Timestamp("2012-05-02"), pd.Timestamp("2012-05-03"),

\] ts = pd.Series(np.random.randn(3), dates)

type(ts.index) ts.index

ts

periods = \[pd.Period("2012-01"), pd.Period("2012-02"), pd.Period("2012-03")\]

ts = pd.Series(np.random.randn(3), periods)

type(ts.index) ts.index

ts

</div>

pandas allows you to capture both representations and convert between them. Under the hood, pandas represents timestamps using instances of `Timestamp` and sequences of timestamps using instances of `DatetimeIndex`. For regular time spans, pandas uses `Period` objects for scalar values and `PeriodIndex` for sequences of spans. Better support for irregular intervals with arbitrary start and end points are forth-coming in future releases.

## Converting to timestamps

To convert a <span class="title-ref">Series</span> or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the `to_datetime` function. When passed a `Series`, this returns a `Series` (with the same index), while a list-like is converted to a `DatetimeIndex`:

<div class="ipython">

python

pd.to\_datetime(pd.Series(\["Jul 31, 2009", "Jan 10, 2010", None\]))

pd.to\_datetime(\["2005/11/23", "2010/12/31"\])

</div>

If you use dates which start with the day first (i.e. European style), you can pass the `dayfirst` flag:

<div class="ipython" data-okwarning="">

python

pd.to\_datetime(\["04-01-2012 10:00"\], dayfirst=True)

pd.to\_datetime(\["04-14-2012 10:00"\], dayfirst=True)

</div>

\> **Warning** \> You see in the above example that `dayfirst` isn't strict. If a date can't be parsed with the day being first it will be parsed as if `dayfirst` were `False` and a warning will also be raised.

If you pass a single string to `to_datetime`, it returns a single `Timestamp`. `Timestamp` can also accept string input, but it doesn't accept string parsing options like `dayfirst` or `format`, so use `to_datetime` if these are required.

<div class="ipython">

python

pd.to\_datetime("2010/11/12")

pd.Timestamp("2010/11/12")

</div>

You can also use the `DatetimeIndex` constructor directly:

<div class="ipython">

python

pd.DatetimeIndex(\["2018-01-01", "2018-01-03", "2018-01-05"\])

</div>

The string 'infer' can be passed in order to set the frequency of the index as the inferred frequency upon creation:

<div class="ipython">

python

pd.DatetimeIndex(\["2018-01-01", "2018-01-03", "2018-01-05"\], freq="infer")

</div>

### Providing a format argument

In addition to the required datetime string, a `format` argument can be passed to ensure specific parsing. This could also potentially speed up the conversion considerably.

<div class="ipython">

python

pd.to\_datetime("2010/11/12", format="%Y/%m/%d")

pd.to\_datetime("12-11-2010 00:00", format="%d-%m-%Y %H:%M")

</div>

For more information on the choices available when specifying the `format` option, see the Python [datetime documentation](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).

### Assembling datetime from multiple DataFrame columns

You can also pass a `DataFrame` of integer or string columns to assemble into a `Series` of `Timestamps`.

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"year": \[2015, 2016\], "month": \[2, 3\], "day": \[4, 5\], "hour": \[2, 3\]}

) pd.to\_datetime(df)

</div>

You can pass only the columns that you need to assemble.

<div class="ipython">

python

pd.to\_datetime(df\[\["year", "month", "day"\]\])

</div>

`pd.to_datetime` looks for standard designations of the datetime component in the column names, including:

  - required: `year`, `month`, `day`
  - optional: `hour`, `minute`, `second`, `millisecond`, `microsecond`, `nanosecond`

### Invalid data

The default behavior, `errors='raise'`, is to raise when unparsable:

<div class="ipython" data-okexcept="">

python

pd.to\_datetime(\['2009/07/31', 'asd'\], errors='raise')

</div>

Pass `errors='coerce'` to convert unparsable data to `NaT` (not a time):

<div class="ipython">

python

pd.to\_datetime(\["2009/07/31", "asd"\], errors="coerce")

</div>

### Epoch timestamps

pandas supports converting integer or float epoch times to `Timestamp` and `DatetimeIndex`. The default unit is nanoseconds, since that is how `Timestamp` objects are stored internally. However, epochs are often stored in another `unit` which can be specified. These are computed from the starting point specified by the `origin` parameter.

<div class="ipython">

python

  - pd.to\_datetime(  
    \[1349720105, 1349806505, 1349892905, 1349979305, 1350065705\], unit="s"

)

  - pd.to\_datetime(  
    \[1349720105100, 1349720105200, 1349720105300, 1349720105400, 1349720105500\], unit="ms",

)

</div>

\> **Note** \> The `unit` parameter does not use the same strings as the `format` parameter that was discussed \[above\<timeseries.converting.format\>\](\#above\<timeseries.converting.format\>). The available units are listed on the documentation for <span class="title-ref">pandas.to\_datetime</span>.

Constructing a <span class="title-ref">Timestamp</span> or <span class="title-ref">DatetimeIndex</span> with an epoch timestamp with the `tz` argument specified will raise a ValueError. If you have epochs in wall time in another timezone, you can read the epochs as timezone-naive timestamps and then localize to the appropriate timezone:

<div class="ipython">

python

pd.Timestamp(1262347200000000000).tz\_localize("US/Pacific") pd.DatetimeIndex(\[1262347200000000000\]).tz\_localize("US/Pacific")

</div>

\> **Note** \> Epoch times will be rounded to the nearest nanosecond.

\> **Warning** \> Conversion of float epoch times can lead to inaccurate and unexpected results. \[Python floats \<python:tut-fp-issues\>\](\#python-floats-\<python:tut-fp-issues\>) have about 15 digits precision in decimal. Rounding during conversion from float to high precision `Timestamp` is unavoidable. The only way to achieve exact precision is to use a fixed-width types (e.g. an int64).

> 
> 
> <div class="ipython">
> 
> python
> 
> pd.to\_datetime(\[1490195805.433, 1490195805.433502912\], unit="s") pd.to\_datetime(1490195805433502912, unit="ns")
> 
> </div>

<div class="seealso">

\[timeseries.origin\](\#timeseries.origin)

</div>

### From timestamps to epoch

To invert the operation from above, namely, to convert from a `Timestamp` to a 'unix' epoch:

<div class="ipython">

python

stamps = pd.date\_range("2012-10-08 18:15:05", periods=4, freq="D") stamps

</div>

We subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide by the "unit" (1 second).

<div class="ipython">

python

(stamps - pd.Timestamp("1970-01-01")) // pd.Timedelta("1s")

</div>

### Using the `origin` parameter

Using the `origin` parameter, one can specify an alternative starting point for creation of a `DatetimeIndex`. For example, to use 1960-01-01 as the starting date:

<div class="ipython">

python

pd.to\_datetime(\[1, 2, 3\], unit="D", origin=pd.Timestamp("1960-01-01"))

</div>

The default is set at `origin='unix'`, which defaults to `1970-01-01 00:00:00`. Commonly called 'unix epoch' or POSIX time.

<div class="ipython">

python

pd.to\_datetime(\[1, 2, 3\], unit="D")

</div>

## Generating ranges of timestamps

To generate an index with timestamps, you can use either the `DatetimeIndex` or `Index` constructor and pass in a list of datetime objects:

<div class="ipython">

python

  - dates = \[  
    datetime.datetime(2012, 5, 1), datetime.datetime(2012, 5, 2), datetime.datetime(2012, 5, 3),

\]

\# Note the frequency information index = pd.DatetimeIndex(dates) index

\# Automatically converted to DatetimeIndex index = pd.Index(dates) index

</div>

In practice this becomes very cumbersome because we often need a very long index with a large number of timestamps. If we need timestamps on a regular frequency, we can use the <span class="title-ref">date\_range</span> and <span class="title-ref">bdate\_range</span> functions to create a `DatetimeIndex`. The default frequency for `date_range` is a **calendar day** while the default for `bdate_range` is a **business day**:

<div class="ipython">

python

start = datetime.datetime(2011, 1, 1) end = datetime.datetime(2012, 1, 1)

index = pd.date\_range(start, end) index

index = pd.bdate\_range(start, end) index

</div>

Convenience functions like `date_range` and `bdate_range` can utilize a variety of \[frequency aliases \<timeseries.offset\_aliases\>\](\#frequency-aliases-\<timeseries.offset\_aliases\>):

<div class="ipython">

python

pd.date\_range(start, periods=1000, freq="ME")

pd.bdate\_range(start, periods=250, freq="BQS")

</div>

`date_range` and `bdate_range` make it easy to generate a range of dates using various combinations of parameters like `start`, `end`, `periods`, and `freq`. The start and end dates are strictly inclusive, so dates outside of those specified will not be generated:

<div class="ipython">

python

pd.date\_range(start, end, freq="BME")

pd.date\_range(start, end, freq="W")

pd.bdate\_range(end=end, periods=20)

pd.bdate\_range(start=start, periods=20)

</div>

Specifying `start`, `end`, and `periods` will generate a range of evenly spaced dates from `start` to `end` inclusively, with `periods` number of elements in the resulting `DatetimeIndex`:

<div class="ipython">

python

pd.date\_range("2018-01-01", "2018-01-05", periods=5)

pd.date\_range("2018-01-01", "2018-01-05", periods=10)

</div>

### Custom frequency ranges

`bdate_range` can also generate a range of custom frequency dates by using the `weekmask` and `holidays` parameters. These parameters will only be used if a custom frequency string is passed.

<div class="ipython">

python

weekmask = "Mon Wed Fri"

holidays = \[datetime.datetime(2011, 1, 5), datetime.datetime(2011, 3, 14)\]

pd.bdate\_range(start, end, freq="C", weekmask=weekmask, holidays=holidays)

pd.bdate\_range(start, end, freq="CBMS", weekmask=weekmask)

</div>

<div class="seealso">

\[timeseries.custombusinessdays\](\#timeseries.custombusinessdays)

</div>

## Timestamp limitations

The limits of timestamp representation depend on the chosen resolution. For nanosecond resolution, the time span that can be represented using a 64-bit integer is limited to approximately 584 years:

<div class="ipython">

python

pd.Timestamp.min pd.Timestamp.max

</div>

When choosing second-resolution, the available range grows to `+/- 2.9e11 years`. Different resolutions can be converted to each other through `as_unit`.

<div class="seealso">

\[timeseries.oob\](\#timeseries.oob)

</div>

## Indexing

One of the main uses for `DatetimeIndex` is as an index for pandas objects. The `DatetimeIndex` class contains many time series related optimizations:

  - A large range of dates for various offsets are pre-computed and cached under the hood in order to make generating subsequent date ranges very fast (just have to grab a slice).
  - Fast shifting using the `shift` method on pandas objects.
  - Unioning of overlapping `DatetimeIndex` objects with the same frequency is very fast (important for fast data alignment).
  - Quick access to date fields via properties such as `year`, `month`, etc.
  - Regularization functions like `snap` and very fast `asof` logic.

`DatetimeIndex` objects have all the basic functionality of regular `Index` objects, and a smorgasbord of advanced time series specific methods for easy frequency processing.

<div class="seealso">

\[Reindexing methods \<basics.reindexing\>\](\#reindexing-methods-\<basics.reindexing\>)

</div>

\> **Note** \> While pandas does not force you to have a sorted date index, some of these methods may have unexpected or incorrect behavior if the dates are unsorted.

`DatetimeIndex` can be used like a regular index and offers all of its intelligent functionality like selection, slicing, etc.

<div class="ipython">

python

rng = pd.date\_range(start, end, freq="BME") ts = pd.Series(np.random.randn(len(rng)), index=rng) ts.index ts\[:5\].index ts\[::2\].index

</div>

### Partial string indexing

Dates and strings that parse to timestamps can be passed as indexing parameters:

<div class="ipython">

python

ts\["1/31/2011"\]

ts\[datetime.datetime(2011, 12, 25):\]

ts\["10/31/2011":"12/31/2011"\]

</div>

To provide convenience for accessing longer time series, you can also pass in the year or year and month as strings:

<div class="ipython">

python

ts\["2011"\]

ts\["2011-6"\]

</div>

This type of slicing will work on a `DataFrame` with a `DatetimeIndex` as well. Since the partial string selection is a form of label slicing, the endpoints **will be** included. This would include matching times on an included date:

\> **Warning** \> Indexing `DataFrame` rows with a *single* string with getitem (e.g. `frame[dtstring]`) is deprecated starting with pandas 1.2.0 (given the ambiguity whether it is indexing the rows or selecting a column) and will be removed in a future version. The equivalent with `.loc` (e.g. `frame.loc[dtstring]`) is still supported.

<div class="ipython">

python

  - dft = pd.DataFrame(  
    np.random.randn(100000, 1), columns=\["A"\], index=pd.date\_range("20130101", periods=100000, freq="min"),

) dft dft.loc\["2013"\]

</div>

This starts on the very first time in the month, and includes the last date and time for the month:

<div class="ipython">

python

dft\["2013-1":"2013-2"\]

</div>

This specifies a stop time **that includes all of the times on the last day**:

<div class="ipython">

python

dft\["2013-1":"2013-2-28"\]

</div>

This specifies an **exact** stop time (and is not the same as the above):

<div class="ipython">

python

dft\["2013-1":"2013-2-28 00:00:00"\]

</div>

We are stopping on the included end-point as it is part of the index:

<div class="ipython">

python

dft\["2013-1-15":"2013-1-15 12:30:00"\]

</div>

`DatetimeIndex` partial string indexing also works on a `DataFrame` with a `MultiIndex`:

<div class="ipython">

python

  - dft2 = pd.DataFrame(  
    np.random.randn(20, 1), columns=\["A"\], index=pd.MultiIndex.from\_product( \[pd.date\_range("20130101", periods=10, freq="12h"), \["a", "b"\]\] ),

) dft2 dft2.loc\["2013-01-05"\] idx = pd.IndexSlice dft2 = dft2.swaplevel(0, 1).sort\_index() dft2.loc\[idx\[:, "2013-01-05"\], :\]

</div>

Slicing with string indexing also honors UTC offset.

<div class="ipython">

python

df = pd.DataFrame(\[0\], index=pd.DatetimeIndex(\["2019-01-01"\], tz="US/Pacific")) df df\["2019-01-01 12:00:00+04:00":"2019-01-01 13:00:00+04:00"\]

</div>

### Slice vs. exact match

The same string used as an indexing parameter can be treated either as a slice or as an exact match depending on the resolution of the index. If the string is less accurate than the index, it will be treated as a slice, otherwise as an exact match.

Consider a `Series` object with a minute resolution index:

<div class="ipython">

python

  - series\_minute = pd.Series(  
    \[1, 2, 3\], pd.DatetimeIndex( \["2011-12-31 23:59:00", "2012-01-01 00:00:00", "2012-01-01 00:02:00"\] ),

) series\_minute.index.resolution

</div>

A timestamp string less accurate than a minute gives a `Series` object.

<div class="ipython">

python

series\_minute\["2011-12-31 23"\]

</div>

A timestamp string with minute resolution (or more accurate), gives a scalar instead, i.e. it is not casted to a slice.

<div class="ipython">

python

series\_minute\["2011-12-31 23:59"\] series\_minute\["2011-12-31 23:59:00"\]

</div>

If index resolution is second, then the minute-accurate timestamp gives a `Series`.

<div class="ipython">

python

  - series\_second = pd.Series(  
    \[1, 2, 3\], pd.DatetimeIndex( \["2011-12-31 23:59:59", "2012-01-01 00:00:00", "2012-01-01 00:00:01"\] ),

) series\_second.index.resolution series\_second\["2011-12-31 23:59"\]

</div>

If the timestamp string is treated as a slice, it can be used to index `DataFrame` with `.loc[]` as well.

<div class="ipython">

python

  - dft\_minute = pd.DataFrame(  
    {"a": \[1, 2, 3\], "b": \[4, 5, 6\]}, index=series\_minute.index

) dft\_minute.loc\["2011-12-31 23"\]

</div>

\> **Warning** \> However, if the string is treated as an exact match, the selection in `DataFrame`'s `[]` will be column-wise and not row-wise, see \[Indexing Basics \<indexing.basics\>\](\#indexing-basics-\<indexing.basics\>). For example `dft_minute['2011-12-31 23:59']` will raise `KeyError` as `'2012-12-31 23:59'` has the same resolution as the index and there is no column with such name:

> To *always* have unambiguous selection, whether the row is treated as a slice or a single selection, use `.loc`.
> 
> <div class="ipython">
> 
> python
> 
> dft\_minute.loc\["2011-12-31 23:59"\]
> 
> </div>

Note also that `DatetimeIndex` resolution cannot be less precise than day.

<div class="ipython">

python

  - series\_monthly = pd.Series(  
    \[1, 2, 3\], pd.DatetimeIndex(\["2011-12", "2012-01", "2012-02"\])

) series\_monthly.index.resolution series\_monthly\["2011-12"\] \# returns Series

</div>

### Exact indexing

As discussed in previous section, indexing a `DatetimeIndex` with a partial string depends on the "accuracy" of the period, in other words how specific the interval is in relation to the resolution of the index. In contrast, indexing with `Timestamp` or `datetime` objects is exact, because the objects have exact meaning. These also follow the semantics of *including both endpoints*.

These `Timestamp` and `datetime` objects have exact `hours, minutes,` and `seconds`, even though they were not explicitly specified (they are `0`).

<div class="ipython">

python

dft\[datetime.datetime(2013, 1, 1): datetime.datetime(2013, 2, 28)\]

</div>

With no defaults.

<div class="ipython">

python

  - dft\[
    
      - datetime.datetime(2013, 1, 1, 10, 12, 0): datetime.datetime(  
        2013, 2, 28, 10, 12, 0
    
    )

\]

</div>

### Truncating & fancy indexing

A <span class="title-ref">\~DataFrame.truncate</span> convenience function is provided that is similar to slicing. Note that `truncate` assumes a 0 value for any unspecified date component in a `DatetimeIndex` in contrast to slicing which returns any partially matching dates:

<div class="ipython">

python

rng2 = pd.date\_range("2011-01-01", "2012-01-01", freq="W") ts2 = pd.Series(np.random.randn(len(rng2)), index=rng2)

ts2.truncate(before="2011-11", after="2011-12") ts2\["2011-11":"2011-12"\]

</div>

Even complicated fancy indexing that breaks the `DatetimeIndex` frequency regularity will result in a `DatetimeIndex`, although frequency is lost:

<div class="ipython">

python

ts2.iloc\[\[0, 2, 6\]\].index

</div>

## Time/date components

There are several time/date properties that one can access from `Timestamp` or a collection of timestamps like a `DatetimeIndex`.

| Property           | Description                                                       |
| ------------------ | ----------------------------------------------------------------- |
| year               | The year of the datetime                                          |
| month              | The month of the datetime                                         |
| day                | The days of the datetime                                          |
| hour               | The hour of the datetime                                          |
| minute             | The minutes of the datetime                                       |
| second             | The seconds of the datetime                                       |
| microsecond        | The microseconds of the datetime                                  |
| nanosecond         | The nanoseconds of the datetime                                   |
| date               | Returns datetime.date (does not contain timezone information)     |
| time               | Returns datetime.time (does not contain timezone information)     |
| timetz             | Returns datetime.time as local time with timezone information     |
| dayofyear          | The ordinal day of year                                           |
| day\_of\_year      | The ordinal day of year                                           |
| dayofweek          | The number of the day of the week with Monday=0, Sunday=6         |
| day\_of\_week      | The number of the day of the week with Monday=0, Sunday=6         |
| weekday            | The number of the day of the week with Monday=0, Sunday=6         |
| quarter            | Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc.               |
| days\_in\_month    | The number of days in the month of the datetime                   |
| is\_month\_start   | Logical indicating if first day of month (defined by frequency)   |
| is\_month\_end     | Logical indicating if last day of month (defined by frequency)    |
| is\_quarter\_start | Logical indicating if first day of quarter (defined by frequency) |
| is\_quarter\_end   | Logical indicating if last day of quarter (defined by frequency)  |
| is\_year\_start    | Logical indicating if first day of year (defined by frequency)    |
| is\_year\_end      | Logical indicating if last day of year (defined by frequency)     |
| is\_leap\_year     | Logical indicating if the date belongs to a leap year             |

\> **Note** \> You can use `DatetimeIndex.isocalendar().week` to access week of year date information.

Furthermore, if you have a `Series` with datetimelike values, then you can access these properties via the `.dt` accessor, as detailed in the section on \[.dt accessors\<basics.dt\_accessors\>\](\#.dt-accessors\<basics.dt\_accessors\>).

You may obtain the year, week and day components of the ISO year from the ISO 8601 standard:

<div class="ipython">

python

idx = pd.date\_range(start="2019-12-29", freq="D", periods=4) idx.isocalendar() idx.to\_series().dt.isocalendar()

</div>

## DateOffset objects

In the preceding examples, frequency strings (e.g. `'D'`) were used to specify a frequency that defined:

  - how the date times in <span class="title-ref">DatetimeIndex</span> were spaced when using <span class="title-ref">date\_range</span>
  - the frequency of a <span class="title-ref">Period</span> or <span class="title-ref">PeriodIndex</span>

These frequency strings map to a <span class="title-ref">DateOffset</span> object and its subclasses. A <span class="title-ref">DateOffset</span> is similar to a <span class="title-ref">Timedelta</span> that represents a duration of time but follows specific calendar duration rules. For example, a <span class="title-ref">Timedelta</span> day will always increment `datetimes` by 24 hours, while a <span class="title-ref">DateOffset</span> day will increment `datetimes` to the same time the next day whether a day represents 23, 24 or 25 hours due to daylight savings time. However, all <span class="title-ref">DateOffset</span> subclasses that are an hour or smaller (`Hour`, `Minute`, `Second`, `Milli`, `Micro`, `Nano`) behave like <span class="title-ref">Timedelta</span> and respect absolute time.

The basic <span class="title-ref">DateOffset</span> acts similar to `dateutil.relativedelta` ([relativedelta documentation](https://dateutil.readthedocs.io/en/stable/relativedelta.html)) that shifts a date time by the corresponding calendar duration specified. The arithmetic operator (`+`) can be used to perform the shift.

<div class="ipython">

python

\# This particular day contains a day light savings time transition ts = pd.Timestamp("2016-10-30 00:00:00", tz="Europe/Helsinki") \# Respects absolute time ts + pd.Timedelta(days=1) \# Respects calendar time ts + pd.DateOffset(days=1) friday = pd.Timestamp("2018-01-05") friday.day\_name() \# Add 2 business days (Friday --\> Tuesday) two\_business\_days = 2 \* pd.offsets.BDay() friday + two\_business\_days (friday + two\_business\_days).day\_name()

</div>

Most `DateOffsets` have associated frequencies strings, or offset aliases, that can be passed into `freq` keyword arguments. The available date offsets and associated frequency strings can be found below:

| Date Offset                                                                                                                                              | Frequency String  | Description                                             |
| -------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- | ------------------------------------------------------- |
| <span class="title-ref">\~pandas.tseries.offsets.DateOffset</span>                                                                                       | None              | Generic offset class, defaults to absolute 24 hours     |
| <span class="title-ref">\~pandas.tseries.offsets.BDay</span> or <span class="title-ref">\~pandas.tseries.offsets.BusinessDay</span>                      | `'B'`             | business day (weekday)                                  |
| <span class="title-ref">\~pandas.tseries.offsets.CDay</span> or <span class="title-ref">\~pandas.tseries.offsets.CustomBusinessDay</span>                | `'C'`             | custom business day                                     |
| <span class="title-ref">\~pandas.tseries.offsets.Week</span>                                                                                             | `'W'`             | one week, optionally anchored on a day of the week      |
| <span class="title-ref">\~pandas.tseries.offsets.WeekOfMonth</span>                                                                                      | `'WOM'`           | the x-th day of the y-th week of each month             |
| <span class="title-ref">\~pandas.tseries.offsets.LastWeekOfMonth</span>                                                                                  | `'LWOM'`          | the x-th day of the last week of each month             |
| <span class="title-ref">\~pandas.tseries.offsets.MonthEnd</span>                                                                                         | `'ME'`            | calendar month end                                      |
| <span class="title-ref">\~pandas.tseries.offsets.MonthBegin</span>                                                                                       | `'MS'`            | calendar month begin                                    |
| <span class="title-ref">\~pandas.tseries.offsets.BMonthEnd</span> or <span class="title-ref">\~pandas.tseries.offsets.BusinessMonthEnd</span>            | `'BME'`           | business month end                                      |
| <span class="title-ref">\~pandas.tseries.offsets.BMonthBegin</span> or <span class="title-ref">\~pandas.tseries.offsets.BusinessMonthBegin</span>        | `'BMS'`           | business month begin                                    |
| <span class="title-ref">\~pandas.tseries.offsets.CBMonthEnd</span> or <span class="title-ref">\~pandas.tseries.offsets.CustomBusinessMonthEnd</span>     | `'CBME'`          | custom business month end                               |
| <span class="title-ref">\~pandas.tseries.offsets.CBMonthBegin</span> or <span class="title-ref">\~pandas.tseries.offsets.CustomBusinessMonthBegin</span> | `'CBMS'`          | custom business month begin                             |
| <span class="title-ref">\~pandas.tseries.offsets.SemiMonthEnd</span>                                                                                     | `'SME'`           | 15th (or other day\_of\_month) and calendar month end   |
| <span class="title-ref">\~pandas.tseries.offsets.SemiMonthBegin</span>                                                                                   | `'SMS'`           | 15th (or other day\_of\_month) and calendar month begin |
| <span class="title-ref">\~pandas.tseries.offsets.QuarterEnd</span>                                                                                       | `'QE'`            | calendar quarter end                                    |
| <span class="title-ref">\~pandas.tseries.offsets.QuarterBegin</span>                                                                                     | `'QS'`            | calendar quarter begin                                  |
| <span class="title-ref">\~pandas.tseries.offsets.BQuarterEnd</span>                                                                                      | `'BQE`            | business quarter end                                    |
| <span class="title-ref">\~pandas.tseries.offsets.BQuarterBegin</span>                                                                                    | `'BQS'`           | business quarter begin                                  |
| <span class="title-ref">\~pandas.tseries.offsets.FY5253Quarter</span>                                                                                    | `'REQ'`           | retail (aka 52-53 week) quarter                         |
| <span class="title-ref">\~pandas.tseries.offsets.YearEnd</span>                                                                                          | `'YE'`            | calendar year end                                       |
| <span class="title-ref">\~pandas.tseries.offsets.YearBegin</span>                                                                                        | `'YS'` or `'BYS'` | calendar year begin                                     |
| <span class="title-ref">\~pandas.tseries.offsets.BYearEnd</span>                                                                                         | `'BYE'`           | business year end                                       |
| <span class="title-ref">\~pandas.tseries.offsets.BYearBegin</span>                                                                                       | `'BYS'`           | business year begin                                     |
| <span class="title-ref">\~pandas.tseries.offsets.FY5253</span>                                                                                           | `'RE'`            | retail (aka 52-53 week) year                            |
| <span class="title-ref">\~pandas.tseries.offsets.Easter</span>                                                                                           | None              | Easter holiday                                          |
| <span class="title-ref">\~pandas.tseries.offsets.BusinessHour</span>                                                                                     | `'bh'`            | business hour                                           |
| <span class="title-ref">\~pandas.tseries.offsets.CustomBusinessHour</span>                                                                               | `'cbh'`           | custom business hour                                    |
| <span class="title-ref">\~pandas.tseries.offsets.Day</span>                                                                                              | `'D'`             | one absolute day                                        |
| <span class="title-ref">\~pandas.tseries.offsets.Hour</span>                                                                                             | `'h'`             | one hour                                                |
| <span class="title-ref">\~pandas.tseries.offsets.Minute</span>                                                                                           | `'min'`           | one minute                                              |
| <span class="title-ref">\~pandas.tseries.offsets.Second</span>                                                                                           | `'s'`             | one second                                              |
| <span class="title-ref">\~pandas.tseries.offsets.Milli</span>                                                                                            | `'ms'`            | one millisecond                                         |
| <span class="title-ref">\~pandas.tseries.offsets.Micro</span>                                                                                            | `'us'`            | one microsecond                                         |
| <span class="title-ref">\~pandas.tseries.offsets.Nano</span>                                                                                             | `'ns'`            | one nanosecond                                          |

`DateOffsets` additionally have <span class="title-ref">rollforward</span> and <span class="title-ref">rollback</span> methods for moving a date forward or backward respectively to a valid offset date relative to the offset. For example, business offsets will roll dates that land on the weekends (Saturday and Sunday) forward to Monday since business offsets operate on the weekdays.

<div class="ipython">

python

ts = pd.Timestamp("2018-01-06 00:00:00") ts.day\_name() \# BusinessHour's valid offset dates are Monday through Friday offset = pd.offsets.BusinessHour(start="09:00") \# Bring the date to the closest offset date (Monday) offset.rollforward(ts) \# Date is brought to the closest offset date first and then the hour is added ts + offset

</div>

These operations preserve time (hour, minute, etc) information by default. To reset time to midnight, use <span class="title-ref">normalize</span> before or after applying the operation (depending on whether you want the time information included in the operation).

<div class="ipython">

python

ts = pd.Timestamp("2014-01-01 09:00") day = pd.offsets.Day() day + ts (day + ts).normalize()

ts = pd.Timestamp("2014-01-01 22:00") hour = pd.offsets.Hour() hour + ts (hour + ts).normalize() (hour + pd.Timestamp("2014-01-01 23:30")).normalize()

</div>

### Parametric offsets

Some of the offsets can be "parameterized" when created to result in different behaviors. For example, the `Week` offset for generating weekly data accepts a `weekday` parameter which results in the generated dates always lying on a particular day of the week:

<div class="ipython">

python

d = datetime.datetime(2008, 8, 18, 9, 0) d d + pd.offsets.Week() d + pd.offsets.Week(weekday=4) (d + pd.offsets.Week(weekday=4)).weekday()

d - pd.offsets.Week()

</div>

The `normalize` option will be effective for addition and subtraction.

<div class="ipython">

python

d + pd.offsets.Week(normalize=True) d - pd.offsets.Week(normalize=True)

</div>

Another example is parameterizing `YearEnd` with the specific ending month:

<div class="ipython">

python

d + pd.offsets.YearEnd() d + pd.offsets.YearEnd(month=6)

</div>

### Using offsets with `Series` / `DatetimeIndex`

Offsets can be used with either a `Series` or `DatetimeIndex` to apply the offset to each element.

<div class="ipython">

python

rng = pd.date\_range("2012-01-01", "2012-01-03") s = pd.Series(rng) rng rng + pd.DateOffset(months=2) s + pd.DateOffset(months=2) s - pd.DateOffset(months=2)

</div>

If the offset class maps directly to a `Timedelta` (`Day`, `Hour`, `Minute`, `Second`, `Micro`, `Milli`, `Nano`) it can be used exactly like a `Timedelta` - see the \[Timedelta section\<timedeltas.operations\>\](\#timedelta-section\<timedeltas.operations\>) for more examples.

<div class="ipython">

python

s - pd.offsets.Day(2) td = s - pd.Series(pd.date\_range("2011-12-29", "2011-12-31")) td td + pd.offsets.Minute(15)

</div>

Note that some offsets (such as `BQuarterEnd`) do not have a vectorized implementation. They can still be used but may calculate significantly slower and will show a `PerformanceWarning`

<div class="ipython" data-okwarning="">

python

rng + pd.offsets.BQuarterEnd()

</div>

### Custom business days

The `CDay` or `CustomBusinessDay` class provides a parametric `BusinessDay` class which can be used to create customized business day calendars which account for local holidays and local weekend conventions.

As an interesting example, let's look at Egypt where a Friday-Saturday weekend is observed.

<div class="ipython">

python

weekmask\_egypt = "Sun Mon Tue Wed Thu"

\# They also observe International Workers' Day so let's \# add that for a couple of years

  - holidays = \[  
    "2012-05-01", datetime.datetime(2013, 5, 1), np.datetime64("2014-05-01"),

\] bday\_egypt = pd.offsets.CustomBusinessDay( holidays=holidays, weekmask=weekmask\_egypt, ) dt = datetime.datetime(2013, 4, 30) dt + 2 \* bday\_egypt

</div>

Let's map to the weekday names:

<div class="ipython">

python

dts = pd.date\_range(dt, periods=5, freq=bday\_egypt)

pd.Series(dts.weekday, dts).map(pd.Series("Mon Tue Wed Thu Fri Sat Sun".split()))

</div>

Holiday calendars can be used to provide the list of holidays. See the \[holiday calendar\<timeseries.holiday\>\](\#holiday-calendar\<timeseries.holiday\>) section for more information.

<div class="ipython">

python

from pandas.tseries.holiday import USFederalHolidayCalendar

bday\_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())

\# Friday before MLK Day dt = datetime.datetime(2014, 1, 17)

\# Tuesday after MLK Day (Monday is skipped because it's a holiday) dt + bday\_us

</div>

Monthly offsets that respect a certain holiday calendar can be defined in the usual way.

<div class="ipython">

python

bmth\_us = pd.offsets.CustomBusinessMonthBegin(calendar=USFederalHolidayCalendar())

\# Skip new years dt = datetime.datetime(2013, 12, 17) dt + bmth\_us

\# Define date index with custom offset pd.date\_range(start="20100101", end="20120101", freq=bmth\_us)

</div>

\> **Note** \> The frequency string 'C' is used to indicate that a CustomBusinessDay DateOffset is used, it is important to note that since CustomBusinessDay is a parameterised type, instances of CustomBusinessDay may differ and this is not detectable from the 'C' frequency string. The user therefore needs to ensure that the 'C' frequency string is used consistently within the user's application.

### Business hour

The `BusinessHour` class provides a business hour representation on `BusinessDay`, allowing to use specific start and end times.

By default, `BusinessHour` uses 9:00 - 17:00 as business hours. Adding `BusinessHour` will increment `Timestamp` by hourly frequency. If target `Timestamp` is out of business hours, move to the next business hour then increment it. If the result exceeds the business hours end, the remaining hours are added to the next business day.

<div class="ipython">

python

bh = pd.offsets.BusinessHour() bh

\# 2014-08-01 is Friday pd.Timestamp("2014-08-01 10:00").weekday() pd.Timestamp("2014-08-01 10:00") + bh

\# Below example is the same as: pd.Timestamp('2014-08-01 09:00') + bh pd.Timestamp("2014-08-01 08:00") + bh

\# If the results is on the end time, move to the next business day pd.Timestamp("2014-08-01 16:00") + bh

\# Remainings are added to the next day pd.Timestamp("2014-08-01 16:30") + bh

\# Adding 2 business hours pd.Timestamp("2014-08-01 10:00") + pd.offsets.BusinessHour(2)

\# Subtracting 3 business hours pd.Timestamp("2014-08-01 10:00") + pd.offsets.BusinessHour(-3)

</div>

You can also specify `start` and `end` time by keywords. The argument must be a `str` with an `hour:minute` representation or a `datetime.time` instance. Specifying seconds, microseconds and nanoseconds as business hour results in `ValueError`.

<div class="ipython">

python

bh = pd.offsets.BusinessHour(start="11:00", end=datetime.time(20, 0)) bh

pd.Timestamp("2014-08-01 13:00") + bh pd.Timestamp("2014-08-01 09:00") + bh pd.Timestamp("2014-08-01 18:00") + bh

</div>

Passing `start` time later than `end` represents midnight business hour. In this case, business hour exceeds midnight and overlap to the next day. Valid business hours are distinguished by whether it started from valid `BusinessDay`.

<div class="ipython">

python

bh = pd.offsets.BusinessHour(start="17:00", end="09:00") bh

pd.Timestamp("2014-08-01 17:00") + bh pd.Timestamp("2014-08-01 23:00") + bh

\# Although 2014-08-02 is Saturday, \# it is valid because it starts from 08-01 (Friday). pd.Timestamp("2014-08-02 04:00") + bh

\# Although 2014-08-04 is Monday, \# it is out of business hours because it starts from 08-03 (Sunday). pd.Timestamp("2014-08-04 04:00") + bh

</div>

Applying `BusinessHour.rollforward` and `rollback` to out of business hours results in the next business hour start or previous day's end. Different from other offsets, `BusinessHour.rollforward` may output different results from `apply` by definition.

This is because one day's business hour end is equal to next day's business hour start. For example, under the default business hours (9:00 - 17:00), there is no gap (0 minutes) between `2014-08-01 17:00` and `2014-08-04 09:00`.

<div class="ipython">

python

\# This adjusts a Timestamp to business hour edge pd.offsets.BusinessHour().rollback(pd.Timestamp("2014-08-02 15:00")) pd.offsets.BusinessHour().rollforward(pd.Timestamp("2014-08-02 15:00"))

\# It is the same as BusinessHour() + pd.Timestamp('2014-08-01 17:00'). \# And it is the same as BusinessHour() + pd.Timestamp('2014-08-04 09:00') pd.offsets.BusinessHour() + pd.Timestamp("2014-08-02 15:00")

\# BusinessDay results (for reference) pd.offsets.BusinessHour().rollforward(pd.Timestamp("2014-08-02"))

\# It is the same as BusinessDay() + pd.Timestamp('2014-08-01') \# The result is the same as rollworward because BusinessDay never overlap. pd.offsets.BusinessHour() + pd.Timestamp("2014-08-02")

</div>

`BusinessHour` regards Saturday and Sunday as holidays. To use arbitrary holidays, you can use `CustomBusinessHour` offset, as explained in the following subsection.

### Custom business hour

The `CustomBusinessHour` is a mixture of `BusinessHour` and `CustomBusinessDay` which allows you to specify arbitrary holidays. `CustomBusinessHour` works as the same as `BusinessHour` except that it skips specified custom holidays.

<div class="ipython">

python

from pandas.tseries.holiday import USFederalHolidayCalendar

bhour\_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar()) \# Friday before MLK Day dt = datetime.datetime(2014, 1, 17, 15)

dt + bhour\_us

\# Tuesday after MLK Day (Monday is skipped because it's a holiday) dt + bhour\_us \* 2

</div>

You can use keyword arguments supported by either `BusinessHour` and `CustomBusinessDay`.

<div class="ipython">

python

bhour\_mon = pd.offsets.CustomBusinessHour(start="10:00", weekmask="Tue Wed Thu Fri")

\# Monday is skipped because it's a holiday, business hour starts from 10:00 dt + bhour\_mon \* 2

</div>

### Offset aliases

A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as *offset aliases*.

| Alias | Description                                      |
| ----- | ------------------------------------------------ |
| B     | business day frequency                           |
| C     | custom business day frequency                    |
| D     | calendar day frequency                           |
| W     | weekly frequency                                 |
| ME    | month end frequency                              |
| SME   | semi-month end frequency (15th and end of month) |
| BME   | business month end frequency                     |
| CBME  | custom business month end frequency              |
| MS    | month start frequency                            |
| SMS   | semi-month start frequency (1st and 15th)        |
| BMS   | business month start frequency                   |
| CBMS  | custom business month start frequency            |
| QE    | quarter end frequency                            |
| BQE   | business quarter end frequency                   |
| QS    | quarter start frequency                          |
| BQS   | business quarter start frequency                 |
| YE    | year end frequency                               |
| BYE   | business year end frequency                      |
| YS    | year start frequency                             |
| BYS   | business year start frequency                    |
| h     | hourly frequency                                 |
| bh    | business hour frequency                          |
| cbh   | custom business hour frequency                   |
| min   | minutely frequency                               |
| s     | secondly frequency                               |
| ms    | milliseconds                                     |
| us    | microseconds                                     |
| ns    | nanoseconds                                      |

<div class="deprecated">

2.2.0

Aliases `H`, `BH`, `CBH`, `T`, `S`, `L`, `U`, and `N` are deprecated in favour of the aliases `h`, `bh`, `cbh`, `min`, `s`, `ms`, `us`, and `ns`.

Aliases `Y`, `M`, and `Q` are deprecated in favour of the aliases `YE`, `ME`, `QE`.

</div>

\> **Note** \> When using the offset aliases above, it should be noted that functions such as <span class="title-ref">date\_range</span>, <span class="title-ref">bdate\_range</span>, will only return timestamps that are in the interval defined by `start_date` and `end_date`. If the `start_date` does not correspond to the frequency, the returned timestamps will start at the next valid timestamp, same for `end_date`, the returned timestamps will stop at the previous valid timestamp.

> For example, for the offset `MS`, if the `start_date` is not the first of the month, the returned timestamps will start with the first day of the next month. If `end_date` is not the first day of a month, the last returned timestamp will be the first day of the corresponding month.
> 
> <div class="ipython">
> 
> python
> 
> dates\_lst\_1 = pd.date\_range("2020-01-06", "2020-04-03", freq="MS") dates\_lst\_1
> 
> dates\_lst\_2 = pd.date\_range("2020-01-01", "2020-04-01", freq="MS") dates\_lst\_2
> 
> </div>
> 
> We can see in the above example <span class="title-ref">date\_range</span> and <span class="title-ref">bdate\_range</span> will only return the valid timestamps between the `start_date` and `end_date`. If these are not valid timestamps for the given frequency it will roll to the next value for `start_date` (respectively previous for the `end_date`)

### Period aliases

A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as *period aliases*.

| Alias | Description            |
| ----- | ---------------------- |
| B     | business day frequency |
| D     | calendar day frequency |
| W     | weekly frequency       |
| M     | monthly frequency      |
| Q     | quarterly frequency    |
| Y     | yearly frequency       |
| h     | hourly frequency       |
| min   | minutely frequency     |
| s     | secondly frequency     |
| ms    | milliseconds           |
| us    | microseconds           |
| ns    | nanoseconds            |

<div class="deprecated">

2.2.0

Aliases `H`, `T`, `S`, `L`, `U`, and `N` are deprecated in favour of the aliases `h`, `min`, `s`, `ms`, `us`, and `ns`.

</div>

### Combining aliases

As we have seen previously, the alias and the offset instance are fungible in most functions:

<div class="ipython">

python

pd.date\_range(start, periods=5, freq="B")

pd.date\_range(start, periods=5, freq=pd.offsets.BDay())

</div>

You can combine together day and intraday offsets:

<div class="ipython">

python

pd.date\_range(start, periods=10, freq="2h20min")

pd.date\_range(start, periods=10, freq="1D10us")

</div>

### Anchored offsets

For some frequencies you can specify an anchoring suffix:

| Alias          | Description                                              |
| -------------- | -------------------------------------------------------- |
| W-SUN          | weekly frequency (Sundays). Same as 'W'                  |
| W-MON          | weekly frequency (Mondays)                               |
| W-TUE          | weekly frequency (Tuesdays)                              |
| W-WED          | weekly frequency (Wednesdays)                            |
| W-THU          | weekly frequency (Thursdays)                             |
| W-FRI          | weekly frequency (Fridays)                               |
| W-SAT          | weekly frequency (Saturdays)                             |
| (B)Q(E)(S)-DEC | quarterly frequency, year ends in December. Same as 'QE' |
| (B)Q(E)(S)-JAN | quarterly frequency, year ends in January                |
| (B)Q(E)(S)-FEB | quarterly frequency, year ends in February               |
| (B)Q(E)(S)-MAR | quarterly frequency, year ends in March                  |
| (B)Q(E)(S)-APR | quarterly frequency, year ends in April                  |
| (B)Q(E)(S)-MAY | quarterly frequency, year ends in May                    |
| (B)Q(E)(S)-JUN | quarterly frequency, year ends in June                   |
| (B)Q(E)(S)-JUL | quarterly frequency, year ends in July                   |
| (B)Q(E)(S)-AUG | quarterly frequency, year ends in August                 |
| (B)Q(E)(S)-SEP | quarterly frequency, year ends in September              |
| (B)Q(E)(S)-OCT | quarterly frequency, year ends in October                |
| (B)Q(E)(S)-NOV | quarterly frequency, year ends in November               |
| (B)Y(E)(S)-DEC | annual frequency, anchored end of December. Same as 'YE' |
| (B)Y(E)(S)-JAN | annual frequency, anchored end of January                |
| (B)Y(E)(S)-FEB | annual frequency, anchored end of February               |
| (B)Y(E)(S)-MAR | annual frequency, anchored end of March                  |
| (B)Y(E)(S)-APR | annual frequency, anchored end of April                  |
| (B)Y(E)(S)-MAY | annual frequency, anchored end of May                    |
| (B)Y(E)(S)-JUN | annual frequency, anchored end of June                   |
| (B)Y(E)(S)-JUL | annual frequency, anchored end of July                   |
| (B)Y(E)(S)-AUG | annual frequency, anchored end of August                 |
| (B)Y(E)(S)-SEP | annual frequency, anchored end of September              |
| (B)Y(E)(S)-OCT | annual frequency, anchored end of October                |
| (B)Y(E)(S)-NOV | annual frequency, anchored end of November               |

These can be used as arguments to `date_range`, `bdate_range`, constructors for `DatetimeIndex`, as well as various other timeseries-related functions in pandas.

### Anchored offset semantics

For those offsets that are anchored to the start or end of specific frequency (`MonthEnd`, `MonthBegin`, `WeekEnd`, etc), the following rules apply to rolling forward and backwards.

When `n` is not 0, if the given date is not on an anchor point, it snapped to the next(previous) anchor point, and moved `|n|-1` additional steps forwards or backwards.

<div class="ipython">

python

pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=1) pd.Timestamp("2014-01-02") + pd.offsets.MonthEnd(n=1)

pd.Timestamp("2014-01-02") - pd.offsets.MonthBegin(n=1) pd.Timestamp("2014-01-02") - pd.offsets.MonthEnd(n=1)

pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=4) pd.Timestamp("2014-01-02") - pd.offsets.MonthBegin(n=4)

</div>

If the given date *is* on an anchor point, it is moved `|n|` points forwards or backwards.

<div class="ipython">

python

pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=1) pd.Timestamp("2014-01-31") + pd.offsets.MonthEnd(n=1)

pd.Timestamp("2014-01-01") - pd.offsets.MonthBegin(n=1) pd.Timestamp("2014-01-31") - pd.offsets.MonthEnd(n=1)

pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=4) pd.Timestamp("2014-01-31") - pd.offsets.MonthBegin(n=4)

</div>

For the case when `n=0`, the date is not moved if on an anchor point, otherwise it is rolled forward to the next anchor point.

<div class="ipython">

python

pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=0) pd.Timestamp("2014-01-02") + pd.offsets.MonthEnd(n=0)

pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=0) pd.Timestamp("2014-01-31") + pd.offsets.MonthEnd(n=0)

</div>

### Holidays / holiday calendars

Holidays and calendars provide a simple way to define holiday rules to be used with `CustomBusinessDay` or in other analysis that requires a predefined set of holidays. The `AbstractHolidayCalendar` class provides all the necessary methods to return a list of holidays and only `rules` need to be defined in a specific holiday calendar class. Furthermore, the `start_date` and `end_date` class attributes determine over what date range holidays are generated. These should be overwritten on the `AbstractHolidayCalendar` class to have the range apply to all calendar subclasses. `USFederalHolidayCalendar` is the only calendar that exists and primarily serves as an example for developing other calendars.

For holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an observance rule determines when that holiday is observed if it falls on a weekend or some other non-observed day. Defined observance rules are:

| Rule                      | Description                                                               |
| ------------------------- | ------------------------------------------------------------------------- |
| next\_workday             | move Saturday and Sunday to Monday                                        |
| previous\_workday         | move Saturday and Sunday to Friday                                        |
| nearest\_workday          | move Saturday to Friday and Sunday to Monday                              |
| before\_nearest\_workday  | apply `nearest_workday` and then move to previous workday before that day |
| after\_nearest\_workday   | apply `nearest_workday` and then move to next workday after that day      |
| sunday\_to\_monday        | move Sunday to following Monday                                           |
| next\_monday\_or\_tuesday | move Saturday to Monday and Sunday/Monday to Tuesday                      |
| previous\_friday          | move Saturday and Sunday to previous Friday                               |
| next\_monday              | move Saturday and Sunday to following Monday                              |
| weekend\_to\_monday       | same as `next_monday`                                                     |

An example of how holidays and holiday calendars are defined:

<div class="ipython">

python

  - from pandas.tseries.holiday import (  
    Holiday, USMemorialDay, AbstractHolidayCalendar, nearest\_workday, MO,

)

  - class ExampleCalendar(AbstractHolidayCalendar):
    
      - rules = \[  
        USMemorialDay, Holiday("July 4th", month=7, day=4, observance=nearest\_workday), Holiday( "Columbus Day", month=10, day=1, offset=pd.DateOffset(weekday=MO(2)), ),
    
    \]

cal = ExampleCalendar() cal.holidays(datetime.datetime(2012, 1, 1), datetime.datetime(2012, 12, 31))

</div>

  - hint  
    **weekday=MO(2)** is same as **2 \* Week(weekday=2)**

Using this calendar, creating an index or doing offset arithmetic skips weekends and holidays (i.e., Memorial Day/July 4th). For example, the below defines a custom business day offset using the `ExampleCalendar`. Like any other offset, it can be used to create a `DatetimeIndex` or added to `datetime` or `Timestamp` objects.

<div class="ipython">

python

  - pd.date\_range(  
    start="7/1/2012", end="7/10/2012", freq=pd.offsets.CDay(calendar=cal)

).to\_pydatetime() offset = pd.offsets.CustomBusinessDay(calendar=cal) datetime.datetime(2012, 5, 25) + offset datetime.datetime(2012, 7, 3) + offset datetime.datetime(2012, 7, 3) + 2 \* offset datetime.datetime(2012, 7, 6) + offset

</div>

Ranges are defined by the `start_date` and `end_date` class attributes of `AbstractHolidayCalendar`. The defaults are shown below.

<div class="ipython">

python

AbstractHolidayCalendar.start\_date AbstractHolidayCalendar.end\_date

</div>

These dates can be overwritten by setting the attributes as datetime/Timestamp/string.

<div class="ipython">

python

AbstractHolidayCalendar.start\_date = datetime.datetime(2012, 1, 1) AbstractHolidayCalendar.end\_date = datetime.datetime(2012, 12, 31) cal.holidays()

</div>

Every calendar class is accessible by name using the `get_calendar` function which returns a holiday class instance. Any imported calendar class will automatically be available by this function. Also, `HolidayCalendarFactory` provides an easy interface to create calendars that are combinations of calendars or calendars with additional rules.

<div class="ipython">

python

from pandas.tseries.holiday import get\_calendar, HolidayCalendarFactory, USLaborDay

cal = get\_calendar("ExampleCalendar") cal.rules new\_cal = HolidayCalendarFactory("NewExampleCalendar", cal, USLaborDay) new\_cal.rules

</div>

## Time Series-related instance methods

### Shifting / lagging

One may want to *shift* or *lag* the values in a time series back and forward in time. The method for this is <span class="title-ref">\~Series.shift</span>, which is available on all of the pandas objects.

<div class="ipython">

python

ts = pd.Series(range(len(rng)), index=rng) ts = ts\[:5\] ts.shift(1)

</div>

The `shift` method accepts an `freq` argument which can accept a `DateOffset` class or other `timedelta`-like object or also an \[offset alias \<timeseries.offset\_aliases\>\](\#offset-alias-\<timeseries.offset\_aliases\>).

When `freq` is specified, `shift` method changes all the dates in the index rather than changing the alignment of the data and the index:

<div class="ipython">

python

ts.shift(5, freq="D") ts.shift(5, freq=pd.offsets.BDay()) ts.shift(5, freq="BME")

</div>

Note that with when `freq` is specified, the leading entry is no longer NaN because the data is not being realigned.

### Frequency conversion

The primary function for changing frequencies is the <span class="title-ref">\~Series.asfreq</span> method. For a `DatetimeIndex`, this is basically just a thin, but convenient wrapper around <span class="title-ref">\~Series.reindex</span> which generates a `date_range` and calls `reindex`.

<div class="ipython">

python

dr = pd.date\_range("1/1/2010", periods=3, freq=3 \* pd.offsets.BDay()) ts = pd.Series(np.random.randn(3), index=dr) ts ts.asfreq(pd.offsets.BDay())

</div>

`asfreq` provides a further convenience so you can specify an interpolation method for any gaps that may appear after the frequency conversion.

<div class="ipython">

python

ts.asfreq(pd.offsets.BDay(), method="pad")

</div>

### Filling forward / backward

Related to `asfreq` and `reindex` is <span class="title-ref">\~Series.fillna</span>, which is documented in the \[missing data section \<missing\_data.fillna\>\](\#missing-data-section-\<missing\_data.fillna\>).

### Converting to Python datetimes

`DatetimeIndex` can be converted to an array of Python native :py\`datetime.datetime\` objects using the `to_pydatetime` method.

## Resampling

pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.

<span class="title-ref">\~Series.resample</span> is a time-based groupby, followed by a reduction method on each of its groups. See some \[cookbook examples \<cookbook.resample\>\](\#cookbook-examples-\<cookbook.resample\>) for some advanced strategies.

The `resample()` method can be used directly from `DataFrameGroupBy` objects, see the \[groupby docs \<groupby.transform.window\_resample\>\](\#groupby-docs-\<groupby.transform.window\_resample\>).

### Basics

<div class="ipython">

python

rng = pd.date\_range("1/1/2012", periods=100, freq="s")

ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)

ts.resample("5Min").sum()

</div>

The `resample` function is very flexible and allows you to specify many different parameters to control the frequency conversion and resampling operation.

Any built-in method available via \[GroupBy \<api.groupby\>\](\#groupby-\<api.groupby\>) is available as a method of the returned object, including `sum`, `mean`, `std`, `sem`, `max`, `min`, `median`, `first`, `last`, `ohlc`:

<div class="ipython">

python

ts.resample("5Min").mean()

ts.resample("5Min").ohlc()

ts.resample("5Min").max()

</div>

For downsampling, `closed` can be set to 'left' or 'right' to specify which end of the interval is closed:

<div class="ipython">

python

ts.resample("5Min", closed="right").mean()

ts.resample("5Min", closed="left").mean()

</div>

Parameters like `label` are used to manipulate the resulting labels. `label` specifies whether the result is labeled with the beginning or the end of the interval.

<div class="ipython">

python

ts.resample("5Min").mean() \# by default label='left'

ts.resample("5Min", label="left").mean()

</div>

\> **Warning** \> The default values for `label` and `closed` is '**left**' for all frequency offsets except for 'ME', 'YE', 'QE', 'BME', 'BYE', 'BQE', and 'W' which all have a default of 'right'.

> This might unintendedly lead to looking ahead, where the value for a later time is pulled back to a previous time as in the following example with the <span class="title-ref">\~pandas.tseries.offsets.BusinessDay</span> frequency:
> 
> <div class="ipython">
> 
> python
> 
> s = pd.date\_range("2000-01-01", "2000-01-05").to\_series() s.iloc\[2\] = pd.NaT s.dt.day\_name()
> 
> \# default: label='left', closed='left' s.resample("B").last().dt.day\_name()
> 
> </div>
> 
> Notice how the value for Sunday got pulled back to the previous Friday. To get the behavior where the value for Sunday is pushed to Monday, use instead
> 
> <div class="ipython">
> 
> python
> 
> s.resample("B", label="right", closed="right").last().dt.day\_name()
> 
> </div>

The `axis` parameter can be set to 0 or 1 and allows you to resample the specified axis for a `DataFrame`.

`kind` can be set to 'timestamp' or 'period' to convert the resulting index to/from timestamp and time span representations. By default `resample` retains the input representation.

`convention` can be set to 'start' or 'end' when resampling period data (detail below). It specifies how low frequency periods are converted to higher frequency periods.

### Upsampling

For upsampling, you can specify a way to upsample and the `limit` parameter to interpolate over the gaps that are created:

<div class="ipython">

python

\# from secondly to every 250 milliseconds

ts\[:2\].resample("250ms").asfreq()

ts\[:2\].resample("250ms").ffill()

ts\[:2\].resample("250ms").ffill(limit=2)

</div>

### Sparse resampling

Sparse timeseries are the ones where you have a lot fewer points relative to the amount of time you are looking to resample. Naively upsampling a sparse series can potentially generate lots of intermediate values. When you don't want to use a method to fill these values, e.g. `fill_method` is `None`, then intermediate values will be filled with `NaN`.

Since `resample` is a time-based groupby, the following is a method to efficiently resample only the groups that are not all `NaN`.

<div class="ipython">

python

rng = pd.date\_range("2014-1-1", periods=100, freq="D") + pd.Timedelta("1s") ts = pd.Series(range(100), index=rng)

</div>

If we want to resample to the full range of the series:

<div class="ipython">

python

ts.resample("3min").sum()

</div>

We can instead only resample those groups where we have points as follows:

<div class="ipython">

python

from functools import partial from pandas.tseries.frequencies import to\_offset

  - def round(t, freq):  
    \# round a Timestamp to a specified freq freq = to\_offset(freq) td = pd.Timedelta(freq) return pd.Timestamp((t.value // td.value) \* td.value)

ts.groupby(partial(round, freq="3min")).sum()

</div>

### Aggregation

The `resample()` method returns a `pandas.api.typing.Resampler` instance. Similar to the \[aggregating API \<basics.aggregate\>\](\#aggregating-api-\<basics.aggregate\>), \[groupby API \<groupby.aggregate\>\](\#groupby-api-\<groupby.aggregate\>), and the \[window API \<window.overview\>\](\#window-api-\<window.overview\>), a `Resampler` can be selectively resampled.

Resampling a `DataFrame`, the default will be to act on all columns with the same function.

<div class="ipython">

python

  - df = pd.DataFrame(  
    np.random.randn(1000, 3), index=pd.date\_range("1/1/2012", freq="s", periods=1000), columns=\["A", "B", "C"\],

) r = df.resample("3min") r.mean()

</div>

We can select a specific column or columns using standard getitem.

<div class="ipython">

python

r\["A"\].mean()

r\[\["A", "B"\]\].mean()

</div>

You can pass a list or dict of functions to do aggregation with, outputting a `DataFrame`:

<div class="ipython">

python

r\["A"\].agg(\["sum", "mean", "std"\])

</div>

On a resampled `DataFrame`, you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index:

<div class="ipython">

python

r.agg(\["sum", "mean"\])

</div>

By passing a dict to `aggregate` you can apply a different aggregation to the columns of a `DataFrame`:

<div class="ipython" data-okexcept="">

python

r.agg({"A": "sum", "B": lambda x: np.std(x, ddof=1)})

</div>

The function names can also be strings. In order for a string to be valid it must be implemented on the resampled object:

<div class="ipython">

python

r.agg({"A": "sum", "B": "std"})

</div>

Furthermore, you can also specify multiple aggregation functions for each column separately.

<div class="ipython">

python

r.agg({"A": \["sum", "std"\], "B": \["mean", "std"\]})

</div>

If a `DataFrame` does not have a datetimelike index, but instead you want to resample based on datetimelike column in the frame, it can passed to the `on` keyword.

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"date": pd.date\_range("2015-01-01", freq="W", periods=5), "a": np.arange(5)}, index=pd.MultiIndex.from\_arrays( \[\[1, 2, 3, 4, 5\], pd.date\_range("2015-01-01", freq="W", periods=5)\], names=\["v", "d"\], ),

) df df.resample("MS", on="date")\[\["a"\]\].sum()

</div>

Similarly, if you instead want to resample by a datetimelike level of `MultiIndex`, its name or location can be passed to the `level` keyword.

<div class="ipython">

python

df.resample("MS", level="d")\[\["a"\]\].sum()

</div>

### Iterating through groups

With the `Resampler` object in hand, iterating through the grouped data is very natural and functions similarly to :py\`itertools.groupby\`:

<div class="ipython">

python

  - small = pd.Series(  
    range(6), index=pd.to\_datetime( \[ "2017-01-01T00:00:00", "2017-01-01T00:30:00", "2017-01-01T00:31:00", "2017-01-01T01:00:00", "2017-01-01T03:00:00", "2017-01-01T03:05:00", \] ),

) resampled = small.resample("h")

  - for name, group in resampled:  
    print("Group: ", name) print("-" \* 27) print(group, end="nn")

</div>

See \[groupby.iterating-label\](\#groupby.iterating-label) or <span class="title-ref">Resampler.\_\_iter\_\_</span> for more.

### Use `origin` or `offset` to adjust the start of the bins

The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like `30D`) or that divide a day evenly (like `90s` or `1min`). This can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can specify a fixed Timestamp with the argument `origin`.

For example:

<div class="ipython">

python

start, end = "2000-10-01 23:30:00", "2000-10-02 00:30:00" middle = "2000-10-02 00:00:00" rng = pd.date\_range(start, end, freq="7min") ts = pd.Series(np.arange(len(rng)) \* 3, index=rng) ts

</div>

Here we can see that, when using `origin` with its default value (`'start_day'`), the result after `'2000-10-02 00:00:00'` are not identical depending on the start of time series:

<div class="ipython">

python

ts.resample("17min", origin="start\_day").sum() ts\[middle:end\].resample("17min", origin="start\_day").sum()

</div>

Here we can see that, when setting `origin` to `'epoch'`, the result after `'2000-10-02 00:00:00'` are identical depending on the start of time series:

<div class="ipython">

python

ts.resample("17min", origin="epoch").sum() ts\[middle:end\].resample("17min", origin="epoch").sum()

</div>

If needed you can use a custom timestamp for `origin`:

<div class="ipython">

python

ts.resample("17min", origin="2001-01-01").sum() ts\[middle:end\].resample("17min", origin=pd.Timestamp("2001-01-01")).sum()

</div>

If needed you can just adjust the bins with an `offset` Timedelta that would be added to the default `origin`. Those two examples are equivalent for this time series:

<div class="ipython">

python

ts.resample("17min", origin="start").sum() ts.resample("17min", offset="23h30min").sum()

</div>

Note the use of `'start'` for `origin` on the last example. In that case, `origin` will be set to the first value of the timeseries.

### Backward resample

<div class="versionadded">

1.3.0

</div>

Instead of adjusting the beginning of bins, sometimes we need to fix the end of the bins to make a backward resample with a given `freq`. The backward resample sets `closed` to `'right'` by default since the last value should be considered as the edge point for the last bin.

We can set `origin` to `'end'`. The value for a specific `Timestamp` index stands for the resample result from the current `Timestamp` minus `freq` to the current `Timestamp` with a right close.

<div class="ipython">

python

ts.resample('17min', origin='end').sum()

</div>

Besides, in contrast with the `'start_day'` option, `end_day` is supported. This will set the origin as the ceiling midnight of the largest `Timestamp`.

<div class="ipython">

python

ts.resample('17min', origin='end\_day').sum()

</div>

The above result uses `2000-10-02 00:29:00` as the last bin's right edge since the following computation.

<div class="ipython">

python

ceil\_mid = rng.max().ceil('D') freq = pd.offsets.Minute(17) bin\_res = ceil\_mid - freq \* ((ceil\_mid - rng.max()) // freq) bin\_res

</div>

## Time span representation

Regular intervals of time are represented by `Period` objects in pandas while sequences of `Period` objects are collected in a `PeriodIndex`, which can be created with the convenience function `period_range`.

### Period

A `Period` represents a span of time (e.g., a day, a month, a quarter, etc). You can specify the span via `freq` keyword using a frequency alias like below. Because `freq` represents a span of `Period`, it cannot be negative like "-3D".

<div class="ipython">

python

pd.Period("2012", freq="Y-DEC")

pd.Period("2012-1-1", freq="D")

pd.Period("2012-1-1 19:00", freq="h")

pd.Period("2012-1-1 19:00", freq="5h")

</div>

Adding and subtracting integers from periods shifts the period by its own frequency. Arithmetic is not allowed between `Period` with different `freq` (span).

<div class="ipython">

python

p = pd.Period("2012", freq="Y-DEC") p + 1 p - 3 p = pd.Period("2012-01", freq="2M") p + 2 p - 1 p == pd.Period("2012-01", freq="3M")

</div>

If `Period` freq is daily or higher (`D`, `h`, `min`, `s`, `ms`, `us`, and `ns`), `offsets` and `timedelta`-like can be added if the result can have the same freq. Otherwise, `ValueError` will be raised.

<div class="ipython">

python

p = pd.Period("2014-07-01 09:00", freq="h") p + pd.offsets.Hour(2) p + datetime.timedelta(minutes=120) p + np.timedelta64(7200, "s")

</div>

<div class="ipython" data-okexcept="">

python

p + pd.offsets.Minute(5)

</div>

If `Period` has other frequencies, only the same `offsets` can be added. Otherwise, `ValueError` will be raised.

<div class="ipython">

python

p = pd.Period("2014-07", freq="M") p + pd.offsets.MonthEnd(3)

</div>

<div class="ipython" data-okexcept="">

python

p + pd.offsets.MonthBegin(3)

</div>

Taking the difference of `Period` instances with the same frequency will return the number of frequency units between them:

<div class="ipython">

python

pd.Period("2012", freq="Y-DEC") - pd.Period("2002", freq="Y-DEC")

</div>

### PeriodIndex and period\_range

Regular sequences of `Period` objects can be collected in a `PeriodIndex`, which can be constructed using the `period_range` convenience function:

<div class="ipython">

python

prng = pd.period\_range("1/1/2011", "1/1/2012", freq="M") prng

</div>

The `PeriodIndex` constructor can also be used directly:

<div class="ipython">

python

pd.PeriodIndex(\["2011-1", "2011-2", "2011-3"\], freq="M")

</div>

Passing multiplied frequency outputs a sequence of `Period` which has multiplied span.

<div class="ipython">

python

pd.period\_range(start="2014-01", freq="3M", periods=4)

</div>

If `start` or `end` are `Period` objects, they will be used as anchor endpoints for a `PeriodIndex` with frequency matching that of the `PeriodIndex` constructor.

<div class="ipython">

python

  - pd.period\_range(  
    start=pd.Period("2017Q1", freq="Q"), end=pd.Period("2017Q2", freq="Q"), freq="M"

)

</div>

Just like `DatetimeIndex`, a `PeriodIndex` can also be used to index pandas objects:

<div class="ipython">

python

ps = pd.Series(np.random.randn(len(prng)), prng) ps

</div>

`PeriodIndex` supports addition and subtraction with the same rule as `Period`.

<div class="ipython">

python

idx = pd.period\_range("2014-07-01 09:00", periods=5, freq="h") idx idx + pd.offsets.Hour(2)

idx = pd.period\_range("2014-07", periods=5, freq="M") idx idx + pd.offsets.MonthEnd(3)

</div>

`PeriodIndex` has its own dtype named `period`, refer to \[Period Dtypes \<timeseries.period\_dtype\>\](\#period-dtypes-\<timeseries.period\_dtype\>).

### Period dtypes

`PeriodIndex` has a custom `period` dtype. This is a pandas extension dtype similar to the \[timezone aware dtype \<timeseries.timezone\_series\>\](\#timezone-aware-dtype-\<timeseries.timezone\_series\>) (`datetime64[ns, tz]`).

The `period` dtype holds the `freq` attribute and is represented with `period[freq]` like `period[D]` or `period[M]`, using \[frequency strings \<timeseries.period\_aliases\>\](\#frequency-strings-\<timeseries.period\_aliases\>).

<div class="ipython">

python

pi = pd.period\_range("2016-01-01", periods=3, freq="M") pi pi.dtype

</div>

The `period` dtype can be used in `.astype(...)`. It allows one to change the `freq` of a `PeriodIndex` like `.asfreq()` and convert a `DatetimeIndex` to `PeriodIndex` like `to_period()`:

<div class="ipython">

python

\# change monthly freq to daily freq pi.astype("period\[D\]")

\# convert to DatetimeIndex pi.astype("datetime64\[ns\]")

\# convert to PeriodIndex dti = pd.date\_range("2011-01-01", freq="ME", periods=3) dti dti.astype("period\[M\]")

</div>

### PeriodIndex partial string indexing

PeriodIndex now supports partial string slicing with non-monotonic indexes.

You can pass in dates and strings to `Series` and `DataFrame` with `PeriodIndex`, in the same manner as `DatetimeIndex`. For details, refer to \[DatetimeIndex Partial String Indexing \<timeseries.partialindexing\>\](\#datetimeindex-partial-string-indexing-\<timeseries.partialindexing\>).

<div class="ipython">

python

ps\["2011-01"\]

ps\[datetime.datetime(2011, 12, 25):\]

ps\["10/31/2011":"12/31/2011"\]

</div>

Passing a string representing a lower frequency than `PeriodIndex` returns partial sliced data.

<div class="ipython">

python

ps\["2011"\]

  - dfp = pd.DataFrame(  
    np.random.randn(600, 1), columns=\["A"\], index=pd.period\_range("2013-01-01 9:00", periods=600, freq="min"),

) dfp dfp.loc\["2013-01-01 10h"\]

</div>

As with `DatetimeIndex`, the endpoints will be included in the result. The example below slices data starting from 10:00 to 11:59.

<div class="ipython">

python

dfp\["2013-01-01 10h":"2013-01-01 11h"\]

</div>

### Frequency conversion and resampling with PeriodIndex

The frequency of `Period` and `PeriodIndex` can be converted via the `asfreq` method. Let's start with the fiscal year 2011, ending in December:

<div class="ipython">

python

p = pd.Period("2011", freq="Y-DEC") p

</div>

We can convert it to a monthly frequency. Using the `how` parameter, we can specify whether to return the starting or ending month:

<div class="ipython">

python

p.asfreq("M", how="start")

p.asfreq("M", how="end")

</div>

The shorthands 's' and 'e' are provided for convenience:

<div class="ipython">

python

p.asfreq("M", "s") p.asfreq("M", "e")

</div>

Converting to a "super-period" (e.g., annual frequency is a super-period of quarterly frequency) automatically returns the super-period that includes the input period:

<div class="ipython">

python

p = pd.Period("2011-12", freq="M")

p.asfreq("Y-NOV")

</div>

Note that since we converted to an annual frequency that ends the year in November, the monthly period of December 2011 is actually in the 2012 Y-NOV period.

<div id="timeseries.quarterly">

Period conversions with anchored frequencies are particularly useful for working with various quarterly data common to economics, business, and other fields. Many organizations define quarters relative to the month in which their fiscal year starts and ends. Thus, first quarter of 2011 could start in 2010 or a few months into 2011. Via anchored frequencies, pandas works for all quarterly frequencies `Q-JAN` through `Q-DEC`.

</div>

`Q-DEC` define regular calendar quarters:

<div class="ipython">

python

p = pd.Period("2012Q1", freq="Q-DEC")

p.asfreq("D", "s")

p.asfreq("D", "e")

</div>

`Q-MAR` defines fiscal year end in March:

<div class="ipython">

python

p = pd.Period("2011Q4", freq="Q-MAR")

p.asfreq("D", "s")

p.asfreq("D", "e")

</div>

## Converting between representations

Timestamped data can be converted to PeriodIndex-ed data using `to_period` and vice-versa using `to_timestamp`:

<div class="ipython">

python

rng = pd.date\_range("1/1/2012", periods=5, freq="ME")

ts = pd.Series(np.random.randn(len(rng)), index=rng)

ts

ps = ts.to\_period()

ps

ps.to\_timestamp()

</div>

Remember that 's' and 'e' can be used to return the timestamps at the start or end of the period:

<div class="ipython">

python

ps.to\_timestamp("D", how="s")

</div>

Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:

<div class="ipython">

python

prng = pd.period\_range("1990Q1", "2000Q4", freq="Q-NOV")

ts = pd.Series(np.random.randn(len(prng)), prng)

ts.index = (prng.asfreq("M", "e") + 1).asfreq("h", "s") + 9

ts.head()

</div>

## Representing out-of-bounds spans

If you have data that is outside of the `Timestamp` bounds, see \[Timestamp limitations \<timeseries.timestamp-limits\>\](\#timestamp-limitations-\<timeseries.timestamp-limits\>), then you can use a `PeriodIndex` and/or `Series` of `Periods` to do computations.

<div class="ipython">

python

span = pd.period\_range("1215-01-01", "1381-01-01", freq="D") span

</div>

To convert from an `int64` based YYYYMMDD representation.

<div class="ipython">

python

s = pd.Series(\[20121231, 20141130, 99991231\]) s

  - def conv(x):  
    return pd.Period(year=x // 10000, month=x // 100 % 100, day=x % 100, freq="D")

s.apply(conv) s.apply(conv)\[2\]

</div>

These can easily be converted to a `PeriodIndex`:

<div class="ipython">

python

span = pd.PeriodIndex(s.apply(conv)) span

</div>

## Time zone handling

pandas provides rich support for working with timestamps in different time zones using the `zoneinfo`, `pytz` and `dateutil` libraries or <span class="title-ref">datetime.timezone</span> objects from the standard library.

### Working with time zones

By default, pandas objects are time zone unaware:

<div class="ipython">

python

rng = pd.date\_range("3/6/2012 00:00", periods=15, freq="D") rng.tz is None

</div>

To localize these dates to a time zone (assign a particular time zone to a naive date), you can use the `tz_localize` method or the `tz` keyword argument in <span class="title-ref">date\_range</span>, <span class="title-ref">Timestamp</span>, or <span class="title-ref">DatetimeIndex</span>. You can either pass `zoneinfo`, `pytz` or `dateutil` time zone objects or Olson time zone database strings. Olson time zone strings will return `pytz` time zone objects by default. To return `dateutil` time zone objects, append `dateutil/` before the string.

  - For `zoneinfo`, a list of available timezones are available from :py\`zoneinfo.available\_timezones\`.
  - In `pytz` you can find a list of common (and less common) time zones using `pytz.all_timezones`.
  - `dateutil` uses the OS time zones so there isn't a fixed list available. For common zones, the names are the same as `pytz` and `zoneinfo`.

<div class="ipython">

python

import dateutil

\# pytz rng\_pytz = pd.date\_range("3/6/2012 00:00", periods=3, freq="D", tz="Europe/London") rng\_pytz.tz

\# dateutil rng\_dateutil = pd.date\_range("3/6/2012 00:00", periods=3, freq="D") rng\_dateutil = rng\_dateutil.tz\_localize("dateutil/Europe/London") rng\_dateutil.tz

\# dateutil - utc special case rng\_utc = pd.date\_range( "3/6/2012 00:00", periods=3, freq="D", tz=dateutil.tz.tzutc(), ) rng\_utc.tz

</div>

<div class="ipython">

python

\# datetime.timezone rng\_utc = pd.date\_range( "3/6/2012 00:00", periods=3, freq="D", tz=datetime.timezone.utc, ) rng\_utc.tz

</div>

Note that the `UTC` time zone is a special case in `dateutil` and should be constructed explicitly as an instance of `dateutil.tz.tzutc`. You can also construct other time zones objects explicitly first.

<div class="ipython">

python

import pytz

\# pytz tz\_pytz = pytz.timezone("Europe/London") rng\_pytz = pd.date\_range("3/6/2012 00:00", periods=3, freq="D") rng\_pytz = rng\_pytz.tz\_localize(tz\_pytz) rng\_pytz.tz == tz\_pytz

\# dateutil tz\_dateutil = dateutil.tz.gettz("Europe/London") rng\_dateutil = pd.date\_range("3/6/2012 00:00", periods=3, freq="D", tz=tz\_dateutil) rng\_dateutil.tz == tz\_dateutil

</div>

To convert a time zone aware pandas object from one time zone to another, you can use the `tz_convert` method.

<div class="ipython">

python

rng\_pytz.tz\_convert("US/Eastern")

</div>

\> **Note** \> When using `pytz` time zones, <span class="title-ref">DatetimeIndex</span> will construct a different time zone object than a <span class="title-ref">Timestamp</span> for the same time zone input. A <span class="title-ref">DatetimeIndex</span> can hold a collection of <span class="title-ref">Timestamp</span> objects that may have different UTC offsets and cannot be succinctly represented by one `pytz` time zone instance while one <span class="title-ref">Timestamp</span> represents one point in time with a specific UTC offset.

> 
> 
> <div class="ipython">
> 
> python
> 
> dti = pd.date\_range("2019-01-01", periods=3, freq="D", tz="US/Pacific") dti.tz ts = pd.Timestamp("2019-01-01", tz="US/Pacific") ts.tz
> 
> </div>

\> **Warning** \> Be wary of conversions between libraries. For some time zones, `pytz` and `dateutil` have different definitions of the zone. This is more of a problem for unusual time zones than for 'standard' zones like `US/Eastern`.

<div class="warning">

<div class="title">

Warning

</div>

Be aware that a time zone definition across versions of time zone libraries may not be considered equal. This may cause problems when working with stored data that is localized using one version and operated on with a different version. See \[here\<io.hdf5-notes\>\](\#here\<io.hdf5-notes\>) for how to handle such a situation.

</div>

<div class="warning">

<div class="title">

Warning

</div>

For `pytz` time zones, it is incorrect to pass a time zone object directly into the `datetime.datetime` constructor (e.g., `datetime.datetime(2011, 1, 1, tzinfo=pytz.timezone('US/Eastern'))`. Instead, the datetime needs to be localized using the `localize` method on the `pytz` time zone object.

</div>

<div class="warning">

<div class="title">

Warning

</div>

Be aware that for times in the future, correct conversion between time zones (and UTC) cannot be guaranteed by any time zone library because a timezone's offset from UTC may be changed by the respective government.

</div>

<div class="warning">

<div class="title">

Warning

</div>

If you are using dates beyond 2038-01-18 with `pytz`, due to current deficiencies in the underlying libraries caused by the year 2038 problem, daylight saving time (DST) adjustments to timezone aware dates will not be applied. If and when the underlying libraries are fixed, the DST transitions will be applied.

For example, for two dates that are in British Summer Time (and so would normally be GMT+1), both the following asserts evaluate as true:

<div class="ipython">

python

import pytz

> d\_2037 = "2037-03-31T010101" d\_2038 = "2038-03-31T010101" DST = pytz.timezone("Europe/London") assert pd.Timestamp(d\_2037, tz=DST) \!= pd.Timestamp(d\_2037, tz="GMT") assert pd.Timestamp(d\_2038, tz=DST) == pd.Timestamp(d\_2038, tz="GMT")

</div>

</div>

Under the hood, all timestamps are stored in UTC. Values from a time zone aware <span class="title-ref">DatetimeIndex</span> or <span class="title-ref">Timestamp</span> will have their fields (day, hour, minute, etc.) localized to the time zone. However, timestamps with the same UTC value are still considered to be equal even if they are in different time zones:

<div class="ipython">

python

rng\_eastern = rng\_utc.tz\_convert("US/Eastern") rng\_berlin = rng\_utc.tz\_convert("Europe/Berlin")

rng\_eastern\[2\] rng\_berlin\[2\] rng\_eastern\[2\] == rng\_berlin\[2\]

</div>

Operations between <span class="title-ref">Series</span> in different time zones will yield UTC <span class="title-ref">Series</span>, aligning the data on the UTC timestamps:

<div class="ipython">

python

ts\_utc = pd.Series(range(3), pd.date\_range("20130101", periods=3, tz="UTC")) eastern = ts\_utc.tz\_convert("US/Eastern") berlin = ts\_utc.tz\_convert("Europe/Berlin") result = eastern + berlin result result.index

</div>

To remove time zone information, use `tz_localize(None)` or `tz_convert(None)`. `tz_localize(None)` will remove the time zone yielding the local time representation. `tz_convert(None)` will remove the time zone after converting to UTC time.

<div class="ipython">

python

didx = pd.date\_range(start="2014-08-01 09:00", freq="h", periods=3, tz="US/Eastern") didx didx.tz\_localize(None) didx.tz\_convert(None)

\# tz\_convert(None) is identical to tz\_convert('UTC').tz\_localize(None) didx.tz\_convert("UTC").tz\_localize(None)

</div>

### Fold

For ambiguous times, pandas supports explicitly specifying the keyword-only fold argument. Due to daylight saving time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds to the first (0) or the second time (1) the wall clock hits the ambiguous time. Fold is supported only for constructing from naive `datetime.datetime` (see [datetime documentation](https://docs.python.org/3/library/datetime.html) for details) or from <span class="title-ref">Timestamp</span> or for constructing from components (see below). Only `dateutil` timezones are supported (see [dateutil documentation](https://dateutil.readthedocs.io/en/stable/tz.html#dateutil.tz.enfold) for `dateutil` methods that deal with ambiguous datetimes) as `pytz` timezones do not support fold (see [pytz documentation](http://pytz.sourceforge.net/index.html) for details on how `pytz` deals with ambiguous datetimes). To localize an ambiguous datetime with `pytz`, please use <span class="title-ref">Timestamp.tz\_localize</span>. In general, we recommend to rely on <span class="title-ref">Timestamp.tz\_localize</span> when localizing ambiguous datetimes if you need direct control over how they are handled.

<div class="ipython">

python

  - pd.Timestamp(  
    datetime.datetime(2019, 10, 27, 1, 30, 0, 0), tz="dateutil/Europe/London", fold=0,

) pd.Timestamp( year=2019, month=10, day=27, hour=1, minute=30, tz="dateutil/Europe/London", fold=1, )

</div>

### Ambiguous times when localizing

`tz_localize` may not be able to determine the UTC offset of a timestamp because daylight savings time (DST) in a local time zone causes some times to occur twice within one day ("clocks fall back"). The following options are available:

  - `'raise'`: Raises a `ValueError` (the default behavior)
  - `'infer'`: Attempt to determine the correct offset base on the monotonicity of the timestamps
  - `'NaT'`: Replaces ambiguous times with `NaT`
  - `bool`: `True` represents a DST time, `False` represents non-DST time. An array-like of `bool` values is supported for a sequence of times.

<div class="ipython">

python

  - rng\_hourly = pd.DatetimeIndex(  
    \["11/06/2011 00:00", "11/06/2011 01:00", "11/06/2011 01:00", "11/06/2011 02:00"\]

)

</div>

This will fail as there are ambiguous times (`'11/06/2011 01:00'`)

<div class="ipython" data-okexcept="">

python

rng\_hourly.tz\_localize('US/Eastern')

</div>

Handle these ambiguous times by specifying the following.

<div class="ipython">

python

rng\_hourly.tz\_localize("US/Eastern", ambiguous="infer") rng\_hourly.tz\_localize("US/Eastern", ambiguous="NaT") rng\_hourly.tz\_localize("US/Eastern", ambiguous=\[True, True, False, False\])

</div>

### Nonexistent times when localizing

A DST transition may also shift the local time ahead by 1 hour creating nonexistent local times ("clocks spring forward"). The behavior of localizing a timeseries with nonexistent times can be controlled by the `nonexistent` argument. The following options are available:

  - `'raise'`: Raises a `ValueError` (the default behavior)
  - `'NaT'`: Replaces nonexistent times with `NaT`
  - `'shift_forward'`: Shifts nonexistent times forward to the closest real time
  - `'shift_backward'`: Shifts nonexistent times backward to the closest real time
  - timedelta object: Shifts nonexistent times by the timedelta duration

<div class="ipython">

python

dti = pd.date\_range(start="2015-03-29 02:30:00", periods=3, freq="h") \# 2:30 is a nonexistent time

</div>

Localization of nonexistent times will raise an error by default.

<div class="ipython" data-okexcept="">

python

dti.tz\_localize('Europe/Warsaw')

</div>

Transform nonexistent times to `NaT` or shift the times.

<div class="ipython">

python

dti dti.tz\_localize("Europe/Warsaw", nonexistent="shift\_forward") dti.tz\_localize("Europe/Warsaw", nonexistent="shift\_backward") dti.tz\_localize("Europe/Warsaw", nonexistent=pd.Timedelta(1, unit="h")) dti.tz\_localize("Europe/Warsaw", nonexistent="NaT")

</div>

### Time zone Series operations

A <span class="title-ref">Series</span> with time zone **naive** values is represented with a dtype of `datetime64[ns]`.

<div class="ipython">

python

s\_naive = pd.Series(pd.date\_range("20130101", periods=3)) s\_naive

</div>

A <span class="title-ref">Series</span> with a time zone **aware** values is represented with a dtype of `datetime64[ns, tz]` where `tz` is the time zone

<div class="ipython">

python

s\_aware = pd.Series(pd.date\_range("20130101", periods=3, tz="US/Eastern")) s\_aware

</div>

Both of these <span class="title-ref">Series</span> time zone information can be manipulated via the `.dt` accessor, see \[the dt accessor section \<basics.dt\_accessors\>\](\#the-dt-accessor-section-\<basics.dt\_accessors\>).

For example, to localize and convert a naive stamp to time zone aware.

<div class="ipython">

python

s\_naive.dt.tz\_localize("UTC").dt.tz\_convert("US/Eastern")

</div>

Time zone information can also be manipulated using the `astype` method. This method can convert between different timezone-aware dtypes.

<div class="ipython">

python

\# convert to a new time zone s\_aware.astype("datetime64\[ns, CET\]")

</div>

\> **Note** \> Using <span class="title-ref">Series.to\_numpy</span> on a `Series`, returns a NumPy array of the data. NumPy does not currently support time zones (even though it is *printing* in the local time zone\!), therefore an object array of Timestamps is returned for time zone aware data:

<div class="ipython">

python

s\_naive.to\_numpy() s\_aware.to\_numpy()

</div>

By converting to an object array of Timestamps, it preserves the time zone information. For example, when converting back to a Series:

<div class="ipython">

python

pd.Series(s\_aware.to\_numpy())

</div>

However, if you want an actual NumPy `datetime64[ns]` array (with the values converted to UTC) instead of an array of objects, you can specify the `dtype` argument:

<div class="ipython">

python

s\_aware.to\_numpy(dtype="datetime64\[ns\]")

</div>

---

visualization.md

---

<div id="visualization">

{{ header }}

</div>

# Chart visualization

\> **Note** \> The examples below assume that you're using [Jupyter](https://jupyter.org/).

This section demonstrates visualization through charting. For information on visualization of tabular data please see the section on [Table Visualization](style.ipynb).

We use the standard convention for referencing the matplotlib API:

<div class="ipython">

python

import matplotlib.pyplot as plt

plt.close("all")

</div>

We provide the basics in pandas to easily create decent looking plots. See [the ecosystem page](https://pandas.pydata.org/community/ecosystem.html) for visualization libraries that go beyond the basics documented here.

\> **Note** \> All calls to `np.random` are seeded with 123456.

## Basic plotting: `plot`

We will demonstrate the basics, see the \[cookbook\<cookbook.plotting\>\](\#cookbook\<cookbook.plotting\>) for some advanced strategies.

The `plot` method on Series and DataFrame is just a simple wrapper around \`plt.plot() \<matplotlib.axes.Axes.plot\>\`:

<div class="ipython">

python

np.random.seed(123456)

ts = pd.Series(np.random.randn(1000), index=pd.date\_range("1/1/2000", periods=1000)) ts = ts.cumsum()

@savefig series\_plot\_basic.png ts.plot();

</div>

If the index consists of dates, it calls <span class="title-ref">gcf().autofmt\_xdate() \<matplotlib.figure.Figure.autofmt\_xdate\></span> to try to format the x-axis nicely as per above.

On DataFrame, <span class="title-ref">\~DataFrame.plot</span> is a convenience to plot all of the columns with labels:

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list("ABCD")) df = df.cumsum()

plt.figure(); @savefig frame\_plot\_basic.png df.plot();

</div>

You can plot one column versus another using the `x` and `y` keywords in \`\~DataFrame.plot\`:

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

df3 = pd.DataFrame(np.random.randn(1000, 2), columns=\["B", "C"\]).cumsum() df3\["A"\] = pd.Series(list(range(len(df))))

@savefig df\_plot\_xy.png df3.plot(x="A", y="B");

</div>

\> **Note** \> For more formatting and styling options, see \[formatting \<visualization.formatting\>\](\#formatting-\<visualization.formatting\>) below.

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

## Other plots

Plotting methods allow for a handful of plot styles other than the default line plot. These methods can be provided as the `kind` keyword argument to <span class="title-ref">\~DataFrame.plot</span>, and include:

  - \['bar' \<visualization.barplot\>\](\#'bar'-\<visualization.barplot\>) or \['barh' \<visualization.barplot\>\](\#'barh'-\<visualization.barplot\>) for bar plots
  - \['hist' \<visualization.hist\>\](\#'hist'-\<visualization.hist\>) for histogram
  - \['box' \<visualization.box\>\](\#'box'-\<visualization.box\>) for boxplot
  - \['kde' \<visualization.kde\>\](\#'kde'-\<visualization.kde\>) or \['density' \<visualization.kde\>\](\#'density'-\<visualization.kde\>) for density plots
  - \['area' \<visualization.area\_plot\>\](\#'area'-\<visualization.area\_plot\>) for area plots
  - \['scatter' \<visualization.scatter\>\](\#'scatter'-\<visualization.scatter\>) for scatter plots
  - \['hexbin' \<visualization.hexbin\>\](\#'hexbin'-\<visualization.hexbin\>) for hexagonal bin plots
  - \['pie' \<visualization.pie\>\](\#'pie'-\<visualization.pie\>) for pie plots

For example, a bar plot can be created the following way:

<div class="ipython">

python

plt.figure();

@savefig bar\_plot\_ex.png df.iloc\[5\].plot(kind="bar");

</div>

You can also create these other plots using the methods `DataFrame.plot.<kind>` instead of providing the `kind` keyword argument. This makes it easier to discover plot methods and the specific arguments they use:

<div class="ipython" data-verbatim="">

In \[14\]: df = pd.DataFrame()

In \[15\]: df.plot.\<TAB\> \# noqa: E225, E999 df.plot.area df.plot.barh df.plot.density df.plot.hist df.plot.line df.plot.scatter df.plot.bar df.plot.box df.plot.hexbin df.plot.kde df.plot.pie

</div>

In addition to these `kind` s, there are the \[DataFrame.hist() \<visualization.hist\>\](\#dataframe.hist()-\<visualization.hist\>), and \[DataFrame.boxplot() \<visualization.box\>\](\#dataframe.boxplot()-\<visualization.box\>) methods, which use a separate interface.

Finally, there are several \[plotting functions \<visualization.tools\>\](\#plotting-functions-\<visualization.tools\>) in `pandas.plotting` that take a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> as an argument. These include:

  - \[Scatter Matrix \<visualization.scatter\_matrix\>\](\#scatter-matrix-\<visualization.scatter\_matrix\>)
  - \[Andrews Curves \<visualization.andrews\_curves\>\](\#andrews-curves-\<visualization.andrews\_curves\>)
  - \[Parallel Coordinates \<visualization.parallel\_coordinates\>\](\#parallel-coordinates-\<visualization.parallel\_coordinates\>)
  - \[Lag Plot \<visualization.lag\>\](\#lag-plot-\<visualization.lag\>)
  - \[Autocorrelation Plot \<visualization.autocorrelation\>\](\#autocorrelation-plot-\<visualization.autocorrelation\>)
  - \[Bootstrap Plot \<visualization.bootstrap\>\](\#bootstrap-plot-\<visualization.bootstrap\>)
  - \[RadViz \<visualization.radviz\>\](\#radviz-\<visualization.radviz\>)

Plots may also be adorned with \[errorbars \<visualization.errorbars\>\](\#errorbars-\<visualization.errorbars\>) or \[tables \<visualization.table\>\](\#tables-\<visualization.table\>).

### Bar plots

For labeled, non-time series data, you may wish to produce a bar plot:

<div class="ipython">

python

plt.figure();

@savefig bar\_plot\_ex.png df.iloc\[5\].plot.bar(); plt.axhline(0, color="k");

</div>

Calling a DataFrame's <span class="title-ref">plot.bar() \<DataFrame.plot.bar\></span> method produces a multiple bar plot:

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

df2 = pd.DataFrame(np.random.rand(10, 4), columns=\["a", "b", "c", "d"\])

@savefig bar\_plot\_multi\_ex.png df2.plot.bar();

</div>

To produce a stacked bar plot, pass `stacked=True`:

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure()

</div>

<div class="ipython">

python

@savefig bar\_plot\_stacked\_ex.png df2.plot.bar(stacked=True);

</div>

To get horizontal bar plots, use the `barh` method:

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure()

</div>

<div class="ipython">

python

@savefig barh\_plot\_stacked\_ex.png df2.plot.barh(stacked=True);

</div>

### Histograms

Histograms can be drawn by using the <span class="title-ref">DataFrame.plot.hist</span> and <span class="title-ref">Series.plot.hist</span> methods.

<div class="ipython">

python

  - df4 = pd.DataFrame(
    
      - {  
        "a": np.random.randn(1000) + 1, "b": np.random.randn(1000), "c": np.random.randn(1000) - 1,
    
    }, columns=\["a", "b", "c"\],

)

plt.figure();

@savefig hist\_new.png df4.plot.hist(alpha=0.5);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

A histogram can be stacked using `stacked=True`. Bin size can be changed using the `bins` keyword.

<div class="ipython">

python

plt.figure();

@savefig hist\_new\_stacked.png df4.plot.hist(stacked=True, bins=20);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

You can pass other keywords supported by matplotlib `hist`. For example, horizontal and cumulative histograms can be drawn by `orientation='horizontal'` and `cumulative=True`.

<div class="ipython">

python

plt.figure();

@savefig hist\_new\_kwargs.png df4\["a"\].plot.hist(orientation="horizontal", cumulative=True);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

See the <span class="title-ref">hist \<matplotlib.axes.Axes.hist\></span> method and the [matplotlib hist documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) for more.

The existing interface `DataFrame.hist` to plot histogram still can be used.

<div class="ipython">

python

plt.figure();

@savefig hist\_plot\_ex.png df\["A"\].diff().hist();

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

<span class="title-ref">DataFrame.hist</span> plots the histograms of the columns on multiple subplots:

<div class="ipython">

python

plt.figure();

@savefig frame\_hist\_ex.png df.diff().hist(color="k", alpha=0.5, bins=50);

</div>

The `by` keyword can be specified to plot grouped histograms:

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

data = pd.Series(np.random.randn(1000))

@savefig grouped\_hist.png data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4));

</div>

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

In addition, the `by` keyword can also be specified in <span class="title-ref">DataFrame.plot.hist</span>.

<div class="versionchanged">

1.4.0

</div>

<div class="ipython">

python

  - data = pd.DataFrame(
    
      - {  
        "a": np.random.choice(\["x", "y", "z"\], 1000), "b": np.random.choice(\["e", "f", "g"\], 1000), "c": np.random.randn(1000), "d": np.random.randn(1000) - 1,
    
    },

)

@savefig grouped\_hist\_by.png data.plot.hist(by=\["a", "b"\], figsize=(10, 5));

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Box plots

Boxplot can be drawn calling <span class="title-ref">Series.plot.box</span> and <span class="title-ref">DataFrame.plot.box</span>, or <span class="title-ref">DataFrame.boxplot</span> to visualize the distribution of values within each column.

For instance, here is a boxplot representing five trials of 10 observations of a uniform random variable on \[0,1).

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.rand(10, 5), columns=\["A", "B", "C", "D", "E"\])

@savefig box\_plot\_new.png df.plot.box();

</div>

Boxplot can be colorized by passing `color` keyword. You can pass a `dict` whose keys are `boxes`, `whiskers`, `medians` and `caps`. If some keys are missing in the `dict`, default colors are used for the corresponding artists. Also, boxplot has `sym` keyword to specify fliers style.

When you pass other type of arguments via `color` keyword, it will be directly passed to matplotlib for all the `boxes`, `whiskers`, `medians` and `caps` colorization.

The colors are applied to every boxes to be drawn. If you want more complicated colorization, you can get each drawn artists by passing \[return\_type \<visualization.box.return\>\](\#return\_type-\<visualization.box.return\>).

<div class="ipython">

python

  - color = {  
    "boxes": "DarkGreen", "whiskers": "DarkOrange", "medians": "DarkBlue", "caps": "Gray",

}

@savefig box\_new\_colorize.png df.plot.box(color=color, sym="r+");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Also, you can pass other keywords supported by matplotlib `boxplot`. For example, horizontal and custom-positioned boxplot can be drawn by `vert=False` and `positions` keywords.

<div class="ipython">

python

@savefig box\_new\_kwargs.png df.plot.box(vert=False, positions=\[1, 4, 5, 6, 8\]);

</div>

See the <span class="title-ref">boxplot \<matplotlib.axes.Axes.boxplot\></span> method and the [matplotlib boxplot documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html) for more.

The existing interface `DataFrame.boxplot` to plot boxplot still can be used.

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame(np.random.rand(10, 5)) plt.figure();

@savefig box\_plot\_ex.png bp = df.boxplot()

</div>

You can create a stratified boxplot using the `by` keyword argument to create groupings. For instance,

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame(np.random.rand(10, 2), columns=\["Col1", "Col2"\]) df\["X"\] = pd.Series(\["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"\])

plt.figure();

@savefig box\_plot\_ex2.png bp = df.boxplot(by="X")

</div>

You can also pass a subset of columns to plot, as well as group by multiple columns:

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame(np.random.rand(10, 3), columns=\["Col1", "Col2", "Col3"\]) df\["X"\] = pd.Series(\["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"\]) df\["Y"\] = pd.Series(\["A", "B", "A", "B", "A", "B", "A", "B", "A", "B"\])

plt.figure();

@savefig box\_plot\_ex3.png bp = df.boxplot(column=\["Col1", "Col2"\], by=\["X", "Y"\])

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

You could also create groupings with <span class="title-ref">DataFrame.plot.box</span>, for instance:

<div class="versionchanged">

1.4.0

</div>

<div class="ipython" data-suppress="">

python

plt.close("all") np.random.seed(123456)

</div>

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame(np.random.rand(10, 3), columns=\["Col1", "Col2", "Col3"\]) df\["X"\] = pd.Series(\["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"\])

plt.figure();

@savefig box\_plot\_ex4.png bp = df.plot.box(column=\["Col1", "Col2"\], by="X")

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

<div id="visualization.box.return">

In `boxplot`, the return type can be controlled by the `return_type`, keyword. The valid choices are `{"axes", "dict", "both", None}`. Faceting, created by `DataFrame.boxplot` with the `by` keyword, will affect the output type as well:

</div>

| `return_type` | Faceted | Output type                |
| ------------- | ------- | -------------------------- |
| `None`        | No      | axes                       |
| `None`        | Yes     | 2-D ndarray of axes        |
| `'axes'`      | No      | axes                       |
| `'axes'`      | Yes     | Series of axes             |
| `'dict'`      | No      | dict of artists            |
| `'dict'`      | Yes     | Series of dicts of artists |
| `'both'`      | No      | namedtuple                 |
| `'both'`      | Yes     | Series of namedtuples      |

`Groupby.boxplot` always returns a `Series` of `return_type`.

<div class="ipython" data-okwarning="">

python

np.random.seed(1234) df\_box = pd.DataFrame(np.random.randn(50, 2)) df\_box\["g"\] = np.random.choice(\["A", "B"\], size=50) df\_box.loc\[df\_box\["g"\] == "B", 1\] += 3

@savefig boxplot\_groupby.png bp = df\_box.boxplot(by="g")

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

The subplots above are split by the numeric columns first, then the value of the `g` column. Below the subplots are first split by the value of `g`, then by the numeric columns.

<div class="ipython" data-okwarning="">

python

@savefig groupby\_boxplot\_vis.png bp = df\_box.groupby("g").boxplot()

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Area plot

You can create area plots with <span class="title-ref">Series.plot.area</span> and <span class="title-ref">DataFrame.plot.area</span>. Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values.

When input data contains `NaN`, it will be automatically filled by 0. If you want to drop or fill by different values, use <span class="title-ref">dataframe.dropna</span> or <span class="title-ref">dataframe.fillna</span> before calling `plot`.

<div class="ipython" data-suppress="">

python

np.random.seed(123456) plt.figure()

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.rand(10, 4), columns=\["a", "b", "c", "d"\])

@savefig area\_plot\_stacked.png df.plot.area();

</div>

To produce an unstacked plot, pass `stacked=False`. Alpha value is set to 0.5 unless otherwise specified:

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure()

</div>

<div class="ipython">

python

@savefig area\_plot\_unstacked.png df.plot.area(stacked=False);

</div>

### Scatter plot

Scatter plot can be drawn by using the <span class="title-ref">DataFrame.plot.scatter</span> method. Scatter plot requires numeric columns for the x and y axes. These can be specified by the `x` and `y` keywords.

<div class="ipython" data-suppress="">

python

np.random.seed(123456) plt.close("all") plt.figure()

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.rand(50, 4), columns=\["a", "b", "c", "d"\]) df\["species"\] = pd.Categorical( \["setosa"\] \* 20 + \["versicolor"\] \* 20 + \["virginica"\] \* 10 )

@savefig scatter\_plot.png df.plot.scatter(x="a", y="b");

</div>

To plot multiple column groups in a single axes, repeat `plot` method specifying target `ax`. It is recommended to specify `color` and `label` keywords to distinguish each groups.

<div class="ipython" data-okwarning="">

python

ax = df.plot.scatter(x="a", y="b", color="DarkBlue", label="Group 1") @savefig scatter\_plot\_repeated.png df.plot.scatter(x="c", y="d", color="DarkGreen", label="Group 2", ax=ax);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

The keyword `c` may be given as the name of a column to provide colors for each point:

<div class="ipython">

python

@savefig scatter\_plot\_colored.png df.plot.scatter(x="a", y="b", c="c", s=50);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

If a categorical column is passed to `c`, then a discrete colorbar will be produced:

<div class="versionadded">

1.3.0

</div>

<div class="ipython">

python

@savefig scatter\_plot\_categorical.png df.plot.scatter(x="a", y="b", c="species", cmap="viridis", s=50);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

You can pass other keywords supported by matplotlib <span class="title-ref">scatter \<matplotlib.axes.Axes.scatter\></span>. The example below shows a bubble chart using a column of the `DataFrame` as the bubble size.

<div class="ipython">

python

@savefig scatter\_plot\_bubble.png df.plot.scatter(x="a", y="b", s=df\["c"\] \* 200);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

See the <span class="title-ref">scatter \<matplotlib.axes.Axes.scatter\></span> method and the [matplotlib scatter documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) for more.

### Hexagonal bin plot

You can create hexagonal bin plots with <span class="title-ref">DataFrame.plot.hexbin</span>. Hexbin plots can be a useful alternative to scatter plots if your data are too dense to plot each point individually.

<div class="ipython" data-suppress="">

python

plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(1000, 2), columns=\["a", "b"\]) df\["b"\] = df\["b"\] + np.arange(1000)

@savefig hexbin\_plot.png df.plot.hexbin(x="a", y="b", gridsize=25);

</div>

A useful keyword argument is `gridsize`; it controls the number of hexagons in the x-direction, and defaults to 100. A larger `gridsize` means more, smaller bins.

By default, a histogram of the counts around each `(x, y)` point is computed. You can specify alternative aggregations by passing values to the `C` and `reduce_C_function` arguments. `C` specifies the value at each `(x, y)` point and `reduce_C_function` is a function of one argument that reduces all the values in a bin to a single number (e.g. `mean`, `max`, `sum`, `std`). In this example the positions are given by columns `a` and `b`, while the value is given by column `z`. The bins are aggregated with NumPy's `max` function.

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(1000, 2), columns=\["a", "b"\]) df\["b"\] = df\["b"\] + np.arange(1000) df\["z"\] = np.random.uniform(0, 3, 1000)

@savefig hexbin\_plot\_agg.png df.plot.hexbin(x="a", y="b", C="z", reduce\_C\_function=np.max, gridsize=25);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

See the <span class="title-ref">hexbin \<matplotlib.axes.Axes.hexbin\></span> method and the [matplotlib hexbin documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hexbin.html) for more.

### Pie plot

You can create a pie plot with <span class="title-ref">DataFrame.plot.pie</span> or <span class="title-ref">Series.plot.pie</span>. If your data includes any `NaN`, they will be automatically filled with 0. A `ValueError` will be raised if there are any negative values in your data.

<div class="ipython" data-suppress="">

python

np.random.seed(123456) plt.figure()

</div>

<div class="ipython" data-okwarning="">

python

series = pd.Series(3 \* np.random.rand(4), index=\["a", "b", "c", "d"\], name="series")

@savefig series\_pie\_plot.png series.plot.pie(figsize=(6, 6));

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

For pie plots it's best to use square figures, i.e. a figure aspect ratio 1. You can create the figure with equal width and height, or force the aspect ratio to be equal after plotting by calling `ax.set_aspect('equal')` on the returned `axes` object.

Note that pie plot with <span class="title-ref">DataFrame</span> requires that you either specify a target column by the `y` argument or `subplots=True`. When `y` is specified, pie plot of selected column will be drawn. If `subplots=True` is specified, pie plots for each column are drawn as subplots. A legend will be drawn in each pie plots by default; specify `legend=False` to hide it.

<div class="ipython" data-suppress="">

python

np.random.seed(123456) plt.figure()

</div>

<div class="ipython">

python

  - df = pd.DataFrame(  
    3 \* np.random.rand(4, 2), index=\["a", "b", "c", "d"\], columns=\["x", "y"\]

)

@savefig df\_pie\_plot.png df.plot.pie(subplots=True, figsize=(8, 4));

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

You can use the `labels` and `colors` keywords to specify the labels and colors of each wedge.

\> **Warning** \> Most pandas plots use the `label` and `color` arguments (note the lack of "s" on those). To be consistent with <span class="title-ref">matplotlib.pyplot.pie</span> you must use `labels` and `colors`.

If you want to hide wedge labels, specify `labels=None`. If `fontsize` is specified, the value will be applied to wedge labels. Also, other keywords supported by <span class="title-ref">matplotlib.pyplot.pie</span> can be used.

<div class="ipython" data-suppress="">

python

plt.figure()

</div>

<div class="ipython">

python

@savefig series\_pie\_plot\_options.png series.plot.pie( labels=\["AA", "BB", "CC", "DD"\], colors=\["r", "g", "b", "c"\], autopct="%.2f", fontsize=20, figsize=(6, 6), );

</div>

If you pass values whose sum total is less than 1.0 they will be rescaled so that they sum to 1.

<div class="ipython" data-suppress="">

python

plt.close("all") plt.figure()

</div>

<div class="ipython" data-okwarning="">

python

series = pd.Series(\[0.1\] \* 4, index=\["a", "b", "c", "d"\], name="series2")

@savefig series\_pie\_plot\_semi.png series.plot.pie(figsize=(6, 6));

</div>

See the [matplotlib pie documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html) for more.

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

## Plotting with missing data

pandas tries to be pragmatic about plotting `DataFrames` or `Series` that contain missing data. Missing values are dropped, left out, or filled depending on the plot type.

| Plot Type      | NaN Handling            |
| -------------- | ----------------------- |
| Line           | Leave gaps at NaNs      |
| Line (stacked) | Fill 0's                |
| Bar            | Fill 0's                |
| Scatter        | Drop NaNs               |
| Histogram      | Drop NaNs (column-wise) |
| Box            | Drop NaNs (column-wise) |
| Area           | Fill 0's                |
| KDE            | Drop NaNs (column-wise) |
| Hexbin         | Drop NaNs               |
| Pie            | Fill 0's                |

If any of these defaults are not what you want, or if you want to be explicit about how missing values are handled, consider using <span class="title-ref">\~pandas.DataFrame.fillna</span> or <span class="title-ref">\~pandas.DataFrame.dropna</span> before plotting.

## Plotting tools

These functions can be imported from `pandas.plotting` and take a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> as an argument.

### Scatter matrix plot

You can create a scatter plot matrix using the `scatter_matrix` method in `pandas.plotting`:

<div class="ipython" data-suppress="">

python

np.random.seed(123456)

</div>

<div class="ipython">

python

from pandas.plotting import scatter\_matrix

df = pd.DataFrame(np.random.randn(1000, 4), columns=\["a", "b", "c", "d"\])

@savefig scatter\_matrix\_kde.png scatter\_matrix(df, alpha=0.2, figsize=(6, 6), diagonal="kde");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Density plot

You can create density plots using the <span class="title-ref">Series.plot.kde</span> and <span class="title-ref">DataFrame.plot.kde</span> methods.

<div class="ipython" data-suppress="">

python

plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

ser = pd.Series(np.random.randn(1000))

@savefig kde\_plot.png ser.plot.kde();

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Andrews curves

Andrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series, see the [Wikipedia entry](https://en.wikipedia.org/wiki/Andrews_plot) for more information. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures.

**Note**: The "Iris" dataset is available [here](https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv).

<div class="ipython">

python

from pandas.plotting import andrews\_curves

data = pd.read\_csv("data/iris.data")

plt.figure();

@savefig andrews\_curves.png andrews\_curves(data, "Name");

</div>

### Parallel coordinates

Parallel coordinates is a plotting technique for plotting multivariate data, see the [Wikipedia entry](https://en.wikipedia.org/wiki/Parallel_coordinates) for an introduction. Parallel coordinates allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together.

<div class="ipython">

python

from pandas.plotting import parallel\_coordinates

data = pd.read\_csv("data/iris.data")

plt.figure();

@savefig parallel\_coordinates.png parallel\_coordinates(data, "Name");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Lag plot

Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The `lag` argument may be passed, and when `lag=1` the plot is essentially `data[:-1]` vs. `data[1:]`.

<div class="ipython" data-suppress="">

python

np.random.seed(123456)

</div>

<div class="ipython">

python

from pandas.plotting import lag\_plot

plt.figure();

spacing = np.linspace(-99 \* np.pi, 99 \* np.pi, num=1000) data = pd.Series(0.1 \* np.random.rand(1000) + 0.9 \* np.sin(spacing))

@savefig lag\_plot.png lag\_plot(data);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Autocorrelation plot

Autocorrelation plots are often used for checking randomness in time series. This is done by computing autocorrelations for data values at varying time lags. If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero. The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. See the [Wikipedia entry](https://en.wikipedia.org/wiki/Correlogram) for more about autocorrelation plots.

<div class="ipython" data-suppress="">

python

np.random.seed(123456)

</div>

<div class="ipython">

python

from pandas.plotting import autocorrelation\_plot

plt.figure();

spacing = np.linspace(-9 \* np.pi, 9 \* np.pi, num=1000) data = pd.Series(0.7 \* np.random.rand(1000) + 0.3 \* np.sin(spacing))

@savefig autocorrelation\_plot.png autocorrelation\_plot(data);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Bootstrap plot

Bootstrap plots are used to visually assess the uncertainty of a statistic, such as mean, median, midrange, etc. A random subset of a specified size is selected from a data set, the statistic in question is computed for this subset and the process is repeated a specified number of times. Resulting plots and histograms are what constitutes the bootstrap plot.

<div class="ipython" data-suppress="">

python

np.random.seed(123456)

</div>

<div class="ipython">

python

from pandas.plotting import bootstrap\_plot

data = pd.Series(np.random.rand(1000))

@savefig bootstrap\_plot.png bootstrap\_plot(data, size=50, samples=500, color="grey");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### RadViz

RadViz is a way of visualizing multi-variate data. It is based on a simple spring tension minimization algorithm. Basically you set up a bunch of points in a plane. In our case they are equally spaced on a unit circle. Each point represents a single attribute. You then pretend that each sample in the data set is attached to each of these points by a spring, the stiffness of which is proportional to the numerical value of that attribute (they are normalized to unit interval). The point in the plane, where our sample settles to (where the forces acting on our sample are at an equilibrium) is where a dot representing our sample will be drawn. Depending on which class that sample belongs it will be colored differently. See the R package [Radviz](https://cran.r-project.org/web/packages/Radviz/index.html) for more information.

**Note**: The "Iris" dataset is available [here](https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv).

<div class="ipython">

python

from pandas.plotting import radviz

data = pd.read\_csv("data/iris.data")

plt.figure();

@savefig radviz.png radviz(data, "Name");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

## Plot formatting

### Setting the plot style

From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the style can be used to easily give plots the general look that you want. Setting the style is as easy as calling `matplotlib.style.use(my_plot_style)` before creating your plot. For example you could write `matplotlib.style.use('ggplot')` for ggplot-style plots.

You can see the various available style names at `matplotlib.style.available` and it's very easy to try them out.

### General plot style arguments

Most plotting methods have a set of keyword arguments that control the layout and formatting of the returned plot:

<div class="ipython">

python

plt.figure(); @savefig series\_plot\_basic2.png ts.plot(style="k--", label="Series");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

For each kind of plot (e.g. `line`, `bar`, `scatter`) any additional arguments keywords are passed along to the corresponding matplotlib function (<span class="title-ref">ax.plot() \<matplotlib.axes.Axes.plot\></span>, <span class="title-ref">ax.bar() \<matplotlib.axes.Axes.bar\></span>, <span class="title-ref">ax.scatter() \<matplotlib.axes.Axes.scatter\></span>). These can be used to control additional styling, beyond what pandas provides.

### Controlling the legend

You may set the `legend` argument to `False` to hide the legend, which is shown by default.

<div class="ipython" data-suppress="">

python

np.random.seed(123456)

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list("ABCD")) df = df.cumsum()

@savefig frame\_plot\_basic\_noleg.png df.plot(legend=False);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Controlling the labels

You may set the `xlabel` and `ylabel` arguments to give the plot custom labels for x and y axis. By default, pandas will pick up index name as xlabel, while leaving it empty for ylabel.

<div class="ipython" data-suppress="">

python

plt.figure();

</div>

<div class="ipython">

python

df.plot();

@savefig plot\_xlabel\_ylabel.png df.plot(xlabel="new x", ylabel="new y");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Scales

You may pass `logy` to get a log-scale Y axis.

<div class="ipython" data-suppress="">

python

plt.figure() np.random.seed(123456)

</div>

<div class="ipython">

python

ts = pd.Series(np.random.randn(1000), index=pd.date\_range("1/1/2000", periods=1000)) ts = np.exp(ts.cumsum())

@savefig series\_plot\_logy.png ts.plot(logy=True);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

See also the `logx` and `loglog` keyword arguments.

### Plotting on a secondary y-axis

To plot data on a secondary y-axis, use the `secondary_y` keyword:

<div class="ipython" data-suppress="">

python

plt.figure()

</div>

<div class="ipython">

python

df\["A"\].plot();

@savefig series\_plot\_secondary\_y.png df\["B"\].plot(secondary\_y=True, style="g");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

To plot some columns in a `DataFrame`, give the column names to the `secondary_y` keyword:

<div class="ipython">

python

plt.figure(); ax = df.plot(secondary\_y=\["A", "B"\]) ax.set\_ylabel("CD scale"); @savefig frame\_plot\_secondary\_y.png ax.right\_ax.set\_ylabel("AB scale");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Note that the columns plotted on the secondary y-axis is automatically marked with "(right)" in the legend. To turn off the automatic marking, use the `mark_right=False` keyword:

<div class="ipython">

python

plt.figure();

@savefig frame\_plot\_secondary\_y\_no\_right.png df.plot(secondary\_y=\["A", "B"\], mark\_right=False);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Custom formatters for timeseries plots

pandas provides custom formatters for timeseries plots. These change the formatting of the axis labels for dates and times. By default, the custom formatters are applied only to plots created by pandas with <span class="title-ref">DataFrame.plot</span> or <span class="title-ref">Series.plot</span>. To have them apply to all plots, including those made by matplotlib, set the option `pd.options.plotting.matplotlib.register_converters = True` or use <span class="title-ref">pandas.plotting.register\_matplotlib\_converters</span>.

### Suppressing tick resolution adjustment

pandas includes automatic tick resolution adjustment for regular frequency time-series data. For limited cases where pandas cannot infer the frequency information (e.g., in an externally created `twinx`), you can choose to suppress this behavior for alignment purposes.

Here is the default behavior, notice how the x-axis tick labeling is performed:

<div class="ipython">

python

plt.figure();

@savefig ser\_plot\_suppress.png df\["A"\].plot();

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Using the `x_compat` parameter, you can suppress this behavior:

<div class="ipython">

python

plt.figure();

@savefig ser\_plot\_suppress\_parm.png df\["A"\].plot(x\_compat=True);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

If you have more than one plot that needs to be suppressed, the `use` method in `pandas.plotting.plot_params` can be used in a `with` statement:

<div class="ipython">

python

plt.figure();

@savefig ser\_plot\_suppress\_context.png with pd.plotting.plot\_params.use("x\_compat", True): df\["A"\].plot(color="r") df\["B"\].plot(color="g") df\["C"\].plot(color="b")

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Automatic date tick adjustment

`TimedeltaIndex` now uses the native matplotlib tick locator methods, it is useful to call the automatic date tick adjustment from matplotlib for figures whose ticklabels overlap.

See the <span class="title-ref">autofmt\_xdate \<matplotlib.figure.autofmt\_xdate\></span> method and the [matplotlib documentation](https://matplotlib.org/2.0.2/users/recipes.html#fixing-common-date-annoyances) for more.

### Subplots

Each `Series` in a `DataFrame` can be plotted on a different axis with the `subplots` keyword:

<div class="ipython">

python

@savefig frame\_plot\_subplots.png df.plot(subplots=True, figsize=(6, 6));

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Using layout and targeting multiple axes

The layout of subplots can be specified by the `layout` keyword. It can accept `(rows, columns)`. The `layout` keyword can be used in `hist` and `boxplot` also. If the input is invalid, a `ValueError` will be raised.

The number of axes which can be contained by rows x columns specified by `layout` must be larger than the number of required subplots. If layout can contain more axes than required, blank axes are not drawn. Similar to a NumPy array's `reshape` method, you can use `-1` for one dimension to automatically calculate the number of rows or columns needed, given the other.

<div class="ipython">

python

@savefig frame\_plot\_subplots\_layout.png df.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

The above example is identical to using:

<div class="ipython">

python

df.plot(subplots=True, layout=(2, -1), figsize=(6, 6), sharex=False);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

The required number of columns (3) is inferred from the number of series to plot and the given number of rows (2).

You can pass multiple axes created beforehand as list-like via `ax` keyword. This allows more complicated layouts. The passed axes must be the same number as the subplots being drawn.

When multiple axes are passed via the `ax` keyword, `layout`, `sharex` and `sharey` keywords don't affect to the output. You should explicitly pass `sharex=False` and `sharey=False`, otherwise you will see a warning.

<div class="ipython">

python

fig, axes = plt.subplots(4, 4, figsize=(9, 9)) plt.subplots\_adjust(wspace=0.5, hspace=0.5) target1 = \[axes\[0\]\[0\], axes\[1\]\[1\], axes\[2\]\[2\], axes\[3\]\[3\]\] target2 = \[axes\[3\]\[0\], axes\[2\]\[1\], axes\[1\]\[2\], axes\[0\]\[3\]\]

df.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False); @savefig frame\_plot\_subplots\_multi\_ax.png (-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Another option is passing an `ax` argument to <span class="title-ref">Series.plot</span> to plot on a particular axis:

<div class="ipython">

python

np.random.seed(123456) ts = pd.Series(np.random.randn(1000), index=pd.date\_range("1/1/2000", periods=1000)) ts = ts.cumsum()

df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list("ABCD")) df = df.cumsum()

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

<div class="ipython">

python

fig, axes = plt.subplots(nrows=2, ncols=2) plt.subplots\_adjust(wspace=0.2, hspace=0.5) df\["A"\].plot(ax=axes\[0, 0\]); axes\[0, 0\].set\_title("A"); df\["B"\].plot(ax=axes\[0, 1\]); axes\[0, 1\].set\_title("B"); df\["C"\].plot(ax=axes\[1, 0\]); axes\[1, 0\].set\_title("C"); df\["D"\].plot(ax=axes\[1, 1\]); @savefig series\_plot\_multi.png axes\[1, 1\].set\_title("D");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Plotting with error bars

Plotting with error bars is supported in <span class="title-ref">DataFrame.plot</span> and <span class="title-ref">Series.plot</span>.

Horizontal and vertical error bars can be supplied to the `xerr` and `yerr` keyword arguments to <span class="title-ref">\~DataFrame.plot</span>. The error values can be specified using a variety of formats:

  - As a <span class="title-ref">DataFrame</span> or `dict` of errors with column names matching the `columns` attribute of the plotting <span class="title-ref">DataFrame</span> or matching the `name` attribute of the <span class="title-ref">Series</span>.
  - As a `str` indicating which of the columns of plotting <span class="title-ref">DataFrame</span> contain the error values.
  - As raw values (`list`, `tuple`, or `np.ndarray`). Must be the same length as the plotting <span class="title-ref">DataFrame</span>/<span class="title-ref">Series</span>.

Here is an example of one way to easily plot group means with standard deviations from the raw data.

<div class="ipython">

python

\# Generate the data ix3 = pd.MultiIndex.from\_arrays( \[ \["a", "a", "a", "a", "a", "b", "b", "b", "b", "b"\], \["foo", "foo", "foo", "bar", "bar", "foo", "foo", "bar", "bar", "bar"\], \], names=\["letter", "word"\], )

  - df3 = pd.DataFrame(
    
      - {  
        "data1": \[9, 3, 2, 4, 3, 2, 4, 6, 3, 2\], "data2": \[9, 6, 5, 7, 5, 4, 5, 6, 5, 1\],
    
    }, index=ix3,

)

\# Group by index labels and take the means and standard deviations \# for each group gp3 = df3.groupby(level=("letter", "word")) means = gp3.mean() errors = gp3.std() means errors

\# Plot fig, ax = plt.subplots() @savefig errorbar\_example.png means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Asymmetrical error bars are also supported, however raw error values must be provided in this case. For a `N` length <span class="title-ref">Series</span>, a `2xN` array should be provided indicating lower and upper (or left and right) errors. For a `MxN` <span class="title-ref">DataFrame</span>, asymmetrical errors should be in a `Mx2xN` array.

Here is an example of one way to plot the min/max range using asymmetrical error bars.

<div class="ipython">

python

mins = gp3.min() maxs = gp3.max()

\# errors should be positive, and defined in the order of lower, upper errors = \[\[means\[c\] - mins\[c\], maxs\[c\] - means\[c\]\] for c in df3.columns\]

\# Plot fig, ax = plt.subplots() @savefig errorbar\_asymmetrical\_example.png means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

### Plotting tables

Plotting with matplotlib table is now supported in <span class="title-ref">DataFrame.plot</span> and <span class="title-ref">Series.plot</span> with a `table` keyword. The `table` keyword can accept `bool`, <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span>. The simple way to draw a table is to specify `table=True`. Data will be transposed to meet matplotlib's default layout.

<div class="ipython">

python

np.random.seed(123456) fig, ax = plt.subplots(1, 1, figsize=(7, 6.5)) df = pd.DataFrame(np.random.rand(5, 3), columns=\["a", "b", "c"\]) ax.xaxis.tick\_top() \# Display x-axis ticks on top.

@savefig line\_plot\_table\_true.png df.plot(table=True, ax=ax);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Also, you can pass a different <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> to the `table` keyword. The data will be drawn as displayed in print method (not transposed automatically). If required, it should be transposed manually as seen in the example below.

<div class="ipython">

python

fig, ax = plt.subplots(1, 1, figsize=(7, 6.75)) ax.xaxis.tick\_top() \# Display x-axis ticks on top.

@savefig line\_plot\_table\_data.png df.plot(table=np.round(df.T, 2), ax=ax);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

There also exists a helper function `pandas.plotting.table`, which creates a table from <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span>, and adds it to an `matplotlib.Axes` instance. This function can accept keywords which the matplotlib [table](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.table.html) has.

<div class="ipython">

python

from pandas.plotting import table

fig, ax = plt.subplots(1, 1)

table(ax, np.round(df.describe(), 2), loc="upper right", colWidths=\[0.2, 0.2, 0.2\]);

@savefig line\_plot\_table\_describe.png df.plot(ax=ax, ylim=(0, 2), legend=None);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

**Note**: You can get table instances on the axes using `axes.tables` property for further decorations. See the [matplotlib table documentation](https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.table) for more.

### Colormaps

A potential issue when plotting a large number of columns is that it can be difficult to distinguish some series due to repetition in the default colors. To remedy this, `DataFrame` plotting supports the use of the `colormap` argument, which accepts either a Matplotlib [colormap](https://matplotlib.org/api/cm_api.html) or a string that is a name of a colormap registered with Matplotlib. A visualization of the default matplotlib colormaps is available [here](https://matplotlib.org/stable/gallery/color/colormap_reference.html).

As matplotlib does not directly support colormaps for line-based plots, the colors are selected based on an even spacing determined by the number of columns in the `DataFrame`. There is no consideration made for background color, so some colormaps will produce lines that are not easily visible.

To use the cubehelix colormap, we can pass `colormap='cubehelix'`.

<div class="ipython">

python

np.random.seed(123456) df = pd.DataFrame(np.random.randn(1000, 10), index=ts.index) df = df.cumsum()

plt.figure();

@savefig cubehelix.png df.plot(colormap="cubehelix");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Alternatively, we can pass the colormap itself:

<div class="ipython">

python

from matplotlib import cm

plt.figure();

@savefig cubehelix\_cm.png df.plot(colormap=cm.cubehelix);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Colormaps can also be used other plot types, like bar charts:

<div class="ipython">

python

np.random.seed(123456) dd = pd.DataFrame(np.random.randn(10, 10)).map(abs) dd = dd.cumsum()

plt.figure();

@savefig greens.png dd.plot.bar(colormap="Greens");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Parallel coordinates charts:

<div class="ipython">

python

plt.figure();

@savefig parallel\_gist\_rainbow.png parallel\_coordinates(data, "Name", colormap="gist\_rainbow");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

Andrews curves charts:

<div class="ipython">

python

plt.figure();

@savefig andrews\_curve\_winter.png andrews\_curves(data, "Name", colormap="winter");

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

## Plotting directly with Matplotlib

In some situations it may still be preferable or necessary to prepare plots directly with matplotlib, for instance when a certain type of plot or customization is not (yet) supported by pandas. `Series` and `DataFrame` objects behave like arrays and can therefore be passed directly to matplotlib functions without explicit casts.

pandas also automatically registers formatters and locators that recognize date indices, thereby extending date and time support to practically all plot types available in matplotlib. Although this formatting does not provide the same level of refinement you would get when plotting via pandas, it can be faster when plotting a large number of points.

<div class="ipython">

python

np.random.seed(123456) price = pd.Series( np.random.randn(150).cumsum(), index=pd.date\_range("2000-1-1", periods=150, freq="B"), ) ma = price.rolling(20).mean() mstd = price.rolling(20).std()

plt.figure();

plt.plot(price.index, price, "k"); plt.plot(ma.index, ma, "b"); @savefig bollinger.png plt.fill\_between(mstd.index, ma - 2 \* mstd, ma + 2 \* mstd, color="b", alpha=0.2);

</div>

<div class="ipython" data-suppress="">

python

plt.close("all")

</div>

## Plotting backends

pandas can be extended with third-party plotting backends. The main idea is letting users select a plotting backend different than the provided one based on Matplotlib.

This can be done by passing 'backend.module' as the argument `backend` in `plot` function. For example:

`` `python     >>> Series([1, 2, 3]).plot(backend="backend.module")  Alternatively, you can also set this option globally, do you don't need to specify ``<span class="title-ref"> the keyword in each </span><span class="title-ref">plot</span>\` call. For example:

`` `python     >>> pd.set_option("plotting.backend", "backend.module")     >>> pd.Series([1, 2, 3]).plot()  Or:  .. code-block:: python      >>> pd.options.plotting.backend = "backend.module"     >>> pd.Series([1, 2, 3]).plot()  This would be more or less equivalent to:  .. code-block:: python      >>> import backend.module     >>> backend.module.plot(pd.Series([1, 2, 3]))  The backend module can then use other visualization tools (Bokeh, Altair, hvplot,...) ``<span class="title-ref"> to generate the plots. Some libraries implementing a backend for pandas are listed on \`the ecosystem page \<https://pandas.pydata.org/community/ecosystem.html\></span>\_.

Developers guide can be found at <https://pandas.pydata.org/docs/dev/development/extending.html#plotting-backends>

---

window.md

---

<div id="window">

{{ header }}

</div>

# Windowing operations

pandas contains a compact set of APIs for performing windowing operations - an operation that performs an aggregation over a sliding partition of values. The API functions similarly to the `groupby` API in that <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> call the windowing method with necessary parameters and then subsequently call the aggregation function.

<div class="ipython">

python

s = pd.Series(range(5)) s.rolling(window=2).sum()

</div>

The windows are comprised by looking back the length of the window from the current observation. The result above can be derived by taking the sum of the following windowed partitions of data:

<div class="ipython">

python

  - for window in s.rolling(window=2):  
    print(window)

</div>

## Overview

pandas supports 4 types of windowing operations:

1.  Rolling window: Generic fixed or variable sliding window over the values.
2.  Weighted window: Weighted, non-rectangular window supplied by the `scipy.signal` library.
3.  Expanding window: Accumulating window over the values.
4.  Exponentially Weighted window: Accumulating and exponentially weighted window over the values.

| Concept                       | Method      | Returned Object                             | Supports time-based windows | Supports chained groupby | Supports table method   | Supports online operations |
| ----------------------------- | ----------- | ------------------------------------------- | --------------------------- | ------------------------ | ----------------------- | -------------------------- |
| Rolling window                | `rolling`   | `pandas.typing.api.Rolling`                 | Yes                         | Yes                      | Yes (as of version 1.3) | No                         |
| Weighted window               | `rolling`   | `pandas.typing.api.Window`                  | No                          | No                       | No                      | No                         |
| Expanding window              | `expanding` | `pandas.typing.api.Expanding`               | No                          | Yes                      | Yes (as of version 1.3) | No                         |
| Exponentially Weighted window | `ewm`       | `pandas.typing.api.ExponentialMovingWindow` | No                          | Yes (as of version 1.2)  | No                      | Yes (as of version 1.3)    |

As noted above, some operations support specifying a window based on a time offset:

<div class="ipython">

python

s = pd.Series(range(5), index=pd.date\_range('2020-01-01', periods=5, freq='1D')) s.rolling(window='2D').sum()

</div>

Additionally, some methods support chaining a `groupby` operation with a windowing operation which will first group the data by the specified keys and then perform a windowing operation per group.

<div class="ipython">

python

df = pd.DataFrame({'A': \['a', 'b', 'a', 'b', 'a'\], 'B': range(5)}) df.groupby('A').expanding().sum()

</div>

\> **Note** \> Windowing operations currently only support numeric data (integer and float) and will always return `float64` values.

\> **Warning** \> Some windowing aggregation, `mean`, `sum`, `var` and `std` methods may suffer from numerical imprecision due to the underlying windowing algorithms accumulating sums. When values differ with magnitude \(1/np.finfo(np.double).eps\) this results in truncation. It must be noted, that large values may have an impact on windows, which do not include these values. [Kahan summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm) is used to compute the rolling sums to preserve accuracy as much as possible.

<div class="versionadded">

1.3.0

</div>

Some windowing operations also support the `method='table'` option in the constructor which performs the windowing operation over an entire <span class="title-ref">DataFrame</span> instead of a single column at a time. This can provide a useful performance benefit for a <span class="title-ref">DataFrame</span> with many columns or the ability to utilize other columns during the windowing operation. The `method='table'` option can only be used if `engine='numba'` is specified in the corresponding method call.

For example, a [weighted mean](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean) calculation can be calculated with <span class="title-ref">\~Rolling.apply</span> by specifying a separate column of weights.

<div class="ipython" data-okwarning="">

python

  - def weighted\_mean(x):  
    arr = np.ones((1, x.shape\[1\])) arr\[:, :2\] = (x\[:, :2\] \* x\[:, 2\]).sum(axis=0) / x\[:, 2\].sum() return arr

df = pd.DataFrame(\[\[1, 2, 0.6\], \[2, 3, 0.4\], \[3, 4, 0.2\], \[4, 5, 0.7\]\]) df.rolling(2, method="table", min\_periods=0).apply(weighted\_mean, raw=True, engine="numba") \# noqa: E501

</div>

<div class="versionadded">

1.3

</div>

Some windowing operations also support an `online` method after constructing a windowing object which returns a new object that supports passing in new <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> objects to continue the windowing calculation with the new values (i.e. online calculations).

The methods on this new windowing objects must call the aggregation method first to "prime" the initial state of the online calculation. Then, new <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> objects can be passed in the `update` argument to continue the windowing calculation.

<div class="ipython">

python

df = pd.DataFrame(\[\[1, 2, 0.6\], \[2, 3, 0.4\], \[3, 4, 0.2\], \[4, 5, 0.7\]\]) df.ewm(0.5).mean()

</div>

<div class="ipython" data-okwarning="">

python

online\_ewm = df.head(2).ewm(0.5).online() online\_ewm.mean() online\_ewm.mean(update=df.tail(1))

</div>

All windowing operations support a `min_periods` argument that dictates the minimum amount of non-`np.nan` values a window must have; otherwise, the resulting value is `np.nan`. `min_periods` defaults to 1 for time-based windows and `window` for fixed windows

<div class="ipython">

python

s = pd.Series(\[np.nan, 1, 2, np.nan, np.nan, 3\]) s.rolling(window=3, min\_periods=1).sum() s.rolling(window=3, min\_periods=2).sum() \# Equivalent to min\_periods=3 s.rolling(window=3, min\_periods=None).sum()

</div>

Additionally, all windowing operations supports the `aggregate` method for returning a result of multiple aggregations applied to a window.

<div class="ipython">

python

df = pd.DataFrame({"A": range(5), "B": range(10, 15)}) df.expanding().agg(\["sum", "mean", "std"\])

</div>

## Rolling window

Generic rolling windows support specifying windows as a fixed number of observations or variable number of observations based on an offset. If a time based offset is provided, the corresponding time based index must be monotonic.

<div class="ipython">

python

times = \['2020-01-01', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-29'\] s = pd.Series(range(5), index=pd.DatetimeIndex(times)) s \# Window with 2 observations s.rolling(window=2).sum() \# Window with 2 days worth of observations s.rolling(window='2D').sum()

</div>

For all supported aggregation functions, see \[api.functions\_rolling\](\#api.functions\_rolling).

### Centering windows

By default the labels are set to the right edge of the window, but a `center` keyword is available so the labels can be set at the center.

<div class="ipython">

python

s = pd.Series(range(10)) s.rolling(window=5).mean() s.rolling(window=5, center=True).mean()

</div>

This can also be applied to datetime-like indices.

<div class="versionadded">

1.3.0

</div>

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"A": \[0, 1, 2, 3, 4\]}, index=pd.date\_range("2020", periods=5, freq="1D")

) df df.rolling("2D", center=False).mean() df.rolling("2D", center=True).mean()

</div>

### Rolling window endpoints

The inclusion of the interval endpoints in rolling window calculations can be specified with the `closed` parameter:

<table>
<thead>
<tr class="header">
<th>Value</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>'right'</code></td>
<td><blockquote>
<p>close right endpoint</p>
</blockquote></td>
</tr>
<tr class="even">
<td><code>'left'</code></td>
<td>close left endpoint</td>
</tr>
<tr class="odd">
<td><code>'both'</code></td>
<td>close both endpoints</td>
</tr>
<tr class="even">
<td><code>'neither'</code></td>
<td>open endpoints</td>
</tr>
</tbody>
</table>

For example, having the right endpoint open is useful in many problems that require that there is no contamination from present information back to past information. This allows the rolling window to compute statistics "up to that point in time", but not including that point in time.

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"x": 1}, index=\[ pd.Timestamp("20130101 09:00:01"), pd.Timestamp("20130101 09:00:02"), pd.Timestamp("20130101 09:00:03"), pd.Timestamp("20130101 09:00:04"), pd.Timestamp("20130101 09:00:06"), \],

)

df\["right"\] = df.rolling("2s", closed="right").x.sum() \# default df\["both"\] = df.rolling("2s", closed="both").x.sum() df\["left"\] = df.rolling("2s", closed="left").x.sum() df\["neither"\] = df.rolling("2s", closed="neither").x.sum()

df

</div>

### Custom window rolling

In addition to accepting an integer or offset as a `window` argument, `rolling` also accepts a `BaseIndexer` subclass that allows a user to define a custom method for calculating window bounds. The `BaseIndexer` subclass will need to define a `get_window_bounds` method that returns a tuple of two arrays, the first being the starting indices of the windows and second being the ending indices of the windows. Additionally, `num_values`, `min_periods`, `center`, `closed` and `step` will automatically be passed to `get_window_bounds` and the defined method must always accept these arguments.

For example, if we have the following <span class="title-ref">DataFrame</span>

<div class="ipython">

python

use\_expanding = \[True, False, True, False, True\] use\_expanding df = pd.DataFrame({"values": range(5)}) df

</div>

and we want to use an expanding window where `use_expanding` is `True` otherwise a window of size 1, we can create the following `BaseIndexer` subclass:

<div class="ipython">

python

from pandas.api.indexers import BaseIndexer

  - class CustomIndexer(BaseIndexer):
    
      - def get\_window\_bounds(self, num\_values, min\_periods, center, closed, step):  
        start = np.empty(num\_values, dtype=np.int64) end = np.empty(num\_values, dtype=np.int64) for i in range(num\_values): if self.use\_expanding\[i\]: start\[i\] = 0 end\[i\] = i + 1 else: start\[i\] = i end\[i\] = i + self.window\_size return start, end

indexer = CustomIndexer(window\_size=1, use\_expanding=use\_expanding)

df.rolling(indexer).sum()

</div>

You can view other examples of `BaseIndexer` subclasses [here](https://github.com/pandas-dev/pandas/blob/main/pandas/core/indexers/objects.py)

One subclass of note within those examples is the `VariableOffsetWindowIndexer` that allows rolling operations over a non-fixed offset like a `BusinessDay`.

<div class="ipython">

python

from pandas.api.indexers import VariableOffsetWindowIndexer

df = pd.DataFrame(range(10), index=pd.date\_range("2020", periods=10)) offset = pd.offsets.BDay(1) indexer = VariableOffsetWindowIndexer(index=df.index, offset=offset) df df.rolling(indexer).sum()

</div>

For some problems knowledge of the future is available for analysis. For example, this occurs when each data point is a full time series read from an experiment, and the task is to extract underlying conditions. In these cases it can be useful to perform forward-looking rolling window computations. <span class="title-ref">FixedForwardWindowIndexer \<pandas.api.indexers.FixedForwardWindowIndexer\></span> class is available for this purpose. This <span class="title-ref">BaseIndexer \<pandas.api.indexers.BaseIndexer\></span> subclass implements a closed fixed-width forward-looking rolling window, and we can use it as follows:

<div class="ipython">

python

from pandas.api.indexers import FixedForwardWindowIndexer indexer = FixedForwardWindowIndexer(window\_size=2) df.rolling(indexer, min\_periods=1).sum()

</div>

We can also achieve this by using slicing, applying rolling aggregation, and then flipping the result as shown in example below:

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - data=\[  
        \[pd.Timestamp("2018-01-01 00:00:00"), 100\], \[pd.Timestamp("2018-01-01 00:00:01"), 101\], \[pd.Timestamp("2018-01-01 00:00:03"), 103\], \[pd.Timestamp("2018-01-01 00:00:04"), 111\],
    
    \], columns=\["time", "value"\],

).set\_index("time") df

reversed\_df = df\[::-1\].rolling("2s").sum()\[::-1\] reversed\_df

</div>

### Rolling apply

The <span class="title-ref">\~Rolling.apply</span> function takes an extra `func` argument and performs generic rolling computations. The `func` argument should be a single function that produces a single value from an ndarray input. `raw` specifies whether the windows are cast as <span class="title-ref">Series</span> objects (`raw=False`) or ndarray objects (`raw=True`).

<div class="ipython">

python

  - def mad(x):  
    return np.fabs(x - x.mean()).mean()

s = pd.Series(range(10)) s.rolling(window=4).apply(mad, raw=True)

</div>

### Numba engine

Additionally, <span class="title-ref">\~Rolling.apply</span> can leverage [Numba](https://numba.pydata.org/) if installed as an optional dependency. The apply aggregation can be executed using Numba by specifying `engine='numba'` and `engine_kwargs` arguments (`raw` must also be set to `True`). See \[enhancing performance with Numba \<enhancingperf.numba\>\](\#enhancing-performance-with-numba-\<enhancingperf.numba\>) for general usage of the arguments and performance considerations.

Numba will be applied in potentially two routines:

1.  If `func` is a standard Python function, the engine will [JIT](https://numba.pydata.org/numba-doc/latest/user/overview.html) the passed function. `func` can also be a JITed function in which case the engine will not JIT the function again.
2.  The engine will JIT the for loop where the apply function is applied to each window.

The `engine_kwargs` argument is a dictionary of keyword arguments that will be passed into the [numba.jit decorator](https://numba.pydata.org/numba-doc/latest/reference/jit-compilation.html#numba.jit). These keyword arguments will be applied to *both* the passed function (if a standard Python function) and the apply for loop over each window.

<div class="versionadded">

1.3.0

</div>

`mean`, `median`, `max`, `min`, and `sum` also support the `engine` and `engine_kwargs` arguments.

### Binary window functions

<span class="title-ref">\~Rolling.cov</span> and <span class="title-ref">\~Rolling.corr</span> can compute moving window statistics about two <span class="title-ref">Series</span> or any combination of <span class="title-ref">DataFrame</span>/<span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span>/<span class="title-ref">DataFrame</span>. Here is the behavior in each case:

  - two \`Series\`: compute the statistic for the pairing.
  - <span class="title-ref">DataFrame</span>/\`Series\`: compute the statistics for each column of the DataFrame with the passed Series, thus returning a DataFrame.

<!-- end list -->

  - \* <span class="title-ref">DataFrame</span>/\`DataFrame\`: by default compute the statistic for matching column  
    names, returning a DataFrame. If the keyword argument `pairwise=True` is passed then computes the statistic for each pair of columns, returning a <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> whose values are the dates in question (see \[the next section \<window.corr\_pairwise\>\](\#the-next-section

\--\<window.corr\_pairwise\>)).

For example:

<div class="ipython">

python

  - df = pd.DataFrame(  
    np.random.randn(10, 4), index=pd.date\_range("2020-01-01", periods=10), columns=\["A", "B", "C", "D"\],

) df = df.cumsum()

df2 = df\[:4\] df2.rolling(window=2).corr(df2\["B"\])

</div>

### Computing rolling pairwise covariances and correlations

In financial data analysis and other fields it's common to compute covariance and correlation matrices for a collection of time series. Often one is also interested in moving-window covariance and correlation matrices. This can be done by passing the `pairwise` keyword argument, which in the case of <span class="title-ref">DataFrame</span> inputs will yield a MultiIndexed <span class="title-ref">DataFrame</span> whose `index` are the dates in question. In the case of a single DataFrame argument the `pairwise` argument can even be omitted:

\> **Note** \> Missing values are ignored and each entry is computed using the pairwise complete observations.

> Assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased. However, for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimated correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See [Estimation of covariance matrices](https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_matrices) for more details.

<div class="ipython">

python

  - covs = (  
    df\[\["B", "C", "D"\]\] .rolling(window=4) .cov(df\[\["A", "B", "C"\]\], pairwise=True)

) covs

</div>

## Weighted window

The `win_type` argument in `.rolling` generates a weighted windows that are commonly used in filtering and spectral estimation. `win_type` must be string that corresponds to a [scipy.signal window function](https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows). Scipy must be installed in order to use these windows, and supplementary arguments that the Scipy window methods take must be specified in the aggregation function.

<div class="ipython">

python

s = pd.Series(range(10)) s.rolling(window=5).mean() s.rolling(window=5, win\_type="triang").mean() \# Supplementary Scipy arguments passed in the aggregation function s.rolling(window=5, win\_type="gaussian").mean(std=0.1)

</div>

For all supported aggregation functions, see \[api.functions\_window\](\#api.functions\_window).

## Expanding window

An expanding window yields the value of an aggregation statistic with all the data available up to that point in time. Since these calculations are a special case of rolling statistics, they are implemented in pandas such that the following two calls are equivalent:

<div class="ipython">

python

df = pd.DataFrame(range(5)) df.rolling(window=len(df), min\_periods=1).mean() df.expanding(min\_periods=1).mean()

</div>

For all supported aggregation functions, see \[api.functions\_expanding\](\#api.functions\_expanding).

## Exponentially weighted window

An exponentially weighted window is similar to an expanding window but with each prior point being exponentially weighted down relative to the current point.

In general, a weighted moving average is calculated as

\[y_t = \frac{\sum_{i=0}^t w_i x_{t-i}}{\sum_{i=0}^t w_i},\]

where \(x_t\) is the input, \(y_t\) is the result and the \(w_i\) are the weights.

For all supported aggregation functions, see \[api.functions\_ewm\](\#api.functions\_ewm).

The EW functions support two variants of exponential weights. The default, `adjust=True`, uses the weights \(w_i = (1 - \alpha)^i\) which gives

\[y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...
+ (1 - \alpha)^t x_{0}}{1 + (1 - \alpha) + (1 - \alpha)^2 + ...
+ (1 - \alpha)^t}\]

When `adjust=False` is specified, moving averages are calculated as

\[\begin{aligned}
y_0 &= x_0 \\
y_t &= (1 - \alpha) y_{t-1} + \alpha x_t,
\end{aligned}\]

which is equivalent to using weights

\[\begin{aligned}
w_i = \begin{cases}
    \alpha (1 - \alpha)^i & \text{if } i < t \\
    (1 - \alpha)^i        & \text{if } i = t.
\end{cases}
\end{aligned}\]

\> **Note** \> These equations are sometimes written in terms of \(\alpha' = 1 - \alpha\), e.g.

> \[y_t = \alpha' y_{t-1} + (1 - \alpha') x_t.\]

The difference between the above two variants arises because we are dealing with series which have finite history. Consider a series of infinite history, with `adjust=True`:

\[y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...}
{1 + (1 - \alpha) + (1 - \alpha)^2 + ...}\]

Noting that the denominator is a geometric series with initial term equal to 1 and a ratio of \(1 - \alpha\) we have

\[\begin{aligned}
y_t &= \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...}
{\frac{1}{1 - (1 - \alpha)}}\\
&= [x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...] \alpha \\
&= \alpha x_t + [(1-\alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...]\alpha \\
&= \alpha x_t + (1 - \alpha)[x_{t-1} + (1 - \alpha) x_{t-2} + ...]\alpha\\
&= \alpha x_t + (1 - \alpha) y_{t-1}
\end{aligned}\]

which is the same expression as `adjust=False` above and therefore shows the equivalence of the two variants for infinite series. When `adjust=False`, we have \(y_0 = x_0\) and \(y_t = \alpha x_t + (1 - \alpha) y_{t-1}\). Therefore, there is an assumption that \(x_0\) is not an ordinary value but rather an exponentially weighted moment of the infinite series up to that point.

One must have \(0 < \alpha \leq 1\), and while it is possible to pass \(\alpha\) directly, it's often easier to think about either the **span**, **center of mass (com)** or **half-life** of an EW moment:

\[\begin{aligned}
\alpha =
 \begin{cases}
     \frac{2}{s + 1},               & \text{for span}\ s \geq 1\\
     \frac{1}{1 + c},               & \text{for center of mass}\ c \geq 0\\
     1 - \exp^{\frac{\log 0.5}{h}}, & \text{for half-life}\ h > 0
 \end{cases}
\end{aligned}\]

One must specify precisely one of **span**, **center of mass**, **half-life** and **alpha** to the EW functions:

  - **Span** corresponds to what is commonly called an "N-day EW moving average".
  - **Center of mass** has a more physical interpretation and can be thought of in terms of span: \(c = (s - 1) / 2\).
  - **Half-life** is the period of time for the exponential weight to reduce to one half.
  - **Alpha** specifies the smoothing factor directly.

You can also specify `halflife` in terms of a timedelta convertible unit to specify the amount of time it takes for an observation to decay to half its value when also specifying a sequence of `times`.

<div class="ipython">

python

df = pd.DataFrame({"B": \[0, 1, 2, np.nan, 4\]}) df times = \["2020-01-01", "2020-01-03", "2020-01-10", "2020-01-15", "2020-01-17"\] df.ewm(halflife="4 days", times=pd.DatetimeIndex(times)).mean()

</div>

The following formula is used to compute exponentially weighted mean with an input vector of times:

\[y_t = \frac{\sum_{i=0}^t 0.5^\frac{t_{t} - t_{i}}{\lambda} x_{t-i}}{\sum_{i=0}^t 0.5^\frac{t_{t} - t_{i}}{\lambda}},\]

ExponentialMovingWindow also has an `ignore_na` argument, which determines how intermediate null values affect the calculation of the weights. When `ignore_na=False` (the default), weights are calculated based on absolute positions, so that intermediate null values affect the result. When `ignore_na=True`, weights are calculated by ignoring intermediate null values. For example, assuming `adjust=True`, if `ignore_na=False`, the weighted average of `3, NaN, 5` would be calculated as

\[\frac{(1-\alpha)^2 \cdot 3 + 1 \cdot 5}{(1-\alpha)^2 + 1}.\]

Whereas if `ignore_na=True`, the weighted average would be calculated as

\[\frac{(1-\alpha) \cdot 3 + 1 \cdot 5}{(1-\alpha) + 1}.\]

The <span class="title-ref">\~Ewm.var</span>, <span class="title-ref">\~Ewm.std</span>, and <span class="title-ref">\~Ewm.cov</span> functions have a `bias` argument, specifying whether the result should contain biased or unbiased statistics. For example, if `bias=True`, `ewmvar(x)` is calculated as `ewmvar(x) = ewma(x**2) - ewma(x)**2`; whereas if `bias=False` (the default), the biased variance statistics are scaled by debiasing factors

\[\frac{\left(\sum_{i=0}^t w_i\right)^2}{\left(\sum_{i=0}^t w_i\right)^2 - \sum_{i=0}^t w_i^2}.\]

(For \(w_i = 1\), this reduces to the usual \(N / (N - 1)\) factor, with \(N = t + 1\).) See [Weighted Sample Variance](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_variance) on Wikipedia for further details.

---

index.md

---

<div id="release">

{{ header }}

</div>

# Release notes

This is the list of changes to pandas between each release. For full details, see the [commit logs](https://github.com/pandas-dev/pandas/commits/). For install and upgrade instructions, see \[install\](\#install).

## Version 3.0

<div class="toctree" data-maxdepth="2">

v3.0.0

</div>

## Version 2.3

<div class="toctree" data-maxdepth="2">

v2.3.0

</div>

## Version 2.2

<div class="toctree" data-maxdepth="2">

v2.2.3 v2.2.2 v2.2.1 v2.2.0

</div>

## Version 2.1

<div class="toctree" data-maxdepth="2">

v2.1.4 v2.1.3 v2.1.2 v2.1.1 v2.1.0

</div>

## Version 2.0

<div class="toctree" data-maxdepth="2">

v2.0.3 v2.0.2 v2.0.1 v2.0.0

</div>

## Version 1.5

<div class="toctree" data-maxdepth="2">

v1.5.3 v1.5.2 v1.5.1 v1.5.0

</div>

## Version 1.4

<div class="toctree" data-maxdepth="2">

v1.4.4 v1.4.3 v1.4.2 v1.4.1 v1.4.0

</div>

## Version 1.3

<div class="toctree" data-maxdepth="2">

v1.3.5 v1.3.4 v1.3.3 v1.3.2 v1.3.1 v1.3.0

</div>

## Version 1.2

<div class="toctree" data-maxdepth="2">

v1.2.5 v1.2.4 v1.2.3 v1.2.2 v1.2.1 v1.2.0

</div>

## Version 1.1

<div class="toctree" data-maxdepth="2">

v1.1.5 v1.1.4 v1.1.3 v1.1.2 v1.1.1 v1.1.0

</div>

## Version 1.0

<div class="toctree" data-maxdepth="2">

v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0

</div>

## Version 0.25

<div class="toctree" data-maxdepth="2">

v0.25.3 v0.25.2 v0.25.1 v0.25.0

</div>

## Version 0.24

<div class="toctree" data-maxdepth="2">

v0.24.2 v0.24.1 v0.24.0

</div>

## Version 0.23

<div class="toctree" data-maxdepth="2">

v0.23.4 v0.23.3 v0.23.2 v0.23.1 v0.23.0

</div>

## Version 0.22

<div class="toctree" data-maxdepth="2">

v0.22.0

</div>

## Version 0.21

<div class="toctree" data-maxdepth="2">

v0.21.1 v0.21.0

</div>

## Version 0.20

<div class="toctree" data-maxdepth="2">

v0.20.3 v0.20.2 v0.20.0

</div>

## Version 0.19

<div class="toctree" data-maxdepth="2">

v0.19.2 v0.19.1 v0.19.0

</div>

## Version 0.18

<div class="toctree" data-maxdepth="2">

v0.18.1 v0.18.0

</div>

## Version 0.17

<div class="toctree" data-maxdepth="2">

v0.17.1 v0.17.0

</div>

## Version 0.16

<div class="toctree" data-maxdepth="2">

v0.16.2 v0.16.1 v0.16.0

</div>

## Version 0.15

<div class="toctree" data-maxdepth="2">

v0.15.2 v0.15.1 v0.15.0

</div>

## Version 0.14

<div class="toctree" data-maxdepth="2">

v0.14.1 v0.14.0

</div>

## Version 0.13

<div class="toctree" data-maxdepth="2">

v0.13.1 v0.13.0

</div>

## Version 0.12

<div class="toctree" data-maxdepth="2">

v0.12.0

</div>

## Version 0.11

<div class="toctree" data-maxdepth="2">

v0.11.0

</div>

## Version 0.10

<div class="toctree" data-maxdepth="2">

v0.10.1 v0.10.0

</div>

## Version 0.9

<div class="toctree" data-maxdepth="2">

v0.9.1 v0.9.0

</div>

## Version 0.8

<div class="toctree" data-maxdepth="2">

v0.8.1 v0.8.0

</div>

## Version 0.7

<div class="toctree" data-maxdepth="2">

v0.7.3 v0.7.2 v0.7.1 v0.7.0

</div>

## Version 0.6

<div class="toctree" data-maxdepth="2">

v0.6.1 v0.6.0

</div>

## Version 0.5

<div class="toctree" data-maxdepth="2">

v0.5.0

</div>

## Version 0.4

<div class="toctree" data-maxdepth="2">

v0.4.x

</div>

---

v0.10.0.md

---

# Version 0.10.0 (December 17, 2012)

{{ header }}

This is a major release from 0.9.1 and includes many new features and enhancements along with a large number of bug fixes. There are also a number of important API changes that long-time pandas users should pay close attention to.

## File parsing new features

The delimited file parsing engine (the guts of `read_csv` and `read_table`) has been rewritten from the ground up and now uses a fraction the amount of memory while parsing, while being 40% or more faster in most use cases (in some cases much faster).

There are also many new features:

  - Much-improved Unicode handling via the `encoding` option.
  - Column filtering (`usecols`)
  - Dtype specification (`dtype` argument)
  - Ability to specify strings to be recognized as True/False
  - Ability to yield NumPy record arrays (`as_recarray`)
  - High performance `delim_whitespace` option
  - Decimal format (e.g. European format) specification
  - Easier CSV dialect options: `escapechar`, `lineterminator`, `quotechar`, etc.
  - More robust handling of many exceptional kinds of files observed in the wild

## API changes

**Deprecated DataFrame BINOP TimeSeries special case behavior**

The default behavior of binary operations between a DataFrame and a Series has always been to align on the DataFrame's columns and broadcast down the rows, **except** in the special case that the DataFrame contains time series. Since there are now method for each binary operator enabling you to specify how you want to broadcast, we are phasing out this special case (Zen of Python: *Special cases aren't special enough to break the rules*). Here's what I'm talking about:

<div class="ipython" data-okwarning="">

python

import pandas as pd

df = pd.DataFrame(np.random.randn(6, 4), index=pd.date\_range("1/1/2000", periods=6)) df \# deprecated now df - df\[0\] \# Change your code to df.sub(df\[0\], axis=0) \# align on axis 0 (rows)

</div>

You will get a deprecation warning in the 0.10.x series, and the deprecated functionality will be removed in 0.11 or later.

**Altered resample default behavior**

The default time series `resample` binning behavior of daily `D` and *higher* frequencies has been changed to `closed='left', label='left'`. Lower nfrequencies are unaffected. The prior defaults were causing a great deal of confusion for users, especially resampling data to daily frequency (which labeled the aggregated group with the end of the interval: the next day).

`` `ipython    In [1]: dates = pd.date_range('1/1/2000', '1/5/2000', freq='4h')     In [2]: series = pd.Series(np.arange(len(dates)), index=dates)     In [3]: series    Out[3]:    2000-01-01 00:00:00     0    2000-01-01 04:00:00     1    2000-01-01 08:00:00     2    2000-01-01 12:00:00     3    2000-01-01 16:00:00     4    2000-01-01 20:00:00     5    2000-01-02 00:00:00     6    2000-01-02 04:00:00     7    2000-01-02 08:00:00     8    2000-01-02 12:00:00     9    2000-01-02 16:00:00    10    2000-01-02 20:00:00    11    2000-01-03 00:00:00    12    2000-01-03 04:00:00    13    2000-01-03 08:00:00    14    2000-01-03 12:00:00    15    2000-01-03 16:00:00    16    2000-01-03 20:00:00    17    2000-01-04 00:00:00    18    2000-01-04 04:00:00    19    2000-01-04 08:00:00    20    2000-01-04 12:00:00    21    2000-01-04 16:00:00    22    2000-01-04 20:00:00    23    2000-01-05 00:00:00    24    Freq: 4H, dtype: int64     In [4]: series.resample('D', how='sum')    Out[4]:    2000-01-01     15    2000-01-02     51    2000-01-03     87    2000-01-04    123    2000-01-05     24    Freq: D, dtype: int64     In [5]: # old behavior    In [6]: series.resample('D', how='sum', closed='right', label='right')    Out[6]:    2000-01-01      0    2000-01-02     21    2000-01-03     57    2000-01-04     93    2000-01-05    129    Freq: D, dtype: int64  - Infinity and negative infinity are no longer treated as NA by ``isnull`and`notnull`. That they ever were was a relic of early pandas. This behavior   can be re-enabled globally by the`mode.use\_inf\_as\_null`option:  .. code-block:: ipython      In [6]: s = pd.Series([1.5, np.inf, 3.4, -np.inf])      In [7]: pd.isnull(s)     Out[7]:     0    False     1    False     2    False     3    False     Length: 4, dtype: bool      In [8]: s.fillna(0)     Out[8]:     0    1.500000     1         inf     2    3.400000     3        -inf     Length: 4, dtype: float64      In [9]: pd.set_option('use_inf_as_null', True)      In [10]: pd.isnull(s)     Out[10]:     0    False     1     True     2    False     3     True     Length: 4, dtype: bool      In [11]: s.fillna(0)     Out[11]:     0    1.5     1    0.0     2    3.4     3    0.0     Length: 4, dtype: float64      In [12]: pd.reset_option('use_inf_as_null')  - Methods with the`inplace`option now all return`None`instead of the   calling object. E.g. code written like`df = df.fillna(0, inplace=True)`may stop working. To fix, simply delete the unnecessary variable assignment.  -`pandas.merge`no longer sorts the group keys (`sort=False`) by   default. This was done for performance reasons: the group-key sorting is   often one of the more expensive parts of the computation and is often   unnecessary.  - The default column names for a file with no header have been changed to the   integers`0`through`N - 1`. This is to create consistency with the   DataFrame constructor with no columns specified. The v0.9.0 behavior (names`X0`,`X1`, ...) can be reproduced by specifying`prefix='X'`:  .. code-block:: ipython      In [6]: import io      In [7]: data = """       ...: a,b,c       ...: 1,Yes,2       ...: 3,No,4       ...: """       ...:      In [8]: print(data)          a,b,c         1,Yes,2         3,No,4      In [9]: pd.read_csv(io.StringIO(data), header=None)     Out[9]:            0    1  2     0      a    b  c     1      1  Yes  2     2      3   No  4      In [10]: pd.read_csv(io.StringIO(data), header=None, prefix="X")     Out[10]:             X0   X1 X2     0       a    b  c     1       1  Yes  2     2       3   No  4  - Values like`'Yes'`and`'No'`are not interpreted as boolean by default,   though this can be controlled by new`true\_values`and`false\_values`arguments:  .. code-block:: ipython      In [4]: print(data)          a,b,c         1,Yes,2         3,No,4      In [5]: pd.read_csv(io.StringIO(data))     Out[5]:            a    b  c     0      1  Yes  2     1      3   No  4      In [6]: pd.read_csv(io.StringIO(data), true_values=["Yes"], false_values=["No"])     Out[6]:            a      b  c     0      1   True  2     1      3  False  4  - The file parsers will not recognize non-string values arising from a   converter function as NA if passed in the`na\_values`argument. It's better   to do post-processing using the`replace`function instead.  - Calling`fillna`on Series or DataFrame with no arguments is no longer   valid code. You must either specify a fill value or an interpolation method:  .. code-block:: ipython      In [6]: s = pd.Series([np.nan, 1.0, 2.0, np.nan, 4])      In [7]: s     Out[7]:     0      NaN     1      1.0     2      2.0     3      NaN     4      4.0     dtype: float64      In [8]: s.fillna(0)     Out[8]:     0      0.0     1      1.0     2      2.0     3      0.0     4      4.0     dtype: float64      In [9]: s.fillna(method="pad")     Out[9]:     0      NaN     1      1.0     2      2.0     3      2.0     4      4.0     dtype: float64  Convenience methods`ffill`and`bfill`have been added:  .. ipython:: python     s = pd.Series([np.nan, 1.0, 2.0, np.nan, 4])    s.ffill()   -`Series.apply``will now operate on a returned value from the applied   function, that is itself a series, and possibly upcast the result to a   DataFrame    .. ipython:: python        def f(x):           return pd.Series([x, x ** 2], index=["x", "x^2"])         s = pd.Series(np.random.rand(5))       s       s.apply(f)  - New API functions for working with pandas options (:issue:`2097`):    -``get\_option`/`set\_option`- get/set the value of an option. Partial     names are accepted.  -`reset\_option`- reset one or more options to     their default value. Partial names are accepted.  -`describe\_option`-     print a description of one or more options. When called with no     arguments. print all registered options.    Note:`set\_printoptions`/`reset\_printoptions``are now deprecated (but   functioning), the print options now live under "display.XYZ". For example:    .. ipython:: python       pd.get_option("display.max_rows")  - to_string() methods now always return unicode strings  (:issue:`2224`).  New features``\` \~\~\~\~\~\~\~\~\~\~\~\~

## Wide DataFrame printing

Instead of printing the summary information, pandas now splits the string representation across multiple rows by default:

<div class="ipython">

python

wide\_frame = pd.DataFrame(np.random.randn(5, 16))

wide\_frame

</div>

The old behavior of printing out summary information can be achieved via the 'expand\_frame\_repr' print option:

<div class="ipython">

python

pd.set\_option("expand\_frame\_repr", False)

wide\_frame

</div>

<div class="ipython" data-suppress="">

python

pd.reset\_option("expand\_frame\_repr")

</div>

The width of each line can be changed via 'line\_width' (80 by default):

`` `python    pd.set_option("line_width", 40)     wide_frame   Updated PyTables support ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

\[Docs \<io.hdf5\>\](\#docs-\<io.hdf5\>) for PyTables `Table` format & several enhancements to the api. Here is a taste of what to expect.

`` `ipython     In [41]: store = pd.HDFStore('store.h5')      In [42]: df = pd.DataFrame(np.random.randn(8, 3),        ....:                   index=pd.date_range('1/1/2000', periods=8),        ....:                   columns=['A', 'B', 'C'])      In [43]: df     Out[43]:                        A         B         C     2000-01-01 -2.036047  0.000830 -0.955697     2000-01-02 -0.898872 -0.725411  0.059904     2000-01-03 -0.449644  1.082900 -1.221265     2000-01-04  0.361078  1.330704  0.855932     2000-01-05 -1.216718  1.488887  0.018993     2000-01-06 -0.877046  0.045976  0.437274     2000-01-07 -0.567182 -0.888657 -0.556383     2000-01-08  0.655457  1.117949 -2.782376      [8 rows x 3 columns]      # appending data frames     In [44]: df1 = df[0:4]      In [45]: df2 = df[4:]      In [46]: store.append('df', df1)      In [47]: store.append('df', df2)      In [48]: store     Out[48]:     <class 'pandas.io.pytables.HDFStore'>     File path: store.h5     /df            frame_table  (typ->appendable,nrows->8,ncols->3,indexers->[index])      # selecting the entire store     In [49]: store.select('df')     Out[49]:                        A         B         C     2000-01-01 -2.036047  0.000830 -0.955697     2000-01-02 -0.898872 -0.725411  0.059904     2000-01-03 -0.449644  1.082900 -1.221265     2000-01-04  0.361078  1.330704  0.855932     2000-01-05 -1.216718  1.488887  0.018993     2000-01-06 -0.877046  0.045976  0.437274     2000-01-07 -0.567182 -0.888657 -0.556383     2000-01-08  0.655457  1.117949 -2.782376      [8 rows x 3 columns]  .. code-block:: ipython      In [50]: wp = pd.Panel(np.random.randn(2, 5, 4), items=['Item1', 'Item2'],        ....:               major_axis=pd.date_range('1/1/2000', periods=5),        ....:               minor_axis=['A', 'B', 'C', 'D'])      In [51]: wp     Out[51]:     <class 'pandas.core.panel.Panel'>     Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)     Items axis: Item1 to Item2     Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00     Minor_axis axis: A to D      # storing a panel     In [52]: store.append('wp', wp)      # selecting via A QUERY     In [53]: store.select('wp', [pd.Term('major_axis>20000102'),        ....:                     pd.Term('minor_axis', '=', ['A', 'B'])])        ....:     Out[53]:     <class 'pandas.core.panel.Panel'>     Dimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)     Items axis: Item1 to Item2     Major_axis axis: 2000-01-03 00:00:00 to 2000-01-05 00:00:00     Minor_axis axis: A to B      # removing data from tables     In [54]: store.remove('wp', pd.Term('major_axis>20000103'))     Out[54]: 8      In [55]: store.select('wp')     Out[55]:     <class 'pandas.core.panel.Panel'>     Dimensions: 2 (items) x 3 (major_axis) x 4 (minor_axis)     Items axis: Item1 to Item2     Major_axis axis: 2000-01-01 00:00:00 to 2000-01-03 00:00:00     Minor_axis axis: A to D      # deleting a store     In [56]: del store['df']      In [57]: store     Out[57]:     <class 'pandas.io.pytables.HDFStore'>     File path: store.h5     /wp            wide_table   (typ->appendable,nrows->12,ncols->2,indexers->[major_axis,minor_axis])   **Enhancements**  - added ability to hierarchical keys     .. code-block:: ipython          In [58]: store.put('foo/bar/bah', df)          In [59]: store.append('food/orange', df)          In [60]: store.append('food/apple', df)          In [61]: store         Out[61]:         <class 'pandas.io.pytables.HDFStore'>         File path: store.h5         /foo/bar/bah            frame        (shape->[8,3])         /food/apple             frame_table  (typ->appendable,nrows->8,ncols->3,indexers->[index])         /food/orange            frame_table  (typ->appendable,nrows->8,ncols->3,indexers->[index])         /wp                     wide_table   (typ->appendable,nrows->12,ncols->2,indexers->[major_axis,minor_axis])          # remove all nodes under this level         In [62]: store.remove('food')          In [63]: store         Out[63]:         <class 'pandas.io.pytables.HDFStore'>         File path: store.h5         /foo/bar/bah            frame        (shape->[8,3])         /wp                     wide_table   (typ->appendable,nrows->12,ncols->2,indexers->[major_axis,minor_axis])  - added mixed-dtype support!     .. code-block:: ipython          In [64]: df['string'] = 'string'          In [65]: df['int'] = 1          In [66]: store.append('df', df)          In [67]: df1 = store.select('df')          In [68]: df1         Out[68]:                            A         B         C  string  int         2000-01-01 -2.036047  0.000830 -0.955697  string    1         2000-01-02 -0.898872 -0.725411  0.059904  string    1         2000-01-03 -0.449644  1.082900 -1.221265  string    1         2000-01-04  0.361078  1.330704  0.855932  string    1         2000-01-05 -1.216718  1.488887  0.018993  string    1         2000-01-06 -0.877046  0.045976  0.437274  string    1         2000-01-07 -0.567182 -0.888657 -0.556383  string    1         2000-01-08  0.655457  1.117949 -2.782376  string    1          [8 rows x 5 columns]          In [69]: df1.get_dtype_counts()         Out[69]:         float64    3         int64      1         object     1         dtype: int64  - performance improvements on table writing ``<span class="title-ref"> - support for arbitrarily indexed dimensions - </span><span class="title-ref">SparseSeries</span><span class="title-ref"> now has a </span><span class="title-ref">density</span><span class="title-ref"> property (:issue:\`2384</span>) - enable `Series.str.strip/lstrip/rstrip` methods to take an input argument to strip arbitrary characters (`2411`) - implement `value_vars` in `melt` to limit values to certain columns and add `melt` to pandas namespace (`2412`)

**Bug Fixes**

  - added `Term` method of specifying where conditions (`1996`).
  - `del store['df']` now call `store.remove('df')` for store deletion
  - deleting of consecutive rows is much faster than before
  - `min_itemsize` parameter can be specified in table creation to force a minimum size for indexing columns (the previous implementation would set the column size based on the first append)
  - indexing support via `create_table_index` (requires PyTables \>= 2.3) (`698`).
  - appending on a store would fail if the table was not first created via `put`
  - fixed issue with missing attributes after loading a pickled dataframe (GH2431)
  - minor change to select and remove: require a table ONLY if where is also provided (and not None)

**Compatibility**

0.10 of `HDFStore` is backwards compatible for reading tables created in a prior version of pandas, however, query terms using the prior (undocumented) methodology are unsupported. You must read in the entire file and write it out using the new format to take advantage of the updates.

## N dimensional panels (experimental)

Adding experimental support for Panel4D and factory functions to create n-dimensional named panels. Here is a taste of what to expect.

`` `ipython   In [58]: p4d = Panel4D(np.random.randn(2, 2, 5, 4),     ....:       labels=['Label1','Label2'],     ....:       items=['Item1', 'Item2'],     ....:       major_axis=date_range('1/1/2000', periods=5),     ....:       minor_axis=['A', 'B', 'C', 'D'])     ....:    In [59]: p4d   Out[59]:   <class 'pandas.core.panelnd.Panel4D'>   Dimensions: 2 (labels) x 2 (items) x 5 (major_axis) x 4 (minor_axis)   Labels axis: Label1 to Label2   Items axis: Item1 to Item2   Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00   Minor_axis axis: A to D      See the [full release notes ](#full-release-notes ) `` \<release\>\` or issue tracker on GitHub for a complete list.

## Contributors

<div class="contributors">

v0.9.0..v0.10.0

</div>

---

v0.10.1.md

---

# Version 0.10.1 (January 22, 2013)

{{ header }}

This is a minor release from 0.10.0 and includes new features, enhancements, and bug fixes. In particular, there is substantial new HDFStore functionality contributed by Jeff Reback.

An undesired API breakage with functions taking the `inplace` option has been reverted and deprecation warnings added.

## API changes

  - Functions taking an `inplace` option return the calling object as before. A deprecation message has been added
  - Groupby aggregations Max/Min no longer exclude non-numeric data (`2700`)
  - Resampling an empty DataFrame now returns an empty DataFrame instead of raising an exception (`2640`)
  - The file reader will now raise an exception when NA values are found in an explicitly specified integer column instead of converting the column to float (`2631`)
  - DatetimeIndex.unique now returns a DatetimeIndex with the same name and
  - timezone instead of an array (`2563`)

## New features

  - MySQL support for database (contribution from Dan Allan)

## HDFStore

You may need to upgrade your existing data files. Please visit the **compatibility** section in the main docs.

<div class="ipython" data-suppress="" data-okexcept="">

python

import os

os.remove("store.h5")

</div>

You can designate (and index) certain columns that you want to be able to perform queries on a table, by passing a list to `data_columns`

<div class="ipython">

python

store = pd.HDFStore("store.h5") df = pd.DataFrame( np.random.randn(8, 3), index=pd.date\_range("1/1/2000", periods=8), columns=\["A", "B", "C"\], ) df\["string"\] = "foo" df.loc\[df.index\[4:6\], "string"\] = np.nan df.loc\[df.index\[7:9\], "string"\] = "bar" df\["string2"\] = "cool" df

\# on-disk operations store.append("df", df, data\_columns=\["B", "C", "string", "string2"\]) store.select("df", "B\>0 and string=='foo'")

\# this is in-memory version of this type of selection df\[(df.B \> 0) & (df.string == "foo")\]

</div>

Retrieving unique values in an indexable or data column.

`` `python    # note that this is deprecated as of 0.14.0    # can be replicated by: store.select_column('df','index').unique()    store.unique("df", "index")    store.unique("df", "string")  You can now store ``datetime64`in data columns  .. ipython:: python      df_mixed = df.copy()     df_mixed["datetime64"] = pd.Timestamp("20010102")     df_mixed.loc[df_mixed.index[3:4], ["A", "B"]] = np.nan      store.append("df_mixed", df_mixed)     df_mixed1 = store.select("df_mixed")     df_mixed1     df_mixed1.dtypes.value_counts()  You can pass`columns`keyword to select to filter a list of the return`<span class="title-ref"> columns, this is equivalent to passing a </span><span class="title-ref">Term('columns',list\_of\_columns\_to\_filter)</span>\`

<div class="ipython">

python

store.select("df", columns=\["A", "B"\])

</div>

`HDFStore` now serializes MultiIndex dataframes when appending tables.

`` `ipython     In [19]: index = pd.MultiIndex(levels=[['foo', 'bar', 'baz', 'qux'],        ....:                               ['one', 'two', 'three']],        ....:                       labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],        ....:                               [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],        ....:                       names=['foo', 'bar'])        ....:      In [20]: df = pd.DataFrame(np.random.randn(10, 3), index=index,        ....:                   columns=['A', 'B', 'C'])        ....:      In [21]: df     Out[21]:                       A         B         C     foo bar     foo one   -0.116619  0.295575 -1.047704         two    1.640556  1.905836  2.772115         three  0.088787 -1.144197 -0.633372     bar one    0.925372 -0.006438 -0.820408         two   -0.600874 -1.039266  0.824758     baz two   -0.824095 -0.337730 -0.927764         three -0.840123  0.248505 -0.109250     qux one    0.431977 -0.460710  0.336505         two   -3.207595 -1.535854  0.409769         three -0.673145 -0.741113 -0.110891      In [22]: store.append('mi', df)      In [23]: store.select('mi')     Out[23]:                       A         B         C     foo bar     foo one   -0.116619  0.295575 -1.047704         two    1.640556  1.905836  2.772115         three  0.088787 -1.144197 -0.633372     bar one    0.925372 -0.006438 -0.820408         two   -0.600874 -1.039266  0.824758     baz two   -0.824095 -0.337730 -0.927764         three -0.840123  0.248505 -0.109250     qux one    0.431977 -0.460710  0.336505         two   -3.207595 -1.535854  0.409769         three -0.673145 -0.741113 -0.110891      # the levels are automatically included as data columns     In [24]: store.select('mi', "foo='bar'")     Out[24]:                     A         B         C     foo bar     bar one  0.925372 -0.006438 -0.820408         two -0.600874 -1.039266  0.824758  Multi-table creation via ``append\_to\_multiple`and selection via`<span class="title-ref"> </span><span class="title-ref">select\_as\_multiple</span><span class="title-ref"> can create/select from multiple tables and return a combined result, by using </span><span class="title-ref">where</span>\` on a selector table.

<div class="ipython">

python

  - df\_mt = pd.DataFrame(  
    np.random.randn(8, 6), index=pd.date\_range("1/1/2000", periods=8), columns=\["A", "B", "C", "D", "E", "F"\],

) df\_mt\["foo"\] = "bar"

\# you can also create the tables individually store.append\_to\_multiple( {"df1\_mt": \["A", "B"\], "df2\_mt": None}, df\_mt, selector="df1\_mt" ) store

\# individual tables were created store.select("df1\_mt") store.select("df2\_mt")

\# as a multiple store.select\_as\_multiple( \["df1\_mt", "df2\_mt"\], where=\["A\>0", "B\>0"\], selector="df1\_mt" )

</div>

<div class="ipython" data-suppress="">

python

store.close() os.remove("store.h5")

</div>

**Enhancements**

  - `HDFStore` now can read native PyTables table format tables
  - You can pass `nan_rep = 'my_nan_rep'` to append, to change the default nan representation on disk (which converts to/from `np.nan`), this defaults to `nan`.
  - You can pass `index` to `append`. This defaults to `True`. This will automagically create indices on the *indexables* and *data columns* of the table
  - You can pass `chunksize=an integer` to `append`, to change the writing chunksize (default is 50000). This will significantly lower your memory usage on writing.
  - You can pass `expectedrows=an integer` to the first `append`, to set the TOTAL number of expected rows that `PyTables` will expected. This will optimize read/write performance.
  - `Select` now supports passing `start` and `stop` to provide selection space limiting in selection.
  - Greatly improved ISO8601 (e.g., yyyy-mm-dd) date parsing for file parsers (`2698`)
  - Allow `DataFrame.merge` to handle combinatorial sizes too large for 64-bit integer (`2690`)
  - Series now has unary negation (-series) and inversion (\~series) operators (`2686`)
  - DataFrame.plot now includes a `logx` parameter to change the x-axis to log scale (`2327`)
  - Series arithmetic operators can now handle constant and ndarray input (`2574`)
  - ExcelFile now takes a `kind` argument to specify the file type (`2613`)
  - A faster implementation for Series.str methods (`2602`)

**Bug Fixes**

  - `HDFStore` tables can now store `float32` types correctly (cannot be mixed with `float64` however)
  - Fixed Google Analytics prefix when specifying request segment (`2713`).
  - Function to reset Google Analytics token store so users can recover from improperly setup client secrets (`2687`).
  - Fixed groupby bug resulting in segfault when passing in MultiIndex (`2706`)
  - Fixed bug where passing a Series with datetime64 values into `to_datetime` results in bogus output values (`2699`)
  - Fixed bug in `pattern in HDFStore` expressions when pattern is not a valid regex (`2694`)
  - Fixed performance issues while aggregating boolean data (`2692`)
  - When given a boolean mask key and a Series of new values, Series \_\_setitem\_\_ will now align the incoming values with the original Series (`2686`)
  - Fixed MemoryError caused by performing counting sort on sorting MultiIndex levels with a very large number of combinatorial values (`2684`)
  - Fixed bug that causes plotting to fail when the index is a DatetimeIndex with a fixed-offset timezone (`2683`)
  - Corrected business day subtraction logic when the offset is more than 5 bdays and the starting date is on a weekend (`2680`)
  - Fixed C file parser behavior when the file has more columns than data (`2668`)
  - Fixed file reader bug that misaligned columns with data in the presence of an implicit column and a specified `usecols` value
  - DataFrames with numerical or datetime indices are now sorted prior to plotting (`2609`)
  - Fixed DataFrame.from\_records error when passed columns, index, but empty records (`2633`)
  - Several bug fixed for Series operations when dtype is datetime64 (`2689`, `2629`, `2626`)

See the \[full release notes \<release\>\](\#full-release-notes \<release\>) or issue tracker on GitHub for a complete list.

## Contributors

<div class="contributors">

v0.10.0..v0.10.1

</div>

---

v0.11.0.md

---

# Version 0.11.0 (April 22, 2013)

{{ header }}

This is a major release from 0.10.1 and includes many new features and enhancements along with a large number of bug fixes. The methods of Selecting Data have had quite a number of additions, and Dtype support is now full-fledged. There are also a number of important API changes that long-time pandas users should pay close attention to.

There is a new section in the documentation, \[10 Minutes to pandas \<10min\>\](\#10-minutes-to-pandas-\<10min\>), primarily geared to new users.

There is a new section in the documentation, \[Cookbook \<cookbook\>\](\#cookbook-\<cookbook\>), a collection of useful recipes in pandas (and that we want contributions\!).

There are several libraries that are now \[Recommended Dependencies \<install.recommended\_dependencies\>\](\#recommended-dependencies-\<install.recommended\_dependencies\>)

## Selection choices

Starting in 0.11.0, object selection has had a number of user-requested additions in order to support more explicit location based indexing. pandas now supports three types of multi-axis indexing.

  - `.loc` is strictly label based, will raise `KeyError` when the items are not found, allowed inputs are:
    
      - A single label, e.g. `5` or `'a'`, (note that `5` is interpreted as a *label* of the index. This use is **not** an integer position along the index)
      - A list or array of labels `['a', 'b', 'c']`
      - A slice object with labels `'a':'f'`, (note that contrary to usual python slices, **both** the start and the stop are included\!)
      - A boolean array
    
    See more at \[Selection by Label \<indexing.label\>\](\#selection-by-label-\<indexing.label\>)

  - `.iloc` is strictly integer position based (from `0` to `length-1` of the axis), will raise `IndexError` when the requested indices are out of bounds. Allowed inputs are:
    
      - An integer e.g. `5`
      - A list or array of integers `[4, 3, 0]`
      - A slice object with ints `1:7`
      - A boolean array
    
    See more at \[Selection by Position \<indexing.integer\>\](\#selection-by-position-\<indexing.integer\>)

  - `.ix` supports mixed integer and label based access. It is primarily label based, but will fallback to integer positional access. `.ix` is the most general and will support any of the inputs to `.loc` and `.iloc`, as well as support for floating point label schemes. `.ix` is especially useful when dealing with mixed positional and label based hierarchical indexes.
    
    As using integer slices with `.ix` have different behavior depending on whether the slice is interpreted as position based or label based, it's usually better to be explicit and use `.iloc` or `.loc`.
    
    See more at \[Advanced Indexing \<advanced\>\](\#advanced-indexing-\<advanced\>) and \[Advanced Hierarchical \<advanced.advanced\_hierarchical\>\](\#advanced-hierarchical-\<advanced.advanced\_hierarchical\>).

## Selection deprecations

Starting in version 0.11.0, these methods *may* be deprecated in future versions.

  - `irow`
  - `icol`
  - `iget_value`

See the section \[Selection by Position \<indexing.integer\>\](\#selection-by-position-\<indexing.integer\>) for substitutes.

## Dtypes

Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the `dtype` keyword, a passed `ndarray`, or a passed `Series`, then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will **NOT** be combined. The following example will give you a taste.

<div class="ipython">

python

df1 = pd.DataFrame(np.random.randn(8, 1), columns=\['A'\], dtype='float32') df1 df1.dtypes df2 = pd.DataFrame({'A': pd.Series(np.random.randn(8), dtype='float16'), 'B': pd.Series(np.random.randn(8)), 'C': pd.Series(range(8), dtype='uint8')}) df2 df2.dtypes

\# here you get some upcasting df3 = df1.reindex\_like(df2).fillna(value=0.0) + df2 df3 df3.dtypes

</div>

## Dtype conversion

This is lower-common-denominator upcasting, meaning you get the dtype which can accommodate all of the types

<div class="ipython">

python

df3.values.dtype

</div>

Conversion

<div class="ipython">

python

df3.astype('float32').dtypes

</div>

Mixed conversion

`` `ipython     In [12]: df3['D'] = '1.'      In [13]: df3['E'] = '1'      In [14]: df3.convert_objects(convert_numeric=True).dtypes     Out[14]:     A    float32     B    float64     C    float64     D    float64     E      int64     dtype: object      # same, but specific dtype conversion     In [15]: df3['D'] = df3['D'].astype('float16')      In [16]: df3['E'] = df3['E'].astype('int32')      In [17]: df3.dtypes     Out[17]:     A    float32     B    float64     C    float64     D    float16     E      int32     dtype: object  Forcing date coercion (and setting ``NaT`when not datelike)  .. code-block:: ipython      In [18]: import datetime      In [19]: s = pd.Series([datetime.datetime(2001, 1, 1, 0, 0), 'foo', 1.0, 1,        ....:                pd.Timestamp('20010104'), '20010105'], dtype='O')        ....:      In [20]: s.convert_objects(convert_dates='coerce')     Out[20]:     0   2001-01-01     1          NaT     2          NaT     3          NaT     4   2001-01-04     5   2001-01-05     dtype: datetime64[ns]  Dtype gotchas`\` \~\~\~\~\~\~\~\~\~\~\~\~\~

**Platform gotchas**

Starting in 0.11.0, construction of DataFrame/Series will use default dtypes of `int64` and `float64`, *regardless of platform*. This is not an apparent change from earlier versions of pandas. If you specify dtypes, they *WILL* be respected, however (`2837`)

The following will all result in `int64` dtypes

`` `ipython     In [21]: pd.DataFrame([1, 2], columns=['a']).dtypes     Out[21]:     a    int64     dtype: object      In [22]: pd.DataFrame({'a': [1, 2]}).dtypes     Out[22]:     a    int64     dtype: object      In [23]: pd.DataFrame({'a': 1}, index=range(2)).dtypes     Out[23]:     a    int64     dtype: object  Keep in mind that ``DataFrame(np.array(\[1,2\]))`**WILL** result in`int32`on 32-bit platforms!   **Upcasting gotchas**  Performing indexing operations on integer type data can easily upcast the data.`<span class="title-ref"> The dtype of the input data will be preserved in cases where </span><span class="title-ref">nans</span>\` are not introduced.

`` `ipython     In [24]: dfi = df3.astype('int32')      In [25]: dfi['D'] = dfi['D'].astype('int64')      In [26]: dfi     Out[26]:       A  B  C  D  E     0  0  0  0  1  1     1 -2  0  1  1  1     2 -2  0  2  1  1     3  0 -1  3  1  1     4  1  0  4  1  1     5  0  0  5  1  1     6  0 -1  6  1  1     7  0  0  7  1  1      In [27]: dfi.dtypes     Out[27]:     A    int32     B    int32     C    int32     D    int64     E    int32     dtype: object      In [28]: casted = dfi[dfi > 0]      In [29]: casted     Out[29]:         A   B    C  D  E     0  NaN NaN  NaN  1  1     1  NaN NaN  1.0  1  1     2  NaN NaN  2.0  1  1     3  NaN NaN  3.0  1  1     4  1.0 NaN  4.0  1  1     5  NaN NaN  5.0  1  1     6  NaN NaN  6.0  1  1     7  NaN NaN  7.0  1  1      In [30]: casted.dtypes     Out[30]:     A    float64     B    float64     C    float64     D      int64     E      int32     dtype: object  While float dtypes are unchanged.  .. code-block:: ipython      In [31]: df4 = df3.copy()      In [32]: df4['A'] = df4['A'].astype('float32')      In [33]: df4.dtypes     Out[33]:     A    float32     B    float64     C    float64     D    float16     E      int32     dtype: object      In [34]: casted = df4[df4 > 0]      In [35]: casted     Out[35]:               A         B    C    D  E     0       NaN       NaN  NaN  1.0  1     1       NaN  0.567020  1.0  1.0  1     2       NaN  0.276232  2.0  1.0  1     3       NaN       NaN  3.0  1.0  1     4  1.933792       NaN  4.0  1.0  1     5       NaN  0.113648  5.0  1.0  1     6       NaN       NaN  6.0  1.0  1     7       NaN  0.524988  7.0  1.0  1      In [36]: casted.dtypes     Out[36]:     A    float32     B    float64     C    float64     D    float16     E      int32     dtype: object  Datetimes conversion ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Datetime64\[ns\] columns in a DataFrame (or a Series) allow the use of `np.nan` to indicate a nan value, in addition to the traditional `NaT`, or not-a-time. This allows convenient nan setting in a generic way. Furthermore `datetime64[ns]` columns are created by default, when passed datetimelike objects (*this change was introduced in 0.10.1*) (`2809`, `2810`)

<div class="ipython">

python

  - df = pd.DataFrame(np.random.randn(6, 2), pd.date\_range('20010102', periods=6),  
    columns=\['A', ' B'\])

df\['timestamp'\] = pd.Timestamp('20010103') df

\# datetime64\[ns\] out of the box df.dtypes.value\_counts()

\# use the traditional nan, which is mapped to NaT internally df.loc\[df.index\[2:4\], \['A', 'timestamp'\]\] = np.nan df

</div>

Astype conversion on `datetime64[ns]` to `object`, implicitly converts `NaT` to `np.nan`

<div class="ipython">

python

import datetime s = pd.Series(\[datetime.datetime(2001, 1, 2, 0, 0) for i in range(3)\]) s.dtype s\[1\] = np.nan s s.dtype s = s.astype('O') s s.dtype

</div>

## API changes

>   - Added to\_series() method to indices, to facilitate the creation of indexers (`3275`)
>   - `HDFStore`
>       - added the method `select_column` to select a single column from a table as a Series.
>       - deprecated the `unique` method, can be replicated by `select_column(key,column).unique()`
>       - `min_itemsize` parameter to `append` will now automatically create data\_columns for passed keys

## Enhancements

>   - Improved performance of df.to\_csv() by up to 10x in some cases. (`3059`)
> 
>   - Numexpr is now a \[Recommended Dependencies \<install.recommended\_dependencies\>\](\#recommended-dependencies-\<install.recommended\_dependencies\>), to accelerate certain types of numerical and boolean operations
> 
>   - Bottleneck is now a \[Recommended Dependencies \<install.recommended\_dependencies\>\](\#recommended-dependencies-\<install.recommended\_dependencies\>), to accelerate certain types of `nan` operations
> 
>   - `HDFStore`
>     
>       - support `read_hdf/to_hdf` API similar to `read_csv/to_csv`
>         
>         <div class="ipython">
>         
>         python
>         
>         df = pd.DataFrame({'A': range(5), 'B': range(5)}) df.to\_hdf('store.h5', key='table', append=True) pd.read\_hdf('store.h5', 'table', where=\['index \> 2'\])
>         
>         </div>
>         
>         <div class="ipython" data-suppress="" data-okexcept="">
>         
>         python
>         
>         import os
>         
>         os.remove('store.h5')
>         
>         </div>
>     
>       - provide dotted attribute access to `get` from stores, e.g. `store.df == store['df']`
>     
>       - new keywords `iterator=boolean`, and `chunksize=number_in_a_chunk` are provided to support iteration on `select` and `select_as_multiple` (`3076`)
> 
>   - You can now select timestamps from an *unordered* timeseries similarly to an *ordered* timeseries (`2437`)
> 
>   - You can now select with a string from a DataFrame with a datelike index, in a similar way to a Series (`3070`)
>     
>       - \`\`\`ipython  
>         In \[30\]: idx = pd.date\_range("2001-10-1", periods=5, freq='M')
>         
>         In \[31\]: ts = pd.Series(np.random.rand(len(idx)), index=idx)
>         
>         In \[32\]: ts\['2001'\] Out\[32\]: 2001-10-31 0.117967 2001-11-30 0.702184 2001-12-31 0.414034 Freq: M, dtype: float64
>         
>         In \[33\]: df = pd.DataFrame({'A': ts})
>         
>         In \[34\]: df\['2001'\] Out\[34\]: A 2001-10-31 0.117967 2001-11-30 0.702184 2001-12-31 0.414034
> 
>   - `Squeeze` to possibly remove length 1 dimensions from an object.
>     
>     ``` python
>     >>> p = pd.Panel(np.random.randn(3, 4, 4), items=['ItemA', 'ItemB', 'ItemC'],
>     ...              major_axis=pd.date_range('20010102', periods=4),
>     ...              minor_axis=['A', 'B', 'C', 'D'])
>     >>> p
>     <class 'pandas.core.panel.Panel'>
>     Dimensions: 3 (items) x 4 (major_axis) x 4 (minor_axis)
>     Items axis: ItemA to ItemC
>     Major_axis axis: 2001-01-02 00:00:00 to 2001-01-05 00:00:00
>     Minor_axis axis: A to D
>     
>     >>> p.reindex(items=['ItemA']).squeeze()
>                        A         B         C         D
>     2001-01-02  0.926089 -2.026458  0.501277 -0.204683
>     2001-01-03 -0.076524  1.081161  1.141361  0.479243
>     2001-01-04  0.641817 -0.185352  1.824568  0.809152
>     2001-01-05  0.575237  0.669934  1.398014 -0.399338
>     
>     >>> p.reindex(items=['ItemA'], minor=['B']).squeeze()
>     2001-01-02   -2.026458
>     2001-01-03    1.081161
>     2001-01-04   -0.185352
>     2001-01-05    0.669934
>     Freq: D, Name: B, dtype: float64
>     ```
> 
>   - In `pd.io.data.Options`,
>     
>       - Fix bug when trying to fetch data for the current month when already past expiry.
>       - Now using lxml to scrape html instead of BeautifulSoup (lxml was faster).
>       - New instance variables for calls and puts are automatically created when a method that creates them is called. This works for current month where the instance variables are simply `calls` and `puts`. Also works for future expiry months and save the instance variable as `callsMMYY` or `putsMMYY`, where `MMYY` are, respectively, the month and year of the option's expiry.
>       - `Options.get_near_stock_price` now allows the user to specify the month for which to get relevant options data.
>       - `Options.get_forward_data` now has optional kwargs `near` and `above_below`. This allows the user to specify if they would like to only return forward looking data for options near the current stock price. This just obtains the data from Options.get\_near\_stock\_price instead of Options.get\_xxx\_data() (`2758`).
> 
>   - Cursor coordinate information is now displayed in time-series plots.
> 
>   - added option `display.max_seq_items` to control the number of elements printed per sequence pprinting it. (`2979`)
> 
>   - added option `display.chop_threshold` to control display of small numerical values. (`2739`)
> 
>   - added option `display.max_info_rows` to prevent verbose\_info from being calculated for frames above 1M rows (configurable). (`2807`, `2918`)
> 
>   - value\_counts() now accepts a "normalize" argument, for normalized histograms. (`2710`).
> 
>   - DataFrame.from\_records now accepts not only dicts but any instance of the collections.Mapping ABC.
> 
>   - added option `display.mpl_style` providing a sleeker visual style for plots. Based on <https://gist.github.com/huyng/816622> (`3075`).
> 
>   - Treat boolean values as integers (values 1 and 0) for numeric operations. (`2641`)
> 
>   - to\_html() now accepts an optional "escape" argument to control reserved HTML character escaping (enabled by default) and escapes `&`, in addition to `<` and `>`. (`2919`)

See the \[full release notes \](\#full-release-notes )\`<span class="title-ref"> \<release\></span> or issue tracker on GitHub for a complete list.

## Contributors

<div class="contributors">

v0.10.1..v0.11.0

</div>

---

v0.12.0.md

---

# Version 0.12.0 (July 24, 2013)

{{ header }}

This is a major release from 0.11.0 and includes several new features and enhancements along with a large number of bug fixes.

Highlights include a consistent I/O API naming scheme, routines to read html, write MultiIndexes to csv files, read & write STATA data files, read & write JSON format files, Python 3 support for `HDFStore`, filtering of groupby expressions via `filter`, and a revamped `replace` routine that accepts regular expressions.

## API changes

>   - The I/O API is now much more consistent with a set of top level `reader` functions accessed like `pd.read_csv()` that generally return a `pandas` object.
>     
>       - `read_csv`
>       - `read_excel`
>       - `read_hdf`
>       - `read_sql`
>       - `read_json`
>       - `read_html`
>       - `read_stata`
>       - `read_clipboard`
>     
>     The corresponding `writer` functions are object methods that are accessed like `df.to_csv()`
>     
>       - `to_csv`
>       - `to_excel`
>       - `to_hdf`
>       - `to_sql`
>       - `to_json`
>       - `to_html`
>       - `to_stata`
>       - `to_clipboard`
> 
>   - Fix modulo and integer division on Series,DataFrames to act similarly to `float` dtypes to return `np.nan` or `np.inf` as appropriate (`3590`). This correct a numpy bug that treats `integer` and `float` dtypes differently.
>     
>     <div class="ipython">
>     
>     python
>     
>     p = pd.DataFrame({"first": \[4, 5, 8\], "second": \[0, 0, 3\]}) p % 0 p % p p / p p / 0
>     
>     </div>
> 
>   - Add `squeeze` keyword to `groupby` to allow reduction from DataFrame -\> Series if groups are unique. This is a Regression from 0.10.1. We are reverting back to the prior behavior. This means groupby will return the same shaped objects whether the groups are unique or not. Revert this issue (`2893`) with (`3596`).
>     
>       - \`\`\`ipython
>         
>           - In \[2\]: df2 = pd.DataFrame(\[{"val1": 1, "val2": 20},  
>             ...: {"val1": 1, "val2": 19}, ...: {"val1": 1, "val2": 27}, ...: {"val1": 1, "val2": 12}\])
>         
>           - In \[3\]: def func(dataf):  
>             ...: return dataf\["val2"\] - dataf\["val2"\].mean() ...:
>         
>           - In \[4\]: \# squeezing the result frame to a series (because we have unique groups)  
>             ...: df2.groupby("val1", squeeze=True).apply(func)
>         
>         Out\[4\]: 0 0.5 1 -0.5 2 7.5 3 -7.5 Name: 1, dtype: float64
>         
>           - In \[5\]: \# no squeezing (the default, and behavior in 0.10.1)  
>             ...: df2.groupby("val1").apply(func)
>         
>         Out\[5\]: val2 0 1 2 3 val1 1 0.5 -0.5 7.5 -7.5
> 
>   - Raise on `iloc` when boolean indexing with a label based indexer mask e.g. a boolean Series, even with integer labels, will raise. Since `iloc` is purely positional based, the labels on the Series are not alignable (`3631`)
>     
>     This case is rarely used, and there are plenty of alternatives. This preserves the `iloc` API to be *purely* positional based.
>     
>     <div class="ipython">
>     
>     python
>     
>     df = pd.DataFrame(range(5), index=list("ABCDE"), columns=\["a"\]) mask = df.a % 2 == 0 mask
>     
>     \# this is what you should use df.loc\[mask\]
>     
>     \# this will work as well df.iloc\[mask.values\]
>     
>     </div>
>     
>     `df.iloc[mask]` will raise a `ValueError`
> 
>   - The `raise_on_error` argument to plotting functions is removed. Instead, plotting functions raise a `TypeError` when the `dtype` of the object is `object` to remind you to avoid `object` arrays whenever possible and thus you should cast to an appropriate numeric dtype if you need to plot something.
> 
>   - Add `colormap` keyword to DataFrame plotting methods. Accepts either a matplotlib colormap object (ie, matplotlib.cm.jet) or a string name of such an object (ie, 'jet'). The colormap is sampled to select the color for each column. Please see \[visualization.colormaps\](\#visualization.colormaps) for more information. (`3860`)
> 
>   - `DataFrame.interpolate()` is now deprecated. Please use `DataFrame.fillna()` and `DataFrame.replace()` instead. (`3582`, `3675`, `3676`)
> 
>   - the `method` and `axis` arguments of `DataFrame.replace()` are deprecated
> 
>   - `DataFrame.replace` 's `infer_types` parameter is removed and now performs conversion by default. (`3907`)
> 
>   - Add the keyword `allow_duplicates` to `DataFrame.insert` to allow a duplicate column to be inserted if `True`, default is `False` (same as prior to 0.12) (`3679`)
> 
>   - Implement `__nonzero__` for `NDFrame` objects (`3691`, `3696`)
> 
>   - IO API
>     
>       - Added top-level function `read_excel` to replace the following, The original API is deprecated and will be removed in a future version
>         
>         ``` python
>         from pandas.io.parsers import ExcelFile
>         
>         xls = ExcelFile("path_to_file.xls")
>         xls.parse("Sheet1", index_col=None, na_values=["NA"])
>         ```
>         
>         With
>         
>         ``` python
>         import pandas as pd
>         
>         pd.read_excel("path_to_file.xls", "Sheet1", index_col=None, na_values=["NA"])
>         ```
>     
>       - Added top-level function `read_sql` that is equivalent to the following
>         
>         ``` python
>         from pandas.io.sql import read_frame
>         
>         read_frame(...)
>         ```
> 
>   - `DataFrame.to_html` and `DataFrame.to_latex` now accept a path for their first argument (`3702`)
> 
>   - Do not allow astypes on `datetime64[ns]` except to `object`, and `timedelta64[ns]` to `object/int` (`3425`)
> 
>   - The behavior of `datetime64` dtypes has changed with respect to certain so-called reduction operations (`3726`). The following operations now raise a `TypeError` when performed on a `Series` and return an *empty* `Series` when performed on a `DataFrame` similar to performing these operations on, for example, a `DataFrame` of `slice` objects:
>     
>       - sum, prod, mean, std, var, skew, kurt, corr, and cov
> 
>   - `read_html` now defaults to `None` when reading, and falls back on `bs4` + `html5lib` when lxml fails to parse. a list of parsers to try until success is also valid
> 
>   - The internal `pandas` class hierarchy has changed (slightly). The previous `PandasObject` now is called `PandasContainer` and a new `PandasObject` has become the base class for `PandasContainer` as well as `Index`, `Categorical`, `GroupBy`, `SparseList`, and `SparseArray` (+ their base classes). Currently, `PandasObject` provides string methods (from `StringMixin`). (`4090`, `4092`)
> 
>   - New `StringMixin` that, given a `__unicode__` method, gets python 2 and python 3 compatible string methods (`__str__`, `__bytes__`, and `__repr__`). Plus string safety throughout. Now employed in many places throughout the pandas library. (`4090`, `4092`)

IO enhancements `` ` ~~~~~~~~~~~~~~~    - ``pd.read\_html()``can now parse HTML strings, files or urls and return     DataFrames, courtesy of @cpcloud. (:issue:`3477`, :issue:`3605`, :issue:`3606`, :issue:`3616`).     It works with a *single* parser backend: BeautifulSoup4 + html5lib [See the docs<io.html>](#see-the-docs<io.html>)      You can use``pd.read\_html()`to read the output from`DataFrame.to\_html()`like so      .. ipython:: python          import io         df = pd.DataFrame({"a": range(3), "b": list("abc")})         print(df)         html = df.to_html()         alist = pd.read_html(io.StringIO(html), index_col=0)         print(df == alist[0])      Note that`alist`here is a Python`list`so`pd.read\_html()`and`DataFrame.to\_html()`are not inverses.      -`pd.read\_html()``no longer performs hard conversion of date strings       (:issue:`3656`).      > **Warning** >        You may have to install an older version of BeautifulSoup4,       [See the installation docs<install.optional_dependencies>](#see-the-installation-docs<install.optional_dependencies>)    - Added module for reading and writing Stata files:``pandas.io.stata``(:issue:`1512`)     accessible via``read\_stata`top-level function for reading,     and`to\_stata`DataFrame method for writing, [See the docs<io.stata>](#see-the-docs<io.stata>)    - Added module for reading and writing json format files:`pandas.io.json`accessible via`read\_json`top-level function for reading,     and`to\_json``DataFrame method for writing, [See the docs<io.json>](#see-the-docs<io.json>)     various issues (:issue:`1226`, :issue:`3804`, :issue:`3876`, :issue:`3867`, :issue:`1305`)    -``MultiIndex`column support for reading and writing csv format files      - The`header`option in`read\_csv`now accepts a       list of the rows from which to read the index.      - The option,`tupleize\_cols`can now be specified in both`to\_csv`and`read\_csv`, to provide compatibility for the pre 0.12 behavior of       writing and reading`MultIndex`columns via a list of tuples. The default in       0.12 is to write lists of tuples and *not* interpret list of tuples as a`MultiIndex`column.        Note: The default behavior in 0.12 remains unchanged from prior versions, but starting with 0.13,       the default *to* write and read`MultiIndex``columns will be in the new       format. (:issue:`3571`, :issue:`1651`, :issue:`3141`)      - If an``index\_col`is not specified (e.g. you don't have an index, or wrote it       with`df.to\_csv(..., index=False`), then any`names`on the columns index will       be *lost*.        .. ipython:: python           mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list("abcd")], names=list("ab"))          mi_col = pd.MultiIndex.from_arrays([[1, 2], list("ab")], names=list("cd"))          df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)          df.to_csv("mi.csv")          print(open("mi.csv").read())          pd.read_csv("mi.csv", header=[0, 1, 2, 3], index_col=[0, 1])        .. ipython:: python          :suppress:           import os           os.remove("mi.csv")    - Support for`HDFStore`(via`PyTables 3.0.0`) on Python3    - Iterator support via`read\_hdf`that automatically opens and closes the     store when iteration is finished. This is only for *tables*`\`ipython In \[25\]: path = 'store\_iterator.h5'

> In \[26\]: pd.DataFrame(np.random.randn(10, 2)).to\_hdf(path, 'df', table=True)
> 
>   - In \[27\]: for df in pd.read\_hdf(path, 'df', chunksize=3):  
>     ....: print(df) ....: 0 1
> 
> 0 0.713216 -0.778461 1 -0.661062 0.862877 2 0.344342 0.149565 0 1 3 -0.626968 -0.875772 4 -0.930687 -0.218983 5 0.949965 -0.442354 0 1 6 -0.402985 1.111358 7 -0.241527 -0.670477 8 0.049355 0.632633 0 1 9 -1.502767 -1.225492
> 
> \- `read_csv` will now throw a more informative error message when a file contains no columns, e.g., all newline characters

Other enhancements `` ` ~~~~~~~~~~~~~~~~~~    - ``DataFrame.replace()`now allows regular expressions on contained`Series`with object dtype. See the examples section in the regular docs     [Replacing via String Expression <missing_data.replace_expression>](#replacing-via-string-expression-<missing_data.replace_expression>)      For example you can do      .. ipython:: python          df = pd.DataFrame({"a": list("ab.."), "b": [1, 2, 3, 4]})         df.replace(regex=r"\s*\.\s*", value=np.nan)      to replace all occurrences of the string`'.'`with zero or more     instances of surrounding white space with`NaN`.      Regular string replacement still works as expected. For example, you can do      .. ipython:: python          df.replace(".", np.nan)      to replace all occurrences of the string`'.'`with`NaN`.    -`pd.melt()`now accepts the optional parameters`var\_name`and`value\_name`to specify custom column names of the returned DataFrame.    -`pd.set\_option()``now allows N option, value pairs (:issue:`3667`).      Let's say that we had an option``'a.b'`and another option`'b.c'`.     We can set them at the same time:`\`ipython In \[31\]: pd.get\_option('a.b') Out\[31\]: 2

> In \[32\]: pd.get\_option('b.c') Out\[32\]: 3
> 
> In \[33\]: pd.set\_option('a.b', 1, 'b.c', 4)
> 
> In \[34\]: pd.get\_option('a.b') Out\[34\]: 1
> 
> In \[35\]: pd.get\_option('b.c') Out\[35\]: 4
> 
> \- The `filter` method for group objects returns a subset of the original object. Suppose we want to take only elements that belong to groups with a group sum greater than 2.
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> sf = pd.Series(\[1, 1, 2, 3, 3, 3\]) sf.groupby(sf).filter(lambda x: x.sum() \> 2)
> 
> The argument of `filter` must a function that, applied to the group as a whole, returns `True` or `False`.
> 
> Another useful operation is filtering out elements that belong to groups with only a couple members.
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> dff = pd.DataFrame({"A": np.arange(8), "B": list("aabbbbcc")}) dff.groupby("B").filter(lambda x: len(x) \> 2)
> 
> Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs.
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> dff.groupby("B").filter(lambda x: len(x) \> 2, dropna=False)
> 
>   - Series and DataFrame hist methods now take a `figsize` argument (`3834`)
> 
> \- DatetimeIndexes no longer try to convert mixed-integer indexes during join operations (`3877`)
> 
> \- Timestamp.min and Timestamp.max now represent valid Timestamp instances instead of the default datetime.min and datetime.max (respectively), thanks @SleepingPills
> 
> \- `read_html` now raises when no tables are found and BeautifulSoup==4.2.0 is detected (`4214`)

Experimental features `` ` ~~~~~~~~~~~~~~~~~~~~~    - Added experimental ``CustomBusinessDay`class to support`DateOffsets``with custom holiday calendars and custom weekmasks. (:issue:`2301`)      > **Note** >         This uses the``numpy.busdaycalendar`API introduced in Numpy 1.7 and        therefore requires Numpy 1.7.0 or newer.      .. ipython:: python        from pandas.tseries.offsets import CustomBusinessDay       from datetime import datetime        # As an interesting example, let's look at Egypt where       # a Friday-Saturday weekend is observed.       weekmask_egypt = "Sun Mon Tue Wed Thu"       # They also observe International Workers' Day so let's       # add that for a couple of years       holidays = ["2012-05-01", datetime(2013, 5, 1), np.datetime64("2014-05-01")]       bday_egypt = CustomBusinessDay(holidays=holidays, weekmask=weekmask_egypt)       dt = datetime(2013, 4, 30)       print(dt + 2 * bday_egypt)       dts = pd.date_range(dt, periods=5, freq=bday_egypt)       print(pd.Series(dts.weekday, dts).map(pd.Series("Mon Tue Wed Thu Fri Sat Sun".split())))  Bug fixes ~~~~~~~~~    - Plotting functions now raise a`TypeError`before trying to plot anything     if the associated objects have a dtype of`object``(:issue:`1818`,     :issue:`3572`, :issue:`3911`, :issue:`3912`), but they will try to convert object arrays to     numeric arrays if possible so that you can still plot, for example, an     object array with floats. This happens before any drawing takes place which     eliminates any spurious plots from showing up.    -``fillna`methods now raise a`TypeError`if the`value`parameter is     a list or tuple.    -`Series.str``now supports iteration (:issue:`3638`). You can iterate over the     individual elements of each string in the``Series`. Each iteration yields     a`Series`with either a single character at each index of the original`Series`or`NaN`. For example,`\`ipython In \[38\]: strs = "go", "bow", "joe", "slow"

> In \[32\]: ds = pd.Series(strs)
> 
>   - In \[33\]: for s in ds.str:  
>     ...: print(s)
> 
> 0 g 1 b 2 j 3 s dtype: object 0 o 1 o 2 o 3 l dtype: object 0 NaN 1 w 2 e 3 o dtype: object 0 NaN 1 NaN 2 NaN 3 w dtype: object
> 
> In \[41\]: s Out\[41\]: 0 NaN 1 NaN 2 NaN 3 w dtype: object
> 
> In \[42\]: s.dropna().values.item() == "w" Out\[42\]: True
> 
> The last element yielded by the iterator will be a `Series` containing the last element of the longest string in the `Series` with all other elements being `NaN`. Here since `'slow'` is the longest string and there are no other strings with the same length `'w'` is the only non-null string in the yielded `Series`.
> 
>   - `HDFStore`
>   - Will retain index attributes (freq,tz,name) on recreation (`3499`)
> 
> \- Will warn with a `AttributeConflictWarning` if you are attempting to append an index with a different frequency than the existing, or attempting to append an index with a different name than the existing - Support datelike columns with a timezone as data\_columns (`2852`)
> 
>   - Non-unique index support clarified (`3468`).
>   - Fix assigning a new index to a duplicate index in a DataFrame would fail (`3468`)
>   - Fix construction of a DataFrame with a duplicate index
> 
> \- ref\_locs support to allow duplicative indices across dtypes, allows iget support to always find the index (even across dtypes) (`2194`) - applymap on a DataFrame with a non-unique index now works (removed warning) (`2786`), and fix (`3230`) - Fix to\_csv to handle non-unique columns (`3495`) - Duplicate indexes with getitem will return items in the correct order (`3455`, `3457`) and handle missing elements like unique indices (`3561`) - Duplicate indexes with and empty DataFrame.from\_records will return a correct frame (`3562`) - Concat to produce a non-unique columns when duplicates are across dtypes is fixed (`3602`) - Allow insert/delete to non-unique columns (`3679`) - Non-unique indexing with a slice via `loc` and friends fixed (`3659`) - Allow insert/delete to non-unique columns (`3679`) - Extend `reindex` to correctly deal with non-unique indices (`3679`) - `DataFrame.itertuples()` now works with frames with duplicate column names (`3873`) - Bug in non-unique indexing via `iloc` (`4017`); added `takeable` argument to `reindex` for location-based taking - Allow non-unique indexing in series via `.ix/.loc` and `__getitem__` (`4246`) - Fixed non-unique indexing memory allocation issue with `.ix/.loc` (`4280`)
> 
>   - `DataFrame.from_records` did not accept empty recarrays (`3682`)
>   - `read_html` now correctly skips tests (`3741`)
> 
> \- Fixed a bug where `DataFrame.replace` with a compiled regular expression in the `to_replace` argument wasn't working (`3907`) - Improved `network` test decorator to catch `IOError` (and therefore `URLError` as well). Added `with_connectivity_check` decorator to allow explicitly checking a website as a proxy for seeing if there is network connectivity. Plus, new `optional_args` decorator factory for decorators. (`3910`, `3914`) - Fixed testing issue where too many sockets where open thus leading to a connection reset issue (`3982`, `3985`, `4028`, `4054`) - Fixed failing tests in test\_yahoo, test\_google where symbols were not retrieved but were being accessed (`3982`, `3985`, `4028`, `4054`) - `Series.hist` will now take the figure from the current environment if one is not passed - Fixed bug where a 1xN DataFrame would barf on a 1xN mask (`4071`) - Fixed running of `tox` under python3 where the pickle import was getting rewritten in an incompatible way (`4062`, `4063`) - Fixed bug where sharex and sharey were not being passed to grouped\_hist (`4089`) - Fixed bug in `DataFrame.replace` where a nested dict wasn't being iterated over when regex=False (`4115`) - Fixed bug in the parsing of microseconds when using the `format` argument in `to_datetime` (`4152`) - Fixed bug in `PandasAutoDateLocator` where `invert_xaxis` triggered incorrectly `MilliSecondLocator` (`3990`) - Fixed bug in plotting that wasn't raising on invalid colormap for matplotlib 1.1.1 (`4215`) - Fixed the legend displaying in `DataFrame.plot(kind='kde')` (`4216`) - Fixed bug where Index slices weren't carrying the name attribute (`4226`) - Fixed bug in initializing `DatetimeIndex` with an array of strings in a certain time zone (`4229`) - Fixed bug where html5lib wasn't being properly skipped (`4265`) - Fixed bug where get\_data\_famafrench wasn't using the correct file edges (`4281`)

See the \[full release notes \](\#full-release-notes )\`<span class="title-ref"> \<release\></span> or issue tracker on GitHub for a complete list.

## Contributors

<div class="contributors">

v0.11.0..v0.12.0

</div>

---

v0.13.0.md

---

# Version 0.13.0 (January 3, 2014)

{{ header }}

This is a major release from 0.12.0 and includes a number of API changes, several new features and enhancements along with a large number of bug fixes.

Highlights include:

  - support for a new index type `Float64Index`, and other Indexing enhancements
  - `HDFStore` has a new string based syntax for query specification
  - support for new methods of interpolation
  - updated `timedelta` operations
  - a new string manipulation method `extract`
  - Nanosecond support for Offsets
  - `isin` for DataFrames

Several experimental features are added, including:

  - new `eval/query` methods for expression evaluation
  - support for `msgpack` serialization
  - an i/o interface to Google's `BigQuery`

Their are several new or updated docs sections including:

  - \[Comparison with SQL\<compare\_with\_sql\>\](\#comparison-with-sql\<compare\_with\_sql\>), which should be useful for those familiar with SQL but still learning pandas.
  - \[Comparison with R\<compare\_with\_r\>\](\#comparison-with-r\<compare\_with\_r\>), idiom translations from R to pandas.
  - \[Enhancing Performance\<enhancingperf\>\](\#enhancing-performance\<enhancingperf\>), ways to enhance pandas performance with `eval/query`.

\> **Warning** \> In 0.13.0 `Series` has internally been refactored to no longer sub-class `ndarray` but instead subclass `NDFrame`, similar to the rest of the pandas containers. This should be a transparent change with only very limited API implications. See \[Internal Refactoring\<whatsnew\_0130.refactoring\>\](\#internal-refactoring\<whatsnew\_0130.refactoring\>)

## API changes

  - `read_excel` now supports an integer in its `sheetname` argument giving the index of the sheet to read in (`4301`).

  - Text parser now treats anything that reads like inf ("inf", "Inf", "-Inf", "iNf", etc.) as infinity. (`4220`, `4219`), affecting `read_table`, `read_csv`, etc.

  - `pandas` now is Python 2/3 compatible without the need for 2to3 thanks to
    
    @jtratner. As a result, pandas now uses iterators more extensively. This also led to the introduction of substantive parts of the Benjamin Peterson's `six` library into compat. (`4384`, `4375`, `4372`)

  - `pandas.util.compat` and `pandas.util.py3compat` have been merged into `pandas.compat`. `pandas.compat` now includes many functions allowing 2/3 compatibility. It contains both list and iterator versions of range, filter, map and zip, plus other necessary elements for Python 3 compatibility. `lmap`, `lzip`, `lrange` and `lfilter` all produce lists instead of iterators, for compatibility with `numpy`, subscripting and `pandas` constructors.(`4384`, `4375`, `4372`)

  - `Series.get` with negative indexers now returns the same as `[]` (`4390`)

  - Changes to how `Index` and `MultiIndex` handle metadata (`levels`, `labels`, and `names`) (`4039`):
    
      - \`\`\`python  
        \# previously, you would have set levels or labels directly \>\>\> pd.index.levels = \[\[1, 2, 3, 4\], \[1, 2, 4, 4\]\]
        
        \# now, you use the set\_levels or set\_labels methods \>\>\> index = pd.index.set\_levels(\[\[1, 2, 3, 4\], \[1, 2, 4, 4\]\])
        
        \# similarly, for names, you can rename the object \# but setting names is not deprecated \>\>\> index = pd.index.set\_names(\["bob", "cranberry"\])
        
        \# and all methods take an inplace kwarg - but return None \>\>\> pd.index.set\_names(\["bob", "cranberry"\], inplace=True)

  - **All** division with `NDFrame` objects is now *truedivision*, regardless of the future import. This means that operating on pandas objects will by default use *floating point* division, and return a floating point dtype. You can use `//` and `floordiv` to do integer division.
    
    Integer division
    
    ``` ipython
    In [3]: arr = np.array([1, 2, 3, 4])
    
    In [4]: arr2 = np.array([5, 3, 2, 1])
    
    In [5]: arr / arr2
    Out[5]: array([0, 0, 1, 4])
    
    In [6]: pd.Series(arr) // pd.Series(arr2)
    Out[6]:
    0    0
    1    0
    2    1
    3    4
    dtype: int64
    ```
    
    True Division
    
    ``` ipython
    In [7]: pd.Series(arr) / pd.Series(arr2)  # no future import required
    Out[7]:
    0    0.200000
    1    0.666667
    2    1.500000
    3    4.000000
    dtype: float64
    ```

\- Infer and downcast dtype if `downcast='infer'` is passed to `fillna/ffill/bfill` (`4604`) `` ` - ``\_\_nonzero\_\_`for all NDFrame objects, will now raise a`ValueError``, this reverts back to (:issue:`1073`, :issue:`4633`)   behavior. See [gotchas<gotchas.truth>](#gotchas<gotchas.truth>) for a more detailed discussion.    This prevents doing boolean comparison on *entire* pandas objects, which is inherently ambiguous. These all will raise a``ValueError`.`\`python \>\>\> df = pd.DataFrame({'A': np.random.randn(10), ... 'B': np.random.randn(10), ... 'C': pd.date\_range('20130101', periods=10) ... }) ... \>\>\> if df: ... pass ... Traceback (most recent call last): ... ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

> \>\>\> df1 = df \>\>\> df2 = df \>\>\> df1 and df2 Traceback (most recent call last): ... ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
> 
> \>\>\> d = \[1, 2, 3\] \>\>\> s1 = pd.Series(d) \>\>\> s2 = pd.Series(d) \>\>\> s1 and s2 Traceback (most recent call last): ... ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
> 
> Added the `.bool()` method to `NDFrame` objects to facilitate evaluating of single-element boolean Series:
> 
> ``` python
> ```
> 
>   - \>\>\> pd.Series(\[True\]).bool()  
>     True
> 
>   - \>\>\> pd.Series(\[False\]).bool()  
>     False
> 
>   - \>\>\> pd.DataFrame(\[\[True\]\]).bool()  
>     True
> 
>   - \>\>\> pd.DataFrame(\[\[False\]\]).bool()  
>     False

  - \- All non-Index NDFrames (`Series`, `DataFrame`, `Panel`, `Panel4D`,  
    `SparsePanel`, etc.), now support the entire set of arithmetic operators and arithmetic flex methods (add, sub, mul, etc.). `SparsePanel` does not support `pow` or `mod` with non-scalars. (`3765`)

  - `` ` - ``Series`and`DataFrame`now have a`mode()``method to calculate the   statistical mode(s) by axis/Series. (:issue:`5367`)  - Chained assignment will now by default warn if the user is assigning to a copy. This can be changed   with the option``mode.chained\_assignment`, allowed options are`raise/warn/None`.    .. ipython:: python       dfc = pd.DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})      pd.set_option('chained_assignment', 'warn')    The following warning / exception will show if this is attempted.    .. ipython:: python      :okwarning:       dfc.loc[0]['A'] = 1111    ::       Traceback (most recent call last)         ...      SettingWithCopyWarning:         A value is trying to be set on a copy of a slice from a DataFrame.         Try using .loc[row_index,col_indexer] = value instead    Here is the correct method of assignment.    .. ipython:: python       dfc.loc[0, 'A'] = 11      dfc  -`Panel.reindex`has the following call signature`Panel.reindex(items=None, major\_axis=None, minor\_axis=None, \*\*kwargs)`to conform with other`NDFrame`objects. See [Internal Refactoring<whatsnew_0130.refactoring>](#internal-refactoring<whatsnew_0130.refactoring>) for more information.  -`Series.argmin`and`Series.argmax`are now aliased to`Series.idxmin`and`Series.idxmax``. These return the *index* of the    min or max element respectively. Prior to 0.13.0 these would return the position of the min / max element. (:issue:`6214`)  Prior version deprecations/changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  These were announced changes in 0.12 or prior that are taking effect as of 0.13.0  - Remove deprecated``Factor``(:issue:`3650`) - Remove deprecated``set\_printoptions/reset\_printoptions``(:issue:`3046`) - Remove deprecated``\_verbose\_info``(:issue:`3215`) - Remove deprecated``read\_clipboard/to\_clipboard/ExcelFile/ExcelWriter`from`pandas.io.parsers``(:issue:`3717`)   These are available as functions in the main pandas namespace (e.g.``pd.read\_clipboard`) - default for`tupleize\_cols`is now`False`for both`to\_csv`and`read\_csv``. Fair warning in 0.12 (:issue:`3604`) - default for``display.max\_seq\_len`is now 100 rather than`None``. This activates   truncated display ("...") of long sequences in various places. (:issue:`3391`)  Deprecations ~~~~~~~~~~~~  Deprecated in 0.13.0  - deprecated``iterkv`, which will be removed in a future release (this was   an alias of iteritems used to bypass`2to3``'s changes).   (:issue:`4384`, :issue:`4375`, :issue:`4372`) - deprecated the string method``match`, whose role is now performed more   idiomatically by`extract`. In a future release, the default behavior   of`match`will change to become analogous to`contains`, which returns   a boolean indexer. (Their   distinction is strictness:`match`relies on`re.match`while`contains`relies on`re.search`.) In this release, the deprecated   behavior is the default, but the new behavior is available through the   keyword argument`as\_indexer=True`.  Indexing API changes ~~~~~~~~~~~~~~~~~~~~  Prior to 0.13, it was impossible to use a label indexer (`.loc/.ix``) to set a value that was not contained in the index of a particular axis. (:issue:`2578`). See [the docs<indexing.basics.partial_setting>](#the-docs<indexing.basics.partial_setting>)  In the``Series`case this is effectively an appending operation  .. ipython:: python     s = pd.Series([1, 2, 3])    s    s[5] = 5.    s  .. ipython:: python     dfi = pd.DataFrame(np.arange(6).reshape(3, 2),                       columns=['A', 'B'])    dfi  This would previously`KeyError`.. ipython:: python     dfi.loc[:, 'C'] = dfi.loc[:, 'A']    dfi  This is like an`append`operation.  .. ipython:: python     dfi.loc[3] = 5    dfi  A Panel setting operation on an arbitrary axis aligns the input to the Panel`\`ipython
    
      - In \[20\]: p = pd.Panel(np.arange(16).reshape(2, 4, 2),  
        ....: items=\['Item1', 'Item2'\], ....: major\_axis=pd.date\_range('2001/1/12', periods=4), ....: minor\_axis=\['A', 'B'\], dtype='float64') ....:
    
    In \[21\]: p Out\[21\]: \<class 'pandas.core.panel.Panel'\> Dimensions: 2 (items) x 4 (major\_axis) x 2 (minor\_axis) Items axis: Item1 to Item2 Major\_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00 Minor\_axis axis: A to B
    
    In \[22\]: p.loc\[:, :, 'C'\] = pd.Series(\[30, 32\], index=p.items)
    
    In \[23\]: p Out\[23\]: \<class 'pandas.core.panel.Panel'\> Dimensions: 2 (items) x 4 (major\_axis) x 3 (minor\_axis) Items axis: Item1 to Item2 Major\_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00 Minor\_axis axis: A to C
    
    In \[24\]: p.loc\[:, :, 'C'\] Out\[24\]: Item1 Item2 2001-01-12 30.0 32.0 2001-01-13 30.0 32.0 2001-01-14 30.0 32.0 2001-01-15 30.0 32.0

Float64Index API change `` ` ~~~~~~~~~~~~~~~~~~~~~~~  - Added a new index type, ``Float64Index`. This will be automatically created when passing floating values in index creation.   This enables a pure label-based slicing paradigm that makes`\[\],ix,loc``for scalar indexing and slicing work exactly the   same. (:issue:`263`)    Construction is by default for floating type values.    .. ipython:: python       index = pd.Index([1.5, 2, 3, 4.5, 5])      index      s = pd.Series(range(5), index=index)      s    Scalar selection for``\[\],.ix,.loc`will always be label based. An integer will match an equal float index (e.g.`3`is equivalent to`3.0`)    .. ipython:: python       s[3]      s.loc[3]    The only positional indexing is via`iloc`.. ipython:: python       s.iloc[3]    A scalar index that is not found will raise`KeyError`Slicing is ALWAYS on the values of the index, for`\[\],ix,loc`and ALWAYS positional with`iloc`.. ipython:: python      :okwarning:       s.loc[2:4]      s.iloc[2:4]    In float indexes, slicing using floats are allowed    .. ipython:: python       s[2.1:4.6]      s.loc[2.1:4.6]  - Indexing on other index types are preserved (and positional fallback for`\[\],ix`), with the exception, that floating point slicing   on indexes on non`Float64Index`will now raise a`TypeError`.`\`ipython In \[1\]: pd.Series(range(5))\[3.5\] TypeError: the label \[3.5\] is not a proper indexer for this index type (Int64Index)

> In \[1\]: pd.Series(range(5))\[3.5:4.5\] TypeError: the slice start \[3.5\] is not a proper indexer for this index type (Int64Index)
> 
> Using a scalar float indexer will be deprecated in a future version, but is allowed for now.
> 
> ``` ipython
> ```
> 
> In \[3\]: pd.Series(range(5))\[3.0\] Out\[3\]: 3

HDFStore API changes `` ` ~~~~~~~~~~~~~~~~~~~~  - Query Format Changes. A much more string-like query format is now supported. See [the docs<io.hdf5-query>](#the-docs<io.hdf5-query>).    .. ipython:: python       path = 'test.h5'      dfq = pd.DataFrame(np.random.randn(10, 4),                         columns=list('ABCD'),                         index=pd.date_range('20130101', periods=10))      dfq.to_hdf(path, key='dfq', format='table', data_columns=True)    Use boolean expressions, with in-line function evaluation.    .. ipython:: python       pd.read_hdf(path, 'dfq',                  where="index>Timestamp('20130104') & columns=['A', 'B']")    Use an inline column reference    .. ipython:: python       pd.read_hdf(path, 'dfq',                  where="A>0 or C>0")    .. ipython:: python      :suppress:       import os      os.remove(path)  - the ``format`keyword now replaces the`table`keyword; allowed values are`fixed(f)`or`table(t)`the same defaults as prior < 0.13.0 remain, e.g.`put`implies`fixed`format and`append`implies`table`format. This default format can be set as an option by setting`io.hdf.default\_format`.    .. ipython:: python       path = 'test.h5'      df = pd.DataFrame(np.random.randn(10, 2))      df.to_hdf(path, key='df_table', format='table')      df.to_hdf(path, key='df_table2', append=True)      df.to_hdf(path, key='df_fixed')      with pd.HDFStore(path) as store:          print(store)    .. ipython:: python      :suppress:       import os      os.remove(path)  - Significant table writing performance improvements - handle a passed`Series``in table format (:issue:`4330`) - can now serialize a``timedelta64\[ns\]``dtype in a table (:issue:`3577`), See [the docs<io.hdf5-timedelta>](#the-docs<io.hdf5-timedelta>). - added an``is\_open``property to indicate if the underlying file handle is_open;   a closed store will now report 'CLOSED' when viewing the store (rather than raising an error)   (:issue:`4409`) - a close of a``HDFStore`now will close that instance of the`HDFStore`but will only close the actual file if the ref count (by`PyTables`) w.r.t. all of the open handles   are 0. Essentially you have a local instance of`HDFStore`referenced by a variable. Once you   close it, it will report closed. Other references (to the same file) will continue to operate   until they themselves are closed. Performing an action on a closed file will raise`ClosedFileError`.. ipython:: python       path = 'test.h5'      df = pd.DataFrame(np.random.randn(10, 2))      store1 = pd.HDFStore(path)      store2 = pd.HDFStore(path)      store1.append('df', df)      store2.append('df2', df)       store1      store2      store1.close()      store2      store2.close()      store2    .. ipython:: python      :suppress:       import os      os.remove(path)  - removed the`\_quiet`attribute, replace by a`DuplicateWarning``if retrieving   duplicate rows from a table (:issue:`4367`) - removed the``warn`argument from`open`. Instead a`PossibleDataLossError`exception will   be raised if you try to use`mode='w'``with an OPEN file handle (:issue:`4367`) - allow a passed locations array or mask as a``where``condition (:issue:`4467`).   See [the docs<io.hdf5-where_mask>](#the-docs<io.hdf5-where_mask>) for an example. - add the keyword``dropna=True`to`append`to change whether ALL nan rows are not written   to the store (default is`True`, ALL nan rows are NOT written), also settable   via the option`io.hdf.dropna\_table``(:issue:`4625`) - pass through store creation arguments; can be used to support in-memory stores  DataFrame repr changes ~~~~~~~~~~~~~~~~~~~~~~  The HTML and plain text representations of `DataFrame` now show a truncated view of the table once it exceeds a certain size, rather than switching to the short info view (:issue:`4886`, :issue:`5550`). This makes the representation more consistent as small DataFrames get larger.  .. image:: ../_static/df_repr_truncated.png    :alt: Truncated HTML representation of a DataFrame  To get the info view, call `DataFrame.info`. If you prefer the info view as the repr for large DataFrames, you can set this by running``set\_option('display.large\_repr', 'info')`.  Enhancements ~~~~~~~~~~~~  -`df.to\_clipboard()`learned a new`excel``keyword that let's you   paste df data directly into excel (enabled by default). (:issue:`5070`). -``read\_html`now raises a`URLError`instead of catching and raising a`ValueError``(:issue:`4303`, :issue:`4305`) - Added a test for``read\_clipboard()`and`to\_clipboard()``(:issue:`4282`) - Clipboard functionality now works with PySide (:issue:`4282`) - Added a more informative error message when plot arguments contain   overlapping color and style arguments (:issue:`4402`) -``to\_dict`now takes`records``as a possible out type.  Returns an array   of column-keyed dictionaries. (:issue:`4936`)  -``NaN``handing in get_dummies (:issue:`4446`) with``dummy\_na`.. ipython:: python       # previously, nan was erroneously counted as 2 here      # now it is not counted at all      pd.get_dummies([1, 2, np.nan])       # unless requested      pd.get_dummies([1, 2, np.nan], dummy_na=True)   -`timedelta64\[ns\]`operations. See [the docs<timedeltas.timedeltas_convert>](#the-docs<timedeltas.timedeltas_convert>).    > **Warning** >       Most of these operations require`numpy \>= 1.7`Using the new top-level`to\_timedelta`, you can convert a scalar or array from the standard   timedelta format (produced by`to\_csv`) into a timedelta type (`np.timedelta64`in`nanoseconds`).`\`ipython In \[53\]: pd.to\_timedelta('1 days 06:05:01.00003') Out\[53\]: Timedelta('1 days 06:05:01.000030')

> In \[54\]: pd.to\_timedelta('15.5us') Out\[54\]: Timedelta('0 days 00:00:00.000015500')
> 
> In \[55\]: pd.to\_timedelta(\['1 days 06:05:01.00003', '15.5us', 'nan'\]) Out\[55\]: TimedeltaIndex(\['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT\], dtype='timedelta64\[ns\]', freq=None)
> 
> In \[56\]: pd.to\_timedelta(np.arange(5), unit='s') Out\[56\]: TimedeltaIndex(\['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02', '0 days 00:00:03', '0 days 00:00:04'\], dtype='timedelta64\[ns\]', freq=None)
> 
> In \[57\]: pd.to\_timedelta(np.arange(5), unit='d') Out\[57\]: TimedeltaIndex(\['0 days', '1 days', '2 days', '3 days', '4 days'\], dtype='timedelta64\[ns\]', freq=None)
> 
> A Series of dtype `timedelta64[ns]` can now be divided by another `timedelta64[ns]` object, or astyped to yield a `float64` dtyped Series. This is frequency conversion. See \[the docs\<timedeltas.timedeltas\_convert\>\](\#the-docs\<timedeltas.timedeltas\_convert\>) for the docs.
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> import datetime td = pd.Series(pd.date\_range('20130101', periods=4)) - pd.Series( pd.date\_range('20121201', periods=4)) td\[2\] += np.timedelta64(datetime.timedelta(minutes=5, seconds=3)) td\[3\] = np.nan td
> 
> ``` ipython
> ```
> 
> \# to days In \[63\]: td / np.timedelta64(1, 'D') Out\[63\]: 0 31.000000 1 31.000000 2 31.003507 3 NaN dtype: float64
> 
> In \[64\]: td.astype('timedelta64\[D\]') Out\[64\]: 0 31.0 1 31.0 2 31.0 3 NaN dtype: float64
> 
> \# to seconds In \[65\]: td / np.timedelta64(1, 's') Out\[65\]: 0 2678400.0 1 2678400.0 2 2678703.0 3 NaN dtype: float64
> 
> In \[66\]: td.astype('timedelta64\[s\]') Out\[66\]: 0 2678400.0 1 2678400.0 2 2678703.0 3 NaN dtype: float64
> 
> Dividing or multiplying a `timedelta64[ns]` Series by an integer or integer Series
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> td \* -1 td \* pd.Series(\[1, 2, 3, 4\])
> 
> Absolute `DateOffset` objects can act equivalently to `timedeltas`
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> from pandas import offsets td + offsets.Minute(5) + offsets.Milli(5)
> 
> Fillna is now supported for timedeltas
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> td.fillna(pd.Timedelta(0)) td.fillna(datetime.timedelta(days=1, seconds=5))
> 
> You can do numeric reduction operations on timedeltas.
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> td.mean() td.quantile(.1)

  - `plot(kind='kde')` now accepts the optional parameters `bw_method` and `ind`, passed to scipy.stats.gaussian\_kde() (for scipy \>= 0.11.0) to set the bandwidth, and to gkde.evaluate() to specify the indices at which it is evaluated, respectively. See scipy docs. (`4298`)

  - DataFrame constructor now accepts a numpy masked record array (`3478`)

  - The new vectorized string method `extract` return regular expression matches more conveniently.
    
    <div class="ipython" data-okwarning="">
    
    python
    
    pd.Series(\['a1', 'b2', 'c3'\]).str.extract('\[ab\](\\d)')
    
    </div>
    
    Elements that do not match return `NaN`. Extracting a regular expression with more than one group returns a DataFrame with one column per group.
    
    <div class="ipython" data-okwarning="">
    
    python
    
    pd.Series(\['a1', 'b2', 'c3'\]).str.extract('(\[ab\])(\\d)')
    
    </div>
    
    Elements that do not match return a row of `NaN`. Thus, a Series of messy strings can be *converted* into a like-indexed Series or DataFrame of cleaned-up or more useful strings, without necessitating `get()` to access tuples or `re.match` objects.
    
    Named groups like
    
    <div class="ipython" data-okwarning="">
    
    python
    
      - pd.Series(\['a1', 'b2', 'c3'\]).str.extract(  
        '(?P\<letter\>\[ab\])(?P\<digit\>\\d)')
    
    </div>
    
    and optional groups can also be used.
    
    <div class="ipython" data-okwarning="">
    
    python
    
      - pd.Series(\['a1', 'b2', '3'\]).str.extract(  
        '(?P\<letter\>\[ab\])?(?P\<digit\>\\d)')
    
    </div>

  - `read_stata` now accepts Stata 13 format (`4291`)

  - `read_fwf` now infers the column specifications from the first 100 rows of the file if the data has correctly separated and properly aligned columns using the delimiter provided to the function (`4488`).

  - support for nanosecond times as an offset
    
    <div class="warning">
    
    <div class="title">
    
    Warning
    
    </div>
    
    These operations require `numpy >= 1.7`
    
    </div>
    
    Period conversions in the range of seconds and below were reworked and extended up to nanoseconds. Periods in the nanosecond range are now available.
    
    ``` python
    In [79]: pd.date_range('2013-01-01', periods=5, freq='5N')
    Out[79]:
    DatetimeIndex([          '2013-01-01 00:00:00',
                   '2013-01-01 00:00:00.000000005',
                   '2013-01-01 00:00:00.000000010',
                   '2013-01-01 00:00:00.000000015',
                   '2013-01-01 00:00:00.000000020'],
                  dtype='datetime64[ns]', freq='5N')
    ```
    
    or with frequency as offset
    
    <div class="ipython">
    
    python
    
    pd.date\_range('2013-01-01', periods=5, freq=pd.offsets.Nano(5))
    
    </div>
    
    Timestamps can be modified in the nanosecond range
    
    <div class="ipython">
    
    python
    
    t = pd.Timestamp('20130101 09:01:02') t + pd.tseries.offsets.Nano(123)
    
    </div>

  - A new method, `isin` for DataFrames, which plays nicely with boolean indexing. The argument to `isin`, what we're comparing the DataFrame to, can be a DataFrame, Series, dict, or array of values. See \[the docs\<indexing.basics.indexing\_isin\>\](\#the-docs\<indexing.basics.indexing\_isin\>) for more.
    
    To get the rows where any of the conditions are met:
    
    <div class="ipython">
    
    python
    
    dfi = pd.DataFrame({'A': \[1, 2, 3, 4\], 'B': \['a', 'b', 'f', 'n'\]}) dfi other = pd.DataFrame({'A': \[1, 3, 3, 7\], 'B': \['e', 'f', 'f', 'e'\]}) mask = dfi.isin(other) mask dfi\[mask.any(axis=1)\]
    
    </div>

  - `Series` now supports a `to_frame` method to convert it to a single-column DataFrame (`5164`)

  - All R datasets listed here <http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html> can now be loaded into pandas objects
    
    ``` python
    # note that pandas.rpy was deprecated in v0.16.0
    import pandas.rpy.common as com
    com.load_data('Titanic')
    ```

  - `tz_localize` can infer a fall daylight savings transition based on the structure of the unlocalized data (`4230`), see \[the docs\<timeseries.timezone\>\](\#the-docs\<timeseries.timezone\>)

  - `DatetimeIndex` is now in the API documentation, see \[the docs\<api.datetimeindex\>\](\#the-docs\<api.datetimeindex\>)

  - <span class="title-ref">\~pandas.io.json.json\_normalize</span> is a new method to allow you to create a flat table from semi-structured JSON data. See \[the docs\<io.json\_normalize\>\](\#the-docs\<io.json\_normalize\>) (`1067`)

  - Added PySide support for the qtpandas DataFrameModel and DataFrameWidget.

  - Python csv parser now supports usecols (`4335`)

  - Frequencies gained several new offsets:
    
      - `LastWeekOfMonth` (`4637`)
      - `FY5253`, and `FY5253Quarter` (`4511`)

  - DataFrame has a new `interpolate` method, similar to Series (`4434`, `1892`)
    
    <div class="ipython">
    
    python
    
      - df = pd.DataFrame({'A': \[1, 2.1, np.nan, 4.7, 5.6, 6.8\],  
        'B': \[.25, np.nan, np.nan, 4, 12.2, 14.4\]})
    
    df.interpolate()
    
    </div>
    
    Additionally, the `method` argument to `interpolate` has been expanded to include `'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'barycentric', 'krogh', 'piecewise_polynomial', 'pchip', 'polynomial', 'spline'` The new methods require [scipy](http://www.scipy.org). Consult the Scipy reference [guide]() and [documentation]() for more information about when the various methods are appropriate. See \[the docs\<missing\_data.interpolate\>\](\#the-docs\<missing\_data.interpolate\>).
    
    Interpolate now also accepts a `limit` keyword argument. This works similar to `fillna`'s limit:
    
    <div class="ipython">
    
    python
    
    ser = pd.Series(\[1, 3, np.nan, np.nan, np.nan, 11\]) ser.interpolate(limit=2)
    
    </div>

  - Added `wide_to_long` panel data convenience function. See \[the docs\<reshaping.melt\>\](\#the-docs\<reshaping.melt\>).
    
    <div class="ipython">
    
    python
    
    np.random.seed(123) df = pd.DataFrame({"A1970" : {0 : "a", 1 : "b", 2 : "c"}, "A1980" : {0 : "d", 1 : "e", 2 : "f"}, "B1970" : {0 : 2.5, 1 : 1.2, 2 : .7}, "B1980" : {0 : 3.2, 1 : 1.3, 2 : .1}, "X" : dict(zip(range(3), np.random.randn(3))) }) df\["id"\] = df.index df pd.wide\_to\_long(df, \["A", "B"\], i="id", j="year")
    
    </div>

<!-- end list -->

  - `` ` .. _documentation: http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation .. _guide: https://docs.scipy.org/doc/scipy/tutorial/interpolate.html  - ``to\_csv`now takes a`date\_format``keyword argument that specifies how   output datetime objects should be formatted. Datetimes encountered in the   index, columns, and values will all have this formatting applied. (:issue:`4313`) -``DataFrame.plot`will scatter plot x versus y by passing`kind='scatter'``(:issue:`2215`) - Added support for Google Analytics v3 API segment IDs that also supports v2 IDs. (:issue:`5271`)  .. _whatsnew_0130.experimental:  Experimental ~~~~~~~~~~~~  - The new `~pandas.eval` function implements expression evaluation using``numexpr`behind the scenes. This results in large speedups for   complicated expressions involving large DataFrames/Series. For example,    .. ipython:: python       nrows, ncols = 20000, 100      df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols))                            for _ in range(4)]    .. ipython:: python       # eval with NumExpr backend      %timeit pd.eval('df1 + df2 + df3 + df4')    .. ipython:: python       # pure Python evaluation      %timeit df1 + df2 + df3 + df4    For more details, see the [the docs<enhancingperf.eval>](#the-docs<enhancingperf.eval>)  - Similar to`pandas.eval``, `~pandas.DataFrame` has a new``DataFrame.eval`method that evaluates an expression in the context of   the`DataFrame``. For example,    .. ipython:: python      :suppress:       try:          del a  # noqa: F821      except NameError:          pass       try:          del b  # noqa: F821      except NameError:          pass    .. ipython:: python       df = pd.DataFrame(np.random.randn(10, 2), columns=['a', 'b'])      df.eval('a + b')  - `~pandas.DataFrame.query` method has been added that allows   you to select elements of a``DataFrame`using a natural query syntax   nearly identical to Python syntax. For example,    .. ipython:: python      :suppress:       try:          del a  # noqa: F821      except NameError:          pass       try:          del b  # noqa: F821      except NameError:          pass       try:          del c  # noqa: F821      except NameError:          pass    .. ipython:: python       n = 20      df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=['a', 'b', 'c'])      df.query('a < b < c')    selects all the rows of`df`where`a \< b \< c`evaluates to`True`.   For more details see the [the docs<indexing.query>](#the-docs<indexing.query>).  -`pd.read\_msgpack()`and`pd.to\_msgpack()`are now a supported method of serialization   of arbitrary pandas (and python objects) in a lightweight portable binary format. See [the docs<io.msgpack>](#the-docs<io.msgpack>)    > **Warning** >       Since this is an EXPERIMENTAL LIBRARY, the storage format may not be stable until a future release.`\`python  
    df = pd.DataFrame(np.random.rand(5, 2), columns=list('AB')) df.to\_msgpack('foo.msg') pd.read\_msgpack('foo.msg')
    
    s = pd.Series(np.random.rand(5), index=pd.date\_range('20130101', periods=5)) pd.to\_msgpack('foo.msg', df, s) pd.read\_msgpack('foo.msg')
    
    You can pass `iterator=True` to iterator over the unpacked results
    
    ``` python
    ```
    
      - for o in pd.read\_msgpack('foo.msg', iterator=True):  
        print(o)
    
    <div class="ipython">
    
    python
    
    </div>
    
      - suppress
    
      - okexcept
    
    os.remove('foo.msg')

  - \- `pandas.io.gbq` provides a simple way to extract from, and load data into,  
    Google's BigQuery Data Sets by way of pandas DataFrames. BigQuery is a high performance SQL-like database service, useful for performing ad-hoc queries against extremely large datasets. \[See the docs \<io.bigquery\>\](\#see-the-docs-\<io.bigquery\>)
    
    ``` python
    from pandas.io import gbq
    
    # A query to select the average monthly temperatures in the
    # in the year 2000 across the USA. The dataset,
    # publicata:samples.gsod, is available on all BigQuery accounts,
    # and is based on NOAA gsod data.
    
    query = """SELECT station_number as STATION,
    month as MONTH, AVG(mean_temp) as MEAN_TEMP
    FROM publicdata:samples.gsod
    WHERE YEAR = 2000
    GROUP BY STATION, MONTH
    ORDER BY STATION, MONTH ASC"""
    
    # Fetch the result set for this query
    
    # Your Google BigQuery Project ID
    # To find this, see your dashboard:
    # https://console.developers.google.com/iam-admin/projects?authuser=0
    projectid = 'xxxxxxxxx'
    df = gbq.read_gbq(query, project_id=projectid)
    
    # Use pandas to process and reshape the dataset
    
    df2 = df.pivot(index='STATION', columns='MONTH', values='MEAN_TEMP')
    df3 = pd.concat([df2.min(), df2.mean(), df2.max()],
                    axis=1, keys=["Min Tem", "Mean Temp", "Max Temp"])
    ```
    
    The resulting DataFrame is:
    
        > df3
                    Min Tem  Mean Temp    Max Temp
         MONTH
         1     -53.336667  39.827892   89.770968
         2     -49.837500  43.685219   93.437932
         3     -77.926087  48.708355   96.099998
         4     -82.892858  55.070087   97.317240
         5     -92.378261  61.428117  102.042856
         6     -77.703334  65.858888  102.900000
         7     -87.821428  68.169663  106.510714
         8     -89.431999  68.614215  105.500000
         9     -86.611112  63.436935  107.142856
         10    -78.209677  56.880838   92.103333
         11    -50.125000  48.861228   94.996428
         12    -50.332258  42.286879   94.396774
    
    <div class="warning">
    
    <div class="title">
    
    Warning
    
    </div>
    
    To use this module, you will need a BigQuery account. See \<<https://cloud.google.com/products/big-query>\> for details.
    
    As of 10/10/13, there is a bug in Google's API preventing result sets from being larger than 100,000 rows. A patch is scheduled for the week of 10/14/13.
    
    </div>

<div id="whatsnew_0130.refactoring">

Internal refactoring `` ` ~~~~~~~~~~~~~~~~~~~~  In 0.13.0 there is a major refactor primarily to subclass ``Series`from`NDFrame`, which is the base class currently for`DataFrame`and`Panel`, to unify methods and behaviors. Series formerly subclassed directly from`ndarray``. (:issue:`4080`, :issue:`3862`, :issue:`816`)  > **Warning** >     There are two potential incompatibilities from < 0.13.0     - Using certain numpy functions would previously return a``Series`if passed a`Series`as an argument. This seems only to affect`np.ones\_like`,`np.empty\_like`,`np.diff`and`np.where`. These now return`ndarrays`.       .. ipython:: python          s = pd.Series([1, 2, 3, 4])       Numpy Usage       .. ipython:: python          np.ones_like(s)         np.diff(s)         np.where(s > 1, s, np.nan)       Pandonic Usage       .. ipython:: python          pd.Series(1, index=s.index)         s.diff()         s.where(s > 1)     - Passing a`Series`directly to a cython function expecting an`ndarray`type will no      long work directly, you must pass`Series.values`, See [Enhancing Performance<enhancingperf.ndarray>](#enhancing-performance<enhancingperf.ndarray>)     -`Series(0.5)`would previously return the scalar`0.5`, instead this will return a 1-element`Series`- This change breaks`rpy2\<=2.3.8``. an Issue has been opened against rpy2 and a workaround      is detailed in :issue:`5698`. Thanks @JanSchulz.  - Pickle compatibility is preserved for pickles created prior to 0.13. These must be unpickled with``pd.read\_pickle`, see [Pickling<io.pickle>](#pickling<io.pickle>).  - Refactor of series.py/frame.py/panel.py to move common code to generic.py    - added`\_setup\_axes`to created generic NDFrame structures   - moved methods      -`from\_axes,\_wrap\_array,axes,ix,loc,iloc,shape,empty,swapaxes,transpose,pop`-`\_\_iter\_\_,keys,\_\_contains\_\_,\_\_len\_\_,\_\_neg\_\_,\_\_invert\_\_`-`convert\_objects,as\_blocks,as\_matrix,values`-`\_\_getstate\_\_,\_\_setstate\_\_`(compat remains in frame/panel)     -`\_\_getattr\_\_,\_\_setattr\_\_`-`\_indexed\_same,reindex\_like,align,where,mask`-`fillna,replace`(`Series`replace is now consistent with`DataFrame`)     -`filter`(also added axis argument to selectively filter on a different axis)     -`reindex,reindex\_axis,take`-`truncate`(moved to become part of`NDFrame`)  - These are API changes which make`Panel`more consistent with`DataFrame`-`swapaxes`on a`Panel`with the same axes specified now return a copy   - support attribute access for setting   - filter supports the same API as the original`DataFrame`filter  - Reindex called with no arguments will now return a copy of the input object  -`TimeSeries`is now an alias for`Series`. the property`is\_time\_series`can be used to distinguish (if desired)  - Refactor of Sparse objects to use BlockManager    - Created a new block type in internals,`SparseBlock`, which can hold multi-dtypes     and is non-consolidatable.`SparseSeries`and`SparseDataFrame`now inherit     more methods from there hierarchy (Series/DataFrame), and no longer inherit     from`SparseArray`(which instead is the object of the`SparseBlock`)   - Sparse suite now supports integration with non-sparse data. Non-float sparse     data is supportable (partially implemented)   - Operations on sparse structures within DataFrames should preserve sparseness,     merging type operations will convert to dense (and back to sparse), so might     be somewhat inefficient   - enable setitem on`SparseSeries`for boolean/integer/slices   -`SparsePanels`implementation is unchanged (e.g. not using BlockManager, needs work)  - added`ftypes`method to Series/DataFrame, similar to`dtypes`, but indicates   if the underlying is sparse/dense (as well as the dtype) - All`NDFrame`objects can now use`\_\_finalize\_\_()`to specify various   values to propagate to new objects from an existing one (e.g.`name`in`Series`will   follow more automatically now) - Internal type checking is now done via a suite of generated classes, allowing`isinstance(value, klass)``without having to directly import the klass, courtesy of @jtratner - Bug in Series update where the parent frame is not updating its cache based on   changes (:issue:`4080`) or types (:issue:`3217`), fillna (:issue:`3386`) - Indexing with dtype conversions fixed (:issue:`4463`, :issue:`4204`) - Refactor``Series.reindex``to core/generic.py (:issue:`4604`, :issue:`4618`), allow``method=`in reindexing   on a Series to work -`Series.copy`no longer accepts the`order`parameter and is now consistent with`NDFrame`copy - Refactor`rename`methods to core/generic.py; fixes`Series.rename``for (:issue:`4605`), and adds``rename`with the same signature for`Panel`- Refactor`clip``methods to core/generic.py (:issue:`4798`) - Refactor of``\_get\_numeric\_data/\_get\_bool\_data`to core/generic.py, allowing Series/Panel functionality -`Series`(for index) /`Panel``(for items) now allow attribute access to its elements  (:issue:`1903`)    .. ipython:: python       s = pd.Series([1, 2, 3], index=list('abc'))      s.b      s.a = 5      s  .. _release.bug_fixes-0.13.0:  Bug fixes ~~~~~~~~~  -``HDFStore`- raising an invalid`TypeError`rather than`ValueError``when     appending with a different block ordering (:issue:`4096`)   -``read\_hdf`was not respecting as passed`mode``(:issue:`4504`)   - appending a 0-len table will work correctly (:issue:`4273`)   -``to\_hdf`was raising when passing both arguments`append`and`table``(:issue:`4584`)   - reading from a store with duplicate columns across dtypes would raise     (:issue:`4767`)   - Fixed a bug where``ValueError``wasn't correctly raised when column     names weren't strings (:issue:`4956`)   - A zero length series written in Fixed format not deserializing properly.     (:issue:`4708`)   - Fixed decoding perf issue on pyt3 (:issue:`5441`)   - Validate levels in a MultiIndex before storing (:issue:`5527`)   - Correctly handle``data\_columns``with a Panel (:issue:`5717`) - Fixed bug in tslib.tz_convert(vals, tz1, tz2): it could raise IndexError   exception while trying to access trans[pos + 1] (:issue:`4496`) - The``by`argument now works correctly with the`layout``argument   (:issue:`4102`, :issue:`4014`) in``\*.hist`plotting methods - Fixed bug in`PeriodIndex.map`where using`str``would return the str   representation of the index (:issue:`4136`) - Fixed test failure``test\_time\_series\_plot\_color\_with\_empty\_kwargs``when   using custom matplotlib default colors (:issue:`4345`) - Fix running of stata IO tests. Now uses temporary files to write   (:issue:`4353`) - Fixed an issue where``DataFrame.sum`was slower than`DataFrame.mean``for integer valued frames (:issue:`4365`) -``read\_html``tests now work with Python 2.6 (:issue:`4351`) - Fixed bug where``network`testing was throwing`NameError``because a   local variable was undefined (:issue:`4381`) - In``to\_json`, raise if a passed`orient``would cause loss of data   because of a duplicate index (:issue:`4359`) - In``to\_json``, fix date handling so milliseconds are the default timestamp   as the docstring says (:issue:`4362`). -``as\_index``is no longer ignored when doing groupby apply (:issue:`4648`,   :issue:`3417`) - JSON NaT handling fixed, NaTs are now serialized to``null``(:issue:`4498`) - Fixed JSON handling of escapable characters in JSON object keys   (:issue:`4593`) - Fixed passing``keep\_default\_na=False`when`na\_values=None``(:issue:`4318`) - Fixed bug with``values``raising an error on a DataFrame with duplicate   columns and mixed dtypes, surfaced in (:issue:`4377`) - Fixed bug with duplicate columns and type conversion in``read\_json`when`orient='split'``(:issue:`4377`) - Fixed JSON bug where locales with decimal separators other than '.' threw   exceptions when encoding / decoding certain values. (:issue:`4918`) - Fix``.iat`indexing with a`PeriodIndex``(:issue:`4390`) - Fixed an issue where``PeriodIndex``joining with self was returning a new   instance rather than the same instance (:issue:`4379`); also adds a test   for this for the other index types - Fixed a bug with all the dtypes being converted to object when using the   CSV cparser with the usecols parameter (:issue:`3192`) - Fix an issue in merging blocks where the resulting DataFrame had partially   set _ref_locs (:issue:`4403`) - Fixed an issue where hist subplots were being overwritten when they were   called using the top level matplotlib API (:issue:`4408`) - Fixed a bug where calling``Series.astype(str)``would truncate the string   (:issue:`4405`, :issue:`4437`) - Fixed a py3 compat issue where bytes were being repr'd as tuples   (:issue:`4455`) - Fixed Panel attribute naming conflict if item is named 'a'   (:issue:`3440`) - Fixed an issue where duplicate indexes were raising when plotting   (:issue:`4486`) - Fixed an issue where cumsum and cumprod didn't work with bool dtypes   (:issue:`4170`, :issue:`4440`) - Fixed Panel slicing issued in``xs``that was returning an incorrect dimmed   object (:issue:`4016`) - Fix resampling bug where custom reduce function not used if only one group   (:issue:`3849`, :issue:`4494`) - Fixed Panel assignment with a transposed frame (:issue:`3830`) - Raise on set indexing with a Panel and a Panel as a value which needs   alignment (:issue:`3777`) - frozenset objects now raise in the``Series``constructor (:issue:`4482`,   :issue:`4480`) - Fixed issue with sorting a duplicate MultiIndex that has multiple dtypes   (:issue:`4516`) - Fixed bug in``DataFrame.set\_values``which was causing name attributes to   be lost when expanding the index. (:issue:`3742`, :issue:`4039`) - Fixed issue where individual``names`,`levels`and`labels`could be   set on`MultiIndex``without validation (:issue:`3714`, :issue:`4039`) - Fixed (:issue:`3334`) in pivot_table. Margins did not compute if values is   the index. - Fix bug in having a rhs of``np.timedelta64`or`np.offsets.DateOffset``when operating with datetimes (:issue:`4532`) - Fix arithmetic with series/datetimeindex and``np.timedelta64``not working   the same (:issue:`4134`) and buggy timedelta in NumPy 1.6 (:issue:`4135`) - Fix bug in``pd.read\_clipboard``on windows with PY3 (:issue:`4561`); not   decoding properly -``tslib.get\_period\_field()`and`tslib.get\_period\_field\_arr()``now raise   if code argument out of range (:issue:`4519`, :issue:`4520`) - Fix boolean indexing on an empty series loses index names (:issue:`4235`),   infer_dtype works with empty arrays. - Fix reindexing with multiple axes; if an axes match was not replacing the   current axes, leading to a possible lazy frequency inference issue   (:issue:`3317`) - Fixed issue where``DataFrame.apply`was reraising exceptions incorrectly   (causing the original stack trace to be truncated). - Fix selection with`ix/loc``and non_unique selectors (:issue:`4619`) - Fix assignment with iloc/loc involving a dtype change in an existing column   (:issue:`4312`, :issue:`5702`) have internal setitem_with_indexer in core/indexing   to use Block.setitem - Fixed bug where thousands operator was not handled correctly for floating   point numbers in csv_import (:issue:`4322`) - Fix an issue with CacheableOffset not properly being used by many   DateOffset; this prevented the DateOffset from being cached (:issue:`4609`) - Fix boolean comparison with a DataFrame on the lhs, and a list/tuple on the   rhs (:issue:`4576`) - Fix error/dtype conversion with setitem of``None`on`Series/DataFrame``(:issue:`4667`) - Fix decoding based on a passed in non-default encoding in``pd.read\_stata``(:issue:`4626`) - Fix``DataFrame.from\_records`with a plain-vanilla`ndarray``.   (:issue:`4727`) - Fix some inconsistencies with``Index.rename`and`MultiIndex.rename``,   etc. (:issue:`4718`, :issue:`4628`) - Bug in using``iloc/loc``with a cross-sectional and duplicate indices   (:issue:`4726`) - Bug with using``QUOTE\_NONE`with`to\_csv`causing`Exception``.   (:issue:`4328`) - Bug with Series indexing not raising an error when the right-hand-side has   an incorrect length (:issue:`2702`) - Bug in MultiIndexing with a partial string selection as one part of a   MultIndex (:issue:`4758`) - Bug with reindexing on the index with a non-unique index will now raise``ValueError``(:issue:`4746`) - Bug in setting with``loc/ix``a single indexer with a MultiIndex axis and   a NumPy array, related to (:issue:`3777`) - Bug in concatenation with duplicate columns across dtypes not merging with   axis=0 (:issue:`4771`, :issue:`4975`) - Bug in``iloc``with a slice index failing (:issue:`4771`) - Incorrect error message with no colspecs or width in``read\_fwf``.   (:issue:`4774`) - Fix bugs in indexing in a Series with a duplicate index (:issue:`4548`,   :issue:`4550`) - Fixed bug with reading compressed files with``read\_fwf``in Python 3.   (:issue:`3963`) - Fixed an issue with a duplicate index and assignment with a dtype change   (:issue:`4686`) - Fixed bug with reading compressed files in as``bytes`rather than`str``in Python 3. Simplifies bytes-producing file-handling in Python 3   (:issue:`3963`, :issue:`4785`). - Fixed an issue related to ticklocs/ticklabels with log scale bar plots   across different versions of matplotlib (:issue:`4789`) - Suppressed DeprecationWarning associated with internal calls issued by   repr() (:issue:`4391`) - Fixed an issue with a duplicate index and duplicate selector with``.loc``(:issue:`4825`) - Fixed an issue with``DataFrame.sort\_index`where, when sorting by a   single column and passing a list for`ascending`, the argument for`ascending`was being interpreted as`True``(:issue:`4839`,   :issue:`4846`) - Fixed``Panel.tshift`not working. Added`freq`support to`Panel.shift``(:issue:`4853`) - Fix an issue in TextFileReader w/ Python engine (i.e. PythonParser)   with thousands != "," (:issue:`4596`) - Bug in getitem with a duplicate index when using where (:issue:`4879`) - Fix Type inference code coerces float column into datetime (:issue:`4601`) - Fixed``\_ensure\_numeric``does not check for complex numbers   (:issue:`4902`) - Fixed a bug in``Series.hist`where two figures were being created when   the`by``argument was passed (:issue:`4112`, :issue:`4113`). - Fixed a bug in``convert\_objects``for > 2 ndims (:issue:`4937`) - Fixed a bug in DataFrame/Panel cache insertion and subsequent indexing   (:issue:`4939`, :issue:`5424`) - Fixed string methods for``FrozenNDArray`and`FrozenList``(:issue:`4929`) - Fixed a bug with setting invalid or out-of-range values in indexing   enlargement scenarios (:issue:`4940`) - Tests for fillna on empty Series (:issue:`4346`), thanks @immerrr - Fixed``copy()``to shallow copy axes/indices as well and thereby keep   separate metadata. (:issue:`4202`, :issue:`4830`) - Fixed skiprows option in Python parser for read_csv (:issue:`4382`) - Fixed bug preventing``cut`from working with`np.inf``levels without   explicitly passing labels (:issue:`3415`) - Fixed wrong check for overlapping in``DatetimeIndex.union``(:issue:`4564`) - Fixed conflict between thousands separator and date parser in csv_parser   (:issue:`4678`) - Fix appending when dtypes are not the same (error showing mixing   float/np.datetime64) (:issue:`4993`) - Fix repr for DateOffset. No longer show duplicate entries in kwds.   Removed unused offset fields. (:issue:`4638`) - Fixed wrong index name during read_csv if using usecols. Applies to c   parser only. (:issue:`4201`) -``Timestamp`objects can now appear in the left hand side of a comparison   operation with a`Series`or`DataFrame``object (:issue:`4982`). - Fix a bug when indexing with``np.nan`via`iloc/loc``(:issue:`5016`) - Fixed a bug where low memory c parser could create different types in   different chunks of the same file. Now coerces to numerical type or raises   warning. (:issue:`3866`) - Fix a bug where reshaping a``Series`to its own shape raised`TypeError``(:issue:`4554`) and other reshaping issues. - Bug in setting with``ix/loc``and a mixed int/string index (:issue:`4544`) - Make sure series-series boolean comparisons are label based (:issue:`4947`) - Bug in multi-level indexing with a Timestamp partial indexer   (:issue:`4294`) - Tests/fix for MultiIndex construction of an all-nan frame (:issue:`4078`) - Fixed a bug where `~pandas.read_html` wasn't correctly inferring   values of tables with commas (:issue:`5029`) - Fixed a bug where `~pandas.read_html` wasn't providing a stable   ordering of returned tables (:issue:`4770`, :issue:`5029`). - Fixed a bug where `~pandas.read_html` was incorrectly parsing when   passed``index\_col=0``(:issue:`5066`). - Fixed a bug where `~pandas.read_html` was incorrectly inferring the   type of headers (:issue:`5048`). - Fixed a bug where``DatetimeIndex`joins with`PeriodIndex``caused a   stack overflow (:issue:`3899`). - Fixed a bug where``groupby``objects didn't allow plots (:issue:`5102`). - Fixed a bug where``groupby``objects weren't tab-completing column names   (:issue:`5102`). - Fixed a bug where``groupby.plot()``and friends were duplicating figures   multiple times (:issue:`5102`). - Provide automatic conversion of``object``dtypes on fillna, related   (:issue:`5103`) - Fixed a bug where default options were being overwritten in the option   parser cleaning (:issue:`5121`). - Treat a list/ndarray identically for``iloc``indexing with list-like   (:issue:`5006`) - Fix``MultiIndex.get\_level\_values()``with missing values (:issue:`5074`) - Fix bound checking for Timestamp() with datetime64 input (:issue:`4065`) - Fix a bug where``TestReadHtml`wasn't calling the correct`read\_html()``function (:issue:`5150`). - Fix a bug with``NDFrame.replace()``which made replacement appear as   though it was (incorrectly) using regular expressions (:issue:`5143`). - Fix better error message for to_datetime (:issue:`4928`) - Made sure different locales are tested on travis-ci (:issue:`4918`). Also   adds a couple of utilities for getting locales and setting locales with a   context manager. - Fixed segfault on``isnull(MultiIndex)``(now raises an error instead)   (:issue:`5123`, :issue:`5125`) - Allow duplicate indices when performing operations that align   (:issue:`5185`, :issue:`5639`) - Compound dtypes in a constructor raise``NotImplementedError``(:issue:`5191`) - Bug in comparing duplicate frames (:issue:`4421`) related - Bug in describe on duplicate frames - Bug in``to\_datetime`with a format and`coerce=True``not raising   (:issue:`5195`) - Bug in``loc``setting with multiple indexers and a rhs of a Series that   needs broadcasting (:issue:`5206`) - Fixed bug where inplace setting of levels or labels on``MultiIndex`would   not clear cached`values`property and therefore return wrong`values``.   (:issue:`5215`) - Fixed bug where filtering a grouped DataFrame or Series did not maintain   the original ordering (:issue:`4621`). - Fixed``Period``with a business date freq to always roll-forward if on a   non-business date. (:issue:`5203`) - Fixed bug in Excel writers where frames with duplicate column names weren't   written correctly. (:issue:`5235`) - Fixed issue with``drop``and a non-unique index on Series (:issue:`5248`) - Fixed segfault in C parser caused by passing more names than columns in   the file. (:issue:`5156`) - Fix``Series.isin``with date/time-like dtypes (:issue:`5021`) - C and Python Parser can now handle the more common MultiIndex column   format which doesn't have a row for index names (:issue:`4702`) - Bug when trying to use an out-of-bounds date as an object dtype   (:issue:`5312`) - Bug when trying to display an embedded PandasObject (:issue:`5324`) - Allows operating of Timestamps to return a datetime if the result is out-of-bounds   related (:issue:`5312`) - Fix return value/type signature of``initObjToJSON()`to be compatible   with numpy's`import\_array()``(:issue:`5334`, :issue:`5326`) - Bug when renaming then set_index on a DataFrame (:issue:`5344`) - Test suite no longer leaves around temporary files when testing graphics. (:issue:`5347`)   (thanks for catching this @yarikoptic!) - Fixed html tests on win32. (:issue:`4580`) - Make sure that``head/tail`are`iloc``based, (:issue:`5370`) - Fixed bug for``PeriodIndex``string representation if there are 1 or 2   elements. (:issue:`5372`) - The GroupBy methods``transform`and`filter``can be used on Series   and DataFrames that have repeated (non-unique) indices. (:issue:`4620`) - Fix empty series not printing name in repr (:issue:`4651`) - Make tests create temp files in temp directory by default. (:issue:`5419`) -``pd.to\_timedelta``of a scalar returns a scalar (:issue:`5410`) -``pd.to\_timedelta`accepts`NaN`and`NaT`, returning`NaT``instead of raising (:issue:`5437`) - performance improvements in``isnull``on larger size pandas objects - Fixed various setitem with 1d ndarray that does not have a matching   length to the indexer (:issue:`5508`) - Bug in getitem with a MultiIndex and``iloc``(:issue:`5528`) - Bug in delitem on a Series (:issue:`5542`) - Bug fix in apply when using custom function and objects are not mutated (:issue:`5545`) - Bug in selecting from a non-unique index with``loc``(:issue:`5553`) - Bug in groupby returning non-consistent types when user function returns a``None``, (:issue:`5592`) - Work around regression in numpy 1.7.0 which erroneously raises IndexError from``ndarray.item\`<span class="title-ref"> (:issue:\`5666</span>) - Bug in repeated indexing of object with resultant non-unique index (`5678`) - Bug in fillna with Series and a passed series/dict (`5703`) - Bug in groupby transform with a datetime-like grouper (`5712`) - Bug in MultiIndex selection in PY3 when using certain keys (`5725`) - Row-wise concat of differing dtypes failing in certain cases (`5754`)

</div>

## Contributors

<div class="contributors">

v0.12.0..v0.13.0

</div>

---

v0.13.1.md

---

# Version 0.13.1 (February 3, 2014)

{{ header }}

This is a minor release from 0.13.0 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

Highlights include:

  - Added `infer_datetime_format` keyword to `read_csv/to_datetime` to allow speedups for homogeneously formatted datetimes.
  - Will intelligently limit display precision for datetime/timedelta formats.
  - Enhanced Panel <span class="title-ref">\~pandas.Panel.apply</span> method.
  - Suggested tutorials in new \[Tutorials\<tutorials\>\](\#tutorials\<tutorials\>) section.
  - Our pandas ecosystem is growing, We now feature related projects in a new [ecosystem page](https://pandas.pydata.org/community/ecosystem.html) section.
  - Much work has been taking place on improving the docs, and a new \[Contributing\<contributing\>\](\#contributing\<contributing\>) section has been added.
  - Even though it may only be of interest to devs, we \<3 our new CI status page: [ScatterCI](http://scatterci.github.io/pydata/pandas).

\> **Warning** \> 0.13.1 fixes a bug that was caused by a combination of having numpy \< 1.8, and doing chained assignment on a string-like array. Chained indexing can have unexpected results and should generally be avoided.

> This would previously segfault:
> 
>   - \`\`\`python  
>     df = pd.DataFrame({"A": np.array(\["foo", "bar", "bah", "foo", "bar"\])}) df\["A"\].iloc\[0\] = np.nan
> 
> The recommended way to do this type of assignment is:
> 
> <div class="ipython">
> 
> python
> 
> df = pd.DataFrame({"A": np.array(\["foo", "bar", "bah", "foo", "bar"\])}) df.loc\[0, "A"\] = np.nan df
> 
> </div>

Output formatting enhancements `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - df.info() view now display dtype info per column (:issue:`5682`)  - df.info() now honors the option ``max\_info\_rows``, to disable null counts for large frames (:issue:`5974`)    .. ipython:: python       max_info_rows = pd.get_option("max_info_rows")       df = pd.DataFrame(          {              "A": np.random.randn(10),              "B": np.random.randn(10),              "C": pd.date_range("20130101", periods=10),          }      )      df.iloc[3:6, [0, 2]] = np.nan    .. ipython:: python       # set to not display the null counts      pd.set_option("max_info_rows", 0)      df.info()    .. ipython:: python       # this is the default (same as in 0.13.0)      pd.set_option("max_info_rows", max_info_rows)      df.info()  - Add``show\_dimensions`display option for the new DataFrame repr to control whether the dimensions print.    .. ipython:: python        df = pd.DataFrame([[1, 2], [3, 4]])       pd.set_option("show_dimensions", False)       df        pd.set_option("show_dimensions", True)       df  - The`ArrayFormatter`for`datetime`and`timedelta64``now intelligently   limit precision based on the values in the array (:issue:`3401`)    Previously output might look like:    ..``\`text age today diff 0 2001-01-01 00:00:00 2013-04-19 00:00:00 4491 days, 00:00:00 1 2004-06-01 00:00:00 2013-04-19 00:00:00 3244 days, 00:00:00

> Now the output looks like:
> 
> <div class="ipython">
> 
> python
> 
>   - df = pd.DataFrame(  
>     \[pd.Timestamp("20010101"), pd.Timestamp("20040601")\], columns=\["age"\]
> 
> ) df\["today"\] = pd.Timestamp("20130419") df\["diff"\] = df\["today"\] - df\["age"\] df
> 
> </div>

API changes `` ` ~~~~~~~~~~~  - Add ``-NaN`and`-nan``to the default set of NA values (:issue:`5952`).   See [NA Values <io.na_values>](#na-values-<io.na_values>).  - Added``Series.str.get\_dummies``vectorized string method (:issue:`6021`), to extract   dummy/indicator variables for separated string columns:    .. ipython:: python        s = pd.Series(["a", "a|b", np.nan, "a|c"])       s.str.get_dummies(sep="|")  - Added the``NDFrame.equals()`method to compare if two NDFrames are   equal have equal axes, dtypes, and values. Added the`array\_equivalent``function to compare if two ndarrays are   equal. NaNs in identical locations are treated as   equal. (:issue:`5283`) See also [the docs<basics.equals>](#the-docs<basics.equals>) for a motivating example.``\`python df = pd.DataFrame({"col": \["foo", 0, np.nan\]}) df2 = pd.DataFrame({"col": \[np.nan, 0, "foo"\]}, index=\[2, 1, 0\]) df.equals(df2) df.equals(df2.sort\_index())

  - `DataFrame.apply` will use the `reduce` argument to determine whether a `Series` or a `DataFrame` should be returned when the `DataFrame` is empty (`6007`).
    
    Previously, calling `DataFrame.apply` an empty `DataFrame` would return either a `DataFrame` if there were no columns, or the function being applied would be called with an empty `Series` to guess whether a `Series` or `DataFrame` should be returned:
    
    ``` ipython
    In [32]: def applied_func(col):
      ....:    print("Apply function being called with: ", col)
      ....:    return col.sum()
      ....:
    
    In [33]: empty = DataFrame(columns=['a', 'b'])
    
    In [34]: empty.apply(applied_func)
    Apply function being called with:  Series([], Length: 0, dtype: float64)
    Out[34]:
    a   NaN
    b   NaN
    Length: 2, dtype: float64
    ```
    
    Now, when `apply` is called on an empty `DataFrame`: if the `reduce` argument is `True` a `Series` will returned, if it is `False` a `DataFrame` will be returned, and if it is `None` (the default) the function being applied will be called with an empty series to try and guess the return type.
    
    ``` ipython
    In [35]: empty.apply(applied_func, reduce=True)
    Out[35]:
    a   NaN
    b   NaN
    Length: 2, dtype: float64
    
    In [36]: empty.apply(applied_func, reduce=False)
    Out[36]:
    Empty DataFrame
    Columns: [a, b]
    Index: []
    
    [0 rows x 2 columns]
    ```

Prior version deprecations/changes `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  There are no announced changes in 0.13 or prior that are taking effect as of 0.13.1  Deprecations ~~~~~~~~~~~~  There are no deprecations of prior behavior in 0.13.1  Enhancements ~~~~~~~~~~~~  - ``pd.read\_csv`and`pd.to\_datetime`learned a new`infer\_datetime\_format``keyword which greatly   improves parsing perf in many cases. Thanks to @lexual for suggesting and @danbirken   for rapidly implementing. (:issue:`5490`, :issue:`6021`)    If``parse\_dates`is enabled and this flag is set, pandas will attempt to   infer the format of the datetime strings in the columns, and if it can   be inferred, switch to a faster method of parsing them.  In some cases   this can increase the parsing speed by ~5-10x.`\`python \# Try to infer the format for the index column df = pd.read\_csv( "foo.csv", index\_col=0, parse\_dates=True, infer\_datetime\_format=True )

  - `date_format` and `datetime_format` keywords can now be specified when writing to `excel` files (`4133`)

  - `MultiIndex.from_product` convenience function for creating a MultiIndex from the cartesian product of a set of iterables (`6055`):
    
    <div class="ipython">
    
    python
    
    shades = \["light", "dark"\] colors = \["red", "green", "blue"\]
    
    pd.MultiIndex.from\_product(\[shades, colors\], names=\["shade", "color"\])
    
    </div>

  - Panel <span class="title-ref">\~pandas.Panel.apply</span> will work on non-ufuncs. See \[the docs\<basics.apply\>\](\#the-docs\<basics.apply\>).
    
    ``` ipython
    In [28]: import pandas._testing as tm
    
    In [29]: panel = tm.makePanel(5)
    
    In [30]: panel
    Out[30]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 3 (items) x 5 (major_axis) x 4 (minor_axis)
    Items axis: ItemA to ItemC
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
    Minor_axis axis: A to D
    
    In [31]: panel['ItemA']
    Out[31]:
                       A         B         C         D
    2000-01-03 -0.673690  0.577046 -1.344312 -1.469388
    2000-01-04  0.113648 -1.715002  0.844885  0.357021
    2000-01-05 -1.478427 -1.039268  1.075770 -0.674600
    2000-01-06  0.524988 -0.370647 -0.109050 -1.776904
    2000-01-07  0.404705 -1.157892  1.643563 -0.968914
    
    [5 rows x 4 columns]
    ```
    
    Specifying an `apply` that operates on a Series (to return a single element)
    
    ``` ipython
    In [32]: panel.apply(lambda x: x.dtype, axis='items')
    Out[32]:
                      A        B        C        D
    2000-01-03  float64  float64  float64  float64
    2000-01-04  float64  float64  float64  float64
    2000-01-05  float64  float64  float64  float64
    2000-01-06  float64  float64  float64  float64
    2000-01-07  float64  float64  float64  float64
    
    [5 rows x 4 columns]
    ```
    
    A similar reduction type operation
    
    ``` ipython
    In [33]: panel.apply(lambda x: x.sum(), axis='major_axis')
    Out[33]:
          ItemA     ItemB     ItemC
    A -1.108775 -1.090118 -2.984435
    B -3.705764  0.409204  1.866240
    C  2.110856  2.960500 -0.974967
    D -4.532785  0.303202 -3.685193
    
    [4 rows x 3 columns]
    ```
    
    This is equivalent to
    
    ``` ipython
    In [34]: panel.sum('major_axis')
    Out[34]:
          ItemA     ItemB     ItemC
    A -1.108775 -1.090118 -2.984435
    B -3.705764  0.409204  1.866240
    C  2.110856  2.960500 -0.974967
    D -4.532785  0.303202 -3.685193
    
    [4 rows x 3 columns]
    ```
    
    A transformation operation that returns a Panel, but is computing the z-score across the major\_axis
    
    ``` ipython
    In [35]: result = panel.apply(lambda x: (x - x.mean()) / x.std(),
      ....:                      axis='major_axis')
      ....:
    
    In [36]: result
    Out[36]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 3 (items) x 5 (major_axis) x 4 (minor_axis)
    Items axis: ItemA to ItemC
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
    Minor_axis axis: A to D
    
    In [37]: result['ItemA']                           # noqa E999
    Out[37]:
                      A         B         C         D
    2000-01-03 -0.535778  1.500802 -1.506416 -0.681456
    2000-01-04  0.397628 -1.108752  0.360481  1.529895
    2000-01-05 -1.489811 -0.339412  0.557374  0.280845
    2000-01-06  0.885279  0.421830 -0.453013 -1.053785
    2000-01-07  0.742682 -0.474468  1.041575 -0.075499
    
    [5 rows x 4 columns]
    ```

  - Panel <span class="title-ref">\~pandas.Panel.apply</span> operating on cross-sectional slabs. (`1148`)
    
    ``` ipython
    In [38]: def f(x):
       ....:     return ((x.T - x.mean(1)) / x.std(1)).T
       ....:
    
    In [39]: result = panel.apply(f, axis=['items', 'major_axis'])
    
    In [40]: result
    Out[40]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 4 (items) x 5 (major_axis) x 3 (minor_axis)
    Items axis: A to D
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
    Minor_axis axis: ItemA to ItemC
    
    In [41]: result.loc[:, :, 'ItemA']
    Out[41]:
                       A         B         C         D
    2000-01-03  0.012922 -0.030874 -0.629546 -0.757034
    2000-01-04  0.392053 -1.071665  0.163228  0.548188
    2000-01-05 -1.093650 -0.640898  0.385734 -1.154310
    2000-01-06  1.005446 -1.154593 -0.595615 -0.809185
    2000-01-07  0.783051 -0.198053  0.919339 -1.052721
    
    [5 rows x 4 columns]
    ```
    
    This is equivalent to the following
    
    ``` ipython
    In [42]: result = pd.Panel({ax: f(panel.loc[:, :, ax]) for ax in panel.minor_axis})
    
    In [43]: result
    Out[43]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 4 (items) x 5 (major_axis) x 3 (minor_axis)
    Items axis: A to D
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
    Minor_axis axis: ItemA to ItemC
    
    In [44]: result.loc[:, :, 'ItemA']
    Out[44]:
                       A         B         C         D
    2000-01-03  0.012922 -0.030874 -0.629546 -0.757034
    2000-01-04  0.392053 -1.071665  0.163228  0.548188
    2000-01-05 -1.093650 -0.640898  0.385734 -1.154310
    2000-01-06  1.005446 -1.154593 -0.595615 -0.809185
    2000-01-07  0.783051 -0.198053  0.919339 -1.052721
    
    [5 rows x 4 columns]
    ```

Performance `` ` ~~~~~~~~~~~  Performance improvements for 0.13.1  - Series datetime/timedelta binary operations (:issue:`5801`) - DataFrame ``count/dropna`for`axis=1`- Series.str.contains now has a`regex=False``keyword which can be faster for plain (non-regex) string patterns. (:issue:`5879`) - Series.str.extract (:issue:`5944`) -``dtypes/ftypes``methods (:issue:`5968`) - indexing with object dtypes (:issue:`5968`) -``DataFrame.apply``(:issue:`6013`) - Regression in JSON IO (:issue:`5765`) - Index construction from Series (:issue:`6150`)  Experimental ~~~~~~~~~~~~  There are no experimental changes in 0.13.1  .. _release.bug_fixes-0.13.1:  Bug fixes ~~~~~~~~~  - Bug in``io.wb.get\_countries``not including all countries (:issue:`6008`) - Bug in Series replace with timestamp dict (:issue:`5797`) - read_csv/read_table now respects the``prefix``kwarg (:issue:`5732`). - Bug in selection with missing values via``.ix``from a duplicate indexed DataFrame failing (:issue:`5835`) - Fix issue of boolean comparison on empty DataFrames (:issue:`5808`) - Bug in isnull handling``NaT``in an object array (:issue:`5443`) - Bug in``to\_datetime`when passed a`np.nan``or integer datelike and a format string (:issue:`5863`) - Bug in groupby dtype conversion with datetimelike (:issue:`5869`) - Regression in handling of empty Series as indexers to Series  (:issue:`5877`) - Bug in internal caching, related to (:issue:`5727`) - Testing bug in reading JSON/msgpack from a non-filepath on windows under py3 (:issue:`5874`) - Bug when assigning to .ix[tuple(...)] (:issue:`5896`) - Bug in fully reindexing a Panel (:issue:`5905`) - Bug in idxmin/max with object dtypes (:issue:`5914`) - Bug in``BusinessDay``when adding n days to a date not on offset when n>5 and n%5==0 (:issue:`5890`) - Bug in assigning to chained series with a series via ix (:issue:`5928`) - Bug in creating an empty DataFrame, copying, then assigning (:issue:`5932`) - Bug in DataFrame.tail with empty frame (:issue:`5846`) - Bug in propagating metadata on``resample``(:issue:`5862`) - Fixed string-representation of``NaT``to be "NaT" (:issue:`5708`) - Fixed string-representation for Timestamp to show nanoseconds if present (:issue:`5912`) -``pd.match`not returning passed sentinel -`Panel.to\_frame()`no longer fails when`major\_axis`is a`MultiIndex``(:issue:`5402`). - Bug in``pd.read\_msgpack`with inferring a`DateTimeIndex``frequency   incorrectly (:issue:`5947`) - Fixed``to\_datetime`for array with both Tz-aware datetimes and`NaT``'s  (:issue:`5961`) - Bug in rolling skew/kurtosis when passed a Series with bad data (:issue:`5749`) - Bug in scipy``interpolate``methods with a datetime index (:issue:`5975`) - Bug in NaT comparison if a mixed datetime/np.datetime64 with NaT were passed (:issue:`5968`) - Fixed bug with``pd.concat``losing dtype information if all inputs are empty (:issue:`5742`) - Recent changes in IPython cause warnings to be emitted when using previous versions   of pandas in QTConsole, now fixed. If you're using an older version and   need to suppress the warnings, see (:issue:`5922`). - Bug in merging``timedelta``dtypes (:issue:`5695`) - Bug in plotting.scatter_matrix function. Wrong alignment among diagonal   and off-diagonal plots, see (:issue:`5497`). - Regression in Series with a MultiIndex via ix (:issue:`6018`) - Bug in Series.xs with a MultiIndex (:issue:`6018`) - Bug in Series construction of mixed type with datelike and an integer (which should result in   object type and not automatic conversion) (:issue:`6028`) - Possible segfault when chained indexing with an object array under NumPy 1.7.1 (:issue:`6026`, :issue:`6056`) - Bug in setting using fancy indexing a single element with a non-scalar (e.g. a list),   (:issue:`6043`) -``to\_sql`did not respect`if\_exists``(:issue:`4110` :issue:`4304`) - Regression in``.get(None)``indexing from 0.12 (:issue:`5652`) - Subtle``iloc``indexing bug, surfaced in (:issue:`6059`) - Bug with insert of strings into DatetimeIndex (:issue:`5818`) - Fixed unicode bug in to_html/HTML repr (:issue:`6098`) - Fixed missing arg validation in get_options_data (:issue:`6105`) - Bug in assignment with duplicate columns in a frame where the locations   are a slice (e.g. next to each other) (:issue:`6120`) - Bug in propagating _ref_locs during construction of a DataFrame with dups   index/columns (:issue:`6121`) - Bug in``DataFrame.apply``when using mixed datelike reductions (:issue:`6125`) - Bug in``DataFrame.append``when appending a row with different columns (:issue:`6129`) - Bug in DataFrame construction with recarray and non-ns datetime dtype (:issue:`6140`) - Bug in``.loc``setitem indexing with a dataframe on rhs, multiple item setting, and   a datetimelike (:issue:`6152`) - Fixed a bug in``query`/`eval``during lexicographic string comparisons (:issue:`6155`). - Fixed a bug in``query`where the index of a single-element`Series``was   being thrown away (:issue:`6148`). - Bug in``HDFStore``on appending a dataframe with MultiIndexed columns to   an existing table (:issue:`6167`) - Consistency with dtypes in setting an empty DataFrame (:issue:`6171`) - Bug in selecting on a MultiIndex``HDFStore``even in the presence of under   specified column spec (:issue:`6169`) - Bug in``nanops.var`with`ddof=1`and 1 elements would sometimes return`inf`rather than`nan``on some platforms (:issue:`6136`) - Bug in Series and DataFrame bar plots ignoring the``use\_index``keyword (:issue:`6209`) - Bug in groupby with mixed str/int under python3 fixed;``argsort\`<span class="title-ref"> was failing (:issue:\`6212</span>)

## Contributors

<div class="contributors">

v0.13.0..v0.13.1

</div>

---

v0.14.0.md

---

# Version 0.14.0 (May 31 , 2014)

{{ header }}

This is a major release from 0.13.1 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

  - Highlights include:
      - Officially support Python 3.4
      - SQL interfaces updated to use `sqlalchemy`, See \[Here\<whatsnew\_0140.sql\>\](\#here\<whatsnew\_0140.sql\>).
      - Display interface changes, See \[Here\<whatsnew\_0140.display\>\](\#here\<whatsnew\_0140.display\>)
      - MultiIndexing Using Slicers, See \[Here\<whatsnew\_0140.slicers\>\](\#here\<whatsnew\_0140.slicers\>).
      - Ability to join a singly-indexed DataFrame with a MultiIndexed DataFrame, see \[Here \<merging.join\_on\_mi\>\](\#here-\<merging.join\_on\_mi\>)
      - More consistency in groupby results and more flexible groupby specifications, See \[Here\<whatsnew\_0140.groupby\>\](\#here\<whatsnew\_0140.groupby\>)
      - Holiday calendars are now supported in `CustomBusinessDay`, see \[Here \<timeseries.holiday\>\](\#here-\<timeseries.holiday\>)
      - Several improvements in plotting functions, including: hexbin, area and pie plots, see \[Here\<whatsnew\_0140.plotting\>\](\#here\<whatsnew\_0140.plotting\>).
      - Performance doc section on I/O operations, See \[Here \<io.perf\>\](\#here-\<io.perf\>)
  - \[Other Enhancements \<whatsnew\_0140.enhancements\>\](\#other-enhancements-\<whatsnew\_0140.enhancements\>)
  - \[API Changes \<whatsnew\_0140.api\>\](\#api-changes-\<whatsnew\_0140.api\>)
  - \[Text Parsing API Changes \<whatsnew\_0140.parsing\>\](\#text-parsing-api-changes-\<whatsnew\_0140.parsing\>)
  - \[Groupby API Changes \<whatsnew\_0140.groupby\>\](\#groupby-api-changes-\<whatsnew\_0140.groupby\>)
  - \[Performance Improvements \<whatsnew\_0140.performance\>\](\#performance-improvements-\<whatsnew\_0140.performance\>)
  - \[Prior Deprecations \<whatsnew\_0140.prior\_deprecations\>\](\#prior-deprecations-\<whatsnew\_0140.prior\_deprecations\>)
  - \[Deprecations \<whatsnew\_0140.deprecations\>\](\#deprecations-\<whatsnew\_0140.deprecations\>)
  - \[Known Issues \<whatsnew\_0140.knownissues\>\](\#known-issues-\<whatsnew\_0140.knownissues\>)
  - \[Bug Fixes \<whatsnew\_0140.bug\_fixes\>\](\#bug-fixes-\<whatsnew\_0140.bug\_fixes\>)

\> **Warning** \> In 0.14.0 all `NDFrame` based containers have undergone significant internal refactoring. Before that each block of homogeneous data had its own labels and extra care was necessary to keep those in sync with the parent container's labels. This should not have any visible user/API behavior changes (`6745`)

## API changes

  - `read_excel` uses 0 as the default sheet (`6573`)

  - `iloc` will now accept out-of-bounds indexers for slices, e.g. a value that exceeds the length of the object being indexed. These will be excluded. This will make pandas conform more with python/numpy indexing of out-of-bounds values. A single indexer that is out-of-bounds and drops the dimensions of the object will still raise `IndexError` (`6296`, `6299`). This could result in an empty axis (e.g. an empty DataFrame being returned)
    
    <div class="ipython">
    
    python
    
    dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB')) dfl dfl.iloc\[:, 2:3\] dfl.iloc\[:, 1:3\] dfl.iloc\[4:6\]
    
    </div>
    
    These are out-of-bounds selections
    
      - \`\`\`python  
        \>\>\> dfl.iloc\[\[4, 5, 6\]\] IndexError: positional indexers are out-of-bounds
        
        \>\>\> dfl.iloc\[:, 4\] IndexError: single positional indexer is out-of-bounds

  - Slicing with negative start, stop & step values handles corner cases better (`6531`):
    
      - `df.iloc[:-len(df)]` is now empty
      - `df.iloc[len(df)::-1]` now enumerates all elements in reverse

<!-- end list -->

  - \- The <span class="title-ref">DataFrame.interpolate</span> keyword `downcast` default has been changed from `infer` to  
    `None`. This is to preserve the original dtype unless explicitly requested otherwise (`6290`).

  - `` ` - When converting a dataframe to HTML it used to return ``Empty DataFrame``. This special case has   been removed, instead a header with the column names is returned (:issue:`6062`). -``Series`and`Index`now internally share more common operations, e.g.`factorize(),nunique(),value\_counts()`are   now supported on`Index`types as well. The`Series.weekday`property from is removed   from Series for API consistency. Using a`DatetimeIndex/PeriodIndex`method on a Series will now raise a`TypeError``.   (:issue:`4551`, :issue:`4056`, :issue:`5519`, :issue:`6380`, :issue:`7206`).  - Add``is\_month\_start`,`is\_month\_end`,`is\_quarter\_start`,`is\_quarter\_end`,`is\_year\_start`,`is\_year\_end`accessors for`DateTimeIndex`/`Timestamp`which return a boolean array of whether the timestamp(s) are at the start/end of the month/quarter/year defined by the frequency of the`DateTimeIndex`/`Timestamp``(:issue:`4565`, :issue:`6998`)  - Local variable usage has changed in   `pandas.eval`/`DataFrame.eval`/`DataFrame.query`   (:issue:`5987`). For the `~pandas.DataFrame` methods, two things have   changed    - Column names are now given precedence over locals   - Local variables must be referred to explicitly. This means that even if     you have a local variable that is *not* a column you must still refer to     it with the``'@'`prefix.   - You can have an expression like`df.query('@a \< a')`with no complaints     from`pandas`about ambiguity of the name`a``.   - The top-level `pandas.eval` function does not allow you use the``'@'`prefix and provides you with an error message telling you so.   -`NameResolutionError``was removed because it isn't necessary anymore.  - Define and document the order of column vs index names in query/eval (:issue:`6676`) -``concat``will now concatenate mixed Series and DataFrames using the Series name   or numbering columns as needed (:issue:`2385`). See [the docs <merging.mixed_ndims>](#the-docs-<merging.mixed_ndims>) - Slicing and advanced/boolean indexing operations on``Index``classes as well   as `Index.delete` and `Index.drop` methods will no longer change the type of the   resulting index (:issue:`6440`, :issue:`7040`)    .. ipython:: python       i = pd.Index([1, 2, 3, 'a', 'b', 'c'])      i[[0, 1, 2]]      i.drop(['a', 'b', 'c'])    Previously, the above operation would return``Int64Index``.  If you'd like   to do this manually, use `Index.astype`    .. ipython:: python       i[[0, 1, 2]].astype(np.int_)  -``set\_index``no longer converts MultiIndexes to an Index of tuples. For example,   the old behavior returned an Index in this case (:issue:`6459`):    .. ipython:: python      :suppress:       np.random.seed(1234)      from itertools import product      tuples = list(product(('a', 'b'), ('c', 'd')))      mi = pd.MultiIndex.from_tuples(tuples)      df_multi = pd.DataFrame(np.random.randn(4, 2), index=mi)      tuple_ind = pd.Index(tuples, tupleize_cols=False)      df_multi.index    .. ipython:: python       # Old behavior, casted MultiIndex to an Index      tuple_ind      df_multi.set_index(tuple_ind)       # New behavior      mi      df_multi.set_index(mi)    This also applies when passing multiple indices to``set\_index`:    .. ipython:: python      @suppress     df_multi.index = tuple_ind      # Old output, 2-level MultiIndex of tuples     df_multi.set_index([df_multi.index, df_multi.index])      @suppress     df_multi.index = mi      # New output, 4-level MultiIndex     df_multi.set_index([df_multi.index, df_multi.index])  -`pairwise`keyword was added to the statistical moment functions`rolling\_cov`,`rolling\_corr`,`ewmcov`,`ewmcorr`,`expanding\_cov`,`expanding\_corr``to allow the calculation of moving   window covariance and correlation matrices (:issue:`4950`). See   [Computing rolling pairwise covariances and correlations   <window.corr_pairwise>](#computing-rolling-pairwise-covariances-and-correlations --<window.corr_pairwise>) in the docs.``\`ipython  
    In \[1\]: df = pd.DataFrame(np.random.randn(10, 4), columns=list('ABCD'))
    
      - In \[4\]: covs = pd.rolling\_cov(df\[\['A', 'B', 'C'\]\],  
        ....: df\[\['B', 'C', 'D'\]\], ....: 5, ....: pairwise=True)
    
    In \[5\]: covs\[df.index\[-1\]\] Out\[5\]: B C D A 0.035310 0.326593 -0.505430 B 0.137748 -0.006888 -0.005383 C -0.006888 0.861040 0.020762

<!-- end list -->

  - `Series.iteritems()` is now lazy (returns an iterator rather than a list). This was the documented behavior prior to 0.14. (`6760`)

\- Added `nunique` and `value_counts` functions to `Index` for counting unique elements. (`6734`) `` ` - ``stack`and`unstack`now raise a`ValueError`when the`level`keyword refers   to a non-unique item in the`Index`(previously raised a`KeyError``). (:issue:`6738`) - drop unused order argument from``Series.sort`; args now are in the same order as`Series.order`;   add`na\_position`arg to conform to`Series.order``(:issue:`6847`) - default sorting algorithm for``Series.order`is now`quicksort`, to conform with`Series.sort`(and numpy defaults) - add`inplace`keyword to`Series.order/sort``to make them inverses (:issue:`6859`) -``DataFrame.sort`now places NaNs at the beginning or end of the sort according to the`na\_position``parameter. (:issue:`3917`) - accept``TextFileReader`in`concat``, which was affecting a common user idiom (:issue:`6583`), this was a regression   from 0.13.1 - Added``factorize`functions to`Index`and`Series``to get indexer and unique values (:issue:`7090`) -``describe``on a DataFrame with a mix of Timestamp and string like objects returns a different Index (:issue:`7088`).   Previously the index was unintentionally sorted. - Arithmetic operations with **only**``bool`dtypes now give a warning indicating   that they are evaluated in Python space for`+`,`-`,   and`\*``operations and raise for all others (:issue:`7011`, :issue:`6762`,   :issue:`7015`, :issue:`7210`)``\`python \>\>\> x = pd.Series(np.random.rand(10) \> 0.5) \>\>\> y = True \>\>\> x + y \# warning generated: should do x | y instead UserWarning: evaluating in Python space because the '+' operator is not supported by numexpr for the bool dtype, use '|' instead \>\>\> x / y \# this raises because it doesn't make sense NotImplementedError: operator '/' not implemented for bool dtypes

\- In `HDFStore`, `select_as_multiple` will always raise a `KeyError`, when a key or the selector is not found (`6177`) `` ` - ``df\['col'\] = value`and`df.loc\[:,'col'\] = value`are now completely equivalent;   previously the`.loc``would not necessarily coerce the dtype of the resultant series (:issue:`6149`) -``dtypes`and`ftypes`now return a series with`dtype=object``on empty containers (:issue:`5740`) -``df.to\_csv``will now return a string of the CSV data if neither a target path nor a buffer is provided   (:issue:`6061`) -``pd.infer\_freq()`will now raise a`TypeError`if given an invalid`Series/Index``type (:issue:`6407`, :issue:`6463`) - A tuple passed to``DataFame.sort\_index``will be interpreted as the levels of   the index, rather than requiring a list of tuple (:issue:`4370`) - all offset operations now return``Timestamp``types (rather than datetime), Business/Week frequencies were incorrect (:issue:`4069`) -``to\_excel`now converts`np.inf`into a string representation,   customizable by the`inf\_rep``keyword argument (Excel has no native inf   representation) (:issue:`6782`) - Replace``pandas.compat.scipy.scoreatpercentile`with`numpy.percentile``(:issue:`6810`) -``.quantile`on a`datetime\[ns\]`series now returns`Timestamp`instead   of`np.datetime64``objects (:issue:`6810`) - change``AssertionError`to`TypeError`for invalid types passed to`concat``(:issue:`6583`) - Raise a``TypeError`when`DataFrame`is passed an iterator as the`data``argument (:issue:`5357`)   .. _whatsnew_0140.display:  Display changes ~~~~~~~~~~~~~~~  - The default way of printing large DataFrames has changed. DataFrames   exceeding``max\_rows`and/or`max\_columns``are now displayed in a   centrally truncated view, consistent with the printing of a   `pandas.Series` (:issue:`5603`).    In previous versions, a DataFrame was truncated once the dimension   constraints were reached and an ellipse (...) signaled that part of   the data was cut off.    .. image:: ../_static/trunc_before.png       :alt: The previous look of truncate.    In the current version, large DataFrames are centrally truncated,   showing a preview of head and tail in both dimensions.    .. image:: ../_static/trunc_after.png      :alt: The new look.  - allow option``'truncate'`for`display.show\_dimensions``to only show the dimensions if the   frame is truncated (:issue:`6547`).    The default for``display.show\_dimensions`will now be`truncate`. This is consistent with   how Series display length.    .. ipython:: python       dfd = pd.DataFrame(np.arange(25).reshape(-1, 5),                         index=[0, 1, 2, 3, 4],                         columns=[0, 1, 2, 3, 4])       # show dimensions since this is truncated      with pd.option_context('display.max_rows', 2, 'display.max_columns', 2,                             'display.show_dimensions', 'truncate'):          print(dfd)       # will not show dimensions since it is not truncated      with pd.option_context('display.max_rows', 10, 'display.max_columns', 40,                             'display.show_dimensions', 'truncate'):          print(dfd)  - Regression in the display of a MultiIndexed Series with`display.max\_rows``is less than the   length of the series (:issue:`7101`) - Fixed a bug in the HTML repr of a truncated Series or DataFrame not showing the class name with the``large\_repr``set to 'info' (:issue:`7105`) - The``verbose`keyword in`DataFrame.info()`, which controls whether to shorten the`info`representation, is now`None`by default. This will follow the global setting in`display.max\_info\_columns`. The global setting can be overridden with`verbose=True`or`verbose=False`. - Fixed a bug with the`info`repr not honoring the`display.max\_info\_columns``setting (:issue:`6939`) - Offset/freq info now in Timestamp __repr__ (:issue:`4553`)  .. _whatsnew_0140.parsing:  Text parsing API changes ~~~~~~~~~~~~~~~~~~~~~~~~  `read_csv`/`read_table` will now be noisier w.r.t invalid options rather than falling back to the``PythonParser`.  - Raise`ValueError`when`sep`specified with`delim\_whitespace=True``in `read_csv`/`read_table`   (:issue:`6607`) - Raise``ValueError`when`engine='c'``specified with unsupported   options in `read_csv`/`read_table` (:issue:`6607`) - Raise``ValueError``when fallback to python parser causes options to be   ignored (:issue:`6607`) - Produce `~pandas.io.parsers.ParserWarning` on fallback to python   parser when no options are ignored (:issue:`6607`) - Translate``sep='s+'`to`delim\_whitespace=True``in   `read_csv`/`read_table` if no other C-unsupported options   specified (:issue:`6607`)  .. _whatsnew_0140.groupby:  GroupBy API changes ~~~~~~~~~~~~~~~~~~~  More consistent behavior for some groupby methods:  - groupby``head`and`tail`now act more like`filter`rather than an aggregation:`\`ipython In \[1\]: df = pd.DataFrame(\[\[1, 2\], \[1, 4\], \[5, 6\]\], columns=\['A', 'B'\])

> In \[2\]: g = df.groupby('A')
> 
> In \[3\]: g.head(1) \# filters DataFrame Out\[3\]: A B 0 1 2 2 5 6
> 
> In \[4\]: g.apply(lambda x: x.head(1)) \# used to simply fall-through Out\[4\]: A B A 1 0 1 2 5 2 5 6

  - groupby head and tail respect column selection:
    
    ``` ipython
    In [19]: g[['B']].head(1)
    Out[19]:
       B
    0  2
    2  6
    
    [2 rows x 1 columns]
    ```

  - groupby `nth` now reduces by default; filtering can be achieved by passing `as_index=False`. With an optional `dropna` argument to ignore NaN. See \[the docs \<groupby.nth\>\](\#the-docs-\<groupby.nth\>).
    
    Reducing
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame(\[\[1, np.nan\], \[1, 4\], \[5, 6\]\], columns=\['A', 'B'\]) g = df.groupby('A') g.nth(0)
    
    \# this is equivalent to g.first() g.nth(0, dropna='any')
    
    \# this is equivalent to g.last() g.nth(-1, dropna='any')
    
    </div>
    
    Filtering
    
    <div class="ipython">
    
    python
    
    gf = df.groupby('A', as\_index=False) gf.nth(0) gf.nth(0, dropna='any')
    
    </div>

  - groupby will now not return the grouped column for non-cython functions (`5610`, `5614`, `6732`), as its already the index
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame(\[\[1, np.nan\], \[1, 4\], \[5, 6\], \[5, 8\]\], columns=\['A', 'B'\]) g = df.groupby('A') g.count() g.describe()
    
    </div>

  - passing `as_index` will leave the grouped column in-place (this is not change in 0.14.0)
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame(\[\[1, np.nan\], \[1, 4\], \[5, 6\], \[5, 8\]\], columns=\['A', 'B'\]) g = df.groupby('A', as\_index=False) g.count() g.describe()
    
    </div>

  - Allow specification of a more complex groupby via `pd.Grouper`, such as grouping by a Time and a string field simultaneously. See \[the docs \<groupby.specify\>\](\#the-docs-\<groupby.specify\>). (`3794`)

  - Better propagation/preservation of Series names when performing groupby operations:
    
      - `SeriesGroupBy.agg` will ensure that the name attribute of the original series is propagated to the result (`6265`).
      - If the function provided to `GroupBy.apply` returns a named series, the name of the series will be kept as the name of the column index of the DataFrame returned by `GroupBy.apply` (`6124`). This facilitates `DataFrame.stack` operations where the name of the column index is used as the name of the inserted column containing the pivoted data.

### SQL

\~\~\~

The SQL reading and writing functions now support more database flavors through SQLAlchemy (`2717`, `4163`, `5950`, `6292`). All databases supported by SQLAlchemy can be used, such as PostgreSQL, MySQL, Oracle, Microsoft SQL server (see documentation of SQLAlchemy on [included dialects](https://sqlalchemy.readthedocs.io/en/latest/dialects/index.html)).

The functionality of providing DBAPI connection objects will only be supported for sqlite3 in the future. The `'mysql'` flavor is deprecated.

The new functions <span class="title-ref">\~pandas.read\_sql\_query</span> and <span class="title-ref">\~pandas.read\_sql\_table</span> are introduced. The function <span class="title-ref">\~pandas.read\_sql</span> is kept as a convenience wrapper around the other two and will delegate to specific function depending on the provided input (database table name or sql query).

In practice, you have to provide a SQLAlchemy `engine` to the sql functions. To connect with SQLAlchemy you use the <span class="title-ref">create\_engine</span> function to create an engine object from database URI. You only need to create the engine once per database you are connecting to. For an in-memory sqlite database:

<div class="ipython">

python

from sqlalchemy import create\_engine \# Create your connection. engine = create\_engine('sqlite:///:memory:')

</div>

This `engine` can then be used to write or read data to/from this database:

<div class="ipython">

python

df = pd.DataFrame({'A': \[1, 2, 3\], 'B': \['a', 'b', 'c'\]}) df.to\_sql(name='db\_table', con=engine, index=False)

</div>

You can read data from a database by specifying the table name:

<div class="ipython">

python

pd.read\_sql\_table('db\_table', engine)

</div>

or by specifying a sql query:

<div class="ipython">

python

pd.read\_sql\_query('SELECT \* FROM db\_table', engine)

</div>

Some other enhancements to the sql functions include:

  - support for writing the index. This can be controlled with the `index` keyword (default is True).
  - specify the column label to use when writing the index with `index_label`.
  - specify string columns to parse as datetimes with the `parse_dates` keyword in <span class="title-ref">\~pandas.read\_sql\_query</span> and <span class="title-ref">\~pandas.read\_sql\_table</span>.

\> **Warning** \> Some of the existing functions or function aliases have been deprecated and will be removed in future versions. This includes: `tquery`, `uquery`, `read_frame`, `frame_query`, `write_frame`.

<div class="warning">

<div class="title">

Warning

</div>

The support for the 'mysql' flavor when using DBAPI connection objects has been deprecated. MySQL will be further supported with SQLAlchemy engines (`6900`).

</div>

## Multi-indexing using slicers

In 0.14.0 we added a new way to slice MultiIndexed objects. You can slice a MultiIndex by providing multiple indexers.

You can provide any of the selectors as if you are indexing by label, see \[Selection by Label \<indexing.label\>\](\#selection-by-label-\<indexing.label\>), including slices, lists of labels, labels, and boolean indexers.

You can use `slice(None)` to select all the contents of *that* level. You do not need to specify all the *deeper* levels, they will be implied as `slice(None)`.

As usual, **both sides** of the slicers are included as this is label indexing.

See \[the docs\<advanced.mi\_slicers\>\](\#the-docs\<advanced.mi\_slicers\>) See also issues (`6134`, `4036`, `3057`, `2598`, `5641`, `7106`)

\> **Warning** \> You should specify all axes in the `.loc` specifier, meaning the indexer for the **index** and for the **columns**. Their are some ambiguous cases where the passed indexer could be mis-interpreted as indexing *both* axes, rather than into say the MultiIndex for the rows.

> You should do this:
> 
>   - \`\`\`python  
>     \>\>\> df.loc\[(slice('A1', 'A3'), ...), :\] \# noqa: E901
> 
> rather than this:
> 
> ``` python
> >>> df.loc[(slice('A1', 'A3'), ...)]  # noqa: E901
> ```

<div class="warning">

<div class="title">

Warning

</div>

You will need to make sure that the selection axes are fully lexsorted\!

</div>

<div class="ipython">

python

  - def mklbl(prefix, n):  
    return \["%s%s" % (prefix, i) for i in range(n)\]

  - index = pd.MultiIndex.from\_product(\[mklbl('A', 4),  
    mklbl('B', 2), mklbl('C', 4), mklbl('D', 2)\])

  - columns = pd.MultiIndex.from\_tuples(\[('a', 'foo'), ('a', 'bar'),  
    ('b', 'foo'), ('b', 'bah')\], names=\['lvl0', 'lvl1'\])

  - df = pd.DataFrame(np.arange(len(index) \* len(columns)).reshape((len(index),  
    len(columns))), index=index, columns=columns).sort\_index().sort\_index(axis=1)

df

</div>

Basic MultiIndex slicing using slices, lists, and labels.

<div class="ipython">

python

df.loc\[(slice('A1', 'A3'), slice(None), \['C1', 'C3'\]), :\]

</div>

You can use a `pd.IndexSlice` to shortcut the creation of these slices

<div class="ipython">

python

idx = pd.IndexSlice df.loc\[idx\[:, :, \['C1', 'C3'\]\], idx\[:, 'foo'\]\]

</div>

It is possible to perform quite complicated selections using this method on multiple `` ` axes at the same time.  .. ipython:: python     df.loc['A1', (slice(None), 'foo')]    df.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]  Using a boolean indexer you can provide selection related to the *values*.  .. ipython:: python     mask = df[('a', 'foo')] > 200    df.loc[idx[mask, :, ['C1', 'C3']], idx[:, 'foo']]  You can also specify the ``axis`argument to`.loc`to interpret the passed slicers on a single axis.  .. ipython:: python     df.loc(axis=0)[:, :, ['C1', 'C3']]  Furthermore you can *set* the values using these methods  .. ipython:: python     df2 = df.copy()    df2.loc(axis=0)[:, :, ['C1', 'C3']] = -10    df2  You can use a right-hand-side of an alignable object as well.  .. ipython:: python     df2 = df.copy()    df2.loc[idx[:, :, ['C1', 'C3']], :] = df2 * 1000    df2  .. _whatsnew_0140.plotting:  Plotting ~~~~~~~~  - Hexagonal bin plots from`DataFrame.plot`with`kind='hexbin'``(:issue:`5478`), See [the docs<visualization.hexbin>](#the-docs<visualization.hexbin>). -``DataFrame.plot`and`Series.plot`now supports area plot with specifying`kind='area'``(:issue:`6656`), See [the docs<visualization.area_plot>](#the-docs<visualization.area_plot>) - Pie plots from``Series.plot`and`DataFrame.plot`with`kind='pie'``(:issue:`6976`), See [the docs<visualization.pie>](#the-docs<visualization.pie>). - Plotting with Error Bars is now supported in the``.plot`method of`DataFrame`and`Series``objects (:issue:`3796`, :issue:`6834`), See [the docs<visualization.errorbars>](#the-docs<visualization.errorbars>). -``DataFrame.plot`and`Series.plot`now support a`table`keyword for plotting`matplotlib.Table`, See [the docs<visualization.table>](#the-docs<visualization.table>). The`table`keyword can receive the following values.    -`False`: Do nothing (default).   -`True`: Draw a table using the`DataFrame`or`Series`called`plot`method. Data will be transposed to meet matplotlib's default layout.   -`DataFrame`or`Series`: Draw matplotlib.table using the passed data. The data will be drawn as displayed in print method (not transposed automatically).     Also, helper function`pandas.tools.plotting.table`is added to create a table from`DataFrame`and`Series`, and add it to an`matplotlib.Axes`.  -`plot(legend='reverse')``will now reverse the order of legend labels for   most plot kinds. (:issue:`6014`) - Line plot and area plot can be stacked by``stacked=True``(:issue:`6656`)  - Following keywords are now acceptable for `DataFrame.plot` with``kind='bar'`and`kind='barh'`:    -`width``: Specify the bar width. In previous versions, static value 0.5 was passed to matplotlib and it cannot be overwritten. (:issue:`6604`)   -``align`: Specify the bar alignment. Default is`center`(different from matplotlib). In previous versions, pandas passes`align='edge'`to matplotlib and adjust the location to`center`by itself, and it results`align``keyword is not applied as expected. (:issue:`4525`)   -``position``: Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1(right/top-end). Default is 0.5 (center). (:issue:`6604`)    Because of the default``align`value changes, coordinates of bar plots are now located on integer values (0.0, 1.0, 2.0 ...). This is intended to make bar plot be located on the same coordinates as line plot. However, bar plot may differs unexpectedly when you manually adjust the bar location or drawing area, such as using`set\_xlim`,`set\_ylim``, etc. In this cases, please modify your script to meet with new coordinates.  - The `parallel_coordinates` function now takes argument``color`instead of`colors`. A`FutureWarning`is raised to alert that   the old`colors``argument will not be supported in a future release. (:issue:`6956`)  - The `parallel_coordinates` and `andrews_curves` functions now take   positional argument``frame`instead of`data`. A`FutureWarning`is   raised if the old`data``argument is used by name. (:issue:`6956`)  - `DataFrame.boxplot` now supports``layout``keyword (:issue:`6769`) - `DataFrame.boxplot` has a new keyword argument,``return\_type`. It accepts`'dict'`,`'axes'`, or`'both'``, in which case a namedtuple with the matplotlib   axes and a dict of matplotlib Lines is returned.   .. _whatsnew_0140.prior_deprecations:  Prior version deprecations/changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  There are prior version deprecations that are taking effect as of 0.14.0.  - Remove `DateRange` in favor of `DatetimeIndex` (:issue:`6816`) - Remove``column`keyword from`DataFrame.sort``(:issue:`4370`) - Remove``precision``keyword from `set_eng_float_format` (:issue:`395`) - Remove``force\_unicode``keyword from `DataFrame.to_string`,   `DataFrame.to_latex`, and `DataFrame.to_html`; these function   encode in unicode by default (:issue:`2224`, :issue:`2225`) - Remove``nanRep``keyword from `DataFrame.to_csv` and   `DataFrame.to_string` (:issue:`275`) - Remove``unique``keyword from `HDFStore.select_column` (:issue:`3256`) - Remove``inferTimeRule``keyword from `Timestamp.offset` (:issue:`391`) - Remove``name``keyword from `get_data_yahoo` and   `get_data_google` ( `commit b921d1a <https://github.com/pandas-dev/pandas/commit/b921d1a2>`__ ) - Remove``offset``keyword from `DatetimeIndex` constructor   ( `commit 3136390 <https://github.com/pandas-dev/pandas/commit/3136390>`__ ) - Remove``time\_rule``from several rolling-moment statistical functions, such   as `rolling_sum` (:issue:`1042`) - Removed neg``-`boolean operations on numpy arrays in favor of inv`\~``, as this is going to   be deprecated in numpy 1.9 (:issue:`6960`)  .. _whatsnew_0140.deprecations:  Deprecations ~~~~~~~~~~~~  - The `pivot_table`/`DataFrame.pivot_table` and `crosstab` functions   now take arguments``index`and`columns`instead of`rows`and`cols`.  A`FutureWarning`is raised to alert that the old`rows`and`cols``arguments   will not be supported in a future release (:issue:`5505`)  - The `DataFrame.drop_duplicates` and `DataFrame.duplicated` methods   now take argument``subset`instead of`cols``to better align with   `DataFrame.dropna`.  A``FutureWarning`is raised to alert that the old`cols``arguments will not be supported in a future release (:issue:`6680`)  - The `DataFrame.to_csv` and `DataFrame.to_excel` functions   now takes argument``columns`instead of`cols`.  A`FutureWarning`is raised to alert that the old`cols``arguments   will not be supported in a future release (:issue:`6645`)  - Indexers will warn``FutureWarning``when used with a scalar indexer and   a non-floating point Index (:issue:`4892`, :issue:`6960`)``\`ipython \# non-floating point indexes can only be indexed by integers / labels In \[1\]: pd.Series(1, np.arange(5))\[3.0\] pandas/core/index.py:469: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point Out\[1\]: 1

>   - In \[2\]: pd.Series(1, np.arange(5)).iloc\[3.0\]  
>     pandas/core/index.py:469: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point
> 
> Out\[2\]: 1
> 
>   - In \[3\]: pd.Series(1, np.arange(5)).iloc\[3.0:4\]  
>     pandas/core/index.py:527: FutureWarning: slice indexers when using iloc should be integers and not floating point
> 
>   - Out\[3\]:  
>     3 1 dtype: int64
> 
> \# these are Float64Indexes, so integer or floating point is acceptable In \[4\]: pd.Series(1, np.arange(5.))\[3\] Out\[4\]: 1
> 
> In \[5\]: pd.Series(1, np.arange(5.))\[3.0\] Out\[6\]: 1

  - Numpy 1.9 compat w.r.t. deprecation warnings (`6960`)

<!-- end list -->

  - \- <span class="title-ref">Panel.shift</span> now has a function signature that matches <span class="title-ref">DataFrame.shift</span>.  
    The old positional argument `lags` has been changed to a keyword argument `periods` with a default value of 1. A `FutureWarning` is raised if the old argument `lags` is used by name. (`6910`)

  - `` ` - The ``order``keyword argument of `factorize` will be removed. (:issue:`6926`).  - Remove the``copy``keyword from `DataFrame.xs`, `Panel.major_xs`, `Panel.minor_xs`. A view will be   returned if possible, otherwise a copy will be made. Previously the user could think that``copy=False``would   ALWAYS return a view. (:issue:`6894`)  - The `parallel_coordinates` function now takes argument``color`instead of`colors`. A`FutureWarning`is raised to alert that   the old`colors``argument will not be supported in a future release. (:issue:`6956`)  - The `parallel_coordinates` and `andrews_curves` functions now take   positional argument``frame`instead of`data`. A`FutureWarning`is   raised if the old`data``argument is used by name. (:issue:`6956`)  - The support for the 'mysql' flavor when using DBAPI connection objects has been deprecated.   MySQL will be further supported with SQLAlchemy engines (:issue:`6900`).  - The following``io.sql`functions have been deprecated:`tquery`,`uquery`,`read\_frame`,`frame\_query`,`write\_frame`.  - The`percentile\_width``keyword argument in `~DataFrame.describe` has been deprecated.   Use the``percentiles``keyword instead, which takes a list of percentiles to display. The   default output is unchanged.  - The default return type of `boxplot` will change from a dict to a matplotlib Axes   in a future release. You can use the future behavior now by passing``return\_type='axes'``to boxplot.  .. _whatsnew_0140.knownissues:  Known issues ~~~~~~~~~~~~  - OpenPyXL 2.0.0 breaks backwards compatibility (:issue:`7169`)   .. _whatsnew_0140.enhancements:  Enhancements ~~~~~~~~~~~~  - DataFrame and Series will create a MultiIndex object if passed a tuples dict, See [the docs<basics.dataframe.from_dict_of_tuples>](#the-docs<basics.dataframe.from_dict_of_tuples>) (:issue:`3323`)    .. ipython:: python       pd.Series({('a', 'b'): 1, ('a', 'a'): 0,                 ('a', 'c'): 2, ('b', 'a'): 3, ('b', 'b'): 4})      pd.DataFrame({('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},                   ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},                   ('a', 'c'): {('A', 'B'): 5, ('A', 'C'): 6},                   ('b', 'a'): {('A', 'C'): 7, ('A', 'B'): 8},                   ('b', 'b'): {('A', 'D'): 9, ('A', 'B'): 10}})  - Added the``sym\_diff`method to`Index``(:issue:`5543`) -``DataFrame.to\_latex``now takes a longtable keyword, which if True will return a table in a longtable environment. (:issue:`6617`) - Add option to turn off escaping in``DataFrame.to\_latex``(:issue:`6472`) -``pd.read\_clipboard`will, if the keyword`sep``is unspecified, try to detect data copied from a spreadsheet   and parse accordingly. (:issue:`6223`) - Joining a singly-indexed DataFrame with a MultiIndexed DataFrame (:issue:`3662`)    See [the docs<merging.join_on_mi>](#the-docs<merging.join_on_mi>). Joining MultiIndex DataFrames on both the left and right is not yet supported ATM.    .. ipython:: python       household = pd.DataFrame({'household_id': [1, 2, 3],                                'male': [0, 1, 0],                                'wealth': [196087.3, 316478.7, 294750]                                },                               columns=['household_id', 'male', 'wealth']                               ).set_index('household_id')      household      portfolio = pd.DataFrame({'household_id': [1, 2, 2, 3, 3, 3, 4],                                'asset_id': ["nl0000301109",                                             "nl0000289783",                                             "gb00b03mlx29",                                             "gb00b03mlx29",                                             "lu0197800237",                                             "nl0000289965",                                             np.nan],                                'name': ["ABN Amro",                                         "Robeco",                                         "Royal Dutch Shell",                                         "Royal Dutch Shell",                                         "AAB Eastern Europe Equity Fund",                                         "Postbank BioTech Fonds",                                         np.nan],                                'share': [1.0, 0.4, 0.6, 0.15, 0.6, 0.25, 1.0]                                },                               columns=['household_id', 'asset_id', 'name', 'share']                               ).set_index(['household_id', 'asset_id'])      portfolio       household.join(portfolio, how='inner')  -``quotechar`,`doublequote`, and`escapechar`can now be specified when   using`DataFrame.to\_csv``(:issue:`5414`, :issue:`4528`) - Partially sort by only the specified levels of a MultiIndex with the``sort\_remaining``boolean kwarg. (:issue:`3984`) - Added``to\_julian\_date`to`TimeStamp`and`DatetimeIndex``.  The Julian   Date is used primarily in astronomy and represents the number of days from   noon, January 1, 4713 BC.  Because nanoseconds are used to define the time   in pandas the actual range of dates that you can use is 1678 AD to 2262 AD. (:issue:`4041`) -``DataFrame.to\_stata``will now check data for compatibility with Stata data types   and will upcast when needed.  When it is not possible to losslessly upcast, a warning   is issued (:issue:`6327`) -``DataFrame.to\_stata`and`StataWriter``will accept keyword arguments time_stamp   and data_label which allow the time stamp and dataset label to be set when creating a   file. (:issue:`6545`) -``pandas.io.gbq``now handles reading unicode strings properly. (:issue:`5940`) - [Holidays Calendars<timeseries.holiday>](#holidays-calendars<timeseries.holiday>) are now available and can be used with the``CustomBusinessDay``offset (:issue:`6719`) -``Float64Index`is now backed by a`float64`dtype ndarray instead of an`object``dtype array (:issue:`6471`). - Implemented``Panel.pct\_change``(:issue:`6904`) - Added``how``option to rolling-moment functions to dictate how to handle resampling; `rolling_max` defaults to max,   `rolling_min` defaults to min, and all others default to mean (:issue:`6297`) -``CustomBusinessMonthBegin`and`CustomBusinessMonthEnd``are now available (:issue:`6866`) - `Series.quantile` and `DataFrame.quantile` now accept an array of   quantiles. - `~DataFrame.describe` now accepts an array of percentiles to include in the summary statistics (:issue:`4196`) -``pivot\_table`can now accept`Grouper`by`index`and`columns``keywords (:issue:`6913`)    .. ipython:: python       import datetime      df = pd.DataFrame({          'Branch': 'A A A A A B'.split(),          'Buyer': 'Carl Mark Carl Carl Joe Joe'.split(),          'Quantity': [1, 3, 5, 1, 8, 1],          'Date': [datetime.datetime(2013, 11, 1, 13, 0),                   datetime.datetime(2013, 9, 1, 13, 5),                   datetime.datetime(2013, 10, 1, 20, 0),                   datetime.datetime(2013, 10, 2, 10, 0),                   datetime.datetime(2013, 11, 1, 20, 0),                   datetime.datetime(2013, 10, 2, 10, 0)],          'PayDay': [datetime.datetime(2013, 10, 4, 0, 0),                     datetime.datetime(2013, 10, 15, 13, 5),                     datetime.datetime(2013, 9, 5, 20, 0),                     datetime.datetime(2013, 11, 2, 10, 0),                     datetime.datetime(2013, 10, 7, 20, 0),                     datetime.datetime(2013, 9, 5, 10, 0)]})      df``\`ipython
    
      - In \[75\]: df.pivot\_table(values='Quantity',  
        ....: index=pd.Grouper(freq='M', key='Date'), ....: columns=pd.Grouper(freq='M', key='PayDay'), ....: aggfunc="sum")
    
    Out\[75\]: PayDay 2013-09-30 2013-10-31 2013-11-30 Date 2013-09-30 NaN 3.0 NaN 2013-10-31 6.0 NaN 1.0 2013-11-30 NaN 9.0 NaN
    
    \[3 rows x 3 columns\]

\- Arrays of strings can be wrapped to a specified width (`str.wrap`) (`6999`) `` ` - Add `~Series.nsmallest` and `Series.nlargest` methods to Series, See [the docs <basics.nsorted>](#the-docs-<basics.nsorted>) (:issue:`3960`)  - ``PeriodIndex`fully supports partial string indexing like`DatetimeIndex``(:issue:`7043`)``\`ipython In \[76\]: prng = pd.period\_range('2013-01-01 09:00', periods=100, freq='H')

> In \[77\]: ps = pd.Series(np.random.randn(len(prng)), index=prng)
> 
> In \[78\]: ps Out\[78\]: 2013-01-01 09:00 0.015696 2013-01-01 10:00 -2.242685 2013-01-01 11:00 1.150036 2013-01-01 12:00 0.991946 2013-01-01 13:00 0.953324 ... 2013-01-05 08:00 0.285296 2013-01-05 09:00 0.484288 2013-01-05 10:00 1.363482 2013-01-05 11:00 -0.781105 2013-01-05 12:00 -0.468018 Freq: H, Length: 100, dtype: float64
> 
> In \[79\]: ps\['2013-01-02'\] Out\[79\]: 2013-01-02 00:00 0.553439 2013-01-02 01:00 1.318152 2013-01-02 02:00 -0.469305 2013-01-02 03:00 0.675554 2013-01-02 04:00 -1.817027 ... 2013-01-02 19:00 0.036142 2013-01-02 20:00 -2.074978 2013-01-02 21:00 0.247792 2013-01-02 22:00 -0.897157 2013-01-02 23:00 -0.136795 Freq: H, Length: 24, dtype: float64

\- `read_excel` can now read milliseconds in Excel dates and times with xlrd \>= 0.9.3. (`5945`) `` ` - ``pd.stats.moments.rolling\_var``now uses Welford's method for increased numerical stability (:issue:`6817`) - pd.expanding_apply and pd.rolling_apply now take args and kwargs that are passed on to   the func (:issue:`6289`) -``DataFrame.rank()``now has a percentage rank option (:issue:`5971`) -``Series.rank()``now has a percentage rank option (:issue:`5971`) -``Series.rank()`and`DataFrame.rank()`now accept`method='dense'``for ranks without gaps (:issue:`6514`) - Support passing``encoding``with xlwt (:issue:`3710`) - Refactor Block classes removing``Block.items``attributes to avoid duplication   in item handling (:issue:`6745`, :issue:`6988`). - Testing statements updated to use specialized asserts (:issue:`6175`)    .. _whatsnew_0140.performance:  Performance ~~~~~~~~~~~  - Performance improvement when converting``DatetimeIndex`to floating ordinals   using`DatetimeConverter``(:issue:`6636`) - Performance improvement for``DataFrame.shift``(:issue:`5609`) - Performance improvement in indexing into a MultiIndexed Series (:issue:`5567`) - Performance improvements in single-dtyped indexing (:issue:`6484`) - Improve performance of DataFrame construction with certain offsets, by removing faulty caching   (e.g. MonthEnd,BusinessMonthEnd), (:issue:`6479`) - Improve performance of``CustomBusinessDay``(:issue:`6584`) - improve performance of slice indexing on Series with string keys (:issue:`6341`, :issue:`6372`) - Performance improvement for``DataFrame.from\_records``when reading a   specified number of rows from an iterable (:issue:`6700`) - Performance improvements in timedelta conversions for integer dtypes (:issue:`6754`) - Improved performance of compatible pickles (:issue:`6899`) - Improve performance in certain reindexing operations by optimizing``take\_2d``(:issue:`6749`) -``GroupBy.count()``is now implemented in Cython and is much faster for large   numbers of groups (:issue:`7016`).  Experimental ~~~~~~~~~~~~  There are no experimental changes in 0.14.0   .. _whatsnew_0140.bug_fixes:  Bug fixes ~~~~~~~~~  - Bug in Series ValueError when index doesn't match data (:issue:`6532`) - Prevent segfault due to MultiIndex not being supported in HDFStore table   format (:issue:`1848`) - Bug in``pd.DataFrame.sort\_index`where mergesort wasn't stable when`ascending=False``(:issue:`6399`) - Bug in``pd.tseries.frequencies.to\_offset``when argument has leading zeros (:issue:`6391`) - Bug in version string gen. for dev versions with shallow clones / install from tarball (:issue:`6127`) - Inconsistent tz parsing``Timestamp`/`to\_datetime``for current year (:issue:`5958`) - Indexing bugs with reordered indexes (:issue:`6252`, :issue:`6254`) - Bug in``.xs``with a Series multiindex (:issue:`6258`, :issue:`5684`) - Bug in conversion of a string types to a DatetimeIndex with a specified frequency (:issue:`6273`, :issue:`6274`) - Bug in``eval``where type-promotion failed for large expressions (:issue:`6205`) - Bug in interpolate with``inplace=True``(:issue:`6281`) -``HDFStore.remove``now handles start and stop (:issue:`6177`) -``HDFStore.select\_as\_multiple`handles start and stop the same way as`select``(:issue:`6177`) -``HDFStore.select\_as\_coordinates`and`select\_column`works with a`where``clause that results in filters (:issue:`6177`) - Regression in join of non_unique_indexes (:issue:`6329`) - Issue with groupby``agg``with a single function and a mixed-type frame (:issue:`6337`) - Bug in``DataFrame.replace()`when passing a non-`bool`  `to\_replace``argument (:issue:`6332`) - Raise when trying to align on different levels of a MultiIndex assignment (:issue:`3738`) - Bug in setting complex dtypes via boolean indexing (:issue:`6345`) - Bug in TimeGrouper/resample when presented with a non-monotonic DatetimeIndex that would return invalid results. (:issue:`4161`) - Bug in index name propagation in TimeGrouper/resample (:issue:`4161`) - TimeGrouper has a more compatible API to the rest of the groupers (e.g.``groups``was missing) (:issue:`3881`) - Bug in multiple grouping with a TimeGrouper depending on target column order (:issue:`6764`) - Bug in``pd.eval`when parsing strings with possible tokens like`'&'``(:issue:`6351`) - Bug correctly handle placements of``-inf``in Panels when dividing by integer 0 (:issue:`6178`) -``DataFrame.shift`with`axis=1``was raising (:issue:`6371`) - Disabled clipboard tests until release time (run locally with``nosetests -A disabled``) (:issue:`6048`). - Bug in``DataFrame.replace()`when passing a nested`dict``that contained   keys not in the values to be replaced (:issue:`6342`) -``str.match``ignored the na flag (:issue:`6609`). - Bug in take with duplicate columns that were not consolidated (:issue:`6240`) - Bug in interpolate changing dtypes (:issue:`6290`) - Bug in``Series.get``, was using a buggy access method (:issue:`6383`) - Bug in hdfstore queries of the form``where=\[('date', '\>=', datetime(2013,1,1)), ('date', '\<=', datetime(2014,1,1))\]``(:issue:`6313`) - Bug in``DataFrame.dropna``with duplicate indices (:issue:`6355`) - Regression in chained getitem indexing with embedded list-like from 0.12 (:issue:`6394`) -``Float64Index``with nans not comparing correctly (:issue:`6401`) -``eval`/`query`expressions with strings containing the`@``character   will now work (:issue:`6366`). - Bug in``Series.reindex`when specifying a`method``with some nan values was inconsistent (noted on a resample) (:issue:`6418`) - Bug in `DataFrame.replace` where nested dicts were erroneously   depending on the order of dictionary keys and values (:issue:`5338`). - Performance issue in concatenating with empty objects (:issue:`3259`) - Clarify sorting of``sym\_diff`on`Index`objects with`NaN``values (:issue:`6444`) - Regression in``MultiIndex.from\_product`with a`DatetimeIndex``as input (:issue:`6439`) - Bug in``str.extract``when passed a non-default index (:issue:`6348`) - Bug in``str.split`when passed`pat=None`and`n=1``(:issue:`6466`) - Bug in``io.data.DataReader`when passed`"F-F\_Momentum\_Factor"`and`data\_source="famafrench"``(:issue:`6460`) - Bug in``sum`of a`timedelta64\[ns\]``series (:issue:`6462`) - Bug in``resample``with a timezone and certain offsets (:issue:`6397`) - Bug in``iat/iloc``with duplicate indices on a Series (:issue:`6493`) - Bug in``read\_html``where nan's were incorrectly being used to indicate   missing values in text. Should use the empty string for consistency with the   rest of pandas (:issue:`5129`). - Bug in``read\_html``tests where redirected invalid URLs would make one test   fail (:issue:`6445`). - Bug in multi-axis indexing using``.loc``on non-unique indices (:issue:`6504`) - Bug that caused _ref_locs corruption when slice indexing across columns axis of a DataFrame (:issue:`6525`) - Regression from 0.13 in the treatment of numpy``datetime64``non-ns dtypes in Series creation (:issue:`6529`) -``.names`attribute of MultiIndexes passed to`set\_index``are now preserved (:issue:`6459`). - Bug in setitem with a duplicate index and an alignable rhs (:issue:`6541`) - Bug in setitem with``.loc``on mixed integer Indexes (:issue:`6546`) - Bug in``pd.read\_stata``which would use the wrong data types and missing values (:issue:`6327`) - Bug in``DataFrame.to\_stata``that lead to data loss in certain cases, and could be exported using the   wrong data types and missing values (:issue:`6335`) -``StataWriter``replaces missing values in string columns by empty string (:issue:`6802`) - Inconsistent types in``Timestamp``addition/subtraction (:issue:`6543`) - Bug in preserving frequency across Timestamp addition/subtraction (:issue:`4547`) - Bug in empty list lookup caused``IndexError``exceptions (:issue:`6536`, :issue:`6551`) -``Series.quantile`raising on an`object``dtype (:issue:`6555`) - Bug in``.xs`with a`nan``in level when dropped (:issue:`6574`) - Bug in fillna with``method='bfill/ffill'`and`datetime64\[ns\]``dtype (:issue:`6587`) - Bug in sql writing with mixed dtypes possibly leading to data loss (:issue:`6509`) - Bug in``Series.pop``(:issue:`6600`) - Bug in``iloc`indexing when positional indexer matched`Int64Index``of the corresponding axis and no reordering happened (:issue:`6612`) - Bug in``fillna`with`limit`and`value`specified - Bug in`DataFrame.to\_stata``when columns have non-string names (:issue:`4558`) - Bug in compat with``np.compress``, surfaced in (:issue:`6658`) - Bug in binary operations with a rhs of a Series not aligning (:issue:`6681`) - Bug in``DataFrame.to\_stata`which incorrectly handles nan values and ignores`with\_index``keyword argument (:issue:`6685`) - Bug in resample with extra bins when using an evenly divisible frequency (:issue:`4076`) - Bug in consistency of groupby aggregation when passing a custom function (:issue:`6715`) - Bug in resample when``how=None``resample freq is the same as the axis frequency (:issue:`5955`) - Bug in downcasting inference with empty arrays (:issue:`6733`) - Bug in``obj.blocks``on sparse containers dropping all but the last items of same for dtype (:issue:`6748`) - Bug in unpickling``NaT (NaTType)``(:issue:`4606`) - Bug in``DataFrame.replace()`where regex meta characters were being treated   as regex even when`regex=False``(:issue:`6777`). - Bug in timedelta ops on 32-bit platforms (:issue:`6808`) - Bug in setting a tz-aware index directly via``.index``(:issue:`6785`) - Bug in expressions.py where numexpr would try to evaluate arithmetic ops   (:issue:`6762`). - Bug in Makefile where it didn't remove Cython generated C files with``make clean``(:issue:`6768`) - Bug with numpy < 1.7.2 when reading long strings from``HDFStore``(:issue:`6166`) - Bug in``DataFrame.\_reduce``where non bool-like (0/1) integers were being   converted into bools. (:issue:`6806`) - Regression from 0.13 with``fillna``and a Series on datetime-like (:issue:`6344`) - Bug in adding``np.timedelta64`to`DatetimeIndex``with timezone outputs incorrect results (:issue:`6818`) - Bug in``DataFrame.replace()``where changing a dtype through replacement   would only replace the first occurrence of a value (:issue:`6689`) - Better error message when passing a frequency of 'MS' in``Period`construction (GH5332) - Bug in`Series.\_\_unicode\_\_`when`max\_rows=None``and the Series has more than 1000 rows. (:issue:`6863`) - Bug in``groupby.get\_group``where a datelike wasn't always accepted (:issue:`5267`) - Bug in``groupBy.get\_group`created by`TimeGrouper`raises`AttributeError``(:issue:`6914`) - Bug in``DatetimeIndex.tz\_localize`and`DatetimeIndex.tz\_convert`converting`NaT``incorrectly (:issue:`5546`) - Bug in arithmetic operations affecting``NaT``(:issue:`6873`) - Bug in``Series.str.extract`where the resulting`Series`from a single   group match wasn't renamed to the group name - Bug in`DataFrame.to\_csv`where setting`index=False`ignored the`header``kwarg (:issue:`6186`) - Bug in``DataFrame.plot`and`Series.plot``, where the legend behave inconsistently when plotting to the same axes repeatedly (:issue:`6678`) - Internal tests for patching``\_\_finalize\_\_``/ bug in merge not finalizing (:issue:`6923`, :issue:`6927`) - accept``TextFileReader`in`concat``, which was affecting a common user idiom (:issue:`6583`) - Bug in C parser with leading white space (:issue:`3374`) - Bug in C parser with``delim\_whitespace=True`and`r``-delimited lines - Bug in python parser with explicit MultiIndex in row following column header (:issue:`6893`) - Bug in``Series.rank`and`DataFrame.rank``that caused small floats (<1e-13) to all receive the same rank (:issue:`6886`) - Bug in``DataFrame.apply`with functions that used`*args\`\` or \`\`*\*kwargs``and returned   an empty result (:issue:`6952`) - Bug in sum/mean on 32-bit platforms on overflows (:issue:`6915`) - Moved``Panel.shift`to`NDFrame.slice\_shift``and fixed to respect multiple dtypes. (:issue:`6959`) - Bug in enabling``subplots=True`in`DataFrame.plot`only has single column raises`TypeError`, and`Series.plot`raises`AttributeError``(:issue:`6951`) - Bug in``DataFrame.plot`draws unnecessary axes when enabling`subplots`and`kind=scatter``(:issue:`6951`) - Bug in``read\_csv``from a filesystem with non-utf-8 encoding (:issue:`6807`) - Bug in``iloc``when setting / aligning (:issue:`6766`) - Bug causing UnicodeEncodeError when get_dummies called with unicode values and a prefix (:issue:`6885`) - Bug in timeseries-with-frequency plot cursor display (:issue:`5453`) - Bug surfaced in``groupby.plot`when using a`Float64Index``(:issue:`7025`) - Stopped tests from failing if options data isn't able to be downloaded from Yahoo (:issue:`7034`) - Bug in``parallel\_coordinates`and`radviz``where reordering of class column   caused possible color/class mismatch (:issue:`6956`) - Bug in``radviz`and`andrews\_curves``where multiple values of 'color'   were being passed to plotting method (:issue:`6956`) - Bug in``Float64Index.isin()`where containing`nan``s would make indices   claim that they contained all the things (:issue:`7066`). - Bug in``DataFrame.boxplot`where it failed to use the axis passed as the`ax``argument (:issue:`3578`) - Bug in the``XlsxWriter`and`XlwtWriter``implementations that resulted in datetime columns being formatted without the time (:issue:`7075`)   were being passed to plotting method - `read_fwf` treats``None`in`colspec`like regular python slices. It now reads from the beginning   or until the end of the line when`colspec`contains a`None`(previously raised a`TypeError`) - Bug in cache coherence with chained indexing and slicing; add`\_is\_view`property to`NDFrame`to correctly predict   views; mark`is\_copy`on`xs``only if its an actual copy (and not a view) (:issue:`7084`) - Bug in DatetimeIndex creation from string ndarray with``dayfirst=True``(:issue:`5917`) - Bug in``MultiIndex.from\_arrays`created from`DatetimeIndex`doesn't preserve`freq`and`tz``(:issue:`7090`) - Bug in``unstack`raises`ValueError`when`MultiIndex`contains`PeriodIndex``(:issue:`4342`) - Bug in``boxplot`and`hist``draws unnecessary axes (:issue:`6769`) - Regression in``groupby.nth()``for out-of-bounds indexers (:issue:`6621`) - Bug in``quantile``with datetime values (:issue:`6965`) - Bug in``Dataframe.set\_index`,`reindex`and`pivot`don't preserve`DatetimeIndex`and`PeriodIndex``attributes (:issue:`3950`, :issue:`5878`, :issue:`6631`) - Bug in``MultiIndex.get\_level\_values`doesn't preserve`DatetimeIndex`and`PeriodIndex``attributes (:issue:`7092`) - Bug in``Groupby`doesn't preserve`tz``(:issue:`3950`) - Bug in``PeriodIndex``partial string slicing (:issue:`6716`) - Bug in the HTML repr of a truncated Series or DataFrame not showing the class name with the``large\_repr``set to 'info'   (:issue:`7105`) - Bug in``DatetimeIndex`specifying`freq`raises`ValueError``when passed value is too short (:issue:`7098`) - Fixed a bug with the``info`repr not honoring the`display.max\_info\_columns``setting (:issue:`6939`) - Bug``PeriodIndex``string slicing with out of bounds values (:issue:`5407`) - Fixed a memory error in the hashtable implementation/factorizer on resizing of large tables (:issue:`7157`) - Bug in``isnull``when applied to 0-dimensional object arrays (:issue:`7176`) - Bug in``query`/`eval``where global constants were not looked up correctly   (:issue:`7178`) - Bug in recognizing out-of-bounds positional list indexers with``iloc\`<span class="title-ref"> and a multi-axis tuple indexer (:issue:\`7189</span>) - Bug in setitem with a single value, MultiIndex and integer indices (`7190`, `7218`) - Bug in expressions evaluation with reversed ops, showing in series-dataframe ops (`7198`, `7192`) - Bug in multi-axis indexing with \> 2 ndim and a MultiIndex (`7199`) - Fix a bug where invalid eval/query operations would blow the stack (`5198`)

## Contributors

<div class="contributors">

v0.13.1..v0.14.0

</div>

---

v0.14.1.md

---

# Version 0.14.1 (July 11, 2014)

{{ header }}

This is a minor release from 0.14.0 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

  - Highlights include:
      - New methods <span class="title-ref">\~pandas.DataFrame.select\_dtypes</span> to select columns based on the dtype and <span class="title-ref">\~pandas.Series.sem</span> to calculate the standard error of the mean.
      - Support for dateutil timezones (see \[docs \<timeseries.timezone\>\](\#docs-\<timeseries.timezone\>)).
      - Support for ignoring full line comments in the <span class="title-ref">\~pandas.read\_csv</span> text parser.
      - New documentation section on \[Options and Settings \<options\>\](\#options-and-settings-\<options\>).
      - Lots of bug fixes.
  - \[Enhancements \<whatsnew\_0141.enhancements\>\](\#enhancements-\<whatsnew\_0141.enhancements\>)
  - \[API Changes \<whatsnew\_0141.api\>\](\#api-changes-\<whatsnew\_0141.api\>)
  - \[Performance Improvements \<whatsnew\_0141.performance\>\](\#performance-improvements-\<whatsnew\_0141.performance\>)
  - \[Experimental Changes \<whatsnew\_0141.experimental\>\](\#experimental-changes-\<whatsnew\_0141.experimental\>)
  - \[Bug Fixes \<whatsnew\_0141.bug\_fixes\>\](\#bug-fixes-\<whatsnew\_0141.bug\_fixes\>)

## API changes

  - Openpyxl now raises a ValueError on construction of the openpyxl writer instead of warning on pandas import (`7284`).

  - For `StringMethods.extract`, when no match is found, the result - only containing `NaN` values - now also has `dtype=object` instead of `float` (`7242`)

  - `Period` objects no longer raise a `TypeError` when compared using `==` with another object that *isn't* a `Period`. Instead when comparing a `Period` with another object using `==` if the other object isn't a `Period` `False` is returned. (`7376`)

  - Previously, the behaviour on resetting the time or not in `offsets.apply`, `rollforward` and `rollback` operations differed between offsets. With the support of the `normalize` keyword for all offsets(see below) with a default value of False (preserve time), the behaviour changed for certain offsets (BusinessMonthBegin, MonthEnd, BusinessMonthEnd, CustomBusinessMonthEnd, BusinessYearBegin, LastWeekOfMonth, FY5253Quarter, LastWeekOfMonth, Easter):
    
    `` `ipython   In [6]: from pandas.tseries import offsets    In [7]: d = pd.Timestamp('2014-01-01 09:00')    # old behaviour < 0.14.1   In [8]: d + offsets.MonthEnd()   Out[8]: pd.Timestamp('2014-01-31 00:00:00')  Starting from 0.14.1 all offsets preserve time by default. The old behaviour can be obtained with ``normalize=True\`\`
    
    <div class="ipython" data-suppress="">
    
    python
    
    import pandas.tseries.offsets as offsets
    
    d = pd.Timestamp("2014-01-01 09:00")
    
    </div>
    
    <div class="ipython">
    
    python
    
    \# new behaviour d + offsets.MonthEnd() d + offsets.MonthEnd(normalize=True)
    
    </div>
    
    Note that for the other offsets the default behaviour did not change.

\- Add back `#N/A N/A` as a default NA value in text parsing, (regression from 0.12) (`5521`) `` ` - Raise a ``TypeError`on inplace-setting with a`.where`and a non`np.nan`value as this is inconsistent   with a set-item expression like`df\[mask\] = None``(:issue:`7656`)   .. _whatsnew_0141.enhancements:  Enhancements ~~~~~~~~~~~~  - Add``dropna`argument to`value\_counts`and`nunique``(:issue:`5569`). - Add `~pandas.DataFrame.select_dtypes` method to allow selection of   columns based on dtype (:issue:`7316`). See [the docs <basics.selectdtypes>](#the-docs-<basics.selectdtypes>). - All``offsets`supports the`normalize`keyword to specify whether`offsets.apply`,`rollforward`and`rollback`resets the time (hour,   minute, etc) or not (default`False``, preserves time) (:issue:`7156`):``\`python import pandas.tseries.offsets as offsets

> day = offsets.Day() day.apply(pd.Timestamp("2014-01-01 09:00"))
> 
> day = offsets.Day(normalize=True) day.apply(pd.Timestamp("2014-01-01 09:00"))

\- `PeriodIndex` is represented as the same format as `DatetimeIndex` (`7601`) `` ` - ``StringMethods``now work on empty Series (:issue:`7242`) - The file parsers``read\_csv`and`read\_table`now ignore line comments provided by   the parameter`comment``, which accepts only a single character for the C reader.   In particular, they allow for comments before file data begins (:issue:`2685`) - Add``NotImplementedError`for simultaneous use of`chunksize`and`nrows``for read_csv() (:issue:`6774`). - Tests for basic reading of public S3 buckets now exist (:issue:`7281`). -``read\_html`now sports an`encoding``argument that is passed to the   underlying parser library. You can use this to read non-ascii encoded web   pages (:issue:`7323`). -``read\_excel`now supports reading from URLs in the same way   that`read\_csv``does.  (:issue:`6809`) - Support for dateutil timezones, which can now be used in the same way as   pytz timezones across pandas. (:issue:`4688`)    .. ipython:: python       rng = pd.date_range(          "3/6/2012 00:00", periods=10, freq="D", tz="dateutil/Europe/London"      )      rng.tz    See [the docs <timeseries.timezone>](#the-docs-<timeseries.timezone>).  - Implemented``sem`(standard error of the mean) operation for`Series`,`DataFrame`,`Panel`, and`Groupby``(:issue:`6897`) - Add``nlargest`and`nsmallest`to the`Series`  `groupby`allowlist,   which means you can now use these methods on a`SeriesGroupBy``object   (:issue:`7053`). - All offsets``apply`,`rollforward`and`rollback`can now handle`np.datetime64`, previously results in`ApplyTypeError``(:issue:`7452`) -``Period`and`PeriodIndex`can contain`NaT``in its values (:issue:`7485`) - Support pickling``Series`,`DataFrame`and`Panel`objects with   non-unique labels along *item* axis (`index`,`columns`and`items``respectively) (:issue:`7370`). - Improved inference of datetime/timedelta with mixed null objects. Regression from 0.13.1 in interpretation of an object Index   with all null elements (:issue:`7431`)  .. _whatsnew_0141.performance:  Performance ~~~~~~~~~~~ - Improvements in dtype inference for numeric operations involving yielding performance gains for dtypes:``int64`,`timedelta64`,`datetime64``(:issue:`7223`) - Improvements in Series.transform for significant performance gains (:issue:`6496`) - Improvements in DataFrame.transform with ufuncs and built-in grouper functions for significant performance gains (:issue:`7383`) - Regression in groupby aggregation of datetime64 dtypes (:issue:`7555`) - Improvements in``MultiIndex.from\_product``for large iterables (:issue:`7627`)   .. _whatsnew_0141.experimental:  Experimental ~~~~~~~~~~~~  -``pandas.io.data.Options`has a new method,`get\_all\_data`method, and now consistently returns a   MultiIndexed`DataFrame``(:issue:`5602`) -``io.gbq.read\_gbq`and`io.gbq.to\_gbq`were refactored to remove the   dependency on the Google`bq.py`command line client. This submodule   now uses`httplib2`and the Google`apiclient`and`oauth2client`API client   libraries which should be more stable and, therefore, reliable than`bq.py``. See [the docs <io.bigquery>](#the-docs-<io.bigquery>). (:issue:`6937`).   .. _whatsnew_0141.bug_fixes:  Bug fixes ~~~~~~~~~ - Bug in``DataFrame.where``with a symmetric shaped frame and a passed other of a DataFrame (:issue:`7506`) - Bug in Panel indexing with a MultiIndex axis (:issue:`7516`) - Regression in datetimelike slice indexing with a duplicated index and non-exact end-points (:issue:`7523`) - Bug in setitem with list-of-lists and single vs mixed types (:issue:`7551`:) - Bug in time ops with non-aligned Series (:issue:`7500`) - Bug in timedelta inference when assigning an incomplete Series (:issue:`7592`) - Bug in groupby``.nth``with a Series and integer-like column name (:issue:`7559`) - Bug in``Series.get``with a boolean accessor (:issue:`7407`) - Bug in``value\_counts`where`NaT`did not qualify as missing (`NaN``) (:issue:`7423`) - Bug in``to\_timedelta``that accepted invalid units and misinterpreted 'm/h' (:issue:`7611`, :issue:`6423`) - Bug in line plot doesn't set correct``xlim`if`secondary\_y=True``(:issue:`7459`) - Bug in grouped``hist`and`scatter`plots use old`figsize``default (:issue:`7394`) - Bug in plotting subplots with``DataFrame.plot`,`hist`clears passed`ax``even if the number of subplots is one (:issue:`7391`). - Bug in plotting subplots with``DataFrame.boxplot`with`by`kw raises`ValueError``if the number of subplots exceeds 1 (:issue:`7391`). - Bug in subplots displays``ticklabels`and`labels``in different rule (:issue:`5897`) - Bug in``Panel.apply``with a MultiIndex as an axis (:issue:`7469`) - Bug in``DatetimeIndex.insert`doesn't preserve`name`and`tz``(:issue:`7299`) - Bug in``DatetimeIndex.asobject`doesn't preserve`name``(:issue:`7299`) - Bug in MultiIndex slicing with datetimelike ranges (strings and Timestamps), (:issue:`7429`) - Bug in``Index.min`and`max`doesn't handle`nan`and`NaT``properly (:issue:`7261`) - Bug in``PeriodIndex.min/max`results in`int``(:issue:`7609`) - Bug in``resample`where`fill\_method`was ignored if you passed`how``(:issue:`2073`) - Bug in``TimeGrouper`doesn't exclude column specified by`key``(:issue:`7227`) - Bug in``DataFrame`and`Series`bar and barh plot raises`TypeError`when`bottom`and`left``keyword is specified (:issue:`7226`) - Bug in``DataFrame.hist`raises`TypeError``when it contains non numeric column (:issue:`7277`) - Bug in``Index.delete`does not preserve`name`and`freq``attributes (:issue:`7302`) - Bug in``DataFrame.query()`/`eval``where local string variables with the @   sign were being treated as temporaries attempting to be deleted   (:issue:`7300`). - Bug in``Float64Index``which didn't allow duplicates (:issue:`7149`). - Bug in``DataFrame.replace()``where truthy values were being replaced   (:issue:`7140`). - Bug in``StringMethods.extract()``where a single match group Series   would use the matcher's name instead of the group name (:issue:`7313`). - Bug in``isnull()`when`mode.use\_inf\_as\_null == True`where isnull   wouldn't test`True`when it encountered an`inf`/`-inf``(:issue:`7315`). - Bug in inferred_freq results in None for eastern hemisphere timezones (:issue:`7310`) - Bug in``Easter``returns incorrect date when offset is negative (:issue:`7195`) - Bug in broadcasting with``.div``, integer dtypes and divide-by-zero (:issue:`7325`) - Bug in``CustomBusinessDay.apply`raises`NameError`when`np.datetime64``object is passed (:issue:`7196`) - Bug in``MultiIndex.append`,`concat`and`pivot\_table``don't preserve timezone (:issue:`6606`) - Bug in``.loc``with a list of indexers on a single-multi index level (that is not nested) (:issue:`7349`) - Bug in``Series.map``when mapping a dict with tuple keys of different lengths (:issue:`7333`) - Bug all``StringMethods``now work on empty Series (:issue:`7242`) - Fix delegation of``read\_sql`to`read\_sql\_query``when query does not contain 'select' (:issue:`7324`). - Bug where a string column name assignment to a``DataFrame`with a`Float64Index`raised a`TypeError`during a call to`np.isnan``(:issue:`7366`). - Bug where``NDFrame.replace()`didn't correctly replace objects with`Period``values (:issue:`7379`). - Bug in``.ix``getitem should always return a Series (:issue:`7150`) - Bug in MultiIndex slicing with incomplete indexers (:issue:`7399`) - Bug in MultiIndex slicing with a step in a sliced level (:issue:`7400`) - Bug where negative indexers in``DatetimeIndex``were not correctly sliced   (:issue:`7408`) - Bug where``NaT`wasn't repr'd correctly in a`MultiIndex``(:issue:`7406`,   :issue:`7409`). - Bug where bool objects were converted to``nan`in`convert\_objects``(:issue:`7416`). - Bug in``quantile``ignoring the axis keyword argument (:issue:`7306`) - Bug where``nanops.\_maybe\_null\_out``doesn't work with complex numbers   (:issue:`7353`) - Bug in several``nanops`functions when`axis==0`for   1-dimensional`nan``arrays (:issue:`7354`) - Bug where``nanops.nanmedian`doesn't work when`axis==None``(:issue:`7352`) - Bug where``nanops.\_has\_infs``doesn't work with many dtypes   (:issue:`7357`) - Bug in``StataReader.data``where reading a 0-observation dta failed (:issue:`7369`) - Bug in``StataReader``when reading Stata 13 (117) files containing fixed width strings (:issue:`7360`) - Bug in``StataWriter``where encoding was ignored (:issue:`7286`) - Bug in``DatetimeIndex`comparison doesn't handle`NaT``properly (:issue:`7529`) - Bug in passing input with``tzinfo`to some offsets`apply`,`rollforward`or`rollback`resets`tzinfo`or raises`ValueError``(:issue:`7465`) - Bug in``DatetimeIndex.to\_period`,`PeriodIndex.asobject`,`PeriodIndex.to\_timestamp`doesn't preserve`name``(:issue:`7485`) - Bug in``DatetimeIndex.to\_period`and`PeriodIndex.to\_timestamp`handle`NaT``incorrectly (:issue:`7228`) - Bug in``offsets.apply`,`rollforward`and`rollback`may return normal`datetime``(:issue:`7502`) - Bug in``resample`raises`ValueError`when target contains`NaT``(:issue:`7227`) - Bug in``Timestamp.tz\_localize`resets`nanosecond``info (:issue:`7534`) - Bug in``DatetimeIndex.asobject`raises`ValueError`when it contains`NaT``(:issue:`7539`) - Bug in``Timestamp.\_\_new\_\_``doesn't preserve nanosecond properly (:issue:`7610`) - Bug in``Index.astype(float)`where it would return an`object`dtype`Index``(:issue:`7464`). - Bug in``DataFrame.reset\_index`loses`tz``(:issue:`3950`) - Bug in``DatetimeIndex.freqstr`raises`AttributeError`when`freq`is`None``(:issue:`7606`) - Bug in``GroupBy.size`created by`TimeGrouper`raises`AttributeError``(:issue:`7453`) - Bug in single column bar plot is misaligned (:issue:`7498`). - Bug in area plot with tz-aware time series raises``ValueError``(:issue:`7471`) - Bug in non-monotonic``Index.union`may preserve`name``incorrectly (:issue:`7458`) - Bug in``DatetimeIndex.intersection``doesn't preserve timezone (:issue:`4690`) - Bug in``rolling\_var``where a window larger than the array would raise an error(:issue:`7297`) - Bug with last plotted timeseries dictating``xlim``(:issue:`2960`) - Bug with``secondary\_y`axis not being considered for timeseries`xlim``(:issue:`3490`) - Bug in``Float64Index``assignment with a non scalar indexer (:issue:`7586`) - Bug in``pandas.core.strings.str\_contains`does not properly match in a case insensitive fashion when`regex=False`and`case=False``(:issue:`7505`) - Bug in``expanding\_cov`,`expanding\_corr`,`rolling\_cov`, and`rolling\_corr``for two arguments with mismatched index  (:issue:`7512`) - Bug in``to\_sql``taking the boolean column as text column (:issue:`7678`) - Bug in grouped``hist`doesn't handle`rot`kw and`sharex``kw properly (:issue:`7234`) - Bug in``.loc`performing fallback integer indexing with`object``dtype indices (:issue:`7496`) - Bug (regression) in``PeriodIndex`constructor when passed`Series\`<span class="title-ref"> objects (:issue:\`7701</span>).

## Contributors

<div class="contributors">

v0.14.0..v0.14.1

</div>

---

v0.15.0.md

---

# Version 0.15.0 (October 18, 2014)

{{ header }}

This is a major release from 0.14.1 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

\> **Warning** \> pandas \>= 0.15.0 will no longer support compatibility with NumPy versions \< 1.7.0. If you want to use the latest versions of pandas, please upgrade to NumPy \>= 1.7.0 (`7711`)

  - Highlights include:
      - The `Categorical` type was integrated as a first-class pandas type, see \[here \<whatsnew\_0150.cat\>\](\#here-\<whatsnew\_0150.cat\>)
      - New scalar type `Timedelta`, and a new index type `TimedeltaIndex`, see \[here \<whatsnew\_0150.timedeltaindex\>\](\#here-\<whatsnew\_0150.timedeltaindex\>)
      - New datetimelike properties accessor `.dt` for Series, see \[Datetimelike Properties \<whatsnew\_0150.dt\>\](\#datetimelike-properties-\<whatsnew\_0150.dt\>)
      - New DataFrame default display for `df.info()` to include memory usage, see \[Memory Usage \<whatsnew\_0150.memory\>\](\#memory-usage-\<whatsnew\_0150.memory\>)
      - `read_csv` will now by default ignore blank lines when parsing, see \[here \<whatsnew\_0150.blanklines\>\](\#here-\<whatsnew\_0150.blanklines\>)
      - API change in using Indexes in set operations, see \[here \<whatsnew\_0150.index\_set\_ops\>\](\#here-\<whatsnew\_0150.index\_set\_ops\>)
      - Enhancements in the handling of timezones, see \[here \<whatsnew\_0150.tz\>\](\#here-\<whatsnew\_0150.tz\>)
      - A lot of improvements to the rolling and expanding moment functions, see \[here \<whatsnew\_0150.roll\>\](\#here-\<whatsnew\_0150.roll\>)
      - Internal refactoring of the `Index` class to no longer sub-class `ndarray`, see \[Internal Refactoring \<whatsnew\_0150.refactoring\>\](\#internal-refactoring-\<whatsnew\_0150.refactoring\>)
      - dropping support for `PyTables` less than version 3.0.0, and `numexpr` less than version 2.1 (`7990`)
      - Split indexing documentation into \[Indexing and Selecting Data \<indexing\>\](\#indexing-and-selecting-data-\<indexing\>) and \[MultiIndex / Advanced Indexing \<advanced\>\](\#multiindex-/-advanced-indexing-\<advanced\>)
      - Split out string methods documentation into \[Working with Text Data \<text\>\](\#working-with-text-data-\<text\>)
  - Check the \[API Changes \<whatsnew\_0150.api\>\](\#api-changes-\<whatsnew\_0150.api\>) and \[deprecations \<whatsnew\_0150.deprecations\>\](\#deprecations-\<whatsnew\_0150.deprecations\>) before updating
  - \[Other Enhancements \<whatsnew\_0150.enhancements\>\](\#other-enhancements-\<whatsnew\_0150.enhancements\>)
  - \[Performance Improvements \<whatsnew\_0150.performance\>\](\#performance-improvements-\<whatsnew\_0150.performance\>)
  - \[Bug Fixes \<whatsnew\_0150.bug\_fixes\>\](\#bug-fixes-\<whatsnew\_0150.bug\_fixes\>)

<div class="warning">

<div class="title">

Warning

</div>

In 0.15.0 `Index` has internally been refactored to no longer sub-class `ndarray` but instead subclass `PandasObject`, similarly to the rest of the pandas objects. This change allows very easy sub-classing and creation of new index types. This should be a transparent change with only very limited API implications (See the \[Internal Refactoring \<whatsnew\_0150.refactoring\>\](\#internal-refactoring-\<whatsnew\_0150.refactoring\>))

</div>

<div class="warning">

<div class="title">

Warning

</div>

The refactoring in <span class="title-ref">\~pandas.Categorical</span> changed the two argument constructor from "codes/labels and levels" to "values and levels (now called 'categories')". This can lead to subtle bugs. If you use <span class="title-ref">\~pandas.Categorical</span> directly, please audit your code before updating to this pandas version and change it to use the <span class="title-ref">\~pandas.Categorical.from\_codes</span> constructor. See more on `Categorical` \[here \<whatsnew\_0150.cat\>\](\#here-\<whatsnew\_0150.cat\>)

</div>

## New features

### Categoricals in Series/DataFrame

<span class="title-ref">\~pandas.Categorical</span> can now be included in `Series` and `DataFrames` and gained new methods to manipulate. Thanks to Jan Schulz for much of this API/implementation. (`3943`, `5313`, `5314`, `7444`, `7839`, `7848`, `7864`, `7914`, `7768`, `8006`, `3678`, `8075`, `8076`, `8143`, `8453`, `8518`).

For full docs, see the \[categorical introduction \<categorical\>\](\#categorical-introduction-\<categorical\>) and the \[API documentation \<api.arrays.categorical\>\](\#api-documentation-\<api.arrays.categorical\>).

<div class="ipython">

python

  - df = pd.DataFrame({"id": \[1, 2, 3, 4, 5, 6\],  
    "raw\_grade": \['a', 'b', 'b', 'a', 'a', 'e'\]})

df\["grade"\] = df\["raw\_grade"\].astype("category") df\["grade"\]

\# Rename the categories df\["grade"\] = df\["grade"\].cat.rename\_categories(\["very good", "good", "very bad"\])

\# Reorder the categories and simultaneously add the missing categories df\["grade"\] = df\["grade"\].cat.set\_categories(\["very bad", "bad", "medium", "good", "very good"\]) df\["grade"\] df.sort\_values("grade") df.groupby("grade", observed=False).size()

</div>

  - `pandas.core.group_agg` and `pandas.core.factor_agg` were removed. As an alternative, construct a dataframe and use `df.groupby(<group>).agg(<func>)`.
  - Supplying "codes/labels and levels" to the <span class="title-ref">\~pandas.Categorical</span> constructor is not supported anymore. Supplying two arguments to the constructor is now interpreted as "values and levels (now called 'categories')". Please change your code to use the <span class="title-ref">\~pandas.Categorical.from\_codes</span> constructor.
  - The `Categorical.labels` attribute was renamed to `Categorical.codes` and is read only. If you want to manipulate codes, please use one of the \[API methods on Categoricals \<api.arrays.categorical\>\](\#api-methods-on-categoricals-\<api.arrays.categorical\>).
  - The `Categorical.levels` attribute is renamed to `Categorical.categories`.

### TimedeltaIndex/scalar

We introduce a new scalar type `Timedelta`, which is a subclass of `datetime.timedelta`, and behaves in a similar manner, but allows compatibility with `np.timedelta64` types as well as a host of custom representation, parsing, and attributes. This type is very similar to how `Timestamp` works for `datetimes`. It is a nice-API box for the type. See the \[docs \<timedeltas.timedeltas\>\](\#docs-\<timedeltas.timedeltas\>). (`3009`, `4533`, `8209`, `8187`, `8190`, `7869`, `7661`, `8345`, `8471`)

\> **Warning** \> `Timedelta` scalars (and `TimedeltaIndex`) component fields are *not the same* as the component fields on a `datetime.timedelta` object. For example, `.seconds` on a `datetime.timedelta` object returns the total number of seconds combined between `hours`, `minutes` and `seconds`. In contrast, the pandas `Timedelta` breaks out hours, minutes, microseconds and nanoseconds separately.

> `` `ipython    # Timedelta accessor    In [9]: tds = pd.Timedelta('31 days 5 min 3 sec')     In [10]: tds.minutes    Out[10]: 5L     In [11]: tds.seconds    Out[11]: 3L     # datetime.timedelta accessor    # this is 5 minutes * 60 + 3 seconds    In [12]: tds.to_pytimedelta().seconds    Out[12]: 303  **Note**: this is no longer true starting from v0.16.0, where full compatibility with ``datetime.timedelta\`\` is introduced. See the \[0.16.0 whatsnew entry \<whatsnew\_0160.api\_breaking.timedelta\>\](\#0.16.0-whatsnew-entry-\<whatsnew\_0160.api\_breaking.timedelta\>)

<div class="warning">

<div class="title">

Warning

</div>

Prior to 0.15.0 `pd.to_timedelta` would return a `Series` for list-like/Series input, and a `np.timedelta64` for scalar input. It will now return a `TimedeltaIndex` for list-like input, `Series` for Series input, and `Timedelta` for scalar input.

The arguments to `pd.to_timedelta` are now `(arg,unit='ns',box=True,coerce=False)`, previously were `(arg,box=True,unit='ns')` as these are more logical.

</div>

Construct a scalar

<div class="ipython">

python

pd.Timedelta('1 days 06:05:01.00003') pd.Timedelta('15.5us') pd.Timedelta('1 hour 15.5us')

\# negative Timedeltas have this string repr \# to be more consistent with datetime.timedelta conventions pd.Timedelta('-1us')

\# a NaT pd.Timedelta('nan')

</div>

Access fields for a `Timedelta`

<div class="ipython">

python

td = pd.Timedelta('1 hour 3m 15.5us') td.seconds td.microseconds td.nanoseconds

</div>

Construct a `TimedeltaIndex`

<div class="ipython" data-suppress="">

python

import datetime

</div>

<div class="ipython">

python

  - pd.TimedeltaIndex(\['1 days', '1 days, 00:00:05',  
    np.timedelta64(2, 'D'), datetime.timedelta(days=2, seconds=2)\])

</div>

Constructing a `TimedeltaIndex` with a regular range

<div class="ipython">

python

pd.timedelta\_range('1 days', periods=5, freq='D')

</div>

``` python
In [20]: pd.timedelta_range(start='1 days', end='2 days', freq='30T')
Out[20]:
TimedeltaIndex(['1 days 00:00:00', '1 days 00:30:00', '1 days 01:00:00',
                '1 days 01:30:00', '1 days 02:00:00', '1 days 02:30:00',
                '1 days 03:00:00', '1 days 03:30:00', '1 days 04:00:00',
                '1 days 04:30:00', '1 days 05:00:00', '1 days 05:30:00',
                '1 days 06:00:00', '1 days 06:30:00', '1 days 07:00:00',
                '1 days 07:30:00', '1 days 08:00:00', '1 days 08:30:00',
                '1 days 09:00:00', '1 days 09:30:00', '1 days 10:00:00',
                '1 days 10:30:00', '1 days 11:00:00', '1 days 11:30:00',
                '1 days 12:00:00', '1 days 12:30:00', '1 days 13:00:00',
                '1 days 13:30:00', '1 days 14:00:00', '1 days 14:30:00',
                '1 days 15:00:00', '1 days 15:30:00', '1 days 16:00:00',
                '1 days 16:30:00', '1 days 17:00:00', '1 days 17:30:00',
                '1 days 18:00:00', '1 days 18:30:00', '1 days 19:00:00',
                '1 days 19:30:00', '1 days 20:00:00', '1 days 20:30:00',
                '1 days 21:00:00', '1 days 21:30:00', '1 days 22:00:00',
                '1 days 22:30:00', '1 days 23:00:00', '1 days 23:30:00',
                '2 days 00:00:00'],
               dtype='timedelta64[ns]', freq='30T')
```

You can now use a `TimedeltaIndex` as the index of a pandas object

<div class="ipython">

python

  - s = pd.Series(np.arange(5),  
    index=pd.timedelta\_range('1 days', periods=5, freq='s'))

s

</div>

You can select with partial string selections

<div class="ipython">

python

s\['1 day 00:00:02'\] s\['1 day':'1 day 00:00:02'\]

</div>

Finally, the combination of `TimedeltaIndex` with `DatetimeIndex` allow certain combination operations that are `NaT` preserving:

<div class="ipython">

python

tdi = pd.TimedeltaIndex(\['1 days', pd.NaT, '2 days'\]) tdi.tolist() dti = pd.date\_range('20130101', periods=3) dti.tolist()

(dti + tdi).tolist() (dti - tdi).tolist()

</div>

  - iteration of a `Series` e.g. `list(Series(...))` of `timedelta64[ns]` would prior to v0.15.0 return `np.timedelta64` for each element. These will now be wrapped in `Timedelta`.

<div id="whatsnew_0150.memory">

Memory usage `` ` ^^^^^^^^^^^^  Implemented methods to find memory usage of a DataFrame. See the [FAQ <df-memory-usage>](#faq-<df-memory-usage>) for more. (:issue:`6852`).  A new display option ``display.memory\_usage`(see [options](#options)) sets the default behavior of the`memory\_usage`argument in the`df.info()`method. By default`display.memory\_usage`is`True``.  .. ipython:: python      dtypes = ['int64', 'float64', 'datetime64[ns]', 'timedelta64[ns]',               'complex128', 'object', 'bool']     n = 5000     data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes}     df = pd.DataFrame(data)     df['categorical'] = df['object'].astype('category')      df.info()  Additionally `~pandas.DataFrame.memory_usage` is an available method for a dataframe object which returns the memory usage of each column.  .. ipython:: python      df.memory_usage(index=True)   .. _whatsnew_0150.dt:  Series.dt accessor ^^^^^^^^^^^^^^^^^^``Series``has gained an accessor to succinctly return datetime like properties for the *values* of the Series, if its a datetime/period like Series. (:issue:`7207`) This will return a Series, indexed like the existing Series. See the [docs <basics.dt_accessors>](#docs-<basics.dt_accessors>)  .. ipython:: python     # datetime    s = pd.Series(pd.date_range('20130101 09:10:12', periods=4))    s    s.dt.hour    s.dt.second    s.dt.day    s.dt.freq  This enables nice expressions like this:  .. ipython:: python     s[s.dt.day == 2]  You can easily produce tz aware transformations:  .. ipython:: python     stz = s.dt.tz_localize('US/Eastern')    stz    stz.dt.tz  You can also chain these types of operations:  .. ipython:: python     s.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')  The``.dt`accessor works for period and timedelta dtypes.  .. ipython:: python     # period    s = pd.Series(pd.period_range('20130101', periods=4, freq='D'))    s    s.dt.year    s.dt.day  .. ipython:: python     # timedelta    s = pd.Series(pd.timedelta_range('1 day 00:00:05', periods=4, freq='s'))    s    s.dt.days    s.dt.seconds    s.dt.components   .. _whatsnew_0150.tz:  Timezone handling improvements ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  -`tz\_localize(None)`for tz-aware`Timestamp`and`DatetimeIndex`now removes timezone holding local time,   previously this resulted in`Exception`or`TypeError``(:issue:`7812`)``\`ipython In \[58\]: ts = pd.Timestamp('2014-08-01 09:00', tz='US/Eastern')

</div>

> In\[59\]: ts Out\[59\]: Timestamp('2014-08-01 09:00:00-0400', tz='US/Eastern')
> 
> In \[60\]: ts.tz\_localize(None) Out\[60\]: Timestamp('2014-08-01 09:00:00')
> 
>   - In \[61\]: didx = pd.date\_range(start='2014-08-01 09:00', freq='H',  
>     ....: periods=10, tz='US/Eastern') ....:
> 
> In \[62\]: didx Out\[62\]: DatetimeIndex(\['2014-08-01 09:00:00-04:00', '2014-08-01 10:00:00-04:00', '2014-08-01 11:00:00-04:00', '2014-08-01 12:00:00-04:00', '2014-08-01 13:00:00-04:00', '2014-08-01 14:00:00-04:00', '2014-08-01 15:00:00-04:00', '2014-08-01 16:00:00-04:00', '2014-08-01 17:00:00-04:00', '2014-08-01 18:00:00-04:00'\], dtype='datetime64\[ns, US/Eastern\]', freq='H')
> 
> In \[63\]: didx.tz\_localize(None) Out\[63\]: DatetimeIndex(\['2014-08-01 09:00:00', '2014-08-01 10:00:00', '2014-08-01 11:00:00', '2014-08-01 12:00:00', '2014-08-01 13:00:00', '2014-08-01 14:00:00', '2014-08-01 15:00:00', '2014-08-01 16:00:00', '2014-08-01 17:00:00', '2014-08-01 18:00:00'\], dtype='datetime64\[ns\]', freq=None)

  - `tz_localize` now accepts the `ambiguous` keyword which allows for passing an array of bools indicating whether the date belongs in DST or not, 'NaT' for setting transition times to NaT, 'infer' for inferring DST/non-DST, and 'raise' (default) for an `AmbiguousTimeError` to be raised. See \[the docs\<timeseries.timezone\_ambiguous\>\](\#the-docs\<timeseries.timezone\_ambiguous\>) for more details (`7943`)
  - `DataFrame.tz_localize` and `DataFrame.tz_convert` now accepts an optional `level` argument for localizing a specific level of a MultiIndex (`7846`)
  - `Timestamp.tz_localize` and `Timestamp.tz_convert` now raise `TypeError` in error cases, rather than `Exception` (`8025`)
  - a timeseries/index localized to UTC when inserted into a Series/DataFrame will preserve the UTC timezone (rather than being a naive `datetime64[ns]`) as `object` dtype (`8411`)
  - `Timestamp.__repr__` displays `dateutil.tz.tzoffset` info (`7907`)

<div id="whatsnew_0150.roll">

Rolling/expanding moments improvements `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  - `rolling_min`, `rolling_max`, `rolling_cov`, and `rolling_corr`   now return objects with all ``NaN`when`len(arg) \< min\_periods \<= window``rather   than raising. (This makes all rolling functions consistent in this behavior). (:issue:`7766`)    Prior to 0.15.0    .. ipython:: python       s = pd.Series([10, 11, 12, 13])``\`ipython In \[15\]: pd.rolling\_min(s, window=10, min\_periods=5) ValueError: min\_periods (5) must be \<= window (4)

</div>

> New behavior
> 
> ``` ipython
> In [4]: pd.rolling_min(s, window=10, min_periods=5)
> Out[4]:
> 0   NaN
> 1   NaN
> 2   NaN
> 3   NaN
> dtype: float64
> ```

  - <span class="title-ref">rolling\_max</span>, <span class="title-ref">rolling\_min</span>, <span class="title-ref">rolling\_sum</span>, <span class="title-ref">rolling\_mean</span>, <span class="title-ref">rolling\_median</span>, <span class="title-ref">rolling\_std</span>, <span class="title-ref">rolling\_var</span>, <span class="title-ref">rolling\_skew</span>, <span class="title-ref">rolling\_kurt</span>, <span class="title-ref">rolling\_quantile</span>, <span class="title-ref">rolling\_cov</span>, <span class="title-ref">rolling\_corr</span>, <span class="title-ref">rolling\_corr\_pairwise</span>, <span class="title-ref">rolling\_window</span>, and <span class="title-ref">rolling\_apply</span> with `center=True` previously would return a result of the same structure as the input `arg` with `NaN` in the final `(window-1)/2` entries.
    
    Now the final `(window-1)/2` entries of the result are calculated as if the input `arg` were followed by `(window-1)/2` `NaN` values (or with shrinking windows, in the case of <span class="title-ref">rolling\_apply</span>). (`7925`, `8269`)
    
    Prior behavior (note final value is `NaN`):
    
    ``` ipython
    In [7]: pd.rolling_sum(Series(range(4)), window=3, min_periods=0, center=True)
    Out[7]:
    0     1
    1     3
    2     6
    3   NaN
    dtype: float64
    ```
    
    New behavior (note final value is `5 = sum([2, 3, NaN])`):
    
    ``` ipython
    In [7]: pd.rolling_sum(pd.Series(range(4)), window=3,
      ....:                min_periods=0, center=True)
    Out[7]:
    0    1
    1    3
    2    6
    3    5
    dtype: float64
    ```

  - <span class="title-ref">rolling\_window</span> now normalizes the weights properly in rolling mean mode (`mean=True`) so that the calculated weighted means (e.g. 'triang', 'gaussian') are distributed about the same means as those calculated without weighting (i.e. 'boxcar'). See \[the note on normalization \<window.weighted\>\](\#the-note-on-normalization-\<window.weighted\>) for further details. (`7618`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\[10.5, 8.8, 11.4, 9.7, 9.3\])
    
    </div>
    
    Behavior prior to 0.15.0:
    
    ``` ipython
    In [39]: pd.rolling_window(s, window=3, win_type='triang', center=True)
    Out[39]:
    0         NaN
    1    6.583333
    2    6.883333
    3    6.683333
    4         NaN
    dtype: float64
    ```
    
    New behavior
    
    ``` ipython
    In [10]: pd.rolling_window(s, window=3, win_type='triang', center=True)
    Out[10]:
    0       NaN
    1     9.875
    2    10.325
    3    10.025
    4       NaN
    dtype: float64
    ```

  - Removed `center` argument from all <span class="title-ref">expanding\_ \<expanding\_apply\></span> functions (see \[list \<api.functions\_expanding\>\](\#list-\<api.functions\_expanding\>)), as the results produced when `center=True` did not make much sense. (`7925`)

  - Added optional `ddof` argument to <span class="title-ref">expanding\_cov</span> and <span class="title-ref">rolling\_cov</span>. The default value of `1` is backwards-compatible. (`8279`)

  - Documented the `ddof` argument to <span class="title-ref">expanding\_var</span>, <span class="title-ref">expanding\_std</span>, <span class="title-ref">rolling\_var</span>, and <span class="title-ref">rolling\_std</span>. These functions' support of a `ddof` argument (with a default value of `1`) was previously undocumented. (`8064`)

  - <span class="title-ref">ewma</span>, <span class="title-ref">ewmstd</span>, <span class="title-ref">ewmvol</span>, <span class="title-ref">ewmvar</span>, <span class="title-ref">ewmcov</span>, and <span class="title-ref">ewmcorr</span> now interpret `min_periods` in the same manner that the <span class="title-ref">rolling\_\*</span> and <span class="title-ref">expanding\_\*</span> functions do: a given result entry will be `NaN` if the (expanding, in this case) window does not contain at least `min_periods` values. The previous behavior was to set to `NaN` the `min_periods` entries starting with the first non- `NaN` value. (`7977`)
    
    Prior behavior (note values start at index `2`, which is `min_periods` after index `0` (the index of the first non-empty value)):
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\[1, None, None, None, 2, 3\])
    
    </div>
    
    ``` ipython
    In [51]: pd.ewma(s, com=3., min_periods=2)
    Out[51]:
    0         NaN
    1         NaN
    2    1.000000
    3    1.000000
    4    1.571429
    5    2.189189
    dtype: float64
    ```
    
    New behavior (note values start at index `4`, the location of the 2nd (since `min_periods=2`) non-empty value):
    
    ``` ipython
    In [2]: pd.ewma(s, com=3., min_periods=2)
    Out[2]:
    0         NaN
    1         NaN
    2         NaN
    3         NaN
    4    1.759644
    5    2.383784
    dtype: float64
    ```

  - <span class="title-ref">ewmstd</span>, <span class="title-ref">ewmvol</span>, <span class="title-ref">ewmvar</span>, <span class="title-ref">ewmcov</span>, and <span class="title-ref">ewmcorr</span> now have an optional `adjust` argument, just like <span class="title-ref">ewma</span> does, affecting how the weights are calculated. The default value of `adjust` is `True`, which is backwards-compatible. See \[Exponentially weighted moment functions \<window.exponentially\_weighted\>\](\#exponentially-weighted-moment-functions-\<window.exponentially\_weighted\>) for details. (`7911`)

<!-- end list -->

  - \- <span class="title-ref">ewma</span>, <span class="title-ref">ewmstd</span>, <span class="title-ref">ewmvol</span>, <span class="title-ref">ewmvar</span>, <span class="title-ref">ewmcov</span>, and <span class="title-ref">ewmcorr</span>  
    now have an optional `ignore_na` argument. When `ignore_na=False` (the default), missing values are taken into account in the weights calculation. When `ignore_na=True` (which reproduces the pre-0.15.0 behavior), missing values are ignored in the weights calculation. (`7543`)
    
    ``` ipython
    In [7]: pd.ewma(pd.Series([None, 1., 8.]), com=2.)
    Out[7]:
    0    NaN
    1    1.0
    2    5.2
    dtype: float64
    
    In [8]: pd.ewma(pd.Series([1., None, 8.]), com=2.,
      ....:         ignore_na=True)  # pre-0.15.0 behavior
    Out[8]:
    0    1.0
    1    1.0
    2    5.2
    dtype: float64
    
    In [9]: pd.ewma(pd.Series([1., None, 8.]), com=2.,
      ....:         ignore_na=False)  # new default
    Out[9]:
    0    1.000000
    1    1.000000
    2    5.846154
    dtype: float64
    ```
    
    \> **Warning**

  - \>  
    By default (`ignore_na=False`) the <span class="title-ref">ewm\*</span> functions' weights calculation in the presence of missing values is different than in pre-0.15.0 versions. To reproduce the pre-0.15.0 calculation of weights in the presence of missing values one must specify explicitly `ignore_na=True`.

  - \- Bug in <span class="title-ref">expanding\_cov</span>, <span class="title-ref">expanding\_corr</span>, <span class="title-ref">rolling\_cov</span>, <span class="title-ref">rolling\_cor</span>, <span class="title-ref">ewmcov</span>, and <span class="title-ref">ewmcorr</span>  
    returning results with columns sorted by name and producing an error for non-unique columns; now handles non-unique columns and returns columns in original order (except for the case of two DataFrames with `pairwise=False`, where behavior is unchanged) (`7542`)

  - `` ` - Bug in `rolling_count` and `expanding_*` functions unnecessarily producing error message for zero-length data (:issue:`8056`) - Bug in `rolling_apply` and `expanding_apply` interpreting ``min\_periods=0`as`min\_periods=1``(:issue:`8080`) - Bug in `expanding_std` and `expanding_var` for a single value producing a confusing error message (:issue:`7900`) - Bug in `rolling_std` and `rolling_var` for a single value producing``0`rather than`NaN``(:issue:`7900`)  - Bug in `ewmstd`, `ewmvol`, `ewmvar`, and `ewmcov`   calculation of de-biasing factors when``bias=False`(the default).   Previously an incorrect constant factor was used, based on`adjust=True`,`ignore\_na=True`,   and an infinite number of observations.   Now a different factor is used for each entry, based on the actual weights   (analogous to the usual`N/(N-1)`factor).   In particular, for a single point a value of`NaN`is returned when`bias=False`,   whereas previously a value of (approximately)`0`was returned.    For example, consider the following pre-0.15.0 results for`ewmvar(..., bias=False)`,   and the corresponding debiasing factors:    .. ipython:: python       s = pd.Series([1., 2., 0., 4.])`\`ipython  
    In \[89\]: pd.ewmvar(s, com=2., bias=False) Out\[89\]: 0 -2.775558e-16 1 3.000000e-01 2 9.556787e-01 3 3.585799e+00 dtype: float64
    
    In \[90\]: pd.ewmvar(s, com=2., bias=False) / pd.ewmvar(s, com=2., bias=True) Out\[90\]: 0 1.25 1 1.25 2 1.25 3 1.25 dtype: float64
    
    Note that entry `0` is approximately 0, and the debiasing factors are a constant 1.25. By comparison, the following 0.15.0 results have a `NaN` for entry `0`, and the debiasing factors are decreasing (towards 1.25):
    
    ``` ipython
    ```
    
    In \[14\]: pd.ewmvar(s, com=2., bias=False) Out\[14\]: 0 NaN 1 0.500000 2 1.210526 3 4.089069 dtype: float64
    
    In \[15\]: pd.ewmvar(s, com=2., bias=False) / pd.ewmvar(s, com=2., bias=True) Out\[15\]: 0 NaN 1 2.083333 2 1.583333 3 1.425439 dtype: float64
    
    See \[Exponentially weighted moment functions \<window.exponentially\_weighted\>\](\#exponentially-weighted-moment-functions-\<window.exponentially\_weighted\>) for details. (`7912`)

<div id="whatsnew_0150.sql">

Improvements in the SQL IO module `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  - Added support for a ``chunksize`parameter to`to\_sql``function. This allows DataFrame to be written in chunks and avoid packet-size overflow errors (:issue:`8062`). - Added support for a``chunksize`parameter to`read\_sql``function. Specifying this argument will return an iterator through chunks of the query result (:issue:`2908`). - Added support for writing``datetime.date`and`datetime.time`object columns with`to\_sql``(:issue:`6932`). - Added support for specifying a``schema`to read from/write to with`read\_sql\_table`and`to\_sql``(:issue:`7441`, :issue:`7952`).   For example:``\`python df.to\_sql('table', engine, schema='other\_schema') \# noqa F821 pd.read\_sql\_table('table', engine, schema='other\_schema') \# noqa F821

</div>

\- Added support for writing `NaN` values with `to_sql` (`2754`). `` ` - Added support for writing datetime64 columns with ``to\_sql``for all database flavors (:issue:`7103`).   .. _whatsnew_0150.api:  Backwards incompatible API changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. _whatsnew_0150.api_breaking:  Breaking changes ^^^^^^^^^^^^^^^^  API changes related to``Categorical`(see [here <whatsnew_0150.cat>](#here-<whatsnew_0150.cat>) for more details):  - The`Categorical``constructor with two arguments changed from   "codes/labels and levels" to "values and levels (now called 'categories')".   This can lead to subtle bugs. If you use `~pandas.Categorical` directly,   please audit your code by changing it to use the `~pandas.Categorical.from_codes`   constructor.    An old function call like (prior to 0.15.0):``\`python pd.Categorical(\[0,1,0,2,1\], levels=\['a', 'b', 'c'\])

> will have to adapted to the following to keep the same behaviour:
> 
> ``` ipython
> In [2]: pd.Categorical.from_codes([0,1,0,2,1], categories=['a', 'b', 'c'])
> Out[2]:
> [a, b, a, c, b]
> Categories (3, object): [a, b, c]
> ```

API changes related to the introduction of the `Timedelta` scalar (see `` ` [above <whatsnew_0150.timedeltaindex>](#above-<whatsnew_0150.timedeltaindex>) for more details):  - Prior to 0.15.0 `to_timedelta` would return a ``Series`for list-like/Series input,   and a`np.timedelta64`for scalar input. It will now return a`TimedeltaIndex`for   list-like input,`Series`for Series input, and`Timedelta`for scalar input.  For API changes related to the rolling and expanding functions, see detailed overview [above <whatsnew_0150.roll>](#above-<whatsnew_0150.roll>).  Other notable API changes:  - Consistency when indexing with`.loc`and a list-like indexer when no values are found.    .. ipython:: python       df = pd.DataFrame([['a'], ['b']], index=[1, 2])      df    In prior versions there was a difference in these two constructs:    -`df.loc\[\[3\]\]`would return a frame reindexed by 3 (with all`np.nan`values)   -`df.loc\[\[3\],:\]`would raise`KeyError`.    Both will now raise a`KeyError`. The rule is that *at least 1* indexer must be found when using a list-like and`.loc``(:issue:`7999`)    Furthermore in prior versions these were also different:    -``df.loc\[\[1,3\]\]`would return a frame reindexed by [1,3]   -`df.loc\[\[1,3\],:\]`would raise`KeyError`.    Both will now return a frame reindex by [1,3]. E.g.`\`ipython In \[3\]: df.loc\[\[1, 3\]\] Out\[3\]: 0 1 a 3 NaN

> In \[4\]: df.loc\[\[1, 3\], :\] Out\[4\]: 0 1 a 3 NaN
> 
> This can also be seen in multi-axis indexing with a `Panel`.
> 
> ``` python
> ```
> 
> \>\>\> p = pd.Panel(np.arange(2 \* 3 \* 4).reshape(2, 3, 4), ... items=\['ItemA', 'ItemB'\], ... major\_axis=\[1, 2, 3\], ... minor\_axis=\['A', 'B', 'C', 'D'\]) \>\>\> p \<class 'pandas.core.panel.Panel'\> Dimensions: 2 (items) x 3 (major\_axis) x 4 (minor\_axis) Items axis: ItemA to ItemB Major\_axis axis: 1 to 3 Minor\_axis axis: A to D
> 
> The following would raise `KeyError` prior to 0.15.0:
> 
> ``` ipython
> ```
> 
> In \[5\]: Out\[5\]: ItemA ItemD 1 3 NaN 2 7 NaN 3 11 NaN
> 
> Furthermore, `.loc` will raise If no values are found in a MultiIndex with a list-like indexer:
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
>   - okexcept
> 
> <!-- end list -->
> 
>   - s = pd.Series(np.arange(3, dtype='int64'),
>     
>       - index=pd.MultiIndex.from\_product(\[\['A'\],  
>         \['foo', 'bar', 'baz'\]\], names=\['one', 'two'\])
>     
>     ).sort\_index()
> 
> s try: s.loc\[\['D'\]\] except KeyError as e: print("KeyError: " + str(e))

  - Assigning values to `None` now considers the dtype when choosing an 'empty' value (`7941`).
    
    Previously, assigning to `None` in numeric containers changed the dtype to object (or errored, depending on the call). It now uses `NaN`:
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\[1., 2., 3.\]) s.loc\[0\] = None s
    
    </div>
    
    `NaT` is now used similarly for datetime containers.
    
    For object containers, we now preserve `None` values (previously these were converted to `NaN` values).
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\["a", "b", "c"\]) s.loc\[0\] = None s
    
    </div>
    
    To insert a `NaN`, you must explicitly use `np.nan`. See the \[docs \<missing.inserting\>\](\#docs-\<missing.inserting\>).

  - In prior versions, updating a pandas object inplace would not reflect in other python references to this object. (`8511`, `5104`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\[1, 2, 3\]) s2 = s s += 1.5
    
    </div>
    
    Behavior prior to v0.15.0
    
    ``` ipython
    # the original object
    In [5]: s
    Out[5]:
    0    2.5
    1    3.5
    2    4.5
    dtype: float64

    # a reference to the original object
    In [7]: s2
    Out[7]:
    0    1
    1    2
    2    3
    dtype: int64
    ```
    
    This is now the correct behavior
    
    <div class="ipython">
    
    python
    
    \# the original object s
    
    \# a reference to the original object s2
    
    </div>

<div id="whatsnew_0150.blanklines">

  - Made both the C-based and Python engines for `read_csv` and `read_table` ignore empty lines in input as well as white space-filled lines, as long as `sep` is not white space. This is an API change that can be controlled by the keyword parameter `skip_blank_lines`. See \[the docs \<io.skiplines\>\](\#the-docs-\<io.skiplines\>) (`4466`)

  - A timeseries/index localized to UTC when inserted into a Series/DataFrame will preserve the UTC timezone and inserted as `object` dtype rather than being converted to a naive `datetime64[ns]` (`8411`).

  - Bug in passing a `DatetimeIndex` with a timezone that was not being retained in DataFrame construction from a dict (`7822`)
    
    In prior versions this would drop the timezone, now it retains the timezone, but gives a column of `object` dtype:
    
    <div class="ipython">
    
    python
    
    i = pd.date\_range('1/1/2011', periods=3, freq='10s', tz='US/Eastern') i df = pd.DataFrame({'a': i}) df df.dtypes
    
    </div>
    
    Previously this would have yielded a column of `datetime64` dtype, but without timezone info.
    
    The behaviour of assigning a column to an existing dataframe as `df['a'] = i` remains unchanged (this already returned an `object` column with a timezone).

  - When passing multiple levels to <span class="title-ref">\~pandas.DataFrame.stack</span>, it will now raise a `ValueError` when the levels aren't all level names or all level numbers (`7660`). See \[Reshaping by stacking and unstacking \<reshaping.stack\_multiple\>\](\#reshaping-by-stacking-and-unstacking-\<reshaping.stack\_multiple\>).

  - Raise a `ValueError` in `df.to_hdf` with 'fixed' format, if `df` has non-unique columns as the resulting file will be broken (`7761`)

  - `SettingWithCopy` raise/warnings (according to the option `mode.chained_assignment`) will now be issued when setting a value on a sliced mixed-dtype DataFrame using chained-assignment. (`7845`, `7950`)
    
    ``` python
    In [1]: df = pd.DataFrame(np.arange(0, 9), columns=['count'])
    
    In [2]: df['group'] = 'b'
    
    In [3]: df.iloc[0:5]['group'] = 'a'
    /usr/local/bin/ipython:1: SettingWithCopyWarning:
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
    ```

  - `merge`, `DataFrame.merge`, and `ordered_merge` now return the same type as the `left` argument (`7737`).

  - Previously an enlargement with a mixed-dtype frame would act unlike `.append` which will preserve dtypes (related `2578`, `8176`):
    
    <div class="ipython">
    
    python
    
      - df = pd.DataFrame(\[\[True, 1\], \[False, 2\]\],  
        columns=\["female", "fitness"\])
    
    df df.dtypes
    
    \# dtypes are now preserved df.loc\[2\] = df.loc\[1\] df df.dtypes
    
    </div>

  - `Series.to_csv()` now returns a string when `path=None`, matching the behaviour of `DataFrame.to_csv()` (`8215`).

  - `read_hdf` now raises `IOError` when a file that doesn't exist is passed in. Previously, a new, empty file was created, and a `KeyError` raised (`7715`).

</div>

\- `DataFrame.info()` now ends its output with a newline character (`8114`) `` ` - Concatenating no objects will now raise a ``ValueError`rather than a bare`Exception`. - Merge errors will now be sub-classes of`ValueError`rather than raw`Exception``(:issue:`8501`) -``DataFrame.plot`and`Series.plot``keywords are now have consistent orders (:issue:`8037`)   .. _whatsnew_0150.refactoring:  Internal refactoring ^^^^^^^^^^^^^^^^^^^^  In 0.15.0``Index`has internally been refactored to no longer sub-class`ndarray`but instead subclass`PandasObject``, similarly to the rest of the pandas objects. This change allows very easy sub-classing and creation of new index types. This should be a transparent change with only very limited API implications (:issue:`5080`, :issue:`7439`, :issue:`7796`, :issue:`8024`, :issue:`8367`, :issue:`7997`, :issue:`8522`):  - you may need to unpickle pandas version < 0.15.0 pickles using``pd.read\_pickle`rather than`pickle.load`. See [pickle docs <io.pickle>](#pickle-docs-<io.pickle>) - when plotting with a`PeriodIndex`, the matplotlib internal axes will now be arrays of`Period`rather than a`PeriodIndex`(this is similar to how a`DatetimeIndex`passes arrays of`datetimes``now) - MultiIndexes will now raise similarly to other pandas objects w.r.t. truth testing, see [here <gotchas.truth>](#here-<gotchas.truth>) (:issue:`7897`). - When plotting a DatetimeIndex directly with matplotlib's``plot`function,   the axis labels will no longer be formatted as dates but as integers (the   internal representation of a`datetime64`). **UPDATE** This is fixed   in 0.15.1, see [here <whatsnew_0151.datetime64_plotting>](#here-<whatsnew_0151.datetime64_plotting>).  .. _whatsnew_0150.deprecations:  Deprecations ^^^^^^^^^^^^  - The attributes`Categorical`  `labels`and`levels`attributes are   deprecated and renamed to`codes`and`categories`. - The`outtype`argument to`pd.DataFrame.to\_dict`has been deprecated in favor of`orient``. (:issue:`7840`) - The``convert\_dummies`method has been deprecated in favor of`get\_dummies``(:issue:`8140`) - The``infer\_dst`argument in`tz\_localize`will be deprecated in favor of`ambiguous`to allow for more flexibility in dealing with DST transitions.   Replace`infer\_dst=True`with`ambiguous='infer'``for the same behavior (:issue:`7943`).   See [the docs<timeseries.timezone_ambiguous>](#the-docs<timeseries.timezone_ambiguous>) for more details. - The top-level``pd.value\_range`has been deprecated and can be replaced by`.describe()``(:issue:`8481`)  .. _whatsnew_0150.index_set_ops:  - The``Index`set operations`+`and`-`were deprecated in order to provide these for numeric type operations on certain index types.`+`can be replaced by`.union()`or`|`, and`-`by`.difference()`. Further the method name`Index.diff()`is deprecated and can be replaced by`Index.difference()``(:issue:`8226`)``\`python \# + pd.Index(\['a', 'b', 'c'\]) + pd.Index(\['b', 'c', 'd'\])

> \# should be replaced by pd.Index(\['a', 'b', 'c'\]).union(pd.Index(\['b', 'c', 'd'\]))
> 
> ``` python
> ```
> 
> \# -pd.Index(\['a', 'b', 'c'\]) - pd.Index(\['b', 'c', 'd'\])
> 
> \# should be replaced by pd.Index(\['a', 'b', 'c'\]).difference(pd.Index(\['b', 'c', 'd'\]))

  - The `infer_types` argument to <span class="title-ref">\~pandas.read\_html</span> now has no effect and is deprecated (`7762`, `7032`).

<div id="whatsnew_0150.prior_deprecations">

Removal of prior version deprecations/changes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  - Remove ``DataFrame.delevel`method in favor of`DataFrame.reset\_index`.. _whatsnew_0150.enhancements:  Enhancements ~~~~~~~~~~~~  Enhancements in the importing/exporting of Stata files:  - Added support for bool, uint8, uint16 and uint32 data types in`to\_stata``(:issue:`7097`, :issue:`7365`) - Added conversion option when importing Stata files (:issue:`8527`) -``DataFrame.to\_stata`and`StataWriter`check string length for   compatibility with limitations imposed in dta files where fixed-width   strings must contain 244 or fewer characters.  Attempting to write Stata   dta files with strings longer than 244 characters raises a`ValueError``. (:issue:`7858`) -``read\_stata`and`StataReader`can import missing data information into a`DataFrame`by setting the argument`convert\_missing`to`True`. When   using this options, missing values are returned as`StataMissingValue`objects and columns containing missing values have`object``data type. (:issue:`8045`)  Enhancements in the plotting functions:  - Added``layout`keyword to`DataFrame.plot`. You can pass a tuple of`(rows, columns)`, one of which can be`-1``to automatically infer (:issue:`6667`, :issue:`8071`). - Allow to pass multiple axes to``DataFrame.plot`,`hist`and`boxplot``(:issue:`5353`, :issue:`6970`, :issue:`7069`) - Added support for``c`,`colormap`and`colorbar`arguments for`DataFrame.plot`with`kind='scatter'``(:issue:`7780`) - Histogram from``DataFrame.plot`with`kind='hist'``(:issue:`7809`), See [the docs<visualization.hist>](#the-docs<visualization.hist>). - Boxplot from``DataFrame.plot`with`kind='box'``(:issue:`7998`), See [the docs<visualization.box>](#the-docs<visualization.box>).  Other:  -``read\_csv`now has a keyword parameter`float\_precision``which specifies which floating-point converter the C engine should use during parsing, see [here <io.float_precision>](#here-<io.float_precision>) (:issue:`8002`, :issue:`8044`)  - Added``searchsorted`method to`Series``objects (:issue:`7447`)  - `describe` on mixed-types DataFrames is more flexible. Type-based column filtering is now possible via the``include`/`exclude``arguments.   See the [docs <basics.describe>](#docs-<basics.describe>) (:issue:`8164`).    .. ipython:: python      df = pd.DataFrame({'catA': ['foo', 'foo', 'bar'] * 8,                        'catB': ['a', 'b', 'c', 'd'] * 6,                        'numC': np.arange(24),                        'numD': np.arange(24.) + .5})     df.describe(include=["object"])     df.describe(include=["number", "object"], exclude=["float"])    Requesting all columns is possible with the shorthand 'all'    .. ipython:: python      df.describe(include='all')    Without those arguments,``describe`will behave as before, including only numerical columns or, if none are, only categorical columns. See also the [docs <basics.describe>](#docs-<basics.describe>)  - Added`split`as an option to the`orient`argument in`pd.DataFrame.to\_dict``. (:issue:`7840`)  - The``get\_dummies`method can now be used on DataFrames. By default only   categorical columns are encoded as 0's and 1's, while other columns are   left untouched.    .. ipython:: python      df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['c', 'c', 'b'],                     'C': [1, 2, 3]})     pd.get_dummies(df)  -`PeriodIndex`supports`resolution`as the same as`DatetimeIndex``(:issue:`7708`) -``pandas.tseries.holiday``has added support for additional holidays and ways to observe holidays (:issue:`7070`) -``pandas.tseries.holiday.Holiday``now supports a list of offsets in Python3 (:issue:`7070`) -``pandas.tseries.holiday.Holiday``now supports a days_of_week parameter (:issue:`7070`) -``GroupBy.nth()``now supports selecting multiple nth values (:issue:`7910`)    .. ipython:: python      business_dates = pd.date_range(start='4/1/2014', end='6/30/2014', freq='B')     df = pd.DataFrame(1, index=business_dates, columns=['a', 'b'])     # get the first, 4th, and last date index for each month     df.groupby([df.index.year, df.index.month]).nth([0, 3, -1])  -``Period`and`PeriodIndex`supports addition/subtraction with`timedelta``-likes (:issue:`7966`)    If``Period`freq is`D`,`H`,`T`,`S`,`L`,`U`,`N`,`Timedelta`-like can be added if the result can have same freq. Otherwise, only the same`offsets`can be added.`\`ipython In \[104\]: idx = pd.period\_range('2014-07-01 09:00', periods=5, freq='H')

</div>

> In \[105\]: idx Out\[105\]: PeriodIndex(\['2014-07-01 09:00', '2014-07-01 10:00', '2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00'\], dtype='period\[H\]')
> 
> In \[106\]: idx + pd.offsets.Hour(2) Out\[106\]: PeriodIndex(\['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00', '2014-07-01 14:00', '2014-07-01 15:00'\], dtype='period\[H\]')
> 
> In \[107\]: idx + pd.Timedelta('120m') Out\[107\]: PeriodIndex(\['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00', '2014-07-01 14:00', '2014-07-01 15:00'\], dtype='period\[H\]')
> 
> In \[108\]: idx = pd.period\_range('2014-07', periods=5, freq='M')
> 
> In \[109\]: idx Out\[109\]: PeriodIndex(\['2014-07', '2014-08', '2014-09', '2014-10', '2014-11'\], dtype='period\[M\]')
> 
> In \[110\]: idx + pd.offsets.MonthEnd(3) Out\[110\]: PeriodIndex(\['2014-10', '2014-11', '2014-12', '2015-01', '2015-02'\], dtype='period\[M\]')

  - Added experimental compatibility with `openpyxl` for versions \>= 2.0. The `DataFrame.to_excel` method `engine` keyword now recognizes `openpyxl1` and `openpyxl2` which will explicitly require openpyxl v1 and v2 respectively, failing if the requested version is not available. The `openpyxl` engine is a now a meta-engine that automatically uses whichever version of openpyxl is installed. (`7177`)

  - `DataFrame.fillna` can now accept a `DataFrame` as a fill value (`8377`)

  - Passing multiple levels to <span class="title-ref">\~pandas.DataFrame.stack</span> will now work when multiple level numbers are passed (`7660`). See \[Reshaping by stacking and unstacking \<reshaping.stack\_multiple\>\](\#reshaping-by-stacking-and-unstacking-\<reshaping.stack\_multiple\>).

  - <span class="title-ref">set\_names</span>, <span class="title-ref">set\_labels</span>, and <span class="title-ref">set\_levels</span> methods now take an optional `level` keyword argument to all modification of specific level(s) of a MultiIndex. Additionally <span class="title-ref">set\_names</span> now accepts a scalar string value when operating on an `Index` or on a specific level of a `MultiIndex` (`7792`)
    
    <div class="ipython">
    
    python
    
      - idx = pd.MultiIndex.from\_product(\[\['a'\], range(3), list("pqr")\],  
        names=\['foo', 'bar', 'baz'\])
    
    idx.set\_names('qux', level=0) idx.set\_names(\['qux', 'corge'\], level=\[0, 1\]) idx.set\_levels(\['a', 'b', 'c'\], level='bar') idx.set\_levels(\[\['a', 'b', 'c'\], \[1, 2, 3\]\], level=\[1, 2\])
    
    </div>

  - `Index.isin` now supports a `level` argument to specify which index level to use for membership tests (`7892`, `7890`)
    
    ``` ipython
    In [1]: idx = pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']])
    
    In [2]: idx.values
    Out[2]: array([(0, 'a'), (0, 'b'), (0, 'c'), (1, 'a'), (1, 'b'), (1, 'c')], dtype=object)
    
    In [3]: idx.isin(['a', 'c', 'e'], level=1)
    Out[3]: array([ True, False,  True,  True, False,  True], dtype=bool)
    ```

  - `Index` now supports `duplicated` and `drop_duplicates`. (`4060`)
    
    <div class="ipython">
    
    python
    
    idx = pd.Index(\[1, 2, 3, 4, 1, 2\]) idx idx.duplicated() idx.drop\_duplicates()
    
    </div>

  - add `copy=True` argument to `pd.concat` to enable pass through of complete blocks (`8252`)

  - Added support for numpy 1.8+ data types (`bool_`, `int_`, `float_`, `string_`) for conversion to R dataframe (`8400`)

<div id="whatsnew_0150.performance">

Performance `` ` ~~~~~~~~~~~  - Performance improvements in ``DatetimeIndex.\_\_iter\_\_``to allow faster iteration (:issue:`7683`) - Performance improvements in``Period`creation (and`PeriodIndex``setitem) (:issue:`5155`) - Improvements in Series.transform for significant performance gains (revised) (:issue:`6496`) - Performance improvements in``StataReader``when reading large files (:issue:`8040`, :issue:`8073`) - Performance improvements in``StataWriter``when writing large files (:issue:`8079`) - Performance and memory usage improvements in multi-key``groupby``(:issue:`8128`) - Performance improvements in groupby``.agg`and`.apply``where builtins max/min were not mapped to numpy/cythonized versions (:issue:`7722`) - Performance improvement in writing to sql (``to\_sql``) of up to 50% (:issue:`8208`). - Performance benchmarking of groupby for large value of ngroups (:issue:`6787`) - Performance improvement in``CustomBusinessDay`,`CustomBusinessMonth``(:issue:`8236`) - Performance improvement for``MultiIndex.values``for multi-level indexes containing datetimes (:issue:`8543`)    .. _whatsnew_0150.bug_fixes:  Bug fixes ~~~~~~~~~  - Bug in pivot_table, when using margins and a dict aggfunc (:issue:`8349`) - Bug in``read\_csv`where`squeeze=True``would return a view (:issue:`8217`) - Bug in checking of table name in``read\_sql``in certain cases (:issue:`7826`). - Bug in``DataFrame.groupby`where`Grouper``does not recognize level when frequency is specified (:issue:`7885`) - Bug in multiindexes dtypes getting mixed up when DataFrame is saved to SQL table (:issue:`8021`) - Bug in``Series``0-division with a float and integer operand dtypes  (:issue:`7785`) - Bug in``Series.astype("unicode")`not calling`unicode``on the values correctly (:issue:`7758`) - Bug in``DataFrame.as\_matrix()`with mixed`datetime64\[ns\]`and`timedelta64\[ns\]``dtypes (:issue:`7778`) - Bug in``HDFStore.select\_column()`not preserving UTC timezone info when selecting a`DatetimeIndex``(:issue:`7777`) - Bug in``to\_datetime`when`format='%Y%m%d'`and`coerce=True`are specified, where previously an object array was returned (rather than   a coerced time-series with`NaT``), (:issue:`7930`) - Bug in``DatetimeIndex`and`PeriodIndex``in-place addition and subtraction cause different result from normal one (:issue:`6527`) - Bug in adding and subtracting``PeriodIndex`with`PeriodIndex`raise`TypeError``(:issue:`7741`) - Bug in``combine\_first`with`PeriodIndex`data raises`TypeError``(:issue:`3367`) - Bug in MultiIndex slicing with missing indexers (:issue:`7866`) - Bug in MultiIndex slicing with various edge cases (:issue:`8132`) - Regression in MultiIndex indexing with a non-scalar type object (:issue:`7914`) - Bug in``Timestamp`comparisons with`==`and`int64``dtype (:issue:`8058`) - Bug in pickles contains``DateOffset`may raise`AttributeError`when`normalize``attribute is referred internally (:issue:`7748`) - Bug in``Panel`when using`major\_xs`and`copy=False`is passed (deprecation warning fails because of missing`warnings``) (:issue:`8152`). - Bug in pickle deserialization that failed for pre-0.14.1 containers with dup items trying to avoid ambiguity   when matching block and manager items, when there's only one block there's no ambiguity (:issue:`7794`) - Bug in putting a``PeriodIndex`into a`Series`would convert to`int64`dtype, rather than`object`of`Periods``(:issue:`7932`) - Bug in``HDFStore``iteration when passing a where (:issue:`8014`) - Bug in``DataFrameGroupby.transform``when transforming with a passed non-sorted key (:issue:`8046`, :issue:`8430`) - Bug in repeated timeseries line and area plot may result in``ValueError``or incorrect kind (:issue:`7733`) - Bug in inference in a``MultiIndex`with`datetime.date``inputs (:issue:`7888`) - Bug in``get`where an`IndexError``would not cause the default value to be returned (:issue:`7725`) - Bug in``offsets.apply`,`rollforward`and`rollback``may reset nanosecond (:issue:`7697`) - Bug in``offsets.apply`,`rollforward`and`rollback`may raise`AttributeError`if`Timestamp`has`dateutil``tzinfo (:issue:`7697`) - Bug in sorting a MultiIndex frame with a``Float64Index``(:issue:`8017`) - Bug in inconsistent panel setitem with a rhs of a``DataFrame``for alignment (:issue:`7763`) - Bug in``is\_superperiod`and`is\_subperiod`cannot handle higher frequencies than`S``(:issue:`7760`, :issue:`7772`, :issue:`7803`) - Bug in 32-bit platforms with``Series.shift``(:issue:`8129`) - Bug in``PeriodIndex.unique`returns int64`np.ndarray``(:issue:`7540`) - Bug in``groupby.apply``with a non-affecting mutation in the function (:issue:`8467`) - Bug in``DataFrame.reset\_index`which has`MultiIndex`contains`PeriodIndex`or`DatetimeIndex`with tz raises`ValueError``(:issue:`7746`, :issue:`7793`) - Bug in``DataFrame.plot`with`subplots=True``may draw unnecessary minor xticks and yticks (:issue:`7801`) - Bug in``StataReader``which did not read variable labels in 117 files due to difference between Stata documentation and implementation (:issue:`7816`) - Bug in``StataReader``where strings were always converted to 244 characters-fixed width irrespective of underlying string size (:issue:`7858`) - Bug in``DataFrame.plot`and`Series.plot`may ignore`rot`and`fontsize``keywords (:issue:`7844`) - Bug in``DatetimeIndex.value\_counts``doesn't preserve tz  (:issue:`7735`) - Bug in``PeriodIndex.value\_counts`results in`Int64Index``(:issue:`7735`) - Bug in``DataFrame.join``when doing left join on index and there are multiple matches (:issue:`5391`) - Bug in``GroupBy.transform()``where int groups with a transform that   didn't preserve the index were incorrectly truncated (:issue:`7972`). - Bug in``groupby`where callable objects without name attributes would take the wrong path,   and produce a`DataFrame`instead of a`Series``(:issue:`7929`) - Bug in``groupby``error message when a DataFrame grouping column is duplicated (:issue:`7511`) - Bug in``read\_html`where the`infer\_types``argument forced coercion of   date-likes incorrectly (:issue:`7762`, :issue:`7032`). - Bug in``Series.str.cat``with an index which was filtered as to not include the first item (:issue:`7857`) - Bug in``Timestamp`cannot parse`nanosecond``from string (:issue:`7878`) - Bug in``Timestamp`with string offset and`tz``results incorrect (:issue:`7833`) - Bug in``tslib.tz\_convert`and`tslib.tz\_convert\_single``may return different results (:issue:`7798`) - Bug in``DatetimeIndex.intersection`of non-overlapping timestamps with tz raises`IndexError``(:issue:`7880`) - Bug in alignment with TimeOps and non-unique indexes (:issue:`8363`) - Bug in``GroupBy.filter()``where fast path vs. slow path made the filter   return a non scalar value that appeared valid but wasn't (:issue:`7870`). - Bug in``date\_range()`/`DatetimeIndex()``when the timezone was inferred from input dates yet incorrect   times were returned when crossing DST boundaries (:issue:`7835`, :issue:`7901`). - Bug in``to\_excel()``where a negative sign was being prepended to positive infinity and was absent for negative infinity (:issue:`7949`) - Bug in area plot draws legend with incorrect``alpha`when`stacked=True``(:issue:`8027`) -``Period`and`PeriodIndex`addition/subtraction with`np.timedelta64``results in incorrect internal representations (:issue:`7740`) - Bug in``Holiday``with no offset or observance (:issue:`7987`) - Bug in``DataFrame.to\_latex`formatting when columns or index is a`MultiIndex``(:issue:`7982`). - Bug in``DateOffset``around Daylight Savings Time produces unexpected results (:issue:`5175`). - Bug in``DataFrame.shift`where empty columns would throw`ZeroDivisionError``on numpy 1.7 (:issue:`8019`) - Bug in installation where``html\_encoding/\*.html``wasn't installed and   therefore some tests were not running correctly (:issue:`7927`). - Bug in``read\_html`where`bytes`objects were not tested for in`\_read``(:issue:`7927`). - Bug in``DataFrame.stack()``when one of the column levels was a datelike (:issue:`8039`) - Bug in broadcasting numpy scalars with``DataFrame``(:issue:`8116`) - Bug in``pivot\_table`performed with nameless`index`and`columns`raises`KeyError``(:issue:`8103`) - Bug in``DataFrame.plot(kind='scatter')`draws points and errorbars with different colors when the color is specified by`c``keyword (:issue:`8081`) - Bug in``Float64Index`where`iat`and`at``were not testing and were   failing (:issue:`8092`). - Bug in``DataFrame.boxplot()``where y-limits were not set correctly when   producing multiple axes (:issue:`7528`, :issue:`5517`). - Bug in``read\_csv`where line comments were not handled correctly given   a custom line terminator or`delim\_whitespace=True``(:issue:`8122`). - Bug in``read\_html`where empty tables caused a`StopIteration``(:issue:`7575`) - Bug in casting when setting a column in a same-dtype block (:issue:`7704`) - Bug in accessing groups from a``GroupBy``when the original grouper   was a tuple (:issue:`8121`). - Bug in``.at``that would accept integer indexers on a non-integer index and do fallback (:issue:`7814`) - Bug with kde plot and NaNs (:issue:`8182`) - Bug in``GroupBy.count``with float32 data type were nan values were not excluded (:issue:`8169`). - Bug with stacked barplots and NaNs (:issue:`8175`). - Bug in resample with non evenly divisible offsets (e.g. '7s') (:issue:`8371`) - Bug in interpolation methods with the``limit``keyword when no values needed interpolating (:issue:`7173`). - Bug where``col\_space`was ignored in`DataFrame.to\_string()`when`header=False``(:issue:`8230`). - Bug with``DatetimeIndex.asof``incorrectly matching partial strings and returning the wrong date (:issue:`8245`). - Bug in plotting methods modifying the global matplotlib rcParams (:issue:`8242`). - Bug in``DataFrame.\_\_setitem\_\_``that caused errors when setting a dataframe column to a sparse array (:issue:`8131`) - Bug where``Dataframe.boxplot()``failed when entire column was empty (:issue:`8181`). - Bug with messed variables in``radviz``visualization (:issue:`8199`). - Bug in interpolation methods with the``limit``keyword when no values needed interpolating (:issue:`7173`). - Bug where``col\_space`was ignored in`DataFrame.to\_string()`when`header=False``(:issue:`8230`). - Bug in``to\_clipboard``that would clip long column data (:issue:`8305`) - Bug in``DataFrame``terminal display: Setting max_column/max_rows to zero did not trigger auto-resizing of dfs to fit terminal width/height (:issue:`7180`). - Bug in OLS where running with "cluster" and "nw_lags" parameters did not work correctly, but also did not throw an error   (:issue:`5884`). - Bug in``DataFrame.dropna``that interpreted non-existent columns in the subset argument as the 'last column' (:issue:`8303`) - Bug in``Index.intersection``on non-monotonic non-unique indexes (:issue:`8362`). - Bug in masked series assignment where mismatching types would break alignment (:issue:`8387`) - Bug in``NDFrame.equals``gives false negatives with dtype=object (:issue:`8437`) - Bug in assignment with indexer where type diversity would break alignment (:issue:`8258`) - Bug in``NDFrame.loc``indexing when row/column names were lost when target was a list/ndarray (:issue:`6552`) - Regression in``NDFrame.loc``indexing when rows/columns were converted to Float64Index if target was an empty list/ndarray (:issue:`7774`) - Bug in``Series`that allows it to be indexed by a`DataFrame``which has unexpected results.  Such indexing is no longer permitted (:issue:`8444`) - Bug in item assignment of a``DataFrame``with MultiIndex columns where right-hand-side columns were not aligned (:issue:`7655`) - Suppress FutureWarning generated by NumPy when comparing object arrays containing NaN for equality (:issue:`7065`) - Bug in``DataFrame.eval()`where the dtype of the`not`operator (`\~`)   was not correctly inferred as`bool\`\`.

</div>

## Contributors

<div class="contributors">

v0.14.1..v0.15.0

</div>

---

v0.15.1.md

---

# Version 0.15.1 (November 9, 2014)

{{ header }}

This is a minor bug-fix release from 0.15.0 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

  - \[Enhancements \<whatsnew\_0151.enhancements\>\](\#enhancements-\<whatsnew\_0151.enhancements\>)
  - \[API Changes \<whatsnew\_0151.api\>\](\#api-changes-\<whatsnew\_0151.api\>)
  - \[Bug Fixes \<whatsnew\_0151.bug\_fixes\>\](\#bug-fixes-\<whatsnew\_0151.bug\_fixes\>)

## API changes

  - `s.dt.hour` and other `.dt` accessors will now return `np.nan` for missing values (rather than previously -1), (`8689`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(pd.date\_range("20130101", periods=5, freq="D")) s.iloc\[2\] = np.nan s
    
    </div>
    
    previous behavior:
    
      - \`\`\`ipython  
        In \[6\]: s.dt.hour Out\[6\]: 0 0 1 0 2 -1 3 0 4 0 dtype: int64
    
    current behavior:
    
    <div class="ipython">
    
    python
    
    s.dt.hour
    
    </div>

  - `groupby` with `as_index=False` will not add erroneous extra columns to result (`8582`):
    
    <div class="ipython">
    
    python
    
    np.random.seed(2718281) df = pd.DataFrame(np.random.randint(0, 100, (10, 2)), columns=\["jim", "joe"\]) df.head()
    
    ts = pd.Series(5 \* np.random.randint(0, 3, 10))
    
    </div>
    
    previous behavior:
    
    ``` ipython
    In [4]: df.groupby(ts, as_index=False).max()
    Out[4]:
       NaN  jim  joe
    0    0   72   83
    1    5   77   84
    2   10   96   65
    ```
    
    current behavior:
    
    ``` ipython
    In [4]: df.groupby(ts, as_index=False).max()
    Out[4]:
       jim  joe
    0   72   83
    1   77   84
    2   96   65
    ```

  - `groupby` will not erroneously exclude columns if the column name conflicts with the grouper name (`8112`):
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame({"jim": range(5), "joe": range(5, 10)}) df gr = df.groupby(df\["jim"\] \< 2)
    
    </div>
    
    previous behavior (excludes 1st column from output):
    
    ``` ipython
    In [4]: gr.apply("sum")
    Out[4]:
           joe
    jim
    False   24
    True    11
    ```
    
    current behavior:
    
    <div class="ipython">
    
    python
    
    gr.apply("sum")
    
    </div>

  - Support for slicing with monotonic decreasing indexes, even if `start` or `stop` is not found in the index (`7860`):
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\["a", "b", "c", "d"\], \[4, 3, 2, 1\]) s
    
    </div>
    
    previous behavior:
    
    ``` ipython
    In [8]: s.loc[3.5:1.5]
    KeyError: 3.5
    ```
    
    current behavior:
    
    <div class="ipython">
    
    python
    
    s.loc\[3.5:1.5\]
    
    </div>

\- `io.data.Options` has been fixed for a change in the format of the Yahoo Options page (`8612`), (`8741`)

> \> **Note**

  - \>  
    As a result of a change in Yahoo's option page layout, when an expiry date is given, `Options` methods now return data for a single expiry date. Previously, methods returned all data for the selected month.
    
    The `month` and `year` parameters have been undeprecated and can be used to get all options data for a given month.
    
    If an expiry date that is not valid is given, data for the next expiry after the given date is returned.
    
    Option data frames are now saved on the instance as `callsYYMMDD` or `putsYYMMDD`. Previously they were saved as `callsMMYY` and `putsMMYY`. The next expiry is saved as `calls` and `puts`.
    
    New features:
    
      - The expiry parameter can now be a single date or a list-like object containing dates.
      - A new property `expiry_dates` was added, which returns all available expiry dates.
    
    Current behavior:
    
    ``` ipython
    In [17]: from pandas.io.data import Options
    
    In [18]: aapl = Options('aapl', 'yahoo')
    
    In [19]: aapl.get_call_data().iloc[0:5, 0:1]
    Out[19]:
                                                 Last
    Strike Expiry     Type Symbol
    80     2014-11-14 call AAPL141114C00080000  29.05
    84     2014-11-14 call AAPL141114C00084000  24.80
    85     2014-11-14 call AAPL141114C00085000  24.05
    86     2014-11-14 call AAPL141114C00086000  22.76
    87     2014-11-14 call AAPL141114C00087000  21.74
    
    In [20]: aapl.expiry_dates
    Out[20]:
    [datetime.date(2014, 11, 14),
     datetime.date(2014, 11, 22),
     datetime.date(2014, 11, 28),
     datetime.date(2014, 12, 5),
     datetime.date(2014, 12, 12),
     datetime.date(2014, 12, 20),
     datetime.date(2015, 1, 17),
     datetime.date(2015, 2, 20),
     datetime.date(2015, 4, 17),
     datetime.date(2015, 7, 17),
     datetime.date(2016, 1, 15),
     datetime.date(2017, 1, 20)]
    
    In [21]: aapl.get_near_stock_price(expiry=aapl.expiry_dates[0:3]).iloc[0:5, 0:1]
    Out[21]:
                                                Last
    Strike Expiry     Type Symbol
    109    2014-11-22 call AAPL141122C00109000  1.48
           2014-11-28 call AAPL141128C00109000  1.79
    110    2014-11-14 call AAPL141114C00110000  0.55
           2014-11-22 call AAPL141122C00110000  1.02
           2014-11-28 call AAPL141128C00110000  1.32
    ```

<div id="whatsnew_0151.datetime64_plotting">

  - pandas now also registers the `datetime64` dtype in matplotlib's units registry to plot such values as datetimes. This is activated once pandas is imported. In previous versions, plotting an array of `datetime64` values will have resulted in plotted integer values. To keep the previous behaviour, you can do `del matplotlib.units.registry[np.datetime64]` (`8614`).

</div>

<div id="whatsnew_0151.enhancements">

Enhancements `` ` ~~~~~~~~~~~~  - ``concat``permits a wider variety of iterables of pandas objects to be   passed as the first parameter (:issue:`8645`):    .. ipython:: python       from collections import deque       df1 = pd.DataFrame([1, 2, 3])      df2 = pd.DataFrame([4, 5, 6])    previous behavior:``\`ipython In \[7\]: pd.concat(deque((df1, df2))) TypeError: first argument must be a list-like of pandas objects, you passed an object of type "deque"

</div>

> current behavior:
> 
> <div class="ipython">
> 
> python
> 
> pd.concat(deque((df1, df2)))
> 
> </div>

  - Represent `MultiIndex` labels with a dtype that utilizes memory based on the level size. In prior versions, the memory usage was a constant 8 bytes per element in each level. In addition, in prior versions, the *reported* memory usage was incorrect as it didn't show the usage for the memory occupied by the underling data array. (`8456`)
    
    <div class="ipython">
    
    python
    
      - dfi = pd.DataFrame(  
        1, index=pd.MultiIndex.from\_product(\[\["a"\], range(1000)\]), columns=\["A"\]
    
    )
    
    </div>
    
    previous behavior:
    
    ``` ipython
    # this was underreported in prior versions
    In [1]: dfi.memory_usage(index=True)
    Out[1]:
    Index    8000 # took about 24008 bytes in < 0.15.1
    A        8000
    dtype: int64
    ```
    
    current behavior:
    
    <div class="ipython">
    
    python
    
    dfi.memory\_usage(index=True)
    
    </div>

  - Added Index properties `is_monotonic_increasing` and `is_monotonic_decreasing` (`8680`).

  - Added option to select columns when importing Stata files (`7935`)

  - Qualify memory usage in `DataFrame.info()` by adding `+` if it is a lower bound (`8578`)

  - Raise errors in certain aggregation cases where an argument such as `numeric_only` is not handled (`8592`).

  - Added support for 3-character ISO and non-standard country codes in <span class="title-ref">io.wb.download</span> (`8482`)

  - World Bank data requests now will warn/raise based on an `errors` argument, as well as a list of hard-coded country codes and the World Bank's JSON response. In prior versions, the error messages didn't look at the World Bank's JSON response. Problem-inducing input were simply dropped prior to the request. The issue was that many good countries were cropped in the hard-coded approach. All countries will work now, but some bad countries will raise exceptions because some edge cases break the entire response. (`8482`)

  - Added option to `Series.str.split()` to return a `DataFrame` rather than a `Series` (`8428`)

  - Added option to `df.info(null_counts=None|True|False)` to override the default display options and force showing of the null-counts (`8701`)

<div id="whatsnew_0151.bug_fixes">

Bug fixes `` ` ~~~~~~~~~  - Bug in unpickling of a ``CustomBusinessDay``object (:issue:`8591`) - Bug in coercing``Categorical`to a records array, e.g.`df.to\_records()``(:issue:`8626`) - Bug in``Categorical`not created properly with`Series.to\_frame()``(:issue:`8626`) - Bug in coercing in astype of a``Categorical`of a passed`pd.Categorical`(this now raises`TypeError``correctly), (:issue:`8626`) - Bug in``cut`/`qcut`when using`Series`and`retbins=True``(:issue:`8589`) - Bug in writing Categorical columns to an SQL database with``to\_sql``(:issue:`8624`). - Bug in comparing``Categorical``of datetime raising when being compared to a scalar datetime (:issue:`8687`) - Bug in selecting from a``Categorical`with`.iloc``(:issue:`8623`) - Bug in groupby-transform with a Categorical (:issue:`8623`) - Bug in duplicated/drop_duplicates with a Categorical (:issue:`8623`) - Bug in``Categorical``reflected comparison operator raising if the first argument was a numpy array scalar (e.g. np.int64) (:issue:`8658`) - Bug in Panel indexing with a list-like (:issue:`8710`) - Compat issue is``DataFrame.dtypes`when`options.mode.use\_inf\_as\_null``is True (:issue:`8722`) - Bug in``read\_csv`,`dialect``parameter would not take a string (:issue:`8703`) - Bug in slicing a MultiIndex level with an empty-list (:issue:`8737`) - Bug in numeric index operations of add/sub with Float/Index Index with numpy arrays (:issue:`8608`) - Bug in setitem with empty indexer and unwanted coercion of dtypes (:issue:`8669`) - Bug in ix/loc block splitting on setitem (manifests with integer-like dtypes, e.g. datetime64) (:issue:`8607`) - Bug when doing label based indexing with integers not found in the index for   non-unique but monotonic indexes (:issue:`8680`). - Bug when indexing a Float64Index with``np.nan``on numpy 1.7 (:issue:`8980`). - Fix``shape`attribute for`MultiIndex``(:issue:`8609`) - Bug in``GroupBy`where a name conflict between the grouper and columns   would break`groupby``operations (:issue:`7115`, :issue:`8112`) - Fixed a bug where plotting a column``y``and specifying a label would mutate the index name of the original DataFrame (:issue:`8494`) - Fix regression in plotting of a DatetimeIndex directly with matplotlib (:issue:`8614`). - Bug in``date\_range``where partially-specified dates would incorporate current date (:issue:`6961`) - Bug in Setting by indexer to a scalar value with a mixed-dtype``Panel4d``was failing (:issue:`8702`) - Bug where``DataReader``'s would fail if one of the symbols passed was invalid.  Now returns data for valid symbols and np.nan for invalid (:issue:`8494`) - Bug in``get\_quote\_yahoo\`<span class="title-ref"> that wouldn't allow non-float return values (:issue:\`5229</span>).

</div>

## Contributors

<div class="contributors">

v0.15.0..v0.15.1

</div>

---

v0.15.2.md

---

# Version 0.15.2 (December 12, 2014)

{{ header }}

This is a minor release from 0.15.1 and includes a large number of bug fixes along with several new features, enhancements, and performance improvements. A small number of API changes were necessary to fix existing bugs. We recommend that all users upgrade to this version.

  - \[Enhancements \<whatsnew\_0152.enhancements\>\](\#enhancements-\<whatsnew\_0152.enhancements\>)
  - \[API Changes \<whatsnew\_0152.api\>\](\#api-changes-\<whatsnew\_0152.api\>)
  - \[Performance Improvements \<whatsnew\_0152.performance\>\](\#performance-improvements-\<whatsnew\_0152.performance\>)
  - \[Bug Fixes \<whatsnew\_0152.bug\_fixes\>\](\#bug-fixes-\<whatsnew\_0152.bug\_fixes\>)

## API changes

  - Indexing in `MultiIndex` beyond lex-sort depth is now supported, though a lexically sorted index will have a better performance. (`2646`)
    
      - \`\`\`ipython
        
          - In \[1\]: df = pd.DataFrame({'jim':\[0, 0, 1, 1\],  
            ...: 'joe':\['x', 'x', 'z', 'y'\], ...: 'jolie':np.random.rand(4)}).set\_index(\['jim', 'joe'\]) ...:
        
        In \[2\]: df Out\[2\]: jolie jim joe 0 x 0.126970 x 0.966718 1 z 0.260476 y 0.897237
        
        \[4 rows x 1 columns\]
        
        In \[3\]: df.index.lexsort\_depth Out\[3\]: 1
        
        \# in prior versions this would raise a KeyError \# will now show a PerformanceWarning In \[4\]: df.loc\[(1, 'z')\] Out\[4\]: jolie jim joe 1 z 0.260476
        
        \[1 rows x 1 columns\]
        
        \# lexically sorting In \[5\]: df2 = df.sort\_index()
        
        In \[6\]: df2 Out\[6\]: jolie jim joe 0 x 0.126970 x 0.966718 1 y 0.897237 z 0.260476
        
        \[4 rows x 1 columns\]
        
        In \[7\]: df2.index.lexsort\_depth Out\[7\]: 2
        
        In \[8\]: df2.loc\[(1,'z')\] Out\[8\]: jolie jim joe 1 z 0.260476
        
        \[1 rows x 1 columns\]

  - Bug in unique of Series with `category` dtype, which returned all categories regardless whether they were "used" or not (see `8559` for the discussion). Previous behaviour was to return all categories:
    
    ``` ipython
    In [3]: cat = pd.Categorical(['a', 'b', 'a'], categories=['a', 'b', 'c'])
    
    In [4]: cat
    Out[4]:
    [a, b, a]
    Categories (3, object): [a < b < c]
    
    In [5]: cat.unique()
    Out[5]: array(['a', 'b', 'c'], dtype=object)
    ```
    
    Now, only the categories that do effectively occur in the array are returned:
    
    <div class="ipython">
    
    python
    
    cat = pd.Categorical(\['a', 'b', 'a'\], categories=\['a', 'b', 'c'\]) cat.unique()
    
    </div>

  - `Series.all` and `Series.any` now support the `level` and `skipna` parameters. `Series.all`, `Series.any`, `Index.all`, and `Index.any` no longer support the `out` and `keepdims` parameters, which existed for compatibility with ndarray. Various index types no longer support the `all` and `any` aggregation functions and will now raise `TypeError`. (`8302`).

  - Allow equality comparisons of Series with a categorical dtype and object dtype; previously these would raise `TypeError` (`8938`)

  - Bug in `NDFrame`: conflicting attribute/column names now behave consistently between getting and setting. Previously, when both a column and attribute named `y` existed, `data.y` would return the attribute, while `data.y = z` would update the column (`8994`)
    
    <div class="ipython">
    
    python
    
    data = pd.DataFrame({'x': \[1, 2, 3\]}) data.y = 2 data\['y'\] = \[2, 4, 6\] data
    
    \# this assignment was inconsistent data.y = 5
    
    </div>
    
    Old behavior:
    
    ``` ipython
    In [6]: data.y
    Out[6]: 2
    
    In [7]: data['y'].values
    Out[7]: array([5, 5, 5])
    ```
    
    New behavior:
    
    <div class="ipython">
    
    python
    
    data.y data\['y'\].values
    
    </div>

  - `Timestamp('now')` is now equivalent to `Timestamp.now()` in that it returns the local time rather than UTC. Also, `Timestamp('today')` is now equivalent to `Timestamp.today()` and both have `tz` as a possible argument. (`9000`)

  - Fix negative step support for label-based slices (`8753`)
    
    Old behavior:
    
    ``` ipython
    In [1]: s = pd.Series(np.arange(3), ['a', 'b', 'c'])
    Out[1]:
    a    0
    b    1
    c    2
    dtype: int64
    
    In [2]: s.loc['c':'a':-1]
    Out[2]:
    c    2
    dtype: int64
    ```
    
    New behavior:
    
    <div class="ipython">
    
    python
    
    s = pd.Series(np.arange(3), \['a', 'b', 'c'\]) s.loc\['c':'a':-1\]
    
    </div>

<div id="whatsnew_0152.enhancements">

Enhancements `` ` ~~~~~~~~~~~~ ``Categorical``enhancements:  - Added ability to export Categorical data to Stata (:issue:`8633`).  See [here <io.stata-categorical>](#here-<io.stata-categorical>) for limitations of categorical variables exported to Stata data files. - Added flag``order\_categoricals`to`StataReader`and`read\_stata``to select whether to order imported categorical data (:issue:`8836`).  See [here <io.stata-categorical>](#here-<io.stata-categorical>) for more information on importing categorical variables from Stata data files. - Added ability to export Categorical data to/from HDF5 (:issue:`7621`). Queries work the same as if it was an object array. However, the``category`dtyped data is stored in a more efficient manner. See [here <io.hdf5-categorical>](#here-<io.hdf5-categorical>) for an example and caveats w.r.t. prior versions of pandas. - Added support for`searchsorted()`on`Categorical``class (:issue:`8420`).  Other enhancements:  - Added the ability to specify the SQL type of columns when writing a DataFrame   to a database (:issue:`8778`).   For example, specifying to use the sqlalchemy``String`type instead of the   default`Text`type for string columns:`\`python from sqlalchemy.types import String data.to\_sql('data\_dtype', engine, dtype={'Col\_1': String}) \# noqa F821

</div>

  - `Series.all` and `Series.any` now support the `level` and `skipna` parameters (`8302`):
    
    ``` python
    >>> s = pd.Series([False, True, False], index=[0, 0, 1])
    >>> s.any(level=0)
    0     True
    1    False
    dtype: bool
    ```

  - `Panel` now supports the `all` and `any` aggregation functions. (`8302`):
    
    ``` python
    >>> p = pd.Panel(np.random.rand(2, 5, 4) > 0.1)
    >>> p.all()
           0      1      2     3
    0   True   True   True  True
    1   True  False   True  True
    2   True   True   True  True
    3  False   True  False  True
    4   True   True   True  True
    ```

\- Added support for `utcfromtimestamp()`, `fromtimestamp()`, and `combine()` on `Timestamp` class (`5351`). `` ` - Added Google Analytics ( ``pandas.io.ga``) basic documentation (:issue:`8835`). See `here <https://pandas.pydata.org/pandas-docs/version/0.15.2/remote_data.html#remote-data-ga>`__. -``Timedelta`arithmetic returns`NotImplemented``in unknown cases, allowing extensions by custom classes (:issue:`8813`). -``Timedelta`now supports arithmetic with`numpy.ndarray``objects of the appropriate dtype (numpy 1.8 or newer only) (:issue:`8884`). - Added``Timedelta.to\_timedelta64()``method to the public API (:issue:`8884`). - Added``gbq.generate\_bq\_schema()``function to the gbq module (:issue:`8325`). -``Series``now works with map objects the same way as generators (:issue:`8909`). - Added context manager to``HDFStore``for automatic closing (:issue:`8791`). -``to\_datetime`gains an`exact`keyword to allow for a format to not require an exact match for a provided format string (if its`False`).`exact`defaults to`True``(meaning that exact matching is still the default)  (:issue:`8904`) - Added``axvlines``boolean option to parallel_coordinates plot function, determines whether vertical lines will be printed, default is True - Added ability to read table footers to read_html (:issue:`8552`) -``to\_sql`now infers data types of non-NA values for columns that contain NA values and have dtype`object``(:issue:`8778`).   .. _whatsnew_0152.performance:  Performance ~~~~~~~~~~~  - Reduce memory usage when skiprows is an integer in read_csv (:issue:`8681`) - Performance boost for``to\_datetime`conversions with a passed`format=`, and the`exact=False``(:issue:`8904`)   .. _whatsnew_0152.bug_fixes:  Bug fixes ~~~~~~~~~  - Bug in concat of Series with``category`dtype which were coercing to`object``. (:issue:`8641`) - Bug in Timestamp-Timestamp not returning a Timedelta type and datelike-datelike ops with timezones (:issue:`8865`) - Made consistent a timezone mismatch exception (either tz operated with None or incompatible timezone), will now return``TypeError`rather than`ValueError``(a couple of edge cases only), (:issue:`8865`) - Bug in using a``pd.Grouper(key=...)``with no level/axis or level only (:issue:`8795`, :issue:`8866`) - Report a``TypeError``when invalid/no parameters are passed in a groupby (:issue:`8015`) - Bug in packaging pandas with``py2app/cx\_Freeze``(:issue:`8602`, :issue:`8831`) - Bug in``groupby``signatures that didn't include \*args or \*\*kwargs (:issue:`8733`). -``io.data.Options`now raises`RemoteDataError``when no expiry dates are available from Yahoo and when it receives no data from Yahoo (:issue:`8761`), (:issue:`8783`). - Unclear error message in csv parsing when passing dtype and names and the parsed data is a different data type (:issue:`8833`) - Bug in slicing a MultiIndex with an empty list and at least one boolean indexer (:issue:`8781`) -``io.data.Options`now raises`RemoteDataError``when no expiry dates are available from Yahoo (:issue:`8761`). -``Timedelta``kwargs may now be numpy ints and floats (:issue:`8757`). - Fixed several outstanding bugs for``Timedelta``arithmetic and comparisons (:issue:`8813`, :issue:`5963`, :issue:`5436`). -``sql\_schema`now generates dialect appropriate`CREATE TABLE``statements (:issue:`8697`) -``slice``string method now takes step into account (:issue:`8754`) - Bug in``BlockManager``where setting values with different type would break block integrity (:issue:`8850`) - Bug in``DatetimeIndex`when using`time``object as key (:issue:`8667`) - Bug in``merge`where`how='left'`and`sort=False``would not preserve left frame order (:issue:`7331`) - Bug in``MultiIndex.reindex``where reindexing at level would not reorder labels (:issue:`4088`) - Bug in certain operations with dateutil timezones, manifesting with dateutil 2.3 (:issue:`8639`) - Regression in DatetimeIndex iteration with a Fixed/Local offset timezone (:issue:`8890`) - Bug in``to\_datetime`when parsing a nanoseconds using the`%f``format (:issue:`8989`) -``io.data.Options`now raises`RemoteDataError``when no expiry dates are available from Yahoo and when it receives no data from Yahoo (:issue:`8761`), (:issue:`8783`). - Fix: The font size was only set on x axis if vertical or the y axis if horizontal. (:issue:`8765`) - Fixed division by 0 when reading big csv files in python 3 (:issue:`8621`) - Bug in outputting a MultiIndex with``to\_html,index=False``which would add an extra column (:issue:`8452`) - Imported categorical variables from Stata files retain the ordinal information in the underlying data (:issue:`8836`). - Defined``.size`attribute across`NDFrame`objects to provide compat with numpy >= 1.9.1; buggy with`np.array\_split``(:issue:`8846`) - Skip testing of histogram plots for matplotlib <= 1.2 (:issue:`8648`). - Bug where``get\_data\_google``returned object dtypes (:issue:`3995`) - Bug in``DataFrame.stack(..., dropna=False)`when the DataFrame's`columns`is a`MultiIndex`whose`labels`do not reference all its`levels``. (:issue:`8844`) - Bug in that Option context applied on``\_\_enter\_\_``(:issue:`8514`) - Bug in resample that causes a ValueError when resampling across multiple days   and the last offset is not calculated from the start of the range (:issue:`8683`) - Bug where``DataFrame.plot(kind='scatter')``fails when checking if an np.array is in the DataFrame (:issue:`8852`) - Bug in``pd.infer\_freq/DataFrame.inferred\_freq``that prevented proper sub-daily frequency inference when the index contained DST days (:issue:`8772`). - Bug where index name was still used when plotting a series with``use\_index=False``(:issue:`8558`). - Bugs when trying to stack multiple columns, when some (or all) of the level names are numbers (:issue:`8584`). - Bug in``MultiIndex`where`\_\_contains\_\_``returns wrong result if index is not lexically sorted or unique (:issue:`7724`) - BUG CSV: fix problem with trailing white space in skipped rows, (:issue:`8679`), (:issue:`8661`), (:issue:`8983`) - Regression in``Timestamp``does not parse 'Z' zone designator for UTC (:issue:`8771`) - Bug in``StataWriter\`<span class="title-ref"> the produces writes strings with 244 characters irrespective of actual size (:issue:\`8969</span>) - Fixed ValueError raised by cummin/cummax when datetime64 Series contains NaT. (`8965`) - Bug in DataReader returns object dtype if there are missing values (`8980`) - Bug in plotting if sharex was enabled and index was a timeseries, would show labels on multiple axes (`3964`). - Bug where passing a unit to the TimedeltaIndex constructor applied the to nano-second conversion twice. (`9011`). - Bug in plotting of a period-like array (`9012`)

## Contributors

<div class="contributors">

v0.15.1..v0.15.2

</div>

---

v0.16.0.md

---

# Version 0.16.0 (March 22, 2015)

{{ header }}

This is a major release from 0.15.2 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

Highlights include:

  - `DataFrame.assign` method, see \[here \<whatsnew\_0160.enhancements.assign\>\](\#here-\<whatsnew\_0160.enhancements.assign\>)
  - `Series.to_coo/from_coo` methods to interact with `scipy.sparse`, see \[here \<whatsnew\_0160.enhancements.sparse\>\](\#here-\<whatsnew\_0160.enhancements.sparse\>)
  - Backwards incompatible change to `Timedelta` to conform the `.seconds` attribute with `datetime.timedelta`, see \[here \<whatsnew\_0160.api\_breaking.timedelta\>\](\#here-\<whatsnew\_0160.api\_breaking.timedelta\>)
  - Changes to the `.loc` slicing API to conform with the behavior of `.ix` see \[here \<whatsnew\_0160.api\_breaking.indexing\>\](\#here-\<whatsnew\_0160.api\_breaking.indexing\>)
  - Changes to the default for ordering in the `Categorical` constructor, see \[here \<whatsnew\_0160.api\_breaking.categorical\>\](\#here-\<whatsnew\_0160.api\_breaking.categorical\>)
  - Enhancement to the `.str` accessor to make string operations easier, see \[here \<whatsnew\_0160.enhancements.string\>\](\#here-\<whatsnew\_0160.enhancements.string\>)
  - The `pandas.tools.rplot`, `pandas.sandbox.qtpandas` and `pandas.rpy` modules are deprecated. We refer users to external packages like [seaborn](http://stanford.edu/~mwaskom/software/seaborn/), [pandas-qt](https://github.com/datalyze-solutions/pandas-qt) and [rpy2](http://rpy2.bitbucket.org/) for similar or equivalent functionality, see \[here \<whatsnew\_0160.deprecations\>\](\#here-\<whatsnew\_0160.deprecations\>)

Check the \[API Changes \<whatsnew\_0160.api\>\](\#api-changes-\<whatsnew\_0160.api\>) and \[deprecations \<whatsnew\_0160.deprecations\>\](\#deprecations-\<whatsnew\_0160.deprecations\>) before updating.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.16.0

</div>

## New features

### DataFrame assign

Inspired by [dplyr's](https://dplyr.tidyverse.org/articles/dplyr.html#mutating-operations) `mutate` verb, DataFrame has a new <span class="title-ref">\~pandas.DataFrame.assign</span> method. The function signature for `assign` is simply `**kwargs`. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a `Series` or NumPy array), or a function of one argument to be called on the `DataFrame`. The new values are inserted, and the entire DataFrame (with all original and new columns) is returned.

<div class="ipython">

python

iris = pd.read\_csv('data/iris.data') iris.head()

iris.assign(sepal\_ratio=iris\['SepalWidth'\] / iris\['SepalLength'\]).head()

</div>

Above was an example of inserting a precomputed value. We can also pass in a function to be evaluated.

<div class="ipython">

python

  - iris.assign(sepal\_ratio=lambda x: (x\['SepalWidth'\]  
    / x\['SepalLength'\])).head()

</div>

The power of `assign` comes when used in chains of operations. For example, we can limit the DataFrame to just those with a Sepal Length greater than 5, calculate the ratio, and plot

<div class="ipython">

python

iris = pd.read\_csv('data/iris.data') (iris.query('SepalLength \> 5') .assign(SepalRatio=lambda x: x.SepalWidth / x.SepalLength, PetalRatio=lambda x: x.PetalWidth / x.PetalLength) .plot(kind='scatter', x='SepalRatio', y='PetalRatio'))

</div>

![image](../_static/whatsnew_assign.png)

See the \[documentation \<dsintro.chained\_assignment\>\](\#documentation-\<dsintro.chained\_assignment\>) for more. (`9229`)

### Interaction with scipy.sparse

Added <span class="title-ref">SparseSeries.to\_coo</span> and <span class="title-ref">SparseSeries.from\_coo</span> methods (`8048`) for converting to and from `scipy.sparse.coo_matrix` instances (see \[here \<sparse.scipysparse\>\](\#here-\<sparse.scipysparse\>)). For example, given a SparseSeries with MultiIndex we can convert to a `scipy.sparse.coo_matrix` by specifying the row and column labels as index levels:

`` `python    s = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan])    s.index = pd.MultiIndex.from_tuples([(1, 2, 'a', 0),                                         (1, 2, 'a', 1),                                         (1, 1, 'b', 0),                                         (1, 1, 'b', 1),                                         (2, 1, 'b', 0),                                         (2, 1, 'b', 1)],                                        names=['A', 'B', 'C', 'D'])     s     # SparseSeries    ss = s.to_sparse()    ss     A, rows, columns = ss.to_coo(row_levels=['A', 'B'],                                 column_levels=['C', 'D'],                                 sort_labels=False)     A    A.todense()    rows    columns  The from_coo method is a convenience method for creating a ``SparseSeries`  `<span class="title-ref"> from a </span><span class="title-ref">scipy.sparse.coo\_matrix</span>\`:

`` `python    from scipy import sparse    A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])),                          shape=(3, 4))    A    A.todense()     ss = pd.SparseSeries.from_coo(A)    ss  .. _whatsnew_0160.enhancements.string:  String methods enhancements ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^

  - Following new methods are accessible via `.str` accessor to apply the function to each values. This is intended to make it more consistent with standard methods on strings. (`9282`, `9352`, `9386`, `9387`, `9439`)
    
    | ..          | ..          | Methods     | ..            | ..            |
    | ----------- | ----------- | ----------- | ------------- | ------------- |
    | `isalnum()` | `isalpha()` | `isdigit()` | `isdigit()`   | `isspace()`   |
    | `islower()` | `isupper()` | `istitle()` | `isnumeric()` | `isdecimal()` |
    | `find()`    | `rfind()`   | `ljust()`   | `rjust()`     | `zfill()`     |

    <div class="ipython">
    
    python
    
    s = pd.Series(\['abcd', '3456', 'EFGH'\]) s.str.isalpha() s.str.find('ab')
    
    </div>

  - <span class="title-ref">Series.str.pad</span> and <span class="title-ref">Series.str.center</span> now accept `fillchar` option to specify filling character (`9352`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\['12', '300', '25'\]) s.str.pad(5, fillchar='\_')
    
    </div>

  - Added <span class="title-ref">Series.str.slice\_replace</span>, which previously raised `NotImplementedError` (`8888`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\['ABCD', 'EFGH', 'IJK'\]) s.str.slice\_replace(1, 3, 'X') \# replaced with empty char s.str.slice\_replace(0, 1)
    
    </div>

### Other enhancements

  - Reindex now supports `method='nearest'` for frames or series with a monotonic increasing or decreasing index (`9258`):
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame({'x': range(5)}) df.reindex(\[0.2, 1.8, 3.5\], method='nearest')
    
    </div>
    
    This method is also exposed by the lower level `Index.get_indexer` and `Index.get_loc` methods.

  - The `read_excel()` function's \[sheetname \<io.excel.specifying\_sheets\>\](\#sheetname-\<io.excel.specifying\_sheets\>) argument now accepts a list and `None`, to get multiple or all sheets respectively. If more than one sheet is specified, a dictionary is returned. (`9450`)
    
      - \`\`\`python  
        \# Returns the 1st and 4th sheet, as a dictionary of DataFrames. pd.read\_excel('path\_to\_file.xls', sheetname=\['Sheet1', 3\])

\- Allow Stata files to be read incrementally with an iterator; support for long strings in Stata files. See the docs \[here\<io.stata\_reader\>\](\#here\<io.stata\_reader\>) (`9493`:). `` ` - Paths beginning with ~ will now be expanded to begin with the user's home directory (:issue:`9066`) - Added time interval selection in ``get\_data\_yahoo``(:issue:`9071`) - Added``Timestamp.to\_datetime64()`to complement`Timedelta.to\_timedelta64()``(:issue:`9255`) -``tseries.frequencies.to\_offset()`now accepts`Timedelta``as input (:issue:`9064`) - Lag parameter was added to the autocorrelation method of``Series``, defaults to lag-1 autocorrelation (:issue:`9192`) -``Timedelta`will now accept`nanoseconds``keyword in constructor (:issue:`9273`) - SQL code now safely escapes table and column names (:issue:`8986`) - Added auto-complete for``Series.str.\<tab\>`,`Series.dt.\<tab\>`and`Series.cat.\<tab\>``(:issue:`9322`) -``Index.get\_indexer`now supports`method='pad'`and`method='backfill'``even for any target array, not just monotonic targets. These methods also work for monotonic decreasing as well as monotonic increasing indexes (:issue:`9258`). -``Index.asof``now works on all index types (:issue:`9258`). - A``verbose`argument has been augmented in`io.read\_excel()``, defaults to False. Set to True to print sheet names as they are parsed. (:issue:`9450`) - Added``days\_in\_month`(compatibility alias`daysinmonth`) property to`Timestamp`,`DatetimeIndex`,`Period`,`PeriodIndex`, and`Series.dt``(:issue:`9572`) - Added``decimal`option in`to\_csv``to provide formatting for non-'.' decimal separators (:issue:`781`) - Added``normalize`option for`Timestamp``to normalized to midnight (:issue:`8794`) - Added example for``DataFrame`import to R using HDF5 file and`rhdf5``library. See the documentation for more   (:issue:`9636`).  .. _whatsnew_0160.api:  Backwards incompatible API changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. _whatsnew_0160.api_breaking:  .. _whatsnew_0160.api_breaking.timedelta:  Changes in timedelta ^^^^^^^^^^^^^^^^^^^^  In v0.15.0 a new scalar type``Timedelta`was introduced, that is a sub-class of`datetime.timedelta`. Mentioned [here <whatsnew_0150.timedeltaindex>](#here-<whatsnew_0150.timedeltaindex>) was a notice of an API change w.r.t. the`.seconds`accessor. The intent was to provide a user-friendly set of accessors that give the 'natural' value for that unit, e.g. if you had a`Timedelta('1 day, 10:11:12')`, then`.seconds`would return 12. However, this is at odds with the definition of`datetime.timedelta`, which defines`.seconds`as`10 \* 3600 + 11 \* 60 + 12 == 36672`.  So in v0.16.0, we are restoring the API to match that of`datetime.timedelta`. Further, the component values are still available through the`.components`accessor. This affects the`.seconds`and`.microseconds`accessors, and removes the`.hours`,`.minutes`,`.milliseconds`accessors. These changes affect`TimedeltaIndex`and the Series`.dt``accessor as well. (:issue:`9185`, :issue:`9139`)  Previous behavior``\`ipython In \[2\]: t = pd.Timedelta('1 day, 10:11:12.100123')

> In \[3\]: t.days Out\[3\]: 1
> 
> In \[4\]: t.seconds Out\[4\]: 12
> 
> In \[5\]: t.microseconds Out\[5\]: 123

New behavior

<div class="ipython">

python

t = pd.Timedelta('1 day, 10:11:12.100123') t.days t.seconds t.microseconds

</div>

Using `.components` allows the full component access

<div class="ipython">

python

t.components t.components.seconds

</div>

<div id="whatsnew_0160.api_breaking.indexing">

Indexing changes `` ` ^^^^^^^^^^^^^^^^  The behavior of a small sub-set of edge cases for using ``.loc``have changed (:issue:`8613`). Furthermore we have improved the content of the error messages that are raised:  - Slicing with``.loc`where the start and/or stop bound is not found in the index is now allowed; this previously would raise a`KeyError`. This makes the behavior the same as`.ix`in this case. This change is only for slicing, not when indexing with a single label.    .. ipython:: python       df = pd.DataFrame(np.random.randn(5, 4),                        columns=list('ABCD'),                        index=pd.date_range('20130101', periods=5))      df      s = pd.Series(range(5), [-2, -1, 1, 2, 3])      s    Previous behavior`\`ipython In \[4\]: df.loc\['2013-01-02':'2013-01-10'\] KeyError: 'stop bound \[2013-01-10\] is not in the \[index\]'

</div>

> In \[6\]: s.loc\[-10:3\] KeyError: 'start bound \[-10\] is not the \[index\]'
> 
> New behavior
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> df.loc\['2013-01-02':'2013-01-10'\] s.loc\[-10:3\]

  - Allow slicing with float-like values on an integer index for `.ix`. Previously this was only enabled for `.loc`:
    
    Previous behavior
    
    ``` ipython
    In [8]: s.ix[-1.0:2]
    TypeError: the slice start value [-1.0] is not a proper indexer for this index type (Int64Index)
    ```
    
    New behavior
    
    ``` python
    In [2]: s.ix[-1.0:2]
    Out[2]:
    -1    1
     1    2
     2    3
    dtype: int64
    ```

  - Provide a useful exception for indexing with an invalid type for that index when using `.loc`. For example trying to use `.loc` on an index of type `DatetimeIndex` or `PeriodIndex` or `TimedeltaIndex`, with an integer (or a float).
    
    Previous behavior
    
    ``` python
    In [4]: df.loc[2:3]
    KeyError: 'start bound [2] is not the [index]'
    ```
    
    New behavior
    
    ``` ipython
    In [4]: df.loc[2:3]
    TypeError: Cannot do slice indexing on <class 'pandas.tseries.index.DatetimeIndex'> with <type 'int'> keys
    ```

<div id="whatsnew_0160.api_breaking.categorical">

Categorical changes `` ` ^^^^^^^^^^^^^^^^^^^  In prior versions, ``Categoricals`that had an unspecified ordering (meaning no`ordered`keyword was passed) were defaulted as`ordered`Categoricals. Going forward, the`ordered`keyword in the`Categorical`constructor will default to`False`. Ordering must now be explicit.  Furthermore, previously you *could* change the`ordered`attribute of a Categorical by just setting the attribute, e.g.`cat.ordered=True`; This is now deprecated and you should use`cat.as\_ordered()`or`cat.as\_unordered()``. These will by default return a **new** object and not modify the existing object. (:issue:`9347`, :issue:`9190`)  Previous behavior``\`ipython In \[3\]: s = pd.Series(\[0, 1, 2\], dtype='category')

</div>

> In \[4\]: s Out\[4\]: 0 0 1 1 2 2 dtype: category Categories (3, int64): \[0 \< 1 \< 2\]
> 
> In \[5\]: s.cat.ordered Out\[5\]: True
> 
> In \[6\]: s.cat.ordered = False
> 
> In \[7\]: s Out\[7\]: 0 0 1 1 2 2 dtype: category Categories (3, int64): \[0, 1, 2\]

New behavior

<div class="ipython">

python

s = pd.Series(\[0, 1, 2\], dtype='category') s s.cat.ordered s = s.cat.as\_ordered() s s.cat.ordered

\# you can set in the constructor of the Categorical s = pd.Series(pd.Categorical(\[0, 1, 2\], ordered=True)) s s.cat.ordered

</div>

For ease of creation of series of categorical data, we have added the ability to pass keywords when calling `.astype()`. These are passed directly to the constructor.

``` python
In [54]: s = pd.Series(["a", "b", "c", "a"]).astype('category', ordered=True)

In [55]: s
Out[55]:
0    a
1    b
2    c
3    a
dtype: category
Categories (3, object): [a < b < c]

In [56]: s = (pd.Series(["a", "b", "c", "a"])
   ....:        .astype('category', categories=list('abcdef'), ordered=False))

In [57]: s
Out[57]:
0    a
1    b
2    c
3    a
dtype: category
Categories (6, object): [a, b, c, d, e, f]
```

<div id="whatsnew_0160.api_breaking.other">

Other API changes `` ` ^^^^^^^^^^^^^^^^^  - ``Index.duplicated`now returns`np.array(dtype=bool)`rather than`Index(dtype=object)`containing`bool``values. (:issue:`8875`) -``DataFrame.to\_json``now returns accurate type serialisation for each column for frames of mixed dtype (:issue:`9037`)    Previously data was coerced to a common dtype before serialisation, which for   example resulted in integers being serialised to floats:``\`ipython In \[2\]: pd.DataFrame({'i': \[1,2\], 'f': \[3.0, 4.2\]}).to\_json() Out\[2\]: '{"f":{"0":3.0,"1":4.2},"i":{"0":1.0,"1":2.0}}'

</div>

> Now each column is serialised using its correct dtype:
> 
> ``` ipython
> In [2]:  pd.DataFrame({'i': [1,2], 'f': [3.0, 4.2]}).to_json()
> Out[2]: '{"f":{"0":3.0,"1":4.2},"i":{"0":1,"1":2}}'
> ```

\- `DatetimeIndex`, `PeriodIndex` and `TimedeltaIndex.summary` now output the same format. (`9116`) `` ` - ``TimedeltaIndex.freqstr`now output the same string format as`DatetimeIndex``. (:issue:`9116`)  - Bar and horizontal bar plots no longer add a dashed line along the info axis. The prior style can be achieved with matplotlib's``axhline`or`axvline``methods (:issue:`9088`).  -``Series`accessors`.dt`,`.cat`and`.str`now raise`AttributeError`instead of`TypeError``if the series does not contain the appropriate type of data (:issue:`9617`). This follows Python's built-in exception hierarchy more closely and ensures that tests like``hasattr(s, 'cat')`are consistent on both Python 2 and 3.  -`Series``now supports bitwise operation for integral types (:issue:`9016`). Previously even if the input dtypes were integral, the output dtype was coerced to``bool`.    Previous behavior`\`ipython In \[2\]: pd.Series(\[0, 1, 2, 3\], list('abcd')) | pd.Series(\[4, 4, 4, 4\], list('abcd')) Out\[2\]: a True b True c True d True dtype: bool

> New behavior. If the input dtypes are integral, the output dtype is also integral and the output values are the result of the bitwise operation.
> 
> ``` ipython
> In [2]: pd.Series([0, 1, 2, 3], list('abcd')) | pd.Series([4, 4, 4, 4], list('abcd'))
> Out[2]:
> a    4
> b    5
> c    6
> d    7
> dtype: int64
> ```

  - During division involving a `Series` or `DataFrame`, `0/0` and `0//0` now give `np.nan` instead of `np.inf`. (`9144`, `8445`)
    
    Previous behavior
    
    ``` ipython
    In [2]: p = pd.Series([0, 1])
    
    In [3]: p / 0
    Out[3]:
    0    inf
    1    inf
    dtype: float64
    
    In [4]: p // 0
    Out[4]:
    0    inf
    1    inf
    dtype: float64
    ```
    
    New behavior
    
    <div class="ipython">
    
    python
    
    p = pd.Series(\[0, 1\]) p / 0 p // 0
    
    </div>

\- `Series.values_counts` and `Series.describe` for categorical data will now put `NaN` entries at the end. (`9443`) `` ` - ``Series.describe`for categorical data will now give counts and frequencies of 0, not`NaN``, for unused categories (:issue:`9443`)  - Due to a bug fix, looking up a partial string label with``DatetimeIndex.asof``now includes values that match the string, even if they are after the start of the partial string label (:issue:`9258`).    Old behavior:``\`ipython In \[4\]: pd.to\_datetime(\['2000-01-31', '2000-02-28'\]).asof('2000-02') Out\[4\]: Timestamp('2000-01-31 00:00:00')

> Fixed behavior:
> 
> <div class="ipython">
> 
> python
> 
> pd.to\_datetime(\['2000-01-31', '2000-02-28'\]).asof('2000-02')
> 
> </div>
> 
> To reproduce the old behavior, simply add more precision to the label (e.g., use `2000-02-01` instead of `2000-02`).

<div id="whatsnew_0160.deprecations">

Deprecations `` ` ^^^^^^^^^^^^  - The ``rplot``trellis plotting interface is deprecated and will be removed   in a future version. We refer to external packages like   `seaborn <http://stanford.edu/~mwaskom/software/seaborn/>`_ for similar   but more refined functionality (:issue:`3445`).   The documentation includes some examples how to convert your existing code   from``rplot``to seaborn `here <https://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html#trellis-plotting-interface>`__.  - The``pandas.sandbox.qtpandas``interface is deprecated and will be removed in a future version.   We refer users to the external package `pandas-qt <https://github.com/datalyze-solutions/pandas-qt>`_. (:issue:`9615`)  - The``pandas.rpy``interface is deprecated and will be removed in a future version.   Similar functionality can be accessed through the `rpy2 <http://rpy2.bitbucket.org/>`_ project (:issue:`9602`)  - Adding``DatetimeIndex/PeriodIndex`to another`DatetimeIndex/PeriodIndex`is being deprecated as a set-operation. This will be changed to a`TypeError`in a future version.`.union()``should be used for the union set operation. (:issue:`9094`) - Subtracting``DatetimeIndex/PeriodIndex`from another`DatetimeIndex/PeriodIndex`is being deprecated as a set-operation. This will be changed to an actual numeric subtraction yielding a`TimeDeltaIndex`in a future version.`.difference()``should be used for the differencing set operation. (:issue:`9094`)   .. _whatsnew_0160.prior_deprecations:  Removal of prior version deprecations/changes ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  -``DataFrame.pivot\_table`and`crosstab`'s`rows`and`cols`keyword arguments were removed in favor   of`index`and`columns``(:issue:`6581`) -``DataFrame.to\_excel`and`DataFrame.to\_csv`  `cols`keyword argument was removed in favor of`columns``(:issue:`6581`) - Removed``convert\_dummies`in favor of`get\_dummies``(:issue:`6581`) - Removed``value\_range`in favor of`describe``(:issue:`6581`)  .. _whatsnew_0160.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Fixed a performance regression for``.loc``indexing with an array or list-like (:issue:`9126`:). -``DataFrame.to\_json``30x performance improvement for mixed dtype frames. (:issue:`9037`) - Performance improvements in``MultiIndex.duplicated``by working with labels instead of values (:issue:`9125`) - Improved the speed of``nunique`by calling`unique`instead of`value\_counts``(:issue:`9129`, :issue:`7771`) - Performance improvement of up to 10x in``DataFrame.count`and`DataFrame.dropna``by taking advantage of homogeneous/heterogeneous dtypes appropriately (:issue:`9136`) - Performance improvement of up to 20x in``DataFrame.count`when using a`MultiIndex`and the`level``keyword argument  (:issue:`9163`) - Performance and memory usage improvements in``merge`when key space exceeds`int64``bounds (:issue:`9151`) - Performance improvements in multi-key``groupby``(:issue:`9429`) - Performance improvements in``MultiIndex.sortlevel``(:issue:`9445`) - Performance and memory usage improvements in``DataFrame.duplicated``(:issue:`9398`) - Cythonized``Period``(:issue:`9440`) - Decreased memory usage on``to\_hdf``(:issue:`9648`)  .. _whatsnew_0160.bug_fixes:  Bug fixes ~~~~~~~~~  - Changed``.to\_html``to remove leading/trailing spaces in table body (:issue:`4987`) - Fixed issue using``read\_csv``on s3 with Python 3 (:issue:`9452`) - Fixed compatibility issue in``DatetimeIndex`affecting architectures where`[numpy.int]()`defaults to`numpy.int32``(:issue:`8943`) - Bug in Panel indexing with an object-like (:issue:`9140`) - Bug in the returned``Series.dt.components``index was reset to the default index (:issue:`9247`) - Bug in``Categorical.\_\_getitem\_\_/\_\_setitem\_\_``with listlike input getting incorrect results from indexer coercion (:issue:`9469`) - Bug in partial setting with a DatetimeIndex (:issue:`9478`) - Bug in groupby for integer and datetime64 columns when applying an aggregator that caused the value to be   changed when the number was sufficiently large (:issue:`9311`, :issue:`6620`) - Fixed bug in``to\_sql`when mapping a`Timestamp``object column (datetime   column with timezone info) to the appropriate sqlalchemy type (:issue:`9085`). - Fixed bug in``to\_sql`  `dtype``argument not accepting an instantiated   SQLAlchemy type  (:issue:`9083`). - Bug in``.loc`partial setting with a`np.datetime64``(:issue:`9516`) - Incorrect dtypes inferred on datetimelike looking``Series`& on`.xs``slices (:issue:`9477`) - Items in``Categorical.unique()`(and`s.unique()`if`s`is of dtype`category``) now appear in the order in which they are originally found, not in sorted order (:issue:`9331`). This is now consistent with the behavior for other dtypes in pandas. - Fixed bug on big endian platforms which produced incorrect results in``StataReader``(:issue:`8688`). - Bug in``MultiIndex.has\_duplicates``when having many levels causes an indexer overflow (:issue:`9075`, :issue:`5873`) - Bug in``pivot`and`unstack`where`nan``values would break index alignment (:issue:`4862`, :issue:`7401`, :issue:`7403`, :issue:`7405`, :issue:`7466`, :issue:`9497`) - Bug in left``join`on MultiIndex with`sort=True``or null values (:issue:`9210`). - Bug in``MultiIndex``where inserting new keys would fail (:issue:`9250`). - Bug in``groupby`when key space exceeds`int64``bounds (:issue:`9096`). - Bug in``unstack`with`TimedeltaIndex`or`DatetimeIndex``and nulls (:issue:`9491`). - Bug in``rank``where comparing floats with tolerance will cause inconsistent behaviour (:issue:`8365`). - Fixed character encoding bug in``read\_stata`and`StataReader``when loading data from a URL (:issue:`9231`). - Bug in adding``offsets.Nano`to other offsets raises`TypeError``(:issue:`9284`) - Bug in``DatetimeIndex``iteration, related to (:issue:`8890`), fixed in (:issue:`9100`) - Bugs in``resample``around DST transitions. This required fixing offset classes so they behave correctly on DST transitions. (:issue:`5172`, :issue:`8744`, :issue:`8653`, :issue:`9173`, :issue:`9468`). - Bug in binary operator method (eg``.mul()``) alignment with integer levels (:issue:`9463`). - Bug in boxplot, scatter and hexbin plot may show an unnecessary warning (:issue:`8877`) - Bug in subplot with``layout``kw may show unnecessary warning (:issue:`9464`) - Bug in using grouper functions that need passed through arguments (e.g. axis), when using wrapped function (e.g.``fillna``), (:issue:`9221`) -``DataFrame`now properly supports simultaneous`copy`and`dtype``arguments in constructor (:issue:`9099`) - Bug in``read\_csv``when using skiprows on a file with CR line endings with the c engine. (:issue:`9079`) -``isnull`now detects`NaT`in`PeriodIndex``(:issue:`9129`) - Bug in groupby``.nth()``with a multiple column groupby (:issue:`8979`) - Bug in``DataFrame.where`and`Series.where``coerce numerics to string incorrectly (:issue:`9280`) - Bug in``DataFrame.where`and`Series.where`raise`ValueError``when string list-like is passed. (:issue:`9280`) - Accessing``Series.str`methods on with non-string values now raises`TypeError``instead of producing incorrect results (:issue:`9184`) - Bug in``DatetimeIndex.\_\_contains\_\_``when index has duplicates and is not monotonic increasing (:issue:`9512`) - Fixed division by zero error for``Series.kurt()``when all values are equal (:issue:`9197`) - Fixed issue in the``xlsxwriter``engine where it added a default 'General' format to cells if no other format was applied. This prevented other row or column formatting being applied. (:issue:`9167`) - Fixes issue with``index\_col=False`when`usecols`is also specified in`read\_csv``. (:issue:`9082`) - Bug where``wide\_to\_long``would modify the input stub names list (:issue:`9204`) - Bug in``to\_sql``not storing float64 values using double precision. (:issue:`9009`) -``SparseSeries`and`SparsePanel``now accept zero argument constructors (same as their non-sparse counterparts) (:issue:`9272`). - Regression in merging``Categorical`and`object``dtypes (:issue:`9426`) - Bug in``read\_csv``with buffer overflows with certain malformed input files (:issue:`9205`) - Bug in groupby MultiIndex with missing pair (:issue:`9049`, :issue:`9344`) - Fixed bug in``Series.groupby`where grouping on`MultiIndex``levels would ignore the sort argument (:issue:`9444`) - Fix bug in``DataFrame.Groupby`where`sort=False``is ignored in the case of Categorical columns. (:issue:`8868`) - Fixed bug with reading CSV files from Amazon S3 on python 3 raising a TypeError (:issue:`9452`) - Bug in the Google BigQuery reader where the 'jobComplete' key may be present but False in the query results (:issue:`8728`) - Bug in``Series.values\_counts`with excluding`NaN`for categorical type`Series`with`dropna=True``(:issue:`9443`) - Fixed missing numeric_only option for``DataFrame.std/var/sem``(:issue:`9201`) - Support constructing``Panel`or`Panel4D``with scalar data (:issue:`8285`) -``Series`text representation disconnected from`max\_rows`/`max\_columns``(:issue:`7508`).  \  -``Series``number formatting inconsistent when truncated (:issue:`8532`).    Previous behavior``\`python In \[2\]: pd.options.display.max\_rows = 10 In \[3\]: s = pd.Series(\[1,1,1,1,1,1,1,1,1,1,0.9999,1,1\]\*10) In \[4\]: s Out\[4\]: 0 1 1 1 2 1 ... 127 0.9999 128 1.0000 129 1.0000 Length: 130, dtype: float64

</div>

> New behavior
> 
> ``` python
> 0      1.0000
> 1      1.0000
> 2      1.0000
> 3      1.0000
> 4      1.0000
> ...
> 125    1.0000
> 126    1.0000
> 127    0.9999
> 128    1.0000
> 129    1.0000
> dtype: float64
> ```

  - A Spurious `SettingWithCopy` Warning was generated when setting a new item in a frame in some cases (`8730`)
    
    The following would previously report a `SettingWithCopy` Warning.
    
    <div class="ipython">
    
    python
    
      - df1 = pd.DataFrame({'x': pd.Series(\['a', 'b', 'c'\]),  
        'y': pd.Series(\['d', 'e', 'f'\])})
    
    df2 = df1\[\['x'\]\] df2\['y'\] = \['g', 'h', 'i'\]
    
    </div>

<div id="whatsnew_0.16.0.contributors">

Contributors \`\`\` \~\~\~\~\~\~\~\~\~\~\~\~

</div>

<div class="contributors">

v0.15.2..v0.16.0

</div>

---

v0.16.1.md

---

# Version 0.16.1 (May 11, 2015)

{{ header }}

This is a minor bug-fix release from 0.16.0 and includes a large number of bug fixes along several new features, enhancements, and performance improvements. We recommend that all users upgrade to this version.

Highlights include:

  - Support for a `CategoricalIndex`, a category based index, see \[here \<whatsnew\_0161.enhancements.categoricalindex\>\](\#here-\<whatsnew\_0161.enhancements.categoricalindex\>)
  - New section on how-to-contribute to *pandas*, see \[here \<contributing\>\](\#here-\<contributing\>)
  - Revised "Merge, join, and concatenate" documentation, including graphical examples to make it easier to understand each operations, see \[here \<merging\>\](\#here-\<merging\>)
  - New method `sample` for drawing random samples from Series, DataFrames and Panels. See \[here \<whatsnew\_0161.enhancements.sample\>\](\#here-\<whatsnew\_0161.enhancements.sample\>)
  - The default `Index` printing has changed to a more uniform format, see \[here \<whatsnew\_0161.index\_repr\>\](\#here-\<whatsnew\_0161.index\_repr\>)
  - `BusinessHour` datetime-offset is now supported, see \[here \<timeseries.businesshour\>\](\#here-\<timeseries.businesshour\>)
  - Further enhancement to the `.str` accessor to make string operations easier, see \[here \<whatsnew\_0161.enhancements.string\>\](\#here-\<whatsnew\_0161.enhancements.string\>)

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.16.1

</div>

<div id="whatsnew_0161.enhancements">

\> **Warning** \> In pandas 0.17.0, the sub-package `pandas.io.data` will be removed in favor of a separately installable package (`8961`).

</div>

## Enhancements

### CategoricalIndex

We introduce a `CategoricalIndex`, a new type of index object that is useful for supporting indexing with duplicates. This is a container around a `Categorical` (introduced in v0.15.0) and allows efficient indexing and storage of an index with a large number of duplicated elements. Prior to 0.16.1, setting the index of a `DataFrame/Series` with a `category` dtype would convert this to regular object-based `Index`.

`` `ipython     In [1]: df = pd.DataFrame({'A': np.arange(6),        ...:                    'B': pd.Series(list('aabbca'))        ...:                           .astype('category', categories=list('cab'))        ...:                    })        ...:      In [2]: df     Out[2]:        A  B     0  0  a     1  1  a     2  2  b     3  3  b     4  4  c     5  5  a      In [3]: df.dtypes     Out[3]:     A       int64     B    category     dtype: object      In [4]: df.B.cat.categories     Out[4]: Index(['c', 'a', 'b'], dtype='object')   setting the index, will create a ``CategoricalIndex`.. code-block:: ipython      In [5]: df2 = df.set_index('B')      In [6]: df2.index     Out[6]: CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'a'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category')  indexing with`\_\_getitem\_\_/.iloc/.loc/.ix`works similarly to an Index with duplicates.`\` The indexers MUST be in the category or the operation will raise.

`` `ipython     In [7]: df2.loc['a']     Out[7]:        A     B     a  0     a  1     a  5  and preserves the ``CategoricalIndex`.. code-block:: ipython      In [8]: df2.loc['a'].index     Out[8]: CategoricalIndex(['a', 'a', 'a'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category')   sorting will order by the order of the categories  .. code-block:: ipython      In [9]: df2.sort_index()     Out[9]:        A     B     c  4     a  0     a  1     a  5     b  2     b  3  groupby operations on the index will preserve the index nature as well  .. code-block:: ipython      In [10]: df2.groupby(level=0).sum()     Out[10]:        A     B     c  4     a  6     b  5      In [11]: df2.groupby(level=0).sum().index     Out[11]: CategoricalIndex(['c', 'a', 'b'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category')   reindexing operations, will return a resulting index based on the type of the passed`<span class="title-ref"> indexer, meaning that passing a list will return a plain-old-</span><span class="title-ref">Index</span><span class="title-ref">; indexing with a </span><span class="title-ref">Categorical</span><span class="title-ref"> will return a </span><span class="title-ref">CategoricalIndex</span><span class="title-ref">, indexed according to the categories of the PASSED </span><span class="title-ref">Categorical</span>\` dtype. This allows one to arbitrarily index these even with values NOT in the categories, similarly to how you can reindex ANY pandas index.

`` `ipython     In [12]: df2.reindex(['a', 'e'])     Out[12]:          A     B     a  0.0     a  1.0     a  5.0     e  NaN      In [13]: df2.reindex(['a', 'e']).index     Out[13]: pd.Index(['a', 'a', 'a', 'e'], dtype='object', name='B')      In [14]: df2.reindex(pd.Categorical(['a', 'e'], categories=list('abcde')))     Out[14]:          A     B     a  0.0     a  1.0     a  5.0     e  NaN      In [15]: df2.reindex(pd.Categorical(['a', 'e'], categories=list('abcde'))).index     Out[15]: pd.CategoricalIndex(['a', 'a', 'a', 'e'],                                  categories=['a', 'b', 'c', 'd', 'e'],                                  ordered=False, name='B',                                  dtype='category')  See the [documentation <advanced.categoricalindex>](#documentation-<advanced.categoricalindex>) for more. (:issue:`7629`, :issue:`10038`, :issue:`10039`)  .. _whatsnew_0161.enhancements.sample:  Sample ``\` ^^^^^^

Series, DataFrames, and Panels now have a new method: <span class="title-ref">\~pandas.DataFrame.sample</span>. The method accepts a specific number of rows or columns to return, or a fraction of the total number or rows or columns. It also has options for sampling with or without replacement, for passing in a column for weights for non-uniform sampling, and for setting seed values to facilitate replication. (`2419`)

<div class="ipython">

python

example\_series = pd.Series(\[0, 1, 2, 3, 4, 5\])

\# When no arguments are passed, returns 1 example\_series.sample()

\# One may specify either a number of rows: example\_series.sample(n=3)

\# Or a fraction of the rows: example\_series.sample(frac=0.5)

\# weights are accepted. example\_weights = \[0, 0, 0.2, 0.2, 0.2, 0.4\] example\_series.sample(n=3, weights=example\_weights)

\# weights will also be normalized if they do not sum to one, \# and missing values will be treated as zeros. example\_weights2 = \[0.5, 0, 0, 0, None, np.nan\] example\_series.sample(n=1, weights=example\_weights2)

</div>

When applied to a DataFrame, one may pass the name of a column to specify sampling weights when sampling from rows.

<div class="ipython">

python

df = pd.DataFrame({"col1": \[9, 8, 7, 6\], "weight\_column": \[0.5, 0.4, 0.1, 0\]}) df.sample(n=3, weights="weight\_column")

</div>

### String methods enhancements

\[Continuing from v0.16.0 \<whatsnew\_0160.enhancements.string\>\](\#continuing-from-v0.16.0-\<whatsnew\_0160.enhancements.string\>), the following enhancements make string operations easier and more consistent with standard python string operations.

  - Added `StringMethods` (`.str` accessor) to `Index` (`9068`)
    
    The `.str` accessor is now available for both `Series` and `Index`.
    
    <div class="ipython">
    
    python
    
    idx = pd.Index(\[" jack", "jill ", " jesse ", "frank"\]) idx.str.strip()
    
    </div>
    
    One special case for the `.str` accessor on `Index` is that if a string method returns `bool`, the `.str` accessor will return a `np.array` instead of a boolean `Index` (`8875`). This enables the following expression to work naturally:
    
    <div class="ipython">
    
    python
    
    idx = pd.Index(\["a1", "a2", "b1", "b2"\]) s = pd.Series(range(4), index=idx) s idx.str.startswith("a") s\[s.index.str.startswith("a")\]
    
    </div>

  - The following new methods are accessible via `.str` accessor to apply the function to each values. (`9766`, `9773`, `10031`, `10045`, `10052`)
    
    | ..                       | ..                      | Methods                     | ..            | ..             |
    | ------------------------ | ----------------------- | --------------------------- | ------------- | -------------- |
    | `capitalize()` `index()` | `swapcase()` `rindex()` | `normalize()` `translate()` | `partition()` | `rpartition()` |

  - `split` now takes `expand` keyword to specify whether to expand dimensionality. `return_type` is deprecated. (`9847`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\["a,b", "a,c", "b,c"\])
    
    \# return Series s.str.split(",")
    
    \# return DataFrame s.str.split(",", expand=True)
    
    idx = pd.Index(\["a,b", "a,c", "b,c"\])
    
    \# return Index idx.str.split(",")
    
    \# return MultiIndex idx.str.split(",", expand=True)
    
    </div>

  - Improved `extract` and `get_dummies` methods for `Index.str` (`9980`)

### Other enhancements

  - `BusinessHour` offset is now supported, which represents business hours starting from 09:00 - 17:00 on `BusinessDay` by default. See \[Here \<timeseries.businesshour\>\](\#here-\<timeseries.businesshour\>) for details. (`7905`)
    
    <div class="ipython">
    
    python
    
    pd.Timestamp("2014-08-01 09:00") + pd.tseries.offsets.BusinessHour() pd.Timestamp("2014-08-01 07:00") + pd.tseries.offsets.BusinessHour() pd.Timestamp("2014-08-01 16:30") + pd.tseries.offsets.BusinessHour()
    
    </div>

  - `DataFrame.diff` now takes an `axis` parameter that determines the direction of differencing (`9727`)

  - Allow `clip`, `clip_lower`, and `clip_upper` to accept array-like arguments as thresholds (This is a regression from 0.11.0). These methods now have an `axis` parameter which determines how the Series or DataFrame will be aligned with the threshold(s). (`6966`)

  - `DataFrame.mask()` and `Series.mask()` now support same keywords as `where` (`8801`)

  - `drop` function can now accept `errors` keyword to suppress `ValueError` raised when any of label does not exist in the target data. (`6736`)
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame(np.random.randn(3, 3), columns=\["A", "B", "C"\]) df.drop(\["A", "X"\], axis=1, errors="ignore")
    
    </div>

  - Add support for separating years and quarters using dashes, for example 2014-Q1. (`9688`)

  - Allow conversion of values with dtype `datetime64` or `timedelta64` to strings using `astype(str)` (`9757`)

  - `get_dummies` function now accepts `sparse` keyword. If set to `True`, the return `DataFrame` is sparse, e.g. `SparseDataFrame`. (`8823`)

  - `Period` now accepts `datetime64` as value input. (`9054`)

  - Allow timedelta string conversion when leading zero is missing from time definition, ie `0:00:00` vs `00:00:00`. (`9570`)

  - Allow `Panel.shift` with `axis='items'` (`9890`)

  - Trying to write an excel file now raises `NotImplementedError` if the `DataFrame` has a `MultiIndex` instead of writing a broken Excel file. (`9794`)

  - Allow `Categorical.add_categories` to accept `Series` or `np.array`. (`9927`)

  - Add/delete `str/dt/cat` accessors dynamically from `__dir__`. (`9910`)

  - Add `normalize` as a `dt` accessor method. (`10047`)

  - `DataFrame` and `Series` now have `_constructor_expanddim` property as overridable constructor for one higher dimensionality data. This should be used only when it is really needed, see \[here \<extending.subclassing-pandas\>\](\#here-\<extending.subclassing-pandas\>)

  - `pd.lib.infer_dtype` now returns `'bytes'` in Python 3 where appropriate. (`10032`)

## API changes

  - When passing in an ax to `df.plot( ..., ax=ax)`, the `sharex` kwarg will now default to `False`. The result is that the visibility of xlabels and xticklabels will not anymore be changed. You have to do that by yourself for the right axes in your figure or set `sharex=True` explicitly (but this changes the visible for all axes in the figure, not only the one which is passed in\!). If pandas creates the subplots itself (e.g. no passed in `ax` kwarg), then the default is still `sharex=True` and the visibility changes are applied.
  - <span class="title-ref">\~pandas.DataFrame.assign</span> now inserts new columns in alphabetical order. Previously the order was arbitrary. (`9777`)
  - By default, `read_csv` and `read_table` will now try to infer the compression type based on the file extension. Set `compression=None` to restore the previous behavior (no decompression). (`9770`)

### Deprecations

  - `Series.str.split`'s `return_type` keyword was removed in favor of `expand` (`9847`)

## Index representation

The string representation of `Index` and its sub-classes have now been unified. These will show a single-line display if there are few values; a wrapped multi-line display for a lot of values (but less than `display.max_seq_items`; if lots of items (\> `display.max_seq_items`) will show a truncated display (the head and tail of the data). The formatting for `MultiIndex` is unchanged (a multi-line wrapped display). The display width responds to the option `display.max_seq_items`, which is defaulted to 100. (`6482`)

Previous behavior

`` `ipython    In [2]: pd.Index(range(4), name='foo')    Out[2]: Int64Index([0, 1, 2, 3], dtype='int64')     In [3]: pd.Index(range(104), name='foo')    Out[3]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...], dtype='int64')     In [4]: pd.date_range('20130101', periods=4, name='foo', tz='US/Eastern')    Out[4]:    <class 'pandas.tseries.index.DatetimeIndex'>    [2013-01-01 00:00:00-05:00, ..., 2013-01-04 00:00:00-05:00]    Length: 4, Freq: D, Timezone: US/Eastern     In [5]: pd.date_range('20130101', periods=104, name='foo', tz='US/Eastern')    Out[5]:    <class 'pandas.tseries.index.DatetimeIndex'>    [2013-01-01 00:00:00-05:00, ..., 2013-04-14 00:00:00-04:00]    Length: 104, Freq: D, Timezone: US/Eastern  New behavior  .. ipython:: python     pd.set_option("display.width", 80)    pd.Index(range(4), name="foo")    pd.Index(range(30), name="foo")    pd.Index(range(104), name="foo")    pd.CategoricalIndex(["a", "bb", "ccc", "dddd"], ordered=True, name="foobar")    pd.CategoricalIndex(["a", "bb", "ccc", "dddd"] * 10, ordered=True, name="foobar")    pd.CategoricalIndex(["a", "bb", "ccc", "dddd"] * 100, ordered=True, name="foobar")    pd.date_range("20130101", periods=4, name="foo", tz="US/Eastern")    pd.date_range("20130101", periods=25, freq="D")    pd.date_range("20130101", periods=104, name="foo", tz="US/Eastern")   .. _whatsnew_0161.performance:  Performance improvements ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

  - Improved csv write performance with mixed dtypes, including datetimes by up to 5x (`9940`)
  - Improved csv write performance generally by 2x (`9940`)
  - Improved the performance of `pd.lib.max_len_string_array` by 5-7x (`10024`)

## Bug fixes

  - Bug where labels did not appear properly in the legend of `DataFrame.plot()`, passing `label=` arguments works, and Series indices are no longer mutated. (`9542`)
  - Bug in json serialization causing a segfault when a frame had zero length. (`9805`)
  - Bug in `read_csv` where missing trailing delimiters would cause segfault. (`5664`)
  - Bug in retaining index name on appending (`9862`)
  - Bug in `scatter_matrix` draws unexpected axis ticklabels (`5662`)
  - Fixed bug in `StataWriter` resulting in changes to input `DataFrame` upon save (`9795`).
  - Bug in `transform` causing length mismatch when null entries were present and a fast aggregator was being used (`9697`)
  - Bug in `equals` causing false negatives when block order differed (`9330`)
  - Bug in grouping with multiple `pd.Grouper` where one is non-time based (`10063`)
  - Bug in `read_sql_table` error when reading postgres table with timezone (`7139`)
  - Bug in `DataFrame` slicing may not retain metadata (`9776`)
  - Bug where `TimdeltaIndex` were not properly serialized in fixed `HDFStore` (`9635`)
  - Bug with `TimedeltaIndex` constructor ignoring `name` when given another `TimedeltaIndex` as data (`10025`).
  - Bug in `DataFrameFormatter._get_formatted_index` with not applying `max_colwidth` to the `DataFrame` index (`7856`)
  - Bug in `.loc` with a read-only ndarray data source (`10043`)
  - Bug in `groupby.apply()` that would raise if a passed user defined function either returned only `None` (for all input). (`9685`)
  - Always use temporary files in pytables tests (`9992`)
  - Bug in plotting continuously using `secondary_y` may not show legend properly. (`9610`, `9779`)
  - Bug in `DataFrame.plot(kind="hist")` results in `TypeError` when `DataFrame` contains non-numeric columns (`9853`)
  - Bug where repeated plotting of `DataFrame` with a `DatetimeIndex` may raise `TypeError` (`9852`)
  - Bug in `setup.py` that would allow an incompat cython version to build (`9827`)
  - Bug in plotting `secondary_y` incorrectly attaches `right_ax` property to secondary axes specifying itself recursively. (`9861`)
  - Bug in `Series.quantile` on empty Series of type `Datetime` or `Timedelta` (`9675`)
  - Bug in `where` causing incorrect results when upcasting was required (`9731`)
  - Bug in `FloatArrayFormatter` where decision boundary for displaying "small" floats in decimal format is off by one order of magnitude for a given display.precision (`9764`)
  - Fixed bug where `DataFrame.plot()` raised an error when both `color` and `style` keywords were passed and there was no color symbol in the style strings (`9671`)
  - Not showing a `DeprecationWarning` on combining list-likes with an `Index` (`10083`)
  - Bug in `read_csv` and `read_table` when using `skip_rows` parameter if blank lines are present. (`9832`)
  - Bug in `read_csv()` interprets `index_col=True` as `1` (`9798`)
  - Bug in index equality comparisons using `==` failing on Index/MultiIndex type incompatibility (`9785`)
  - Bug in which `SparseDataFrame` could not take `nan` as a column name (`8822`)
  - Bug in `to_msgpack` and `read_msgpack` zlib and blosc compression support (`9783`)
  - Bug `GroupBy.size` doesn't attach index name properly if grouped by `TimeGrouper` (`9925`)
  - Bug causing an exception in slice assignments because `length_of_indexer` returns wrong results (`9995`)
  - Bug in csv parser causing lines with initial white space plus one non-space character to be skipped. (`9710`)
  - Bug in C csv parser causing spurious NaNs when data started with newline followed by white space. (`10022`)
  - Bug causing elements with a null group to spill into the final group when grouping by a `Categorical` (`9603`)
  - Bug where .iloc and .loc behavior is not consistent on empty dataframes (`9964`)
  - Bug in invalid attribute access on a `TimedeltaIndex` incorrectly raised `ValueError` instead of `AttributeError` (`9680`)
  - Bug in unequal comparisons between categorical data and a scalar, which was not in the categories (e.g. `Series(Categorical(list("abc"), ordered=True)) > "d"`. This returned `False` for all elements, but now raises a `TypeError`. Equality comparisons also now return `False` for `==` and `True` for `!=`. (`9848`)
  - Bug in DataFrame `__setitem__` when right hand side is a dictionary (`9874`)
  - Bug in `where` when dtype is `datetime64/timedelta64`, but dtype of other is not (`9804`)
  - Bug in `MultiIndex.sortlevel()` results in unicode level name breaks (`9856`)
  - Bug in which `groupby.transform` incorrectly enforced output dtypes to match input dtypes. (`9807`)
  - Bug in `DataFrame` constructor when `columns` parameter is set, and `data` is an empty list (`9939`)
  - Bug in bar plot with `log=True` raises `TypeError` if all values are less than 1 (`9905`)
  - Bug in horizontal bar plot ignores `log=True` (`9905`)
  - Bug in PyTables queries that did not return proper results using the index (`8265`, `9676`)
  - Bug where dividing a dataframe containing values of type `Decimal` by another `Decimal` would raise. (`9787`)
  - Bug where using DataFrames asfreq would remove the name of the index. (`9885`)
  - Bug causing extra index point when resample BM/BQ (`9756`)
  - Changed caching in `AbstractHolidayCalendar` to be at the instance level rather than at the class level as the latter can result in unexpected behaviour. (`9552`)
  - Fixed latex output for MultiIndexed dataframes (`9778`)
  - Bug causing an exception when setting an empty range using `DataFrame.loc` (`9596`)
  - Bug in hiding ticklabels with subplots and shared axes when adding a new plot to an existing grid of axes (`9158`)
  - Bug in `transform` and `filter` when grouping on a categorical variable (`9921`)
  - Bug in `transform` when groups are equal in number and dtype to the input index (`9700`)
  - Google BigQuery connector now imports dependencies on a per-method basis.(`9713`)
  - Updated BigQuery connector to no longer use deprecated `oauth2client.tools.run()` (`8327`)
  - Bug in subclassed `DataFrame`. It may not return the correct class, when slicing or subsetting it. (`9632`)
  - Bug in `.median()` where non-float null values are not handled correctly (`10040`)
  - Bug in Series.fillna() where it raises if a numerically convertible string is given (`10092`)

## Contributors

<div class="contributors">

v0.16.0..v0.16.1

</div>

---

v0.16.2.md

---

# Version 0.16.2 (June 12, 2015)

{{ header }}

This is a minor bug-fix release from 0.16.1 and includes a large number of bug fixes along some new features (<span class="title-ref">\~DataFrame.pipe</span> method), enhancements, and performance improvements.

We recommend that all users upgrade to this version.

Highlights include:

  - A new `pipe` method, see \[here \<whatsnew\_0162.enhancements.pipe\>\](\#here-\<whatsnew\_0162.enhancements.pipe\>)
  - Documentation on how to use [numba](http://numba.pydata.org) with *pandas*, see \[here \<enhancingperf.numba\>\](\#here-\<enhancingperf.numba\>)

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.16.2

</div>

## New features

### Pipe

We've introduced a new method <span class="title-ref">DataFrame.pipe</span>. As suggested by the name, `pipe` should be used to pipe data through a chain of function calls. The goal is to avoid confusing nested function calls like

`` `python    # df is a DataFrame    # f, g, and h are functions that take and return DataFrames    f(g(h(df), arg1=1), arg2=2, arg3=3)  # noqa F821  The logic flows from inside out, and function names are separated from their keyword arguments. ``\` This can be rewritten as

`` `python    (        df.pipe(h)  # noqa F821        .pipe(g, arg1=1)  # noqa F821        .pipe(f, arg2=2, arg3=3)  # noqa F821    )  Now both the code and the logic flow from top to bottom. Keyword arguments are next to ``\` their functions. Overall the code is much more readable.

In the example above, the functions `f`, `g`, and `h` each expected the DataFrame as the first positional argument. When the function you wish to apply takes its data anywhere other than the first argument, pass a tuple of `(function, keyword)` indicating where the DataFrame should flow. For example:

`` `ipython     In [1]: import statsmodels.formula.api as sm      In [2]: bb = pd.read_csv("data/baseball.csv", index_col="id")      # sm.ols takes (formula, data)     In [3]: (     ...:     bb.query("h > 0")     ...:     .assign(ln_h=lambda df: np.log(df.h))     ...:     .pipe((sm.ols, "data"), "hr ~ ln_h + year + g + C(lg)")     ...:     .fit()     ...:     .summary()     ...: )     ...:     Out[3]:     <class 'statsmodels.iolib.summary.Summary'>     """                                 OLS Regression Results     ==============================================================================     Dep. Variable:                     hr   R-squared:                       0.685     Model:                            OLS   Adj. R-squared:                  0.665     Method:                 Least Squares   F-statistic:                     34.28     Date:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15     Time:                        05:35:23   Log-Likelihood:                -205.92     No. Observations:                  68   AIC:                             421.8     Df Residuals:                      63   BIC:                             432.9     Df Model:                           4     Covariance Type:            nonrobust     ===============================================================================                     coef    std err          t      P>|t|      [0.025      0.975]     -------------------------------------------------------------------------------     Intercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780     C(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375     ln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395     year            4.2277      2.324      1.819      0.074      -0.417       8.872     g               0.1841      0.029      6.258      0.000       0.125       0.243     ==============================================================================     Omnibus:                       10.875   Durbin-Watson:                   1.999     Prob(Omnibus):                  0.004   Jarque-Bera (JB):               17.298     Skew:                           0.537   Prob(JB):                     0.000175     Kurtosis:                       5.225   Cond. No.                     1.49e+07     ==============================================================================      Notes:     [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.     [2] The condition number is large, 1.49e+07. This might indicate that there are     strong multicollinearity or other numerical problems.     """  The pipe method is inspired by unix pipes, which stream text through ``<span class="title-ref"> processes. More recently dplyr\_ and magrittr\_ have introduced the popular </span><span class="title-ref">(%\>%)</span>\` pipe operator for [R](http://www.r-project.org).

See the \[documentation \<basics.pipe\>\](\#documentation-\<basics.pipe\>) for more. (`10129`)

### Other enhancements

  - Added `rsplit` to Index/Series StringMethods (`10303`)

  - Removed the hard-coded size limits on the `DataFrame` HTML representation in the IPython notebook, and leave this to IPython itself (only for IPython v3.0 or greater). This eliminates the duplicate scroll bars that appeared in the notebook with large frames (`10231`).
    
    Note that the notebook has a `toggle output scrolling` feature to limit the display of very large frames (by clicking left of the output). You can also configure the way DataFrames are displayed using the pandas options, see here \[here \<options.frequently\_used\>\](\#here-\<options.frequently\_used\>).

  - `axis` parameter of `DataFrame.quantile` now accepts also `index` and `column`. (`9543`)

## API changes

  - `Holiday` now raises `NotImplementedError` if both `offset` and `observance` are used in the constructor instead of returning an incorrect result (`10217`).

## Performance improvements

  - Improved `Series.resample` performance with `dtype=datetime64[ns]` (`7754`)
  - Increase performance of `str.split` when `expand=True` (`10081`)

## Bug fixes

  - Bug in `Series.hist` raises an error when a one row `Series` was given (`10214`)
  - Bug where `HDFStore.select` modifies the passed columns list (`7212`)
  - Bug in `Categorical` repr with `display.width` of `None` in Python 3 (`10087`)
  - Bug in `to_json` with certain orients and a `CategoricalIndex` would segfault (`10317`)
  - Bug where some of the nan functions do not have consistent return dtypes (`10251`)
  - Bug in `DataFrame.quantile` on checking that a valid axis was passed (`9543`)
  - Bug in `groupby.apply` aggregation for `Categorical` not preserving categories (`10138`)
  - Bug in `to_csv` where `date_format` is ignored if the `datetime` is fractional (`10209`)
  - Bug in `DataFrame.to_json` with mixed data types (`10289`)
  - Bug in cache updating when consolidating (`10264`)
  - Bug in `mean()` where integer dtypes can overflow (`10172`)
  - Bug where `Panel.from_dict` does not set dtype when specified (`10058`)
  - Bug in `Index.union` raises `AttributeError` when passing array-likes. (`10149`)
  - Bug in `Timestamp`'s' `microsecond`, `quarter`, `dayofyear`, `week` and `daysinmonth` properties return `np.int` type, not built-in `int`. (`10050`)
  - Bug in `NaT` raises `AttributeError` when accessing to `daysinmonth`, `dayofweek` properties. (`10096`)
  - Bug in Index repr when using the `max_seq_items=None` setting (`10182`).
  - Bug in getting timezone data with `dateutil` on various platforms ( `9059`, `8639`, `9663`, `10121`)
  - Bug in displaying datetimes with mixed frequencies; display 'ms' datetimes to the proper precision. (`10170`)
  - Bug in `setitem` where type promotion is applied to the entire block (`10280`)
  - Bug in `Series` arithmetic methods may incorrectly hold names (`10068`)
  - Bug in `GroupBy.get_group` when grouping on multiple keys, one of which is categorical. (`10132`)
  - Bug in `DatetimeIndex` and `TimedeltaIndex` names are lost after timedelta arithmetic ( `9926`)
  - Bug in `DataFrame` construction from nested `dict` with `datetime64` (`10160`)
  - Bug in `Series` construction from `dict` with `datetime64` keys (`9456`)
  - Bug in `Series.plot(label="LABEL")` not correctly setting the label (`10119`)
  - Bug in `plot` not defaulting to matplotlib `axes.grid` setting (`9792`)
  - Bug causing strings containing an exponent, but no decimal to be parsed as `int` instead of `float` in `engine='python'` for the `read_csv` parser (`9565`)
  - Bug in `Series.align` resets `name` when `fill_value` is specified (`10067`)
  - Bug in `read_csv` causing index name not to be set on an empty DataFrame (`10184`)
  - Bug in `SparseSeries.abs` resets `name` (`10241`)
  - Bug in `TimedeltaIndex` slicing may reset freq (`10292`)
  - Bug in `GroupBy.get_group` raises `ValueError` when group key contains `NaT` (`6992`)
  - Bug in `SparseSeries` constructor ignores input data name (`10258`)
  - Bug in `Categorical.remove_categories` causing a `ValueError` when removing the `NaN` category if underlying dtype is floating-point (`10156`)
  - Bug where infer\_freq infers time rule (WOM-5XXX) unsupported by to\_offset (`9425`)
  - Bug in `DataFrame.to_hdf()` where table format would raise a seemingly unrelated error for invalid (non-string) column names. This is now explicitly forbidden. (`9057`)
  - Bug to handle masking empty `DataFrame` (`10126`).
  - Bug where MySQL interface could not handle numeric table/column names (`10255`)
  - Bug in `read_csv` with a `date_parser` that returned a `datetime64` array of other time resolution than `[ns]` (`10245`)
  - Bug in `Panel.apply` when the result has ndim=0 (`10332`)
  - Bug in `read_hdf` where `auto_close` could not be passed (`9327`).
  - Bug in `read_hdf` where open stores could not be used (`10330`).
  - Bug in adding empty `DataFrames`, now results in a `DataFrame` that `.equals` an empty `DataFrame` (`10181`).
  - Bug in `to_hdf` and `HDFStore` which did not check that complib choices were valid (`4582`, `8874`).

## Contributors

<div class="contributors">

v0.16.1..v0.16.2

</div>

---

v0.17.0.md

---

# Version 0.17.0 (October 9, 2015)

{{ header }}

This is a major release from 0.16.2 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

\> **Warning** \> pandas \>= 0.17.0 will no longer support compatibility with Python version 3.2 (`9118`)

<div class="warning">

<div class="title">

Warning

</div>

The `pandas.io.data` package is deprecated and will be replaced by the [pandas-datareader package](https://github.com/pydata/pandas-datareader). This will allow the data modules to be independently updated to your pandas installation. The API for `pandas-datareader v0.1.1` is exactly the same as in `pandas v0.17.0` (`8961`, `10861`).

After installing pandas-datareader, you can easily change your imports:

  - \`\`\`python  
    from pandas.io import data, wb

becomes

``` python
from pandas_datareader import data, wb
```

</div>

Highlights include:

\- Release the Global Interpreter Lock (GIL) on some cython operations, see \[here \<whatsnew\_0170.gil\>\](\#here-\<whatsnew\_0170.gil\>) `` ` - Plotting methods are now available as attributes of the ``.plot`accessor, see [here <whatsnew_0170.plot>](#here-<whatsnew_0170.plot>) - The sorting API has been revamped to remove some long-time inconsistencies, see [here <whatsnew_0170.api_breaking.sorting>](#here-<whatsnew_0170.api_breaking.sorting>) - Support for a`datetime64\[ns\]`with timezones as a first-class dtype, see [here <whatsnew_0170.tz>](#here-<whatsnew_0170.tz>) - The default for`to\_datetime`will now be to`raise`when presented with unparsable formats,   previously this would return the original input. Also, date parse   functions now return consistent results. See [here <whatsnew_0170.api_breaking.to_datetime>](#here-<whatsnew_0170.api_breaking.to_datetime>) - The default for`dropna`in`HDFStore`has changed to`False`, to store by default all rows even   if they are all`NaN`, see [here <whatsnew_0170.api_breaking.hdf_dropna>](#here-<whatsnew_0170.api_breaking.hdf_dropna>) - Datetime accessor (`dt`) now supports`Series.dt.strftime`to generate formatted strings for datetime-likes, and`Series.dt.total\_seconds`to generate each duration of the timedelta in seconds. See [here <whatsnew_0170.strftime>](#here-<whatsnew_0170.strftime>) -`Period`and`PeriodIndex`can handle multiplied freq like`3D`, which corresponding to 3 days span. See [here <whatsnew_0170.periodfreq>](#here-<whatsnew_0170.periodfreq>) - Development installed versions of pandas will now have`PEP440``compliant version strings (:issue:`9518`) - Development support for benchmarking with the `Air Speed Velocity library <https://github.com/spacetelescope/asv/>`_ (:issue:`8361`) - Support for reading SAS xport files, see [here <whatsnew_0170.enhancements.sas_xport>](#here-<whatsnew_0170.enhancements.sas_xport>) - Documentation comparing SAS to *pandas*, see [here <compare_with_sas>](#here-<compare_with_sas>) - Removal of the automatic TimeSeries broadcasting, deprecated since 0.8.0, see [here <whatsnew_0170.prior_deprecations>](#here-<whatsnew_0170.prior_deprecations>) - Display format with plain text can optionally align with Unicode East Asian Width, see [here <whatsnew_0170.east_asian_width>](#here-<whatsnew_0170.east_asian_width>) - Compatibility with Python 3.5 (:issue:`11097`) - Compatibility with matplotlib 1.5.0 (:issue:`11111`)  Check the [API Changes <whatsnew_0170.api>](#api-changes-<whatsnew_0170.api>) and [deprecations <whatsnew_0170.deprecations>](#deprecations-<whatsnew_0170.deprecations>) before updating.  .. contents:: What's new in v0.17.0     :local:     :backlinks: none  .. _whatsnew_0170.enhancements:  New features ~~~~~~~~~~~~  .. _whatsnew_0170.tz:  Datetime with TZ ^^^^^^^^^^^^^^^^  We are adding an implementation that natively supports datetime with timezones. A``Series`or a`DataFrame`column previously *could* be assigned a datetime with timezones, and would work as an`object``dtype. This had performance issues with a large number rows. See the [docs <timeseries.timezone_series>](#docs-<timeseries.timezone_series>) for more details. (:issue:`8260`, :issue:`10763`, :issue:`11034`).  The new implementation allows for having a single-timezone across all rows, with operations in a performant manner.  .. ipython:: python     df = pd.DataFrame(        {            "A": pd.date_range("20130101", periods=3),            "B": pd.date_range("20130101", periods=3, tz="US/Eastern"),            "C": pd.date_range("20130101", periods=3, tz="CET"),        }    )    df    df.dtypes  .. ipython:: python     df.B    df.B.dt.tz_localize(None)  This uses a new-dtype representation as well, that is very similar in look-and-feel to its numpy cousin``datetime64\[ns\]`.. ipython:: python     df["B"].dtype    type(df["B"].dtype)  > **Note** >     There is a slightly different string repr for the underlying`DatetimeIndex`as a result of the dtype changes, but    functionally these are the same.     Previous behavior:`\`ipython In \[1\]: pd.date\_range('20130101', periods=3, tz='US/Eastern') Out\[1\]: DatetimeIndex(\['2013-01-01 00:00:00-05:00', '2013-01-02 00:00:00-05:00', '2013-01-03 00:00:00-05:00'\], dtype='datetime64\[ns\]', freq='D', tz='US/Eastern')

> In \[2\]: pd.date\_range('20130101', periods=3, tz='US/Eastern').dtype Out\[2\]: dtype('\<M8\[ns\]')
> 
> New behavior:
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> pd.date\_range("20130101", periods=3, tz="US/Eastern") pd.date\_range("20130101", periods=3, tz="US/Eastern").dtype

<div id="whatsnew_0170.gil">

Releasing the GIL `` ` ^^^^^^^^^^^^^^^^^  We are releasing the global-interpreter-lock (GIL) on some cython operations. This will allow other threads to run simultaneously during computation, potentially allowing performance improvements from multi-threading. Notably ``groupby`,`nsmallest`,`value\_counts``and some indexing operations benefit from this. (:issue:`8882`)  For example the groupby expression in the following code will have the GIL released during the factorization step, e.g.``df.groupby('key')`as well as the`.sum()`operation.`\`python N = 1000000 ngroups = 10 df = DataFrame( {"key": np.random.randint(0, ngroups, size=N), "data": np.random.randn(N)} ) df.groupby("key")\["data"\].sum()

</div>

Releasing of the GIL could benefit an application that uses threads for user interactions (e.g. [QT]()), or performing multi-threaded computations. A nice example of a library that can handle these types of computation-in-parallel is the [dask](https://dask.readthedocs.io/en/latest/) library.

  - `` ` .. _QT: https://wiki.python.org/moin/PyQt  .. _whatsnew_0170.plot:  Plot submethods ^^^^^^^^^^^^^^^  The Series and DataFrame ``.plot()`method allows for customizing [plot types<visualization.other>](#plot-types<visualization.other>) by supplying the`kind`keyword arguments. Unfortunately, many of these kinds of plots use different required and optional keyword arguments, which makes it difficult to discover what any given plot kind uses out of the dozens of possible arguments.  To alleviate this issue, we have added a new, optional plotting interface, which exposes each kind of plot as a method of the`.plot`attribute. Instead of writing`series.plot(kind=\<kind\>, ...)`, you can now also use`series.plot.\<kind\>(...)`:  .. ipython::     :verbatim:      In [13]: df = pd.DataFrame(np.random.rand(10, 2), columns=['a', 'b'])      In [14]: df.plot.bar()  .. image:: ../_static/whatsnew_plot_submethods.png  As a result of this change, these methods are now all discoverable via tab-completion:  .. ipython::     :verbatim:      In [15]: df.plot.<TAB>  # noqa: E225, E999     df.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter     df.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie  Each method signature only includes relevant arguments. Currently, these are limited to required arguments, but in the future these will include optional arguments, as well. For an overview, see the new [api.dataframe.plotting](#api.dataframe.plotting) API documentation.  .. _whatsnew_0170.strftime:  Additional methods for`dt`accessor ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Series.dt.strftime """"""""""""""""""  We are now supporting a`Series.dt.strftime``method for datetime-likes to generate a formatted string (:issue:`10110`). Examples:  .. ipython:: python     # DatetimeIndex    s = pd.Series(pd.date_range("20130101", periods=4))    s    s.dt.strftime("%Y/%m/%d")  .. ipython:: python     # PeriodIndex    s = pd.Series(pd.period_range("20130101", periods=4))    s    s.dt.strftime("%Y/%m/%d")  The string format is as the python standard library and details can be found `here <https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior>`_  Series.dt.total_seconds """""""""""""""""""""""``pd.Series`of type`timedelta64`has new method`.dt.total\_seconds()``returning the duration of the timedelta in seconds (:issue:`10817`)  .. ipython:: python     # TimedeltaIndex    s = pd.Series(pd.timedelta_range("1 minutes", periods=4))    s    s.dt.total_seconds()  .. _whatsnew_0170.periodfreq:  Period frequency enhancement ^^^^^^^^^^^^^^^^^^^^^^^^^^^^``Period`,`PeriodIndex`and`period\_range`can now accept multiplied freq. Also,`Period.freq`and`PeriodIndex.freq`are now stored as a`DateOffset`instance like`DatetimeIndex`, and not as`str``(:issue:`7811`)  A multiplied freq represents a span of corresponding length. The example below creates a period of 3 days. Addition and subtraction will shift the period by its span.  .. ipython:: python     p = pd.Period("2015-08-01", freq="3D")    p    p + 1    p - 2    p.to_timestamp()    p.to_timestamp(how="E")  You can use the multiplied freq in``PeriodIndex`and`period\_range``.  .. ipython:: python     idx = pd.period_range("2015-08-01", periods=4, freq="2D")    idx    idx + 1  .. _whatsnew_0170.enhancements.sas_xport:  Support for SAS XPORT files ^^^^^^^^^^^^^^^^^^^^^^^^^^^  `~pandas.io.read_sas` provides support for reading *SAS XPORT* format files. (:issue:`4052`).``\`python  
    df = pd.read\_sas("sas\_xport.xpt")

It is also possible to obtain an iterator and read an XPORT file `` ` incrementally. ``\`python for df in pd.read\_sas("sas\_xport.xpt", chunksize=10000): do\_something(df)

See the \[docs \<io.sas\>\](\#docs-\<io.sas\>) for more details.

<div id="whatsnew_0170.matheval">

Support for math functions in .eval() `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `~pandas.eval` now supports calling math functions (:issue:`4893`) ``\`python df = pd.DataFrame({"a": np.random.randn(10)}) df.eval("b = sin(a)")

</div>

The support math functions are `sin`, `cos`, `exp`, `log`, `expm1`, `log1p`, `` ` ``sqrt`,`sinh`,`cosh`,`tanh`,`arcsin`,`arccos`,`arctan`,`arccosh`,`arcsinh`,`arctanh`,`abs`and`arctan2`.  These functions map to the intrinsics for the`NumExpr`engine.  For the Python engine, they are mapped to`NumPy`calls.  Changes to Excel with`MultiIndex`^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In version 0.16.2 a`DataFrame`with`MultiIndex`columns could not be written to Excel via`to\_excel``. That functionality has been added (:issue:`10564`), along with updating``read\_excel`so that the data can be read back with, no loss of information, by specifying which columns/rows make up the`MultiIndex`in the`header`and`index\_col``parameters (:issue:`4679`)  See the [documentation <io.excel>](#documentation-<io.excel>) for more details.  .. ipython:: python     df = pd.DataFrame(        [[1, 2, 3, 4], [5, 6, 7, 8]],        columns=pd.MultiIndex.from_product(            [["foo", "bar"], ["a", "b"]], names=["col1", "col2"]        ),        index=pd.MultiIndex.from_product([["j"], ["l", "k"]], names=["i1", "i2"]),    )     df    df.to_excel("test.xlsx")     df = pd.read_excel("test.xlsx", header=[0, 1], index_col=[0, 1])    df  .. ipython:: python    :suppress:     import os     os.remove("test.xlsx")  Previously, it was necessary to specify the``has\_index\_names`argument in`read\_excel`, if the serialized data had index names.  For version 0.17.0 the output format of`to\_excel`has been changed to make this keyword unnecessary - the change is shown below.  **Old**  .. image:: ../_static/old-excel-index.png  **New**  .. image:: ../_static/new-excel-index.png  > **Warning** >     Excel files saved in version 0.16.2 or prior that had index names will still able to be read in,    but the`has\_index\_names`argument must specified to`True``.  .. _whatsnew_0170.gbq:  Google BigQuery enhancements ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ - Added ability to automatically create a table/dataset using the `pandas.io.gbq.to_gbq` function if the destination table/dataset does not exist. (:issue:`8325`, :issue:`11121`). - Added ability to replace an existing table and schema when calling the `pandas.io.gbq.to_gbq` function via the``if\_exists``argument. See the `docs <https://pandas-gbq.readthedocs.io/en/latest/writing.html>`__ for more details (:issue:`8325`). -``InvalidColumnOrder`and`InvalidPageToken`in the gbq module will raise`ValueError`instead of`IOError`. - The`generate\_bq\_schema()``function is now deprecated and will be removed in a future version (:issue:`11121`) - The gbq module will now support Python 3 (:issue:`11094`).  .. _whatsnew_0170.east_asian_width:  Display alignment with Unicode East Asian width ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  > **Warning** >     Enabling this option will affect the performance for printing of``DataFrame`and`Series`(about 2 times slower).    Use only when it is actually required.  Some East Asian countries use Unicode characters its width is corresponding to 2 alphabets. If a`DataFrame`or`Series`contains these characters, the default output cannot be aligned properly. The following options are added to enable precise handling for these characters.  -`display.unicode.east\_asian\_width``: Whether to use the Unicode East Asian Width to calculate the display text width. (:issue:`2612`) -``display.unicode.ambiguous\_as\_wide``: Whether to handle Unicode characters belong to Ambiguous as Wide. (:issue:`11102`)  .. ipython:: python     df = pd.DataFrame({u"å›½ç±": ["UK", u"æ—¥æœ¬"], u"åå‰": ["Alice", u"ã—ã®ã¶"]})    df  .. ipython:: python     pd.set_option("display.unicode.east_asian_width", True)    df  For further details, see [here <options.east_asian_width>](#here-<options.east_asian_width>)  .. ipython:: python    :suppress:     pd.set_option("display.unicode.east_asian_width", False)  .. _whatsnew_0170.enhancements.other:  Other enhancements ^^^^^^^^^^^^^^^^^^  - Support for``openpyxl``>= 2.2. The API for style support is now stable (:issue:`10125`) -``merge`now accepts the argument`indicator`which adds a Categorical-type column (by default called`\_merge``) to the output object that takes on the values (:issue:`8790`)    ===================================   ================   Observation Origin``\_merge`value   ===================================   ================   Merge key only in`'left'`frame`left\_only`Merge key only in`'right'`frame`right\_only`Merge key in both frames`both`===================================   ================    .. ipython:: python      df1 = pd.DataFrame({"col1": [0, 1], "col_left": ["a", "b"]})     df2 = pd.DataFrame({"col1": [1, 2, 2], "col_right": [2, 2, 2]})     pd.merge(df1, df2, on="col1", how="outer", indicator=True)    For more, see the [updated docs <merging.indicator>](#updated-docs-<merging.indicator>)  -`pd.to\_numeric``is a new function to coerce strings to numbers (possibly with coercion) (:issue:`11133`)  -``pd.merge``will now allow duplicate column names if they are not merged upon (:issue:`10639`).  -``pd.pivot`will now allow passing index as`None``(:issue:`3962`).  -``pd.concat``will now use existing Series names if provided (:issue:`10698`).    .. ipython:: python       foo = pd.Series([1, 2], name="foo")      bar = pd.Series([1, 2])      baz = pd.Series([4, 5])    Previous behavior:``\`ipython In \[1\]: pd.concat(\[foo, bar, baz\], axis=1) Out\[1\]: 0 1 2 0 1 1 4 1 2 2 5

> New behavior:
> 
> <div class="ipython">
> 
> python
> 
> pd.concat(\[foo, bar, baz\], axis=1)
> 
> </div>

  - `DataFrame` has gained the `nlargest` and `nsmallest` methods (`10393`)

  - Add a `limit_direction` keyword argument that works with `limit` to enable `interpolate` to fill `NaN` values forward, backward, or both (`9218`, `10420`, `11115`)
    
    <div class="ipython">
    
    python
    
    ser = pd.Series(\[np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13\]) ser.interpolate(limit=1, limit\_direction="both")
    
    </div>

  - Added a `DataFrame.round` method to round the values to a variable number of decimal places (`10568`).
    
    <div class="ipython">
    
    python
    
      - df = pd.DataFrame(  
        np.random.random(\[3, 3\]), columns=\["A", "B", "C"\], index=\["first", "second", "third"\],
    
    ) df df.round(2) df.round({"A": 0, "C": 2})
    
    </div>

  - `drop_duplicates` and `duplicated` now accept a `keep` keyword to target first, last, and all duplicates. The `take_last` keyword is deprecated, see \[here \<whatsnew\_0170.deprecations\>\](\#here-\<whatsnew\_0170.deprecations\>) (`6511`, `8505`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(\["A", "B", "C", "A", "B", "D"\]) s.drop\_duplicates() s.drop\_duplicates(keep="last") s.drop\_duplicates(keep=False)
    
    </div>

  - Reindex now has a `tolerance` argument that allows for finer control of \[basics.limits\_on\_reindex\_fill\](\#basics.limits\_on\_reindex\_fill) (`10411`):
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame({"x": range(5), "t": pd.date\_range("2000-01-01", periods=5)}) df.reindex(\[0.1, 1.9, 3.5\], method="nearest", tolerance=0.2)
    
    </div>
    
    When used on a `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`, `tolerance` will coerced into a `Timedelta` if possible. This allows you to specify tolerance with a string:
    
    <div class="ipython">
    
    python
    
    df = df.set\_index("t") df.reindex(pd.to\_datetime(\["1999-12-31"\]), method="nearest", tolerance="1 day")
    
    </div>
    
    `tolerance` is also exposed by the lower level `Index.get_indexer` and `Index.get_loc` methods.

  - Added functionality to use the `base` argument when resampling a `TimeDeltaIndex` (`10530`)

  - `DatetimeIndex` can be instantiated using strings contains `NaT` (`7599`)

  - `to_datetime` can now accept the `yearfirst` keyword (`7599`)

  - `pandas.tseries.offsets` larger than the `Day` offset can now be used with a `Series` for addition/subtraction (`10699`). See the \[docs \<timeseries.offsetseries\>\](\#docs-\<timeseries.offsetseries\>) for more details.

  - `pd.Timedelta.total_seconds()` now returns Timedelta duration to ns precision (previously microsecond precision) (`10939`)

  - `PeriodIndex` now supports arithmetic with `np.ndarray` (`10638`)

  - Support pickling of `Period` objects (`10439`)

  - `.as_blocks` will now take a `copy` optional argument to return a copy of the data, default is to copy (no change in behavior from prior versions), (`9607`)

  - `regex` argument to `DataFrame.filter` now handles numeric column names instead of raising `ValueError` (`10384`).

  - Enable reading gzip compressed files via URL, either by explicitly setting the compression parameter or by inferring from the presence of the HTTP Content-Encoding header in the response (`8685`)

  - Enable writing Excel files in \[memory \<io.excel\_writing\_buffer\>\](\#memory-\<io.excel\_writing\_buffer\>) using StringIO/BytesIO (`7074`)

  - Enable serialization of lists and dicts to strings in `ExcelWriter` (`8188`)

  - SQL io functions now accept a SQLAlchemy connectable. (`7877`)

  - `pd.read_sql` and `to_sql` can accept database URI as `con` parameter (`10214`)

  - `read_sql_table` will now allow reading from views (`10750`).

  - Enable writing complex values to `HDFStores` when using the `table` format (`10447`)

  - Enable `pd.read_hdf` to be used without specifying a key when the HDF file contains a single dataset (`10443`)

  - `pd.read_stata` will now read Stata 118 type files. (`9882`)

  - `msgpack` submodule has been updated to 0.4.6 with backward compatibility (`10581`)

  - `DataFrame.to_dict` now accepts `orient='index'` keyword argument (`10844`).

  - `DataFrame.apply` will return a Series of dicts if the passed function returns a dict and `reduce=True` (`8735`).

  - Allow passing `kwargs` to the interpolation methods (`10378`).

  - Improved error message when concatenating an empty iterable of `Dataframe` objects (`9157`)

  - `pd.read_csv` can now read bz2-compressed files incrementally, and the C parser can read bz2-compressed files from AWS S3 (`11070`, `11072`).

  - In `pd.read_csv`, recognize `s3n://` and `s3a://` URLs as designating S3 file storage (`11070`, `11071`).

  - Read CSV files from AWS S3 incrementally, instead of first downloading the entire file. (Full file download still required for compressed files in Python 2.) (`11070`, `11073`)

  - `pd.read_csv` is now able to infer compression type for files read from AWS S3 storage (`11070`, `11074`).

<div id="whatsnew_0170.api">

<div id="whatsnew_0170.api_breaking">

Backwards incompatible API changes `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. _whatsnew_0170.api_breaking.sorting:  Changes to sorting API ^^^^^^^^^^^^^^^^^^^^^^  The sorting API has had some longtime inconsistencies. (:issue:`9816`, :issue:`8239`).  Here is a summary of the API **PRIOR** to 0.17.0:  - ``Series.sort`is **INPLACE** while`DataFrame.sort`returns a new object. -`Series.order`returns a new object - It was possible to use`Series/DataFrame.sort\_index`to sort by **values** by passing the`by`keyword. -`Series/DataFrame.sortlevel`worked only on a`MultiIndex``for sorting by index.  To address these issues, we have revamped the API:  - We have introduced a new method, `DataFrame.sort_values`, which is the merger of``DataFrame.sort()`,`Series.sort()`,   and`Series.order()`, to handle sorting of **values**. - The existing methods`Series.sort()`,`Series.order()`, and`DataFrame.sort()`have been deprecated and will be removed in a   future version. - The`by`argument of`DataFrame.sort\_index()`has been deprecated and will be removed in a future version. - The existing method`.sort\_index()`will gain the`level`keyword to enable level sorting.  We now have two distinct and non-overlapping methods of sorting. A`\*`marks items that will show a`FutureWarning`.  To sort by the **values**:  ==================================    ==================================== Previous                              Replacement ==================================    ==================================== \*`Series.order()`  `Series.sort\_values()`\*`Series.sort()`  `Series.sort\_values(inplace=True)`\*`DataFrame.sort(columns=...)`  `DataFrame.sort\_values(by=...)`==================================    ====================================  To sort by the **index**:  ==================================    ==================================== Previous                              Replacement ==================================    ====================================`Series.sort\_index()`  `Series.sort\_index()`  `Series.sortlevel(level=...)`  `Series.sort\_index(level=...`)`DataFrame.sort\_index()`  `DataFrame.sort\_index()`  `DataFrame.sortlevel(level=...)`  `DataFrame.sort\_index(level=...)`\*`DataFrame.sort()`  `DataFrame.sort\_index()`==================================    ====================================  We have also deprecated and changed similar methods in two Series-like classes,`Index`and`Categorical`.  ==================================    ==================================== Previous                              Replacement ==================================    ==================================== \*`Index.order()`  `Index.sort\_values()`\*`Categorical.order()`  `Categorical.sort\_values()`==================================    ====================================  .. _whatsnew_0170.api_breaking.to_datetime:  Changes to to_datetime and to_timedelta ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Error handling """"""""""""""  The default for`pd.to\_datetime`error handling has changed to`errors='raise'`. In prior versions it was`errors='ignore'`. Furthermore, the`coerce`argument has been deprecated in favor of`errors='coerce'``. This means that invalid parsing will raise rather that return the original input as in previous versions. (:issue:`10636`)  Previous behavior:``\`ipython In \[2\]: pd.to\_datetime(\['2009-07-31', 'asd'\]) Out\[2\]: array(\['2009-07-31', 'asd'\], dtype=object)

</div>

</div>

New behavior:

``` ipython
In [3]: pd.to_datetime(['2009-07-31', 'asd'])
ValueError: Unknown string format
```

Of course you can coerce this as well.

<div class="ipython">

python

pd.to\_datetime(\["2009-07-31", "asd"\], errors="coerce")

</div>

To keep the previous behavior, you can use `errors='ignore'`:

``` ipython
In [4]: pd.to_datetime(["2009-07-31", "asd"], errors="ignore")
Out[4]: Index(['2009-07-31', 'asd'], dtype='object')
```

Furthermore, `pd.to_timedelta` has gained a similar API, of `errors='raise'|'ignore'|'coerce'`, and the `coerce` keyword `` ` has been deprecated in favor of ``errors='coerce'`.  Consistent parsing """"""""""""""""""  The string parsing of`to\_datetime`,`Timestamp`and`DatetimeIndex``has been made consistent. (:issue:`7599`)  Prior to v0.17.0,``Timestamp`and`to\_datetime`may parse year-only datetime-string incorrectly using today's date, otherwise`DatetimeIndex`uses the beginning of the year.`Timestamp`and`to\_datetime`may raise`ValueError`in some types of datetime-string which`DatetimeIndex`can parse, such as a quarterly string.  Previous behavior:`\`ipython In \[1\]: pd.Timestamp('2012Q2') Traceback ... ValueError: Unable to parse 2012Q2

> \# Results in today's date. In \[2\]: pd.Timestamp('2014') Out \[2\]: 2014-08-12 00:00:00

v0.17.0 can parse them as below. It works on `DatetimeIndex` also.

New behavior:

<div class="ipython">

python

pd.Timestamp("2012Q2") pd.Timestamp("2014") pd.DatetimeIndex(\["2012Q2", "2014"\])

</div>

\> **Note** \> If you want to perform calculations based on today's date, use `Timestamp.now()` and `pandas.tseries.offsets`.

> 
> 
> <div class="ipython">
> 
> python
> 
> import pandas.tseries.offsets as offsets
> 
> pd.Timestamp.now() pd.Timestamp.now() + offsets.DateOffset(years=1)
> 
> </div>

Changes to Index comparisons `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Operator equal on ``Index`should behavior similarly to`Series``(:issue:`9947`, :issue:`10637`)  Starting in v0.17.0, comparing``Index`objects of different lengths will raise a`ValueError`. This is to be consistent with the behavior of`Series`.  Previous behavior:`\`ipython In \[2\]: pd.Index(\[1, 2, 3\]) == pd.Index(\[1, 4, 5\]) Out\[2\]: array(\[ True, False, False\], dtype=bool)

> In \[3\]: pd.Index(\[1, 2, 3\]) == pd.Index(\[2\]) Out\[3\]: array(\[False, True, False\], dtype=bool)
> 
> In \[4\]: pd.Index(\[1, 2, 3\]) == pd.Index(\[1, 2\]) Out\[4\]: False

New behavior:

``` ipython
In [8]: pd.Index([1, 2, 3]) == pd.Index([1, 4, 5])
Out[8]: array([ True, False, False], dtype=bool)

In [9]: pd.Index([1, 2, 3]) == pd.Index([2])
ValueError: Lengths must match to compare

In [10]: pd.Index([1, 2, 3]) == pd.Index([1, 2])
ValueError: Lengths must match to compare
```

Note that this is different from the `numpy` behavior where a comparison can `` ` be broadcast:  .. ipython:: python     np.array([1, 2, 3]) == np.array([1])  or it can return False if broadcasting can not be done: ``\`ipython In \[11\]: np.array(\[1, 2, 3\]) == np.array(\[1, 2\]) Out\[11\]: False

Changes to boolean comparisons vs. None `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Boolean comparisons of a ``Series`vs`None`will now be equivalent to comparing with`np.nan`, rather than raise`TypeError``. (:issue:`1079`).  .. ipython:: python     s = pd.Series(range(3), dtype="float")    s.iloc[1] = None    s  Previous behavior:``\`ipython In \[5\]: s == None TypeError: Could not compare \<type 'NoneType'\> type with Series

New behavior:

<div class="ipython">

python

s == None

</div>

Usually you simply want to know which values are null.

<div class="ipython">

python

s.isnull()

</div>

\> **Warning** \> You generally will want to use `isnull/notnull` for these types of comparisons, as `isnull/notnull` tells you which elements are null. One has to be mindful that `nan's` don't compare equal, but `None's` do. Note that pandas/numpy uses the fact that `np.nan != np.nan`, and treats `None` like `np.nan`.

> 
> 
> <div class="ipython">
> 
> python
> 
> None == None np.nan == np.nan
> 
> </div>

<div id="whatsnew_0170.api_breaking.hdf_dropna">

HDFStore dropna behavior `` ` ^^^^^^^^^^^^^^^^^^^^^^^^  The default behavior for HDFStore write functions with ``format='table'`is now to keep rows that are all missing. Previously, the behavior was to drop rows that were all missing save the index. The previous behavior can be replicated using the`dropna=True``option. (:issue:`9382`)  Previous behavior:  .. ipython:: python     df_with_missing = pd.DataFrame(        {"col1": [0, np.nan, 2], "col2": [1, np.nan, np.nan]}    )     df_with_missing``\`ipython In \[27\]: df\_with\_missing.to\_hdf('file.h5', key='df\_with\_missing', format='table', mode='w')

</div>

> In \[28\]: pd.read\_hdf('file.h5', 'df\_with\_missing')
> 
>   - Out \[28\]:  
>     col1 col2 0 0 1 2 2 NaN

New behavior:

<div class="ipython">

python

df\_with\_missing.to\_hdf("file.h5", key="df\_with\_missing", format="table", mode="w")

pd.read\_hdf("file.h5", "df\_with\_missing")

</div>

<div class="ipython" data-suppress="">

python

import os

os.remove("file.h5")

</div>

See the \[docs \<io.hdf5\>\](\#docs-\<io.hdf5\>) for more details.

<div id="whatsnew_0170.api_breaking.display_precision">

Changes to `display.precision` option `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The ``display.precision``option has been clarified to refer to decimal places (:issue:`10451`).  Earlier versions of pandas would format floating point numbers to have one less decimal place than the value in``display.precision`.`\`ipython In \[1\]: pd.set\_option('display.precision', 2)

</div>

> In \[2\]: pd.DataFrame({'x': \[123.456789\]}) Out\[2\]: x 0 123.5

If interpreting precision as "significant figures" this did work for scientific notation but that same interpretation `` ` did not work for values with standard formatting. It was also out of step with how numpy handles formatting.  Going forward the value of ``display.precision`will directly control the number of places after the decimal, for regular formatting as well as scientific notation, similar to how numpy's`precision`print option works.  .. ipython:: python    pd.set_option("display.precision", 2)   pd.DataFrame({"x": [123.456789]})  To preserve output behavior with prior versions the default value of`display.precision`has been reduced to`6`from`7`.  .. ipython:: python   :suppress:    pd.set_option("display.precision", 6)  .. _whatsnew_0170.api_breaking.categorical_unique:  Changes to`Categorical.unique`^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^`Categorical.unique`now returns new`Categoricals`with`categories`and`codes`that are unique, rather than returning`np.array``(:issue:`10508`)  - unordered category: values and categories are sorted by appearance order. - ordered category: values are sorted by appearance order, categories keep existing order.  .. ipython:: python     cat = pd.Categorical(["C", "A", "B", "C"], categories=["A", "B", "C"], ordered=True)    cat    cat.unique()     cat = pd.Categorical(["C", "A", "B", "C"], categories=["A", "B", "C"])    cat    cat.unique()  Changes to``bool`passed as`header`in parsers ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In earlier versions of pandas, if a bool was passed the`header`argument of`read\_csv`,`read\_excel`, or`read\_html`it was implicitly converted to an integer, resulting in`header=0`for`False`and`header=1`for`True``(:issue:`6113`)  A``bool`input to`header`will now raise a`TypeError`  `\`ipython In \[29\]: df = pd.read\_csv('data.csv', header=False) TypeError: Passing a bool to header is invalid. Use header=None for no header or header=int or list-like of ints to specify the row(s) making up the column names

<div id="whatsnew_0170.api_breaking.other">

Other API changes `` ` ^^^^^^^^^^^^^^^^^  - Line and kde plot with ``subplots=True`now uses default colors, not all black. Specify`color='k'``to draw all lines in black (:issue:`9894`) - Calling the``.value\_counts()`method on a Series with a`categorical`dtype now returns a Series with a`CategoricalIndex``(:issue:`10704`) - The metadata properties of subclasses of pandas objects will now be serialized (:issue:`10553`). -``groupby`using`Categorical`follows the same rule as`Categorical.unique``described above  (:issue:`10508`) - When constructing``DataFrame`with an array of`complex64`dtype previously meant the corresponding column   was automatically promoted to the`complex128``dtype. pandas will now preserve the itemsize of the input for complex data (:issue:`10952`) - some numeric reduction operators would return``ValueError`, rather than`TypeError``on object types that includes strings and numbers (:issue:`11131`) - Passing currently unsupported``chunksize`argument to`read\_excel`or`ExcelFile.parse`will now raise`NotImplementedError``(:issue:`8011`) - Allow an``ExcelFile`object to be passed into`read\_excel``(:issue:`11198`) -``DatetimeIndex.union`does not infer`freq`if`self`and the input have`None`as`freq``(:issue:`11086`) -``NaT`'s methods now either raise`ValueError`, or return`np.nan`or`NaT``(:issue:`9513`)    ===============================     ===============================================================   Behavior                            Methods   ===============================     ===============================================================   return``np.nan`  `weekday`,`isoweekday`return`NaT`  `date`,`now`,`replace`,`to\_datetime`,`today`return`np.datetime64('NaT')`  `to\_datetime64`(unchanged)   raise`ValueError`All other public methods (names not beginning with underscores)   ===============================     ===============================================================  .. _whatsnew_0170.deprecations:  Deprecations ^^^^^^^^^^^^  - For`Series``the following indexing functions are deprecated (:issue:`10177`).    =====================  =================================   Deprecated Function    Replacement   =====================  =================================``.irow(i)`  `.iloc\[i\]`or`.iat\[i\]`  `.iget(i)`  `.iloc\[i\]`or`.iat\[i\]`  `.iget\_value(i)`  `.iloc\[i\]`or`.iat\[i\]`=====================  =================================  - For`DataFrame``the following indexing functions are deprecated (:issue:`10177`).    =====================  =================================   Deprecated Function    Replacement   =====================  =================================``.irow(i)`  `.iloc\[i\]`  `.iget\_value(i, j)`  `.iloc\[i, j\]`or`.iat\[i, j\]`  `.icol(j)`  `.iloc\[:, j\]`=====================  =================================  .. note:: These indexing function have been deprecated in the documentation since 0.11.0.  -`Categorical.name`was deprecated to make`Categorical`more`numpy.ndarray`like. Use`Series(cat, name="whatever")``instead (:issue:`10482`). - Setting missing values (NaN) in a``Categorical`'s`categories``will issue a warning (:issue:`10748`). You can still have missing values in the``values`. -`drop\_duplicates`and`duplicated`'s`take\_last`keyword was deprecated in favor of`keep``. (:issue:`6511`, :issue:`8505`) -``Series.nsmallest`and`nlargest`'s`take\_last`keyword was deprecated in favor of`keep``. (:issue:`10792`) -``DataFrame.combineAdd`and`DataFrame.combineMult`are deprecated. They   can easily be replaced by using the`add`and`mul`methods:`DataFrame.add(other, fill\_value=0)`and`DataFrame.mul(other, fill\_value=1.)``(:issue:`10735`). -``TimeSeries`deprecated in favor of`Series``(note that this has been an alias since 0.13.0), (:issue:`10890`) -``SparsePanel``deprecated and will be removed in a future version (:issue:`11157`). -``Series.is\_time\_series`deprecated in favor of`Series.index.is\_all\_dates``(:issue:`11135`) - Legacy offsets (like``'<A@JAN'>``) are deprecated (note that this has been alias since 0.8.0) (:issue:`10878`) -``WidePanel`deprecated in favor of`Panel`,`LongPanel`in favor of`DataFrame``(note these have been aliases since < 0.11.0), (:issue:`10892`) -``DataFrame.convert\_objects`has been deprecated in favor of type-specific functions`pd.to\_datetime`,`pd.to\_timestamp`and`pd.to\_numeric``(new in 0.17.0) (:issue:`11133`).  .. _whatsnew_0170.prior_deprecations:  Removal of prior version deprecations/changes ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  - Removal of``na\_last`parameters from`Series.order()`and`Series.sort()`, in favor of`na\_position``. (:issue:`5231`) - Remove of``percentile\_width`from`.describe()`, in favor of`percentiles``. (:issue:`7088`) - Removal of``colSpace`parameter from`DataFrame.to\_string()`, in favor of`col\_space``, circa 0.8.0 version. - Removal of automatic time-series broadcasting (:issue:`2304`)    .. ipython:: python       np.random.seed(1234)      df = pd.DataFrame(          np.random.randn(5, 2),          columns=list("AB"),          index=pd.date_range("2013-01-01", periods=5),      )      df    Previously``\`ipython In \[3\]: df + df.A FutureWarning: TimeSeries broadcasting along DataFrame index by default is deprecated. Please use DataFrame.\<op\> to explicitly broadcast arithmetic operations along the index

</div>

>   - Out\[3\]:  
>     A B
> 
> 2013-01-01 0.942870 -0.719541 2013-01-02 2.865414 1.120055 2013-01-03 -1.441177 0.166574 2013-01-04 1.719177 0.223065 2013-01-05 0.031393 -2.226989
> 
> Current
> 
> <div class="ipython">
> 
> python
> 
> </div>
> 
> df.add(df.A, axis="index")

\- Remove `table` keyword in `HDFStore.put/append`, in favor of using `format=` (`4645`) `` ` - Remove ``kind`in`read\_excel/ExcelFile``as its unused (:issue:`4712`) - Remove``infer\_type`keyword from`pd.read\_html``as its unused (:issue:`4770`, :issue:`7032`) - Remove``offset`and`timeRule`keywords from`Series.tshift/shift`, in favor of`freq``(:issue:`4853`, :issue:`4864`) - Remove``pd.load/pd.save`aliases in favor of`pd.to\_pickle/pd.read\_pickle``(:issue:`3787`)  .. _whatsnew_0170.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Development support for benchmarking with the `Air Speed Velocity library <https://github.com/spacetelescope/asv/>`_ (:issue:`8361`) - Added vbench benchmarks for alternative ExcelWriter engines and reading Excel files (:issue:`7171`) - Performance improvements in``Categorical.value\_counts``(:issue:`10804`) - Performance improvements in``SeriesGroupBy.nunique`and`SeriesGroupBy.value\_counts`and`SeriesGroupby.transform``(:issue:`10820`, :issue:`11077`) - Performance improvements in``DataFrame.drop\_duplicates``with integer dtypes (:issue:`10917`) - Performance improvements in``DataFrame.duplicated``with wide frames. (:issue:`10161`, :issue:`11180`) - 4x improvement in``timedelta``string parsing (:issue:`6755`, :issue:`10426`) - 8x improvement in``timedelta64`and`datetime64``ops (:issue:`6755`) - Significantly improved performance of indexing``MultiIndex``with slicers (:issue:`10287`) - 8x improvement in``iloc``using list-like input (:issue:`10791`) - Improved performance of``Series.isin``for datetimelike/integer Series (:issue:`10287`) - 20x improvement in``concat``of Categoricals when categories are identical (:issue:`10587`) - Improved performance of``to\_datetime``when specified format string is ISO8601 (:issue:`10178`) - 2x improvement of``Series.value\_counts``for float dtype (:issue:`10821`) - Enable``infer\_datetime\_format`in`to\_datetime``when date components do not have 0 padding (:issue:`11142`) - Regression from 0.16.1 in constructing``DataFrame``from nested dictionary (:issue:`11084`) - Performance improvements in addition/subtraction operations for``DateOffset`with`Series`or`DatetimeIndex``(:issue:`10744`, :issue:`11205`)  .. _whatsnew_0170.bug_fixes:  Bug fixes ~~~~~~~~~  - Bug in incorrect computation of``.mean()`on`timedelta64\[ns\]``because of overflow (:issue:`9442`) - Bug in``.isin``on older numpies (:issue:`11232`) - Bug in``DataFrame.to\_html(index=False)`renders unnecessary`name``row (:issue:`10344`) - Bug in``DataFrame.to\_latex()`the`column\_format``argument could not be passed (:issue:`9402`) - Bug in``DatetimeIndex`when localizing with`NaT``(:issue:`10477`) - Bug in``Series.dt``ops in preserving meta-data (:issue:`10477`) - Bug in preserving``NaT`when passed in an otherwise invalid`to\_datetime``construction (:issue:`10477`) - Bug in``DataFrame.apply``when function returns categorical series. (:issue:`9573`) - Bug in``to\_datetime``with invalid dates and formats supplied (:issue:`10154`) - Bug in``Index.drop\_duplicates``dropping name(s) (:issue:`10115`) - Bug in``Series.quantile``dropping name (:issue:`10881`) - Bug in``pd.Series`when setting a value on an empty`Series``whose index has a frequency. (:issue:`10193`) - Bug in``pd.Series.interpolate`with invalid`order``keyword values. (:issue:`10633`) - Bug in``DataFrame.plot`raises`ValueError``when color name is specified by multiple characters (:issue:`10387`) - Bug in``Index``construction with a mixed list of tuples (:issue:`10697`) - Bug in``DataFrame.reset\_index`when index contains`NaT``. (:issue:`10388`) - Bug in``ExcelReader``when worksheet is empty (:issue:`6403`) - Bug in``BinGrouper.group\_info``where returned values are not compatible with base class (:issue:`10914`) - Bug in clearing the cache on``DataFrame.pop``and a subsequent inplace op (:issue:`10912`) - Bug in indexing with a mixed-integer``Index`causing an`ImportError``(:issue:`10610`) - Bug in``Series.count``when index has nulls (:issue:`10946`) - Bug in pickling of a non-regular freq``DatetimeIndex``(:issue:`11002`) - Bug causing``DataFrame.where`to not respect the`axis``parameter when the frame has a symmetric shape. (:issue:`9736`) - Bug in``Table.select\_column``where name is not preserved (:issue:`10392`) - Bug in``offsets.generate\_range`where`start`and`end`have finer precision than`offset``(:issue:`9907`) - Bug in``[pd.rolling]()\*`where`Series.name``would be lost in the output (:issue:`10565`) - Bug in``stack``when index or columns are not unique. (:issue:`10417`) - Bug in setting a``Panel``when an axis has a MultiIndex (:issue:`10360`) - Bug in``USFederalHolidayCalendar`where`USMemorialDay`and`USMartinLutherKingJr``were incorrect (:issue:`10278` and :issue:`9760` ) - Bug in``.sample()`where returned object, if set, gives unnecessary`SettingWithCopyWarning``(:issue:`10738`) - Bug in``.sample()`where weights passed as`Series``were not aligned along axis before being treated positionally, potentially causing problems if weight indices were not aligned with sampled object. (:issue:`10738`)  - Regression fixed in (:issue:`9311`, :issue:`6620`, :issue:`9345`), where groupby with a datetime-like converting to float with certain aggregators (:issue:`10979`)  - Bug in``DataFrame.interpolate`with`axis=1`and`inplace=True``(:issue:`10395`) - Bug in``io.sql.get\_schema``when specifying multiple columns as primary   key (:issue:`10385`).  - Bug in``groupby(sort=False)`with datetime-like`Categorical`raises`ValueError``(:issue:`10505`) - Bug in``groupby(axis=1)`with`filter()`throws`IndexError``(:issue:`11041`) - Bug in``test\_categorical``on big-endian builds (:issue:`10425`) - Bug in``Series.shift`and`DataFrame.shift``not supporting categorical data (:issue:`9416`) - Bug in``Series.map`using categorical`Series`raises`AttributeError``(:issue:`10324`) - Bug in``MultiIndex.get\_level\_values`including`Categorical`raises`AttributeError``(:issue:`10460`) - Bug in``pd.get\_dummies`with`sparse=True`not returning`SparseDataFrame``(:issue:`10531`) - Bug in``Index`subtypes (such as`PeriodIndex`) not returning their own type for`.drop`and`.insert``methods (:issue:`10620`) - Bug in``algos.outer\_join\_indexer`when`right``array is empty (:issue:`10618`)  - Bug in``filter`(regression from 0.16.0) and`transform``when grouping on multiple keys, one of which is datetime-like (:issue:`10114`)   - Bug in``to\_datetime`and`to\_timedelta`causing`Index``name to be lost (:issue:`10875`) - Bug in``len(DataFrame.groupby)`causing`IndexError``when there's a column containing only NaNs (:issue:`11016`)  - Bug that caused segfault when resampling an empty Series (:issue:`10228`) - Bug in``DatetimeIndex`and`PeriodIndex.value\_counts`resets name from its result, but retains in result's`Index``. (:issue:`10150`) - Bug in``pd.eval`using`numexpr``engine coerces 1 element numpy array to scalar (:issue:`10546`) - Bug in``pd.concat`with`axis=0`when column is of dtype`category``(:issue:`10177`) - Bug in``read\_msgpack``where input type is not always checked (:issue:`10369`, :issue:`10630`) - Bug in``pd.read\_csv`with kwargs`index\_col=False`,`index\_col=\['a', 'b'\]`or`dtype``(:issue:`10413`, :issue:`10467`, :issue:`10577`) - Bug in``Series.from\_csv`with`header`kwarg not setting the`Series.name`or the`Series.index.name``(:issue:`10483`) - Bug in``groupby.var``which caused variance to be inaccurate for small float values (:issue:`10448`) - Bug in``Series.plot(kind='hist')``Y Label not informative (:issue:`10485`) - Bug in``read\_csv`when using a converter which generates a`uint8``type (:issue:`9266`)  - Bug causes memory leak in time-series line and area plot (:issue:`9003`)  - Bug when setting a``Panel`sliced along the major or minor axes when the right-hand side is a`DataFrame``(:issue:`11014`) - Bug that returns``None`and does not raise`NotImplementedError`when operator functions (e.g.`.add`) of`Panel``are not implemented (:issue:`7692`)  - Bug in line and kde plot cannot accept multiple colors when``subplots=True``(:issue:`9894`) - Bug in``DataFrame.plot`raises`ValueError``when color name is specified by multiple characters (:issue:`10387`)  - Bug in left and right``align`of`Series`with`MultiIndex``may be inverted (:issue:`10665`) - Bug in left and right``join`of with`MultiIndex``may be inverted (:issue:`10741`)  - Bug in``read\_stata`when reading a file with a different order set in`columns``(:issue:`10757`) - Bug in``Categorical`may not representing properly when category contains`tz`or`Period``(:issue:`10713`) - Bug in``Categorical.\_\_iter\_\_`may not returning correct`datetime`and`Period``(:issue:`10713`) - Bug in indexing with a``PeriodIndex`on an object with a`PeriodIndex``(:issue:`4125`) - Bug in``read\_csv`with`engine='c'``: EOF preceded by a comment, blank line, etc. was not handled correctly (:issue:`10728`, :issue:`10548`)  - Reading "famafrench" data via``DataReader``results in HTTP 404 error because of the website url is changed (:issue:`10591`). - Bug in``read\_msgpack``where DataFrame to decode has duplicate column names (:issue:`9618`) - Bug in``io.common.get\_filepath\_or\_buffer``which caused reading of valid S3 files to fail if the bucket also contained keys for which the user does not have read permission (:issue:`10604`) - Bug in vectorised setting of timestamp columns with python``datetime.date`and numpy`datetime64``(:issue:`10408`, :issue:`10412`) - Bug in``Index.take`may add unnecessary`freq``attribute (:issue:`10791`) - Bug in``merge`with empty`DataFrame`may raise`IndexError``(:issue:`10824`) - Bug in``to\_latex``where unexpected keyword argument for some documented arguments (:issue:`10888`) - Bug in indexing of large``DataFrame`where`IndexError``is uncaught (:issue:`10645` and :issue:`10692`) - Bug in``read\_csv`when using the`nrows`or`chunksize``parameters if file contains only a header line (:issue:`9535`) - Bug in serialization of``category``types in HDF5 in presence of alternate encodings. (:issue:`10366`) - Bug in``pd.DataFrame``when constructing an empty DataFrame with a string dtype (:issue:`9428`) - Bug in``pd.DataFrame.diff``when DataFrame is not consolidated (:issue:`10907`) - Bug in``pd.unique`for arrays with the`datetime64`or`timedelta64``dtype that meant an array with object dtype was returned instead the original dtype (:issue:`9431`) - Bug in``Timedelta``raising error when slicing from 0s (:issue:`10583`) - Bug in``DatetimeIndex.take`and`TimedeltaIndex.take`may not raise`IndexError``against invalid index (:issue:`10295`) - Bug in``Series(\[np.nan\]).astype('M8\[ms\]')`, which now returns`Series(\[pd.NaT\])``(:issue:`10747`) - Bug in``PeriodIndex.order``reset freq (:issue:`10295`) - Bug in``date\_range`when`freq`divides`end``as nanos (:issue:`10885`) - Bug in``iloc``allowing memory outside bounds of a Series to be accessed with negative integers (:issue:`10779`) - Bug in``read\_msgpack``where encoding is not respected (:issue:`10581`) - Bug preventing access to the first index when using``iloc``with a list containing the appropriate negative integer (:issue:`10547`, :issue:`10779`) - Bug in``TimedeltaIndex`formatter causing error while trying to save`DataFrame`with`TimedeltaIndex`using`to\_csv``(:issue:`10833`) - Bug in``DataFrame.where``when handling Series slicing (:issue:`10218`, :issue:`9558`) - Bug where``pd.read\_gbq`throws`ValueError``when Bigquery returns zero rows (:issue:`10273`) - Bug in``to\_json``which was causing segmentation fault when serializing 0-rank ndarray (:issue:`9576`) - Bug in plotting functions may raise``IndexError`when plotted on`GridSpec``(:issue:`10819`) - Bug in plot result may show unnecessary minor ticklabels (:issue:`10657`) - Bug in``groupby`incorrect computation for aggregation on`DataFrame`with`NaT`(E.g`first`,`last`,`min``). (:issue:`10590`, :issue:`11010`) - Bug when constructing``DataFrame``where passing a dictionary with only scalar values and specifying columns did not raise an error (:issue:`10856`) - Bug in``.var()``causing roundoff errors for highly similar values (:issue:`10242`) - Bug in``DataFrame.plot(subplots=True)``with duplicated columns outputs incorrect result (:issue:`10962`) - Bug in``Index``arithmetic may result in incorrect class (:issue:`10638`) - Bug in``date\_range``results in empty if freq is negative annually, quarterly and monthly (:issue:`11018`) - Bug in``DatetimeIndex``cannot infer negative freq (:issue:`11018`) - Remove use of some deprecated numpy comparison operations, mainly in tests. (:issue:`10569`) - Bug in``Index``dtype may not applied properly (:issue:`11017`) - Bug in``io.gbq``when testing for minimum google api client version (:issue:`10652`) - Bug in``DataFrame`construction from nested`dict`with`timedelta``keys (:issue:`11129`) - Bug in``.fillna`against may raise`TypeError``when data contains datetime dtype (:issue:`7095`, :issue:`11153`) - Bug in``.groupby``when number of keys to group by is same as length of index (:issue:`11185`) - Bug in``convert\_objects`where converted values might not be returned if all null and`coerce``(:issue:`9589`) - Bug in``convert\_objects`where`copy\`<span class="title-ref"> keyword was not respected (:issue:\`9589</span>)

## Contributors

<div class="contributors">

v0.16.2..v0.17.0

</div>

---

v0.17.1.md

---

# Version 0.17.1 (November 21, 2015)

{{ header }}

\> **Note** \> We are proud to announce that *pandas* has become a sponsored project of the ([NumFOCUS organization](http://www.numfocus.org/blog/numfocus-announces-new-fiscally-sponsored-project-pandas)). This will help ensure the success of development of *pandas* as a world-class open-source project.

This is a minor bug-fix release from 0.17.0 and includes a large number of bug fixes along several new features, enhancements, and performance improvements. We recommend that all users upgrade to this version.

Highlights include:

  - Support for Conditional HTML Formatting, see \[here \<whatsnew\_0171.style\>\](\#here-\<whatsnew\_0171.style\>)
  - Releasing the GIL on the csv reader & other ops, see \[here \<whatsnew\_0171.performance\>\](\#here-\<whatsnew\_0171.performance\>)
  - Fixed regression in `DataFrame.drop_duplicates` from 0.16.2, causing incorrect results on integer values (`11376`)

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.17.1

</div>

## New features

### Conditional HTML formatting

<div class="warning">

<div class="title">

Warning

</div>

This is a new feature and is under active development. We'll be adding features an possibly making breaking changes in future releases. Feedback is welcome in `11610`

</div>

We've added *experimental* support for conditional HTML formatting: the visual styling of a DataFrame based on the data. The styling is accomplished with HTML and CSS. Accesses the styler class with the <span class="title-ref">pandas.DataFrame.style</span>, attribute, an instance of <span class="title-ref">.Styler</span> with your data attached.

Here's a quick example:

> 
> 
> <div class="ipython">
> 
> python
> 
> np.random.seed(123) df = pd.DataFrame(np.random.randn(10, 5), columns=list("abcde")) html = df.style.background\_gradient(cmap="viridis", low=0.5)
> 
> </div>

We can render the HTML to get the following table.

<span class="title-ref">.Styler</span> interacts nicely with the Jupyter Notebook. See the \[documentation \</user\_guide/style.ipynb\>\](\#documentation-\</user\_guide/style.ipynb\>) for more.

## Enhancements

  - `DatetimeIndex` now supports conversion to strings with `astype(str)` (`10442`)

  - Support for `compression` (gzip/bz2) in <span class="title-ref">pandas.DataFrame.to\_csv</span> (`7615`)

  - `pd.read_*` functions can now also accept <span class="title-ref">python:pathlib.Path</span>, or <span class="title-ref">py:py.\_path.local.LocalPath</span> objects for the `filepath_or_buffer` argument. (`11033`)
    
      - The `DataFrame` and `Series` functions `.to_csv()`, `.to_html()` and `.to_latex()` can now handle paths beginning with tildes (e.g. `~/Documents/`) (`11438`)

  - `DataFrame` now uses the fields of a `namedtuple` as columns, if columns are not supplied (`11181`)

  - `DataFrame.itertuples()` now returns `namedtuple` objects, when possible. (`11269`, `11625`)

  - Added `axvlines_kwds` to parallel coordinates plot (`10709`)

  - Option to `.info()` and `.memory_usage()` to provide for deep introspection of memory consumption. Note that this can be expensive to compute and therefore is an optional parameter. (`11595`)
    
    <div class="ipython">
    
    python
    
    df = pd.DataFrame({"A": \["foo"\] \* 1000}) \# noqa: F821 df\["B"\] = df\["A"\].astype("category")
    
    \# shows the '+' as we have object dtypes df.info()
    
    \# we have an accurate memory assessment (but can be expensive to compute this) df.info(memory\_usage="deep")
    
    </div>

  - `Index` now has a `fillna` method (`10089`)
    
    <div class="ipython">
    
    python
    
    pd.Index(\[1, np.nan, 3\]).fillna(2)
    
    </div>

  - Series of type `category` now make `.str.<...>` and `.dt.<...>` accessor methods / properties available, if the categories are of that type. (`10661`)
    
    <div class="ipython">
    
    python
    
    s = pd.Series(list("aabb")).astype("category") s s.str.contains("a")
    
    date = pd.Series(pd.date\_range("1/1/2015", periods=5)).astype("category") date date.dt.day
    
    </div>

  - `pivot_table` now has a `margins_name` argument so you can use something other than the default of 'All' (`3335`)

  - Implement export of `datetime64[ns, tz]` dtypes with a fixed HDF5 store (`11411`)

  - Pretty printing sets (e.g. in DataFrame cells) now uses set literal syntax (`{x, y}`) instead of Legacy Python syntax (`set([x, y])`) (`11215`)

  - Improve the error message in <span class="title-ref">pandas.io.gbq.to\_gbq</span> when a streaming insert fails (`11285`) and when the DataFrame does not match the schema of the destination table (`11359`)

## API changes

  - raise `NotImplementedError` in `Index.shift` for non-supported index types (`8038`)
  - `min` and `max` reductions on `datetime64` and `timedelta64` dtyped series now result in `NaT` and not `nan` (`11245`).
  - Indexing with a null key will raise a `TypeError`, instead of a `ValueError` (`11356`)
  - `Series.ptp` will now ignore missing values by default (`11163`)

### Deprecations

  - The `pandas.io.ga` module which implements `google-analytics` support is deprecated and will be removed in a future version (`11308`)
  - Deprecate the `engine` keyword in `.to_csv()`, which will be removed in a future version (`11274`)

## Performance improvements

  - Checking monotonic-ness before sorting on an index (`11080`)
  - `Series.dropna` performance improvement when its dtype can't contain `NaN` (`11159`)
  - Release the GIL on most datetime field operations (e.g. `DatetimeIndex.year`, `Series.dt.year`), normalization, and conversion to and from `Period`, `DatetimeIndex.to_period` and `PeriodIndex.to_timestamp` (`11263`)
  - Release the GIL on some rolling algos: `rolling_median`, `rolling_mean`, `rolling_max`, `rolling_min`, `rolling_var`, `rolling_kurt`, `rolling_skew` (`11450`)
  - Release the GIL when reading and parsing text files in `read_csv`, `read_table` (`11272`)
  - Improved performance of `rolling_median` (`11450`)
  - Improved performance of `to_excel` (`11352`)
  - Performance bug in repr of `Categorical` categories, which was rendering the strings before chopping them for display (`11305`)
  - Performance improvement in `Categorical.remove_unused_categories`, (`11643`).
  - Improved performance of `Series` constructor with no data and `DatetimeIndex` (`11433`)
  - Improved performance of `shift`, `cumprod`, and `cumsum` with groupby (`4095`)

## Bug fixes

  - `SparseArray.__iter__()` now does not cause `PendingDeprecationWarning` in Python 3.5 (`11622`)
  - Regression from 0.16.2 for output formatting of long floats/nan, restored in (`11302`)
  - `Series.sort_index()` now correctly handles the `inplace` option (`11402`)
  - Incorrectly distributed .c file in the build on `PyPi` when reading a csv of floats and passing `na_values=<a scalar>` would show an exception (`11374`)
  - Bug in `.to_latex()` output broken when the index has a name (`10660`)
  - Bug in `HDFStore.append` with strings whose encoded length exceeded the max unencoded length (`11234`)
  - Bug in merging `datetime64[ns, tz]` dtypes (`11405`)
  - Bug in `HDFStore.select` when comparing with a numpy scalar in a where clause (`11283`)
  - Bug in using `DataFrame.ix` with a MultiIndex indexer (`11372`)
  - Bug in `date_range` with ambiguous endpoints (`11626`)
  - Prevent adding new attributes to the accessors `.str`, `.dt` and `.cat`. Retrieving such a value was not possible, so error out on setting it. (`10673`)
  - Bug in tz-conversions with an ambiguous time and `.dt` accessors (`11295`)
  - Bug in output formatting when using an index of ambiguous times (`11619`)
  - Bug in comparisons of Series vs list-likes (`11339`)
  - Bug in `DataFrame.replace` with a `datetime64[ns, tz]` and a non-compat to\_replace (`11326`, `11153`)
  - Bug in `isnull` where `numpy.datetime64('NaT')` in a `numpy.array` was not determined to be null(`11206`)
  - Bug in list-like indexing with a mixed-integer Index (`11320`)
  - Bug in `pivot_table` with `margins=True` when indexes are of `Categorical` dtype (`10993`)
  - Bug in `DataFrame.plot` cannot use hex strings colors (`10299`)
  - Regression in `DataFrame.drop_duplicates` from 0.16.2, causing incorrect results on integer values (`11376`)
  - Bug in `pd.eval` where unary ops in a list error (`11235`)
  - Bug in `squeeze()` with zero length arrays (`11230`, `8999`)
  - Bug in `describe()` dropping column names for hierarchical indexes (`11517`)
  - Bug in `DataFrame.pct_change()` not propagating `axis` keyword on `.fillna` method (`11150`)
  - Bug in `.to_csv()` when a mix of integer and string column names are passed as the `columns` parameter (`11637`)
  - Bug in indexing with a `range`, (`11652`)
  - Bug in inference of numpy scalars and preserving dtype when setting columns (`11638`)
  - Bug in `to_sql` using unicode column names giving UnicodeEncodeError with (`11431`).
  - Fix regression in setting of `xticks` in `plot` (`11529`).
  - Bug in `holiday.dates` where observance rules could not be applied to holiday and doc enhancement (`11477`, `11533`)
  - Fix plotting issues when having plain `Axes` instances instead of `SubplotAxes` (`11520`, `11556`).
  - Bug in `DataFrame.to_latex()` produces an extra rule when `header=False` (`7124`)
  - Bug in `df.groupby(...).apply(func)` when a func returns a `Series` containing a new datetimelike column (`11324`)
  - Bug in `pandas.json` when file to load is big (`11344`)
  - Bugs in `to_excel` with duplicate columns (`11007`, `10982`, `10970`)
  - Fixed a bug that prevented the construction of an empty series of dtype `datetime64[ns, tz]` (`11245`).
  - Bug in `read_excel` with MultiIndex containing integers (`11317`)
  - Bug in `to_excel` with openpyxl 2.2+ and merging (`11408`)
  - Bug in `DataFrame.to_dict()` produces a `np.datetime64` object instead of `Timestamp` when only datetime is present in data (`11327`)
  - Bug in `DataFrame.corr()` raises exception when computes Kendall correlation for DataFrames with boolean and not boolean columns (`11560`)
  - Bug in the link-time error caused by C `inline` functions on FreeBSD 10+ (with `clang`) (`10510`)
  - Bug in `DataFrame.to_csv` in passing through arguments for formatting `MultiIndexes`, including `date_format` (`7791`)
  - Bug in `DataFrame.join()` with `how='right'` producing a `TypeError` (`11519`)
  - Bug in `Series.quantile` with empty list results has `Index` with `object` dtype (`11588`)
  - Bug in `pd.merge` results in empty `Int64Index` rather than `Index(dtype=object)` when the merge result is empty (`11588`)
  - Bug in `Categorical.remove_unused_categories` when having `NaN` values (`11599`)
  - Bug in `DataFrame.to_sparse()` loses column names for MultiIndexes (`11600`)
  - Bug in `DataFrame.round()` with non-unique column index producing a Fatal Python error (`11611`)
  - Bug in `DataFrame.round()` with `decimals` being a non-unique indexed Series producing extra columns (`11618`)

## Contributors

<div class="contributors">

v0.17.0..v0.17.1

</div>

---

v0.18.0.md

---

# Version 0.18.0 (March 13, 2016)

{{ header }}

This is a major release from 0.17.1 and includes a small number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

\> **Warning** \> pandas \>= 0.18.0 no longer supports compatibility with Python version 2.6 and 3.3 (`7718`, `11273`)

<div class="warning">

<div class="title">

Warning

</div>

`numexpr` version 2.4.4 will now show a warning and not be used as a computation back-end for pandas because of some buggy behavior. This does not affect other versions (\>= 2.1 and \>= 2.4.6). (`12489`)

</div>

Highlights include:

  - Moving and expanding window functions are now methods on Series and DataFrame, similar to `.groupby`, see \[here \<whatsnew\_0180.enhancements.moments\>\](\#here-\<whatsnew\_0180.enhancements.moments\>).
  - Adding support for a `RangeIndex` as a specialized form of the `Int64Index` for memory savings, see \[here \<whatsnew\_0180.enhancements.rangeindex\>\](\#here-\<whatsnew\_0180.enhancements.rangeindex\>).
  - API breaking change to the `.resample` method to make it more `.groupby` like, see \[here \<whatsnew\_0180.breaking.resample\>\](\#here-\<whatsnew\_0180.breaking.resample\>).
  - Removal of support for positional indexing with floats, which was deprecated since 0.14.0. This will now raise a `TypeError`, see \[here \<whatsnew\_0180.float\_indexers\>\](\#here-\<whatsnew\_0180.float\_indexers\>).
  - The `.to_xarray()` function has been added for compatibility with the [xarray package](http://xarray.pydata.org/en/stable/), see \[here \<whatsnew\_0180.enhancements.xarray\>\](\#here-\<whatsnew\_0180.enhancements.xarray\>).
  - The `read_sas` function has been enhanced to read `sas7bdat` files, see \[here \<whatsnew\_0180.enhancements.sas\>\](\#here-\<whatsnew\_0180.enhancements.sas\>).
  - Addition of the \[.str.extractall() method \<whatsnew\_0180.enhancements.extract\>\](\#.str.extractall()-method-\<whatsnew\_0180.enhancements.extract\>), and API changes to the \[.str.extract() method \<whatsnew\_0180.enhancements.extract\>\](\#.str.extract()-method-\<whatsnew\_0180.enhancements.extract\>) and \[.str.cat() method \<whatsnew\_0180.enhancements.strcat\>\](\#.str.cat()-method-\<whatsnew\_0180.enhancements.strcat\>).
  - `pd.test()` top-level nose test runner is available (`4327`).

Check the \[API Changes \<whatsnew\_0180.api\_breaking\>\](\#api-changes-\<whatsnew\_0180.api\_breaking\>) and \[deprecations \<whatsnew\_0180.deprecations\>\](\#deprecations-\<whatsnew\_0180.deprecations\>) before updating.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.18.0

</div>

## New features

### Window functions are now methods

Window functions have been refactored to be methods on `Series/DataFrame` objects, rather than top-level functions, which are now deprecated. This allows these window-type functions, to have a similar API to that of `.groupby`. See the full documentation \[here \<window.overview\>\](\#here-\<window.overview\>) (`11603`, `12373`)

<div class="ipython">

python

np.random.seed(1234) df = pd.DataFrame({'A': range(10), 'B': np.random.randn(10)}) df

</div>

Previous behavior:

`` `ipython    In [8]: pd.rolling_mean(df, window=3)            FutureWarning: pd.rolling_mean is deprecated for DataFrame and will be removed in a future version, replace with                           DataFrame.rolling(window=3,center=False).mean()    Out[8]:        A         B    0 NaN       NaN    1 NaN       NaN    2   1  0.237722    3   2 -0.023640    4   3  0.133155    5   4 -0.048693    6   5  0.342054    7   6  0.370076    8   7  0.079587    9   8 -0.954504  New behavior:  .. ipython:: python     r = df.rolling(window=3)  These show a descriptive repr  .. ipython:: python     r ``\` with tab-completion of available methods and properties.

`` `ipython    In [9]: r.<TAB>  # noqa E225, E999    r.A           r.agg         r.apply       r.count       r.exclusions  r.max         r.median      r.name        r.skew        r.sum    r.B           r.aggregate   r.corr        r.cov         r.kurt        r.mean        r.min         r.quantile    r.std         r.var  The methods operate on the ``Rolling`object itself  .. ipython:: python     r.mean()  They provide getitem accessors  .. ipython:: python     r['A'].mean()  And multiple aggregations  .. ipython:: python     r.agg({'A': ['mean', 'std'],           'B': ['mean', 'std']})  .. _whatsnew_0180.enhancements.rename:  Changes to rename`\` ^^^^^^^^^^^^^^^^^

`Series.rename` and `NDFrame.rename_axis` can now take a scalar or list-like argument for altering the Series or axis *name*, in addition to their old behaviors of altering labels. (`9494`, `11965`)

<div class="ipython">

python

s = pd.Series(np.random.randn(5)) s.rename('newname')

</div>

<div class="ipython">

python

df = pd.DataFrame(np.random.randn(5, 2)) (df.rename\_axis("indexname") .rename\_axis("columns\_name", axis="columns"))

</div>

The new functionality works well in method chains. Previously these methods only accepted functions or dicts mapping a *label* to a new label. This continues to work as before for function or dict-like values.

### Range Index

A `RangeIndex` has been added to the `Int64Index` sub-classes to support a memory saving alternative for common use cases. This has a similar implementation to the python `range` object (`xrange` in python 2), in that it only stores the start, stop, and step values for the index. It will transparently interact with the user API, converting to `Int64Index` if needed.

This will now be the default constructed index for `NDFrame` objects, rather than previous an `Int64Index`. (`939`, `12070`, `12071`, `12109`, `12888`)

Previous behavior:

`` `ipython    In [3]: s = pd.Series(range(1000))     In [4]: s.index    Out[4]:    Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,                ...                990, 991, 992, 993, 994, 995, 996, 997, 998, 999], dtype='int64', length=1000)     In [6]: s.index.nbytes    Out[6]: 8000   New behavior:  .. ipython:: python     s = pd.Series(range(1000))    s.index    s.index.nbytes  .. _whatsnew_0180.enhancements.extract:  Changes to str.extract ``\` ^^^^^^^^^^^^^^^^^^^^^^

The \[.str.extract \<text.extract\>\](\#.str.extract-\<text.extract\>) method takes a regular expression with capture groups, finds the first match in each subject string, and returns the contents of the capture groups (`11386`).

In v0.18.0, the `expand` argument was added to `extract`.

  - `expand=False`: it returns a `Series`, `Index`, or `DataFrame`, depending on the subject and regular expression pattern (same behavior as pre-0.18.0).
  - `expand=True`: it always returns a `DataFrame`, which is more consistent and less confusing from the perspective of a user.

Currently the default is `expand=None` which gives a `FutureWarning` and uses `expand=False`. To avoid this warning, please explicitly specify `expand`.

`` `ipython    In [1]: pd.Series(['a1', 'b2', 'c3']).str.extract(r'[ab](\d)', expand=None)    FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame)    but in a future version of pandas this will be changed to expand=True (return DataFrame)     Out[1]:    0      1    1      2    2    NaN    dtype: object  Extracting a regular expression with one group returns a Series if ``<span class="title-ref"> </span><span class="title-ref">expand=False</span>\`.

<div class="ipython">

python

pd.Series(\['a1', 'b2', 'c3'\]).str.extract(r'\[ab\](d)', expand=False)

</div>

It returns a `DataFrame` with one column if `expand=True`.

<div class="ipython">

python

pd.Series(\['a1', 'b2', 'c3'\]).str.extract(r'\[ab\](d)', expand=True)

</div>

Calling on an `Index` with a regex with exactly one capture group returns an `Index` if `expand=False`.

<div class="ipython">

python

s = pd.Series(\["a1", "b2", "c3"\], \["A11", "B22", "C33"\]) s.index s.index.str.extract("(?P\<letter\>\[a-zA-Z\])", expand=False)

</div>

It returns a `DataFrame` with one column if `expand=True`.

<div class="ipython">

python

s.index.str.extract("(?P\<letter\>\[a-zA-Z\])", expand=True)

</div>

Calling on an `Index` with a regex with more than one capture group raises `ValueError` if `expand=False`.

`` `python     >>> s.index.str.extract("(?P<letter>[a-zA-Z])([0-9]+)", expand=False)     ValueError: only one regex group is supported with Index  It returns a ``DataFrame`if`expand=True`.  .. ipython:: python     s.index.str.extract("(?P<letter>[a-zA-Z])([0-9]+)", expand=True)  In summary,`extract(expand=True)`always returns a`DataFrame`  `\` with a row for every subject string, and a column for every capture group.

### Addition of str.extractall

The \[.str.extractall \<text.extractall\>\](\#.str.extractall-\<text.extractall\>) method was added (`11386`). Unlike `extract`, which returns only the first match.

<div class="ipython">

python

s = pd.Series(\["a1a2", "b1", "c1"\], \["A", "B", "C"\]) s s.str.extract(r"(?P\<letter\>\[ab\])(?P\<digit\>d)", expand=False)

</div>

The `extractall` method returns all matches.

<div class="ipython">

python

s.str.extractall(r"(?P\<letter\>\[ab\])(?P\<digit\>d)")

</div>

### Changes to str.cat

The method `.str.cat()` concatenates the members of a `Series`. Before, if `NaN` values were present in the Series, calling `.str.cat()` on it would return `NaN`, unlike the rest of the `Series.str.*` API. This behavior has been amended to ignore `NaN` values by default. (`11435`).

A new, friendlier `ValueError` is added to protect against the mistake of supplying the `sep` as an arg, rather than as a kwarg. (`11334`).

<div class="ipython">

python

pd.Series(\['a', 'b', np.nan, 'c'\]).str.cat(sep=' ') pd.Series(\['a', 'b', np.nan, 'c'\]).str.cat(sep=' ', na\_rep='?')

</div>

`` `ipython     In [2]: pd.Series(['a', 'b', np.nan, 'c']).str.cat(' ')     ValueError: Did you mean to supply a ``sep`keyword?   .. _whatsnew_0180.enhancements.rounding:  Datetimelike rounding`\` ^^^^^^^^^^^^^^^^^^^^^

`DatetimeIndex`, `Timestamp`, `TimedeltaIndex`, `Timedelta` have gained the `.round()`, `.floor()` and `.ceil()` method for datetimelike rounding, flooring and ceiling. (`4314`, `11963`)

Naive datetimes

<div class="ipython">

python

dr = pd.date\_range('20130101 09:12:56.1234', periods=3) dr dr.round('s')

\# Timestamp scalar dr\[0\] dr\[0\].round('10s')

</div>

Tz-aware are rounded, floored and ceiled in local times

<div class="ipython">

python

dr = dr.tz\_localize('US/Eastern') dr dr.round('s')

</div>

Timedeltas

`` `ipython    In [37]: t = pd.timedelta_range('1 days 2 hr 13 min 45 us', periods=3, freq='d')     In [38]: t    Out[38]:    TimedeltaIndex(['1 days 02:13:00.000045', '2 days 02:13:00.000045',                    '3 days 02:13:00.000045'],                   dtype='timedelta64[ns]', freq='D')     In [39]: t.round('10min')    Out[39]:    TimedeltaIndex(['1 days 02:10:00', '2 days 02:10:00',                    '3 days 02:10:00'],                   dtype='timedelta64[ns]', freq=None)     # Timedelta scalar    In [40]: t[0]    Out[40]: Timedelta('1 days 02:13:00.000045')     In [41]: t[0].round('2h')    Out[41]: Timedelta('1 days 02:00:00')   In addition, ``.round()`,`.floor()`and`.ceil()`will be available through the`.dt`accessor of`Series`.  .. ipython:: python     s = pd.Series(dr)    s    s.dt.round('D')  Formatting of integers in FloatIndex`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Integers in `FloatIndex`, e.g. 1., are now formatted with a decimal point and a `0` digit, e.g. `1.0` (`11713`) This change not only affects the display to the console, but also the output of IO methods like `.to_csv` or `.to_html`.

Previous behavior:

`` `ipython    In [2]: s = pd.Series([1, 2, 3], index=np.arange(3.))     In [3]: s    Out[3]:    0    1    1    2    2    3    dtype: int64     In [4]: s.index    Out[4]: Float64Index([0.0, 1.0, 2.0], dtype='float64')     In [5]: print(s.to_csv(path=None))    0,1    1,2    2,3   New behavior:  .. ipython:: python     s = pd.Series([1, 2, 3], index=np.arange(3.))    s    s.index    print(s.to_csv(path_or_buf=None, header=False))  Changes to dtype assignment behaviors ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When a DataFrame's slice is updated with a new slice of the same dtype, the dtype of the DataFrame will now remain the same. (`10503`)

Previous behavior:

`` `ipython    In [5]: df = pd.DataFrame({'a': [0, 1, 1],                               'b': pd.Series([100, 200, 300], dtype='uint32')})     In [7]: df.dtypes    Out[7]:    a     int64    b    uint32    dtype: object     In [8]: ix = df['a'] == 1     In [9]: df.loc[ix, 'b'] = df.loc[ix, 'b']     In [11]: df.dtypes    Out[11]:    a    int64    b    int64    dtype: object  New behavior:  .. ipython:: python     df = pd.DataFrame({'a': [0, 1, 1],                       'b': pd.Series([100, 200, 300], dtype='uint32')})    df.dtypes    ix = df['a'] == 1    df.loc[ix, 'b'] = df.loc[ix, 'b']    df.dtypes  When a DataFrame's integer slice is partially updated with a new slice of floats that could potentially be down-casted to integer without losing precision, the dtype of the slice will be set to float instead of integer.  Previous behavior:  .. code-block:: ipython     In [4]: df = pd.DataFrame(np.array(range(1,10)).reshape(3,3),                              columns=list('abc'),                              index=[[4,4,8], [8,10,12]])     In [5]: df    Out[5]:          a  b  c    4 8   1  2  3      10  4  5  6    8 12  7  8  9     In [7]: df.ix[4, 'c'] = np.array([0., 1.])     In [8]: df    Out[8]:          a  b  c    4 8   1  2  0      10  4  5  1    8 12  7  8  9  New behavior:  .. ipython:: python     df = pd.DataFrame(np.array(range(1,10)).reshape(3,3),                      columns=list('abc'),                      index=[[4,4,8], [8,10,12]])    df    df.loc[4, 'c'] = np.array([0., 1.])    df  .. _whatsnew_0180.enhancements.xarray:  Method to_xarray ``\` ^^^^^^^^^^^^^^^^

In a future version of pandas, we will be deprecating `Panel` and other \> 2 ndim objects. In order to provide for continuity, all `NDFrame` objects have gained the `.to_xarray()` method in order to convert to `xarray` objects, which has a pandas-like interface for \> 2 ndim. (`11972`)

See the [xarray full-documentation here](http://xarray.pydata.org/en/stable/).

`` `ipython    In [1]: p = Panel(np.arange(2*3*4).reshape(2,3,4))     In [2]: p.to_xarray()    Out[2]:    <xarray.DataArray (items: 2, major_axis: 3, minor_axis: 4)>    array([[[ 0,  1,  2,  3],            [ 4,  5,  6,  7],            [ 8,  9, 10, 11]],            [[12, 13, 14, 15],            [16, 17, 18, 19],            [20, 21, 22, 23]]])    Coordinates:      * items       (items) int64 0 1      * major_axis  (major_axis) int64 0 1 2      * minor_axis  (minor_axis) int64 0 1 2 3  Latex representation ``\` ^^^^^^^^^^^^^^^^^^^^

`DataFrame` has gained a `._repr_latex_()` method in order to allow for conversion to latex in a ipython/jupyter notebook using nbconvert. (`11778`)

Note that this must be activated by setting the option `pd.display.latex.repr=True` (`12182`)

For example, if you have a jupyter notebook you plan to convert to latex using nbconvert, place the statement `pd.display.latex.repr=True` in the first cell to have the contained DataFrame output also stored as latex.

The options `display.latex.escape` and `display.latex.longtable` have also been added to the configuration and are used automatically by the `to_latex` method. See the \[available options docs \<options.available\>\](\#available-options-docs-\<options.available\>) for more info.

### `pd.read_sas()` changes

`read_sas` has gained the ability to read SAS7BDAT files, including compressed files. The files can be read in entirety, or incrementally. For full details see \[here \<io.sas\>\](\#here-\<io.sas\>). (`4052`)

### Other enhancements

  - Handle truncated floats in SAS xport files (`11713`)
  - Added option to hide index in `Series.to_string` (`11729`)
  - `read_excel` now supports s3 urls of the format `s3://bucketname/filename` (`11447`)
  - add support for `AWS_S3_HOST` env variable when reading from s3 (`12198`)
  - A simple version of `Panel.round()` is now implemented (`11763`)
  - For Python 3.x, `round(DataFrame)`, `round(Series)`, `round(Panel)` will work (`11763`)
  - `sys.getsizeof(obj)` returns the memory usage of a pandas object, including the values it contains (`11597`)
  - `Series` gained an `is_unique` attribute (`11946`)
  - `DataFrame.quantile` and `Series.quantile` now accept `interpolation` keyword (`10174`).
  - Added `DataFrame.style.format` for more flexible formatting of cell values (`11692`)
  - `DataFrame.select_dtypes` now allows the `np.float16` type code (`11990`)
  - `pivot_table()` now accepts most iterables for the `values` parameter (`12017`)
  - Added Google `BigQuery` service account authentication support, which enables authentication on remote servers. (`11881`, `12572`). For further details see [here](https://pandas-gbq.readthedocs.io/en/latest/intro.html)
  - `HDFStore` is now iterable: `for k in store` is equivalent to `for k in store.keys()` (`12221`).
  - Add missing methods/fields to `.dt` for `Period` (`8848`)
  - The entire code base has been `PEP`-ified (`12096`)

## Backwards incompatible API changes

  - the leading white spaces have been removed from the output of `.to_string(index=False)` method (`11833`)
  - the `out` parameter has been removed from the `Series.round()` method. (`11763`)
  - `DataFrame.round()` leaves non-numeric columns unchanged in its return, rather than raises. (`11885`)
  - `DataFrame.head(0)` and `DataFrame.tail(0)` return empty frames, rather than `self`. (`11937`)
  - `Series.head(0)` and `Series.tail(0)` return empty series, rather than `self`. (`11937`)
  - `to_msgpack` and `read_msgpack` encoding now defaults to `'utf-8'`. (`12170`)
  - the order of keyword arguments to text file parsing functions (`.read_csv()`, `.read_table()`, `.read_fwf()`) changed to group related arguments. (`11555`)
  - `NaTType.isoformat` now returns the string `'NaT` to allow the result to be passed to the constructor of `Timestamp`. (`12300`)

### NaT and Timedelta operations

`NaT` and `Timedelta` have expanded arithmetic operations, which are extended to `Series` arithmetic where applicable. Operations defined for `datetime64[ns]` or `timedelta64[ns]` are now also defined for `NaT` (`11564`).

`NaT` now supports arithmetic operations with integers and floats.

<div class="ipython">

python

pd.NaT \* 1 pd.NaT \* 1.5 pd.NaT / 2 pd.NaT \* np.nan

</div>

`NaT` defines more arithmetic operations with `datetime64[ns]` and `timedelta64[ns]`.

<div class="ipython">

python

pd.NaT / pd.NaT pd.Timedelta('1s') / pd.NaT

</div>

`NaT` may represent either a `datetime64[ns]` null or a `timedelta64[ns]` null. Given the ambiguity, it is treated as a `timedelta64[ns]`, which allows more operations to succeed.

<div class="ipython">

python

pd.NaT + pd.NaT

\# same as pd.Timedelta('1s') + pd.Timedelta('1s')

</div>

as opposed to

`` `ipython    In [3]: pd.Timestamp('19900315') + pd.Timestamp('19900315')    TypeError: unsupported operand type(s) for +: 'Timestamp' and 'Timestamp'  However, when wrapped in a ``Series`whose`dtype`is`datetime64\[ns\]`or`timedelta64\[ns\]`,`<span class="title-ref"> the </span><span class="title-ref">dtype</span>\` information is respected.

`` `ipython    In [1]: pd.Series([pd.NaT], dtype='<M8[ns]') + pd.Series([pd.NaT], dtype='<M8[ns]')    TypeError: can only operate on a datetimes for subtraction,               but the operator [__add__] was passed  .. ipython:: python     pd.Series([pd.NaT], dtype='<m8[ns]') + pd.Series([pd.NaT], dtype='<m8[ns]') ``Timedelta`division by`floats`now works.  .. ipython:: python     pd.Timedelta('1s') / 2.0  Subtraction by`Timedelta`in a`Series`by a`Timestamp``works (:issue:`11925`)  .. ipython:: python     ser = pd.Series(pd.timedelta_range('1 day', periods=3))    ser    pd.Timestamp('2012-01-01') - ser``NaT.isoformat()`now returns`'NaT'`. This change allows`<span class="title-ref"> </span><span class="title-ref">pd.Timestamp</span><span class="title-ref"> to rehydrate any timestamp like object from its isoformat (:issue:\`12300</span>).

### Changes to msgpack

Forward incompatible changes in `msgpack` writing format were made over 0.17.0 and 0.18.0; older versions of pandas cannot read files packed by newer versions (`12129`, `10527`)

Bugs in `to_msgpack` and `read_msgpack` introduced in 0.17.0 and fixed in 0.18.0, caused files packed in Python 2 unreadable by Python 3 (`12142`). The following table describes the backward and forward compat of msgpacks.

\> **Warning** \> +----------------------+------------------------+ | Packed with | Can be unpacked with | +======================+========================+ | pre-0.17 / Python 2 | any | +----------------------+------------------------+ | pre-0.17 / Python 3 | any | +----------------------+------------------------+ | 0.17 / Python 2 | - ==0.17 / Python 2 | | | - \>=0.18 / any Python | +----------------------+------------------------+ | 0.17 / Python 3 | \>=0.18 / any Python | +----------------------+------------------------+ | 0.18 | \>= 0.18 | +----------------------+------------------------+

> 0.18.0 is backward-compatible for reading files packed by older versions, except for files packed with 0.17 in Python 2, in which case only they can only be unpacked in Python 2.

### Signature change for .rank

`Series.rank` and `DataFrame.rank` now have the same signature (`11759`)

Previous signature

`` `ipython    In [3]: pd.Series([0,1]).rank(method='average', na_option='keep',                                  ascending=True, pct=False)    Out[3]:    0    1    1    2    dtype: float64     In [4]: pd.DataFrame([0,1]).rank(axis=0, numeric_only=None,                                     method='average', na_option='keep',                                     ascending=True, pct=False)    Out[4]:       0    0  1    1  2  New signature  .. ipython:: python     pd.Series([0,1]).rank(axis=0, method='average', numeric_only=False,                          na_option='keep', ascending=True, pct=False)    pd.DataFrame([0,1]).rank(axis=0, method='average', numeric_only=False,                             na_option='keep', ascending=True, pct=False)   Bug in QuarterBegin with n=0 ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions, the behavior of the QuarterBegin offset was inconsistent depending on the date when the `n` parameter was 0. (`11406`)

The general semantics of anchored offsets for `n=0` is to not move the date when it is an anchor point (e.g., a quarter start date), and otherwise roll forward to the next anchor point.

<div class="ipython">

python

d = pd.Timestamp('2014-02-01') d d + pd.offsets.QuarterBegin(n=0, startingMonth=2) d + pd.offsets.QuarterBegin(n=0, startingMonth=1)

</div>

For the `QuarterBegin` offset in previous versions, the date would be rolled *backwards* if date was in the same month as the quarter start date.

`` `ipython    In [3]: d = pd.Timestamp('2014-02-15')     In [4]: d + pd.offsets.QuarterBegin(n=0, startingMonth=2)    Out[4]: Timestamp('2014-02-01 00:00:00')  This behavior has been corrected in version 0.18.0, which is consistent with ``<span class="title-ref"> other anchored offsets like </span><span class="title-ref">MonthBegin</span><span class="title-ref"> and </span><span class="title-ref">YearBegin</span>\`.

<div class="ipython">

python

d = pd.Timestamp('2014-02-15') d + pd.offsets.QuarterBegin(n=0, startingMonth=2)

</div>

### Resample API

Like the change in the window functions API \[above \<whatsnew\_0180.enhancements.moments\>\](\#above-\<whatsnew\_0180.enhancements.moments\>), `.resample(...)` is changing to have a more groupby-like API. (`11732`, `12702`, `12202`, `12332`, `12334`, `12348`, `12448`).

<div class="ipython">

python

np.random.seed(1234) df = pd.DataFrame(np.random.rand(10,4), columns=list('ABCD'), index=pd.date\_range('2010-01-01 09:00:00', periods=10, freq='s')) df

</div>

**Previous API**:

You would write a resampling operation that immediately evaluates. If a `how` parameter was not provided, it would default to `how='mean'`.

`` `ipython    In [6]: df.resample('2s')    Out[6]:                             A         B         C         D    2010-01-01 09:00:00  0.485748  0.447351  0.357096  0.793615    2010-01-01 09:00:02  0.820801  0.794317  0.364034  0.531096    2010-01-01 09:00:04  0.433985  0.314582  0.424104  0.625733    2010-01-01 09:00:06  0.624988  0.609738  0.633165  0.612452    2010-01-01 09:00:08  0.510470  0.534317  0.573201  0.806949  You could also specify a ``how`directly  .. code-block:: ipython     In [7]: df.resample('2s', how='sum')    Out[7]:                             A         B         C         D    2010-01-01 09:00:00  0.971495  0.894701  0.714192  1.587231    2010-01-01 09:00:02  1.641602  1.588635  0.728068  1.062191    2010-01-01 09:00:04  0.867969  0.629165  0.848208  1.251465    2010-01-01 09:00:06  1.249976  1.219477  1.266330  1.224904    2010-01-01 09:00:08  1.020940  1.068634  1.146402  1.613897  **New API**:  Now, you can write`.resample(..)`as a 2-stage operation like`.groupby(...)`, which`<span class="title-ref"> yields a </span><span class="title-ref">Resampler</span>\`.

<div class="ipython" data-okwarning="">

python

r = df.resample('2s') r

</div>

#### Downsampling

You can then use this object to perform operations. These are downsampling operations (going from a higher frequency to a lower one).

<div class="ipython">

python

r.mean()

</div>

<div class="ipython">

python

r.sum()

</div>

Furthermore, resample now supports `getitem` operations to perform the resample on specific columns.

<div class="ipython">

python

r\[\['A','C'\]\].mean()

</div>

and `.aggregate` type operations.

<div class="ipython">

python

r.agg({'A' : 'mean', 'B' : 'sum'})

</div>

These accessors can of course, be combined

<div class="ipython">

python

r\[\['A','B'\]\].agg(\['mean','sum'\])

</div>

#### Upsampling

<div class="currentmodule">

pandas.tseries.resample

</div>

Upsampling operations take you from a lower frequency to a higher frequency. These are now performed with the `Resampler` objects with <span class="title-ref">\~Resampler.backfill</span>, <span class="title-ref">\~Resampler.ffill</span>, <span class="title-ref">\~Resampler.fillna</span> and <span class="title-ref">\~Resampler.asfreq</span> methods.

`` `ipython    In [89]: s = pd.Series(np.arange(5, dtype='int64'),                  index=pd.date_range('2010-01-01', periods=5, freq='Q'))     In [90]: s    Out[90]:    2010-03-31    0    2010-06-30    1    2010-09-30    2    2010-12-31    3    2011-03-31    4    Freq: Q-DEC, Length: 5, dtype: int64  Previously  .. code-block:: ipython     In [6]: s.resample('M', fill_method='ffill')    Out[6]:    2010-03-31    0    2010-04-30    0    2010-05-31    0    2010-06-30    1    2010-07-31    1    2010-08-31    1    2010-09-30    2    2010-10-31    2    2010-11-30    2    2010-12-31    3    2011-01-31    3    2011-02-28    3    2011-03-31    4    Freq: M, dtype: int64  New API  .. code-block:: ipython     In [91]: s.resample('M').ffill()    Out[91]:    2010-03-31    0    2010-04-30    0    2010-05-31    0    2010-06-30    1    2010-07-31    1    2010-08-31    1    2010-09-30    2    2010-10-31    2    2010-11-30    2    2010-12-31    3    2011-01-31    3    2011-02-28    3    2011-03-31    4    Freq: M, Length: 13, dtype: int64  > **Note** >     In the new API, you can either downsample OR upsample. The prior implementation would allow you to pass an aggregator function (like ``mean`) even though you were upsampling, providing a bit of confusion.  Previous API will work but with deprecations`\` """"""""""""""""""""""""""""""""""""""""""""

\> **Warning** \> This new API for resample includes some internal changes for the prior-to-0.18.0 API, to work with a deprecation warning in most cases, as the resample operation returns a deferred object. We can intercept operations and just do what the (pre 0.18.0) API did (with a warning). Here is a typical use case:

> `` `ipython    In [4]: r = df.resample('2s')     In [6]: r*10    pandas/tseries/resample.py:80: FutureWarning: .resample() is now a deferred operation    use .resample(...).mean() instead of .resample(...)     Out[6]:                          A         B         C         D    2010-01-01 09:00:00  4.857476  4.473507  3.570960  7.936154    2010-01-01 09:00:02  8.208011  7.943173  3.640340  5.310957    2010-01-01 09:00:04  4.339846  3.145823  4.241039  6.257326    2010-01-01 09:00:06  6.249881  6.097384  6.331650  6.124518    2010-01-01 09:00:08  5.104699  5.343172  5.732009  8.069486  However, getting and assignment operations directly on a ``Resampler`will raise a`ValueError`:  .. code-block:: ipython     In [7]: r.iloc[0] = 5    ValueError: .resample() is now a deferred operation    use .resample(...).mean() instead of .resample(...)  There is a situation where the new API can not perform all the operations when using original code. This code is intending to resample every 2s, take the`mean`AND then take the`min\`\` of those results.
> 
> ``` ipython
> In [4]: df.resample('2s').min()
> Out[4]:
> A    0.433985
> B    0.314582
> C    0.357096
> D    0.531096
> dtype: float64
> ```
> 
> The new API will:
> 
> <div class="ipython">
> 
> python
> 
> df.resample('2s').min()
> 
> </div>
> 
> The good news is the return dimensions will differ between the new API and the old API, so this should loudly raise an exception.
> 
> To replicate the original operation
> 
> <div class="ipython">
> 
> python
> 
> df.resample('2s').mean().min()
> 
> </div>

Changes to eval `` ` ^^^^^^^^^^^^^^^  In prior versions, new columns assignments in an ``eval`expression resulted in an inplace change to the`DataFrame``. (:issue:`9297`, :issue:`8664`, :issue:`10486`)  .. ipython:: python     df = pd.DataFrame({'a': np.linspace(0, 10, 5), 'b': range(5)})    df  .. ipython:: python    :suppress:     df.eval('c = a + b', inplace=True)``\`ipython In \[12\]: df.eval('c = a + b') FutureWarning: eval expressions containing an assignment currentlydefault to operating inplace. This will change in a future version of pandas, use inplace=True to avoid this warning.

> In \[13\]: df Out\[13\]: a b c 0 0.0 0 0.0 1 2.5 1 3.5 2 5.0 2 7.0 3 7.5 3 10.5 4 10.0 4 14.0

In version 0.18.0, a new `inplace` keyword was added to choose whether the `` ` assignment should be done inplace or return a copy.  .. ipython:: python     df    df.eval('d = c - b', inplace=False)    df    df.eval('d = c - b', inplace=True)    df  > **Warning** >     For backwards compatibility, ``inplace`defaults to`True`if not specified.    This will change in a future version of pandas. If your code depends on an    inplace assignment you should update to explicitly set`inplace=True`The`inplace`keyword parameter was also added the`query`method.  .. ipython:: python     df.query('a > 5')    df.query('a > 5', inplace=True)    df  .. warning::     Note that the default value for`inplace`in a`query`is`False`, which is consistent with prior versions.`eval`has also been updated to allow multi-line expressions for multiple assignments.  These expressions will be evaluated one at a time in order.  Only assignments are valid for multi-line expressions.  .. ipython:: python     df    df.eval("""    e = d + a    f = e - 22    g = f / 2.0""", inplace=True)    df   .. _whatsnew_0180.api:  Other API changes ^^^^^^^^^^^^^^^^^ -`DataFrame.between\_time`and`Series.between\_time`now only parse a fixed set of time strings. Parsing of date strings is no longer supported and raises a`ValueError``. (:issue:`11818`)``\`ipython In \[107\]: s = pd.Series(range(10), pd.date\_range('2015-01-01', freq='H', periods=10))

> In \[108\]: s.between\_time("7:00am", "9:00am") Out\[108\]: 2015-01-01 07:00:00 7 2015-01-01 08:00:00 8 2015-01-01 09:00:00 9 Freq: H, Length: 3, dtype: int64
> 
> This will now raise.
> 
> ``` ipython
> ```
> 
> In \[2\]: s.between\_time('20150101 07:00:00','20150101 09:00:00') ValueError: Cannot convert arg \['20150101 07:00:00'\] to a time.

\- `.memory_usage()` now includes values in the index, as does memory\_usage in `.info()` (`11597`) `` ` - ``DataFrame.to\_latex()`now supports non-ascii encodings (eg`utf-8`) in Python 2 with the parameter`encoding``(:issue:`7061`) -``pandas.merge()`and`DataFrame.merge()`will show a specific error message when trying to merge with an object that is not of type`DataFrame``or a subclass (:issue:`12081`) -``DataFrame.unstack`and`Series.unstack`now take`fill\_value`keyword to allow direct replacement of missing values when an unstack results in missing values in the resulting`DataFrame`. As an added benefit, specifying`fill\_value``will preserve the data type of the original stacked data.  (:issue:`9746`) - As part of the new API for [window functions <whatsnew_0180.enhancements.moments>](#window-functions-<whatsnew_0180.enhancements.moments>) and [resampling <whatsnew_0180.breaking.resample>](#resampling-<whatsnew_0180.breaking.resample>), aggregation functions have been clarified, raising more informative error messages on invalid aggregations. (:issue:`9052`). A full set of examples are presented in [groupby <groupby.aggregate>](#groupby-<groupby.aggregate>). - Statistical functions for``NDFrame`objects (like`sum(), mean(), min()`) will now raise if non-numpy-compatible arguments are passed in for`\*\*kwargs``(:issue:`12301`) -``.to\_latex`and`.to\_html`gain a`decimal`parameter like`.to\_csv`; the default is`'.'``(:issue:`12031`) - More helpful error message when constructing a``DataFrame``with empty data but with indices (:issue:`8020`) -``.describe()``will now properly handle bool dtype as a categorical (:issue:`6625`) - More helpful error message with an invalid``.transform``with user defined input (:issue:`10165`) - Exponentially weighted functions now allow specifying alpha directly (:issue:`10789`) and raise``ValueError`if parameters violate`0 \< alpha \<= 1``(:issue:`12492`)  .. _whatsnew_0180.deprecations:  Deprecations ^^^^^^^^^^^^  .. _whatsnew_0180.window_deprecations:  - The functions``[pd.rolling]()*\`\`, \`\`pd.expanding\_*`, and`pd.ewm\*``are deprecated and replaced by the corresponding method call. Note that   the new suggested syntax includes all of the arguments (even if default) (:issue:`11603`)``\`ipython In \[1\]: s = pd.Series(range(3))

>   - In \[2\]: pd.rolling\_mean(s,window=2,min\_periods=1)
>     
>       - FutureWarning: pd.rolling\_mean is deprecated for Series and  
>         will be removed in a future version, replace with Series.rolling(min\_periods=1,window=2,center=False).mean()
> 
>   - Out\[2\]:  
>     0 0.0 1 0.5 2 1.5 dtype: float64
> 
>   - In \[3\]: pd.rolling\_cov(s, s, window=2)
>     
>       - FutureWarning: pd.rolling\_cov is deprecated for Series and  
>         will be removed in a future version, replace with Series.rolling(window=2).cov(other=\<Series\>)
> 
>   - Out\[3\]:  
>     0 NaN 1 0.5 2 0.5 dtype: float64

  - The `freq` and `how` arguments to the `.rolling`, `.expanding`, and `.ewm` (new) functions are deprecated, and will be removed in a future version. You can simply resample the input prior to creating a window function. (`11603`).
    
    For example, instead of `s.rolling(window=5,freq='D').max()` to get the max value on a rolling 5 Day window, one could use `s.resample('D').mean().rolling(window=5).max()`, which first resamples the data to daily data, then provides a rolling 5 day window.

\- `pd.tseries.frequencies.get_offset_name` function is deprecated. Use offset's `.freqstr` property as alternative (`11192`) `` ` - ``pandas.stats.fama\_macbeth``routines are deprecated and will be removed in a future version (:issue:`6077`) -``pandas.stats.ols`,`pandas.stats.plm`and`pandas.stats.var``routines are deprecated and will be removed in a future version (:issue:`6077`) - show a``FutureWarning`rather than a`DeprecationWarning`on using long-time deprecated syntax in`HDFStore.select`, where the`where``clause is not a string-like (:issue:`12027`)  - The``pandas.options.display.mpl\_style``configuration has been deprecated   and will be removed in a future version of pandas. This functionality   is better handled by matplotlib's `style sheets`_ (:issue:`11783`).     .. _whatsnew_0180.float_indexers:  Removal of deprecated float indexers ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In :issue:`4892` indexing with floating point numbers on a non-``Float64Index`was deprecated (in version 0.14.0). In 0.18.0, this deprecation warning is removed and these will now raise a`TypeError``. (:issue:`12165`, :issue:`12333`)  .. ipython:: python     s = pd.Series([1, 2, 3], index=[4, 5, 6])    s    s2 = pd.Series([1, 2, 3], index=list('abc'))    s2  Previous behavior:``\`ipython \# this is label indexing In \[2\]: s\[5.0\] FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point Out\[2\]: 2

> \# this is positional indexing In \[3\]: s.iloc\[1.0\] FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point Out\[3\]: 2
> 
> \# this is label indexing In \[4\]: s.loc\[5.0\] FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point Out\[4\]: 2
> 
> \# .ix would coerce 1.0 to the positional 1, and index In \[5\]: s2.ix\[1.0\] = 10 FutureWarning: scalar indexers for index type Index should be integers and not floating point
> 
> In \[6\]: s2 Out\[6\]: a 1 b 10 c 3 dtype: int64

New behavior:

For iloc, getting & setting via a float scalar will always raise.

``` ipython
In [3]: s.iloc[2.0]
TypeError: cannot do label indexing on <class 'pandas.indexes.numeric.Int64Index'> with these indexers [2.0] of <type 'float'>
```

Other indexers will coerce to a like integer for both getting and setting. The `FutureWarning` has been dropped for `.loc`, `.ix` and `[]`.

<div class="ipython">

python

s\[5.0\] s.loc\[5.0\]

</div>

and setting

<div class="ipython">

python

s\_copy = s.copy() s\_copy\[5.0\] = 10 s\_copy s\_copy = s.copy() s\_copy.loc\[5.0\] = 10 s\_copy

</div>

Positional setting with `.ix` and a float indexer will ADD this value to the index, rather than previously setting the value by position.

``` ipython
In [3]: s2.ix[1.0] = 10
In [4]: s2
Out[4]:
a       1
b       2
c       3
1.0    10
dtype: int64
```

Slicing will also coerce integer-like floats to integers for a non-`Float64Index`.

<div class="ipython">

python

s.loc\[5.0:6\]

</div>

Note that for floats that are NOT coercible to ints, the label based bounds will be excluded

<div class="ipython">

python

s.loc\[5.1:6\]

</div>

Float indexing on a `Float64Index` is unchanged.

<div class="ipython">

python

s = pd.Series(\[1, 2, 3\], index=np.arange(3.)) s\[1.0\] s\[1.0:2.5\]

</div>

<div id="whatsnew_0180.prior_deprecations">

Removal of prior version deprecations/changes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  - Removal of ``rolling\_corr\_pairwise`in favor of`.rolling().corr(pairwise=True)``(:issue:`4950`) - Removal of``expanding\_corr\_pairwise`in favor of`.expanding().corr(pairwise=True)``(:issue:`4950`) - Removal of``DataMatrix``module. This was not imported into the pandas namespace in any event (:issue:`12111`) - Removal of``cols`keyword in favor of`subset`in`DataFrame.duplicated()`and`DataFrame.drop\_duplicates()``(:issue:`6680`) - Removal of the``read\_frame`and`frame\_query`(both aliases for`pd.read\_sql`)   and`write\_frame`(alias of`to\_sql`) functions in the`pd.io.sql``namespace,   deprecated since 0.14.0 (:issue:`6292`). - Removal of the``order`keyword from`.factorize()``(:issue:`6930`)  .. _whatsnew_0180.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Improved performance of``andrews\_curves``(:issue:`11534`) - Improved huge``DatetimeIndex`,`PeriodIndex`and`TimedeltaIndex`'s ops performance including`NaT``(:issue:`10277`) - Improved performance of``pandas.concat``(:issue:`11958`) - Improved performance of``StataReader``(:issue:`11591`) - Improved performance in construction of``Categoricals`with`Series`of datetimes containing`NaT``(:issue:`12077`)   - Improved performance of ISO 8601 date parsing for dates without separators (:issue:`11899`), leading zeros (:issue:`11871`) and with white space preceding the time zone (:issue:`9714`)     .. _whatsnew_0180.bug_fixes:  Bug fixes ~~~~~~~~~  - Bug in``GroupBy.size``when data-frame is empty. (:issue:`11699`) - Bug in``Period.end\_time``when a multiple of time period is requested (:issue:`11738`) - Regression in``.clip``with tz-aware datetimes (:issue:`11838`) - Bug in``date\_range``when the boundaries fell on the frequency (:issue:`11804`, :issue:`12409`) - Bug in consistency of passing nested dicts to``.groupby(...).agg(...)``(:issue:`9052`) - Accept unicode in``Timedelta``constructor (:issue:`11995`) - Bug in value label reading for``StataReader``when reading incrementally (:issue:`12014`) - Bug in vectorized``DateOffset`when`n`parameter is`0``(:issue:`11370`) - Compat for numpy 1.11 w.r.t.``NaT``comparison changes (:issue:`12049`) - Bug in``read\_csv`when reading from a`StringIO``in threads (:issue:`11790`) - Bug in not treating``NaT`as a missing value in datetimelikes when factorizing & with`Categoricals``(:issue:`12077`) - Bug in getitem when the values of a``Series``were tz-aware (:issue:`12089`) - Bug in``Series.str.get\_dummies``when one of the variables was 'name' (:issue:`12180`) - Bug in``pd.concat``while concatenating tz-aware NaT series. (:issue:`11693`, :issue:`11755`, :issue:`12217`) - Bug in``pd.read\_stata``with version <= 108 files (:issue:`12232`) - Bug in``Series.resample`using a frequency of`Nano`when the index is a`DatetimeIndex``and contains non-zero nanosecond parts (:issue:`12037`) - Bug in resampling with``.nunique``and a sparse index (:issue:`12352`) - Removed some compiler warnings (:issue:`12471`) - Work around compat issues with``boto``in python 3.5 (:issue:`11915`) - Bug in``NaT`subtraction from`Timestamp`or`DatetimeIndex``with timezones (:issue:`11718`) - Bug in subtraction of``Series`of a single tz-aware`Timestamp``(:issue:`12290`) - Use compat iterators in PY2 to support``.next()``(:issue:`12299`) - Bug in``Timedelta.round``with negative values (:issue:`11690`) - Bug in``.loc`against`CategoricalIndex`may result in normal`Index``(:issue:`11586`) - Bug in``DataFrame.info``when duplicated column names exist (:issue:`11761`) - Bug in``.copy``of datetime tz-aware objects (:issue:`11794`) - Bug in``Series.apply`and`Series.map`where`timedelta64``was not boxed (:issue:`11349`) - Bug in``DataFrame.set\_index()`with tz-aware`Series``(:issue:`12358`)    - Bug in subclasses of``DataFrame`where`AttributeError``did not propagate (:issue:`11808`) - Bug groupby on tz-aware data where selection not returning``Timestamp``(:issue:`11616`) - Bug in``pd.read\_clipboard`and`pd.to\_clipboard`functions not supporting Unicode; upgrade included`pyperclip``to v1.5.15 (:issue:`9263`) - Bug in``DataFrame.query``containing an assignment (:issue:`8664`)  - Bug in``from\_msgpack`where`\_\_contains\_\_()`fails for columns of the unpacked`DataFrame`, if the`DataFrame``has object columns. (:issue:`11880`) - Bug in``.resample`on categorical data with`TimedeltaIndex``(:issue:`12169`)   - Bug in timezone info lost when broadcasting scalar datetime to``DataFrame``(:issue:`11682`) - Bug in``Index`creation from`Timestamp``with mixed tz coerces to UTC (:issue:`11488`) - Bug in``to\_numeric``where it does not raise if input is more than one dimension (:issue:`11776`) - Bug in parsing timezone offset strings with non-zero minutes (:issue:`11708`) - Bug in``df.plot``using incorrect colors for bar plots under matplotlib 1.5+ (:issue:`11614`) - Bug in the``groupby`  `plot``method when using keyword arguments (:issue:`11805`). - Bug in``DataFrame.duplicated`and`drop\_duplicates`causing spurious matches when setting`keep=False``(:issue:`11864`) - Bug in``.loc`result with duplicated key may have`Index``with incorrect dtype (:issue:`11497`) - Bug in``pd.rolling\_median``where memory allocation failed even with sufficient memory (:issue:`11696`) - Bug in``DataFrame.style``with spurious zeros (:issue:`12134`) - Bug in``DataFrame.style``with integer columns not starting at 0 (:issue:`12125`) - Bug in``.style.bar``may not rendered properly using specific browser (:issue:`11678`) - Bug in rich comparison of``Timedelta`with a`numpy.array`of`Timedelta``that caused an infinite recursion (:issue:`11835`) - Bug in``DataFrame.round``dropping column index name (:issue:`11986`) - Bug in``df.replace`while replacing value in mixed dtype`Dataframe``(:issue:`11698`) - Bug in``Index`prevents copying name of passed`Index``, when a new name is not provided (:issue:`11193`) - Bug in``read\_excel`failing to read any non-empty sheets when empty sheets exist and`sheetname=None``(:issue:`11711`) - Bug in``read\_excel`failing to raise`NotImplemented`error when keywords`parse\_dates`and`date\_parser``are provided (:issue:`11544`) - Bug in``read\_sql`with`pymysql``connections failing to return chunked data (:issue:`11522`) - Bug in``.to\_csv`ignoring formatting parameters`decimal`,`na\_rep`,`float\_format``for float indexes (:issue:`11553`) - Bug in``Int64Index`and`Float64Index``preventing the use of the modulo operator (:issue:`9244`) - Bug in``MultiIndex.drop``for not lexsorted MultiIndexes (:issue:`12078`)  - Bug in``DataFrame`when masking an empty`DataFrame``(:issue:`11859`)   - Bug in``.plot`potentially modifying the`colors``input when the number of columns didn't match the number of series provided (:issue:`12039`). - Bug in``Series.plot`failing when index has a`CustomBusinessDay``frequency (:issue:`7222`). - Bug in``.to\_sql`for`datetime.time``values with sqlite fallback (:issue:`8341`) - Bug in``read\_excel`failing to read data with one column when`squeeze=True``(:issue:`12157`) - Bug in``read\_excel``failing to read one empty column (:issue:`12292`, :issue:`9002`) - Bug in``.groupby`where a`KeyError``was not raised for a wrong column if there was only one row in the dataframe (:issue:`11741`) - Bug in``.read\_csv``with dtype specified on empty data producing an error (:issue:`12048`) - Bug in``.read\_csv`where strings like`'2E'``are treated as valid floats (:issue:`12237`) - Bug in building *pandas* with debugging symbols (:issue:`12123`)   - Removed``millisecond`property of`DatetimeIndex`. This would always raise a`ValueError``(:issue:`12019`). - Bug in``Series``constructor with read-only data (:issue:`11502`) - Removed``pandas.\_testing.choice()`.  Should use`np.random.choice()``, instead. (:issue:`12386`) - Bug in``.loc``setitem indexer preventing the use of a TZ-aware DatetimeIndex (:issue:`12050`) - Bug in``.style``indexes and MultiIndexes not appearing (:issue:`11655`) - Bug in``to\_msgpack`and`from\_msgpack`which did not correctly serialize or deserialize`NaT``(:issue:`12307`). - Bug in``.skew`and`.kurt``due to roundoff error for highly similar values (:issue:`11974`) - Bug in``Timestamp``constructor where microsecond resolution was lost if HHMMSS were not separated with ':' (:issue:`10041`) - Bug in``buffer\_rd\_bytes``src->buffer could be freed more than once if reading failed, causing a segfault (:issue:`12098`)  - Bug in``crosstab`where arguments with non-overlapping indexes would return a`KeyError``(:issue:`10291`)  - Bug in``DataFrame.apply`in which reduction was not being prevented for cases in which`dtype``was not a numpy dtype (:issue:`12244`) - Bug when initializing categorical series with a scalar value. (:issue:`12336`) - Bug when specifying a UTC``DatetimeIndex`by setting`utc=True`in`.to\_datetime``(:issue:`11934`) - Bug when increasing the buffer size of CSV reader in``read\_csv``(:issue:`12494`) - Bug when setting columns of a``DataFrame\`<span class="title-ref"> with duplicate column names (:issue:\`12344</span>)

</div>

## Contributors

<div class="contributors">

v0.17.1..v0.18.0

</div>

---

v0.18.1.md

---

# Version 0.18.1 (May 3, 2016)

{{ header }}

This is a minor bug-fix release from 0.18.0 and includes a large number of bug fixes along with several new features, enhancements, and performance improvements. We recommend that all users upgrade to this version.

Highlights include:

  - `.groupby(...)` has been enhanced to provide convenient syntax when working with `.rolling(..)`, `.expanding(..)` and `.resample(..)` per group, see \[here \<whatsnew\_0181.deferred\_ops\>\](\#here-\<whatsnew\_0181.deferred\_ops\>)
  - `pd.to_datetime()` has gained the ability to assemble dates from a `DataFrame`, see \[here \<whatsnew\_0181.enhancements.assembling\>\](\#here-\<whatsnew\_0181.enhancements.assembling\>)
  - Method chaining improvements, see \[here \<whatsnew\_0181.enhancements.method\_chain\>\](\#here-\<whatsnew\_0181.enhancements.method\_chain\>).
  - Custom business hour offset, see \[here \<whatsnew\_0181.enhancements.custombusinesshour\>\](\#here-\<whatsnew\_0181.enhancements.custombusinesshour\>).
  - Many bug fixes in the handling of `sparse`, see \[here \<whatsnew\_0181.sparse\>\](\#here-\<whatsnew\_0181.sparse\>)
  - Expanded the \[Tutorials section \<tutorial-modern\>\](\#tutorials-section-\<tutorial-modern\>) with a feature on modern pandas, courtesy of [@TomAugsburger](https://twitter.com/TomAugspurger). (`13045`).

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.18.1

</div>

## New features

### Custom business hour

The `CustomBusinessHour` is a mixture of `BusinessHour` and `CustomBusinessDay` which allows you to specify arbitrary holidays. For details, see \[Custom Business Hour \<timeseries.custombusinesshour\>\](\#custom-business-hour-\<timeseries.custombusinesshour\>) (`11514`)

<div class="ipython">

python

from pandas.tseries.offsets import CustomBusinessHour from pandas.tseries.holiday import USFederalHolidayCalendar

bhour\_us = CustomBusinessHour(calendar=USFederalHolidayCalendar())

</div>

Friday before MLK Day

<div class="ipython">

python

import datetime

dt = datetime.datetime(2014, 1, 17, 15)

dt + bhour\_us

</div>

Tuesday after MLK Day (Monday is skipped because it's a holiday)

<div class="ipython">

python

dt + bhour\_us \* 2

</div>

### Method `.groupby(..)` syntax with window and resample operations

`.groupby(...)` has been enhanced to provide convenient syntax when working with `.rolling(..)`, `.expanding(..)` and `.resample(..)` per group, see (`12486`, `12738`).

You can now use `.rolling(..)` and `.expanding(..)` as methods on groupbys. These return another deferred object (similar to what `.rolling()` and `.expanding()` do on ungrouped pandas objects). You can then operate on these `RollingGroupby` objects in a similar manner.

Previously you would have to do this to get a rolling window mean per-group:

<div class="ipython">

python

df = pd.DataFrame({"A": \[1\] \* 20 + \[2\] \* 12 + \[3\] \* 8, "B": np.arange(40)}) df

</div>

`` `ipython    In [1]: df.groupby("A").apply(lambda x: x.rolling(4).B.mean())    Out[1]:    A    1  0      NaN       1      NaN       2      NaN       3      1.5       4      2.5       5      3.5       6      4.5       7      5.5       8      6.5       9      7.5       10     8.5       11     9.5       12    10.5       13    11.5       14    12.5       15    13.5       16    14.5       17    15.5       18    16.5       19    17.5    2  20     NaN       21     NaN       22     NaN       23    21.5       24    22.5       25    23.5       26    24.5       27    25.5       28    26.5       29    27.5       30    28.5       31    29.5    3  32     NaN       33     NaN       34     NaN       35    33.5       36    34.5       37    35.5       38    36.5       39    37.5    Name: B, dtype: float64  Now you can do:  .. ipython:: python     df.groupby("A").rolling(4).B.mean()  For ``.resample(..)`type of operations, previously you would have to:  .. ipython:: python     df = pd.DataFrame(        {            "date": pd.date_range(start="2016-01-01", periods=4, freq="W"),            "group": [1, 1, 2, 2],            "val": [5, 6, 7, 8],        }    ).set_index("date")     df  .. code-block:: ipython     In[1]: df.groupby("group").apply(lambda x: x.resample("1D").ffill())    Out[1]:                      group  val    group date    1     2016-01-03      1    5          2016-01-04      1    5          2016-01-05      1    5          2016-01-06      1    5          2016-01-07      1    5          2016-01-08      1    5          2016-01-09      1    5          2016-01-10      1    6    2     2016-01-17      2    7          2016-01-18      2    7          2016-01-19      2    7          2016-01-20      2    7          2016-01-21      2    7          2016-01-22      2    7          2016-01-23      2    7          2016-01-24      2    8  Now you can do:  .. code-block:: ipython     In[1]: df.groupby("group").resample("1D").ffill()    Out[1]:                      group  val    group date    1     2016-01-03      1    5          2016-01-04      1    5          2016-01-05      1    5          2016-01-06      1    5          2016-01-07      1    5          2016-01-08      1    5          2016-01-09      1    5          2016-01-10      1    6    2     2016-01-17      2    7          2016-01-18      2    7          2016-01-19      2    7          2016-01-20      2    7          2016-01-21      2    7          2016-01-22      2    7          2016-01-23      2    7          2016-01-24      2    8  .. _whatsnew_0181.enhancements.method_chain:  Method chaining improvements`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The following methods / indexers now accept a `callable`. It is intended to make these more useful in method chains, see the \[documentation \<indexing.callable\>\](\#documentation-\<indexing.callable\>). (`11485`, `12533`)

  - `.where()` and `.mask()`
  - `.loc[]`, `iloc[]` and `.ix[]`
  - `[]` indexing

#### Methods `.where()` and `.mask()`

These can accept a callable for the condition and `other` arguments.

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 2, 3\], "B": \[4, 5, 6\], "C": \[7, 8, 9\]}) df.where(lambda x: x \> 4, lambda x: x + 10)

</div>

#### Methods `.loc[]`, `.iloc[]`, `.ix[]`

These can accept a callable, and a tuple of callable as a slicer. The callable can return a valid boolean indexer or anything which is valid for these indexer's input.

<div class="ipython">

python

\# callable returns bool indexer df.loc\[lambda x: x.A \>= 2, lambda x: x.sum() \> 10\]

\# callable returns list of labels df.loc\[lambda x: \[1, 2\], lambda x: \["A", "B"\]\]

</div>

#### Indexing with `[]`

Finally, you can use a callable in `[]` indexing of Series, DataFrame and Panel. The callable must return a valid input for `[]` indexing depending on its class and index type.

<div class="ipython">

python

df\[lambda x: "A"\]

</div>

Using these methods / indexers, you can chain data selection operations without using temporary variable.

<div class="ipython">

python

bb = pd.read\_csv("data/baseball.csv", index\_col="id") (bb.groupby(\["year", "team"\]).sum(numeric\_only=True).loc\[lambda df: df.r \> 100\])

</div>

### Partial string indexing on `DatetimeIndex` when part of a `MultiIndex`

Partial string indexing now matches on `DateTimeIndex` when part of a `MultiIndex` (`10331`)

`` `ipython    In [20]: dft2 = pd.DataFrame(       ....:     np.random.randn(20, 1),       ....:     columns=["A"],       ....:     index=pd.MultiIndex.from_product(       ....:         [pd.date_range("20130101", periods=10, freq="12H"), ["a", "b"]]       ....:     ),       ....: )       ....:     In [21]: dft2    Out[21]:                                  A    2013-01-01 00:00:00 a  0.469112                        b -0.282863    2013-01-01 12:00:00 a -1.509059                        b -1.135632    2013-01-02 00:00:00 a  1.212112    ...                         ...    2013-01-04 12:00:00 b  0.271860    2013-01-05 00:00:00 a -0.424972                        b  0.567020    2013-01-05 12:00:00 a  0.276232                        b -1.087401     [20 rows x 1 columns]     In [22]: dft2.loc["2013-01-05"]    Out[22]:                                  A    2013-01-05 00:00:00 a -0.424972                        b  0.567020    2013-01-05 12:00:00 a  0.276232                        b -1.087401     [4 rows x 1 columns]  On other levels  .. code-block:: ipython     In [26]: idx = pd.IndexSlice     In [27]: dft2 = dft2.swaplevel(0, 1).sort_index()     In [28]: dft2    Out[28]:                                  A    a 2013-01-01 00:00:00  0.469112      2013-01-01 12:00:00 -1.509059      2013-01-02 00:00:00  1.212112      2013-01-02 12:00:00  0.119209      2013-01-03 00:00:00 -0.861849    ...                         ...    b 2013-01-03 12:00:00  1.071804      2013-01-04 00:00:00 -0.706771      2013-01-04 12:00:00  0.271860      2013-01-05 00:00:00  0.567020      2013-01-05 12:00:00 -1.087401     [20 rows x 1 columns]     In [29]: dft2.loc[idx[:, "2013-01-05"], :]    Out[29]:                                  A    a 2013-01-05 00:00:00 -0.424972      2013-01-05 12:00:00  0.276232    b 2013-01-05 00:00:00  0.567020      2013-01-05 12:00:00 -1.087401     [4 rows x 1 columns]  .. _whatsnew_0181.enhancements.assembling:  Assembling datetimes ``\` ^^^^^^^^^^^^^^^^^^^^

`pd.to_datetime()` has gained the ability to assemble datetimes from a passed in `DataFrame` or a dict. (`8158`).

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"year": \[2015, 2016\], "month": \[2, 3\], "day": \[4, 5\], "hour": \[2, 3\]}

) df

</div>

Assembling using the passed frame.

<div class="ipython">

python

pd.to\_datetime(df)

</div>

You can pass only the columns that you need to assemble.

<div class="ipython">

python

pd.to\_datetime(df\[\["year", "month", "day"\]\])

</div>

### Other enhancements

  - `pd.read_csv()` now supports `delim_whitespace=True` for the Python engine (`12958`)

  - `pd.read_csv()` now supports opening ZIP files that contains a single CSV, via extension inference or explicit `compression='zip'` (`12175`)

  - `pd.read_csv()` now supports opening files using xz compression, via extension inference or explicit `compression='xz'` is specified; `xz` compressions is also supported by `DataFrame.to_csv` in the same way (`11852`)

  - `pd.read_msgpack()` now always gives writeable ndarrays even when compression is used (`12359`).

  - `pd.read_msgpack()` now supports serializing and de-serializing categoricals with msgpack (`12573`)

  - `.to_json()` now supports `NDFrames` that contain categorical and sparse data (`10778`)

  - `interpolate()` now supports `method='akima'` (`7588`).

  - `pd.read_excel()` now accepts path objects (e.g. `pathlib.Path`, `py.path.local`) for the file path, in line with other `read_*` functions (`12655`)

  - Added `.weekday_name` property as a component to `DatetimeIndex` and the `.dt` accessor. (`11128`)

  - `Index.take` now handles `allow_fill` and `fill_value` consistently (`12631`)
    
    <div class="ipython">
    
    python
    
    idx = pd.Index(\[1.0, 2.0, 3.0, 4.0\], dtype="float")
    
    \# default, allow\_fill=True, fill\_value=None idx.take(\[2, -1\]) idx.take(\[2, -1\], fill\_value=True)
    
    </div>

  - `Index` now supports `.str.get_dummies()` which returns `MultiIndex`, see \[Creating Indicator Variables \<text.indicator\>\](\#creating-indicator-variables-\<text.indicator\>) (`10008`, `10103`)
    
    <div class="ipython">
    
    python
    
    idx = pd.Index(\["ac", "b")
    
    </div>

  - `pd.crosstab()` has gained a `normalize` argument for normalizing frequency tables (`12569`). Examples in the updated docs \[here \<reshaping.crosstabulations\>\](\#here-\<reshaping.crosstabulations\>).

  - `.resample(..).interpolate()` is now supported (`12925`)

  - `.isin()` now accepts passed `sets` (`12988`)

## Sparse changes

These changes conform sparse handling to return the correct types and work to make a smoother experience with indexing.

`SparseArray.take` now returns a scalar for scalar input, `SparseArray` for others. Furthermore, it handles a negative indexer with the same rule as `Index` (`10560`, `12796`)

`` `python    s = pd.SparseArray([np.nan, np.nan, 1, 2, 3, np.nan, 4, 5, np.nan, 6])    s.take(0)    s.take([1, 2, 3])  - Bug in ``SparseSeries\[\]`indexing with`Ellipsis`raises`KeyError``(:issue:`9467`)``<span class="title-ref"> - Bug in </span><span class="title-ref">SparseArray\[\]</span><span class="title-ref"> indexing with tuples are not handled properly (:issue:\`12966</span>) - Bug in `SparseSeries.loc[]` with list-like input raises `TypeError` (`10560`) - Bug in `SparseSeries.iloc[]` with scalar input may raise `IndexError` (`10560`) - Bug in `SparseSeries.loc[]`, `.iloc[]` with `slice` returns `SparseArray`, rather than `SparseSeries` (`10560`) - Bug in `SparseDataFrame.loc[]`, `.iloc[]` may results in dense `Series`, rather than `SparseSeries` (`12787`) - Bug in `SparseArray` addition ignores `fill_value` of right hand side (`12910`) - Bug in `SparseArray` mod raises `AttributeError` (`12910`) - Bug in `SparseArray` pow calculates `1 ** np.nan` as `np.nan` which must be 1 (`12910`) - Bug in `SparseArray` comparison output may incorrect result or raise `ValueError` (`12971`) - Bug in `SparseSeries.__repr__` raises `TypeError` when it is longer than `max_rows` (`10560`) - Bug in `SparseSeries.shape` ignores `fill_value` (`10452`) - Bug in `SparseSeries` and `SparseArray` may have different `dtype` from its dense values (`12908`) - Bug in `SparseSeries.reindex` incorrectly handle `fill_value` (`12797`) - Bug in `SparseArray.to_frame()` results in `DataFrame`, rather than `SparseDataFrame` (`9850`) - Bug in `SparseSeries.value_counts()` does not count `fill_value` (`6749`) - Bug in `SparseArray.to_dense()` does not preserve `dtype` (`10648`) - Bug in `SparseArray.to_dense()` incorrectly handle `fill_value` (`12797`) - Bug in `pd.concat()` of `SparseSeries` results in dense (`10536`) - Bug in `pd.concat()` of `SparseDataFrame` incorrectly handle `fill_value` (`9765`) - Bug in `pd.concat()` of `SparseDataFrame` may raise `AttributeError` (`12174`) - Bug in `SparseArray.shift()` may raise `NameError` or `TypeError` (`12908`)

## API changes

### Method `.groupby(..).nth()` changes

The index in `.groupby(..).nth()` output is now more consistent when the `as_index` argument is passed (`11039`):

<div class="ipython">

python

df = pd.DataFrame({"A": \["a", "b", "a"\], "B": \[1, 2, 3\]}) df

</div>

Previous behavior:

`` `ipython    In [3]: df.groupby('A', as_index=True)['B'].nth(0)    Out[3]:    0    1    1    2    Name: B, dtype: int64     In [4]: df.groupby('A', as_index=False)['B'].nth(0)    Out[4]:    0    1    1    2    Name: B, dtype: int64  New behavior:  .. ipython:: python      df.groupby("A", as_index=True)["B"].nth(0)     df.groupby("A", as_index=False)["B"].nth(0)  Furthermore, previously, a ``.groupby`would always sort, regardless if`sort=False`was passed with`.nth()`.  .. ipython:: python     np.random.seed(1234)    df = pd.DataFrame(np.random.randn(100, 2), columns=["a", "b"])    df["c"] = np.random.randint(0, 4, 100)  Previous behavior:  .. code-block:: ipython     In [4]: df.groupby('c', sort=True).nth(1)    Out[4]:              a         b    c    0 -0.334077  0.002118    1  0.036142 -2.074978    2 -0.720589  0.887163    3  0.859588 -0.636524     In [5]: df.groupby('c', sort=False).nth(1)    Out[5]:              a         b    c    0 -0.334077  0.002118    1  0.036142 -2.074978    2 -0.720589  0.887163    3  0.859588 -0.636524  New behavior:  .. ipython:: python     df.groupby("c", sort=True).nth(1)    df.groupby("c", sort=False).nth(1)   .. _whatsnew_0181.numpy_compatibility:  NumPy function compatibility`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Compatibility between pandas array-like methods (e.g. `sum` and `take`) and their `numpy` counterparts has been greatly increased by augmenting the signatures of the `pandas` methods so as to accept arguments that can be passed in from `numpy`, even if they are not necessarily used in the `pandas` implementation (`12644`, `12638`, `12687`)

  - `.searchsorted()` for `Index` and `TimedeltaIndex` now accept a `sorter` argument to maintain compatibility with numpy's `searchsorted` function (`12238`)
  - Bug in numpy compatibility of `np.round()` on a `Series` (`12600`)

An example of this signature augmentation is illustrated below:

`` `python    sp = pd.SparseDataFrame([1, 2, 3])    sp  Previous behaviour:  .. code-block:: ipython     In [2]: np.cumsum(sp, axis=0)    ...    TypeError: cumsum() takes at most 2 arguments (4 given)  New behaviour:  .. code-block:: python     np.cumsum(sp, axis=0)  .. _whatsnew_0181.apply_resample:  Using ``.apply`on GroupBy resampling`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using `apply` on resampling groupby operations (using a `pd.TimeGrouper`) now has the same output types as similar `apply` calls on other groupby operations. (`11742`).

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"date": pd.to\_datetime(\["10/10/2000", "11/10/2000"\]), "value": \[10, 13\]}

) df

</div>

Previous behavior:

`` `ipython     In [1]: df.groupby(pd.TimeGrouper(key='date',        ...:                           freq='M')).apply(lambda x: x.value.sum())     Out[1]:     ...     TypeError: cannot concatenate a non-NDFrame object      # Output is a Series     In [2]: df.groupby(pd.TimeGrouper(key='date',        ...:                           freq='M')).apply(lambda x: x[['value']].sum())     Out[2]:     date     2000-10-31  value    10     2000-11-30  value    13     dtype: int64  New behavior:  .. code-block:: ipython      # Output is a Series     In [55]: df.groupby(pd.TimeGrouper(key='date',         ...:                           freq='M')).apply(lambda x: x.value.sum())     Out[55]:     date     2000-10-31    10     2000-11-30    13     Freq: M, dtype: int64      # Output is a DataFrame     In [56]: df.groupby(pd.TimeGrouper(key='date',         ...:                           freq='M')).apply(lambda x: x[['value']].sum())     Out[56]:                 value     date     2000-10-31     10     2000-11-30     13  .. _whatsnew_0181.read_csv_exceptions:  Changes in ``read\_csv`exceptions`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In order to standardize the `read_csv` API for both the `c` and `python` engines, both will now raise an `EmptyDataError`, a subclass of `ValueError`, in response to empty columns or header (`12493`, `12506`)

Previous behaviour:

`` `ipython    In [1]: import io     In [2]: df = pd.read_csv(io.StringIO(''), engine='c')    ...    ValueError: No columns to parse from file     In [3]: df = pd.read_csv(io.StringIO(''), engine='python')    ...    StopIteration  New behaviour:  .. code-block:: ipython     In [1]: df = pd.read_csv(io.StringIO(''), engine='c')    ...    pandas.io.common.EmptyDataError: No columns to parse from file     In [2]: df = pd.read_csv(io.StringIO(''), engine='python')    ...    pandas.io.common.EmptyDataError: No columns to parse from file  In addition to this error change, several others have been made as well:  - ``CParserError`now sub-classes`ValueError`instead of just a`Exception``(:issue:`12551`)``<span class="title-ref"> - A </span><span class="title-ref">CParserError</span><span class="title-ref"> is now raised instead of a generic </span><span class="title-ref">Exception</span><span class="title-ref"> in </span><span class="title-ref">read\_csv</span><span class="title-ref"> when the </span><span class="title-ref">c</span><span class="title-ref"> engine cannot parse a column (:issue:\`12506</span>) - A `ValueError` is now raised instead of a generic `Exception` in `read_csv` when the `c` engine encounters a `NaN` value in an integer column (`12506`) - A `ValueError` is now raised instead of a generic `Exception` in `read_csv` when `true_values` is specified, and the `c` engine encounters an element in a column containing unencodable bytes (`12506`) - `pandas.parser.OverflowError` exception has been removed and has been replaced with Python's built-in `OverflowError` exception (`12506`) - `pd.read_csv()` no longer allows a combination of strings and integers for the `usecols` parameter (`12678`)

### Method `to_datetime` error changes

Bugs in `pd.to_datetime()` when passing a `unit` with convertible entries and `errors='coerce'` or non-convertible with `errors='ignore'`. Furthermore, an `OutOfBoundsDateime` exception will be raised when an out-of-range value is encountered for that unit when `errors='raise'`. (`11758`, `13052`, `13059`)

Previous behaviour:

`` `ipython    In [27]: pd.to_datetime(1420043460, unit='s', errors='coerce')    Out[27]: NaT     In [28]: pd.to_datetime(11111111, unit='D', errors='ignore')    OverflowError: Python int too large to convert to C long     In [29]: pd.to_datetime(11111111, unit='D', errors='raise')    OverflowError: Python int too large to convert to C long  New behaviour:  .. code-block:: ipython     In [2]: pd.to_datetime(1420043460, unit='s', errors='coerce')    Out[2]: Timestamp('2014-12-31 16:31:00')     In [3]: pd.to_datetime(11111111, unit='D', errors='ignore')    Out[3]: 11111111     In [4]: pd.to_datetime(11111111, unit='D', errors='raise')    OutOfBoundsDatetime: cannot convert input with unit 'D'  .. _whatsnew_0181.api.other:  Other API changes ``\` ^^^^^^^^^^^^^^^^^

  - `.swaplevel()` for `Series`, `DataFrame`, `Panel`, and `MultiIndex` now features defaults for its first two parameters `i` and `j` that swap the two innermost levels of the index. (`12934`)
  - `.searchsorted()` for `Index` and `TimedeltaIndex` now accept a `sorter` argument to maintain compatibility with numpy's `searchsorted` function (`12238`)
  - `Period` and `PeriodIndex` now raises `IncompatibleFrequency` error which inherits `ValueError` rather than raw `ValueError` (`12615`)
  - `Series.apply` for category dtype now applies the passed function to each of the `.categories` (and not the `.codes`), and returns a `category` dtype if possible (`12473`)
  - `read_csv` will now raise a `TypeError` if `parse_dates` is neither a boolean, list, or dictionary (matches the doc-string) (`5636`)
  - The default for `.query()/.eval()` is now `engine=None`, which will use `numexpr` if it's installed; otherwise it will fallback to the `python` engine. This mimics the pre-0.18.1 behavior if `numexpr` is installed (and which, previously, if numexpr was not installed, `.query()/.eval()` would raise). (`12749`)
  - `pd.show_versions()` now includes `pandas_datareader` version (`12740`)
  - Provide a proper `__name__` and `__qualname__` attributes for generic functions (`12021`)
  - `pd.concat(ignore_index=True)` now uses `RangeIndex` as default (`12695`)
  - `pd.merge()` and `DataFrame.join()` will show a `UserWarning` when merging/joining a single- with a multi-leveled dataframe (`9455`, `12219`)
  - Compat with `scipy` \> 0.17 for deprecated `piecewise_polynomial` interpolation method; support for the replacement `from_derivatives` method (`12887`)

### Deprecations

  - The method name `Index.sym_diff()` is deprecated and can be replaced by `Index.symmetric_difference()` (`12591`)
  - The method name `Categorical.sort()` is deprecated in favor of `Categorical.sort_values()` (`12882`)

## Performance improvements

  - Improved speed of SAS reader (`12656`, `12961`)
  - Performance improvements in `.groupby(..).cumcount()` (`11039`)
  - Improved memory usage in `pd.read_csv()` when using `skiprows=an_integer` (`13005`)
  - Improved performance of `DataFrame.to_sql` when checking case sensitivity for tables. Now only checks if table has been created correctly when table name is not lower case. (`12876`)
  - Improved performance of `Period` construction and time series plotting (`12903`, `11831`).
  - Improved performance of `.str.encode()` and `.str.decode()` methods (`13008`)
  - Improved performance of `to_numeric` if input is numeric dtype (`12777`)
  - Improved performance of sparse arithmetic with `IntIndex` (`13036`)

## Bug fixes

  - `usecols` parameter in `pd.read_csv` is now respected even when the lines of a CSV file are not even (`12203`)
  - Bug in `groupby.transform(..)` when `axis=1` is specified with a non-monotonic ordered index (`12713`)
  - Bug in `Period` and `PeriodIndex` creation raises `KeyError` if `freq="Minute"` is specified. Note that "Minute" freq is deprecated in v0.17.0, and recommended to use `freq="T"` instead (`11854`)
  - Bug in `.resample(...).count()` with a `PeriodIndex` always raising a `TypeError` (`12774`)
  - Bug in `.resample(...)` with a `PeriodIndex` casting to a `DatetimeIndex` when empty (`12868`)
  - Bug in `.resample(...)` with a `PeriodIndex` when resampling to an existing frequency (`12770`)
  - Bug in printing data which contains `Period` with different `freq` raises `ValueError` (`12615`)
  - Bug in `Series` construction with `Categorical` and `dtype='category'` is specified (`12574`)
  - Bugs in concatenation with a coercible dtype was too aggressive, resulting in different dtypes in output formatting when an object was longer than `display.max_rows` (`12411`, `12045`, `11594`, `10571`, `12211`)
  - Bug in `float_format` option with option not being validated as a callable. (`12706`)
  - Bug in `GroupBy.filter` when `dropna=False` and no groups fulfilled the criteria (`12768`)
  - Bug in `__name__` of `.cum*` functions (`12021`)
  - Bug in `.astype()` of a `Float64Inde/Int64Index` to an `Int64Index` (`12881`)
  - Bug in round tripping an integer based index in `.to_json()/.read_json()` when `orient='index'` (the default) (`12866`)
  - Bug in plotting `Categorical` dtypes cause error when attempting stacked bar plot (`13019`)
  - Compat with \>= `numpy` 1.11 for `NaT` comparisons (`12969`)
  - Bug in `.drop()` with a non-unique `MultiIndex`. (`12701`)
  - Bug in `.concat` of datetime tz-aware and naive DataFrames (`12467`)
  - Bug in correctly raising a `ValueError` in `.resample(..).fillna(..)` when passing a non-string (`12952`)
  - Bug fixes in various encoding and header processing issues in `pd.read_sas()` (`12659`, `12654`, `12647`, `12809`)
  - Bug in `pd.crosstab()` where would silently ignore `aggfunc` if `values=None` (`12569`).
  - Potential segfault in `DataFrame.to_json` when serialising `datetime.time` (`11473`).
  - Potential segfault in `DataFrame.to_json` when attempting to serialise 0d array (`11299`).
  - Segfault in `to_json` when attempting to serialise a `DataFrame` or `Series` with non-ndarray values; now supports serialization of `category`, `sparse`, and `datetime64[ns, tz]` dtypes (`10778`).
  - Bug in `DataFrame.to_json` with unsupported dtype not passed to default handler (`12554`).
  - Bug in `.align` not returning the sub-class (`12983`)
  - Bug in aligning a `Series` with a `DataFrame` (`13037`)
  - Bug in `ABCPanel` in which `Panel4D` was not being considered as a valid instance of this generic type (`12810`)
  - Bug in consistency of `.name` on `.groupby(..).apply(..)` cases (`12363`)
  - Bug in `Timestamp.__repr__` that caused `pprint` to fail in nested structures (`12622`)
  - Bug in `Timedelta.min` and `Timedelta.max`, the properties now report the true minimum/maximum `timedeltas` as recognized by pandas. See the \[documentation \<timedeltas.limitations\>\](\#documentation-\<timedeltas.limitations\>). (`12727`)
  - Bug in `.quantile()` with interpolation may coerce to `float` unexpectedly (`12772`)
  - Bug in `.quantile()` with empty `Series` may return scalar rather than empty `Series` (`12772`)
  - Bug in `.loc` with out-of-bounds in a large indexer would raise `IndexError` rather than `KeyError` (`12527`)
  - Bug in resampling when using a `TimedeltaIndex` and `.asfreq()`, would previously not include the final fencepost (`12926`)
  - Bug in equality testing with a `Categorical` in a `DataFrame` (`12564`)
  - Bug in `GroupBy.first()`, `.last()` returns incorrect row when `TimeGrouper` is used (`7453`)
  - Bug in `pd.read_csv()` with the `c` engine when specifying `skiprows` with newlines in quoted items (`10911`, `12775`)
  - Bug in `DataFrame` timezone lost when assigning tz-aware datetime `Series` with alignment (`12981`)
  - Bug in `.value_counts()` when `normalize=True` and `dropna=True` where nulls still contributed to the normalized count (`12558`)
  - Bug in `Series.value_counts()` loses name if its dtype is `category` (`12835`)
  - Bug in `Series.value_counts()` loses timezone info (`12835`)
  - Bug in `Series.value_counts(normalize=True)` with `Categorical` raises `UnboundLocalError` (`12835`)
  - Bug in `Panel.fillna()` ignoring `inplace=True` (`12633`)
  - Bug in `pd.read_csv()` when specifying `names`, `usecols`, and `parse_dates` simultaneously with the `c` engine (`9755`)
  - Bug in `pd.read_csv()` when specifying `delim_whitespace=True` and `lineterminator` simultaneously with the `c` engine (`12912`)
  - Bug in `Series.rename`, `DataFrame.rename` and `DataFrame.rename_axis` not treating `Series` as mappings to relabel (`12623`).
  - Clean in `.rolling.min` and `.rolling.max` to enhance dtype handling (`12373`)
  - Bug in `groupby` where complex types are coerced to float (`12902`)
  - Bug in `Series.map` raises `TypeError` if its dtype is `category` or tz-aware `datetime` (`12473`)
  - Bugs on 32bit platforms for some test comparisons (`12972`)
  - Bug in index coercion when falling back from `RangeIndex` construction (`12893`)
  - Better error message in window functions when invalid argument (e.g. a float window) is passed (`12669`)
  - Bug in slicing subclassed `DataFrame` defined to return subclassed `Series` may return normal `Series` (`11559`)
  - Bug in `.str` accessor methods may raise `ValueError` if input has `name` and the result is `DataFrame` or `MultiIndex` (`12617`)
  - Bug in `DataFrame.last_valid_index()` and `DataFrame.first_valid_index()` on empty frames (`12800`)
  - Bug in `CategoricalIndex.get_loc` returns different result from regular `Index` (`12531`)
  - Bug in `PeriodIndex.resample` where name not propagated (`12769`)
  - Bug in `date_range` `closed` keyword and timezones (`12684`).
  - Bug in `pd.concat` raises `AttributeError` when input data contains tz-aware datetime and timedelta (`12620`)
  - Bug in `pd.concat` did not handle empty `Series` properly (`11082`)
  - Bug in `.plot.bar` alignment when `width` is specified with `int` (`12979`)
  - Bug in `fill_value` is ignored if the argument to a binary operator is a constant (`12723`)
  - Bug in `pd.read_html()` when using bs4 flavor and parsing table with a header and only one column (`9178`)
  - Bug in `.pivot_table` when `margins=True` and `dropna=True` where nulls still contributed to margin count (`12577`)
  - Bug in `.pivot_table` when `dropna=False` where table index/column names disappear (`12133`)
  - Bug in `pd.crosstab()` when `margins=True` and `dropna=False` which raised (`12642`)
  - Bug in `Series.name` when `name` attribute can be a hashable type (`12610`)
  - Bug in `.describe()` resets categorical columns information (`11558`)
  - Bug where `loffset` argument was not applied when calling `resample().count()` on a timeseries (`12725`)
  - `pd.read_excel()` now accepts column names associated with keyword argument `names` (`12870`)
  - Bug in `pd.to_numeric()` with `Index` returns `np.ndarray`, rather than `Index` (`12777`)
  - Bug in `pd.to_numeric()` with datetime-like may raise `TypeError` (`12777`)
  - Bug in `pd.to_numeric()` with scalar raises `ValueError` (`12777`)

## Contributors

<div class="contributors">

v0.18.0..v0.18.1

</div>

---

v0.19.0.md

---

# Version 0.19.0 (October 2, 2016)

{{ header }}

This is a major release from 0.18.1 and includes number of API changes, several new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

Highlights include:

  - <span class="title-ref">merge\_asof</span> for asof-style time-series joining, see \[here \<whatsnew\_0190.enhancements.asof\_merge\>\](\#here-\<whatsnew\_0190.enhancements.asof\_merge\>)
  - `.rolling()` is now time-series aware, see \[here \<whatsnew\_0190.enhancements.rolling\_ts\>\](\#here-\<whatsnew\_0190.enhancements.rolling\_ts\>)
  - <span class="title-ref">read\_csv</span> now supports parsing `Categorical` data, see \[here \<whatsnew\_0190.enhancements.read\_csv\_categorical\>\](\#here-\<whatsnew\_0190.enhancements.read\_csv\_categorical\>)
  - A function <span class="title-ref">union\_categorical</span> has been added for combining categoricals, see \[here \<whatsnew\_0190.enhancements.union\_categoricals\>\](\#here-\<whatsnew\_0190.enhancements.union\_categoricals\>)
  - `PeriodIndex` now has its own `period` dtype, and changed to be more consistent with other `Index` classes. See \[here \<whatsnew\_0190.api.period\>\](\#here-\<whatsnew\_0190.api.period\>)
  - Sparse data structures gained enhanced support of `int` and `bool` dtypes, see \[here \<whatsnew\_0190.sparse\>\](\#here-\<whatsnew\_0190.sparse\>)
  - Comparison operations with `Series` no longer ignores the index, see \[here \<whatsnew\_0190.api.series\_ops\>\](\#here-\<whatsnew\_0190.api.series\_ops\>) for an overview of the API changes.
  - Introduction of a pandas development API for utility functions, see \[here \<whatsnew\_0190.dev\_api\>\](\#here-\<whatsnew\_0190.dev\_api\>).
  - Deprecation of `Panel4D` and `PanelND`. We recommend to represent these types of n-dimensional data with the [xarray package](http://xarray.pydata.org/en/stable/).
  - Removal of the previously deprecated modules `pandas.io.data`, `pandas.io.wb`, `pandas.tools.rplot`.

\> **Warning** \> pandas \>= 0.19.0 will no longer silence numpy ufunc warnings upon import, see \[here \<whatsnew\_0190.errstate\>\](\#here-\<whatsnew\_0190.errstate\>).

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.19.0

</div>

## New features

### Function `merge_asof` for asof-style time-series joining

A long-time requested feature has been added through the <span class="title-ref">merge\_asof</span> function, to support asof style joining of time-series (`1870`, `13695`, `13709`, `13902`). Full documentation is \[here \<merging.merge\_asof\>\](\#here-\<merging.merge\_asof\>).

The <span class="title-ref">merge\_asof</span> performs an asof merge, which is similar to a left-join except that we match on nearest key rather than equal keys.

<div class="ipython">

python

left = pd.DataFrame({"a": \[1, 5, 10\], "left\_val": \["a", "b", "c"\]}) right = pd.DataFrame({"a": \[1, 2, 3, 6, 7\], "right\_val": \[1, 2, 3, 6, 7\]})

left right

</div>

We typically want to match exactly when possible, and use the most recent value otherwise.

<div class="ipython">

python

pd.merge\_asof(left, right, on="a")

</div>

We can also match rows ONLY with prior data, and not an exact match.

<div class="ipython">

python

pd.merge\_asof(left, right, on="a", allow\_exact\_matches=False)

</div>

In a typical time-series example, we have `trades` and `quotes` and we want to `asof-join` them. This also illustrates using the `by` parameter to group data before merging.

<div class="ipython">

python

  - trades = pd.DataFrame(
    
      - {
        
          - "time": pd.to\_datetime(
            
              - \[  
                "20160525 13:30:00.023", "20160525 13:30:00.038", "20160525 13:30:00.048", "20160525 13:30:00.048", "20160525 13:30:00.048",
            
            \]
        
        ), "ticker": \["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"\], "price": \[51.95, 51.95, 720.77, 720.92, 98.00\], "quantity": \[75, 155, 100, 100, 100\],
    
    }, columns=\["time", "ticker", "price", "quantity"\],

)

  - quotes = pd.DataFrame(
    
      - {
        
          - "time": pd.to\_datetime(
            
              - \[  
                "20160525 13:30:00.023", "20160525 13:30:00.023", "20160525 13:30:00.030", "20160525 13:30:00.041", "20160525 13:30:00.048", "20160525 13:30:00.049", "20160525 13:30:00.072", "20160525 13:30:00.075",
            
            \]
        
        ), "ticker": \["GOOG", "MSFT", "MSFT", "MSFT", "GOOG", "AAPL", "GOOG", "MSFT"\], "bid": \[720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01\], "ask": \[720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03\],
    
    }, columns=\["time", "ticker", "bid", "ask"\],

)

</div>

<div class="ipython">

python

trades quotes

</div>

An asof merge joins on the `on`, typically a datetimelike field, which is ordered, and in this case we are using a grouper in the `by` field. This is like a left-outer join, except that forward filling happens automatically taking the most recent non-NaN value.

<div class="ipython">

python

pd.merge\_asof(trades, quotes, on="time", by="ticker")

</div>

This returns a merged DataFrame with the entries in the same order as the original left passed DataFrame (`trades` in this case), with the fields of the `quotes` merged.

### Method `.rolling()` is now time-series aware

`.rolling()` objects are now time-series aware and can accept a time-series offset (or convertible) for the `window` argument (`13327`, `12995`). See the full documentation \[here \<window.generic\>\](\#here-\<window.generic\>).

<div class="ipython">

python

  - dft = pd.DataFrame(  
    {"B": \[0, 1, 2, np.nan, 4\]}, index=pd.date\_range("20130101 09:00:00", periods=5, freq="s"),

) dft

</div>

This is a regular frequency index. Using an integer window parameter works to roll along the window frequency.

<div class="ipython">

python

dft.rolling(2).sum() dft.rolling(2, min\_periods=1).sum()

</div>

Specifying an offset allows a more intuitive specification of the rolling frequency.

<div class="ipython">

python

dft.rolling("2s").sum()

</div>

Using a non-regular, but still monotonic index, rolling with an integer window does not impart any special calculation.

<div class="ipython">

python

  - dft = pd.DataFrame(  
    {"B": \[0, 1, 2, np.nan, 4\]}, index=pd.Index( \[ pd.Timestamp("20130101 09:00:00"), pd.Timestamp("20130101 09:00:02"), pd.Timestamp("20130101 09:00:03"), pd.Timestamp("20130101 09:00:05"), pd.Timestamp("20130101 09:00:06"), \], name="foo", ),

)

dft dft.rolling(2).sum()

</div>

Using the time-specification generates variable windows for this sparse data.

<div class="ipython">

python

dft.rolling("2s").sum()

</div>

Furthermore, we now allow an optional `on` parameter to specify a column (rather than the default of the index) in a DataFrame.

<div class="ipython">

python

dft = dft.reset\_index() dft dft.rolling("2s", on="foo").sum()

</div>

### Method `read_csv` has improved support for duplicate column names

<div class="ipython" data-suppress="">

python

from io import StringIO

</div>

\[Duplicate column names \<io.dupe\_names\>\](\#duplicate-column-names-\<io.dupe\_names\>) are now supported in <span class="title-ref">read\_csv</span> whether they are in the file or passed in as the `names` parameter (`7160`, `9424`)

<div class="ipython">

python

data = "0,1,2n3,4,5" names = \["a", "b", "a"\]

</div>

**Previous behavior**:

`` `ipython    In [2]: pd.read_csv(StringIO(data), names=names)    Out[2]:       a  b  a    0  2  1  2    1  5  4  5  The first ``a`column contained the same data as the second`a`column, when it should have`<span class="title-ref"> contained the values </span><span class="title-ref">\[0, 3\]</span>\`.

**New behavior**:

<div class="ipython" data-okexcept="">

python

pd.read\_csv(StringIO(data), names=names)

</div>

### Method `read_csv` supports parsing `Categorical` directly

The <span class="title-ref">read\_csv</span> function now supports parsing a `Categorical` column when specified as a dtype (`10153`). Depending on the structure of the data, this can result in a faster parse time and lower memory usage compared to converting to `Categorical` after parsing. See the io \[docs here \<io.categorical\>\](\#docs-here-\<io.categorical\>).

<div class="ipython">

python

data = """ col1,col2,col3 a,b,1 a,b,2 c,d,3 """

pd.read\_csv(StringIO(data)) pd.read\_csv(StringIO(data)).dtypes pd.read\_csv(StringIO(data), dtype="category").dtypes

</div>

Individual columns can be parsed as a `Categorical` using a dict specification

<div class="ipython">

python

pd.read\_csv(StringIO(data), dtype={"col1": "category"}).dtypes

</div>

\> **Note** \> The resulting categories will always be parsed as strings (object dtype). If the categories are numeric they can be converted using the <span class="title-ref">to\_numeric</span> function, or as appropriate, another converter such as <span class="title-ref">to\_datetime</span>.

> 
> 
> <div class="ipython">
> 
> python
> 
> df = pd.read\_csv(StringIO(data), dtype="category") df.dtypes df\["col3"\] new\_categories = pd.to\_numeric(df\["col3"\].cat.categories) df\["col3"\] = df\["col3"\].cat.rename\_categories(new\_categories) df\["col3"\]
> 
> </div>

### Categorical concatenation

  - A function <span class="title-ref">union\_categoricals</span> has been added for combining categoricals, see \[Unioning Categoricals\<categorical.union\>\](\#unioning-categoricals\<categorical.union\>) (`13361`, `13763`, `13846`, `14173`)
    
    <div class="ipython">
    
    python
    
    from pandas.api.types import union\_categoricals
    
    a = pd.Categorical(\["b", "c"\]) b = pd.Categorical(\["a", "b"\]) union\_categoricals(\[a, b\])
    
    </div>

  - `concat` and `append` now can concat `category` dtypes with different `categories` as `object` dtype (`13524`)
    
    <div class="ipython">
    
    python
    
    s1 = pd.Series(\["a", "b"\], dtype="category") s2 = pd.Series(\["b", "c"\], dtype="category")
    
    </div>

**Previous behavior**:

`` `ipython    In [1]: pd.concat([s1, s2])    ValueError: incompatible categories in categorical concat  **New behavior**:  .. ipython:: python     pd.concat([s1, s2])  .. _whatsnew_0190.enhancements.semi_month_offsets:  Semi-month offsets ``\` ^^^^^^^^^^^^^^^^^^

pandas has gained new frequency offsets, `SemiMonthEnd` ('SM') and `SemiMonthBegin` ('SMS'). These provide date offsets anchored (by default) to the 15th and end of month, and 15th and 1st of month respectively. (`1543`)

<div class="ipython">

python

from pandas.tseries.offsets import SemiMonthEnd, SemiMonthBegin

</div>

**SemiMonthEnd**:

`` `python    In [46]: pd.Timestamp("2016-01-01") + SemiMonthEnd()    Out[46]: Timestamp('2016-01-15 00:00:00')     In [47]: pd.date_range("2015-01-01", freq="SM", periods=4)    Out[47]: DatetimeIndex(['2015-01-15', '2015-01-31', '2015-02-15', '2015-02-28'], dtype='datetime64[ns]', freq='SM-15')  **SemiMonthBegin**:  .. ipython:: python     pd.Timestamp("2016-01-01") + SemiMonthBegin()     pd.date_range("2015-01-01", freq="SMS", periods=4)  Using the anchoring suffix, you can also specify the day of month to use instead of the 15th.  .. code-block:: python     In [50]: pd.date_range("2015-01-01", freq="SMS-16", periods=4)    Out[50]: DatetimeIndex(['2015-01-01', '2015-01-16', '2015-02-01', '2015-02-16'], dtype='datetime64[ns]', freq='SMS-16')     In [51]: pd.date_range("2015-01-01", freq="SM-14", periods=4)    Out[51]: DatetimeIndex(['2015-01-14', '2015-01-31', '2015-02-14', '2015-02-28'], dtype='datetime64[ns]', freq='SM-14')  .. _whatsnew_0190.enhancements.index:  New Index methods ``\` ^^^^^^^^^^^^^^^^^

The following methods and options are added to `Index`, to be more consistent with the `Series` and `DataFrame` API.

`Index` now supports the `.where()` function for same shape indexing (`13170`)

<div class="ipython">

python

idx = pd.Index(\["a", "b", "c"\]) idx.where(\[True, False, True\])

</div>

`Index` now supports `.dropna()` to exclude missing values (`6194`)

<div class="ipython">

python

idx = pd.Index(\[1, 2, np.nan, 4\]) idx.dropna()

</div>

For `MultiIndex`, values are dropped if any level is missing by default. Specifying `how='all'` only drops values where all levels are missing.

<div class="ipython">

python

midx = pd.MultiIndex.from\_arrays(\[\[1, 2, np.nan, 4\], \[1, 2, np.nan, np.nan\]\]) midx midx.dropna() midx.dropna(how="all")

</div>

`Index` now supports `.str.extractall()` which returns a `DataFrame`, see the \[docs here \<text.extractall\>\](\#docs-here-\<text.extractall\>) (`10008`, `13156`)

<div class="ipython">

python

idx = pd.Index(\["a1a2", "b1", "c1"\]) idx.str.extractall(r"\[ab\](?P\<digit\>d)")

</div>

`Index.astype()` now accepts an optional boolean argument `copy`, which allows optional copying if the requirements on dtype are satisfied (`13209`)

### Google BigQuery enhancements

  - The <span class="title-ref">read\_gbq</span> method has gained the `dialect` argument to allow users to specify whether to use BigQuery's legacy SQL or BigQuery's standard SQL. See the [docs](https://pandas-gbq.readthedocs.io/en/latest/reading.html) for more details (`13615`).
  - The <span class="title-ref">\~DataFrame.to\_gbq</span> method now allows the DataFrame column order to differ from the destination table schema (`11359`).

### Fine-grained NumPy errstate

Previous versions of pandas would permanently silence numpy's ufunc error handling when `pandas` was imported. pandas did this in order to silence the warnings that would arise from using numpy ufuncs on missing data, which are usually represented as `NaN` s. Unfortunately, this silenced legitimate warnings arising in non-pandas code in the application. Starting with 0.19.0, pandas will use the `numpy.errstate` context manager to silence these warnings in a more fine-grained manner, only around where these operations are actually used in the pandas code base. (`13109`, `13145`)

After upgrading pandas, you may see *new* `RuntimeWarnings` being issued from your code. These are likely legitimate, and the underlying cause likely existed in the code when using previous versions of pandas that simply silenced the warning. Use [numpy.errstate](https://numpy.org/doc/stable/reference/generated/numpy.errstate.html) around the source of the `RuntimeWarning` to control how these conditions are handled.

### Method `get_dummies` now returns integer dtypes

The `pd.get_dummies` function now returns dummy-encoded columns as small integers, rather than floats (`8725`). This should provide an improved memory footprint.

**Previous behavior**:

`` `ipython    In [1]: pd.get_dummies(['a', 'b', 'a', 'c']).dtypes     Out[1]:    a    float64    b    float64    c    float64    dtype: object  **New behavior**:  .. ipython:: python     pd.get_dummies(["a", "b", "a", "c"]).dtypes   .. _whatsnew_0190.enhancements.to_numeric_downcast:  Downcast values to smallest possible dtype in ``to\_numeric`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

`pd.to_numeric()` now accepts a `downcast` parameter, which will downcast the data if possible to smallest specified numerical dtype (`13352`)

<div class="ipython">

python

s = \["1", 2, 3\] pd.to\_numeric(s, downcast="unsigned") pd.to\_numeric(s, downcast="integer")

</div>

### pandas development API

As part of making pandas API more uniform and accessible in the future, we have created a standard sub-package of pandas, `pandas.api` to hold public API's. We are starting by exposing type introspection functions in `pandas.api.types`. More sub-packages and officially sanctioned API's will be published in future versions of pandas (`13147`, `13634`)

The following are now part of this API:

<div class="ipython">

python

import pprint from pandas.api import types

funcs = \[f for f in dir(types) if not f.startswith("\_")\] pprint.pprint(funcs)

</div>

\> **Note** \> Calling these functions from the internal module `pandas.core.common` will now show a `DeprecationWarning` (`13990`)

### Other enhancements

  - `Timestamp` can now accept positional and keyword parameters similar to <span class="title-ref">datetime.datetime</span> (`10758`, `11630`)
    
    <div class="ipython">
    
    python
    
    pd.Timestamp(2012, 1, 1)
    
    pd.Timestamp(year=2012, month=1, day=1, hour=8, minute=30)
    
    </div>

  - The `.resample()` function now accepts a `on=` or `level=` parameter for resampling on a datetimelike column or `MultiIndex` level (`13500`)
    
    <div class="ipython">
    
    python
    
      - df = pd.DataFrame(  
        {"date": pd.date\_range("2015-01-01", freq="W", periods=5), "a": np.arange(5)}, index=pd.MultiIndex.from\_arrays( \[\[1, 2, 3, 4, 5\], pd.date\_range("2015-01-01", freq="W", periods=5)\], names=\["v", "d"\], ),
    
    ) df
    
    </div>
    
      - \`\`\`ipython  
        In \[74\]: df.resample("M", on="date")\[\["a"\]\].sum() Out\[74\]: a date 2015-01-31 6 2015-02-28 4
        
        \[2 rows x 1 columns\]
        
        In \[75\]: df.resample("M", level="d")\[\["a"\]\].sum() Out\[75\]: a d 2015-01-31 6 2015-02-28 4
        
        \[2 rows x 1 columns\]

\- The `.get_credentials()` method of `GbqConnector` can now first try to fetch [the application default credentials](https://developers.google.com/identity/protocols/application-default-credentials). See the docs for more details (`13577`). `` ` - The ``.tz\_localize()`method of`DatetimeIndex`and`Timestamp`has gained the`errors`keyword, so you can potentially coerce nonexistent timestamps to`NaT`. The default behavior remains to raising a`NonExistentTimeError``(:issue:`13057`) -``.to\_hdf/read\_hdf()`now accept path objects (e.g.`pathlib.Path`,`py.path.local``) for the file path (:issue:`11773`) - The``pd.read\_csv()`with`engine='python'`has gained support for the`decimal``(:issue:`12933`),``na\_filter``(:issue:`13321`) and the``memory\_map``option (:issue:`13381`). - Consistent with the Python API,``pd.read\_csv()`will now interpret`+inf``as positive infinity (:issue:`13274`) - The``pd.read\_html()`has gained support for the`na\_values`,`converters`,`keep\_default\_na``options (:issue:`13461`) -``Categorical.astype()`now accepts an optional boolean argument`copy``, effective when dtype is categorical (:issue:`13209`) -``DataFrame`has gained the`.asof()``method to return the last non-NaN values according to the selected subset (:issue:`13358`) - The``DataFrame`constructor will now respect key ordering if a list of`OrderedDict``objects are passed in (:issue:`13304`) -``pd.read\_html()`has gained support for the`decimal``option (:issue:`12907`) -``Series`has gained the properties`.is\_monotonic`,`.is\_monotonic\_increasing`,`.is\_monotonic\_decreasing`, similar to`Index``(:issue:`13336`) -``DataFrame.to\_sql()``now allows a single value as the SQL type for all columns (:issue:`11886`). -``Series.append`now supports the`ignore\_index``option (:issue:`13677`) -``.to\_stata()`and`StataWriter``can now write variable labels to Stata dta files using a dictionary to make column names to labels (:issue:`13535`, :issue:`13536`) -``.to\_stata()`and`StataWriter`will automatically convert`datetime64\[ns\]`columns to Stata format`%tc`, rather than raising a`ValueError``(:issue:`12259`) -``read\_stata()`and`StataReader`raise with a more explicit error message when reading Stata files with repeated value labels when`convert\_categoricals=True``(:issue:`13923`) -``DataFrame.style``will now render sparsified MultiIndexes (:issue:`11655`) -``DataFrame.style`will now show column level names (e.g.`DataFrame.columns.names``) (:issue:`13775`) -``DataFrame`has gained support to re-order the columns based on the values   in a row using`df.sort\_values(by='...', axis=1)``(:issue:`10806`)    .. ipython:: python       df = pd.DataFrame({"A": [2, 7], "B": [3, 5], "C": [4, 8]}, index=["row1", "row2"])      df      df.sort_values(by="row2", axis=1)  - Added documentation to [I/O<io.dtypes>](#i/o<io.dtypes>) regarding the perils of reading in columns with mixed dtypes and how to handle it (:issue:`13746`) - `~DataFrame.to_html` now has a``border`argument to control the value in the opening`\<table\>`tag. The default is the value of the`html.border``option, which defaults to 1. This also affects the notebook HTML repr, but since Jupyter's CSS includes a border-width attribute, the visual effect is the same. (:issue:`11563`). - Raise``ImportError`in the sql functions when`sqlalchemy``is not installed and a connection string is used (:issue:`11920`). - Compatibility with matplotlib 2.0. Older versions of pandas should also work with matplotlib 2.0 (:issue:`13333`) -``Timestamp`,`Period`,`DatetimeIndex`,`PeriodIndex`and`.dt`accessor have gained a`.is\_leap\_year``property to check whether the date belongs to a leap year. (:issue:`13727`) -``astype()`will now accept a dict of column name to data types mapping as the`dtype``argument. (:issue:`12086`) - The``pd.read\_json`and`DataFrame.to\_json`has gained support for reading and writing json lines with`lines``option see [Line delimited json <io.jsonl>](#line-delimited-json-<io.jsonl>) (:issue:`9180`) - `read_excel` now supports the true_values and false_values keyword arguments (:issue:`13347`) -``groupby()`will now accept a scalar and a single-element list for specifying`level`on a non-`MultiIndex``grouper. (:issue:`13907`) - Non-convertible dates in an excel date column will be returned without conversion and the column will be``object``dtype, rather than raising an exception (:issue:`10001`). -``pd.Timedelta(None)`is now accepted and will return`NaT`, mirroring`pd.Timestamp``(:issue:`13687`) -``pd.read\_stata()``can now handle some format 111 files, which are produced by SAS when generating Stata dta files (:issue:`11526`) -``Series`and`Index`now support`divmod``which will return a tuple of   series or indices. This behaves like a standard binary operator with regards   to broadcasting rules (:issue:`14208`).   .. _whatsnew_0190.api:  API changes ~~~~~~~~~~~``Series.tolist()`will now return Python types ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^`Series.tolist()`will now return Python types in the output, mimicking NumPy`.tolist()``behavior (:issue:`10904`)   .. ipython:: python     s = pd.Series([1, 2, 3])  **Previous behavior**:``\`ipython In \[7\]: type(s.tolist()\[0\]) Out\[7\]: \<class 'numpy.int64'\>

**New behavior**:

<div class="ipython">

python

type(s.tolist()\[0\])

</div>

<div id="whatsnew_0190.api.series_ops">

`Series` operators for different indexes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Following ``Series`operators have been changed to make all operators consistent, including`DataFrame``(:issue:`1134`, :issue:`4581`, :issue:`13538`)  -``Series`comparison operators now raise`ValueError`when`index`are different. -`Series`logical operators align both`index`of left and right hand side.  .. warning::    Until 0.18.1, comparing`Series`with the same length, would succeed even if    the`.index`are different (the result ignores`.index`). As of 0.19.0, this will raises`ValueError`to be more strict. This section also describes how to keep previous behavior or align different indexes, using the flexible comparison methods like`.eq`.   As a result,`Series`and`DataFrame`operators behave as below:  Arithmetic operators """"""""""""""""""""  Arithmetic operators align both`index`(no changes).  .. ipython:: python     s1 = pd.Series([1, 2, 3], index=list("ABC"))    s2 = pd.Series([2, 2, 2], index=list("ABD"))    s1 + s2     df1 = pd.DataFrame([1, 2, 3], index=list("ABC"))    df2 = pd.DataFrame([2, 2, 2], index=list("ABD"))    df1 + df2  Comparison operators """"""""""""""""""""  Comparison operators raise`ValueError`when`.index`are different.  **Previous behavior** (`Series`):`Series`compared values ignoring the`.index`as long as both had the same length:`\`ipython In \[1\]: s1 == s2 Out\[1\]: A False B True C False dtype: bool

</div>

**New behavior** (`Series`):

``` ipython
In [2]: s1 == s2
Out[2]:
ValueError: Can only compare identically-labeled Series objects
```

\> **Note** \> To achieve the same result as previous versions (compare values based on locations ignoring `.index`), compare both `.values`.

> 
> 
> <div class="ipython">
> 
> python
> 
> s1.values == s2.values
> 
> </div>
> 
> If you want to compare `Series` aligning its `.index`, see flexible comparison methods section below:
> 
> <div class="ipython">
> 
> python
> 
> s1.eq(s2)
> 
> </div>

**Current behavior** (`DataFrame`, no change):

``` ipython
In [3]: df1 == df2
Out[3]:
ValueError: Can only compare identically-labeled DataFrame objects
```

Logical operators `` ` """""""""""""""""  Logical operators align both ``.index`of left and right hand side.  **Previous behavior** (`Series`), only left hand side`index`was kept:`\`ipython In \[4\]: s1 = pd.Series(\[True, False, True\], index=list('ABC')) In \[5\]: s2 = pd.Series(\[True, True, True\], index=list('ABD')) In \[6\]: s1 & s2 Out\[6\]: A True B False C False dtype: bool

**New behavior** (`Series`):

<div class="ipython">

python

s1 = pd.Series(\[True, False, True\], index=list("ABC")) s2 = pd.Series(\[True, True, True\], index=list("ABD")) s1 & s2

</div>

<div class="note">

<div class="title">

Note

</div>

`Series` logical operators fill a `NaN` result with `False`.

</div>

<div class="note">

<div class="title">

Note

</div>

To achieve the same result as previous versions (compare values based on only left hand side index), you can use `reindex_like`:

<div class="ipython">

python

s1 & s2.reindex\_like(s1)

</div>

</div>

**Current behavior** (`DataFrame`, no change):

<div class="ipython">

python

df1 = pd.DataFrame(\[True, False, True\], index=list("ABC")) df2 = pd.DataFrame(\[True, True, True\], index=list("ABD")) df1 & df2

</div>

Flexible comparison methods `` ` """"""""""""""""""""""""""" ``Series`flexible comparison methods like`eq`,`ne`,`le`,`lt`,`ge`and`gt`now align both`index`. Use these operators if you want to compare two`Series`which has the different`index`.  .. ipython:: python     s1 = pd.Series([1, 2, 3], index=["a", "b", "c"])    s2 = pd.Series([2, 2, 2], index=["b", "c", "d"])    s1.eq(s2)    s1.ge(s2)  Previously, this worked the same as comparison operators (see above).  .. _whatsnew_0190.api.promote:`Series`type promotion on assignment ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  A`Series``will now correctly promote its dtype for assignment with incompat values to the current dtype (:issue:`13234`)   .. ipython:: python    :okwarning:     s = pd.Series()  **Previous behavior**:``\`ipython In \[2\]: s\["a"\] = pd.Timestamp("2016-01-01")

> In \[3\]: s\["b"\] = 3.0 TypeError: invalid type promotion

**New behavior**:

<div class="ipython">

python

s\["a"\] = pd.Timestamp("2016-01-01") s\["b"\] = 3.0 s s.dtype

</div>

<div id="whatsnew_0190.api.to_datetime_coerce">

Function `.to_datetime()` changes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously if ``.to\_datetime()`encountered mixed integers/floats and strings, but no datetimes with`errors='coerce'`it would convert all to`NaT`.  **Previous behavior**:`\`ipython In \[2\]: pd.to\_datetime(\[1, 'foo'\], errors='coerce') Out\[2\]: DatetimeIndex(\['NaT', 'NaT'\], dtype='datetime64\[ns\]', freq=None)

</div>

**Current behavior**:

This will now convert integers/floats with the default unit of `ns`.

<div class="ipython">

python

pd.to\_datetime(\[1, "foo"\], errors="coerce")

</div>

Bug fixes related to `.to_datetime()`:

\- Bug in `pd.to_datetime()` when passing integers or floats, and no `unit` and `errors='coerce'` (`13180`). `` ` - Bug in ``pd.to\_datetime()`when passing invalid data types (e.g. bool); will now respect the`errors``keyword (:issue:`13176`) - Bug in``pd.to\_datetime()`which overflowed on`int8`, and`int16``dtypes (:issue:`13451`) - Bug in``pd.to\_datetime()`raise`AttributeError`with`NaN`and the other string is not valid when`errors='ignore'``(:issue:`12424`) - Bug in``pd.to\_datetime()`did not cast floats correctly when`unit``was specified, resulting in truncated datetime (:issue:`13834`)  .. _whatsnew_0190.api.merging:  Merging changes ^^^^^^^^^^^^^^^  Merging will now preserve the dtype of the join keys (:issue:`8596`)  .. ipython:: python     df1 = pd.DataFrame({"key": [1], "v1": [10]})    df1    df2 = pd.DataFrame({"key": [1, 2], "v1": [20, 30]})    df2  **Previous behavior**:``\`ipython In \[5\]: pd.merge(df1, df2, how='outer') Out\[5\]: key v1 0 1.0 10.0 1 1.0 20.0 2 2.0 30.0

> In \[6\]: pd.merge(df1, df2, how='outer').dtypes Out\[6\]: key float64 v1 float64 dtype: object

**New behavior**:

We are able to preserve the join keys

<div class="ipython">

python

pd.merge(df1, df2, how="outer") pd.merge(df1, df2, how="outer").dtypes

</div>

Of course if you have missing values that are introduced, then the `` ` resulting dtype will be upcast, which is unchanged from previous.  .. ipython:: python     pd.merge(df1, df2, how="outer", on="key")    pd.merge(df1, df2, how="outer", on="key").dtypes  .. _whatsnew_0190.api.describe:  Method ``.describe()`changes ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Percentile identifiers in the index of a`.describe()``output will now be rounded to the least precision that keeps them distinct (:issue:`13104`)  .. ipython:: python     s = pd.Series([0, 1, 2, 3, 4])    df = pd.DataFrame([0, 1, 2, 3, 4])  **Previous behavior**:  The percentiles were rounded to at most one decimal place, which could raise``ValueError`for a data frame if the percentiles were duplicated.`\`ipython In \[3\]: s.describe(percentiles=\[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999\]) Out\[3\]: count 5.000000 mean 2.000000 std 1.581139 min 0.000000 0.0% 0.000400 0.1% 0.002000 0.1% 0.004000 50% 2.000000 99.9% 3.996000 100.0% 3.998000 100.0% 3.999600 max 4.000000 dtype: float64

> In \[4\]: df.describe(percentiles=\[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999\]) Out\[4\]: ... ValueError: cannot reindex from a duplicate axis

**New behavior**:

<div class="ipython">

python

s.describe(percentiles=\[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999\]) df.describe(percentiles=\[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999\])

</div>

Furthermore:

\- Passing duplicated `percentiles` will now raise a `ValueError`. `` ` - Bug in ``.describe()`on a DataFrame with a mixed-dtype column index, which would previously raise a`TypeError``(:issue:`13288`)  .. _whatsnew_0190.api.period:``Period`changes ^^^^^^^^^^^^^^^^^^  The`PeriodIndex`now has`period`dtype """"""""""""""""""""""""""""""""""""""""""""`PeriodIndex`now has its own`period`dtype. The`period`dtype is a pandas extension dtype like`category`or the [timezone aware dtype <timeseries.timezone_series>](#timezone-aware-dtype-<timeseries.timezone_series>) (`datetime64\[ns, tz\]``) (:issue:`13941`). As a consequence of this change,``PeriodIndex`no longer has an integer dtype:  **Previous behavior**:`\`ipython In \[1\]: pi = pd.PeriodIndex(\['2016-08-01'\], freq='D')

> In \[2\]: pi Out\[2\]: PeriodIndex(\['2016-08-01'\], dtype='int64', freq='D')
> 
> In \[3\]: pd.api.types.is\_integer\_dtype(pi) Out\[3\]: True
> 
> In \[4\]: pi.dtype Out\[4\]: dtype('int64')

**New behavior**:

<div class="ipython" data-okwarning="">

python

pi = pd.PeriodIndex(\["2016-08-01"\], freq="D") pi pd.api.types.is\_integer\_dtype(pi) pd.api.types.is\_period\_dtype(pi) pi.dtype type(pi.dtype)

</div>

<div id="whatsnew_0190.api.periodnat">

`Period('NaT')` now returns `pd.NaT` `` ` """"""""""""""""""""""""""""""""""""""""  Previously, ``Period`has its own`Period('NaT')`representation different from`pd.NaT`. Now`Period('NaT')`has been changed to return`pd.NaT``. (:issue:`12759`, :issue:`13582`)  **Previous behavior**:``\`ipython In \[5\]: pd.Period('NaT', freq='D') Out\[5\]: Period('NaT', 'D')

</div>

**New behavior**:

These result in `pd.NaT` without providing `freq` option.

<div class="ipython">

python

pd.Period("NaT") pd.Period(None)

</div>

To be compatible with `Period` addition and subtraction, `pd.NaT` now supports addition and subtraction with `int`. Previously it raised `ValueError`.

**Previous behavior**:

``` ipython
In [5]: pd.NaT + 1
...
ValueError: Cannot add integral value to Timestamp without freq.
```

**New behavior**:

<div class="ipython">

python

pd.NaT + 1 pd.NaT - 1

</div>

`PeriodIndex.values` now returns array of `Period` object `` ` """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""" ``.values`is changed to return an array of`Period``objects, rather than an array of integers (:issue:`13988`).  **Previous behavior**:``\`ipython In \[6\]: pi = pd.PeriodIndex(\['2011-01', '2011-02'\], freq='M') In \[7\]: pi.values Out\[7\]: array(\[492, 493\])

**New behavior**:

<div class="ipython">

python

pi = pd.PeriodIndex(\["2011-01", "2011-02"\], freq="M") pi.values

</div>

<div id="whatsnew_0190.api.setops">

Index `+` / `-` no longer used for set operations `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Addition and subtraction of the base Index type and of DatetimeIndex (not the numeric index types) previously performed set operations (set union and difference). This behavior was already deprecated since 0.15.0 (in favor using the specific ``.union()`and`.difference()`methods), and is now disabled. When possible,`+`and`-``are now used for element-wise operations, for example for concatenating strings or subtracting datetimes (:issue:`8227`, :issue:`14127`).  Previous behavior:``\`ipython In \[1\]: pd.Index(\['a', 'b'\]) + pd.Index(\['a', 'c'\]) FutureWarning: using '+' to provide set union with Indexes is deprecated, use '|' or .union() Out\[1\]: Index(\['a', 'b', 'c'\], dtype='object')

</div>

**New behavior**: the same operation will now perform element-wise addition:

<div class="ipython">

python

pd.Index(\["a", "b"\]) + pd.Index(\["a", "c"\])

</div>

Note that numeric Index objects already performed element-wise operations. `` ` For example, the behavior of adding two integer Indexes is unchanged. The base ``Index`is now made consistent with this behavior.  .. ipython:: python     pd.Index([1, 2, 3]) + pd.Index([2, 3, 4])  Further, because of this change, it is now possible to subtract two DatetimeIndex objects resulting in a TimedeltaIndex:  **Previous behavior**:`\`ipython In \[1\]: (pd.DatetimeIndex(\['2016-01-01', '2016-01-02'\]) ...: - pd.DatetimeIndex(\['2016-01-02', '2016-01-03'\])) FutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference() Out\[1\]: DatetimeIndex(\['2016-01-01'\], dtype='datetime64\[ns\]', freq=None)

**New behavior**:

<div class="ipython">

python

  - (  
    pd.DatetimeIndex(\["2016-01-01", "2016-01-02"\]) - pd.DatetimeIndex(\["2016-01-02", "2016-01-03"\])

)

</div>

<div id="whatsnew_0190.api.difference">

`Index.difference` and `.symmetric_difference` changes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ``Index.difference`and`Index.symmetric\_difference`will now, more consistently, treat`NaN``values as any other values. (:issue:`13514`)  .. ipython:: python     idx1 = pd.Index([1, 2, 3, np.nan])    idx2 = pd.Index([0, 1, np.nan])  **Previous behavior**:``\`ipython In \[3\]: idx1.difference(idx2) Out\[3\]: Float64Index(\[nan, 2.0, 3.0\], dtype='float64')

</div>

> In \[4\]: idx1.symmetric\_difference(idx2) Out\[4\]: Float64Index(\[0.0, nan, 2.0, 3.0\], dtype='float64')

**New behavior**:

<div class="ipython">

python

idx1.difference(idx2) idx1.symmetric\_difference(idx2)

</div>

<div id="whatsnew_0190.api.unique_index">

`Index.unique` consistently returns `Index` `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ``Index.unique()`now returns unique values as an`Index`of the appropriate`dtype``. (:issue:`13395`). Previously, most``Index`classes returned`np.ndarray`, and`DatetimeIndex`,`TimedeltaIndex`and`PeriodIndex`returned`Index`to keep metadata like timezone.  **Previous behavior**:`\`ipython In \[1\]: pd.Index(\[1, 2, 3\]).unique() Out\[1\]: array(\[1, 2, 3\])

</div>

>   - In \[2\]: pd.DatetimeIndex(\['2011-01-01', '2011-01-02',  
>     ...: '2011-01-03'\], tz='Asia/Tokyo').unique()
> 
> Out\[2\]: DatetimeIndex(\['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00', '2011-01-03 00:00:00+09:00'\], dtype='datetime64\[ns, Asia/Tokyo\]', freq=None)

**New behavior**:

<div class="ipython">

python

pd.Index(\[1, 2, 3\]).unique() pd.DatetimeIndex( \["2011-01-01", "2011-01-02", "2011-01-03"\], tz="Asia/Tokyo" ).unique()

</div>

<div id="whatsnew_0190.api.multiindex">

`MultiIndex` constructors, `groupby` and `set_index` preserve categorical dtypes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ``MultiIndex.from\_arrays`and`MultiIndex.from\_product`will now preserve categorical dtype in`MultiIndex``levels (:issue:`13743`, :issue:`13854`).  .. ipython:: python     cat = pd.Categorical(["a", "b"], categories=list("bac"))    lvl1 = ["foo", "bar"]    midx = pd.MultiIndex.from_arrays([cat, lvl1])    midx  **Previous behavior**:``\`ipython In \[4\]: midx.levels\[0\] Out\[4\]: Index(\['b', 'a', 'c'\], dtype='object')

</div>

> In \[5\]: midx.get\_level\_values\[0\] Out\[5\]: Index(\['a', 'b'\], dtype='object')

**New behavior**: the single level is now a `CategoricalIndex`:

<div class="ipython">

python

midx.levels\[0\] midx.get\_level\_values(0)

</div>

An analogous change has been made to `MultiIndex.from_product`. `` ` As a consequence, ``groupby`and`set\_index`also preserve categorical dtypes in indexes  .. ipython:: python     df = pd.DataFrame({"A": [0, 1], "B": [10, 11], "C": cat})    df_grouped = df.groupby(by=["A", "C"], observed=False).first()    df_set_idx = df.set_index(["A", "C"])  **Previous behavior**:`\`ipython In \[11\]: df\_grouped.index.levels\[1\] Out\[11\]: Index(\['b', 'a', 'c'\], dtype='object', name='C') In \[12\]: df\_grouped.reset\_index().dtypes Out\[12\]: A int64 C object B float64 dtype: object

> In \[13\]: df\_set\_idx.index.levels\[1\] Out\[13\]: Index(\['b', 'a', 'c'\], dtype='object', name='C') In \[14\]: df\_set\_idx.reset\_index().dtypes Out\[14\]: A int64 C object B int64 dtype: object

**New behavior**:

<div class="ipython">

python

df\_grouped.index.levels\[1\] df\_grouped.reset\_index().dtypes

df\_set\_idx.index.levels\[1\] df\_set\_idx.reset\_index().dtypes

</div>

<div id="whatsnew_0190.api.autogenerated_chunksize_index">

Function `read_csv` will progressively enumerate chunks `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  When `read_csv` is called with ``chunksize=n`and without specifying an index, each chunk used to have an independently generated index from`0`to`n-1`. They are now given instead a progressive index, starting from`0`for the first chunk, from`n``for the second, and so on, so that, when concatenated, they are identical to the result of calling `read_csv` without the``chunksize=``argument (:issue:`12185`).  .. ipython:: python     data = "A,B\n0,1\n2,3\n4,5\n6,7"  **Previous behavior**:``\`ipython In \[2\]: pd.concat(pd.read\_csv(StringIO(data), chunksize=2)) Out\[2\]: A B 0 0 1 1 2 3 0 4 5 1 6 7

</div>

**New behavior**:

<div class="ipython">

python

pd.concat(pd.read\_csv(StringIO(data), chunksize=2))

</div>

<div id="whatsnew_0190.sparse">

Sparse changes `` ` ^^^^^^^^^^^^^^  These changes allow pandas to handle sparse data with more dtypes, and for work to make a smoother experience with data handling.  Types ``int64`and`bool`support enhancements """""""""""""""""""""""""""""""""""""""""""""""""  Sparse data structures now gained enhanced support of`int64`and`bool`  `dtype``(:issue:`667`, :issue:`13849`).  Previously, sparse data were``float64`dtype by default, even if all inputs were of`int`or`bool`dtype. You had to specify`dtype`explicitly to create sparse data with`int64`dtype. Also,`fill\_value`had to be specified explicitly because the default was`np.nan`which doesn't appear in`int64`or`bool`data.`\`ipython In \[1\]: pd.SparseArray(\[1, 2, 0, 0\]) Out\[1\]: \[1.0, 2.0, 0.0, 0.0\] Fill: nan IntIndex Indices: array(\[0, 1, 2, 3\], dtype=int32)

</div>

> \# specifying int64 dtype, but all values are stored in sp\_values because \# fill\_value default is np.nan In \[2\]: pd.SparseArray(\[1, 2, 0, 0\], dtype=np.int64) Out\[2\]: \[1, 2, 0, 0\] Fill: nan IntIndex Indices: array(\[0, 1, 2, 3\], dtype=int32)
> 
> In \[3\]: pd.SparseArray(\[1, 2, 0, 0\], dtype=np.int64, fill\_value=0) Out\[3\]: \[1, 2, 0, 0\] Fill: 0 IntIndex Indices: array(\[0, 1\], dtype=int32)

As of v0.19.0, sparse data keeps the input dtype, and uses more appropriate `fill_value` defaults (`0` for `int64` dtype, `False` for `bool` dtype).

<div class="ipython">

python

pd.arrays.SparseArray(\[1, 2, 0, 0\], dtype=np.int64) pd.arrays.SparseArray(\[True, False, False, False\])

</div>

See the \[docs \<sparse.dtype\>\](\#docs-\<sparse.dtype\>) for more details.

Operators now preserve dtypes `` ` """""""""""""""""""""""""""""  - Sparse data structure now can preserve ``dtype``after arithmetic ops (:issue:`13848`)``\`python s = pd.SparseSeries(\[0, 2, 0, 1\], fill\_value=0, dtype=np.int64) s.dtype

> s + 1

  - Sparse data structure now support `astype` to convert internal `dtype` (`13900`)

<!-- end list -->

``` python
s = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)
s
s.astype(np.int64)
```

`astype` fails if data contains values which cannot be converted to specified `dtype`. `` ` Note that the limitation is applied to ``fill\_value`which default is`np.nan`.`\`ipython In \[7\]: pd.SparseSeries(\[1., np.nan, 2., np.nan\], fill\_value=np.nan).astype(np.int64) Out\[7\]: ValueError: unable to coerce current fill\_value nan to int64 dtype

Other sparse fixes `` ` """"""""""""""""""  - Subclassed ``SparseDataFrame`and`SparseSeries``now preserve class types when slicing or transposing. (:issue:`13787`) -``SparseArray`with`bool``dtype now supports logical (bool) operators (:issue:`14000`) - Bug in``SparseSeries`with`MultiIndex`  `\[\]`indexing may raise`IndexError``(:issue:`13144`) - Bug in``SparseSeries`with`MultiIndex`  `\[\]`indexing result may have normal`Index``(:issue:`13144`) - Bug in``SparseDataFrame`in which`axis=None`did not default to`axis=0``(:issue:`13048`) - Bug in``SparseSeries`and`SparseDataFrame`creation with`object`dtype may raise`TypeError``(:issue:`11633`) - Bug in``SparseDataFrame`doesn't respect passed`SparseArray`or`SparseSeries`'s dtype and`fill\_value``(:issue:`13866`) - Bug in``SparseArray`and`SparseSeries`don't apply ufunc to`fill\_value``(:issue:`13853`) - Bug in``SparseSeries.abs`incorrectly keeps negative`fill\_value``(:issue:`13853`) - Bug in single row slicing on multi-type``SparseDataFrame``s, types were previously forced to float (:issue:`13917`) - Bug in``SparseSeries``slicing changes integer dtype to float (:issue:`8292`) - Bug in``SparseDataFarme`comparison ops may raise`TypeError``(:issue:`13001`) - Bug in``SparseDataFarme.isnull`raises`ValueError``(:issue:`8276`) - Bug in``SparseSeries`representation with`bool`dtype may raise`IndexError``(:issue:`13110`) - Bug in``SparseSeries`and`SparseDataFrame`of`bool`or`int64`dtype may display its values like`float64``dtype (:issue:`13110`) - Bug in sparse indexing using``SparseArray`with`bool``dtype may return incorrect result  (:issue:`13985`) - Bug in``SparseArray`created from`SparseSeries`may lose`dtype``(:issue:`13999`) - Bug in``SparseSeries`comparison with dense returns normal`Series`rather than`SparseSeries``(:issue:`13999`)   .. _whatsnew_0190.indexer_dtype:  Indexer dtype changes ^^^^^^^^^^^^^^^^^^^^^  > **Note** >     This change only affects 64 bit python running on Windows, and only affects relatively advanced    indexing operations  Methods such as``Index.get\_indexer`that return an indexer array, coerce that array to a "platform int", so that it can be directly used in 3rd party library operations like`numpy.take`.  Previously, a platform int was defined as`[np.int]()`which corresponds to a C integer, but the correct type, and what is being used now, is`np.intp``, which corresponds to the C integer size that can hold a pointer (:issue:`3033`, :issue:`13972`).  These types are the same on many platform, but for 64 bit python on Windows,``[np.int]()`is 32 bits, and`np.intp`is 64 bits.  Changing this behavior improves performance for many operations on that platform.  **Previous behavior**:`\`ipython In \[1\]: i = pd.Index(\['a', 'b', 'c'\])

> In \[2\]: i.get\_indexer(\['b', 'b', 'c'\]).dtype Out\[2\]: dtype('int32')

**New behavior**:

``` ipython
In [1]: i = pd.Index(['a', 'b', 'c'])

In [2]: i.get_indexer(['b', 'b', 'c']).dtype
Out[2]: dtype('int64')
```

<div id="whatsnew_0190.api.other">

Other API changes `` ` ^^^^^^^^^^^^^^^^^  - ``Timestamp.to\_pydatetime`will issue a`UserWarning`when`warn=True``, and the instance has a non-zero number of nanoseconds, previously this would print a message to stdout (:issue:`14101`). -``Series.unique()`with datetime and timezone now returns return array of`Timestamp``with timezone (:issue:`13565`). -``Panel.to\_sparse()`will raise a`NotImplementedError``exception when called (:issue:`13778`). -``Index.reshape()`will raise a`NotImplementedError``exception when called (:issue:`12882`). -``.filter()``enforces mutual exclusion of the keyword arguments (:issue:`12399`). -``eval`'s upcasting rules for`float32`types have been updated to be more consistent with NumPy's rules.  New behavior will not upcast to`float64`if you multiply a pandas`float32``object by a scalar float64 (:issue:`12388`). - An``UnsupportedFunctionCall`error is now raised if NumPy ufuncs like`np.mean``are called on groupby or resample objects (:issue:`12811`). -``\_\_setitem\_\_`will no longer apply a callable rhs as a function instead of storing it. Call`where``directly to get the previous behavior (:issue:`13299`). - Calls to``.sample()`will respect the random seed set via`numpy.random.seed(n)``(:issue:`13161`) -``Styler.apply`is now more strict about the outputs your function must return. For`axis=0`or`axis=1`, the output shape must be identical. For`axis=None``, the output must be a DataFrame with identical columns and index labels (:issue:`13222`). -``Float64Index.astype(int)`will now raise`ValueError`if`Float64Index`contains`NaN``values (:issue:`13149`) -``TimedeltaIndex.astype(int)`and`DatetimeIndex.astype(int)`will now return`Int64Index`instead of`np.array``(:issue:`13209`) - Passing``Period`with multiple frequencies to normal`Index`now returns`Index`with`object``dtype (:issue:`13664`) -``PeriodIndex.fillna`with`Period`has different freq now coerces to`object``dtype (:issue:`13664`) - Faceted boxplots from``DataFrame.boxplot(by=col)`now return a`Series`when`return\_type`is not None. Previously these returned an`OrderedDict`. Note that when`return\_type=None``, the default, these still return a 2-D NumPy array (:issue:`12216`, :issue:`7096`). -``pd.read\_hdf`will now raise a`ValueError`instead of`KeyError`, if a mode other than`r`,`r+`and`a``is supplied. (:issue:`13623`) -``pd.read\_csv()`,`pd.read\_table()`, and`pd.read\_hdf()`raise the builtin`FileNotFoundError`exception for Python 3.x when called on a nonexistent file; this is back-ported as`IOError``in Python 2.x (:issue:`14086`) - More informative exceptions are passed through the csv parser. The exception type would now be the original exception type instead of``CParserError``(:issue:`13652`). -``pd.read\_csv()`in the C engine will now issue a`ParserWarning`or raise a`ValueError`when`sep``encoded is more than one character long (:issue:`14065`) -``DataFrame.values`will now return`float64`with a`DataFrame`of mixed`int64`and`uint64`dtypes, conforming to`np.find\_common\_type``(:issue:`10364`, :issue:`13917`) -``.groupby.groups`will now return a dictionary of`Index`objects, rather than a dictionary of`np.ndarray`or`lists``(:issue:`14293`)  .. _whatsnew_0190.deprecations:  Deprecations ~~~~~~~~~~~~ -``Series.reshape`and`Categorical.reshape``have been deprecated and will be removed in a subsequent release (:issue:`12882`, :issue:`12882`) -``PeriodIndex.to\_datetime`has been deprecated in favor of`PeriodIndex.to\_timestamp``(:issue:`8254`) -``Timestamp.to\_datetime`has been deprecated in favor of`Timestamp.to\_pydatetime``(:issue:`8254`) -``Index.to\_datetime`and`DatetimeIndex.to\_datetime`have been deprecated in favor of`pd.to\_datetime``(:issue:`8254`) -``pandas.core.datetools``module has been deprecated and will be removed in a subsequent release (:issue:`14094`) -``SparseList``has been deprecated and will be removed in a future version (:issue:`13784`) -``DataFrame.to\_html()`and`DataFrame.to\_latex()`have dropped the`colSpace`parameter in favor of`col\_space``(:issue:`13857`) -``DataFrame.to\_sql()`has deprecated the`flavor``parameter, as it is superfluous when SQLAlchemy is not installed (:issue:`13611`) - Deprecated``read\_csv`keywords:    -`compact\_ints`and`use\_unsigned``have been deprecated and will be removed in a future version (:issue:`13320`)   -``buffer\_lines``has been deprecated and will be removed in a future version (:issue:`13360`)   -``as\_recarray``has been deprecated and will be removed in a future version (:issue:`13373`)   -``skip\_footer`has been deprecated in favor of`skipfooter``and will be removed in a future version (:issue:`13349`)  - top-level``pd.ordered\_merge()`has been renamed to`pd.merge\_ordered()``and the original name will be removed in a future version (:issue:`13358`) -``Timestamp.offset`property (and named arg in the constructor), has been deprecated in favor of`freq``(:issue:`12160`) -``pd.tseries.util.pivot\_annual`is deprecated. Use`pivot\_table``as alternative, an example is [here <cookbook.pivot>](#here-<cookbook.pivot>) (:issue:`736`) -``pd.tseries.util.isleapyear`has been deprecated and will be removed in a subsequent release. Datetime-likes now have a`.is\_leap\_year``property (:issue:`13727`) -``Panel4D`and`PanelND``constructors are deprecated and will be removed in a future version. The recommended way to represent these types of n-dimensional data are with the `xarray package <http://xarray.pydata.org/en/stable/>`__. pandas provides a `~Panel4D.to_xarray` method to automate this conversion (:issue:`13564`). -``pandas.tseries.frequencies.get\_standard\_freq`is deprecated. Use`pandas.tseries.frequencies.to\_offset(freq).rule\_code``instead (:issue:`13874`) -``pandas.tseries.frequencies.to\_offset`'s`freqstr`keyword is deprecated in favor of`freq``(:issue:`13874`) -``Categorical.from\_array``has been deprecated and will be removed in a future version (:issue:`13854`)  .. _whatsnew_0190.prior_deprecations:  Removal of prior version deprecations/changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - The``SparsePanel``class has been removed (:issue:`13778`) - The``pd.sandbox`module has been removed in favor of the external library`pandas-qt``(:issue:`13670`) - The``pandas.io.data`and`pandas.io.wb``modules are removed in favor of   the `pandas-datareader package <https://github.com/pydata/pandas-datareader>`__ (:issue:`13724`). - The``pandas.tools.rplot``module has been removed in favor of   the `seaborn package <https://github.com/mwaskom/seaborn>`__ (:issue:`13855`) -``DataFrame.to\_csv()`has dropped the`engine``parameter, as was deprecated in 0.17.1 (:issue:`11274`, :issue:`13419`) -``DataFrame.to\_dict()`has dropped the`outtype`parameter in favor of`orient``(:issue:`13627`, :issue:`8486`) -``pd.Categorical`has dropped setting of the`ordered`attribute directly in favor of the`set\_ordered``method (:issue:`13671`) -``pd.Categorical`has dropped the`levels`attribute in favor of`categories``(:issue:`8376`) -``DataFrame.to\_sql()`has dropped the`mysql`option for the`flavor``parameter (:issue:`13611`) -``Panel.shift()`has dropped the`lags`parameter in favor of`periods``(:issue:`14041`) -``pd.Index`has dropped the`diff`method in favor of`difference``(:issue:`13669`) -``pd.DataFrame`has dropped the`to\_wide`method in favor of`to\_panel``(:issue:`14039`) -``Series.to\_csv`has dropped the`nanRep`parameter in favor of`na\_rep``(:issue:`13804`) -``Series.xs`,`DataFrame.xs`,`Panel.xs`,`Panel.major\_xs`, and`Panel.minor\_xs`have dropped the`copy``parameter (:issue:`13781`) -``str.split`has dropped the`return\_type`parameter in favor of`expand``(:issue:`13701`) - Removal of the legacy time rules (offset aliases), deprecated since 0.17.0 (this has been alias since 0.8.0) (:issue:`13590`, :issue:`13868`). Now legacy time rules raises``ValueError`. For the list of currently supported offsets, see [here <timeseries.offset_aliases>](#here-<timeseries.offset_aliases>). - The default value for the`return\_type`parameter for`DataFrame.plot.box`and`DataFrame.boxplot`changed from`None`to`"axes"``. These methods will now return a matplotlib axes by default instead of a dictionary of artists. See [here <visualization.box.return>](#here-<visualization.box.return>) (:issue:`6581`). - The``tquery`and`uquery`functions in the`pandas.io.sql``module are removed (:issue:`5950`).   .. _whatsnew_0190.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Improved performance of sparse``IntIndex.intersect``(:issue:`13082`) - Improved performance of sparse arithmetic with``BlockIndex`when the number of blocks are large, though recommended to use`IntIndex``in such cases (:issue:`13082`) - Improved performance of``DataFrame.quantile()``as it now operates per-block (:issue:`11623`) - Improved performance of float64 hash table operations, fixing some very slow indexing and groupby operations in python 3 (:issue:`13166`, :issue:`13334`) - Improved performance of``DataFrameGroupBy.transform``(:issue:`12737`) - Improved performance of``Index`and`Series`  `.duplicated``(:issue:`10235`) - Improved performance of``Index.difference``(:issue:`12044`) - Improved performance of``RangeIndex.is\_monotonic\_increasing`and`is\_monotonic\_decreasing``(:issue:`13749`) - Improved performance of datetime string parsing in``DatetimeIndex``(:issue:`13692`) - Improved performance of hashing``Period``(:issue:`12817`) - Improved performance of``factorize``of datetime with timezone (:issue:`13750`) - Improved performance of by lazily creating indexing hashtables on larger Indexes (:issue:`14266`) - Improved performance of``groupby.groups``(:issue:`14293`) - Unnecessary materializing of a MultiIndex when introspecting for memory usage (:issue:`14308`)  .. _whatsnew_0190.bug_fixes:  Bug fixes ~~~~~~~~~  - Bug in``groupby().shift()``, which could cause a segfault or corruption in rare circumstances when grouping by columns with missing values (:issue:`13813`) - Bug in``groupby().cumsum()`calculating`cumprod`when`axis=1``. (:issue:`13994`) - Bug in``pd.to\_timedelta()`in which the`errors``parameter was not being respected (:issue:`13613`) - Bug in``io.json.json\_normalize()``, where non-ascii keys raised an exception (:issue:`13213`) - Bug when passing a not-default-indexed``Series`as`xerr`or`yerr`in`.plot()``(:issue:`11858`) - Bug in area plot draws legend incorrectly if subplot is enabled or legend is moved after plot (matplotlib 1.5.0 is required to draw area plot legend properly) (:issue:`9161`, :issue:`13544`) - Bug in``DataFrame`assignment with an object-dtyped`Index``where the resultant column is mutable to the original object. (:issue:`13522`) - Bug in matplotlib``AutoDataFormatter``; this restores the second scaled formatting and re-adds micro-second scaled formatting (:issue:`13131`) - Bug in selection from a``HDFStore`with a fixed format and`start`and/or`stop``specified will now return the selected range (:issue:`8287`) - Bug in``Categorical.from\_codes()`where an unhelpful error was raised when an invalid`ordered``parameter was passed in (:issue:`14058`) - Bug in``Series``construction from a tuple of integers on windows not returning default dtype (int64) (:issue:`13646`) - Bug in``TimedeltaIndex``addition with a Datetime-like object where addition overflow was not being caught (:issue:`14068`) - Bug in``.groupby(..).resample(..)``when the same object is called multiple times (:issue:`13174`) - Bug in``.to\_records()``when index name is a unicode string (:issue:`13172`) - Bug in calling``.memory\_usage()``on object which doesn't implement (:issue:`12924`) - Regression in``Series.quantile`with nans (also shows up in`.median()`and`.describe()`); furthermore now names the`Series``with the quantile (:issue:`13098`, :issue:`13146`) - Bug in``SeriesGroupBy.transform``with datetime values and missing groups (:issue:`13191`) - Bug where empty``Series``were incorrectly coerced in datetime-like numeric operations (:issue:`13844`) - Bug in``Categorical`constructor when passed a`Categorical``containing datetimes with timezones (:issue:`14190`) - Bug in``Series.str.extractall()`with`str`index raises`ValueError``(:issue:`13156`) - Bug in``Series.str.extractall()``with single group and quantifier  (:issue:`13382`) - Bug in``DatetimeIndex`and`Period`subtraction raises`ValueError`or`AttributeError`rather than`TypeError``(:issue:`13078`) - Bug in``Index`and`Series`created with`NaN`and`NaT`mixed data may not have`datetime64``dtype  (:issue:`13324`) - Bug in``Index`and`Series`may ignore`np.datetime64('nat')`and`np.timdelta64('nat')``to infer dtype (:issue:`13324`) - Bug in``PeriodIndex`and`Period`subtraction raises`AttributeError``(:issue:`13071`) - Bug in``PeriodIndex`construction returning a`float64``index in some circumstances (:issue:`13067`) - Bug in``.resample(..)`with a`PeriodIndex`not changing its`freq``appropriately when empty (:issue:`13067`) - Bug in``.resample(..)`with a`PeriodIndex`not retaining its type or name with an empty`DataFrame``appropriately when empty (:issue:`13212`) - Bug in``groupby(..).apply(..)``when the passed function returns scalar values per group (:issue:`13468`). - Bug in``groupby(..).resample(..)``where passing some keywords would raise an exception (:issue:`13235`) - Bug in``.tz\_convert`on a tz-aware`DateTimeIndex``that relied on index being sorted for correct results (:issue:`13306`) - Bug in``.tz\_localize`with`dateutil.tz.tzlocal``may return incorrect result (:issue:`13583`) - Bug in``DatetimeTZDtype`dtype with`dateutil.tz.tzlocal``cannot be regarded as valid dtype (:issue:`13583`) - Bug in``pd.read\_hdf()``where attempting to load an HDF file with a single dataset, that had one or more categorical columns, failed unless the key argument was set to the name of the dataset. (:issue:`13231`) - Bug in``.rolling()`that allowed a negative integer window in construction of the`Rolling()``object, but would later fail on aggregation (:issue:`13383`) - Bug in``Series``indexing with tuple-valued data and a numeric index (:issue:`13509`) - Bug in printing``pd.DataFrame`where unusual elements with the`object``dtype were causing segfaults (:issue:`13717`) - Bug in ranking``Series``which could result in segfaults (:issue:`13445`) - Bug in various index types, which did not propagate the name of passed index (:issue:`12309`) - Bug in``DatetimeIndex`, which did not honour the`copy=True``(:issue:`13205`) - Bug in``DatetimeIndex.is\_normalized``returns incorrectly for normalized date_range in case of local timezones (:issue:`13459`) - Bug in``pd.concat`and`.append`may coerces`datetime64`and`timedelta`to`object`dtype containing python built-in`datetime`or`timedelta`rather than`Timestamp`or`Timedelta``(:issue:`13626`) - Bug in``PeriodIndex.append`may raises`AttributeError`when the result is`object``dtype (:issue:`13221`) - Bug in``CategoricalIndex.append`may accept normal`list``(:issue:`13626`) - Bug in``pd.concat`and`.append``with the same timezone get reset to UTC (:issue:`7795`) - Bug in``Series`and`DataFrame`  `.append`raises`AmbiguousTimeError``if data contains datetime near DST boundary (:issue:`13626`) - Bug in``DataFrame.to\_csv()``in which float values were being quoted even though quotations were specified for non-numeric values only (:issue:`12922`, :issue:`13259`) - Bug in``DataFrame.describe()`raising`ValueError``with only boolean columns (:issue:`13898`) - Bug in``MultiIndex``slicing where extra elements were returned when level is non-unique (:issue:`12896`) - Bug in``.str.replace`does not raise`TypeError``for invalid replacement (:issue:`13438`) - Bug in``MultiIndex.from\_arrays``which didn't check for input array lengths matching (:issue:`13599`) - Bug in``cartesian\_product`and`MultiIndex.from\_product``which may raise with empty input arrays (:issue:`12258`) - Bug in``pd.read\_csv()``which may cause a segfault or corruption when iterating in large chunks over a stream/file under rare circumstances (:issue:`13703`) - Bug in``pd.read\_csv()`which caused errors to be raised when a dictionary containing scalars is passed in for`na\_values``(:issue:`12224`) - Bug in``pd.read\_csv()``which caused BOM files to be incorrectly parsed by not ignoring the BOM (:issue:`4793`) - Bug in``pd.read\_csv()`with`engine='python'`which raised errors when a numpy array was passed in for`usecols``(:issue:`12546`) - Bug in``pd.read\_csv()`where the index columns were being incorrectly parsed when parsed as dates with a`thousands``parameter (:issue:`14066`) - Bug in``pd.read\_csv()`with`engine='python'`in which`NaN``values weren't being detected after data was converted to numeric values (:issue:`13314`) - Bug in``pd.read\_csv()`in which the`nrows``argument was not properly validated for both engines (:issue:`10476`) - Bug in``pd.read\_csv()`with`engine='python'``in which infinities of mixed-case forms were not being interpreted properly (:issue:`13274`) - Bug in``pd.read\_csv()`with`engine='python'`in which trailing`NaN``values were not being parsed (:issue:`13320`) - Bug in``pd.read\_csv()`with`engine='python'`when reading from a`tempfile.TemporaryFile``on Windows with Python 3 (:issue:`13398`) - Bug in``pd.read\_csv()`that prevents`usecols``kwarg from accepting single-byte unicode strings (:issue:`13219`) - Bug in``pd.read\_csv()`that prevents`usecols``from being an empty set (:issue:`13402`) - Bug in``pd.read\_csv()``in the C engine where the NULL character was not being parsed as NULL (:issue:`14012`) - Bug in``pd.read\_csv()`with`engine='c'`in which NULL`quotechar`was not accepted even though`quoting`was specified as`None``(:issue:`13411`) - Bug in``pd.read\_csv()`with`engine='c'``in which fields were not properly cast to float when quoting was specified as non-numeric (:issue:`13411`) - Bug in``pd.read\_csv()``in Python 2.x with non-UTF8 encoded, multi-character separated data (:issue:`3404`) - Bug in``pd.read\_csv()``, where aliases for utf-xx (e.g. UTF-xx, UTF_xx, utf_xx) raised UnicodeDecodeError (:issue:`13549`) - Bug in``pd.read\_csv`,`pd.read\_table`,`pd.read\_fwf`,`pd.read\_stata`and`pd.read\_sas`where files were opened by parsers but not closed if both`chunksize`and`iterator`were`None``. (:issue:`13940`) - Bug in``StataReader`,`StataWriter`,`XportReader`and`SAS7BDATReader``where a file was not properly closed when an error was raised. (:issue:`13940`) - Bug in``pd.pivot\_table()`where`margins\_name`is ignored when`aggfunc``is a list (:issue:`13354`) - Bug in``pd.Series.str.zfill`,`center`,`ljust`,`rjust`, and`pad`when passing non-integers, did not raise`TypeError``(:issue:`13598`) - Bug in checking for any null objects in a``TimedeltaIndex`, which always returned`True``(:issue:`13603`) - Bug in``Series`arithmetic raises`TypeError`if it contains datetime-like as`object``dtype (:issue:`13043`) - Bug``Series.isnull()`and`Series.notnull()`ignore`Period('NaT')``(:issue:`13737`) - Bug``Series.fillna()`and`Series.dropna()`don't affect to`Period('NaT')``(:issue:`13737` - Bug in``.fillna(value=np.nan)`incorrectly raises`KeyError`on a`category`dtyped`Series``(:issue:`14021`) - Bug in extension dtype creation where the created types were not is/identical (:issue:`13285`) - Bug in``.resample(..)``where incorrect warnings were triggered by IPython introspection (:issue:`13618`) - Bug in``NaT`-`Period`raises`AttributeError``(:issue:`13071`) - Bug in``Series`comparison may output incorrect result if rhs contains`NaT``(:issue:`9005`) - Bug in``Series`and`Index`comparison may output incorrect result if it contains`NaT`with`object``dtype (:issue:`13592`) - Bug in``Period`addition raises`TypeError`if`Period``is on right hand side (:issue:`13069`) - Bug in``Period`and`Series`or`Index`comparison raises`TypeError``(:issue:`13200`) - Bug in``pd.set\_eng\_float\_format()``that would prevent NaN and Inf from formatting (:issue:`11981`) - Bug in``.unstack`with`Categorical`dtype resets`.ordered`to`True``(:issue:`13249`) - Clean some compile time warnings in datetime parsing (:issue:`13607`) - Bug in``factorize`raises`AmbiguousTimeError``if data contains datetime near DST boundary (:issue:`13750`) - Bug in``.set\_index`raises`AmbiguousTimeError``if new index contains DST boundary and multi levels (:issue:`12920`) - Bug in``.shift`raises`AmbiguousTimeError``if data contains datetime near DST boundary (:issue:`13926`) - Bug in``pd.read\_hdf()`returns incorrect result when a`DataFrame`with a`categorical``column and a query which doesn't match any values (:issue:`13792`) - Bug in``.iloc``when indexing with a non lexsorted MultiIndex (:issue:`13797`) - Bug in``.loc`when indexing with date strings in a reverse sorted`DatetimeIndex``(:issue:`14316`) - Bug in``Series``comparison operators when dealing with zero dim NumPy arrays (:issue:`13006`) - Bug in``.combine\_first`may return incorrect`dtype``(:issue:`7630`, :issue:`10567`) - Bug in``groupby`where`apply`returns different result depending on whether first result is`None``or not (:issue:`12824`) - Bug in``groupby(..).nth()`where the group key is included inconsistently if called after`.head()/.tail()``(:issue:`12839`) - Bug in``.to\_html`,`.to\_latex`and`.to\_string`silently ignore custom datetime formatter passed through the`formatters``key word (:issue:`10690`) - Bug in``DataFrame.iterrows()`, not yielding a`Series``subclasse if defined (:issue:`13977`) - Bug in``pd.to\_numeric`when`errors='coerce'``and input contains non-hashable objects (:issue:`13324`) - Bug in invalid``Timedelta`arithmetic and comparison may raise`ValueError`rather than`TypeError``(:issue:`13624`) - Bug in invalid datetime parsing in``to\_datetime`and`DatetimeIndex`may raise`TypeError`rather than`ValueError``(:issue:`11169`, :issue:`11287`) - Bug in``Index`created with tz-aware`Timestamp`and mismatched`tz``option incorrectly coerces timezone (:issue:`13692`) - Bug in``DatetimeIndex`with nanosecond frequency does not include timestamp specified with`end``(:issue:`13672`) - Bug in``Series`when setting a slice with a`np.timedelta64``(:issue:`14155`) - Bug in``Index`raises`OutOfBoundsDatetime`if`datetime`exceeds`datetime64\[ns\]`bounds, rather than coercing to`object``dtype (:issue:`13663`) - Bug in``Index`may ignore specified`datetime64`or`timedelta64`passed as`dtype``(:issue:`13981`) - Bug in``RangeIndex`can be created without no arguments rather than raises`TypeError``(:issue:`13793`) - Bug in``.value\_counts()`raises`OutOfBoundsDatetime`if data exceeds`datetime64\[ns\]``bounds (:issue:`13663`) - Bug in``DatetimeIndex`may raise`OutOfBoundsDatetime`if input`np.datetime64`has other unit than`ns``(:issue:`9114`) - Bug in``Series`creation with`np.datetime64`which has other unit than`ns`as`object``dtype results in incorrect values (:issue:`13876`) - Bug in``resample``with timedelta data where data was casted to float (:issue:`13119`). - Bug in``pd.isnull()`  `pd.notnull()`raise`TypeError`if input datetime-like has other unit than`ns``(:issue:`13389`) - Bug in``pd.merge()`may raise`TypeError`if input datetime-like has other unit than`ns``(:issue:`13389`) - Bug in``HDFStore`/`read\_hdf()`discarded`DatetimeIndex.name`if`tz``was set (:issue:`13884`) - Bug in``Categorical.remove\_unused\_categories()`changes`.codes``dtype to platform int (:issue:`13261`) - Bug in``groupby`with`as\_index=False``returns all NaN's when grouping on multiple columns including a categorical one (:issue:`13204`) - Bug in``df.groupby(...)\[...\]`where getitem with`Int64Index``raised an error (:issue:`13731`) - Bug in the CSS classes assigned to``DataFrame.style`for index names. Previously they were assigned`"col\_heading level\<n\> col\<c\>"`where`n`was the number of levels + 1. Now they are assigned`"index\_name level\<n\>"`, where`n`is the correct level for that MultiIndex. - Bug where`pd.read\_gbq()`could throw`ImportError: No module named discovery``as a result of a naming conflict with another python package called apiclient  (:issue:`13454`) - Bug in``Index.union``returns an incorrect result with a named empty index (:issue:`13432`) - Bugs in``Index.difference`and`DataFrame.join``raise in Python3 when using mixed-integer indexes (:issue:`13432`, :issue:`12814`) - Bug in subtract tz-aware``datetime.datetime`from tz-aware`datetime64``series (:issue:`14088`) - Bug in``.to\_excel()``when DataFrame contains a MultiIndex which contains a label with a NaN value (:issue:`13511`) - Bug in invalid frequency offset string like "D1", "-2-3H" may not raise``ValueError``(:issue:`13930`) - Bug in``concat`and`groupby`for hierarchical frames with`RangeIndex``levels (:issue:`13542`). - Bug in``Series.str.contains()`for Series containing only`NaN`values of`object``dtype (:issue:`14171`) - Bug in``agg()`function on groupby dataframe changes dtype of`datetime64\[ns\]`column to`float64``(:issue:`12821`) - Bug in using NumPy ufunc with``PeriodIndex`to add or subtract integer raise`IncompatibleFrequency`. Note that using standard operator like`+`or`-``is recommended, because standard operators use more efficient path (:issue:`13980`) - Bug in operations on``NaT`returning`float`instead of`datetime64\[ns\]``(:issue:`12941`) - Bug in``Series`flexible arithmetic methods (like`.add()`) raises`ValueError`when`axis=None``(:issue:`13894`) - Bug in``DataFrame.to\_csv()`with`MultiIndex``columns in which a stray empty line was added (:issue:`6618`) - Bug in``DatetimeIndex`,`TimedeltaIndex`and`PeriodIndex.equals()`may return`True`when input isn't`Index``but contains the same values (:issue:`13107`) - Bug in assignment against datetime with timezone may not work if it contains datetime near DST boundary (:issue:`14146`) - Bug in``pd.eval()`and`HDFStore``query truncating long float literals with python 2 (:issue:`14241`) - Bug in``Index`raises`KeyError``displaying incorrect column when column is not in the df and columns contains duplicate values (:issue:`13822`) - Bug in``Period`and`PeriodIndex``creating wrong dates when frequency has combined offset aliases (:issue:`13874`) - Bug in``.to\_string()`when called with an integer`line\_width`and`index=False`raises an UnboundLocalError exception because`idx`referenced before assignment. - Bug in`eval()`where the`resolvers``argument would not accept a list (:issue:`14095`) - Bugs in``stack`,`get\_dummies`,`make\_axis\_dummies``which don't preserve categorical dtypes in (multi)indexes (:issue:`13854`) -``PeriodIndex`can now accept`list`and`array`which contains`pd.NaT``(:issue:`13430`) - Bug in``df.groupby`where`.median()``returns arbitrary values if grouped dataframe contains empty bins (:issue:`13629`) - Bug in``Index.copy()`where`name\`<span class="title-ref"> parameter was ignored (:issue:\`14302</span>)

</div>

## Contributors

<div class="contributors">

v0.18.1..v0.19.0

</div>

---

v0.19.1.md

---

# Version 0.19.1 (November 3, 2016)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a minor bug-fix release from 0.19.0 and includes some small regression fixes, bug fixes and performance improvements. We recommend that all users upgrade to this version.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.19.1

</div>

## Performance improvements

  - Fixed performance regression in factorization of `Period` data (`14338`)
  - Fixed performance regression in `Series.asof(where)` when `where` is a scalar (`14461`)
  - Improved performance in `DataFrame.asof(where)` when `where` is a scalar (`14461`)
  - Improved performance in `.to_json()` when `lines=True` (`14408`)
  - Improved performance in certain types of `loc` indexing with a MultiIndex (`14551`).

## Bug fixes

  - Source installs from PyPI will now again work without `cython` installed, as in previous versions (`14204`)
  - Compat with Cython 0.25 for building (`14496`)
  - Fixed regression where user-provided file handles were closed in `read_csv` (c engine) (`14418`).
  - Fixed regression in `DataFrame.quantile` when missing values where present in some columns (`14357`).
  - Fixed regression in `Index.difference` where the `freq` of a `DatetimeIndex` was incorrectly set (`14323`)
  - Added back `pandas.core.common.array_equivalent` with a deprecation warning (`14555`).
  - Bug in `pd.read_csv` for the C engine in which quotation marks were improperly parsed in skipped rows (`14459`)
  - Bug in `pd.read_csv` for Python 2.x in which Unicode quote characters were no longer being respected (`14477`)
  - Fixed regression in `Index.append` when categorical indices were appended (`14545`).
  - Fixed regression in `pd.DataFrame` where constructor fails when given dict with `None` value (`14381`)
  - Fixed regression in `DatetimeIndex._maybe_cast_slice_bound` when index is empty (`14354`).
  - Bug in localizing an ambiguous timezone when a boolean is passed (`14402`)
  - Bug in `TimedeltaIndex` addition with a Datetime-like object where addition overflow in the negative direction was not being caught (`14068`, `14453`)
  - Bug in string indexing against data with `object` `Index` may raise `AttributeError` (`14424`)
  - Correctly raise `ValueError` on empty input to `pd.eval()` and `df.query()` (`13139`)
  - Bug in `RangeIndex.intersection` when result is a empty set (`14364`).
  - Bug in groupby-transform broadcasting that could cause incorrect dtype coercion (`14457`)
  - Bug in `Series.__setitem__` which allowed mutating read-only arrays (`14359`).
  - Bug in `DataFrame.insert` where multiple calls with duplicate columns can fail (`14291`)
  - `pd.merge()` will raise `ValueError` with non-boolean parameters in passed boolean type arguments (`14434`)
  - Bug in `Timestamp` where dates very near the minimum (1677-09) could underflow on creation (`14415`)
  - Bug in `pd.concat` where names of the `keys` were not propagated to the resulting `MultiIndex` (`14252`)
  - Bug in `pd.concat` where `axis` cannot take string parameters `'rows'` or `'columns'` (`14369`)
  - Bug in `pd.concat` with dataframes heterogeneous in length and tuple `keys` (`14438`)
  - Bug in `MultiIndex.set_levels` where illegal level values were still set after raising an error (`13754`)
  - Bug in `DataFrame.to_json` where `lines=True` and a value contained a `}` character (`14391`)
  - Bug in `df.groupby` causing an `AttributeError` when grouping a single index frame by a column and the index level (`14327`)
  - Bug in `df.groupby` where `TypeError` raised when `pd.Grouper(key=...)` is passed in a list (`14334`)
  - Bug in `pd.pivot_table` may raise `TypeError` or `ValueError` when `index` or `columns` is not scalar and `values` is not specified (`14380`)

## Contributors

<div class="contributors">

v0.19.0..v0.19.1

</div>

---

v0.19.2.md

---

# Version 0.19.2 (December 24, 2016)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a minor bug-fix release in the 0.19.x series and includes some small regression fixes, bug fixes and performance improvements. We recommend that all users upgrade to this version.

Highlights include:

  - Compatibility with Python 3.6
  - Added a [Pandas Cheat Sheet](https://github.com/pandas-dev/pandas/tree/main/doc/cheatsheet/Pandas_Cheat_Sheet.pdf). (`13202`).

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.19.2

</div>

## Enhancements

The `pd.merge_asof()`, added in 0.19.0, gained some improvements:

  - `pd.merge_asof()` gained `left_index`/`right_index` and `left_by`/`right_by` arguments (`14253`)
  - `pd.merge_asof()` can take multiple columns in `by` parameter and has specialized dtypes for better performance (`13936`)

## Performance improvements

  - Performance regression with `PeriodIndex` (`14822`)
  - Performance regression in indexing with getitem (`14930`)
  - Improved performance of `.replace()` (`12745`)
  - Improved performance `Series` creation with a datetime index and dictionary data (`14894`)

## Bug fixes

  - Compat with python 3.6 for pickling of some offsets (`14685`)
  - Compat with python 3.6 for some indexing exception types (`14684`, `14689`)
  - Compat with python 3.6 for deprecation warnings in the test suite (`14681`)
  - Compat with python 3.6 for Timestamp pickles (`14689`)
  - Compat with `dateutil==2.6.0`; segfault reported in the testing suite (`14621`)
  - Allow `nanoseconds` in `Timestamp.replace` as a kwarg (`14621`)
  - Bug in `pd.read_csv` in which aliasing was being done for `na_values` when passed in as a dictionary (`14203`)
  - Bug in `pd.read_csv` in which column indices for a dict-like `na_values` were not being respected (`14203`)
  - Bug in `pd.read_csv` where reading files fails, if the number of headers is equal to the number of lines in the file (`14515`)
  - Bug in `pd.read_csv` for the Python engine in which an unhelpful error message was being raised when multi-char delimiters were not being respected with quotes (`14582`)
  - Fix bugs (`14734`, `13654`) in `pd.read_sas` and `pandas.io.sas.sas7bdat.SAS7BDATReader` that caused problems when reading a SAS file incrementally.
  - Bug in `pd.read_csv` for the Python engine in which an unhelpful error message was being raised when `skipfooter` was not being respected by Python's CSV library (`13879`)
  - Bug in `.fillna()` in which timezone aware datetime64 values were incorrectly rounded (`14872`)
  - Bug in `.groupby(..., sort=True)` of a non-lexsorted MultiIndex when grouping with multiple levels (`14776`)
  - Bug in `pd.cut` with negative values and a single bin (`14652`)
  - Bug in `pd.to_numeric` where a 0 was not unsigned on a `downcast='unsigned'` argument (`14401`)
  - Bug in plotting regular and irregular timeseries using shared axes (`sharex=True` or `ax.twinx()`) (`13341`, `14322`).
  - Bug in not propagating exceptions in parsing invalid datetimes, noted in python 3.6 (`14561`)
  - Bug in resampling a `DatetimeIndex` in local TZ, covering a DST change, which would raise `AmbiguousTimeError` (`14682`)
  - Bug in indexing that transformed `RecursionError` into `KeyError` or `IndexingError` (`14554`)
  - Bug in `HDFStore` when writing a `MultiIndex` when using `data_columns=True` (`14435`)
  - Bug in `HDFStore.append()` when writing a `Series` and passing a `min_itemsize` argument containing a value for the `index` (`11412`)
  - Bug when writing to a `HDFStore` in `table` format with a `min_itemsize` value for the `index` and without asking to append (`10381`)
  - Bug in `Series.groupby.nunique()` raising an `IndexError` for an empty `Series` (`12553`)
  - Bug in `DataFrame.nlargest` and `DataFrame.nsmallest` when the index had duplicate values (`13412`)
  - Bug in clipboard functions on linux with python2 with unicode and separators (`13747`)
  - Bug in clipboard functions on Windows 10 and python 3 (`14362`, `12807`)
  - Bug in `.to_clipboard()` and Excel compat (`12529`)
  - Bug in `DataFrame.combine_first()` for integer columns (`14687`).
  - Bug in `pd.read_csv()` in which the `dtype` parameter was not being respected for empty data (`14712`)
  - Bug in `pd.read_csv()` in which the `nrows` parameter was not being respected for large input when using the C engine for parsing (`7626`)
  - Bug in `pd.merge_asof()` could not handle timezone-aware DatetimeIndex when a tolerance was specified (`14844`)
  - Explicit check in `to_stata` and `StataWriter` for out-of-range values when writing doubles (`14618`)
  - Bug in `.plot(kind='kde')` which did not drop missing values to generate the KDE Plot, instead generating an empty plot. (`14821`)
  - Bug in `unstack()` if called with a list of column(s) as an argument, regardless of the dtypes of all columns, they get coerced to `object` (`11847`)

## Contributors

<div class="contributors">

v0.19.1..v0.19.2

</div>

---

v0.20.0.md

---

# Version 0.20.1 (May 5, 2017)

{{ header }}

This is a major release from 0.19.2 and includes a number of API changes, deprecations, new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

Highlights include:

  - New `.agg()` API for Series/DataFrame similar to the groupby-rolling-resample API's, see \[here \<whatsnew\_0200.enhancements.agg\>\](\#here-\<whatsnew\_0200.enhancements.agg\>)
  - Integration with the `feather-format`, including a new top-level `pd.read_feather()` and `DataFrame.to_feather()` method, see \[here \<io.feather\>\](\#here-\<io.feather\>).
  - The `.ix` indexer has been deprecated, see \[here \<whatsnew\_0200.api\_breaking.deprecate\_ix\>\](\#here-\<whatsnew\_0200.api\_breaking.deprecate\_ix\>)
  - `Panel` has been deprecated, see \[here \<whatsnew\_0200.api\_breaking.deprecate\_panel\>\](\#here-\<whatsnew\_0200.api\_breaking.deprecate\_panel\>)
  - Addition of an `IntervalIndex` and `Interval` scalar type, see \[here \<whatsnew\_0200.enhancements.intervalindex\>\](\#here-\<whatsnew\_0200.enhancements.intervalindex\>)
  - Improved user API when grouping by index levels in `.groupby()`, see \[here \<whatsnew\_0200.enhancements.groupby\_access\>\](\#here-\<whatsnew\_0200.enhancements.groupby\_access\>)
  - Improved support for `UInt64` dtypes, see \[here \<whatsnew\_0200.enhancements.uint64\_support\>\](\#here-\<whatsnew\_0200.enhancements.uint64\_support\>)
  - A new orient for JSON serialization, `orient='table'`, that uses the Table Schema spec and that gives the possibility for a more interactive repr in the Jupyter Notebook, see \[here \<whatsnew\_0200.enhancements.table\_schema\>\](\#here-\<whatsnew\_0200.enhancements.table\_schema\>)
  - Experimental support for exporting styled DataFrames (`DataFrame.style`) to Excel, see \[here \<whatsnew\_0200.enhancements.style\_excel\>\](\#here-\<whatsnew\_0200.enhancements.style\_excel\>)
  - Window binary corr/cov operations now return a MultiIndexed `DataFrame` rather than a `Panel`, as `Panel` is now deprecated, see \[here \<whatsnew\_0200.api\_breaking.rolling\_pairwise\>\](\#here-\<whatsnew\_0200.api\_breaking.rolling\_pairwise\>)
  - Support for S3 handling now uses `s3fs`, see \[here \<whatsnew\_0200.api\_breaking.s3\>\](\#here-\<whatsnew\_0200.api\_breaking.s3\>)
  - Google BigQuery support now uses the `pandas-gbq` library, see \[here \<whatsnew\_0200.api\_breaking.gbq\>\](\#here-\<whatsnew\_0200.api\_breaking.gbq\>)

\> **Warning** \> pandas has changed the internal structure and layout of the code base. This can affect imports that are not from the top-level `pandas.*` namespace, please see the changes \[here \<whatsnew\_0200.privacy\>\](\#here-\<whatsnew\_0200.privacy\>).

Check the \[API Changes \<whatsnew\_0200.api\_breaking\>\](\#api-changes-\<whatsnew\_0200.api\_breaking\>) and \[deprecations \<whatsnew\_0200.deprecations\>\](\#deprecations-\<whatsnew\_0200.deprecations\>) before updating.

\> **Note** \> This is a combined release for 0.20.0 and 0.20.1. Version 0.20.1 contains one additional change for backwards-compatibility with downstream projects using pandas' `utils` routines. (`16250`)

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.20.0

</div>

## New features

### Method `agg` API for DataFrame/Series

Series & DataFrame have been enhanced to support the aggregation API. This is a familiar API from groupby, window operations, and resampling. This allows aggregation operations in a concise way by using <span class="title-ref">\~DataFrame.agg</span> and <span class="title-ref">\~DataFrame.transform</span>. The full documentation is \[here \<basics.aggregate\>\](\#here-\<basics.aggregate\>) (`1623`).

Here is a sample

<div class="ipython">

python

  - df = pd.DataFrame(np.random.randn(10, 3), columns=\['A', 'B', 'C'\],  
    index=pd.date\_range('1/1/2000', periods=10))

df.iloc\[3:7\] = np.nan df

</div>

One can operate using string function names, callables, lists, or dictionaries of these.

Using a single function is equivalent to `.apply`.

<div class="ipython">

python

df.agg('sum')

</div>

Multiple aggregations with a list of functions.

<div class="ipython">

python

df.agg(\['sum', 'min'\])

</div>

Using a dict provides the ability to apply specific aggregations per column. You will get a matrix-like output of all of the aggregators. The output has one column per unique function. Those functions applied to a particular column will be `NaN`:

<div class="ipython">

python

df.agg({'A': \['sum', 'min'\], 'B': \['min', 'max'\]})

</div>

The API also supports a `.transform()` function for broadcasting results.

<div class="ipython" data-okwarning="">

python

df.transform(\['abs', lambda x: x - x.min()\])

</div>

When presented with mixed dtypes that cannot be aggregated, `.agg()` will only take the valid aggregations. This is similar to how groupby `.agg()` works. (`15015`)

<div class="ipython">

python

  - df = pd.DataFrame({'A': \[1, 2, 3\],  
    'B': \[1., 2., 3.\], 'C': \['foo', 'bar', 'baz'\], 'D': pd.date\_range('20130101', periods=3)})

df.dtypes

</div>

`` `python    In [10]: df.agg(['min', 'sum'])    Out[10]:         A    B          C          D    min  1  1.0        bar 2013-01-01    sum  6  6.0  foobarbaz        NaT  .. _whatsnew_0200.enhancements.dataio_dtype:  Keyword argument ``dtype`for data IO`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The `'python'` engine for <span class="title-ref">read\_csv</span>, as well as the <span class="title-ref">read\_fwf</span> function for parsing fixed-width text files and <span class="title-ref">read\_excel</span> for parsing Excel files, now accept the `dtype` keyword argument for specifying the types of specific columns (`14295`). See the \[io docs \<io.dtypes\>\](\#io-docs-\<io.dtypes\>) for more information.

<div class="ipython" data-suppress="">

python

from io import StringIO

</div>

<div class="ipython">

python

data = "a bn1 2n3 4" pd.read\_fwf(StringIO(data)).dtypes pd.read\_fwf(StringIO(data), dtype={'a': 'float64', 'b': 'object'}).dtypes

</div>

### Method `.to_datetime()` has gained an `origin` parameter

<span class="title-ref">to\_datetime</span> has gained a new parameter, `origin`, to define a reference date from where to compute the resulting timestamps when parsing numerical values with a specific `unit` specified. (`11276`, `11745`)

For example, with 1960-01-01 as the starting date:

<div class="ipython">

python

pd.to\_datetime(\[1, 2, 3\], unit='D', origin=pd.Timestamp('1960-01-01'))

</div>

The default is set at `origin='unix'`, which defaults to `1970-01-01 00:00:00`, which is commonly called 'unix epoch' or POSIX time. This was the previous default, so this is a backward compatible change.

<div class="ipython">

python

pd.to\_datetime(\[1, 2, 3\], unit='D')

</div>

### GroupBy enhancements

Strings passed to `DataFrame.groupby()` as the `by` parameter may now reference either column names or index level names. Previously, only column names could be referenced. This allows to easily group by a column and index level at the same time. (`5677`)

<div class="ipython">

python

  - arrays = \[\['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'\],  
    \['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'\]\]

index = pd.MultiIndex.from\_arrays(arrays, names=\['first', 'second'\])

  - df = pd.DataFrame({'A': \[1, 1, 1, 1, 2, 2, 3, 3\],  
    'B': np.arange(8)}, index=index)

df

df.groupby(\['second', 'A'\]).sum()

</div>

### Better support for compressed URLs in `read_csv`

The compression code was refactored (`12688`). As a result, reading dataframes from URLs in <span class="title-ref">read\_csv</span> or <span class="title-ref">read\_table</span> now supports additional compression methods: `xz`, `bz2`, and `zip` (`14570`). Previously, only `gzip` compression was supported. By default, compression of URLs and paths are now inferred using their file extensions. Additionally, support for bz2 compression in the python 2 C-engine improved (`14874`).

<div class="ipython">

python

  - url = ('[https://github.com/{repo}/raw/{branch}/{path}](https://github.com/%7Brepo%7D/raw/%7Bbranch%7D/%7Bpath%7D)'
    
      - .format(repo='pandas-dev/pandas',  
        branch='main', path='pandas/tests/io/parser/data/salaries.csv.bz2'))

\# default, infer compression df = pd.read\_csv(url, sep='t', compression='infer') \# explicitly specify compression df = pd.read\_csv(url, sep='t', compression='bz2') df.head(2)

</div>

### Pickle file IO now supports compression

<span class="title-ref">read\_pickle</span>, <span class="title-ref">DataFrame.to\_pickle</span> and <span class="title-ref">Series.to\_pickle</span> can now read from and write to compressed pickle files. Compression methods can be an explicit parameter or be inferred from the file extension. See \[the docs here. \<io.pickle.compression\>\](\#the-docs-here.-\<io.pickle.compression\>)

<div class="ipython">

python

  - df = pd.DataFrame({'A': np.random.randn(1000),  
    'B': 'foo', 'C': pd.date\_range('20130101', periods=1000, freq='s')})

</div>

Using an explicit compression type

<div class="ipython">

python

df.to\_pickle("data.pkl.compress", compression="gzip") rt = pd.read\_pickle("data.pkl.compress", compression="gzip") rt.head()

</div>

The default is to infer the compression type from the extension (`compression='infer'`):

<div class="ipython">

python

df.to\_pickle("data.pkl.gz") rt = pd.read\_pickle("data.pkl.gz") rt.head() df\["A"\].to\_pickle("s1.pkl.bz2") rt = pd.read\_pickle("s1.pkl.bz2") rt.head()

</div>

<div class="ipython" data-suppress="">

python

import os os.remove("data.pkl.compress") os.remove("data.pkl.gz") os.remove("s1.pkl.bz2")

</div>

### UInt64 support improved

pandas has significantly improved support for operations involving unsigned, or purely non-negative, integers. Previously, handling these integers would result in improper rounding or data-type casting, leading to incorrect results. Notably, a new numerical index, `UInt64Index`, has been created (`14937`)

`` `ipython    In [1]: idx = pd.UInt64Index([1, 2, 3])    In [2]: df = pd.DataFrame({'A': ['a', 'b', 'c']}, index=idx)    In [3]: df.index    Out[3]: UInt64Index([1, 2, 3], dtype='uint64')  - Bug in converting object elements of array-like objects to unsigned 64-bit integers (:issue:`4471`, :issue:`14982`) ``<span class="title-ref"> - Bug in </span><span class="title-ref">Series.unique()</span><span class="title-ref"> in which unsigned 64-bit integers were causing overflow (:issue:\`14721</span>) - Bug in `DataFrame` construction in which unsigned 64-bit integer elements were being converted to objects (`14881`) - Bug in `pd.read_csv()` in which unsigned 64-bit integer elements were being improperly converted to the wrong data types (`14983`) - Bug in `pd.unique()` in which unsigned 64-bit integers were causing overflow (`14915`) - Bug in `pd.value_counts()` in which unsigned 64-bit integers were being erroneously truncated in the output (`14934`)

### GroupBy on categoricals

In previous versions, `.groupby(..., sort=False)` would fail with a `ValueError` when grouping on a categorical series with some categories not appearing in the data. (`13179`)

<div class="ipython">

python

chromosomes = [np.r]()\[np.arange(1, 23).astype(str), \['X', 'Y'\]\] df = pd.DataFrame({ 'A': np.random.randint(100), 'B': np.random.randint(100), 'C': np.random.randint(100), 'chromosomes': pd.Categorical(np.random.choice(chromosomes, 100), categories=chromosomes, ordered=True)}) df

</div>

**Previous behavior**:

`` `ipython    In [3]: df[df.chromosomes != '1'].groupby('chromosomes', observed=False, sort=False).sum()    ---------------------------------------------------------------------------    ValueError: items in new_categories are not the same as in old categories  **New behavior**:  .. ipython:: python     df[df.chromosomes != '1'].groupby('chromosomes', observed=False, sort=False).sum()  .. _whatsnew_0200.enhancements.table_schema:  Table schema output ``\` ^^^^^^^^^^^^^^^^^^^

The new orient `'table'` for <span class="title-ref">DataFrame.to\_json</span> will generate a [Table Schema](http://specs.frictionlessdata.io/json-table-schema/) compatible string representation of the data.

`` `ipython    In [38]: df = pd.DataFrame(       ....: {'A': [1, 2, 3],       ....:  'B': ['a', 'b', 'c'],       ....:  'C': pd.date_range('2016-01-01', freq='d', periods=3)},       ....: index=pd.Index(range(3), name='idx'))    In [39]: df    Out[39]:         A  B          C    idx    0    1  a 2016-01-01    1    2  b 2016-01-02    2    3  c 2016-01-03     [3 rows x 3 columns]     In [40]: df.to_json(orient='table')    Out[40]:    '{"schema":{"fields":[{"name":"idx","type":"integer"},{"name":"A","type":"integer"},{"name":"B","type":"string"},{"name":"C","type":"datetime"}],"primaryKey":["idx"],"pandas_version":"1.4.0"},"data":[{"idx":0,"A":1,"B":"a","C":"2016-01-01T00:00:00.000"},{"idx":1,"A":2,"B":"b","C":"2016-01-02T00:00:00.000"},{"idx":2,"A":3,"B":"c","C":"2016-01-03T00:00:00.000"}]}'   See [IO: Table Schema for more information <io.table_schema>](#io:-table-schema-for-more-information-<io.table_schema>).  Additionally, the repr for ``DataFrame`and`Series`can now publish`<span class="title-ref"> this JSON Table schema representation of the Series or DataFrame if you are using IPython (or another frontend like \`nteract</span>\_ using the Jupyter messaging protocol). This gives frontends like the Jupyter notebook and [nteract](https://nteract.io/) more flexibility in how they display pandas objects, since they have more information about the data. You must enable this by setting the `display.html.table_schema` option to `True`.

### SciPy sparse matrix from/to SparseDataFrame

pandas now supports creating sparse dataframes directly from `scipy.sparse.spmatrix` instances. See the \[documentation \<sparse.scipysparse\>\](\#documentation-\<sparse.scipysparse\>) for more information. (`4343`)

All sparse formats are supported, but matrices that are not in `COOrdinate <scipy.sparse>` format will be converted, copying data as needed.

`` `python    from scipy.sparse import csr_matrix    arr = np.random.random(size=(1000, 5))    arr[arr < .9] = 0    sp_arr = csr_matrix(arr)    sp_arr    sdf = pd.SparseDataFrame(sp_arr)    sdf  To convert a ``SparseDataFrame`back to sparse SciPy matrix in COO format, you can use:  .. code-block:: python     sdf.to_coo()  .. _whatsnew_0200.enhancements.style_excel:  Excel output for styled DataFrames`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Experimental support has been added to export `DataFrame.style` formats to Excel using the `openpyxl` engine. (`15530`)

For example, after running the following, `styled.xlsx` renders as below:

<div class="ipython">

python

np.random.seed(24) df = pd.DataFrame({'A': np.linspace(1, 10, 10)}) df = pd.concat(\[df, pd.DataFrame(np.random.RandomState(24).randn(10, 4), columns=list('BCDE'))\], axis=1) df.iloc\[0, 2\] = np.nan df styled = (df.style .map(lambda val: 'color:red;' if val \< 0 else 'color:black;') .highlight\_max()) styled.to\_excel('styled.xlsx', engine='openpyxl')

</div>

![image](../_static/style-excel.png)

<div class="ipython" data-suppress="">

python

import os os.remove('styled.xlsx')

</div>

See the \[Style documentation \</user\_guide/style.ipynb\#Export-to-Excel\>\](\#style-documentation-\</user\_guide/style.ipynb\#export-to-excel\>) for more detail.

### IntervalIndex

pandas has gained an `IntervalIndex` with its own dtype, `interval` as well as the `Interval` scalar type. These allow first-class support for interval notation, specifically as a return type for the categories in <span class="title-ref">cut</span> and <span class="title-ref">qcut</span>. The `IntervalIndex` allows some unique indexing, see the \[docs \<advanced.intervalindex\>\](\#docs-\<advanced.intervalindex\>). (`7640`, `8625`)

\> **Warning** \> These indexing behaviors of the IntervalIndex are provisional and may change in a future version of pandas. Feedback on usage is welcome.

Previous behavior:

The returned categories were strings, representing Intervals

`` `ipython    In [1]: c = pd.cut(range(4), bins=2)     In [2]: c    Out[2]:    [(-0.003, 1.5], (-0.003, 1.5], (1.5, 3], (1.5, 3]]    Categories (2, object): [(-0.003, 1.5] < (1.5, 3]]     In [3]: c.categories    Out[3]: Index(['(-0.003, 1.5]', '(1.5, 3]'], dtype='object')  New behavior:  .. ipython:: python     c = pd.cut(range(4), bins=2)    c    c.categories  Furthermore, this allows one to bin *other* data with these same bins, with ``NaN`representing a missing`\` value similar to other dtypes.

<div class="ipython">

python

pd.cut(\[0, 3, 5, 1\], bins=c.categories)

</div>

An `IntervalIndex` can also be used in `Series` and `DataFrame` as the index.

<div class="ipython">

python

  - df = pd.DataFrame({'A': range(4),  
    'B': pd.cut(\[0, 3, 1, 1\], bins=c.categories) }).set\_index('B')

df

</div>

Selecting via a specific interval:

<div class="ipython">

python

df.loc\[pd.Interval(1.5, 3.0)\]

</div>

Selecting via a scalar value that is contained *in* the intervals.

<div class="ipython">

python

df.loc\[0\]

</div>

### Other enhancements

  - `DataFrame.rolling()` now accepts the parameter `closed='right'|'left'|'both'|'neither'` to choose the rolling window-endpoint closedness. See the \[documentation \<window.endpoints\>\](\#documentation-\<window.endpoints\>) (`13965`)
  - Integration with the `feather-format`, including a new top-level `pd.read_feather()` and `DataFrame.to_feather()` method, see \[here \<io.feather\>\](\#here-\<io.feather\>).
  - `Series.str.replace()` now accepts a callable, as replacement, which is passed to `re.sub` (`15055`)
  - `Series.str.replace()` now accepts a compiled regular expression as a pattern (`15446`)
  - `Series.sort_index` accepts parameters `kind` and `na_position` (`13589`, `14444`)
  - `DataFrame` and `DataFrame.groupby()` have gained a `nunique()` method to count the distinct values over an axis (`14336`, `15197`).
  - `DataFrame` has gained a `melt()` method, equivalent to `pd.melt()`, for unpivoting from a wide to long format (`12640`).
  - `pd.read_excel()` now preserves sheet order when using `sheetname=None` (`9930`)
  - Multiple offset aliases with decimal points are now supported (e.g. `0.5min` is parsed as `30s`) (`8419`)
  - `.isnull()` and `.notnull()` have been added to `Index` object to make them more consistent with the `Series` API (`15300`)
  - New `UnsortedIndexError` (subclass of `KeyError`) raised when indexing/slicing into an unsorted MultiIndex (`11897`). This allows differentiation between errors due to lack of sorting or an incorrect key. See \[here \<advanced.unsorted\>\](\#here-\<advanced.unsorted\>)
  - `MultiIndex` has gained a `.to_frame()` method to convert to a `DataFrame` (`12397`)
  - `pd.cut` and `pd.qcut` now support datetime64 and timedelta64 dtypes (`14714`, `14798`)
  - `pd.qcut` has gained the `duplicates='raise'|'drop'` option to control whether to raise on duplicated edges (`7751`)
  - `Series` provides a `to_excel` method to output Excel files (`8825`)
  - The `usecols` argument in `pd.read_csv()` now accepts a callable function as a value (`14154`)
  - The `skiprows` argument in `pd.read_csv()` now accepts a callable function as a value (`10882`)
  - The `nrows` and `chunksize` arguments in `pd.read_csv()` are supported if both are passed (`6774`, `15755`)
  - `DataFrame.plot` now prints a title above each subplot if `suplots=True` and `title` is a list of strings (`14753`)
  - `DataFrame.plot` can pass the matplotlib 2.0 default color cycle as a single string as color parameter, see [here](http://matplotlib.org/2.0.0/users/colors.html#cn-color-selection). (`15516`)
  - `Series.interpolate()` now supports timedelta as an index type with `method='time'` (`6424`)
  - Addition of a `level` keyword to `DataFrame/Series.rename` to rename labels in the specified level of a MultiIndex (`4160`).
  - `DataFrame.reset_index()` will now interpret a tuple `index.name` as a key spanning across levels of `columns`, if this is a `MultiIndex` (`16164`)
  - `Timedelta.isoformat` method added for formatting Timedeltas as an [ISO 8601 duration](https://en.wikipedia.org/wiki/ISO_8601#Durations). See the \[Timedelta docs \<timedeltas.isoformat\>\](\#timedelta-docs-\<timedeltas.isoformat\>) (`15136`)
  - `.select_dtypes()` now allows the string `datetimetz` to generically select datetimes with tz (`14910`)
  - The `.to_latex()` method will now accept `multicolumn` and `multirow` arguments to use the accompanying LaTeX enhancements
  - `pd.merge_asof()` gained the option `direction='backward'|'forward'|'nearest'` (`14887`)
  - `Series/DataFrame.asfreq()` have gained a `fill_value` parameter, to fill missing values (`3715`).
  - `Series/DataFrame.resample.asfreq` have gained a `fill_value` parameter, to fill missing values during resampling (`3715`).
  - <span class="title-ref">pandas.util.hash\_pandas\_object</span> has gained the ability to hash a `MultiIndex` (`15224`)
  - `Series/DataFrame.squeeze()` have gained the `axis` parameter. (`15339`)
  - `DataFrame.to_excel()` has a new `freeze_panes` parameter to turn on Freeze Panes when exporting to Excel (`15160`)
  - `pd.read_html()` will parse multiple header rows, creating a MultiIndex header. (`13434`).
  - HTML table output skips `colspan` or `rowspan` attribute if equal to 1. (`15403`)
  - <span class="title-ref">pandas.io.formats.style.Styler</span> template now has blocks for easier extension, see the \[example notebook \</user\_guide/style.ipynb\#Subclassing\>\](\#example-notebook-\</user\_guide/style.ipynb\#subclassing\>) (`15649`)
  - <span class="title-ref">Styler.render() \<pandas.io.formats.style.Styler.render\></span> now accepts `**kwargs` to allow user-defined variables in the template (`15649`)
  - Compatibility with Jupyter notebook 5.0; MultiIndex column labels are left-aligned and MultiIndex row-labels are top-aligned (`15379`)
  - `TimedeltaIndex` now has a custom date-tick formatter specifically designed for nanosecond level precision (`8711`)
  - `pd.api.types.union_categoricals` gained the `ignore_ordered` argument to allow ignoring the ordered attribute of unioned categoricals (`13410`). See the \[categorical union docs \<categorical.union\>\](\#categorical-union-docs-\<categorical.union\>) for more information.
  - `DataFrame.to_latex()` and `DataFrame.to_string()` now allow optional header aliases. (`15536`)
  - Re-enable the `parse_dates` keyword of `pd.read_excel()` to parse string columns as dates (`14326`)
  - Added `.empty` property to subclasses of `Index`. (`15270`)
  - Enabled floor division for `Timedelta` and `TimedeltaIndex` (`15828`)
  - `pandas.io.json.json_normalize()` gained the option `errors='ignore'|'raise'`; the default is `errors='raise'` which is backward compatible. (`14583`)
  - `pandas.io.json.json_normalize()` with an empty `list` will return an empty `DataFrame` (`15534`)
  - `pandas.io.json.json_normalize()` has gained a `sep` option that accepts `str` to separate joined fields; the default is ".", which is backward compatible. (`14883`)
  - <span class="title-ref">MultiIndex.remove\_unused\_levels</span> has been added to facilitate \[removing unused levels \<advanced.shown\_levels\>\](\#removing-unused-levels-\<advanced.shown\_levels\>). (`15694`)
  - `pd.read_csv()` will now raise a `ParserError` error whenever any parsing error occurs (`15913`, `15925`)
  - `pd.read_csv()` now supports the `error_bad_lines` and `warn_bad_lines` arguments for the Python parser (`15925`)
  - The `display.show_dimensions` option can now also be used to specify whether the length of a `Series` should be shown in its repr (`7117`).
  - `parallel_coordinates()` has gained a `sort_labels` keyword argument that sorts class labels and the colors assigned to them (`15908`)
  - Options added to allow one to turn on/off using `bottleneck` and `numexpr`, see \[here \<basics.accelerate\>\](\#here-\<basics.accelerate\>) (`16157`)
  - `DataFrame.style.bar()` now accepts two more options to further customize the bar chart. Bar alignment is set with `align='left'|'mid'|'zero'`, the default is "left", which is backward compatible; You can now pass a list of `color=[color_negative, color_positive]`. (`14757`)

## Backwards incompatible API changes

### Possible incompatibility for HDF5 formats created with pandas \< 0.13.0

`pd.TimeSeries` was deprecated officially in 0.17.0, though has already been an alias since 0.13.0. It has been dropped in favor of `pd.Series`. (`15098`).

This *may* cause HDF5 files that were created in prior versions to become unreadable if `pd.TimeSeries` was used. This is most likely to be for pandas \< 0.13.0. If you find yourself in this situation. You can use a recent prior version of pandas to read in your HDF5 files, then write them out again after applying the procedure below.

`` `ipython    In [2]: s = pd.TimeSeries([1, 2, 3], index=pd.date_range('20130101', periods=3))     In [3]: s    Out[3]:    2013-01-01    1    2013-01-02    2    2013-01-03    3    Freq: D, dtype: int64     In [4]: type(s)    Out[4]: pandas.core.series.TimeSeries     In [5]: s = pd.Series(s)     In [6]: s    Out[6]:    2013-01-01    1    2013-01-02    2    2013-01-03    3    Freq: D, dtype: int64     In [7]: type(s)    Out[7]: pandas.core.series.Series   .. _whatsnew_0200.api_breaking.index_map:  Map on Index types now return other Index types ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

`map` on an `Index` now returns an `Index`, not a numpy array (`12766`)

<div class="ipython">

python

idx = pd.Index(\[1, 2\]) idx mi = pd.MultiIndex.from\_tuples(\[(1, 2), (2, 4)\]) mi

</div>

Previous behavior:

`` `ipython    In [5]: idx.map(lambda x: x * 2)    Out[5]: array([2, 4])     In [6]: idx.map(lambda x: (x, x * 2))    Out[6]: array([(1, 2), (2, 4)], dtype=object)     In [7]: mi.map(lambda x: x)    Out[7]: array([(1, 2), (2, 4)], dtype=object)     In [8]: mi.map(lambda x: x[0])    Out[8]: array([1, 2])  New behavior:  .. ipython:: python     idx.map(lambda x: x * 2)    idx.map(lambda x: (x, x * 2))     mi.map(lambda x: x)     mi.map(lambda x: x[0]) ``map`on a`Series`with`datetime64`values may return`int64`dtypes rather than`int32`.. code-block:: ipython     In [64]: s = pd.Series(pd.date_range('2011-01-02T00:00', '2011-01-02T02:00', freq='H')       ....:               .tz_localize('Asia/Tokyo'))       ....:     In [65]: s    Out[65]:    0   2011-01-02 00:00:00+09:00    1   2011-01-02 01:00:00+09:00    2   2011-01-02 02:00:00+09:00    Length: 3, dtype: datetime64[ns, Asia/Tokyo]  Previous behavior:  .. code-block:: ipython     In [9]: s.map(lambda x: x.hour)    Out[9]:    0    0    1    1    2    2    dtype: int32  New behavior:  .. code-block:: ipython     In [66]: s.map(lambda x: x.hour)    Out[66]:    0    0    1    1    2    2    Length: 3, dtype: int64   .. _whatsnew_0200.api_breaking.index_dt_field:  Accessing datetime fields of Index now return Index`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The datetime-related attributes (see \[here \<timeseries.components\>\](\#here-\<timeseries.components\>) for an overview) of `DatetimeIndex`, `PeriodIndex` and `TimedeltaIndex` previously returned numpy arrays. They will now return a new `Index` object, except in the case of a boolean field, where the result will still be a boolean ndarray. (`15022`)

Previous behaviour:

`` `ipython    In [1]: idx = pd.date_range("2015-01-01", periods=5, freq='10H')     In [2]: idx.hour    Out[2]: array([ 0, 10, 20,  6, 16], dtype=int32)  New behavior:  .. code-block:: ipython     In [67]: idx = pd.date_range("2015-01-01", periods=5, freq='10H')     In [68]: idx.hour    Out[68]: Index([0, 10, 20, 6, 16], dtype='int32')  This has the advantage that specific ``Index`methods are still available on the`<span class="title-ref"> result. On the other hand, this might have backward incompatibilities: e.g. compared to numpy arrays, </span><span class="title-ref">Index</span><span class="title-ref"> objects are not mutable. To get the original ndarray, you can always convert explicitly using </span><span class="title-ref">np.asarray(idx.hour)</span>\`.

### pd.unique will now be consistent with extension types

In prior versions, using <span class="title-ref">Series.unique</span> and <span class="title-ref">pandas.unique</span> on `Categorical` and tz-aware data-types would yield different return types. These are now made consistent. (`15903`)

  - Datetime tz-aware
    
    Previous behaviour:
    
      - \`\`\`ipython  
        \# Series In \[5\]: pd.Series(\[pd.Timestamp('20160101', tz='US/Eastern'), ...: pd.Timestamp('20160101', tz='US/Eastern')\]).unique() Out\[5\]: array(\[Timestamp('2016-01-01 00:00:00-0500', tz='US/Eastern')\], dtype=object)
        
          - In \[6\]: pd.unique(pd.Series(\[pd.Timestamp('20160101', tz='US/Eastern'),  
            ...: pd.Timestamp('20160101', tz='US/Eastern')\]))
        
        Out\[6\]: array(\['2016-01-01T05:00:00.000000000'\], dtype='datetime64\[ns\]')
        
        \# Index In \[7\]: pd.Index(\[pd.Timestamp('20160101', tz='US/Eastern'), ...: pd.Timestamp('20160101', tz='US/Eastern')\]).unique() Out\[7\]: DatetimeIndex(\['2016-01-01 00:00:00-05:00'\], dtype='datetime64\[ns, US/Eastern\]', freq=None)
        
          - In \[8\]: pd.unique(\[pd.Timestamp('20160101', tz='US/Eastern'),  
            ...: pd.Timestamp('20160101', tz='US/Eastern')\])
        
        Out\[8\]: array(\['2016-01-01T05:00:00.000000000'\], dtype='datetime64\[ns\]')
    
    New behavior:
    
    <div class="ipython">
    
    python
    
    \# Series, returns an array of Timestamp tz-aware pd.Series(\[pd.Timestamp(r'20160101', tz=r'US/Eastern'), pd.Timestamp(r'20160101', tz=r'US/Eastern')\]).unique() pd.unique(pd.Series(\[pd.Timestamp('20160101', tz='US/Eastern'), pd.Timestamp('20160101', tz='US/Eastern')\]))
    
    \# Index, returns a DatetimeIndex pd.Index(\[pd.Timestamp('20160101', tz='US/Eastern'), pd.Timestamp('20160101', tz='US/Eastern')\]).unique() pd.unique(pd.Index(\[pd.Timestamp('20160101', tz='US/Eastern'), pd.Timestamp('20160101', tz='US/Eastern')\]))
    
    </div>

  - Categoricals
    
    Previous behaviour:
    
    ``` ipython
    In [1]: pd.Series(list('baabc'), dtype='category').unique()
    Out[1]:
    [b, a, c]
    Categories (3, object): [b, a, c]
    
    In [2]: pd.unique(pd.Series(list('baabc'), dtype='category'))
    Out[2]: array(['b', 'a', 'c'], dtype=object)
    ```
    
    New behavior:
    
    <div class="ipython">
    
    python
    
    \# returns a Categorical pd.Series(list('baabc'), dtype='category').unique() pd.unique(pd.Series(list('baabc'), dtype='category'))
    
    </div>

<div id="whatsnew_0200.api_breaking.s3">

S3 file handling `` ` ^^^^^^^^^^^^^^^^  pandas now uses `s3fs <http://s3fs.readthedocs.io/>`_ for handling S3 connections. This shouldn't break any code. However, since ``s3fs`is not a required dependency, you will need to install it separately, like`boto``in prior versions of pandas. (:issue:`11915`).  .. _whatsnew_0200.api_breaking.partial_string_indexing:  Partial string indexing changes ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  [DatetimeIndex Partial String Indexing <timeseries.partialindexing>](#datetimeindex-partial-string-indexing-<timeseries.partialindexing>) now works as an exact match, provided that string resolution coincides with index resolution, including a case when both are seconds (:issue:`14826`). See [Slice vs. Exact Match <timeseries.slice_vs_exact_match>](#slice-vs.-exact-match-<timeseries.slice_vs_exact_match>) for details.  .. ipython:: python     df = pd.DataFrame({'a': [1, 2, 3]}, pd.DatetimeIndex(['2011-12-31 23:59:59',                                                          '2012-01-01 00:00:00',                                                          '2012-01-01 00:00:01'])) Previous behavior:``\`ipython In \[4\]: df\['2011-12-31 23:59:59'\] Out\[4\]: a 2011-12-31 23:59:59 1

</div>

> In \[5\]: df\['a'\]\['2011-12-31 23:59:59'\] Out\[5\]: 2011-12-31 23:59:59 1 Name: a, dtype: int64

New behavior:

``` ipython
In [4]: df['2011-12-31 23:59:59']
KeyError: '2011-12-31 23:59:59'

In [5]: df['a']['2011-12-31 23:59:59']
Out[5]: 1
```

<div id="whatsnew_0200.api_breaking.concat_dtypes">

Concat of different float dtypes will not automatically upcast `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously, ``concat`of multiple objects with different`float`dtypes would automatically upcast results to a dtype of`float64``. Now the smallest acceptable dtype will be used (:issue:`13247`)  .. ipython:: python     df1 = pd.DataFrame(np.array([1.0], dtype=np.float32, ndmin=2))    df1.dtypes     df2 = pd.DataFrame(np.array([np.nan], dtype=np.float32, ndmin=2))    df2.dtypes  Previous behavior:``\`ipython In \[7\]: pd.concat(\[df1, df2\]).dtypes Out\[7\]: 0 float64 dtype: object

</div>

New behavior:

<div class="ipython">

python

pd.concat(\[df1, df2\]).dtypes

</div>

<div id="whatsnew_0200.api_breaking.gbq">

pandas Google BigQuery support has moved `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  pandas has split off Google BigQuery support into a separate package ``pandas-gbq`. You can`conda install pandas-gbq -c conda-forge`or`pip install pandas-gbq``to get it. The functionality of `read_gbq` and `DataFrame.to_gbq` remain the same with the currently released version of``pandas-gbq=0.1.4``. Documentation is now hosted `here <https://pandas-gbq.readthedocs.io/>`__  (:issue:`15347`)  .. _whatsnew_0200.api_breaking.memory_usage:  Memory usage for Index is more accurate ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In previous versions, showing``.memory\_usage()`on a pandas structure that has an index, would only include actual index values and not include structures that facilitated fast indexing. This will generally be different for`Index`and`MultiIndex``and less-so for other index types. (:issue:`15237`)  Previous behavior:``\`ipython In \[8\]: index = pd.Index(\['foo', 'bar', 'baz'\])

</div>

> In \[9\]: index.memory\_usage(deep=True) Out\[9\]: 180
> 
> In \[10\]: index.get\_loc('foo') Out\[10\]: 0
> 
> In \[11\]: index.memory\_usage(deep=True) Out\[11\]: 180

New behavior:

``` ipython
In [8]: index = pd.Index(['foo', 'bar', 'baz'])

In [9]: index.memory_usage(deep=True)
Out[9]: 180

In [10]: index.get_loc('foo')
Out[10]: 0

In [11]: index.memory_usage(deep=True)
Out[11]: 260
```

<div id="whatsnew_0200.api_breaking.sort_index">

DataFrame.sort\_index changes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In certain cases, calling ``.sort\_index()`on a MultiIndexed DataFrame would return the *same* DataFrame without seeming to sort. This would happen with a`lexsorted``, but non-monotonic levels. (:issue:`15622`, :issue:`15687`, :issue:`14015`, :issue:`13431`, :issue:`15797`)  This is *unchanged* from prior versions, but shown for illustration purposes:``\`python In \[81\]: df = pd.DataFrame(np.arange(6), columns=\['value'\], ....: index=pd.MultiIndex.from\_product(\[list('BA'), range(3)\])) ....: In \[82\]: df

</div>

>   - Out\[82\]:  
>     value
> 
>   - B 0 0  
>     1 1 2 2
> 
>   - A 0 3  
>     1 4 2 5
> 
> \[6 rows x 1 columns\]

``` python
In [87]: df.index.is_lexsorted()
Out[87]: False

In [88]: df.index.is_monotonic
Out[88]: False
```

Sorting works as expected

<div class="ipython">

python

df.sort\_index()

</div>

``` python
In [90]: df.sort_index().index.is_lexsorted()
Out[90]: True

In [91]: df.sort_index().index.is_monotonic
Out[91]: True
```

However, this example, which has a non-monotonic 2nd level, `` ` doesn't behave as desired.  .. ipython:: python     df = pd.DataFrame({'value': [1, 2, 3, 4]},                      index=pd.MultiIndex([['a', 'b'], ['bb', 'aa']],                                          [[0, 0, 1, 1], [0, 1, 0, 1]]))    df  Previous behavior: ``\`python In \[11\]: df.sort\_index() Out\[11\]: value a bb 1 aa 2 b bb 3 aa 4

> In \[14\]: df.sort\_index().index.is\_lexsorted() Out\[14\]: True
> 
> In \[15\]: df.sort\_index().index.is\_monotonic Out\[15\]: False

New behavior:

``` python
In [94]: df.sort_index()
Out[94]:
      value
a aa      2
  bb      1
b aa      4
  bb      3

[4 rows x 1 columns]

In [95]: df.sort_index().index.is_lexsorted()
Out[95]: True

In [96]: df.sort_index().index.is_monotonic
Out[96]: True
```

<div id="whatsnew_0200.api_breaking.groupby_describe">

GroupBy describe formatting `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^  The output formatting of ``groupby.describe()`now labels the`describe()`metrics in the columns instead of the index. This format is consistent with`groupby.agg()``when applying multiple functions at once. (:issue:`4792`)  Previous behavior:``\`ipython In \[1\]: df = pd.DataFrame({'A': \[1, 1, 2, 2\], 'B': \[1, 2, 3, 4\]})

</div>

> In \[2\]: df.groupby('A').describe() Out\[2\]: B A 1 count 2.000000 mean 1.500000 std 0.707107 min 1.000000 25% 1.250000 50% 1.500000 75% 1.750000 max 2.000000 2 count 2.000000 mean 3.500000 std 0.707107 min 3.000000 25% 3.250000 50% 3.500000 75% 3.750000 max 4.000000
> 
> In \[3\]: df.groupby('A').agg(\["mean", "std", "min", "max"\]) Out\[3\]: B mean std amin amax A 1 1.5 0.707107 1 2 2 3.5 0.707107 3 4

New behavior:

<div class="ipython">

python

df = pd.DataFrame({'A': \[1, 1, 2, 2\], 'B': \[1, 2, 3, 4\]})

df.groupby('A').describe()

df.groupby('A').agg(\["mean", "std", "min", "max"\])

</div>

<div id="whatsnew_0200.api_breaking.rolling_pairwise">

Window binary corr/cov operations return a MultiIndex DataFrame `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  A binary window operation, like ``.corr()`or`.cov()`, when operating on a`.rolling(..)`,`.expanding(..)`, or`.ewm(..)`object, will now return a 2-level`MultiIndexed DataFrame`rather than a`Panel`, as`Panel`is now deprecated, see [here <whatsnew_0200.api_breaking.deprecate_panel>](#here-<whatsnew_0200.api_breaking.deprecate_panel>). These are equivalent in function, but a MultiIndexed`DataFrame``enjoys more support in pandas. See the section on [Windowed Binary Operations <window.cov_corr>](#windowed-binary-operations-<window.cov_corr>) for more information. (:issue:`15677`)  .. ipython:: python     np.random.seed(1234)    df = pd.DataFrame(np.random.rand(100, 2),                      columns=pd.Index(['A', 'B'], name='bar'),                      index=pd.date_range('20160101',                                          periods=100, freq='D', name='foo'))    df.tail()  Previous behavior:``\`ipython In \[2\]: df.rolling(12).corr() Out\[2\]: \<class 'pandas.core.panel.Panel'\> Dimensions: 100 (items) x 2 (major\_axis) x 2 (minor\_axis) Items axis: 2016-01-01 00:00:00 to 2016-04-09 00:00:00 Major\_axis axis: A to B Minor\_axis axis: A to B

</div>

New behavior:

<div class="ipython">

python

res = df.rolling(12).corr() res.tail()

</div>

Retrieving a correlation matrix for a cross-section

<div class="ipython">

python

df.rolling(12).corr().loc\['2016-04-07'\]

</div>

<div id="whatsnew_0200.api_breaking.hdfstore_where">

HDFStore where string comparison `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In previous versions most types could be compared to string column in a ``HDFStore`usually resulting in an invalid comparison, returning an empty result frame. These comparisons will now raise a`TypeError``(:issue:`15492`)  .. ipython:: python     df = pd.DataFrame({'unparsed_date': ['2014-01-01', '2014-01-01']})    df.to_hdf('store.h5', key='key', format='table', data_columns=True)    df.dtypes  Previous behavior:``\`ipython In \[4\]: pd.read\_hdf('store.h5', 'key', where='unparsed\_date \> ts') File "\<string\>", line 1 (unparsed\_date \> 1970-01-01 00:00:01.388552400) ^ SyntaxError: invalid token

</div>

New behavior:

``` ipython
In [18]: ts = pd.Timestamp('2014-01-01')

In [19]: pd.read_hdf('store.h5', 'key', where='unparsed_date > ts')
TypeError: Cannot compare 2014-01-01 00:00:00 of
type <class 'pandas.tslib.Timestamp'> to string column
```

<div class="ipython" data-suppress="">

python

import os os.remove('store.h5')

</div>

<div id="whatsnew_0200.api_breaking.index_order">

Index.intersection and inner join now preserve the order of the left Index `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `Index.intersection` now preserves the order of the calling ``Index`(left) instead of the other`Index``(right) (:issue:`15582`). This affects inner joins, `DataFrame.join` and `merge`, and the``.align`method.  -`Index.intersection`.. ipython:: python       left = pd.Index([2, 1, 0])      left      right = pd.Index([1, 2, 3])      right    Previous behavior:`\`ipython In \[4\]: left.intersection(right) Out\[4\]: Int64Index(\[1, 2\], dtype='int64')

</div>

> New behavior:
> 
> <div class="ipython">
> 
> python
> 
> left.intersection(right)
> 
> </div>

  - `DataFrame.join` and `pd.merge`
    
    <div class="ipython">
    
    python
    
    left = pd.DataFrame({'a': \[20, 10, 0\]}, index=\[2, 1, 0\]) left right = pd.DataFrame({'b': \[100, 200, 300\]}, index=\[1, 2, 3\]) right
    
    </div>
    
    Previous behavior:
    
    ``` ipython
    In [4]: left.join(right, how='inner')
    Out[4]:
       a    b
    1  10  100
    2  20  200
    ```
    
    New behavior:
    
    <div class="ipython">
    
    python
    
    left.join(right, how='inner')
    
    </div>

<div id="whatsnew_0200.api_breaking.pivot_table">

Pivot table always returns a DataFrame `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The documentation for `pivot_table` states that a ``DataFrame`is *always* returned. Here a bug is fixed that allowed this to return a`Series``under certain circumstance. (:issue:`4386`)  .. ipython:: python     df = pd.DataFrame({'col1': [3, 4, 5],                       'col2': ['C', 'D', 'E'],                       'col3': [1, 3, 9]})    df  Previous behavior:``\`ipython In \[2\]: df.pivot\_table('col1', index=\['col3', 'col2'\], aggfunc="sum") Out\[2\]: col3 col2 1 C 3 3 D 4 9 E 5 Name: col1, dtype: int64

</div>

New behavior:

<div class="ipython">

python

df.pivot\_table('col1', index=\['col3', 'col2'\], aggfunc="sum")

</div>

<div id="whatsnew_0200.api">

Other API changes `` ` ^^^^^^^^^^^^^^^^^  - ``numexpr``version is now required to be >= 2.4.6 and it will not be used at all if this requisite is not fulfilled (:issue:`15213`). -``CParserError`has been renamed to`ParserError`in`pd.read\_csv()``and will be removed in the future (:issue:`12665`) -``SparseArray.cumsum()`and`SparseSeries.cumsum()`will now always return`SparseArray`and`SparseSeries``respectively (:issue:`12855`) -``DataFrame.applymap()`with an empty`DataFrame`will return a copy of the empty`DataFrame`instead of a`Series``(:issue:`8222`) -``Series.map()`now respects default values of dictionary subclasses with a`\_\_missing\_\_`method, such as`collections.Counter``(:issue:`15999`) -``.loc`has compat with`.ix``for accepting iterators, and NamedTuples (:issue:`15120`) -``interpolate()`and`fillna()`will raise a`ValueError`if the`limit``keyword argument is not greater than 0. (:issue:`9217`) -``pd.read\_csv()`will now issue a`ParserWarning`whenever there are conflicting values provided by the`dialect``parameter and the user (:issue:`14898`) -``pd.read\_csv()`will now raise a`ValueError``for the C engine if the quote character is larger than one byte (:issue:`11592`) -``inplace`arguments now require a boolean value, else a`ValueError``is thrown (:issue:`14189`) -``pandas.api.types.is\_datetime64\_ns\_dtype`will now report`True`on a tz-aware dtype, similar to`pandas.api.types.is\_datetime64\_any\_dtype`-`DataFrame.asof()`will return a null filled`Series`instead the scalar`NaN``if a match is not found (:issue:`15118`) - Specific support for``copy.copy()`and`copy.deepcopy()``functions on NDFrame objects (:issue:`15444`) -``Series.sort\_values()`accepts a one element list of bool for consistency with the behavior of`DataFrame.sort\_values()``(:issue:`15604`) -``.merge()`and`.join()`on`category``dtype columns will now preserve the category dtype when possible (:issue:`10409`) -``SparseDataFrame.default\_fill\_value`will be 0, previously was`nan`in the return from`pd.get\_dummies(..., sparse=True)``(:issue:`15594`) - The default behaviour of``Series.str.match`has changed from extracting   groups to matching the pattern. The extracting behaviour was deprecated   since pandas version 0.13.0 and can be done with the`Series.str.extract``method (:issue:`5224`). As a consequence, the``as\_indexer`keyword is   ignored (no longer needed to specify the new behaviour) and is deprecated. -`NaT`will now correctly report`False`for datetimelike boolean operations such as`is\_month\_start``(:issue:`15781`) -``NaT`will now correctly return`np.nan`for`Timedelta`and`Period`accessors such as`days`and`quarter``(:issue:`15782`) -``NaT`will now returns`NaT`for`tz\_localize`and`tz\_convert``methods (:issue:`15830`) -``DataFrame`and`Panel`constructors with invalid input will now raise`ValueError`rather than`PandasError``, if called with scalar inputs and not axes (:issue:`15541`) -``DataFrame`and`Panel`constructors with invalid input will now raise`ValueError`rather than`pandas.core.common.PandasError`, if called with scalar inputs and not axes; The exception`PandasError``is removed as well. (:issue:`15541`) - The exception``pandas.core.common.AmbiguousIndexError``is removed as it is not referenced (:issue:`15541`)   .. _whatsnew_0200.privacy:  Reorganization of the library: privacy changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. _whatsnew_0200.privacy.extensions:  Modules privacy has changed ^^^^^^^^^^^^^^^^^^^^^^^^^^^  Some formerly public python/c/c++/cython extension modules have been moved and/or renamed. These are all removed from the public API. Furthermore, the``pandas.core`,`pandas.compat`, and`pandas.util``top-level modules are now considered to be PRIVATE. If indicated, a deprecation warning will be issued if you reference these modules. (:issue:`12588`)  .. csv-table::     :header: "Previous Location", "New Location", "Deprecated"     :widths: 30, 30, 4      "pandas.lib", "pandas._libs.lib", "X"     "pandas.tslib", "pandas._libs.tslib", "X"     "pandas.computation", "pandas.core.computation", "X"     "pandas.msgpack", "pandas.io.msgpack", ""     "pandas.index", "pandas._libs.index", ""     "pandas.algos", "pandas._libs.algos", ""     "pandas.hashtable", "pandas._libs.hashtable", ""     "pandas.indexes", "pandas.core.indexes", ""     "pandas.json", "pandas._libs.json / pandas.io.json", "X"     "pandas.parser", "pandas._libs.parsers", "X"     "pandas.formats", "pandas.io.formats", ""     "pandas.sparse", "pandas.core.sparse", ""     "pandas.tools", "pandas.core.reshape", "X"     "pandas.types", "pandas.core.dtypes", "X"     "pandas.io.sas.saslib", "pandas.io.sas._sas", ""     "pandas._join", "pandas._libs.join", ""     "pandas._hash", "pandas._libs.hashing", ""     "pandas._period", "pandas._libs.period", ""     "pandas._sparse", "pandas._libs.sparse", ""     "pandas._testing", "pandas._libs.testing", ""     "pandas._window", "pandas._libs.window", ""   Some new subpackages are created with public functionality that is not directly exposed in the top-level namespace:``pandas.errors`,`pandas.plotting`and`pandas.testing`(more details below). Together with`pandas.api.types`and certain functions in the`pandas.io`and`pandas.tseries``submodules, these are now the public subpackages.  Further changes:  - The function `~pandas.api.types.union_categoricals` is now importable from``pandas.api.types`, formerly from`pandas.types.concat``(:issue:`15998`) - The type import``pandas.tslib.NaTType`is deprecated and can be replaced by using`type(pandas.NaT)``(:issue:`16146`) - The public functions in``pandas.tools.hashing`deprecated from that locations, but are now importable from`pandas.util``(:issue:`16223`) - The modules in``pandas.util`:`decorators`,`print\_versions`,`doctools`,`validators`,`depr\_module`are now private. Only the functions exposed in`pandas.util``itself are public (:issue:`16223`)  .. _whatsnew_0200.privacy.errors:``pandas.errors`^^^^^^^^^^^^^^^^^  We are adding a standard public module for all pandas exceptions & warnings`pandas.errors``. (:issue:`14800`). Previously these exceptions & warnings could be imported from``pandas.core.common`or`pandas.io.common`. These exceptions and warnings will be removed from the`\*.common``locations in a future release. (:issue:`15541`)  The following are now part of this API:``\`python \['DtypeWarning', 'EmptyDataError', 'OutOfBoundsDatetime', 'ParserError', 'ParserWarning', 'PerformanceWarning', 'UnsortedIndexError', 'UnsupportedFunctionCall'\]

</div>

<div id="whatsnew_0200.privacy.testing">

`pandas.testing` `` ` ^^^^^^^^^^^^^^^^^^  We are adding a standard module that exposes the public testing functions in ``pandas.testing``(:issue:`9895`). Those functions can be used when writing tests for functionality using pandas objects.  The following testing functions are now part of this API:  - `testing.assert_frame_equal` - `testing.assert_series_equal` - `testing.assert_index_equal`   .. _whatsnew_0200.privacy.plotting:``pandas.plotting`^^^^^^^^^^^^^^^^^^^  A new public`pandas.plotting`module has been added that holds plotting functionality that was previously in either`pandas.tools.plotting`or in the top-level namespace. See the [deprecations sections <whatsnew_0200.privacy.deprecate_plotting>](#deprecations-sections-<whatsnew_0200.privacy.deprecate_plotting>) for more details.  .. _whatsnew_0200.privacy.development:  Other development changes ^^^^^^^^^^^^^^^^^^^^^^^^^  - Building pandas for development now requires`cython \>= 0.23``(:issue:`14831`) - Require at least 0.23 version of cython to avoid problems with character encodings (:issue:`14699`) - Switched the test framework to use `pytest <http://doc.pytest.org/en/latest>`__ (:issue:`13097`) - Reorganization of tests directory layout (:issue:`14854`, :issue:`15707`).   .. _whatsnew_0200.deprecations:  Deprecations ~~~~~~~~~~~~  .. _whatsnew_0200.api_breaking.deprecate_ix:  Deprecate``.ix`^^^^^^^^^^^^^^^^^  The`.ix`indexer is deprecated, in favor of the more strict`.iloc`and`.loc`indexers.`.ix`offers a lot of magic on the inference of what the user wants to do. More specifically,`.ix``can decide to index *positionally* OR via *labels*, depending on the data type of the index. This has caused quite a bit of user confusion over the years. The full indexing documentation is [here <indexing>](#here-<indexing>). (:issue:`14218`)  The recommended methods of indexing are:  -``.loc`if you want to *label* index -`.iloc`if you want to *positionally* index.  Using`.ix`will now show a`DeprecationWarning``with a link to some examples of how to convert code `here <https://pandas.pydata.org/pandas-docs/version/1.0/user_guide/indexing.html#ix-indexer-is-deprecated>`__.   .. ipython:: python     df = pd.DataFrame({'A': [1, 2, 3],                       'B': [4, 5, 6]},                      index=list('abc'))     df  Previous behavior, where you wish to get the 0th and the 2nd elements from the index in the 'A' column.``\`ipython In \[3\]: df.ix\[\[0, 2\], 'A'\] Out\[3\]: a 1 c 3 Name: A, dtype: int64

</div>

Using `.loc`. Here we will select the appropriate indexes from the index, then use *label* indexing.

<div class="ipython">

python

df.loc\[df.index\[\[0, 2\]\], 'A'\]

</div>

Using `.iloc`. Here we will get the location of the 'A' column, then use *positional* indexing to select things.

<div class="ipython">

python

df.iloc\[\[0, 2\], df.columns.get\_loc('A')\]

</div>

<div id="whatsnew_0200.api_breaking.deprecate_panel">

Deprecate Panel `` ` ^^^^^^^^^^^^^^^ ``Panel`is deprecated and will be removed in a future version. The recommended way to represent 3-D data are with a`MultiIndex`on a`DataFrame``via the `~Panel.to_frame` or with the `xarray package <http://xarray.pydata.org/en/stable/>`__. pandas provides a `~Panel.to_xarray` method to automate this conversion (:issue:`13563`).``\`ipython In \[133\]: import pandas.\_testing as tm

</div>

> In \[134\]: p = tm.makePanel()
> 
> In \[135\]: p Out\[135\]: \<class 'pandas.core.panel.Panel'\> Dimensions: 3 (items) x 3 (major\_axis) x 4 (minor\_axis) Items axis: ItemA to ItemC Major\_axis axis: 2000-01-03 00:00:00 to 2000-01-05 00:00:00 Minor\_axis axis: A to D

Convert to a MultiIndex DataFrame

``` ipython
In [136]: p.to_frame()
Out[136]:
                     ItemA     ItemB     ItemC
major      minor
2000-01-03 A      0.628776 -1.409432  0.209395
           B      0.988138 -1.347533 -0.896581
           C     -0.938153  1.272395 -0.161137
           D     -0.223019 -0.591863 -1.051539
2000-01-04 A      0.186494  1.422986 -0.592886
           B     -0.072608  0.363565  1.104352
           C     -1.239072 -1.449567  0.889157
           D      2.123692 -0.414505 -0.319561
2000-01-05 A      0.952478 -2.147855 -1.473116
           B     -0.550603 -0.014752 -0.431550
           C      0.139683 -1.195524  0.288377
           D      0.122273 -1.425795 -0.619993

[12 rows x 3 columns]
```

Convert to an xarray DataArray

``` ipython
In [137]: p.to_xarray()
Out[137]:
<xarray.DataArray (items: 3, major_axis: 3, minor_axis: 4)>
array([[[ 0.628776,  0.988138, -0.938153, -0.223019],
        [ 0.186494, -0.072608, -1.239072,  2.123692],
        [ 0.952478, -0.550603,  0.139683,  0.122273]],

       [[-1.409432, -1.347533,  1.272395, -0.591863],
        [ 1.422986,  0.363565, -1.449567, -0.414505],
        [-2.147855, -0.014752, -1.195524, -1.425795]],

       [[ 0.209395, -0.896581, -0.161137, -1.051539],
        [-0.592886,  1.104352,  0.889157, -0.319561],
        [-1.473116, -0.43155 ,  0.288377, -0.619993]]])
Coordinates:
  * items       (items) object 'ItemA' 'ItemB' 'ItemC'
  * major_axis  (major_axis) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05
  * minor_axis  (minor_axis) object 'A' 'B' 'C' 'D'
```

<div id="whatsnew_0200.api_breaking.deprecate_group_agg_dict">

Deprecate groupby.agg() with a dictionary when renaming `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The ``.groupby(..).agg(..)`,`.rolling(..).agg(..)`, and`.resample(..).agg(..)`syntax can accept a variable of inputs, including scalars, list, and a dict of column names to scalars or lists. This provides a useful syntax for constructing multiple (potentially different) aggregations.  However,`.agg(..)`can *also* accept a dict that allows 'renaming' of the result columns. This is a complicated and confusing syntax, as well as not consistent between`Series`and`DataFrame`. We are deprecating this 'renaming' functionality.  - We are deprecating passing a dict to a grouped/rolled/resampled`Series`. This allowed   one to`rename`the resulting aggregation, but this had a completely different   meaning than passing a dictionary to a grouped`DataFrame`, which accepts column-to-aggregations. - We are deprecating passing a dict-of-dicts to a grouped/rolled/resampled`DataFrame`in a similar manner.  This is an illustrative example:  .. ipython:: python     df = pd.DataFrame({'A': [1, 1, 1, 2, 2],                       'B': range(5),                       'C': range(5)})    df  Here is a typical useful syntax for computing different aggregations for different columns. This is a natural, and useful syntax. We aggregate from the dict-to-list by taking the specified columns and applying the list of functions. This returns a`MultiIndex`for the columns (this is *not* deprecated).  .. ipython:: python     df.groupby('A').agg({'B': 'sum', 'C': 'min'})  Here's an example of the first deprecation, passing a dict to a grouped`Series`. This is a combination aggregation & renaming:`\`ipython In \[6\]: df.groupby('A').B.agg({'foo': 'count'}) FutureWarning: using a dict on a Series for aggregation is deprecated and will be removed in a future version

</div>

>   - Out\[6\]:  
>     foo
> 
> A 1 3 2 2

You can accomplish the same operation, more idiomatically by:

<div class="ipython">

python

df.groupby('A').B.agg(\['count'\]).rename(columns={'count': 'foo'})

</div>

Here's an example of the second deprecation, passing a dict-of-dict to a grouped `DataFrame`:

``` python
In [23]: (df.groupby('A')
    ...:    .agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})
    ...:  )
FutureWarning: using a dict with renaming is deprecated and
will be removed in a future version

Out[23]:
     B   C
   foo bar
A
1   3   0
2   7   3
```

You can accomplish nearly the same by:

<div class="ipython">

python

  - (df.groupby('A')  
    .agg({'B': 'sum', 'C': 'min'}) .rename(columns={'B': 'foo', 'C': 'bar'}) )

</div>

<div id="whatsnew_0200.privacy.deprecate_plotting">

Deprecate .plotting `` ` ^^^^^^^^^^^^^^^^^^^  The ``pandas.tools.plotting`module has been deprecated,  in favor of the top level`pandas.plotting`module. All the public plotting functions are now available from`pandas.plotting``(:issue:`12548`).  Furthermore, the top-level``pandas.scatter\_matrix`and`pandas.plot\_params`are deprecated. Users can import these from`pandas.plotting`as well.  Previous script:`\`python pd.tools.plotting.scatter\_matrix(df) pd.scatter\_matrix(df)

</div>

Should be changed to:

``` python
pd.plotting.scatter_matrix(df)
```

<div id="whatsnew_0200.deprecations.other">

Other deprecations `` ` ^^^^^^^^^^^^^^^^^^  - ``SparseArray.to\_dense()`has deprecated the`fill``parameter, as that parameter was not being respected (:issue:`14647`) -``SparseSeries.to\_dense()`has deprecated the`sparse\_only``parameter (:issue:`14647`) -``Series.repeat()`has deprecated the`reps`parameter in favor of`repeats``(:issue:`12662`) - The``Series`constructor and`.astype`method have deprecated accepting timestamp dtypes without a frequency (e.g.`np.datetime64`) for the`dtype``parameter (:issue:`15524`) -``Index.repeat()`and`MultiIndex.repeat()`have deprecated the`n`parameter in favor of`repeats``(:issue:`12662`) -``Categorical.searchsorted()`and`Series.searchsorted()`have deprecated the`v`parameter in favor of`value``(:issue:`12662`) -``TimedeltaIndex.searchsorted()`,`DatetimeIndex.searchsorted()`, and`PeriodIndex.searchsorted()`have deprecated the`key`parameter in favor of`value``(:issue:`12662`) -``DataFrame.astype()`has deprecated the`raise\_on\_error`parameter in favor of`errors``(:issue:`14878`) -``Series.sortlevel`and`DataFrame.sortlevel`have been deprecated in favor of`Series.sort\_index`and`DataFrame.sort\_index``(:issue:`15099`) - importing``concat`from`pandas.tools.merge`has been deprecated in favor of imports from the`pandas``namespace. This should only affect explicit imports (:issue:`15358`) -``Series/DataFrame/Panel.consolidate()``been deprecated as a public method. (:issue:`15483`) - The``as\_indexer`keyword of`Series.str.match()``has been deprecated (ignored keyword) (:issue:`15257`). - The following top-level pandas functions have been deprecated and will be removed in a future version (:issue:`13790`, :issue:`15940`)    *``pd.pnow()`, replaced by`Period.now()`*`pd.Term`, is removed, as it is not applicable to user code. Instead use in-line string expressions in the where clause when searching in HDFStore   *`pd.Expr`, is removed, as it is not applicable to user code.   *`pd.match()`, is removed.   *`pd.groupby()`, replaced by using the`.groupby()`method directly on a`Series/DataFrame`*`pd.get\_store()`, replaced by a direct call to`pd.HDFStore(...)`-`is\_any\_int\_dtype`,`is\_floating\_dtype`, and`is\_sequence`are deprecated from`pandas.api.types``(:issue:`16042`)  .. _whatsnew_0200.prior_deprecations:  Removal of prior version deprecations/changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - The``pandas.rpy``module is removed. Similar functionality can be accessed   through the `rpy2 <https://rpy2.readthedocs.io/>`__ project.   See the `R interfacing docs <https://pandas.pydata.org/pandas-docs/version/0.20/r_interface.html>`__ for more details. - The``pandas.io.ga`module with a`google-analytics``interface is removed (:issue:`11308`).   Similar functionality can be found in the `Google2Pandas <https://github.com/panalysis/Google2Pandas>`__ package. -``pd.to\_datetime`and`pd.to\_timedelta`have dropped the`coerce`parameter in favor of`errors``(:issue:`13602`) -``pandas.stats.fama\_macbeth`,`pandas.stats.ols`,`pandas.stats.plm`and`pandas.stats.var`, as well as the top-level`pandas.fama\_macbeth`and`pandas.ols``routines are removed. Similar functionality can be found in the `statsmodels <https://www.statsmodels.org/dev/>`__ package. (:issue:`11898`) - The``TimeSeries`and`SparseTimeSeries`classes, aliases of`Series`and`SparseSeries``, are removed (:issue:`10890`, :issue:`15098`). -``Series.is\_time\_series`is dropped in favor of`Series.index.is\_all\_dates``(:issue:`15098`) - The deprecated``irow`,`icol`,`iget`and`iget\_value`methods are removed   in favor of`iloc`and`iat``as explained [here <whatsnew_0170.deprecations>](#here-<whatsnew_0170.deprecations>) (:issue:`10711`). - The deprecated``DataFrame.iterkv()`has been removed in favor of`DataFrame.iteritems()``(:issue:`10711`) - The``Categorical`constructor has dropped the`name``parameter (:issue:`10632`) -``Categorical`has dropped support for`NaN``categories (:issue:`10748`) - The``take\_last`parameter has been dropped from`duplicated()`,`drop\_duplicates()`,`nlargest()`, and`nsmallest()``methods (:issue:`10236`, :issue:`10792`, :issue:`10920`) -``Series`,`Index`, and`DataFrame`have dropped the`sort`and`order``methods (:issue:`10726`) - Where clauses in``pytables``are only accepted as strings and expressions types and not other data-types (:issue:`12027`) -``DataFrame`has dropped the`combineAdd`and`combineMult`methods in favor of`add`and`mul``respectively (:issue:`10735`)  .. _whatsnew_0200.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Improved performance of``pd.wide\_to\_long()``(:issue:`14779`) - Improved performance of``pd.factorize()`by releasing the GIL with`object``dtype when inferred as strings (:issue:`14859`, :issue:`16057`) - Improved performance of timeseries plotting with an irregular DatetimeIndex   (or with``compat\_x=True``) (:issue:`15073`). - Improved performance of``groupby().cummin()`and`groupby().cummax()``(:issue:`15048`, :issue:`15109`, :issue:`15561`, :issue:`15635`) - Improved performance and reduced memory when indexing with a``MultiIndex``(:issue:`15245`) - When reading buffer object in``read\_sas()``method without specified format, filepath string is inferred rather than buffer object. (:issue:`14947`) - Improved performance of``.rank()``for categorical data (:issue:`15498`) - Improved performance when using``.unstack()``(:issue:`15503`) - Improved performance of merge/join on``category``columns (:issue:`10409`) - Improved performance of``drop\_duplicates()`on`bool``columns (:issue:`12963`) - Improve performance of``pd.core.groupby.GroupBy.apply`when the applied   function used the`.name``attribute of the group DataFrame (:issue:`15062`). - Improved performance of``iloc``indexing with a list or array (:issue:`15504`). - Improved performance of``Series.sort\_index()``with a monotonic index (:issue:`15694`) - Improved performance in``pd.read\_csv()``on some platforms with buffered reads (:issue:`16039`)  .. _whatsnew_0200.bug_fixes:  Bug fixes ~~~~~~~~~  Conversion ^^^^^^^^^^  - Bug in``Timestamp.replace`now raises`TypeError`when incorrect argument names are given; previously this raised`ValueError``(:issue:`15240`) - Bug in``Timestamp.replace``with compat for passing long integers (:issue:`15030`) - Bug in``Timestamp``returning UTC based time/date attributes when a timezone was provided (:issue:`13303`, :issue:`6538`) - Bug in``Timestamp``incorrectly localizing timezones during construction (:issue:`11481`, :issue:`15777`) - Bug in``TimedeltaIndex``addition where overflow was being allowed without error (:issue:`14816`) - Bug in``TimedeltaIndex`raising a`ValueError`when boolean indexing with`loc``(:issue:`14946`) - Bug in catching an overflow in``Timestamp`+`Timedelta/Offset``operations (:issue:`15126`) - Bug in``DatetimeIndex.round()`and`Timestamp.round()``floating point accuracy when rounding by milliseconds or less (:issue:`14440`, :issue:`15578`) - Bug in``astype()`where`inf`values were incorrectly converted to integers. Now raises error now with`astype()``for Series and DataFrames (:issue:`14265`) - Bug in``DataFrame(..).apply(to\_numeric)``when values are of type decimal.Decimal. (:issue:`14827`) - Bug in``describe()`when passing a numpy array which does not contain the median to the`percentiles``keyword argument (:issue:`14908`) - Cleaned up``PeriodIndex``constructor, including raising on floats more consistently (:issue:`13277`) - Bug in using``\_\_deepcopy\_\_``on empty NDFrame objects (:issue:`15370`) - Bug in``.replace()``may result in incorrect dtypes. (:issue:`12747`, :issue:`15765`) - Bug in``Series.replace`and`DataFrame.replace``which failed on empty replacement dicts (:issue:`15289`) - Bug in``Series.replace``which replaced a numeric by string (:issue:`15743`) - Bug in``Index`construction with`NaN``elements and integer dtype specified (:issue:`15187`) - Bug in``Series``construction with a datetimetz (:issue:`14928`) - Bug in``Series.dt.round()`inconsistent behaviour on`NaT``'s with different arguments (:issue:`14940`) - Bug in``Series`constructor when both`copy=True`and`dtype``arguments are provided (:issue:`15125`) - Incorrect dtyped``Series`was returned by comparison methods (e.g.,`lt`,`gt`, ...) against a constant for an empty`DataFrame``(:issue:`15077`) - Bug in``Series.ffill()``with mixed dtypes containing tz-aware datetimes. (:issue:`14956`) - Bug in``DataFrame.fillna()`where the argument`downcast`was ignored when fillna value was of type`dict``(:issue:`15277`) - Bug in``.asfreq()`, where frequency was not set for empty`Series``(:issue:`14320`) - Bug in``DataFrame``construction with nulls and datetimes in a list-like (:issue:`15869`) - Bug in``DataFrame.fillna()``with tz-aware datetimes (:issue:`15855`) - Bug in``is\_string\_dtype`,`is\_timedelta64\_ns\_dtype`, and`is\_string\_like\_dtype`in which an error was raised when`None``was passed in (:issue:`15941`) - Bug in the return type of``pd.unique`on a`Categorical`, which was returning an ndarray and not a`Categorical``(:issue:`15903`) - Bug in``Index.to\_series()``where the index was not copied (and so mutating later would change the original), (:issue:`15949`) - Bug in indexing with partial string indexing with a len-1 DataFrame (:issue:`16071`) - Bug in``Series``construction where passing invalid dtype didn't raise an error. (:issue:`15520`)  Indexing ^^^^^^^^  - Bug in``Index``power operations with reversed operands (:issue:`14973`) - Bug in``DataFrame.sort\_values()`when sorting by multiple columns where one column is of type`int64`and contains`NaT``(:issue:`14922`) - Bug in``DataFrame.reindex()`in which`method`was ignored when passing`columns``(:issue:`14992`) - Bug in``DataFrame.loc`with indexing a`MultiIndex`with a`Series``indexer (:issue:`14730`, :issue:`15424`) - Bug in``DataFrame.loc`with indexing a`MultiIndex``with a numpy array (:issue:`15434`) - Bug in``Series.asof`which raised if the series contained all`np.nan``(:issue:`15713`) - Bug in``.at``when selecting from a tz-aware column (:issue:`15822`) - Bug in``Series.where()`and`DataFrame.where()``where array-like conditionals were being rejected (:issue:`15414`) - Bug in``Series.where()``where TZ-aware data was converted to float representation (:issue:`15701`) - Bug in``.loc``that would not return the correct dtype for scalar access for a DataFrame (:issue:`11617`) - Bug in output formatting of a``MultiIndex``when names are integers (:issue:`12223`, :issue:`15262`) - Bug in``Categorical.searchsorted()``where alphabetical instead of the provided categorical order was used (:issue:`14522`) - Bug in``Series.iloc`where a`Categorical`object for list-like indexes input was returned, where a`Series``was expected. (:issue:`14580`) - Bug in``DataFrame.isin``comparing datetimelike to empty frame (:issue:`15473`) - Bug in``.reset\_index()`when an all`NaN`level of a`MultiIndex``would fail (:issue:`6322`) - Bug in``.reset\_index()`when raising error for index name already present in`MultiIndex``columns (:issue:`16120`) - Bug in creating a``MultiIndex`with tuples and not passing a list of names; this will now raise`ValueError``(:issue:`15110`) - Bug in the HTML display with a``MultiIndex``and truncation (:issue:`14882`) - Bug in the display of``.info()`where a qualifier (+) would always be displayed with a`MultiIndex``that contains only non-strings (:issue:`15245`) - Bug in``pd.concat()`where the names of`MultiIndex`of resulting`DataFrame`are not handled correctly when`None`is presented in the names of`MultiIndex`of input`DataFrame``(:issue:`15787`) - Bug in``DataFrame.sort\_index()`and`Series.sort\_index()`where`na\_position`doesn't work with a`MultiIndex``(:issue:`14784`, :issue:`16604`) - Bug in``pd.concat()`when combining objects with a`CategoricalIndex``(:issue:`16111`) - Bug in indexing with a scalar and a``CategoricalIndex``(:issue:`16123`)  IO ^^  - Bug in``pd.to\_numeric()``in which float and unsigned integer elements were being improperly casted (:issue:`14941`, :issue:`15005`) - Bug in``pd.read\_fwf()``where the skiprows parameter was not being respected during column width inference (:issue:`11256`) - Bug in``pd.read\_csv()`in which the`dialect``parameter was not being verified before processing (:issue:`14898`) - Bug in``pd.read\_csv()`in which missing data was being improperly handled with`usecols``(:issue:`6710`) - Bug in``pd.read\_csv()``in which a file containing a row with many columns followed by rows with fewer columns would cause a crash (:issue:`14125`) - Bug in``pd.read\_csv()`for the C engine where`usecols`were being indexed incorrectly with`parse\_dates``(:issue:`14792`) - Bug in``pd.read\_csv()`with`parse\_dates``when multi-line headers are specified (:issue:`15376`) - Bug in``pd.read\_csv()`with`float\_precision='round\_trip'``which caused a segfault when a text entry is parsed (:issue:`15140`) - Bug in``pd.read\_csv()``when an index was specified and no values were specified as null values (:issue:`15835`) - Bug in``pd.read\_csv()``in which certain invalid file objects caused the Python interpreter to crash (:issue:`15337`) - Bug in``pd.read\_csv()`in which invalid values for`nrows`and`chunksize``were allowed (:issue:`15767`) - Bug in``pd.read\_csv()``for the Python engine in which unhelpful error messages were being raised when parsing errors occurred (:issue:`15910`) - Bug in``pd.read\_csv()`in which the`skipfooter``parameter was not being properly validated (:issue:`15925`) - Bug in``pd.to\_csv()``in which there was numeric overflow when a timestamp index was being written (:issue:`15982`) - Bug in``pd.util.hashing.hash\_pandas\_object()``in which hashing of categoricals depended on the ordering of categories, instead of just their values. (:issue:`15143`) - Bug in``.to\_json()`where`lines=True``and contents (keys or values) contain escaped characters (:issue:`15096`) - Bug in``.to\_json()``causing single byte ascii characters to be expanded to four byte unicode (:issue:`15344`) - Bug in``.to\_json()``for the C engine where rollover was not correctly handled for case where frac is odd and diff is exactly 0.5 (:issue:`15716`, :issue:`15864`) - Bug in``pd.read\_json()`for Python 2 where`lines=True``and contents contain non-ascii unicode characters (:issue:`15132`) - Bug in``pd.read\_msgpack()`in which`Series``categoricals were being improperly processed (:issue:`14901`) - Bug in``pd.read\_msgpack()`which did not allow loading of a dataframe with an index of type`CategoricalIndex``(:issue:`15487`) - Bug in``pd.read\_msgpack()`when deserializing a`CategoricalIndex``(:issue:`15487`) - Bug in``DataFrame.to\_records()`with converting a`DatetimeIndex``with a timezone (:issue:`13937`) - Bug in``DataFrame.to\_records()``which failed with unicode characters in column names (:issue:`11879`) - Bug in``.to\_sql()``when writing a DataFrame with numeric index names (:issue:`15404`). - Bug in``DataFrame.to\_html()`with`index=False`and`max\_rows`raising in`IndexError``(:issue:`14998`) - Bug in``pd.read\_hdf()`passing a`Timestamp`to the`where``parameter with a non date column (:issue:`15492`) - Bug in``DataFrame.to\_stata()`and`StataWriter``which produces incorrectly formatted files to be produced for some locales (:issue:`13856`) - Bug in``StataReader`and`StataWriter``which allows invalid encodings (:issue:`15723`) - Bug in the``Series``repr not showing the length when the output was truncated (:issue:`15962`).  Plotting ^^^^^^^^  - Bug in``DataFrame.hist`where`plt.tight\_layout`caused an`AttributeError`(use`matplotlib \>= 2.0.1``) (:issue:`9351`) - Bug in``DataFrame.boxplot`where`fontsize``was not applied to the tick labels on both axes (:issue:`15108`) - Bug in the date and time converters pandas registers with matplotlib not handling multiple dimensions (:issue:`16026`) - Bug in``pd.scatter\_matrix()`could accept either`color`or`c``, but not both (:issue:`14855`)  GroupBy/resample/rolling ^^^^^^^^^^^^^^^^^^^^^^^^  - Bug in``.groupby(..).resample()`when passed the`on=``kwarg. (:issue:`15021`) - Properly set``\_\_name\_\_`and`\_\_qualname\_\_`for`Groupby.\*``functions (:issue:`14620`) - Bug in``GroupBy.get\_group()``failing with a categorical grouper (:issue:`15155`) - Bug in``.groupby(...).rolling(...)`when`on`is specified and using a`DatetimeIndex``(:issue:`15130`, :issue:`13966`) - Bug in groupby operations with``timedelta64`when passing`numeric\_only=False``(:issue:`5724`) - Bug in``groupby.apply()`coercing`object``dtypes to numeric types, when not all values were numeric (:issue:`14423`, :issue:`15421`, :issue:`15670`) - Bug in``resample`, where a non-string`loffset``argument would not be applied when resampling a timeseries (:issue:`13218`) - Bug in``DataFrame.groupby().describe()`when grouping on`Index``containing tuples (:issue:`14848`) - Bug in``groupby().nunique()``with a datetimelike-grouper where bins counts were incorrect (:issue:`13453`) - Bug in``groupby.transform()``that would coerce the resultant dtypes back to the original (:issue:`10972`, :issue:`11444`) - Bug in``groupby.agg()`incorrectly localizing timezone on`datetime``(:issue:`15426`, :issue:`10668`, :issue:`13046`) - Bug in``.rolling/expanding()`functions where`count()`was not counting`np.Inf`, nor handling`object``dtypes (:issue:`12541`) - Bug in``.rolling()`where`pd.Timedelta`or`datetime.timedelta`was not accepted as a`window``argument (:issue:`15440`) - Bug in``Rolling.quantile``function that caused a segmentation fault when called with a quantile value outside of the range [0, 1] (:issue:`15463`) - Bug in``DataFrame.resample().median()``if duplicate column names are present (:issue:`14233`)  Sparse ^^^^^^  - Bug in``SparseSeries.reindex``on single level with list of length 1 (:issue:`15447`) - Bug in repr-formatting a``SparseDataFrame``after a value was set on (a copy of) one of its series (:issue:`15488`) - Bug in``SparseDataFrame``construction with lists not coercing to dtype (:issue:`15682`) - Bug in sparse array indexing in which indices were not being validated (:issue:`15863`)  Reshaping ^^^^^^^^^  - Bug in``pd.merge\_asof()`where`left\_index`or`right\_index`caused a failure when multiple`by``was specified (:issue:`15676`) - Bug in``pd.merge\_asof()`where`left\_index`/`right\_index`together caused a failure when`tolerance``was specified (:issue:`15135`) - Bug in``DataFrame.pivot\_table()`where`dropna=True`would not drop all-NaN columns when the columns was a`category``dtype (:issue:`15193`) - Bug in``pd.melt()`where passing a tuple value for`value\_vars`caused a`TypeError``(:issue:`15348`) - Bug in``pd.pivot\_table()``where no error was raised when values argument was not in the columns (:issue:`14938`) - Bug in``pd.concat()`in which concatenating with an empty dataframe with`join='inner'``was being improperly handled (:issue:`15328`) - Bug with``sort=True`in`DataFrame.join`and`pd.merge``when joining on indexes (:issue:`15582`) - Bug in``DataFrame.nsmallest`and`DataFrame.nlargest``where identical values resulted in duplicated rows (:issue:`15297`) - Bug in `pandas.pivot_table` incorrectly raising``UnicodeError`when passing unicode input for`margins``keyword (:issue:`13292`)  Numeric ^^^^^^^  - Bug in``.rank()``which incorrectly ranks ordered categories (:issue:`15420`) - Bug in``.corr()`and`.cov()``where the column and index were the same object (:issue:`14617`) - Bug in``.mode()`where`mode``was not returned if was only a single value (:issue:`15714`) - Bug in``pd.cut()``with a single bin on an all 0s array (:issue:`15428`) - Bug in``pd.qcut()``with a single quantile and an array with identical values (:issue:`15431`) - Bug in``pandas.tools.utils.cartesian\_product()``with large input can cause overflow on windows (:issue:`15265`) - Bug in``.eval()``which caused multi-line evals to fail with local variables not on the first line (:issue:`15342`)  Other ^^^^^  - Compat with SciPy 0.19.0 for testing on``.interpolate()``(:issue:`15662`) - Compat for 32-bit platforms for``.qcut/cut`; bins will now be`int64``dtype (:issue:`14866`) - Bug in interactions with``Qt`when a`QtApplication``already exists (:issue:`14372`) - Avoid use of``np.finfo()`during`import pandas\`<span class="title-ref"> removed to mitigate deadlock on Python GIL misuse (:issue:\`14641</span>)

</div>

## Contributors

<div class="contributors">

v0.19.2..v0.20.0

</div>

---

v0.20.2.md

---

# Version 0.20.2 (June 4, 2017)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a minor bug-fix release in the 0.20.x series and includes some small regression fixes, bug fixes and performance improvements. We recommend that all users upgrade to this version.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.20.2

</div>

## Enhancements

  - Unblocked access to additional compression types supported in pytables: 'blosc:blosclz, 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd' (`14478`)
  - `Series` provides a `to_latex` method (`16180`)
  - A new groupby method <span class="title-ref">.GroupBy.ngroup</span>, parallel to the existing <span class="title-ref">.GroupBy.cumcount</span>, has been added to return the group order (`11642`); see \[here \<groupby.ngroup\>\](\#here-\<groupby.ngroup\>).

## Performance improvements

  - Performance regression fix when indexing with a list-like (`16285`)
  - Performance regression fix for MultiIndexes (`16319`, `16346`)
  - Improved performance of `.clip()` with scalar arguments (`15400`)
  - Improved performance of groupby with categorical groupers (`16413`)
  - Improved performance of `MultiIndex.remove_unused_levels()` (`16556`)

## Bug fixes

  - Silenced a warning on some Windows environments about "tput: terminal attributes: No such device or address" when detecting the terminal size. This fix only applies to python 3 (`16496`)
  - Bug in using `pathlib.Path` or `py.path.local` objects with io functions (`16291`)
  - Bug in `Index.symmetric_difference()` on two equal MultiIndex's, results in a `TypeError` (`13490`)
  - Bug in `DataFrame.update()` with `overwrite=False` and `NaN values` (`15593`)
  - Passing an invalid engine to <span class="title-ref">read\_csv</span> now raises an informative `ValueError` rather than `UnboundLocalError`. (`16511`)
  - Bug in <span class="title-ref">unique</span> on an array of tuples (`16519`)
  - Bug in <span class="title-ref">cut</span> when `labels` are set, resulting in incorrect label ordering (`16459`)
  - Fixed a compatibility issue with IPython 6.0's tab completion showing deprecation warnings on `Categoricals` (`16409`)

### Conversion

  - Bug in <span class="title-ref">to\_numeric</span> in which empty data inputs were causing a segfault of the interpreter (`16302`)
  - Silence numpy warnings when broadcasting `DataFrame` to `Series` with comparison ops (`16378`, `16306`)

### Indexing

  - Bug in `DataFrame.reset_index(level=)` with single level index (`16263`)
  - Bug in partial string indexing with a monotonic, but not strictly-monotonic, index incorrectly reversing the slice bounds (`16515`)
  - Bug in `MultiIndex.remove_unused_levels()` that would not return a `MultiIndex` equal to the original. (`16556`)

### IO

  - Bug in <span class="title-ref">read\_csv</span> when `comment` is passed in a space delimited text file (`16472`)
  - Bug in <span class="title-ref">read\_csv</span> not raising an exception with nonexistent columns in `usecols` when it had the correct length (`14671`)
  - Bug that would force importing of the clipboard routines unnecessarily, potentially causing an import error on startup (`16288`)
  - Bug that raised `IndexError` when HTML-rendering an empty `DataFrame` (`15953`)
  - Bug in <span class="title-ref">read\_csv</span> in which tarfile object inputs were raising an error in Python 2.x for the C engine (`16530`)
  - Bug where `DataFrame.to_html()` ignored the `index_names` parameter (`16493`)
  - Bug where `pd.read_hdf()` returns numpy strings for index names (`13492`)
  - Bug in `HDFStore.select_as_multiple()` where start/stop arguments were not respected (`16209`)

### Plotting

  - Bug in `DataFrame.plot` with a single column and a list-like `color` (`3486`)
  - Bug in `plot` where `NaT` in `DatetimeIndex` results in `Timestamp.min` (`12405`)
  - Bug in `DataFrame.boxplot` where `figsize` keyword was not respected for non-grouped boxplots (`11959`)

### GroupBy/resample/rolling

  - Bug in creating a time-based rolling window on an empty `DataFrame` (`15819`)
  - Bug in `rolling.cov()` with offset window (`16058`)
  - Bug in `.resample()` and `.groupby()` when aggregating on integers (`16361`)

### Sparse

  - Bug in construction of `SparseDataFrame` from `scipy.sparse.dok_matrix` (`16179`)

### Reshaping

  - Bug in `DataFrame.stack` with unsorted levels in `MultiIndex` columns (`16323`)
  - Bug in `pd.wide_to_long()` where no error was raised when `i` was not a unique identifier (`16382`)
  - Bug in `Series.isin(..)` with a list of tuples (`16394`)
  - Bug in construction of a `DataFrame` with mixed dtypes including an all-NaT column. (`16395`)
  - Bug in `DataFrame.agg()` and `Series.agg()` with aggregating on non-callable attributes (`16405`)

### Numeric

  - Bug in `.interpolate()`, where `limit_direction` was not respected when `limit=None` (default) was passed (`16282`)

### Categorical

  - Fixed comparison operations considering the order of the categories when both categoricals are unordered (`16014`)

### Other

  - Bug in `DataFrame.drop()` with an empty-list with non-unique indices (`16270`)

## Contributors

<div class="contributors">

v0.20.0..v0.20.2

</div>

---

v0.20.3.md

---

# Version 0.20.3 (July 7, 2017)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a minor bug-fix release in the 0.20.x series and includes some small regression fixes and bug fixes. We recommend that all users upgrade to this version.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.20.3

</div>

## Bug fixes

  - Fixed a bug in failing to compute rolling computations of a column-MultiIndexed `DataFrame` (`16789`, `16825`)
  - Fixed a pytest marker failing downstream packages' tests suites (`16680`)

### Conversion

  - Bug in pickle compat prior to the v0.20.x series, when `UTC` is a timezone in a Series/DataFrame/Index (`16608`)
  - Bug in `Series` construction when passing a `Series` with `dtype='category'` (`16524`).
  - Bug in <span class="title-ref">DataFrame.astype</span> when passing a `Series` as the `dtype` kwarg. (`16717`).

### Indexing

  - Bug in `Float64Index` causing an empty array instead of `None` to be returned from `.get(np.nan)` on a Series whose index did not contain any `NaN` s (`8569`)
  - Bug in `MultiIndex.isin` causing an error when passing an empty iterable (`16777`)
  - Fixed a bug in a slicing DataFrame/Series that have a `TimedeltaIndex` (`16637`)

### IO

  - Bug in <span class="title-ref">read\_csv</span> in which files weren't opened as binary files by the C engine on Windows, causing EOF characters mid-field, which would fail (`16039`, `16559`, `16675`)
  - Bug in <span class="title-ref">read\_hdf</span> in which reading a `Series` saved to an HDF file in 'fixed' format fails when an explicit `mode='r'` argument is supplied (`16583`)
  - Bug in <span class="title-ref">DataFrame.to\_latex</span> where `bold_rows` was wrongly specified to be `True` by default, whereas in reality row labels remained non-bold whatever parameter provided. (`16707`)
  - Fixed an issue with <span class="title-ref">DataFrame.style</span> where generated element ids were not unique (`16780`)
  - Fixed loading a `DataFrame` with a `PeriodIndex`, from a `format='fixed'` HDFStore, in Python 3, that was written in Python 2 (`16781`)

### Plotting

  - Fixed regression that prevented RGB and RGBA tuples from being used as color arguments (`16233`)
  - Fixed an issue with <span class="title-ref">DataFrame.plot.scatter</span> that incorrectly raised a `KeyError` when categorical data is used for plotting (`16199`)

### Reshaping

  - `PeriodIndex` / `TimedeltaIndex.join` was missing the `sort=` kwarg (`16541`)
  - Bug in joining on a `MultiIndex` with a `category` dtype for a level (`16627`).
  - Bug in <span class="title-ref">merge</span> when merging/joining with multiple categorical columns (`16767`)

### Categorical

  - Bug in `DataFrame.sort_values` not respecting the `kind` parameter with categorical data (`16793`)

## Contributors

<div class="contributors">

v0.20.2..v0.20.3

</div>

---

v0.21.0.md

---

# Version 0.21.0 (October 27, 2017)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a major release from 0.20.3 and includes a number of API changes, deprecations, new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

Highlights include:

  - Integration with [Apache Parquet](https://parquet.apache.org/), including a new top-level <span class="title-ref">read\_parquet</span> function and <span class="title-ref">DataFrame.to\_parquet</span> method, see \[here \<whatsnew\_0210.enhancements.parquet\>\](\#here-\<whatsnew\_0210.enhancements.parquet\>).
  - New user-facing <span class="title-ref">pandas.api.types.CategoricalDtype</span> for specifying categoricals independent of the data, see \[here \<whatsnew\_0210.enhancements.categorical\_dtype\>\](\#here-\<whatsnew\_0210.enhancements.categorical\_dtype\>).
  - The behavior of `sum` and `prod` on all-NaN Series/DataFrames is now consistent and no longer depends on whether [bottleneck](https://bottleneck.readthedocs.io) is installed, and `sum` and `prod` on empty Series now return NaN instead of 0, see \[here \<whatsnew\_0210.api\_breaking.bottleneck\>\](\#here-\<whatsnew\_0210.api\_breaking.bottleneck\>).
  - Compatibility fixes for pypy, see \[here \<whatsnew\_0210.pypy\>\](\#here-\<whatsnew\_0210.pypy\>).
  - Additions to the `drop`, `reindex` and `rename` API to make them more consistent, see \[here \<whatsnew\_0210.enhancements.drop\_api\>\](\#here-\<whatsnew\_0210.enhancements.drop\_api\>).
  - Addition of the new methods `DataFrame.infer_objects` (see \[here \<whatsnew\_0210.enhancements.infer\_objects\>\](\#here-\<whatsnew\_0210.enhancements.infer\_objects\>)) and `GroupBy.pipe` (see \[here \<whatsnew\_0210.enhancements.GroupBy\_pipe\>\](\#here-\<whatsnew\_0210.enhancements.groupby\_pipe\>)).
  - Indexing with a list of labels, where one or more of the labels is missing, is deprecated and will raise a KeyError in a future version, see \[here \<whatsnew\_0210.api\_breaking.loc\>\](\#here-\<whatsnew\_0210.api\_breaking.loc\>).

Check the \[API Changes \<whatsnew\_0210.api\_breaking\>\](\#api-changes-\<whatsnew\_0210.api\_breaking\>) and \[deprecations \<whatsnew\_0210.deprecations\>\](\#deprecations-\<whatsnew\_0210.deprecations\>) before updating.

<div class="contents" data-local="" data-backlinks="none" data-depth="2">

What's new in v0.21.0

</div>

## New features

### Integration with Apache Parquet file format

Integration with [Apache Parquet](https://parquet.apache.org/), including a new top-level <span class="title-ref">read\_parquet</span> and <span class="title-ref">DataFrame.to\_parquet</span> method, see \[here \<io.parquet\>\](\#here-\<io.parquet\>) (`15838`, `17438`).

[Apache Parquet](https://parquet.apache.org/) provides a cross-language, binary file format for reading and writing data frames efficiently. Parquet is designed to faithfully serialize and de-serialize `DataFrame` s, supporting all of the pandas dtypes, including extension dtypes such as datetime with timezones.

This functionality depends on either the [pyarrow](http://arrow.apache.org/docs/python/) or [fastparquet](https://fastparquet.readthedocs.io/en/latest/) library. For more details, see \[the IO docs on Parquet \<io.parquet\>\](\#the-io-docs-on-parquet-\<io.parquet\>).

### Method `infer_objects` type conversion

The <span class="title-ref">DataFrame.infer\_objects</span> and <span class="title-ref">Series.infer\_objects</span> methods have been added to perform dtype inference on object columns, replacing some of the functionality of the deprecated `convert_objects` method. See the documentation \[here \<basics.object\_conversion\>\](\#here-\<basics.object\_conversion\>) for more details. (`11221`)

This method only performs soft conversions on object columns, converting Python objects to native types, but not any coercive conversions. For example:

<div class="ipython">

python

  - df = pd.DataFrame({'A': \[1, 2, 3\],  
    'B': np.array(\[1, 2, 3\], dtype='object'), 'C': \['1', '2', '3'\]})

df.dtypes df.infer\_objects().dtypes

</div>

Note that column `'C'` was not converted - only scalar numeric types will be converted to a new type. Other types of conversion should be accomplished using the <span class="title-ref">to\_numeric</span> function (or <span class="title-ref">to\_datetime</span>, <span class="title-ref">to\_timedelta</span>).

<div class="ipython">

python

df = df.infer\_objects() df\['C'\] = pd.to\_numeric(df\['C'\], errors='coerce') df.dtypes

</div>

### Improved warnings when attempting to create columns

New users are often puzzled by the relationship between column operations and attribute access on `DataFrame` instances (`7175`). One specific instance of this confusion is attempting to create a new column by setting an attribute on the `DataFrame`:

`` `ipython    In [1]: df = pd.DataFrame({'one': [1., 2., 3.]})    In [2]: df.two = [4, 5, 6]  This does not raise any obvious exceptions, but also does not create a new column:  .. code-block:: ipython     In [3]: df    Out[3]:        one    0  1.0    1  2.0    2  3.0  Setting a list-like data structure into a new attribute now raises a ``UserWarning`about the potential for unexpected behavior. See [Attribute Access <indexing.attribute_access>](#attribute-access-<indexing.attribute_access>).  .. _whatsnew_0210.enhancements.drop_api:  Method`drop`now also accepts index/columns keywords`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The <span class="title-ref">\~DataFrame.drop</span> method has gained `index`/`columns` keywords as an alternative to specifying the `axis`. This is similar to the behavior of `reindex` (`12392`).

For example:

<div class="ipython">

python

  - df = pd.DataFrame(np.arange(8).reshape(2, 4),  
    columns=\['A', 'B', 'C', 'D'\])

df df.drop(\['B', 'C'\], axis=1) \# the following is now equivalent df.drop(columns=\['B', 'C'\])

</div>

### Methods `rename`, `reindex` now also accept axis keyword

The <span class="title-ref">DataFrame.rename</span> and <span class="title-ref">DataFrame.reindex</span> methods have gained the `axis` keyword to specify the axis to target with the operation (`12392`).

Here's `rename`:

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 2, 3\], "B": \[4, 5, 6\]}) df.rename(str.lower, axis='columns') df.rename(id, axis='index')

</div>

And `reindex`:

<div class="ipython">

python

df.reindex(\['A', 'B', 'C'\], axis='columns') df.reindex(\[0, 1, 3\], axis='index')

</div>

The "index, columns" style continues to work as before.

<div class="ipython">

python

df.rename(index=id, columns=str.lower) df.reindex(index=\[0, 1, 3\], columns=\['A', 'B', 'C'\])

</div>

We *highly* encourage using named arguments to avoid confusion when using either style.

### `CategoricalDtype` for specifying categoricals

<span class="title-ref">pandas.api.types.CategoricalDtype</span> has been added to the public API and expanded to include the `categories` and `ordered` attributes. A `CategoricalDtype` can be used to specify the set of categories and orderedness of an array, independent of the data. This can be useful for example, when converting string data to a `Categorical` (`14711`, `15078`, `16015`, `17643`):

<div class="ipython">

python

from pandas.api.types import CategoricalDtype

s = pd.Series(\['a', 'b', 'c', 'a'\]) \# strings dtype = CategoricalDtype(categories=\['a', 'b', 'c', 'd'\], ordered=True) s.astype(dtype)

</div>

One place that deserves special mention is in <span class="title-ref">read\_csv</span>. Previously, with `dtype={'col': 'category'}`, the returned values and categories would always be strings.

<div class="ipython" data-suppress="">

python

from io import StringIO

</div>

<div class="ipython">

python

data = 'A,Bna,1nb,2nc,3' pd.read\_csv(StringIO(data), dtype={'B': 'category'}).B.cat.categories

</div>

Notice the "object" dtype.

With a `CategoricalDtype` of all numerics, datetimes, or timedeltas, we can automatically convert to the correct type

<div class="ipython">

python

dtype = {'B': CategoricalDtype(\[1, 2, 3\])} pd.read\_csv(StringIO(data), dtype=dtype).B.cat.categories

</div>

The values have been correctly interpreted as integers.

The `.dtype` property of a `Categorical`, `CategoricalIndex` or a `Series` with categorical type will now return an instance of `CategoricalDtype`. While the repr has changed, `str(CategoricalDtype())` is still the string `'category'`. We'll take this moment to remind users that the *preferred* way to detect categorical data is to use <span class="title-ref">pandas.api.types.is\_categorical\_dtype</span>, and not `str(dtype) == 'category'`.

See the \[CategoricalDtype docs \<categorical.categoricaldtype\>\](\#categoricaldtype-docs-\<categorical.categoricaldtype\>) for more.

### `GroupBy` objects now have a `pipe` method

`GroupBy` objects now have a `pipe` method, similar to the one on `DataFrame` and `Series`, that allow for functions that take a `GroupBy` to be composed in a clean, readable syntax. (`17871`)

For a concrete example on combining `.groupby` and `.pipe` , imagine having a DataFrame with columns for stores, products, revenue and sold quantity. We'd like to do a groupwise calculation of *prices* (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable.

First we set the data:

<div class="ipython">

python

import numpy as np n = 1000 df = pd.DataFrame({'Store': np.random.choice(\['Store\_1', 'Store\_2'\], n), 'Product': np.random.choice(\['Product\_1', 'Product\_2', 'Product\_3' \], n), 'Revenue': (np.random.random(n) \* 50 + 10).round(2), 'Quantity': np.random.randint(1, 10, size=n)}) df.head(2)

</div>

Now, to find prices per store/product, we can simply do:

<div class="ipython">

python

  - (df.groupby(\['Store', 'Product'\])  
    .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum()) .unstack().round(2))

</div>

See the \[documentation \<groupby.pipe\>\](\#documentation-\<groupby.pipe\>) for more.

### `Categorical.rename_categories` accepts a dict-like

<span class="title-ref">\~Series.cat.rename\_categories</span> now accepts a dict-like argument for `new_categories`. The previous categories are looked up in the dictionary's keys and replaced if found. The behavior of missing and extra keys is the same as in <span class="title-ref">DataFrame.rename</span>.

<div class="ipython">

python

c = pd.Categorical(\['a', 'a', 'b'\]) c.rename\_categories({"a": "eh", "b": "bee"})

</div>

\> **Warning** \> To assist with upgrading pandas, `rename_categories` treats `Series` as list-like. Typically, Series are considered to be dict-like (e.g. in `.rename`, `.map`). In a future version of pandas `rename_categories` will change to treat them as dict-like. Follow the warning message's recommendations for writing future-proof code.

>   - \`\`\`ipython  
>     In \[33\]: c.rename\_categories(pd.Series(\[0, 1\], index=\['a', 'c'\])) FutureWarning: Treating Series 'new\_categories' as a list-like and using the values. In a future version, 'rename\_categories' will treat Series like a dictionary. For dict-like, use 'new\_categories.to\_dict()' For list-like, use 'new\_categories.values'. Out\[33\]: \[0, 0, 1\] Categories (2, int64): \[0, 1\]

<div id="whatsnew_0210.enhancements.other">

Other enhancements `` ` ^^^^^^^^^^^^^^^^^^  New functions or methods """"""""""""""""""""""""  - `.Resampler.nearest` is added to support nearest-neighbor upsampling (:issue:`17496`). - `~pandas.Index` has added support for a ``to\_frame``method (:issue:`15230`).  New keywords """"""""""""  - Added a``skipna``parameter to `~pandas.api.types.infer_dtype` to   support type inference in the presence of missing values (:issue:`17059`). - `Series.to_dict` and `DataFrame.to_dict` now support an``into`keyword which allows you to specify the`collections.Mapping`subclass that you would like returned.  The default is`dict``, which is backwards compatible. (:issue:`16122`) - `Series.set_axis` and `DataFrame.set_axis` now support the``inplace``parameter. (:issue:`14636`) - `Series.to_pickle` and `DataFrame.to_pickle` have gained a``protocol``parameter (:issue:`16252`). By default, this parameter is set to `HIGHEST_PROTOCOL <https://docs.python.org/3/library/pickle.html#data-stream-format>`__ - `read_feather` has gained the``nthreads``parameter for multi-threaded operations (:issue:`16359`) - `DataFrame.clip` and `Series.clip` have gained an``inplace``argument. (:issue:`15388`) - `crosstab` has gained a``margins\_name`parameter to define the name of the row / column that will contain the totals when`margins=True``. (:issue:`15972`) - `read_json` now accepts a``chunksize`parameter that can be used when`lines=True`. If`chunksize`is passed, read_json now returns an iterator which reads in`chunksize``lines with each iteration. (:issue:`17048`) - `read_json` and `~DataFrame.to_json` now accept a``compression``argument which allows them to transparently handle compressed files. (:issue:`17798`)  Various enhancements """"""""""""""""""""  - Improved the import time of pandas by about 2.25x.  (:issue:`16764`) - Support for `PEP 519 -- Adding a file system path protocol   <https://www.python.org/dev/peps/pep-0519/>`_ on most readers (e.g.   `read_csv`) and writers (e.g. `DataFrame.to_csv`) (:issue:`13823`). - Added a``\_\_fspath\_\_`method to`pd.HDFStore`,`pd.ExcelFile`,   and`pd.ExcelWriter``to work properly with the file system path protocol (:issue:`13823`). - The``validate``argument for `merge` now checks whether a merge is one-to-one, one-to-many, many-to-one, or many-to-many. If a merge is found to not be an example of specified merge type, an exception of type``MergeError``will be raised. For more, see [here <merging.validation>](#here-<merging.validation>) (:issue:`16270`) - Added support for `PEP 518 <https://www.python.org/dev/peps/pep-0518/>`_ (``pyproject.toml``) to the build system (:issue:`16745`) - `RangeIndex.append` now returns a``RangeIndex``object when possible (:issue:`16212`) - `Series.rename_axis` and `DataFrame.rename_axis` with``inplace=True`now return`None``while renaming the axis inplace. (:issue:`15704`) - `api.types.infer_dtype` now infers decimals. (:issue:`15690`) - `DataFrame.select_dtypes` now accepts scalar values for include/exclude as well as list-like. (:issue:`16855`) - `date_range` now accepts 'YS' in addition to 'AS' as an alias for start of year. (:issue:`9313`) - `date_range` now accepts 'Y' in addition to 'A' as an alias for end of year. (:issue:`9313`) - `DataFrame.add_prefix` and `DataFrame.add_suffix` now accept strings containing the '%' character. (:issue:`17151`) - Read/write methods that infer compression (`read_csv`, `read_table`, `read_pickle`, and `~DataFrame.to_pickle`) can now infer from path-like objects, such as``pathlib.Path``. (:issue:`17206`) - `read_sas` now recognizes much more of the most frequently used date (datetime) formats in SAS7BDAT files. (:issue:`15871`) - `DataFrame.items` and `Series.items` are now present in both Python 2 and 3 and is lazy in all cases. (:issue:`13918`, :issue:`17213`) - `pandas.io.formats.style.Styler.where` has been implemented as a convenience for `pandas.io.formats.style.Styler.applymap`. (:issue:`17474`) - `MultiIndex.is_monotonic_decreasing` has been implemented.  Previously returned``False``in all cases. (:issue:`16554`) - `read_excel` raises``ImportError`with a better message if`xlrd``is not installed. (:issue:`17613`) - `DataFrame.assign` will preserve the original order of``\*\*kwargs``for Python 3.6+ users instead of sorting the column names. (:issue:`14207`) - `Series.reindex`, `DataFrame.reindex`, `Index.get_indexer` now support list-like argument for``tolerance``. (:issue:`17367`)  .. _whatsnew_0210.api_breaking:  Backwards incompatible API changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. _whatsnew_0210.api_breaking.deps:  Dependencies have increased minimum versions ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  We have updated our minimum supported versions of dependencies (:issue:`15206`, :issue:`15543`, :issue:`15214`). If installed, we now require:     +--------------+-----------------+----------+    | Package      | Minimum Version | Required |    +==============+=================+==========+    | Numpy        | 1.9.0           |    X     |    +--------------+-----------------+----------+    | Matplotlib   | 1.4.3           |          |    +--------------+-----------------+----------+    | Scipy        | 0.14.0          |          |    +--------------+-----------------+----------+    | Bottleneck   | 1.0.0           |          |    +--------------+-----------------+----------+  Additionally, support has been dropped for Python 3.4 (:issue:`15251`).   .. _whatsnew_0210.api_breaking.bottleneck:  Sum/prod of all-NaN or empty Series/DataFrames is now consistently NaN ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  > **Note** >     The changes described here have been partially reverted. See    the [v0.22.0 Whatsnew <whatsnew_0220>](#v0.22.0-whatsnew-<whatsnew_0220>) for more.   The behavior of``sum`and`prod``on all-NaN Series/DataFrames no longer depends on whether `bottleneck <https://bottleneck.readthedocs.io>`__ is installed, and return value of``sum`and`prod``on an empty Series has changed (:issue:`9422`, :issue:`15507`).  Calling``sum`or`prod`on an empty or all-`NaN`  `Series`, or columns of a`DataFrame`, will result in`NaN`. See the [docs <missing_data.calculations>](#docs-<missing_data.calculations>).  .. ipython:: python     s = pd.Series([np.nan])  Previously WITHOUT`bottleneck`installed:`\`ipython In \[2\]: s.sum() Out\[2\]: np.nan

</div>

Previously WITH `bottleneck`:

``` ipython
In [2]: s.sum()
Out[2]: 0.0
```

New behavior, without regard to the bottleneck installation:

<div class="ipython">

python

s.sum()

</div>

Note that this also changes the sum of an empty `Series`. Previously this always returned 0 regardless of a `bottleneck` installation:

``` ipython
In [1]: pd.Series([]).sum()
Out[1]: 0
```

but for consistency with the all-NaN case, this was changed to return 0 as well:

``` ipython
In [2]: pd.Series([]).sum()
Out[2]: 0
```

<div id="whatsnew_0210.api_breaking.loc">

Indexing with a list with missing labels is deprecated `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously, selecting with a list of labels, where one or more labels were missing would always succeed, returning ``NaN`for missing labels. This will now show a`FutureWarning`. In the future this will raise a`KeyError``(:issue:`15747`). This warning will trigger on a``DataFrame`or a`Series`for using`.loc\[\]`or`\[\[\]\]`when passing a list-of-labels with at least 1 missing label.   .. ipython:: python     s = pd.Series([1, 2, 3])    s  Previous behavior`\`ipython In \[4\]: s.loc\[\[1, 2, 3\]\] Out\[4\]: 1 2.0 2 3.0 3 NaN dtype: float64

</div>

Current behavior

``` ipython
In [4]: s.loc[[1, 2, 3]]
Passing list-likes to .loc or [] with any missing label will raise
KeyError in the future, you can use .reindex() as an alternative.

See the documentation here:
https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike

Out[4]:
1    2.0
2    3.0
3    NaN
dtype: float64
```

The idiomatic way to achieve selecting potentially not-found elements is via `.reindex()`

<div class="ipython">

python

s.reindex(\[1, 2, 3\])

</div>

Selection with all keys found is unchanged.

<div class="ipython">

python

s.loc\[\[1, 2\]\]

</div>

<div id="whatsnew_0210.api.na_changes">

NA naming changes `` ` ^^^^^^^^^^^^^^^^^  In order to promote more consistency among the pandas API, we have added additional top-level functions `isna` and `notna` that are aliases for `isnull` and `notnull`. The naming scheme is now more consistent with methods like ``.dropna()`and`.fillna()`. Furthermore in all cases where`.isnull()`and`.notnull()`methods are defined, these have additional methods named`.isna()`and`.notna()`, these are included for classes`Categorical`,`Index`,`Series`, and`DataFrame``. (:issue:`15001`).  The configuration option``pd.options.mode.use\_inf\_as\_null`is deprecated, and`pd.options.mode.use\_inf\_as\_na`is added as a replacement.   .. _whatsnew_0210.api_breaking.iteration_scalars:  Iteration of Series/Index will now return Python scalars ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously, when using certain iteration methods for a`Series`with dtype`int`or`float`, you would receive a`numpy`scalar, e.g. a`np.int64`, rather than a Python`int``. Issue (:issue:`10904`) corrected this for``Series.tolist()`and`list(Series)`. This change makes all iteration methods consistent, in particular, for`\_\_iter\_\_()`and`.map()``; note that this only affects int/float dtypes. (:issue:`13236`, :issue:`13258`, :issue:`14216`).  .. ipython:: python     s = pd.Series([1, 2, 3])    s  Previously:``\`ipython In \[2\]: type(list(s)\[0\]) Out\[2\]: numpy.int64

</div>

New behavior:

<div class="ipython">

python

type(list(s)\[0\])

</div>

Furthermore this will now correctly box the results of iteration for <span class="title-ref">DataFrame.to\_dict</span> as well.

<div class="ipython">

python

d = {'a': \[1\], 'b': \['b'\]} df = pd.DataFrame(d)

</div>

Previously:

``` ipython
In [8]: type(df.to_dict()['a'][0])
Out[8]: numpy.int64
```

New behavior:

<div class="ipython">

python

type(df.to\_dict()\['a'\]\[0\])

</div>

<div id="whatsnew_0210.api_breaking.loc_with_index">

Indexing with a Boolean Index `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously when passing a boolean ``Index`to`.loc`, if the index of the`Series/DataFrame`had`boolean`labels, you would get a label based selection, potentially duplicating result labels, rather than a boolean indexing selection (where`True``selects elements), this was inconsistent how a boolean numpy array indexed. The new behavior is to act like a boolean numpy array indexer. (:issue:`17738`)  Previous behavior:  .. ipython:: python     s = pd.Series([1, 2, 3], index=[False, True, False])    s``\`ipython In \[59\]: s.loc\[pd.Index(\[True, False, True\])\] Out\[59\]: True 2 False 1 False 3 True 2 dtype: int64

</div>

Current behavior

<div class="ipython">

python

s.loc\[pd.Index(\[True, False, True\])\]

</div>

Furthermore, previously if you had an index that was non-numeric (e.g. strings), then a boolean Index would raise a `KeyError`. `` ` This will now be treated as a boolean indexer.  Previously behavior:  .. ipython:: python     s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])    s ``\`ipython In \[39\]: s.loc\[pd.Index(\[True, False, True\])\] KeyError: "None of \[Index(\[True, False, True\], dtype='object')\] are in the \[index\]"

Current behavior

<div class="ipython">

python

s.loc\[pd.Index(\[True, False, True\])\]

</div>

<div id="whatsnew_0210.api_breaking.period_index_resampling">

`PeriodIndex` resampling `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^  In previous versions of pandas, resampling a ``Series`/`DataFrame`indexed by a`PeriodIndex`returned a`DatetimeIndex``in some cases (:issue:`12884`). Resampling to a multiplied frequency now returns a``PeriodIndex``(:issue:`15944`). As a minor enhancement, resampling a``PeriodIndex`can now handle`NaT``values (:issue:`13224`)  Previous behavior:``\`ipython In \[1\]: pi = pd.period\_range('2017-01', periods=12, freq='M')

</div>

> In \[2\]: s = pd.Series(np.arange(12), index=pi)
> 
> In \[3\]: resampled = s.resample('2Q').mean()
> 
> In \[4\]: resampled Out\[4\]: 2017-03-31 1.0 2017-09-30 5.5 2018-03-31 10.0 Freq: 2Q-DEC, dtype: float64
> 
> In \[5\]: resampled.index Out\[5\]: DatetimeIndex(\['2017-03-31', '2017-09-30', '2018-03-31'\], dtype='datetime64\[ns\]', freq='2Q-DEC')

New behavior:

``` ipython
In [1]: pi = pd.period_range('2017-01', periods=12, freq='M')

In [2]: s = pd.Series(np.arange(12), index=pi)

In [3]: resampled = s.resample('2Q').mean()

In [4]: resampled
Out[4]:
2017Q1    2.5
2017Q3    8.5
Freq: 2Q-DEC, dtype: float64

In [5]: resampled.index
Out[5]: PeriodIndex(['2017Q1', '2017Q3'], dtype='period[2Q-DEC]')
```

Upsampling and calling `.ohlc()` previously returned a `Series`, basically identical to calling `.asfreq()`. OHLC upsampling now returns a DataFrame with columns `open`, `high`, `low` and `close` (`13083`). This is consistent with downsampling and `DatetimeIndex` behavior.

Previous behavior:

``` ipython
In [1]: pi = pd.period_range(start='2000-01-01', freq='D', periods=10)

In [2]: s = pd.Series(np.arange(10), index=pi)

In [3]: s.resample('H').ohlc()
Out[3]:
2000-01-01 00:00    0.0
                ...
2000-01-10 23:00    NaN
Freq: H, Length: 240, dtype: float64

In [4]: s.resample('M').ohlc()
Out[4]:
         open  high  low  close
2000-01     0     9    0      9
```

New behavior:

``` ipython
In [56]: pi = pd.period_range(start='2000-01-01', freq='D', periods=10)

In [57]: s = pd.Series(np.arange(10), index=pi)

In [58]: s.resample('H').ohlc()
Out[58]:
                  open  high  low  close
2000-01-01 00:00   0.0   0.0  0.0    0.0
2000-01-01 01:00   NaN   NaN  NaN    NaN
2000-01-01 02:00   NaN   NaN  NaN    NaN
2000-01-01 03:00   NaN   NaN  NaN    NaN
2000-01-01 04:00   NaN   NaN  NaN    NaN
...                ...   ...  ...    ...
2000-01-10 19:00   NaN   NaN  NaN    NaN
2000-01-10 20:00   NaN   NaN  NaN    NaN
2000-01-10 21:00   NaN   NaN  NaN    NaN
2000-01-10 22:00   NaN   NaN  NaN    NaN
2000-01-10 23:00   NaN   NaN  NaN    NaN

[240 rows x 4 columns]

In [59]: s.resample('M').ohlc()
Out[59]:
         open  high  low  close
2000-01     0     9    0      9

[1 rows x 4 columns]
```

<div id="whatsnew_0210.api_breaking.pandas_eval">

Improved error handling during item assignment in pd.eval `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `eval` will now raise a ``ValueError``when item assignment malfunctions, or inplace operations are specified, but there is no item assignment in the expression (:issue:`16732`)  .. ipython:: python     arr = np.array([1, 2, 3])  Previously, if you attempted the following expression, you would get a not very helpful error message:``\`ipython In \[3\]: pd.eval("a = 1 + 2", target=arr, inplace=True) ... IndexError: only integers, slices (<span class="title-ref">:</span>), ellipsis (<span class="title-ref">...</span>), numpy.newaxis (<span class="title-ref">None</span>) and integer or boolean arrays are valid indices

</div>

This is a very long way of saying numpy arrays don't support string-item indexing. With this `` ` change, the error message is now this: ``\`python In \[3\]: pd.eval("a = 1 + 2", target=arr, inplace=True) ... ValueError: Cannot assign expression output to target

It also used to be possible to evaluate expressions inplace, even if there was no item assignment:

``` ipython
In [4]: pd.eval("1 + 2", target=arr, inplace=True)
Out[4]: 3
```

However, this input does not make much sense because the output is not being assigned to `` ` the target. Now, a ``ValueError`will be raised when such an input is passed in:`\`ipython In \[4\]: pd.eval("1 + 2", target=arr, inplace=True) ... ValueError: Cannot operate inplace if there is no assignment

<div id="whatsnew_0210.api_breaking.dtype_conversions">

Dtype conversions `` ` ^^^^^^^^^^^^^^^^^  Previously assignments, ``.where()`and`.fillna()`with a`bool`assignment, would coerce to same the type (e.g. int / float), or raise for datetimelikes. These will now preserve the bools with`object``dtypes. (:issue:`16821`).  .. ipython:: python     s = pd.Series([1, 2, 3])``\`python In \[5\]: s\[1\] = True

</div>

> In \[6\]: s Out\[6\]: 0 1 1 1 2 3 dtype: int64

New behavior

``` ipython
In [7]: s[1] = True

In [8]: s
Out[8]:
0       1
1    True
2       3
Length: 3, dtype: object
```

Previously, as assignment to a datetimelike with a non-datetimelike would coerce the `` ` non-datetime-like item being assigned (:issue:`14145`).  .. ipython:: python     s = pd.Series([pd.Timestamp('2011-01-01'), pd.Timestamp('2012-01-01')]) ``\`python In \[1\]: s\[1\] = 1

> In \[2\]: s Out\[2\]: 0 2011-01-01 00:00:00.000000000 1 1970-01-01 00:00:00.000000001 dtype: datetime64\[ns\]

These now coerce to `object` dtype.

``` python
In [1]: s[1] = 1

In [2]: s
Out[2]:
0    2011-01-01 00:00:00
1                      1
dtype: object
```

\- Inconsistent behavior in `.where()` with datetimelikes which would raise rather than coerce to `object` (`16402`) `` ` - Bug in assignment against ``int64`data with`np.ndarray`with`float64`dtype may keep`int64``dtype (:issue:`14001`)   .. _whatsnew_210.api.multiindex_single:  MultiIndex constructor with a single level ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The``MultiIndex`constructors no longer squeezes a MultiIndex with all length-one levels down to a regular`Index`. This affects all the`MultiIndex``constructors. (:issue:`17178`)  Previous behavior:``\`ipython In \[2\]: pd.MultiIndex.from\_tuples(\[('a',), ('b',)\]) Out\[2\]: Index(\['a', 'b'\], dtype='object')

Length 1 levels are no longer special-cased. They behave exactly as if you had `` ` length 2+ levels, so a `MultiIndex` is always returned from all of the ``MultiIndex``constructors:  .. ipython:: python     pd.MultiIndex.from_tuples([('a',), ('b',)])  .. _whatsnew_0210.api.utc_localization_with_series:  UTC localization with Series ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously, `to_datetime` did not localize datetime``Series`data when`utc=True``was passed. Now, `to_datetime` will correctly localize``Series`with a`datetime64\[ns, UTC\]`dtype to be consistent with how list-like and`Index``data are handled. (:issue:`6415`).  Previous behavior  .. ipython:: python     s = pd.Series(['20130101 00:00:00'] * 3)``\`ipython In \[12\]: pd.to\_datetime(s, utc=True) Out\[12\]: 0 2013-01-01 1 2013-01-01 2 2013-01-01 dtype: datetime64\[ns\]

New behavior

<div class="ipython">

python

pd.to\_datetime(s, utc=True)

</div>

Additionally, DataFrames with datetime columns that were parsed by <span class="title-ref">read\_sql\_table</span> and <span class="title-ref">read\_sql\_query</span> will also be localized to UTC only if the original SQL columns were timezone aware datetime columns.

<div id="whatsnew_0210.api.consistency_of_range_functions">

Consistency of range functions `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In previous versions, there were some inconsistencies between the various range functions: `date_range`, `bdate_range`, `period_range`, `timedelta_range`, and `interval_range`. (:issue:`17471`).  One of the inconsistent behaviors occurred when the ``start`,`end`and`period`parameters were all specified, potentially leading to ambiguous ranges.  When all three parameters were passed,`interval\_range`ignored the`period`parameter,`period\_range`ignored the`end`parameter, and the other range functions raised.  To promote consistency among the range functions, and avoid potentially ambiguous ranges,`interval\_range`and`period\_range`will now raise when all three parameters are passed.  Previous behavior:`\`ipython In \[2\]: pd.interval\_range(start=0, end=4, periods=6) Out\[2\]: IntervalIndex(\[(0, 1\], (1, 2\], (2, 3\]\] closed='right', dtype='interval\[int64\]')

</div>

> In \[3\]: pd.period\_range(start='2017Q1', end='2017Q4', periods=6, freq='Q') Out\[3\]: PeriodIndex(\['2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1', '2018Q2'\], dtype='period\[Q-DEC\]', freq='Q-DEC')

New behavior:

``` ipython
In [2]: pd.interval_range(start=0, end=4, periods=6)
---------------------------------------------------------------------------
ValueError: Of the three parameters: start, end, and periods, exactly two must be specified

In [3]: pd.period_range(start='2017Q1', end='2017Q4', periods=6, freq='Q')
---------------------------------------------------------------------------
ValueError: Of the three parameters: start, end, and periods, exactly two must be specified
```

Additionally, the endpoint parameter `end` was not included in the intervals produced by `interval_range`. However, all other range functions include `end` in their output. To promote consistency among the range functions, `interval_range` will now include `end` as the right endpoint of the final interval, except if `freq` is specified in a way which skips `end`.

Previous behavior:

``` ipython
In [4]: pd.interval_range(start=0, end=4)
Out[4]:
IntervalIndex([(0, 1], (1, 2], (2, 3]]
              closed='right',
              dtype='interval[int64]')
```

New behavior:

<div class="ipython">

python

pd.interval\_range(start=0, end=4)

</div>

<div id="whatsnew_0210.api.mpl_converters">

No automatic Matplotlib converters `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  pandas no longer registers our ``date`,`time`,`datetime`,`datetime64`, and`Period`converters with matplotlib when pandas is imported. Matplotlib plot methods (`plt.plot`,`ax.plot`, ...), will not nicely format the x-axis for`DatetimeIndex`or`PeriodIndex`values. You must explicitly register these methods:  pandas built-in`Series.plot`and`DataFrame.plot``*will* register these converters on first-use (:issue:`17710`).  > **Note** >    This change has been temporarily reverted in pandas 0.21.1,   for more details see [here <whatsnew_0211.converters>](#here-<whatsnew_0211.converters>).  .. _whatsnew_0210.api:  Other API changes ^^^^^^^^^^^^^^^^^  - The Categorical constructor no longer accepts a scalar for the``categories``keyword. (:issue:`16022`) - Accessing a non-existent attribute on a closed `~pandas.HDFStore` will now   raise an``AttributeError`rather than a`ClosedFileError``(:issue:`16301`) - `read_csv` now issues a``UserWarning`if the`names``parameter contains duplicates (:issue:`17095`) - `read_csv` now treats``'null'`and`'n/a'``strings as missing values by default (:issue:`16471`, :issue:`16078`) - `pandas.HDFStore`'s string representation is now faster and less detailed. For the previous behavior, use``pandas.HDFStore.info()``. (:issue:`16503`). - Compression defaults in HDF stores now follow pytables standards. Default is no compression and if``complib`is missing and`complevel`> 0`zlib``is used (:issue:`15943`) -``Index.get\_indexer\_non\_unique()`now returns a ndarray indexer rather than an`Index`; this is consistent with`Index.get\_indexer()``(:issue:`16819`) - Removed the``@slow`decorator from`pandas.\_testing`, which caused issues for some downstream packages' test suites. Use`@pytest.mark.slow``instead, which achieves the same thing (:issue:`16850`) - Moved definition of``MergeError`to the`pandas.errors``module. - The signature of `Series.set_axis` and `DataFrame.set_axis` has been changed from``set\_axis(axis, labels)`to`set\_axis(labels, axis=0)`, for consistency with the rest of the API. The old signature is deprecated and will show a`FutureWarning``(:issue:`14636`) - `Series.argmin` and `Series.argmax` will now raise a``TypeError`when used with`object`dtypes, instead of a`ValueError``(:issue:`13595`) - `Period` is now immutable, and will now raise an``AttributeError`when a user tries to assign a new value to the`ordinal`or`freq``attributes (:issue:`17116`). - `to_datetime` when passed a tz-aware``origin=`kwarg will now raise a more informative`ValueError`rather than a`TypeError``(:issue:`16842`) - `to_datetime` now raises a``ValueError`when format includes`%W`or`%U``without also including day of the week and calendar year (:issue:`16774`) - Renamed non-functional``index`to`index\_col``in `read_stata` to improve API consistency (:issue:`16342`) - Bug in `DataFrame.drop` caused boolean labels``False`and`True``to be treated as labels 0 and 1 respectively when dropping indices from a numeric index. This will now raise a ValueError (:issue:`16877`) - Restricted DateOffset keyword arguments.  Previously,``DateOffset``subclasses allowed arbitrary keyword arguments which could lead to unexpected behavior.  Now, only valid arguments will be accepted. (:issue:`17176`).  .. _whatsnew_0210.deprecations:  Deprecations ~~~~~~~~~~~~  - `DataFrame.from_csv` and `Series.from_csv` have been deprecated in favor of `read_csv` (:issue:`4191`) - `read_excel` has deprecated``sheetname`in favor of`sheet\_name`for consistency with`.to\_excel()``(:issue:`10559`). - `read_excel` has deprecated``parse\_cols`in favor of`usecols``for consistency with `read_csv` (:issue:`4988`) - `read_csv` has deprecated the``tupleize\_cols`argument. Column tuples will always be converted to a`MultiIndex``(:issue:`17060`) - `DataFrame.to_csv` has deprecated the``tupleize\_cols``argument. MultiIndex columns will be always written as rows in the CSV file (:issue:`17060`) - The``convert`parameter has been deprecated in the`.take()``method, as it was not being respected (:issue:`16948`) -``pd.options.html.border`has been deprecated in favor of`pd.options.display.html.border``(:issue:`15793`). - `SeriesGroupBy.nth` has deprecated``True`in favor of`'all'`for its kwarg`dropna``(:issue:`11038`). - `DataFrame.as_blocks` is deprecated, as this is exposing the internal implementation (:issue:`17302`) -``pd.TimeGrouper``is deprecated in favor of `pandas.Grouper` (:issue:`16747`) -``cdate\_range``has been deprecated in favor of `bdate_range`, which has gained``weekmask`and`holidays``parameters for building custom frequency date ranges. See the [documentation <timeseries.custom-freq-ranges>](#documentation-<timeseries.custom-freq-ranges>) for more details (:issue:`17596`) - passing``categories`or`ordered``kwargs to `Series.astype` is deprecated, in favor of passing a [CategoricalDtype <whatsnew_0210.enhancements.categorical_dtype>](#categoricaldtype-<whatsnew_0210.enhancements.categorical_dtype>) (:issue:`17636`) -``.get\_value`and`.set\_value`on`Series`,`DataFrame`,`Panel`,`SparseSeries`, and`SparseDataFrame`are deprecated in favor of using`.iat\[\]`or`.at\[\]``accessors (:issue:`15269`) - Passing a non-existent column in``.to\_excel(..., columns=)`is deprecated and will raise a`KeyError``in the future (:issue:`17295`) -``raise\_on\_error``parameter to `Series.where`, `Series.mask`, `DataFrame.where`, `DataFrame.mask` is deprecated, in favor of``errors=``(:issue:`14968`) - Using `DataFrame.rename_axis` and `Series.rename_axis` to alter index or column *labels* is now deprecated in favor of using``.rename`.`rename\_axis``may still be used to alter the name of the index or columns (:issue:`17833`). - `~DataFrame.reindex_axis` has been deprecated in favor of `~DataFrame.reindex`. See [here <whatsnew_0210.enhancements.rename_reindex_axis>](#here-<whatsnew_0210.enhancements.rename_reindex_axis>) for more (:issue:`17833`).  .. _whatsnew_0210.deprecations.select:  Series.select and DataFrame.select ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The `Series.select` and `DataFrame.select` methods are deprecated in favor of using``df.loc\[labels.map(crit)\]``(:issue:`12401`)  .. ipython:: python     df = pd.DataFrame({'A': [1, 2, 3]}, index=['foo', 'bar', 'baz'])``\`ipython In \[3\]: df.select(lambda x: x in \['bar', 'baz'\]) FutureWarning: select is deprecated and will be removed in a future release. You can use .loc\[crit\] as a replacement Out\[3\]: A bar 2 baz 3

</div>

<div class="ipython">

python

df.loc\[df.index.map(lambda x: x in \['bar', 'baz'\])\]

</div>

<div id="whatsnew_0210.deprecations.argmin_min">

Series.argmax and Series.argmin `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The behavior of `Series.argmax` and `Series.argmin` have been deprecated in favor of `Series.idxmax` and `Series.idxmin`, respectively (:issue:`16830`).  For compatibility with NumPy arrays, ``pd.Series`implements`argmax`and`argmin`. Since pandas 0.13.0,`argmax``has been an alias for `pandas.Series.idxmax`, and``argmin``has been an alias for `pandas.Series.idxmin`. They return the *label* of the maximum or minimum, rather than the *position*.  We've deprecated the current behavior of``Series.argmax`and`Series.argmin`. Using either of these will emit a`FutureWarning``. Use `Series.idxmax` if you want the label of the maximum. Use``Series.values.argmax()`if you want the position of the maximum. Likewise for the minimum. In a future release`Series.argmax`and`Series.argmin``will return the position of the maximum or minimum.  .. _whatsnew_0210.prior_deprecations:  Removal of prior version deprecations/changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - `read_excel` has dropped the``has\_index\_names``parameter (:issue:`10967`) - The``pd.options.display.height``configuration has been dropped (:issue:`3663`) - The``pd.options.display.line\_width``configuration has been dropped (:issue:`2881`) - The``pd.options.display.mpl\_style``configuration has been dropped (:issue:`12190`) -``Index`has dropped the`.sym\_diff()`method in favor of`.symmetric\_difference()``(:issue:`12591`) -``Categorical`has dropped the`.order()`and`.sort()`methods in favor of`.sort\_values()``(:issue:`12882`) - `eval` and `DataFrame.eval` have changed the default of``inplace`from`None`to`False``(:issue:`11149`) - The function``get\_offset\_name`has been dropped in favor of the`.freqstr``attribute for an offset (:issue:`11834`) - pandas no longer tests for compatibility with hdf5-files created with pandas < 0.11 (:issue:`17404`).    .. _whatsnew_0210.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Improved performance of instantiating `SparseDataFrame` (:issue:`16773`) - `Series.dt` no longer performs frequency inference, yielding a large speedup when accessing the attribute (:issue:`17210`) - Improved performance of `~Series.cat.set_categories` by not materializing the values (:issue:`17508`) - `Timestamp.microsecond` no longer re-computes on attribute access (:issue:`17331`) - Improved performance of the `CategoricalIndex` for data that is already categorical dtype (:issue:`17513`) - Improved performance of `RangeIndex.min` and `RangeIndex.max` by using``RangeIndex``properties to perform the computations (:issue:`17607`)  .. _whatsnew_0210.docs:  Documentation changes ~~~~~~~~~~~~~~~~~~~~~  - Several``NaT``method docstrings (e.g. `NaT.ctime`) were incorrect (:issue:`17327`) - The documentation has had references to versions < v0.17 removed and cleaned up (:issue:`17442`, :issue:`17442`, :issue:`17404` & :issue:`17504`)  .. _whatsnew_0210.bug_fixes:  Bug fixes ~~~~~~~~~  Conversion ^^^^^^^^^^  - Bug in assignment against datetime-like data with``int``may incorrectly convert to datetime-like (:issue:`14145`) - Bug in assignment against``int64`data with`np.ndarray`with`float64`dtype may keep`int64``dtype (:issue:`14001`) - Fixed the return type of``IntervalIndex.is\_non\_overlapping\_monotonic`to be a Python`bool`for consistency with similar attributes/methods.  Previously returned a`[numpy.bool]()``. (:issue:`17237`) - Bug in``IntervalIndex.is\_non\_overlapping\_monotonic``when intervals are closed on both sides and overlap at a point (:issue:`16560`) - Bug in `Series.fillna` returns frame when``inplace=True`and`value``is dict (:issue:`16156`) - Bug in `Timestamp.weekday_name` returning a UTC-based weekday name when localized to a timezone (:issue:`17354`) - Bug in``Timestamp.replace`when replacing`tzinfo``around DST changes (:issue:`15683`) - Bug in``Timedelta`construction and arithmetic that would not propagate the`Overflow``exception (:issue:`17367`) - Bug in `~DataFrame.astype` converting to object dtype when passed extension type classes (``DatetimeTZDtype`,`CategoricalDtype`) rather than instances. Now a`TypeError``is raised when a class is passed (:issue:`17780`). - Bug in `to_numeric` in which elements were not always being coerced to numeric when``errors='coerce'``(:issue:`17007`, :issue:`17125`) - Bug in``DataFrame`and`Series`constructors where`range`objects are converted to`int32`dtype on Windows instead of`int64``(:issue:`16804`)  Indexing ^^^^^^^^  - When called with a null slice (e.g.``df.iloc\[:\]`), the`.iloc`and`.loc``indexers return a shallow copy of the original object. Previously they returned the original object. (:issue:`13873`). - When called on an unsorted``MultiIndex`, the`loc`indexer now will raise`UnsortedIndexError``only if proper slicing is used on non-sorted levels (:issue:`16734`). - Fixes regression in 0.20.3 when indexing with a string on a``TimedeltaIndex``(:issue:`16896`). - Fixed `TimedeltaIndex.get_loc` handling of``np.timedelta64``inputs (:issue:`16909`). - Fix `MultiIndex.sort_index` ordering when``ascending``argument is a list, but not all levels are specified, or are in a different order (:issue:`16934`). - Fixes bug where indexing with``np.inf`caused an`OverflowError``to be raised (:issue:`16957`) - Bug in reindexing on an empty``CategoricalIndex``(:issue:`16770`) - Fixes``DataFrame.loc`for setting with alignment and tz-aware`DatetimeIndex``(:issue:`16889`) - Avoids``IndexError`when passing an Index or Series to`.iloc``with older numpy (:issue:`17193`) - Allow unicode empty strings as placeholders in multilevel columns in Python 2 (:issue:`17099`) - Bug in``.iloc`when used with inplace addition or assignment and an int indexer on a`MultiIndex``causing the wrong indexes to be read from and written to (:issue:`17148`) - Bug in``.isin()`in which checking membership in empty`Series``objects raised an error (:issue:`16991`) - Bug in``CategoricalIndex``reindexing in which specified indices containing duplicates were not being respected (:issue:`17323`) - Bug in intersection of``RangeIndex``with negative step (:issue:`17296`) - Bug in``IntervalIndex``where performing a scalar lookup fails for included right endpoints of non-overlapping monotonic decreasing indexes (:issue:`16417`, :issue:`17271`) - Bug in `DataFrame.first_valid_index` and `DataFrame.last_valid_index` when no valid entry (:issue:`17400`) - Bug in `Series.rename` when called with a callable, incorrectly alters the name of the``Series`, rather than the name of the`Index``. (:issue:`17407`) - Bug in `String.str_get` raises``IndexError``instead of inserting NaNs when using a negative index. (:issue:`17704`)  IO ^^  - Bug in `read_hdf` when reading a timezone aware index from``fixed``format HDFStore (:issue:`17618`) - Bug in `read_csv` in which columns were not being thoroughly de-duplicated (:issue:`17060`) - Bug in `read_csv` in which specified column names were not being thoroughly de-duplicated (:issue:`17095`) - Bug in `read_csv` in which non integer values for the header argument generated an unhelpful / unrelated error message (:issue:`16338`) - Bug in `read_csv` in which memory management issues in exception handling, under certain conditions, would cause the interpreter to segfault (:issue:`14696`, :issue:`16798`). - Bug in `read_csv` when called with``low\_memory=False`in which a CSV with at least one column > 2GB in size would incorrectly raise a`MemoryError``(:issue:`16798`). - Bug in `read_csv` when called with a single-element list``header`would return a`DataFrame``of all NaN values (:issue:`7757`) - Bug in `DataFrame.to_csv` defaulting to 'ascii' encoding in Python 3, instead of 'utf-8' (:issue:`17097`) - Bug in `read_stata` where value labels could not be read when using an iterator (:issue:`16923`) - Bug in `read_stata` where the index was not set (:issue:`16342`) - Bug in `read_html` where import check fails when run in multiple threads (:issue:`16928`) - Bug in `read_csv` where automatic delimiter detection caused a``TypeError``to be thrown when a bad line was encountered rather than the correct error message (:issue:`13374`) - Bug in `DataFrame.to_html` with``notebook=True``where DataFrames with named indices or non-MultiIndex indices had undesired horizontal or vertical alignment for column or row labels, respectively (:issue:`16792`) - Bug in `DataFrame.to_html` in which there was no validation of the``justify``parameter (:issue:`17527`) - Bug in `HDFStore.select` when reading a contiguous mixed-data table featuring VLArray (:issue:`17021`) - Bug in `to_json` where several conditions (including objects with unprintable symbols, objects with deep recursion, overlong labels) caused segfaults instead of raising the appropriate exception (:issue:`14256`)  Plotting ^^^^^^^^ - Bug in plotting methods using``secondary\_y`and`fontsize``not setting secondary axis font size (:issue:`12565`) - Bug when plotting``timedelta`and`datetime``dtypes on y-axis (:issue:`16953`) - Line plots no longer assume monotonic x data when calculating xlims, they show the entire lines now even for unsorted x data. (:issue:`11310`, :issue:`11471`) - With matplotlib 2.0.0 and above, calculation of x limits for line plots is left to matplotlib, so that its new default settings are applied. (:issue:`15495`) - Bug in``Series.plot.bar`or`DataFrame.plot.bar`with`y`not respecting user-passed`color``(:issue:`16822`) - Bug causing``plotting.parallel\_coordinates``to reset the random seed when using random colors (:issue:`17525`)   GroupBy/resample/rolling ^^^^^^^^^^^^^^^^^^^^^^^^  - Bug in``DataFrame.resample(...).size()`where an empty`DataFrame`did not return a`Series``(:issue:`14962`) - Bug in `infer_freq` causing indices with 2-day gaps during the working week to be wrongly inferred as business daily (:issue:`16624`) - Bug in``.rolling(...).quantile()``which incorrectly used different defaults than `Series.quantile` and `DataFrame.quantile` (:issue:`9413`, :issue:`16211`) - Bug in``groupby.transform()``that would coerce boolean dtypes back to float (:issue:`16875`) - Bug in``Series.resample(...).apply()`where an empty`Series`modified the source index and did not return the name of a`Series``(:issue:`14313`) - Bug in``.rolling(...).apply(...)`with a`DataFrame`with a`DatetimeIndex`, a`window`of a timedelta-convertible and`min\_periods \>= 1``(:issue:`15305`) - Bug in``DataFrame.groupby``where index and column keys were not recognized correctly when the number of keys equaled the number of elements on the groupby axis (:issue:`16859`) - Bug in``groupby.nunique()`with`TimeGrouper`which cannot handle`NaT``correctly (:issue:`17575`) - Bug in``DataFrame.groupby`where a single level selection from a`MultiIndex``unexpectedly sorts (:issue:`17537`) - Bug in``DataFrame.groupby`where spurious warning is raised when`Grouper``object is used to override ambiguous column name (:issue:`17383`) - Bug in``TimeGrouper``differs when passes as a list and as a scalar (:issue:`17530`)  Sparse ^^^^^^  - Bug in``SparseSeries`raises`AttributeError``when a dictionary is passed in as data (:issue:`16905`) - Bug in `SparseDataFrame.fillna` not filling all NaNs when frame was instantiated from SciPy sparse matrix (:issue:`16112`) - Bug in `SparseSeries.unstack` and `SparseDataFrame.stack` (:issue:`16614`, :issue:`15045`) - Bug in `make_sparse` treating two numeric/boolean data, which have same bits, as same when array``dtype`is`object``(:issue:`17574`) - `SparseArray.all` and `SparseArray.any` are now implemented to handle``SparseArray``, these were used but not implemented (:issue:`17570`)  Reshaping ^^^^^^^^^ - Joining/Merging with a non unique``PeriodIndex`raised a`TypeError``(:issue:`16871`) - Bug in `crosstab` where non-aligned series of integers were casted to float (:issue:`17005`) - Bug in merging with categorical dtypes with datetimelikes incorrectly raised a``TypeError``(:issue:`16900`) - Bug when using `isin` on a large object series and large comparison array (:issue:`16012`) - Fixes regression from 0.20, `Series.aggregate` and `DataFrame.aggregate` allow dictionaries as return values again (:issue:`16741`) - Fixes dtype of result with integer dtype input, from `pivot_table` when called with``margins=True``(:issue:`17013`) - Bug in `crosstab` where passing two``Series`with the same name raised a`KeyError``(:issue:`13279`) - `Series.argmin`, `Series.argmax`, and their counterparts on``DataFrame``and groupby objects work correctly with floating point data that contains infinite values (:issue:`13595`). - Bug in `unique` where checking a tuple of strings raised a``TypeError``(:issue:`17108`) - Bug in `concat` where order of result index was unpredictable if it contained non-comparable elements (:issue:`17344`) - Fixes regression when sorting by multiple columns on a``datetime64`dtype`Series`with`NaT``values (:issue:`16836`) - Bug in `pivot_table` where the result's columns did not preserve the categorical dtype of``columns`when`dropna`was`False``(:issue:`17842`) - Bug in``DataFrame.drop\_duplicates`where dropping with non-unique column names raised a`ValueError``(:issue:`17836`) - Bug in `unstack` which, when called on a list of levels, would discard the``fillna``argument (:issue:`13971`) - Bug in the alignment of``range`objects and other list-likes with`DataFrame``leading to operations being performed row-wise instead of column-wise (:issue:`17901`)  Numeric ^^^^^^^ - Bug in``.clip()`with`axis=1`and a list-like for`threshold`is passed; previously this raised`ValueError``(:issue:`15390`) - `Series.clip` and `DataFrame.clip` now treat NA values for upper and lower arguments as``None`instead of raising`ValueError``(:issue:`17276`).   Categorical ^^^^^^^^^^^ - Bug in `Series.isin` when called with a categorical (:issue:`16639`) - Bug in the categorical constructor with empty values and categories causing the``.categories`to be an empty`Float64Index`rather than an empty`Index``with object dtype (:issue:`17248`) - Bug in categorical operations with [Series.cat <categorical.cat>](#series.cat-<categorical.cat>) not preserving the original Series' name (:issue:`17509`) - Bug in `DataFrame.merge` failing for categorical columns with boolean/int data types (:issue:`17187`) - Bug in constructing a``Categorical`/`CategoricalDtype`when the specified`categories``are of categorical type (:issue:`17884`).  .. _whatsnew_0210.pypy:  PyPy ^^^^  - Compatibility with PyPy in `read_csv` with``usecols=\[\<unsorted ints\>\]``and   `read_json` (:issue:`17351`) - Split tests into cases for CPython and PyPy where needed, which highlights the fragility   of index matching with``float('nan')`,`np.nan`and`NAT``(:issue:`17351`) - Fix `DataFrame.memory_usage` to support PyPy. Objects on PyPy do not have a fixed size,   so an approximation is used instead (:issue:`17228`)  Other ^^^^^ - Bug where some inplace operators were not being wrapped and produced a copy when invoked (:issue:`12962`) - Bug in `eval` where the``inplace\`<span class="title-ref"> parameter was being incorrectly handled (:issue:\`16732</span>)

</div>

## Contributors

<div class="contributors">

v0.20.3..v0.21.0

</div>

---

v0.21.1.md

---

# Version 0.21.1 (December 12, 2017)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a minor bug-fix release in the 0.21.x series and includes some small regression fixes, bug fixes and performance improvements. We recommend that all users upgrade to this version.

Highlights include:

  - Temporarily restore matplotlib datetime plotting functionality. This should resolve issues for users who implicitly relied on pandas to plot datetimes with matplotlib. See \[here \<whatsnew\_0211.converters\>\](\#here-\<whatsnew\_0211.converters\>).
  - Improvements to the Parquet IO functions introduced in 0.21.0. See \[here \<whatsnew\_0211.enhancements.parquet\>\](\#here-\<whatsnew\_0211.enhancements.parquet\>).

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.21.1

</div>

## Restore Matplotlib datetime converter registration

pandas implements some matplotlib converters for nicely formatting the axis labels on plots with `datetime` or `Period` values. Prior to pandas 0.21.0, these were implicitly registered with matplotlib, as a side effect of `import pandas`.

In pandas 0.21.0, we required users to explicitly register the converter. This caused problems for some users who relied on those converters being present for regular `matplotlib.pyplot` plotting methods, so we're temporarily reverting that change; pandas 0.21.1 again registers the converters on import, just like before 0.21.0.

We've added a new option to control the converters: `pd.options.plotting.matplotlib.register_converters`. By default, they are registered. Toggling this to `False` removes pandas' formatters and restore any converters we overwrote when registering them (`18301`).

We're working with the matplotlib developers to make this easier. We're trying to balance user convenience (automatically registering the converters) with import performance and best practices (importing pandas shouldn't have the side effect of overwriting any custom converters you've already set). In the future we hope to have most of the datetime formatting functionality in matplotlib, with just the pandas-specific converters in pandas. We'll then gracefully deprecate the automatic registration of converters in favor of users explicitly registering them when they want them.

## New features

### Improvements to the Parquet IO functionality

  - <span class="title-ref">DataFrame.to\_parquet</span> will now write non-default indexes when the underlying engine supports it. The indexes will be preserved when reading back in with <span class="title-ref">read\_parquet</span> (`18581`).
  - <span class="title-ref">read\_parquet</span> now allows to specify the columns to read from a parquet file (`18154`)
  - <span class="title-ref">read\_parquet</span> now allows to specify kwargs which are passed to the respective engine (`18216`)

### Other enhancements

  - <span class="title-ref">Timestamp.timestamp</span> is now available in Python 2.7. (`17329`)
  - <span class="title-ref">Grouper</span> and <span class="title-ref">TimeGrouper</span> now have a friendly repr output (`18203`).

## Deprecations

  - `pandas.tseries.register` has been renamed to <span class="title-ref">pandas.plotting.register\_matplotlib\_converters</span> (`18301`)

## Performance improvements

  - Improved performance of plotting large series/dataframes (`18236`).

## Bug fixes

### Conversion

  - Bug in <span class="title-ref">TimedeltaIndex</span> subtraction could incorrectly overflow when `NaT` is present (`17791`)
  - Bug in <span class="title-ref">DatetimeIndex</span> subtracting datetimelike from DatetimeIndex could fail to overflow (`18020`)
  - Bug in <span class="title-ref">IntervalIndex.copy</span> when copying and `IntervalIndex` with non-default `closed` (`18339`)
  - Bug in <span class="title-ref">DataFrame.to\_dict</span> where columns of datetime that are tz-aware were not converted to required arrays when used with `orient='records'`, raising `TypeError` (`18372`)
  - Bug in <span class="title-ref">DateTimeIndex</span> and <span class="title-ref">date\_range</span> where mismatching tz-aware `start` and `end` timezones would not raise an err if `end.tzinfo` is None (`18431`)
  - Bug in <span class="title-ref">Series.fillna</span> which raised when passed a long integer on Python 2 (`18159`).

### Indexing

  - Bug in a boolean comparison of a `datetime.datetime` and a `datetime64[ns]` dtype Series (`17965`)
  - Bug where a `MultiIndex` with more than a million records was not raising `AttributeError` when trying to access a missing attribute (`18165`)
  - Bug in <span class="title-ref">IntervalIndex</span> constructor when a list of intervals is passed with non-default `closed` (`18334`)
  - Bug in `Index.putmask` when an invalid mask passed (`18368`)
  - Bug in masked assignment of a `timedelta64[ns]` dtype `Series`, incorrectly coerced to float (`18493`)

### IO

  - Bug in <span class="title-ref">\~pandas.io.stata.StataReader</span> not converting date/time columns with display formatting addressed (`17990`). Previously columns with display formatting were normally left as ordinal numbers and not converted to datetime objects.
  - Bug in <span class="title-ref">read\_csv</span> when reading a compressed UTF-16 encoded file (`18071`)
  - Bug in <span class="title-ref">read\_csv</span> for handling null values in index columns when specifying `na_filter=False` (`5239`)
  - Bug in <span class="title-ref">read\_csv</span> when reading numeric category fields with high cardinality (`18186`)
  - Bug in <span class="title-ref">DataFrame.to\_csv</span> when the table had `MultiIndex` columns, and a list of strings was passed in for `header` (`5539`)
  - Bug in parsing integer datetime-like columns with specified format in `read_sql` (`17855`).
  - Bug in <span class="title-ref">DataFrame.to\_msgpack</span> when serializing data of the `numpy.bool_` datatype (`18390`)
  - Bug in <span class="title-ref">read\_json</span> not decoding when reading line delimited JSON from S3 (`17200`)
  - Bug in <span class="title-ref">pandas.io.json.json\_normalize</span> to avoid modification of `meta` (`18610`)
  - Bug in <span class="title-ref">to\_latex</span> where repeated MultiIndex values were not printed even though a higher level index differed from the previous row (`14484`)
  - Bug when reading NaN-only categorical columns in <span class="title-ref">HDFStore</span> (`18413`)
  - Bug in <span class="title-ref">DataFrame.to\_latex</span> with `longtable=True` where a latex multicolumn always spanned over three columns (`17959`)

### Plotting

  - Bug in `DataFrame.plot()` and `Series.plot()` with <span class="title-ref">DatetimeIndex</span> where a figure generated by them is not picklable in Python 3 (`18439`)

### GroupBy/resample/rolling

  - Bug in `DataFrame.resample(...).apply(...)` when there is a callable that returns different columns (`15169`)
  - Bug in `DataFrame.resample(...)` when there is a time change (DST) and resampling frequency is 12h or higher (`15549`)
  - Bug in `pd.DataFrameGroupBy.count()` when counting over a datetimelike column (`13393`)
  - Bug in `rolling.var` where calculation is inaccurate with a zero-valued array (`18430`)

### Reshaping

  - Error message in `pd.merge_asof()` for key datatype mismatch now includes datatype of left and right key (`18068`)
  - Bug in `pd.concat` when empty and non-empty DataFrames or Series are concatenated (`18178` `18187`)
  - Bug in `DataFrame.filter(...)` when <span class="title-ref">unicode</span> is passed as a condition in Python 2 (`13101`)
  - Bug when merging empty DataFrames when `np.seterr(divide='raise')` is set (`17776`)

### Numeric

  - Bug in `pd.Series.rolling.skew()` and `rolling.kurt()` with all equal values has floating issue (`18044`)

### Categorical

  - Bug in <span class="title-ref">DataFrame.astype</span> where casting to 'category' on an empty `DataFrame` causes a segmentation fault (`18004`)
  - Error messages in the testing module have been improved when items have different `CategoricalDtype` (`18069`)
  - `CategoricalIndex` can now correctly take a `pd.api.types.CategoricalDtype` as its dtype (`18116`)
  - Bug in `Categorical.unique()` returning read-only `codes` array when all categories were `NaN` (`18051`)
  - Bug in `DataFrame.groupby(axis=1)` with a `CategoricalIndex` (`18432`)

### String

  - <span class="title-ref">Series.str.split</span> will now propagate `NaN` values across all expanded columns instead of `None` (`18450`)

## Contributors

<div class="contributors">

v0.21.0..v0.21.1

</div>

---

v0.22.0.md

---

# Version 0.22.0 (December 29, 2017)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a major release from 0.21.1 and includes a single, API-breaking change. We recommend that all users upgrade to this version after carefully reading the release note (singular\!).

## Backwards incompatible API changes

pandas 0.22.0 changes the handling of empty and all-*NA* sums and products. The summary is that

  - The sum of an empty or all-*NA* `Series` is now `0`
  - The product of an empty or all-*NA* `Series` is now `1`
  - We've added a `min_count` parameter to `.sum()` and `.prod()` controlling the minimum number of valid values for the result to be valid. If fewer than `min_count` non-*NA* values are present, the result is *NA*. The default is `0`. To return `NaN`, the 0.21 behavior, use `min_count=1`.

Some background: In pandas 0.21, we fixed a long-standing inconsistency in the return value of all-*NA* series depending on whether or not bottleneck was installed. See \[whatsnew\_0210.api\_breaking.bottleneck\](\#whatsnew\_0210.api\_breaking.bottleneck). At the same time, we changed the sum and prod of an empty `Series` to also be `NaN`.

Based on feedback, we've partially reverted those changes.

### Arithmetic operations

The default sum for empty or all-*NA* `Series` is now `0`.

*pandas 0.21.x*

`` `ipython    In [1]: pd.Series([]).sum()    Out[1]: nan     In [2]: pd.Series([np.nan]).sum()    Out[2]: nan  *pandas 0.22.0*  .. ipython:: python    :okwarning:     pd.Series([]).sum()    pd.Series([np.nan]).sum()  The default behavior is the same as pandas 0.20.3 with bottleneck installed. It ``<span class="title-ref"> also matches the behavior of NumPy's </span><span class="title-ref">np.nansum</span>\` on empty and all-*NA* arrays.

To have the sum of an empty series return `NaN` (the default behavior of pandas 0.20.3 without bottleneck, or pandas 0.21.x), use the `min_count` keyword.

<div class="ipython" data-okwarning="">

python

pd.Series(\[\]).sum(min\_count=1)

</div>

Thanks to the `skipna` parameter, the `.sum` on an all-*NA* series is conceptually the same as the `.sum` of an empty one with `skipna=True` (the default).

<div class="ipython">

python

pd.Series(\[np.nan\]).sum(min\_count=1) \# skipna=True by default

</div>

The `min_count` parameter refers to the minimum number of *non-null* values required for a non-NA sum or product.

<span class="title-ref">Series.prod</span> has been updated to behave the same as <span class="title-ref">Series.sum</span>, returning `1` instead.

<div class="ipython" data-okwarning="">

python

pd.Series(\[\]).prod() pd.Series(\[np.nan\]).prod() pd.Series(\[\]).prod(min\_count=1)

</div>

These changes affect <span class="title-ref">DataFrame.sum</span> and <span class="title-ref">DataFrame.prod</span> as well. Finally, a few less obvious places in pandas are affected by this change.

### Grouping by a Categorical

Grouping by a `Categorical` and summing now returns `0` instead of `NaN` for categories with no observations. The product now returns `1` instead of `NaN`.

*pandas 0.21.x*

`` `ipython    In [8]: grouper = pd.Categorical(['a', 'a'], categories=['a', 'b'])     In [9]: pd.Series([1, 2]).groupby(grouper, observed=False).sum()    Out[9]:    a    3.0    b    NaN    dtype: float64  *pandas 0.22*  .. ipython:: python     grouper = pd.Categorical(["a", "a"], categories=["a", "b"])    pd.Series([1, 2]).groupby(grouper, observed=False).sum()  To restore the 0.21 behavior of returning ``NaN`for unobserved groups,`<span class="title-ref"> use </span><span class="title-ref">min\_count\>=1</span>\`.

<div class="ipython">

python

pd.Series(\[1, 2\]).groupby(grouper, observed=False).sum(min\_count=1)

</div>

### Resample

The sum and product of all-*NA* bins has changed from `NaN` to `0` for sum and `1` for product.

*pandas 0.21.x*

`` `ipython    In [11]: s = pd.Series([1, 1, np.nan, np.nan],       ....:               index=pd.date_range('2017', periods=4))       ....: s    Out[11]:    2017-01-01    1.0    2017-01-02    1.0    2017-01-03    NaN    2017-01-04    NaN    Freq: D, dtype: float64     In [12]: s.resample('2d').sum()    Out[12]:    2017-01-01    2.0    2017-01-03    NaN    Freq: 2D, dtype: float64  *pandas 0.22.0*  .. code-block:: ipython     In [11]: s = pd.Series([1, 1, np.nan, np.nan],       ....:               index=pd.date_range("2017", periods=4))     In [12]: s.resample("2d").sum()    Out[12]:    2017-01-01    2.0    2017-01-03    0.0    Freq: 2D, Length: 2, dtype: float64  To restore the 0.21 behavior of returning ``NaN`, use`min\_count\>=1`.  .. code-block:: ipython     In [13]: s.resample("2d").sum(min_count=1)    Out[13]:    2017-01-01    2.0    2017-01-03    NaN    Freq: 2D, Length: 2, dtype: float64   In particular, upsampling and taking the sum or product is affected, as`\` upsampling introduces missing values even if the original series was entirely valid.

*pandas 0.21.x*

`` `ipython    In [14]: idx = pd.DatetimeIndex(['2017-01-01', '2017-01-02'])     In [15]: pd.Series([1, 2], index=idx).resample('12H').sum()    Out[15]:    2017-01-01 00:00:00    1.0    2017-01-01 12:00:00    NaN    2017-01-02 00:00:00    2.0    Freq: 12H, dtype: float64  *pandas 0.22.0*  .. code-block:: ipython     In [14]: idx = pd.DatetimeIndex(["2017-01-01", "2017-01-02"])    In [15]: pd.Series([1, 2], index=idx).resample("12H").sum()    Out[15]:    2017-01-01 00:00:00    1    2017-01-01 12:00:00    0    2017-01-02 00:00:00    2    Freq: 12H, Length: 3, dtype: int64  Once again, the ``min\_count`keyword is available to restore the 0.21 behavior.  .. code-block:: ipython     In [16]: pd.Series([1, 2], index=idx).resample("12H").sum(min_count=1)    Out[16]:    2017-01-01 00:00:00    1.0    2017-01-01 12:00:00    NaN    2017-01-02 00:00:00    2.0    Freq: 12H, Length: 3, dtype: float64   Rolling and expanding`\` ^^^^^^^^^^^^^^^^^^^^^

Rolling and expanding already have a `min_periods` keyword that behaves similar to `min_count`. The only case that changes is when doing a rolling or expanding sum with `min_periods=0`. Previously this returned `NaN`, when fewer than `min_periods` non-*NA* values were in the window. Now it returns `0`.

*pandas 0.21.1*

`` `ipython    In [17]: s = pd.Series([np.nan, np.nan])     In [18]: s.rolling(2, min_periods=0).sum()    Out[18]:    0   NaN    1   NaN    dtype: float64  *pandas 0.22.0*  .. ipython:: python     s = pd.Series([np.nan, np.nan])    s.rolling(2, min_periods=0).sum()  The default behavior of ``min\_periods=None`, implying that`min\_periods`  `\` equals the window size, is unchanged.

## Compatibility

If you maintain a library that should work across pandas versions, it may be easiest to exclude pandas 0.21 from your requirements. Otherwise, all your `sum()` calls would need to check if the `Series` is empty before summing.

With setuptools, in your `setup.py` use:

    install_requires=['pandas!=0.21.*', ...]

With conda, use

`` `yaml     requirements:       run:         - pandas !=0.21.0,!=0.21.1  Note that the inconsistency in the return value for all-*NA* series is still ``\` there for pandas 0.20.3 and earlier. Avoiding pandas 0.21 will only help with the empty case.

## Contributors

<div class="contributors">

v0.21.1..v0.22.0

</div>

---

v0.23.0.md

---

# What's new in 0.23.0 (May 15, 2018)

{{ header }}

<div class="ipython" data-suppress="">

python

from pandas import \* \# noqa F401, F403

</div>

This is a major release from 0.22.0 and includes a number of API changes, deprecations, new features, enhancements, and performance improvements along with a large number of bug fixes. We recommend that all users upgrade to this version.

Highlights include:

  - \[Round-trippable JSON format with 'table' orient \<whatsnew\_0230.enhancements.round-trippable\_json\>\](\#round-trippable-json-format-with-'table'-orient-\<whatsnew\_0230.enhancements.round-trippable\_json\>).
  - \[Instantiation from dicts respects order for Python 3.6+ \<whatsnew\_0230.api\_breaking.dict\_insertion\_order\>\](\#instantiation-from-dicts-respects-order-for-python-3.6+-\<whatsnew\_0230.api\_breaking.dict\_insertion\_order\>).
  - \[Dependent column arguments for assign \<whatsnew\_0230.enhancements.assign\_dependent\>\](\#dependent-column-arguments-for-assign-\<whatsnew\_0230.enhancements.assign\_dependent\>).
  - \[Merging / sorting on a combination of columns and index levels \<whatsnew\_0230.enhancements.merge\_on\_columns\_and\_levels\>\](\#merging-/-sorting-on-a-combination-of-columns-and-index-levels-\<whatsnew\_0230.enhancements.merge\_on\_columns\_and\_levels\>).
  - \[Extending pandas with custom types \<whatsnew\_023.enhancements.extension\>\](\#extending-pandas-with-custom-types-\<whatsnew\_023.enhancements.extension\>).
  - \[Excluding unobserved categories from groupby \<whatsnew\_0230.enhancements.categorical\_grouping\>\](\#excluding-unobserved-categories-from-groupby-\<whatsnew\_0230.enhancements.categorical\_grouping\>).
  - \[Changes to make output shape of DataFrame.apply consistent \<whatsnew\_0230.api\_breaking.apply\>\](\#changes-to-make-output-shape-of-dataframe.apply-consistent-\<whatsnew\_0230.api\_breaking.apply\>).

Check the \[API Changes \<whatsnew\_0230.api\_breaking\>\](\#api-changes-\<whatsnew\_0230.api\_breaking\>) and \[deprecations \<whatsnew\_0230.deprecations\>\](\#deprecations-\<whatsnew\_0230.deprecations\>) before updating.

\> **Warning** \> Starting January 1, 2019, pandas feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more.

<div class="contents" data-local="" data-backlinks="none" data-depth="2">

What's new in v0.23.0

</div>

## New features

### JSON read/write round-trippable with `orient='table'`

A `DataFrame` can now be written to and subsequently read back via JSON while preserving metadata through usage of the `orient='table'` argument (see `18912` and `9146`). Previously, none of the available `orient` values guaranteed the preservation of dtypes and index names, amongst other metadata.

`` `ipython    In [1]: df = pd.DataFrame({'foo': [1, 2, 3, 4],       ...:                    'bar': ['a', 'b', 'c', 'd'],       ...:                    'baz': pd.date_range('2018-01-01', freq='d', periods=4),       ...:                    'qux': pd.Categorical(['a', 'b', 'c', 'c'])},       ...:                   index=pd.Index(range(4), name='idx'))     In [2]: df    Out[2]:         foo bar        baz qux    idx    0      1   a 2018-01-01   a    1      2   b 2018-01-02   b    2      3   c 2018-01-03   c    3      4   d 2018-01-04   c     [4 rows x 4 columns]     In [3]: df.dtypes    Out[3]:    foo             int64    bar            object    baz    datetime64[ns]    qux          category    Length: 4, dtype: object     In [4]: df.to_json('test.json', orient='table')     In [5]: new_df = pd.read_json('test.json', orient='table')     In [6]: new_df    Out[6]:         foo bar        baz qux    idx    0      1   a 2018-01-01   a    1      2   b 2018-01-02   b    2      3   c 2018-01-03   c    3      4   d 2018-01-04   c     [4 rows x 4 columns]     In [7]: new_df.dtypes    Out[7]:    foo             int64    bar            object    baz    datetime64[ns]    qux          category    Length: 4, dtype: object  Please note that the string ``index`is not supported with the round trip format, as it is used by default in`write\_json`to indicate a missing index name.  .. ipython:: python    :okwarning:     df.index.name = 'index'     df.to_json('test.json', orient='table')    new_df = pd.read_json('test.json', orient='table')    new_df    new_df.dtypes  .. ipython:: python    :suppress:     import os    os.remove('test.json')   .. _whatsnew_0230.enhancements.assign_dependent:   Method`.assign()`accepts dependent arguments`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The <span class="title-ref">DataFrame.assign</span> now accepts dependent keyword arguments for python version later than 3.6 (see also [PEP 468](https://www.python.org/dev/peps/pep-0468/)). Later keyword arguments may now refer to earlier ones if the argument is a callable. See the \[documentation here \<dsintro.chained\_assignment\>\](\#documentation-here-\<dsintro.chained\_assignment\>) (`14207`)

<div class="ipython">

python

df = pd.DataFrame({'A': \[1, 2, 3\]}) df df.assign(B=df.A, C=lambda x: x\['A'\] + x\['B'\])

</div>

\> **Warning** \> This may subtly change the behavior of your code when you're using `.assign()` to update an existing column. Previously, callables referring to other variables being updated would get the "old" values

> Previous behavior:
> 
>   - \`\`\`ipython  
>     In \[2\]: df = pd.DataFrame({"A": \[1, 2, 3\]})
>     
>     In \[3\]: df.assign(A=lambda df: df.A + 1, C=lambda df: df.A \* -1) Out\[3\]: A C 0 2 -1 1 3 -2 2 4 -3
> 
> New behavior:
> 
> <div class="ipython">
> 
> python
> 
> df.assign(A=df.A + 1, C=lambda df: df.A \* -1)
> 
> </div>

<div id="whatsnew_0230.enhancements.merge_on_columns_and_levels">

Merging on a combination of columns and index levels `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Strings passed to `DataFrame.merge` as the ``on`,`left\_on`, and`right\_on`parameters may now refer to either column names or index level names. This enables merging`DataFrame``instances on a combination of index levels and columns without resetting indexes. See the [Merge on columns and levels <merging.merge_on_columns_and_levels>](#merge-on-columns-and levels-<merging.merge_on_columns_and_levels>) documentation section. (:issue:`14355`)  .. ipython:: python     left_index = pd.Index(['K0', 'K0', 'K1', 'K2'], name='key1')     left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],                         'B': ['B0', 'B1', 'B2', 'B3'],                         'key2': ['K0', 'K1', 'K0', 'K1']},                        index=left_index)     right_index = pd.Index(['K0', 'K1', 'K2', 'K2'], name='key1')     right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],                          'D': ['D0', 'D1', 'D2', 'D3'],                          'key2': ['K0', 'K0', 'K0', 'K1']},                         index=right_index)     left.merge(right, on=['key1', 'key2'])  .. _whatsnew_0230.enhancements.sort_by_columns_and_levels:  Sorting by a combination of columns and index levels ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Strings passed to `DataFrame.sort_values` as the``by`parameter may now refer to either column names or index level names.  This enables sorting`DataFrame``instances by a combination of index levels and columns without resetting indexes. See the [Sorting by Indexes and Values <basics.sort_indexes_and_values>](#sorting-by-indexes-and-values <basics.sort_indexes_and_values>) documentation section. (:issue:`14353`)  .. ipython:: python     # Build MultiIndex    idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 2),                                     ('b', 2), ('b', 1), ('b', 1)])    idx.names = ['first', 'second']     # Build DataFrame    df_multi = pd.DataFrame({'A': np.arange(6, 0, -1)},                            index=idx)    df_multi     # Sort by 'second' (index) and 'A' (column)    df_multi.sort_values(by=['second', 'A'])   .. _whatsnew_023.enhancements.extension:  Extending pandas with custom types (experimental) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  pandas now supports storing array-like objects that aren't necessarily 1-D NumPy arrays as columns in a DataFrame or values in a Series. This allows third-party libraries to implement extensions to NumPy's types, similar to how pandas implemented categoricals, datetimes with timezones, periods, and intervals.  As a demonstration, we'll use cyberpandas_, which provides an``IPArray`type for storing ip addresses.`\`ipython In \[1\]: from cyberpandas import IPArray

</div>

>   - In \[2\]: values = IPArray(\[  
>     ...: 0, ...: 3232235777, ...: 42540766452641154071740215577757643572 ...: \]) ...: ...:

`IPArray` isn't a normal 1-D NumPy array, but because it's a pandas `` ` `~pandas.api.extensions.ExtensionArray`, it can be stored properly inside pandas' containers. ``\`ipython In \[3\]: ser = pd.Series(values)

> In \[4\]: ser Out\[4\]: 0 0.0.0.0 1 192.168.1.1 2 2001:db8:85a3::8a2e:370:7334 dtype: ip

Notice that the dtype is `ip`. The missing value semantics of the underlying `` ` array are respected: ``\`ipython In \[5\]: ser.isna() Out\[5\]: 0 True 1 False 2 False dtype: bool

For more, see the \[extension types \<extending.extension-types\>\](\#extension-types-\<extending.extension-types\>) `` ` documentation. If you build an extension array, publicize it on `the ecosystem page <https://pandas.pydata.org/community/ecosystem.html>`_.     .. _whatsnew_0230.enhancements.categorical_grouping:  New ``observed`keyword for excluding unobserved categories in`GroupBy`^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Grouping by a categorical includes the unobserved categories in the output. When grouping by multiple categorical columns, this means you get the cartesian product of all the categories, including combinations where there are no observations, which can result in a large number of groups. We have added a keyword`observed`to control this behavior, it defaults to`observed=False``for backward-compatibility. (:issue:`14942`, :issue:`8138`, :issue:`15217`, :issue:`17594`, :issue:`8669`, :issue:`20583`, :issue:`20902`)  .. ipython:: python     cat1 = pd.Categorical(["a", "a", "b", "b"],                          categories=["a", "b", "z"], ordered=True)    cat2 = pd.Categorical(["c", "d", "c", "d"],                          categories=["c", "d", "y"], ordered=True)    df = pd.DataFrame({"A": cat1, "B": cat2, "values": [1, 2, 3, 4]})    df['C'] = ['foo', 'bar'] * 2    df  To show all values, the previous behavior:  .. ipython:: python     df.groupby(['A', 'B', 'C'], observed=False).count()   To show only observed values:  .. ipython:: python     df.groupby(['A', 'B', 'C'], observed=True).count()  For pivoting operations, this behavior is *already* controlled by the``dropna`keyword:  .. ipython:: python     cat1 = pd.Categorical(["a", "a", "b", "b"],                          categories=["a", "b", "z"], ordered=True)    cat2 = pd.Categorical(["c", "d", "c", "d"],                          categories=["c", "d", "y"], ordered=True)    df = pd.DataFrame({"A": cat1, "B": cat2, "values": [1, 2, 3, 4]})    df`\`ipython In \[1\]: pd.pivot\_table(df, values='values', index=\['A', 'B'\], dropna=True)

>   - Out\[1\]:  
>     values
> 
> A B a c 1.0 d 2.0 b c 3.0 d 4.0
> 
> In \[2\]: pd.pivot\_table(df, values='values', index=\['A', 'B'\], dropna=False)
> 
>   - Out\[2\]:  
>     values
> 
> A B a c 1.0 d 2.0 y NaN b c 3.0 d 4.0 y NaN z c NaN d NaN y NaN

<div id="whatsnew_0230.enhancements.window_raw">

Rolling/Expanding.apply() accepts `raw=False` to pass a `Series` to the function `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `Series.rolling().apply() <.Rolling.apply>`, `DataFrame.rolling().apply() <.Rolling.apply>`, `Series.expanding().apply() <.Expanding.apply>`, and `DataFrame.expanding().apply() <.Expanding.apply>` have gained a ``raw=None``parameter. This is similar to `DataFame.apply`. This parameter, if``True`allows one to send a`np.ndarray`to the applied function. If`False`a`Series`will be passed. The default is`None`, which preserves backward compatibility, so this will default to`True`, sending an`np.ndarray`. In a future version the default will be changed to`False`, sending a`Series``. (:issue:`5071`, :issue:`20584`)  .. ipython:: python     s = pd.Series(np.arange(5), np.arange(5) + 1)    s  Pass a``Series`:  .. ipython:: python     s.rolling(2, min_periods=1).apply(lambda x: x.iloc[-1], raw=False)  Mimic the original behavior of passing a ndarray:  .. ipython:: python     s.rolling(2, min_periods=1).apply(lambda x: x[-1], raw=True)   .. _whatsnew_0210.enhancements.limit_area:`DataFrame.interpolate`has gained the`limit\_area``kwarg ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `DataFrame.interpolate` has gained a``limit\_area`parameter to allow further control of which`NaN`s are replaced. Use`limit\_area='inside'`to fill only NaNs surrounded by valid values or use`limit\_area='outside'`to fill only`NaN``s outside the existing valid values while preserving those inside.  (:issue:`16284`) See the [full documentation here <missing_data.interp_limits>](#full-documentation-here-<missing_data.interp_limits>).   .. ipython:: python     ser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan,                     np.nan, 13, np.nan, np.nan])    ser  Fill one consecutive inside value in both directions  .. ipython:: python     ser.interpolate(limit_direction='both', limit_area='inside', limit=1)  Fill all consecutive outside values backward  .. ipython:: python     ser.interpolate(limit_direction='backward', limit_area='outside')  Fill all consecutive outside values in both directions  .. ipython:: python     ser.interpolate(limit_direction='both', limit_area='outside')  .. _whatsnew_0210.enhancements.get_dummies_dtype:  Function``get\_dummies`now supports`dtype``argument ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The `get_dummies` now accepts a``dtype``argument, which specifies a dtype for the new columns. The default remains uint8. (:issue:`18330`)  .. ipython:: python     df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})    pd.get_dummies(df, columns=['c']).dtypes    pd.get_dummies(df, columns=['c'], dtype=bool).dtypes   .. _whatsnew_0230.enhancements.timedelta_mod:  Timedelta mod method ^^^^^^^^^^^^^^^^^^^^``mod`(%) and`divmod`operations are now defined on`Timedelta``objects when operating with either timedelta-like or with numeric arguments. See the [documentation here <timedeltas.mod_divmod>](#documentation-here-<timedeltas.mod_divmod>). (:issue:`19365`)  .. ipython:: python      td = pd.Timedelta(hours=37)     td % pd.Timedelta(minutes=45)  .. _whatsnew_0230.enhancements.ran_inf:  Method``.rank()`handles`inf`values when`NaN`are present ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In previous versions,`.rank()`would assign`inf`elements`NaN``as their ranks. Now ranks are calculated properly. (:issue:`6945`)  .. ipython:: python      s = pd.Series([-np.inf, 0, 1, np.nan, np.inf])     s  Previous behavior:``\`ipython In \[11\]: s.rank() Out\[11\]: 0 1.0 1 2.0 2 3.0 3 NaN 4 NaN dtype: float64

</div>

Current behavior:

<div class="ipython">

python

s.rank()

</div>

Furthermore, previously if you rank `inf` or `-inf` values together with `NaN` values, the calculation won't distinguish `NaN` from infinity when using 'top' or 'bottom' argument.

<div class="ipython">

python

s = pd.Series(\[np.nan, np.nan, -np.inf, -np.inf\]) s

</div>

Previous behavior:

``` ipython
In [15]: s.rank(na_option='top')
Out[15]:
0    2.5
1    2.5
2    2.5
3    2.5
dtype: float64
```

Current behavior:

<div class="ipython">

python

s.rank(na\_option='top')

</div>

These bugs were squashed:

\- Bug in <span class="title-ref">DataFrame.rank</span> and <span class="title-ref">Series.rank</span> when `method='dense'` and `pct=True` in which percentile ranks were not being used with the number of distinct observations (`15630`) `` ` - Bug in `Series.rank` and `DataFrame.rank` when ``ascending='False'`failed to return correct ranks for infinity if`NaN``were present (:issue:`19538`) - Bug in `DataFrameGroupBy.rank` where ranks were incorrect when both infinity and``NaN``were present (:issue:`20561`)   .. _whatsnew_0230.enhancements.str_cat_align:``Series.str.cat`has gained the`join``kwarg ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Previously, `Series.str.cat` did not -- in contrast to most of``pandas``-- align `Series` on their index before concatenation (see :issue:`18657`). The method has now gained a keyword``join`to control the manner of alignment, see examples below and [here <text.concatenate>](#here-<text.concatenate>).  In v.0.23`join`will default to None (meaning no alignment), but this default will change to`'left'``in a future version of pandas.  .. ipython:: python    :okwarning:      s = pd.Series(['a', 'b', 'c', 'd'])     t = pd.Series(['b', 'd', 'e', 'c'], index=[1, 3, 4, 2])     s.str.cat(t)     s.str.cat(t, join='left', na_rep='-')  Furthermore, `Series.str.cat` now works for``CategoricalIndex`as well (previously raised a`ValueError``; see :issue:`20842`).  .. _whatsnew_0230.enhancements.astype_category:``DataFrame.astype`performs column-wise conversion to`Categorical``^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `DataFrame.astype` can now perform column-wise conversion to``Categorical`by supplying the string`'category'``or a `~pandas.api.types.CategoricalDtype`. Previously, attempting this would raise a``NotImplementedError``. See the [categorical.objectcreation](#categorical.objectcreation) section of the documentation for more details and examples. (:issue:`12860`, :issue:`18099`)  Supplying the string``'category'`performs column-wise conversion, with only labels appearing in a given column set as categories:  .. ipython:: python      df = pd.DataFrame({'A': list('abca'), 'B': list('bccd')})     df = df.astype('category')     df['A'].dtype     df['B'].dtype   Supplying a`CategoricalDtype`will make the categories in each column consistent with the supplied dtype:  .. ipython:: python      from pandas.api.types import CategoricalDtype     df = pd.DataFrame({'A': list('abca'), 'B': list('bccd')})     cdt = CategoricalDtype(categories=list('abcd'), ordered=True)     df = df.astype(cdt)     df['A'].dtype     df['B'].dtype   .. _whatsnew_0230.enhancements.other:  Other enhancements ^^^^^^^^^^^^^^^^^^  - Unary`+`now permitted for`Series`and`DataFrame``as  numeric operator (:issue:`16073`) - Better support for `~pandas.io.formats.style.Styler.to_excel` output with the``xlsxwriter``engine. (:issue:`16149`) - `pandas.tseries.frequencies.to_offset` now accepts leading '+' signs e.g. '+1h'. (:issue:`18171`) - `MultiIndex.unique` now supports the``level=``argument, to get unique values from a specific index level (:issue:`17896`) - `pandas.io.formats.style.Styler` now has method``hide\_index()``to determine whether the index will be rendered in output (:issue:`14194`) - `pandas.io.formats.style.Styler` now has method``hide\_columns()``to determine whether columns will be hidden in output (:issue:`14194`) - Improved wording of``ValueError``raised in `to_datetime` when``unit=``is passed with a non-convertible value (:issue:`14350`) - `Series.fillna` now accepts a Series or a dict as a``value``for a categorical dtype (:issue:`17033`) - `pandas.read_clipboard` updated to use qtpy, falling back to PyQt5 and then PyQt4, adding compatibility with Python3 and multiple python-qt bindings (:issue:`17722`) - Improved wording of``ValueError``raised in `read_csv` when the``usecols``argument cannot match all columns. (:issue:`17301`) - `DataFrame.corrwith` now silently drops non-numeric columns when passed a Series. Before, an exception was raised (:issue:`18570`). - `IntervalIndex` now supports time zone aware``Interval``objects (:issue:`18537`, :issue:`18538`) - `Series` / `DataFrame` tab completion also returns identifiers in the first level of a `MultiIndex`. (:issue:`16326`) - `read_excel` has gained the``nrows``parameter (:issue:`16645`) - `DataFrame.append` can now in more cases preserve the type of the calling dataframe's columns (e.g. if both are``CategoricalIndex``) (:issue:`18359`) - `DataFrame.to_json` and `Series.to_json` now accept an``index``argument which allows the user to exclude the index from the JSON output (:issue:`17394`) -``IntervalIndex.to\_tuples()`has gained the`na\_tuple``parameter to control whether NA is returned as a tuple of NA, or NA itself (:issue:`18756`) -``Categorical.rename\_categories`,`CategoricalIndex.rename\_categories``and `Series.cat.rename_categories`   can now take a callable as their argument (:issue:`18862`) - `Interval` and `IntervalIndex` have gained a``length``attribute (:issue:`18789`) -``Resampler``objects now have a functioning `.Resampler.pipe` method.   Previously, calls to``pipe`were diverted to  the`mean``method (:issue:`17905`). - `~pandas.api.types.is_scalar` now returns``True`for`DateOffset``objects (:issue:`18943`). - `DataFrame.pivot` now accepts a list for the``values=``kwarg (:issue:`17160`). - Added `pandas.api.extensions.register_dataframe_accessor`,   `pandas.api.extensions.register_series_accessor`, and   `pandas.api.extensions.register_index_accessor`, accessor for libraries downstream of pandas   to register custom accessors like``.cat``on pandas objects. See   [Registering Custom Accessors <extending.register-accessors>](#registering-custom-accessors-<extending.register-accessors>) for more (:issue:`14781`).  -``IntervalIndex.astype`now supports conversions between subtypes when passed an`IntervalDtype``(:issue:`19197`) - `IntervalIndex` and its associated constructor methods (``from\_arrays`,`from\_breaks`,`from\_tuples`) have gained a`dtype``parameter (:issue:`19262`) - Added `.SeriesGroupBy.is_monotonic_increasing` and `.SeriesGroupBy.is_monotonic_decreasing` (:issue:`17015`) - For subclassed``DataFrames``, `DataFrame.apply` will now preserve the``Series``subclass (if defined) when passing the data to the applied function (:issue:`19822`) - `DataFrame.from_dict` now accepts a``columns`argument that can be used to specify the column names when`orient='index'``is used (:issue:`18529`) - Added option``display.html.use\_mathjax``so `MathJax <https://www.mathjax.org/>`_ can be disabled when rendering tables in``Jupyter``notebooks (:issue:`19856`, :issue:`19824`) - `DataFrame.replace` now supports the``method`parameter, which can be used to specify the replacement method when`to\_replace`is a scalar, list or tuple and`value`is`None``(:issue:`19632`) - `Timestamp.month_name`, `DatetimeIndex.month_name`, and `Series.dt.month_name` are now available (:issue:`12805`) - `Timestamp.day_name` and `DatetimeIndex.day_name` are now available to return day names with a specified locale (:issue:`12806`) - `DataFrame.to_sql` now performs a multi-value insert if the underlying connection supports itk rather than inserting row by row.``SQLAlchemy`dialects supporting multi-value inserts include:`mysql`,`postgresql`,`sqlite`and any dialect with`supports\_multivalues\_insert``. (:issue:`14315`, :issue:`8953`) - `read_html` now accepts a``displayed\_only`keyword argument to controls whether or not hidden elements are parsed (`True``by default) (:issue:`20027`) - `read_html` now reads all``\<tbody\>`elements in a`\<table\>``, not just the first. (:issue:`20690`) - `.Rolling.quantile` and `.Expanding.quantile` now accept the``interpolation`keyword,`linear``by default (:issue:`20497`) - zip compression is supported via``compression=zip``in `DataFrame.to_pickle`, `Series.to_pickle`, `DataFrame.to_csv`, `Series.to_csv`, `DataFrame.to_json`, `Series.to_json`. (:issue:`17778`) - `~pandas.tseries.offsets.WeekOfMonth` constructor now supports``n=0``(:issue:`20517`). - `DataFrame` and `Series` now support matrix multiplication (``@``) operator (:issue:`10259`) for Python>=3.5 - Updated `DataFrame.to_gbq` and `pandas.read_gbq` signature and documentation to reflect changes from   the pandas-gbq library version 0.4.0. Adds intersphinx mapping to pandas-gbq   library. (:issue:`20564`) - Added new writer for exporting Stata dta files in version 117,``StataWriter117``.  This format supports exporting strings with lengths up to 2,000,000 characters (:issue:`16450`) - `to_hdf` and `read_hdf` now accept an``errors``keyword argument to control encoding error handling (:issue:`20835`) - `cut` has gained the``duplicates='raise'|'drop'``option to control whether to raise on duplicated edges (:issue:`20947`) - `date_range`, `timedelta_range`, and `interval_range` now return a linearly spaced index if``start`,`stop`, and`periods`are specified, but`freq``is not. (:issue:`20808`, :issue:`20983`, :issue:`20976`)  .. _whatsnew_0230.api_breaking:  Backwards incompatible API changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. _whatsnew_0230.api_breaking.deps:  Dependencies have increased minimum versions ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  We have updated our minimum supported versions of dependencies (:issue:`15184`). If installed, we now require:  +-----------------+-----------------+----------+---------------+ | Package         | Minimum Version | Required |     Issue     | +=================+=================+==========+===============+ | python-dateutil | 2.5.0           |    X     | :issue:`15184`| +-----------------+-----------------+----------+---------------+ | openpyxl        | 2.4.0           |          | :issue:`15184`| +-----------------+-----------------+----------+---------------+ | beautifulsoup4  | 4.2.1           |          | :issue:`20082`| +-----------------+-----------------+----------+---------------+ | setuptools      | 24.2.0          |          | :issue:`20698`| +-----------------+-----------------+----------+---------------+  .. _whatsnew_0230.api_breaking.dict_insertion_order:  Instantiation from dicts preserves dict insertion order for Python 3.6+ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Until Python 3.6, dicts in Python had no formally defined ordering. For Python version 3.6 and later, dicts are ordered by insertion order, see `PEP 468 <https://www.python.org/dev/peps/pep-0468/>`_. pandas will use the dict's insertion order, when creating a``Series`or`DataFrame``from a dict and you're using Python version 3.6 or higher. (:issue:`19884`)  Previous behavior (and current behavior if on Python < 3.6):``\`ipython In \[16\]: pd.Series({'Income': 2000, ....: 'Expenses': -1500, ....: 'Taxes': -200, ....: 'Net result': 300}) Out\[16\]: Expenses -1500 Income 2000 Net result 300 Taxes -200 dtype: int64

Note the Series above is ordered alphabetically by the index values.

New behavior (for Python \>= 3.6):

<div class="ipython">

python

  - pd.Series({'Income': 2000,  
    'Expenses': -1500, 'Taxes': -200, 'Net result': 300})

</div>

Notice that the Series is now ordered by insertion order. This new behavior is `` ` used for all relevant pandas types ( ``Series`,`DataFrame`,`SparseSeries`and`SparseDataFrame`).  If you wish to retain the old behavior while using Python >= 3.6, you can use`.sort\_index()`:  .. ipython:: python      pd.Series({'Income': 2000,                'Expenses': -1500,                'Taxes': -200,                'Net result': 300}).sort_index()  .. _whatsnew_0230.api_breaking.deprecate_panel:  Deprecate Panel ^^^^^^^^^^^^^^^`Panel`was deprecated in the 0.20.x release, showing as a`DeprecationWarning`. Using`Panel`will now show a`FutureWarning`. The recommended way to represent 3-D data are with a`MultiIndex`on a`DataFrame``via the `~Panel.to_frame` or with the `xarray package <http://xarray.pydata.org/en/stable/>`__. pandas provides a `~Panel.to_xarray` method to automate this conversion (:issue:`13563`, :issue:`18324`).``\`ipython In \[75\]: import pandas.\_testing as tm

> In \[76\]: p = tm.makePanel()
> 
> In \[77\]: p Out\[77\]: \<class 'pandas.core.panel.Panel'\> Dimensions: 3 (items) x 3 (major\_axis) x 4 (minor\_axis) Items axis: ItemA to ItemC Major\_axis axis: 2000-01-03 00:00:00 to 2000-01-05 00:00:00 Minor\_axis axis: A to D

Convert to a MultiIndex DataFrame

``` ipython
In [78]: p.to_frame()
Out[78]:
                     ItemA     ItemB     ItemC
major      minor
2000-01-03 A      0.469112  0.721555  0.404705
           B     -1.135632  0.271860 -1.039268
           C      0.119209  0.276232 -1.344312
           D     -2.104569  0.113648 -0.109050
2000-01-04 A     -0.282863 -0.706771  0.577046
           B      1.212112 -0.424972 -0.370647
           C     -1.044236 -1.087401  0.844885
           D     -0.494929 -1.478427  1.643563
2000-01-05 A     -1.509059 -1.039575 -1.715002
           B     -0.173215  0.567020 -1.157892
           C     -0.861849 -0.673690  1.075770
           D      1.071804  0.524988 -1.469388

[12 rows x 3 columns]
```

Convert to an xarray DataArray

``` ipython
In [79]: p.to_xarray()
Out[79]:
<xarray.DataArray (items: 3, major_axis: 3, minor_axis: 4)>
array([[[ 0.469112, -1.135632,  0.119209, -2.104569],
        [-0.282863,  1.212112, -1.044236, -0.494929],
        [-1.509059, -0.173215, -0.861849,  1.071804]],

       [[ 0.721555,  0.27186 ,  0.276232,  0.113648],
        [-0.706771, -0.424972, -1.087401, -1.478427],
        [-1.039575,  0.56702 , -0.67369 ,  0.524988]],

       [[ 0.404705, -1.039268, -1.344312, -0.10905 ],
        [ 0.577046, -0.370647,  0.844885,  1.643563],
        [-1.715002, -1.157892,  1.07577 , -1.469388]]])
Coordinates:
  * items       (items) object 'ItemA' 'ItemB' 'ItemC'
  * major_axis  (major_axis) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05
  * minor_axis  (minor_axis) object 'A' 'B' 'C' 'D'
```

<div id="whatsnew_0230.api_breaking.core_common">

pandas.core.common removals `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^  The following error & warning messages are removed from ``pandas.core.common``(:issue:`13634`, :issue:`19769`):  -``PerformanceWarning`-`UnsupportedFunctionCall`-`UnsortedIndexError`-`AbstractMethodError`These are available from import from`pandas.errors`(since 0.19.0).   .. _whatsnew_0230.api_breaking.apply:  Changes to make output of`DataFrame.apply``consistent ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  `DataFrame.apply` was inconsistent when applying an arbitrary user-defined-function that returned a list-like with``axis=1`. Several bugs and inconsistencies are resolved. If the applied function returns a Series, then pandas will return a DataFrame; otherwise a Series will be returned, this includes the case where a list-like (e.g.`tuple`or`list``is returned) (:issue:`16353`, :issue:`17437`, :issue:`17970`, :issue:`17348`, :issue:`17892`, :issue:`18573`, :issue:`17602`, :issue:`18775`, :issue:`18901`, :issue:`18919`).  .. ipython:: python      df = pd.DataFrame(np.tile(np.arange(3), 6).reshape(6, -1) + 1,                       columns=['A', 'B', 'C'])     df  Previous behavior: if the returned shape happened to match the length of original columns, this would return a``DataFrame`. If the return shape did not match, a`Series`with lists was returned.`\`python In \[3\]: df.apply(lambda x: \[1, 2, 3\], axis=1) Out\[3\]: A B C 0 1 2 3 1 1 2 3 2 1 2 3 3 1 2 3 4 1 2 3 5 1 2 3

</div>

> In \[4\]: df.apply(lambda x: \[1, 2\], axis=1) Out\[4\]: 0 \[1, 2\] 1 \[1, 2\] 2 \[1, 2\] 3 \[1, 2\] 4 \[1, 2\] 5 \[1, 2\] dtype: object

New behavior: When the applied function returns a list-like, this will now *always* return a `Series`.

<div class="ipython">

python

df.apply(lambda x: \[1, 2, 3\], axis=1) df.apply(lambda x: \[1, 2\], axis=1)

</div>

To have expanded columns, you can use `result_type='expand'`

<div class="ipython">

python

df.apply(lambda x: \[1, 2, 3\], axis=1, result\_type='expand')

</div>

To broadcast the result across the original columns (the old behaviour for `` ` list-likes of the correct length), you can use ``result\_type='broadcast'`. The shape must match the original columns.  .. ipython:: python      df.apply(lambda x: [1, 2, 3], axis=1, result_type='broadcast')  Returning a`Series``allows one to control the exact return structure and column names:  .. ipython:: python      df.apply(lambda x: pd.Series([1, 2, 3], index=['D', 'E', 'F']), axis=1)  .. _whatsnew_0230.api_breaking.concat:  Concatenation will no longer sort ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  In a future version of pandas `pandas.concat` will no longer sort the non-concatenation axis when it is not already aligned. The current behavior is the same as the previous (sorting), but now a warning is issued when``sort``is not specified and the non-concatenation axis is not aligned (:issue:`4588`).  .. ipython:: python    :okwarning:     df1 = pd.DataFrame({"a": [1, 2], "b": [1, 2]}, columns=['b', 'a'])    df2 = pd.DataFrame({"a": [4, 5]})     pd.concat([df1, df2])  To keep the previous behavior (sorting) and silence the warning, pass``sort=True`.. ipython:: python     pd.concat([df1, df2], sort=True)  To accept the future behavior (no sorting), pass`sort=False``.. ipython     pd.concat([df1, df2], sort=False)  Note that this change also applies to `DataFrame.append`, which has also received a``sort`keyword for controlling this behavior.   .. _whatsnew_0230.api_breaking.build_changes:  Build changes ^^^^^^^^^^^^^  - Building pandas for development now requires`cython \>= 0.24``(:issue:`18613`) - Building from source now explicitly requires``setuptools`in`setup.py``(:issue:`18113`) - Updated conda recipe to be in compliance with conda-build 3.0+ (:issue:`18002`)  .. _whatsnew_0230.api_breaking.index_division_by_zero:  Index division by zero fills correctly ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Division operations on``Index`and subclasses will now fill division of positive numbers by zero with`np.inf`, division of negative numbers by zero with`-np.inf`and`0 / 0`with`np.nan`.  This matches existing`Series``behavior. (:issue:`19322`, :issue:`19347`)  Previous behavior:``\`ipython In \[6\]: index = pd.Int64Index(\[-1, 0, 1\])

> In \[7\]: index / 0 Out\[7\]: Int64Index(\[0, 0, 0\], dtype='int64')
> 
> \# Previous behavior yielded different results depending on the type of zero in the divisor In \[8\]: index / 0.0 Out\[8\]: Float64Index(\[-inf, nan, inf\], dtype='float64')
> 
> In \[9\]: index = pd.UInt64Index(\[0, 1\])
> 
> In \[10\]: index / np.array(\[0, 0\], dtype=np.uint64) Out\[10\]: UInt64Index(\[0, 0\], dtype='uint64')
> 
> In \[11\]: pd.RangeIndex(1, 5) / 0 ZeroDivisionError: integer division or modulo by zero

Current behavior:

``` ipython
In [12]: index = pd.Int64Index([-1, 0, 1])
# division by zero gives -infinity where negative,
# +infinity where positive, and NaN for 0 / 0
In [13]: index / 0

# The result of division by zero should not depend on
# whether the zero is int or float
In [14]: index / 0.0

In [15]: index = pd.UInt64Index([0, 1])
In [16]: index / np.array([0, 0], dtype=np.uint64)

In [17]: pd.RangeIndex(1, 5) / 0
```

<div id="whatsnew_0230.api_breaking.extract">

Extraction of matching patterns from strings `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  By default, extracting matching patterns from strings with `str.extract` used to return a ``Series`if a single group was being extracted (a`DataFrame``if more than one group was extracted). As of pandas 0.23.0 `str.extract` always returns a``DataFrame`, unless`expand`is set to`False`. Finally,`None`was an accepted value for the`expand`parameter (which was equivalent to`False`), but now raises a`ValueError``. (:issue:`11386`)  Previous behavior:``\`ipython In \[1\]: s = pd.Series(\['number 10', '12 eggs'\])

</div>

> In \[2\]: extracted = s.str.extract(r'.*(dd).*')
> 
> In \[3\]: extracted Out \[3\]: 0 10 1 12 dtype: object
> 
> In \[4\]: type(extracted) Out \[4\]: pandas.core.series.Series

New behavior:

<div class="ipython">

python

s = pd.Series(\['number 10', '12 eggs'\]) extracted = s.str.extract(r'.*(dd).*') extracted type(extracted)

</div>

To restore previous behavior, simply set `expand` to `False`:

<div class="ipython">

python

s = pd.Series(\['number 10', '12 eggs'\]) extracted = s.str.extract(r'.*(dd).*', expand=False) extracted type(extracted)

</div>

<div id="whatsnew_0230.api_breaking.cdt_ordered">

Default value for the `ordered` parameter of `CategoricalDtype` `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  The default value of the ``ordered``parameter for `~pandas.api.types.CategoricalDtype` has changed from``False`to`None`to allow updating of`categories`without impacting`ordered``.  Behavior should remain consistent for downstream objects, such as `Categorical` (:issue:`18790`)  In previous versions, the default value for the``ordered`parameter was`False`.  This could potentially lead to the`ordered`parameter unintentionally being changed from`True`to`False`when users attempt to update`categories`if`ordered`is not explicitly specified, as it would silently default to`False`.  The new behavior for`ordered=None`is to retain the existing value of`ordered`.  New behavior:`\`ipython In \[2\]: from pandas.api.types import CategoricalDtype

</div>

> In \[3\]: cat = pd.Categorical(list('abcaba'), ordered=True, categories=list('cba'))
> 
> In \[4\]: cat Out\[4\]: \[a, b, c, a, b, a\] Categories (3, object): \[c \< b \< a\]
> 
> In \[5\]: cdt = CategoricalDtype(categories=list('cbad'))
> 
> In \[6\]: cat.astype(cdt) Out\[6\]: \[a, b, c, a, b, a\] Categories (4, object): \[c \< b \< a \< d\]

Notice in the example above that the converted `Categorical` has retained `ordered=True`. Had the default value for `ordered` remained as `False`, the converted `Categorical` would have become unordered, despite `ordered=False` never being explicitly specified. To change the value of `ordered`, explicitly pass it to the new dtype, e.g. `CategoricalDtype(categories=list('cbad'), ordered=False)`.

Note that the unintentional conversion of `ordered` discussed above did not arise in previous versions due to separate bugs that prevented `astype` from doing any type of category to category conversion (`10696`, `18593`). These bugs have been fixed in this release, and motivated changing the default value of `ordered`.

<div id="whatsnew_0230.api_breaking.pretty_printing">

Better pretty-printing of DataFrames in a terminal `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Previously, the default value for the maximum number of columns was ``pd.options.display.max\_columns=20`. This meant that relatively wide data frames would not fit within the terminal width, and pandas would introduce line breaks to display these 20 columns. This resulted in an output that was relatively difficult to read:  .. image:: ../_static/print_df_old.png  If Python runs in a terminal, the maximum number of columns is now determined automatically so that the printed data frame fits within the current terminal width (`pd.options.display.max\_columns=0``) (:issue:`17023`). If Python runs as a Jupyter kernel (such as the Jupyter QtConsole or a Jupyter notebook, as well as in many IDEs), this value cannot be inferred automatically and is thus set to``20`as in previous versions. In a terminal, this results in a much nicer output:  .. image:: ../_static/print_df_new.png  Note that if you don't like the new default, you can always set this option yourself. To revert to the old setting, you can run this line:`\`python pd.options.display.max\_columns = 20

</div>

<div id="whatsnew_0230.api.datetimelike">

Datetimelike API changes `` ` ^^^^^^^^^^^^^^^^^^^^^^^^  - The default ``Timedelta`constructor now accepts an`ISO 8601 Duration``string as an argument (:issue:`19040`) - Subtracting``NaT``from a `Series` with``dtype='datetime64\[ns\]'`returns a`Series`with`dtype='timedelta64\[ns\]'`instead of`dtype='datetime64\[ns\]'``(:issue:`18808`) - Addition or subtraction of``NaT``from `TimedeltaIndex` will return``TimedeltaIndex`instead of`DatetimeIndex``(:issue:`19124`) - `DatetimeIndex.shift` and `TimedeltaIndex.shift` will now raise``NullFrequencyError`(which subclasses`ValueError`, which was raised in older versions) when the index object frequency is`None``(:issue:`19147`) - Addition and subtraction of``NaN``from a `Series` with``dtype='timedelta64\[ns\]'`will raise a`TypeError`instead of treating the`NaN`as`NaT``(:issue:`19274`) -``NaT``division with `datetime.timedelta` will now return``NaN``instead of raising (:issue:`17876`) - Operations between a `Series` with dtype``dtype='datetime64\[ns\]'``and a `PeriodIndex` will correctly raises``TypeError``(:issue:`18850`) - Subtraction of `Series` with timezone-aware``dtype='datetime64\[ns\]'`with mismatched timezones will raise`TypeError`instead of`ValueError``(:issue:`18817`) - `Timestamp` will no longer silently ignore unused or invalid``tz`or`tzinfo``keyword arguments (:issue:`17690`) - `Timestamp` will no longer silently ignore invalid``freq``arguments (:issue:`5168`) - `CacheableOffset` and `WeekDay` are no longer available in the``pandas.tseries.offsets``module (:issue:`17830`) -``pandas.tseries.frequencies.get\_freq\_group()`and`pandas.tseries.frequencies.DAYS``are removed from the public API (:issue:`18034`) - `Series.truncate` and `DataFrame.truncate` will raise a``ValueError`if the index is not sorted instead of an unhelpful`KeyError``(:issue:`17935`) - `Series.first` and `DataFrame.first` will now raise a``TypeError`rather than`NotImplementedError``when index is not a `DatetimeIndex` (:issue:`20725`). - `Series.last` and `DataFrame.last` will now raise a``TypeError`rather than`NotImplementedError``when index is not a `DatetimeIndex` (:issue:`20725`). - Restricted``DateOffset`keyword arguments. Previously,`DateOffset``subclasses allowed arbitrary keyword arguments which could lead to unexpected behavior. Now, only valid arguments will be accepted. (:issue:`17176`, :issue:`18226`). - `pandas.merge` provides a more informative error message when trying to merge on timezone-aware and timezone-naive columns (:issue:`15800`) - For `DatetimeIndex` and `TimedeltaIndex` with``freq=None`, addition or subtraction of integer-dtyped array or`Index`will raise`NullFrequencyError`instead of`TypeError``(:issue:`19895`) - `Timestamp` constructor now accepts a``nanosecond``keyword or positional argument (:issue:`18898`) - `DatetimeIndex` will now raise an``AttributeError`when the`tz``attribute is set after instantiation (:issue:`3746`) - `DatetimeIndex` with a``pytz`timezone will now return a consistent`pytz``timezone (:issue:`18595`)  .. _whatsnew_0230.api.other:  Other API changes ^^^^^^^^^^^^^^^^^  - `Series.astype` and `Index.astype` with an incompatible dtype will now raise a``TypeError`rather than a`ValueError``(:issue:`18231`) -``Series`construction with an`object`dtyped tz-aware datetime and`dtype=object`specified, will now return an`object`dtyped`Series``, previously this would infer the datetime dtype (:issue:`18231`) - A `Series` of``dtype=category`constructed from an empty`dict`will now have categories of`dtype=object`rather than`dtype=float64``, consistently with the case in which an empty list is passed (:issue:`18515`) - All-NaN levels in a``MultiIndex`are now assigned`float`rather than`object`dtype, promoting consistency with`Index``(:issue:`17929`). - Levels names of a``MultiIndex`(when not None) are now required to be unique: trying to create a`MultiIndex`with repeated names will raise a`ValueError``(:issue:`18872`) - Both construction and renaming of``Index`/`MultiIndex`with non-hashable`name`/`names`will now raise`TypeError``(:issue:`20527`) - `Index.map` can now accept``Series``and dictionary input objects (:issue:`12756`, :issue:`18482`, :issue:`18509`). - `DataFrame.unstack` will now default to filling with``np.nan`for`object``columns. (:issue:`12815`) - `IntervalIndex` constructor will raise if the``closed``parameter conflicts with how the input data is inferred to be closed (:issue:`18421`) - Inserting missing values into indexes will work for all types of indexes and automatically insert the correct type of missing value (``NaN`,`NaT``, etc.) regardless of the type passed in (:issue:`18295`) - When created with duplicate labels,``MultiIndex`now raises a`ValueError``. (:issue:`17464`) - `Series.fillna` now raises a``TypeError`instead of a`ValueError`when passed a list, tuple or DataFrame as a`value``(:issue:`18293`) - `pandas.DataFrame.merge` no longer casts a``float`column to`object`when merging on`int`and`float``columns (:issue:`16572`) - `pandas.merge` now raises a``ValueError``when trying to merge on incompatible data types (:issue:`9780`) - The default NA value for `UInt64Index` has changed from 0 to``NaN`, which impacts methods that mask with NA, such as`UInt64Index.where()``(:issue:`18398`) - Refactored``setup.py`to use`find\_packages``instead of explicitly listing out all subpackages (:issue:`18535`) - Rearranged the order of keyword arguments in `read_excel` to align with `read_csv` (:issue:`16672`) - `wide_to_long` previously kept numeric-like suffixes as``object``dtype. Now they are cast to numeric if possible (:issue:`17627`) - In `read_excel`, the``comment``argument is now exposed as a named parameter (:issue:`18735`) - Rearranged the order of keyword arguments in `read_excel` to align with `read_csv` (:issue:`16672`) - The options``html.border`and`mode.use\_inf\_as\_null`were deprecated in prior versions, these will now show`FutureWarning`rather than a`DeprecationWarning``(:issue:`19003`) - `IntervalIndex` and``IntervalDtype``no longer support categorical, object, and string subtypes (:issue:`19016`) -``IntervalDtype`now returns`True`when compared against`'interval'`regardless of subtype, and`IntervalDtype.name`now returns`'interval'``regardless of subtype (:issue:`18980`) -``KeyError`now raises instead of`ValueError``in `~DataFrame.drop`, `~Panel.drop`, `~Series.drop`, `~Index.drop` when dropping a non-existent element in an axis with duplicates (:issue:`19186`) - `Series.to_csv` now accepts a``compression`argument that works in the same way as the`compression``argument in `DataFrame.to_csv` (:issue:`18958`) - Set operations (union, difference...) on `IntervalIndex` with incompatible index types will now raise a``TypeError`rather than a`ValueError``(:issue:`19329`) - `DateOffset` objects render more simply, e.g.``\<DateOffset: days=1\>`instead of`\<DateOffset: kwds={'days': 1}\>``(:issue:`19403`) -``Categorical.fillna`now validates its`value`and`method``keyword arguments. It now raises when both or none are specified, matching the behavior of `Series.fillna` (:issue:`19682`) -``pd.to\_datetime('today')`now returns a datetime, consistent with`pd.Timestamp('today')`; previously`pd.to\_datetime('today')`returned a`.normalized()``datetime (:issue:`19935`) - `Series.str.replace` now takes an optional``regex`keyword which, when set to`False``, uses literal string replacement rather than regex replacement (:issue:`16808`) - `DatetimeIndex.strftime` and `PeriodIndex.strftime` now return an``Index``instead of a numpy array to be consistent with similar accessors (:issue:`20127`) - Constructing a Series from a list of length 1 no longer broadcasts this list when a longer index is specified (:issue:`19714`, :issue:`20391`). - `DataFrame.to_dict` with``orient='index'``no longer casts int columns to float for a DataFrame with only int and float columns (:issue:`18580`) - A user-defined-function that is passed to `Series.rolling().aggregate() <.Rolling.aggregate>`, `DataFrame.rolling().aggregate() <.Rolling.aggregate>`, or its expanding cousins, will now *always* be passed a``Series`, rather than a`np.array`;`.apply()`only has the`raw`keyword, see [here <whatsnew_0230.enhancements.window_raw>](#here-<whatsnew_0230.enhancements.window_raw>). This is consistent with the signatures of`.aggregate()``across pandas (:issue:`20584`) - Rolling and Expanding types raise``NotImplementedError``upon iteration (:issue:`11704`).  .. _whatsnew_0230.deprecations:  Deprecations ~~~~~~~~~~~~  -``Series.from\_array`and`SparseSeries.from\_array`are deprecated. Use the normal constructor`Series(..)`and`SparseSeries(..)``instead (:issue:`18213`). -``DataFrame.as\_matrix`is deprecated. Use`DataFrame.values``instead (:issue:`18458`). -``Series.asobject`,`DatetimeIndex.asobject`,`PeriodIndex.asobject`and`TimeDeltaIndex.asobject`have been deprecated. Use`.astype(object)``instead (:issue:`18572`) - Grouping by a tuple of keys now emits a``FutureWarning`and is deprecated.   In the future, a tuple passed to`'by'``will always refer to a single key   that is the actual tuple, instead of treating the tuple as multiple keys. To   retain the previous behavior, use a list instead of a tuple (:issue:`18314`) -``Series.valid``is deprecated. Use `Series.dropna` instead (:issue:`18800`). - `read_excel` has deprecated the``skip\_footer`parameter. Use`skipfooter``instead (:issue:`18836`) - `ExcelFile.parse` has deprecated``sheetname`in favor of`sheet\_name``for consistency with `read_excel` (:issue:`20920`). - The``is\_copy``attribute is deprecated and will be removed in a future version (:issue:`18801`). -``IntervalIndex.from\_intervals``is deprecated in favor of the `IntervalIndex` constructor (:issue:`19263`) -``DataFrame.from\_items``is deprecated. Use `DataFrame.from_dict` instead, or``DataFrame.from\_dict(OrderedDict())``if you wish to preserve the key order (:issue:`17320`, :issue:`17312`) - Indexing a `MultiIndex` or a `FloatIndex` with a list containing some missing keys will now show a `FutureWarning`, which is consistent with other types of indexes (:issue:`17758`).  - The``broadcast`parameter of`.apply()`is deprecated in favor of`result\_type='broadcast'``(:issue:`18577`) - The``reduce`parameter of`.apply()`is deprecated in favor of`result\_type='reduce'``(:issue:`18577`) - The``order``parameter of `factorize` is deprecated and will be removed in a future release (:issue:`19727`) - `Timestamp.weekday_name`, `DatetimeIndex.weekday_name`, and `Series.dt.weekday_name` are deprecated in favor of `Timestamp.day_name`, `DatetimeIndex.day_name`, and `Series.dt.day_name` (:issue:`12806`)  -``pandas.tseries.plotting.tsplot``is deprecated. Use `Series.plot` instead (:issue:`18627`) -``Index.summary()``is deprecated and will be removed in a future version (:issue:`18217`) -``NDFrame.get\_ftype\_counts()``is deprecated and will be removed in a future version (:issue:`18243`) - The``convert\_datetime64``parameter in `DataFrame.to_records` has been deprecated and will be removed in a future version. The NumPy bug motivating this parameter has been resolved. The default value for this parameter has also changed from``True`to`None``(:issue:`18160`). - `Series.rolling().apply() <.Rolling.apply>`, `DataFrame.rolling().apply() <.Rolling.apply>`, `Series.expanding().apply() <.Expanding.apply>`, and `DataFrame.expanding().apply() <.Expanding.apply>` have deprecated passing an``np.array`by default. One will need to pass the new`raw``parameter to be explicit about what is passed (:issue:`20584`) - The``data`,`base`,`strides`,`flags`and`itemsize`properties   of the`Series`and`Index``classes have been deprecated and will be   removed in a future version (:issue:`20419`). -``DatetimeIndex.offset`is deprecated. Use`DatetimeIndex.freq``instead (:issue:`20716`) - Floor division between an integer ndarray and a `Timedelta` is deprecated. Divide by `Timedelta.value` instead (:issue:`19761`) - Setting``PeriodIndex.freq``(which was not guaranteed to work correctly) is deprecated. Use `PeriodIndex.asfreq` instead (:issue:`20678`) -``Index.get\_duplicates()``is deprecated and will be removed in a future version (:issue:`20239`) - The previous default behavior of negative indices in``Categorical.take``is deprecated. In a future version it will change from meaning missing values to meaning positional indices from the right. The future behavior is consistent with `Series.take` (:issue:`20664`). - Passing multiple axes to the``axis``parameter in `DataFrame.dropna` has been deprecated and will be removed in a future version (:issue:`20987`)   .. _whatsnew_0230.prior_deprecations:  Removal of prior version deprecations/changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - Warnings against the obsolete usage``Categorical(codes, categories)`, which were emitted for instance when the first two arguments to`Categorical()`had different dtypes, and recommended the use of`Categorical.from\_codes``, have now been removed (:issue:`8074`) - The``levels`and`labels`attributes of a`MultiIndex``can no longer be set directly (:issue:`4039`). -``pd.tseries.util.pivot\_annual`has been removed (deprecated since v0.19). Use`pivot\_table``instead (:issue:`18370`) -``pd.tseries.util.isleapyear`has been removed (deprecated since v0.19). Use`.is\_leap\_year``property in Datetime-likes instead (:issue:`18370`) -``pd.ordered\_merge`has been removed (deprecated since v0.19). Use`pd.merge\_ordered``instead (:issue:`18459`) - The``SparseList``class has been removed (:issue:`14007`) - The``pandas.io.wb`and`pandas.io.data``stub modules have been removed (:issue:`13735`) -``Categorical.from\_array``has been removed (:issue:`13854`) - The``freq`and`how`parameters have been removed from the`rolling`/`expanding`/`ewm``methods of DataFrame   and Series (deprecated since v0.18). Instead, resample before calling the methods. (:issue:`18601` & :issue:`18668`) -``DatetimeIndex.to\_datetime`,`Timestamp.to\_datetime`,`PeriodIndex.to\_datetime`, and`Index.to\_datetime``have been removed (:issue:`8254`, :issue:`14096`, :issue:`14113`) - `read_csv` has dropped the``skip\_footer``parameter (:issue:`13386`) - `read_csv` has dropped the``as\_recarray``parameter (:issue:`13373`) - `read_csv` has dropped the``buffer\_lines``parameter (:issue:`13360`) - `read_csv` has dropped the``compact\_ints`and`use\_unsigned``parameters (:issue:`13323`) - The``Timestamp`class has dropped the`offset`attribute in favor of`freq``(:issue:`13593`) - The``Series`,`Categorical`, and`Index`classes have dropped the`reshape``method (:issue:`13012`) -``pandas.tseries.frequencies.get\_standard\_freq`has been removed in favor of`pandas.tseries.frequencies.to\_offset(freq).rule\_code``(:issue:`13874`) - The``freqstr`keyword has been removed from`pandas.tseries.frequencies.to\_offset`in favor of`freq``(:issue:`13874`) - The``Panel4D`and`PanelND``classes have been removed (:issue:`13776`) - The``Panel`class has dropped the`to\_long`and`toLong``methods (:issue:`19077`) - The options``display.line\_with`and`display.height`are removed in favor of`display.width`and`display.max\_rows``respectively (:issue:`4391`, :issue:`19107`) - The``labels`attribute of the`Categorical``class has been removed in favor of `Categorical.codes` (:issue:`7768`) - The``flavor``parameter have been removed from `to_sql` method (:issue:`13611`) - The modules``pandas.tools.hashing`and`pandas.util.hashing``have been removed (:issue:`16223`) - The top-level functions``[pd.rolling]()*\`\`, \`\`pd.expanding\_*`and`pd.ewm\*``have been removed (Deprecated since v0.18).   Instead, use the DataFrame/Series methods `~DataFrame.rolling`, `~DataFrame.expanding` and `~DataFrame.ewm` (:issue:`18723`) - Imports from``pandas.core.common`for functions such as`is\_datetime64\_dtype`are now removed. These are located in`pandas.api.types``. (:issue:`13634`, :issue:`19769`) - The``infer\_dst``keyword in `Series.tz_localize`, `DatetimeIndex.tz_localize`   and `DatetimeIndex` have been removed.``infer\_dst=True`is equivalent to`ambiguous='infer'`, and`infer\_dst=False`to`ambiguous='raise'``(:issue:`7963`). - When``.resample()`was changed from an eager to a lazy operation, like`.groupby()`in v0.18.0, we put in place compatibility (with a`FutureWarning`),   so operations would continue to work. This is now fully removed, so a`Resampler``will no longer forward compat operations (:issue:`20554`) - Remove long deprecated``axis=None`parameter from`.replace()``(:issue:`20271`)  .. _whatsnew_0230.performance:  Performance improvements ~~~~~~~~~~~~~~~~~~~~~~~~  - Indexers on``Series`or`DataFrame``no longer create a reference cycle (:issue:`17956`) - Added a keyword argument,``cache``, to `to_datetime` that improved the performance of converting duplicate datetime arguments (:issue:`11665`) - `DateOffset` arithmetic performance is improved (:issue:`18218`) - Converting a``Series`of`Timedelta``objects to days, seconds, etc... sped up through vectorization of underlying methods (:issue:`18092`) - Improved performance of``.map()`with a`Series/dict``input (:issue:`15081`) - The overridden``Timedelta``properties of days, seconds and microseconds have been removed, leveraging their built-in Python versions instead (:issue:`18242`) -``Series``construction will reduce the number of copies made of the input data in certain cases (:issue:`17449`) - Improved performance of `Series.dt.date` and `DatetimeIndex.date` (:issue:`18058`) - Improved performance of `Series.dt.time` and `DatetimeIndex.time` (:issue:`18461`) - Improved performance of `IntervalIndex.symmetric_difference` (:issue:`18475`) - Improved performance of``DatetimeIndex`and`Series``arithmetic operations with Business-Month and Business-Quarter frequencies (:issue:`18489`) - `Series` / `DataFrame` tab completion limits to 100 values, for better performance. (:issue:`18587`) - Improved performance of `DataFrame.median` with``axis=1``when bottleneck is not installed (:issue:`16468`) - Improved performance of `MultiIndex.get_loc` for large indexes, at the cost of a reduction in performance for small ones (:issue:`18519`) - Improved performance of `MultiIndex.remove_unused_levels` when there are no unused levels, at the cost of a reduction in performance when there are (:issue:`19289`) - Improved performance of `Index.get_loc` for non-unique indexes (:issue:`19478`) - Improved performance of pairwise``.rolling()`and`.expanding()`with`.cov()`and`.corr()``operations (:issue:`17917`) - Improved performance of `.GroupBy.rank` (:issue:`15779`) - Improved performance of variable``.rolling()`on`.min()`and`.max()``(:issue:`19521`) - Improved performance of `.GroupBy.ffill` and `.GroupBy.bfill` (:issue:`11296`) - Improved performance of `.GroupBy.any` and `.GroupBy.all` (:issue:`15435`) - Improved performance of `.GroupBy.pct_change` (:issue:`19165`) - Improved performance of `Series.isin` in the case of categorical dtypes (:issue:`20003`) - Improved performance of``getattr(Series, attr)`when the Series has certain index types. This manifested in slow printing of large Series with a`DatetimeIndex``(:issue:`19764`) - Fixed a performance regression for `GroupBy.nth` and `GroupBy.last` with some object columns (:issue:`19283`) - Improved performance of `.Categorical.from_codes` (:issue:`18501`)  .. _whatsnew_0230.docs:  Documentation changes ~~~~~~~~~~~~~~~~~~~~~  Thanks to all of the contributors who participated in the pandas Documentation Sprint, which took place on March 10th. We had about 500 participants from over 30 locations across the world. You should notice that many of the [API docstrings <api>](#api-docstrings-<api>) have greatly improved.  There were too many simultaneous contributions to include a release note for each improvement, but this `GitHub search`_ should give you an idea of how many docstrings were improved.  Special thanks to `Marc Garcia`_ for organizing the sprint. For more information, read the `NumFOCUS blogpost`_ recapping the sprint.      - Changed spelling of "numpy" to "NumPy", and "python" to "Python". (:issue:`19017`) - Consistency when introducing code samples, using either colon or period.   Rewrote some sentences for greater clarity, added more dynamic references   to functions, methods and classes.   (:issue:`18941`, :issue:`18948`, :issue:`18973`, :issue:`19017`) - Added a reference to `DataFrame.assign` in the concatenate section of the merging documentation (:issue:`18665`)  .. _whatsnew_0230.bug_fixes:  Bug fixes ~~~~~~~~~  Categorical ^^^^^^^^^^^  > **Warning** >     A class of bugs were introduced in pandas 0.21 with``CategoricalDtype`that    affects the correctness of operations like`merge`,`concat`, and    indexing when comparing multiple unordered`Categorical`arrays that have    the same categories, but in a different order. We highly recommend upgrading    or manually aligning your categories before doing these operations.  - Bug in`Categorical.equals`returning the wrong result when comparing two   unordered`Categorical``arrays with the same categories, but in a different   order (:issue:`16603`) - Bug in `pandas.api.types.union_categoricals` returning the wrong result   when for unordered categoricals with the categories in a different order.   This affected `pandas.concat` with Categorical data (:issue:`19096`). - Bug in `pandas.merge` returning the wrong result when joining on an   unordered``Categorical``that had the same categories but in a different   order (:issue:`19551`) - Bug in `CategoricalIndex.get_indexer` returning the wrong result when``target`was an unordered`Categorical`that had the same categories as`self``but in a different order (:issue:`19551`) - Bug in `Index.astype` with a categorical dtype where the resultant index is not converted to a `CategoricalIndex` for all types of index (:issue:`18630`) - Bug in `Series.astype` and``Categorical.astype()``where an existing categorical data does not get updated (:issue:`10696`, :issue:`18593`) - Bug in `Series.str.split` with``expand=True``incorrectly raising an IndexError on empty strings (:issue:`20002`). - Bug in `Index` constructor with``dtype=CategoricalDtype(...)`where`categories`and`ordered``are not maintained (:issue:`19032`) - Bug in `Series` constructor with scalar and``dtype=CategoricalDtype(...)`where`categories`and`ordered``are not maintained (:issue:`19565`) - Bug in``Categorical.\_\_iter\_\_``not converting to Python types (:issue:`19909`) - Bug in `pandas.factorize` returning the unique codes for the``uniques`. This now returns a`Categorical``with the same dtype as the input (:issue:`19721`) - Bug in `pandas.factorize` including an item for missing values in the``uniques``return value (:issue:`19721`) - Bug in `Series.take` with categorical data interpreting``-1`in`indices``as missing value markers, rather than the last element of the Series (:issue:`20664`)  Datetimelike ^^^^^^^^^^^^  - Bug in `Series.__sub__` subtracting a non-nanosecond``np.datetime64`object from a`Series``gave incorrect results (:issue:`7996`) - Bug in `DatetimeIndex`, `TimedeltaIndex` addition and subtraction of zero-dimensional integer arrays gave incorrect results (:issue:`19012`) - Bug in `DatetimeIndex` and `TimedeltaIndex` where adding or subtracting an array-like of``DateOffset`objects either raised (`np.array`,`pd.Index`) or broadcast incorrectly (`pd.Series``) (:issue:`18849`) - Bug in `Series.__add__` adding Series with dtype``timedelta64\[ns\]`to a timezone-aware`DatetimeIndex``incorrectly dropped timezone information (:issue:`13905`) - Adding a``Period`object to a`datetime`or`Timestamp`object will now correctly raise a`TypeError``(:issue:`17983`) - Bug in `Timestamp` where comparison with an array of``Timestamp`objects would result in a`RecursionError``(:issue:`15183`) - Bug in `Series` floor-division where operating on a scalar``timedelta``raises an exception (:issue:`18846`) - Bug in `DatetimeIndex` where the repr was not showing high-precision time values at the end of a day (e.g., 23:59:59.999999999) (:issue:`19030`) - Bug in``.astype()``to non-ns timedelta units would hold the incorrect dtype (:issue:`19176`, :issue:`19223`, :issue:`12425`) - Bug in subtracting `Series` from``NaT`incorrectly returning`NaT``(:issue:`19158`) - Bug in `Series.truncate` which raises``TypeError`with a monotonic`PeriodIndex``(:issue:`17717`) - Bug in `~DataFrame.pct_change` using``periods`and`freq``returned different length outputs (:issue:`7292`) - Bug in comparison of `DatetimeIndex` against``None`or`datetime.date`objects raising`TypeError`for`==`and`\!=`comparisons instead of all-`False`and all-`True``, respectively (:issue:`19301`) - Bug in `Timestamp` and `to_datetime` where a string representing a barely out-of-bounds timestamp would be incorrectly rounded down instead of raising``OutOfBoundsDatetime``(:issue:`19382`) - Bug in `Timestamp.floor` `DatetimeIndex.floor` where time stamps far in the future and past were not rounded correctly (:issue:`19206`) - Bug in `to_datetime` where passing an out-of-bounds datetime with``errors='coerce'`and`utc=True`would raise`OutOfBoundsDatetime`instead of parsing to`NaT``(:issue:`19612`) - Bug in `DatetimeIndex` and `TimedeltaIndex` addition and subtraction where name of the returned object was not always set consistently. (:issue:`19744`) - Bug in `DatetimeIndex` and `TimedeltaIndex` addition and subtraction where operations with numpy arrays raised``TypeError``(:issue:`19847`) - Bug in `DatetimeIndex` and `TimedeltaIndex` where setting the``freq``attribute was not fully supported (:issue:`20678`)  Timedelta ^^^^^^^^^  - Bug in `Timedelta.__mul__` where multiplying by``NaT`returned`NaT`instead of raising a`TypeError``(:issue:`19819`) - Bug in `Series` with``dtype='timedelta64\[ns\]'`where addition or subtraction of`TimedeltaIndex`had results cast to`dtype='int64'``(:issue:`17250`) - Bug in `Series` with``dtype='timedelta64\[ns\]'`where addition or subtraction of`TimedeltaIndex`could return a`Series``with an incorrect name (:issue:`19043`) - Bug in `Timedelta.__floordiv__` and `Timedelta.__rfloordiv__` dividing by many incompatible numpy objects was incorrectly allowed (:issue:`18846`) - Bug where dividing a scalar timedelta-like object with `TimedeltaIndex` performed the reciprocal operation (:issue:`19125`) - Bug in `TimedeltaIndex` where division by a``Series`would return a`TimedeltaIndex`instead of a`Series``(:issue:`19042`) - Bug in `Timedelta.__add__`, `Timedelta.__sub__` where adding or subtracting a``np.timedelta64`object would return another`np.timedelta64`instead of a`Timedelta``(:issue:`19738`) - Bug in `Timedelta.__floordiv__`, `Timedelta.__rfloordiv__` where operating with a``Tick`object would raise a`TypeError``instead of returning a numeric value (:issue:`19738`) - Bug in `Period.asfreq` where periods near``datetime(1, 1, 1)``could be converted incorrectly (:issue:`19643`, :issue:`19834`) - Bug in `Timedelta.total_seconds` causing precision errors, for example``Timedelta('30S').total\_seconds()==30.000000000000004``(:issue:`19458`) - Bug in `Timedelta.__rmod__` where operating with a``numpy.timedelta64`returned a`timedelta64`object instead of a`Timedelta``(:issue:`19820`) - Multiplication of `TimedeltaIndex` by``TimedeltaIndex`will now raise`TypeError`instead of raising`ValueError``in cases of length mismatch (:issue:`19333`) - Bug in indexing a `TimedeltaIndex` with a``np.timedelta64`object which was raising a`TypeError``(:issue:`20393`)   Timezones ^^^^^^^^^  - Bug in creating a``Series`from an array that contains both tz-naive and tz-aware values will result in a`Series``whose dtype is tz-aware instead of object (:issue:`16406`) - Bug in comparison of timezone-aware `DatetimeIndex` against``NaT`incorrectly raising`TypeError``(:issue:`19276`) - Bug in `DatetimeIndex.astype` when converting between timezone aware dtypes, and converting from timezone aware to naive (:issue:`18951`) - Bug in comparing `DatetimeIndex`, which failed to raise``TypeError``when attempting to compare timezone-aware and timezone-naive datetimelike objects (:issue:`18162`) - Bug in localization of a naive, datetime string in a``Series`constructor with a`datetime64\[ns, tz\]``dtype (:issue:`174151`) - `Timestamp.replace` will now handle Daylight Savings transitions gracefully (:issue:`18319`) - Bug in tz-aware `DatetimeIndex` where addition/subtraction with a `TimedeltaIndex` or array with``dtype='timedelta64\[ns\]'``was incorrect (:issue:`17558`) - Bug in `DatetimeIndex.insert` where inserting``NaT``into a timezone-aware index incorrectly raised (:issue:`16357`) - Bug in `DataFrame` constructor, where tz-aware Datetimeindex and a given column name will result in an empty``DataFrame``(:issue:`19157`) - Bug in `Timestamp.tz_localize` where localizing a timestamp near the minimum or maximum valid values could overflow and return a timestamp with an incorrect nanosecond value (:issue:`12677`) - Bug when iterating over `DatetimeIndex` that was localized with fixed timezone offset that rounded nanosecond precision to microseconds (:issue:`19603`) - Bug in `DataFrame.diff` that raised an``IndexError``with tz-aware values (:issue:`18578`) - Bug in `melt` that converted tz-aware dtypes to tz-naive (:issue:`15785`) - Bug in `Dataframe.count` that raised an``ValueError``, if `Dataframe.dropna` was called for a single column with timezone-aware values. (:issue:`13407`)  Offsets ^^^^^^^  - Bug in `WeekOfMonth` and `Week` where addition and subtraction did not roll correctly (:issue:`18510`, :issue:`18672`, :issue:`18864`) - Bug in `WeekOfMonth` and `LastWeekOfMonth` where default keyword arguments for constructor raised``ValueError``(:issue:`19142`) - Bug in `FY5253Quarter`, `LastWeekOfMonth` where rollback and rollforward behavior was inconsistent with addition and subtraction behavior (:issue:`18854`) - Bug in `FY5253` where``datetime``addition and subtraction incremented incorrectly for dates on the year-end but not normalized to midnight (:issue:`18854`) - Bug in `FY5253` where date offsets could incorrectly raise an``AssertionError``in arithmetic operations (:issue:`14774`)  Numeric ^^^^^^^ - Bug in `Series` constructor with an int or float list where specifying``dtype=str`,`dtype='str'`or`dtype='U'``failed to convert the data elements to strings (:issue:`16605`) - Bug in `Index` multiplication and division methods where operating with a``Series`would return an`Index`object instead of a`Series``object (:issue:`19042`) - Bug in the `DataFrame` constructor in which data containing very large positive or very large negative numbers was causing``OverflowError``(:issue:`18584`) - Bug in `Index` constructor with``dtype='uint64'``where int-like floats were not coerced to `UInt64Index` (:issue:`18400`) - Bug in `DataFrame` flex arithmetic (e.g.``df.add(other, fill\_value=foo)`) with a`fill\_value`other than`None`failed to raise`NotImplementedError`in corner cases where either the frame or`other``has length zero (:issue:`19522`) - Multiplication and division of numeric-dtyped `Index` objects with timedelta-like scalars returns``TimedeltaIndex`instead of raising`TypeError``(:issue:`19333`) - Bug where``NaN``was returned instead of 0 by `Series.pct_change` and `DataFrame.pct_change` when``fill\_method`is not`None``(:issue:`19873`)  Strings ^^^^^^^ - Bug in `Series.str.get` with a dictionary in the values and the index not in the keys, raising``KeyError``(:issue:`20671`)   Indexing ^^^^^^^^  - Bug in `Index` construction from list of mixed type tuples (:issue:`18505`) - Bug in `Index.drop` when passing a list of both tuples and non-tuples (:issue:`18304`) - Bug in `DataFrame.drop`, `Panel.drop`, `Series.drop`, `Index.drop` where no``KeyError``is raised when dropping a non-existent element from an axis that contains duplicates (:issue:`19186`) - Bug in indexing a datetimelike``Index`that raised`ValueError`instead of`IndexError``(:issue:`18386`). - `Index.to_series` now accepts``index`and`name``kwargs (:issue:`18699`) - `DatetimeIndex.to_series` now accepts``index`and`name``kwargs (:issue:`18699`) - Bug in indexing non-scalar value from``Series`having non-unique`Index``will return value flattened (:issue:`17610`) - Bug in indexing with iterator containing only missing keys, which raised no error (:issue:`20748`) - Fixed inconsistency in``.ix``between list and scalar keys when the index has integer dtype and does not include the desired keys (:issue:`20753`) - Bug in``\_\_setitem\_\_``when indexing a `DataFrame` with a 2-d boolean ndarray (:issue:`18582`) - Bug in``str.extractall``when there were no matches empty `Index` was returned instead of appropriate `MultiIndex` (:issue:`19034`) - Bug in `IntervalIndex` where empty and purely NA data was constructed inconsistently depending on the construction method (:issue:`18421`) - Bug in `IntervalIndex.symmetric_difference` where the symmetric difference with a non-``IntervalIndex``did not raise (:issue:`18475`) - Bug in `IntervalIndex` where set operations that returned an empty``IntervalIndex``had the wrong dtype (:issue:`19101`) - Bug in `DataFrame.drop_duplicates` where no``KeyError`is raised when passing in columns that don't exist on the`DataFrame``(:issue:`19726`) - Bug in``Index``subclasses constructors that ignore unexpected keyword arguments (:issue:`19348`) - Bug in `Index.difference` when taking difference of an``Index``with itself (:issue:`20040`) - Bug in `DataFrame.first_valid_index` and `DataFrame.last_valid_index` in presence of entire rows of NaNs in the middle of values (:issue:`20499`). - Bug in `IntervalIndex` where some indexing operations were not supported for overlapping or non-monotonic``uint64``data (:issue:`20636`) - Bug in``Series.is\_unique`where extraneous output in stderr is shown if Series contains objects with`\_\_ne\_\_``defined (:issue:`20661`) - Bug in``.loc``assignment with a single-element list-like incorrectly assigns as a list (:issue:`19474`) - Bug in partial string indexing on a``Series/DataFrame`with a monotonic decreasing`DatetimeIndex``(:issue:`19362`) - Bug in performing in-place operations on a``DataFrame`with a duplicate`Index``(:issue:`17105`) - Bug in `IntervalIndex.get_loc` and `IntervalIndex.get_indexer` when used with an `IntervalIndex` containing a single interval (:issue:`17284`, :issue:`20921`) - Bug in``.loc`with a`uint64``indexer (:issue:`20722`)  MultiIndex ^^^^^^^^^^  - Bug in `MultiIndex.__contains__` where non-tuple keys would return``True``even if they had been dropped (:issue:`19027`) - Bug in `MultiIndex.set_labels` which would cause casting (and potentially clipping) of the new labels if the``level``argument is not 0 or a list like [0, 1, ... ]  (:issue:`19057`) - Bug in `MultiIndex.get_level_values` which would return an invalid index on level of ints with missing values (:issue:`17924`) - Bug in `MultiIndex.unique` when called on empty `MultiIndex` (:issue:`20568`) - Bug in `MultiIndex.unique` which would not preserve level names (:issue:`20570`) - Bug in `MultiIndex.remove_unused_levels` which would fill nan values (:issue:`18417`) - Bug in `MultiIndex.from_tuples` which would fail to take zipped tuples in python3 (:issue:`18434`) - Bug in `MultiIndex.get_loc` which would fail to automatically cast values between float and int (:issue:`18818`, :issue:`15994`) - Bug in `MultiIndex.get_loc` which would cast boolean to integer labels (:issue:`19086`) - Bug in `MultiIndex.get_loc` which would fail to locate keys containing``NaN``(:issue:`18485`) - Bug in `MultiIndex.get_loc` in large `MultiIndex`, would fail when levels had different dtypes (:issue:`18520`) - Bug in indexing where nested indexers having only numpy arrays are handled incorrectly (:issue:`19686`)   IO ^^  - `read_html` now rewinds seekable IO objects after parse failure, before attempting to parse with a new parser. If a parser errors and the object is non-seekable, an informative error is raised suggesting the use of a different parser (:issue:`17975`) - `DataFrame.to_html` now has an option to add an id to the leading``\<table\>``tag (:issue:`8496`) - Bug in `read_msgpack` with a non existent file is passed in Python 2 (:issue:`15296`) - Bug in `read_csv` where a``MultiIndex``with duplicate columns was not being mangled appropriately (:issue:`18062`) - Bug in `read_csv` where missing values were not being handled properly when``keep\_default\_na=False`with dictionary`na\_values``(:issue:`19227`) - Bug in `read_csv` causing heap corruption on 32-bit, big-endian architectures (:issue:`20785`) - Bug in `read_sas` where a file with 0 variables gave an``AttributeError`incorrectly. Now it gives an`EmptyDataError``(:issue:`18184`) - Bug in `DataFrame.to_latex` where pairs of braces meant to serve as invisible placeholders were escaped (:issue:`18667`) - Bug in `DataFrame.to_latex` where a``NaN`in a`MultiIndex`would cause an`IndexError``or incorrect output (:issue:`14249`) - Bug in `DataFrame.to_latex` where a non-string index-level name would result in an``AttributeError``(:issue:`19981`) - Bug in `DataFrame.to_latex` where the combination of an index name and the``index\_names=False``option would result in incorrect output (:issue:`18326`) - Bug in `DataFrame.to_latex` where a``MultiIndex``with an empty string as its name would result in incorrect output (:issue:`18669`) - Bug in `DataFrame.to_latex` where missing space characters caused wrong escaping and produced non-valid latex in some cases (:issue:`20859`) - Bug in `read_json` where large numeric values were causing an``OverflowError``(:issue:`18842`) - Bug in `DataFrame.to_parquet` where an exception was raised if the write destination is S3 (:issue:`19134`) - `Interval` now supported in `DataFrame.to_excel` for all Excel file types (:issue:`19242`) - `Timedelta` now supported in `DataFrame.to_excel` for all Excel file types (:issue:`19242`, :issue:`9155`, :issue:`19900`) - Bug in `pandas.io.stata.StataReader.value_labels` raising an``AttributeError``when called on very old files. Now returns an empty dict (:issue:`19417`) - Bug in `read_pickle` when unpickling objects with `TimedeltaIndex` or `Float64Index` created with pandas prior to version 0.20 (:issue:`19939`) - Bug in `pandas.io.json.json_normalize` where sub-records are not properly normalized if any sub-records values are NoneType (:issue:`20030`) - Bug in``usecols``parameter in `read_csv` where error is not raised correctly when passing a string. (:issue:`20529`) - Bug in `HDFStore.keys` when reading a file with a soft link causes exception (:issue:`20523`) - Bug in `HDFStore.select_column` where a key which is not a valid store raised an``AttributeError`instead of a`KeyError``(:issue:`17912`)  Plotting ^^^^^^^^  - Better error message when attempting to plot but matplotlib is not installed (:issue:`19810`). - `DataFrame.plot` now raises a``ValueError`when the`x`or`y``argument is improperly formed (:issue:`18671`) - Bug in `DataFrame.plot` when``x`and`y``arguments given as positions caused incorrect referenced columns for line, bar and area plots (:issue:`20056`) - Bug in formatting tick labels with``datetime.time()``and fractional seconds (:issue:`18478`). - `Series.plot.kde` has exposed the args``ind`and`bw\_method``in the docstring (:issue:`18461`). The argument``ind``may now also be an integer (number of sample points). - `DataFrame.plot` now supports multiple columns to the``y``argument (:issue:`19699`)   GroupBy/resample/rolling ^^^^^^^^^^^^^^^^^^^^^^^^  - Bug when grouping by a single column and aggregating with a class like``list`or`tuple``(:issue:`18079`) - Fixed regression in `DataFrame.groupby` which would not emit an error when called with a tuple key not in the index (:issue:`18798`) - Bug in `DataFrame.resample` which silently ignored unsupported (or mistyped) options for``label`,`closed`and`convention``(:issue:`19303`) - Bug in `DataFrame.groupby` where tuples were interpreted as lists of keys rather than as keys (:issue:`17979`, :issue:`18249`) - Bug in `DataFrame.groupby` where aggregation by``first`/`last`/`min`/`max``was causing timestamps to lose precision (:issue:`19526`) - Bug in `DataFrame.transform` where particular aggregation functions were being incorrectly cast to match the dtype(s) of the grouped data (:issue:`19200`) - Bug in `DataFrame.groupby` passing the``on=`kwarg, and subsequently using`.apply()``(:issue:`17813`) - Bug in `DataFrame.resample().aggregate <.Resampler.aggregate>` not raising a``KeyError``when aggregating a non-existent column (:issue:`16766`, :issue:`19566`) - Bug in `DataFrameGroupBy.cumsum` and `DataFrameGroupBy.cumprod` when``skipna``was passed (:issue:`19806`) - Bug in `DataFrame.resample` that dropped timezone information (:issue:`13238`) - Bug in `DataFrame.groupby` where transformations using``np.all`and`np.any`were raising a`ValueError``(:issue:`20653`) - Bug in `DataFrame.resample` where``ffill`,`bfill`,`pad`,`backfill`,`fillna`,`interpolate`, and`asfreq`were ignoring`loffset``. (:issue:`20744`) - Bug in `DataFrame.groupby` when applying a function that has mixed data types and the user supplied function can fail on the grouping column (:issue:`20949`) - Bug in `DataFrameGroupBy.rolling().apply() <.Rolling.apply>` where operations performed against the associated `DataFrameGroupBy` object could impact the inclusion of the grouped item(s) in the result (:issue:`14013`)  Sparse ^^^^^^  - Bug in which creating a `SparseDataFrame` from a dense``Series``or an unsupported type raised an uncontrolled exception (:issue:`19374`) - Bug in `SparseDataFrame.to_csv` causing exception (:issue:`19384`) - Bug in `SparseSeries.memory_usage` which caused segfault by accessing non sparse elements (:issue:`19368`) - Bug in constructing a `SparseArray`: if``data`is a scalar and`index`is defined it will coerce to`float64``regardless of scalar's dtype. (:issue:`19163`)  Reshaping ^^^^^^^^^  - Bug in `DataFrame.merge` where referencing a``CategoricalIndex`by name, where the`by`kwarg would`KeyError``(:issue:`20777`) - Bug in `DataFrame.stack` which fails trying to sort mixed type levels under Python 3 (:issue:`18310`) - Bug in `DataFrame.unstack` which casts int to float if``columns`is a`MultiIndex``with unused levels (:issue:`17845`) - Bug in `DataFrame.unstack` which raises an error if``index`is a`MultiIndex``with unused labels on the unstacked level (:issue:`18562`) - Fixed construction of a `Series` from a``dict`containing`NaN``as key (:issue:`18480`) - Fixed construction of a `DataFrame` from a``dict`containing`NaN``as key (:issue:`18455`) - Disabled construction of a `Series` where len(index) > len(data) = 1, which previously would broadcast the data item, and now raises a``ValueError``(:issue:`18819`) - Suppressed error in the construction of a `DataFrame` from a``dict``containing scalar values when the corresponding keys are not included in the passed index (:issue:`18600`)  - Fixed (changed from``object`to`float64``) dtype of `DataFrame` initialized with axes, no data, and``dtype=int``(:issue:`19646`) - Bug in `Series.rank` where``Series`containing`NaT`modifies the`Series``inplace (:issue:`18521`) - Bug in `cut` which fails when using readonly arrays (:issue:`18773`) - Bug in `DataFrame.pivot_table` which fails when the``aggfunc`arg is of type string.  The behavior is now consistent with other methods like`agg`and`apply``(:issue:`18713`) - Bug in `DataFrame.merge` in which merging using``Index``objects as vectors raised an Exception (:issue:`19038`) - Bug in `DataFrame.stack`, `DataFrame.unstack`, `Series.unstack` which were not returning subclasses (:issue:`15563`) - Bug in timezone comparisons, manifesting as a conversion of the index to UTC in``.concat()``(:issue:`18523`) - Bug in `concat` when concatenating sparse and dense series it returns only a``SparseDataFrame`. Should be a`DataFrame``. (:issue:`18914`, :issue:`18686`, and :issue:`16874`) - Improved error message for `DataFrame.merge` when there is no common merge key (:issue:`19427`) - Bug in `DataFrame.join` which does an``outer`instead of a`left``join when being called with multiple DataFrames and some have non-unique indices (:issue:`19624`) - `Series.rename` now accepts``axis``as a kwarg (:issue:`18589`) - Bug in `~DataFrame.rename` where an Index of same-length tuples was converted to a MultiIndex (:issue:`19497`) - Comparisons between `Series` and `Index` would return a``Series`with an incorrect name, ignoring the`Index``'s name attribute (:issue:`19582`) - Bug in `qcut` where datetime and timedelta data with``NaT`present raised a`ValueError``(:issue:`19768`) - Bug in `DataFrame.iterrows`, which would infers strings not compliant to `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ to datetimes (:issue:`19671`) - Bug in `Series` constructor with``Categorical`where a`ValueError``is not raised when an index of different length is given (:issue:`19342`) - Bug in `DataFrame.astype` where column metadata is lost when converting to categorical or a dictionary of dtypes (:issue:`19920`) - Bug in `cut` and `qcut` where timezone information was dropped (:issue:`19872`) - Bug in `Series` constructor with a``dtype=str``, previously raised in some cases (:issue:`19853`) - Bug in `get_dummies`, and `select_dtypes`, where duplicate column names caused incorrect behavior (:issue:`20848`) - Bug in `isna`, which cannot handle ambiguous typed lists (:issue:`20675`) - Bug in `concat` which raises an error when concatenating TZ-aware dataframes and all-NaT dataframes (:issue:`12396`) - Bug in `concat` which raises an error when concatenating empty TZ-aware series (:issue:`18447`)  Other ^^^^^  - Improved error message when attempting to use a Python keyword as an identifier in a``numexpr``backed query (:issue:`18221`) - Bug in accessing a `pandas.get_option`, which raised``KeyError`rather than`OptionError\`<span class="title-ref"> when looking up a non-existent option key in some cases (:issue:\`19789</span>) - Bug in <span class="title-ref">testing.assert\_series\_equal</span> and <span class="title-ref">testing.assert\_frame\_equal</span> for Series or DataFrames with differing unicode data (`20503`)

</div>

## Contributors

<div class="contributors">

v0.22.0..v0.23.0

</div>

---

v0.23.1.md

---

# What's new in 0.23.1 (June 12, 2018)

{{ header }}

This is a minor bug-fix release in the 0.23.x series and includes some small regression fixes and bug fixes. We recommend that all users upgrade to this version.

\> **Warning** \> Starting January 1, 2019, pandas feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.23.1

</div>

## Fixed regressions

**Comparing Series with datetime.date**

We've reverted a 0.23.0 change to comparing a <span class="title-ref">Series</span> holding datetimes and a `datetime.date` object (`21152`). In pandas 0.22 and earlier, comparing a Series holding datetimes and `datetime.date` objects would coerce the `datetime.date` to a datetime before comparing. This was inconsistent with Python, NumPy, and <span class="title-ref">DatetimeIndex</span>, which never consider a datetime and `datetime.date` equal.

In 0.23.0, we unified operations between DatetimeIndex and Series, and in the process changed comparisons between a Series of datetimes and `datetime.date` without warning.

We've temporarily restored the 0.22.0 behavior, so datetimes and dates may again compare equal, but restore the 0.23.0 behavior in a future release.

To summarize, here's the behavior in 0.22.0, 0.23.0, 0.23.1:

  - `` `python    # 0.22.0... Silently coerce the datetime.date    >>> import datetime    >>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)    0     True    1    False    dtype: bool     # 0.23.0... Do not coerce the datetime.date    >>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)    0    False    1    False    dtype: bool     # 0.23.1... Coerce the datetime.date with a warning    >>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)    /bin/python:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the    'datetime.date' is coerced to a datetime. In the future pandas will    not coerce, and the values not compare equal to the 'datetime.date'.    To retain the current behavior, convert the 'datetime.date' to a    datetime with 'pd.Timestamp'.      #!/bin/python3    0     True    1    False    dtype: bool  In addition, ordering comparisons will raise a ``TypeError``in the future.  **Other fixes**  - Reverted the ability of `~DataFrame.to_sql` to perform multivalue   inserts as this caused regression in certain cases (:issue:`21103`).   In the future this will be made configurable.``<span class="title-ref"> - Fixed regression in the \`DatetimeIndex.date</span> and <span class="title-ref">DatetimeIndex.time</span>  
    attributes in case of timezone-aware data: <span class="title-ref">DatetimeIndex.time</span> returned a tz-aware time instead of tz-naive (`21267`) and <span class="title-ref">DatetimeIndex.date</span> returned incorrect date when the input date has a non-UTC timezone (`21230`).

  - \- Fixed regression in <span class="title-ref">pandas.io.json.json\_normalize</span> when called with `None` values  
    in nested levels in JSON, and to not drop keys with value as `None` (`21158`, `21356`).

<!-- end list -->

  - Bug in <span class="title-ref">\~DataFrame.to\_csv</span> causes encoding error when compression and encoding are specified (`21241`, `21118`)
  - Bug preventing pandas from being importable with -OO optimization (`21071`)
  - Bug in <span class="title-ref">Categorical.fillna</span> incorrectly raising a `TypeError` when `value` the individual categories are iterable and `value` is an iterable (`21097`, `19788`)
  - Fixed regression in constructors coercing NA values like `None` to strings when passing `dtype=str` (`21083`)
  - Regression in <span class="title-ref">pivot\_table</span> where an ordered `Categorical` with missing values for the pivot's `index` would give a mis-aligned result (`21133`)
  - Fixed regression in merging on boolean index/columns (`21119`).

## Performance improvements

  - Improved performance of <span class="title-ref">CategoricalIndex.is\_monotonic\_increasing</span>, <span class="title-ref">CategoricalIndex.is\_monotonic\_decreasing</span> and <span class="title-ref">CategoricalIndex.is\_monotonic</span> (`21025`)
  - Improved performance of <span class="title-ref">CategoricalIndex.is\_unique</span> (`21107`)

## Bug fixes

**Groupby/resample/rolling**

  - Bug in <span class="title-ref">DataFrame.agg</span> where applying multiple aggregation functions to a <span class="title-ref">DataFrame</span> with duplicated column names would cause a stack overflow (`21063`)
  - Bug in <span class="title-ref">.GroupBy.ffill</span> and <span class="title-ref">.GroupBy.bfill</span> where the fill within a grouping would not always be applied as intended due to the implementations' use of a non-stable sort (`21207`)
  - Bug in <span class="title-ref">.GroupBy.rank</span> where results did not scale to 100% when specifying `method='dense'` and `pct=True`
  - Bug in <span class="title-ref">pandas.DataFrame.rolling</span> and <span class="title-ref">pandas.Series.rolling</span> which incorrectly accepted a 0 window size rather than raising (`21286`)

**Data-type specific**

  - Bug in <span class="title-ref">Series.str.replace</span> where the method throws `TypeError` on Python 3.5.2 (`21078`)
  - Bug in <span class="title-ref">Timedelta</span> where passing a float with a unit would prematurely round the float precision (`14156`)
  - Bug in <span class="title-ref">pandas.testing.assert\_index\_equal</span> which raised `AssertionError` incorrectly, when comparing two <span class="title-ref">CategoricalIndex</span> objects with param `check_categorical=False` (`19776`)

**Sparse**

  - Bug in <span class="title-ref">SparseArray.shape</span> which previously only returned the shape <span class="title-ref">SparseArray.sp\_values</span> (`21126`)

**Indexing**

  - Bug in <span class="title-ref">Series.reset\_index</span> where appropriate error was not raised with an invalid level name (`20925`)
  - Bug in <span class="title-ref">interval\_range</span> when `start`/`periods` or `end`/`periods` are specified with float `start` or `end` (`21161`)
  - Bug in <span class="title-ref">MultiIndex.set\_names</span> where error raised for a `MultiIndex` with `nlevels == 1` (`21149`)
  - Bug in <span class="title-ref">IntervalIndex</span> constructors where creating an `IntervalIndex` from categorical data was not fully supported (`21243`, `21253`)
  - Bug in <span class="title-ref">MultiIndex.sort\_index</span> which was not guaranteed to sort correctly with `level=1`; this was also causing data misalignment in particular <span class="title-ref">DataFrame.stack</span> operations (`20994`, `20945`, `21052`)

**Plotting**

  - New keywords (sharex, sharey) to turn on/off sharing of x/y-axis by subplots generated with pandas.DataFrame().groupby().boxplot() (`20968`)

**I/O**

  - Bug in IO methods specifying `compression='zip'` which produced uncompressed zip archives (`17778`, `21144`)
  - Bug in <span class="title-ref">DataFrame.to\_stata</span> which prevented exporting DataFrames to buffers and most file-like objects (`21041`)
  - Bug in <span class="title-ref">read\_stata</span> and <span class="title-ref">StataReader</span> which did not correctly decode utf-8 strings on Python 3 from Stata 14 files (dta version 118) (`21244`)
  - Bug in IO JSON <span class="title-ref">read\_json</span> reading empty JSON schema with `orient='table'` back to <span class="title-ref">DataFrame</span> caused an error (`21287`)

**Reshaping**

  - Bug in <span class="title-ref">concat</span> where error was raised in concatenating <span class="title-ref">Series</span> with numpy scalar and tuple names (`21015`)
  - Bug in <span class="title-ref">concat</span> warning message providing the wrong guidance for future behavior (`21101`)

**Other**

  - Tab completion on <span class="title-ref">Index</span> in IPython no longer outputs deprecation warnings (`21125`)
  - Bug preventing pandas being used on Windows without C++ redistributable installed (`21106`)

## Contributors

<div class="contributors">

v0.23.0..v0.23.1

</div>

---

v0.23.2.md

---

# What's new in 0.23.2 (July 5, 2018)

{{ header }}

This is a minor bug-fix release in the 0.23.x series and includes some small regression fixes and bug fixes. We recommend that all users upgrade to this version.

\> **Note** \> pandas 0.23.2 is first pandas release that's compatible with Python 3.7 (`20552`)

\> **Warning** \> Starting January 1, 2019, pandas feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.23.2

</div>

## Logical reductions over entire DataFrame

<span class="title-ref">DataFrame.all</span> and <span class="title-ref">DataFrame.any</span> now accept `axis=None` to reduce over all axes to a scalar (`19976`)

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 2\], "B": \[True, False\]}) df.all(axis=None)

</div>

This also provides compatibility with NumPy 1.15, which now dispatches to `DataFrame.all`. With NumPy 1.15 and pandas 0.23.1 or earlier, <span class="title-ref">numpy.all</span> will no longer reduce over every axis:

`` `python    >>> # NumPy 1.15, pandas 0.23.1    >>> np.any(pd.DataFrame({"A": [False], "B": [False]}))    A    False    B    False    dtype: bool  With pandas 0.23.2, that will correctly return False, as it did with NumPy < 1.15.  .. ipython:: python     np.any(pd.DataFrame({"A": [False], "B": [False]}))   .. _whatsnew_0232.fixed_regressions:  Fixed regressions ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

  - Fixed regression in <span class="title-ref">to\_csv</span> when handling file-like object incorrectly (`21471`)
  - Re-allowed duplicate level names of a `MultiIndex`. Accessing a level that has a duplicate name by name still raises an error (`19029`).
  - Bug in both <span class="title-ref">DataFrame.first\_valid\_index</span> and <span class="title-ref">Series.first\_valid\_index</span> raised for a row index having duplicate values (`21441`)
  - Fixed printing of DataFrames with hierarchical columns with long names (`21180`)
  - Fixed regression in <span class="title-ref">\~DataFrame.reindex</span> and <span class="title-ref">\~DataFrame.groupby</span> with a MultiIndex or multiple keys that contains categorical datetime-like values (`21390`).
  - Fixed regression in unary negative operations with object dtype (`21380`)
  - Bug in <span class="title-ref">Timestamp.ceil</span> and <span class="title-ref">Timestamp.floor</span> when timestamp is a multiple of the rounding frequency (`21262`)
  - Fixed regression in <span class="title-ref">to\_clipboard</span> that defaulted to copying dataframes with space delimited instead of tab delimited (`21104`)

## Build changes

  - The source and binary distributions no longer include test data files, resulting in smaller download sizes. Tests relying on these data files will be skipped when using `pandas.test()`. (`19320`)

## Bug fixes

**Conversion**

  - Bug in constructing <span class="title-ref">Index</span> with an iterator or generator (`21470`)
  - Bug in <span class="title-ref">Series.nlargest</span> for signed and unsigned integer dtypes when the minimum value is present (`21426`)

**Indexing**

  - Bug in <span class="title-ref">Index.get\_indexer\_non\_unique</span> with categorical key (`21448`)
  - Bug in comparison operations for <span class="title-ref">MultiIndex</span> where error was raised on equality / inequality comparison involving a MultiIndex with `nlevels == 1` (`21149`)
  - Bug in <span class="title-ref">DataFrame.drop</span> behaviour is not consistent for unique and non-unique indexes (`21494`)
  - Bug in <span class="title-ref">DataFrame.duplicated</span> with a large number of columns causing a 'maximum recursion depth exceeded' (`21524`).

**I/O**

  - Bug in <span class="title-ref">read\_csv</span> that caused it to incorrectly raise an error when `nrows=0`, `low_memory=True`, and `index_col` was not `None` (`21141`)
  - Bug in <span class="title-ref">json\_normalize</span> when formatting the `record_prefix` with integer columns (`21536`)

**Categorical**

  - Bug in rendering <span class="title-ref">Series</span> with `Categorical` dtype in rare conditions under Python 2.7 (`21002`)

**Timezones**

  - Bug in <span class="title-ref">Timestamp</span> and <span class="title-ref">DatetimeIndex</span> where passing a <span class="title-ref">Timestamp</span> localized after a DST transition would return a datetime before the DST transition (`20854`)
  - Bug in comparing <span class="title-ref">DataFrame</span> with tz-aware <span class="title-ref">DatetimeIndex</span> columns with a DST transition that raised a `KeyError` (`19970`)
  - Bug in <span class="title-ref">DatetimeIndex.shift</span> where an `AssertionError` would raise when shifting across DST (`8616`)
  - Bug in <span class="title-ref">Timestamp</span> constructor where passing an invalid timezone offset designator (`Z`) would not raise a `ValueError` (`8910`)
  - Bug in <span class="title-ref">Timestamp.replace</span> where replacing at a DST boundary would retain an incorrect offset (`7825`)
  - Bug in <span class="title-ref">DatetimeIndex.reindex</span> when reindexing a tz-naive and tz-aware <span class="title-ref">DatetimeIndex</span> (`8306`)
  - Bug in <span class="title-ref">DatetimeIndex.resample</span> when downsampling across a DST boundary (`8531`)

**Timedelta**

  - Bug in <span class="title-ref">Timedelta</span> where non-zero timedeltas shorter than 1 microsecond were considered False (`21484`)

## Contributors

<div class="contributors">

v0.23.1..v0.23.2

</div>

---

v0.23.3.md

---

# What's new in 0.23.3 (July 7, 2018)

{{ header }}

This release fixes a build issue with the sdist for Python 3.7 (`21785`) There are no other changes.

## Contributors

<div class="contributors">

v0.23.2..v0.23.3

</div>

---

v0.23.4.md

---

# What's new in 0.23.4 (August 3, 2018)

{{ header }}

This is a minor bug-fix release in the 0.23.x series and includes some small regression fixes and bug fixes. We recommend that all users upgrade to this version.

\> **Warning** \> Starting January 1, 2019, pandas feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more.

<div class="contents" data-local="" data-backlinks="none">

What's new in v0.23.4

</div>

## Fixed regressions

  - Python 3.7 with Windows gave all missing values for rolling variance calculations (`21813`)

## Bug fixes

**Groupby/resample/rolling**

  - Bug where calling <span class="title-ref">DataFrameGroupBy.agg</span> with a list of functions including `ohlc` as the non-initial element would raise a `ValueError` (`21716`)
  - Bug in `roll_quantile` caused a memory leak when calling `.rolling(...).quantile(q)` with `q` in (0,1) (`21965`)

**Missing**

  - Bug in <span class="title-ref">Series.clip</span> and <span class="title-ref">DataFrame.clip</span> cannot accept list-like threshold containing `NaN` (`19992`)

## Contributors

<div class="contributors">

v0.23.3..v0.23.4

</div>

---

v0.24.0.md

---

# What's new in 0.24.0 (January 25, 2019)

\> **Warning** \> The 0.24.x series of releases will be the last to support Python 2. Future feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more details.

{{ header }}

This is a major release from 0.23.4 and includes a number of API changes, new features, enhancements, and performance improvements along with a large number of bug fixes.

Highlights include:

  - \[Optional Integer NA Support \<whatsnew\_0240.enhancements.intna\>\](\#optional-integer-na-support-\<whatsnew\_0240.enhancements.intna\>)
  - \[New APIs for accessing the array backing a Series or Index \<whatsnew\_0240.values\_api\>\](\#new-apis-for-accessing-the-array-backing-a-series-or-index-\<whatsnew\_0240.values\_api\>)
  - \[A new top-level method for creating arrays \<whatsnew\_0240.enhancements.array\>\](\#a-new-top-level-method-for-creating-arrays-\<whatsnew\_0240.enhancements.array\>)
  - \[Store Interval and Period data in a Series or DataFrame \<whatsnew\_0240.enhancements.interval\>\](\#store-interval-and-period-data-in-a-series-or-dataframe-\<whatsnew\_0240.enhancements.interval\>)
  - \[Support for joining on two MultiIndexes \<whatsnew\_0240.enhancements.join\_with\_two\_multiindexes\>\](\#support-for-joining-on-two-multiindexes-\<whatsnew\_0240.enhancements.join\_with\_two\_multiindexes\>)

Check the \[API Changes \<whatsnew\_0240.api\_breaking\>\](\#api-changes-\<whatsnew\_0240.api\_breaking\>) and \[deprecations \<whatsnew\_0240.deprecations\>\](\#deprecations-\<whatsnew\_0240.deprecations\>) before updating.

These are the changes in pandas 0.24.0. See \[release\](\#release) for a full changelog including other versions of pandas.

## Enhancements

### Optional integer NA support

pandas has gained the ability to hold integer dtypes with missing values. This long requested feature is enabled through the use of \[extension types \<extending.extension-types\>\](\#extension-types-\<extending.extension-types\>).

\> **Note** \> IntegerArray is currently experimental. Its API or implementation may change without warning.

We can construct a `Series` with the specified dtype. The dtype string `Int64` is a pandas `ExtensionDtype`. Specifying a list or array using the traditional missing value marker of `np.nan` will infer to integer dtype. The display of the `Series` will also use the `NaN` to indicate missing values in string outputs. (`20700`, `20747`, `22441`, `21789`, `22346`)

<div class="ipython">

python

s = pd.Series(\[1, 2, np.nan\], dtype='Int64') s

</div>

Operations on these dtypes will propagate `NaN` as other pandas operations.

<div class="ipython">

python

\# arithmetic s + 1

\# comparison s == 1

\# indexing s.iloc\[1:3\]

\# operate with other dtypes s + s.iloc\[1:3\].astype('Int8')

\# coerce when needed s + 0.01

</div>

These dtypes can operate as part of a `DataFrame`.

<div class="ipython">

python

df = pd.DataFrame({'A': s, 'B': \[1, 1, 3\], 'C': list('aab')}) df df.dtypes

</div>

These dtypes can be merged, reshaped, and casted.

<div class="ipython">

python

pd.concat(\[df\[\['A'\]\], df\[\['B', 'C'\]\]\], axis=1).dtypes df\['A'\].astype(float)

</div>

Reduction and groupby operations such as `sum` work.

<div class="ipython">

python

df.sum() df.groupby('B').A.sum()

</div>

\> **Warning** \> The Integer NA support currently uses the capitalized dtype version, e.g. `Int8` as compared to the traditional `int8`. This may be changed at a future date.

See \[integer\_na\](\#integer\_na) for more.

### Accessing the values in a Series or Index

<span class="title-ref">Series.array</span> and <span class="title-ref">Index.array</span> have been added for extracting the array backing a `Series` or `Index`. (`19954`, `23623`)

<div class="ipython">

python

idx = pd.period\_range('2000', periods=4) idx.array pd.Series(idx).array

</div>

Historically, this would have been done with `series.values`, but with `.values` it was unclear whether the returned value would be the actual array, some transformation of it, or one of pandas custom arrays (like `Categorical`). For example, with <span class="title-ref">PeriodIndex</span>, `.values` generates a new ndarray of period objects each time.

<div class="ipython">

python

idx.values id(idx.values) id(idx.values)

</div>

If you need an actual NumPy array, use <span class="title-ref">Series.to\_numpy</span> or <span class="title-ref">Index.to\_numpy</span>.

<div class="ipython">

python

idx.to\_numpy() pd.Series(idx).to\_numpy()

</div>

For Series and Indexes backed by normal NumPy arrays, <span class="title-ref">Series.array</span> will return a new <span class="title-ref">arrays.PandasArray</span>, which is a thin (no-copy) wrapper around a <span class="title-ref">numpy.ndarray</span>. <span class="title-ref">\~arrays.PandasArray</span> isn't especially useful on its own, but it does provide the same interface as any extension array defined in pandas or by a third-party library.

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3\]) ser.array ser.to\_numpy()

</div>

We haven't removed or deprecated <span class="title-ref">Series.values</span> or <span class="title-ref">DataFrame.values</span>, but we highly recommend and using `.array` or `.to_numpy()` instead.

See \[Dtypes \<basics.dtypes\>\](\#dtypes-\<basics.dtypes\>) and \[Attributes and Underlying Data \<basics.attrs\>\](\#attributes-and-underlying-data-\<basics.attrs\>) for more.

### `pandas.array`: a new top-level method for creating arrays

A new top-level method <span class="title-ref">array</span> has been added for creating 1-dimensional arrays (`22860`). This can be used to create any \[extension array \<extending.extension-types\>\](\#extension-array-\<extending.extension-types\>), including extension arrays registered by [3rd party libraries](https://pandas.pydata.org/community/ecosystem.html). See the \[dtypes docs \<basics.dtypes\>\](\#dtypes-docs-\<basics.dtypes\>) for more on extension arrays.

<div class="ipython">

python

pd.array(\[1, 2, np.nan\], dtype='Int64') pd.array(\['a', 'b', 'c'\], dtype='category')

</div>

Passing data for which there isn't dedicated extension type (e.g. float, integer, etc.) will return a new <span class="title-ref">arrays.PandasArray</span>, which is just a thin (no-copy) wrapper around a <span class="title-ref">numpy.ndarray</span> that satisfies the pandas extension array interface.

<div class="ipython">

python

pd.array(\[1, 2, 3\])

</div>

On their own, a <span class="title-ref">\~arrays.PandasArray</span> isn't a very useful object. But if you need write low-level code that works generically for any <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span>, <span class="title-ref">\~arrays.PandasArray</span> satisfies that need.

Notice that by default, if no `dtype` is specified, the dtype of the returned array is inferred from the data. In particular, note that the first example of `[1, 2, np.nan]` would have returned a floating-point array, since `NaN` is a float.

<div class="ipython">

python

pd.array(\[1, 2, np.nan\])

</div>

### Storing Interval and Period data in Series and DataFrame

<span class="title-ref">Interval</span> and <span class="title-ref">Period</span> data may now be stored in a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span>, in addition to an <span class="title-ref">IntervalIndex</span> and <span class="title-ref">PeriodIndex</span> like previously (`19453`, `22862`).

<div class="ipython">

python

ser = pd.Series(pd.interval\_range(0, 5)) ser ser.dtype

</div>

For periods:

<div class="ipython">

python

pser = pd.Series(pd.period\_range("2000", freq="D", periods=5)) pser pser.dtype

</div>

Previously, these would be cast to a NumPy array with object dtype. In general, this should result in better performance when storing an array of intervals or periods in a <span class="title-ref">Series</span> or column of a <span class="title-ref">DataFrame</span>.

Use <span class="title-ref">Series.array</span> to extract the underlying array of intervals or periods from the `Series`:

<div class="ipython">

python

ser.array pser.array

</div>

These return an instance of <span class="title-ref">arrays.IntervalArray</span> or <span class="title-ref">arrays.PeriodArray</span>, the new extension arrays that back interval and period data.

\> **Warning** \> For backwards compatibility, <span class="title-ref">Series.values</span> continues to return a NumPy array of objects for Interval and Period data. We recommend using <span class="title-ref">Series.array</span> when you need the array of data stored in the `Series`, and <span class="title-ref">Series.to\_numpy</span> when you know you need a NumPy array.

> See \[Dtypes \<basics.dtypes\>\](\#dtypes-\<basics.dtypes\>) and \[Attributes and Underlying Data \<basics.attrs\>\](\#attributes-and-underlying-data-\<basics.attrs\>) for more.

### Joining with two multi-indexes

<span class="title-ref">DataFrame.merge</span> and <span class="title-ref">DataFrame.join</span> can now be used to join multi-indexed `Dataframe` instances on the overlapping index levels (`6360`)

See the \[Merge, join, and concatenate \<merging.Join\_with\_two\_multi\_indexes\>\](\#merge,-join,-and-concatenate \<merging.join\_with\_two\_multi\_indexes\>) documentation section.

<div class="ipython">

python

  - index\_left = pd.MultiIndex.from\_tuples(\[('K0', 'X0'), ('K0', 'X1'),  
    ('K1', 'X2')\], names=\['key', 'X'\])

  - left = pd.DataFrame({'A': \['A0', 'A1', 'A2'\],  
    'B': \['B0', 'B1', 'B2'\]}, index=index\_left)

  - index\_right = pd.MultiIndex.from\_tuples(\[('K0', 'Y0'), ('K1', 'Y1'),  
    ('K2', 'Y2'), ('K2', 'Y3')\], names=\['key', 'Y'\])

  - right = pd.DataFrame({'C': \['C0', 'C1', 'C2', 'C3'\],  
    'D': \['D0', 'D1', 'D2', 'D3'\]}, index=index\_right)

left.join(right)

</div>

For earlier versions this can be done using the following.

<div class="ipython">

python

  - pd.merge(left.reset\_index(), right.reset\_index(),  
    on=\['key'\], how='inner').set\_index(\['key', 'X', 'Y'\])

</div>

### Function `read_html` enhancements

<span class="title-ref">read\_html</span> previously ignored `colspan` and `rowspan` attributes. Now it understands them, treating them as sequences of cells with the same value. (`17054`)

<div class="ipython">

python

from io import StringIO result = pd.read\_html(StringIO(""" \<table\> \<thead\> \<tr\> \<th\>A\</th\>\<th\>B\</th\>\<th\>C\</th\> \</tr\> \</thead\> \<tbody\> \<tr\> \<td colspan="2"\>1\</td\>\<td\>2\</td\> \</tr\> \</tbody\> \</table\>"""))

</div>

*Previous behavior*:

`` `ipython     In [13]: result     Out [13]:     [   A  B   C      0  1  2 NaN]  *New behavior*:  .. ipython:: python      result   New ``Styler.pipe()`method`<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The </span>\~pandas.io.formats.style.Styler\` class has gained a <span class="title-ref">\~pandas.io.formats.style.Styler.pipe</span> method. This provides a convenient way to apply users' predefined styling functions, and can help reduce "boilerplate" when using DataFrame styling functionality repeatedly within a notebook. (`23229`)

<div class="ipython">

python

df = pd.DataFrame({'N': \[1250, 1500, 1750\], 'X': \[0.25, 0.35, 0.50\]})

  - def format\_and\_align(styler):
    
      - return (styler.format({'N': '{:,}', 'X': '{:.1%}'})  
        .set\_properties(\*\*{'text-align': 'right'}))

df.style.pipe(format\_and\_align).set\_caption('Summary of results.')

</div>

Similar methods already exist for other classes in pandas, including <span class="title-ref">DataFrame.pipe</span>, <span class="title-ref">GroupBy.pipe() \<.GroupBy.pipe\></span>, and <span class="title-ref">Resampler.pipe() \<.Resampler.pipe\></span>.

### Renaming names in a MultiIndex

<span class="title-ref">DataFrame.rename\_axis</span> now supports `index` and `columns` arguments and <span class="title-ref">Series.rename\_axis</span> supports `index` argument (`19978`).

This change allows a dictionary to be passed so that some of the names of a `MultiIndex` can be changed.

Example:

<div class="ipython">

python

  - mi = pd.MultiIndex.from\_product(\[list('AB'), list('CD'), list('EF')\],  
    names=\['AB', 'CD', 'EF'\])

df = pd.DataFrame(list(range(len(mi))), index=mi, columns=\['N'\]) df df.rename\_axis(index={'CD': 'New'})

</div>

See the \[Advanced documentation on renaming\<advanced.index\_names\>\](\#advanced-documentation-on-renaming\<advanced.index\_names\>) for more details.

### Other enhancements

  - <span class="title-ref">merge</span> now directly allows merge between objects of type `DataFrame` and named `Series`, without the need to convert the `Series` object into a `DataFrame` beforehand (`21220`)
  - `ExcelWriter` now accepts `mode` as a keyword argument, enabling append to existing workbooks when using the `openpyxl` engine (`3441`)
  - `FrozenList` has gained the `.union()` and `.difference()` methods. This functionality greatly simplifies groupby's that rely on explicitly excluding certain columns. See \[Splitting an object into groups \<groupby.split\>\](\#splitting-an-object-into-groups-\<groupby.split\>) for more information (`15475`, `15506`).
  - <span class="title-ref">DataFrame.to\_parquet</span> now accepts `index` as an argument, allowing the user to override the engine's default behavior to include or omit the dataframe's indexes from the resulting Parquet file. (`20768`)
  - <span class="title-ref">read\_feather</span> now accepts `columns` as an argument, allowing the user to specify which columns should be read. (`24025`)
  - <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">Series.corr</span> now accept a callable for generic calculation methods of correlation, e.g. histogram intersection (`22684`)
  - <span class="title-ref">DataFrame.to\_string</span> now accepts `decimal` as an argument, allowing the user to specify which decimal separator should be used in the output. (`23614`)
  - <span class="title-ref">DataFrame.to\_html</span> now accepts `render_links` as an argument, allowing the user to generate HTML with links to any URLs that appear in the DataFrame. See the \[section on writing HTML \<io.html\>\](\#section-on-writing-html-\<io.html\>) in the IO docs for example usage. (`2679`)
  - <span class="title-ref">pandas.read\_csv</span> now supports pandas extension types as an argument to `dtype`, allowing the user to use pandas extension types when reading CSVs. (`23228`)
  - The <span class="title-ref">\~DataFrame.shift</span> method now accepts `fill_value` as an argument, allowing the user to specify a value which will be used instead of NA/NaT in the empty periods. (`15486`)
  - <span class="title-ref">to\_datetime</span> now supports the `%Z` and `%z` directive when passed into `format` (`13486`)
  - <span class="title-ref">Series.mode</span> and <span class="title-ref">DataFrame.mode</span> now support the `dropna` parameter which can be used to specify whether `NaN`/`NaT` values should be considered (`17534`)
  - <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span> now support the `compression` keyword when a file handle is passed. (`21227`)
  - <span class="title-ref">Index.droplevel</span> is now implemented also for flat indexes, for compatibility with <span class="title-ref">MultiIndex</span> (`21115`)
  - <span class="title-ref">Series.droplevel</span> and <span class="title-ref">DataFrame.droplevel</span> are now implemented (`20342`)
  - Added support for reading from/writing to Google Cloud Storage via the `gcsfs` library (`19454`, `23094`)
  - <span class="title-ref">DataFrame.to\_gbq</span> and <span class="title-ref">read\_gbq</span> signature and documentation updated to reflect changes from the [pandas-gbq library version 0.8.0](https://pandas-gbq.readthedocs.io/en/latest/changelog.html#changelog-0-8-0). Adds a `credentials` argument, which enables the use of any kind of [google-auth credentials](https://google-auth.readthedocs.io/en/latest/). (`21627`, `22557`, `23662`)
  - New method <span class="title-ref">HDFStore.walk</span> will recursively walk the group hierarchy of an HDF5 file (`10932`)
  - <span class="title-ref">read\_html</span> copies cell data across `colspan` and `rowspan`, and it treats all-`th` table rows as headers if `header` kwarg is not given and there is no `thead` (`17054`)
  - <span class="title-ref">Series.nlargest</span>, <span class="title-ref">Series.nsmallest</span>, <span class="title-ref">DataFrame.nlargest</span>, and <span class="title-ref">DataFrame.nsmallest</span> now accept the value `"all"` for the `keep` argument. This keeps all ties for the nth largest/smallest value (`16818`)
  - <span class="title-ref">IntervalIndex</span> has gained the <span class="title-ref">\~IntervalIndex.set\_closed</span> method to change the existing `closed` value (`21670`)
  - <span class="title-ref">\~DataFrame.to\_csv</span>, <span class="title-ref">\~Series.to\_csv</span>, <span class="title-ref">\~DataFrame.to\_json</span>, and <span class="title-ref">\~Series.to\_json</span> now support `compression='infer'` to infer compression based on filename extension (`15008`). The default compression for `to_csv`, `to_json`, and `to_pickle` methods has been updated to `'infer'` (`22004`).
  - <span class="title-ref">DataFrame.to\_sql</span> now supports writing `TIMESTAMP WITH TIME ZONE` types for supported databases. For databases that don't support timezones, datetime data will be stored as timezone unaware local timestamps. See the \[io.sql\_datetime\_data\](\#io.sql\_datetime\_data) for implications (`9086`).
  - <span class="title-ref">to\_timedelta</span> now supports iso-formatted timedelta strings (`21877`)
  - <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> now support <span class="title-ref">Iterable</span> objects in the constructor (`2193`)
  - <span class="title-ref">DatetimeIndex</span> has gained the <span class="title-ref">DatetimeIndex.timetz</span> attribute. This returns the local time with timezone information. (`21358`)
  - <span class="title-ref">\~Timestamp.round</span>, <span class="title-ref">\~Timestamp.ceil</span>, and <span class="title-ref">\~Timestamp.floor</span> for <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">Timestamp</span> now support an `ambiguous` argument for handling datetimes that are rounded to ambiguous times (`18946`) and a `nonexistent` argument for handling datetimes that are rounded to nonexistent times. See \[timeseries.timezone\_nonexistent\](\#timeseries.timezone\_nonexistent) (`22647`)
  - The result of <span class="title-ref">\~DataFrame.resample</span> is now iterable similar to `groupby()` (`15314`).
  - <span class="title-ref">Series.resample</span> and <span class="title-ref">DataFrame.resample</span> have gained the <span class="title-ref">.Resampler.quantile</span> (`15023`).
  - <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">Series.resample</span> with a <span class="title-ref">PeriodIndex</span> will now respect the `base` argument in the same fashion as with a <span class="title-ref">DatetimeIndex</span>. (`23882`)
  - <span class="title-ref">pandas.api.types.is\_list\_like</span> has gained a keyword `allow_sets` which is `True` by default; if `False`, all instances of `set` will not be considered "list-like" anymore (`23061`)
  - <span class="title-ref">Index.to\_frame</span> now supports overriding column name(s) (`22580`).
  - <span class="title-ref">Categorical.from\_codes</span> now can take a `dtype` parameter as an alternative to passing `categories` and `ordered` (`24398`).
  - New attribute `__git_version__` will return git commit sha of current build (`21295`).
  - Compatibility with Matplotlib 3.0 (`22790`).
  - Added <span class="title-ref">Interval.overlaps</span>, <span class="title-ref">arrays.IntervalArray.overlaps</span>, and <span class="title-ref">IntervalIndex.overlaps</span> for determining overlaps between interval-like objects (`21998`)
  - <span class="title-ref">read\_fwf</span> now accepts keyword `infer_nrows` (`15138`).
  - <span class="title-ref">\~DataFrame.to\_parquet</span> now supports writing a `DataFrame` as a directory of parquet files partitioned by a subset of the columns when `engine = 'pyarrow'` (`23283`)
  - <span class="title-ref">Timestamp.tz\_localize</span>, <span class="title-ref">DatetimeIndex.tz\_localize</span>, and <span class="title-ref">Series.tz\_localize</span> have gained the `nonexistent` argument for alternative handling of nonexistent times. See \[timeseries.timezone\_nonexistent\](\#timeseries.timezone\_nonexistent) (`8917`, `24466`)
  - <span class="title-ref">Index.difference</span>, <span class="title-ref">Index.intersection</span>, <span class="title-ref">Index.union</span>, and <span class="title-ref">Index.symmetric\_difference</span> now have an optional `sort` parameter to control whether the results should be sorted if possible (`17839`, `24471`)
  - <span class="title-ref">read\_excel</span> now accepts `usecols` as a list of column names or callable (`18273`)
  - <span class="title-ref">MultiIndex.to\_flat\_index</span> has been added to flatten multiple levels into a single-level <span class="title-ref">Index</span> object.
  - <span class="title-ref">DataFrame.to\_stata</span> and <span class="title-ref">pandas.io.stata.StataWriter117</span> can write mixed string columns to Stata strl format (`23633`)
  - <span class="title-ref">DataFrame.between\_time</span> and <span class="title-ref">DataFrame.at\_time</span> have gained the `axis` parameter (`8839`)
  - <span class="title-ref">DataFrame.to\_records</span> now accepts `index_dtypes` and `column_dtypes` parameters to allow different data types in stored column and index records (`18146`)
  - <span class="title-ref">IntervalIndex</span> has gained the <span class="title-ref">\~IntervalIndex.is\_overlapping</span> attribute to indicate if the `IntervalIndex` contains any overlapping intervals (`23309`)
  - <span class="title-ref">pandas.DataFrame.to\_sql</span> has gained the `method` argument to control SQL insertion clause. See the \[insertion method \<io.sql.method\>\](\#insertion-method-\<io.sql.method\>) section in the documentation. (`8953`)
  - <span class="title-ref">DataFrame.corrwith</span> now supports Spearman's rank correlation, Kendall's tau as well as callable correlation methods. (`21925`)
  - <span class="title-ref">DataFrame.to\_json</span>, <span class="title-ref">DataFrame.to\_csv</span>, <span class="title-ref">DataFrame.to\_pickle</span>, and other export methods now support tilde(\~) in path argument. (`23473`)

## Backwards incompatible API changes

pandas 0.24.0 includes a number of API breaking changes.

### Increased minimum versions for dependencies

We have updated our minimum supported versions of dependencies (`21242`, `18742`, `23774`, `24767`). If installed, we now require:

<table style="width:65%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.12.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.2.0</td>
<td></td>
</tr>
<tr class="odd">
<td>fastparquet</td>
<td>0.2.1</td>
<td></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>2.0.0</td>
<td></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.6.1</td>
<td></td>
</tr>
<tr class="even">
<td>pandas-gbq</td>
<td>0.8.0</td>
<td></td>
</tr>
<tr class="odd">
<td>pyarrow</td>
<td>0.9.0</td>
<td></td>
</tr>
<tr class="even">
<td>pytables</td>
<td>3.4.2</td>
<td></td>
</tr>
<tr class="odd">
<td>scipy</td>
<td>0.18.1</td>
<td></td>
</tr>
<tr class="even">
<td>xlrd</td>
<td>1.0.0</td>
<td></td>
</tr>
<tr class="odd">
<td>pytest (dev)</td>
<td>3.6</td>
<td></td>
</tr>
</tbody>
</table>

Additionally we no longer depend on `feather-format` for feather based storage and replaced it with references to `pyarrow` (`21639` and `23053`).

### `os.linesep` is used for `line_terminator` of `DataFrame.to_csv`

<span class="title-ref">DataFrame.to\_csv</span> now uses <span class="title-ref">os.linesep</span> rather than `'\n'` for the default line terminator (`20353`). This change only affects when running on Windows, where `'\r\n'` was used for line terminator even when `'\n'` was passed in `line_terminator`.

*Previous behavior* on Windows:

`` `ipython     In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],        ...:                      "string_with_crlf": ["a\r\nbc"]})      In [2]: # When passing file PATH to to_csv,        ...: # line_terminator does not work, and csv is saved with '\r\n'.        ...: # Also, this converts all '\n's in the data to '\r\n'.        ...: data.to_csv("test.csv", index=False, line_terminator='\n')      In [3]: with open("test.csv", mode='rb') as f:        ...:     print(f.read())     Out[3]: b'string_with_lf,string_with_crlf\r\n"a\r\nbc","a\r\r\nbc"\r\n'      In [4]: # When passing file OBJECT with newline option to        ...: # to_csv, line_terminator works.        ...: with open("test2.csv", mode='w', newline='\n') as f:        ...:     data.to_csv(f, index=False, line_terminator='\n')      In [5]: with open("test2.csv", mode='rb') as f:        ...:     print(f.read())     Out[5]: b'string_with_lf,string_with_crlf\n"a\nbc","a\r\nbc"\n'   *New behavior* on Windows:  Passing ``line\_terminator`explicitly, set the`line terminator`to that character.  .. code-block:: ipython     In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],       ...:                      "string_with_crlf": ["a\r\nbc"]})     In [2]: data.to_csv("test.csv", index=False, line_terminator='\n')     In [3]: with open("test.csv", mode='rb') as f:       ...:     print(f.read())    Out[3]: b'string_with_lf,string_with_crlf\n"a\nbc","a\r\nbc"\n'   On Windows, the value of`os.linesep`is`'rn'`, so if`line\_terminator`is not`<span class="title-ref"> set, </span><span class="title-ref">'rn'</span>\` is used for line terminator.

`` `ipython    In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],       ...:                      "string_with_crlf": ["a\r\nbc"]})     In [2]: data.to_csv("test.csv", index=False)     In [3]: with open("test.csv", mode='rb') as f:       ...:     print(f.read())    Out[3]: b'string_with_lf,string_with_crlf\r\n"a\nbc","a\r\nbc"\r\n'   For file objects, specifying ``newline`is not sufficient to set the line terminator.`<span class="title-ref"> You must pass in the </span><span class="title-ref">line\_terminator</span>\` explicitly, even in this case.

`` `ipython    In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],       ...:                      "string_with_crlf": ["a\r\nbc"]})     In [2]: with open("test2.csv", mode='w', newline='\n') as f:       ...:     data.to_csv(f, index=False)     In [3]: with open("test2.csv", mode='rb') as f:       ...:     print(f.read())    Out[3]: b'string_with_lf,string_with_crlf\r\n"a\nbc","a\r\nbc"\r\n'  .. _whatsnew_0240.bug_fixes.nan_with_str_dtype:  Proper handling of ``np.nan`in a string data-typed column with the Python engine`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There was bug in <span class="title-ref">read\_excel</span> and <span class="title-ref">read\_csv</span> with the Python engine, where missing values turned to `'nan'` with `dtype=str` and `na_filter=True`. Now, these missing values are converted to the string missing indicator, `np.nan`. (`20377`)

<div class="ipython" data-suppress="">

python

from io import StringIO

</div>

*Previous behavior*:

`` `ipython    In [5]: data = 'a,b,c\n1,,3\n4,5,6'    In [6]: df = pd.read_csv(StringIO(data), engine='python', dtype=str, na_filter=True)    In [7]: df.loc[0, 'b']    Out[7]:    'nan'  *New behavior*:  .. ipython:: python     data = 'a,b,c\n1,,3\n4,5,6'    df = pd.read_csv(StringIO(data), engine='python', dtype=str, na_filter=True)    df.loc[0, 'b']  Notice how we now instead output ``np.nan`itself instead of a stringified form of it.  .. _whatsnew_0240.api.timezone_offset_parsing:  Parsing datetime strings with timezone offsets`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, parsing datetime strings with UTC offsets with <span class="title-ref">to\_datetime</span> or <span class="title-ref">DatetimeIndex</span> would automatically convert the datetime to UTC without timezone localization. This is inconsistent from parsing the same datetime string with <span class="title-ref">Timestamp</span> which would preserve the UTC offset in the `tz` attribute. Now, <span class="title-ref">to\_datetime</span> preserves the UTC offset in the `tz` attribute when all the datetime strings have the same UTC offset (`17697`, `11736`, `22457`)

*Previous behavior*:

`` `ipython     In [2]: pd.to_datetime("2015-11-18 15:30:00+05:30")     Out[2]: Timestamp('2015-11-18 10:00:00')      In [3]: pd.Timestamp("2015-11-18 15:30:00+05:30")     Out[3]: Timestamp('2015-11-18 15:30:00+0530', tz='pytz.FixedOffset(330)')      # Different UTC offsets would automatically convert the datetimes to UTC (without a UTC timezone)     In [4]: pd.to_datetime(["2015-11-18 15:30:00+05:30", "2015-11-18 16:30:00+06:30"])     Out[4]: DatetimeIndex(['2015-11-18 10:00:00', '2015-11-18 10:00:00'], dtype='datetime64[ns]', freq=None)  *New behavior*:  .. ipython:: python      pd.to_datetime("2015-11-18 15:30:00+05:30")     pd.Timestamp("2015-11-18 15:30:00+05:30")  Parsing datetime strings with the same UTC offset will preserve the UTC offset in the ``tz`.. ipython:: python      pd.to_datetime(["2015-11-18 15:30:00+05:30"] * 2)  Parsing datetime strings with different UTC offsets will now create an Index of`<span class="title-ref"> </span><span class="title-ref">datetime.datetime</span>\` objects with different UTC offsets

`` `ipython     In [59]: idx = pd.to_datetime(["2015-11-18 15:30:00+05:30",                                    "2015-11-18 16:30:00+06:30"])      In[60]: idx     Out[60]: Index([2015-11-18 15:30:00+05:30, 2015-11-18 16:30:00+06:30], dtype='object')      In[61]: idx[0]     Out[61]: Timestamp('2015-11-18 15:30:00+0530', tz='UTC+05:30')      In[62]: idx[1]     Out[62]: Timestamp('2015-11-18 16:30:00+0630', tz='UTC+06:30')  Passing ``utc=True`will mimic the previous behavior but will correctly indicate`\` that the dates have been converted to UTC

<div class="ipython">

python

  - pd.to\_datetime(\["2015-11-18 15:30:00+05:30",  
    "2015-11-18 16:30:00+06:30"\], utc=True)

</div>

### Parsing mixed-timezones with <span class="title-ref">read\_csv</span>

<span class="title-ref">read\_csv</span> no longer silently converts mixed-timezone columns to UTC (`24987`).

*Previous behavior*

`` `python    >>> import io    >>> content = """\    ... a    ... 2000-01-01T00:00:00+05:00    ... 2000-01-01T00:00:00+06:00"""    >>> df = pd.read_csv(io.StringIO(content), parse_dates=['a'])    >>> df.a    0   1999-12-31 19:00:00    1   1999-12-31 18:00:00    Name: a, dtype: datetime64[ns]  *New behavior*  .. code-block:: ipython     In[64]: import io     In[65]: content = """\       ...: a       ...: 2000-01-01T00:00:00+05:00       ...: 2000-01-01T00:00:00+06:00"""     In[66]: df = pd.read_csv(io.StringIO(content), parse_dates=['a'])     In[67]: df.a    Out[67]:    0   2000-01-01 00:00:00+05:00    1   2000-01-01 00:00:00+06:00    Name: a, Length: 2, dtype: object  As can be seen, the ``dtype`is object; each value in the column is a string.`<span class="title-ref"> To convert the strings to an array of datetimes, the </span><span class="title-ref">date\_parser</span>\` argument

`` `ipython    In [3]: df = pd.read_csv(       ...:     io.StringIO(content),       ...:     parse_dates=['a'],       ...:     date_parser=lambda col: pd.to_datetime(col, utc=True),       ...: )     In [4]: df.a    Out[4]:    0   1999-12-31 19:00:00+00:00    1   1999-12-31 18:00:00+00:00    Name: a, dtype: datetime64[ns, UTC]  See [whatsnew_0240.api.timezone_offset_parsing](#whatsnew_0240.api.timezone_offset_parsing) for more.  .. _whatsnew_0240.api_breaking.period_end_time:  Time values in ``dt.end\_time`and`to\_timestamp(how='end')`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The time values in <span class="title-ref">Period</span> and <span class="title-ref">PeriodIndex</span> objects are now set to '23:59:59.999999999' when calling <span class="title-ref">Series.dt.end\_time</span>, <span class="title-ref">Period.end\_time</span>, <span class="title-ref">PeriodIndex.end\_time</span>, <span class="title-ref">Period.to\_timestamp</span> with `how='end'`, or <span class="title-ref">PeriodIndex.to\_timestamp</span> with `how='end'` (`17157`)

*Previous behavior*:

`` `ipython    In [2]: p = pd.Period('2017-01-01', 'D')    In [3]: pi = pd.PeriodIndex([p])     In [4]: pd.Series(pi).dt.end_time[0]    Out[4]: Timestamp(2017-01-01 00:00:00)     In [5]: p.end_time    Out[5]: Timestamp(2017-01-01 23:59:59.999999999)  *New behavior*:  Calling `Series.dt.end_time` will now result in a time of '23:59:59.999999999' as ``<span class="title-ref"> is the case with \`Period.end\_time</span>, for example

<div class="ipython">

python

p = pd.Period('2017-01-01', 'D') pi = pd.PeriodIndex(\[p\])

pd.Series(pi).dt.end\_time\[0\]

p.end\_time

</div>

### Series.unique for timezone-aware data

The return type of <span class="title-ref">Series.unique</span> for datetime with timezone values has changed from an <span class="title-ref">numpy.ndarray</span> of <span class="title-ref">Timestamp</span> objects to a <span class="title-ref">arrays.DatetimeArray</span> (`24024`).

<div class="ipython">

python

  - ser = pd.Series(\[pd.Timestamp('2000', tz='UTC'),  
    pd.Timestamp('2000', tz='UTC')\])

</div>

*Previous behavior*:

`` `ipython    In [3]: ser.unique()    Out[3]: array([Timestamp('2000-01-01 00:00:00+0000', tz='UTC')], dtype=object)   *New behavior*:  .. ipython:: python     ser.unique()   .. _whatsnew_0240.api_breaking.sparse_values:  Sparse data structure refactor ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

`SparseArray`, the array backing `SparseSeries` and the columns in a `SparseDataFrame`, is now an extension array (`21978`, `19056`, `22835`). To conform to this interface and for consistency with the rest of pandas, some API breaking changes were made:

  - `SparseArray` is no longer a subclass of <span class="title-ref">numpy.ndarray</span>. To convert a `SparseArray` to a NumPy array, use <span class="title-ref">numpy.asarray</span>.
  - `SparseArray.dtype` and `SparseSeries.dtype` are now instances of <span class="title-ref">SparseDtype</span>, rather than `np.dtype`. Access the underlying dtype with `SparseDtype.subtype`.
  - `numpy.asarray(sparse_array)` now returns a dense array with all the values, not just the non-fill-value values (`14167`)
  - `SparseArray.take` now matches the API of <span class="title-ref">pandas.api.extensions.ExtensionArray.take</span> (`19506`):
      - The default value of `allow_fill` has changed from `False` to `True`.
      - The `out` and `mode` parameters are now longer accepted (previously, this raised if they were specified).
      - Passing a scalar for `indices` is no longer allowed.
  - The result of <span class="title-ref">concat</span> with a mix of sparse and dense Series is a Series with sparse values, rather than a `SparseSeries`.
  - `SparseDataFrame.combine` and `DataFrame.combine_first` no longer supports combining a sparse column with a dense column while preserving the sparse subtype. The result will be an object-dtype SparseArray.
  - Setting <span class="title-ref">SparseArray.fill\_value</span> to a fill value with a different dtype is now allowed.
  - `DataFrame[column]` is now a <span class="title-ref">Series</span> with sparse values, rather than a <span class="title-ref">SparseSeries</span>, when slicing a single column with sparse values (`23559`).
  - The result of <span class="title-ref">Series.where</span> is now a `Series` with sparse values, like with other extension arrays (`24077`)

Some new warnings are issued for operations that require or are likely to materialize a large dense array:

  - A <span class="title-ref">errors.PerformanceWarning</span> is issued when using fillna with a `method`, as a dense array is constructed to create the filled array. Filling with a `value` is the efficient way to fill a sparse array.
  - A <span class="title-ref">errors.PerformanceWarning</span> is now issued when concatenating sparse Series with differing fill values. The fill value from the first sparse array continues to be used.

In addition to these API breaking changes, many \[Performance Improvements and Bug Fixes have been made \<whatsnew\_0240.bug\_fixes.sparse\>\](\#performance-improvements-and-bug-fixes-have-been-made-\<whatsnew\_0240.bug\_fixes.sparse\>).

Finally, a `Series.sparse` accessor was added to provide sparse-specific methods like <span class="title-ref">Series.sparse.from\_coo</span>.

<div class="ipython">

python

s = pd.Series(\[0, 0, 1, 1, 1\], dtype='Sparse\[int\]') s.sparse.density

</div>

### <span class="title-ref">get\_dummies</span> always returns a DataFrame

Previously, when `sparse=True` was passed to <span class="title-ref">get\_dummies</span>, the return value could be either a <span class="title-ref">DataFrame</span> or a <span class="title-ref">SparseDataFrame</span>, depending on whether all or a just a subset of the columns were dummy-encoded. Now, a <span class="title-ref">DataFrame</span> is always returned (`24284`).

*Previous behavior*

The first <span class="title-ref">get\_dummies</span> returns a <span class="title-ref">DataFrame</span> because the column `A` is not dummy encoded. When just `["B", "C"]` are passed to `get_dummies`, then all the columns are dummy-encoded, and a <span class="title-ref">SparseDataFrame</span> was returned.

`` `ipython    In [2]: df = pd.DataFrame({"A": [1, 2], "B": ['a', 'b'], "C": ['a', 'a']})     In [3]: type(pd.get_dummies(df, sparse=True))    Out[3]: pandas.DataFrame     In [4]: type(pd.get_dummies(df[['B', 'C']], sparse=True))    Out[4]: pandas.core.sparse.frame.SparseDataFrame  .. ipython:: python    :suppress:     df = pd.DataFrame({"A": [1, 2], "B": ['a', 'b'], "C": ['a', 'a']})  *New behavior*  Now, the return type is consistently a `DataFrame`.  .. ipython:: python     type(pd.get_dummies(df, sparse=True))    type(pd.get_dummies(df[['B', 'C']], sparse=True))  > **Note** >     There's no difference in memory usage between a `SparseDataFrame`    and a `DataFrame` with sparse values. The memory usage will    be the same as in the previous version of pandas.  .. _whatsnew_0240.api_breaking.frame_to_dict_index_orient:  Raise ValueError in ``DataFrame.to\_dict(orient='index')`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Bug in <span class="title-ref">DataFrame.to\_dict</span> raises `ValueError` when used with `orient='index'` and a non-unique index instead of losing data (`22801`)

<div class="ipython" data-okexcept="">

python

df = pd.DataFrame({'a': \[1, 2\], 'b': \[0.5, 0.75\]}, index=\['A', 'A'\]) df

df.to\_dict(orient='index')

</div>

### Tick DateOffset normalize restrictions

Creating a `Tick` object (<span class="title-ref">Day</span>, <span class="title-ref">Hour</span>, <span class="title-ref">Minute</span>, <span class="title-ref">Second</span>, <span class="title-ref">Milli</span>, <span class="title-ref">Micro</span>, <span class="title-ref">Nano</span>) with `normalize=True` is no longer supported. This prevents unexpected behavior where addition could fail to be monotone or associative. (`21427`)

*Previous behavior*:

`` `ipython    In [2]: ts = pd.Timestamp('2018-06-11 18:01:14')     In [3]: ts    Out[3]: Timestamp('2018-06-11 18:01:14')     In [4]: tic = pd.offsets.Hour(n=2, normalize=True)       ...:     In [5]: tic    Out[5]: <2 * Hours>     In [6]: ts + tic    Out[6]: Timestamp('2018-06-11 00:00:00')     In [7]: ts + tic + tic + tic == ts + (tic + tic + tic)    Out[7]: False  *New behavior*:  .. ipython:: python      ts = pd.Timestamp('2018-06-11 18:01:14')     tic = pd.offsets.Hour(n=2)     ts + tic + tic + tic == ts + (tic + tic + tic)   .. _whatsnew_0240.api.datetimelike:   .. _whatsnew_0240.api.period_subtraction:  Period subtraction ``\` ^^^^^^^^^^^^^^^^^^

Subtraction of a `Period` from another `Period` will give a `DateOffset`. instead of an integer (`21314`)

*Previous behavior*:

`` `ipython     In [2]: june = pd.Period('June 2018')      In [3]: april = pd.Period('April 2018')      In [4]: june - april     Out [4]: 2  *New behavior*:  .. ipython:: python      june = pd.Period('June 2018')     april = pd.Period('April 2018')     june - april  Similarly, subtraction of a ``Period`from a`PeriodIndex`will now return`<span class="title-ref"> an </span><span class="title-ref">Index</span><span class="title-ref"> of </span><span class="title-ref">DateOffset</span><span class="title-ref"> objects instead of an </span><span class="title-ref">Int64Index</span>\`

*Previous behavior*:

`` `ipython     In [2]: pi = pd.period_range('June 2018', freq='M', periods=3)      In [3]: pi - pi[0]     Out[3]: Int64Index([0, 1, 2], dtype='int64')  *New behavior*:  .. ipython:: python      pi = pd.period_range('June 2018', freq='M', periods=3)     pi - pi[0]   .. _whatsnew_0240.api.timedelta64_subtract_nan:  Addition/subtraction of ``NaN`` from `DataFrame` ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Adding or subtracting `NaN` from a <span class="title-ref">DataFrame</span> column with `timedelta64[ns]` dtype will now raise a `TypeError` instead of returning all-`NaT`. This is for compatibility with `TimedeltaIndex` and `Series` behavior (`22163`)

<div class="ipython">

python

df = pd.DataFrame(\[pd.Timedelta(days=1)\]) df

</div>

*Previous behavior*:

`` `ipython     In [4]: df = pd.DataFrame([pd.Timedelta(days=1)])      In [5]: df - np.nan     Out[5]:         0     0 NaT  *New behavior*:  .. code-block:: ipython      In [2]: df - np.nan     ...     TypeError: unsupported operand type(s) for -: 'TimedeltaIndex' and 'float'  .. _whatsnew_0240.api.dataframe_cmp_broadcasting:  DataFrame comparison operations broadcasting changes ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Previously, the broadcasting behavior of \`DataFrame</span> comparison operations (`==`, `!=`, ...) was inconsistent with the behavior of arithmetic operations (`+`, `-`, ...). The behavior of the comparison operations has been changed to match the arithmetic operations in these cases. (`22880`)

The affected cases are:

  - operating against a 2-dimensional `np.ndarray` with either 1 row or 1 column will now broadcast the same way a `np.ndarray` would (`23000`).
  - a list or tuple with length matching the number of rows in the <span class="title-ref">DataFrame</span> will now raise `ValueError` instead of operating column-by-column (`22880`.
  - a list or tuple with length matching the number of columns in the <span class="title-ref">DataFrame</span> will now operate row-by-row instead of raising `ValueError` (`22880`).

<div class="ipython">

python

arr = np.arange(6).reshape(3, 2) df = pd.DataFrame(arr) df

</div>

*Previous behavior*:

`` `ipython    In [5]: df == arr[[0], :]        ...: # comparison previously broadcast where arithmetic would raise    Out[5]:           0      1    0   True   True    1  False  False    2  False  False    In [6]: df + arr[[0], :]    ...    ValueError: Unable to coerce to DataFrame, shape must be (3, 2): given (1, 2)     In [7]: df == (1, 2)        ...: # length matches number of columns;        ...: # comparison previously raised where arithmetic would broadcast    ...    ValueError: Invalid broadcasting comparison [(1, 2)] with block values    In [8]: df + (1, 2)    Out[8]:       0  1    0  1  3    1  3  5    2  5  7     In [9]: df == (1, 2, 3)        ...:  # length matches number of rows        ...:  # comparison previously broadcast where arithmetic would raise    Out[9]:           0      1    0  False   True    1   True  False    2  False  False    In [10]: df + (1, 2, 3)    ...    ValueError: Unable to coerce to Series, length must be 2: given 3  *New behavior*:  .. ipython:: python     # Comparison operations and arithmetic operations both broadcast.    df == arr[[0], :]    df + arr[[0], :]  .. ipython:: python     # Comparison operations and arithmetic operations both broadcast.    df == (1, 2)    df + (1, 2)  .. code-block:: ipython     # Comparison operations and arithmetic operations both raise ValueError.    In [6]: df == (1, 2, 3)    ...    ValueError: Unable to coerce to Series, length must be 2: given 3     In [7]: df + (1, 2, 3)    ...    ValueError: Unable to coerce to Series, length must be 2: given 3  .. _whatsnew_0240.api.dataframe_arithmetic_broadcasting:  DataFrame arithmetic operations broadcasting changes ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">DataFrame</span> arithmetic operations when operating with 2-dimensional `np.ndarray` objects now broadcast in the same way as `np.ndarray` broadcast. (`23000`)

<div class="ipython">

python

arr = np.arange(6).reshape(3, 2) df = pd.DataFrame(arr) df

</div>

*Previous behavior*:

`` `ipython    In [5]: df + arr[[0], :]   # 1 row, 2 columns    ...    ValueError: Unable to coerce to DataFrame, shape must be (3, 2): given (1, 2)    In [6]: df + arr[:, [1]]   # 1 column, 3 rows    ...    ValueError: Unable to coerce to DataFrame, shape must be (3, 2): given (3, 1)  *New behavior*:  .. ipython:: python     df + arr[[0], :]   # 1 row, 2 columns    df + arr[:, [1]]   # 1 column, 3 rows  .. _whatsnew_0240.api.incompatibilities:  Series and Index data-dtype incompatibilities ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

`Series` and `Index` constructors now raise when the data is incompatible with a passed `dtype=` (`15832`)

*Previous behavior*:

`` `ipython     In [4]: pd.Series([-1], dtype="uint64")     Out [4]:     0    18446744073709551615     dtype: uint64  *New behavior*:  .. code-block:: ipython      In [4]: pd.Series([-1], dtype="uint64")     Out [4]:     ...     OverflowError: Trying to coerce negative values to unsigned integers  .. _whatsnew_0240.api.concat_categorical:  Concatenation changes ``\` ^^^^^^^^^^^^^^^^^^^^^

Calling <span class="title-ref">pandas.concat</span> on a `Categorical` of ints with NA values now causes them to be processed as objects when concatenating with anything other than another `Categorical` of ints (`19214`)

<div class="ipython">

python

s = pd.Series(\[0, 1, np.nan\]) c = pd.Series(\[0, 1, np.nan\], dtype="category")

</div>

*Previous behavior*

`` `ipython     In [3]: pd.concat([s, c])     Out[3]:     0    0.0     1    1.0     2    NaN     0    0.0     1    1.0     2    NaN     dtype: float64  *New behavior*  .. ipython:: python      pd.concat([s, c])  Datetimelike API changes ``\` ^^^^^^^^^^^^^^^^^^^^^^^^

  - For <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span> with non-`None` `freq` attribute, addition or subtraction of integer-dtyped array or `Index` will return an object of the same class (`19959`)
  - <span class="title-ref">DateOffset</span> objects are now immutable. Attempting to alter one of these will now raise `AttributeError` (`21341`)
  - <span class="title-ref">PeriodIndex</span> subtraction of another `PeriodIndex` will now return an object-dtype <span class="title-ref">Index</span> of <span class="title-ref">DateOffset</span> objects instead of raising a `TypeError` (`20049`)
  - <span class="title-ref">cut</span> and <span class="title-ref">qcut</span> now returns a <span class="title-ref">DatetimeIndex</span> or <span class="title-ref">TimedeltaIndex</span> bins when the input is datetime or timedelta dtype respectively and `retbins=True` (`19891`)
  - <span class="title-ref">DatetimeIndex.to\_period</span> and <span class="title-ref">Timestamp.to\_period</span> will issue a warning when timezone information will be lost (`21333`)
  - <span class="title-ref">PeriodIndex.tz\_convert</span> and <span class="title-ref">PeriodIndex.tz\_localize</span> have been removed (`21781`)

### Other API changes

  - A newly constructed empty <span class="title-ref">DataFrame</span> with integer as the `dtype` will now only be cast to `float64` if `index` is specified (`22858`)
  - <span class="title-ref">Series.str.cat</span> will now raise if `others` is a `set` (`23009`)
  - Passing scalar values to <span class="title-ref">DatetimeIndex</span> or <span class="title-ref">TimedeltaIndex</span> will now raise `TypeError` instead of `ValueError` (`23539`)
  - `max_rows` and `max_cols` parameters removed from <span class="title-ref">HTMLFormatter</span> since truncation is handled by <span class="title-ref">DataFrameFormatter</span> (`23818`)
  - <span class="title-ref">read\_csv</span> will now raise a `ValueError` if a column with missing values is declared as having dtype `bool` (`20591`)
  - The column order of the resultant <span class="title-ref">DataFrame</span> from <span class="title-ref">MultiIndex.to\_frame</span> is now guaranteed to match the <span class="title-ref">MultiIndex.names</span> order. (`22420`)
  - Incorrectly passing a <span class="title-ref">DatetimeIndex</span> to <span class="title-ref">MultiIndex.from\_tuples</span>, rather than a sequence of tuples, now raises a `TypeError` rather than a `ValueError` (`24024`)
  - <span class="title-ref">pd.offsets.generate\_range</span> argument `time_rule` has been removed; use `offset` instead (`24157`)
  - In 0.23.x, pandas would raise a `ValueError` on a merge of a numeric column (e.g. `int` dtyped column) and an `object` dtyped column (`9780`). We have re-enabled the ability to merge `object` and other dtypes; pandas will still raise on a merge between a numeric and an `object` dtyped column that is composed only of strings (`21681`)
  - Accessing a level of a `MultiIndex` with a duplicate name (e.g. in <span class="title-ref">\~MultiIndex.get\_level\_values</span>) now raises a `ValueError` instead of a `KeyError` (`21678`).
  - Invalid construction of `IntervalDtype` will now always raise a `TypeError` rather than a `ValueError` if the subdtype is invalid (`21185`)
  - Trying to reindex a `DataFrame` with a non unique `MultiIndex` now raises a `ValueError` instead of an `Exception` (`21770`)
  - <span class="title-ref">Index</span> subtraction will attempt to operate element-wise instead of raising `TypeError` (`19369`)
  - <span class="title-ref">pandas.io.formats.style.Styler</span> supports a `number-format` property when using <span class="title-ref">\~pandas.io.formats.style.Styler.to\_excel</span> (`22015`)
  - <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">Series.corr</span> now raise a `ValueError` along with a helpful error message instead of a `KeyError` when supplied with an invalid method (`22298`)
  - <span class="title-ref">shift</span> will now always return a copy, instead of the previous behaviour of returning self when shifting by 0 (`22397`)
  - <span class="title-ref">DataFrame.set\_index</span> now gives a better (and less frequent) KeyError, raises a `ValueError` for incorrect types, and will not fail on duplicate column names with `drop=True`. (`22484`)
  - Slicing a single row of a DataFrame with multiple ExtensionArrays of the same type now preserves the dtype, rather than coercing to object (`22784`)
  - <span class="title-ref">DateOffset</span> attribute `_cacheable` and method `_should_cache` have been removed (`23118`)
  - <span class="title-ref">Series.searchsorted</span>, when supplied a scalar value to search for, now returns a scalar instead of an array (`23801`).
  - <span class="title-ref">Categorical.searchsorted</span>, when supplied a scalar value to search for, now returns a scalar instead of an array (`23466`).
  - <span class="title-ref">Categorical.searchsorted</span> now raises a `KeyError` rather that a `ValueError`, if a searched for key is not found in its categories (`23466`).
  - <span class="title-ref">Index.hasnans</span> and <span class="title-ref">Series.hasnans</span> now always return a python boolean. Previously, a python or a numpy boolean could be returned, depending on circumstances (`23294`).
  - The order of the arguments of <span class="title-ref">DataFrame.to\_html</span> and <span class="title-ref">DataFrame.to\_string</span> is rearranged to be consistent with each other. (`23614`)
  - <span class="title-ref">CategoricalIndex.reindex</span> now raises a `ValueError` if the target index is non-unique and not equal to the current index. It previously only raised if the target index was not of a categorical dtype (`23963`).
  - <span class="title-ref">Series.to\_list</span> and <span class="title-ref">Index.to\_list</span> are now aliases of `Series.tolist` respectively `Index.tolist` (`8826`)
  - The result of `SparseSeries.unstack` is now a <span class="title-ref">DataFrame</span> with sparse values, rather than a <span class="title-ref">SparseDataFrame</span> (`24372`).
  - <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span> no longer ignore the dtype precision. Passing a non-nanosecond resolution dtype will raise a `ValueError` (`24753`)

## Extension type changes

**Equality and hashability**

pandas now requires that extension dtypes be hashable (i.e. the respective `ExtensionDtype` objects; hashability is not a requirement for the values of the corresponding `ExtensionArray`). The base class implements a default `__eq__` and `__hash__`. If you have a parametrized dtype, you should update the `ExtensionDtype._metadata` tuple to match the signature of your `__init__` method. See <span class="title-ref">pandas.api.extensions.ExtensionDtype</span> for more (`22476`).

**New and changed methods**

  - <span class="title-ref">\~pandas.api.types.ExtensionArray.dropna</span> has been added (`21185`)
  - <span class="title-ref">\~pandas.api.types.ExtensionArray.repeat</span> has been added (`24349`)
  - The `ExtensionArray` constructor, `_from_sequence` now take the keyword arg `copy=False` (`21185`)
  - <span class="title-ref">pandas.api.extensions.ExtensionArray.shift</span> added as part of the basic `ExtensionArray` interface (`22387`).
  - <span class="title-ref">\~pandas.api.types.ExtensionArray.searchsorted</span> has been added (`24350`)
  - Support for reduction operations such as `sum`, `mean` via opt-in base class method override (`22762`)
  - <span class="title-ref">ExtensionArray.isna</span> is allowed to return an `ExtensionArray` (`22325`).

**Dtype changes**

  - `ExtensionDtype` has gained the ability to instantiate from string dtypes, e.g. `decimal` would instantiate a registered `DecimalDtype`; furthermore the `ExtensionDtype` has gained the method `construct_array_type` (`21185`)
  - Added `ExtensionDtype._is_numeric` for controlling whether an extension dtype is considered numeric (`22290`).
  - Added <span class="title-ref">pandas.api.types.register\_extension\_dtype</span> to register an extension type with pandas (`22664`)
  - Updated the `.type` attribute for `PeriodDtype`, `DatetimeTZDtype`, and `IntervalDtype` to be instances of the dtype (`Period`, `Timestamp`, and `Interval` respectively) (`22938`)

<div id="whatsnew_0240.enhancements.extension_array_operators">

**Operator support**

</div>

A `Series` based on an `ExtensionArray` now supports arithmetic and comparison operators (`19577`). There are two approaches for providing operator support for an `ExtensionArray`:

1.  Define each of the operators on your `ExtensionArray` subclass.
2.  Use an operator implementation from pandas that depends on operators that are already defined on the underlying elements (scalars) of the `ExtensionArray`.

See the \[ExtensionArray Operator Support \<extending.extension.operator\>\](\#extensionarray-operator-support \<extending.extension.operator\>) documentation section for details on both ways of adding operator support.

**Other changes**

  - A default repr for <span class="title-ref">pandas.api.extensions.ExtensionArray</span> is now provided (`23601`).
  - <span class="title-ref">ExtensionArray.\_formatting\_values</span> is deprecated. Use <span class="title-ref">ExtensionArray.\_formatter</span> instead. (`23601`)
  - An `ExtensionArray` with a boolean dtype now works correctly as a boolean indexer. <span class="title-ref">pandas.api.types.is\_bool\_dtype</span> now properly considers them boolean (`22326`)

**Bug fixes**

  - Bug in <span class="title-ref">Series.get</span> for `Series` using `ExtensionArray` and integer index (`21257`)
  - <span class="title-ref">\~Series.shift</span> now dispatches to <span class="title-ref">ExtensionArray.shift</span> (`22386`)
  - <span class="title-ref">Series.combine</span> works correctly with <span class="title-ref">\~pandas.api.extensions.ExtensionArray</span> inside of <span class="title-ref">Series</span> (`20825`)
  - <span class="title-ref">Series.combine</span> with scalar argument now works for any function type (`21248`)
  - <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> now dispatch to <span class="title-ref">ExtensionArray.astype</span> (`21185`).
  - Slicing a single row of a `DataFrame` with multiple ExtensionArrays of the same type now preserves the dtype, rather than coercing to object (`22784`)
  - Bug when concatenating multiple `Series` with different extension dtypes not casting to object dtype (`22994`)
  - Series backed by an `ExtensionArray` now work with <span class="title-ref">util.hash\_pandas\_object</span> (`23066`)
  - <span class="title-ref">DataFrame.stack</span> no longer converts to object dtype for DataFrames where each column has the same extension dtype. The output Series will have the same dtype as the columns (`23077`).
  - <span class="title-ref">Series.unstack</span> and <span class="title-ref">DataFrame.unstack</span> no longer convert extension arrays to object-dtype ndarrays. Each column in the output `DataFrame` will now have the same dtype as the input (`23077`).
  - Bug when grouping <span class="title-ref">Dataframe.groupby</span> and aggregating on `ExtensionArray` it was not returning the actual `ExtensionArray` dtype (`23227`).
  - Bug in <span class="title-ref">pandas.merge</span> when merging on an extension array-backed column (`23020`).

## Deprecations

  - <span class="title-ref">MultiIndex.labels</span> has been deprecated and replaced by <span class="title-ref">MultiIndex.codes</span>. The functionality is unchanged. The new name better reflects the natures of these codes and makes the `MultiIndex` API more similar to the API for <span class="title-ref">CategoricalIndex</span> (`13443`). As a consequence, other uses of the name `labels` in `MultiIndex` have also been deprecated and replaced with `codes`:
      - You should initialize a `MultiIndex` instance using a parameter named `codes` rather than `labels`.
      - `MultiIndex.set_labels` has been deprecated in favor of <span class="title-ref">MultiIndex.set\_codes</span>.
      - For method <span class="title-ref">MultiIndex.copy</span>, the `labels` parameter has been deprecated and replaced by a `codes` parameter.
  - <span class="title-ref">DataFrame.to\_stata</span>, <span class="title-ref">read\_stata</span>, <span class="title-ref">StataReader</span> and <span class="title-ref">StataWriter</span> have deprecated the `encoding` argument. The encoding of a Stata dta file is determined by the file type and cannot be changed (`21244`)
  - <span class="title-ref">MultiIndex.to\_hierarchical</span> is deprecated and will be removed in a future version (`21613`)
  - <span class="title-ref">Series.ptp</span> is deprecated. Use `numpy.ptp` instead (`21614`)
  - <span class="title-ref">Series.compress</span> is deprecated. Use `Series[condition]` instead (`18262`)
  - The signature of <span class="title-ref">Series.to\_csv</span> has been uniformed to that of \`DataFrame.to\_csv\`: the name of the first argument is now `path_or_buf`, the order of subsequent arguments has changed, the `header` argument now defaults to `True`. (`19715`)
  - <span class="title-ref">Categorical.from\_codes</span> has deprecated providing float values for the `codes` argument. (`21767`)
  - <span class="title-ref">pandas.read\_table</span> is deprecated. Instead, use <span class="title-ref">read\_csv</span> passing `sep='\t'` if necessary. This deprecation has been removed in 0.25.0. (`21948`)
  - <span class="title-ref">Series.str.cat</span> has deprecated using arbitrary list-likes *within* list-likes. A list-like container may still contain many `Series`, `Index` or 1-dimensional `np.ndarray`, or alternatively, only scalar values. (`21950`)
  - <span class="title-ref">FrozenNDArray.searchsorted</span> has deprecated the `v` parameter in favor of `value` (`14645`)
  - <span class="title-ref">DatetimeIndex.shift</span> and <span class="title-ref">PeriodIndex.shift</span> now accept `periods` argument instead of `n` for consistency with <span class="title-ref">Index.shift</span> and <span class="title-ref">Series.shift</span>. Using `n` throws a deprecation warning (`22458`, `22912`)
  - The `fastpath` keyword of the different Index constructors is deprecated (`23110`).
  - <span class="title-ref">Timestamp.tz\_localize</span>, <span class="title-ref">DatetimeIndex.tz\_localize</span>, and <span class="title-ref">Series.tz\_localize</span> have deprecated the `errors` argument in favor of the `nonexistent` argument (`8917`)
  - The class `FrozenNDArray` has been deprecated. When unpickling, `FrozenNDArray` will be unpickled to `np.ndarray` once this class is removed (`9031`)
  - The methods <span class="title-ref">DataFrame.update</span> and <span class="title-ref">Panel.update</span> have deprecated the `raise_conflict=False|True` keyword in favor of `errors='ignore'|'raise'` (`23585`)
  - The methods <span class="title-ref">Series.str.partition</span> and <span class="title-ref">Series.str.rpartition</span> have deprecated the `pat` keyword in favor of `sep` (`22676`)
  - Deprecated the `nthreads` keyword of <span class="title-ref">pandas.read\_feather</span> in favor of `use_threads` to reflect the changes in `pyarrow>=0.11.0`. (`23053`)
  - <span class="title-ref">pandas.read\_excel</span> has deprecated accepting `usecols` as an integer. Please pass in a list of ints from 0 to `usecols` inclusive instead (`23527`)
  - Constructing a <span class="title-ref">TimedeltaIndex</span> from data with `datetime64`-dtyped data is deprecated, will raise `TypeError` in a future version (`23539`)
  - Constructing a <span class="title-ref">DatetimeIndex</span> from data with `timedelta64`-dtyped data is deprecated, will raise `TypeError` in a future version (`23675`)
  - The `keep_tz=False` option (the default) of the `keep_tz` keyword of <span class="title-ref">DatetimeIndex.to\_series</span> is deprecated (`17832`).
  - Timezone converting a tz-aware `datetime.datetime` or <span class="title-ref">Timestamp</span> with <span class="title-ref">Timestamp</span> and the `tz` argument is now deprecated. Instead, use <span class="title-ref">Timestamp.tz\_convert</span> (`23579`)
  - <span class="title-ref">pandas.api.types.is\_period</span> is deprecated in favor of `pandas.api.types.is_period_dtype` (`23917`)
  - <span class="title-ref">pandas.api.types.is\_datetimetz</span> is deprecated in favor of `pandas.api.types.is_datetime64tz` (`23917`)
  - Creating a <span class="title-ref">TimedeltaIndex</span>, <span class="title-ref">DatetimeIndex</span>, or <span class="title-ref">PeriodIndex</span> by passing range arguments `start`, `end`, and `periods` is deprecated in favor of <span class="title-ref">timedelta\_range</span>, <span class="title-ref">date\_range</span>, or <span class="title-ref">period\_range</span> (`23919`)
  - Passing a string alias like `'datetime64[ns, UTC]'` as the `unit` parameter to <span class="title-ref">DatetimeTZDtype</span> is deprecated. Use <span class="title-ref">DatetimeTZDtype.construct\_from\_string</span> instead (`23990`).
  - The `skipna` parameter of <span class="title-ref">\~pandas.api.types.infer\_dtype</span> will switch to `True` by default in a future version of pandas (`17066`, `24050`)
  - In <span class="title-ref">Series.where</span> with Categorical data, providing an `other` that is not present in the categories is deprecated. Convert the categorical to a different dtype or add the `other` to the categories first (`24077`).
  - <span class="title-ref">Series.clip\_lower</span>, <span class="title-ref">Series.clip\_upper</span>, <span class="title-ref">DataFrame.clip\_lower</span> and <span class="title-ref">DataFrame.clip\_upper</span> are deprecated and will be removed in a future version. Use `Series.clip(lower=threshold)`, `Series.clip(upper=threshold)` and the equivalent `DataFrame` methods (`24203`)
  - <span class="title-ref">Series.nonzero</span> is deprecated and will be removed in a future version (`18262`)
  - Passing an integer to <span class="title-ref">Series.fillna</span> and <span class="title-ref">DataFrame.fillna</span> with `timedelta64[ns]` dtypes is deprecated, will raise `TypeError` in a future version. Use `obj.fillna(pd.Timedelta(...))` instead (`24694`)
  - `Series.cat.categorical`, `Series.cat.name` and `Series.cat.index` have been deprecated. Use the attributes on `Series.cat` or `Series` directly. (`24751`).
  - Passing a dtype without a precision like `np.dtype('datetime64')` or `timedelta64` to <span class="title-ref">Index</span>, <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span> is now deprecated. Use the nanosecond-precision dtype instead (`24753`).

### Integer addition/subtraction with datetimes and timedeltas is deprecated

In the past, users couldâ€”in some casesâ€”add or subtract integers or integer-dtype arrays from <span class="title-ref">Timestamp</span>, <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span>.

This usage is now deprecated. Instead add or subtract integer multiples of the object's `freq` attribute (`21939`, `23878`).

*Previous behavior*:

`` `ipython     In [5]: ts = pd.Timestamp('1994-05-06 12:15:16', freq=pd.offsets.Hour())     In [6]: ts + 2     Out[6]: Timestamp('1994-05-06 14:15:16', freq='H')      In [7]: tdi = pd.timedelta_range('1D', periods=2)     In [8]: tdi - np.array([2, 1])     Out[8]: TimedeltaIndex(['-1 days', '1 days'], dtype='timedelta64[ns]', freq=None)      In [9]: dti = pd.date_range('2001-01-01', periods=2, freq='7D')     In [10]: dti + pd.Index([1, 2])     Out[10]: DatetimeIndex(['2001-01-08', '2001-01-22'], dtype='datetime64[ns]', freq=None)  *New behavior*:  .. code-block:: ipython      In [108]: ts = pd.Timestamp('1994-05-06 12:15:16', freq=pd.offsets.Hour())      In[109]: ts + 2 * ts.freq     Out[109]: Timestamp('1994-05-06 14:15:16', freq='H')      In [110]: tdi = pd.timedelta_range('1D', periods=2)      In [111]: tdi - np.array([2 * tdi.freq, 1 * tdi.freq])     Out[111]: TimedeltaIndex(['-1 days', '1 days'], dtype='timedelta64[ns]', freq=None)      In [112]: dti = pd.date_range('2001-01-01', periods=2, freq='7D')      In [113]: dti + pd.Index([1 * dti.freq, 2 * dti.freq])     Out[113]: DatetimeIndex(['2001-01-08', '2001-01-22'], dtype='datetime64[ns]', freq=None)   .. _whatsnew_0240.deprecations.integer_tz:  Passing integer data and a timezone to DatetimeIndex ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The behavior of <span class="title-ref">DatetimeIndex</span> when passed integer data and a timezone is changing in a future version of pandas. Previously, these were interpreted as wall times in the desired timezone. In the future, these will be interpreted as wall times in UTC, which are then converted to the desired timezone (`24559`).

The default behavior remains the same, but issues a warning:

`` `ipython    In [3]: pd.DatetimeIndex([946684800000000000], tz="US/Central")    /bin/ipython:1: FutureWarning:        Passing integer-dtype data and a timezone to DatetimeIndex. Integer values        will be interpreted differently in a future version of pandas. Previously,        these were viewed as datetime64[ns] values representing the wall time        *in the specified timezone*. In the future, these will be viewed as        datetime64[ns] values representing the wall time *in UTC*. This is similar        to a nanosecond-precision UNIX epoch. To accept the future behavior, use             pd.to_datetime(integer_data, utc=True).tz_convert(tz)         To keep the previous behavior, use             pd.to_datetime(integer_data).tz_localize(tz)      #!/bin/python3     Out[3]: DatetimeIndex(['2000-01-01 00:00:00-06:00'], dtype='datetime64[ns, US/Central]', freq=None)  As the warning message explains, opt in to the future behavior by specifying that ``\` the integer values are UTC, and then converting to the final timezone:

<div class="ipython">

python

pd.to\_datetime(\[946684800000000000\], utc=True).tz\_convert('US/Central')

</div>

The old behavior can be retained with by localizing directly to the final timezone:

<div class="ipython">

python

pd.to\_datetime(\[946684800000000000\]).tz\_localize('US/Central')

</div>

### Converting timezone-aware Series and Index to NumPy arrays

The conversion from a <span class="title-ref">Series</span> or <span class="title-ref">Index</span> with timezone-aware datetime data will change to preserve timezones by default (`23569`).

NumPy doesn't have a dedicated dtype for timezone-aware datetimes. In the past, converting a <span class="title-ref">Series</span> or <span class="title-ref">DatetimeIndex</span> with timezone-aware datatimes would convert to a NumPy array by

1.  converting the tz-aware data to UTC
2.  dropping the timezone-info
3.  returning a <span class="title-ref">numpy.ndarray</span> with `datetime64[ns]` dtype

Future versions of pandas will preserve the timezone information by returning an object-dtype NumPy array where each value is a <span class="title-ref">Timestamp</span> with the correct timezone attached

<div class="ipython">

python

ser = pd.Series(pd.date\_range('2000', periods=2, tz="CET")) ser

</div>

The default behavior remains the same, but issues a warning

`` `python    In [8]: np.asarray(ser)    /bin/ipython:1: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive          ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray          with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.             To accept the future behavior, pass 'dtype=object'.            To keep the old behavior, pass 'dtype="datetime64[ns]"'.      #!/bin/python3    Out[8]:    array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],          dtype='datetime64[ns]')  The previous or future behavior can be obtained, without any warnings, by specifying ``<span class="title-ref"> the </span><span class="title-ref">dtype</span>\`

*Previous behavior*

<div class="ipython">

python

np.asarray(ser, dtype='datetime64\[ns\]')

</div>

*Future behavior*

<div class="ipython">

python

\# New behavior np.asarray(ser, dtype=object)

</div>

Or by using <span class="title-ref">Series.to\_numpy</span>

<div class="ipython">

python

ser.to\_numpy() ser.to\_numpy(dtype="datetime64\[ns\]")

</div>

All the above applies to a <span class="title-ref">DatetimeIndex</span> with tz-aware values as well.

## Removal of prior version deprecations/changes

  - The `LongPanel` and `WidePanel` classes have been removed (`10892`)
  - <span class="title-ref">Series.repeat</span> has renamed the `reps` argument to `repeats` (`14645`)
  - Several private functions were removed from the (non-public) module `pandas.core.common` (`22001`)
  - Removal of the previously deprecated module `pandas.core.datetools` (`14105`, `14094`)
  - Strings passed into <span class="title-ref">DataFrame.groupby</span> that refer to both column and index levels will raise a `ValueError` (`14432`)
  - <span class="title-ref">Index.repeat</span> and <span class="title-ref">MultiIndex.repeat</span> have renamed the `n` argument to `repeats` (`14645`)
  - The `Series` constructor and `.astype` method will now raise a `ValueError` if timestamp dtypes are passed in without a unit (e.g. `np.datetime64`) for the `dtype` parameter (`15987`)
  - Removal of the previously deprecated `as_indexer` keyword completely from `str.match()` (`22356`, `6581`)
  - The modules `pandas.types`, `pandas.computation`, and `pandas.util.decorators` have been removed (`16157`, `16250`)
  - Removed the `pandas.formats.style` shim for <span class="title-ref">pandas.io.formats.style.Styler</span> (`16059`)
  - `pandas.pnow`, `pandas.match`, `pandas.groupby`, `pd.get_store`, `pd.Expr`, and `pd.Term` have been removed (`15538`, `15940`)
  - <span class="title-ref">Categorical.searchsorted</span> and <span class="title-ref">Series.searchsorted</span> have renamed the `v` argument to `value` (`14645`)
  - `pandas.parser`, `pandas.lib`, and `pandas.tslib` have been removed (`15537`)
  - <span class="title-ref">Index.searchsorted</span> have renamed the `key` argument to `value` (`14645`)
  - `DataFrame.consolidate` and `Series.consolidate` have been removed (`15501`)
  - Removal of the previously deprecated module `pandas.json` (`19944`)
  - The module `pandas.tools` has been removed (`15358`, `16005`)
  - <span class="title-ref">SparseArray.get\_values</span> and <span class="title-ref">SparseArray.to\_dense</span> have dropped the `fill` parameter (`14686`)
  - `DataFrame.sortlevel` and `Series.sortlevel` have been removed (`15099`)
  - <span class="title-ref">SparseSeries.to\_dense</span> has dropped the `sparse_only` parameter (`14686`)
  - <span class="title-ref">DataFrame.astype</span> and <span class="title-ref">Series.astype</span> have renamed the `raise_on_error` argument to `errors` (`14967`)
  - `is_sequence`, `is_any_int_dtype`, and `is_floating_dtype` have been removed from `pandas.api.types` (`16163`, `16189`)

## Performance improvements

  - Slicing Series and DataFrames with an monotonically increasing <span class="title-ref">CategoricalIndex</span> is now very fast and has speed comparable to slicing with an `Int64Index`. The speed increase is both when indexing by label (using .loc) and position(.iloc) (`20395`) Slicing a monotonically increasing <span class="title-ref">CategoricalIndex</span> itself (i.e. `ci[1000:2000]`) shows similar speed improvements as above (`21659`)
  - Improved performance of <span class="title-ref">CategoricalIndex.equals</span> when comparing to another <span class="title-ref">CategoricalIndex</span> (`24023`)
  - Improved performance of <span class="title-ref">Series.describe</span> in case of numeric dtpyes (`21274`)
  - Improved performance of <span class="title-ref">.GroupBy.rank</span> when dealing with tied rankings (`21237`)
  - Improved performance of <span class="title-ref">DataFrame.set\_index</span> with columns consisting of <span class="title-ref">Period</span> objects (`21582`, `21606`)
  - Improved performance of <span class="title-ref">Series.at</span> and <span class="title-ref">Index.get\_value</span> for Extension Arrays values (e.g. <span class="title-ref">Categorical</span>) (`24204`)
  - Improved performance of membership checks in <span class="title-ref">Categorical</span> and <span class="title-ref">CategoricalIndex</span> (i.e. `x in cat`-style checks are much faster). <span class="title-ref">CategoricalIndex.contains</span> is likewise much faster (`21369`, `21508`)
  - Improved performance of <span class="title-ref">HDFStore.groups</span> (and dependent functions like <span class="title-ref">HDFStore.keys</span>. (i.e. `x in store` checks are much faster) (`21372`)
  - Improved the performance of <span class="title-ref">pandas.get\_dummies</span> with `sparse=True` (`21997`)
  - Improved performance of <span class="title-ref">IndexEngine.get\_indexer\_non\_unique</span> for sorted, non-unique indexes (`9466`)
  - Improved performance of <span class="title-ref">PeriodIndex.unique</span> (`23083`)
  - Improved performance of <span class="title-ref">concat</span> for `Series` objects (`23404`)
  - Improved performance of <span class="title-ref">DatetimeIndex.normalize</span> and <span class="title-ref">Timestamp.normalize</span> for timezone naive or UTC datetimes (`23634`)
  - Improved performance of <span class="title-ref">DatetimeIndex.tz\_localize</span> and various `DatetimeIndex` attributes with dateutil UTC timezone (`23772`)
  - Fixed a performance regression on Windows with Python 3.7 of <span class="title-ref">read\_csv</span> (`23516`)
  - Improved performance of <span class="title-ref">Categorical</span> constructor for `Series` objects (`23814`)
  - Improved performance of <span class="title-ref">\~DataFrame.where</span> for Categorical data (`24077`)
  - Improved performance of iterating over a <span class="title-ref">Series</span>. Using <span class="title-ref">DataFrame.itertuples</span> now creates iterators without internally allocating lists of all elements (`20783`)
  - Improved performance of <span class="title-ref">Period</span> constructor, additionally benefitting `PeriodArray` and `PeriodIndex` creation (`24084`, `24118`)
  - Improved performance of tz-aware <span class="title-ref">DatetimeArray</span> binary operations (`24491`)

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">Categorical.from\_codes</span> where `NaN` values in `codes` were silently converted to `0` (`21767`). In the future this will raise a `ValueError`. Also changes the behavior of `.from_codes([1.1, 2.0])`.
  - Bug in <span class="title-ref">Categorical.sort\_values</span> where `NaN` values were always positioned in front regardless of `na_position` value. (`22556`).
  - Bug when indexing with a boolean-valued `Categorical`. Now a boolean-valued `Categorical` is treated as a boolean mask (`22665`)
  - Constructing a <span class="title-ref">CategoricalIndex</span> with empty values and boolean categories was raising a `ValueError` after a change to dtype coercion (`22702`).
  - Bug in <span class="title-ref">Categorical.take</span> with a user-provided `fill_value` not encoding the `fill_value`, which could result in a `ValueError`, incorrect results, or a segmentation fault (`23296`).
  - In <span class="title-ref">Series.unstack</span>, specifying a `fill_value` not present in the categories now raises a `TypeError` rather than ignoring the `fill_value` (`23284`)
  - Bug when resampling <span class="title-ref">DataFrame.resample</span> and aggregating on categorical data, the categorical dtype was getting lost. (`23227`)
  - Bug in many methods of the `.str`-accessor, which always failed on calling the `CategoricalIndex.str` constructor (`23555`, `23556`)
  - Bug in <span class="title-ref">Series.where</span> losing the categorical dtype for categorical data (`24077`)
  - Bug in <span class="title-ref">Categorical.apply</span> where `NaN` values could be handled unpredictably. They now remain unchanged (`24241`)
  - Bug in <span class="title-ref">Categorical</span> comparison methods incorrectly raising `ValueError` when operating against a <span class="title-ref">DataFrame</span> (`24630`)
  - Bug in <span class="title-ref">Categorical.set\_categories</span> where setting fewer new categories with `rename=True` caused a segmentation fault (`24675`)

### Datetimelike

  - Fixed bug where two <span class="title-ref">DateOffset</span> objects with different `normalize` attributes could evaluate as equal (`21404`)
  - Fixed bug where <span class="title-ref">Timestamp.resolution</span> incorrectly returned 1-microsecond `timedelta` instead of 1-nanosecond <span class="title-ref">Timedelta</span> (`21336`, `21365`)
  - Bug in <span class="title-ref">to\_datetime</span> that did not consistently return an <span class="title-ref">Index</span> when `box=True` was specified (`21864`)
  - Bug in <span class="title-ref">DatetimeIndex</span> comparisons where string comparisons incorrectly raises `TypeError` (`22074`)
  - Bug in <span class="title-ref">DatetimeIndex</span> comparisons when comparing against `timedelta64[ns]` dtyped arrays; in some cases `TypeError` was incorrectly raised, in others it incorrectly failed to raise (`22074`)
  - Bug in <span class="title-ref">DatetimeIndex</span> comparisons when comparing against object-dtyped arrays (`22074`)
  - Bug in <span class="title-ref">DataFrame</span> with `datetime64[ns]` dtype addition and subtraction with `Timedelta`-like objects (`22005`, `22163`)
  - Bug in <span class="title-ref">DataFrame</span> with `datetime64[ns]` dtype addition and subtraction with `DateOffset` objects returning an `object` dtype instead of `datetime64[ns]` dtype (`21610`, `22163`)
  - Bug in <span class="title-ref">DataFrame</span> with `datetime64[ns]` dtype comparing against `NaT` incorrectly (`22242`, `22163`)
  - Bug in <span class="title-ref">DataFrame</span> with `datetime64[ns]` dtype subtracting `Timestamp`-like object incorrectly returned `datetime64[ns]` dtype instead of `timedelta64[ns]` dtype (`8554`, `22163`)
  - Bug in <span class="title-ref">DataFrame</span> with `datetime64[ns]` dtype subtracting `np.datetime64` object with non-nanosecond unit failing to convert to nanoseconds (`18874`, `22163`)
  - Bug in <span class="title-ref">DataFrame</span> comparisons against `Timestamp`-like objects failing to raise `TypeError` for inequality checks with mismatched types (`8932`, `22163`)
  - Bug in <span class="title-ref">DataFrame</span> with mixed dtypes including `datetime64[ns]` incorrectly raising `TypeError` on equality comparisons (`13128`, `22163`)
  - Bug in <span class="title-ref">DataFrame.values</span> returning a <span class="title-ref">DatetimeIndex</span> for a single-column `DataFrame` with tz-aware datetime values. Now a 2-D <span class="title-ref">numpy.ndarray</span> of <span class="title-ref">Timestamp</span> objects is returned (`24024`)
  - Bug in <span class="title-ref">DataFrame.eq</span> comparison against `NaT` incorrectly returning `True` or `NaN` (`15697`, `22163`)
  - Bug in <span class="title-ref">DatetimeIndex</span> subtraction that incorrectly failed to raise `OverflowError` (`22492`, `22508`)
  - Bug in <span class="title-ref">DatetimeIndex</span> incorrectly allowing indexing with `Timedelta` object (`20464`)
  - Bug in <span class="title-ref">DatetimeIndex</span> where frequency was being set if original frequency was `None` (`22150`)
  - Bug in rounding methods of <span class="title-ref">DatetimeIndex</span> (<span class="title-ref">\~DatetimeIndex.round</span>, <span class="title-ref">\~DatetimeIndex.ceil</span>, <span class="title-ref">\~DatetimeIndex.floor</span>) and <span class="title-ref">Timestamp</span> (<span class="title-ref">\~Timestamp.round</span>, <span class="title-ref">\~Timestamp.ceil</span>, <span class="title-ref">\~Timestamp.floor</span>) could give rise to loss of precision (`22591`)
  - Bug in <span class="title-ref">to\_datetime</span> with an <span class="title-ref">Index</span> argument that would drop the `name` from the result (`21697`)
  - Bug in <span class="title-ref">PeriodIndex</span> where adding or subtracting a <span class="title-ref">timedelta</span> or <span class="title-ref">Tick</span> object produced incorrect results (`22988`)
  - Bug in the <span class="title-ref">Series</span> repr with period-dtype data missing a space before the data (`23601`)
  - Bug in <span class="title-ref">date\_range</span> when decrementing a start date to a past end date by a negative frequency (`23270`)
  - Bug in <span class="title-ref">Series.min</span> which would return `NaN` instead of `NaT` when called on a series of `NaT` (`23282`)
  - Bug in <span class="title-ref">Series.combine\_first</span> not properly aligning categoricals, so that missing values in `self` where not filled by valid values from `other` (`24147`)
  - Bug in <span class="title-ref">DataFrame.combine</span> with datetimelike values raising a TypeError (`23079`)
  - Bug in <span class="title-ref">date\_range</span> with frequency of `Day` or higher where dates sufficiently far in the future could wrap around to the past instead of raising `OutOfBoundsDatetime` (`14187`)
  - Bug in <span class="title-ref">period\_range</span> ignoring the frequency of `start` and `end` when those are provided as <span class="title-ref">Period</span> objects (`20535`).
  - Bug in <span class="title-ref">PeriodIndex</span> with attribute `freq.n` greater than 1 where adding a <span class="title-ref">DateOffset</span> object would return incorrect results (`23215`)
  - Bug in <span class="title-ref">Series</span> that interpreted string indices as lists of characters when setting datetimelike values (`23451`)
  - Bug in <span class="title-ref">DataFrame</span> when creating a new column from an ndarray of <span class="title-ref">Timestamp</span> objects with timezones creating an object-dtype column, rather than datetime with timezone (`23932`)
  - Bug in <span class="title-ref">Timestamp</span> constructor which would drop the frequency of an input <span class="title-ref">Timestamp</span> (`22311`)
  - Bug in <span class="title-ref">DatetimeIndex</span> where calling `np.array(dtindex, dtype=object)` would incorrectly return an array of `long` objects (`23524`)
  - Bug in <span class="title-ref">Index</span> where passing a timezone-aware <span class="title-ref">DatetimeIndex</span> and `dtype=object` would incorrectly raise a `ValueError` (`23524`)
  - Bug in <span class="title-ref">Index</span> where calling `np.array(dtindex, dtype=object)` on a timezone-naive <span class="title-ref">DatetimeIndex</span> would return an array of `datetime` objects instead of <span class="title-ref">Timestamp</span> objects, potentially losing nanosecond portions of the timestamps (`23524`)
  - Bug in <span class="title-ref">Categorical.\_\_setitem\_\_</span> not allowing setting with another `Categorical` when both are unordered and have the same categories, but in a different order (`24142`)
  - Bug in <span class="title-ref">date\_range</span> where using dates with millisecond resolution or higher could return incorrect values or the wrong number of values in the index (`24110`)
  - Bug in <span class="title-ref">DatetimeIndex</span> where constructing a <span class="title-ref">DatetimeIndex</span> from a <span class="title-ref">Categorical</span> or <span class="title-ref">CategoricalIndex</span> would incorrectly drop timezone information (`18664`)
  - Bug in <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span> where indexing with `Ellipsis` would incorrectly lose the index's `freq` attribute (`21282`)
  - Clarified error message produced when passing an incorrect `freq` argument to <span class="title-ref">DatetimeIndex</span> with `NaT` as the first entry in the passed data (`11587`)
  - Bug in <span class="title-ref">to\_datetime</span> where `box` and `utc` arguments were ignored when passing a <span class="title-ref">DataFrame</span> or `dict` of unit mappings (`23760`)
  - Bug in <span class="title-ref">Series.dt</span> where the cache would not update properly after an in-place operation (`24408`)
  - Bug in <span class="title-ref">PeriodIndex</span> where comparisons against an array-like object with length 1 failed to raise `ValueError` (`23078`)
  - Bug in <span class="title-ref">DatetimeIndex.astype</span>, <span class="title-ref">PeriodIndex.astype</span> and <span class="title-ref">TimedeltaIndex.astype</span> ignoring the sign of the `dtype` for unsigned integer dtypes (`24405`).
  - Fixed bug in <span class="title-ref">Series.max</span> with `datetime64[ns]`-dtype failing to return `NaT` when nulls are present and `skipna=False` is passed (`24265`)
  - Bug in <span class="title-ref">to\_datetime</span> where arrays of `datetime` objects containing both timezone-aware and timezone-naive `datetimes` would fail to raise `ValueError` (`24569`)
  - Bug in <span class="title-ref">to\_datetime</span> with invalid datetime format doesn't coerce input to `NaT` even if `errors='coerce'` (`24763`)

### Timedelta

  - Bug in <span class="title-ref">DataFrame</span> with `timedelta64[ns]` dtype division by `Timedelta`-like scalar incorrectly returning `timedelta64[ns]` dtype instead of `float64` dtype (`20088`, `22163`)
  - Bug in adding a <span class="title-ref">Index</span> with object dtype to a <span class="title-ref">Series</span> with `timedelta64[ns]` dtype incorrectly raising (`22390`)
  - Bug in multiplying a <span class="title-ref">Series</span> with numeric dtype against a `timedelta` object (`22390`)
  - Bug in <span class="title-ref">Series</span> with numeric dtype when adding or subtracting an array or `Series` with `timedelta64` dtype (`22390`)
  - Bug in <span class="title-ref">Index</span> with numeric dtype when multiplying or dividing an array with dtype `timedelta64` (`22390`)
  - Bug in <span class="title-ref">TimedeltaIndex</span> incorrectly allowing indexing with `Timestamp` object (`20464`)
  - Fixed bug where subtracting <span class="title-ref">Timedelta</span> from an object-dtyped array would raise `TypeError` (`21980`)
  - Fixed bug in adding a <span class="title-ref">DataFrame</span> with all-`timedelta64[ns]` dtypes to a <span class="title-ref">DataFrame</span> with all-integer dtypes returning incorrect results instead of raising `TypeError` (`22696`)
  - Bug in <span class="title-ref">TimedeltaIndex</span> where adding a timezone-aware datetime scalar incorrectly returned a timezone-naive <span class="title-ref">DatetimeIndex</span> (`23215`)
  - Bug in <span class="title-ref">TimedeltaIndex</span> where adding `np.timedelta64('NaT')` incorrectly returned an all-`NaT` <span class="title-ref">DatetimeIndex</span> instead of an all-`NaT` <span class="title-ref">TimedeltaIndex</span> (`23215`)
  - Bug in <span class="title-ref">Timedelta</span> and <span class="title-ref">to\_timedelta</span> have inconsistencies in supported unit string (`21762`)
  - Bug in <span class="title-ref">TimedeltaIndex</span> division where dividing by another <span class="title-ref">TimedeltaIndex</span> raised `TypeError` instead of returning a <span class="title-ref">Float64Index</span> (`23829`, `22631`)
  - Bug in <span class="title-ref">TimedeltaIndex</span> comparison operations where comparing against non-`Timedelta`-like objects would raise `TypeError` instead of returning all-`False` for `__eq__` and all-`True` for `__ne__` (`24056`)
  - Bug in <span class="title-ref">Timedelta</span> comparisons when comparing with a `Tick` object incorrectly raising `TypeError` (`24710`)

### Timezones

  - Bug in <span class="title-ref">Index.shift</span> where an `AssertionError` would raise when shifting across DST (`8616`)
  - Bug in <span class="title-ref">Timestamp</span> constructor where passing an invalid timezone offset designator (`Z`) would not raise a `ValueError` (`8910`)
  - Bug in <span class="title-ref">Timestamp.replace</span> where replacing at a DST boundary would retain an incorrect offset (`7825`)
  - Bug in <span class="title-ref">Series.replace</span> with `datetime64[ns, tz]` data when replacing `NaT` (`11792`)
  - Bug in <span class="title-ref">Timestamp</span> when passing different string date formats with a timezone offset would produce different timezone offsets (`12064`)
  - Bug when comparing a tz-naive <span class="title-ref">Timestamp</span> to a tz-aware <span class="title-ref">DatetimeIndex</span> which would coerce the <span class="title-ref">DatetimeIndex</span> to tz-naive (`12601`)
  - Bug in <span class="title-ref">Series.truncate</span> with a tz-aware <span class="title-ref">DatetimeIndex</span> which would cause a core dump (`9243`)
  - Bug in <span class="title-ref">Series</span> constructor which would coerce tz-aware and tz-naive <span class="title-ref">Timestamp</span> to tz-aware (`13051`)
  - Bug in <span class="title-ref">Index</span> with `datetime64[ns, tz]` dtype that did not localize integer data correctly (`20964`)
  - Bug in <span class="title-ref">DatetimeIndex</span> where constructing with an integer and tz would not localize correctly (`12619`)
  - Fixed bug where <span class="title-ref">DataFrame.describe</span> and <span class="title-ref">Series.describe</span> on tz-aware datetimes did not show `first` and `last` result (`21328`)
  - Bug in <span class="title-ref">DatetimeIndex</span> comparisons failing to raise `TypeError` when comparing timezone-aware `DatetimeIndex` against `np.datetime64` (`22074`)
  - Bug in `DataFrame` assignment with a timezone-aware scalar (`19843`)
  - Bug in <span class="title-ref">DataFrame.asof</span> that raised a `TypeError` when attempting to compare tz-naive and tz-aware timestamps (`21194`)
  - Bug when constructing a <span class="title-ref">DatetimeIndex</span> with <span class="title-ref">Timestamp</span> constructed with the `replace` method across DST (`18785`)
  - Bug when setting a new value with <span class="title-ref">DataFrame.loc</span> with a <span class="title-ref">DatetimeIndex</span> with a DST transition (`18308`, `20724`)
  - Bug in <span class="title-ref">Index.unique</span> that did not re-localize tz-aware dates correctly (`21737`)
  - Bug when indexing a <span class="title-ref">Series</span> with a DST transition (`21846`)
  - Bug in <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">Series.resample</span> where an `AmbiguousTimeError` or `NonExistentTimeError` would raise if a timezone aware timeseries ended on a DST transition (`19375`, `10117`)
  - Bug in <span class="title-ref">DataFrame.drop</span> and <span class="title-ref">Series.drop</span> when specifying a tz-aware Timestamp key to drop from a <span class="title-ref">DatetimeIndex</span> with a DST transition (`21761`)
  - Bug in <span class="title-ref">DatetimeIndex</span> constructor where `NaT` and `dateutil.tz.tzlocal` would raise an `OutOfBoundsDatetime` error (`23807`)
  - Bug in <span class="title-ref">DatetimeIndex.tz\_localize</span> and <span class="title-ref">Timestamp.tz\_localize</span> with `dateutil.tz.tzlocal` near a DST transition that would return an incorrectly localized datetime (`23807`)
  - Bug in <span class="title-ref">Timestamp</span> constructor where a `dateutil.tz.tzutc` timezone passed with a `datetime.datetime` argument would be converted to a `pytz.UTC` timezone (`23807`)
  - Bug in <span class="title-ref">to\_datetime</span> where `utc=True` was not respected when specifying a `unit` and `errors='ignore'` (`23758`)
  - Bug in <span class="title-ref">to\_datetime</span> where `utc=True` was not respected when passing a <span class="title-ref">Timestamp</span> (`24415`)
  - Bug in <span class="title-ref">DataFrame.any</span> returns wrong value when `axis=1` and the data is of datetimelike type (`23070`)
  - Bug in <span class="title-ref">DatetimeIndex.to\_period</span> where a timezone aware index was converted to UTC first before creating <span class="title-ref">PeriodIndex</span> (`22905`)
  - Bug in <span class="title-ref">DataFrame.tz\_localize</span>, <span class="title-ref">DataFrame.tz\_convert</span>, <span class="title-ref">Series.tz\_localize</span>, and <span class="title-ref">Series.tz\_convert</span> where `copy=False` would mutate the original argument inplace (`6326`)
  - Bug in <span class="title-ref">DataFrame.max</span> and <span class="title-ref">DataFrame.min</span> with `axis=1` where a <span class="title-ref">Series</span> with `NaN` would be returned when all columns contained the same timezone (`10390`)

### Offsets

  - Bug in <span class="title-ref">FY5253</span> where date offsets could incorrectly raise an `AssertionError` in arithmetic operations (`14774`)
  - Bug in <span class="title-ref">DateOffset</span> where keyword arguments `week` and `milliseconds` were accepted and ignored. Passing these will now raise `ValueError` (`19398`)
  - Bug in adding <span class="title-ref">DateOffset</span> with <span class="title-ref">DataFrame</span> or <span class="title-ref">PeriodIndex</span> incorrectly raising `TypeError` (`23215`)
  - Bug in comparing <span class="title-ref">DateOffset</span> objects with non-DateOffset objects, particularly strings, raising `ValueError` instead of returning `False` for equality checks and `True` for not-equal checks (`23524`)

### Numeric

  - Bug in <span class="title-ref">Series</span> `__rmatmul__` doesn't support matrix vector multiplication (`21530`)
  - Bug in <span class="title-ref">factorize</span> fails with read-only array (`12813`)
  - Fixed bug in <span class="title-ref">unique</span> handled signed zeros inconsistently: for some inputs 0.0 and -0.0 were treated as equal and for some inputs as different. Now they are treated as equal for all inputs (`21866`)
  - Bug in <span class="title-ref">DataFrame.agg</span>, <span class="title-ref">DataFrame.transform</span> and <span class="title-ref">DataFrame.apply</span> where, when supplied with a list of functions and `axis=1` (e.g. `df.apply(['sum', 'mean'], axis=1)`), a `TypeError` was wrongly raised. For all three methods such calculation are now done correctly. (`16679`).
  - Bug in <span class="title-ref">Series</span> comparison against datetime-like scalars and arrays (`22074`)
  - Bug in <span class="title-ref">DataFrame</span> multiplication between boolean dtype and integer returning `object` dtype instead of integer dtype (`22047`, `22163`)
  - Bug in <span class="title-ref">DataFrame.apply</span> where, when supplied with a string argument and additional positional or keyword arguments (e.g. `df.apply('sum', min_count=1)`), a `TypeError` was wrongly raised (`22376`)
  - Bug in <span class="title-ref">DataFrame.astype</span> to extension dtype may raise `AttributeError` (`22578`)
  - Bug in <span class="title-ref">DataFrame</span> with `timedelta64[ns]` dtype arithmetic operations with `ndarray` with integer dtype incorrectly treating the narray as `timedelta64[ns]` dtype (`23114`)
  - Bug in <span class="title-ref">Series.rpow</span> with object dtype `NaN` for `1 ** NA` instead of `1` (`22922`).
  - <span class="title-ref">Series.agg</span> can now handle numpy NaN-aware methods like <span class="title-ref">numpy.nansum</span> (`19629`)
  - Bug in <span class="title-ref">Series.rank</span> and <span class="title-ref">DataFrame.rank</span> when `pct=True` and more than 2<sup>24</sup> rows are present resulted in percentages greater than 1.0 (`18271`)
  - Calls such as <span class="title-ref">DataFrame.round</span> with a non-unique <span class="title-ref">CategoricalIndex</span> now return expected data. Previously, data would be improperly duplicated (`21809`).
  - Added `log10`, `floor` and `ceil` to the list of supported functions in <span class="title-ref">DataFrame.eval</span> (`24139`, `24353`)
  - Logical operations `&, |, ^` between <span class="title-ref">Series</span> and <span class="title-ref">Index</span> will no longer raise `ValueError` (`22092`)
  - Checking PEP 3141 numbers in <span class="title-ref">\~pandas.api.types.is\_scalar</span> function returns `True` (`22903`)
  - Reduction methods like <span class="title-ref">Series.sum</span> now accept the default value of `keepdims=False` when called from a NumPy ufunc, rather than raising a `TypeError`. Full support for `keepdims` has not been implemented (`24356`).

### Conversion

  - Bug in <span class="title-ref">DataFrame.combine\_first</span> in which column types were unexpectedly converted to float (`20699`)
  - Bug in <span class="title-ref">DataFrame.clip</span> in which column types are not preserved and casted to float (`24162`)
  - Bug in <span class="title-ref">DataFrame.clip</span> when order of columns of dataframes doesn't match, result observed is wrong in numeric values (`20911`)
  - Bug in <span class="title-ref">DataFrame.astype</span> where converting to an extension dtype when duplicate column names are present causes a `RecursionError` (`24704`)

### Strings

  - Bug in <span class="title-ref">Index.str.partition</span> was not nan-safe (`23558`).
  - Bug in <span class="title-ref">Index.str.split</span> was not nan-safe (`23677`).
  - Bug <span class="title-ref">Series.str.contains</span> not respecting the `na` argument for a `Categorical` dtype `Series` (`22158`)
  - Bug in <span class="title-ref">Index.str.cat</span> when the result contained only `NaN` (`24044`)

### Interval

  - Bug in the <span class="title-ref">IntervalIndex</span> constructor where the `closed` parameter did not always override the inferred `closed` (`19370`)
  - Bug in the `IntervalIndex` repr where a trailing comma was missing after the list of intervals (`20611`)
  - Bug in <span class="title-ref">Interval</span> where scalar arithmetic operations did not retain the `closed` value (`22313`)
  - Bug in <span class="title-ref">IntervalIndex</span> where indexing with datetime-like values raised a `KeyError` (`20636`)
  - Bug in `IntervalTree` where data containing `NaN` triggered a warning and resulted in incorrect indexing queries with <span class="title-ref">IntervalIndex</span> (`23352`)

### Indexing

  - Bug in <span class="title-ref">DataFrame.ne</span> fails if columns contain column name "dtype" (`22383`)
  - The traceback from a `KeyError` when asking `.loc` for a single missing label is now shorter and more clear (`21557`)
  - <span class="title-ref">PeriodIndex</span> now emits a `KeyError` when a malformed string is looked up, which is consistent with the behavior of <span class="title-ref">DatetimeIndex</span> (`22803`)
  - When `.ix` is asked for a missing integer label in a <span class="title-ref">MultiIndex</span> with a first level of integer type, it now raises a `KeyError`, consistently with the case of a flat <span class="title-ref">Int64Index</span>, rather than falling back to positional indexing (`21593`)
  - Bug in <span class="title-ref">Index.reindex</span> when reindexing a tz-naive and tz-aware <span class="title-ref">DatetimeIndex</span> (`8306`)
  - Bug in <span class="title-ref">Series.reindex</span> when reindexing an empty series with a `datetime64[ns, tz]` dtype (`20869`)
  - Bug in <span class="title-ref">DataFrame</span> when setting values with `.loc` and a timezone aware <span class="title-ref">DatetimeIndex</span> (`11365`)
  - `DataFrame.__getitem__` now accepts dictionaries and dictionary keys as list-likes of labels, consistently with `Series.__getitem__` (`21294`)
  - Fixed `DataFrame[np.nan]` when columns are non-unique (`21428`)
  - Bug when indexing <span class="title-ref">DatetimeIndex</span> with nanosecond resolution dates and timezones (`11679`)
  - Bug where indexing with a Numpy array containing negative values would mutate the indexer (`21867`)
  - Bug where mixed indexes wouldn't allow integers for `.at` (`19860`)
  - `Float64Index.get_loc` now raises `KeyError` when boolean key passed. (`19087`)
  - Bug in <span class="title-ref">DataFrame.loc</span> when indexing with an <span class="title-ref">IntervalIndex</span> (`19977`)
  - <span class="title-ref">Index</span> no longer mangles `None`, `NaN` and `NaT`, i.e. they are treated as three different keys. However, for numeric Index all three are still coerced to a `NaN` (`22332`)
  - Bug in `scalar in Index` if scalar is a float while the `Index` is of integer dtype (`22085`)
  - Bug in <span class="title-ref">MultiIndex.set\_levels</span> when levels value is not subscriptable (`23273`)
  - Bug where setting a timedelta column by `Index` causes it to be casted to double, and therefore lose precision (`23511`)
  - Bug in <span class="title-ref">Index.union</span> and <span class="title-ref">Index.intersection</span> where name of the `Index` of the result was not computed correctly for certain cases (`9943`, `9862`)
  - Bug in <span class="title-ref">Index</span> slicing with boolean <span class="title-ref">Index</span> may raise `TypeError` (`22533`)
  - Bug in `PeriodArray.__setitem__` when accepting slice and list-like value (`23978`)
  - Bug in <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span> where indexing with `Ellipsis` would lose their `freq` attribute (`21282`)
  - Bug in `iat` where using it to assign an incompatible value would create a new column (`23236`)

### Missing

  - Bug in <span class="title-ref">DataFrame.fillna</span> where a `ValueError` would raise when one column contained a `datetime64[ns, tz]` dtype (`15522`)
  - Bug in <span class="title-ref">Series.hasnans</span> that could be incorrectly cached and return incorrect answers if null elements are introduced after an initial call (`19700`)
  - <span class="title-ref">Series.isin</span> now treats all NaN-floats as equal also for `np.object_`-dtype. This behavior is consistent with the behavior for float64 (`22119`)
  - <span class="title-ref">unique</span> no longer mangles NaN-floats and the `NaT`-object for `np.object_`-dtype, i.e. `NaT` is no longer coerced to a NaN-value and is treated as a different entity. (`22295`)
  - <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> now properly handle numpy masked arrays with hardened masks. Previously, constructing a DataFrame or Series from a masked array with a hard mask would create a pandas object containing the underlying value, rather than the expected NaN. (`24574`)
  - Bug in <span class="title-ref">DataFrame</span> constructor where `dtype` argument was not honored when handling numpy masked record arrays. (`24874`)

### MultiIndex

  - Bug in <span class="title-ref">io.formats.style.Styler.applymap</span> where `subset=` with <span class="title-ref">MultiIndex</span> slice would reduce to <span class="title-ref">Series</span> (`19861`)
  - Removed compatibility for <span class="title-ref">MultiIndex</span> pickles prior to version 0.8.0; compatibility with <span class="title-ref">MultiIndex</span> pickles from version 0.13 forward is maintained (`21654`)
  - <span class="title-ref">MultiIndex.get\_loc\_level</span> (and as a consequence, `.loc` on a `Series` or `DataFrame` with a <span class="title-ref">MultiIndex</span> index) will now raise a `KeyError`, rather than returning an empty `slice`, if asked a label which is present in the `levels` but is unused (`22221`)
  - <span class="title-ref">MultiIndex</span> has gained the <span class="title-ref">MultiIndex.from\_frame</span>, it allows constructing a <span class="title-ref">MultiIndex</span> object from a <span class="title-ref">DataFrame</span> (`22420`)
  - Fix `TypeError` in Python 3 when creating <span class="title-ref">MultiIndex</span> in which some levels have mixed types, e.g. when some labels are tuples (`15457`)

### IO

  - Bug in <span class="title-ref">read\_csv</span> in which a column specified with `CategoricalDtype` of boolean categories was not being correctly coerced from string values to booleans (`20498`)
  - Bug in <span class="title-ref">read\_csv</span> in which unicode column names were not being properly recognized with Python 2.x (`13253`)
  - Bug in <span class="title-ref">DataFrame.to\_sql</span> when writing timezone aware data (`datetime64[ns, tz]` dtype) would raise a `TypeError` (`9086`)
  - Bug in <span class="title-ref">DataFrame.to\_sql</span> where a naive <span class="title-ref">DatetimeIndex</span> would be written as `TIMESTAMP WITH TIMEZONE` type in supported databases, e.g. PostgreSQL (`23510`)
  - Bug in <span class="title-ref">read\_excel</span> when `parse_cols` is specified with an empty dataset (`9208`)
  - <span class="title-ref">read\_html</span> no longer ignores all-whitespace `<tr>` within `<thead>` when considering the `skiprows` and `header` arguments. Previously, users had to decrease their `header` and `skiprows` values on such tables to work around the issue. (`21641`)
  - <span class="title-ref">read\_excel</span> will correctly show the deprecation warning for previously deprecated `sheetname` (`17994`)
  - <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> will throw `UnicodeError` and not coredump on badly encoded strings (`22748`)
  - <span class="title-ref">read\_csv</span> will correctly parse timezone-aware datetimes (`22256`)
  - Bug in <span class="title-ref">read\_csv</span> in which memory management was prematurely optimized for the C engine when the data was being read in chunks (`23509`)
  - Bug in <span class="title-ref">read\_csv</span> in unnamed columns were being improperly identified when extracting a multi-index (`23687`)
  - <span class="title-ref">read\_sas</span> will parse numbers in sas7bdat-files that have width less than 8 bytes correctly. (`21616`)
  - <span class="title-ref">read\_sas</span> will correctly parse sas7bdat files with many columns (`22628`)
  - <span class="title-ref">read\_sas</span> will correctly parse sas7bdat files with data page types having also bit 7 set (so page type is 128 + 256 = 384) (`16615`)
  - Bug in <span class="title-ref">read\_sas</span> in which an incorrect error was raised on an invalid file format. (`24548`)
  - Bug in <span class="title-ref">detect\_client\_encoding</span> where potential `IOError` goes unhandled when importing in a mod\_wsgi process due to restricted access to stdout. (`21552`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> with `index=False` misses truncation indicators (...) on truncated DataFrame (`15019`, `22783`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> with `index=False` when both columns and row index are `MultiIndex` (`22579`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> with `index_names=False` displaying index name (`22747`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> with `header=False` not displaying row index names (`23788`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> with `sparsify=False` that caused it to raise `TypeError` (`22887`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> that broke column alignment when `index=False` and width of first column's values is greater than the width of first column's header (`16839`, `13032`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> that caused representations of <span class="title-ref">DataFrame</span> to not take up the whole window (`22984`)
  - Bug in <span class="title-ref">DataFrame.to\_csv</span> where a single level MultiIndex incorrectly wrote a tuple. Now just the value of the index is written (`19589`).
  - <span class="title-ref">HDFStore</span> will raise `ValueError` when the `format` kwarg is passed to the constructor (`13291`)
  - Bug in <span class="title-ref">HDFStore.append</span> when appending a <span class="title-ref">DataFrame</span> with an empty string column and `min_itemsize` \< 8 (`12242`)
  - Bug in <span class="title-ref">read\_csv</span> in which memory leaks occurred in the C engine when parsing `NaN` values due to insufficient cleanup on completion or error (`21353`)
  - Bug in <span class="title-ref">read\_csv</span> in which incorrect error messages were being raised when `skipfooter` was passed in along with `nrows`, `iterator`, or `chunksize` (`23711`)
  - Bug in <span class="title-ref">read\_csv</span> in which <span class="title-ref">MultiIndex</span> index names were being improperly handled in the cases when they were not provided (`23484`)
  - Bug in <span class="title-ref">read\_csv</span> in which unnecessary warnings were being raised when the dialect's values conflicted with the default arguments (`23761`)
  - Bug in <span class="title-ref">read\_html</span> in which the error message was not displaying the valid flavors when an invalid one was provided (`23549`)
  - Bug in <span class="title-ref">read\_excel</span> in which extraneous header names were extracted, even though none were specified (`11733`)
  - Bug in <span class="title-ref">read\_excel</span> in which column names were not being properly converted to string sometimes in Python 2.x (`23874`)
  - Bug in <span class="title-ref">read\_excel</span> in which `index_col=None` was not being respected and parsing index columns anyway (`18792`, `20480`)
  - Bug in <span class="title-ref">read\_excel</span> in which `usecols` was not being validated for proper column names when passed in as a string (`20480`)
  - Bug in <span class="title-ref">DataFrame.to\_dict</span> when the resulting dict contains non-Python scalars in the case of numeric data (`23753`)
  - <span class="title-ref">DataFrame.to\_string</span>, <span class="title-ref">DataFrame.to\_html</span>, <span class="title-ref">DataFrame.to\_latex</span> will correctly format output when a string is passed as the `float_format` argument (`21625`, `22270`)
  - Bug in <span class="title-ref">read\_csv</span> that caused it to raise `OverflowError` when trying to use 'inf' as `na_value` with integer index column (`17128`)
  - Bug in <span class="title-ref">read\_csv</span> that caused the C engine on Python 3.6+ on Windows to improperly read CSV filenames with accented or special characters (`15086`)
  - Bug in <span class="title-ref">read\_fwf</span> in which the compression type of a file was not being properly inferred (`22199`)
  - Bug in <span class="title-ref">pandas.io.json.json\_normalize</span> that caused it to raise `TypeError` when two consecutive elements of `record_path` are dicts (`22706`)
  - Bug in <span class="title-ref">DataFrame.to\_stata</span>, <span class="title-ref">pandas.io.stata.StataWriter</span> and <span class="title-ref">pandas.io.stata.StataWriter117</span> where a exception would leave a partially written and invalid dta file (`23573`)
  - Bug in <span class="title-ref">DataFrame.to\_stata</span> and <span class="title-ref">pandas.io.stata.StataWriter117</span> that produced invalid files when using strLs with non-ASCII characters (`23573`)
  - Bug in <span class="title-ref">HDFStore</span> that caused it to raise `ValueError` when reading a Dataframe in Python 3 from fixed format written in Python 2 (`24510`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> and more generally in the floating `repr` formatter. Zeros were not trimmed if `inf` was present in a columns while it was the case with NA values. Zeros are now trimmed as in the presence of NA (`24861`).
  - Bug in the `repr` when truncating the number of columns and having a wide last column (`24849`).

### Plotting

  - Bug in <span class="title-ref">DataFrame.plot.scatter</span> and <span class="title-ref">DataFrame.plot.hexbin</span> caused x-axis label and ticklabels to disappear when colorbar was on in IPython inline backend (`10611`, `10678`, and `20455`)
  - Bug in plotting a Series with datetimes using <span class="title-ref">matplotlib.axes.Axes.scatter</span> (`22039`)
  - Bug in <span class="title-ref">DataFrame.plot.bar</span> caused bars to use multiple colors instead of a single one (`20585`)
  - Bug in validating color parameter caused extra color to be appended to the given color array. This happened to multiple plotting functions using matplotlib. (`20726`)

### GroupBy/resample/rolling

  - Bug in <span class="title-ref">.Rolling.min</span> and <span class="title-ref">.Rolling.max</span> with `closed='left'`, a datetime-like index and only one entry in the series leading to segfault (`24718`)
  - Bug in <span class="title-ref">.GroupBy.first</span> and <span class="title-ref">.GroupBy.last</span> with `as_index=False` leading to the loss of timezone information (`15884`)
  - Bug in <span class="title-ref">DateFrame.resample</span> when downsampling across a DST boundary (`8531`)
  - Bug in date anchoring for <span class="title-ref">DateFrame.resample</span> with offset <span class="title-ref">Day</span> when n \> 1 (`24127`)
  - Bug where `ValueError` is wrongly raised when calling <span class="title-ref">.SeriesGroupBy.count</span> method of a `SeriesGroupBy` when the grouping variable only contains NaNs and numpy version \< 1.13 (`21956`).
  - Multiple bugs in <span class="title-ref">.Rolling.min</span> with `closed='left'` and a datetime-like index leading to incorrect results and also segfault. (`21704`)
  - Bug in <span class="title-ref">.Resampler.apply</span> when passing positional arguments to applied func (`14615`).
  - Bug in <span class="title-ref">Series.resample</span> when passing `numpy.timedelta64` to `loffset` kwarg (`7687`).
  - Bug in <span class="title-ref">.Resampler.asfreq</span> when frequency of `TimedeltaIndex` is a subperiod of a new frequency (`13022`).
  - Bug in <span class="title-ref">.SeriesGroupBy.mean</span> when values were integral but could not fit inside of int64, overflowing instead. (`22487`)
  - <span class="title-ref">.RollingGroupby.agg</span> and <span class="title-ref">.ExpandingGroupby.agg</span> now support multiple aggregation functions as parameters (`15072`)
  - Bug in <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">Series.resample</span> when resampling by a weekly offset (`'W'`) across a DST transition (`9119`, `21459`)
  - Bug in <span class="title-ref">DataFrame.expanding</span> in which the `axis` argument was not being respected during aggregations (`23372`)
  - Bug in <span class="title-ref">.GroupBy.transform</span> which caused missing values when the input function can accept a <span class="title-ref">DataFrame</span> but renames it (`23455`).
  - Bug in <span class="title-ref">.GroupBy.nth</span> where column order was not always preserved (`20760`)
  - Bug in <span class="title-ref">.GroupBy.rank</span> with `method='dense'` and `pct=True` when a group has only one member would raise a `ZeroDivisionError` (`23666`).
  - Calling <span class="title-ref">.GroupBy.rank</span> with empty groups and `pct=True` was raising a `ZeroDivisionError` (`22519`)
  - Bug in <span class="title-ref">DataFrame.resample</span> when resampling `NaT` in `TimeDeltaIndex` (`13223`).
  - Bug in <span class="title-ref">DataFrame.groupby</span> did not respect the `observed` argument when selecting a column and instead always used `observed=False` (`23970`)
  - Bug in <span class="title-ref">.SeriesGroupBy.pct\_change</span> or <span class="title-ref">.DataFrameGroupBy.pct\_change</span> would previously work across groups when calculating the percent change, where it now correctly works per group (`21200`, `21235`).
  - Bug preventing hash table creation with very large number (2^32) of rows (`22805`)
  - Bug in groupby when grouping on categorical causes `ValueError` and incorrect grouping if `observed=True` and `nan` is present in categorical column (`24740`, `21151`).

### Reshaping

  - Bug in <span class="title-ref">pandas.concat</span> when joining resampled DataFrames with timezone aware index (`13783`)
  - Bug in <span class="title-ref">pandas.concat</span> when joining only `Series` the `names` argument of `concat` is no longer ignored (`23490`)
  - Bug in <span class="title-ref">Series.combine\_first</span> with `datetime64[ns, tz]` dtype which would return tz-naive result (`21469`)
  - Bug in <span class="title-ref">Series.where</span> and <span class="title-ref">DataFrame.where</span> with `datetime64[ns, tz]` dtype (`21546`)
  - Bug in <span class="title-ref">DataFrame.where</span> with an empty DataFrame and empty `cond` having non-bool dtype (`21947`)
  - Bug in <span class="title-ref">Series.mask</span> and <span class="title-ref">DataFrame.mask</span> with `list` conditionals (`21891`)
  - Bug in <span class="title-ref">DataFrame.replace</span> raises RecursionError when converting OutOfBounds `datetime64[ns, tz]` (`20380`)
  - <span class="title-ref">.GroupBy.rank</span> now raises a `ValueError` when an invalid value is passed for argument `na_option` (`22124`)
  - Bug in <span class="title-ref">get\_dummies</span> with Unicode attributes in Python 2 (`22084`)
  - Bug in <span class="title-ref">DataFrame.replace</span> raises `RecursionError` when replacing empty lists (`22083`)
  - Bug in <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> when dict is used as the `to_replace` value and one key in the dict is another key's value, the results were inconsistent between using integer key and using string key (`20656`)
  - Bug in <span class="title-ref">DataFrame.drop\_duplicates</span> for empty `DataFrame` which incorrectly raises an error (`20516`)
  - Bug in <span class="title-ref">pandas.wide\_to\_long</span> when a string is passed to the stubnames argument and a column name is a substring of that stubname (`22468`)
  - Bug in <span class="title-ref">merge</span> when merging `datetime64[ns, tz]` data that contained a DST transition (`18885`)
  - Bug in <span class="title-ref">merge\_asof</span> when merging on float values within defined tolerance (`22981`)
  - Bug in <span class="title-ref">pandas.concat</span> when concatenating a multicolumn DataFrame with tz-aware data against a DataFrame with a different number of columns (`22796`)
  - Bug in <span class="title-ref">merge\_asof</span> where confusing error message raised when attempting to merge with missing values (`23189`)
  - Bug in <span class="title-ref">DataFrame.nsmallest</span> and <span class="title-ref">DataFrame.nlargest</span> for dataframes that have a <span class="title-ref">MultiIndex</span> for columns (`23033`).
  - Bug in <span class="title-ref">pandas.melt</span> when passing column names that are not present in `DataFrame` (`23575`)
  - Bug in <span class="title-ref">DataFrame.append</span> with a <span class="title-ref">Series</span> with a dateutil timezone would raise a `TypeError` (`23682`)
  - Bug in <span class="title-ref">Series</span> construction when passing no data and `dtype=str` (`22477`)
  - Bug in <span class="title-ref">cut</span> with `bins` as an overlapping `IntervalIndex` where multiple bins were returned per item instead of raising a `ValueError` (`23980`)
  - Bug in <span class="title-ref">pandas.concat</span> when joining `Series` datetimetz with `Series` category would lose timezone (`23816`)
  - Bug in <span class="title-ref">DataFrame.join</span> when joining on partial MultiIndex would drop names (`20452`).
  - <span class="title-ref">DataFrame.nlargest</span> and <span class="title-ref">DataFrame.nsmallest</span> now returns the correct n values when keep \!= 'all' also when tied on the first columns (`22752`)
  - Constructing a DataFrame with an index argument that wasn't already an instance of <span class="title-ref">.Index</span> was broken (`22227`).
  - Bug in <span class="title-ref">DataFrame</span> prevented list subclasses to be used to construction (`21226`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> and <span class="title-ref">DataFrame.pivot\_table</span> returning a misleading error message when the resulting DataFrame has more elements than int32 can handle. Now, the error message is improved, pointing towards the actual problem (`20601`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> where a `ValueError` was raised when unstacking timezone aware values (`18338`)
  - Bug in <span class="title-ref">DataFrame.stack</span> where timezone aware values were converted to timezone naive values (`19420`)
  - Bug in <span class="title-ref">merge\_asof</span> where a `TypeError` was raised when `by_col` were timezone aware values (`21184`)
  - Bug showing an incorrect shape when throwing error during `DataFrame` construction. (`20742`)

### Sparse

  - Updating a boolean, datetime, or timedelta column to be Sparse now works (`22367`)
  - Bug in <span class="title-ref">Series.to\_sparse</span> with Series already holding sparse data not constructing properly (`22389`)
  - Providing a `sparse_index` to the SparseArray constructor no longer defaults the na-value to `np.nan` for all dtypes. The correct na\_value for `data.dtype` is now used.
  - Bug in `SparseArray.nbytes` under-reporting its memory usage by not including the size of its sparse index.
  - Improved performance of <span class="title-ref">Series.shift</span> for non-NA `fill_value`, as values are no longer converted to a dense array.
  - Bug in `DataFrame.groupby` not including `fill_value` in the groups for non-NA `fill_value` when grouping by a sparse column (`5078`)
  - Bug in unary inversion operator (`~`) on a `SparseSeries` with boolean values. The performance of this has also been improved (`22835`)
  - Bug in <span class="title-ref">SparseArary.unique</span> not returning the unique values (`19595`)
  - Bug in <span class="title-ref">SparseArray.nonzero</span> and <span class="title-ref">SparseDataFrame.dropna</span> returning shifted/incorrect results (`21172`)
  - Bug in <span class="title-ref">DataFrame.apply</span> where dtypes would lose sparseness (`23744`)
  - Bug in <span class="title-ref">concat</span> when concatenating a list of <span class="title-ref">Series</span> with all-sparse values changing the `fill_value` and converting to a dense Series (`24371`)

### Style

  - <span class="title-ref">\~pandas.io.formats.style.Styler.background\_gradient</span> now takes a `text_color_threshold` parameter to automatically lighten the text color based on the luminance of the background color. This improves readability with dark background colors without the need to limit the background colormap range. (`21258`)
  - <span class="title-ref">\~pandas.io.formats.style.Styler.background\_gradient</span> now also supports tablewise application (in addition to rowwise and columnwise) with `axis=None` (`15204`)
  - <span class="title-ref">\~pandas.io.formats.style.Styler.bar</span> now also supports tablewise application (in addition to rowwise and columnwise) with `axis=None` and setting clipping range with `vmin` and `vmax` (`21548` and `21526`). `NaN` values are also handled properly.

### Build changes

  - Building pandas for development now requires `cython >= 0.28.2` (`21688`)
  - Testing pandas now requires `hypothesis>=3.58`. You can find [the Hypothesis docs here](https://hypothesis.readthedocs.io/en/latest/index.html), and a pandas-specific introduction \[in the contributing guide \<using-hypothesis\>\](\#in-the-contributing-guide-\<using-hypothesis\>). (`22280`)
  - Building pandas on macOS now targets minimum macOS 10.9 if run on macOS 10.9 or above (`23424`)

### Other

  - Bug where C variables were declared with external linkage causing import errors if certain other C libraries were imported before pandas. (`24113`)

## Contributors

<div class="contributors">

v0.23.4..v0.24.0

</div>

---

v0.24.1.md

---

# What's new in 0.24.1 (February 3, 2019)

\> **Warning** \> The 0.24.x series of releases will be the last to support Python 2. Future feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more.

{{ header }}

These are the changes in pandas 0.24.1. See \[release\](\#release) for a full changelog including other versions of pandas. See \[whatsnew\_0240\](\#whatsnew\_0240) for the 0.24.0 changelog.

## API changes

### Changing the `sort` parameter for <span class="title-ref">Index</span> set operations

The default `sort` value for <span class="title-ref">Index.union</span> has changed from `True` to `None` (`24959`). The default *behavior*, however, remains the same: the result is sorted, unless

1.  `self` and `other` are identical
2.  `self` or `other` is empty
3.  `self` or `other` contain values that can not be compared (a `RuntimeWarning` is raised).

This change will allow `sort=True` to mean "always sort" in a future release.

The same change applies to <span class="title-ref">Index.difference</span> and <span class="title-ref">Index.symmetric\_difference</span>, which would not sort the result when the values could not be compared.

The `sort` option for <span class="title-ref">Index.intersection</span> has changed in three ways.

1.  The default has changed from `True` to `False`, to restore the pandas 0.23.4 and earlier behavior of not sorting by default.
2.  The behavior of `sort=True` can now be obtained with `sort=None`. This will sort the result only if the values in `self` and `other` are not identical.
3.  The value `sort=True` is no longer allowed. A future version of pandas will properly support `sort=True` meaning "always sort".

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.to\_dict</span> with `records` orient raising an `AttributeError` when the `DataFrame` contained more than 255 columns, or wrongly converting column names that were not valid python identifiers (`24939`, `24940`).
  - Fixed regression in <span class="title-ref">read\_sql</span> when passing certain queries with MySQL/pymysql (`24988`).
  - Fixed regression in <span class="title-ref">Index.intersection</span> incorrectly sorting the values by default (`24959`).
  - Fixed regression in <span class="title-ref">merge</span> when merging an empty `DataFrame` with multiple timezone-aware columns on one of the timezone-aware columns (`25014`).
  - Fixed regression in <span class="title-ref">Series.rename\_axis</span> and <span class="title-ref">DataFrame.rename\_axis</span> where passing `None` failed to remove the axis name (`25034`)
  - Fixed regression in <span class="title-ref">to\_timedelta</span> with `box=False` incorrectly returning a `datetime64` object instead of a `timedelta64` object (`24961`)
  - Fixed regression where custom hashable types could not be used as column keys in <span class="title-ref">DataFrame.set\_index</span> (`24969`)

## Bug fixes

**Reshaping**

  - Bug in <span class="title-ref">DataFrame.groupby</span> with <span class="title-ref">Grouper</span> when there is a time change (DST) and grouping frequency is `'1d'` (`24972`)

**Visualization**

  - Fixed the warning for implicitly registered matplotlib converters not showing. See \[whatsnew\_0211.converters\](\#whatsnew\_0211.converters) for more (`24963`).

**Other**

  - Fixed AttributeError when printing a DataFrame's HTML repr after accessing the IPython config object (`25036`)

## Contributors

A total of 7 people contributed patches to this release. People with a "+" by their names contributed a patch for the first time.

  - Alex Buchkovsky
  - Roman Yurchak
  - h-vetinari
  - jbrockmendel
  - Jeremy Schendel
  - Joris Van den Bossche
  - Tom Augspurger

---

v0.24.2.md

---

# What's new in 0.24.2 (March 12, 2019)

\> **Warning** \> The 0.24.x series of releases will be the last to support Python 2. Future feature releases will support Python 3 only. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more.

{{ header }}

These are the changes in pandas 0.24.2. See \[release\](\#release) for a full changelog including other versions of pandas.

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.all</span> and <span class="title-ref">DataFrame.any</span> where `bool_only=True` was ignored (`25101`)
  - Fixed issue in `DataFrame` construction with passing a mixed list of mixed types could segfault. (`25075`)
  - Fixed regression in <span class="title-ref">DataFrame.apply</span> causing `RecursionError` when `dict`-like classes were passed as argument. (`25196`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> where `regex=True` was only replacing patterns matching the start of the string (`25259`)
  - Fixed regression in <span class="title-ref">DataFrame.duplicated</span>, where empty dataframe was not returning a boolean dtyped Series. (`25184`)
  - Fixed regression in <span class="title-ref">Series.min</span> and <span class="title-ref">Series.max</span> where `numeric_only=True` was ignored when the `Series` contained `Categorical` data (`25299`)
  - Fixed regression in subtraction between <span class="title-ref">Series</span> objects with `datetime64[ns]` dtype incorrectly raising `OverflowError` when the `Series` on the right contains null values (`25317`)
  - Fixed regression in <span class="title-ref">TimedeltaIndex</span> where `np.sum(index)` incorrectly returned a zero-dimensional object instead of a scalar (`25282`)
  - Fixed regression in `IntervalDtype` construction where passing an incorrect string with 'Interval' as a prefix could result in a `RecursionError`. (`25338`)
  - Fixed regression in creating a period-dtype array from a read-only NumPy array of period objects. (`25403`)
  - Fixed regression in <span class="title-ref">Categorical</span>, where constructing it from a categorical `Series` and an explicit `categories=` that differed from that in the `Series` created an invalid object which could trigger segfaults. (`25318`)
  - Fixed regression in <span class="title-ref">to\_timedelta</span> losing precision when converting floating data to `Timedelta` data (`25077`).
  - Fixed pip installing from source into an environment without NumPy (`25193`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> where large strings of numbers would be coerced into `int64`, causing an `OverflowError` (`25616`)
  - Fixed regression in <span class="title-ref">factorize</span> when passing a custom `na_sentinel` value with `sort=True` (`25409`).
  - Fixed regression in <span class="title-ref">DataFrame.to\_csv</span> writing duplicate line endings with gzip compress (`25311`)

## Bug fixes

**I/O**

  - Better handling of terminal printing when the terminal dimensions are not known (`25080`)
  - Bug in reading a HDF5 table-format `DataFrame` created in Python 2, in Python 3 (`24925`)
  - Bug in reading a JSON with `orient='table'` generated by <span class="title-ref">DataFrame.to\_json</span> with `index=False` (`25170`)
  - Bug where float indexes could have misaligned values when printing (`25061`)

**Categorical**

  - Bug where calling <span class="title-ref">Series.replace</span> on categorical data could return a `Series` with incorrect dimensions (`24971`)
  - 
**Reshaping**

  - Bug in <span class="title-ref">.GroupBy.transform</span> where applying a function to a timezone aware column would return a timezone naive result (`24198`)
  - Bug in <span class="title-ref">DataFrame.join</span> when joining on a timezone aware <span class="title-ref">DatetimeIndex</span> (`23931`)

**Visualization**

  - Bug in <span class="title-ref">Series.plot</span> where a secondary y axis could not be set to log scale (`25545`)

**Other**

  - Bug in <span class="title-ref">Series.is\_unique</span> where single occurrences of `NaN` were not considered unique (`25180`)
  - Bug in <span class="title-ref">merge</span> when merging an empty `DataFrame` with an `Int64` column or a non-empty `DataFrame` with an `Int64` column that is all `NaN` (`25183`)
  - Bug in `IntervalTree` where a `RecursionError` occurs upon construction due to an overflow when adding endpoints, which also causes <span class="title-ref">IntervalIndex</span> to crash during indexing operations (`25485`)
  - Bug in <span class="title-ref">Series.size</span> raising for some extension-array-backed `Series`, rather than returning the size (`25580`)
  - Bug in resampling raising for nullable integer-dtype columns (`25580`)

## Contributors

A total of 25 people contributed patches to this release. People with a "+" by their names contributed a patch for the first time.

  - Albert Villanova del Moral
  - Arno Veenstra +
  - chris-b1
  - Devin Petersohn +
  - EternalLearner42 +
  - Flavien Lambert +
  - gfyoung
  - Gioia Ballin
  - jbrockmendel
  - Jeff Reback
  - Jeremy Schendel
  - Johan von Forstner +
  - Joris Van den Bossche
  - Josh
  - Justin Zheng
  - Kendall Masse
  - Matthew Roeschke
  - Max Bolingbroke +
  - rbenes +
  - Sterling Paramore +
  - Tao He +
  - Thomas A Caswell
  - Tom Augspurger
  - Vibhu Agarwal +
  - William Ayd
  - Zach Angell

---

v0.25.0.md

---

# What's new in 0.25.0 (July 18, 2019)

\> **Warning** \> Starting with the 0.25.x series of releases, pandas only supports Python 3.5.3 and higher. See [Dropping Python 2.7](https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27) for more details.

<div class="warning">

<div class="title">

Warning

</div>

The minimum supported Python version will be bumped to 3.6 in a future release.

</div>

<div class="warning">

<div class="title">

Warning

</div>

`Panel` has been fully removed. For N-D labeled data structures, please use [xarray](https://xarray.pydata.org/en/stable/)

</div>

<div class="warning">

<div class="title">

Warning

</div>

<span class="title-ref">read\_pickle</span> and <span class="title-ref">read\_msgpack</span> are only guaranteed backwards compatible back to pandas version 0.20.3 (`27082`)

</div>

{{ header }}

These are the changes in pandas 0.25.0. See \[release\](\#release) for a full changelog including other versions of pandas.

## Enhancements

### GroupBy aggregation with relabeling

pandas has added special groupby behavior, known as "named aggregation", for naming the output columns when applying multiple aggregation functions to specific columns (`18366`, `26512`).

<div class="ipython">

python

  - animals = pd.DataFrame({'kind': \['cat', 'dog', 'cat', 'dog'\],  
    'height': \[9.1, 6.0, 9.5, 34.0\], 'weight': \[7.9, 7.5, 9.9, 198.0\]})

animals animals.groupby("kind").agg( min\_height=pd.NamedAgg(column='height', aggfunc='min'), max\_height=pd.NamedAgg(column='height', aggfunc='max'), average\_weight=pd.NamedAgg(column='weight', aggfunc="mean"), )

</div>

Pass the desired columns names as the `**kwargs` to `.agg`. The values of `**kwargs` should be tuples where the first element is the column selection, and the second element is the aggregation function to apply. pandas provides the `pandas.NamedAgg` namedtuple to make it clearer what the arguments to the function are, but plain tuples are accepted as well.

<div class="ipython">

python

  - animals.groupby("kind").agg(  
    min\_height=('height', 'min'), max\_height=('height', 'max'), average\_weight=('weight', 'mean'),

)

</div>

Named aggregation is the recommended replacement for the deprecated "dict-of-dicts" approach to naming the output of column-specific aggregations (\[whatsnew\_0200.api\_breaking.deprecate\_group\_agg\_dict\](\#whatsnew\_0200.api\_breaking.deprecate\_group\_agg\_dict)).

A similar approach is now available for Series groupby objects as well. Because there's no need for column selection, the values can just be the functions to apply

<div class="ipython">

python

  - animals.groupby("kind").height.agg(  
    min\_height="min", max\_height="max",

)

</div>

This type of aggregation is the recommended alternative to the deprecated behavior when passing a dict to a Series groupby aggregation (\[whatsnew\_0200.api\_breaking.deprecate\_group\_agg\_dict\](\#whatsnew\_0200.api\_breaking.deprecate\_group\_agg\_dict)).

See \[groupby.aggregate.named\](\#groupby.aggregate.named) for more.

### GroupBy aggregation with multiple lambdas

You can now provide multiple lambda functions to a list-like aggregation in <span class="title-ref">.GroupBy.agg</span> (`26430`).

<div class="ipython">

python

  - animals.groupby('kind').height.agg(\[  
    lambda x: x.iloc\[0\], lambda x: x.iloc\[-1\]

\])

  - animals.groupby('kind').agg(\[  
    lambda x: x.iloc\[0\] - x.iloc\[1\], lambda x: x.iloc\[0\] + x.iloc\[1\]

\])

</div>

Previously, these raised a `SpecificationError`.

### Better repr for MultiIndex

Printing of <span class="title-ref">MultiIndex</span> instances now shows tuples of each row and ensures that the tuple items are vertically aligned, so it's now easier to understand the structure of the `MultiIndex`. (`13480`):

The repr now looks like this:

<div class="ipython">

python

pd.MultiIndex.from\_product(\[\['a', 'abc'\], range(500)\])

</div>

Previously, outputting a <span class="title-ref">MultiIndex</span> printed all the `levels` and `codes` of the `MultiIndex`, which was visually unappealing and made the output more difficult to navigate. For example (limiting the range to 5):

`` `ipython    In [1]: pd.MultiIndex.from_product([['a', 'abc'], range(5)])    Out[1]: MultiIndex(levels=[['a', 'abc'], [0, 1, 2, 3]],       ...:            codes=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 2, 3, 0, 1, 2, 3]])  In the new repr, all values will be shown, if the number of rows is smaller ``<span class="title-ref"> than \`options.display.max\_seq\_items</span> (default: 100 items). Horizontally, the output will truncate, if it's wider than <span class="title-ref">options.display.width</span> (default: 80 characters).

### Shorter truncated repr for Series and DataFrame

Currently, the default display options of pandas ensure that when a Series or DataFrame has more than 60 rows, its repr gets truncated to this maximum of 60 rows (the `display.max_rows` option). However, this still gives a repr that takes up a large part of the vertical screen estate. Therefore, a new option `display.min_rows` is introduced with a default of 10 which determines the number of rows showed in the truncated repr:

  - For small Series or DataFrames, up to `max_rows` number of rows is shown (default: 60).
  - For larger Series of DataFrame with a length above `max_rows`, only `min_rows` number of rows is shown (default: 10, i.e. the first and last 5 rows).

This dual option allows to still see the full content of relatively small objects (e.g. `df.head(20)` shows all 20 rows), while giving a brief repr for large objects.

To restore the previous behaviour of a single threshold, set `pd.options.display.min_rows = None`.

### JSON normalize with max\_level param support

<span class="title-ref">json\_normalize</span> normalizes the provided input dict to all nested levels. The new max\_level parameter provides more control over which level to end normalization (`23843`):

The repr now looks like this:

`` `ipython     from pandas.io.json import json_normalize     data = [{         'CreatedBy': {'Name': 'User001'},         'Lookup': {'TextField': 'Some text',                    'UserField': {'Id': 'ID001', 'Name': 'Name001'}},         'Image': {'a': 'b'}     }]     json_normalize(data, max_level=1)   .. _whatsnew_0250.enhancements.explode:  Series.explode to split list-like values to rows ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> have gained the <span class="title-ref">DataFrame.explode</span> methods to transform list-likes to individual rows. See \[section on Exploding list-like column \<reshaping.explode\>\](\#section-on-exploding-list-like-column-\<reshaping.explode\>) in docs for more information (`16538`, `10511`)

Here is a typical usecase. You have comma separated string in a column.

<div class="ipython">

python

  - df = pd.DataFrame(\[{'var1': 'a,b,c', 'var2': 1},  
    {'var1': 'd,e,f', 'var2': 2}\])

df

</div>

Creating a long form `DataFrame` is now straightforward using chained operations

<div class="ipython">

python

df.assign(var1=df.var1.str.split(',')).explode('var1')

</div>

### Other enhancements

  - <span class="title-ref">DataFrame.plot</span> keywords `logy`, `logx` and `loglog` can now accept the value `'sym'` for symlog scaling. (`24867`)
  - Added support for ISO week year format ('%G-%V-%u') when parsing datetimes using <span class="title-ref">to\_datetime</span> (`16607`)
  - Indexing of `DataFrame` and `Series` now accepts zerodim `np.ndarray` (`24919`)
  - <span class="title-ref">Timestamp.replace</span> now supports the `fold` argument to disambiguate DST transition times (`25017`)
  - <span class="title-ref">DataFrame.at\_time</span> and <span class="title-ref">Series.at\_time</span> now support <span class="title-ref">datetime.time</span> objects with timezones (`24043`)
  - <span class="title-ref">DataFrame.pivot\_table</span> now accepts an `observed` parameter which is passed to underlying calls to <span class="title-ref">DataFrame.groupby</span> to speed up grouping categorical data. (`24923`)
  - `Series.str` has gained <span class="title-ref">Series.str.casefold</span> method to removes all case distinctions present in a string (`25405`)
  - <span class="title-ref">DataFrame.set\_index</span> now works for instances of `abc.Iterator`, provided their output is of the same length as the calling frame (`22484`, `24984`)
  - <span class="title-ref">DatetimeIndex.union</span> now supports the `sort` argument. The behavior of the sort parameter matches that of <span class="title-ref">Index.union</span> (`24994`)
  - <span class="title-ref">RangeIndex.union</span> now supports the `sort` argument. If `sort=False` an unsorted `Int64Index` is always returned. `sort=None` is the default and returns a monotonically increasing `RangeIndex` if possible or a sorted `Int64Index` if not (`24471`)
  - <span class="title-ref">TimedeltaIndex.intersection</span> now also supports the `sort` keyword (`24471`)
  - <span class="title-ref">DataFrame.rename</span> now supports the `errors` argument to raise errors when attempting to rename nonexistent keys (`13473`)
  - Added \[api.frame.sparse\](\#api.frame.sparse) for working with a `DataFrame` whose values are sparse (`25681`)
  - <span class="title-ref">RangeIndex</span> has gained <span class="title-ref">\~RangeIndex.start</span>, <span class="title-ref">\~RangeIndex.stop</span>, and <span class="title-ref">\~RangeIndex.step</span> attributes (`25710`)
  - <span class="title-ref">datetime.timezone</span> objects are now supported as arguments to timezone methods and constructors (`25065`)
  - <span class="title-ref">DataFrame.query</span> and <span class="title-ref">DataFrame.eval</span> now supports quoting column names with backticks to refer to names with spaces (`6508`)
  - <span class="title-ref">merge\_asof</span> now gives a more clear error message when merge keys are categoricals that are not equal (`26136`)
  - <span class="title-ref">.Rolling</span> supports exponential (or Poisson) window type (`21303`)
  - Error message for missing required imports now includes the original import error's text (`23868`)
  - <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span> now have a `mean` method (`24757`)
  - <span class="title-ref">DataFrame.describe</span> now formats integer percentiles without decimal point (`26660`)
  - Added support for reading SPSS .sav files using <span class="title-ref">read\_spss</span> (`26537`)
  - Added new option `plotting.backend` to be able to select a plotting backend different than the existing `matplotlib` one. Use `pandas.set_option('plotting.backend', '<backend-module>')` where `<backend-module` is a library implementing the pandas plotting API (`14130`)
  - <span class="title-ref">pandas.offsets.BusinessHour</span> supports multiple opening hours intervals (`15481`)
  - <span class="title-ref">read\_excel</span> can now use `openpyxl` to read Excel files via the `engine='openpyxl'` argument. This will become the default in a future release (`11499`)
  - <span class="title-ref">pandas.io.excel.read\_excel</span> supports reading OpenDocument tables. Specify `engine='odf'` to enable. Consult the \[IO User Guide \<io.ods\>\](\#io-user-guide-\<io.ods\>) for more details (`9070`)
  - <span class="title-ref">Interval</span>, <span class="title-ref">IntervalIndex</span>, and <span class="title-ref">\~arrays.IntervalArray</span> have gained an <span class="title-ref">\~Interval.is\_empty</span> attribute denoting if the given interval(s) are empty (`27219`)

## Backwards incompatible API changes

### Indexing with date strings with UTC offsets

Indexing a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with a <span class="title-ref">DatetimeIndex</span> with a date string with a UTC offset would previously ignore the UTC offset. Now, the UTC offset is respected in indexing. (`24076`, `16785`)

<div class="ipython">

python

df = pd.DataFrame(\[0\], index=pd.DatetimeIndex(\['2019-01-01'\], tz='US/Pacific')) df

</div>

*Previous behavior*:

`` `ipython     In [3]: df['2019-01-01 00:00:00+04:00':'2019-01-01 01:00:00+04:00']     Out[3]:                                0     2019-01-01 00:00:00-08:00  0  *New behavior*:  .. ipython:: python      df['2019-01-01 12:00:00+04:00':'2019-01-01 13:00:00+04:00']   .. _whatsnew_0250.api_breaking.multi_indexing: ``MultiIndex`constructed from levels and codes`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Constructing a <span class="title-ref">MultiIndex</span> with `NaN` levels or codes value \< -1 was allowed previously. Now, construction with codes value \< -1 is not allowed and `NaN` levels' corresponding codes would be reassigned as -1. (`19387`)

*Previous behavior*:

`` `ipython     In [1]: pd.MultiIndex(levels=[[np.nan, None, pd.NaT, 128, 2]],        ...:               codes=[[0, -1, 1, 2, 3, 4]])        ...:     Out[1]: MultiIndex(levels=[[nan, None, NaT, 128, 2]],                        codes=[[0, -1, 1, 2, 3, 4]])      In [2]: pd.MultiIndex(levels=[[1, 2]], codes=[[0, -2]])     Out[2]: MultiIndex(levels=[[1, 2]],                        codes=[[0, -2]])  *New behavior*:  .. ipython:: python     :okexcept:      pd.MultiIndex(levels=[[np.nan, None, pd.NaT, 128, 2]],                   codes=[[0, -1, 1, 2, 3, 4]])     pd.MultiIndex(levels=[[1, 2]], codes=[[0, -2]])   .. _whatsnew_0250.api_breaking.groupby_apply_first_group_once: ``GroupBy.apply`on`DataFrame`evaluates first group only once`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The implementation of <span class="title-ref">.DataFrameGroupBy.apply</span> previously evaluated the supplied function consistently twice on the first group to infer if it is safe to use a fast code path. Particularly for functions with side effects, this was an undesired behavior and may have led to surprises. (`2936`, `2656`, `7739`, `10519`, `12155`, `20084`, `21417`)

Now every group is evaluated only a single time.

<div class="ipython">

python

df = pd.DataFrame({"a": \["x", "y"\], "b": \[1, 2\]}) df

  - def func(group):  
    print(group.name) return group

</div>

*Previous behavior*:

`` `python    In [3]: df.groupby('a').apply(func)    x    x    y    Out[3]:       a  b    0  x  1    1  y  2  *New behavior*:  .. code-block:: python     In [3]: df.groupby('a').apply(func)    x    y    Out[3]:       a  b    0  x  1    1  y  2  Concatenating sparse values ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^

When passed DataFrames whose values are sparse, <span class="title-ref">concat</span> will now return a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with sparse values, rather than a <span class="title-ref">SparseDataFrame</span> (`25702`).

<div class="ipython">

python

df = pd.DataFrame({"A": pd.arrays.SparseArray(\[0, 1\])})

</div>

*Previous behavior*:

`` `ipython    In [2]: type(pd.concat([df, df]))    pandas.core.sparse.frame.SparseDataFrame  *New behavior*:  .. ipython:: python     type(pd.concat([df, df]))   This now matches the existing behavior of `concat` on ``Series`with sparse values.`<span class="title-ref"> \`concat</span> will continue to return a `SparseDataFrame` when all the values are instances of `SparseDataFrame`.

This change also affects routines using <span class="title-ref">concat</span> internally, like <span class="title-ref">get\_dummies</span>, which now returns a <span class="title-ref">DataFrame</span> in all cases (previously a `SparseDataFrame` was returned if all the columns were dummy encoded, and a <span class="title-ref">DataFrame</span> otherwise).

Providing any `SparseSeries` or `SparseDataFrame` to <span class="title-ref">concat</span> will cause a `SparseSeries` or `SparseDataFrame` to be returned, as before.

### The `.str`-accessor performs stricter type checks

Due to the lack of more fine-grained dtypes, <span class="title-ref">Series.str</span> so far only checked whether the data was of `object` dtype. <span class="title-ref">Series.str</span> will now infer the dtype data *within* the Series; in particular, `'bytes'`-only data will raise an exception (except for <span class="title-ref">Series.str.decode</span>, <span class="title-ref">Series.str.get</span>, <span class="title-ref">Series.str.len</span>, <span class="title-ref">Series.str.slice</span>), see `23163`, `23011`, `23551`.

*Previous behavior*:

`` `python     In [1]: s = pd.Series(np.array(['a', 'ba', 'cba'], 'S'), dtype=object)      In [2]: s     Out[2]:     0      b'a'     1     b'ba'     2    b'cba'     dtype: object      In [3]: s.str.startswith(b'a')     Out[3]:     0     True     1    False     2    False     dtype: bool  *New behavior*:  .. ipython:: python     :okexcept:      s = pd.Series(np.array(['a', 'ba', 'cba'], 'S'), dtype=object)     s     s.str.startswith(b'a')  .. _whatsnew_0250.api_breaking.groupby_categorical:  Categorical dtypes are preserved during GroupBy ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, columns that were categorical, but not the groupby key(s) would be converted to `object` dtype during groupby operations. pandas now will preserve these dtypes. (`18502`)

<div class="ipython">

python

cat = pd.Categorical(\["foo", "bar", "bar", "qux"\], ordered=True) df = pd.DataFrame({'payload': \[-1, -2, -1, -2\], 'col': cat}) df df.dtypes

</div>

*Previous Behavior*:

`` `python    In [5]: df.groupby('payload').first().col.dtype    Out[5]: dtype('O')  *New Behavior*:  .. ipython:: python     df.groupby('payload').first().col.dtype   .. _whatsnew_0250.api_breaking.incompatible_index_unions:  Incompatible Index type unions ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When performing <span class="title-ref">Index.union</span> operations between objects of incompatible dtypes, the result will be a base <span class="title-ref">Index</span> of dtype `object`. This behavior holds true for unions between <span class="title-ref">Index</span> objects that previously would have been prohibited. The dtype of empty <span class="title-ref">Index</span> objects will now be evaluated before performing union operations rather than simply returning the other <span class="title-ref">Index</span> object. <span class="title-ref">Index.union</span> can now be considered commutative, such that `A.union(B) == B.union(A)` (`23525`).

*Previous behavior*:

`` `python     In [1]: pd.period_range('19910905', periods=2).union(pd.Int64Index([1, 2, 3]))     ...     ValueError: can only call with other PeriodIndex-ed objects      In [2]: pd.Index([], dtype=object).union(pd.Index([1, 2, 3]))     Out[2]: Int64Index([1, 2, 3], dtype='int64')  *New behavior*:  .. code-block:: python      In [3]: pd.period_range('19910905', periods=2).union(pd.Int64Index([1, 2, 3]))     Out[3]: Index([1991-09-05, 1991-09-06, 1, 2, 3], dtype='object')     In [4]: pd.Index([], dtype=object).union(pd.Index([1, 2, 3]))     Out[4]: Index([1, 2, 3], dtype='object')  Note that integer- and floating-dtype indexes are considered "compatible". The integer ``\` values are coerced to floating point, which may result in loss of precision. See \[indexing.set\_ops\](\#indexing.set\_ops) for more.

### `DataFrame` GroupBy ffill/bfill no longer return group labels

The methods `ffill`, `bfill`, `pad` and `backfill` of <span class="title-ref">.DataFrameGroupBy</span> previously included the group labels in the return value, which was inconsistent with other groupby transforms. Now only the filled values are returned. (`21521`)

<div class="ipython">

python

df = pd.DataFrame({"a": \["x", "y"\], "b": \[1, 2\]}) df

</div>

*Previous behavior*:

`` `python    In [3]: df.groupby("a").ffill()    Out[3]:       a  b    0  x  1    1  y  2  *New behavior*:  .. ipython:: python      df.groupby("a").ffill() ``DataFrame`describe on an empty Categorical / object column will return top and freq`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When calling <span class="title-ref">DataFrame.describe</span> with an empty categorical / object column, the 'top' and 'freq' columns were previously omitted, which was inconsistent with the output for non-empty columns. Now the 'top' and 'freq' columns will always be included, with <span class="title-ref">numpy.nan</span> in the case of an empty <span class="title-ref">DataFrame</span> (`26397`)

<div class="ipython">

python

df = pd.DataFrame({"empty\_col": pd.Categorical(\[\])}) df

</div>

*Previous behavior*:

`` `python    In [3]: df.describe()    Out[3]:            empty_col    count           0    unique          0  *New behavior*:  .. ipython:: python     df.describe() ``\_\_str\_\_`methods now call`\_\_repr\_\_`rather than vice versa`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas has until now mostly defined string representations in a pandas objects' `__str__`/`__unicode__`/`__bytes__` methods, and called `__str__` from the `__repr__` method, if a specific `__repr__` method is not found. This is not needed for Python3. In pandas 0.25, the string representations of pandas objects are now generally defined in `__repr__`, and calls to `__str__` in general now pass the call on to the `__repr__`, if a specific `__str__` method doesn't exist, as is standard for Python. This change is backward compatible for direct usage of pandas, but if you subclass pandas objects *and* give your subclasses specific `__str__`/`__repr__` methods, you may have to adjust your `__str__`/`__repr__` methods (`26495`).

### Indexing an `IntervalIndex` with `Interval` objects

Indexing methods for <span class="title-ref">IntervalIndex</span> have been modified to require exact matches only for <span class="title-ref">Interval</span> queries. `IntervalIndex` methods previously matched on any overlapping `Interval`. Behavior with scalar points, e.g. querying with an integer, is unchanged (`16316`).

<div class="ipython">

python

ii = pd.IntervalIndex.from\_tuples(\[(0, 4), (1, 5), (5, 8)\]) ii

</div>

The `in` operator (`__contains__`) now only returns `True` for exact matches to `Intervals` in the `IntervalIndex`, whereas this would previously return `True` for any `Interval` overlapping an `Interval` in the `IntervalIndex`.

*Previous behavior*:

`` `python    In [4]: pd.Interval(1, 2, closed='neither') in ii    Out[4]: True     In [5]: pd.Interval(-10, 10, closed='both') in ii    Out[5]: True  *New behavior*:  .. ipython:: python     pd.Interval(1, 2, closed='neither') in ii    pd.Interval(-10, 10, closed='both') in ii  The `~IntervalIndex.get_loc` method now only returns locations for exact matches to ``Interval`queries, as opposed to the previous behavior of`<span class="title-ref"> returning locations for overlapping matches. A </span><span class="title-ref">KeyError</span>\` will be raised if an exact match is not found.

*Previous behavior*:

`` `python    In [6]: ii.get_loc(pd.Interval(1, 5))    Out[6]: array([0, 1])     In [7]: ii.get_loc(pd.Interval(2, 6))    Out[7]: array([0, 1, 2])  *New behavior*:  .. code-block:: python     In [6]: ii.get_loc(pd.Interval(1, 5))    Out[6]: 1     In [7]: ii.get_loc(pd.Interval(2, 6))    ---------------------------------------------------------------------------    KeyError: Interval(2, 6, closed='right')  Likewise, `~IntervalIndex.get_indexer` and `~IntervalIndex.get_indexer_non_unique` will also only return locations for exact matches ``<span class="title-ref"> to </span><span class="title-ref">Interval</span><span class="title-ref"> queries, with </span><span class="title-ref">-1</span>\` denoting that an exact match was not found.

These indexing changes extend to querying a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with an `IntervalIndex` index.

<div class="ipython">

python

s = pd.Series(list('abc'), index=ii) s

</div>

Selecting from a `Series` or `DataFrame` using `[]` (`__getitem__`) or `loc` now only returns exact matches for `Interval` queries.

*Previous behavior*:

`` `python    In [8]: s[pd.Interval(1, 5)]    Out[8]:    (0, 4]    a    (1, 5]    b    dtype: object     In [9]: s.loc[pd.Interval(1, 5)]    Out[9]:    (0, 4]    a    (1, 5]    b    dtype: object  *New behavior*:  .. ipython:: python     s[pd.Interval(1, 5)]    s.loc[pd.Interval(1, 5)]  Similarly, a ``KeyError``will be raised for non-exact matches instead of returning overlapping matches.  *Previous behavior*:  .. code-block:: python     In [9]: s[pd.Interval(2, 3)]    Out[9]:    (0, 4]    a    (1, 5]    b    dtype: object     In [10]: s.loc[pd.Interval(2, 3)]    Out[10]:    (0, 4]    a    (1, 5]    b    dtype: object  *New behavior*:  .. code-block:: python     In [6]: s[pd.Interval(2, 3)]    ---------------------------------------------------------------------------    KeyError: Interval(2, 3, closed='right')     In [7]: s.loc[pd.Interval(2, 3)]    ---------------------------------------------------------------------------    KeyError: Interval(2, 3, closed='right')  The `~IntervalIndex.overlaps` method can be used to create a boolean indexer that replicates the``\` previous behavior of returning overlapping matches.

*New behavior*:

<div class="ipython">

python

idxr = s.index.overlaps(pd.Interval(2, 3)) idxr s\[idxr\] s.loc\[idxr\]

</div>

### Binary ufuncs on Series now align

Applying a binary ufunc like <span class="title-ref">numpy.power</span> now aligns the inputs when both are <span class="title-ref">Series</span> (`23293`).

<div class="ipython">

python

s1 = pd.Series(\[1, 2, 3\], index=\['a', 'b', 'c'\]) s2 = pd.Series(\[3, 4, 5\], index=\['d', 'c', 'b'\]) s1 s2

</div>

*Previous behavior*

`` `ipython    In [5]: np.power(s1, s2)    Out[5]:    a      1    b     16    c    243    dtype: int64  *New behavior*  .. ipython:: python     np.power(s1, s2)  This matches the behavior of other binary operations in pandas, like `Series.add`. ``<span class="title-ref"> To retain the previous behavior, convert the other </span><span class="title-ref">Series</span>\` to an array before applying the ufunc.

<div class="ipython">

python

np.power(s1, s2.array)

</div>

### Categorical.argsort now places missing values at the end

<span class="title-ref">Categorical.argsort</span> now places missing values at the end of the array, making it consistent with NumPy and the rest of pandas (`21801`).

<div class="ipython">

python

cat = pd.Categorical(\['b', None, 'a'\], categories=\['a', 'b'\], ordered=True)

</div>

*Previous behavior*

`` `ipython    In [2]: cat = pd.Categorical(['b', None, 'a'], categories=['a', 'b'], ordered=True)     In [3]: cat.argsort()    Out[3]: array([1, 2, 0])     In [4]: cat[cat.argsort()]    Out[4]:    [NaN, a, b]    categories (2, object): [a < b]  *New behavior*  .. ipython:: python     cat.argsort()    cat[cat.argsort()]  .. _whatsnew_0250.api_breaking.list_of_dict:  Column order is preserved when passing a list of dicts to DataFrame ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Starting with Python 3.7 the key-order of `dict` is [guaranteed](https://mail.python.org/pipermail/python-dev/2017-December/151283.html). In practice, this has been true since Python 3.6. The <span class="title-ref">DataFrame</span> constructor now treats a list of dicts in the same way as it does a list of `OrderedDict`, i.e. preserving the order of the dicts. This change applies only when pandas is running on Python\>=3.6 (`27309`).

<div class="ipython">

python

  - data = \[  
    {'name': 'Joe', 'state': 'NY', 'age': 18}, {'name': 'Jane', 'state': 'KY', 'age': 19, 'hobby': 'Minecraft'}, {'name': 'Jean', 'state': 'OK', 'age': 20, 'finances': 'good'}

\]

</div>

*Previous Behavior*:

The columns were lexicographically sorted previously,

`` `python    In [1]: pd.DataFrame(data)    Out[1]:       age finances      hobby  name state    0   18      NaN        NaN   Joe    NY    1   19      NaN  Minecraft  Jane    KY    2   20     good        NaN  Jean    OK  *New Behavior*:  The column order now matches the insertion-order of the keys in the ``dict`,`\` considering all the records from top to bottom. As a consequence, the column order of the resulting DataFrame has changed compared to previous pandas versions.

<div class="ipython">

python

pd.DataFrame(data)

</div>

### Increased minimum versions for dependencies

Due to dropping support for Python 2.7, a number of optional dependencies have updated minimum versions (`25725`, `24942`, `25752`). Independently, some minimum supported versions of dependencies were updated (`23519`, `25554`). If installed, we now require:

<table style="width:65%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.13.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytz</td>
<td>2015.4</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.6.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.2.1</td>
<td></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.6.2</td>
<td></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>4.0.2</td>
<td></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

| Package        | Minimum Version |
| -------------- | --------------- |
| beautifulsoup4 | 4.6.0           |
| fastparquet    | 0.2.1           |
| gcsfs          | 0.2.2           |
| lxml           | 3.8.0           |
| matplotlib     | 2.2.2           |
| openpyxl       | 2.4.8           |
| pyarrow        | 0.9.0           |
| pymysql        | 0.7.1           |
| pytables       | 3.4.2           |
| scipy          | 0.19.0          |
| sqlalchemy     | 1.1.4           |
| xarray         | 0.8.2           |
| xlrd           | 1.1.0           |
| xlsxwriter     | 0.9.8           |
| xlwt           | 1.2.0           |

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - <span class="title-ref">DatetimeTZDtype</span> will now standardize pytz timezones to a common timezone instance (`24713`)
  - <span class="title-ref">Timestamp</span> and <span class="title-ref">Timedelta</span> scalars now implement the <span class="title-ref">to\_numpy</span> method as aliases to <span class="title-ref">Timestamp.to\_datetime64</span> and <span class="title-ref">Timedelta.to\_timedelta64</span>, respectively. (`24653`)
  - <span class="title-ref">Timestamp.strptime</span> will now rise a `NotImplementedError` (`25016`)
  - Comparing <span class="title-ref">Timestamp</span> with unsupported objects now returns :py`NotImplemented` instead of raising `TypeError`. This implies that unsupported rich comparisons are delegated to the other object, and are now consistent with Python 3 behavior for `datetime` objects (`24011`)
  - Bug in <span class="title-ref">DatetimeIndex.snap</span> which didn't preserving the `name` of the input <span class="title-ref">Index</span> (`25575`)
  - The `arg` argument in <span class="title-ref">.DataFrameGroupBy.agg</span> has been renamed to `func` (`26089`)
  - The `arg` argument in <span class="title-ref">.Window.aggregate</span> has been renamed to `func` (`26372`)
  - Most pandas classes had a `__bytes__` method, which was used for getting a python2-style bytestring representation of the object. This method has been removed as a part of dropping Python2 (`26447`)
  - The `.str`-accessor has been disabled for 1-level <span class="title-ref">MultiIndex</span>, use <span class="title-ref">MultiIndex.to\_flat\_index</span> if necessary (`23679`)
  - Removed support of gtk package for clipboards (`26563`)
  - Using an unsupported version of Beautiful Soup 4 will now raise an `ImportError` instead of a `ValueError` (`27063`)
  - <span class="title-ref">Series.to\_excel</span> and <span class="title-ref">DataFrame.to\_excel</span> will now raise a `ValueError` when saving timezone aware data. (`27008`, `7056`)
  - <span class="title-ref">ExtensionArray.argsort</span> places NA values at the end of the sorted array. (`21801`)
  - <span class="title-ref">DataFrame.to\_hdf</span> and <span class="title-ref">Series.to\_hdf</span> will now raise a `NotImplementedError` when saving a <span class="title-ref">MultiIndex</span> with extension data types for a `fixed` format. (`7775`)
  - Passing duplicate `names` in <span class="title-ref">read\_csv</span> will now raise a `ValueError` (`17346`)

## Deprecations

### Sparse subclasses

The `SparseSeries` and `SparseDataFrame` subclasses are deprecated. Their functionality is better-provided by a `Series` or `DataFrame` with sparse values.

**Previous way**

`` `python    df = pd.SparseDataFrame({"A": [0, 0, 1, 2]})    df.dtypes  **New way**  .. ipython:: python     df = pd.DataFrame({"A": pd.arrays.SparseArray([0, 0, 1, 2])})    df.dtypes  The memory usage of the two approaches is identical (:issue:`19239`).  msgpack format ``\` ^^^^^^^^^^^^^^

The msgpack format is deprecated as of 0.25 and will be removed in a future version. It is recommended to use pyarrow for on-the-wire transmission of pandas objects. (`27084`)

### Other deprecations

  - The deprecated `.ix[]` indexer now raises a more visible `FutureWarning` instead of `DeprecationWarning` (`26438`).
  - Deprecated the `units=M` (months) and `units=Y` (year) parameters for `units` of <span class="title-ref">pandas.to\_timedelta</span>, <span class="title-ref">pandas.Timedelta</span> and <span class="title-ref">pandas.TimedeltaIndex</span> (`16344`)
  - <span class="title-ref">pandas.concat</span> has deprecated the `join_axes`-keyword. Instead, use <span class="title-ref">DataFrame.reindex</span> or <span class="title-ref">DataFrame.reindex\_like</span> on the result or on the inputs (`21951`)
  - The <span class="title-ref">SparseArray.values</span> attribute is deprecated. You can use `np.asarray(...)` or the <span class="title-ref">SparseArray.to\_dense</span> method instead (`26421`).
  - The functions <span class="title-ref">pandas.to\_datetime</span> and <span class="title-ref">pandas.to\_timedelta</span> have deprecated the `box` keyword. Instead, use <span class="title-ref">to\_numpy</span> or <span class="title-ref">Timestamp.to\_datetime64</span> or <span class="title-ref">Timedelta.to\_timedelta64</span>. (`24416`)
  - The <span class="title-ref">DataFrame.compound</span> and <span class="title-ref">Series.compound</span> methods are deprecated and will be removed in a future version (`26405`).
  - The internal attributes `_start`, `_stop` and `_step` attributes of <span class="title-ref">RangeIndex</span> have been deprecated. Use the public attributes <span class="title-ref">\~RangeIndex.start</span>, <span class="title-ref">\~RangeIndex.stop</span> and <span class="title-ref">\~RangeIndex.step</span> instead (`26581`).
  - The <span class="title-ref">Series.ftype</span>, <span class="title-ref">Series.ftypes</span> and <span class="title-ref">DataFrame.ftypes</span> methods are deprecated and will be removed in a future version. Instead, use <span class="title-ref">Series.dtype</span> and <span class="title-ref">DataFrame.dtypes</span> (`26705`).
  - The <span class="title-ref">Series.get\_values</span>, <span class="title-ref">DataFrame.get\_values</span>, <span class="title-ref">Index.get\_values</span>, <span class="title-ref">SparseArray.get\_values</span> and <span class="title-ref">Categorical.get\_values</span> methods are deprecated. One of `np.asarray(..)` or <span class="title-ref">\~Series.to\_numpy</span> can be used instead (`19617`).
  - The 'outer' method on NumPy ufuncs, e.g. `np.subtract.outer` has been deprecated on <span class="title-ref">Series</span> objects. Convert the input to an array with <span class="title-ref">Series.array</span> first (`27186`)
  - <span class="title-ref">Timedelta.resolution</span> is deprecated and replaced with <span class="title-ref">Timedelta.resolution\_string</span>. In a future version, <span class="title-ref">Timedelta.resolution</span> will be changed to behave like the standard library <span class="title-ref">datetime.timedelta.resolution</span> (`21344`)
  - <span class="title-ref">read\_table</span> has been undeprecated. (`25220`)
  - <span class="title-ref">Index.dtype\_str</span> is deprecated. (`18262`)
  - <span class="title-ref">Series.imag</span> and <span class="title-ref">Series.real</span> are deprecated. (`18262`)
  - <span class="title-ref">Series.put</span> is deprecated. (`18262`)
  - <span class="title-ref">Index.item</span> and <span class="title-ref">Series.item</span> is deprecated. (`18262`)
  - The default value `ordered=None` in <span class="title-ref">\~pandas.api.types.CategoricalDtype</span> has been deprecated in favor of `ordered=False`. When converting between categorical types `ordered=True` must be explicitly passed in order to be preserved. (`26336`)
  - <span class="title-ref">Index.contains</span> is deprecated. Use `key in index` (`__contains__`) instead (`17753`).
  - <span class="title-ref">DataFrame.get\_dtype\_counts</span> is deprecated. (`18262`)
  - <span class="title-ref">Categorical.ravel</span> will return a <span class="title-ref">Categorical</span> instead of a `np.ndarray` (`27199`)

## Removal of prior version deprecations/changes

  - Removed `Panel` (`25047`, `25191`, `25231`)
  - Removed the previously deprecated `sheetname` keyword in <span class="title-ref">read\_excel</span> (`16442`, `20938`)
  - Removed the previously deprecated `TimeGrouper` (`16942`)
  - Removed the previously deprecated `parse_cols` keyword in <span class="title-ref">read\_excel</span> (`16488`)
  - Removed the previously deprecated `pd.options.html.border` (`16970`)
  - Removed the previously deprecated `convert_objects` (`11221`)
  - Removed the previously deprecated `select` method of `DataFrame` and `Series` (`17633`)
  - Removed the previously deprecated behavior of <span class="title-ref">Series</span> treated as list-like in <span class="title-ref">\~Series.cat.rename\_categories</span> (`17982`)
  - Removed the previously deprecated `DataFrame.reindex_axis` and `Series.reindex_axis` (`17842`)
  - Removed the previously deprecated behavior of altering column or index labels with <span class="title-ref">Series.rename\_axis</span> or <span class="title-ref">DataFrame.rename\_axis</span> (`17842`)
  - Removed the previously deprecated `tupleize_cols` keyword argument in <span class="title-ref">read\_html</span>, <span class="title-ref">read\_csv</span>, and <span class="title-ref">DataFrame.to\_csv</span> (`17877`, `17820`)
  - Removed the previously deprecated `DataFrame.from.csv` and `Series.from_csv` (`17812`)
  - Removed the previously deprecated `raise_on_error` keyword argument in <span class="title-ref">DataFrame.where</span> and <span class="title-ref">DataFrame.mask</span> (`17744`)
  - Removed the previously deprecated `ordered` and `categories` keyword arguments in `astype` (`17742`)
  - Removed the previously deprecated `cdate_range` (`17691`)
  - Removed the previously deprecated `True` option for the `dropna` keyword argument in <span class="title-ref">SeriesGroupBy.nth</span> (`17493`)
  - Removed the previously deprecated `convert` keyword argument in <span class="title-ref">Series.take</span> and <span class="title-ref">DataFrame.take</span> (`17352`)
  - Removed the previously deprecated behavior of arithmetic operations with `datetime.date` objects (`21152`)

## Performance improvements

  - Significant speedup in <span class="title-ref">SparseArray</span> initialization that benefits most operations, fixing performance regression introduced in v0.20.0 (`24985`)
  - <span class="title-ref">DataFrame.to\_stata</span> is now faster when outputting data with any string or non-native endian columns (`25045`)
  - Improved performance of <span class="title-ref">Series.searchsorted</span>. The speedup is especially large when the dtype is int8/int16/int32 and the searched key is within the integer bounds for the dtype (`22034`)
  - Improved performance of <span class="title-ref">.GroupBy.quantile</span> (`20405`)
  - Improved performance of slicing and other selected operation on a <span class="title-ref">RangeIndex</span> (`26565`, `26617`, `26722`)
  - <span class="title-ref">RangeIndex</span> now performs standard lookup without instantiating an actual hashtable, hence saving memory (`16685`)
  - Improved performance of <span class="title-ref">read\_csv</span> by faster tokenizing and faster parsing of small float numbers (`25784`)
  - Improved performance of <span class="title-ref">read\_csv</span> by faster parsing of N/A and boolean values (`25804`)
  - Improved performance of <span class="title-ref">IntervalIndex.is\_monotonic</span>, <span class="title-ref">IntervalIndex.is\_monotonic\_increasing</span> and <span class="title-ref">IntervalIndex.is\_monotonic\_decreasing</span> by removing conversion to <span class="title-ref">MultiIndex</span> (`24813`)
  - Improved performance of <span class="title-ref">DataFrame.to\_csv</span> when writing datetime dtypes (`25708`)
  - Improved performance of <span class="title-ref">read\_csv</span> by much faster parsing of `MM/YYYY` and `DD/MM/YYYY` datetime formats (`25922`)
  - Improved performance of nanops for dtypes that cannot store NaNs. Speedup is particularly prominent for <span class="title-ref">Series.all</span> and <span class="title-ref">Series.any</span> (`25070`)
  - Improved performance of <span class="title-ref">Series.map</span> for dictionary mappers on categorical series by mapping the categories instead of mapping all values (`23785`)
  - Improved performance of <span class="title-ref">IntervalIndex.intersection</span> (`24813`)
  - Improved performance of <span class="title-ref">read\_csv</span> by faster concatenating date columns without extra conversion to string for integer/float zero and float `NaN`; by faster checking the string for the possibility of being a date (`25754`)
  - Improved performance of <span class="title-ref">IntervalIndex.is\_unique</span> by removing conversion to `MultiIndex` (`24813`)
  - Restored performance of <span class="title-ref">DatetimeIndex.\_\_iter\_\_</span> by re-enabling specialized code path (`26702`)
  - Improved performance when building <span class="title-ref">MultiIndex</span> with at least one <span class="title-ref">CategoricalIndex</span> level (`22044`)
  - Improved performance by removing the need for a garbage collect when checking for `SettingWithCopyWarning` (`27031`)
  - For <span class="title-ref">to\_datetime</span> changed default value of cache parameter to `True` (`26043`)
  - Improved performance of <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">PeriodIndex</span> slicing given non-unique, monotonic data (`27136`).
  - Improved performance of <span class="title-ref">pd.read\_json</span> for index-oriented data. (`26773`)
  - Improved performance of <span class="title-ref">MultiIndex.shape</span> (`27384`).

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">DataFrame.at</span> and <span class="title-ref">Series.at</span> that would raise exception if the index was a <span class="title-ref">CategoricalIndex</span> (`20629`)
  - Fixed bug in comparison of ordered <span class="title-ref">Categorical</span> that contained missing values with a scalar which sometimes incorrectly resulted in `True` (`26504`)
  - Bug in <span class="title-ref">DataFrame.dropna</span> when the <span class="title-ref">DataFrame</span> has a <span class="title-ref">CategoricalIndex</span> containing <span class="title-ref">Interval</span> objects incorrectly raised a `TypeError` (`25087`)

### Datetimelike

  - Bug in <span class="title-ref">to\_datetime</span> which would raise an (incorrect) `ValueError` when called with a date far into the future and the `format` argument specified instead of raising `OutOfBoundsDatetime` (`23830`)
  - Bug in <span class="title-ref">to\_datetime</span> which would raise `InvalidIndexError: Reindexing only valid with uniquely valued Index objects` when called with `cache=True`, with `arg` including at least two different elements from the set `{None, numpy.nan, pandas.NaT}` (`22305`)
  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> where timezone aware data with `dtype='datetime64[ns]` was not cast to naive (`25843`)
  - Improved <span class="title-ref">Timestamp</span> type checking in various datetime functions to prevent exceptions when using a subclassed `datetime` (`25851`)
  - Bug in <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> repr where `np.datetime64('NaT')` and `np.timedelta64('NaT')` with `dtype=object` would be represented as `NaN` (`25445`)
  - Bug in <span class="title-ref">to\_datetime</span> which does not replace the invalid argument with `NaT` when error is set to coerce (`26122`)
  - Bug in adding <span class="title-ref">DateOffset</span> with nonzero month to <span class="title-ref">DatetimeIndex</span> would raise `ValueError` (`26258`)
  - Bug in <span class="title-ref">to\_datetime</span> which raises unhandled `OverflowError` when called with mix of invalid dates and `NaN` values with `format='%Y%m%d'` and `error='coerce'` (`25512`)
  - Bug in <span class="title-ref">isin</span> for datetimelike indexes; <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span> and <span class="title-ref">PeriodIndex</span> where the `levels` parameter was ignored. (`26675`)
  - Bug in <span class="title-ref">to\_datetime</span> which raises `TypeError` for `format='%Y%m%d'` when called for invalid integer dates with length \>= 6 digits with `errors='ignore'`
  - Bug when comparing a <span class="title-ref">PeriodIndex</span> against a zero-dimensional numpy array (`26689`)
  - Bug in constructing a `Series` or `DataFrame` from a numpy `datetime64` array with a non-ns unit and out-of-bound timestamps generating rubbish data, which will now correctly raise an `OutOfBoundsDatetime` error (`26206`).
  - Bug in <span class="title-ref">date\_range</span> with unnecessary `OverflowError` being raised for very large or very small dates (`26651`)
  - Bug where adding <span class="title-ref">Timestamp</span> to a `np.timedelta64` object would raise instead of returning a <span class="title-ref">Timestamp</span> (`24775`)
  - Bug where comparing a zero-dimensional numpy array containing a `np.datetime64` object to a <span class="title-ref">Timestamp</span> would incorrect raise `TypeError` (`26916`)
  - Bug in <span class="title-ref">to\_datetime</span> which would raise `ValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True` when called with `cache=True`, with `arg` including datetime strings with different offset (`26097`)
  - 
### Timedelta

  - Bug in <span class="title-ref">TimedeltaIndex.intersection</span> where for non-monotonic indices in some cases an empty `Index` was returned when in fact an intersection existed (`25913`)
  - Bug with comparisons between <span class="title-ref">Timedelta</span> and `NaT` raising `TypeError` (`26039`)
  - Bug when adding or subtracting a <span class="title-ref">BusinessHour</span> to a <span class="title-ref">Timestamp</span> with the resulting time landing in a following or prior day respectively (`26381`)
  - Bug when comparing a <span class="title-ref">TimedeltaIndex</span> against a zero-dimensional numpy array (`26689`)

### Timezones

  - Bug in <span class="title-ref">DatetimeIndex.to\_frame</span> where timezone aware data would be converted to timezone naive data (`25809`)
  - Bug in <span class="title-ref">to\_datetime</span> with `utc=True` and datetime strings that would apply previously parsed UTC offsets to subsequent arguments (`24992`)
  - Bug in <span class="title-ref">Timestamp.tz\_localize</span> and <span class="title-ref">Timestamp.tz\_convert</span> does not propagate `freq` (`25241`)
  - Bug in <span class="title-ref">Series.at</span> where setting <span class="title-ref">Timestamp</span> with timezone raises `TypeError` (`25506`)
  - Bug in <span class="title-ref">DataFrame.update</span> when updating with timezone aware data would return timezone naive data (`25807`)
  - Bug in <span class="title-ref">to\_datetime</span> where an uninformative `RuntimeError` was raised when passing a naive <span class="title-ref">Timestamp</span> with datetime strings with mixed UTC offsets (`25978`)
  - Bug in <span class="title-ref">to\_datetime</span> with `unit='ns'` would drop timezone information from the parsed argument (`26168`)
  - Bug in <span class="title-ref">DataFrame.join</span> where joining a timezone aware index with a timezone aware column would result in a column of `NaN` (`26335`)
  - Bug in <span class="title-ref">date\_range</span> where ambiguous or nonexistent start or end times were not handled by the `ambiguous` or `nonexistent` keywords respectively (`27088`)
  - Bug in <span class="title-ref">DatetimeIndex.union</span> when combining a timezone aware and timezone unaware <span class="title-ref">DatetimeIndex</span> (`21671`)
  - Bug when applying a numpy reduction function (e.g. <span class="title-ref">numpy.minimum</span>) to a timezone aware <span class="title-ref">Series</span> (`15552`)

### Numeric

  - Bug in <span class="title-ref">to\_numeric</span> in which large negative numbers were being improperly handled (`24910`)
  - Bug in <span class="title-ref">to\_numeric</span> in which numbers were being coerced to float, even though `errors` was not `coerce` (`24910`)
  - Bug in <span class="title-ref">to\_numeric</span> in which invalid values for `errors` were being allowed (`26466`)
  - Bug in <span class="title-ref">format</span> in which floating point complex numbers were not being formatted to proper display precision and trimming (`25514`)
  - Bug in error messages in <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">Series.corr</span>. Added the possibility of using a callable. (`25729`)
  - Bug in <span class="title-ref">Series.divmod</span> and <span class="title-ref">Series.rdivmod</span> which would raise an (incorrect) `ValueError` rather than return a pair of <span class="title-ref">Series</span> objects as result (`25557`)
  - Raises a helpful exception when a non-numeric index is sent to <span class="title-ref">interpolate</span> with methods which require numeric index. (`21662`)
  - Bug in <span class="title-ref">\~pandas.eval</span> when comparing floats with scalar operators, for example: `x < -0.1` (`25928`)
  - Fixed bug where casting all-boolean array to integer extension array failed (`25211`)
  - Bug in `divmod` with a <span class="title-ref">Series</span> object containing zeros incorrectly raising `AttributeError` (`26987`)
  - Inconsistency in <span class="title-ref">Series</span> floor-division (`//`) and `divmod` filling positive//zero with `NaN` instead of `Inf` (`27321`)
  - 
### Conversion

  - Bug in <span class="title-ref">DataFrame.astype</span> when passing a dict of columns and types the `errors` parameter was ignored. (`25905`)
  - 
### Strings

  - Bug in the `__name__` attribute of several methods of <span class="title-ref">Series.str</span>, which were set incorrectly (`23551`)
  - Improved error message when passing <span class="title-ref">Series</span> of wrong dtype to <span class="title-ref">Series.str.cat</span> (`22722`)
  - 
### Interval

  - Construction of <span class="title-ref">Interval</span> is restricted to numeric, <span class="title-ref">Timestamp</span> and <span class="title-ref">Timedelta</span> endpoints (`23013`)
  - Fixed bug in <span class="title-ref">Series</span>/<span class="title-ref">DataFrame</span> not displaying `NaN` in <span class="title-ref">IntervalIndex</span> with missing values (`25984`)
  - Bug in <span class="title-ref">IntervalIndex.get\_loc</span> where a `KeyError` would be incorrectly raised for a decreasing <span class="title-ref">IntervalIndex</span> (`25860`)
  - Bug in <span class="title-ref">Index</span> constructor where passing mixed closed <span class="title-ref">Interval</span> objects would result in a `ValueError` instead of an `object` dtype `Index` (`27172`)

### Indexing

  - Improved exception message when calling <span class="title-ref">DataFrame.iloc</span> with a list of non-numeric objects (`25753`).
  - Improved exception message when calling `.iloc` or `.loc` with a boolean indexer with different length (`26658`).
  - Bug in `KeyError` exception message when indexing a <span class="title-ref">MultiIndex</span> with a non-existent key not displaying the original key (`27250`).
  - Bug in `.iloc` and `.loc` with a boolean indexer not raising an `IndexError` when too few items are passed (`26658`).
  - Bug in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> where `KeyError` was not raised for a `MultiIndex` when the key was less than or equal to the number of levels in the <span class="title-ref">MultiIndex</span> (`14885`).
  - Bug in which <span class="title-ref">DataFrame.append</span> produced an erroneous warning indicating that a `KeyError` will be thrown in the future when the data to be appended contains new columns (`22252`).
  - Bug in which <span class="title-ref">DataFrame.to\_csv</span> caused a segfault for a reindexed data frame, when the indices were single-level <span class="title-ref">MultiIndex</span> (`26303`).
  - Fixed bug where assigning a <span class="title-ref">arrays.PandasArray</span> to a <span class="title-ref">.DataFrame</span> would raise error (`26390`)
  - Allow keyword arguments for callable local reference used in the <span class="title-ref">DataFrame.query</span> string (`26426`)
  - Fixed a `KeyError` when indexing a <span class="title-ref">MultiIndex</span> level with a list containing exactly one label, which is missing (`27148`)
  - Bug which produced `AttributeError` on partial matching <span class="title-ref">Timestamp</span> in a <span class="title-ref">MultiIndex</span> (`26944`)
  - Bug in <span class="title-ref">Categorical</span> and <span class="title-ref">CategoricalIndex</span> with <span class="title-ref">Interval</span> values when using the `in` operator (`__contains`) with objects that are not comparable to the values in the `Interval` (`23705`)
  - Bug in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">DataFrame.iloc</span> on a <span class="title-ref">DataFrame</span> with a single timezone-aware datetime64\[ns\] column incorrectly returning a scalar instead of a <span class="title-ref">Series</span> (`27110`)
  - Bug in <span class="title-ref">CategoricalIndex</span> and <span class="title-ref">Categorical</span> incorrectly raising `ValueError` instead of `TypeError` when a list is passed using the `in` operator (`__contains__`) (`21729`)
  - Bug in setting a new value in a <span class="title-ref">Series</span> with a <span class="title-ref">Timedelta</span> object incorrectly casting the value to an integer (`22717`)
  - Bug in <span class="title-ref">Series</span> setting a new key (`__setitem__`) with a timezone-aware datetime incorrectly raising `ValueError` (`12862`)
  - Bug in <span class="title-ref">DataFrame.iloc</span> when indexing with a read-only indexer (`17192`)
  - Bug in <span class="title-ref">Series</span> setting an existing tuple key (`__setitem__`) with timezone-aware datetime values incorrectly raising `TypeError` (`20441`)

### Missing

  - Fixed misleading exception message in <span class="title-ref">Series.interpolate</span> if argument `order` is required, but omitted (`10633`, `24014`).
  - Fixed class type displayed in exception message in <span class="title-ref">DataFrame.dropna</span> if invalid `axis` parameter passed (`25555`)
  - A `ValueError` will now be thrown by <span class="title-ref">DataFrame.fillna</span> when `limit` is not a positive integer (`27042`)
  - 
### MultiIndex

  - Bug in which incorrect exception raised by <span class="title-ref">Timedelta</span> when testing the membership of <span class="title-ref">MultiIndex</span> (`24570`)
  - 
### IO

  - Bug in <span class="title-ref">DataFrame.to\_html</span> where values were truncated using display options instead of outputting the full content (`17004`)
  - Fixed bug in missing text when using <span class="title-ref">to\_clipboard</span> if copying utf-16 characters in Python 3 on Windows (`25040`)
  - Bug in <span class="title-ref">read\_json</span> for `orient='table'` when it tries to infer dtypes by default, which is not applicable as dtypes are already defined in the JSON schema (`21345`)
  - Bug in <span class="title-ref">read\_json</span> for `orient='table'` and float index, as it infers index dtype by default, which is not applicable because index dtype is already defined in the JSON schema (`25433`)
  - Bug in <span class="title-ref">read\_json</span> for `orient='table'` and string of float column names, as it makes a column name type conversion to <span class="title-ref">Timestamp</span>, which is not applicable because column names are already defined in the JSON schema (`25435`)
  - Bug in <span class="title-ref">json\_normalize</span> for `errors='ignore'` where missing values in the input data, were filled in resulting `DataFrame` with the string `"nan"` instead of `numpy.nan` (`25468`)
  - <span class="title-ref">DataFrame.to\_html</span> now raises `TypeError` when using an invalid type for the `classes` parameter instead of `AssertionError` (`25608`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> and <span class="title-ref">DataFrame.to\_latex</span> that would lead to incorrect output when the `header` keyword is used (`16718`)
  - Bug in <span class="title-ref">read\_csv</span> not properly interpreting the UTF8 encoded filenames on Windows on Python 3.6+ (`15086`)
  - Improved performance in <span class="title-ref">pandas.read\_stata</span> and <span class="title-ref">pandas.io.stata.StataReader</span> when converting columns that have missing values (`25772`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> where header numbers would ignore display options when rounding (`17280`)
  - Bug in <span class="title-ref">read\_hdf</span> where reading a table from an HDF5 file written directly with PyTables fails with a `ValueError` when using a sub-selection via the `start` or `stop` arguments (`11188`)
  - Bug in <span class="title-ref">read\_hdf</span> not properly closing store after a `KeyError` is raised (`25766`)
  - Improved the explanation for the failure when value labels are repeated in Stata dta files and suggested workarounds (`25772`)
  - Improved <span class="title-ref">pandas.read\_stata</span> and <span class="title-ref">pandas.io.stata.StataReader</span> to read incorrectly formatted 118 format files saved by Stata (`25960`)
  - Improved the `col_space` parameter in <span class="title-ref">DataFrame.to\_html</span> to accept a string so CSS length values can be set correctly (`25941`)
  - Fixed bug in loading objects from S3 that contain `#` characters in the URL (`25945`)
  - Adds `use_bqstorage_api` parameter to <span class="title-ref">read\_gbq</span> to speed up downloads of large data frames. This feature requires version 0.10.0 of the `pandas-gbq` library as well as the `google-cloud-bigquery-storage` and `fastavro` libraries. (`26104`)
  - Fixed memory leak in <span class="title-ref">DataFrame.to\_json</span> when dealing with numeric data (`24889`)
  - Bug in <span class="title-ref">read\_json</span> where date strings with `Z` were not converted to a UTC timezone (`26168`)
  - Added `cache_dates=True` parameter to <span class="title-ref">read\_csv</span>, which allows to cache unique dates when they are parsed (`25990`)
  - <span class="title-ref">DataFrame.to\_excel</span> now raises a `ValueError` when the caller's dimensions exceed the limitations of Excel (`26051`)
  - Fixed bug in <span class="title-ref">pandas.read\_csv</span> where a BOM would result in incorrect parsing using engine='python' (`26545`)
  - <span class="title-ref">read\_excel</span> now raises a `ValueError` when input is of type <span class="title-ref">pandas.io.excel.ExcelFile</span> and `engine` param is passed since <span class="title-ref">pandas.io.excel.ExcelFile</span> has an engine defined (`26566`)
  - Bug while selecting from <span class="title-ref">HDFStore</span> with `where=''` specified (`26610`).
  - Fixed bug in <span class="title-ref">DataFrame.to\_excel</span> where custom objects (i.e. `PeriodIndex`) inside merged cells were not being converted into types safe for the Excel writer (`27006`)
  - Bug in <span class="title-ref">read\_hdf</span> where reading a timezone aware <span class="title-ref">DatetimeIndex</span> would raise a `TypeError` (`11926`)
  - Bug in <span class="title-ref">to\_msgpack</span> and <span class="title-ref">read\_msgpack</span> which would raise a `ValueError` rather than a `FileNotFoundError` for an invalid path (`27160`)
  - Fixed bug in <span class="title-ref">DataFrame.to\_parquet</span> which would raise a `ValueError` when the dataframe had no columns (`27339`)
  - Allow parsing of <span class="title-ref">PeriodDtype</span> columns when using <span class="title-ref">read\_csv</span> (`26934`)

### Plotting

  - Fixed bug where <span class="title-ref">api.extensions.ExtensionArray</span> could not be used in matplotlib plotting (`25587`)
  - Bug in an error message in <span class="title-ref">DataFrame.plot</span>. Improved the error message if non-numerics are passed to <span class="title-ref">DataFrame.plot</span> (`25481`)
  - Bug in incorrect ticklabel positions when plotting an index that are non-numeric / non-datetime (`7612`, `15912`, `22334`)
  - Fixed bug causing plots of <span class="title-ref">PeriodIndex</span> timeseries to fail if the frequency is a multiple of the frequency rule code (`14763`)
  - Fixed bug when plotting a <span class="title-ref">DatetimeIndex</span> with `datetime.timezone.utc` timezone (`17173`)
  - 
### GroupBy/resample/rolling

  - Bug in <span class="title-ref">.Resampler.agg</span> with a timezone aware index where `OverflowError` would raise when passing a list of functions (`22660`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.nunique</span> in which the names of column levels were lost (`23222`)
  - Bug in <span class="title-ref">.GroupBy.agg</span> when applying an aggregation function to timezone aware data (`23683`)
  - Bug in <span class="title-ref">.GroupBy.first</span> and <span class="title-ref">.GroupBy.last</span> where timezone information would be dropped (`21603`)
  - Bug in <span class="title-ref">.GroupBy.size</span> when grouping only NA values (`23050`)
  - Bug in <span class="title-ref">Series.groupby</span> where `observed` kwarg was previously ignored (`24880`)
  - Bug in <span class="title-ref">Series.groupby</span> where using `groupby` with a <span class="title-ref">MultiIndex</span> Series with a list of labels equal to the length of the series caused incorrect grouping (`25704`)
  - Ensured that ordering of outputs in `groupby` aggregation functions is consistent across all versions of Python (`25692`)
  - Ensured that result group order is correct when grouping on an ordered `Categorical` and specifying `observed=True` (`25871`, `25167`)
  - Bug in <span class="title-ref">.Rolling.min</span> and <span class="title-ref">.Rolling.max</span> that caused a memory leak (`25893`)
  - Bug in <span class="title-ref">.Rolling.count</span> and `.Expanding.count` was previously ignoring the `axis` keyword (`13503`)
  - Bug in <span class="title-ref">.GroupBy.idxmax</span> and <span class="title-ref">.GroupBy.idxmin</span> with datetime column would return incorrect dtype (`25444`, `15306`)
  - Bug in <span class="title-ref">.GroupBy.cumsum</span>, <span class="title-ref">.GroupBy.cumprod</span>, <span class="title-ref">.GroupBy.cummin</span> and <span class="title-ref">.GroupBy.cummax</span> with categorical column having absent categories, would return incorrect result or segfault (`16771`)
  - Bug in <span class="title-ref">.GroupBy.nth</span> where NA values in the grouping would return incorrect results (`26011`)
  - Bug in <span class="title-ref">.SeriesGroupBy.transform</span> where transforming an empty group would raise a `ValueError` (`26208`)
  - Bug in <span class="title-ref">.DataFrame.groupby</span> where passing a <span class="title-ref">.Grouper</span> would return incorrect groups when using the `.groups` accessor (`26326`)
  - Bug in <span class="title-ref">.GroupBy.agg</span> where incorrect results are returned for uint64 columns. (`26310`)
  - Bug in <span class="title-ref">.Rolling.median</span> and <span class="title-ref">.Rolling.quantile</span> where MemoryError is raised with empty window (`26005`)
  - Bug in <span class="title-ref">.Rolling.median</span> and <span class="title-ref">.Rolling.quantile</span> where incorrect results are returned with `closed='left'` and `closed='neither'` (`26005`)
  - Improved <span class="title-ref">.Rolling</span>, <span class="title-ref">.Window</span> and <span class="title-ref">.ExponentialMovingWindow</span> functions to exclude nuisance columns from results instead of raising errors and raise a `DataError` only if all columns are nuisance (`12537`)
  - Bug in <span class="title-ref">.Rolling.max</span> and <span class="title-ref">.Rolling.min</span> where incorrect results are returned with an empty variable window (`26005`)
  - Raise a helpful exception when an unsupported weighted window function is used as an argument of <span class="title-ref">.Window.aggregate</span> (`26597`)

### Reshaping

  - Bug in <span class="title-ref">pandas.merge</span> adds a string of `None`, if `None` is assigned in suffixes instead of remain the column name as-is (`24782`).
  - Bug in <span class="title-ref">merge</span> when merging by index name would sometimes result in an incorrectly numbered index (missing index values are now assigned NA) (`24212`, `25009`)
  - <span class="title-ref">to\_records</span> now accepts dtypes to its `column_dtypes` parameter (`24895`)
  - Bug in <span class="title-ref">concat</span> where order of `OrderedDict` (and `dict` in Python 3.6+) is not respected, when passed in as `objs` argument (`21510`)
  - Bug in <span class="title-ref">pivot\_table</span> where columns with `NaN` values are dropped even if `dropna` argument is `False`, when the `aggfunc` argument contains a `list` (`22159`)
  - Bug in <span class="title-ref">concat</span> where the resulting `freq` of two <span class="title-ref">DatetimeIndex</span> with the same `freq` would be dropped (`3232`).
  - Bug in <span class="title-ref">merge</span> where merging with equivalent Categorical dtypes was raising an error (`22501`)
  - bug in <span class="title-ref">DataFrame</span> instantiating with a dict of iterators or generators (e.g. `pd.DataFrame({'A': reversed(range(3))})`) raised an error (`26349`).
  - Bug in <span class="title-ref">DataFrame</span> instantiating with a `range` (e.g. `pd.DataFrame(range(3))`) raised an error (`26342`).
  - Bug in <span class="title-ref">DataFrame</span> constructor when passing non-empty tuples would cause a segmentation fault (`25691`)
  - Bug in <span class="title-ref">Series.apply</span> failed when the series is a timezone aware <span class="title-ref">DatetimeIndex</span> (`25959`)
  - Bug in <span class="title-ref">pandas.cut</span> where large bins could incorrectly raise an error due to an integer overflow (`26045`)
  - Bug in <span class="title-ref">DataFrame.sort\_index</span> where an error is thrown when a multi-indexed `DataFrame` is sorted on all levels with the initial level sorted last (`26053`)
  - Bug in <span class="title-ref">Series.nlargest</span> treats `True` as smaller than `False` (`26154`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> with a <span class="title-ref">IntervalIndex</span> as pivot index would raise `TypeError` (`25814`)
  - Bug in which <span class="title-ref">DataFrame.from\_dict</span> ignored order of `OrderedDict` when `orient='index'` (`8425`).
  - Bug in <span class="title-ref">DataFrame.transpose</span> where transposing a DataFrame with a timezone-aware datetime column would incorrectly raise `ValueError` (`26825`)
  - Bug in <span class="title-ref">pivot\_table</span> when pivoting a timezone aware column as the `values` would remove timezone information (`14948`)
  - Bug in <span class="title-ref">merge\_asof</span> when specifying multiple `by` columns where one is `datetime64[ns, tz]` dtype (`26649`)

### Sparse

  - Significant speedup in <span class="title-ref">SparseArray</span> initialization that benefits most operations, fixing performance regression introduced in v0.20.0 (`24985`)
  - Bug in <span class="title-ref">SparseFrame</span> constructor where passing `None` as the data would cause `default_fill_value` to be ignored (`16807`)
  - Bug in <span class="title-ref">SparseDataFrame</span> when adding a column in which the length of values does not match length of index, `AssertionError` is raised instead of raising `ValueError` (`25484`)
  - Introduce a better error message in <span class="title-ref">Series.sparse.from\_coo</span> so it returns a `TypeError` for inputs that are not coo matrices (`26554`)
  - Bug in <span class="title-ref">numpy.modf</span> on a <span class="title-ref">SparseArray</span>. Now a tuple of <span class="title-ref">SparseArray</span> is returned (`26946`).

### Build changes

  - Fix install error with PyPy on macOS (`26536`)

### ExtensionArray

  - Bug in <span class="title-ref">factorize</span> when passing an `ExtensionArray` with a custom `na_sentinel` (`25696`).
  - <span class="title-ref">Series.count</span> miscounts NA values in ExtensionArrays (`26835`)
  - Added `Series.__array_ufunc__` to better handle NumPy ufuncs applied to Series backed by extension arrays (`23293`).
  - Keyword argument `deep` has been removed from <span class="title-ref">ExtensionArray.copy</span> (`27083`)

### Other

  - Removed unused C functions from vendored UltraJSON implementation (`26198`)
  - Allow <span class="title-ref">Index</span> and <span class="title-ref">RangeIndex</span> to be passed to numpy `min` and `max` functions (`26125`)
  - Use actual class name in repr of empty objects of a `Series` subclass (`27001`).
  - Bug in <span class="title-ref">DataFrame</span> where passing an object array of timezone-aware `datetime` objects would incorrectly raise `ValueError` (`13287`)

## Contributors

<div class="contributors">

v0.24.2..v0.25.0

</div>

---

v0.25.1.md

---

# What's new in 0.25.1 (August 21, 2019)

These are the changes in pandas 0.25.1. See \[release\](\#release) for a full changelog including other versions of pandas.

## IO and LZMA

Some users may unknowingly have an incomplete Python installation lacking the `lzma` module from the standard library. In this case, `import pandas` failed due to an `ImportError` (`27575`). pandas will now warn, rather than raising an `ImportError` if the `lzma` module is not present. Any subsequent attempt to use `lzma` methods will raise a `RuntimeError`. A possible fix for the lack of the `lzma` module is to ensure you have the necessary libraries and then re-install Python. For example, on MacOS installing Python with `pyenv` may lead to an incomplete Python installation due to unmet system dependencies at compilation time (like `xz`). Compilation will succeed, but Python might fail at run time. The issue can be solved by installing the necessary dependencies and then re-installing Python.

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">Categorical.fillna</span> that would replace all values, not just those that are `NaN` (`26215`)

### Datetimelike

  - Bug in <span class="title-ref">to\_datetime</span> where passing a timezone-naive <span class="title-ref">DatetimeArray</span> or <span class="title-ref">DatetimeIndex</span> and `utc=True` would incorrectly return a timezone-naive result (`27733`)
  - Bug in <span class="title-ref">Period.to\_timestamp</span> where a <span class="title-ref">Period</span> outside the <span class="title-ref">Timestamp</span> implementation bounds (roughly 1677-09-21 to 2262-04-11) would return an incorrect <span class="title-ref">Timestamp</span> instead of raising `OutOfBoundsDatetime` (`19643`)
  - Bug in iterating over <span class="title-ref">DatetimeIndex</span> when the underlying data is read-only (`28055`)

### Timezones

  - Bug in <span class="title-ref">Index</span> where a numpy object array with a timezone aware <span class="title-ref">Timestamp</span> and `np.nan` would not return a <span class="title-ref">DatetimeIndex</span> (`27011`)

### Numeric

  - Bug in <span class="title-ref">Series.interpolate</span> when using a timezone aware <span class="title-ref">DatetimeIndex</span> (`27548`)
  - Bug when printing negative floating point complex numbers would raise an `IndexError` (`27484`)
  - Bug where <span class="title-ref">DataFrame</span> arithmetic operators such as <span class="title-ref">DataFrame.mul</span> with a <span class="title-ref">Series</span> with axis=1 would raise an `AttributeError` on <span class="title-ref">DataFrame</span> larger than the minimum threshold to invoke numexpr (`27636`)
  - Bug in <span class="title-ref">DataFrame</span> arithmetic where missing values in results were incorrectly masked with `NaN` instead of `Inf` (`27464`)

### Conversion

  - Improved the warnings for the deprecated methods <span class="title-ref">Series.real</span> and <span class="title-ref">Series.imag</span> (`27610`)

### Interval

  - Bug in <span class="title-ref">IntervalIndex</span> where `dir(obj)` would raise `ValueError` (`27571`)

### Indexing

  - Bug in partial-string indexing returning a NumPy array rather than a `Series` when indexing with a scalar like `.loc['2015']` (`27516`)
  - Break reference cycle involving <span class="title-ref">Index</span> and other index classes to allow garbage collection of index objects without running the GC. (`27585`, `27840`)
  - Fix regression in assigning values to a single column of a DataFrame with a `MultiIndex` columns (`27841`).
  - Fix regression in `.ix` fallback with an `IntervalIndex` (`27865`).

### Missing

  - Bug in <span class="title-ref">pandas.isnull</span> or <span class="title-ref">pandas.isna</span> when the input is a type e.g. `type(pandas.Series())` (`27482`)

### IO

  - Avoid calling `S3File.s3` when reading parquet, as this was removed in s3fs version 0.3.0 (`27756`)
  - Better error message when a negative header is passed in <span class="title-ref">pandas.read\_csv</span> (`27779`)
  - Follow the `min_rows` display option (introduced in v0.25.0) correctly in the HTML repr in the notebook (`27991`).

### Plotting

  - Added a `pandas_plotting_backends` entrypoint group for registering plot backends. See \[extending.plotting-backends\](\#extending.plotting-backends) for more (`26747`).
  - Fixed the re-instatement of Matplotlib datetime converters after calling <span class="title-ref">pandas.plotting.deregister\_matplotlib\_converters</span> (`27481`).
  - Fix compatibility issue with matplotlib when passing a pandas `Index` to a plot call (`27775`).

### GroupBy/resample/rolling

  - Fixed regression in <span class="title-ref">pands.core.groupby.DataFrameGroupBy.quantile</span> raising when multiple quantiles are given (`27526`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> where applying a timezone conversion lambda function would drop timezone information (`27496`)
  - Bug in <span class="title-ref">.GroupBy.nth</span> where `observed=False` was being ignored for Categorical groupers (`26385`)
  - Bug in windowing over read-only arrays (`27766`)
  - Fixed segfault in `.DataFrameGroupBy.quantile` when an invalid quantile was passed (`27470`)

### Reshaping

  - A `KeyError` is now raised if `.unstack()` is called on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a flat <span class="title-ref">Index</span> passing a name which is not the correct one (`18303`)
  - Bug <span class="title-ref">merge\_asof</span> could not merge <span class="title-ref">Timedelta</span> objects when passing `tolerance` kwarg (`27642`)
  - Bug in <span class="title-ref">DataFrame.crosstab</span> when `margins` set to `True` and `normalize` is not `False`, an error is raised. (`27500`)
  - <span class="title-ref">DataFrame.join</span> now suppresses the `FutureWarning` when the sort parameter is specified (`21952`)
  - Bug in <span class="title-ref">DataFrame.join</span> raising with readonly arrays (`27943`)

### Sparse

  - Bug in reductions for <span class="title-ref">Series</span> with Sparse dtypes (`27080`)

### Other

  - Bug in <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> when replacing timezone-aware timestamps using a dict-like replacer (`27720`)
  - Bug in <span class="title-ref">Series.rename</span> when using a custom type indexer. Now any value that isn't callable or dict-like is treated as a scalar. (`27814`)

## Contributors

<div class="contributors">

v0.25.0..v0.25.1

</div>

---

v0.25.2.md

---

# What's new in 0.25.2 (October 15, 2019)

These are the changes in pandas 0.25.2. See \[release\](\#release) for a full changelog including other versions of pandas.

\> **Note** \> pandas 0.25.2 adds compatibility for Python 3.8 (`28147`).

## Bug fixes

### Indexing

  - Fix regression in <span class="title-ref">DataFrame.reindex</span> not following the `limit` argument (`28631`).
  - Fix regression in <span class="title-ref">RangeIndex.get\_indexer</span> for decreasing <span class="title-ref">RangeIndex</span> where target values may be improperly identified as missing/present (`28678`)

### IO

  - Fix regression in notebook display where `<th>` tags were missing for <span class="title-ref">DataFrame.index</span> values (`28204`).
  - Regression in <span class="title-ref">\~DataFrame.to\_csv</span> where writing a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> indexed by an <span class="title-ref">IntervalIndex</span> would incorrectly raise a `TypeError` (`28210`)
  - Fix <span class="title-ref">\~DataFrame.to\_csv</span> with `ExtensionArray` with list-like values (`28840`).

### GroupBy/resample/rolling

  - Bug incorrectly raising an `IndexError` when passing a list of quantiles to <span class="title-ref">.DataFrameGroupBy.quantile</span> (`28113`).
  - Bug in <span class="title-ref">.GroupBy.shift</span>, <span class="title-ref">.GroupBy.bfill</span> and <span class="title-ref">.GroupBy.ffill</span> where timezone information would be dropped (`19995`, `27992`)

### Other

  - Compatibility with Python 3.8 in <span class="title-ref">DataFrame.query</span> (`27261`)
  - Fix to ensure that tab-completion in an IPython console does not raise warnings for deprecated attributes (`27900`).

## Contributors

<div class="contributors">

v0.25.1..v0.25.2

</div>

---

v0.25.3.md

---

# What's new in 0.25.3 (October 31, 2019)

These are the changes in pandas 0.25.3. See \[release\](\#release) for a full changelog including other versions of pandas.

## Bug fixes

### GroupBy/resample/rolling

  - Bug in <span class="title-ref">DataFrameGroupBy.quantile</span> where NA values in the grouping could cause segfaults or incorrect results (`28882`)

## Contributors

<div class="contributors">

v0.25.2..v0.25.3

</div>

---

v0.4.x.md

---

# Versions 0.4.1 through 0.4.3 (September 25 - October 9, 2011)

{{ header }}

## New features

  - Added Python 3 support using 2to3 (`200`)
  - \[Added \<dsintro.name\_attribute\>\](\#added-\<dsintro.name\_attribute\>) `name` attribute to `Series`, now prints as part of `Series.__repr__`
  - <span class="title-ref">Series.isnull</span><span class="title-ref"> and \`Series.notnull</span> (`209`, `203`)
  - \[Added \<basics.align\>\](\#added-\<basics.align\>) `Series.align` method for aligning two series with choice of join method ([ENH56](https://github.com/pandas-dev/pandas/commit/56e0c9ffafac79ce262b55a6a13e1b10a88fbe93))
  - \[Added \<advanced.get\_level\_values\>\](\#added-\<advanced.get\_level\_values\>) method `get_level_values` to `MultiIndex` (`188`)
  - Set values in mixed-type `DataFrame` objects via `.ix` indexing attribute (`135`)
  - Added new `DataFrame` \[methods \<basics.dtypes\>\](\#methods-\<basics.dtypes\>) `get_dtype_counts` and property `dtypes` ([ENHdc](https://github.com/pandas-dev/pandas/commit/dca3c5c5a6a3769ee01465baca04cfdfa66a4f76))
  - Added \[ignore\_index \<merging.ignore\_index\>\](\#ignore\_index-\<merging.ignore\_index\>) option to `DataFrame.append` to stack DataFrames ([ENH1b](https://github.com/pandas-dev/pandas/commit/1ba56251f0013ff7cd8834e9486cef2b10098371))
  - `read_csv` tries to \[sniff \<io.sniff\>\](\#sniff-\<io.sniff\>) delimiters using `csv.Sniffer` (`146`)
  - `read_csv` can \[read \<io.csv\_multiindex\>\](\#read-\<io.csv\_multiindex\>) multiple columns into a `MultiIndex`; DataFrame's `to_csv` method writes out a corresponding `MultiIndex` (`151`)

<!-- end list -->

  - \- `DataFrame.rename` has a new `copy` parameter to \[rename  
    \<basics.rename\>\](\#rename

\--\<basics.rename\>) a DataFrame in place ([ENHed](https://github.com/pandas-dev/pandas/commit/edd9f1945fc010a57fa0ae3b3444d1fffe592591)) - \[Enable \<reshaping.unstack\_by\_name\>\](\#enable-\<reshaping.unstack\_by\_name\>) unstacking by name (`142`) - \[Enable \<advanced.sortlevel\_byname\>\](\#enable-\<advanced.sortlevel\_byname\>) `sortlevel` to work by level (`141`)

## Performance enhancements

  - Altered binary operations on differently-indexed SparseSeries objects to use the integer-based (dense) alignment logic which is faster with a larger number of blocks (`205`)
  - Wrote faster Cython data alignment / merging routines resulting in substantial speed increases
  - Improved performance of `isnull` and `notnull`, a regression from v0.3.0 (`187`)
  - Refactored code related to `DataFrame.join` so that intermediate aligned copies of the data in each `DataFrame` argument do not need to be created. Substantial performance increases result (`176`)
  - Substantially improved performance of generic `Index.intersection` and `Index.union`
  - Implemented `BlockManager.take` resulting in significantly faster `take` performance on mixed-type `DataFrame` objects (`104`)
  - Improved performance of `Series.sort_index`
  - Significant groupby performance enhancement: removed unnecessary integrity checks in DataFrame internals that were slowing down slicing operations to retrieve groups
  - Optimized `_ensure_index` function resulting in performance savings in type-checking Index objects
  - Wrote fast time series merging / joining methods in Cython. Will be integrated later into DataFrame.join and related functions

## Contributors

<div class="contributors">

v0.4.1..v0.4.3

</div>

---

v0.5.0.md

---

# Version 0.5.0 (October 24, 2011)

{{ header }}

## New features

  - \[Added \<basics.df\_join\>\](\#added-\<basics.df\_join\>) `DataFrame.align` method with standard join options
  - \[Added \<io.parse\_dates\>\](\#added-\<io.parse\_dates\>) `parse_dates` option to `read_csv` and `read_table` methods to optionally try to parse dates in the index columns
  - \[Added \<io.parse\_dates\>\](\#added-\<io.parse\_dates\>) `nrows`, `chunksize`, and `iterator` arguments to `read_csv` and `read_table`. The last two return a new `TextParser` class capable of lazily iterating through chunks of a flat file (`242`)
  - \[Added \<merging.multikey\_join\>\](\#added-\<merging.multikey\_join\>) ability to join on multiple columns in `DataFrame.join` (`214`)
  - Added private `_get_duplicates` function to `Index` for identifying duplicate values more easily ([ENH5c](https://github.com/pandas-dev/pandas/commit/5ca6ff5d822ee4ddef1ec0d87b6d83d8b4bbd3eb))
  - \[Added \<indexing.df\_cols\>\](\#added-\<indexing.df\_cols\>) column attribute access to DataFrame.
  - \[Added \<indexing.df\_cols\>\](\#added-\<indexing.df\_cols\>) Python tab completion hook for DataFrame columns. (`233`, `230`)
  - \[Implemented \<basics.describe\>\](\#implemented-\<basics.describe\>) `Series.describe` for Series containing objects (`241`)
  - \[Added \<merging.df\_inner\_join\>\](\#added-\<merging.df\_inner\_join\>) inner join option to `DataFrame.join` when joining on key(s) (`248`)
  - \[Implemented \<indexing.df\_cols\>\](\#implemented-\<indexing.df\_cols\>) selecting DataFrame columns by passing a list to `__getitem__` (`253`)
  - \[Implemented \<indexing.set\_ops\>\](\#implemented-\<indexing.set\_ops\>) & and | to intersect / union Index objects, respectively (`261`)
  - \[Added\<reshaping.pivot\>\](\#added\<reshaping.pivot\>) `pivot_table` convenience function to pandas namespace (`234`)
  - \[Implemented \<basics.rename\_axis\>\](\#implemented-\<basics.rename\_axis\>) `Panel.rename_axis` function (`243`)
  - DataFrame will show index level names in console output (`334`)
  - \[Implemented \<advanced.take\>\](\#implemented-\<advanced.take\>) `Panel.take`
  - \[Added\<basics.console\_output\>\](\#added\<basics.console\_output\>) `set_eng_float_format` for alternate DataFrame floating point string formatting ([ENH61](https://github.com/pandas-dev/pandas/commit/6141961))
  - \[Added \<indexing.set\_index\>\](\#added-\<indexing.set\_index\>) convenience `set_index` function for creating a DataFrame index from its existing columns
  - \[Implemented \<groupby.multiindex\>\](\#implemented-\<groupby.multiindex\>) `groupby` hierarchical index level name (`223`)
  - \[Added \<io.store\_in\_csv\>\](\#added-\<io.store\_in\_csv\>) support for different delimiters in `DataFrame.to_csv` (`244`)

## Performance enhancements

  - VBENCH Major performance improvements in file parsing functions `read_csv` and `read_table`
  - VBENCH Added Cython function for converting tuples to ndarray very fast. Speeds up many MultiIndex-related operations
  - VBENCH Refactored merging / joining code into a tidy class and disabled unnecessary computations in the float/object case, thus getting about 10% better performance (`211`)
  - VBENCH Improved speed of `DataFrame.xs` on mixed-type DataFrame objects by about 5x, regression from 0.3.0 (`215`)
  - VBENCH With new `DataFrame.align` method, speeding up binary operations between differently-indexed DataFrame objects by 10-25%.
  - VBENCH Significantly sped up conversion of nested dict into DataFrame (`212`)
  - VBENCH Significantly speed up DataFrame `__repr__` and `count` on large mixed-type DataFrame objects

## Contributors

<div class="contributors">

v0.4.0..v0.5.0

</div>

---

v0.6.0.md

---

# Version 0.6.0 (November 25, 2011)

{{ header }}

## New features

  - \[Added \<reshaping.melt\>\](\#added-\<reshaping.melt\>) `melt` function to `pandas.core.reshape`
  - \[Added \<groupby.multiindex\>\](\#added-\<groupby.multiindex\>) `level` parameter to group by level in Series and DataFrame descriptive statistics (`313`)
  - \[Added \<basics.head\_tail\>\](\#added-\<basics.head\_tail\>) `head` and `tail` methods to Series, analogous to DataFrame (`296`)
  - \[Added \<indexing.boolean\>\](\#added-\<indexing.boolean\>) `Series.isin` function which checks if each value is contained in a passed sequence (`289`)
  - \[Added \<io.formatting\>\](\#added-\<io.formatting\>) `float_format` option to `Series.to_string`
  - \[Added \<io.parse\_dates\>\](\#added-\<io.parse\_dates\>) `skip_footer` (`291`) and `converters` (`343`) options to `read_csv` and `read_table`
  - \[Added \<indexing.duplicate\>\](\#added-\<indexing.duplicate\>) `drop_duplicates` and `duplicated` functions for removing duplicate DataFrame rows and checking for duplicate rows, respectively (`319`)
  - \[Implemented \<dsintro.boolean\>\](\#implemented-\<dsintro.boolean\>) operators '&', '|', '^', '-' on DataFrame (`347`)
  - \[Added \<basics.stats\>\](\#added-\<basics.stats\>) `Series.mad`, mean absolute deviation
  - \[Added \<timeseries.offsets\>\](\#added-\<timeseries.offsets\>) `QuarterEnd` DateOffset (`321`)
  - \[Added \<dsintro.numpy\_interop\>\](\#added-\<dsintro.numpy\_interop\>) `dot` to DataFrame (`65`)
  - Added `orient` option to `Panel.from_dict` (`359`, `301`)
  - \[Added \<basics.dataframe.from\_dict\>\](\#added-\<basics.dataframe.from\_dict\>) `orient` option to `DataFrame.from_dict`
  - \[Added \<basics.dataframe.from\_records\>\](\#added-\<basics.dataframe.from\_records\>) passing list of tuples or list of lists to `DataFrame.from_records` (`357`)
  - \[Added \<groupby.multiindex\>\](\#added-\<groupby.multiindex\>) multiple levels to groupby (`103`)
  - \[Allow \<basics.sorting\>\](\#allow-\<basics.sorting\>) multiple columns in `by` argument of `DataFrame.sort_index` (`92`, `362`)
  - \[Added \<indexing.basics.get\_value\>\](\#added-\<indexing.basics.get\_value\>) fast `get_value` and `put_value` methods to DataFrame (`360`)
  - Added `cov` instance methods to Series and DataFrame (`194`, `362`)
  - \[Added \<visualization.barplot\>\](\#added-\<visualization.barplot\>) `kind='bar'` option to `DataFrame.plot` (`348`)
  - \[Added \<basics.idxmin\>\](\#added-\<basics.idxmin\>) `idxmin` and `idxmax` to Series and DataFrame (`286`)
  - \[Added \<io.clipboard\>\](\#added-\<io.clipboard\>) `read_clipboard` function to parse DataFrame from clipboard (`300`)
  - \[Added \<basics.stats\>\](\#added-\<basics.stats\>) `nunique` function to Series for counting unique elements (`297`)
  - \[Made \<basics.dataframe\>\](\#made-\<basics.dataframe\>) DataFrame constructor use Series name if no columns passed (`373`)
  - \[Support \<io.parse\_dates\>\](\#support-\<io.parse\_dates\>) regular expressions in read\_table/read\_csv (`364`)
  - \[Added \<io.html\>\](\#added-\<io.html\>) `DataFrame.to_html` for writing DataFrame to HTML (`387`)
  - \[Added \<basics.dataframe\>\](\#added-\<basics.dataframe\>) support for MaskedArray data in DataFrame, masked values converted to NaN (`396`)
  - \[Added \<visualization.box\>\](\#added-\<visualization.box\>) `DataFrame.boxplot` function (`368`)
  - \[Can \<basics.apply\>\](\#can-\<basics.apply\>) pass extra args, kwds to DataFrame.apply (`376`)
  - \[Implement \<merging.multikey\_join\>\](\#implement-\<merging.multikey\_join\>) `DataFrame.join` with vector `on` argument (`312`)
  - \[Added \<visualization.basic\>\](\#added-\<visualization.basic\>) `legend` boolean flag to `DataFrame.plot` (`324`)
  - \[Can \<reshaping.stacking\>\](\#can-\<reshaping.stacking\>) pass multiple levels to `stack` and `unstack` (`370`)
  - \[Can \<reshaping.pivot\>\](\#can-\<reshaping.pivot\>) pass multiple values columns to `pivot_table` (`381`)
  - \[Use \<groupby.multiindex\>\](\#use-\<groupby.multiindex\>) Series name in GroupBy for result index (`363`)
  - \[Added \<basics.apply\>\](\#added-\<basics.apply\>) `raw` option to `DataFrame.apply` for performance if only need ndarray (`309`)
  - Added proper, tested weighted least squares to standard and panel OLS (`303`)

## Performance enhancements

  - VBENCH Cythonized `cache_readonly`, resulting in substantial micro-performance enhancements throughout the code base (`361`)
  - VBENCH Special Cython matrix iterator for applying arbitrary reduction operations with 3-5x better performance than `np.apply_along_axis` (`309`)
  - VBENCH Improved performance of `MultiIndex.from_tuples`
  - VBENCH Special Cython matrix iterator for applying arbitrary reduction operations
  - VBENCH + DOCUMENT Add `raw` option to `DataFrame.apply` for getting better performance when
  - VBENCH Faster cythonized count by level in Series and DataFrame (`341`)
  - VBENCH? Significant GroupBy performance enhancement with multiple keys with many "empty" combinations
  - VBENCH New Cython vectorized function `map_infer` speeds up `Series.apply` and `Series.map` significantly when passed elementwise Python function, motivated by (`355`)
  - VBENCH Significantly improved performance of `Series.order`, which also makes np.unique called on a Series faster (`327`)
  - VBENCH Vastly improved performance of GroupBy on axes with a MultiIndex (`299`)

## Contributors

<div class="contributors">

v0.5.0..v0.6.0

</div>

---

v0.6.1.md

---

# Version 0.6.1 (December 13, 2011)

## New features

  - Can append single rows (as Series) to a DataFrame
  - Add Spearman and Kendall rank correlation options to Series.corr and DataFrame.corr (`428`)
  - \[Added \<indexing.basics.get\_value\>\](\#added-\<indexing.basics.get\_value\>) `get_value` and `set_value` methods to Series, DataFrame, and Panel for very low-overhead access (\>2x faster in many cases) to scalar elements (`437`, `438`). `set_value` is capable of producing an enlarged object.
  - Add PyQt table widget to sandbox (`435`)
  - DataFrame.align can \[accept Series arguments \<basics.align.frame.series\>\](\#accept-series-arguments-\<basics.align.frame.series\>) and an \[axis option \<basics.df\_join\>\](\#axis-option-\<basics.df\_join\>) (`461`)
  - Implement new \[SparseArray \<sparse.array\>\](\#sparsearray-\<sparse.array\>) and `SparseList` data structures. SparseSeries now derives from SparseArray (`463`)
  - \[Better console printing options \<basics.console\_output\>\](\#better-console-printing-options-\<basics.console\_output\>) (`453`)
  - Implement fast data ranking for Series and DataFrame, fast versions of scipy.stats.rankdata (`428`)
  - Implement `DataFrame.from_items` alternate constructor (`444`)
  - DataFrame.convert\_objects method for \[inferring better dtypes \<basics.cast\>\](\#inferring-better-dtypes-\<basics.cast\>) for object columns (`302`)
  - Add \[rolling\_corr\_pairwise \<window.corr\_pairwise\>\](\#rolling\_corr\_pairwise-\<window.corr\_pairwise\>) function for computing Panel of correlation matrices (`189`)

<!-- end list -->

  - \- Add \[margins \<reshaping.pivot.margins\>\](\#margins-\<reshaping.pivot.margins\>) option to \[pivot\_table  
    \<reshaping.pivot\>\](\#pivot\_table

\--\<reshaping.pivot\>) for computing subgroup aggregates (`114`) - Add `Series.from_csv` function (`482`) - \[Can pass \<window.cov\_corr\>\](\#can-pass-\<window.cov\_corr\>) DataFrame/DataFrame and DataFrame/Series to rolling\_corr/rolling\_cov (GH \#462) - MultiIndex.get\_level\_values can \[accept the level name \<advanced.get\_level\_values\>\](\#accept-the-level-name-\<advanced.get\_level\_values\>)

## Performance improvements

  - Improve memory usage of `DataFrame.describe` (do not copy data unnecessarily) (PR \#425)
  - Optimize scalar value lookups in the general case by 25% or more in Series and DataFrame
  - Fix performance regression in cross-sectional count in DataFrame, affecting DataFrame.dropna speed
  - Column deletion in DataFrame copies no data (computes views on blocks) (GH \#158)

## Contributors

<div class="contributors">

v0.6.0..v0.6.1

</div>

---

v0.7.0.md

---

# Version 0.7.0 (February 9, 2012)

{{ header }}

## New features

  - New unified \[merge function \<merging.join\>\](\#merge-function-\<merging.join\>) for efficiently performing full gamut of database / relational-algebra operations. Refactored existing join methods to use the new infrastructure, resulting in substantial performance gains (`220`, `249`, `267`)
  - New \[unified concatenation function \<merging.concat\>\](\#unified-concatenation-function-\<merging.concat\>) for concatenating Series, DataFrame or Panel objects along an axis. Can form union or intersection of the other axes. Improves performance of `Series.append` and `DataFrame.append` (`468`, `479`, `273`)
  - Can pass multiple DataFrames to `DataFrame.append` to concatenate (stack) and multiple Series to `Series.append` too
  - \[Can\<basics.dataframe.from\_list\_of\_dicts\>\](\#can\<basics.dataframe.from\_list\_of\_dicts\>) pass list of dicts (e.g., a list of JSON objects) to DataFrame constructor (`526`)
  - You can now \[set multiple columns \<indexing.columns.multiple\>\](\#set-multiple-columns-\<indexing.columns.multiple\>) in a DataFrame via `__getitem__`, useful for transformation (`342`)
  - Handle differently-indexed output values in `DataFrame.apply` (`498`)

`` `ipython    In [1]: df = pd.DataFrame(np.random.randn(10, 4))    In [2]: df.apply(lambda x: x.describe())    Out[2]:                   0          1          2          3    count  10.000000  10.000000  10.000000  10.000000    mean    0.190912  -0.395125  -0.731920  -0.403130    std     0.730951   0.813266   1.112016   0.961912    min    -0.861849  -2.104569  -1.776904  -1.469388    25%    -0.411391  -0.698728  -1.501401  -1.076610    50%     0.380863  -0.228039  -1.191943  -1.004091    75%     0.658444   0.057974  -0.034326   0.461706    max     1.212112   0.577046   1.643563   1.071804     [8 rows x 4 columns]  - [Add<advanced.reorderlevels>](#add<advanced.reorderlevels>) ``reorder\_levels``method to Series and   DataFrame (:issue:`534`)  - [Add<indexing.dictionarylike>](#add<indexing.dictionarylike>) dict-like``get``function to DataFrame   and Panel (:issue:`521`)  - [Add<basics.iterrows>](#add<basics.iterrows>)``DataFrame.iterrows`method for efficiently   iterating through the rows of a DataFrame  - Add`DataFrame.to\_panel`with code adapted from`LongPanel.to\_long`- [Add <basics.reindexing>](#add-<basics.reindexing>)`reindex\_axis`method added to DataFrame  - [Add <basics.stats>](#add-<basics.stats>)`level`option to binary arithmetic functions on`DataFrame`and`Series`- [Add <advanced.advanced_reindex>](#add-<advanced.advanced_reindex>)`level`option to the`reindex`and`align``methods on Series and DataFrame for broadcasting values across   a level (:issue:`542`, :issue:`552`, others)  - Add attribute-based item access to``Panel``and add IPython completion (:issue:`563`)  - [Add <visualization.basic>](#add-<visualization.basic>)``logy`option to`Series.plot`for   log-scaling on the Y axis  - [Add <io.formatting>](#add-<io.formatting>)`index`and`header`options to`DataFrame.to\_string`- [Can <merging.multiple_join>](#can-<merging.multiple_join>) pass multiple DataFrames to`DataFrame.join``to join on index (:issue:`115`)  - [Can <merging.multiple_join>](#can-<merging.multiple_join>) pass multiple Panels to``Panel.join``(:issue:`115`)  - [Added <io.formatting>](#added-<io.formatting>)``justify`argument to`DataFrame.to\_string`to allow different alignment of column headers  - [Add <groupby.attributes>](#add-<groupby.attributes>)`sort``option to GroupBy to allow disabling   sorting of the group keys for potential speedups (:issue:`595`)  - [Can <basics.dataframe.from_series>](#can-<basics.dataframe.from_series>) pass MaskedArray to Series   constructor (:issue:`563`)  - Add Panel item access via attributes   and IPython completion (:issue:`554`)  - Implement``DataFrame.lookup``, fancy-indexing analogue for retrieving values   given a sequence of row and column labels (:issue:`338`)  - Can pass a [list of functions <groupby.aggregate.multifunc>](#list-of-functions-<groupby.aggregate.multifunc>) to   aggregate with groupby on a DataFrame, yielding an aggregated result with   hierarchical columns (:issue:`166`)  - Can call``cummin`and`cummax``on Series and DataFrame to get cumulative   minimum and maximum, respectively (:issue:`647`)  -``value\_range``added as utility function to get min and max of a dataframe   (:issue:`288`)  - Added``encoding`argument to`read\_csv`,`read\_table`,`to\_csv`and`from\_csv``for non-ascii text (:issue:`717`)  - [Added <basics.stats>](#added-<basics.stats>)``abs`method to pandas objects  - [Added <reshaping.pivot>](#added-<reshaping.pivot>)`crosstab`function for easily computing frequency tables  - [Added <indexing.set_ops>](#added-<indexing.set_ops>)`isin`method to index objects  - [Added <advanced.xs>](#added-<advanced.xs>)`level`argument to`xs`method of DataFrame.   API changes to integer indexing`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

One of the potentially riskiest API changes in 0.7.0, but also one of the most important, was a complete review of how **integer indexes** are handled with regard to label-based indexing. Here is an example:

`` `ipython     In [3]: s = pd.Series(np.random.randn(10), index=range(0, 20, 2))     In [4]: s     Out[4]:     0    -1.294524     2     0.413738     4     0.276662     6    -0.472035     8    -0.013960     10   -0.362543     12   -0.006154     14   -0.923061     16    0.895717     18    0.805244     Length: 10, dtype: float64      In [5]: s[0]     Out[5]: -1.2945235902555294      In [6]: s[2]     Out[6]: 0.41373810535784006      In [7]: s[4]     Out[7]: 0.2766617129497566  This is all exactly identical to the behavior before. However, if you ask for a ``<span class="title-ref"> key \*\*not\*\* contained in the Series, in versions 0.6.1 and prior, Series would \*fall back\* on a location-based lookup. This now raises a </span><span class="title-ref">KeyError</span>\`:

`` `ipython    In [2]: s[1]    KeyError: 1  This change also has the same impact on DataFrame:  .. code-block:: ipython     In [3]: df = pd.DataFrame(np.random.randn(8, 4), index=range(0, 16, 2))     In [4]: df        0        1       2       3    0   0.88427  0.3363 -0.1787  0.03162    2   0.14451 -0.1415  0.2504  0.58374    4  -1.44779 -0.9186 -1.4996  0.27163    6  -0.26598 -2.4184 -0.2658  0.11503    8  -0.58776  0.3144 -0.8566  0.61941    10  0.10940 -0.7175 -1.0108  0.47990    12 -1.16919 -0.3087 -0.6049 -0.43544    14 -0.07337  0.3410  0.0424 -0.16037     In [5]: df.ix[3]    KeyError: 3  In order to support purely integer-based indexing, the following methods have ``\` been added:

| Method                       | Description                                  |
| ---------------------------- | -------------------------------------------- |
| `Series.iget_value(i)`       | Retrieve value stored at location `i`        |
| `Series.iget(i)`             | Alias for `iget_value`                       |
| `DataFrame.irow(i)`          | Retrieve the `i`-th row                      |
| `DataFrame.icol(j)`          | Retrieve the `j`-th column                   |
| `DataFrame.iget_value(i, j)` | Retrieve the value at row `i` and column `j` |

## API tweaks regarding label-based slicing

Label-based slicing using `ix` now requires that the index be sorted (monotonic) **unless** both the start and endpoint are contained in the index:

`` `python    In [1]: s = pd.Series(np.random.randn(6), index=list('gmkaec'))     In [2]: s    Out[2]:    g   -1.182230    m   -0.276183    k   -0.243550    a    1.628992    e    0.073308    c   -0.539890    dtype: float64  Then this is OK:  .. code-block:: python     In [3]: s.ix['k':'e']    Out[3]:    k   -0.243550    a    1.628992    e    0.073308    dtype: float64  But this is not:  .. code-block:: ipython     In [12]: s.ix['b':'h']    KeyError 'b'  If the index had been sorted, the "range selection" would have been possible:  .. code-block:: python     In [4]: s2 = s.sort_index()     In [5]: s2    Out[5]:    a    1.628992    c   -0.539890    e    0.073308    g   -1.182230    k   -0.243550    m   -0.276183    dtype: float64     In [6]: s2.ix['b':'h']    Out[6]:    c   -0.539890    e    0.073308    g   -1.182230    dtype: float64  Changes to Series ``\[\]`operator`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

As as notational convenience, you can pass a sequence of labels or a label slice to a Series when getting and setting values via `[]` (i.e. the `__getitem__` and `__setitem__` methods). The behavior will be the same as passing similar input to `ix` **except in the case of integer indexing**:

`` `ipython   In [8]: s = pd.Series(np.random.randn(6), index=list('acegkm'))    In [9]: s   Out[9]:   a   -1.206412   c    2.565646   e    1.431256   g    1.340309   k   -1.170299   m   -0.226169   Length: 6, dtype: float64    In [10]: s[['m', 'a', 'c', 'e']]   Out[10]:   m   -0.226169   a   -1.206412   c    2.565646   e    1.431256   Length: 4, dtype: float64    In [11]: s['b':'l']   Out[11]:   c    2.565646   e    1.431256   g    1.340309   k   -1.170299   Length: 4, dtype: float64    In [12]: s['c':'k']   Out[12]:   c    2.565646   e    1.431256   g    1.340309   k   -1.170299   Length: 4, dtype: float64  In the case of integer indexes, the behavior will be exactly as before ``<span class="title-ref"> (shadowing </span><span class="title-ref">ndarray</span>\`):

`` `ipython   In [13]: s = pd.Series(np.random.randn(6), index=range(0, 12, 2))    In [14]: s[[4, 0, 2]]   Out[14]:   4    0.132003   0    0.410835   2    0.813850   Length: 3, dtype: float64    In [15]: s[1:5]   Out[15]:   2    0.813850   4    0.132003   6   -0.827317   8   -0.076467   Length: 4, dtype: float64  If you wish to do indexing with sequences and slicing on an integer index with ``<span class="title-ref"> label semantics, use </span><span class="title-ref">ix</span>\`.

## Other API changes

  - The deprecated `LongPanel` class has been completely removed
  - If `Series.sort` is called on a column of a DataFrame, an exception will now be raised. Before it was possible to accidentally mutate a DataFrame's column by doing `df[col].sort()` instead of the side-effect free method `df[col].order()` (`316`)
  - Miscellaneous renames and deprecations which will (harmlessly) raise `FutureWarning`
  - `drop` added as an optional parameter to `DataFrame.reset_index` (`699`)

## Performance improvements

  - \[Cythonized GroupBy aggregations \<groupby.aggregate.builtin\>\](\#cythonized-groupby-aggregations-\<groupby.aggregate.builtin\>) no longer presort the data, thus achieving a significant speedup (`93`). GroupBy aggregations with Python functions significantly sped up by clever manipulation of the ndarray data type in Cython (`496`).
  - Better error message in DataFrame constructor when passed column labels don't match data (`497`)
  - Substantially improve performance of multi-GroupBy aggregation when a Python function is passed, reuse ndarray object in Cython (`496`)
  - Can store objects indexed by tuples and floats in HDFStore (`492`)
  - Don't print length by default in Series.to\_string, add `length` option (`489`)
  - Improve Cython code for multi-groupby to aggregate without having to sort the data (`93`)
  - Improve MultiIndex reindexing speed by storing tuples in the MultiIndex, test for backwards unpickling compatibility
  - Improve column reindexing performance by using specialized Cython take function
  - Further performance tweaking of Series.\_\_getitem\_\_ for standard use cases
  - Avoid Index dict creation in some cases (i.e. when getting slices, etc.), regression from prior versions
  - Friendlier error message in setup.py if NumPy not installed
  - Use common set of NA-handling operations (sum, mean, etc.) in Panel class also (`536`)
  - Default name assignment when calling `reset_index` on DataFrame with a regular (non-hierarchical) index (`476`)
  - Use Cythonized groupers when possible in Series/DataFrame stat ops with `level` parameter passed (`545`)
  - Ported skiplist data structure to C to speed up `rolling_median` by about 5-10x in most typical use cases (`374`)

## Contributors

<div class="contributors">

v0.6.1..v0.7.0

</div>

---

v0.7.1.md

---

# Version 0.7.1 (February 29, 2012)

{{ header }}

This release includes a few new features and addresses over a dozen bugs in 0.7.0.

## New features

>   - Add `to_clipboard` function to pandas namespace for writing objects to the system clipboard (`774`)
>   - Add `itertuples` method to DataFrame for iterating through the rows of a dataframe as tuples (`818`)
>   - Add ability to pass fill\_value and method to DataFrame and Series align method (`806`, `807`)
>   - Add fill\_value option to reindex, align methods (`784`)
>   - Enable concat to produce DataFrame from Series (`787`)
>   - Add `between` method to Series (`802`)
>   - Add HTML representation hook to DataFrame for the IPython HTML notebook (`773`)
>   - Support for reading Excel 2007 XML documents using openpyxl

## Performance improvements

>   - Improve performance and memory usage of fillna on DataFrame
>   - Can concatenate a list of Series along axis=1 to obtain a DataFrame (`787`)

## Contributors

<div class="contributors">

v0.7.0..v0.7.1

</div>

---

v0.7.2.md

---

# Version 0.7.2 (March 16, 2012)

{{ header }}

This release targets bugs in 0.7.1, and adds a few minor features.

## New features

>   - Add additional tie-breaking methods in DataFrame.rank (`874`)
>   - Add ascending parameter to rank in Series, DataFrame (`875`)
>   - Add coerce\_float option to DataFrame.from\_records (`893`)
>   - Add sort\_columns parameter to allow unsorted plots (`918`)
>   - Enable column access via attributes on GroupBy (`882`)
>   - Can pass dict of values to DataFrame.fillna (`661`)
>   - Can select multiple hierarchical groups by passing list of values in .ix (`134`)
>   - Add `axis` option to DataFrame.fillna (`174`)
>   - Add level keyword to `drop` for dropping values from a level (`159`)

## Performance improvements

>   - Use khash for Series.value\_counts, add raw function to algorithms.py (`861`)
>   - Intercept \_\_builtin\_\_.sum in groupby (`885`)

## Contributors

<div class="contributors">

v0.7.1..v0.7.2

</div>

---

v0.7.3.md

---

# Version 0.7.3 (April 12, 2012)

{{ header }}

This is a minor release from 0.7.2 and fixes many minor bugs and adds a number of nice new features. There are also a couple of API changes to note; these should not affect very many users, and we are inclined to call them "bug fixes" even though they do constitute a change in behavior. See the \[full release notes \<release\>\](\#full-release notes-\<release\>) or issue tracker on GitHub for a complete list.

## New features

  - New \[fixed width file reader \<io.fwf\>\](\#fixed-width-file-reader-\<io.fwf\>), `read_fwf`
  - New \[scatter\_matrix \<visualization.scatter\_matrix\>\](\#scatter\_matrix-\<visualization.scatter\_matrix\>) function for making a scatter plot matrix

`` `python    from pandas.tools.plotting import scatter_matrix     scatter_matrix(df, alpha=0.2)  # noqa F821   - Add ``stacked`argument to Series and DataFrame's`plot`method for   [stacked bar plots <visualization.barplot>](#stacked-bar-plots-<visualization.barplot>).  .. code-block:: python     df.plot(kind="bar", stacked=True)  # noqa F821   .. code-block:: python     df.plot(kind="barh", stacked=True)  # noqa F821   - Add log x and y [scaling options <visualization.basic>](#scaling-options-<visualization.basic>) to`DataFrame.plot`and`Series.plot`  `<span class="title-ref"> - Add </span><span class="title-ref">kurt</span>\` methods to Series and DataFrame for computing kurtosis

## NA boolean comparison API change

Reverted some changes to how NA values (represented typically as `NaN` or `None`) are handled in non-numeric Series:

`` `ipython    In [1]: series = pd.Series(["Steve", np.nan, "Joe"])     In [2]: series == "Steve"    Out[2]:    0     True    1    False    2    False    Length: 3, dtype: bool     In [3]: series != "Steve"    Out[3]:    0    False    1     True    2     True    Length: 3, dtype: bool  In comparisons, NA / NaN will always come through as ``False`except with`<span class="title-ref"> </span><span class="title-ref">\!=</span><span class="title-ref"> which is </span><span class="title-ref">True</span>\`. *Be very careful* with boolean arithmetic, especially negation, in the presence of NA data. You may wish to add an explicit NA filter into boolean array operations if you are worried about this:

`` `ipython    In [4]: mask = series == "Steve"     In [5]: series[mask & series.notnull()]    Out[5]:    0    Steve    Length: 1, dtype: object  While propagating NA in comparisons may seem like the right behavior to some ``\` users (and you could argue on purely technical grounds that this is the right thing to do), the evaluation was made that propagating NA everywhere, including in numerical arrays, would cause a large amount of problems for users. Thus, a "practicality beats purity" approach was taken. This issue may be revisited at some point in the future.

## Other API changes

When calling `apply` on a grouped Series, the return value will also be a Series, to be more consistent with the `groupby` behavior with DataFrame:

`` `ipython       In [6]: df = pd.DataFrame(          ...:     {          ...:         "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],          ...:         "B": ["one", "one", "two", "three", "two", "two", "one", "three"],          ...:         "C": np.random.randn(8),          ...:         "D": np.random.randn(8),          ...:     }          ...: )          ...:        In [7]: df       Out[7]:          A      B         C         D       0  foo    one  0.469112 -0.861849       1  bar    one -0.282863 -2.104569       2  foo    two -1.509059 -0.494929       3  bar  three -1.135632  1.071804       4  foo    two  1.212112  0.721555       5  bar    two -0.173215 -0.706771       6  foo    one  0.119209 -1.039575       7  foo  three -1.044236  0.271860        [8 rows x 4 columns]        In [8]: grouped = df.groupby("A")["C"]        In [9]: grouped.describe()       Out[9]:          count      mean       std       min       25%       50%       75%       max       A       bar    3.0 -0.530570  0.526860 -1.135632 -0.709248 -0.282863 -0.228039 -0.173215       foo    5.0 -0.150572  1.113308 -1.509059 -1.044236  0.119209  0.469112  1.212112        [2 rows x 8 columns]        In [10]: grouped.apply(lambda x: x.sort_values()[-2:])  # top 2 values       Out[10]:       A       bar  1   -0.282863            5   -0.173215       foo  0    0.469112            4    1.212112       Name: C, Length: 4, dtype: float64   .. _whatsnew_0.7.3.contributors:  Contributors ``\` \~\~\~\~\~\~\~\~\~\~\~\~

<div class="contributors">

v0.7.2..v0.7.3

</div>

---

v0.8.0.md

---

# Version 0.8.0 (June 29, 2012)

{{ header }}

This is a major release from 0.7.3 and includes extensive work on the time series handling and processing infrastructure as well as a great deal of new functionality throughout the library. It includes over 700 commits from more than 20 distinct authors. Most pandas 0.7.3 and earlier users should not experience any issues upgrading, but due to the migration to the NumPy datetime64 dtype, there may be a number of bugs and incompatibilities lurking. Lingering incompatibilities will be fixed ASAP in a 0.8.1 release if necessary. See the \[full release notes \<release\>\](\#full-release-notes \<release\>) or issue tracker on GitHub for a complete list.

## Support for non-unique indexes

All objects can now work with non-unique indexes. Data alignment / join operations work according to SQL join semantics (including, if application, index duplication in many-to-many joins)

## NumPy datetime64 dtype and 1.6 dependency

Time series data are now represented using NumPy's datetime64 dtype; thus, pandas 0.8.0 now requires at least NumPy 1.6. It has been tested and verified to work with the development version (1.7+) of NumPy as well which includes some significant user-facing API changes. NumPy 1.6 also has a number of bugs having to do with nanosecond resolution data, so I recommend that you steer clear of NumPy 1.6's datetime64 API functions (though limited as they are) and only interact with this data using the interface that pandas provides.

See the end of the 0.8.0 section for a "porting" guide listing potential issues for users migrating legacy code bases from pandas 0.7 or earlier to 0.8.0.

Bug fixes to the 0.7.x series for legacy NumPy \< 1.6 users will be provided as they arise. There will be no more further development in 0.7.x beyond bug fixes.

## Time Series changes and improvements

\> **Note** \> With this release, legacy scikits.timeseries users should be able to port their code to use pandas.

<div class="note">

<div class="title">

Note

</div>

See \[documentation \<timeseries\>\](\#documentation-\<timeseries\>) for overview of pandas timeseries API.

</div>

  - New datetime64 representation **speeds up join operations and data alignment**, **reduces memory usage**, and improve serialization / deserialization performance significantly over datetime.datetime
  - High performance and flexible **resample** method for converting from high-to-low and low-to-high frequency. Supports interpolation, user-defined aggregation functions, and control over how the intervals and result labeling are defined. A suite of high performance Cython/C-based resampling functions (including Open-High-Low-Close) have also been implemented.
  - Revamp of \[frequency aliases \<timeseries.offset\_aliases\>\](\#frequency-aliases-\<timeseries.offset\_aliases\>) and support for **frequency shortcuts** like '15min', or '1h30min'
  - New \[DatetimeIndex class \<timeseries.datetimeindex\>\](\#datetimeindex-class-\<timeseries.datetimeindex\>) supports both fixed frequency and irregular time series. Replaces now deprecated DateRange class
  - New `PeriodIndex` and `Period` classes for representing \[time spans \<timeseries.periods\>\](\#time-spans-\<timeseries.periods\>) and performing **calendar logic**, including the `12 fiscal quarterly frequencies <timeseries.quarterly>`. This is a partial port of, and a substantial enhancement to, elements of the scikits.timeseries code base. Support for conversion between PeriodIndex and DatetimeIndex
  - New Timestamp data type subclasses `datetime.datetime`, providing the same interface while enabling working with nanosecond-resolution data. Also provides \[easy time zone conversions \<timeseries.timezone\>\](\#easy-time-zone-conversions-\<timeseries.timezone\>).
  - Enhanced support for \[time zones \<timeseries.timezone\>\](\#time-zones-\<timeseries.timezone\>). Add `tz_convert` and `tz_localize` methods to TimeSeries and DataFrame. All timestamps are stored as UTC; Timestamps from DatetimeIndex objects with time zone set will be localized to local time. Time zone conversions are therefore essentially free. User needs to know very little about pytz library now; only time zone names as strings are required. Time zone-aware timestamps are equal if and only if their UTC timestamps match. Operations between time zone-aware time series with different time zones will result in a UTC-indexed time series.
  - Time series **string indexing conveniences** / shortcuts: slice years, year and month, and index values with strings
  - Enhanced time series **plotting**; adaptation of scikits.timeseries matplotlib-based plotting code

<!-- end list -->

  - \- New `date_range`, `bdate_range`, and `period_range` \[factory  
    functions \<timeseries.daterange\>\](\#factory

\--functions-\<timeseries.daterange\>) - Robust **frequency inference** function `infer_freq` and `inferred_freq` property of DatetimeIndex, with option to infer frequency on construction of DatetimeIndex - to\_datetime function efficiently **parses array of strings** to DatetimeIndex. DatetimeIndex will parse array or list of strings to datetime64 - **Optimized** support for datetime64-dtype data in Series and DataFrame columns - New NaT (Not-a-Time) type to represent **NA** in timestamp arrays - Optimize Series.asof for looking up **"as of" values** for arrays of timestamps - Milli, Micro, Nano date offset objects - Can index time series with datetime.time objects to select all data at particular **time of day** (`TimeSeries.at_time`) or **between two times** (`TimeSeries.between_time`) - Add \[tshift \<timeseries.advanced\_datetime\>\](\#tshift-\<timeseries.advanced\_datetime\>) method for leading/lagging using the frequency (if any) of the index, as opposed to a naive lead/lag using shift

## Other new features

  - New \[cut \<reshaping.tile.cut\>\](\#cut-\<reshaping.tile.cut\>) and `qcut` functions (like R's cut function) for computing a categorical variable from a continuous variable by binning values either into value-based (`cut`) or quantile-based (`qcut`) bins
  - Rename `Factor` to `Categorical` and add a number of usability features
  - Add \[limit \<missing\_data.fillna.limit\>\](\#limit-\<missing\_data.fillna.limit\>) argument to fillna/reindex
  - More flexible multiple function application in GroupBy, and can pass list (name, function) tuples to get result in particular order with given names
  - Add flexible \[replace \<missing\_data.replace\>\](\#replace-\<missing\_data.replace\>) method for efficiently substituting values
  - Enhanced \[read\_csv/read\_table \<io.parse\_dates\>\](\#read\_csv/read\_table-\<io.parse\_dates\>) for reading time series data and converting multiple columns to dates
  - Add \[comments \<io.comments\>\](\#comments-\<io.comments\>) option to parser functions: read\_csv, etc.
  - Add \[dayfirst \<io.dayfirst\>\](\#dayfirst-\<io.dayfirst\>) option to parser functions for parsing international DD/MM/YYYY dates
  - Allow the user to specify the CSV reader \[dialect \<io.dialect\>\](\#dialect-\<io.dialect\>) to control quoting etc.
  - Handling \[thousands \<io.thousands\>\](\#thousands-\<io.thousands\>) separators in read\_csv to improve integer parsing.
  - Enable unstacking of multiple levels in one shot. Alleviate `pivot_table` bugs (empty columns being introduced)
  - Move to klib-based hash tables for indexing; better performance and less memory usage than Python's dict
  - Add first, last, min, max, and prod optimized GroupBy functions
  - New \[ordered\_merge \<merging.merge\_ordered\>\](\#ordered\_merge-\<merging.merge\_ordered\>) function
  - Add flexible \[comparison \<basics.binop\>\](\#comparison-\<basics.binop\>) instance methods eq, ne, lt, gt, etc. to DataFrame, Series
  - Improve \[scatter\_matrix \<visualization.scatter\_matrix\>\](\#scatter\_matrix-\<visualization.scatter\_matrix\>) plotting function and add histogram or kernel density estimates to diagonal
  - Add \['kde' \<visualization.kde\>\](\#'kde'-\<visualization.kde\>) plot option for density plots
  - Support for converting DataFrame to R data.frame through rpy2
  - Improved support for complex numbers in Series and DataFrame
  - Add `pct_change` method to all data structures
  - Add max\_colwidth configuration option for DataFrame console output
  - \[Interpolate \<missing\_data.interpolate\>\](\#interpolate-\<missing\_data.interpolate\>) Series values using index values
  - Can select multiple columns from GroupBy
  - Add \[update \<merging.combine\_first.update\>\](\#update-\<merging.combine\_first.update\>) methods to Series/DataFrame for updating values in place
  - Add `any` and `all` method to DataFrame

## New plotting methods

`` `python    import pandas as pd     fx = pd.read_pickle("data/fx_prices")    import matplotlib.pyplot as plt ``Series.plot`now supports a`secondary\_y`option:  .. code-block:: python     plt.figure()     fx["FR"].plot(style="g")     fx["IT"].plot(style="k--", secondary_y=True)  Vytautas Jancauskas, the 2012 GSOC participant, has added many new plot`<span class="title-ref"> types. For example, </span><span class="title-ref">'kde'</span>\` is a new option:

`` `python    s = pd.Series(        np.concatenate((np.random.randn(1000), np.random.randn(1000) * 0.5 + 3))    )    plt.figure()    s.hist(density=True, alpha=0.2)    s.plot(kind="kde")  See [the plotting page <visualization.other>](#the-plotting-page-<visualization.other>) for much more.  Other API changes ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

  - Deprecation of `offset`, `time_rule`, and `timeRule` arguments names in time series functions. Warnings will be printed until pandas 0.9 or 1.0.

## Potential porting issues for pandas \<= 0.7.3 users

The major change that may affect you in pandas 0.8.0 is that time series indexes use NumPy's `datetime64` data type instead of `dtype=object` arrays of Python's built-in `datetime.datetime` objects. `DateRange` has been replaced by `DatetimeIndex` but otherwise behaved identically. But, if you have code that converts `DateRange` or `Index` objects that used to contain `datetime.datetime` values to plain NumPy arrays, you may have bugs lurking with code using scalar values because you are handing control over to NumPy:

<div class="ipython">

python

import datetime

rng = pd.date\_range("1/1/2000", periods=10) rng\[5\] isinstance(rng\[5\], datetime.datetime) rng\_asarray = np.asarray(rng) scalar\_val = rng\_asarray\[5\] type(scalar\_val)

</div>

pandas's `Timestamp` object is a subclass of `datetime.datetime` that has nanosecond support (the `nanosecond` field store the nanosecond value between 0 and 999). It should substitute directly into any code that used `datetime.datetime` values before. Thus, I recommend not casting `DatetimeIndex` to regular NumPy arrays.

If you have code that requires an array of `datetime.datetime` objects, you have a couple of options. First, the `astype(object)` method of `DatetimeIndex` produces an array of `Timestamp` objects:

<div class="ipython">

python

stamp\_array = rng.astype(object) stamp\_array stamp\_array\[5\]

</div>

To get an array of proper `datetime.datetime` objects, use the `to_pydatetime` method:

<div class="ipython">

python

dt\_array = rng.to\_pydatetime() dt\_array dt\_array\[5\]

</div>

matplotlib knows how to handle `datetime.datetime` but not Timestamp objects. While I recommend that you plot time series using `TimeSeries.plot`, you can either use `to_pydatetime` or register a converter for the Timestamp type. See [matplotlib documentation](http://matplotlib.org/api/units_api.html) for more on this.

\> **Warning** \> There are bugs in the user-facing API with the nanosecond datetime64 unit in NumPy 1.6. In particular, the string version of the array shows garbage values, and conversion to `dtype=object` is similarly broken.

> 
> 
> <div class="ipython">
> 
> python
> 
> rng = pd.date\_range("1/1/2000", periods=10) rng np.asarray(rng) converted = np.asarray(rng, dtype=object) converted\[5\]
> 
> </div>
> 
> **Trust me: don't panic**. If you are using NumPy 1.6 and restrict your interaction with `datetime64` values to pandas's API you will be just fine. There is nothing wrong with the data-type (a 64-bit integer internally); all of the important data processing happens in pandas and is heavily tested. I strongly recommend that you **do not work directly with datetime64 arrays in NumPy 1.6** and only use the pandas API.

**Support for non-unique indexes**: In the latter case, you may have code inside a `try:... catch:` block that failed due to the index not being unique. In many cases it will no longer fail (some method like `append` still check for uniqueness unless disabled). However, all is not lost: you can inspect `index.is_unique` and raise an exception explicitly if it is `False` or go to a different code branch.

## Contributors

<div class="contributors">

v0.7.3..v0.8.0

</div>

---

v0.8.1.md

---

# Version 0.8.1 (July 22, 2012)

{{ header }}

This release includes a few new features, performance enhancements, and over 30 bug fixes from 0.8.0. New features include notably NA friendly string processing functionality and a series of new plot types and options.

## New features

>   - Add \[vectorized string processing methods \<text.string\_methods\>\](\#vectorized-string-processing-methods-\<text.string\_methods\>) accessible via Series.str (`620`)
>   - Add option to disable adjustment in EWMA (`1584`)
>   - \[Radviz plot \<visualization.radviz\>\](\#radviz-plot-\<visualization.radviz\>) (`1566`)
>   - \[Parallel coordinates plot \<visualization.parallel\_coordinates\>\](\#parallel-coordinates-plot-\<visualization.parallel\_coordinates\>)
>   - \[Bootstrap plot \<visualization.bootstrap\>\](\#bootstrap-plot-\<visualization.bootstrap\>)
>   - Per column styles and secondary y-axis plotting (`1559`)
>   - New datetime converters millisecond plotting (`1599`)
>   - Add option to disable "sparse" display of hierarchical indexes (`1538`)
>   - Series/DataFrame's `set_index` method can \[append levels \<indexing.set\_index\>\](\#append-levels

\----\<indexing.set\_index\>) to an existing Index/MultiIndex (`1569`, `1577`)

## Performance improvements

>   - Improved implementation of rolling min and max (thanks to [Bottleneck](https://bottleneck.readthedocs.io) \!)
>   - Add accelerated `'median'` GroupBy option (`1358`)
>   - Significantly improve the performance of parsing ISO8601-format date strings with `DatetimeIndex` or `to_datetime` (`1571`)
>   - Improve the performance of GroupBy on single-key aggregations and use with Categorical types
>   - Significant datetime parsing performance improvements

## Contributors

<div class="contributors">

v0.8.0..v0.8.1

</div>

---

v0.9.0.md

---

<div id="whatsnew_0900">

{{ header }}

</div>

# Version 0.9.0 (October 7, 2012)

This is a major release from 0.8.1 and includes several new features and enhancements along with a large number of bug fixes. New features include vectorized unicode encoding/decoding for `Series.str`, `to_latex` method to DataFrame, more flexible parsing of boolean values, and enabling the download of options data from Yahoo\! Finance.

## New features

>   - Add `encode` and `decode` for unicode handling to \[vectorized string processing methods \<text.string\_methods\>\](\#vectorized

  - \----string-processing-methods-\<text.string\_methods\>) in Series.str (`1706`)
    
      - Add `DataFrame.to_latex` method (`1735`)
      - Add convenient expanding window equivalents of all [rolling]()\* ops (`1785`)
      - Add Options class to pandas.io.data for fetching options data from Yahoo\! Finance (`1748`, `1739`)
      - More flexible parsing of boolean values (Yes, No, TRUE, FALSE, etc) (`1691`, `1295`)
      - Add `level` parameter to `Series.reset_index`
      - `TimeSeries.between_time` can now select times across midnight (`1871`)
      - Series constructor can now handle generator as input (`1679`)
      - `DataFrame.dropna` can now take multiple axes (tuple/list) as input (`924`)
      - Enable `skip_footer` parameter in `ExcelFile.parse` (`1843`)

## API changes

>   - The default column names when `header=None` and no columns names passed to functions like `read_csv` has changed to be more Pythonic and amenable to attribute access:

<div class="ipython">

python

import io

data = """ 0,0,1 1,1,0 0,1,0 """ df = pd.read\_csv(io.StringIO(data), header=None) df

</div>

  - Creating a Series from another Series, passing an index, will cause reindexing to happen inside rather than treating the Series like an ndarray. Technically improper usages like `Series(df[col1], index=df[col2])` that worked before "by accident" (this was never intended) will lead to all NA Series in some cases. To be perfectly clear:

<div class="ipython">

python

s1 = pd.Series(\[1, 2, 3\]) s1

s2 = pd.Series(s1, index=\["foo", "bar", "baz"\]) s2

</div>

  - Deprecated `day_of_year` API removed from PeriodIndex, use `dayofyear` (`1723`)
  - Don't modify NumPy suppress printoption to True at import time
  - The internal HDF5 data arrangement for DataFrames has been transposed. Legacy files will still be readable by HDFStore (`1834`, `1824`)
  - Legacy cruft removed: pandas.stats.misc.quantileTS
  - Use ISO8601 format for Period repr: monthly, daily, and on down (`1776`)
  - Empty DataFrame columns are now created as object dtype. This will prevent a class of TypeErrors that was occurring in code where the dtype of a column would depend on the presence of data or not (e.g. a SQL query having results) (`1783`)
  - Setting parts of DataFrame/Panel using ix now aligns input Series/DataFrame (`1630`)
  - `first` and `last` methods in `GroupBy` no longer drop non-numeric columns (`1809`)
  - Resolved inconsistencies in specifying custom NA values in text parser. `na_values` of type dict no longer override default NAs unless `keep_default_na` is set to false explicitly (`1657`)
  - `DataFrame.dot` will not do data alignment, and also work with Series (`1915`)

See the \[full release notes \<release\>\](\#full-release-notes \<release\>) or issue tracker on GitHub for a complete list.

## Contributors

<div class="contributors">

v0.8.1..v0.9.0

</div>

---

v0.9.1.md

---

# Version 0.9.1 (November 14, 2012)

{{ header }}

This is a bug fix release from 0.9.0 and includes several new features and enhancements along with a large number of bug fixes. The new features include by-column sort order for DataFrame and Series, improved NA handling for the rank method, masking functions for DataFrame, and intraday time-series filtering for DataFrame.

## New features

>   - `Series.sort`, `DataFrame.sort`, and `DataFrame.sort_index` can now be specified in a per-column manner to support multiple sort orders (`928`)
>     
>       - \`\`\`ipython
>         
>           - In \[2\]: df = pd.DataFrame(np.random.randint(0, 2, (6, 3)),  
>             ...: columns=\['A', 'B', 'C'\])
>         
>         In \[3\]: df.sort(\['A', 'B'\], ascending=\[1, 0\])
>         
>           - Out\[3\]:  
>             A B C
>         
>         3 0 1 1 4 0 1 1 2 0 0 1 0 1 0 0 1 1 0 0 5 1 0 0
> 
>   - `DataFrame.rank` now supports additional argument values for the `na_option` parameter so missing values can be assigned either the largest or the smallest rank (`1508`, `2159`)
>     
>     <div class="ipython">
>     
>     python
>     
>     df = pd.DataFrame(np.random.randn(6, 3), columns=\['A', 'B', 'C'\])
>     
>     df.loc\[2:4\] = np.nan
>     
>     df.rank()
>     
>     df.rank(na\_option='top')
>     
>     df.rank(na\_option='bottom')
>     
>     </div>
> 
>   - DataFrame has new `where` and `mask` methods to select values according to a given boolean mask (`2109`, `2151`)
>     
>     > DataFrame currently supports slicing via a boolean vector the same length as the DataFrame (inside the `[]`). The returned DataFrame has the same number of columns as the original, but is sliced on its index.
>     > 
>     > <div class="ipython">
>     > 
>     > python
>     > 
>     > df = pd.DataFrame(np.random.randn(5, 3), columns=\['A', 'B', 'C'\])
>     > 
>     > df
>     > 
>     > df\[df\['A'\] \> 0\]
>     > 
>     > </div>
>     > 
>     > If a DataFrame is sliced with a DataFrame based boolean condition (with the same size as the original DataFrame), then a DataFrame the same size (index and columns) as the original is returned, with elements that do not meet the boolean condition as `NaN`. This is accomplished via the new method `DataFrame.where`. In addition, `where` takes an optional `other` argument for replacement.
>     > 
>     > <div class="ipython">
>     > 
>     > python
>     > 
>     > df\[df \> 0\]
>     > 
>     > df.where(df \> 0)
>     > 
>     > df.where(df \> 0, -df)
>     > 
>     > </div>
>     > 
>     > Furthermore, `where` now aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via `.ix` (but on the contents rather than the axis labels)
>     > 
>     > <div class="ipython">
>     > 
>     > python
>     > 
>     > df2 = df.copy() df2\[df2\[1:4\] \> 0\] = 3 df2
>     > 
>     > </div>
>     > 
>     > `DataFrame.mask` is the inverse boolean operation of `where`.
>     > 
>     > <div class="ipython">
>     > 
>     > python
>     > 
>     > df.mask(df \<= 0)
>     > 
>     > </div>
> 
>   - Enable referencing of Excel columns by their column names (`1936`)
>     
>     ``` ipython
>     In [1]: xl = pd.ExcelFile('data/test.xls')
>     
>     In [2]: xl.parse('Sheet1', index_col=0, parse_dates=True,
>                      parse_cols='A:D')
>     ```
> 
>   - Added option to disable pandas-style tick locators and formatters using `series.plot(x_compat=True)` or `pandas.plot_params['x_compat'] = True` (`2205`)
> 
>   - Existing TimeSeries methods `at_time` and `between_time` were added to DataFrame (`2149`)
> 
>   - DataFrame.dot can now accept ndarrays (`2042`)
> 
>   - DataFrame.drop now supports non-unique indexes (`2101`)
> 
>   - Panel.shift now supports negative periods (`2164`)
> 
>   - DataFrame now support unary \~ operator (`2110`)

API changes `` ` ~~~~~~~~~~~    - Upsampling data with a PeriodIndex will result in a higher frequency     TimeSeries that spans the original time window ``\`ipython In \[1\]: prng = pd.period\_range('2012Q1', periods=2, freq='Q')

> In \[2\]: s = pd.Series(np.random.randn(len(prng)), prng)
> 
> In \[4\]: s.resample('M') Out\[4\]: 2012-01 -1.471992 2012-02 NaN 2012-03 NaN 2012-04 -0.493593 2012-05 NaN 2012-06 NaN Freq: M, dtype: float64
> 
> \- Period.end\_time now returns the last nanosecond in the time interval (`2124`, `2125`, `1764`)
> 
> <div class="ipython">
> 
> python
> 
> p = pd.Period('2012')
> 
> p.end\_time
> 
> </div>
> 
> \- File parsers no longer coerce to float or bool for columns that have custom converters specified (`2184`)
> 
> <div class="ipython">
> 
> python
> 
> import io
> 
>   - data = ('A,B,Cn'  
>     '00001,001,5n' '00002,002,6')
> 
> pd.read\_csv(io.StringIO(data), converters={'A': lambda x: x.strip()})
> 
> </div>

See the \[full release notes \](\#full-release-notes )\`<span class="title-ref"> \<release\></span> or issue tracker on GitHub for a complete list.

## Contributors

<div class="contributors">

v0.9.0..v0.9.1

</div>

---

v1.0.0.md

---

# What's new in 1.0.0 (January 29, 2020)

These are the changes in pandas 1.0.0. See \[release\](\#release) for a full changelog including other versions of pandas.

\> **Note** \> The pandas 1.0 release removed a lot of functionality that was deprecated in previous releases (see \[below \<whatsnew\_100.prior\_deprecations\>\](\#below-\<whatsnew\_100.prior\_deprecations\>) for an overview). It is recommended to first upgrade to pandas 0.25 and to ensure your code is working without warnings, before upgrading to pandas 1.0.

## New deprecation policy

Starting with pandas 1.0.0, pandas will adopt a variant of [SemVer](https://semver.org) to version releases. Briefly,

  - Deprecations will be introduced in minor releases (e.g. 1.1.0, 1.2.0, 2.1.0, ...)
  - Deprecations will be enforced in major releases (e.g. 1.0.0, 2.0.0, 3.0.0, ...)
  - API-breaking changes will be made only in major releases (except for experimental features)

See \[policies.version\](\#policies.version) for more.

{{ header }}

## Enhancements

### Using Numba in `rolling.apply` and `expanding.apply`

We've added an `engine` keyword to <span class="title-ref">\~core.window.rolling.Rolling.apply</span> and <span class="title-ref">\~core.window.expanding.Expanding.apply</span> that allows the user to execute the routine using [Numba](https://numba.pydata.org/) instead of Cython. Using the Numba engine can yield significant performance gains if the apply function can operate on numpy arrays and the data set is larger (1 million rows or greater). For more details, see \[rolling apply documentation \<window.numba\_engine\>\](\#rolling-apply-documentation-\<window.numba\_engine\>) (`28987`, `30936`)

### Defining custom windows for rolling operations

We've added a <span class="title-ref">pandas.api.indexers.BaseIndexer</span> class that allows users to define how window bounds are created during `rolling` operations. Users can define their own `get_window_bounds` method on a <span class="title-ref">pandas.api.indexers.BaseIndexer</span> subclass that will generate the start and end indices used for each window during the rolling aggregation. For more details and example usage, see the \[custom window rolling documentation \<window.custom\_rolling\_window\>\](\#custom-window-rolling-documentation-\<window.custom\_rolling\_window\>)

### Converting to markdown

We've added <span class="title-ref">\~DataFrame.to\_markdown</span> for creating a markdown table (`11052`)

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 2, 3\], "B": \[1, 2, 3\]}, index=\['a', 'a', 'b'\]) print(df.to\_markdown())

</div>

## Experimental new features

### Experimental `NA` scalar to denote missing values

A new `pd.NA` value (singleton) is introduced to represent scalar missing values. Up to now, pandas used several values to represent missing data: `np.nan` is used for this for float data, `np.nan` or `None` for object-dtype data and `pd.NaT` for datetime-like data. The goal of `pd.NA` is to provide a "missing" indicator that can be used consistently across data types. `pd.NA` is currently used by the nullable integer and boolean data types and the new string data type (`28095`).

\> **Warning** \> Experimental: the behaviour of `pd.NA` can still change without warning.

For example, creating a Series using the nullable integer dtype:

<div class="ipython">

python

s = pd.Series(\[1, 2, None\], dtype="Int64") s s\[2\]

</div>

Compared to `np.nan`, `pd.NA` behaves differently in certain operations. In addition to arithmetic operations, `pd.NA` also propagates as "missing" or "unknown" in comparison operations:

<div class="ipython">

python

np.nan \> 1 pd.NA \> 1

</div>

For logical operations, `pd.NA` follows the rules of the [three-valued logic](https://en.wikipedia.org/wiki/Three-valued_logic) (or *Kleene logic*). For example:

<div class="ipython">

python

pd.NA | True

</div>

For more, see \[NA section \<missing\_data.NA\>\](\#na-section-\<missing\_data.na\>) in the user guide on missing data.

### Dedicated string data type

We've added <span class="title-ref">StringDtype</span>, an extension type dedicated to string data. Previously, strings were typically stored in object-dtype NumPy arrays. (`29975`)

\> **Warning** \> `StringDtype` is currently considered experimental. The implementation and parts of the API may change without warning.

The `'string'` extension type solves several issues with object-dtype NumPy arrays:

1.  You can accidentally store a *mixture* of strings and non-strings in an `object` dtype array. A `StringArray` can only store strings.
2.  `object` dtype breaks dtype-specific operations like <span class="title-ref">DataFrame.select\_dtypes</span>. There isn't a clear way to select *just* text while excluding non-text, but still object-dtype columns.
3.  When reading code, the contents of an `object` dtype array is less clear than `string`.

<div class="ipython">

python

pd.Series(\['abc', None, 'def'\], dtype=pd.StringDtype())

</div>

You can use the alias `"string"` as well.

<div class="ipython">

python

s = pd.Series(\['abc', None, 'def'\], dtype="string") s

</div>

The usual string accessor methods work. Where appropriate, the return type of the Series or columns of a DataFrame will also have string dtype.

<div class="ipython">

python

s.str.upper() s.str.split('b', expand=True).dtypes

</div>

String accessor methods returning integers will return a value with <span class="title-ref">Int64Dtype</span>

<div class="ipython">

python

s.str.count("a")

</div>

We recommend explicitly using the `string` data type when working with strings. See \[text.types\](\#text.types) for more.

### Boolean data type with missing values support

We've added <span class="title-ref">BooleanDtype</span> / <span class="title-ref">\~arrays.BooleanArray</span>, an extension type dedicated to boolean data that can hold missing values. The default `bool` data type based on a bool-dtype NumPy array, the column can only hold `True` or `False`, and not missing values. This new <span class="title-ref">\~arrays.BooleanArray</span> can store missing values as well by keeping track of this in a separate mask. (`29555`, `30095`, `31131`)

<div class="ipython">

python

pd.Series(\[True, False, None\], dtype=pd.BooleanDtype())

</div>

You can use the alias `"boolean"` as well.

<div class="ipython">

python

s = pd.Series(\[True, False, None\], dtype="boolean") s

</div>

### Method `convert_dtypes` to ease use of supported extension dtypes

In order to encourage use of the extension dtypes `StringDtype`, `BooleanDtype`, `Int64Dtype`, `Int32Dtype`, etc., that support `pd.NA`, the methods <span class="title-ref">DataFrame.convert\_dtypes</span> and <span class="title-ref">Series.convert\_dtypes</span> have been introduced. (`29752`) (`30929`)

Example:

<div class="ipython">

python

  - df = pd.DataFrame({'x': \['abc', None, 'def'\],  
    'y': \[1, 2, np.nan\], 'z': \[True, False, True\]})

df df.dtypes

</div>

<div class="ipython">

python

converted = df.convert\_dtypes() converted converted.dtypes

</div>

This is especially useful after reading in data using readers such as <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_excel</span>. See \[here \<missing\_data.NA.conversion\>\](\#here-\<missing\_data.na.conversion\>) for a description.

## Other enhancements

  - <span class="title-ref">DataFrame.to\_string</span> added the `max_colwidth` parameter to control when wide columns are truncated (`9784`)
  - Added the `na_value` argument to <span class="title-ref">Series.to\_numpy</span>, <span class="title-ref">Index.to\_numpy</span> and <span class="title-ref">DataFrame.to\_numpy</span> to control the value used for missing data (`30322`)
  - <span class="title-ref">MultiIndex.from\_product</span> infers level names from inputs if not explicitly provided (`27292`)
  - <span class="title-ref">DataFrame.to\_latex</span> now accepts `caption` and `label` arguments (`25436`)
  - DataFrames with \[nullable integer \<integer\_na\>\](\#nullable-integer-\<integer\_na\>), the \[new string dtype \<text.types\>\](\#new-string-dtype-\<text.types\>) and period data type can now be converted to `pyarrow` (\>=0.15.0), which means that it is supported in writing to the Parquet file format when using the `pyarrow` engine (`28368`). Full roundtrip to parquet (writing and reading back in with <span class="title-ref">\~DataFrame.to\_parquet</span> / <span class="title-ref">read\_parquet</span>) is supported starting with pyarrow \>= 0.16 (`20612`).
  - <span class="title-ref">to\_parquet</span> now appropriately handles the `schema` argument for user defined schemas in the pyarrow engine. (`30270`)
  - <span class="title-ref">DataFrame.to\_json</span> now accepts an `indent` integer argument to enable pretty printing of JSON output (`12004`)
  - <span class="title-ref">read\_stata</span> can read Stata 119 dta files. (`28250`)
  - Implemented <span class="title-ref">.Window.var</span> and <span class="title-ref">.Window.std</span> functions (`26597`)
  - Added `encoding` argument to <span class="title-ref">DataFrame.to\_string</span> for non-ascii text (`28766`)
  - Added `encoding` argument to <span class="title-ref">DataFrame.to\_html</span> for non-ascii text (`28663`)
  - <span class="title-ref">Styler.background\_gradient</span> now accepts `vmin` and `vmax` arguments (`12145`)
  - <span class="title-ref">Styler.format</span> added the `na_rep` parameter to help format the missing values (`21527`, `28358`)
  - <span class="title-ref">read\_excel</span> now can read binary Excel (`.xlsb`) files by passing `engine='pyxlsb'`. For more details and example usage, see the \[Binary Excel files documentation \<io.xlsb\>\](\#binary-excel-files-documentation-\<io.xlsb\>). Closes `8540`.
  - The `partition_cols` argument in <span class="title-ref">DataFrame.to\_parquet</span> now accepts a string (`27117`)
  - <span class="title-ref">pandas.read\_json</span> now parses `NaN`, `Infinity` and `-Infinity` (`12213`)
  - DataFrame constructor preserve `ExtensionArray` dtype with `ExtensionArray` (`11363`)
  - <span class="title-ref">DataFrame.sort\_values</span> and <span class="title-ref">Series.sort\_values</span> have gained `ignore_index` keyword to be able to reset index after sorting (`30114`)
  - <span class="title-ref">DataFrame.sort\_index</span> and <span class="title-ref">Series.sort\_index</span> have gained `ignore_index` keyword to reset index (`30114`)
  - <span class="title-ref">DataFrame.drop\_duplicates</span> has gained `ignore_index` keyword to reset index (`30114`)
  - Added new writer for exporting Stata dta files in versions 118 and 119, `StataWriterUTF8`. These files formats support exporting strings containing Unicode characters. Format 119 supports data sets with more than 32,767 variables (`23573`, `30959`)
  - <span class="title-ref">Series.map</span> now accepts `collections.abc.Mapping` subclasses as a mapper (`29733`)
  - Added an experimental <span class="title-ref">\~DataFrame.attrs</span> for storing global metadata about a dataset (`29062`)
  - <span class="title-ref">Timestamp.fromisocalendar</span> is now compatible with python 3.8 and above (`28115`)
  - <span class="title-ref">DataFrame.to\_pickle</span> and <span class="title-ref">read\_pickle</span> now accept URL (`30163`)

## Backwards incompatible API changes

### Avoid using names from `MultiIndex.levels`

As part of a larger refactor to <span class="title-ref">MultiIndex</span> the level names are now stored separately from the levels (`27242`). We recommend using <span class="title-ref">MultiIndex.names</span> to access the names, and <span class="title-ref">Index.set\_names</span> to update the names.

For backwards compatibility, you can still *access* the names via the levels.

<div class="ipython">

python

mi = pd.MultiIndex.from\_product(\[\[1, 2\], \['a', 'b'\]\], names=\['x', 'y'\]) mi.levels\[0\].name

</div>

However, it is no longer possible to *update* the names of the `MultiIndex` via the level.

<div class="ipython" data-okexcept="">

python

mi.levels\[0\].name = "new name" mi.names

</div>

To update, use `MultiIndex.set_names`, which returns a new `MultiIndex`.

<div class="ipython">

python

mi2 = mi.set\_names("new name", level=0) mi2.names

</div>

### New repr for <span class="title-ref">\~pandas.arrays.IntervalArray</span>

<span class="title-ref">pandas.arrays.IntervalArray</span> adopts a new `__repr__` in accordance with other array classes (`25022`)

*pandas 0.25.x*

`` `ipython    In [1]: pd.arrays.IntervalArray.from_tuples([(0, 1), (2, 3)])    Out[2]:    IntervalArray([(0, 1], (2, 3]],                  closed='right',                  dtype='interval[int64]')  *pandas 1.0.0*  .. ipython:: python     pd.arrays.IntervalArray.from_tuples([(0, 1), (2, 3)]) ``DataFrame.rename`now only accepts one positional argument`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">DataFrame.rename</span> would previously accept positional arguments that would lead to ambiguous or undefined behavior. From pandas 1.0, only the very first argument, which maps labels to their new names along the default axis, is allowed to be passed by position (`29136`).

<div class="ipython" data-suppress="">

python

df = pd.DataFrame(\[\[1\]\])

</div>

*pandas 0.25.x*

`` `ipython    In [1]: df = pd.DataFrame([[1]])    In [2]: df.rename({0: 1}, {0: 2})    Out[2]:    FutureWarning: ...Use named arguments to resolve ambiguity...       2    1  1  *pandas 1.0.0*  .. code-block:: ipython     In [3]: df.rename({0: 1}, {0: 2})    Traceback (most recent call last):    ...    TypeError: rename() takes from 1 to 2 positional arguments but 3 were given  Note that errors will now be raised when conflicting or potentially ambiguous arguments are provided.  *pandas 0.25.x*  .. code-block:: ipython     In [4]: df.rename({0: 1}, index={0: 2})    Out[4]:       0    1  1     In [5]: df.rename(mapper={0: 1}, index={0: 2})    Out[5]:       0    2  1  *pandas 1.0.0*  .. code-block:: ipython     In [6]: df.rename({0: 1}, index={0: 2})    Traceback (most recent call last):    ...    TypeError: Cannot specify both 'mapper' and any of 'index' or 'columns'     In [7]: df.rename(mapper={0: 1}, index={0: 2})    Traceback (most recent call last):    ...    TypeError: Cannot specify both 'mapper' and any of 'index' or 'columns'  You can still change the axis along which the first positional argument is applied by ``<span class="title-ref"> supplying the </span><span class="title-ref">axis</span>\` keyword argument.

<div class="ipython">

python

df.rename({0: 1}) df.rename({0: 1}, axis=1)

</div>

If you would like to update both the index and column labels, be sure to use the respective keywords.

<div class="ipython">

python

df.rename(index={0: 1}, columns={0: 2})

</div>

### Extended verbose info output for <span class="title-ref">\~pandas.DataFrame</span>

<span class="title-ref">DataFrame.info</span> now shows line numbers for the columns summary (`17304`)

*pandas 0.25.x*

`` `ipython    In [1]: df = pd.DataFrame({"int_col": [1, 2, 3],    ...                    "text_col": ["a", "b", "c"],    ...                    "float_col": [0.0, 0.1, 0.2]})    In [2]: df.info(verbose=True)    <class 'pandas.DataFrame'>    RangeIndex: 3 entries, 0 to 2    Data columns (total 3 columns):    int_col      3 non-null int64    text_col     3 non-null object    float_col    3 non-null float64    dtypes: float64(1), int64(1), object(1)    memory usage: 152.0+ bytes  *pandas 1.0.0*  .. ipython:: python     df = pd.DataFrame({"int_col": [1, 2, 3],                       "text_col": ["a", "b", "c"],                       "float_col": [0.0, 0.1, 0.2]})    df.info(verbose=True)  `pandas.array` inference changes ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">pandas.array</span> now infers pandas' new extension types in several cases (`29791`):

1.  String data (including missing values) now returns a <span class="title-ref">arrays.StringArray</span>.
2.  Integer data (including missing values) now returns a <span class="title-ref">arrays.IntegerArray</span>.
3.  Boolean data (including missing values) now returns the new <span class="title-ref">arrays.BooleanArray</span>

*pandas 0.25.x*

`` `ipython    In [1]: pd.array(["a", None])    Out[1]:    <PandasArray>    ['a', None]    Length: 2, dtype: object     In [2]: pd.array([1, None])    Out[2]:    <PandasArray>    [1, None]    Length: 2, dtype: object   *pandas 1.0.0*  .. ipython:: python     pd.array(["a", None])    pd.array([1, None])  As a reminder, you can specify the ``dtype`` to disable all inference.  `arrays.IntegerArray` now uses `pandas.NA` ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">arrays.IntegerArray</span> now uses <span class="title-ref">pandas.NA</span> rather than <span class="title-ref">numpy.nan</span> as its missing value marker (`29964`).

*pandas 0.25.x*

`` `ipython    In [1]: a = pd.array([1, 2, None], dtype="Int64")    In [2]: a    Out[2]:    <IntegerArray>    [1, 2, NaN]    Length: 3, dtype: Int64     In [3]: a[2]    Out[3]:    nan  *pandas 1.0.0*  .. ipython:: python     a = pd.array([1, 2, None], dtype="Int64")    a    a[2]  This has a few API-breaking consequences.  **Converting to a NumPy ndarray**  When converting to a NumPy array missing values will be ``pd.NA`, which cannot`<span class="title-ref"> be converted to a float. So calling </span><span class="title-ref">np.asarray(integer\_array, dtype="float")</span>\` will now raise.

*pandas 0.25.x*

`` `ipython     In [1]: np.asarray(a, dtype="float")     Out[1]:     array([ 1.,  2., nan])  *pandas 1.0.0*  .. ipython:: python    :okexcept:     np.asarray(a, dtype="float")  Use `arrays.IntegerArray.to_numpy` with an explicit ``na\_value`instead.  .. ipython:: python     a.to_numpy(dtype="float", na_value=np.nan)  **Reductions can return**`pd.NA`When performing a reduction such as a sum with`skipna=False`, the result`<span class="title-ref"> will now be </span><span class="title-ref">pd.NA</span><span class="title-ref"> instead of </span><span class="title-ref">np.nan</span><span class="title-ref"> in presence of missing values (:issue:\`30958</span>).

*pandas 0.25.x*

`` `ipython     In [1]: pd.Series(a).sum(skipna=False)     Out[1]:     nan  *pandas 1.0.0*  .. ipython:: python     pd.Series(a).sum(skipna=False)  **value_counts returns a nullable integer dtype**  `Series.value_counts` with a nullable integer dtype now returns a nullable ``\` integer dtype for the values.

*pandas 0.25.x*

`` `ipython    In [1]: pd.Series([2, 1, 1, None], dtype="Int64").value_counts().dtype    Out[1]:    dtype('int64')  *pandas 1.0.0*  .. ipython:: python     pd.Series([2, 1, 1, None], dtype="Int64").value_counts().dtype  See [missing_data.NA](#missing_data.na) for more on the differences between `pandas.NA` ``<span class="title-ref"> and \`numpy.nan</span>.

### <span class="title-ref">arrays.IntegerArray</span> comparisons return <span class="title-ref">arrays.BooleanArray</span>

Comparison operations on a <span class="title-ref">arrays.IntegerArray</span> now returns a <span class="title-ref">arrays.BooleanArray</span> rather than a NumPy array (`29964`).

*pandas 0.25.x*

`` `ipython    In [1]: a = pd.array([1, 2, None], dtype="Int64")    In [2]: a    Out[2]:    <IntegerArray>    [1, 2, NaN]    Length: 3, dtype: Int64     In [3]: a > 1    Out[3]:    array([False,  True, False])  *pandas 1.0.0*  .. ipython:: python     a = pd.array([1, 2, None], dtype="Int64")    a > 1  Note that missing values now propagate, rather than always comparing unequal ``<span class="title-ref"> like \`numpy.nan</span>. See \[missing\_data.NA\](\#missing\_data.na) for more.

### By default <span class="title-ref">Categorical.min</span> now returns the minimum instead of np.nan

When <span class="title-ref">Categorical</span> contains `np.nan`, <span class="title-ref">Categorical.min</span> no longer return `np.nan` by default (skipna=True) (`25303`)

*pandas 0.25.x*

`` `ipython    In [1]: pd.Categorical([1, 2, np.nan], ordered=True).min()    Out[1]: nan   *pandas 1.0.0*  .. ipython:: python     pd.Categorical([1, 2, np.nan], ordered=True).min()   Default dtype of empty `pandas.Series` ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Initialising an empty <span class="title-ref">pandas.Series</span> without specifying a dtype will raise a `DeprecationWarning` now (`17261`). The default dtype will change from `float64` to `object` in future releases so that it is consistent with the behaviour of <span class="title-ref">DataFrame</span> and <span class="title-ref">Index</span>.

*pandas 1.0.0*

`` `ipython    In [1]: pd.Series()    Out[2]:    DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.    Series([], dtype: float64)  Result dtype inference changes for resample operations ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The rules for the result dtype in <span class="title-ref">DataFrame.resample</span> aggregations have changed for extension types (`31359`). Previously, pandas would attempt to convert the result back to the original dtype, falling back to the usual inference rules if that was not possible. Now, pandas will only return a result of the original dtype if the scalar values in the result are instances of the extension dtype's scalar type.

<div class="ipython">

python

  - df = pd.DataFrame({"A": \['a', 'b'\]}, dtype='category',  
    index=pd.date\_range('2000', periods=2))

df

</div>

*pandas 0.25.x*

`` `ipython    In [1]> df.resample("2D").agg(lambda x: 'a').A.dtype    Out[1]:    CategoricalDtype(categories=['a', 'b'], ordered=False)  *pandas 1.0.0*  .. ipython:: python     df.resample("2D").agg(lambda x: 'a').A.dtype  This fixes an inconsistency between ``resample`and`groupby`.`\` This also fixes a potential bug, where the **values** of the result might change depending on how the results are cast back to the original dtype.

*pandas 0.25.x*

`` `ipython    In [1] df.resample("2D").agg(lambda x: 'c')    Out[1]:          A    0  NaN  *pandas 1.0.0*  .. ipython:: python     df.resample("2D").agg(lambda x: 'c')   .. _whatsnew_100.api_breaking.python:  Increased minimum version for Python ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 1.0.0 supports Python 3.6.1 and higher (`29212`).

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated (`29766`, `29723`). If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.13.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>pytz</td>
<td>2015.4</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.6.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.2.1</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.6.2</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>4.0.2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.6.0</td>
<td></td>
</tr>
<tr class="even">
<td>fastparquet</td>
<td>0.3.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>gcsfs</td>
<td>0.2.2</td>
<td></td>
</tr>
<tr class="even">
<td>lxml</td>
<td>3.8.0</td>
<td></td>
</tr>
<tr class="odd">
<td>matplotlib</td>
<td>2.2.2</td>
<td></td>
</tr>
<tr class="even">
<td>numba</td>
<td>0.46.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>openpyxl</td>
<td>2.5.7</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pyarrow</td>
<td>0.13.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pymysql</td>
<td>0.7.1</td>
<td></td>
</tr>
<tr class="even">
<td>pytables</td>
<td>3.4.2</td>
<td></td>
</tr>
<tr class="odd">
<td>s3fs</td>
<td>0.3.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>scipy</td>
<td>0.19.0</td>
<td></td>
</tr>
<tr class="odd">
<td>sqlalchemy</td>
<td>1.1.4</td>
<td></td>
</tr>
<tr class="even">
<td>xarray</td>
<td>0.8.2</td>
<td></td>
</tr>
<tr class="odd">
<td>xlrd</td>
<td>1.1.0</td>
<td></td>
</tr>
<tr class="even">
<td>xlsxwriter</td>
<td>0.9.8</td>
<td></td>
</tr>
<tr class="odd">
<td>xlwt</td>
<td>1.2.0</td>
<td></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Build changes

pandas has added a [pyproject.toml](https://www.python.org/dev/peps/pep-0517/) file and will no longer include cythonized files in the source distribution uploaded to PyPI (`28341`, `20775`). If you're installing a built distribution (wheel) or via conda, this shouldn't have any effect on you. If you're building pandas from source, you should no longer need to install Cython into your build environment before calling `pip install pandas`.

### Other API changes

  - <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> now raises on invalid operation names (`27489`)
  - <span class="title-ref">pandas.api.types.infer\_dtype</span> will now return "integer-na" for integer and `np.nan` mix (`27283`)
  - <span class="title-ref">MultiIndex.from\_arrays</span> will no longer infer names from arrays if `names=None` is explicitly provided (`27292`)
  - In order to improve tab-completion, pandas does not include most deprecated attributes when introspecting a pandas object using `dir` (e.g. `dir(df)`). To see which attributes are excluded, see an object's `_deprecations` attribute, for example `pd.DataFrame._deprecations` (`28805`).
  - The returned dtype of <span class="title-ref">unique</span> now matches the input dtype. (`27874`)
  - Changed the default configuration value for `options.matplotlib.register_converters` from `True` to `"auto"` (`18720`). Now, pandas custom formatters will only be applied to plots created by pandas, through <span class="title-ref">\~DataFrame.plot</span>. Previously, pandas' formatters would be applied to all plots created *after* a <span class="title-ref">\~DataFrame.plot</span>. See \[units registration \<whatsnew\_100.matplotlib\_units\>\](\#units-registration-\<whatsnew\_100.matplotlib\_units\>) for more.
  - <span class="title-ref">Series.dropna</span> has dropped its `**kwargs` argument in favor of a single `how` parameter. Supplying anything else than `how` to `**kwargs` raised a `TypeError` previously (`29388`)
  - When testing pandas, the new minimum required version of pytest is 5.0.1 (`29664`)
  - <span class="title-ref">Series.str.\_\_iter\_\_</span> was deprecated and will be removed in future releases (`28277`).
  - Added `<NA>` to the list of default NA values for <span class="title-ref">read\_csv</span> (`30821`)

### Documentation improvements

  - Added new section on \[scale\](\#scale) (`28315`).
  - Added sub-section on \[io.query\_multi\](\#io.query\_multi) for HDF5 datasets (`28791`).

## Deprecations

  - <span class="title-ref">Series.item</span> and <span class="title-ref">Index.item</span> have been \_[undeprecated]() (`29250`)
  - `Index.set_value` has been deprecated. For a given index `idx`, array `arr`, value in `idx` of `idx_val` and a new value of `val`, `idx.set_value(arr, idx_val, val)` is equivalent to `arr[idx.get_loc(idx_val)] = val`, which should be used instead (`28621`).
  - <span class="title-ref">is\_extension\_type</span> is deprecated, <span class="title-ref">is\_extension\_array\_dtype</span> should be used instead (`29457`)
  - <span class="title-ref">eval</span> keyword argument "truediv" is deprecated and will be removed in a future version (`29812`)
  - <span class="title-ref">DateOffset.isAnchored</span> and <span class="title-ref">DatetOffset.onOffset</span> are deprecated and will be removed in a future version, use <span class="title-ref">DateOffset.is\_anchored</span> and <span class="title-ref">DateOffset.is\_on\_offset</span> instead (`30340`)
  - `pandas.tseries.frequencies.get_offset` is deprecated and will be removed in a future version, use `pandas.tseries.frequencies.to_offset` instead (`4205`)
  - <span class="title-ref">Categorical.take\_nd</span> and <span class="title-ref">CategoricalIndex.take\_nd</span> are deprecated, use <span class="title-ref">Categorical.take</span> and <span class="title-ref">CategoricalIndex.take</span> instead (`27745`)
  - The parameter `numeric_only` of <span class="title-ref">Categorical.min</span> and <span class="title-ref">Categorical.max</span> is deprecated and replaced with `skipna` (`25303`)
  - The parameter `label` in <span class="title-ref">lreshape</span> has been deprecated and will be removed in a future version (`29742`)
  - `pandas.core.index` has been deprecated and will be removed in a future version, the public classes are available in the top-level namespace (`19711`)
  - <span class="title-ref">pandas.json\_normalize</span> is now exposed in the top-level namespace. Usage of `json_normalize` as `pandas.io.json.json_normalize` is now deprecated and it is recommended to use `json_normalize` as <span class="title-ref">pandas.json\_normalize</span> instead (`27586`).
  - The `numpy` argument of <span class="title-ref">pandas.read\_json</span> is deprecated (`28512`).
  - <span class="title-ref">DataFrame.to\_stata</span>, <span class="title-ref">DataFrame.to\_feather</span>, and <span class="title-ref">DataFrame.to\_parquet</span> argument "fname" is deprecated, use "path" instead (`23574`)
  - The deprecated internal attributes `_start`, `_stop` and `_step` of <span class="title-ref">RangeIndex</span> now raise a `FutureWarning` instead of a `DeprecationWarning` (`26581`)
  - The `pandas.util.testing` module has been deprecated. Use the public API in `pandas.testing` documented at \[api.general.testing\](\#api.general.testing) (`16232`).
  - `pandas.SparseArray` has been deprecated. Use `pandas.arrays.SparseArray` (<span class="title-ref">arrays.SparseArray</span>) instead. (`30642`)
  - The parameter `is_copy` of <span class="title-ref">Series.take</span> and <span class="title-ref">DataFrame.take</span> has been deprecated and will be removed in a future version. (`27357`)
  - Support for multi-dimensional indexing (e.g. `index[:, None]`) on a <span class="title-ref">Index</span> is deprecated and will be removed in a future version, convert to a numpy array before indexing instead (`30588`)
  - The `pandas.np` submodule is now deprecated. Import numpy directly instead (`30296`)
  - The `pandas.datetime` class is now deprecated. Import from `datetime` instead (`30610`)
  - <span class="title-ref">\~DataFrame.diff</span> will raise a `TypeError` rather than implicitly losing the dtype of extension types in the future. Convert to the correct dtype before calling `diff` instead (`31025`)

**Selecting Columns from a Grouped DataFrame**

When selecting columns from a <span class="title-ref">DataFrameGroupBy</span> object, passing individual keys (or a tuple of keys) inside single brackets is deprecated, a list of items should be used instead. (`23566`) For example:

`` `ipython     df = pd.DataFrame({         "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],         "B": np.random.randn(8),         "C": np.random.randn(8),     })     g = df.groupby('A')      # single key, returns SeriesGroupBy     g['B']      # tuple of single key, returns SeriesGroupBy     g[('B',)]      # tuple of multiple keys, returns DataFrameGroupBy, raises FutureWarning     g[('B', 'C')]      # multiple keys passed directly, returns DataFrameGroupBy, raises FutureWarning     # (implicitly converts the passed strings into a single tuple)     g['B', 'C']      # proper way, returns DataFrameGroupBy     g[['B', 'C']]  .. ---------------------------------------------------------------------------  .. _whatsnew_100.prior_deprecations:  Removal of prior version deprecations/changes ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

**Removed SparseSeries and SparseDataFrame**

`SparseSeries`, `SparseDataFrame` and the `DataFrame.to_sparse` method have been removed (`28425`). We recommend using a `Series` or `DataFrame` with sparse values instead.

<div id="whatsnew_100.matplotlib_units">

**Matplotlib unit registration**

</div>

Previously, pandas would register converters with matplotlib as a side effect of importing pandas (`18720`). This changed the output of plots made via matplotlib plots after pandas was imported, even if you were using matplotlib directly rather than <span class="title-ref">\~DataFrame.plot</span>.

To use pandas formatters with a matplotlib plot, specify

`` `ipython    In [1]: import pandas as pd    In [2]: pd.options.plotting.matplotlib.register_converters = True  Note that plots created by `DataFrame.plot` and `Series.plot` *do* register the converters ``<span class="title-ref"> automatically. The only behavior change is when plotting a date-like object via </span><span class="title-ref">matplotlib.pyplot.plot</span><span class="title-ref"> or </span><span class="title-ref">matplotlib.Axes.plot</span>\`. See \[plotting.formatters\](\#plotting.formatters) for more.

**Other removals**

  - Removed the previously deprecated keyword "index" from <span class="title-ref">read\_stata</span>, <span class="title-ref">StataReader</span>, and <span class="title-ref">StataReader.read</span>, use "index\_col" instead (`17328`)
  - Removed `StataReader.data` method, use <span class="title-ref">StataReader.read</span> instead (`9493`)
  - Removed `pandas.plotting._matplotlib.tsplot`, use <span class="title-ref">Series.plot</span> instead (`19980`)
  - `pandas.tseries.converter.register` has been moved to <span class="title-ref">pandas.plotting.register\_matplotlib\_converters</span> (`18307`)
  - <span class="title-ref">Series.plot</span> no longer accepts positional arguments, pass keyword arguments instead (`30003`)
  - <span class="title-ref">DataFrame.hist</span> and <span class="title-ref">Series.hist</span> no longer allows `figsize="default"`, specify figure size by passing a tuple instead (`30003`)
  - Floordiv of integer-dtyped array by <span class="title-ref">Timedelta</span> now raises `TypeError` (`21036`)
  - <span class="title-ref">TimedeltaIndex</span> and <span class="title-ref">DatetimeIndex</span> no longer accept non-nanosecond dtype strings like "timedelta64" or "datetime64", use "timedelta64\[ns\]" and "datetime64\[ns\]" instead (`24806`)
  - Changed the default "skipna" argument in <span class="title-ref">pandas.api.types.infer\_dtype</span> from `False` to `True` (`24050`)
  - Removed `Series.ix` and `DataFrame.ix` (`26438`)
  - Removed `Index.summary` (`18217`)
  - Removed the previously deprecated keyword "fastpath" from the <span class="title-ref">Index</span> constructor (`23110`)
  - Removed `Series.get_value`, `Series.set_value`, `DataFrame.get_value`, `DataFrame.set_value` (`17739`)
  - Removed `Series.compound` and `DataFrame.compound` (`26405`)
  - Changed the default "inplace" argument in <span class="title-ref">DataFrame.set\_index</span> and <span class="title-ref">Series.set\_axis</span> from `None` to `False` (`27600`)
  - Removed `Series.cat.categorical`, `Series.cat.index`, `Series.cat.name` (`24751`)
  - Removed the previously deprecated keyword "box" from <span class="title-ref">to\_datetime</span> and <span class="title-ref">to\_timedelta</span>; in addition these now always returns <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span>, <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> (`24486`)
  - <span class="title-ref">to\_timedelta</span>, <span class="title-ref">Timedelta</span>, and <span class="title-ref">TimedeltaIndex</span> no longer allow "M", "y", or "Y" for the "unit" argument (`23264`)
  - Removed the previously deprecated keyword "time\_rule" from (non-public) `offsets.generate_range`, which has been moved to <span class="title-ref">core.arrays.\_ranges.generate\_range</span> (`24157`)
  - <span class="title-ref">DataFrame.loc</span> or <span class="title-ref">Series.loc</span> with listlike indexers and missing labels will no longer reindex (`17295`)
  - <span class="title-ref">DataFrame.to\_excel</span> and <span class="title-ref">Series.to\_excel</span> with non-existent columns will no longer reindex (`17295`)
  - Removed the previously deprecated keyword "join\_axes" from <span class="title-ref">concat</span>; use `reindex_like` on the result instead (`22318`)
  - Removed the previously deprecated keyword "by" from <span class="title-ref">DataFrame.sort\_index</span>, use <span class="title-ref">DataFrame.sort\_values</span> instead (`10726`)
  - Removed support for nested renaming in <span class="title-ref">DataFrame.aggregate</span>, <span class="title-ref">Series.aggregate</span>, <span class="title-ref">core.groupby.DataFrameGroupBy.aggregate</span>, <span class="title-ref">core.groupby.SeriesGroupBy.aggregate</span>, <span class="title-ref">core.window.rolling.Rolling.aggregate</span> (`18529`)
  - Passing `datetime64` data to <span class="title-ref">TimedeltaIndex</span> or `timedelta64` data to `DatetimeIndex` now raises `TypeError` (`23539`, `23937`)
  - Passing `int64` values to <span class="title-ref">DatetimeIndex</span> and a timezone now interprets the values as nanosecond timestamps in UTC, not wall times in the given timezone (`24559`)
  - A tuple passed to <span class="title-ref">DataFrame.groupby</span> is now exclusively treated as a single key (`18314`)
  - Removed `Index.contains`, use `key in index` instead (`30103`)
  - Addition and subtraction of `int` or integer-arrays is no longer allowed in <span class="title-ref">Timestamp</span>, <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span>, use `obj + n * obj.freq` instead of `obj + n` (`22535`)
  - Removed `Series.ptp` (`21614`)
  - Removed `Series.from_array` (`18258`)
  - Removed `DataFrame.from_items` (`18458`)
  - Removed `DataFrame.as_matrix`, `Series.as_matrix` (`18458`)
  - Removed `Series.asobject` (`18477`)
  - Removed `DataFrame.as_blocks`, `Series.as_blocks`, `DataFrame.blocks`, `Series.blocks` (`17656`)
  - <span class="title-ref">pandas.Series.str.cat</span> now defaults to aligning `others`, using `join='left'` (`27611`)
  - <span class="title-ref">pandas.Series.str.cat</span> does not accept list-likes *within* list-likes anymore (`27611`)
  - <span class="title-ref">Series.where</span> with `Categorical` dtype (or <span class="title-ref">DataFrame.where</span> with `Categorical` column) no longer allows setting new categories (`24114`)
  - Removed the previously deprecated keywords "start", "end", and "periods" from the <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span>, and <span class="title-ref">PeriodIndex</span> constructors; use <span class="title-ref">date\_range</span>, <span class="title-ref">timedelta\_range</span>, and <span class="title-ref">period\_range</span> instead (`23919`)
  - Removed the previously deprecated keyword "verify\_integrity" from the <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">TimedeltaIndex</span> constructors (`23919`)
  - Removed the previously deprecated keyword "fastpath" from `pandas.core.internals.blocks.make_block` (`19265`)
  - Removed the previously deprecated keyword "dtype" from <span class="title-ref">Block.make\_block\_same\_class</span> (`19434`)
  - Removed `ExtensionArray._formatting_values`. Use <span class="title-ref">ExtensionArray.\_formatter</span> instead. (`23601`)
  - Removed `MultiIndex.to_hierarchical` (`21613`)
  - Removed `MultiIndex.labels`, use <span class="title-ref">MultiIndex.codes</span> instead (`23752`)
  - Removed the previously deprecated keyword "labels" from the <span class="title-ref">MultiIndex</span> constructor, use "codes" instead (`23752`)
  - Removed `MultiIndex.set_labels`, use <span class="title-ref">MultiIndex.set\_codes</span> instead (`23752`)
  - Removed the previously deprecated keyword "labels" from <span class="title-ref">MultiIndex.set\_codes</span>, <span class="title-ref">MultiIndex.copy</span>, <span class="title-ref">MultiIndex.drop</span>, use "codes" instead (`23752`)
  - Removed support for legacy HDF5 formats (`29787`)
  - Passing a dtype alias (e.g. 'datetime64\[ns, UTC\]') to <span class="title-ref">DatetimeTZDtype</span> is no longer allowed, use <span class="title-ref">DatetimeTZDtype.construct\_from\_string</span> instead (`23990`)
  - Removed the previously deprecated keyword "skip\_footer" from <span class="title-ref">read\_excel</span>; use "skipfooter" instead (`18836`)
  - <span class="title-ref">read\_excel</span> no longer allows an integer value for the parameter `usecols`, instead pass a list of integers from 0 to `usecols` inclusive (`23635`)
  - Removed the previously deprecated keyword "convert\_datetime64" from <span class="title-ref">DataFrame.to\_records</span> (`18902`)
  - Removed `IntervalIndex.from_intervals` in favor of the <span class="title-ref">IntervalIndex</span> constructor (`19263`)
  - Changed the default "keep\_tz" argument in <span class="title-ref">DatetimeIndex.to\_series</span> from `None` to `True` (`23739`)
  - Removed `api.types.is_period` and `api.types.is_datetimetz` (`23917`)
  - Ability to read pickles containing <span class="title-ref">Categorical</span> instances created with pre-0.16 version of pandas has been removed (`27538`)
  - Removed `pandas.tseries.plotting.tsplot` (`18627`)
  - Removed the previously deprecated keywords "reduce" and "broadcast" from <span class="title-ref">DataFrame.apply</span> (`18577`)
  - Removed the previously deprecated `assert_raises_regex` function in `pandas._testing` (`29174`)
  - Removed the previously deprecated `FrozenNDArray` class in `pandas.core.indexes.frozen` (`29335`)
  - Removed the previously deprecated keyword "nthreads" from <span class="title-ref">read\_feather</span>, use "use\_threads" instead (`23053`)
  - Removed `Index.is_lexsorted_for_tuple` (`29305`)
  - Removed support for nested renaming in <span class="title-ref">DataFrame.aggregate</span>, <span class="title-ref">Series.aggregate</span>, <span class="title-ref">core.groupby.DataFrameGroupBy.aggregate</span>, <span class="title-ref">core.groupby.SeriesGroupBy.aggregate</span>, <span class="title-ref">core.window.rolling.Rolling.aggregate</span> (`29608`)
  - Removed `Series.valid`; use <span class="title-ref">Series.dropna</span> instead (`18800`)
  - Removed `DataFrame.is_copy`, `Series.is_copy` (`18812`)
  - Removed `DataFrame.get_ftype_counts`, `Series.get_ftype_counts` (`18243`)
  - Removed `DataFrame.ftypes`, `Series.ftypes`, `Series.ftype` (`26744`)
  - Removed `Index.get_duplicates`, use `idx[idx.duplicated()].unique()` instead (`20239`)
  - Removed `Series.clip_upper`, `Series.clip_lower`, `DataFrame.clip_upper`, `DataFrame.clip_lower` (`24203`)
  - Removed the ability to alter <span class="title-ref">DatetimeIndex.freq</span>, <span class="title-ref">TimedeltaIndex.freq</span>, or <span class="title-ref">PeriodIndex.freq</span> (`20772`)
  - Removed `DatetimeIndex.offset` (`20730`)
  - Removed `DatetimeIndex.asobject`, `TimedeltaIndex.asobject`, `PeriodIndex.asobject`, use `astype(object)` instead (`29801`)
  - Removed the previously deprecated keyword "order" from <span class="title-ref">factorize</span> (`19751`)
  - Removed the previously deprecated keyword "encoding" from <span class="title-ref">read\_stata</span> and <span class="title-ref">DataFrame.to\_stata</span> (`21400`)
  - Changed the default "sort" argument in <span class="title-ref">concat</span> from `None` to `False` (`20613`)
  - Removed the previously deprecated keyword "raise\_conflict" from <span class="title-ref">DataFrame.update</span>, use "errors" instead (`23585`)
  - Removed the previously deprecated keyword "n" from <span class="title-ref">DatetimeIndex.shift</span>, <span class="title-ref">TimedeltaIndex.shift</span>, <span class="title-ref">PeriodIndex.shift</span>, use "periods" instead (`22458`)
  - Removed the previously deprecated keywords "how", "fill\_method", and "limit" from <span class="title-ref">DataFrame.resample</span> (`30139`)
  - Passing an integer to <span class="title-ref">Series.fillna</span> or <span class="title-ref">DataFrame.fillna</span> with `timedelta64[ns]` dtype now raises `TypeError` (`24694`)
  - Passing multiple axes to <span class="title-ref">DataFrame.dropna</span> is no longer supported (`20995`)
  - Removed `Series.nonzero`, use `to_numpy().nonzero()` instead (`24048`)
  - Passing floating dtype `codes` to <span class="title-ref">Categorical.from\_codes</span> is no longer supported, pass `codes.astype(np.int64)` instead (`21775`)
  - Removed the previously deprecated keyword "pat" from <span class="title-ref">Series.str.partition</span> and <span class="title-ref">Series.str.rpartition</span>, use "sep" instead (`23767`)
  - Removed `Series.put` (`27106`)
  - Removed `Series.real`, `Series.imag` (`27106`)
  - Removed `Series.to_dense`, `DataFrame.to_dense` (`26684`)
  - Removed `Index.dtype_str`, use `str(index.dtype)` instead (`27106`)
  - <span class="title-ref">Categorical.ravel</span> returns a <span class="title-ref">Categorical</span> instead of a `ndarray` (`27199`)
  - The 'outer' method on Numpy ufuncs, e.g. `np.subtract.outer` operating on <span class="title-ref">Series</span> objects is no longer supported, and will raise `NotImplementedError` (`27198`)
  - Removed `Series.get_dtype_counts` and `DataFrame.get_dtype_counts` (`27145`)
  - Changed the default "fill\_value" argument in <span class="title-ref">Categorical.take</span> from `True` to `False` (`20841`)
  - Changed the default value for the `raw` argument in <span class="title-ref">Series.rolling().apply() \<.Rolling.apply\></span>, <span class="title-ref">DataFrame.rolling().apply() \<.Rolling.apply\></span>, <span class="title-ref">Series.expanding().apply() \<.Expanding.apply\></span>, and <span class="title-ref">DataFrame.expanding().apply() \<.Expanding.apply\></span> from `None` to `False` (`20584`)
  - Removed deprecated behavior of <span class="title-ref">Series.argmin</span> and <span class="title-ref">Series.argmax</span>, use <span class="title-ref">Series.idxmin</span> and <span class="title-ref">Series.idxmax</span> for the old behavior (`16955`)
  - Passing a tz-aware `datetime.datetime` or <span class="title-ref">Timestamp</span> into the <span class="title-ref">Timestamp</span> constructor with the `tz` argument now raises a `ValueError` (`23621`)
  - Removed `Series.base`, `Index.base`, `Categorical.base`, `Series.flags`, `Index.flags`, `PeriodArray.flags`, `Series.strides`, `Index.strides`, `Series.itemsize`, `Index.itemsize`, `Series.data`, `Index.data` (`20721`)
  - Changed <span class="title-ref">Timedelta.resolution</span> to match the behavior of the standard library `datetime.timedelta.resolution`, for the old behavior, use <span class="title-ref">Timedelta.resolution\_string</span> (`26839`)
  - Removed `Timestamp.weekday_name`, `DatetimeIndex.weekday_name`, and `Series.dt.weekday_name` (`18164`)
  - Removed the previously deprecated keyword "errors" in <span class="title-ref">Timestamp.tz\_localize</span>, <span class="title-ref">DatetimeIndex.tz\_localize</span>, and <span class="title-ref">Series.tz\_localize</span> (`22644`)
  - Changed the default "ordered" argument in <span class="title-ref">CategoricalDtype</span> from `None` to `False` (`26336`)
  - <span class="title-ref">Series.set\_axis</span> and <span class="title-ref">DataFrame.set\_axis</span> now require "labels" as the first argument and "axis" as an optional named parameter (`30089`)
  - Removed `to_msgpack`, `read_msgpack`, `DataFrame.to_msgpack`, `Series.to_msgpack` (`27103`)
  - Removed `Series.compress` (`21930`)
  - Removed the previously deprecated keyword "fill\_value" from <span class="title-ref">Categorical.fillna</span>, use "value" instead (`19269`)
  - Removed the previously deprecated keyword "data" from <span class="title-ref">andrews\_curves</span>, use "frame" instead (`6956`)
  - Removed the previously deprecated keyword "data" from <span class="title-ref">parallel\_coordinates</span>, use "frame" instead (`6956`)
  - Removed the previously deprecated keyword "colors" from <span class="title-ref">parallel\_coordinates</span>, use "color" instead (`6956`)
  - Removed the previously deprecated keywords "verbose" and "private\_key" from <span class="title-ref">read\_gbq</span> (`30200`)
  - Calling `np.array` and `np.asarray` on tz-aware <span class="title-ref">Series</span> and <span class="title-ref">DatetimeIndex</span> will now return an object array of tz-aware <span class="title-ref">Timestamp</span> (`24596`)
  - 
## Performance improvements

  - Performance improvement in <span class="title-ref">DataFrame</span> arithmetic and comparison operations with scalars (`24990`, `29853`)
  - Performance improvement in indexing with a non-unique <span class="title-ref">IntervalIndex</span> (`27489`)
  - Performance improvement in <span class="title-ref">MultiIndex.is\_monotonic</span> (`27495`)
  - Performance improvement in <span class="title-ref">cut</span> when `bins` is an <span class="title-ref">IntervalIndex</span> (`27668`)
  - Performance improvement when initializing a <span class="title-ref">DataFrame</span> using a `range` (`30171`)
  - Performance improvement in <span class="title-ref">DataFrame.corr</span> when `method` is `"spearman"` (`28139`)
  - Performance improvement in <span class="title-ref">DataFrame.replace</span> when provided a list of values to replace (`28099`)
  - Performance improvement in <span class="title-ref">DataFrame.select\_dtypes</span> by using vectorization instead of iterating over a loop (`28317`)
  - Performance improvement in <span class="title-ref">Categorical.searchsorted</span> and <span class="title-ref">CategoricalIndex.searchsorted</span> (`28795`)
  - Performance improvement when comparing a <span class="title-ref">Categorical</span> with a scalar and the scalar is not found in the categories (`29750`)
  - Performance improvement when checking if values in a <span class="title-ref">Categorical</span> are equal, equal or larger or larger than a given scalar. The improvement is not present if checking if the <span class="title-ref">Categorical</span> is less than or less than or equal than the scalar (`29820`)
  - Performance improvement in <span class="title-ref">Index.equals</span> and <span class="title-ref">MultiIndex.equals</span> (`29134`)
  - Performance improvement in <span class="title-ref">\~pandas.api.types.infer\_dtype</span> when `skipna` is `True` (`28814`)

## Bug fixes

### Categorical

  - Added test to assert the <span class="title-ref">fillna</span> raises the correct `ValueError` message when the value isn't a value from categories (`13628`)
  - Bug in <span class="title-ref">Categorical.astype</span> where `NaN` values were handled incorrectly when casting to int (`28406`)
  - <span class="title-ref">DataFrame.reindex</span> with a <span class="title-ref">CategoricalIndex</span> would fail when the targets contained duplicates, and wouldn't fail if the source contained duplicates (`28107`)
  - Bug in <span class="title-ref">Categorical.astype</span> not allowing for casting to extension dtypes (`28668`)
  - Bug where <span class="title-ref">merge</span> was unable to join on categorical and extension dtype columns (`28668`)
  - <span class="title-ref">Categorical.searchsorted</span> and <span class="title-ref">CategoricalIndex.searchsorted</span> now work on unordered categoricals also (`21667`)
  - Added test to assert roundtripping to parquet with <span class="title-ref">DataFrame.to\_parquet</span> or <span class="title-ref">read\_parquet</span> will preserve Categorical dtypes for string types (`27955`)
  - Changed the error message in <span class="title-ref">Categorical.remove\_categories</span> to always show the invalid removals as a set (`28669`)
  - Using date accessors on a categorical dtyped <span class="title-ref">Series</span> of datetimes was not returning an object of the same type as if one used the <span class="title-ref">.str.</span> / <span class="title-ref">.dt.</span> on a <span class="title-ref">Series</span> of that type. E.g. when accessing <span class="title-ref">Series.dt.tz\_localize</span> on a <span class="title-ref">Categorical</span> with duplicate entries, the accessor was skipping duplicates (`27952`)
  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> that would give incorrect results on categorical data (`26988`)
  - Bug where calling <span class="title-ref">Categorical.min</span> or <span class="title-ref">Categorical.max</span> on an empty Categorical would raise a numpy exception (`30227`)
  - The following methods now also correctly output values for unobserved categories when called through `groupby(..., observed=False)` (`17605`)
      - <span class="title-ref">core.groupby.SeriesGroupBy.count</span>
      - <span class="title-ref">core.groupby.SeriesGroupBy.size</span>
      - <span class="title-ref">core.groupby.SeriesGroupBy.nunique</span>
      - <span class="title-ref">core.groupby.SeriesGroupBy.nth</span>

### Datetimelike

  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> incorrectly casting `np.timedelta64("NaT")` to `np.datetime64("NaT")` when inserting into a <span class="title-ref">Series</span> with datetime64 dtype (`27311`)
  - Bug in <span class="title-ref">Series.dt</span> property lookups when the underlying data is read-only (`27529`)
  - Bug in `HDFStore.__getitem__` incorrectly reading tz attribute created in Python 2 (`26443`)
  - Bug in <span class="title-ref">to\_datetime</span> where passing arrays of malformed `str` with errors="coerce" could incorrectly lead to raising `ValueError` (`28299`)
  - Bug in <span class="title-ref">core.groupby.SeriesGroupBy.nunique</span> where `NaT` values were interfering with the count of unique values (`27951`)
  - Bug in <span class="title-ref">Timestamp</span> subtraction when subtracting a <span class="title-ref">Timestamp</span> from a `np.datetime64` object incorrectly raising `TypeError` (`28286`)
  - Addition and subtraction of integer or integer-dtype arrays with <span class="title-ref">Timestamp</span> will now raise `NullFrequencyError` instead of `ValueError` (`28268`)
  - Bug in <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> with integer dtype failing to raise `TypeError` when adding or subtracting a `np.datetime64` object (`28080`)
  - Bug in <span class="title-ref">Series.astype</span>, <span class="title-ref">Index.astype</span>, and <span class="title-ref">DataFrame.astype</span> failing to handle `NaT` when casting to an integer dtype (`28492`)
  - Bug in <span class="title-ref">Week</span> with `weekday` incorrectly raising `AttributeError` instead of `TypeError` when adding or subtracting an invalid type (`28530`)
  - Bug in <span class="title-ref">DataFrame</span> arithmetic operations when operating with a <span class="title-ref">Series</span> with dtype `'timedelta64[ns]'` (`28049`)
  - Bug in <span class="title-ref">core.groupby.generic.SeriesGroupBy.apply</span> raising `ValueError` when a column in the original DataFrame is a datetime and the column labels are not standard integers (`28247`)
  - Bug in <span class="title-ref">pandas.\_config.localization.get\_locales</span> where the `locales -a` encodes the locales list as windows-1252 (`23638`, `24760`, `27368`)
  - Bug in <span class="title-ref">Series.var</span> failing to raise `TypeError` when called with `timedelta64[ns]` dtype (`28289`)
  - Bug in <span class="title-ref">DatetimeIndex.strftime</span> and <span class="title-ref">Series.dt.strftime</span> where `NaT` was converted to the string `'NaT'` instead of `np.nan` (`29578`)
  - Bug in masking datetime-like arrays with a boolean mask of an incorrect length not raising an `IndexError` (`30308`)
  - Bug in <span class="title-ref">Timestamp.resolution</span> being a property instead of a class attribute (`29910`)
  - Bug in <span class="title-ref">pandas.to\_datetime</span> when called with `None` raising `TypeError` instead of returning `NaT` (`30011`)
  - Bug in <span class="title-ref">pandas.to\_datetime</span> failing for `deque` objects when using `cache=True` (the default) (`29403`)
  - Bug in <span class="title-ref">Series.item</span> with `datetime64` or `timedelta64` dtype, <span class="title-ref">DatetimeIndex.item</span>, and <span class="title-ref">TimedeltaIndex.item</span> returning an integer instead of a <span class="title-ref">Timestamp</span> or <span class="title-ref">Timedelta</span> (`30175`)
  - Bug in <span class="title-ref">DatetimeIndex</span> addition when adding a non-optimized <span class="title-ref">DateOffset</span> incorrectly dropping timezone information (`30336`)
  - Bug in <span class="title-ref">DataFrame.drop</span> where attempting to drop non-existent values from a DatetimeIndex would yield a confusing error message (`30399`)
  - Bug in <span class="title-ref">DataFrame.append</span> would remove the timezone-awareness of new data (`30238`)
  - Bug in <span class="title-ref">Series.cummin</span> and <span class="title-ref">Series.cummax</span> with timezone-aware dtype incorrectly dropping its timezone (`15553`)
  - Bug in <span class="title-ref">DatetimeArray</span>, <span class="title-ref">TimedeltaArray</span>, and <span class="title-ref">PeriodArray</span> where inplace addition and subtraction did not actually operate inplace (`24115`)
  - Bug in <span class="title-ref">pandas.to\_datetime</span> when called with `Series` storing `IntegerArray` raising `TypeError` instead of returning `Series` (`30050`)
  - Bug in <span class="title-ref">date\_range</span> with custom business hours as `freq` and given number of `periods` (`30593`)
  - Bug in <span class="title-ref">PeriodIndex</span> comparisons with incorrectly casting integers to <span class="title-ref">Period</span> objects, inconsistent with the <span class="title-ref">Period</span> comparison behavior (`30722`)
  - Bug in <span class="title-ref">DatetimeIndex.insert</span> raising a `ValueError` instead of a `TypeError` when trying to insert a timezone-aware <span class="title-ref">Timestamp</span> into a timezone-naive <span class="title-ref">DatetimeIndex</span>, or vice-versa (`30806`)

### Timedelta

  - Bug in subtracting a <span class="title-ref">TimedeltaIndex</span> or <span class="title-ref">TimedeltaArray</span> from a `np.datetime64` object (`29558`)
  - 
### Timezones

  - 
### Numeric

  - Bug in <span class="title-ref">DataFrame.quantile</span> with zero-column <span class="title-ref">DataFrame</span> incorrectly raising (`23925`)
  - <span class="title-ref">DataFrame</span> flex inequality comparisons methods (<span class="title-ref">DataFrame.lt</span>, <span class="title-ref">DataFrame.le</span>, <span class="title-ref">DataFrame.gt</span>, <span class="title-ref">DataFrame.ge</span>) with object-dtype and `complex` entries failing to raise `TypeError` like their <span class="title-ref">Series</span> counterparts (`28079`)
  - Bug in <span class="title-ref">DataFrame</span> logical operations (`&`, `|`, `^`) not matching <span class="title-ref">Series</span> behavior by filling NA values (`28741`)
  - Bug in <span class="title-ref">DataFrame.interpolate</span> where specifying axis by name references variable before it is assigned (`29142`)
  - Bug in <span class="title-ref">Series.var</span> not computing the right value with a nullable integer dtype series not passing through ddof argument (`29128`)
  - Improved error message when using `frac` \> 1 and `replace` = False (`27451`)
  - Bug in numeric indexes resulted in it being possible to instantiate an <span class="title-ref">Int64Index</span>, <span class="title-ref">UInt64Index</span>, or <span class="title-ref">Float64Index</span> with an invalid dtype (e.g. datetime-like) (`29539`)
  - Bug in <span class="title-ref">UInt64Index</span> precision loss while constructing from a list with values in the `np.uint64` range (`29526`)
  - Bug in <span class="title-ref">NumericIndex</span> construction that caused indexing to fail when integers in the `np.uint64` range were used (`28023`)
  - Bug in <span class="title-ref">NumericIndex</span> construction that caused <span class="title-ref">UInt64Index</span> to be casted to <span class="title-ref">Float64Index</span> when integers in the `np.uint64` range were used to index a <span class="title-ref">DataFrame</span> (`28279`)
  - Bug in <span class="title-ref">Series.interpolate</span> when using method=\`index\` with an unsorted index, would previously return incorrect results. (`21037`)
  - Bug in <span class="title-ref">DataFrame.round</span> where a <span class="title-ref">DataFrame</span> with a <span class="title-ref">CategoricalIndex</span> of <span class="title-ref">IntervalIndex</span> columns would incorrectly raise a `TypeError` (`30063`)
  - Bug in <span class="title-ref">Series.pct\_change</span> and <span class="title-ref">DataFrame.pct\_change</span> when there are duplicated indices (`30463`)
  - Bug in <span class="title-ref">DataFrame</span> cumulative operations (e.g. cumsum, cummax) incorrect casting to object-dtype (`19296`)
  - Bug in <span class="title-ref">\~DataFrame.diff</span> losing the dtype for extension types (`30889`)
  - Bug in <span class="title-ref">DataFrame.diff</span> raising an `IndexError` when one of the columns was a nullable integer dtype (`30967`)

### Conversion

  - 
### Strings

  - Calling <span class="title-ref">Series.str.isalnum</span> (and other "ismethods") on an empty `Series` would return an `object` dtype instead of `bool` (`29624`)
  - 
### Interval

  - Bug in <span class="title-ref">IntervalIndex.get\_indexer</span> where a <span class="title-ref">Categorical</span> or <span class="title-ref">CategoricalIndex</span> `target` would incorrectly raise a `TypeError` (`30063`)
  - Bug in `pandas.core.dtypes.cast.infer_dtype_from_scalar` where passing `pandas_dtype=True` did not infer <span class="title-ref">IntervalDtype</span> (`30337`)
  - Bug in <span class="title-ref">Series</span> constructor where constructing a `Series` from a `list` of <span class="title-ref">Interval</span> objects resulted in `object` dtype instead of <span class="title-ref">IntervalDtype</span> (`23563`)
  - Bug in <span class="title-ref">IntervalDtype</span> where the `kind` attribute was incorrectly set as `None` instead of `"O"` (`30568`)
  - Bug in <span class="title-ref">IntervalIndex</span>, <span class="title-ref">\~arrays.IntervalArray</span>, and <span class="title-ref">Series</span> with interval data where equality comparisons were incorrect (`24112`)

### Indexing

  - Bug in assignment using a reverse slicer (`26939`)
  - Bug in <span class="title-ref">DataFrame.explode</span> would duplicate frame in the presence of duplicates in the index (`28010`)
  - Bug in reindexing a <span class="title-ref">PeriodIndex</span> with another type of index that contained a `Period` (`28323`) (`28337`)
  - Fix assignment of column via `.loc` with numpy non-ns datetime type (`27395`)
  - Bug in <span class="title-ref">Float64Index.astype</span> where `np.inf` was not handled properly when casting to an integer dtype (`28475`)
  - <span class="title-ref">Index.union</span> could fail when the left contained duplicates (`28257`)
  - Bug when indexing with `.loc` where the index was a <span class="title-ref">CategoricalIndex</span> with non-string categories didn't work (`17569`, `30225`)
  - <span class="title-ref">Index.get\_indexer\_non\_unique</span> could fail with `TypeError` in some cases, such as when searching for ints in a string index (`28257`)
  - Bug in <span class="title-ref">Float64Index.get\_loc</span> incorrectly raising `TypeError` instead of `KeyError` (`29189`)
  - Bug in <span class="title-ref">DataFrame.loc</span> with incorrect dtype when setting Categorical value in 1-row DataFrame (`25495`)
  - <span class="title-ref">MultiIndex.get\_loc</span> can't find missing values when input includes missing values (`19132`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> incorrectly assigning values with boolean indexer when the length of new data matches the number of `True` values and new data is not a `Series` or an `np.array` (`30567`)
  - Bug in indexing with a <span class="title-ref">PeriodIndex</span> incorrectly accepting integers representing years, use e.g. `ser.loc["2007"]` instead of `ser.loc[2007]` (`30763`)

### Missing

  - 
### MultiIndex

  - Constructor for <span class="title-ref">MultiIndex</span> verifies that the given `sortorder` is compatible with the actual `lexsort_depth` if `verify_integrity` parameter is `True` (the default) (`28735`)
  - Series and MultiIndex `.drop` with `MultiIndex` raise exception if labels not in given in level (`8594`)
  - 
### IO

  - <span class="title-ref">read\_csv</span> now accepts binary mode file buffers when using the Python csv engine (`23779`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> where using a Tuple as a column or index value and using `orient="columns"` or `orient="index"` would produce invalid JSON (`20500`)
  - Improve infinity parsing. <span class="title-ref">read\_csv</span> now interprets `Infinity`, `+Infinity`, `-Infinity` as floating point values (`10065`)
  - Bug in <span class="title-ref">DataFrame.to\_csv</span> where values were truncated when the length of `na_rep` was shorter than the text input data. (`25099`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> where values were truncated using display options instead of outputting the full content (`9784`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> where a datetime column label would not be written out in ISO format with `orient="table"` (`28130`)
  - Bug in <span class="title-ref">DataFrame.to\_parquet</span> where writing to GCS would fail with `engine='fastparquet'` if the file did not already exist (`28326`)
  - Bug in <span class="title-ref">read\_hdf</span> closing stores that it didn't open when Exceptions are raised (`28699`)
  - Bug in <span class="title-ref">DataFrame.read\_json</span> where using `orient="index"` would not maintain the order (`28557`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> where the length of the `formatters` argument was not verified (`28469`)
  - Bug in <span class="title-ref">DataFrame.read\_excel</span> with `engine='ods'` when `sheet_name` argument references a non-existent sheet (`27676`)
  - Bug in <span class="title-ref">pandas.io.formats.style.Styler</span> formatting for floating values not displaying decimals correctly (`13257`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> when using `formatters=<list>` and `max_cols` together. (`25955`)
  - Bug in <span class="title-ref">Styler.background\_gradient</span> not able to work with dtype `Int64` (`28869`)
  - Bug in <span class="title-ref">DataFrame.to\_clipboard</span> which did not work reliably in ipython (`22707`)
  - Bug in <span class="title-ref">read\_json</span> where default encoding was not set to `utf-8` (`29565`)
  - Bug in <span class="title-ref">PythonParser</span> where str and bytes were being mixed when dealing with the decimal field (`29650`)
  - <span class="title-ref">read\_gbq</span> now accepts `progress_bar_type` to display progress bar while the data downloads. (`29857`)
  - Bug in <span class="title-ref">pandas.io.json.json\_normalize</span> where a missing value in the location specified by `record_path` would raise a `TypeError` (`30148`)
  - <span class="title-ref">read\_excel</span> now accepts binary data (`15914`)
  - Bug in <span class="title-ref">read\_csv</span> in which encoding handling was limited to just the string `utf-16` for the C engine (`24130`)

### Plotting

  - Bug in <span class="title-ref">Series.plot</span> not able to plot boolean values (`23719`)
  - Bug in <span class="title-ref">DataFrame.plot</span> not able to plot when no rows (`27758`)
  - Bug in <span class="title-ref">DataFrame.plot</span> producing incorrect legend markers when plotting multiple series on the same axis (`18222`)
  - Bug in <span class="title-ref">DataFrame.plot</span> when `kind='box'` and data contains datetime or timedelta data. These types are now automatically dropped (`22799`)
  - Bug in <span class="title-ref">DataFrame.plot.line</span> and <span class="title-ref">DataFrame.plot.area</span> produce wrong xlim in x-axis (`27686`, `25160`, `24784`)
  - Bug where <span class="title-ref">DataFrame.boxplot</span> would not accept a `color` parameter like <span class="title-ref">DataFrame.plot.box</span> (`26214`)
  - Bug in the `xticks` argument being ignored for <span class="title-ref">DataFrame.plot.bar</span> (`14119`)
  - <span class="title-ref">set\_option</span> now validates that the plot backend provided to `'plotting.backend'` implements the backend when the option is set, rather than when a plot is created (`28163`)
  - <span class="title-ref">DataFrame.plot</span> now allow a `backend` keyword argument to allow changing between backends in one session (`28619`).
  - Bug in color validation incorrectly raising for non-color styles (`29122`).
  - Allow <span class="title-ref">DataFrame.plot.scatter</span> to plot `objects` and `datetime` type data (`18755`, `30391`)
  - Bug in <span class="title-ref">DataFrame.hist</span>, `xrot=0` does not work with `by` and subplots (`30288`).

### GroupBy/resample/rolling

  - Bug in <span class="title-ref">core.groupby.DataFrameGroupBy.apply</span> only showing output from a single group when function returns an <span class="title-ref">Index</span> (`28652`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> with multiple groups where an `IndexError` would be raised if any group contained all NA values (`20519`)
  - Bug in <span class="title-ref">.Resampler.size</span> and <span class="title-ref">.Resampler.count</span> returning wrong dtype when used with an empty <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> (`28427`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> not allowing for rolling over datetimes when `axis=1` (`28192`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> not allowing rolling over multi-index levels (`15584`).
  - Bug in <span class="title-ref">DataFrame.rolling</span> not allowing rolling on monotonic decreasing time indexes (`19248`).
  - Bug in <span class="title-ref">DataFrame.groupby</span> not offering selection by column name when `axis=1` (`27614`)
  - Bug in <span class="title-ref">core.groupby.DataFrameGroupby.agg</span> not able to use lambda function with named aggregation (`27519`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> losing column name information when grouping by a categorical column (`28787`)
  - Remove error raised due to duplicated input functions in named aggregation in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span>. Previously error will be raised if the same function is applied on the same column and now it is allowed if new assigned names are different. (`28426`)
  - <span class="title-ref">core.groupby.SeriesGroupBy.value\_counts</span> will be able to handle the case even when the <span class="title-ref">Grouper</span> makes empty groups (`28479`)
  - Bug in <span class="title-ref">core.window.rolling.Rolling.quantile</span> ignoring `interpolation` keyword argument when used within a groupby (`28779`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> where `any`, `all`, `nunique` and transform functions would incorrectly handle duplicate column labels (`21668`)
  - Bug in <span class="title-ref">core.groupby.DataFrameGroupBy.agg</span> with timezone-aware datetime64 column incorrectly casting results to the original dtype (`29641`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> when using axis=1 and having a single level columns index (`30208`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> when using nunique on axis=1 (`30253`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> with multiple list-like q value and integer column names (`30289`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.pct\_change</span> and <span class="title-ref">.SeriesGroupBy.pct\_change</span> causes `TypeError` when `fill_method` is `None` (`30463`)
  - Bug in <span class="title-ref">Rolling.count</span> and <span class="title-ref">Expanding.count</span> argument where `min_periods` was ignored (`26996`)

### Reshaping

  - Bug in <span class="title-ref">DataFrame.apply</span> that caused incorrect output with empty <span class="title-ref">DataFrame</span> (`28202`, `21959`)
  - Bug in <span class="title-ref">DataFrame.stack</span> not handling non-unique indexes correctly when creating MultiIndex (`28301`)
  - Bug in <span class="title-ref">pivot\_table</span> not returning correct type `float` when `margins=True` and `aggfunc='mean'` (`24893`)
  - Bug <span class="title-ref">merge\_asof</span> could not use <span class="title-ref">datetime.timedelta</span> for `tolerance` kwarg (`28098`)
  - Bug in <span class="title-ref">merge</span>, did not append suffixes correctly with MultiIndex (`28518`)
  - <span class="title-ref">qcut</span> and <span class="title-ref">cut</span> now handle boolean input (`20303`)
  - Fix to ensure all int dtypes can be used in <span class="title-ref">merge\_asof</span> when using a tolerance value. Previously every non-int64 type would raise an erroneous `MergeError` (`28870`).
  - Better error message in <span class="title-ref">get\_dummies</span> when `columns` isn't a list-like value (`28383`)
  - Bug in <span class="title-ref">Index.join</span> that caused infinite recursion error for mismatched `MultiIndex` name orders. (`25760`, `28956`)
  - Bug <span class="title-ref">Series.pct\_change</span> where supplying an anchored frequency would throw a `ValueError` (`28664`)
  - Bug where <span class="title-ref">DataFrame.equals</span> returned True incorrectly in some cases when two DataFrames had the same columns in different orders (`28839`)
  - Bug in <span class="title-ref">DataFrame.replace</span> that caused non-numeric replacer's dtype not respected (`26632`)
  - Bug in <span class="title-ref">melt</span> where supplying mixed strings and numeric values for `id_vars` or `value_vars` would incorrectly raise a `ValueError` (`29718`)
  - Dtypes are now preserved when transposing a `DataFrame` where each column is the same extension dtype (`30091`)
  - Bug in <span class="title-ref">merge\_asof</span> merging on a tz-aware `left_index` and `right_on` a tz-aware column (`29864`)
  - Improved error message and docstring in <span class="title-ref">cut</span> and <span class="title-ref">qcut</span> when `labels=True` (`13318`)
  - Bug in missing `fill_na` parameter to <span class="title-ref">DataFrame.unstack</span> with list of levels (`30740`)

### Sparse

  - Bug in <span class="title-ref">SparseDataFrame</span> arithmetic operations incorrectly casting inputs to float (`28107`)
  - Bug in `DataFrame.sparse` returning a `Series` when there was a column named `sparse` rather than the accessor (`30758`)
  - Fixed <span class="title-ref">operator.xor</span> with a boolean-dtype `SparseArray`. Now returns a sparse result, rather than object dtype (`31025`)

### ExtensionArray

  - Bug in <span class="title-ref">arrays.PandasArray</span> when setting a scalar string (`28118`, `28150`).
  - Bug where nullable integers could not be compared to strings (`28930`)
  - Bug where <span class="title-ref">DataFrame</span> constructor raised `ValueError` with list-like data and `dtype` specified (`30280`)

### Other

  - Trying to set the `display.precision`, `display.max_rows` or `display.max_columns` using <span class="title-ref">set\_option</span> to anything but a `None` or a positive int will raise a `ValueError` (`23348`)
  - Using <span class="title-ref">DataFrame.replace</span> with overlapping keys in a nested dictionary will no longer raise, now matching the behavior of a flat dictionary (`27660`)
  - <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span> now support dicts as `compression` argument with key `'method'` being the compression method and others as additional compression options when the compression method is `'zip'`. (`26023`)
  - Bug in <span class="title-ref">Series.diff</span> where a boolean series would incorrectly raise a `TypeError` (`17294`)
  - <span class="title-ref">Series.append</span> will no longer raise a `TypeError` when passed a tuple of `Series` (`28410`)
  - Fix corrupted error message when calling `pandas.libs._json.encode()` on a 0d array (`18878`)
  - Backtick quoting in <span class="title-ref">DataFrame.query</span> and <span class="title-ref">DataFrame.eval</span> can now also be used to use invalid identifiers like names that start with a digit, are python keywords, or are using single character operators. (`27017`)
  - Bug in `pd.core.util.hashing.hash_pandas_object` where arrays containing tuples were incorrectly treated as non-hashable (`28969`)
  - Bug in <span class="title-ref">DataFrame.append</span> that raised `IndexError` when appending with empty list (`28769`)
  - Fix <span class="title-ref">AbstractHolidayCalendar</span> to return correct results for years after 2030 (now goes up to 2200) (`27790`)
  - Fixed <span class="title-ref">\~arrays.IntegerArray</span> returning `inf` rather than `NaN` for operations dividing by `0` (`27398`)
  - Fixed `pow` operations for <span class="title-ref">\~arrays.IntegerArray</span> when the other value is `0` or `1` (`29997`)
  - Bug in <span class="title-ref">Series.count</span> raises if use\_inf\_as\_na is enabled (`29478`)
  - Bug in <span class="title-ref">Index</span> where a non-hashable name could be set without raising `TypeError` (`29069`)
  - Bug in <span class="title-ref">DataFrame</span> constructor when passing a 2D `ndarray` and an extension dtype (`12513`)
  - Bug in <span class="title-ref">DataFrame.to\_csv</span> when supplied a series with a `dtype="string"` and a `na_rep`, the `na_rep` was being truncated to 2 characters. (`29975`)
  - Bug where <span class="title-ref">DataFrame.itertuples</span> would incorrectly determine whether or not namedtuples could be used for dataframes of 255 columns (`28282`)
  - Handle nested NumPy `object` arrays in <span class="title-ref">testing.assert\_series\_equal</span> for ExtensionArray implementations (`30841`)
  - Bug in <span class="title-ref">Index</span> constructor incorrectly allowing 2-dimensional input arrays (`13601`, `27125`)

## Contributors

<div class="contributors">

v0.25.3..v1.0.0

</div>

---

v1.0.1.md

---

# What's new in 1.0.1 (February 5, 2020)

These are the changes in pandas 1.0.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame</span> setting values with a slice (e.g. `df[-4:] = 1`) indexing by label instead of position (`31469`)
  - Fixed regression when indexing a `Series` or `DataFrame` indexed by `DatetimeIndex` with a slice containing a <span class="title-ref">datetime.date</span> (`31501`)
  - Fixed regression in `DataFrame.__setitem__` raising an `AttributeError` with a <span class="title-ref">MultiIndex</span> and a non-monotonic indexer (`31449`)
  - Fixed regression in <span class="title-ref">Series</span> multiplication when multiplying a numeric <span class="title-ref">Series</span> with \>10000 elements with a timedelta-like scalar (`31457`)
  - Fixed regression in `.groupby().agg()` raising an `AssertionError` for some reductions like `min` on object-dtype columns (`31522`)
  - Fixed regression in `.groupby()` aggregations with categorical dtype using Cythonized reduction functions (e.g. `first`) (`31450`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> if called with a function which returned a non-pandas non-scalar object (e.g. a list or numpy array) (`31441`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby</span> whereby taking the minimum or maximum of a column with period dtype would raise a `TypeError`. (`31471`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby</span> with an empty DataFrame grouping by a level of a MultiIndex (`31670`).
  - Fixed regression in <span class="title-ref">DataFrame.apply</span> with object dtype and non-reducing function (`31505`)
  - Fixed regression in <span class="title-ref">to\_datetime</span> when parsing non-nanosecond resolution datetimes (`31491`)
  - Fixed regression in <span class="title-ref">\~DataFrame.to\_csv</span> where specifying an `na_rep` might truncate the values written (`31447`)
  - Fixed regression in <span class="title-ref">Categorical</span> construction with `numpy.str_` categories (`31499`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">DataFrame.iloc</span> when selecting a row containing a single `datetime64` or `timedelta64` column (`31649`)
  - Fixed regression where setting <span class="title-ref">pd.options.display.max\_colwidth</span> was not accepting negative integer. In addition, this behavior has been deprecated in favor of using `None` (`31532`)
  - Fixed regression in objTOJSON.c fix return-type warning (`31463`)
  - Fixed regression in <span class="title-ref">qcut</span> when passed a nullable integer. (`31389`)
  - Fixed regression in assigning to a <span class="title-ref">Series</span> using a nullable integer dtype (`31446`)
  - Fixed performance regression when indexing a `DataFrame` or `Series` with a <span class="title-ref">MultiIndex</span> for the index using a list of labels (`31648`)
  - Fixed regression in <span class="title-ref">read\_csv</span> used in file like object `RawIOBase` is not recognize `encoding` option (`31575`)

## Deprecations

  - Support for negative integer for <span class="title-ref">pd.options.display.max\_colwidth</span> is deprecated in favor of using `None` (`31532`)

## Bug fixes

**Datetimelike**

  - Fixed bug in <span class="title-ref">to\_datetime</span> raising when `cache=True` and out-of-bound values are present (`31491`)

**Numeric**

  - Bug in dtypes being lost in `DataFrame.__invert__` (`~` operator) with mixed dtypes (`31183`) and for extension-array backed `Series` and `DataFrame` (`23087`)

**Plotting**

  - Plotting tz-aware timeseries no longer gives UserWarning (`31205`)

**Interval**

  - Bug in <span class="title-ref">Series.shift</span> with `interval` dtype raising a `TypeError` when shifting an interval array of integers or datetimes (`34195`)

## Contributors

<div class="contributors">

v1.0.0..v1.0.1|HEAD

</div>

---

v1.0.2.md

---

# What's new in 1.0.2 (March 12, 2020)

These are the changes in pandas 1.0.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

**Groupby**

  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span> which were failing on frames with <span class="title-ref">MultiIndex</span> columns and a custom function (`31777`)
  - Fixed regression in `groupby(..).rolling(..).apply()` (`RollingGroupby`) where the `raw` parameter was ignored (`31754`)
  - Fixed regression in <span class="title-ref">rolling(..).corr() \<.Rolling.corr\></span> when using a time offset (`31789`)
  - Fixed regression in <span class="title-ref">groupby(..).nunique() \<.DataFrameGroupBy.nunique\></span> which was modifying the original values if `NaN` values were present (`31950`)
  - Fixed regression in `DataFrame.groupby` raising a `ValueError` from an internal operation (`31802`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span> calling a user-provided function an extra time on an empty input (`31760`)

**I/O**

  - Fixed regression in <span class="title-ref">read\_csv</span> in which the `encoding` option was not recognized with certain file-like objects (`31819`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_excel</span> when the `columns` keyword argument is passed (`31677`)
  - Fixed regression in <span class="title-ref">ExcelFile</span> where the stream passed into the function was closed by the destructor. (`31467`)
  - Fixed regression where <span class="title-ref">read\_pickle</span> raised a `UnicodeDecodeError` when reading a py27 pickle with <span class="title-ref">MultiIndex</span> column (`31988`).

**Reindexing/alignment**

  - Fixed regression in <span class="title-ref">Series.align</span> when `other` is a <span class="title-ref">DataFrame</span> and `method` is not `None` (`31785`)
  - Fixed regression in <span class="title-ref">DataFrame.reindex</span> and <span class="title-ref">Series.reindex</span> when reindexing with (tz-aware) index and `method=nearest` (`26683`)
  - Fixed regression in <span class="title-ref">DataFrame.reindex\_like</span> on a <span class="title-ref">DataFrame</span> subclass raised an `AssertionError` (`31925`)
  - Fixed regression in <span class="title-ref">DataFrame</span> arithmetic operations with mis-matched columns (`31623`)

**Other**

  - Fixed regression in joining on <span class="title-ref">DatetimeIndex</span> or <span class="title-ref">TimedeltaIndex</span> to preserve `freq` in simple cases (`32166`)
  - Fixed regression in <span class="title-ref">Series.shift</span> with `datetime64` dtype when passing an integer `fill_value` (`32591`)
  - Fixed regression in the repr of an object-dtype <span class="title-ref">Index</span> with bools and missing values (`32146`)

## Indexing with nullable Boolean arrays

Previously indexing with a nullable Boolean array containing `NA` would raise a `ValueError`, however this is now permitted with `NA` being treated as `False`. (`31503`)

<div class="ipython">

python

s = pd.Series(\[1, 2, 3, 4\]) mask = pd.array(\[True, True, False, None\], dtype="boolean") s mask

</div>

*pandas 1.0.0-1.0.1*

`` `python     >>> s[mask]     Traceback (most recent call last):     ...     ValueError: cannot mask with array containing NA / NaN values  *pandas 1.0.2*  .. ipython:: python      s[mask]  .. _whatsnew_102.bug_fixes:  Bug fixes ``\` \~\~\~\~\~\~\~\~\~

**Datetimelike**

  - Bug in <span class="title-ref">Series.astype</span> not copying for tz-naive and tz-aware `datetime64` dtype (`32490`)
  - Bug where <span class="title-ref">to\_datetime</span> would raise when passed `pd.NA` (`32213`)
  - Improved error message when subtracting two <span class="title-ref">Timestamp</span> that result in an out-of-bounds <span class="title-ref">Timedelta</span> (`31774`)

**Categorical**

  - Fixed bug where <span class="title-ref">Categorical.from\_codes</span> improperly raised a `ValueError` when passed nullable integer codes. (`31779`)
  - Fixed bug where <span class="title-ref">Categorical</span> constructor would raise a `TypeError` when given a numpy array containing `pd.NA`. (`31927`)
  - Bug in <span class="title-ref">Categorical</span> that would ignore or crash when calling <span class="title-ref">Series.replace</span> with a list-like `to_replace` (`31720`)

**I/O**

  - Using `pd.NA` with <span class="title-ref">DataFrame.to\_json</span> now correctly outputs a null value instead of an empty object (`31615`)
  - Bug in <span class="title-ref">pandas.json\_normalize</span> when value in meta path is not iterable (`31507`)
  - Fixed pickling of `pandas.NA`. Previously a new object was returned, which broke computations relying on `NA` being a singleton (`31847`)
  - Fixed bug in parquet roundtrip with nullable unsigned integer dtypes (`31896`).

**Experimental dtypes**

  - Fixed bug in <span class="title-ref">DataFrame.convert\_dtypes</span> for columns that were already using the `"string"` dtype (`31731`).
  - Fixed bug in <span class="title-ref">DataFrame.convert\_dtypes</span> for series with mix of integers and strings (`32117`)
  - Fixed bug in <span class="title-ref">DataFrame.convert\_dtypes</span> where `BooleanDtype` columns were converted to `Int64` (`32287`)
  - Fixed bug in setting values using a slice indexer with string dtype (`31772`)
  - Fixed bug where <span class="title-ref">.DataFrameGroupBy.first</span>, <span class="title-ref">.SeriesGroupBy.first</span>, <span class="title-ref">.DataFrameGroupBy.last</span>, and <span class="title-ref">.SeriesGroupBy.last</span> would raise a `TypeError` when groups contained `pd.NA` in a column of object dtype (`32123`)
  - Fixed bug where <span class="title-ref">DataFrameGroupBy.mean</span>, <span class="title-ref">DataFrameGroupBy.median</span>, <span class="title-ref">DataFrameGroupBy.var</span>, and <span class="title-ref">DataFrameGroupBy.std</span> would raise a `TypeError` on `Int64` dtype columns (`32219`)

**Strings**

  - Using `pd.NA` with <span class="title-ref">Series.str.repeat</span> now correctly outputs a null value instead of raising error for vector inputs (`31632`)

**Rolling**

  - Fixed rolling operations with variable window (defined by time duration) on decreasing time index (`32385`).

## Contributors

<div class="contributors">

v1.0.1..v1.0.2

</div>

---

v1.0.3.md

---

# What's new in 1.0.3 (March 17, 2020)

These are the changes in pandas 1.0.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in `resample.agg` when the underlying data is non-writeable (`31710`)
  - Fixed regression in <span class="title-ref">DataFrame</span> exponentiation with reindexing (`32685`)

## Bug fixes

## Contributors

<div class="contributors">

v1.0.2..v1.0.3

</div>

---

v1.0.4.md

---

# What's new in 1.0.4 (May 28, 2020)

These are the changes in pandas 1.0.4. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fix regression where <span class="title-ref">Series.isna</span> and <span class="title-ref">DataFrame.isna</span> would raise for categorical dtype when `pandas.options.mode.use_inf_as_na` was set to `True` (`33594`)
  - Fix regression in <span class="title-ref">.DataFrameGroupBy.first</span>, <span class="title-ref">.SeriesGroupBy.first</span>, <span class="title-ref">.DataFrameGroupBy.last</span>, and <span class="title-ref">.SeriesGroupBy.last</span> where None is not preserved in object dtype (`32800`)
  - Fix regression in DataFrame reductions using `numeric_only=True` and ExtensionArrays (`33256`).
  - Fix performance regression in `memory_usage(deep=True)` for object dtype (`33012`)
  - Fix regression where <span class="title-ref">Categorical.replace</span> would replace with `NaN` whenever the new value and replacement value were equal (`33288`)
  - Fix regression where an ordered <span class="title-ref">Categorical</span> containing only `NaN` values would raise rather than returning `NaN` when taking the minimum or maximum (`33450`)
  - Fix regression in <span class="title-ref">DataFrameGroupBy.agg</span> with dictionary input losing `ExtensionArray` dtypes (`32194`)
  - Fix to preserve the ability to index with the "nearest" method with xarray's CFTimeIndex, an <span class="title-ref">Index</span> subclass ([pydata/xarray\#3751](https://github.com/pydata/xarray/issues/3751), `32905`).
  - Fix regression in <span class="title-ref">DataFrame.describe</span> raising `TypeError: unhashable type: 'dict'` (`32409`)
  - Fix regression in <span class="title-ref">DataFrame.replace</span> casts columns to `object` dtype if items in `to_replace` not in values (`32988`)
  - Fix regression in <span class="title-ref">Series.groupby</span> would raise `ValueError` when grouping by <span class="title-ref">PeriodIndex</span> level (`34010`)
  - Fix regression in <span class="title-ref">DataFrameGroupBy.rolling.apply</span> and <span class="title-ref">SeriesGroupBy.rolling.apply</span> ignoring args and kwargs parameters (`33433`)
  - Fix regression in error message with `np.min` or `np.max` on unordered <span class="title-ref">Categorical</span> (`33115`)
  - Fix regression in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> throwing an error when a `datetime64[ns, tz]` value is provided (`32395`)

## Bug fixes

  - Bug in <span class="title-ref">SeriesGroupBy.first</span>, <span class="title-ref">SeriesGroupBy.last</span>, <span class="title-ref">SeriesGroupBy.min</span>, and <span class="title-ref">SeriesGroupBy.max</span> returning floats when applied to nullable Booleans (`33071`)
  - Bug in <span class="title-ref">Rolling.min</span> and \`Rolling.max\`: Growing memory usage after multiple calls when using a fixed window (`30726`)
  - Bug in <span class="title-ref">\~DataFrame.to\_parquet</span> was not raising `PermissionError` when writing to a private s3 bucket with invalid creds. (`27679`)
  - Bug in <span class="title-ref">\~DataFrame.to\_csv</span> was silently failing when writing to an invalid s3 bucket. (`32486`)
  - Bug in <span class="title-ref">read\_parquet</span> was raising a `FileNotFoundError` when passed an s3 directory path. (`26388`)
  - Bug in <span class="title-ref">\~DataFrame.to\_parquet</span> was throwing an `AttributeError` when writing a partitioned parquet file to s3 (`27596`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> causes the quantiles to be shifted when the `by` axis contains `NaN` (`33200`, `33569`)

## Contributors

<div class="contributors">

v1.0.3..v1.0.4

</div>

---

v1.0.5.md

---

# What's new in 1.0.5 (June 17, 2020)

These are the changes in pandas 1.0.5. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fix regression in <span class="title-ref">read\_parquet</span> when reading from file-like objects (`34467`).
  - Fix regression in reading from public S3 buckets (`34626`).

Note this disables the ability to read Parquet files from directories on S3 again (`26388`, `34632`), which was added in the 1.0.4 release, but is now targeted for pandas 1.1.0.

  - Fixed regression in <span class="title-ref">\~DataFrame.replace</span> raising an `AssertionError` when replacing values in an extension dtype with values of a different dtype (`34530`)

## Bug fixes

  - Fixed building from source with Python 3.8 fetching the wrong version of NumPy (`34666`)

## Contributors

<div class="contributors">

v1.0.4..v1.0.5|HEAD

</div>

---

v1.1.0.md

---

# What's new in 1.1.0 (July 28, 2020)

These are the changes in pandas 1.1.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### KeyErrors raised by loc specify missing labels

Previously, if labels were missing for a `.loc` call, a KeyError was raised stating that this was no longer supported.

Now the error message also includes a list of the missing labels (max 10 items, display width 80 characters). See `34272`.

### All dtypes can now be converted to `StringDtype`

Previously, declaring or converting to <span class="title-ref">StringDtype</span> was in general only possible if the data was already only `str` or nan-like (`31204`). <span class="title-ref">StringDtype</span> now works in all situations where `astype(str)` or `dtype=str` work:

For example, the below now works:

<div class="ipython">

python

ser = pd.Series(\[1, "abc", np.nan\], dtype="string") ser ser\[0\] pd.Series(\[1, 2, np.nan\], dtype="Int64").astype("string")

</div>

### Non-monotonic PeriodIndex partial string slicing

<span class="title-ref">PeriodIndex</span> now supports partial string slicing for non-monotonic indexes, mirroring <span class="title-ref">DatetimeIndex</span> behavior (`31096`)

For example:

<div class="ipython">

python

dti = pd.date\_range("2014-01-01", periods=30, freq="30D") pi = dti.to\_period("D") ser\_monotonic = pd.Series(np.arange(30), index=pi) shuffler = list(range(0, 30, 2)) + list(range(1, 31, 2)) ser = ser\_monotonic.iloc\[shuffler\] ser

</div>

<div class="ipython">

python

ser\["2014"\] ser.loc\["May 2015"\]

</div>

### Comparing two `DataFrame` or two `Series` and summarizing the differences

We've added <span class="title-ref">DataFrame.compare</span> and <span class="title-ref">Series.compare</span> for comparing two `DataFrame` or two `Series` (`30429`)

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "col1": \["a", "a", "b", "b", "a"\], "col2": \[1.0, 2.0, 3.0, np.nan, 5.0\], "col3": \[1.0, 2.0, 3.0, 4.0, 5.0\]
    
    }, columns=\["col1", "col2", "col3"\],

) df

</div>

<div class="ipython">

python

df2 = df.copy() df2.loc\[0, 'col1'\] = 'c' df2.loc\[2, 'col3'\] = 4.0 df2

</div>

<div class="ipython">

python

df.compare(df2)

</div>

See \[User Guide \<merging.compare\>\](\#user-guide-\<merging.compare\>) for more details.

### Allow NA in groupby key

With \[groupby \<groupby.dropna\>\](\#groupby-\<groupby.dropna\>) , we've added a `dropna` keyword to <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> in order to allow `NA` values in group keys. Users can define `dropna` to `False` if they want to include `NA` values in groupby keys. The default is set to `True` for `dropna` to keep backwards compatibility (`3729`)

<div class="ipython">

python

df\_list = \[\[1, 2, 3\], \[1, None, 4\], \[2, 1, 3\], \[1, 2, 2\]\] df\_dropna = pd.DataFrame(df\_list, columns=\["a", "b", "c"\])

df\_dropna

</div>

<div class="ipython">

python

\# Default `dropna` is set to True, which will exclude NaNs in keys df\_dropna.groupby(by=\["b"\], dropna=True).sum()

\# In order to allow NaN in keys, set `dropna` to False df\_dropna.groupby(by=\["b"\], dropna=False).sum()

</div>

The default setting of `dropna` argument is `True` which means `NA` are not included in group keys.

### Sorting with keys

We've added a `key` argument to the <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> sorting methods, including <span class="title-ref">DataFrame.sort\_values</span>, <span class="title-ref">DataFrame.sort\_index</span>, <span class="title-ref">Series.sort\_values</span>, and <span class="title-ref">Series.sort\_index</span>. The `key` can be any callable function which is applied column-by-column to each column used for sorting, before sorting is performed (`27237`). See \[sort\_values with keys \<basics.sort\_value\_key\>\](\#sort\_values-with-keys-\<basics.sort\_value\_key\>) and \[sort\_index with keys \<basics.sort\_index\_key\>\](\#sort\_index-with-keys \<basics.sort\_index\_key\>) for more information.

<div class="ipython">

python

s = pd.Series(\['C', 'a', 'B'\]) s

</div>

<div class="ipython">

python

s.sort\_values()

</div>

Note how this is sorted with capital letters first. If we apply the <span class="title-ref">Series.str.lower</span> method, we get

<div class="ipython">

python

s.sort\_values(key=lambda x: x.str.lower())

</div>

When applied to a `DataFrame`, they key is applied per-column to all columns or a subset if `by` is specified, e.g.

<div class="ipython">

python

  - df = pd.DataFrame({'a': \['C', 'C', 'a', 'a', 'B', 'B'\],  
    'b': \[1, 2, 3, 4, 5, 6\]})

df

</div>

<div class="ipython">

python

df.sort\_values(by=\['a'\], key=lambda col: col.str.lower())

</div>

For more details, see examples and documentation in <span class="title-ref">DataFrame.sort\_values</span>, <span class="title-ref">Series.sort\_values</span>, and <span class="title-ref">\~DataFrame.sort\_index</span>.

### Fold argument support in Timestamp constructor

<span class="title-ref">Timestamp:</span> now supports the keyword-only fold argument according to [PEP 495](https://www.python.org/dev/peps/pep-0495/#the-fold-attribute) similar to parent `datetime.datetime` class. It supports both accepting fold as an initialization argument and inferring fold from other constructor arguments (`25057`, `31338`). Support is limited to `dateutil` timezones as `pytz` doesn't support fold.

For example:

<div class="ipython">

python

ts = pd.Timestamp("2019-10-27 01:30:00+00:00") ts.fold

</div>

<div class="ipython">

python

  - ts = pd.Timestamp(year=2019, month=10, day=27, hour=1, minute=30,  
    tz="dateutil/Europe/London", fold=1)

ts

</div>

For more on working with fold, see \[Fold subsection \<timeseries.fold\>\](\#fold-subsection-\<timeseries.fold\>) in the user guide.

### Parsing timezone-aware format with different timezones in to\_datetime

<span class="title-ref">to\_datetime</span> now supports parsing formats containing timezone names (`%Z`) and UTC offsets (`%z`) from different timezones then converting them to UTC by setting `utc=True`. This would return a <span class="title-ref">DatetimeIndex</span> with timezone at UTC as opposed to an <span class="title-ref">Index</span> with `object` dtype if `utc=True` is not set (`32792`).

For example:

<div class="ipython">

python

  - tz\_strs = \["2010-01-01 12:00:00 +0100", "2010-01-01 12:00:00 -0100",  
    "2010-01-01 12:00:00 +0300", "2010-01-01 12:00:00 +0400"\]

pd.to\_datetime(tz\_strs, format='%Y-%m-%d %H:%M:%S %z', utc=True)

</div>

`` `ipython    In[37]: pd.to_datetime(tz_strs, format='%Y-%m-%d %H:%M:%S %z')    Out[37]:    Index([2010-01-01 12:00:00+01:00, 2010-01-01 12:00:00-01:00,           2010-01-01 12:00:00+03:00, 2010-01-01 12:00:00+04:00],          dtype='object')  .. _whatsnew_110.grouper_resample_origin:  Grouper and resample now supports the arguments origin and offset ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">Grouper</span> and <span class="title-ref">DataFrame.resample</span> now supports the arguments `origin` and `offset`. It let the user control the timestamp on which to adjust the grouping. (`31809`)

The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like `30D`) or that divides a day (like `90s` or `1min`). But it can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can now specify a fixed timestamp with the argument `origin`.

Two arguments are now deprecated (more information in the documentation of <span class="title-ref">DataFrame.resample</span>):

  - `base` should be replaced by `offset`.
  - `loffset` should be replaced by directly adding an offset to the index <span class="title-ref">DataFrame</span> after being resampled.

Small example of the use of `origin`:

<div class="ipython">

python

start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00' middle = '2000-10-02 00:00:00' rng = pd.date\_range(start, end, freq='7min') ts = pd.Series(np.arange(len(rng)) \* 3, index=rng) ts

</div>

Resample with the default behavior `'start_day'` (origin is `2000-10-01 00:00:00`):

<div class="ipython">

python

ts.resample('17min').sum() ts.resample('17min', origin='start\_day').sum()

</div>

Resample using a fixed origin:

<div class="ipython">

python

ts.resample('17min', origin='epoch').sum() ts.resample('17min', origin='2000-01-01').sum()

</div>

If needed you can adjust the bins with the argument `offset` (a <span class="title-ref">Timedelta</span>) that would be added to the default `origin`.

For a full example, see: \[timeseries.adjust-the-start-of-the-bins\](\#timeseries.adjust-the-start-of-the-bins).

### fsspec now used for filesystem handling

For reading and writing to filesystems other than local and reading from HTTP(S), the optional dependency `fsspec` will be used to dispatch operations (`33452`). This will give unchanged functionality for S3 and GCS storage, which were already supported, but also add support for several other storage implementations such as [Azure Datalake and Blob](https://github.com/fsspec/adlfs), SSH, FTP, dropbox and github. For docs and capabilities, see the [fsspec docs](https://filesystem-spec.readthedocs.io/en/latest/).

The existing capability to interface with S3 and GCS will be unaffected by this change, as `fsspec` will still bring in the same packages as before.

### Other enhancements

  - Compatibility with matplotlib 3.3.0 (`34850`)
  - <span class="title-ref">IntegerArray.astype</span> now supports `datetime64` dtype (`32538`)
  - <span class="title-ref">IntegerArray</span> now implements the `sum` operation (`33172`)
  - Added <span class="title-ref">pandas.errors.InvalidIndexError</span> (`34570`).
  - Added <span class="title-ref">DataFrame.value\_counts</span> (`5377`)
  - Added a <span class="title-ref">pandas.api.indexers.FixedForwardWindowIndexer</span> class to support forward-looking windows during `rolling` operations.
  - Added a <span class="title-ref">pandas.api.indexers.VariableOffsetWindowIndexer</span> class to support `rolling` operations with non-fixed offsets (`34994`)
  - <span class="title-ref">\~DataFrame.describe</span> now includes a `datetime_is_numeric` keyword to control how datetime columns are summarized (`30164`, `34798`)
  - <span class="title-ref">\~pandas.io.formats.style.Styler</span> may now render CSS more efficiently where multiple cells have the same styling (`30876`)
  - <span class="title-ref">\~pandas.io.formats.style.Styler.highlight\_null</span> now accepts `subset` argument (`31345`)
  - When writing directly to a sqlite connection <span class="title-ref">DataFrame.to\_sql</span> now supports the `multi` method (`29921`)
  - <span class="title-ref">pandas.errors.OptionError</span> is now exposed in `pandas.errors` (`27553`)
  - Added <span class="title-ref">api.extensions.ExtensionArray.argmax</span> and <span class="title-ref">api.extensions.ExtensionArray.argmin</span> (`24382`)
  - <span class="title-ref">timedelta\_range</span> will now infer a frequency when passed `start`, `stop`, and `periods` (`32377`)
  - Positional slicing on a <span class="title-ref">IntervalIndex</span> now supports slices with `step > 1` (`31658`)
  - <span class="title-ref">Series.str</span> now has a `fullmatch` method that matches a regular expression against the entire string in each row of the <span class="title-ref">Series</span>, similar to `re.fullmatch` (`32806`).
  - <span class="title-ref">DataFrame.sample</span> will now also allow array-like and BitGenerator objects to be passed to `random_state` as seeds (`32503`)
  - <span class="title-ref">Index.union</span> will now raise `RuntimeWarning` for <span class="title-ref">MultiIndex</span> objects if the object inside are unsortable. Pass `sort=False` to suppress this warning (`33015`)
  - Added <span class="title-ref">Series.dt.isocalendar</span> and <span class="title-ref">DatetimeIndex.isocalendar</span> that returns a <span class="title-ref">DataFrame</span> with year, week, and day calculated according to the ISO 8601 calendar (`33206`, `34392`).
  - The <span class="title-ref">DataFrame.to\_feather</span> method now supports additional keyword arguments (e.g. to set the compression) that are added in pyarrow 0.17 (`33422`).
  - The <span class="title-ref">cut</span> will now accept parameter `ordered` with default `ordered=True`. If `ordered=False` and no labels are provided, an error will be raised (`33141`)
  - <span class="title-ref">DataFrame.to\_csv</span>, <span class="title-ref">DataFrame.to\_pickle</span>, and <span class="title-ref">DataFrame.to\_json</span> now support passing a dict of compression arguments when using the `gzip` and `bz2` protocols. This can be used to set a custom compression level, e.g., `df.to_csv(path, compression={'method': 'gzip', 'compresslevel': 1}` (`33196`)
  - <span class="title-ref">melt</span> has gained an `ignore_index` (default `True`) argument that, if set to `False`, prevents the method from dropping the index (`17440`).
  - <span class="title-ref">Series.update</span> now accepts objects that can be coerced to a <span class="title-ref">Series</span>, such as `dict` and `list`, mirroring the behavior of <span class="title-ref">DataFrame.update</span> (`33215`)
  - <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.DataFrameGroupBy.aggregate</span> have gained `engine` and `engine_kwargs` arguments that support executing functions with `Numba` (`32854`, `33388`)
  - <span class="title-ref">.Resampler.interpolate</span> now supports SciPy interpolation method <span class="title-ref">scipy.interpolate.CubicSpline</span> as method `cubicspline` (`33670`)
  - <span class="title-ref">.DataFrameGroupBy</span> and <span class="title-ref">.SeriesGroupBy</span> now implement the `sample` method for doing random sampling within groups (`31775`)
  - <span class="title-ref">DataFrame.to\_numpy</span> now supports the `na_value` keyword to control the NA sentinel in the output array (`33820`)
  - Added <span class="title-ref">api.extension.ExtensionArray.equals</span> to the extension array interface, similar to <span class="title-ref">Series.equals</span> (`27081`)
  - The minimum supported dta version has increased to 105 in <span class="title-ref">read\_stata</span> and <span class="title-ref">\~pandas.io.stata.StataReader</span> (`26667`).
  - <span class="title-ref">\~DataFrame.to\_stata</span> supports compression using the `compression` keyword argument. Compression can either be inferred or explicitly set using a string or a dictionary containing both the method and any additional arguments that are passed to the compression library. Compression was also added to the low-level Stata-file writers <span class="title-ref">\~pandas.io.stata.StataWriter</span>, <span class="title-ref">\~pandas.io.stata.StataWriter117</span>, and <span class="title-ref">\~pandas.io.stata.StataWriterUTF8</span> (`26599`).
  - <span class="title-ref">HDFStore.put</span> now accepts a `track_times` parameter. This parameter is passed to the `create_table` method of `PyTables` (`32682`).
  - <span class="title-ref">Series.plot</span> and <span class="title-ref">DataFrame.plot</span> now accepts `xlabel` and `ylabel` parameters to present labels on x and y axis (`9093`).
  - Made <span class="title-ref">.Rolling</span> and <span class="title-ref">.Expanding</span> iterableï¼ˆ`11704`)
  - Made `option_context` a <span class="title-ref">contextlib.ContextDecorator</span>, which allows it to be used as a decorator over an entire function (`34253`).
  - <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span> now accept an `errors` argument (`22610`)
  - <span class="title-ref">.DataFrameGroupBy.groupby.transform</span> now allows `func` to be `pad`, `backfill` and `cumcount` (`31269`).
  - <span class="title-ref">read\_json</span> now accepts an `nrows` parameter. (`33916`).
  - <span class="title-ref">DataFrame.hist</span>, <span class="title-ref">Series.hist</span>, <span class="title-ref">core.groupby.DataFrameGroupBy.hist</span>, and <span class="title-ref">core.groupby.SeriesGroupBy.hist</span> have gained the `legend` argument. Set to True to show a legend in the histogram. (`6279`)
  - <span class="title-ref">concat</span> and <span class="title-ref">\~DataFrame.append</span> now preserve extension dtypes, for example combining a nullable integer column with a numpy integer column will no longer result in object dtype but preserve the integer dtype (`33607`, `34339`, `34095`).
  - <span class="title-ref">read\_gbq</span> now allows to disable progress bar (`33360`).
  - <span class="title-ref">read\_gbq</span> now supports the `max_results` kwarg from `pandas-gbq` (`34639`).
  - <span class="title-ref">DataFrame.cov</span> and <span class="title-ref">Series.cov</span> now support a new parameter `ddof` to support delta degrees of freedom as in the corresponding numpy methods (`34611`).
  - <span class="title-ref">DataFrame.to\_html</span> and <span class="title-ref">DataFrame.to\_string</span>'s `col_space` parameter now accepts a list or dict to change only some specific columns' width (`28917`).
  - <span class="title-ref">DataFrame.to\_excel</span> can now also write OpenOffice spreadsheet (.ods) files (`27222`)
  - <span class="title-ref">\~Series.explode</span> now accepts `ignore_index` to reset the index, similar to <span class="title-ref">pd.concat</span> or <span class="title-ref">DataFrame.sort\_values</span> (`34932`).
  - <span class="title-ref">DataFrame.to\_markdown</span> and <span class="title-ref">Series.to\_markdown</span> now accept `index` argument as an alias for tabulate's `showindex` (`32667`)
  - <span class="title-ref">read\_csv</span> now accepts string values like "0", "0.0", "1", "1.0" as convertible to the nullable Boolean dtype (`34859`)
  - <span class="title-ref">.ExponentialMovingWindow</span> now supports a `times` argument that allows `mean` to be calculated with observations spaced by the timestamps in `times` (`34839`)
  - <span class="title-ref">DataFrame.agg</span> and <span class="title-ref">Series.agg</span> now accept named aggregation for renaming the output columns/indexes. (`26513`)
  - `compute.use_numba` now exists as a configuration option that utilizes the numba engine when available (`33966`, `35374`)
  - <span class="title-ref">Series.plot</span> now supports asymmetric error bars. Previously, if <span class="title-ref">Series.plot</span> received a "2xN" array with error values for `yerr` and/or `xerr`, the left/lower values (first row) were mirrored, while the right/upper values (second row) were ignored. Now, the first row represents the left/lower error values and the second row the right/upper error values. (`9536`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### `MultiIndex.get_indexer` interprets `method` argument correctly

This restores the behavior of <span class="title-ref">MultiIndex.get\_indexer</span> with `method='backfill'` or `method='pad'` to the behavior before pandas 0.23.0. In particular, MultiIndexes are treated as a list of tuples and padding or backfilling is done with respect to the ordering of these lists of tuples (`29896`).

As an example of this, given:

<div class="ipython">

python

  - df = pd.DataFrame({  
    'a': \[0, 0, 0, 0\], 'b': \[0, 2, 3, 4\], 'c': \['A', 'B', 'C', 'D'\],

}).set\_index(\['a', 'b'\]) mi\_2 = pd.MultiIndex.from\_product(\[\[0\], \[-1, 0, 1, 3, 4, 5\]\])

</div>

The differences in reindexing `df` with `mi_2` and using `method='backfill'` can be seen here:

*pandas \>= 0.23, \< 1.1.0*:

`` `ipython     In [1]: df.reindex(mi_2, method='backfill')     Out[1]:           c     0 -1  A        0  A        1  D        3  A        4  A        5  C  *pandas <0.23, >= 1.1.0*  .. ipython:: python          df.reindex(mi_2, method='backfill')  And the differences in reindexing ``df`with`mi\_2`and using`method='pad'`can be seen here:  *pandas >= 0.23, < 1.1.0*  .. code-block:: ipython      In [1]: df.reindex(mi_2, method='pad')     Out[1]:             c     0 -1  NaN        0  NaN        1    D        3  NaN        4    A        5    C  *pandas < 0.23, >= 1.1.0*  .. ipython:: python          df.reindex(mi_2, method='pad')  .. _whatsnew_110.notable_bug_fixes.indexing_raises_key_errors:  Failed label-based lookups always raise KeyError`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Label lookups `series[key]`, `series.loc[key]` and `frame.loc[key]` used to raise either `KeyError` or `TypeError` depending on the type of key and type of <span class="title-ref">Index</span>. These now consistently raise `KeyError` (`31867`)

<div class="ipython">

python

ser1 = pd.Series(range(3), index=\[0, 1, 2\]) ser2 = pd.Series(range(3), index=pd.date\_range("2020-02-01", periods=3))

</div>

*Previous behavior*:

`` `ipython     In [3]: ser1[1.5]     ...     TypeError: cannot do label indexing on Int64Index with these indexers [1.5] of type float      In [4] ser1["foo"]     ...     KeyError: 'foo'      In [5]: ser1.loc[1.5]     ...     TypeError: cannot do label indexing on Int64Index with these indexers [1.5] of type float      In [6]: ser1.loc["foo"]     ...     KeyError: 'foo'      In [7]: ser2.loc[1]     ...     TypeError: cannot do label indexing on DatetimeIndex with these indexers [1] of type int      In [8]: ser2.loc[pd.Timestamp(0)]     ...     KeyError: Timestamp('1970-01-01 00:00:00')  *New behavior*:  .. code-block:: ipython      In [3]: ser1[1.5]     ...     KeyError: 1.5      In [4] ser1["foo"]     ...     KeyError: 'foo'      In [5]: ser1.loc[1.5]     ...     KeyError: 1.5      In [6]: ser1.loc["foo"]     ...     KeyError: 'foo'      In [7]: ser2.loc[1]     ...     KeyError: 1      In [8]: ser2.loc[pd.Timestamp(0)]     ...     KeyError: Timestamp('1970-01-01 00:00:00')   Similarly, `DataFrame.at` and `Series.at` will raise a ``TypeError`instead of a`ValueError`if an incompatible key is passed, and`KeyError`if a missing key is passed, matching the behavior of`.loc\[\]``(:issue:`31722`)  .. _whatsnew_110.notable_bug_fixes.indexing_int_multiindex_raises_key_errors:  Failed Integer Lookups on MultiIndex Raise KeyError``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Indexing with integers with a \`MultiIndex</span> that has an integer-dtype first level incorrectly failed to raise `KeyError` when one or more of those integer keys is not present in the first level of the index (`33539`)

<div class="ipython">

python

idx = pd.Index(range(4)) dti = pd.date\_range("2000-01-03", periods=3) mi = pd.MultiIndex.from\_product(\[idx, dti\]) ser = pd.Series(range(len(mi)), index=mi)

</div>

*Previous behavior*:

`` `ipython     In [5]: ser[[5]]     Out[5]: Series([], dtype: int64)  *New behavior*:  .. code-block:: ipython      In [5]: ser[[5]]     ...     KeyError: '[5] not in index'  `DataFrame.merge` preserves right frame's row order ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \`DataFrame.merge</span> now preserves the right frame's row order when executing a right merge (`27453`)

<div class="ipython">

python

  - left\_df = pd.DataFrame({'animal': \['dog', 'pig'\],  
    'max\_speed': \[40, 11\]})

  - right\_df = pd.DataFrame({'animal': \['quetzal', 'pig'\],  
    'max\_speed': \[80, 11\]})

left\_df right\_df

</div>

*Previous behavior*:

`` `python     >>> left_df.merge(right_df, on=['animal', 'max_speed'], how="right")         animal  max_speed     0      pig         11     1  quetzal         80  *New behavior*:  .. ipython:: python      left_df.merge(right_df, on=['animal', 'max_speed'], how="right")  .. ---------------------------------------------------------------------------  .. _whatsnew_110.notable_bug_fixes.assignment_to_multiple_columns:  Assignment to multiple columns of a DataFrame when some columns do not exist ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Assignment to multiple columns of a <span class="title-ref">DataFrame</span> when some of the columns do not exist would previously assign the values to the last column. Now, new columns will be constructed with the right values. (`13658`)

<div class="ipython">

python

df = pd.DataFrame({'a': \[0, 1, 2\], 'b': \[3, 4, 5\]}) df

</div>

*Previous behavior*:

`` `ipython    In [3]: df[['a', 'c']] = 1    In [4]: df    Out[4]:       a  b    0  1  1    1  1  1    2  1  1  *New behavior*:  .. ipython:: python     df[['a', 'c']] = 1    df  .. _whatsnew_110.notable_bug_fixes.groupby_consistency:  Consistency across groupby reductions ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using <span class="title-ref">DataFrame.groupby</span> with `as_index=True` and the aggregation `nunique` would include the grouping column(s) in the columns of the result. Now the grouping column(s) only appear in the index, consistent with other reductions. (`32579`)

<div class="ipython">

python

df = pd.DataFrame({"a": \["x", "x", "y", "y"\], "b": \[1, 1, 2, 3\]}) df

</div>

*Previous behavior*:

`` `ipython    In [3]: df.groupby("a", as_index=True).nunique()    Out[4]:       a  b    a    x  1  1    y  1  2  *New behavior*:  .. ipython:: python     df.groupby("a", as_index=True).nunique()  Using `DataFrame.groupby` with ``as\_index=False`and the function`idxmax`,`idxmin`,`mad`,`nunique`,`sem`,`skew`, or`std``would modify the grouping column. Now the grouping column remains unchanged, consistent with other reductions. (:issue:`21090`, :issue:`10355`)  *Previous behavior*:  .. code-block:: ipython     In [3]: df.groupby("a", as_index=False).nunique()    Out[4]:       a  b    0  1  1    1  1  2  *New behavior*:  .. ipython:: python     df.groupby("a", as_index=False).nunique()  The method `.DataFrameGroupBy.size` would previously ignore``as\_index=False``. Now the grouping columns are returned as columns, making the result a `DataFrame` instead of a `Series`. (:issue:`32599`)  *Previous behavior*:  .. code-block:: ipython     In [3]: df.groupby("a", as_index=False).size()    Out[4]:    a    x    2    y    2    dtype: int64  *New behavior*:  .. ipython:: python     df.groupby("a", as_index=False).size()  .. _whatsnew_110.api_breaking.groupby_results_lost_as_index_false:  `.DataFrameGroupby.agg` lost results with``as\_index=False`when relabeling columns`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously <span class="title-ref">.DataFrameGroupby.agg</span> lost the result columns, when the `as_index` option was set to `False` and the result columns were relabeled. In this case the result values were replaced with the previous index (`32240`).

<div class="ipython">

python

  - df = pd.DataFrame({"key": \["x", "y", "z", "x", "y", "z"\],  
    "val": \[1.0, 0.8, 2.0, 3.0, 3.6, 0.75\]})

df

</div>

*Previous behavior*:

`` `ipython    In [2]: grouped = df.groupby("key", as_index=False)    In [3]: result = grouped.agg(min_val=pd.NamedAgg(column="val", aggfunc="min"))    In [4]: result    Out[4]:         min_val     0   x     1   y     2   z  *New behavior*:  .. ipython:: python     grouped = df.groupby("key", as_index=False)    result = grouped.agg(min_val=pd.NamedAgg(column="val", aggfunc="min"))    result   .. _whatsnew_110.notable_bug_fixes.apply_applymap_first_once:  apply and applymap on ``DataFrame`evaluates first row/column only once`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<div class="ipython">

python

df = pd.DataFrame({'a': \[1, 2\], 'b': \[3, 6\]})

  - def func(row):  
    print(row) return row

</div>

*Previous behavior*:

`` `ipython     In [4]: df.apply(func, axis=1)     a    1     b    3     Name: 0, dtype: int64     a    1     b    3     Name: 0, dtype: int64     a    2     b    6     Name: 1, dtype: int64     Out[4]:        a  b     0  1  3     1  2  6  *New behavior*:  .. ipython:: python      df.apply(func, axis=1)  .. _whatsnew_110.api_breaking:  Backwards incompatible API changes ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

### Added `check_freq` argument to `testing.assert_frame_equal` and `testing.assert_series_equal`

The `check_freq` argument was added to <span class="title-ref">testing.assert\_frame\_equal</span> and <span class="title-ref">testing.assert\_series\_equal</span> in pandas 1.1.0 and defaults to `True`. <span class="title-ref">testing.assert\_frame\_equal</span> and <span class="title-ref">testing.assert\_series\_equal</span> now raise `AssertionError` if the indexes do not have the same frequency. Before pandas 1.1.0, the index frequency was not checked.

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated (`33718`, `29766`, `29723`, pytables \>= 3.4.3). If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.15.4</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytz</td>
<td>2015.4</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.7.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.2.1</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.6.2</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>4.0.2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.6.0</td>
<td></td>
</tr>
<tr class="even">
<td>fastparquet</td>
<td>0.3.2</td>
<td></td>
</tr>
<tr class="odd">
<td>fsspec</td>
<td>0.7.4</td>
<td></td>
</tr>
<tr class="even">
<td>gcsfs</td>
<td>0.6.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>lxml</td>
<td>3.8.0</td>
<td></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>2.2.2</td>
<td></td>
</tr>
<tr class="odd">
<td>numba</td>
<td>0.46.0</td>
<td></td>
</tr>
<tr class="even">
<td>openpyxl</td>
<td>2.5.7</td>
<td></td>
</tr>
<tr class="odd">
<td>pyarrow</td>
<td>0.13.0</td>
<td></td>
</tr>
<tr class="even">
<td>pymysql</td>
<td>0.7.1</td>
<td></td>
</tr>
<tr class="odd">
<td>pytables</td>
<td>3.4.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>s3fs</td>
<td>0.4.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>scipy</td>
<td>1.2.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>sqlalchemy</td>
<td>1.1.4</td>
<td></td>
</tr>
<tr class="odd">
<td>xarray</td>
<td>0.8.2</td>
<td></td>
</tr>
<tr class="even">
<td>xlrd</td>
<td>1.1.0</td>
<td></td>
</tr>
<tr class="odd">
<td>xlsxwriter</td>
<td>0.9.8</td>
<td></td>
</tr>
<tr class="even">
<td>xlwt</td>
<td>1.2.0</td>
<td></td>
</tr>
<tr class="odd">
<td>pandas-gbq</td>
<td>1.2.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Development changes

  - The minimum version of Cython is now the most recent bug-fix version (0.29.16) (`33334`).

## Deprecations

  - Lookups on a <span class="title-ref">Series</span> with a single-item list containing a slice (e.g. `ser[[slice(0, 4)]]`) are deprecated and will raise in a future version. Either convert the list to a tuple, or pass the slice directly instead (`31333`)
  - <span class="title-ref">DataFrame.mean</span> and <span class="title-ref">DataFrame.median</span> with `numeric_only=None` will include `datetime64` and `datetime64tz` columns in a future version (`29941`)
  - Setting values with `.loc` using a positional slice is deprecated and will raise in a future version. Use `.loc` with labels or `.iloc` with positions instead (`31840`)
  - <span class="title-ref">DataFrame.to\_dict</span> has deprecated accepting short names for `orient` and will raise in a future version (`32515`)
  - <span class="title-ref">Categorical.to\_dense</span> is deprecated and will be removed in a future version, use `np.asarray(cat)` instead (`32639`)
  - The `fastpath` keyword in the `SingleBlockManager` constructor is deprecated and will be removed in a future version (`33092`)
  - Providing `suffixes` as a `set` in <span class="title-ref">pandas.merge</span> is deprecated. Provide a tuple instead (`33740`, `34741`).
  - Indexing a <span class="title-ref">Series</span> with a multi-dimensional indexer like `[:, None]` to return an `ndarray` now raises a `FutureWarning`. Convert to a NumPy array before indexing instead (`27837`)
  - <span class="title-ref">Index.is\_mixed</span> is deprecated and will be removed in a future version, check `index.inferred_type` directly instead (`32922`)
  - Passing any arguments but the first one to <span class="title-ref">read\_html</span> as positional arguments is deprecated. All other arguments should be given as keyword arguments (`27573`).
  - Passing any arguments but `path_or_buf` (the first one) to <span class="title-ref">read\_json</span> as positional arguments is deprecated. All other arguments should be given as keyword arguments (`27573`).
  - Passing any arguments but the first two to <span class="title-ref">read\_excel</span> as positional arguments is deprecated. All other arguments should be given as keyword arguments (`27573`).
  - <span class="title-ref">pandas.api.types.is\_categorical</span> is deprecated and will be removed in a future version; use <span class="title-ref">pandas.api.types.is\_categorical\_dtype</span> instead (`33385`)
  - <span class="title-ref">Index.get\_value</span> is deprecated and will be removed in a future version (`19728`)
  - <span class="title-ref">Series.dt.week</span> and <span class="title-ref">Series.dt.weekofyear</span> are deprecated and will be removed in a future version, use <span class="title-ref">Series.dt.isocalendar().week</span> instead (`33595`)
  - <span class="title-ref">DatetimeIndex.week</span> and `DatetimeIndex.weekofyear` are deprecated and will be removed in a future version, use `DatetimeIndex.isocalendar().week` instead (`33595`)
  - <span class="title-ref">DatetimeArray.week</span> and `DatetimeArray.weekofyear` are deprecated and will be removed in a future version, use `DatetimeArray.isocalendar().week` instead (`33595`)
  - <span class="title-ref">DateOffset.\_\_call\_\_</span> is deprecated and will be removed in a future version, use `offset + other` instead (`34171`)
  - <span class="title-ref">\~pandas.tseries.offsets.BusinessDay.apply\_index</span> is deprecated and will be removed in a future version. Use `offset + other` instead (`34580`)
  - <span class="title-ref">DataFrame.tshift</span> and <span class="title-ref">Series.tshift</span> are deprecated and will be removed in a future version, use <span class="title-ref">DataFrame.shift</span> and <span class="title-ref">Series.shift</span> instead (`11631`)
  - Indexing an <span class="title-ref">Index</span> object with a float key is deprecated, and will raise an `IndexError` in the future. You can manually convert to an integer key instead (`34191`).
  - The `squeeze` keyword in <span class="title-ref">\~DataFrame.groupby</span> is deprecated and will be removed in a future version (`32380`)
  - The `tz` keyword in <span class="title-ref">Period.to\_timestamp</span> is deprecated and will be removed in a future version; use `per.to_timestamp(...).tz_localize(tz)` instead (`34522`)
  - <span class="title-ref">DatetimeIndex.to\_perioddelta</span> is deprecated and will be removed in a future version. Use `index - index.to_period(freq).to_timestamp()` instead (`34853`)
  - <span class="title-ref">DataFrame.melt</span> accepting a `value_name` that already exists is deprecated, and will be removed in a future version (`34731`)
  - The `center` keyword in the <span class="title-ref">DataFrame.expanding</span> function is deprecated and will be removed in a future version (`20647`)

## Performance improvements

  - Performance improvement in <span class="title-ref">Timedelta</span> constructor (`30543`)
  - Performance improvement in <span class="title-ref">Timestamp</span> constructor (`30543`)
  - Performance improvement in flex arithmetic ops between <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> with `axis=0` (`31296`)
  - Performance improvement in arithmetic ops between <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> with `axis=1` (`33600`)
  - The internal index method <span class="title-ref">\~Index.\_shallow\_copy</span> now copies cached attributes over to the new index, avoiding creating these again on the new index. This can speed up many operations that depend on creating copies of existing indexes (`28584`, `32640`, `32669`)
  - Significant performance improvement when creating a <span class="title-ref">DataFrame</span> with sparse values from `scipy.sparse` matrices using the <span class="title-ref">DataFrame.sparse.from\_spmatrix</span> constructor (`32821`, `32825`, `32826`, `32856`, `32858`).
  - Performance improvement for groupby methods <span class="title-ref">.Groupby.first</span> and <span class="title-ref">.Groupby.last</span> (`34178`)
  - Performance improvement in <span class="title-ref">factorize</span> for nullable (integer and Boolean) dtypes (`33064`).
  - Performance improvement when constructing <span class="title-ref">Categorical</span> objects (`33921`)
  - Fixed performance regression in <span class="title-ref">pandas.qcut</span> and <span class="title-ref">pandas.cut</span> (`33921`)
  - Performance improvement in reductions (`sum`, `prod`, `min`, `max`) for nullable (integer and Boolean) dtypes (`30982`, `33261`, `33442`).
  - Performance improvement in arithmetic operations between two <span class="title-ref">DataFrame</span> objects (`32779`)
  - Performance improvement in <span class="title-ref">.RollingGroupby</span> (`34052`)
  - Performance improvement in arithmetic operations (`sub`, `add`, `mul`, `div`) for <span class="title-ref">MultiIndex</span> (`34297`)
  - Performance improvement in `DataFrame[bool_indexer]` when `bool_indexer` is a `list` (`33924`)
  - Significant performance improvement of <span class="title-ref">io.formats.style.Styler.render</span> with styles added with various ways such as <span class="title-ref">io.formats.style.Styler.apply</span>, <span class="title-ref">io.formats.style.Styler.applymap</span> or <span class="title-ref">io.formats.style.Styler.bar</span> (`19917`)

## Bug fixes

### Categorical

  - Passing an invalid `fill_value` to <span class="title-ref">Categorical.take</span> raises a `ValueError` instead of `TypeError` (`33660`)
  - Combining a <span class="title-ref">Categorical</span> with integer categories and which contains missing values with a float dtype column in operations such as <span class="title-ref">concat</span> or <span class="title-ref">\~DataFrame.append</span> will now result in a float column instead of an object dtype column (`33607`)
  - Bug where <span class="title-ref">merge</span> was unable to join on non-unique categorical indices (`28189`)
  - Bug when passing categorical data to <span class="title-ref">Index</span> constructor along with `dtype=object` incorrectly returning a <span class="title-ref">CategoricalIndex</span> instead of object-dtype <span class="title-ref">Index</span> (`32167`)
  - Bug where <span class="title-ref">Categorical</span> comparison operator `__ne__` would incorrectly evaluate to `False` when either element was missing (`32276`)
  - <span class="title-ref">Categorical.fillna</span> now accepts <span class="title-ref">Categorical</span> `other` argument (`32420`)
  - Repr of <span class="title-ref">Categorical</span> was not distinguishing between `int` and `str` (`33676`)

### Datetimelike

  - Passing an integer dtype other than `int64` to `np.array(period_index, dtype=...)` will now raise `TypeError` instead of incorrectly using `int64` (`32255`)
  - <span class="title-ref">Series.to\_timestamp</span> now raises a `TypeError` if the axis is not a <span class="title-ref">PeriodIndex</span>. Previously an `AttributeError` was raised (`33327`)
  - <span class="title-ref">Series.to\_period</span> now raises a `TypeError` if the axis is not a <span class="title-ref">DatetimeIndex</span>. Previously an `AttributeError` was raised (`33327`)
  - <span class="title-ref">Period</span> no longer accepts tuples for the `freq` argument (`34658`)
  - Bug in <span class="title-ref">Timestamp</span> where constructing a <span class="title-ref">Timestamp</span> from ambiguous epoch time and calling constructor again changed the <span class="title-ref">Timestamp.value</span> property (`24329`)
  - <span class="title-ref">DatetimeArray.searchsorted</span>, <span class="title-ref">TimedeltaArray.searchsorted</span>, <span class="title-ref">PeriodArray.searchsorted</span> not recognizing non-pandas scalars and incorrectly raising `ValueError` instead of `TypeError` (`30950`)
  - Bug in <span class="title-ref">Timestamp</span> where constructing <span class="title-ref">Timestamp</span> with dateutil timezone less than 128 nanoseconds before daylight saving time switch from winter to summer would result in nonexistent time (`31043`)
  - Bug in <span class="title-ref">Period.to\_timestamp</span>, <span class="title-ref">Period.start\_time</span> with microsecond frequency returning a timestamp one nanosecond earlier than the correct time (`31475`)
  - <span class="title-ref">Timestamp</span> raised a confusing error message when year, month or day is missing (`31200`)
  - Bug in <span class="title-ref">DatetimeIndex</span> constructor incorrectly accepting `bool`-dtype inputs (`32668`)
  - Bug in <span class="title-ref">DatetimeIndex.searchsorted</span> not accepting a `list` or <span class="title-ref">Series</span> as its argument (`32762`)
  - Bug where <span class="title-ref">PeriodIndex</span> raised when passed a <span class="title-ref">Series</span> of strings (`26109`)
  - Bug in <span class="title-ref">Timestamp</span> arithmetic when adding or subtracting an `np.ndarray` with `timedelta64` dtype (`33296`)
  - Bug in <span class="title-ref">DatetimeIndex.to\_period</span> not inferring the frequency when called with no arguments (`33358`)
  - Bug in <span class="title-ref">DatetimeIndex.tz\_localize</span> incorrectly retaining `freq` in some cases where the original `freq` is no longer valid (`30511`)
  - Bug in <span class="title-ref">DatetimeIndex.intersection</span> losing `freq` and timezone in some cases (`33604`)
  - Bug in <span class="title-ref">DatetimeIndex.get\_indexer</span> where incorrect output would be returned for mixed datetime-like targets (`33741`)
  - Bug in <span class="title-ref">DatetimeIndex</span> addition and subtraction with some types of <span class="title-ref">DateOffset</span> objects incorrectly retaining an invalid `freq` attribute (`33779`)
  - Bug in <span class="title-ref">DatetimeIndex</span> where setting the `freq` attribute on an index could silently change the `freq` attribute on another index viewing the same data (`33552`)
  - <span class="title-ref">DataFrame.min</span> and <span class="title-ref">DataFrame.max</span> were not returning consistent results with <span class="title-ref">Series.min</span> and <span class="title-ref">Series.max</span> when called on objects initialized with empty <span class="title-ref">pd.to\_datetime</span>
  - Bug in <span class="title-ref">DatetimeIndex.intersection</span> and <span class="title-ref">TimedeltaIndex.intersection</span> with results not having the correct `name` attribute (`33904`)
  - Bug in <span class="title-ref">DatetimeArray.\_\_setitem\_\_</span>, <span class="title-ref">TimedeltaArray.\_\_setitem\_\_</span>, <span class="title-ref">PeriodArray.\_\_setitem\_\_</span> incorrectly allowing values with `int64` dtype to be silently cast (`33717`)
  - Bug in subtracting <span class="title-ref">TimedeltaIndex</span> from <span class="title-ref">Period</span> incorrectly raising `TypeError` in some cases where it should succeed and `IncompatibleFrequency` in some cases where it should raise `TypeError` (`33883`)
  - Bug in constructing a <span class="title-ref">Series</span> or <span class="title-ref">Index</span> from a read-only NumPy array with non-ns resolution which converted to object dtype instead of coercing to `datetime64[ns]` dtype when within the timestamp bounds (`34843`).
  - The `freq` keyword in <span class="title-ref">Period</span>, <span class="title-ref">date\_range</span>, <span class="title-ref">period\_range</span>, <span class="title-ref">pd.tseries.frequencies.to\_offset</span> no longer allows tuples, pass as string instead (`34703`)
  - Bug in <span class="title-ref">DataFrame.append</span> when appending a <span class="title-ref">Series</span> containing a scalar tz-aware <span class="title-ref">Timestamp</span> to an empty <span class="title-ref">DataFrame</span> resulted in an object column instead of `datetime64[ns, tz]` dtype (`35038`)
  - `OutOfBoundsDatetime` issues an improved error message when timestamp is out of implementation bounds. (`32967`)
  - Bug in <span class="title-ref">AbstractHolidayCalendar.holidays</span> when no rules were defined (`31415`)
  - Bug in <span class="title-ref">Tick</span> comparisons raising `TypeError` when comparing against timedelta-like objects (`34088`)
  - Bug in <span class="title-ref">Tick</span> multiplication raising `TypeError` when multiplying by a float (`34486`)

### Timedelta

  - Bug in constructing a <span class="title-ref">Timedelta</span> with a high precision integer that would round the <span class="title-ref">Timedelta</span> components (`31354`)
  - Bug in dividing `np.nan` or `None` by <span class="title-ref">Timedelta</span> incorrectly returning `NaT` (`31869`)
  - <span class="title-ref">Timedelta</span> now understands `Âµs` as an identifier for microsecond (`32899`)
  - <span class="title-ref">Timedelta</span> string representation now includes nanoseconds, when nanoseconds are non-zero (`9309`)
  - Bug in comparing a <span class="title-ref">Timedelta</span> object against an `np.ndarray` with `timedelta64` dtype incorrectly viewing all entries as unequal (`33441`)
  - Bug in <span class="title-ref">timedelta\_range</span> that produced an extra point on a edge case (`30353`, `33498`)
  - Bug in <span class="title-ref">DataFrame.resample</span> that produced an extra point on a edge case (`30353`, `13022`, `33498`)
  - Bug in <span class="title-ref">DataFrame.resample</span> that ignored the `loffset` argument when dealing with timedelta (`7687`, `33498`)
  - Bug in <span class="title-ref">Timedelta</span> and <span class="title-ref">pandas.to\_timedelta</span> that ignored the `unit` argument for string input (`12136`)

### Timezones

  - Bug in <span class="title-ref">to\_datetime</span> with `infer_datetime_format=True` where timezone names (e.g. `UTC`) would not be parsed correctly (`33133`)

### Numeric

  - Bug in <span class="title-ref">DataFrame.floordiv</span> with `axis=0` not treating division-by-zero like <span class="title-ref">Series.floordiv</span> (`31271`)
  - Bug in <span class="title-ref">to\_numeric</span> with string argument `"uint64"` and `errors="coerce"` silently fails (`32394`)
  - Bug in <span class="title-ref">to\_numeric</span> with `downcast="unsigned"` fails for empty data (`32493`)
  - Bug in <span class="title-ref">DataFrame.mean</span> with `numeric_only=False` and either `datetime64` dtype or `PeriodDtype` column incorrectly raising `TypeError` (`32426`)
  - Bug in <span class="title-ref">DataFrame.count</span> with `level="foo"` and index level `"foo"` containing NaNs causes segmentation fault (`21824`)
  - Bug in <span class="title-ref">DataFrame.diff</span> with `axis=1` returning incorrect results with mixed dtypes (`32995`)
  - Bug in <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">DataFrame.cov</span> raising when handling nullable integer columns with `pandas.NA` (`33803`)
  - Bug in arithmetic operations between <span class="title-ref">DataFrame</span> objects with non-overlapping columns with duplicate labels causing an infinite loop (`35194`)
  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> addition and subtraction between object-dtype objects and `datetime64` dtype objects (`33824`)
  - Bug in <span class="title-ref">Index.difference</span> giving incorrect results when comparing a <span class="title-ref">Float64Index</span> and object <span class="title-ref">Index</span> (`35217`)
  - Bug in <span class="title-ref">DataFrame</span> reductions (e.g. `df.min()`, `df.max()`) with `ExtensionArray` dtypes (`34520`, `32651`)
  - <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> now raise a ValueError if `limit_direction` is `'forward'` or `'both'` and `method` is `'backfill'` or `'bfill'` or `limit_direction` is `'backward'` or `'both'` and `method` is `'pad'` or `'ffill'` (`34746`)

### Conversion

  - Bug in <span class="title-ref">Series</span> construction from NumPy array with big-endian `datetime64` dtype (`29684`)
  - Bug in <span class="title-ref">Timedelta</span> construction with large nanoseconds keyword value (`32402`)
  - Bug in <span class="title-ref">DataFrame</span> construction where sets would be duplicated rather than raising (`32582`)
  - The <span class="title-ref">DataFrame</span> constructor no longer accepts a list of <span class="title-ref">DataFrame</span> objects. Because of changes to NumPy, <span class="title-ref">DataFrame</span> objects are now consistently treated as 2D objects, so a list of <span class="title-ref">DataFrame</span> objects is considered 3D, and no longer acceptable for the <span class="title-ref">DataFrame</span> constructor (`32289`).
  - Bug in <span class="title-ref">DataFrame</span> when initiating a frame with lists and assign `columns` with nested list for `MultiIndex` (`32173`)
  - Improved error message for invalid construction of list when creating a new index (`35190`)

### Strings

  - Bug in the <span class="title-ref">\~Series.astype</span> method when converting "string" dtype data to nullable integer dtype (`32450`).
  - Fixed issue where taking `min` or `max` of a `StringArray` or `Series` with `StringDtype` type would raise. (`31746`)
  - Bug in <span class="title-ref">Series.str.cat</span> returning `NaN` output when other had <span class="title-ref">Index</span> type (`33425`)
  - <span class="title-ref">pandas.api.dtypes.is\_string\_dtype</span> no longer incorrectly identifies categorical series as string.

### Interval

  - Bug in <span class="title-ref">IntervalArray</span> incorrectly allowing the underlying data to be changed when setting values (`32782`)

### Indexing

  - <span class="title-ref">DataFrame.xs</span> now raises a `TypeError` if a `level` keyword is supplied and the axis is not a <span class="title-ref">MultiIndex</span>. Previously an `AttributeError` was raised (`33610`)
  - Bug in slicing on a <span class="title-ref">DatetimeIndex</span> with a partial-timestamp dropping high-resolution indices near the end of a year, quarter, or month (`31064`)
  - Bug in <span class="title-ref">PeriodIndex.get\_loc</span> treating higher-resolution strings differently from <span class="title-ref">PeriodIndex.get\_value</span> (`31172`)
  - Bug in <span class="title-ref">Series.at</span> and <span class="title-ref">DataFrame.at</span> not matching `.loc` behavior when looking up an integer in a <span class="title-ref">Float64Index</span> (`31329`)
  - Bug in <span class="title-ref">PeriodIndex.is\_monotonic</span> incorrectly returning `True` when containing leading `NaT` entries (`31437`)
  - Bug in <span class="title-ref">DatetimeIndex.get\_loc</span> raising `KeyError` with converted-integer key instead of the user-passed key (`31425`)
  - Bug in <span class="title-ref">Series.xs</span> incorrectly returning `Timestamp` instead of `datetime64` in some object-dtype cases (`31630`)
  - Bug in <span class="title-ref">DataFrame.iat</span> incorrectly returning `Timestamp` instead of `datetime` in some object-dtype cases (`32809`)
  - Bug in <span class="title-ref">DataFrame.at</span> when either columns or index is non-unique (`33041`)
  - Bug in <span class="title-ref">Series.loc</span> and <span class="title-ref">DataFrame.loc</span> when indexing with an integer key on a object-dtype <span class="title-ref">Index</span> that is not all-integers (`31905`)
  - Bug in <span class="title-ref">DataFrame.iloc.\_\_setitem\_\_</span> on a <span class="title-ref">DataFrame</span> with duplicate columns incorrectly setting values for all matching columns (`15686`, `22036`)
  - Bug in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> with a <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span>, or <span class="title-ref">PeriodIndex</span> incorrectly allowing lookups of non-matching datetime-like dtypes (`32650`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> indexing with non-standard scalars, e.g. `np.dtype` (`32684`)
  - Bug in <span class="title-ref">Index</span> constructor where an unhelpful error message was raised for NumPy scalars (`33017`)
  - Bug in <span class="title-ref">DataFrame.lookup</span> incorrectly raising an `AttributeError` when `frame.index` or `frame.columns` is not unique; this will now raise a `ValueError` with a helpful error message (`33041`)
  - Bug in <span class="title-ref">Interval</span> where a <span class="title-ref">Timedelta</span> could not be added or subtracted from a <span class="title-ref">Timestamp</span> interval (`32023`)
  - Bug in <span class="title-ref">DataFrame.copy</span> not invalidating \_item\_cache after copy caused post-copy value updates to not be reflected (`31784`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> throwing an error when a `datetime64[ns, tz]` value is provided (`32395`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> with an integer key and a <span class="title-ref">MultiIndex</span> with leading integer level failing to raise `KeyError` if the key is not present in the first level (`33355`)
  - Bug in <span class="title-ref">DataFrame.iloc</span> when slicing a single column <span class="title-ref">DataFrame</span> with `ExtensionDtype` (e.g. `df.iloc[:, :1]`) returning an invalid result (`32957`)
  - Bug in <span class="title-ref">DatetimeIndex.insert</span> and <span class="title-ref">TimedeltaIndex.insert</span> causing index `freq` to be lost when setting an element into an empty <span class="title-ref">Series</span> (`33573`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> with an <span class="title-ref">IntervalIndex</span> and a list-like key of integers (`33473`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> allowing missing labels with `np.ndarray`, <span class="title-ref">Index</span>, <span class="title-ref">Series</span> indexers but not `list`, these now all raise `KeyError` (`33646`)
  - Bug in <span class="title-ref">DataFrame.truncate</span> and <span class="title-ref">Series.truncate</span> where index was assumed to be monotone increasing (`33756`)
  - Indexing with a list of strings representing datetimes failed on <span class="title-ref">DatetimeIndex</span> or <span class="title-ref">PeriodIndex</span> (`11278`)
  - Bug in <span class="title-ref">Series.at</span> when used with a <span class="title-ref">MultiIndex</span> would raise an exception on valid inputs (`26989`)
  - Bug in <span class="title-ref">DataFrame.loc</span> with dictionary of values changes columns with dtype of `int` to `float` (`34573`)
  - Bug in <span class="title-ref">Series.loc</span> when used with a <span class="title-ref">MultiIndex</span> would raise an `IndexingError` when accessing a `None` value (`34318`)
  - Bug in <span class="title-ref">DataFrame.reset\_index</span> and <span class="title-ref">Series.reset\_index</span> would not preserve data types on an empty <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with a <span class="title-ref">MultiIndex</span> (`19602`)
  - Bug in <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> indexing with a `time` key on a <span class="title-ref">DatetimeIndex</span> with `NaT` entries (`35114`)

### Missing

  - Calling <span class="title-ref">fillna</span> on an empty <span class="title-ref">Series</span> now correctly returns a shallow copied object. The behaviour is now consistent with <span class="title-ref">Index</span>, <span class="title-ref">DataFrame</span> and a non-empty <span class="title-ref">Series</span> (`32543`).
  - Bug in <span class="title-ref">Series.replace</span> when argument `to_replace` is of type dict/list and is used on a <span class="title-ref">Series</span> containing `<NA>` was raising a `TypeError`. The method now handles this by ignoring `<NA>` values when doing the comparison for the replacement (`32621`)
  - Bug in <span class="title-ref">\~Series.any</span> and <span class="title-ref">\~Series.all</span> incorrectly returning `<NA>` for all `False` or all `True` values using the nulllable Boolean dtype and with `skipna=False` (`33253`)
  - Clarified documentation on interpolate with `method=akima`. The `der` parameter must be scalar or `None` (`33426`)
  - <span class="title-ref">DataFrame.interpolate</span> uses the correct axis convention now. Previously interpolating along columns lead to interpolation along indices and vice versa. Furthermore interpolating with methods `pad`, `ffill`, `bfill` and `backfill` are identical to using these methods with <span class="title-ref">DataFrame.fillna</span> (`12918`, `29146`)
  - Bug in <span class="title-ref">DataFrame.interpolate</span> when called on a <span class="title-ref">DataFrame</span> with column names of string type was throwing a ValueError. The method is now independent of the type of the column names (`33956`)
  - Passing <span class="title-ref">NA</span> into a format string using format specs will now work. For example `"{:.1f}".format(pd.NA)` would previously raise a `ValueError`, but will now return the string `"<NA>"` (`34740`)
  - Bug in <span class="title-ref">Series.map</span> not raising on invalid `na_action` (`32815`)

### MultiIndex

  - <span class="title-ref">DataFrame.swaplevels</span> now raises a `TypeError` if the axis is not a <span class="title-ref">MultiIndex</span>. Previously an `AttributeError` was raised (`31126`)
  - Bug in <span class="title-ref">Dataframe.loc</span> when used with a <span class="title-ref">MultiIndex</span>. The returned values were not in the same order as the given inputs (`22797`)

<div class="ipython">

python

  - df = pd.DataFrame(np.arange(4),  
    index=\[\["a", "a", "b", "b"\], \[1, 2, 1, 2\]\])

\# Rows are now ordered as the requested keys df.loc\[(\['b', 'a'\], \[2, 1\]), :\]

</div>

  - Bug in <span class="title-ref">MultiIndex.intersection</span> was not guaranteed to preserve order when `sort=False`. (`31325`)
  - Bug in <span class="title-ref">DataFrame.truncate</span> was dropping <span class="title-ref">MultiIndex</span> names. (`34564`)

<div class="ipython">

python

left = pd.MultiIndex.from\_arrays(\[\["b", "a"\], \[2, 1\]\]) right = pd.MultiIndex.from\_arrays(\[\["a", "b", "c"\], \[1, 2, 3\]\]) \# Common elements are now guaranteed to be ordered by the left side left.intersection(right, sort=False)

</div>

  - Bug when joining two <span class="title-ref">MultiIndex</span> without specifying level with different columns. Return-indexers parameter was ignored. (`34074`)

### IO

  - Passing a `set` as `names` argument to <span class="title-ref">pandas.read\_csv</span>, <span class="title-ref">pandas.read\_table</span>, or <span class="title-ref">pandas.read\_fwf</span> will raise `ValueError: Names should be an ordered collection.` (`34946`)
  - Bug in print-out when `display.precision` is zero. (`20359`)
  - Bug in <span class="title-ref">read\_json</span> where integer overflow was occurring when json contains big number strings. (`30320`)
  - <span class="title-ref">read\_csv</span> will now raise a `ValueError` when the arguments `header` and `prefix` both are not `None`. (`27394`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> was raising `NotFoundError` when `path_or_buf` was an S3 URI (`28375`)
  - Bug in <span class="title-ref">DataFrame.to\_parquet</span> overwriting pyarrow's default for `coerce_timestamps`; following pyarrow's default allows writing nanosecond timestamps with `version="2.0"` (`31652`).
  - Bug in <span class="title-ref">read\_csv</span> was raising `TypeError` when `sep=None` was used in combination with `comment` keyword (`31396`)
  - Bug in <span class="title-ref">HDFStore</span> that caused it to set to `int64` the dtype of a `datetime64` column when reading a <span class="title-ref">DataFrame</span> in Python 3 from fixed format written in Python 2 (`31750`)
  - <span class="title-ref">read\_sas</span> now handles dates and datetimes larger than <span class="title-ref">Timestamp.max</span> returning them as <span class="title-ref">datetime.datetime</span> objects (`20927`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> where `Timedelta` objects would not be serialized correctly with `date_format="iso"` (`28256`)
  - <span class="title-ref">read\_csv</span> will raise a `ValueError` when the column names passed in `parse_dates` are missing in the <span class="title-ref">Dataframe</span> (`31251`)
  - Bug in <span class="title-ref">read\_excel</span> where a UTF-8 string with a high surrogate would cause a segmentation violation (`23809`)
  - Bug in <span class="title-ref">read\_csv</span> was causing a file descriptor leak on an empty file (`31488`)
  - Bug in <span class="title-ref">read\_csv</span> was causing a segfault when there were blank lines between the header and data rows (`28071`)
  - Bug in <span class="title-ref">read\_csv</span> was raising a misleading exception on a permissions issue (`23784`)
  - Bug in <span class="title-ref">read\_csv</span> was raising an `IndexError` when `header=None` and two extra data columns
  - Bug in <span class="title-ref">read\_sas</span> was raising an `AttributeError` when reading files from Google Cloud Storage (`33069`)
  - Bug in <span class="title-ref">DataFrame.to\_sql</span> where an `AttributeError` was raised when saving an out of bounds date (`26761`)
  - Bug in <span class="title-ref">read\_excel</span> did not correctly handle multiple embedded spaces in OpenDocument text cells. (`32207`)
  - Bug in <span class="title-ref">read\_json</span> was raising `TypeError` when reading a `list` of Booleans into a <span class="title-ref">Series</span>. (`31464`)
  - Bug in <span class="title-ref">pandas.io.json.json\_normalize</span> where location specified by `record_path` doesn't point to an array. (`26284`)
  - <span class="title-ref">pandas.read\_hdf</span> has a more explicit error message when loading an unsupported HDF file (`9539`)
  - Bug in <span class="title-ref">\~DataFrame.read\_feather</span> was raising an `ArrowIOError` when reading an s3 or http file path (`29055`)
  - Bug in <span class="title-ref">\~DataFrame.to\_excel</span> could not handle the column name `render` and was raising an `KeyError` (`34331`)
  - Bug in <span class="title-ref">\~SQLDatabase.execute</span> was raising a `ProgrammingError` for some DB-API drivers when the SQL statement contained the `%` character and no parameters were present (`34211`)
  - Bug in <span class="title-ref">\~pandas.io.stata.StataReader</span> which resulted in categorical variables with different dtypes when reading data using an iterator. (`31544`)
  - <span class="title-ref">HDFStore.keys</span> has now an optional `include` parameter that allows the retrieval of all native HDF5 table names (`29916`)
  - `TypeError` exceptions raised by <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> were showing as `parser_f` when an unexpected keyword argument was passed (`25648`)
  - Bug in <span class="title-ref">read\_excel</span> for ODS files removes 0.0 values (`27222`)
  - Bug in <span class="title-ref">ujson.encode</span> was raising an `OverflowError` with numbers larger than `sys.maxsize` (`34395`)
  - Bug in <span class="title-ref">HDFStore.append\_to\_multiple</span> was raising a `ValueError` when the `min_itemsize` parameter is set (`11238`)
  - Bug in <span class="title-ref">\~HDFStore.create\_table</span> now raises an error when `column` argument was not specified in `data_columns` on input (`28156`)
  - <span class="title-ref">read\_json</span> now could read line-delimited json file from a file url while `lines` and `chunksize` are set.
  - Bug in <span class="title-ref">DataFrame.to\_sql</span> when reading DataFrames with `-np.inf` entries with MySQL now has a more explicit `ValueError` (`34431`)
  - Bug where capitalised files extensions were not decompressed by [read]()\* functions (`35164`)
  - Bug in <span class="title-ref">read\_excel</span> that was raising a `TypeError` when `header=None` and `index_col` is given as a `list` (`31783`)
  - Bug in <span class="title-ref">read\_excel</span> where datetime values are used in the header in a <span class="title-ref">MultiIndex</span> (`34748`)
  - <span class="title-ref">read\_excel</span> no longer takes `**kwds` arguments. This means that passing in the keyword argument `chunksize` now raises a `TypeError` (previously raised a `NotImplementedError`), while passing in the keyword argument `encoding` now raises a `TypeError` (`34464`)
  - Bug in <span class="title-ref">DataFrame.to\_records</span> was incorrectly losing timezone information in timezone-aware `datetime64` columns (`32535`)

### Plotting

  - <span class="title-ref">DataFrame.plot</span> for line/bar now accepts color by dictionary (`8193`).
  - Bug in <span class="title-ref">DataFrame.plot.hist</span> where weights are not working for multiple columns (`33173`)
  - Bug in <span class="title-ref">DataFrame.boxplot</span> and <span class="title-ref">DataFrame.plot.boxplot</span> lost color attributes of `medianprops`, `whiskerprops`, `capprops` and `boxprops` (`30346`)
  - Bug in <span class="title-ref">DataFrame.hist</span> where the order of `column` argument was ignored (`29235`)
  - Bug in <span class="title-ref">DataFrame.plot.scatter</span> that when adding multiple plots with different `cmap`, colorbars always use the first `cmap` (`33389`)
  - Bug in <span class="title-ref">DataFrame.plot.scatter</span> was adding a colorbar to the plot even if the argument `c` was assigned to a column containing color names (`34316`)
  - Bug in <span class="title-ref">pandas.plotting.bootstrap\_plot</span> was causing cluttered axes and overlapping labels (`34905`)
  - Bug in <span class="title-ref">DataFrame.plot.scatter</span> caused an error when plotting variable marker sizes (`32904`)

### GroupBy/resample/rolling

  - Using a <span class="title-ref">pandas.api.indexers.BaseIndexer</span> with `count`, `min`, `max`, `median`, `skew`, `cov`, `corr` will now return correct results for any monotonic <span class="title-ref">pandas.api.indexers.BaseIndexer</span> descendant (`32865`)
  - <span class="title-ref">DataFrameGroupby.mean</span> and <span class="title-ref">SeriesGroupby.mean</span> (and similarly for <span class="title-ref">\~DataFrameGroupby.median</span>, <span class="title-ref">\~DataFrameGroupby.std</span> and <span class="title-ref">\~DataFrameGroupby.var</span>) now raise a `TypeError` if a non-accepted keyword argument is passed into it. Previously an `UnsupportedFunctionCall` was raised (`AssertionError` if `min_count` passed into <span class="title-ref">\~DataFrameGroupby.median</span>) (`31485`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> raising `ValueError` when the `by` axis is not sorted, has duplicates, and the applied `func` does not mutate passed in objects (`30667`)
  - Bug in <span class="title-ref">DataFrameGroupBy.transform</span> produces an incorrect result with transformation functions (`30918`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> were returning the wrong result when grouping by multiple keys of which some were categorical and others not (`32494`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.count</span> and <span class="title-ref">.SeriesGroupBy.count</span> causing segmentation fault when grouped-by columns contain NaNs (`32841`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> produces inconsistent type when aggregating Boolean <span class="title-ref">Series</span> (`32894`)
  - Bug in <span class="title-ref">DataFrameGroupBy.sum</span> and <span class="title-ref">SeriesGroupBy.sum</span> where a large negative number would be returned when the number of non-null values was below `min_count` for nullable integer dtypes (`32861`)
  - Bug in <span class="title-ref">SeriesGroupBy.quantile</span> was raising on nullable integers (`33136`)
  - Bug in <span class="title-ref">DataFrame.resample</span> where an `AmbiguousTimeError` would be raised when the resulting timezone aware <span class="title-ref">DatetimeIndex</span> had a DST transition at midnight (`25758`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> where a `ValueError` would be raised when grouping by a categorical column with read-only categories and `sort=False` (`33410`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.agg</span>, <span class="title-ref">.SeriesGroupBy.agg</span>, <span class="title-ref">.DataFrameGroupBy.transform</span>, <span class="title-ref">.SeriesGroupBy.transform</span>, <span class="title-ref">.DataFrameGroupBy.resample</span>, and <span class="title-ref">.SeriesGroupBy.resample</span> where subclasses are not preserved (`28330`)
  - Bug in <span class="title-ref">SeriesGroupBy.agg</span> where any column name was accepted in the named aggregation of <span class="title-ref">SeriesGroupBy</span> previously. The behaviour now allows only `str` and callables else would raise `TypeError`. (`34422`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> lost the name of the <span class="title-ref">Index</span> when one of the `agg` keys referenced an empty list (`32580`)
  - Bug in <span class="title-ref">Rolling.apply</span> where `center=True` was ignored when `engine='numba'` was specified (`34784`)
  - Bug in <span class="title-ref">DataFrame.ewm.cov</span> was throwing `AssertionError` for <span class="title-ref">MultiIndex</span> inputs (`34440`)
  - Bug in <span class="title-ref">core.groupby.DataFrameGroupBy.quantile</span> raised `TypeError` for non-numeric types rather than dropping the columns (`27892`)
  - Bug in <span class="title-ref">core.groupby.DataFrameGroupBy.transform</span> when `func='nunique'` and columns are of type `datetime64`, the result would also be of type `datetime64` instead of `int64` (`35109`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> raising an `AttributeError` when selecting a column and aggregating with `as_index=False` (`35246`).
  - Bug in <span class="title-ref">DataFrameGroupBy.first</span> and <span class="title-ref">DataFrameGroupBy.last</span> that would raise an unnecessary `ValueError` when grouping on multiple `Categoricals` (`34951`)

### Reshaping

  - Bug effecting all numeric and Boolean reduction methods not returning subclassed data type. (`25596`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> when only <span class="title-ref">MultiIndexed</span> columns is set (`17038`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> and <span class="title-ref">Series.unstack</span> can take tuple names in <span class="title-ref">MultiIndexed</span> data (`19966`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> when `margin` is `True` and only `column` is defined (`31016`)
  - Fixed incorrect error message in <span class="title-ref">DataFrame.pivot</span> when `columns` is set to `None`. (`30924`)
  - Bug in <span class="title-ref">crosstab</span> when inputs are two <span class="title-ref">Series</span> and have tuple names, the output will keep a dummy <span class="title-ref">MultiIndex</span> as columns. (`18321`)
  - <span class="title-ref">DataFrame.pivot</span> can now take lists for `index` and `columns` arguments (`21425`)
  - Bug in <span class="title-ref">concat</span> where the resulting indices are not copied when `copy=True` (`29879`)
  - Bug in <span class="title-ref">SeriesGroupBy.aggregate</span> was resulting in aggregations being overwritten when they shared the same name (`30880`)
  - Bug where <span class="title-ref">Index.astype</span> would lose the <span class="title-ref">name</span> attribute when converting from `Float64Index` to `Int64Index`, or when casting to an `ExtensionArray` dtype (`32013`)
  - <span class="title-ref">Series.append</span> will now raise a `TypeError` when passed a <span class="title-ref">DataFrame</span> or a sequence containing <span class="title-ref">DataFrame</span> (`31413`)
  - <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> will raise a `TypeError` if `to_replace` is not an expected type. Previously the `replace` would fail silently (`18634`)
  - Bug on inplace operation of a <span class="title-ref">Series</span> that was adding a column to the <span class="title-ref">DataFrame</span> from where it was originally dropped from (using `inplace=True`) (`30484`)
  - Bug in <span class="title-ref">DataFrame.apply</span> where callback was called with <span class="title-ref">Series</span> parameter even though `raw=True` requested. (`32423`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> losing timezone information when creating a <span class="title-ref">MultiIndex</span> level from a column with timezone-aware dtype (`32558`)
  - Bug in <span class="title-ref">concat</span> where when passing a non-dict mapping as `objs` would raise a `TypeError` (`32863`)
  - <span class="title-ref">DataFrame.agg</span> now provides more descriptive `SpecificationError` message when attempting to aggregate a non-existent column (`32755`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> when <span class="title-ref">MultiIndex</span> columns and <span class="title-ref">MultiIndex</span> rows were used (`32624`, `24729` and `28306`)
  - Appending a dictionary to a <span class="title-ref">DataFrame</span> without passing `ignore_index=True` will raise `TypeError: Can only append a dict if ignore_index=True` instead of ``TypeError: Can only append a `Series` if ignore_index=True or if the `Series` has a name`` (`30871`)
  - Bug in <span class="title-ref">DataFrame.corrwith</span>, <span class="title-ref">DataFrame.memory\_usage</span>, <span class="title-ref">DataFrame.dot</span>, <span class="title-ref">DataFrame.idxmin</span>, <span class="title-ref">DataFrame.idxmax</span>, <span class="title-ref">DataFrame.duplicated</span>, <span class="title-ref">DataFrame.isin</span>, <span class="title-ref">DataFrame.count</span>, <span class="title-ref">Series.explode</span>, <span class="title-ref">Series.asof</span> and <span class="title-ref">DataFrame.asof</span> not returning subclassed types. (`31331`)
  - Bug in <span class="title-ref">concat</span> was not allowing for concatenation of <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> with duplicate keys (`33654`)
  - Bug in <span class="title-ref">cut</span> raised an error when the argument `labels` contains duplicates (`33141`)
  - Ensure only named functions can be used in <span class="title-ref">eval</span> (`32460`)
  - Bug in <span class="title-ref">Dataframe.aggregate</span> and <span class="title-ref">Series.aggregate</span> was causing a recursive loop in some cases (`34224`)
  - Fixed bug in <span class="title-ref">melt</span> where melting <span class="title-ref">MultiIndex</span> columns with `col_level > 0` would raise a `KeyError` on `id_vars` (`34129`)
  - Bug in <span class="title-ref">Series.where</span> with an empty <span class="title-ref">Series</span> and empty `cond` having non-bool dtype (`34592`)
  - Fixed regression where <span class="title-ref">DataFrame.apply</span> would raise `ValueError` for elements with `S` dtype (`34529`)

### Sparse

  - Creating a <span class="title-ref">SparseArray</span> from timezone-aware dtype will issue a warning before dropping timezone information, instead of doing so silently (`32501`)
  - Bug in <span class="title-ref">arrays.SparseArray.from\_spmatrix</span> wrongly read scipy sparse matrix (`31991`)
  - Bug in <span class="title-ref">Series.sum</span> with `SparseArray` raised a `TypeError` (`25777`)
  - Bug where <span class="title-ref">DataFrame</span> containing an all-sparse <span class="title-ref">SparseArray</span> filled with `NaN` when indexed by a list-like (`27781`, `29563`)
  - The repr of <span class="title-ref">SparseDtype</span> now includes the repr of its `fill_value` attribute. Previously it used `fill_value`'s string representation (`34352`)
  - Bug where empty <span class="title-ref">DataFrame</span> could not be cast to <span class="title-ref">SparseDtype</span> (`33113`)
  - Bug in <span class="title-ref">arrays.SparseArray</span> was returning the incorrect type when indexing a sparse dataframe with an iterable (`34526`, `34540`)

### ExtensionArray

  - Fixed bug where <span class="title-ref">Series.value\_counts</span> would raise on empty input of `Int64` dtype (`33317`)
  - Fixed bug in <span class="title-ref">concat</span> when concatenating <span class="title-ref">DataFrame</span> objects with non-overlapping columns resulting in object-dtype columns rather than preserving the extension dtype (`27692`, `33027`)
  - Fixed bug where <span class="title-ref">StringArray.isna</span> would return `False` for NA values when `pandas.options.mode.use_inf_as_na` was set to `True` (`33655`)
  - Fixed bug in <span class="title-ref">Series</span> construction with EA dtype and index but no data or scalar data fails (`26469`)
  - Fixed bug that caused <span class="title-ref">Series.\_\_repr\_\_</span> to crash for extension types whose elements are multidimensional arrays (`33770`).
  - Fixed bug where <span class="title-ref">Series.update</span> would raise a `ValueError` for `ExtensionArray` dtypes with missing values (`33980`)
  - Fixed bug where <span class="title-ref">StringArray.memory\_usage</span> was not implemented (`33963`)
  - Fixed bug where <span class="title-ref">DataFrameGroupBy</span> would ignore the `min_count` argument for aggregations on nullable Boolean dtypes (`34051`)
  - Fixed bug where the constructor of <span class="title-ref">DataFrame</span> with `dtype='string'` would fail (`27953`, `33623`)
  - Bug where <span class="title-ref">DataFrame</span> column set to scalar extension type was considered an object type rather than the extension type (`34832`)
  - Fixed bug in <span class="title-ref">IntegerArray.astype</span> to correctly copy the mask as well (`34931`).

### Other

  - Set operations on an object-dtype <span class="title-ref">Index</span> now always return object-dtype results (`31401`)
  - Fixed <span class="title-ref">pandas.testing.assert\_series\_equal</span> to correctly raise if the `left` argument is a different subclass with `check_series_type=True` (`32670`).
  - Getting a missing attribute in a <span class="title-ref">DataFrame.query</span> or <span class="title-ref">DataFrame.eval</span> string raises the correct `AttributeError` (`32408`)
  - Fixed bug in <span class="title-ref">pandas.testing.assert\_series\_equal</span> where dtypes were checked for `Interval` and `ExtensionArray` operands when `check_dtype` was `False` (`32747`)
  - Bug in <span class="title-ref">DataFrame.\_\_dir\_\_</span> caused a segfault when using unicode surrogates in a column name (`25509`)
  - Bug in <span class="title-ref">DataFrame.equals</span> and <span class="title-ref">Series.equals</span> in allowing subclasses to be equal (`34402`).

## Contributors

<div class="contributors">

v1.0.5..v1.1.0|HEAD

</div>

---

v1.1.1.md

---

# What's new in 1.1.1 (August 20, 2020)

These are the changes in pandas 1.1.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">CategoricalIndex.format</span> where, when stringified scalars had different lengths, the shorter string would be right-filled with spaces, so it had the same length as the longest string (`35439`)
  - Fixed regression in <span class="title-ref">Series.truncate</span> when trying to truncate a single-element series (`35544`)
  - Fixed regression where <span class="title-ref">DataFrame.to\_numpy</span> would raise a `RuntimeError` for mixed dtypes when converting to `str` (`35455`)
  - Fixed regression where <span class="title-ref">read\_csv</span> would raise a `ValueError` when `pandas.options.mode.use_inf_as_na` was set to `True` (`35493`)
  - Fixed regression where <span class="title-ref">pandas.testing.assert\_series\_equal</span> would raise an error when non-numeric dtypes were passed with `check_exact=True` (`35446`)
  - Fixed regression in `.groupby(..).rolling(..)` where column selection was ignored (`35486`)
  - Fixed regression where <span class="title-ref">DataFrame.interpolate</span> would raise a `TypeError` when the <span class="title-ref">DataFrame</span> was empty (`35598`)
  - Fixed regression in <span class="title-ref">DataFrame.shift</span> with `axis=1` and heterogeneous dtypes (`35488`)
  - Fixed regression in <span class="title-ref">DataFrame.diff</span> with read-only data (`35559`)
  - Fixed regression in `.groupby(..).rolling(..)` where a segfault would occur with `center=True` and an odd number of values (`35552`)
  - Fixed regression in <span class="title-ref">DataFrame.apply</span> where functions that altered the input in-place only operated on a single row (`35462`)
  - Fixed regression in <span class="title-ref">DataFrame.reset\_index</span> would raise a `ValueError` on empty <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> with a `datetime64` dtype level (`35606`, `35657`)
  - Fixed regression where <span class="title-ref">pandas.merge\_asof</span> would raise a `UnboundLocalError` when `left_index`, `right_index` and `tolerance` were set (`35558`)
  - Fixed regression in `.groupby(..).rolling(..)` where a custom `BaseIndexer` would be ignored (`35557`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> where compiled regular expressions would be ignored during replacement (`35680`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.aggregate</span> where a list of functions would produce the wrong results if at least one of the functions did not aggregate (`35490`)
  - Fixed memory usage issue when instantiating large <span class="title-ref">pandas.arrays.StringArray</span> (`35499`)

## Bug fixes

  - Bug in <span class="title-ref">\~pandas.io.formats.style.Styler</span> whereby `cell_ids` argument had no effect due to other recent changes (`35588`) (`35663`)
  - Bug in <span class="title-ref">pandas.testing.assert\_series\_equal</span> and <span class="title-ref">pandas.testing.assert\_frame\_equal</span> where extension dtypes were not ignored when `check_dtypes` was set to `False` (`35715`)
  - Bug in <span class="title-ref">to\_timedelta</span> fails when `arg` is a <span class="title-ref">Series</span> with `Int64` dtype containing null values (`35574`)
  - Bug in `.groupby(..).rolling(..)` where passing `closed` with column selection would raise a `ValueError` (`35549`)
  - Bug in <span class="title-ref">DataFrame</span> constructor failing to raise `ValueError` in some cases when `data` and `index` have mismatched lengths (`33437`)

## Contributors

<div class="contributors">

v1.1.0..v1.1.1

</div>

---

v1.1.2.md

---

# What's new in 1.1.2 (September 8, 2020)

These are the changes in pandas 1.1.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Regression in <span class="title-ref">DatetimeIndex.intersection</span> incorrectly raising `AssertionError` when intersecting against a list (`35876`)
  - Fix regression in updating a column inplace (e.g. using `df['col'].fillna(.., inplace=True)`) (`35731`)
  - Fix regression in <span class="title-ref">DataFrame.append</span> mixing tz-aware and tz-naive datetime columns (`35460`)
  - Performance regression for <span class="title-ref">RangeIndex.format</span> (`35712`)
  - Regression where <span class="title-ref">MultiIndex.get\_loc</span> would return a slice spanning the full index when passed an empty list (`35878`)
  - Fix regression in invalid cache after an indexing operation; this can manifest when setting which does not update the data (`35521`)
  - Regression in <span class="title-ref">DataFrame.replace</span> where a `TypeError` would be raised when attempting to replace elements of type <span class="title-ref">Interval</span> (`35931`)
  - Fix regression in pickle roundtrip of the `closed` attribute of <span class="title-ref">IntervalIndex</span> (`35658`)
  - Fixed regression in <span class="title-ref">DataFrameGroupBy.agg</span> where a `ValueError: buffer source array is read-only` would be raised when the underlying array is read-only (`36014`)
  - Fixed regression in <span class="title-ref">Series.groupby.rolling</span> number of levels of <span class="title-ref">MultiIndex</span> in input was compressed to one (`36018`)
  - Fixed regression in <span class="title-ref">DataFrameGroupBy</span> on an empty <span class="title-ref">DataFrame</span> (`36197`)

## Bug fixes

  - Bug in <span class="title-ref">DataFrame.eval</span> with `object` dtype column binary operations (`35794`)
  - Bug in <span class="title-ref">Series</span> constructor raising a `TypeError` when constructing sparse datetime64 dtypes (`35762`)
  - Bug in <span class="title-ref">DataFrame.apply</span> with `result_type="reduce"` returning with incorrect index (`35683`)
  - Bug in <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> not respecting the `errors` argument when set to `"ignore"` for extension dtypes (`35471`)
  - Bug in <span class="title-ref">DateTimeIndex.format</span> and <span class="title-ref">PeriodIndex.format</span> with `name=True` setting the first item to `"None"` where it should be `""` (`35712`)
  - Bug in <span class="title-ref">Float64Index.\_\_contains\_\_</span> incorrectly raising `TypeError` instead of returning `False` (`35788`)
  - Bug in <span class="title-ref">Series</span> constructor incorrectly raising a `TypeError` when passed an ordered set (`36044`)
  - Bug in <span class="title-ref">Series.dt.isocalendar</span> and <span class="title-ref">DatetimeIndex.isocalendar</span> that returned incorrect year for certain dates (`36032`)
  - Bug in <span class="title-ref">DataFrame</span> indexing returning an incorrect <span class="title-ref">Series</span> in some cases when the series has been altered and a cache not invalidated (`33675`)
  - Bug in <span class="title-ref">DataFrame.corr</span> causing subsequent indexing lookups to be incorrect (`35882`)
  - Bug in <span class="title-ref">import\_optional\_dependency</span> returning incorrect package names in cases where package name is different from import name (`35948`)
  - Bug when setting empty <span class="title-ref">DataFrame</span> column to a <span class="title-ref">Series</span> in preserving name of index in frame (`31368`)

## Other

  - <span class="title-ref">factorize</span> now supports `na_sentinel=None` to include NaN in the uniques of the values and remove `dropna` keyword which was unintentionally exposed to public facing API in 1.1 version from <span class="title-ref">factorize</span> (`35667`)
  - <span class="title-ref">DataFrame.plot</span> and <span class="title-ref">Series.plot</span> raise `UserWarning` about usage of `FixedFormatter` and `FixedLocator` (`35684` and `35945`)

## Contributors

<div class="contributors">

v1.1.1..v1.1.2

</div>

---

v1.1.3.md

---

# What's new in 1.1.3 (October 5, 2020)

These are the changes in pandas 1.1.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### Added support for new Python version

pandas 1.1.3 now supports Python 3.9 (`36296`).

### Development Changes

  - The minimum version of Cython is now the most recent bug-fix version (0.29.21) (`36296`).

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.agg</span>, <span class="title-ref">DataFrame.apply</span>, <span class="title-ref">Series.agg</span>, and <span class="title-ref">Series.apply</span> where internal suffix is exposed to the users when no relabelling is applied (`36189`)
  - Fixed regression in <span class="title-ref">IntegerArray</span> unary plus and minus operations raising a `TypeError` (`36063`)
  - Fixed regression when adding a <span class="title-ref">timedelta\_range</span> to a <span class="title-ref">Timestamp</span> raised a `ValueError` (`35897`)
  - Fixed regression in <span class="title-ref">Series.\_\_getitem\_\_</span> incorrectly raising when the input was a tuple (`35534`)
  - Fixed regression in <span class="title-ref">Series.\_\_getitem\_\_</span> incorrectly raising when the input was a frozenset (`35747`)
  - Fixed regression in modulo of <span class="title-ref">Index</span>, <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> using `numexpr` using C not Python semantics (`36047`, `36526`)
  - Fixed regression in <span class="title-ref">read\_excel</span> with `engine="odf"` caused `UnboundLocalError` in some cases where cells had nested child nodes (`36122`, `35802`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> inconsistent replace when using a float in the replace method (`35376`)
  - Fixed regression in <span class="title-ref">Series.loc</span> on a <span class="title-ref">Series</span> with a <span class="title-ref">MultiIndex</span> containing <span class="title-ref">Timestamp</span> raising `InvalidIndexError` (`35858`)
  - Fixed regression in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> comparisons between numeric arrays and strings (`35700`, `36377`)
  - Fixed regression in <span class="title-ref">DataFrame.apply</span> with `raw=True` and user-function returning string (`35940`)
  - Fixed regression when setting empty <span class="title-ref">DataFrame</span> column to a <span class="title-ref">Series</span> in preserving name of index in frame (`36527`)
  - Fixed regression in <span class="title-ref">Period</span> incorrect value for ordinal over the maximum timestamp (`36430`)
  - Fixed regression in <span class="title-ref">read\_table</span> raised `ValueError` when `delim_whitespace` was set to `True` (`35958`)
  - Fixed regression in <span class="title-ref">Series.dt.normalize</span> when normalizing pre-epoch dates the result was shifted one day (`36294`)

## Bug fixes

  - Bug in <span class="title-ref">read\_spss</span> where passing a `pathlib.Path` as `path` would raise a `TypeError` (`33666`)
  - Bug in <span class="title-ref">Series.str.startswith</span> and <span class="title-ref">Series.str.endswith</span> with `category` dtype not propagating `na` parameter (`36241`)
  - Bug in <span class="title-ref">Series</span> constructor where integer overflow would occur for sufficiently large scalar inputs when an index was provided (`36291`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> raising an `AttributeError` when sorting on a key that casts column to categorical dtype (`36383`)
  - Bug in <span class="title-ref">DataFrame.stack</span> raising a `ValueError` when stacking <span class="title-ref">MultiIndex</span> columns based on position when the levels had duplicate names (`36353`)
  - Bug in <span class="title-ref">Series.astype</span> showing too much precision when casting from `np.float32` to string dtype (`36451`)
  - Bug in <span class="title-ref">Series.isin</span> and <span class="title-ref">DataFrame.isin</span> when using `NaN` and a row length above 1,000,000 (`22205`)
  - Bug in <span class="title-ref">cut</span> raising a `ValueError` when passed a <span class="title-ref">Series</span> of labels with `ordered=False` (`36603`)

## Other

  - Reverted enhancement added in pandas-1.1.0 where <span class="title-ref">timedelta\_range</span> infers a frequency when passed `start`, `stop`, and `periods` (`32377`)

## Contributors

<div class="contributors">

v1.1.2..v1.1.3

</div>

---

v1.1.4.md

---

# What's new in 1.1.4 (October 30, 2020)

These are the changes in pandas 1.1.4. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">read\_csv</span> raising a `ValueError` when `names` was of type `dict_keys` (`36928`)
  - Fixed regression in <span class="title-ref">read\_csv</span> with more than 1M rows and specifying a `index_col` argument (`37094`)
  - Fixed regression where attempting to mutate a <span class="title-ref">DateOffset</span> object would no longer raise an `AttributeError` (`36940`)
  - Fixed regression where <span class="title-ref">DataFrame.agg</span> would fail with <span class="title-ref">TypeError</span> when passed positional arguments to be passed on to the aggregation function (`36948`).
  - Fixed regression in <span class="title-ref">RollingGroupby</span> with `sort=False` not being respected (`36889`)
  - Fixed regression in <span class="title-ref">Series.astype</span> converting `None` to `"nan"` when casting to string (`36904`)
  - Fixed regression in <span class="title-ref">Series.rank</span> method failing for read-only data (`37290`)
  - Fixed regression in <span class="title-ref">RollingGroupby</span> causing a segmentation fault with Index of dtype object (`36727`)
  - Fixed regression in <span class="title-ref">DataFrame.resample(...).apply(...)</span> raised `AttributeError` when input was a <span class="title-ref">DataFrame</span> and only a <span class="title-ref">Series</span> was evaluated (`36951`)
  - Fixed regression in `DataFrame.groupby(..).std()` with nullable integer dtype (`37415`)
  - Fixed regression in <span class="title-ref">PeriodDtype</span> comparing both equal and unequal to its string representation (`37265`)
  - Fixed regression where slicing <span class="title-ref">DatetimeIndex</span> raised <span class="title-ref">AssertionError</span> on irregular time series with `pd.NaT` or on unsorted indices (`36953` and `35509`)
  - Fixed regression in certain offsets (<span class="title-ref">pd.offsets.Day() \<pandas.tseries.offsets.Day\></span> and below) no longer being hashable (`37267`)
  - Fixed regression in <span class="title-ref">StataReader</span> which required `chunksize` to be manually set when using an iterator to read a dataset (`37280`)
  - Fixed regression in setitem with <span class="title-ref">DataFrame.iloc</span> which raised error when trying to set a value while filtering with a boolean list (`36741`)
  - Fixed regression in setitem with a Series getting aligned before setting the values (`37427`)
  - Fixed regression in <span class="title-ref">MultiIndex.is\_monotonic\_increasing</span> returning wrong results with `NaN` in at least one of the levels (`37220`)
  - Fixed regression in inplace arithmetic operation (`+=`) on a Series not updating the parent DataFrame/Series (`36373`)

## Bug fixes

  - Bug causing `groupby(...).sum()` and similar to not preserve metadata (`29442`)
  - Bug in <span class="title-ref">Series.isin</span> and <span class="title-ref">DataFrame.isin</span> raising a `ValueError` when the target was read-only (`37174`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.fillna</span> and <span class="title-ref">.SeriesGroupBy.fillna</span> that introduced a performance regression after 1.0.5 (`36757`)
  - Bug in <span class="title-ref">DataFrame.info</span> was raising a `KeyError` when the DataFrame has integer column names (`37245`)
  - Bug in <span class="title-ref">DataFrameGroupby.apply</span> would drop a <span class="title-ref">CategoricalIndex</span> when grouped on (`35792`)

## Contributors

<div class="contributors">

v1.1.3..v1.1.4

</div>

---

v1.1.5.md

---

# What's new in 1.1.5 (December 07, 2020)

These are the changes in pandas 1.1.5. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in addition of a timedelta-like scalar to a <span class="title-ref">DatetimeIndex</span> raising incorrectly (`37295`)
  - Fixed regression in <span class="title-ref">Series.groupby</span> raising when the <span class="title-ref">Index</span> of the <span class="title-ref">Series</span> had a tuple as its name (`37755`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> for `__setitem__` when one-dimensional tuple was given to select from <span class="title-ref">MultiIndex</span> (`37711`)
  - Fixed regression in inplace operations on <span class="title-ref">Series</span> with `ExtensionDtype` with NumPy dtyped operand (`37910`)
  - Fixed regression in metadata propagation for `groupby` iterator (`37343`)
  - Fixed regression in <span class="title-ref">MultiIndex</span> constructed from a <span class="title-ref">DatetimeIndex</span> not retaining frequency (`35563`)
  - Fixed regression in <span class="title-ref">Index</span> constructor raising a `AttributeError` when passed a <span class="title-ref">SparseArray</span> with datetime64 values (`35843`)
  - Fixed regression in <span class="title-ref">DataFrame.unstack</span> with columns with integer dtype (`37115`)
  - Fixed regression in indexing on a <span class="title-ref">Series</span> with `CategoricalDtype` after unpickling (`37631`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby</span> aggregation with out-of-bounds datetime objects in an object-dtype column (`36003`)
  - Fixed regression in `df.groupby(..).rolling(..)` with the resulting <span class="title-ref">MultiIndex</span> when grouping by a label that is in the index (`37641`)
  - Fixed regression in <span class="title-ref">DataFrame.fillna</span> not filling `NaN` after other operations such as <span class="title-ref">DataFrame.pivot</span> (`36495`).
  - Fixed performance regression in `df.groupby(..).rolling(..)` (`38038`)
  - Fixed regression in <span class="title-ref">MultiIndex.intersection</span> returning duplicates when at least one of the indexes had duplicates (`36915`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.first</span>, <span class="title-ref">.SeriesGroupBy.first</span>, <span class="title-ref">.DataFrameGroupBy.last</span>, and <span class="title-ref">.SeriesGroupBy.last</span> where `None` was considered a non-NA value (`38286`)

## Bug fixes

  - Bug in pytables methods in python 3.9 (`38041`)

## Other

  - Only set `-Werror` as a compiler flag in the CI jobs (`33315`, `33314`)

## Contributors

<div class="contributors">

v1.1.4..v1.1.5|HEAD

</div>

---

v1.2.0.md

---

# What's new in 1.2.0 (December 26, 2020)

These are the changes in pandas 1.2.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

\> **Warning** \> The [xlwt](https://xlwt.readthedocs.io/en/latest/) package for writing old-style `.xls` excel files is no longer maintained. The [xlrd](https://xlrd.readthedocs.io/en/latest/) package is now only for reading old-style `.xls` files.

> Previously, the default argument `engine=None` to <span class="title-ref">\~pandas.read\_excel</span> would result in using the `xlrd` engine in many cases, including new Excel 2007+ (`.xlsx`) files. If [openpyxl](https://openpyxl.readthedocs.io/en/stable/) is installed, many of these cases will now default to using the `openpyxl` engine. See the <span class="title-ref">read\_excel</span> documentation for more details.
> 
> Thus, it is strongly encouraged to install `openpyxl` to read Excel 2007+ (`.xlsx`) files. **Please do not report issues when using \`\`xlrd\`\` to read \`\`.xlsx\`\` files.** This is no longer supported, switch to using `openpyxl` instead.
> 
> Attempting to use the `xlwt` engine will raise a `FutureWarning` unless the option <span class="title-ref">io.excel.xls.writer</span> is set to `"xlwt"`. While this option is now deprecated and will also raise a `FutureWarning`, it can be globally set and the warning suppressed. Users are recommended to write `.xlsx` files using the `openpyxl` engine instead.

## Enhancements

### Optionally disallow duplicate labels

<span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> can now be created with `allows_duplicate_labels=False` flag to control whether the index or columns can contain duplicate labels (`28394`). This can be used to prevent accidental introduction of duplicate labels, which can affect downstream operations.

By default, duplicates continue to be allowed.

`` `ipython     In [1]: pd.Series([1, 2], index=['a', 'a'])     Out[1]:     a    1     a    2     Length: 2, dtype: int64      In [2]: pd.Series([1, 2], index=['a', 'a']).set_flags(allows_duplicate_labels=False)     ...     DuplicateLabelError: Index has duplicates.           positions     label     a        [0, 1]  pandas will propagate the ``allows\_duplicate\_labels`property through many operations.  .. code-block:: ipython      In [3]: a = (        ...:     pd.Series([1, 2], index=['a', 'b'])        ...:       .set_flags(allows_duplicate_labels=False)        ...: )      In [4]: a     Out[4]:     a    1     b    2     Length: 2, dtype: int64      # An operation introducing duplicates     In [5]: a.reindex(['a', 'b', 'a'])     ...     DuplicateLabelError: Index has duplicates.           positions     label     a        [0, 2]      [1 rows x 1 columns]  > **Warning** >     This is an experimental feature. Currently, many methods fail to    propagate the`allows\_duplicate\_labels`value. In future versions    it is expected that every method taking or returning one or more    DataFrame or Series objects will propagate`allows\_duplicate\_labels`.  See [duplicates](#duplicates) for more.  The`allows\_duplicate\_labels`` flag is stored in the new `DataFrame.flags` ``<span class="title-ref"> attribute. This stores global attributes that apply to the \*pandas object\*. This differs from \`DataFrame.attrs</span>, which stores information that applies to the dataset.

### Passing arguments to fsspec backends

Many read/write functions have acquired the `storage_options` optional argument, to pass a dictionary of parameters to the storage backend. This allows, for example, for passing credentials to S3 and GCS storage. The details of what parameters can be passed to which backends can be found in the documentation of the individual storage backends (detailed from the fsspec docs for [builtin implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations) and linked to [external ones](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)). See Section \[io.remote\](\#io.remote).

`35655` added fsspec support (including `storage_options`) for reading excel files.

### Support for binary file handles in `to_csv`

<span class="title-ref">to\_csv</span> supports file handles in binary mode (`19827` and `35058`) with `encoding` (`13068` and `23854`) and `compression` (`22555`). If pandas does not automatically detect whether the file handle is opened in binary or text mode, it is necessary to provide `mode="wb"`.

For example:

<div class="ipython">

python

import io

data = pd.DataFrame(\[0, 1, 2\]) buffer = io.BytesIO() data.to\_csv(buffer, encoding="utf-8", compression="gzip")

</div>

### Support for short caption and table position in `to_latex`

<span class="title-ref">DataFrame.to\_latex</span> now allows one to specify a floating table position (`35281`) and a short caption (`36267`).

The keyword `position` has been added to set the position.

<div class="ipython" data-okwarning="">

python

data = pd.DataFrame({'a': \[1, 2\], 'b': \[3, 4\]}) table = data.to\_latex(position='ht') print(table)

</div>

Usage of the keyword `caption` has been extended. Besides taking a single string as an argument, one can optionally provide a tuple `(full_caption, short_caption)` to add a short caption macro.

<div class="ipython" data-okwarning="">

python

data = pd.DataFrame({'a': \[1, 2\], 'b': \[3, 4\]}) table = data.to\_latex(caption=('the full long caption', 'short caption')) print(table)

</div>

### Change in default floating precision for `read_csv` and `read_table`

For the C parsing engine, the methods <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> previously defaulted to a parser that could read floating point numbers slightly incorrectly with respect to the last bit in precision. The option `floating_precision="high"` has always been available to avoid this issue. Beginning with this version, the default is now to use the more accurate parser by making `floating_precision=None` correspond to the high precision parser, and the new option `floating_precision="legacy"` to use the legacy parser. The change to using the higher precision parser by default should have no impact on performance. (`17154`)

### Experimental nullable data types for float data

We've added <span class="title-ref">Float32Dtype</span> / <span class="title-ref">Float64Dtype</span> and <span class="title-ref">\~arrays.FloatingArray</span>. These are extension data types dedicated to floating point data that can hold the `pd.NA` missing value indicator (`32265`, `34307`).

While the default float data type already supports missing values using `np.nan`, these new data types use `pd.NA` (and its corresponding behavior) as the missing value indicator, in line with the already existing nullable \[integer \<integer\_na\>\](\#integer-\<integer\_na\>) and \[boolean \<boolean\>\](\#boolean-\<boolean\>) data types.

One example where the behavior of `np.nan` and `pd.NA` is different is comparison operations:

<div class="ipython">

python

\# the default NumPy float64 dtype s1 = pd.Series(\[1.5, None\]) s1 s1 \> 1

</div>

<div class="ipython">

python

\# the new nullable float64 dtype s2 = pd.Series(\[1.5, None\], dtype="Float64") s2 s2 \> 1

</div>

See the \[missing\_data.NA\](\#missing\_data.na) doc section for more details on the behavior when using the `pd.NA` missing value indicator.

As shown above, the dtype can be specified using the "Float64" or "Float32" string (capitalized to distinguish it from the default "float64" data type). Alternatively, you can also use the dtype object:

<div class="ipython">

python

pd.Series(\[1.5, None\], dtype=pd.Float32Dtype())

</div>

Operations with the existing integer or boolean nullable data types that give float results will now also use the nullable floating data types (`38178`).

\> **Warning** \> Experimental: the new floating data types are currently experimental, and their behavior or API may still change without warning. Especially the behavior regarding NaN (distinct from NA missing values) is subject to change.

### Index/column name preservation when aggregating

When aggregating using <span class="title-ref">concat</span> or the <span class="title-ref">DataFrame</span> constructor, pandas will now attempt to preserve index and column names whenever possible (`35847`). In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. Here is an example where the index name is preserved:

<div class="ipython">

python

idx = pd.Index(range(5), name='abc') ser = pd.Series(range(5, 10), index=idx) pd.concat({'x': ser\[1:\], 'y': ser\[:-1\]}, axis=1)

</div>

The same is true for <span class="title-ref">MultiIndex</span>, but the logic is applied separately on a level-by-level basis.

### GroupBy supports EWM operations directly

<span class="title-ref">.DataFrameGroupBy</span> now supports exponentially weighted window operations directly (`16037`).

<div class="ipython">

python

df = pd.DataFrame({'A': \['a', 'b', 'a', 'b'\], 'B': range(4)}) df df.groupby('A').ewm(com=1.0).mean()

</div>

Additionally `mean` supports execution via [Numba](https://numba.pydata.org/) with the `engine` and `engine_kwargs` arguments. Numba must be installed as an optional dependency to use this feature.

### Other enhancements

  - Added `day_of_week` (compatibility alias `dayofweek`) property to <span class="title-ref">Timestamp</span>, <span class="title-ref">.DatetimeIndex</span>, <span class="title-ref">Period</span>, <span class="title-ref">PeriodIndex</span> (`9605`)
  - Added `day_of_year` (compatibility alias `dayofyear`) property to <span class="title-ref">Timestamp</span>, <span class="title-ref">.DatetimeIndex</span>, <span class="title-ref">Period</span>, <span class="title-ref">PeriodIndex</span> (`9605`)
  - Added <span class="title-ref">\~DataFrame.set\_flags</span> for setting table-wide flags on a Series or DataFrame (`28394`)
  - <span class="title-ref">DataFrame.applymap</span> now supports `na_action` (`23803`)
  - <span class="title-ref">Index</span> with object dtype supports division and multiplication (`34160`)
  - <span class="title-ref">io.sql.get\_schema</span> now supports a `schema` keyword argument that will add a schema into the create table statement (`28486`)
  - <span class="title-ref">DataFrame.explode</span> and <span class="title-ref">Series.explode</span> now support exploding of sets (`35614`)
  - <span class="title-ref">DataFrame.hist</span> now supports time series (datetime) data (`32590`)
  - <span class="title-ref">.Styler.set\_table\_styles</span> now allows the direct styling of rows and columns and can be chained (`35607`)
  - <span class="title-ref">.Styler</span> now allows direct CSS class name addition to individual data cells (`36159`)
  - <span class="title-ref">.Rolling.mean</span> and <span class="title-ref">.Rolling.sum</span> use Kahan summation to calculate the mean to avoid numerical problems (`10319`, `11645`, `13254`, `32761`, `36031`)
  - <span class="title-ref">.DatetimeIndex.searchsorted</span>, <span class="title-ref">.TimedeltaIndex.searchsorted</span>, <span class="title-ref">PeriodIndex.searchsorted</span>, and <span class="title-ref">Series.searchsorted</span> with datetime-like dtypes will now try to cast string arguments (list-like and scalar) to the matching datetime-like type (`36346`)
  - Added methods <span class="title-ref">IntegerArray.prod</span>, <span class="title-ref">IntegerArray.min</span>, and <span class="title-ref">IntegerArray.max</span> (`33790`)
  - Calling a NumPy ufunc on a `DataFrame` with extension types now preserves the extension types when possible (`23743`)
  - Calling a binary-input NumPy ufunc on multiple `DataFrame` objects now aligns, matching the behavior of binary operations and ufuncs on `Series` (`23743`). This change has been reverted in pandas 1.2.1, and the behaviour to not align DataFrames is deprecated instead, see the \[the 1.2.1 release notes \<whatsnew\_121.ufunc\_deprecation\>\](\#the-1.2.1-release-notes-\<whatsnew\_121.ufunc\_deprecation\>).
  - Where possible <span class="title-ref">RangeIndex.difference</span> and <span class="title-ref">RangeIndex.symmetric\_difference</span> will return <span class="title-ref">RangeIndex</span> instead of <span class="title-ref">Int64Index</span> (`36564`)
  - <span class="title-ref">DataFrame.to\_parquet</span> now supports <span class="title-ref">MultiIndex</span> for columns in parquet format (`34777`)
  - <span class="title-ref">read\_parquet</span> gained a `use_nullable_dtypes=True` option to use nullable dtypes that use `pd.NA` as missing value indicator where possible for the resulting DataFrame (default is `False`, and only applicable for `engine="pyarrow"`) (`31242`)
  - Added <span class="title-ref">.Rolling.sem</span> and <span class="title-ref">Expanding.sem</span> to compute the standard error of the mean (`26476`)
  - <span class="title-ref">.Rolling.var</span> and <span class="title-ref">.Rolling.std</span> use Kahan summation and Welford's Method to avoid numerical issues (`37051`)
  - <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">DataFrame.cov</span> use Welford's Method to avoid numerical issues (`37448`)
  - <span class="title-ref">DataFrame.plot</span> now recognizes `xlabel` and `ylabel` arguments for plots of type `scatter` and `hexbin` (`37001`)
  - <span class="title-ref">DataFrame</span> now supports the `divmod` operation (`37165`)
  - <span class="title-ref">DataFrame.to\_parquet</span> now returns a `bytes` object when no `path` argument is passed (`37105`)
  - <span class="title-ref">.Rolling</span> now supports the `closed` argument for fixed windows (`34315`)
  - <span class="title-ref">.DatetimeIndex</span> and <span class="title-ref">Series</span> with `datetime64` or `datetime64tz` dtypes now support `std` (`37436`)
  - <span class="title-ref">Window</span> now supports all Scipy window types in `win_type` with flexible keyword argument support (`34556`)
  - <span class="title-ref">testing.assert\_index\_equal</span> now has a `check_order` parameter that allows indexes to be checked in an order-insensitive manner (`37478`)
  - <span class="title-ref">read\_csv</span> supports memory-mapping for compressed files (`37621`)
  - Add support for `min_count` keyword for <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">DataFrame.resample</span> for functions `min`, `max`, `first` and `last` (`37821`, `37768`)
  - Improve error reporting for <span class="title-ref">DataFrame.merge</span> when invalid merge column definitions were given (`16228`)
  - Improve numerical stability for <span class="title-ref">.Rolling.skew</span>, <span class="title-ref">.Rolling.kurt</span>, <span class="title-ref">Expanding.skew</span> and <span class="title-ref">Expanding.kurt</span> through implementation of Kahan summation (`6929`)
  - Improved error reporting for subsetting columns of a <span class="title-ref">.DataFrameGroupBy</span> with `axis=1` (`37725`)
  - Implement method `cross` for <span class="title-ref">DataFrame.merge</span> and <span class="title-ref">DataFrame.join</span> (`5401`)
  - When <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_sas</span> and <span class="title-ref">read\_json</span> are called with `chunksize`/`iterator` they can be used in a `with` statement as they return context-managers (`38225`)
  - Augmented the list of named colors available for styling Excel exports, enabling all of CSS4 colors (`38247`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### Consistency of DataFrame Reductions

<span class="title-ref">DataFrame.any</span> and <span class="title-ref">DataFrame.all</span> with `bool_only=True` now determines whether to exclude object-dtype columns on a column-by-column basis, instead of checking if *all* object-dtype columns can be considered boolean.

This prevents pathological behavior where applying the reduction on a subset of columns could result in a larger Series result. See (`37799`).

<div class="ipython">

python

df = pd.DataFrame({"A": \["foo", "bar"\], "B": \[True, False\]}, dtype=object) df\["C"\] = pd.Series(\[True, True\])

</div>

*Previous behavior*:

`` `ipython     In [5]: df.all(bool_only=True)     Out[5]:     C    True     dtype: bool      In [6]: df[["B", "C"]].all(bool_only=True)     Out[6]:     B    False     C    True     dtype: bool  *New behavior*:  .. ipython:: python    :okwarning:      In [5]: df.all(bool_only=True)      In [6]: df[["B", "C"]].all(bool_only=True)   Other DataFrame reductions with ``numeric\_only=None`will also avoid`<span class="title-ref"> this pathological behavior (:issue:\`37827</span>):

<div class="ipython">

python

df = pd.DataFrame({"A": \[0, 1, 2\], "B": \["a", "b", "c"\]}, dtype=object)

</div>

*Previous behavior*:

`` `ipython     In [3]: df.mean()     Out[3]: Series([], dtype: float64)      In [4]: df[["A"]].mean()     Out[4]:     A    1.0     dtype: float64  *New behavior*:  .. code-block:: ipython      In [3]: df.mean()     Out[3]:     A    1.0     dtype: float64      In [4]: df[["A"]].mean()     Out[4]:     A    1.0     dtype: float64  Moreover, DataFrame reductions with ``numeric\_only=None`will now be`<span class="title-ref"> consistent with their Series counterparts. In particular, for reductions where the Series method raises </span><span class="title-ref">TypeError</span><span class="title-ref">, the DataFrame reduction will now consider that column non-numeric instead of casting to a NumPy array which may have different semantics (:issue:\`36076</span>, `28949`, `21020`).

<div class="ipython" data-okwarning="">

python

ser = pd.Series(\[0, 1\], dtype="category", name="A") df = ser.to\_frame()

</div>

*Previous behavior*:

`` `ipython     In [5]: df.any()     Out[5]:     A    True     dtype: bool  *New behavior*:  .. code-block:: ipython      In [5]: df.any()     Out[5]: Series([], dtype: bool)   .. _whatsnew_120.api_breaking.python:  Increased minimum version for Python ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 1.2.0 supports Python 3.7.1 and higher (`35214`).

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated (`35214`). If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.16.5</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytz</td>
<td>2017.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.7.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.2.1</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.6.8</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>5.0.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>mypy (dev)</td>
<td>0.782</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.6.0</td>
<td></td>
</tr>
<tr class="even">
<td>fastparquet</td>
<td>0.3.2</td>
<td></td>
</tr>
<tr class="odd">
<td>fsspec</td>
<td>0.7.4</td>
<td></td>
</tr>
<tr class="even">
<td>gcsfs</td>
<td>0.6.0</td>
<td></td>
</tr>
<tr class="odd">
<td>lxml</td>
<td>4.3.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>2.2.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>numba</td>
<td>0.46.0</td>
<td></td>
</tr>
<tr class="even">
<td>openpyxl</td>
<td>2.6.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pyarrow</td>
<td>0.15.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pymysql</td>
<td>0.7.11</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pytables</td>
<td>3.5.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>s3fs</td>
<td>0.4.0</td>
<td></td>
</tr>
<tr class="odd">
<td>scipy</td>
<td>1.2.0</td>
<td></td>
</tr>
<tr class="even">
<td>sqlalchemy</td>
<td>1.2.8</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xarray</td>
<td>0.12.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xlrd</td>
<td>1.2.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xlsxwriter</td>
<td>1.0.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xlwt</td>
<td>1.3.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pandas-gbq</td>
<td>0.12.0</td>
<td></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - Sorting in descending order is now stable for <span class="title-ref">Series.sort\_values</span> and <span class="title-ref">Index.sort\_values</span> for Datetime-like <span class="title-ref">Index</span> subclasses. This will affect sort order when sorting a DataFrame on multiple columns, sorting with a key function that produces duplicates, or requesting the sorting index when using <span class="title-ref">Index.sort\_values</span>. When using <span class="title-ref">Series.value\_counts</span>, the count of missing values is no longer necessarily last in the list of duplicate counts. Instead, its position corresponds to the position in the original Series. When using <span class="title-ref">Index.sort\_values</span> for Datetime-like <span class="title-ref">Index</span> subclasses, NaTs ignored the `na_position` argument and were sorted to the beginning. Now they respect `na_position`, the default being `last`, same as other <span class="title-ref">Index</span> subclasses (`35992`)
  - Passing an invalid `fill_value` to <span class="title-ref">Categorical.take</span>, <span class="title-ref">.DatetimeArray.take</span>, <span class="title-ref">TimedeltaArray.take</span>, or <span class="title-ref">PeriodArray.take</span> now raises a `TypeError` instead of a `ValueError` (`37733`)
  - Passing an invalid `fill_value` to <span class="title-ref">Series.shift</span> with a `CategoricalDtype` now raises a `TypeError` instead of a `ValueError` (`37733`)
  - Passing an invalid value to <span class="title-ref">IntervalIndex.insert</span> or <span class="title-ref">CategoricalIndex.insert</span> now raises a `TypeError` instead of a `ValueError` (`37733`)
  - Attempting to reindex a Series with a <span class="title-ref">CategoricalIndex</span> with an invalid `fill_value` now raises a `TypeError` instead of a `ValueError` (`37733`)
  - <span class="title-ref">CategoricalIndex.append</span> with an index that contains non-category values will now cast instead of raising `TypeError` (`38098`)

## Deprecations

  - Deprecated parameter `inplace` in <span class="title-ref">MultiIndex.set\_codes</span> and <span class="title-ref">MultiIndex.set\_levels</span> (`35626`)
  - Deprecated parameter `dtype` of method <span class="title-ref">\~Index.copy</span> for all <span class="title-ref">Index</span> subclasses. Use the <span class="title-ref">\~Index.astype</span> method instead for changing dtype (`35853`)
  - Deprecated parameters `levels` and `codes` in <span class="title-ref">MultiIndex.copy</span>. Use the <span class="title-ref">\~MultiIndex.set\_levels</span> and <span class="title-ref">\~MultiIndex.set\_codes</span> methods instead (`36685`)
  - Date parser functions <span class="title-ref">\~pandas.io.date\_converters.parse\_date\_time</span>, <span class="title-ref">\~pandas.io.date\_converters.parse\_date\_fields</span>, <span class="title-ref">\~pandas.io.date\_converters.parse\_all\_fields</span> and <span class="title-ref">\~pandas.io.date\_converters.generic\_parser</span> from `pandas.io.date_converters` are deprecated and will be removed in a future version; use <span class="title-ref">to\_datetime</span> instead (`35741`)
  - <span class="title-ref">DataFrame.lookup</span> is deprecated and will be removed in a future version, use <span class="title-ref">DataFrame.melt</span> and <span class="title-ref">DataFrame.loc</span> instead (`35224`)
  - The method <span class="title-ref">Index.to\_native\_types</span> is deprecated. Use `.astype(str)` instead (`28867`)
  - Deprecated indexing <span class="title-ref">DataFrame</span> rows with a single datetime-like string as `df[string]` (given the ambiguity whether it is indexing the rows or selecting a column), use `df.loc[string]` instead (`36179`)
  - Deprecated <span class="title-ref">Index.is\_all\_dates</span> (`27744`)
  - The default value of `regex` for <span class="title-ref">Series.str.replace</span> will change from `True` to `False` in a future release. In addition, single character regular expressions will *not* be treated as literal strings when `regex=True` is set (`24804`)
  - Deprecated automatic alignment on comparison operations between <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span>, do `frame, ser = frame.align(ser, axis=1, copy=False)` before e.g. `frame == ser` (`28759`)
  - <span class="title-ref">Rolling.count</span> with `min_periods=None` will default to the size of the window in a future version (`31302`)
  - Using "outer" ufuncs on DataFrames to return 4d ndarray is now deprecated. Convert to an ndarray first (`23743`)
  - Deprecated slice-indexing on tz-aware <span class="title-ref">DatetimeIndex</span> with naive `datetime` objects, to match scalar indexing behavior (`36148`)
  - <span class="title-ref">Index.ravel</span> returning a `np.ndarray` is deprecated, in the future this will return a view on the same index (`19956`)
  - Deprecate use of strings denoting units with 'M', 'Y' or 'y' in <span class="title-ref">\~pandas.to\_timedelta</span> (`36666`)
  - <span class="title-ref">Index</span> methods `&`, `|`, and `^` behaving as the set operations <span class="title-ref">Index.intersection</span>, <span class="title-ref">Index.union</span>, and <span class="title-ref">Index.symmetric\_difference</span>, respectively, are deprecated and in the future will behave as pointwise boolean operations matching <span class="title-ref">Series</span> behavior. Use the named set methods instead (`36758`)
  - <span class="title-ref">Categorical.is\_dtype\_equal</span> and <span class="title-ref">CategoricalIndex.is\_dtype\_equal</span> are deprecated, will be removed in a future version (`37545`)
  - <span class="title-ref">Series.slice\_shift</span> and <span class="title-ref">DataFrame.slice\_shift</span> are deprecated, use <span class="title-ref">Series.shift</span> or <span class="title-ref">DataFrame.shift</span> instead (`37601`)
  - Partial slicing on unordered <span class="title-ref">.DatetimeIndex</span> objects with keys that are not in the index is deprecated and will be removed in a future version (`18531`)
  - The `how` keyword in <span class="title-ref">PeriodIndex.astype</span> is deprecated and will be removed in a future version, use `index.to_timestamp(how=how)` instead (`37982`)
  - Deprecated <span class="title-ref">Index.asi8</span> for <span class="title-ref">Index</span> subclasses other than <span class="title-ref">.DatetimeIndex</span>, <span class="title-ref">.TimedeltaIndex</span>, and <span class="title-ref">PeriodIndex</span> (`37877`)
  - The `inplace` parameter of <span class="title-ref">Categorical.remove\_unused\_categories</span> is deprecated and will be removed in a future version (`37643`)
  - The `null_counts` parameter of <span class="title-ref">DataFrame.info</span> is deprecated and replaced by `show_counts`. It will be removed in a future version (`37999`)

**Calling NumPy ufuncs on non-aligned DataFrames**

Calling NumPy ufuncs on non-aligned DataFrames changed behaviour in pandas 1.2.0 (to align the inputs before calling the ufunc), but this change is reverted in pandas 1.2.1. The behaviour to not align is now deprecated instead, see the \[the 1.2.1 release notes \<whatsnew\_121.ufunc\_deprecation\>\](\#the-1.2.1-release-notes-\<whatsnew\_121.ufunc\_deprecation\>) for more details.

## Performance improvements

  - Performance improvements when creating DataFrame or Series with dtype `str` or <span class="title-ref">StringDtype</span> from array with many string elements (`36304`, `36317`, `36325`, `36432`, `37371`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span> with the `numba` engine (`35759`)
  - Performance improvements when creating <span class="title-ref">Series.map</span> from a huge dictionary (`34717`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> with the `numba` engine (`36240`)
  - <span class="title-ref">.Styler</span> uuid method altered to compress data transmission over web whilst maintaining reasonably low table collision probability (`36345`)
  - Performance improvement in <span class="title-ref">to\_datetime</span> with non-ns time unit for `float` `dtype` columns (`20445`)
  - Performance improvement in setting values on an <span class="title-ref">IntervalArray</span> (`36310`)
  - The internal index method <span class="title-ref">\~Index.\_shallow\_copy</span> now makes the new index and original index share cached attributes, avoiding creating these again, if created on either. This can speed up operations that depend on creating copies of existing indexes (`36840`)
  - Performance improvement in <span class="title-ref">.RollingGroupby.count</span> (`35625`)
  - Small performance decrease to <span class="title-ref">.Rolling.min</span> and <span class="title-ref">.Rolling.max</span> for fixed windows (`36567`)
  - Reduced peak memory usage in <span class="title-ref">DataFrame.to\_pickle</span> when using `protocol=5` in python 3.8+ (`34244`)
  - Faster `dir` calls when the object has many index labels, e.g. `dir(ser)` (`37450`)
  - Performance improvement in <span class="title-ref">ExpandingGroupby</span> (`37064`)
  - Performance improvement in <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> for <span class="title-ref">Categorical</span> (`8628`)
  - Performance improvement in <span class="title-ref">DataFrame.groupby</span> for `float` `dtype` (`28303`), changes of the underlying hash-function can lead to changes in float based indexes sort ordering for ties (e.g. <span class="title-ref">Index.value\_counts</span>)
  - Performance improvement in <span class="title-ref">pd.isin</span> for inputs with more than 1e6 elements (`36611`)
  - Performance improvement for <span class="title-ref">DataFrame.\_\_setitem\_\_</span> with list-like indexers (`37954`)
  - <span class="title-ref">read\_json</span> now avoids reading entire file into memory when chunksize is specified (`34548`)

## Bug fixes

### Categorical

  - <span class="title-ref">Categorical.fillna</span> will always return a copy, validate a passed fill value regardless of whether there are any NAs to fill, and disallow an `NaT` as a fill value for numeric categories (`36530`)
  - Bug in <span class="title-ref">Categorical.\_\_setitem\_\_</span> that incorrectly raised when trying to set a tuple value (`20439`)
  - Bug in <span class="title-ref">CategoricalIndex.equals</span> incorrectly casting non-category entries to `np.nan` (`37667`)
  - Bug in <span class="title-ref">CategoricalIndex.where</span> incorrectly setting non-category entries to `np.nan` instead of raising `TypeError` (`37977`)
  - Bug in <span class="title-ref">Categorical.to\_numpy</span> and `np.array(categorical)` with tz-aware `datetime64` categories incorrectly dropping the time zone information instead of casting to object dtype (`38136`)

### Datetime-like

  - Bug in <span class="title-ref">DataFrame.combine\_first</span> that would convert datetime-like column on other <span class="title-ref">DataFrame</span> to integer when the column is not present in original <span class="title-ref">DataFrame</span> (`28481`)
  - Bug in <span class="title-ref">.DatetimeArray.date</span> where a `ValueError` would be raised with a read-only backing array (`33530`)
  - Bug in `NaT` comparisons failing to raise `TypeError` on invalid inequality comparisons (`35046`)
  - Bug in <span class="title-ref">.DateOffset</span> where attributes reconstructed from pickle files differ from original objects when input values exceed normal ranges (e.g. months=12) (`34511`)
  - Bug in <span class="title-ref">.DatetimeIndex.get\_slice\_bound</span> where `datetime.date` objects were not accepted or naive <span class="title-ref">Timestamp</span> with a tz-aware <span class="title-ref">.DatetimeIndex</span> (`35690`)
  - Bug in <span class="title-ref">.DatetimeIndex.slice\_locs</span> where `datetime.date` objects were not accepted (`34077`)
  - Bug in <span class="title-ref">.DatetimeIndex.searchsorted</span>, <span class="title-ref">.TimedeltaIndex.searchsorted</span>, <span class="title-ref">PeriodIndex.searchsorted</span>, and <span class="title-ref">Series.searchsorted</span> with `datetime64`, `timedelta64` or <span class="title-ref">Period</span> dtype placement of `NaT` values being inconsistent with NumPy (`36176`, `36254`)
  - Inconsistency in <span class="title-ref">.DatetimeArray</span>, <span class="title-ref">.TimedeltaArray</span>, and <span class="title-ref">.PeriodArray</span> method `__setitem__` casting arrays of strings to datetime-like scalars but not scalar strings (`36261`)
  - Bug in <span class="title-ref">.DatetimeArray.take</span> incorrectly allowing `fill_value` with a mismatched time zone (`37356`)
  - Bug in <span class="title-ref">.DatetimeIndex.shift</span> incorrectly raising when shifting empty indexes (`14811`)
  - <span class="title-ref">Timestamp</span> and <span class="title-ref">.DatetimeIndex</span> comparisons between tz-aware and tz-naive objects now follow the standard library `datetime` behavior, returning `True`/`False` for `!=`/`==` and raising for inequality comparisons (`28507`)
  - Bug in <span class="title-ref">.DatetimeIndex.equals</span> and <span class="title-ref">.TimedeltaIndex.equals</span> incorrectly considering `int64` indexes as equal (`36744`)
  - <span class="title-ref">Series.to\_json</span>, <span class="title-ref">DataFrame.to\_json</span>, and <span class="title-ref">read\_json</span> now implement time zone parsing when orient structure is `table` (`35973`)
  - <span class="title-ref">astype</span> now attempts to convert to `datetime64[ns, tz]` directly from `object` with inferred time zone from string (`35973`)
  - Bug in <span class="title-ref">.TimedeltaIndex.sum</span> and <span class="title-ref">Series.sum</span> with `timedelta64` dtype on an empty index or series returning `NaT` instead of `Timedelta(0)` (`31751`)
  - Bug in <span class="title-ref">.DatetimeArray.shift</span> incorrectly allowing `fill_value` with a mismatched time zone (`37299`)
  - Bug in adding a <span class="title-ref">.BusinessDay</span> with nonzero `offset` to a non-scalar other (`37457`)
  - Bug in <span class="title-ref">to\_datetime</span> with a read-only array incorrectly raising (`34857`)
  - Bug in <span class="title-ref">Series.isin</span> with `datetime64[ns]` dtype and <span class="title-ref">.DatetimeIndex.isin</span> incorrectly casting integers to datetimes (`36621`)
  - Bug in <span class="title-ref">Series.isin</span> with `datetime64[ns]` dtype and <span class="title-ref">.DatetimeIndex.isin</span> failing to consider tz-aware and tz-naive datetimes as always different (`35728`)
  - Bug in <span class="title-ref">Series.isin</span> with `PeriodDtype` dtype and <span class="title-ref">PeriodIndex.isin</span> failing to consider arguments with different `PeriodDtype` as always different (`37528`)
  - Bug in <span class="title-ref">Period</span> constructor now correctly handles nanoseconds in the `value` argument (`34621` and `17053`)

### Timedelta

  - Bug in <span class="title-ref">.TimedeltaIndex</span>, <span class="title-ref">Series</span>, and <span class="title-ref">DataFrame</span> floor-division with `timedelta64` dtypes and `NaT` in the denominator (`35529`)
  - Bug in parsing of ISO 8601 durations in <span class="title-ref">Timedelta</span> and <span class="title-ref">to\_datetime</span> (`29773`, `36204`)
  - Bug in <span class="title-ref">to\_timedelta</span> with a read-only array incorrectly raising (`34857`)
  - Bug in <span class="title-ref">Timedelta</span> incorrectly truncating to sub-second portion of a string input when it has precision higher than nanoseconds (`36738`)

### Timezones

  - Bug in <span class="title-ref">date\_range</span> was raising `AmbiguousTimeError` for valid input with `ambiguous=False` (`35297`)
  - Bug in <span class="title-ref">Timestamp.replace</span> was losing fold information (`37610`)

### Numeric

  - Bug in <span class="title-ref">to\_numeric</span> where float precision was incorrect (`31364`)
  - Bug in <span class="title-ref">DataFrame.any</span> with `axis=1` and `bool_only=True` ignoring the `bool_only` keyword (`32432`)
  - Bug in <span class="title-ref">Series.equals</span> where a `ValueError` was raised when NumPy arrays were compared to scalars (`35267`)
  - Bug in <span class="title-ref">Series</span> where two Series each have a <span class="title-ref">.DatetimeIndex</span> with different time zones having those indexes incorrectly changed when performing arithmetic operations (`33671`)
  - Bug in `pandas.testing` module functions when used with `check_exact=False` on complex numeric types (`28235`)
  - Bug in <span class="title-ref">DataFrame.\_\_rmatmul\_\_</span> error handling reporting transposed shapes (`21581`)
  - Bug in <span class="title-ref">Series</span> flex arithmetic methods where the result when operating with a `list`, `tuple` or `np.ndarray` would have an incorrect name (`36760`)
  - Bug in <span class="title-ref">.IntegerArray</span> multiplication with `timedelta` and `np.timedelta64` objects (`36870`)
  - Bug in <span class="title-ref">MultiIndex</span> comparison with tuple incorrectly treating tuple as array-like (`21517`)
  - Bug in <span class="title-ref">DataFrame.diff</span> with `datetime64` dtypes including `NaT` values failing to fill `NaT` results correctly (`32441`)
  - Bug in <span class="title-ref">DataFrame</span> arithmetic ops incorrectly accepting keyword arguments (`36843`)
  - Bug in <span class="title-ref">.IntervalArray</span> comparisons with <span class="title-ref">Series</span> not returning Series (`36908`)
  - Bug in <span class="title-ref">DataFrame</span> allowing arithmetic operations with list of array-likes with undefined results. Behavior changed to raising `ValueError` (`36702`)
  - Bug in <span class="title-ref">DataFrame.std</span> with `timedelta64` dtype and `skipna=False` (`37392`)
  - Bug in <span class="title-ref">DataFrame.min</span> and <span class="title-ref">DataFrame.max</span> with `datetime64` dtype and `skipna=False` (`36907`)
  - Bug in <span class="title-ref">DataFrame.idxmax</span> and <span class="title-ref">DataFrame.idxmin</span> with mixed dtypes incorrectly raising `TypeError` (`38195`)

### Conversion

  - Bug in <span class="title-ref">DataFrame.to\_dict</span> with `orient='records'` now returns python native datetime objects for datetime-like columns (`21256`)
  - Bug in <span class="title-ref">Series.astype</span> conversion from `string` to `float` raised in presence of `pd.NA` values (`37626`)

### Strings

  - Bug in <span class="title-ref">Series.to\_string</span>, <span class="title-ref">DataFrame.to\_string</span>, and <span class="title-ref">DataFrame.to\_latex</span> adding a leading space when `index=False` (`24980`)
  - Bug in <span class="title-ref">to\_numeric</span> raising a `TypeError` when attempting to convert a string dtype Series containing only numeric strings and `NA` (`37262`)

### Interval

  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> where <span class="title-ref">Interval</span> dtypes would be converted to object dtypes (`34871`)
  - Bug in <span class="title-ref">IntervalIndex.take</span> with negative indices and `fill_value=None` (`37330`)
  - Bug in <span class="title-ref">IntervalIndex.putmask</span> with datetime-like dtype incorrectly casting to object dtype (`37968`)
  - Bug in <span class="title-ref">IntervalArray.astype</span> incorrectly dropping dtype information with a <span class="title-ref">CategoricalDtype</span> object (`37984`)

### Indexing

  - Bug in <span class="title-ref">PeriodIndex.get\_loc</span> incorrectly raising `ValueError` on non-datelike strings instead of `KeyError`, causing similar errors in <span class="title-ref">Series.\_\_getitem\_\_</span>, <span class="title-ref">Series.\_\_contains\_\_</span>, and <span class="title-ref">Series.loc.\_\_getitem\_\_</span> (`34240`)
  - Bug in <span class="title-ref">Index.sort\_values</span> where, when empty values were passed, the method would break by trying to compare missing values instead of pushing them to the end of the sort order (`35584`)
  - Bug in <span class="title-ref">Index.get\_indexer</span> and <span class="title-ref">Index.get\_indexer\_non\_unique</span> where `int64` arrays are returned instead of `intp` (`36359`)
  - Bug in <span class="title-ref">DataFrame.sort\_index</span> where parameter ascending passed as a list on a single level index gives wrong result (`32334`)
  - Bug in <span class="title-ref">DataFrame.reset\_index</span> was incorrectly raising a `ValueError` for input with a <span class="title-ref">MultiIndex</span> with missing values in a level with `Categorical` dtype (`24206`)
  - Bug in indexing with boolean masks on datetime-like values sometimes returning a view instead of a copy (`36210`)
  - Bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> and <span class="title-ref">DataFrame.loc.\_\_getitem\_\_</span> with <span class="title-ref">IntervalIndex</span> columns and a numeric indexer (`26490`)
  - Bug in <span class="title-ref">Series.loc.\_\_getitem\_\_</span> with a non-unique <span class="title-ref">MultiIndex</span> and an empty-list indexer (`13691`)
  - Bug in indexing on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> and a level named `"0"` (`37194`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> when using an unsigned integer array as an indexer giving incorrect results or segfaulting instead of raising `KeyError` (`37218`)
  - Bug in <span class="title-ref">Index.where</span> incorrectly casting numeric values to strings (`37591`)
  - Bug in <span class="title-ref">DataFrame.loc</span> returning empty result when indexer is a slice with negative step size (`38071`)
  - Bug in <span class="title-ref">Series.loc</span> and <span class="title-ref">DataFrame.loc</span> raises when the index was of `object` dtype and the given numeric label was in the index (`26491`)
  - Bug in <span class="title-ref">DataFrame.loc</span> returned requested key plus missing values when `loc` was applied to single level from a <span class="title-ref">MultiIndex</span> (`27104`)
  - Bug in indexing on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a <span class="title-ref">CategoricalIndex</span> using a list-like indexer containing NA values (`37722`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> expanding an empty <span class="title-ref">DataFrame</span> with mixed dtypes (`37932`)
  - Bug in <span class="title-ref">DataFrame.xs</span> ignored `droplevel=False` for columns (`19056`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> raising `IndexingError` wrongly for empty DataFrame with `tolerance` not `None` or `method="nearest"` (`27315`)
  - Bug in indexing on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a <span class="title-ref">CategoricalIndex</span> using list-like indexer that contains elements that are in the index's `categories` but not in the index itself failing to raise `KeyError` (`37901`)
  - Bug on inserting a boolean label into a <span class="title-ref">DataFrame</span> with a numeric <span class="title-ref">Index</span> columns incorrectly casting to integer (`36319`)
  - Bug in <span class="title-ref">DataFrame.iloc</span> and <span class="title-ref">Series.iloc</span> aligning objects in `__setitem__` (`22046`)
  - Bug in <span class="title-ref">MultiIndex.drop</span> does not raise if labels are partially found (`37820`)
  - Bug in <span class="title-ref">DataFrame.loc</span> did not raise `KeyError` when missing combination was given with `slice(None)` for remaining levels (`19556`)
  - Bug in <span class="title-ref">DataFrame.loc</span> raising `TypeError` when non-integer slice was given to select values from <span class="title-ref">MultiIndex</span> (`25165`, `24263`)
  - Bug in <span class="title-ref">Series.at</span> returning <span class="title-ref">Series</span> with one element instead of scalar when index is a <span class="title-ref">MultiIndex</span> with one level (`38053`)
  - Bug in <span class="title-ref">DataFrame.loc</span> returning and assigning elements in wrong order when indexer is differently ordered than the <span class="title-ref">MultiIndex</span> to filter (`31330`, `34603`)
  - Bug in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">DataFrame.\_\_getitem\_\_</span> raising `KeyError` when columns were <span class="title-ref">MultiIndex</span> with only one level (`29749`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">DataFrame.\_\_getitem\_\_</span> raising blank `KeyError` without missing keys for <span class="title-ref">IntervalIndex</span> (`27365`)
  - Bug in setting a new label on a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with a <span class="title-ref">CategoricalIndex</span> incorrectly raising `TypeError` when the new label is not among the index's categories (`38098`)
  - Bug in <span class="title-ref">Series.loc</span> and <span class="title-ref">Series.iloc</span> raising `ValueError` when inserting a list-like `np.array`, `list` or `tuple` in an `object` Series of equal length (`37748`, `37486`)
  - Bug in <span class="title-ref">Series.loc</span> and <span class="title-ref">Series.iloc</span> setting all the values of an `object` Series with those of a list-like `ExtensionArray` instead of inserting it (`38271`)

### Missing

  - Bug in <span class="title-ref">.SeriesGroupBy.transform</span> now correctly handles missing values for `dropna=False` (`35014`)
  - Bug in <span class="title-ref">Series.nunique</span> with `dropna=True` was returning incorrect results when both `NA` and `None` missing values were present (`37566`)
  - Bug in <span class="title-ref">Series.interpolate</span> where kwarg `limit_area` and `limit_direction` had no effect when using methods `pad` and `backfill` (`31048`)

### MultiIndex

  - Bug in <span class="title-ref">DataFrame.xs</span> when used with <span class="title-ref">IndexSlice</span> raises `TypeError` with message `"Expected label or tuple of labels"` (`35301`)
  - Bug in <span class="title-ref">DataFrame.reset\_index</span> with `NaT` values in index raises `ValueError` with message `"cannot convert float NaN to integer"` (`36541`)
  - Bug in <span class="title-ref">DataFrame.combine\_first</span> when used with <span class="title-ref">MultiIndex</span> containing string and `NaN` values raises `TypeError` (`36562`)
  - Bug in <span class="title-ref">MultiIndex.drop</span> dropped `NaN` values when non existing key was given as input (`18853`)
  - Bug in <span class="title-ref">MultiIndex.drop</span> dropping more values than expected when index has duplicates and is not sorted (`33494`)

### I/O

  - <span class="title-ref">read\_sas</span> no longer leaks resources on failure (`35566`)
  - Bug in <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span> caused a `ValueError` when it was called with a filename in combination with `mode` containing a `b` (`35058`)
  - Bug in <span class="title-ref">read\_csv</span> with `float_precision='round_trip'` did not handle `decimal` and `thousands` parameters (`35365`)
  - <span class="title-ref">to\_pickle</span> and <span class="title-ref">read\_pickle</span> were closing user-provided file objects (`35679`)
  - <span class="title-ref">to\_csv</span> passes compression arguments for `'gzip'` always to `gzip.GzipFile` (`28103`)
  - <span class="title-ref">to\_csv</span> did not support zip compression for binary file object not having a filename (`35058`)
  - <span class="title-ref">to\_csv</span> and <span class="title-ref">read\_csv</span> did not honor `compression` and `encoding` for path-like objects that are internally converted to file-like objects (`35677`, `26124`, `32392`)
  - <span class="title-ref">DataFrame.to\_pickle</span>, <span class="title-ref">Series.to\_pickle</span>, and <span class="title-ref">read\_pickle</span> did not support compression for file-objects (`26237`, `29054`, `29570`)
  - Bug in <span class="title-ref">LongTableBuilder.middle\_separator</span> was duplicating LaTeX longtable entries in the List of Tables of a LaTeX document (`34360`)
  - Bug in <span class="title-ref">read\_csv</span> with `engine='python'` truncating data if multiple items present in first row and first element started with BOM (`36343`)
  - Removed `private_key` and `verbose` from <span class="title-ref">read\_gbq</span> as they are no longer supported in `pandas-gbq` (`34654`, `30200`)
  - Bumped minimum pytables version to 3.5.1 to avoid a `ValueError` in <span class="title-ref">read\_hdf</span> (`24839`)
  - Bug in <span class="title-ref">read\_table</span> and <span class="title-ref">read\_csv</span> when `delim_whitespace=True` and `sep=default` (`36583`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> and <span class="title-ref">Series.to\_json</span> when used with `lines=True` and `orient='records'` the last line of the record is not appended with 'new line character' (`36888`)
  - Bug in <span class="title-ref">read\_parquet</span> with fixed offset time zones. String representation of time zones was not recognized (`35997`, `36004`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span>, <span class="title-ref">DataFrame.to\_string</span>, and <span class="title-ref">DataFrame.to\_latex</span> ignoring the `na_rep` argument when `float_format` was also specified (`9046`, `13828`)
  - Bug in output rendering of complex numbers showing too many trailing zeros (`36799`)
  - Bug in <span class="title-ref">HDFStore</span> threw a `TypeError` when exporting an empty DataFrame with `datetime64[ns, tz]` dtypes with a fixed HDF5 store (`20594`)
  - Bug in <span class="title-ref">HDFStore</span> was dropping time zone information when exporting a Series with `datetime64[ns, tz]` dtypes with a fixed HDF5 store (`20594`)
  - <span class="title-ref">read\_csv</span> was closing user-provided binary file handles when `engine="c"` and an `encoding` was requested (`36980`)
  - Bug in <span class="title-ref">DataFrame.to\_hdf</span> was not dropping missing rows with `dropna=True` (`35719`)
  - Bug in <span class="title-ref">read\_html</span> was raising a `TypeError` when supplying a `pathlib.Path` argument to the `io` parameter (`37705`)
  - <span class="title-ref">DataFrame.to\_excel</span>, <span class="title-ref">Series.to\_excel</span>, <span class="title-ref">DataFrame.to\_markdown</span>, and <span class="title-ref">Series.to\_markdown</span> now support writing to fsspec URLs such as S3 and Google Cloud Storage (`33987`)
  - Bug in <span class="title-ref">read\_fwf</span> with `skip_blank_lines=True` was not skipping blank lines (`37758`)
  - Parse missing values using <span class="title-ref">read\_json</span> with `dtype=False` to `NaN` instead of `None` (`28501`)
  - <span class="title-ref">read\_fwf</span> was inferring compression with `compression=None` which was not consistent with the other `read_*` functions (`37909`)
  - <span class="title-ref">DataFrame.to\_html</span> was ignoring `formatters` argument for `ExtensionDtype` columns (`36525`)
  - Bumped minimum xarray version to 0.12.3 to avoid reference to the removed `Panel` class (`27101`, `37983`)
  - <span class="title-ref">DataFrame.to\_csv</span> was re-opening file-like handles that also implement `os.PathLike` (`38125`)
  - Bug in the conversion of a sliced `pyarrow.Table` with missing values to a DataFrame (`38525`)
  - Bug in <span class="title-ref">read\_sql\_table</span> raising a `sqlalchemy.exc.OperationalError` when column names contained a percentage sign (`37517`)

### Period

  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> where <span class="title-ref">Period</span> dtypes would be converted to object dtypes (`34871`)

### Plotting

  - Bug in <span class="title-ref">DataFrame.plot</span> was rotating xticklabels when `subplots=True`, even if the x-axis wasn't an irregular time series (`29460`)
  - Bug in <span class="title-ref">DataFrame.plot</span> where a marker letter in the `style` keyword sometimes caused a `ValueError` (`21003`)
  - Bug in <span class="title-ref">DataFrame.plot.bar</span> and <span class="title-ref">Series.plot.bar</span> where ticks positions were assigned by value order instead of using the actual value for numeric or a smart ordering for string (`26186`, `11465`). This fix has been reverted in pandas 1.2.1, see \[v1.2.1\](v1.2.1.md)
  - Twinned axes were losing their tick labels which should only happen to all but the last row or column of 'externally' shared axes (`33819`)
  - Bug in <span class="title-ref">Series.plot</span> and <span class="title-ref">DataFrame.plot</span> was throwing a <span class="title-ref">ValueError</span> when the Series or DataFrame was indexed by a <span class="title-ref">.TimedeltaIndex</span> with a fixed frequency and the x-axis lower limit was greater than the upper limit (`37454`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.boxplot</span> when `subplots=False` would raise a `KeyError` (`16748`)
  - Bug in <span class="title-ref">DataFrame.plot</span> and <span class="title-ref">Series.plot</span> was overwriting matplotlib's shared y axes behavior when no `sharey` parameter was passed (`37942`)
  - Bug in <span class="title-ref">DataFrame.plot</span> was raising a `TypeError` with `ExtensionDtype` columns (`32073`)

### Styler

  - Bug in <span class="title-ref">Styler.render</span> HTML was generated incorrectly because of formatting error in `rowspan` attribute, it now matches with w3 syntax (`38234`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">.DataFrameGroupBy.count</span> and <span class="title-ref">SeriesGroupBy.sum</span> returning `NaN` for missing categories when grouped on multiple `Categoricals`. Now returning `0` (`35028`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> that would sometimes throw an erroneous `ValueError` if the grouping axis had duplicate entries (`16646`)
  - Bug in <span class="title-ref">DataFrame.resample</span> that would throw a `ValueError` when resampling from `"D"` to `"24H"` over a transition into daylight savings time (DST) (`35219`)
  - Bug when combining methods <span class="title-ref">DataFrame.groupby</span> with <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">DataFrame.interpolate</span> raising a `TypeError` (`35325`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> where a non-nuisance grouping column would be dropped from the output columns if another groupby method was called before `.apply` (`34656`)
  - Bug when subsetting columns on a <span class="title-ref">.DataFrameGroupBy</span> (e.g. `df.groupby('a')[['b']])`) would reset the attributes `axis`, `dropna`, `group_keys`, `level`, `mutated`, `sort`, and `squeeze` to their default values (`9959`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.tshift</span> failing to raise `ValueError` when a frequency cannot be inferred for the index of a group (`35937`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> does not always maintain column index name for `any`, `all`, `bfill`, `ffill`, `shift` (`29764`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> raising error with `np.nan` group(s) when `dropna=False` (`35889`)
  - Bug in <span class="title-ref">.Rolling.sum</span> returned wrong values when dtypes where mixed between float and integer and `axis=1` (`20649`, `35596`)
  - Bug in <span class="title-ref">.Rolling.count</span> returned `np.nan` with <span class="title-ref">\~pandas.api.indexers.FixedForwardWindowIndexer</span> as window, `min_periods=0` and only missing values in the window (`35579`)
  - Bug where <span class="title-ref">.Rolling</span> produces incorrect window sizes when using a `PeriodIndex` (`34225`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.ffill</span> and <span class="title-ref">.DataFrameGroupBy.bfill</span> where a `NaN` group would return filled values instead of `NaN` when `dropna=True` (`34725`)
  - Bug in <span class="title-ref">.RollingGroupby.count</span> where a `ValueError` was raised when specifying the `closed` parameter (`35869`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.rolling</span> returning wrong values with partial centered window (`36040`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.rolling</span> returned wrong values with time aware window containing `NaN`. Raises `ValueError` because windows are not monotonic now (`34617`)
  - Bug in <span class="title-ref">.Rolling.\_\_iter\_\_</span> where a `ValueError` was not raised when `min_periods` was larger than `window` (`37156`)
  - Using <span class="title-ref">.Rolling.var</span> instead of <span class="title-ref">.Rolling.std</span> avoids numerical issues for <span class="title-ref">.Rolling.corr</span> when <span class="title-ref">.Rolling.var</span> is still within floating point precision while <span class="title-ref">.Rolling.std</span> is not (`31286`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.Resampler.quantile</span> raised `TypeError` when values were of type `Timedelta` (`29485`)
  - Bug in <span class="title-ref">.Rolling.median</span> and <span class="title-ref">.Rolling.quantile</span> returned wrong values for <span class="title-ref">.BaseIndexer</span> subclasses with non-monotonic starting or ending points for windows (`37153`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> dropped `nan` groups from result with `dropna=False` when grouping over a single column (`35646`, `35542`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.head</span>, <span class="title-ref">DataFrameGroupBy.tail</span>, <span class="title-ref">SeriesGroupBy.head</span>, and <span class="title-ref">SeriesGroupBy.tail</span> would raise when used with `axis=1` (`9772`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> would raise when used with `axis=1` and a transformation kernel (e.g. "shift") (`36308`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.resample</span> using `.agg` with sum produced different result than just calling `.sum` (`33548`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> dropped values on `nan` group when returning the same axes with the original frame (`38227`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> couldn't handle with arraylike `q` when grouping by columns (`33795`)
  - Bug in <span class="title-ref">DataFrameGroupBy.rank</span> with `datetime64tz` or period dtype incorrectly casting results to those dtypes instead of returning `float64` dtype (`38187`)

### Reshaping

  - Bug in <span class="title-ref">DataFrame.crosstab</span> was returning incorrect results on inputs with duplicate row names, duplicate column names or duplicate names between row and column labels (`22529`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> with `aggfunc='count'` or `aggfunc='sum'` returning `NaN` for missing categories when pivoted on a `Categorical`. Now returning `0` (`31422`)
  - Bug in <span class="title-ref">concat</span> and <span class="title-ref">DataFrame</span> constructor where input index names are not preserved in some cases (`13475`)
  - Bug in func <span class="title-ref">crosstab</span> when using multiple columns with `margins=True` and `normalize=True` (`35144`)
  - Bug in <span class="title-ref">DataFrame.stack</span> where an empty DataFrame.stack would raise an error (`36113`). Now returning an empty Series with empty MultiIndex.
  - Bug in <span class="title-ref">Series.unstack</span>. Now a Series with single level of Index trying to unstack would raise a `ValueError` (`36113`)
  - Bug in <span class="title-ref">DataFrame.agg</span> with `func={'name':<FUNC>}` incorrectly raising `TypeError` when `DataFrame.columns==['Name']` (`36212`)
  - Bug in <span class="title-ref">Series.transform</span> would give incorrect results or raise when the argument `func` was a dictionary (`35811`)
  - Bug in <span class="title-ref">DataFrame.pivot</span> did not preserve <span class="title-ref">MultiIndex</span> level names for columns when rows and columns are both multiindexed (`36360`)
  - Bug in <span class="title-ref">DataFrame.pivot</span> modified `index` argument when `columns` was passed but `values` was not (`37635`)
  - Bug in <span class="title-ref">DataFrame.join</span> returned a non deterministic level-order for the resulting <span class="title-ref">MultiIndex</span> (`36910`)
  - Bug in <span class="title-ref">DataFrame.combine\_first</span> caused wrong alignment with dtype `string` and one level of `MultiIndex` containing only `NA` (`37591`)
  - Fixed regression in <span class="title-ref">merge</span> on merging <span class="title-ref">.DatetimeIndex</span> with empty DataFrame (`36895`)
  - Bug in <span class="title-ref">DataFrame.apply</span> not setting index of return value when `func` return type is `dict` (`37544`)
  - Bug in <span class="title-ref">DataFrame.merge</span> and <span class="title-ref">pandas.merge</span> returning inconsistent ordering in result for `how=right` and `how=left` (`35382`)
  - Bug in <span class="title-ref">merge\_ordered</span> couldn't handle list-like `left_by` or `right_by` (`35269`)
  - Bug in <span class="title-ref">merge\_ordered</span> returned wrong join result when length of `left_by` or `right_by` equals to the rows of `left` or `right` (`38166`)
  - Bug in <span class="title-ref">merge\_ordered</span> didn't raise when elements in `left_by` or `right_by` not exist in `left` columns or `right` columns (`38167`)
  - Bug in <span class="title-ref">DataFrame.drop\_duplicates</span> not validating bool dtype for `ignore_index` keyword (`38274`)

### ExtensionArray

  - Fixed bug where <span class="title-ref">DataFrame</span> column set to scalar extension type via a dict instantiation was considered an object type rather than the extension type (`35965`)
  - Fixed bug where `astype()` with equal dtype and `copy=False` would return a new object (`28488`)
  - Fixed bug when applying a NumPy ufunc with multiple outputs to an <span class="title-ref">.IntegerArray</span> returning `None` (`36913`)
  - Fixed an inconsistency in <span class="title-ref">.PeriodArray</span>'s `__init__` signature to those of <span class="title-ref">.DatetimeArray</span> and <span class="title-ref">.TimedeltaArray</span> (`37289`)
  - Reductions for <span class="title-ref">.BooleanArray</span>, <span class="title-ref">.Categorical</span>, <span class="title-ref">.DatetimeArray</span>, <span class="title-ref">.FloatingArray</span>, <span class="title-ref">.IntegerArray</span>, <span class="title-ref">.PeriodArray</span>, <span class="title-ref">.TimedeltaArray</span>, and <span class="title-ref">.PandasArray</span> are now keyword-only methods (`37541`)
  - Fixed a bug where a `TypeError` was wrongly raised if a membership check was made on an `ExtensionArray` containing nan-like values (`37867`)

### Other

  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> incorrectly raising an `AssertionError` instead of a `ValueError` when invalid parameter combinations are passed (`36045`)
  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> with numeric values and string `to_replace` (`34789`)
  - Fixed metadata propagation in <span class="title-ref">Series.abs</span> and ufuncs called on Series and DataFrames (`28283`)
  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> incorrectly casting from `PeriodDtype` to object dtype (`34871`)
  - Fixed bug in metadata propagation incorrectly copying DataFrame columns as metadata when the column name overlaps with the metadata name (`37037`)
  - Fixed metadata propagation in the <span class="title-ref">Series.dt</span>, <span class="title-ref">Series.str</span> accessors, <span class="title-ref">DataFrame.duplicated</span>, <span class="title-ref">DataFrame.stack</span>, <span class="title-ref">DataFrame.unstack</span>, <span class="title-ref">DataFrame.pivot</span>, <span class="title-ref">DataFrame.append</span>, <span class="title-ref">DataFrame.diff</span>, <span class="title-ref">DataFrame.applymap</span> and <span class="title-ref">DataFrame.update</span> methods (`28283`, `37381`)
  - Fixed metadata propagation when selecting columns with `DataFrame.__getitem__` (`28283`)
  - Bug in <span class="title-ref">Index.intersection</span> with non-<span class="title-ref">Index</span> failing to set the correct name on the returned <span class="title-ref">Index</span> (`38111`)
  - Bug in <span class="title-ref">RangeIndex.intersection</span> failing to set the correct name on the returned <span class="title-ref">Index</span> in some corner cases (`38197`)
  - Bug in <span class="title-ref">Index.difference</span> failing to set the correct name on the returned <span class="title-ref">Index</span> in some corner cases (`38268`)
  - Bug in <span class="title-ref">Index.union</span> behaving differently depending on whether operand is an <span class="title-ref">Index</span> or other list-like (`36384`)
  - Bug in <span class="title-ref">Index.intersection</span> with non-matching numeric dtypes casting to `object` dtype instead of minimal common dtype (`38122`)
  - Bug in <span class="title-ref">IntervalIndex.union</span> returning an incorrectly-typed <span class="title-ref">Index</span> when empty (`38282`)
  - Passing an array with 2 or more dimensions to the <span class="title-ref">Series</span> constructor now raises the more specific `ValueError` rather than a bare `Exception` (`35744`)
  - Bug in `dir` where `dir(obj)` wouldn't show attributes defined on the instance for pandas objects (`37173`)
  - Bug in <span class="title-ref">Index.drop</span> raising `InvalidIndexError` when index has duplicates (`38051`)
  - Bug in <span class="title-ref">RangeIndex.difference</span> returning <span class="title-ref">Int64Index</span> in some cases where it should return <span class="title-ref">RangeIndex</span> (`38028`)
  - Fixed bug in <span class="title-ref">assert\_series\_equal</span> when comparing a datetime-like array with an equivalent non extension dtype array (`37609`)
  - Bug in <span class="title-ref">.is\_bool\_dtype</span> would raise when passed a valid string such as `"boolean"` (`38386`)
  - Fixed regression in logical operators raising `ValueError` when columns of <span class="title-ref">DataFrame</span> are a <span class="title-ref">CategoricalIndex</span> with unused categories (`38367`)

## Contributors

<div class="contributors">

v1.1.5..v1.2.0

</div>

---

v1.2.1.md

---

# What's new in 1.2.1 (January 20, 2021)

These are the changes in pandas 1.2.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">\~DataFrame.to\_csv</span> that created corrupted zip files when there were more rows than `chunksize` (`38714`)
  - Fixed regression in <span class="title-ref">\~DataFrame.to\_csv</span> opening `codecs.StreamReaderWriter` in binary mode instead of in text mode (`39247`)
  - Fixed regression in <span class="title-ref">read\_csv</span> and other read functions were the encoding error policy (`errors`) did not default to `"replace"` when no encoding was specified (`38989`)
  - Fixed regression in <span class="title-ref">read\_excel</span> with non-rawbyte file handles (`38788`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_stata</span> not removing the created file when an error occurred (`39202`)
  - Fixed regression in `DataFrame.__setitem__` raising `ValueError` when expanding <span class="title-ref">DataFrame</span> and new column is from type `"0 - name"` (`39010`)
  - Fixed regression in setting with <span class="title-ref">DataFrame.loc</span> raising `ValueError` when <span class="title-ref">DataFrame</span> has unsorted <span class="title-ref">MultiIndex</span> columns and indexer is a scalar (`38601`)
  - Fixed regression in setting with <span class="title-ref">DataFrame.loc</span> raising `KeyError` with <span class="title-ref">MultiIndex</span> and list-like columns indexer enlarging <span class="title-ref">DataFrame</span> (`39147`)
  - Fixed regression in <span class="title-ref">\~DataFrame.groupby</span> with <span class="title-ref">Categorical</span> grouping column not showing unused categories for `grouped.indices` (`38642`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.sem</span> and <span class="title-ref">.SeriesGroupBy.sem</span> where the presence of non-numeric columns would cause an error instead of being dropped (`38774`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.diff</span> raising for `int8` and `int16` columns (`39050`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby</span> when aggregating an `ExtensionDType` that could fail for non-numeric values (`38980`)
  - Fixed regression in <span class="title-ref">.Rolling.skew</span> and <span class="title-ref">.Rolling.kurt</span> modifying the object inplace (`38908`)
  - Fixed regression in <span class="title-ref">DataFrame.any</span> and <span class="title-ref">DataFrame.all</span> not returning a result for tz-aware `datetime64` columns (`38723`)
  - Fixed regression in <span class="title-ref">DataFrame.apply</span> with `axis=1` using str accessor in apply function (`38979`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> raising `ValueError` when <span class="title-ref">DataFrame</span> has dtype `bytes` (`38900`)
  - Fixed regression in <span class="title-ref">Series.fillna</span> that raised `RecursionError` with `datetime64[ns, UTC]` dtype (`38851`)
  - Fixed regression in comparisons between `NaT` and `datetime.date` objects incorrectly returning `True` (`39151`)
  - Fixed regression in calling NumPy <span class="title-ref">\~numpy.ufunc.accumulate</span> ufuncs on DataFrames, e.g. `np.maximum.accumulate(df)` (`39259`)
  - Fixed regression in repr of float-like strings of an `object` dtype having trailing 0's truncated after the decimal (`38708`)
  - Fixed regression that raised `AttributeError` with PyArrow versions \[0.16.0, 1.0.0) (`38801`)
  - Fixed regression in <span class="title-ref">pandas.testing.assert\_frame\_equal</span> raising `TypeError` with `check_like=True` when <span class="title-ref">Index</span> or columns have mixed dtype (`39168`)

We have reverted a commit that resulted in several plotting related regressions in pandas 1.2.0 (`38969`, `38736`, `38865`, `38947` and `39126`). As a result, bugs reported as fixed in pandas 1.2.0 related to inconsistent tick labeling in bar plots are again present (`26186` and `11465`)

## Calling NumPy ufuncs on non-aligned DataFrames

Before pandas 1.2.0, calling a NumPy ufunc on non-aligned DataFrames (or DataFrame / Series combination) would ignore the indices, only match the inputs by shape, and use the index/columns of the first DataFrame for the result:

`` `ipython     In [1]: df1 = pd.DataFrame({"a": [1, 2], "b": [3, 4]}, index=[0, 1])     In [2]: df2 = pd.DataFrame({"a": [1, 2], "b": [3, 4]}, index=[1, 2])     In [3]: df1     Out[3]:        a  b     0  1  3     1  2  4     In [4]: df2     Out[4]:        a  b     1  1  3     2  2  4      In [5]: np.add(df1, df2)     Out[5]:        a  b     0  2  6     1  4  8  This contrasts with how other pandas operations work, which first align ``\` the inputs:

`` `ipython     In [6]: df1 + df2     Out[6]:          a    b     0  NaN  NaN     1  3.0  7.0     2  NaN  NaN  In pandas 1.2.0, we refactored how NumPy ufuncs are called on DataFrames, and ``<span class="title-ref"> this started to align the inputs first (:issue:\`39184</span>), as happens in other pandas operations and as it happens for ufuncs called on Series objects.

For pandas 1.2.1, we restored the previous behaviour to avoid a breaking change, but the above example of `np.add(df1, df2)` with non-aligned inputs will now to raise a warning, and a future pandas 2.0 release will start aligning the inputs first (`39184`). Calling a NumPy ufunc on Series objects (eg `np.add(s1, s2)`) already aligns and continues to do so.

To avoid the warning and keep the current behaviour of ignoring the indices, convert one of the arguments to a NumPy array:

`` `ipython     In [7]: np.add(df1, np.asarray(df2))     Out[7]:        a  b     0  2  6     1  4  8  To obtain the future behaviour and silence the warning, you can align manually ``\` before passing the arguments to the ufunc:

`` `ipython     In [8]: df1, df2 = df1.align(df2)     In [9]: np.add(df1, df2)     Out[9]:          a    b     0  NaN  NaN     1  3.0  7.0     2  NaN  NaN  .. ---------------------------------------------------------------------------  .. _whatsnew_121.bug_fixes:  Bug fixes ``\` \~\~\~\~\~\~\~\~\~

  - Bug in <span class="title-ref">read\_csv</span> with `float_precision="high"` caused segfault or wrong parsing of long exponent strings. This resulted in a regression in some cases as the default for `float_precision` was changed in pandas 1.2.0 (`38753`)
  - Bug in <span class="title-ref">read\_csv</span> not closing an opened file handle when a `csv.Error` or `UnicodeDecodeError` occurred while initializing (`39024`)
  - Bug in <span class="title-ref">pandas.testing.assert\_index\_equal</span> raising `TypeError` with `check_order=False` when <span class="title-ref">Index</span> has mixed dtype (`39168`)

## Other

  - The deprecated attributes `_AXIS_NAMES` and `_AXIS_NUMBERS` of <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> will no longer show up in `dir` or `inspect.getmembers` calls (`38740`)
  - Bumped minimum fastparquet version to 0.4.0 to avoid `AttributeError` from numba (`38344`)
  - Bumped minimum pymysql version to 0.8.1 to avoid test failures (`38344`)
  - Fixed build failure on MacOS 11 in Python 3.9.1 (`38766`)
  - Added reference to backwards incompatible `check_freq` arg of <span class="title-ref">testing.assert\_frame\_equal</span> and <span class="title-ref">testing.assert\_series\_equal</span> in \[pandas 1.1.0 what's new \<whatsnew\_110.api\_breaking.testing.check\_freq\>\](\#pandas-1.1.0-what's-new-\<whatsnew\_110.api\_breaking.testing.check\_freq\>) (`34050`)

## Contributors

<div class="contributors">

v1.2.0..v1.2.1

</div>

---

v1.2.2.md

---

# What's new in 1.2.2 (February 09, 2021)

These are the changes in pandas 1.2.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">read\_excel</span> that caused it to raise `AttributeError` when checking version of older xlrd versions (`38955`)
  - Fixed regression in <span class="title-ref">DataFrame</span> constructor reordering element when construction from datetime ndarray with dtype not `"datetime64[ns]"` (`39422`)
  - Fixed regression in <span class="title-ref">DataFrame.astype</span> and <span class="title-ref">Series.astype</span> not casting to bytes dtype (`39474`)
  - Fixed regression in <span class="title-ref">\~DataFrame.to\_pickle</span> failing to create bz2/xz compressed pickle files with `protocol=5` (`39002`)
  - Fixed regression in <span class="title-ref">pandas.testing.assert\_series\_equal</span> and <span class="title-ref">pandas.testing.assert\_frame\_equal</span> always raising `AssertionError` when comparing extension dtypes (`39410`)
  - Fixed regression in <span class="title-ref">\~DataFrame.to\_csv</span> opening `codecs.StreamWriter` in binary mode instead of in text mode and ignoring user-provided `mode` (`39247`)
  - Fixed regression in <span class="title-ref">Categorical.astype</span> casting to incorrect dtype when `np.int32` is passed to dtype argument (`39402`)
  - Fixed regression in <span class="title-ref">\~DataFrame.to\_excel</span> creating corrupt files when appending (`mode="a"`) to an existing file (`39576`)
  - Fixed regression in <span class="title-ref">DataFrame.transform</span> failing in case of an empty DataFrame or Series (`39636`)
  - Fixed regression in <span class="title-ref">\~DataFrame.groupby</span> or <span class="title-ref">\~DataFrame.resample</span> when aggregating an all-NaN or numeric object dtype column (`39329`)
  - Fixed regression in <span class="title-ref">.Rolling.count</span> where the `min_periods` argument would be set to `0` after the operation (`39554`)
  - Fixed regression in <span class="title-ref">read\_excel</span> that incorrectly raised when the argument `io` was a non-path and non-buffer and the `engine` argument was specified (`39528`)

## Bug fixes

  - <span class="title-ref">pandas.read\_excel</span> error message when a specified `sheetname` does not exist is now uniform across engines (`39250`)
  - Fixed bug in <span class="title-ref">pandas.read\_excel</span> producing incorrect results when the engine `openpyxl` is used and the excel file is missing or has incorrect dimension information; the fix requires `openpyxl` \>= 3.0.0, prior versions may still fail (`38956`, `39001`)
  - Fixed bug in <span class="title-ref">pandas.read\_excel</span> sometimes producing a `DataFrame` with trailing rows of `np.nan` when the engine `openpyxl` is used (`39181`)

## Contributors

<div class="contributors">

v1.2.1..v1.2.2

</div>

---

v1.2.3.md

---

# What's new in 1.2.3 (March 02, 2021)

These are the changes in pandas 1.2.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">\~DataFrame.to\_excel</span> raising `KeyError` when giving duplicate columns with `columns` attribute (`39695`)
  - Fixed regression in nullable integer unary ops propagating mask on assignment (`39943`)
  - Fixed regression in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> not aligning <span class="title-ref">DataFrame</span> on right-hand side for boolean indexer (`39931`)
  - Fixed regression in <span class="title-ref">\~DataFrame.to\_json</span> failing to use `compression` with URL-like paths that are internally opened in binary mode or with user-provided file objects that are opened in binary mode (`39985`)
  - Fixed regression in <span class="title-ref">Series.sort\_index</span> and <span class="title-ref">DataFrame.sort\_index</span>, which exited with an ungraceful error when having kwarg `ascending=None` passed. Passing `ascending=None` is still considered invalid, and the improved error message suggests a proper usage (`ascending` must be a boolean or a list-like of boolean) (`39434`)
  - Fixed regression in <span class="title-ref">DataFrame.transform</span> and <span class="title-ref">Series.transform</span> giving incorrect column labels when passed a dictionary with a mix of list and non-list values (`40018`)

## Contributors

<div class="contributors">

v1.2.2..v1.2.3

</div>

---

v1.2.4.md

---

# What's new in 1.2.4 (April 12, 2021)

These are the changes in pandas 1.2.4. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.sum</span> when `min_count` greater than the <span class="title-ref">DataFrame</span> shape was passed resulted in a `ValueError` (`39738`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_json</span> raising `AttributeError` when run on PyPy (`39837`)
  - Fixed regression in (in)equality comparison of `pd.NaT` with a non-datetimelike numpy array returning a scalar instead of an array (`40722`)
  - Fixed regression in <span class="title-ref">DataFrame.where</span> not returning a copy in the case of an all True condition (`39595`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> raising `IndexError` when `regex` was a multi-key dictionary (`39338`)
  - Fixed regression in repr of floats in an `object` column not respecting `float_format` when printed in the console or outputted through <span class="title-ref">DataFrame.to\_string</span>, <span class="title-ref">DataFrame.to\_html</span>, and <span class="title-ref">DataFrame.to\_latex</span> (`40024`)
  - Fixed regression in NumPy ufuncs such as `np.add` not passing through all arguments for <span class="title-ref">DataFrame</span> (`40662`)

## Contributors

<div class="contributors">

v1.2.3..v1.2.4

</div>

---

v1.2.5.md

---

# What's new in 1.2.5 (June 22, 2021)

These are the changes in pandas 1.2.5. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">concat</span> between two <span class="title-ref">DataFrame</span> where one has an <span class="title-ref">Index</span> that is all-None and the other is <span class="title-ref">DatetimeIndex</span> incorrectly raising (`40841`)
  - Fixed regression in <span class="title-ref">DataFrame.sum</span> and <span class="title-ref">DataFrame.prod</span> when `min_count` and `numeric_only` are both given (`41074`)
  - Fixed regression in <span class="title-ref">read\_csv</span> when using `memory_map=True` with an non-UTF8 encoding (`40986`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> when the values to replace is a NumPy float array (`40371`)
  - Fixed regression in <span class="title-ref">ExcelFile</span> when a corrupt file is opened but not closed (`41778`)
  - Fixed regression in <span class="title-ref">DataFrame.astype</span> with `dtype=str` failing to convert `NaN` in categorical columns (`41797`)

## Contributors

<div class="contributors">

v1.2.4..v1.2.5|HEAD

</div>

---

v1.3.0.md

---

# What's new in 1.3.0 (July 2, 2021)

These are the changes in pandas 1.3.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

\> **Warning** \> When reading new Excel 2007+ (`.xlsx`) files, the default argument `engine=None` to <span class="title-ref">read\_excel</span> will now result in using the [openpyxl](https://openpyxl.readthedocs.io/en/stable/) engine in all cases when the option <span class="title-ref">io.excel.xlsx.reader</span> is set to `"auto"`. Previously, some cases would use the [xlrd](https://xlrd.readthedocs.io/en/latest/) engine instead. See \[What's new 1.2.0 \<whatsnew\_120\>\](\#what's-new-1.2.0-\<whatsnew\_120\>) for background on this change.

## Enhancements

### Custom HTTP(s) headers when reading csv or json files

When reading from a remote URL that is not handled by fsspec (e.g. HTTP and HTTPS) the dictionary passed to `storage_options` will be used to create the headers included in the request. This can be used to control the User-Agent header or send other custom headers (`36688`). For example:

`` `ipython     In [1]: headers = {"User-Agent": "pandas"}     In [2]: df = pd.read_csv(        ...:     "https://download.bls.gov/pub/time.series/cu/cu.item",        ...:     sep="\t",        ...:     storage_options=headers        ...: )  .. _whatsnew_130.enhancements.read_to_xml:  Read and write XML documents ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We added I/O support to read and render shallow versions of [XML](https://www.w3.org/standards/xml/core) documents with <span class="title-ref">read\_xml</span> and <span class="title-ref">DataFrame.to\_xml</span>. Using [lxml](https://lxml.de) as parser, both XPath 1.0 and XSLT 1.0 are available. (`27554`)

`` `ipython     In [1]: xml = """<?xml version='1.0' encoding='utf-8'?>        ...: <data>        ...:  <row>        ...:     <shape>square</shape>        ...:     <degrees>360</degrees>        ...:     <sides>4.0</sides>        ...:  </row>        ...:  <row>        ...:     <shape>circle</shape>        ...:     <degrees>360</degrees>        ...:     <sides/>        ...:  </row>        ...:  <row>        ...:     <shape>triangle</shape>        ...:     <degrees>180</degrees>        ...:     <sides>3.0</sides>        ...:  </row>        ...:  </data>"""      In [2]: df = pd.read_xml(xml)     In [3]: df     Out[3]:           shape  degrees  sides     0    square      360    4.0     1    circle      360    NaN     2  triangle      180    3.0      In [4]: df.to_xml()     Out[4]:     <?xml version='1.0' encoding='utf-8'?>     <data>       <row>         <index>0</index>         <shape>square</shape>         <degrees>360</degrees>         <sides>4.0</sides>       </row>       <row>         <index>1</index>         <shape>circle</shape>         <degrees>360</degrees>         <sides/>       </row>       <row>         <index>2</index>         <shape>triangle</shape>         <degrees>180</degrees>         <sides>3.0</sides>       </row>     </data>  For more, see [io.xml](#io.xml) in the user guide on IO tools.  .. _whatsnew_130.enhancements.styler:  Styler enhancements ``\` ^^^^^^^^^^^^^^^^^^^

We provided some focused development on <span class="title-ref">.Styler</span>. See also the [Styler documentation](../user_guide/style.ipynb) which has been revised and improved (`39720`, `39317`, `40493`).

>   - The method <span class="title-ref">.Styler.set\_table\_styles</span> can now accept more natural CSS language for arguments, such as `'color:red;'` instead of `[('color', 'red')]` (`39563`)
>   - The methods <span class="title-ref">.Styler.highlight\_null</span>, <span class="title-ref">.Styler.highlight\_min</span>, and <span class="title-ref">.Styler.highlight\_max</span> now allow custom CSS highlighting instead of the default background coloring (`40242`)
>   - <span class="title-ref">.Styler.apply</span> now accepts functions that return an `ndarray` when `axis=None`, making it now consistent with the `axis=0` and `axis=1` behavior (`39359`)
>   - When incorrectly formatted CSS is given via <span class="title-ref">.Styler.apply</span> or <span class="title-ref">.Styler.applymap</span>, an error is now raised upon rendering (`39660`)
>   - <span class="title-ref">.Styler.format</span> now accepts the keyword argument `escape` for optional HTML and LaTeX escaping (`40388`, `41619`)
>   - <span class="title-ref">.Styler.background\_gradient</span> has gained the argument `gmap` to supply a specific gradient map for shading (`22727`)
>   - <span class="title-ref">.Styler.clear</span> now clears <span class="title-ref">Styler.hidden\_index</span> and <span class="title-ref">Styler.hidden\_columns</span> as well (`40484`)
>   - Added the method <span class="title-ref">.Styler.highlight\_between</span> (`39821`)
>   - Added the method <span class="title-ref">.Styler.highlight\_quantile</span> (`40926`)
>   - Added the method <span class="title-ref">.Styler.text\_gradient</span> (`41098`)
>   - Added the method <span class="title-ref">.Styler.set\_tooltips</span> to allow hover tooltips; this can be used enhance interactive displays (`21266`, `40284`)
>   - Added the parameter `precision` to the method <span class="title-ref">.Styler.format</span> to control the display of floating point numbers (`40134`)
>   - <span class="title-ref">.Styler</span> rendered HTML output now follows the [w3 HTML Style Guide](https://www.w3schools.com/html/html5_syntax.asp) (`39626`)
>   - Many features of the <span class="title-ref">.Styler</span> class are now either partially or fully usable on a DataFrame with a non-unique indexes or columns (`41143`)
>   - One has greater control of the display through separate sparsification of the index or columns using the \[new styler options \<options.available\>\](\#new-styler-options-\<options.available\>), which are also usable via <span class="title-ref">option\_context</span> (`41142`)
>   - Added the option `styler.render.max_elements` to avoid browser overload when styling large DataFrames (`40712`)
>   - Added the method <span class="title-ref">.Styler.to\_latex</span> (`21673`, `42320`), which also allows some limited CSS conversion (`40731`)
>   - Added the method <span class="title-ref">.Styler.to\_html</span> (`13379`)
>   - Added the method <span class="title-ref">.Styler.set\_sticky</span> to make index and column headers permanently visible in scrolling HTML frames (`29072`)

### DataFrame constructor honors `copy=False` with dict

When passing a dictionary to <span class="title-ref">DataFrame</span> with `copy=False`, a copy will no longer be made (`32960`).

<div class="ipython">

python

arr = np.array(\[1, 2, 3\]) df = pd.DataFrame({"A": arr, "B": arr.copy()}, copy=False) df

</div>

`df["A"]` remains a view on `arr`:

<div class="ipython">

python

arr\[0\] = 0 assert df.iloc\[0, 0\] == 0

</div>

The default behavior when not passing `copy` will remain unchanged, i.e. a copy will be made.

### PyArrow backed string data type

We've enhanced the <span class="title-ref">StringDtype</span>, an extension type dedicated to string data. (`39908`)

It is now possible to specify a `storage` keyword option to <span class="title-ref">StringDtype</span>. Use pandas options or specify the dtype using `dtype='string[pyarrow]'` to allow the StringArray to be backed by a PyArrow array instead of a NumPy array of Python objects.

The PyArrow backed StringArray requires pyarrow 1.0.0 or greater to be installed.

\> **Warning** \> `string[pyarrow]` is currently considered experimental. The implementation and parts of the API may change without warning.

<div class="ipython">

python

pd.Series(\['abc', None, 'def'\], dtype=pd.StringDtype(storage="pyarrow"))

</div>

You can use the alias `"string[pyarrow]"` as well.

<div class="ipython">

python

s = pd.Series(\['abc', None, 'def'\], dtype="string\[pyarrow\]") s

</div>

You can also create a PyArrow backed string array using pandas options.

<div class="ipython">

python

  - with pd.option\_context("string\_storage", "pyarrow"):  
    s = pd.Series(\['abc', None, 'def'\], dtype="string")

s

</div>

The usual string accessor methods work. Where appropriate, the return type of the Series or columns of a DataFrame will also have string dtype.

<div class="ipython">

python

s.str.upper() s.str.split('b', expand=True).dtypes

</div>

String accessor methods returning integers will return a value with <span class="title-ref">Int64Dtype</span>

<div class="ipython">

python

s.str.count("a")

</div>

### Centered datetime-like rolling windows

When performing rolling calculations on DataFrame and Series objects with a datetime-like index, a centered datetime-like window can now be used (`38780`). For example:

<div class="ipython">

python

  - df = pd.DataFrame(  
    {"A": \[0, 1, 2, 3, 4\]}, index=pd.date\_range("2020", periods=5, freq="1D")

) df df.rolling("2D", center=True).mean()

</div>

### Other enhancements

  - <span class="title-ref">DataFrame.rolling</span>, <span class="title-ref">Series.rolling</span>, <span class="title-ref">DataFrame.expanding</span>, and <span class="title-ref">Series.expanding</span> now support a `method` argument with a `'table'` option that performs the windowing operation over an entire <span class="title-ref">DataFrame</span>. See \[Window Overview \<window.overview\>\](\#window-overview-\<window.overview\>) for performance and functional benefits (`15095`, `38995`)
  - <span class="title-ref">.ExponentialMovingWindow</span> now support a `online` method that can perform `mean` calculations in an online fashion. See \[Window Overview \<window.overview\>\](\#window-overview-\<window.overview\>) (`41673`)
  - Added <span class="title-ref">MultiIndex.dtypes</span> (`37062`)
  - Added `end` and `end_day` options for the `origin` argument in <span class="title-ref">DataFrame.resample</span> (`37804`)
  - Improved error message when `usecols` and `names` do not match for <span class="title-ref">read\_csv</span> and `engine="c"` (`29042`)
  - Improved consistency of error messages when passing an invalid `win_type` argument in \[Window methods \<api.window\>\](\#window-methods-\<api.window\>) (`15969`)
  - <span class="title-ref">read\_sql\_query</span> now accepts a `dtype` argument to cast the columnar data from the SQL database based on user input (`10285`)
  - <span class="title-ref">read\_csv</span> now raising `ParserWarning` if length of header or given names does not match length of data when `usecols` is not specified (`21768`)
  - Improved integer type mapping from pandas to SQLAlchemy when using <span class="title-ref">DataFrame.to\_sql</span> (`35076`)
  - <span class="title-ref">to\_numeric</span> now supports downcasting of nullable `ExtensionDtype` objects (`33013`)
  - Added support for dict-like names in <span class="title-ref">MultiIndex.set\_names</span> and <span class="title-ref">MultiIndex.rename</span> (`20421`)
  - <span class="title-ref">read\_excel</span> can now auto-detect .xlsb files and older .xls files (`35416`, `41225`)
  - <span class="title-ref">ExcelWriter</span> now accepts an `if_sheet_exists` parameter to control the behavior of append mode when writing to existing sheets (`40230`)
  - <span class="title-ref">.Rolling.sum</span>, <span class="title-ref">.Expanding.sum</span>, <span class="title-ref">.Rolling.mean</span>, <span class="title-ref">.Expanding.mean</span>, <span class="title-ref">.ExponentialMovingWindow.mean</span>, <span class="title-ref">.Rolling.median</span>, <span class="title-ref">.Expanding.median</span>, <span class="title-ref">.Rolling.max</span>, <span class="title-ref">.Expanding.max</span>, <span class="title-ref">.Rolling.min</span>, and <span class="title-ref">.Expanding.min</span> now support [Numba](http://numba.pydata.org/) execution with the `engine` keyword (`38895`, `41267`)
  - <span class="title-ref">DataFrame.apply</span> can now accept NumPy unary operators as strings, e.g. `df.apply("sqrt")`, which was already the case for <span class="title-ref">Series.apply</span> (`39116`)
  - <span class="title-ref">DataFrame.apply</span> can now accept non-callable DataFrame properties as strings, e.g. `df.apply("size")`, which was already the case for <span class="title-ref">Series.apply</span> (`39116`)
  - <span class="title-ref">DataFrame.applymap</span> can now accept kwargs to pass on to the user-provided `func` (`39987`)
  - Passing a <span class="title-ref">DataFrame</span> indexer to `iloc` is now disallowed for <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">DataFrame.\_\_getitem\_\_</span> (`39004`)
  - <span class="title-ref">Series.apply</span> can now accept list-like or dictionary-like arguments that aren't lists or dictionaries, e.g. `ser.apply(np.array(["sum", "mean"]))`, which was already the case for <span class="title-ref">DataFrame.apply</span> (`39140`)
  - <span class="title-ref">DataFrame.plot.scatter</span> can now accept a categorical column for the argument `c` (`12380`, `31357`)
  - <span class="title-ref">Series.loc</span> now raises a helpful error message when the Series has a <span class="title-ref">MultiIndex</span> and the indexer has too many dimensions (`35349`)
  - <span class="title-ref">read\_stata</span> now supports reading data from compressed files (`26599`)
  - Added support for parsing `ISO 8601`-like timestamps with negative signs to <span class="title-ref">Timedelta</span> (`37172`)
  - Added support for unary operators in <span class="title-ref">FloatingArray</span> (`38749`)
  - <span class="title-ref">RangeIndex</span> can now be constructed by passing a `range` object directly e.g. `pd.RangeIndex(range(3))` (`12067`)
  - <span class="title-ref">Series.round</span> and <span class="title-ref">DataFrame.round</span> now work with nullable integer and floating dtypes (`38844`)
  - <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_json</span> expose the argument `encoding_errors` to control how encoding errors are handled (`39450`)
  - <span class="title-ref">.DataFrameGroupBy.any</span>, <span class="title-ref">.SeriesGroupBy.any</span>, <span class="title-ref">.DataFrameGroupBy.all</span>, and <span class="title-ref">.SeriesGroupBy.all</span> use Kleene logic with nullable data types (`37506`)
  - <span class="title-ref">.DataFrameGroupBy.any</span>, <span class="title-ref">.SeriesGroupBy.any</span>, <span class="title-ref">.DataFrameGroupBy.all</span>, and <span class="title-ref">.SeriesGroupBy.all</span> return a `BooleanDtype` for columns with nullable data types (`33449`)
  - <span class="title-ref">.DataFrameGroupBy.any</span>, <span class="title-ref">.SeriesGroupBy.any</span>, <span class="title-ref">.DataFrameGroupBy.all</span>, and <span class="title-ref">.SeriesGroupBy.all</span> raising with `object` data containing `pd.NA` even when `skipna=True` (`37501`)
  - <span class="title-ref">.DataFrameGroupBy.rank</span> and <span class="title-ref">.SeriesGroupBy.rank</span> now supports object-dtype data (`38278`)
  - Constructing a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with the `data` argument being a Python iterable that is *not* a NumPy `ndarray` consisting of NumPy scalars will now result in a dtype with a precision the maximum of the NumPy scalars; this was already the case when `data` is a NumPy `ndarray` (`40908`)
  - Add keyword `sort` to <span class="title-ref">pivot\_table</span> to allow non-sorting of the result (`39143`)
  - Add keyword `dropna` to <span class="title-ref">DataFrame.value\_counts</span> to allow counting rows that include `NA` values (`41325`)
  - <span class="title-ref">Series.replace</span> will now cast results to `PeriodDtype` where possible instead of `object` dtype (`41526`)
  - Improved error message in `corr` and `cov` methods on <span class="title-ref">.Rolling</span>, <span class="title-ref">.Expanding</span>, and <span class="title-ref">.ExponentialMovingWindow</span> when `other` is not a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> (`41741`)
  - <span class="title-ref">Series.between</span> can now accept `left` or `right` as arguments to `inclusive` to include only the left or right boundary (`40245`)
  - <span class="title-ref">DataFrame.explode</span> now supports exploding multiple columns. Its `column` argument now also accepts a list of str or tuples for exploding on multiple columns at the same time (`39240`)
  - <span class="title-ref">DataFrame.sample</span> now accepts the `ignore_index` argument to reset the index after sampling, similar to <span class="title-ref">DataFrame.drop\_duplicates</span> and <span class="title-ref">DataFrame.sort\_values</span> (`38581`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### `Categorical.unique` now always maintains same dtype as original

Previously, when calling <span class="title-ref">Categorical.unique</span> with categorical data, unused categories in the new array would be removed, making the dtype of the new array different than the original (`18291`)

As an example of this, given:

<div class="ipython">

python

dtype = pd.CategoricalDtype(\['bad', 'neutral', 'good'\], ordered=True) cat = pd.Categorical(\['good', 'good', 'bad', 'bad'\], dtype=dtype) original = pd.Series(cat) unique = original.unique()

</div>

*Previous behavior*:

`` `ipython     In [1]: unique     ['good', 'bad']     Categories (2, object): ['bad' < 'good']     In [2]: original.dtype == unique.dtype     False  *New behavior*:  .. ipython:: python          unique         original.dtype == unique.dtype  .. _whatsnew_130.notable_bug_fixes.combine_first_preserves_dtype:  Preserve dtypes in `DataFrame.combine_first` ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">DataFrame.combine\_first</span> will now preserve dtypes (`7509`)

<div class="ipython">

python

df1 = pd.DataFrame({"A": \[1, 2, 3\], "B": \[1, 2, 3\]}, index=\[0, 1, 2\]) df1 df2 = pd.DataFrame({"B": \[4, 5, 6\], "C": \[1, 2, 3\]}, index=\[2, 3, 4\]) df2 combined = df1.combine\_first(df2)

</div>

*Previous behavior*:

`` `ipython    In [1]: combined.dtypes    Out[2]:    A    float64    B    float64    C    float64    dtype: object  *New behavior*:  .. ipython:: python     combined.dtypes  .. _whatsnew_130.notable_bug_fixes.groupby_preserves_dtype:  Groupby methods agg and transform no longer changes return dtype for callables ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously the methods <span class="title-ref">.DataFrameGroupBy.aggregate</span>, <span class="title-ref">.SeriesGroupBy.aggregate</span>, <span class="title-ref">.DataFrameGroupBy.transform</span>, and <span class="title-ref">.SeriesGroupBy.transform</span> might cast the result dtype when the argument `func` is callable, possibly leading to undesirable results (`21240`). The cast would occur if the result is numeric and casting back to the input dtype does not change any values as measured by `np.allclose`. Now no such casting occurs.

<div class="ipython">

python

df = pd.DataFrame({'key': \[1, 1\], 'a': \[True, False\], 'b': \[True, True\]}) df

</div>

*Previous behavior*:

`` `ipython     In [5]: df.groupby('key').agg(lambda x: x.sum())     Out[5]:             a  b     key     1    True  2  *New behavior*:  .. ipython:: python      df.groupby('key').agg(lambda x: x.sum())  .. _whatsnew_130.notable_bug_fixes.groupby_reductions_float_result: ``float`` result for `.DataFrameGroupBy.mean`, `.DataFrameGroupBy.median`, and `.GDataFrameGroupBy.var`, `.SeriesGroupBy.mean`, `.SeriesGroupBy.median`, and `.SeriesGroupBy.var` ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, these methods could result in different dtypes depending on the input values. Now, these methods will always return a float dtype. (`41137`)

<div class="ipython">

python

df = pd.DataFrame({'a': \[True\], 'b': \[1\], 'c': \[1.0\]})

</div>

*Previous behavior*:

`` `ipython     In [5]: df.groupby(df.index).mean()     Out[5]:             a  b    c     0    True  1  1.0  *New behavior*:  .. ipython:: python      df.groupby(df.index).mean()  .. _whatsnew_130.notable_bug_fixes.setitem_column_try_inplace:  Try operating inplace when setting values with ``loc`and`iloc`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When setting an entire column using `loc` or `iloc`, pandas will try to insert the values into the existing data rather than create an entirely new array.

<div class="ipython">

python

df = pd.DataFrame(range(3), columns=\["A"\], dtype="float64") values = df.values new = np.array(\[5, 6, 7\], dtype="int64") df.loc\[\[0, 1, 2\], "A"\] = new

</div>

In both the new and old behavior, the data in `values` is overwritten, but in the old behavior the dtype of `df["A"]` changed to `int64`.

*Previous behavior*:

`` `ipython    In [1]: df.dtypes    Out[1]:    A    int64    dtype: object    In [2]: np.shares_memory(df["A"].values, new)    Out[2]: False    In [3]: np.shares_memory(df["A"].values, values)    Out[3]: False  In pandas 1.3.0, ``df`continues to share data with`values`*New behavior*:  .. ipython:: python     df.dtypes    np.shares_memory(df["A"], new)    np.shares_memory(df["A"], values)   .. _whatsnew_130.notable_bug_fixes.setitem_never_inplace:  Never operate inplace when setting`frame\[keys\] = values`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When setting multiple columns using `frame[keys] = values` new arrays will replace pre-existing arrays for these keys, which will *not* be over-written (`39510`). As a result, the columns will retain the dtype(s) of `values`, never casting to the dtypes of the existing arrays.

<div class="ipython">

python

df = pd.DataFrame(range(3), columns=\["A"\], dtype="float64") df\[\["A"\]\] = 5

</div>

In the old behavior, `5` was cast to `float64` and inserted into the existing array backing `df`:

*Previous behavior*:

`` `ipython    In [1]: df.dtypes    Out[1]:    A    float64  In the new behavior, we get a new array, and retain an integer-dtyped ``5`:  *New behavior*:  .. ipython:: python     df.dtypes   .. _whatsnew_130.notable_bug_fixes.setitem_with_bool_casting:  Consistent casting with setting into Boolean Series`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Setting non-boolean values into a <span class="title-ref">Series</span> with `dtype=bool` now consistently casts to `dtype=object` (`38709`)

`` `ipython    In [1]: orig = pd.Series([True, False])     In [2]: ser = orig.copy()     In [3]: ser.iloc[1] = np.nan     In [4]: ser2 = orig.copy()     In [5]: ser2.iloc[1] = 2.0  *Previous behavior*:  .. code-block:: ipython     In [1]: ser    Out [1]:    0    1.0    1    NaN    dtype: float64     In [2]:ser2    Out [2]:    0    True    1     2.0    dtype: object  *New behavior*:  .. code-block:: ipython     In [1]: ser    Out [1]:    0    True    1     NaN    dtype: object     In [2]:ser2    Out [2]:    0    True    1     2.0    dtype: object   .. _whatsnew_130.notable_bug_fixes.rolling_groupby_column:  DataFrameGroupBy.rolling and SeriesGroupBy.rolling no longer return grouped-by column in values ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The group-by column will now be dropped from the result of a `groupby.rolling` operation (`32262`)

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 1, 2, 3\], "B": \[0, 1, 2, 3\]}) df

</div>

*Previous behavior*:

`` `ipython     In [1]: df.groupby("A").rolling(2).sum()     Out[1]:            A    B     A     1 0  NaN  NaN     1    2.0  1.0     2 2  NaN  NaN     3 3  NaN  NaN  *New behavior*:  .. ipython:: python      df.groupby("A").rolling(2).sum()  .. _whatsnew_130.notable_bug_fixes.rolling_var_precision:  Removed artificial truncation in rolling variance and standard deviation ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">.Rolling.std</span> and <span class="title-ref">.Rolling.var</span> will no longer artificially truncate results that are less than `~1e-8` and `~1e-15` respectively to zero (`37051`, `40448`, `39872`).

However, floating point artifacts may now exist in the results when rolling over larger values.

<div class="ipython">

python

s = pd.Series(\[7, 5, 5, 5\]) s.rolling(3).var()

</div>

### DataFrameGroupBy.rolling and SeriesGroupBy.rolling with MultiIndex no longer drop levels in the result

<span class="title-ref">DataFrameGroupBy.rolling</span> and <span class="title-ref">SeriesGroupBy.rolling</span> will no longer drop levels of a <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> in the result. This can lead to a perceived duplication of levels in the resulting <span class="title-ref">MultiIndex</span>, but this change restores the behavior that was present in version 1.1.3 (`38787`, `38523`).

<div class="ipython">

python

index = pd.MultiIndex.from\_tuples(\[('idx1', 'idx2')\], names=\['label1', 'label2'\]) df = pd.DataFrame({'a': \[1\], 'b': \[2\]}, index=index) df

</div>

*Previous behavior*:

`` `ipython     In [1]: df.groupby('label1').rolling(1).sum()     Out[1]:               a    b     label1     idx1    1.0  2.0  *New behavior*:  .. ipython:: python      df.groupby('label1').rolling(1).sum()   .. ---------------------------------------------------------------------------  .. _whatsnew_130.api_breaking:  Backwards incompatible API changes ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated. If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.17.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytz</td>
<td>2017.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.7.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.2.1</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.7.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>6.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>mypy (dev)</td>
<td>0.812</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>setuptools</td>
<td>38.6.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.6.0</td>
<td></td>
</tr>
<tr class="even">
<td>fastparquet</td>
<td>0.4.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>fsspec</td>
<td>0.7.4</td>
<td></td>
</tr>
<tr class="even">
<td>gcsfs</td>
<td>0.6.0</td>
<td></td>
</tr>
<tr class="odd">
<td>lxml</td>
<td>4.3.0</td>
<td></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>2.2.3</td>
<td></td>
</tr>
<tr class="odd">
<td>numba</td>
<td>0.46.0</td>
<td></td>
</tr>
<tr class="even">
<td>openpyxl</td>
<td>3.0.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pyarrow</td>
<td>0.17.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pymysql</td>
<td>0.8.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pytables</td>
<td>3.5.1</td>
<td></td>
</tr>
<tr class="even">
<td>s3fs</td>
<td>0.4.0</td>
<td></td>
</tr>
<tr class="odd">
<td>scipy</td>
<td>1.2.0</td>
<td></td>
</tr>
<tr class="even">
<td>sqlalchemy</td>
<td>1.3.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>tabulate</td>
<td>0.8.7</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xarray</td>
<td>0.12.0</td>
<td></td>
</tr>
<tr class="odd">
<td>xlrd</td>
<td>1.2.0</td>
<td></td>
</tr>
<tr class="even">
<td>xlsxwriter</td>
<td>1.0.2</td>
<td></td>
</tr>
<tr class="odd">
<td>xlwt</td>
<td>1.3.0</td>
<td></td>
</tr>
<tr class="even">
<td>pandas-gbq</td>
<td>0.12.0</td>
<td></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - Partially initialized <span class="title-ref">CategoricalDtype</span> objects (i.e. those with `categories=None`) will no longer compare as equal to fully initialized dtype objects (`38516`)
  - Accessing `_constructor_expanddim` on a <span class="title-ref">DataFrame</span> and `_constructor_sliced` on a <span class="title-ref">Series</span> now raise an `AttributeError`. Previously a `NotImplementedError` was raised (`38782`)
  - Added new `engine` and `**engine_kwargs` parameters to <span class="title-ref">DataFrame.to\_sql</span> to support other future "SQL engines". Currently we still only use `SQLAlchemy` under the hood, but more engines are planned to be supported such as [turbodbc](https://turbodbc.readthedocs.io/en/latest/) (`36893`)
  - Removed redundant `freq` from <span class="title-ref">PeriodIndex</span> string representation (`41653`)
  - <span class="title-ref">ExtensionDtype.construct\_array\_type</span> is now a required method instead of an optional one for <span class="title-ref">ExtensionDtype</span> subclasses (`24860`)
  - Calling `hash` on non-hashable pandas objects will now raise `TypeError` with the built-in error message (e.g. `unhashable type: 'Series'`). Previously it would raise a custom message such as `'Series' objects are mutable, thus they cannot be hashed`. Furthermore, `isinstance(<Series>, abc.collections.Hashable)` will now return `False` (`40013`)
  - <span class="title-ref">.Styler.from\_custom\_template</span> now has two new arguments for template names, and removed the old `name`, due to template inheritance having been introducing for better parsing (`42053`). Subclassing modifications to Styler attributes are also needed.

### Build

  - Documentation in `.pptx` and `.pdf` formats are no longer included in wheels or source distributions. (`30741`)

## Deprecations

### Deprecated dropping nuisance columns in DataFrame reductions and DataFrameGroupBy operations

Calling a reduction (e.g. `.min`, `.max`, `.sum`) on a <span class="title-ref">DataFrame</span> with `numeric_only=None` (the default), columns where the reduction raises a `TypeError` are silently ignored and dropped from the result.

This behavior is deprecated. In a future version, the `TypeError` will be raised, and users will need to select only valid columns before calling the function.

For example:

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 2, 3, 4\], "B": pd.date\_range("2016-01-01", periods=4)}) df

</div>

*Old behavior*:

`` `ipython     In [3]: df.prod()     Out[3]:     Out[3]:     A    24     dtype: int64  *Future behavior*:  .. code-block:: ipython      In [4]: df.prod()     ...     TypeError: 'DatetimeArray' does not implement reduction 'prod'      In [5]: df[["A"]].prod()     Out[5]:     A    24     dtype: int64   Similarly, when applying a function to `DataFrameGroupBy`, columns on which ``<span class="title-ref"> the function raises </span><span class="title-ref">TypeError</span>\` are currently silently ignored and dropped from the result.

This behavior is deprecated. In a future version, the `TypeError` will be raised, and users will need to select only valid columns before calling the function.

For example:

<div class="ipython">

python

df = pd.DataFrame({"A": \[1, 2, 3, 4\], "B": pd.date\_range("2016-01-01", periods=4)}) gb = df.groupby(\[1, 1, 2, 2\])

</div>

*Old behavior*:

`` `ipython     In [4]: gb.prod(numeric_only=False)     Out[4]:     A     1   2     2  12  *Future behavior*:  .. code-block:: ipython      In [5]: gb.prod(numeric_only=False)     ...     TypeError: datetime64 type does not support prod operations      In [6]: gb[["A"]].prod(numeric_only=False)     Out[6]:         A     1   2     2  12  .. _whatsnew_130.deprecations.other:  Other Deprecations ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^ - Deprecated allowing scalars to be passed to the \`Categorical</span> constructor (`38433`) - Deprecated constructing <span class="title-ref">CategoricalIndex</span> without passing list-like data (`38944`) - Deprecated allowing subclass-specific keyword arguments in the <span class="title-ref">Index</span> constructor, use the specific subclass directly instead (`14093`, `21311`, `22315`, `26974`) - Deprecated the <span class="title-ref">astype</span> method of datetimelike (`timedelta64[ns]`, `datetime64[ns]`, `Datetime64TZDtype`, `PeriodDtype`) to convert to integer dtypes, use `values.view(...)` instead (`38544`). This deprecation was later reverted in pandas 1.4.0. - Deprecated <span class="title-ref">MultiIndex.is\_lexsorted</span> and <span class="title-ref">MultiIndex.lexsort\_depth</span>, use <span class="title-ref">MultiIndex.is\_monotonic\_increasing</span> instead (`32259`) - Deprecated keyword `try_cast` in <span class="title-ref">Series.where</span>, <span class="title-ref">Series.mask</span>, <span class="title-ref">DataFrame.where</span>, <span class="title-ref">DataFrame.mask</span>; cast results manually if desired (`38836`) - Deprecated comparison of <span class="title-ref">Timestamp</span> objects with `datetime.date` objects. Instead of e.g. `ts <= mydate` use `ts <= pd.Timestamp(mydate)` or `ts.date() <= mydate` (`36131`) - Deprecated <span class="title-ref">Rolling.win\_type</span> returning `"freq"` (`38963`) - Deprecated <span class="title-ref">Rolling.is\_datetimelike</span> (`38963`) - Deprecated <span class="title-ref">DataFrame</span> indexer for <span class="title-ref">Series.\_\_setitem\_\_</span> and <span class="title-ref">DataFrame.\_\_setitem\_\_</span> (`39004`) - Deprecated <span class="title-ref">ExponentialMovingWindow.vol</span> (`39220`) - Using `.astype` to convert between `datetime64[ns]` dtype and <span class="title-ref">DatetimeTZDtype</span> is deprecated and will raise in a future version, use `obj.tz_localize` or `obj.dt.tz_localize` instead (`38622`) - Deprecated casting `datetime.date` objects to `datetime64` when used as `fill_value` in <span class="title-ref">DataFrame.unstack</span>, <span class="title-ref">DataFrame.shift</span>, <span class="title-ref">Series.shift</span>, and <span class="title-ref">DataFrame.reindex</span>, pass `pd.Timestamp(dateobj)` instead (`39767`) - Deprecated <span class="title-ref">.Styler.set\_na\_rep</span> and <span class="title-ref">.Styler.set\_precision</span> in favor of <span class="title-ref">.Styler.format</span> with `na_rep` and `precision` as existing and new input arguments respectively (`40134`, `40425`) - Deprecated <span class="title-ref">.Styler.where</span> in favor of using an alternative formulation with <span class="title-ref">Styler.applymap</span> (`40821`) - Deprecated allowing partial failure in <span class="title-ref">Series.transform</span> and <span class="title-ref">DataFrame.transform</span> when `func` is list-like or dict-like and raises anything but `TypeError`; `func` raising anything but a `TypeError` will raise in a future version (`40211`) - Deprecated arguments `error_bad_lines` and `warn_bad_lines` in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> in favor of argument `on_bad_lines` (`15122`) - Deprecated support for `np.ma.mrecords.MaskedRecords` in the <span class="title-ref">DataFrame</span> constructor, pass `{name: data[name] for name in data.dtype.names}` instead (`40363`) - Deprecated using <span class="title-ref">merge</span>, <span class="title-ref">DataFrame.merge</span>, and <span class="title-ref">DataFrame.join</span> on a different number of levels (`34862`) - Deprecated the use of `**kwargs` in <span class="title-ref">.ExcelWriter</span>; use the keyword argument `engine_kwargs` instead (`40430`) - Deprecated the `level` keyword for <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> aggregations; use groupby instead (`39983`) - Deprecated the `inplace` parameter of <span class="title-ref">Categorical.remove\_categories</span>, <span class="title-ref">Categorical.add\_categories</span>, <span class="title-ref">Categorical.reorder\_categories</span>, <span class="title-ref">Categorical.rename\_categories</span>, <span class="title-ref">Categorical.set\_categories</span> and will be removed in a future version (`37643`) - Deprecated <span class="title-ref">merge</span> producing duplicated columns through the `suffixes` keyword and already existing columns (`22818`) - Deprecated setting <span class="title-ref">Categorical.\_codes</span>, create a new <span class="title-ref">Categorical</span> with the desired codes instead (`40606`) - Deprecated the `convert_float` optional argument in <span class="title-ref">read\_excel</span> and <span class="title-ref">ExcelFile.parse</span> (`41127`) - Deprecated behavior of <span class="title-ref">DatetimeIndex.union</span> with mixed timezones; in a future version both will be cast to UTC instead of object dtype (`39328`) - Deprecated using `usecols` with out of bounds indices for <span class="title-ref">read\_csv</span> with `engine="c"` (`25623`) - Deprecated special treatment of lists with first element a Categorical in the <span class="title-ref">DataFrame</span> constructor; pass as `pd.DataFrame({col: categorical, ...})` instead (`38845`) - Deprecated behavior of <span class="title-ref">DataFrame</span> constructor when a `dtype` is passed and the data cannot be cast to that dtype. In a future version, this will raise instead of being silently ignored (`24435`) - Deprecated the <span class="title-ref">Timestamp.freq</span> attribute. For the properties that use it (`is_month_start`, `is_month_end`, `is_quarter_start`, `is_quarter_end`, `is_year_start`, `is_year_end`), when you have a `freq`, use e.g. `freq.is_month_start(ts)` (`15146`) - Deprecated construction of <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with `DatetimeTZDtype` data and `datetime64[ns]` dtype. Use `Series(data).dt.tz_localize(None)` instead (`41555`, `33401`) - Deprecated behavior of <span class="title-ref">Series</span> construction with large-integer values and small-integer dtype silently overflowing; use `Series(data).astype(dtype)` instead (`41734`) - Deprecated behavior of <span class="title-ref">DataFrame</span> construction with floating data and integer dtype casting even when lossy; in a future version this will remain floating, matching <span class="title-ref">Series</span> behavior (`41770`) - Deprecated inference of `timedelta64[ns]`, `datetime64[ns]`, or `DatetimeTZDtype` dtypes in <span class="title-ref">Series</span> construction when data containing strings is passed and no `dtype` is passed (`33558`) - In a future version, constructing <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with `datetime64[ns]` data and `DatetimeTZDtype` will treat the data as wall-times instead of as UTC times (matching DatetimeIndex behavior). To treat the data as UTC times, use `pd.Series(data).dt.tz_localize("UTC").dt.tz_convert(dtype.tz)` or `pd.Series(data.view("int64"), dtype=dtype)` (`33401`) - Deprecated passing lists as `key` to <span class="title-ref">DataFrame.xs</span> and <span class="title-ref">Series.xs</span> (`41760`) - Deprecated boolean arguments of `inclusive` in <span class="title-ref">Series.between</span> to have `{"left", "right", "neither", "both"}` as standard argument values (`40628`) - Deprecated passing arguments as positional for all of the following, with exceptions noted (`41485`):

>   - <span class="title-ref">concat</span> (other than `objs`)
>   - <span class="title-ref">read\_csv</span> (other than `filepath_or_buffer`)
>   - <span class="title-ref">read\_table</span> (other than `filepath_or_buffer`)
>   - <span class="title-ref">DataFrame.clip</span> and <span class="title-ref">Series.clip</span> (other than `upper` and `lower`)
>   - <span class="title-ref">DataFrame.drop\_duplicates</span> (except for `subset`), <span class="title-ref">Series.drop\_duplicates</span>, <span class="title-ref">Index.drop\_duplicates</span> and <span class="title-ref">MultiIndex.drop\_duplicates</span>
>   - <span class="title-ref">DataFrame.drop</span> (other than `labels`) and <span class="title-ref">Series.drop</span>
>   - <span class="title-ref">DataFrame.dropna</span> and <span class="title-ref">Series.dropna</span>
>   - <span class="title-ref">DataFrame.ffill</span>, <span class="title-ref">Series.ffill</span>, <span class="title-ref">DataFrame.bfill</span>, and <span class="title-ref">Series.bfill</span>
>   - <span class="title-ref">DataFrame.fillna</span> and <span class="title-ref">Series.fillna</span> (apart from `value`)
>   - <span class="title-ref">DataFrame.interpolate</span> and <span class="title-ref">Series.interpolate</span> (other than `method`)
>   - <span class="title-ref">DataFrame.mask</span> and <span class="title-ref">Series.mask</span> (other than `cond` and `other`)
>   - <span class="title-ref">DataFrame.reset\_index</span> (other than `level`) and <span class="title-ref">Series.reset\_index</span>
>   - <span class="title-ref">DataFrame.set\_axis</span> and <span class="title-ref">Series.set\_axis</span> (other than `labels`)
>   - <span class="title-ref">DataFrame.set\_index</span> (other than `keys`)
>   - <span class="title-ref">DataFrame.sort\_index</span> and <span class="title-ref">Series.sort\_index</span>
>   - <span class="title-ref">DataFrame.sort\_values</span> (other than `by`) and <span class="title-ref">Series.sort\_values</span>
>   - <span class="title-ref">DataFrame.where</span> and <span class="title-ref">Series.where</span> (other than `cond` and `other`)
>   - <span class="title-ref">Index.set\_names</span> and <span class="title-ref">MultiIndex.set\_names</span> (except for `names`)
>   - <span class="title-ref">MultiIndex.codes</span> (except for `codes`)
>   - <span class="title-ref">MultiIndex.set\_levels</span> (except for `levels`)
>   - <span class="title-ref">Resampler.interpolate</span> (other than `method`)

## Performance improvements

  - Performance improvement in <span class="title-ref">IntervalIndex.isin</span> (`38353`)
  - Performance improvement in <span class="title-ref">Series.mean</span> for nullable data types (`34814`)
  - Performance improvement in <span class="title-ref">Series.isin</span> for nullable data types (`38340`)
  - Performance improvement in <span class="title-ref">DataFrame.fillna</span> with `method="pad"` or `method="backfill"` for nullable floating and nullable integer dtypes (`39953`)
  - Performance improvement in <span class="title-ref">DataFrame.corr</span> for `method=kendall` (`28329`)
  - Performance improvement in <span class="title-ref">DataFrame.corr</span> for `method=spearman` (`40956`, `41885`)
  - Performance improvement in <span class="title-ref">.Rolling.corr</span> and <span class="title-ref">.Rolling.cov</span> (`39388`)
  - Performance improvement in <span class="title-ref">.RollingGroupby.corr</span>, <span class="title-ref">.ExpandingGroupby.corr</span>, <span class="title-ref">.ExpandingGroupby.corr</span> and <span class="title-ref">.ExpandingGroupby.cov</span> (`39591`)
  - Performance improvement in <span class="title-ref">unique</span> for object data type (`37615`)
  - Performance improvement in <span class="title-ref">json\_normalize</span> for basic cases (including separators) (`40035` `15621`)
  - Performance improvement in <span class="title-ref">.ExpandingGroupby</span> aggregation methods (`39664`)
  - Performance improvement in <span class="title-ref">.Styler</span> where render times are more than 50% reduced and now matches <span class="title-ref">DataFrame.to\_html</span> (`39972` `39952`, `40425`)
  - The method <span class="title-ref">.Styler.set\_td\_classes</span> is now as performant as <span class="title-ref">.Styler.apply</span> and <span class="title-ref">.Styler.applymap</span>, and even more so in some cases (`40453`)
  - Performance improvement in <span class="title-ref">.ExponentialMovingWindow.mean</span> with `times` (`39784`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> when requiring the Python fallback implementation (`40176`)
  - Performance improvement in the conversion of a PyArrow Boolean array to a pandas nullable Boolean array (`41051`)
  - Performance improvement for concatenation of data with type <span class="title-ref">CategoricalDtype</span> (`40193`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.cummin</span>, <span class="title-ref">.SeriesGroupBy.cummin</span>, <span class="title-ref">.DataFrameGroupBy.cummax</span>, and <span class="title-ref">.SeriesGroupBy.cummax</span> with nullable data types (`37493`)
  - Performance improvement in <span class="title-ref">Series.nunique</span> with nan values (`40865`)
  - Performance improvement in <span class="title-ref">DataFrame.transpose</span>, <span class="title-ref">Series.unstack</span> with `DatetimeTZDtype` (`40149`)
  - Performance improvement in <span class="title-ref">Series.plot</span> and <span class="title-ref">DataFrame.plot</span> with entry point lazy loading (`41492`)

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">CategoricalIndex</span> incorrectly failing to raise `TypeError` when scalar data is passed (`38614`)
  - Bug in `CategoricalIndex.reindex` failed when the <span class="title-ref">Index</span> passed was not categorical but whose values were all labels in the category (`28690`)
  - Bug where constructing a <span class="title-ref">Categorical</span> from an object-dtype array of `date` objects did not round-trip correctly with `astype` (`38552`)
  - Bug in constructing a <span class="title-ref">DataFrame</span> from an `ndarray` and a <span class="title-ref">CategoricalDtype</span> (`38857`)
  - Bug in setting categorical values into an object-dtype column in a <span class="title-ref">DataFrame</span> (`39136`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> was raising an `IndexError` when the new index contained duplicates and the old index was a <span class="title-ref">CategoricalIndex</span> (`38906`)
  - Bug in <span class="title-ref">Categorical.fillna</span> with a tuple-like category raising `NotImplementedError` instead of `ValueError` when filling with a non-category tuple (`41914`)

### Datetimelike

  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> constructors sometimes dropping nanoseconds from <span class="title-ref">Timestamp</span> (resp. <span class="title-ref">Timedelta</span>) `data`, with `dtype=datetime64[ns]` (resp. `timedelta64[ns]`) (`38032`)
  - Bug in <span class="title-ref">DataFrame.first</span> and <span class="title-ref">Series.first</span> with an offset of one month returning an incorrect result when the first day is the last day of a month (`29623`)
  - Bug in constructing a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with mismatched `datetime64` data and `timedelta64` dtype, or vice-versa, failing to raise a `TypeError` (`38575`, `38764`, `38792`)
  - Bug in constructing a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a `datetime` object out of bounds for `datetime64[ns]` dtype or a `timedelta` object out of bounds for `timedelta64[ns]` dtype (`38792`, `38965`)
  - Bug in <span class="title-ref">DatetimeIndex.intersection</span>, <span class="title-ref">DatetimeIndex.symmetric\_difference</span>, <span class="title-ref">PeriodIndex.intersection</span>, <span class="title-ref">PeriodIndex.symmetric\_difference</span> always returning object-dtype when operating with <span class="title-ref">CategoricalIndex</span> (`38741`)
  - Bug in <span class="title-ref">DatetimeIndex.intersection</span> giving incorrect results with non-Tick frequencies with `n != 1` (`42104`)
  - Bug in <span class="title-ref">Series.where</span> incorrectly casting `datetime64` values to `int64` (`37682`)
  - Bug in <span class="title-ref">Categorical</span> incorrectly typecasting `datetime` object to `Timestamp` (`38878`)
  - Bug in comparisons between <span class="title-ref">Timestamp</span> object and `datetime64` objects just outside the implementation bounds for nanosecond `datetime64` (`39221`)
  - Bug in <span class="title-ref">Timestamp.round</span>, <span class="title-ref">Timestamp.floor</span>, <span class="title-ref">Timestamp.ceil</span> for values near the implementation bounds of <span class="title-ref">Timestamp</span> (`39244`)
  - Bug in <span class="title-ref">Timedelta.round</span>, <span class="title-ref">Timedelta.floor</span>, <span class="title-ref">Timedelta.ceil</span> for values near the implementation bounds of <span class="title-ref">Timedelta</span> (`38964`)
  - Bug in <span class="title-ref">date\_range</span> incorrectly creating <span class="title-ref">DatetimeIndex</span> containing `NaT` instead of raising `OutOfBoundsDatetime` in corner cases (`24124`)
  - Bug in <span class="title-ref">infer\_freq</span> incorrectly fails to infer 'H' frequency of <span class="title-ref">DatetimeIndex</span> if the latter has a timezone and crosses DST boundaries (`39556`)
  - Bug in <span class="title-ref">Series</span> backed by <span class="title-ref">DatetimeArray</span> or <span class="title-ref">TimedeltaArray</span> sometimes failing to set the array's `freq` to `None` (`41425`)

### Timedelta

  - Bug in constructing <span class="title-ref">Timedelta</span> from `np.timedelta64` objects with non-nanosecond units that are out of bounds for `timedelta64[ns]` (`38965`)
  - Bug in constructing a <span class="title-ref">TimedeltaIndex</span> incorrectly accepting `np.datetime64("NaT")` objects (`39462`)
  - Bug in constructing <span class="title-ref">Timedelta</span> from an input string with only symbols and no digits failed to raise an error (`39710`)
  - Bug in <span class="title-ref">TimedeltaIndex</span> and <span class="title-ref">to\_timedelta</span> failing to raise when passed non-nanosecond `timedelta64` arrays that overflow when converting to `timedelta64[ns]` (`40008`)

### Timezones

  - Bug in different `tzinfo` objects representing UTC not being treated as equivalent (`39216`)
  - Bug in `dateutil.tz.gettz("UTC")` not being recognized as equivalent to other UTC-representing tzinfos (`39276`)

### Numeric

  - Bug in <span class="title-ref">DataFrame.quantile</span>, <span class="title-ref">DataFrame.sort\_values</span> causing incorrect subsequent indexing behavior (`38351`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> raising an <span class="title-ref">IndexError</span> for empty `by` (`40258`)
  - Bug in <span class="title-ref">DataFrame.select\_dtypes</span> with `include=np.number` would drop numeric `ExtensionDtype` columns (`35340`)
  - Bug in <span class="title-ref">DataFrame.mode</span> and <span class="title-ref">Series.mode</span> not keeping consistent integer <span class="title-ref">Index</span> for empty input (`33321`)
  - Bug in <span class="title-ref">DataFrame.rank</span> when the DataFrame contained `np.inf` (`32593`)
  - Bug in <span class="title-ref">DataFrame.rank</span> with `axis=0` and columns holding incomparable types raising an `IndexError` (`38932`)
  - Bug in <span class="title-ref">Series.rank</span>, <span class="title-ref">DataFrame.rank</span>, <span class="title-ref">.DataFrameGroupBy.rank</span>, and <span class="title-ref">.SeriesGroupBy.rank</span> treating the most negative `int64` value as missing (`32859`)
  - Bug in <span class="title-ref">DataFrame.select\_dtypes</span> different behavior between Windows and Linux with `include="int"` (`36596`)
  - Bug in <span class="title-ref">DataFrame.apply</span> and <span class="title-ref">DataFrame.agg</span> when passed the argument `func="size"` would operate on the entire `DataFrame` instead of rows or columns (`39934`)
  - Bug in <span class="title-ref">DataFrame.transform</span> would raise a `SpecificationError` when passed a dictionary and columns were missing; will now raise a `KeyError` instead (`40004`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.rank</span> and <span class="title-ref">.SeriesGroupBy.rank</span> giving incorrect results with `pct=True` and equal values between consecutive groups (`40518`)
  - Bug in <span class="title-ref">Series.count</span> would result in an `int32` result on 32-bit platforms when argument `level=None` (`40908`)
  - Bug in <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> reductions with methods `any` and `all` not returning Boolean results for object data (`12863`, `35450`, `27709`)
  - Bug in <span class="title-ref">Series.clip</span> would fail if the Series contains NA values and has nullable int or float as a data type (`40851`)
  - Bug in <span class="title-ref">UInt64Index.where</span> and <span class="title-ref">UInt64Index.putmask</span> with an `np.int64` dtype `other` incorrectly raising `TypeError` (`41974`)
  - Bug in <span class="title-ref">DataFrame.agg</span> not sorting the aggregated axis in the order of the provided aggregation functions when one or more aggregation function fails to produce results (`33634`)
  - Bug in <span class="title-ref">DataFrame.clip</span> not interpreting missing values as no threshold (`40420`)

### Conversion

  - Bug in <span class="title-ref">Series.to\_dict</span> with `orient='records'` now returns Python native types (`25969`)
  - Bug in <span class="title-ref">Series.view</span> and <span class="title-ref">Index.view</span> when converting between datetime-like (`datetime64[ns]`, `datetime64[ns, tz]`, `timedelta64`, `period`) dtypes (`39788`)
  - Bug in creating a <span class="title-ref">DataFrame</span> from an empty `np.recarray` not retaining the original dtypes (`40121`)
  - Bug in <span class="title-ref">DataFrame</span> failing to raise a `TypeError` when constructing from a `frozenset` (`40163`)
  - Bug in <span class="title-ref">Index</span> construction silently ignoring a passed `dtype` when the data cannot be cast to that dtype (`21311`)
  - Bug in <span class="title-ref">StringArray.astype</span> falling back to NumPy and raising when converting to `dtype='categorical'` (`40450`)
  - Bug in <span class="title-ref">factorize</span> where, when given an array with a numeric NumPy dtype lower than int64, uint64 and float64, the unique values did not keep their original dtype (`41132`)
  - Bug in <span class="title-ref">DataFrame</span> construction with a dictionary containing an array-like with `ExtensionDtype` and `copy=True` failing to make a copy (`38939`)
  - Bug in <span class="title-ref">qcut</span> raising error when taking `Float64DType` as input (`40730`)
  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> construction with `datetime64[ns]` data and `dtype=object` resulting in `datetime` objects instead of <span class="title-ref">Timestamp</span> objects (`41599`)
  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> construction with `timedelta64[ns]` data and `dtype=object` resulting in `np.timedelta64` objects instead of <span class="title-ref">Timedelta</span> objects (`41599`)
  - Bug in <span class="title-ref">DataFrame</span> construction when given a two-dimensional object-dtype `np.ndarray` of <span class="title-ref">Period</span> or <span class="title-ref">Interval</span> objects failing to cast to <span class="title-ref">PeriodDtype</span> or <span class="title-ref">IntervalDtype</span>, respectively (`41812`)
  - Bug in constructing a <span class="title-ref">Series</span> from a list and a <span class="title-ref">PandasDtype</span> (`39357`)
  - Bug in creating a <span class="title-ref">Series</span> from a `range` object that does not fit in the bounds of `int64` dtype (`30173`)
  - Bug in creating a <span class="title-ref">Series</span> from a `dict` with all-tuple keys and an <span class="title-ref">Index</span> that requires reindexing (`41707`)
  - Bug in <span class="title-ref">.infer\_dtype</span> not recognizing Series, Index, or array with a Period dtype (`23553`)
  - Bug in <span class="title-ref">.infer\_dtype</span> raising an error for general <span class="title-ref">.ExtensionArray</span> objects. It will now return `"unknown-array"` instead of raising (`37367`)
  - Bug in <span class="title-ref">DataFrame.convert\_dtypes</span> incorrectly raised a `ValueError` when called on an empty DataFrame (`40393`)

### Strings

  - Bug in the conversion from `pyarrow.ChunkedArray` to <span class="title-ref">\~arrays.StringArray</span> when the original had zero chunks (`41040`)
  - Bug in <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> ignoring replacements with `regex=True` for `StringDType` data (`41333`, `35977`)
  - Bug in <span class="title-ref">Series.str.extract</span> with <span class="title-ref">\~arrays.StringArray</span> returning object dtype for an empty <span class="title-ref">DataFrame</span> (`41441`)
  - Bug in <span class="title-ref">Series.str.replace</span> where the `case` argument was ignored when `regex=False` (`41602`)

### Interval

  - Bug in <span class="title-ref">IntervalIndex.intersection</span> and <span class="title-ref">IntervalIndex.symmetric\_difference</span> always returning object-dtype when operating with <span class="title-ref">CategoricalIndex</span> (`38653`, `38741`)
  - Bug in <span class="title-ref">IntervalIndex.intersection</span> returning duplicates when at least one of the <span class="title-ref">Index</span> objects have duplicates which are present in the other (`38743`)
  - <span class="title-ref">IntervalIndex.union</span>, <span class="title-ref">IntervalIndex.intersection</span>, <span class="title-ref">IntervalIndex.difference</span>, and <span class="title-ref">IntervalIndex.symmetric\_difference</span> now cast to the appropriate dtype instead of raising a `TypeError` when operating with another <span class="title-ref">IntervalIndex</span> with incompatible dtype (`39267`)
  - <span class="title-ref">PeriodIndex.union</span>, <span class="title-ref">PeriodIndex.intersection</span>, <span class="title-ref">PeriodIndex.symmetric\_difference</span>, <span class="title-ref">PeriodIndex.difference</span> now cast to object dtype instead of raising `IncompatibleFrequency` when operating with another <span class="title-ref">PeriodIndex</span> with incompatible dtype (`39306`)
  - Bug in <span class="title-ref">IntervalIndex.is\_monotonic</span>, <span class="title-ref">IntervalIndex.get\_loc</span>, <span class="title-ref">IntervalIndex.get\_indexer\_for</span>, and <span class="title-ref">IntervalIndex.\_\_contains\_\_</span> when NA values are present (`41831`)

### Indexing

  - Bug in <span class="title-ref">Index.union</span> and <span class="title-ref">MultiIndex.union</span> dropping duplicate `Index` values when `Index` was not monotonic or `sort` was set to `False` (`36289`, `31326`, `40862`)
  - Bug in <span class="title-ref">CategoricalIndex.get\_indexer</span> failing to raise `InvalidIndexError` when non-unique (`38372`)
  - Bug in <span class="title-ref">IntervalIndex.get\_indexer</span> when `target` has `CategoricalDtype` and both the index and the target contain NA values (`41934`)
  - Bug in <span class="title-ref">Series.loc</span> raising a `ValueError` when input was filtered with a Boolean list and values to set were a list with lower dimension (`20438`)
  - Bug in inserting many new columns into a <span class="title-ref">DataFrame</span> causing incorrect subsequent indexing behavior (`38380`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> raising a `ValueError` when setting multiple values to duplicate columns (`15695`)
  - Bug in <span class="title-ref">DataFrame.loc</span>, <span class="title-ref">Series.loc</span>, <span class="title-ref">DataFrame.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_getitem\_\_</span> returning incorrect elements for non-monotonic <span class="title-ref">DatetimeIndex</span> for string slices (`33146`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> and <span class="title-ref">Series.reindex</span> with timezone aware indexes raising a `TypeError` for `method="ffill"` and `method="bfill"` and specified `tolerance` (`38566`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> with `datetime64[ns]` or `timedelta64[ns]` incorrectly casting to integers when the `fill_value` requires casting to object dtype (`39755`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> raising a `ValueError` when setting on an empty <span class="title-ref">DataFrame</span> using specified columns and a nonempty <span class="title-ref">DataFrame</span> value (`38831`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> raising a `ValueError` when operating on a unique column when the <span class="title-ref">DataFrame</span> has duplicate columns (`38521`)
  - Bug in <span class="title-ref">DataFrame.iloc.\_\_setitem\_\_</span> and <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> with mixed dtypes when setting with a dictionary value (`38335`)
  - Bug in <span class="title-ref">Series.loc.\_\_setitem\_\_</span> and <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> raising `KeyError` when provided a Boolean generator (`39614`)
  - Bug in <span class="title-ref">Series.iloc</span> and <span class="title-ref">DataFrame.iloc</span> raising a `KeyError` when provided a generator (`39614`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> not raising a `ValueError` when the right hand side is a <span class="title-ref">DataFrame</span> with wrong number of columns (`38604`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> raising a `ValueError` when setting a <span class="title-ref">Series</span> with a scalar indexer (`38303`)
  - Bug in <span class="title-ref">DataFrame.loc</span> dropping levels of a <span class="title-ref">MultiIndex</span> when the <span class="title-ref">DataFrame</span> used as input has only one row (`10521`)
  - Bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_getitem\_\_</span> always raising `KeyError` when slicing with existing strings where the <span class="title-ref">Index</span> has milliseconds (`33589`)
  - Bug in setting `timedelta64` or `datetime64` values into numeric <span class="title-ref">Series</span> failing to cast to object dtype (`39086`, `39619`)
  - Bug in setting <span class="title-ref">Interval</span> values into a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with mismatched <span class="title-ref">IntervalDtype</span> incorrectly casting the new values to the existing dtype (`39120`)
  - Bug in setting `datetime64` values into a <span class="title-ref">Series</span> with integer-dtype incorrectly casting the datetime64 values to integers (`39266`)
  - Bug in setting `np.datetime64("NaT")` into a <span class="title-ref">Series</span> with <span class="title-ref">Datetime64TZDtype</span> incorrectly treating the timezone-naive value as timezone-aware (`39769`)
  - Bug in <span class="title-ref">Index.get\_loc</span> not raising `KeyError` when `key=NaN` and `method` is specified but `NaN` is not in the <span class="title-ref">Index</span> (`39382`)
  - Bug in <span class="title-ref">DatetimeIndex.insert</span> when inserting `np.datetime64("NaT")` into a timezone-aware index incorrectly treating the timezone-naive value as timezone-aware (`39769`)
  - Bug in incorrectly raising in <span class="title-ref">Index.insert</span>, when setting a new column that cannot be held in the existing `frame.columns`, or in <span class="title-ref">Series.reset\_index</span> or <span class="title-ref">DataFrame.reset\_index</span> instead of casting to a compatible dtype (`39068`)
  - Bug in <span class="title-ref">RangeIndex.append</span> where a single object of length 1 was concatenated incorrectly (`39401`)
  - Bug in <span class="title-ref">RangeIndex.astype</span> where when converting to <span class="title-ref">CategoricalIndex</span>, the categories became a <span class="title-ref">Int64Index</span> instead of a <span class="title-ref">RangeIndex</span> (`41263`)
  - Bug in setting `numpy.timedelta64` values into an object-dtype <span class="title-ref">Series</span> using a Boolean indexer (`39488`)
  - Bug in setting numeric values into a into a boolean-dtypes <span class="title-ref">Series</span> using `at` or `iat` failing to cast to object-dtype (`39582`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> and <span class="title-ref">DataFrame.iloc.\_\_setitem\_\_</span> raising `ValueError` when trying to index with a row-slice and setting a list as values (`40440`)
  - Bug in <span class="title-ref">DataFrame.loc</span> not raising `KeyError` when the key was not found in <span class="title-ref">MultiIndex</span> and the levels were not fully specified (`41170`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> when setting-with-expansion incorrectly raising when the index in the expanding axis contained duplicates (`40096`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_getitem\_\_</span> with <span class="title-ref">MultiIndex</span> casting to float when at least one index column has float dtype and we retrieve a scalar (`41369`)
  - Bug in <span class="title-ref">DataFrame.loc</span> incorrectly matching non-Boolean index elements (`20432`)
  - Bug in indexing with `np.nan` on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a <span class="title-ref">CategoricalIndex</span> incorrectly raising `KeyError` when `np.nan` keys are present (`41933`)
  - Bug in <span class="title-ref">Series.\_\_delitem\_\_</span> with `ExtensionDtype` incorrectly casting to `ndarray` (`40386`)
  - Bug in <span class="title-ref">DataFrame.at</span> with a <span class="title-ref">CategoricalIndex</span> returning incorrect results when passed integer keys (`41846`)
  - Bug in <span class="title-ref">DataFrame.loc</span> returning a <span class="title-ref">MultiIndex</span> in the wrong order if an indexer has duplicates (`40978`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> raising a `TypeError` when using a `str` subclass as the column name with a <span class="title-ref">DatetimeIndex</span> (`37366`)
  - Bug in <span class="title-ref">PeriodIndex.get\_loc</span> failing to raise a `KeyError` when given a <span class="title-ref">Period</span> with a mismatched `freq` (`41670`)
  - Bug `.loc.__getitem__` with a <span class="title-ref">UInt64Index</span> and negative-integer keys raising `OverflowError` instead of `KeyError` in some cases, wrapping around to positive integers in others (`41777`)
  - Bug in <span class="title-ref">Index.get\_indexer</span> failing to raise `ValueError` in some cases with invalid `method`, `limit`, or `tolerance` arguments (`41918`)
  - Bug when slicing a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a <span class="title-ref">TimedeltaIndex</span> when passing an invalid string raising `ValueError` instead of a `TypeError` (`41821`)
  - Bug in <span class="title-ref">Index</span> constructor sometimes silently ignoring a specified `dtype` (`38879`)
  - <span class="title-ref">Index.where</span> behavior now mirrors <span class="title-ref">Index.putmask</span> behavior, i.e. `index.where(mask, other)` matches `index.putmask(~mask, other)` (`39412`)

### Missing

  - Bug in <span class="title-ref">Grouper</span> did not correctly propagate the `dropna` argument; <span class="title-ref">.DataFrameGroupBy.transform</span> now correctly handles missing values for `dropna=True` (`35612`)
  - Bug in <span class="title-ref">isna</span>, <span class="title-ref">Series.isna</span>, <span class="title-ref">Index.isna</span>, <span class="title-ref">DataFrame.isna</span>, and the corresponding `notna` functions not recognizing `Decimal("NaN")` objects (`39409`)
  - Bug in <span class="title-ref">DataFrame.fillna</span> not accepting a dictionary for the `downcast` keyword (`40809`)
  - Bug in <span class="title-ref">isna</span> not returning a copy of the mask for nullable types, causing any subsequent mask modification to change the original array (`40935`)
  - Bug in <span class="title-ref">DataFrame</span> construction with float data containing `NaN` and an integer `dtype` casting instead of retaining the `NaN` (`26919`)
  - Bug in <span class="title-ref">Series.isin</span> and <span class="title-ref">MultiIndex.isin</span> didn't treat all nans as equivalent if they were in tuples (`41836`)

### MultiIndex

  - Bug in <span class="title-ref">DataFrame.drop</span> raising a `TypeError` when the <span class="title-ref">MultiIndex</span> is non-unique and `level` is not provided (`36293`)
  - Bug in <span class="title-ref">MultiIndex.intersection</span> duplicating `NaN` in the result (`38623`)
  - Bug in <span class="title-ref">MultiIndex.equals</span> incorrectly returning `True` when the <span class="title-ref">MultiIndex</span> contained `NaN` even when they are differently ordered (`38439`)
  - Bug in <span class="title-ref">MultiIndex.intersection</span> always returning an empty result when intersecting with <span class="title-ref">CategoricalIndex</span> (`38653`)
  - Bug in <span class="title-ref">MultiIndex.difference</span> incorrectly raising `TypeError` when indexes contain non-sortable entries (`41915`)
  - Bug in <span class="title-ref">MultiIndex.reindex</span> raising a `ValueError` when used on an empty <span class="title-ref">MultiIndex</span> and indexing only a specific level (`41170`)
  - Bug in <span class="title-ref">MultiIndex.reindex</span> raising `TypeError` when reindexing against a flat <span class="title-ref">Index</span> (`41707`)

### I/O

  - Bug in <span class="title-ref">Index.\_\_repr\_\_</span> when `display.max_seq_items=1` (`38415`)
  - Bug in <span class="title-ref">read\_csv</span> not recognizing scientific notation if the argument `decimal` is set and `engine="python"` (`31920`)
  - Bug in <span class="title-ref">read\_csv</span> interpreting `NA` value as comment, when `NA` does contain the comment string fixed for `engine="python"` (`34002`)
  - Bug in <span class="title-ref">read\_csv</span> raising an `IndexError` with multiple header columns and `index_col` is specified when the file has no data rows (`38292`)
  - Bug in <span class="title-ref">read\_csv</span> not accepting `usecols` with a different length than `names` for `engine="python"` (`16469`)
  - Bug in <span class="title-ref">read\_csv</span> returning object dtype when `delimiter=","` with `usecols` and `parse_dates` specified for `engine="python"` (`35873`)
  - Bug in <span class="title-ref">read\_csv</span> raising a `TypeError` when `names` and `parse_dates` is specified for `engine="c"` (`33699`)
  - Bug in <span class="title-ref">read\_clipboard</span> and <span class="title-ref">DataFrame.to\_clipboard</span> not working in WSL (`38527`)
  - Allow custom error values for the `parse_dates` argument of <span class="title-ref">read\_sql</span>, <span class="title-ref">read\_sql\_query</span> and <span class="title-ref">read\_sql\_table</span> (`35185`)
  - Bug in <span class="title-ref">DataFrame.to\_hdf</span> and <span class="title-ref">Series.to\_hdf</span> raising a `KeyError` when trying to apply for subclasses of `DataFrame` or `Series` (`33748`)
  - Bug in <span class="title-ref">.HDFStore.put</span> raising a wrong `TypeError` when saving a DataFrame with non-string dtype (`34274`)
  - Bug in <span class="title-ref">json\_normalize</span> resulting in the first element of a generator object not being included in the returned DataFrame (`35923`)
  - Bug in <span class="title-ref">read\_csv</span> applying the thousands separator to date columns when the column should be parsed for dates and `usecols` is specified for `engine="python"` (`39365`)
  - Bug in <span class="title-ref">read\_excel</span> forward filling <span class="title-ref">MultiIndex</span> names when multiple header and index columns are specified (`34673`)
  - Bug in <span class="title-ref">read\_excel</span> not respecting <span class="title-ref">set\_option</span> (`34252`)
  - Bug in <span class="title-ref">read\_csv</span> not switching `true_values` and `false_values` for nullable Boolean dtype (`34655`)
  - Bug in <span class="title-ref">read\_json</span> when `orient="split"` not maintaining a numeric string index (`28556`)
  - <span class="title-ref">read\_sql</span> returned an empty generator if `chunksize` was non-zero and the query returned no results. Now returns a generator with a single empty DataFrame (`34411`)
  - Bug in <span class="title-ref">read\_hdf</span> returning unexpected records when filtering on categorical string columns using the `where` parameter (`39189`)
  - Bug in <span class="title-ref">read\_sas</span> raising a `ValueError` when `datetimes` were null (`39725`)
  - Bug in <span class="title-ref">read\_excel</span> dropping empty values from single-column spreadsheets (`39808`)
  - Bug in <span class="title-ref">read\_excel</span> loading trailing empty rows/columns for some filetypes (`41167`)
  - Bug in <span class="title-ref">read\_excel</span> raising an `AttributeError` when the excel file had a `MultiIndex` header followed by two empty rows and no index (`40442`)
  - Bug in <span class="title-ref">read\_excel</span>, <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_table</span>, <span class="title-ref">read\_fwf</span>, and <span class="title-ref">read\_clipboard</span> where one blank row after a `MultiIndex` header with no index would be dropped (`40442`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> misplacing the truncation column when `index=False` (`40904`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> adding an extra dot and misaligning the truncation row when `index=False` (`40904`)
  - Bug in <span class="title-ref">read\_orc</span> always raising an `AttributeError` (`40918`)
  - Bug in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> silently ignoring `prefix` if `names` and `prefix` are defined, now raising a `ValueError` (`39123`)
  - Bug in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_excel</span> not respecting the dtype for a duplicated column name when `mangle_dupe_cols` is set to `True` (`35211`)
  - Bug in <span class="title-ref">read\_csv</span> silently ignoring `sep` if `delimiter` and `sep` are defined, now raising a `ValueError` (`39823`)
  - Bug in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> misinterpreting arguments when `sys.setprofile` had been previously called (`41069`)
  - Bug in the conversion from PyArrow to pandas (e.g. for reading Parquet) with nullable dtypes and a PyArrow array whose data buffer size is not a multiple of the dtype size (`40896`)
  - Bug in <span class="title-ref">read\_excel</span> would raise an error when pandas could not determine the file type even though the user specified the `engine` argument (`41225`)
  - Bug in <span class="title-ref">read\_clipboard</span> copying from an excel file shifts values into the wrong column if there are null values in first column (`41108`)
  - Bug in <span class="title-ref">DataFrame.to\_hdf</span> and <span class="title-ref">Series.to\_hdf</span> raising a `TypeError` when trying to append a string column to an incompatible column (`41897`)

### Period

  - Comparisons of <span class="title-ref">Period</span> objects or <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> with mismatched `PeriodDtype` now behave like other mismatched-type comparisons, returning `False` for equals, `True` for not-equal, and raising `TypeError` for inequality checks (`39274`)

### Plotting

  - Bug in <span class="title-ref">plotting.scatter\_matrix</span> raising when 2d `ax` argument passed (`16253`)
  - Prevent warnings when Matplotlib's `constrained_layout` is enabled (`25261`)
  - Bug in <span class="title-ref">DataFrame.plot</span> was showing the wrong colors in the legend if the function was called repeatedly and some calls used `yerr` while others didn't (`39522`)
  - Bug in <span class="title-ref">DataFrame.plot</span> was showing the wrong colors in the legend if the function was called repeatedly and some calls used `secondary_y` and others use `legend=False` (`40044`)
  - Bug in <span class="title-ref">DataFrame.plot.box</span> when `dark_background` theme was selected, caps or min/max markers for the plot were not visible (`40769`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span> with <span class="title-ref">PeriodDtype</span> columns incorrectly casting results too aggressively (`38254`)
  - Bug in <span class="title-ref">.SeriesGroupBy.value\_counts</span> where unobserved categories in a grouped categorical Series were not tallied (`38672`)
  - Bug in <span class="title-ref">.SeriesGroupBy.value\_counts</span> where an error was raised on an empty Series (`39172`)
  - Bug in <span class="title-ref">.GroupBy.indices</span> would contain non-existent indices when null values were present in the groupby keys (`9304`)
  - Fixed bug in <span class="title-ref">.DataFrameGroupBy.sum</span> and <span class="title-ref">.SeriesGroupBy.sum</span> causing a loss of precision by now using Kahan summation (`38778`)
  - Fixed bug in <span class="title-ref">.DataFrameGroupBy.cumsum</span>, <span class="title-ref">.SeriesGroupBy.cumsum</span>, <span class="title-ref">.DataFrameGroupBy.mean</span>, and <span class="title-ref">.SeriesGroupBy.mean</span> causing loss of precision through using Kahan summation (`38934`)
  - Bug in <span class="title-ref">.Resampler.aggregate</span> and <span class="title-ref">DataFrame.transform</span> raising a `TypeError` instead of `SpecificationError` when missing keys had mixed dtypes (`39025`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.idxmin</span> and <span class="title-ref">.DataFrameGroupBy.idxmax</span> with `ExtensionDtype` columns (`38733`)
  - Bug in <span class="title-ref">Series.resample</span> would raise when the index was a <span class="title-ref">PeriodIndex</span> consisting of `NaT` (`39227`)
  - Bug in <span class="title-ref">.RollingGroupby.corr</span> and <span class="title-ref">.ExpandingGroupby.corr</span> where the groupby column would return `0` instead of `np.nan` when providing `other` that was longer than each group (`39591`)
  - Bug in <span class="title-ref">.ExpandingGroupby.corr</span> and <span class="title-ref">.ExpandingGroupby.cov</span> where `1` would be returned instead of `np.nan` when providing `other` that was longer than each group (`39591`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.mean</span>, <span class="title-ref">.SeriesGroupBy.mean</span>, <span class="title-ref">.DataFrameGroupBy.median</span>, <span class="title-ref">.SeriesGroupBy.median</span>, and <span class="title-ref">DataFrame.pivot\_table</span> not propagating metadata (`28283`)
  - Bug in <span class="title-ref">Series.rolling</span> and <span class="title-ref">DataFrame.rolling</span> not calculating window bounds correctly when window is an offset and dates are in descending order (`40002`)
  - Bug in <span class="title-ref">Series.groupby</span> and <span class="title-ref">DataFrame.groupby</span> on an empty `Series` or `DataFrame` would lose index, columns, and/or data types when directly using the methods `idxmax`, `idxmin`, `mad`, `min`, `max`, `sum`, `prod`, and `skew` or using them through `apply`, `aggregate`, or `resample` (`26411`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> where a <span class="title-ref">MultiIndex</span> would be created instead of an <span class="title-ref">Index</span> when used on a <span class="title-ref">.RollingGroupby</span> object (`39732`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.sample</span> where an error was raised when `weights` was specified and the index was an <span class="title-ref">Int64Index</span> (`39927`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.aggregate</span> and <span class="title-ref">.Resampler.aggregate</span> would sometimes raise a `SpecificationError` when passed a dictionary and columns were missing; will now always raise a `KeyError` instead (`40004`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.sample</span> where column selection was not applied before computing the result (`39928`)
  - Bug in <span class="title-ref">.ExponentialMovingWindow</span> when calling `__getitem__` would incorrectly raise a `ValueError` when providing `times` (`40164`)
  - Bug in <span class="title-ref">.ExponentialMovingWindow</span> when calling `__getitem__` would not retain `com`, `span`, `alpha` or `halflife` attributes (`40164`)
  - <span class="title-ref">.ExponentialMovingWindow</span> now raises a `NotImplementedError` when specifying `times` with `adjust=False` due to an incorrect calculation (`40098`)
  - Bug in <span class="title-ref">.ExponentialMovingWindowGroupby.mean</span> where the `times` argument was ignored when `engine='numba'` (`40951`)
  - Bug in <span class="title-ref">.ExponentialMovingWindowGroupby.mean</span> where the wrong times were used the in case of multiple groups (`40951`)
  - Bug in <span class="title-ref">.ExponentialMovingWindowGroupby</span> where the times vector and values became out of sync for non-trivial groups (`40951`)
  - Bug in <span class="title-ref">Series.asfreq</span> and <span class="title-ref">DataFrame.asfreq</span> dropping rows when the index was not sorted (`39805`)
  - Bug in aggregation functions for <span class="title-ref">DataFrame</span> not respecting `numeric_only` argument when `level` keyword was given (`40660`)
  - Bug in <span class="title-ref">.SeriesGroupBy.aggregate</span> where using a user-defined function to aggregate a Series with an object-typed <span class="title-ref">Index</span> causes an incorrect <span class="title-ref">Index</span> shape (`40014`)
  - Bug in <span class="title-ref">.RollingGroupby</span> where `as_index=False` argument in `groupby` was ignored (`39433`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.any</span>, <span class="title-ref">.SeriesGroupBy.any</span>, <span class="title-ref">.DataFrameGroupBy.all</span> and <span class="title-ref">.SeriesGroupBy.all</span> raising a `ValueError` when using with nullable type columns holding `NA` even with `skipna=True` (`40585`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cummin</span>, <span class="title-ref">.SeriesGroupBy.cummin</span>, <span class="title-ref">.DataFrameGroupBy.cummax</span> and <span class="title-ref">.SeriesGroupBy.cummax</span> incorrectly rounding integer values near the `int64` implementations bounds (`40767`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.rank</span> and <span class="title-ref">.SeriesGroupBy.rank</span> with nullable dtypes incorrectly raising a `TypeError` (`41010`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cummin</span>, <span class="title-ref">.SeriesGroupBy.cummin</span>, <span class="title-ref">.DataFrameGroupBy.cummax</span> and <span class="title-ref">.SeriesGroupBy.cummax</span> computing wrong result with nullable data types too large to roundtrip when casting to float (`37493`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> returning mean zero for all `NaN` window with `min_periods=0` if calculation is not numerical stable (`41053`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> returning sum not zero for all `NaN` window with `min_periods=0` if calculation is not numerical stable (`41053`)
  - Bug in <span class="title-ref">.SeriesGroupBy.agg</span> failing to retain ordered <span class="title-ref">CategoricalDtype</span> on order-preserving aggregations (`41147`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.min</span>, <span class="title-ref">.SeriesGroupBy.min</span>, <span class="title-ref">.DataFrameGroupBy.max</span> and <span class="title-ref">.SeriesGroupBy.max</span> with multiple object-dtype columns and `numeric_only=False` incorrectly raising a `ValueError` (`41111`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.rank</span> with the GroupBy object's `axis=0` and the `rank` method's keyword `axis=1` (`41320`)
  - Bug in <span class="title-ref">DataFrameGroupBy.\_\_getitem\_\_</span> with non-unique columns incorrectly returning a malformed <span class="title-ref">SeriesGroupBy</span> instead of <span class="title-ref">DataFrameGroupBy</span> (`41427`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> with non-unique columns incorrectly raising an `AttributeError` (`41427`)
  - Bug in <span class="title-ref">.Resampler.apply</span> with non-unique columns incorrectly dropping duplicated columns (`41445`)
  - Bug in <span class="title-ref">Series.groupby</span> aggregations incorrectly returning empty <span class="title-ref">Series</span> instead of raising `TypeError` on aggregations that are invalid for its dtype, e.g. `.prod` with `datetime64[ns]` dtype (`41342`)
  - Bug in <span class="title-ref">DataFrameGroupBy</span> aggregations incorrectly failing to drop columns with invalid dtypes for that aggregation when there are no valid columns (`41291`)
  - Bug in <span class="title-ref">DataFrame.rolling.\_\_iter\_\_</span> where `on` was not assigned to the index of the resulting objects (`40373`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.DataFrameGroupBy.agg</span> with `engine="numba"` where `*args` were being cached with the user passed function (`41647`)
  - Bug in <span class="title-ref">DataFrameGroupBy</span> methods `agg`, `transform`, `sum`, `bfill`, `ffill`, `pad`, `pct_change`, `shift`, `ohlc` dropping `.columns.names` (`41497`)

### Reshaping

  - Bug in <span class="title-ref">merge</span> raising error when performing an inner join with partial index and `right_index=True` when there was no overlap between indices (`33814`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> with missing levels led to incorrect index names (`37510`)
  - Bug in <span class="title-ref">merge\_asof</span> propagating the right Index with `left_index=True` and `right_on` specification instead of left Index (`33463`)
  - Bug in <span class="title-ref">DataFrame.join</span> on a DataFrame with a <span class="title-ref">MultiIndex</span> returned the wrong result when one of both indexes had only one level (`36909`)
  - <span class="title-ref">merge\_asof</span> now raises a `ValueError` instead of a cryptic `TypeError` in case of non-numerical merge columns (`29130`)
  - Bug in <span class="title-ref">DataFrame.join</span> not assigning values correctly when the DataFrame had a <span class="title-ref">MultiIndex</span> where at least one dimension had dtype `Categorical` with non-alphabetically sorted categories (`38502`)
  - <span class="title-ref">Series.value\_counts</span> and <span class="title-ref">Series.mode</span> now return consistent keys in original order (`12679`, `11227` and `39007`)
  - Bug in <span class="title-ref">DataFrame.stack</span> not handling `NaN` in <span class="title-ref">MultiIndex</span> columns correctly (`39481`)
  - Bug in <span class="title-ref">DataFrame.apply</span> would give incorrect results when the argument `func` was a string, `axis=1`, and the axis argument was not supported; now raises a `ValueError` instead (`39211`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> not reshaping the index correctly after sorting on columns when `ignore_index=True` (`39464`)
  - Bug in <span class="title-ref">DataFrame.append</span> returning incorrect dtypes with combinations of `ExtensionDtype` dtypes (`39454`)
  - Bug in <span class="title-ref">DataFrame.append</span> returning incorrect dtypes when used with combinations of `datetime64` and `timedelta64` dtypes (`39574`)
  - Bug in <span class="title-ref">DataFrame.append</span> with a <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> and appending a <span class="title-ref">Series</span> whose <span class="title-ref">Index</span> is not a <span class="title-ref">MultiIndex</span> (`41707`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> returning a <span class="title-ref">MultiIndex</span> for a single value when operating on an empty DataFrame (`13483`)
  - <span class="title-ref">Index</span> can now be passed to the <span class="title-ref">numpy.all</span> function (`40180`)
  - Bug in <span class="title-ref">DataFrame.stack</span> not preserving `CategoricalDtype` in a <span class="title-ref">MultiIndex</span> (`36991`)
  - Bug in <span class="title-ref">to\_datetime</span> raising an error when the input sequence contained unhashable items (`39756`)
  - Bug in <span class="title-ref">Series.explode</span> preserving the index when `ignore_index` was `True` and values were scalars (`40487`)
  - Bug in <span class="title-ref">to\_datetime</span> raising a `ValueError` when <span class="title-ref">Series</span> contains `None` and `NaT` and has more than 50 elements (`39882`)
  - Bug in <span class="title-ref">Series.unstack</span> and <span class="title-ref">DataFrame.unstack</span> with object-dtype values containing timezone-aware datetime objects incorrectly raising `TypeError` (`41875`)
  - Bug in <span class="title-ref">DataFrame.melt</span> raising `InvalidIndexError` when <span class="title-ref">DataFrame</span> has duplicate columns used as `value_vars` (`41951`)

### Sparse

  - Bug in <span class="title-ref">DataFrame.sparse.to\_coo</span> raising a `KeyError` with columns that are a numeric <span class="title-ref">Index</span> without a `0` (`18414`)
  - Bug in <span class="title-ref">SparseArray.astype</span> with `copy=False` producing incorrect results when going from integer dtype to floating dtype (`34456`)
  - Bug in <span class="title-ref">SparseArray.max</span> and <span class="title-ref">SparseArray.min</span> would always return an empty result (`40921`)

### ExtensionArray

  - Bug in <span class="title-ref">DataFrame.where</span> when `other` is a Series with an <span class="title-ref">ExtensionDtype</span> (`38729`)
  - Fixed bug where <span class="title-ref">Series.idxmax</span>, <span class="title-ref">Series.idxmin</span>, <span class="title-ref">Series.argmax</span>, and <span class="title-ref">Series.argmin</span> would fail when the underlying data is an <span class="title-ref">ExtensionArray</span> (`32749`, `33719`, `36566`)
  - Fixed bug where some properties of subclasses of <span class="title-ref">PandasExtensionDtype</span> where improperly cached (`40329`)
  - Bug in <span class="title-ref">DataFrame.mask</span> where masking a DataFrame with an <span class="title-ref">ExtensionDtype</span> raises a `ValueError` (`40941`)

### Styler

  - Bug in <span class="title-ref">.Styler</span> where the `subset` argument in methods raised an error for some valid MultiIndex slices (`33562`)
  - <span class="title-ref">.Styler</span> rendered HTML output has seen minor alterations to support w3 good code standards (`39626`)
  - Bug in <span class="title-ref">.Styler</span> where rendered HTML was missing a column class identifier for certain header cells (`39716`)
  - Bug in <span class="title-ref">.Styler.background\_gradient</span> where text-color was not determined correctly (`39888`)
  - Bug in <span class="title-ref">.Styler.set\_table\_styles</span> where multiple elements in CSS-selectors of the `table_styles` argument were not correctly added (`34061`)
  - Bug in <span class="title-ref">.Styler</span> where copying from Jupyter dropped the top left cell and misaligned headers (`12147`)
  - Bug in <span class="title-ref">Styler.where</span> where `kwargs` were not passed to the applicable callable (`40845`)
  - Bug in <span class="title-ref">.Styler</span> causing CSS to duplicate on multiple renders (`39395`, `40334`)

### Other

  - `inspect.getmembers(Series)` no longer raises an `AbstractMethodError` (`38782`)
  - Bug in <span class="title-ref">Series.where</span> with numeric dtype and `other=None` not casting to `nan` (`39761`)
  - Bug in <span class="title-ref">.assert\_series\_equal</span>, <span class="title-ref">.assert\_frame\_equal</span>, <span class="title-ref">.assert\_index\_equal</span> and <span class="title-ref">.assert\_extension\_array\_equal</span> incorrectly raising when an attribute has an unrecognized NA type (`39461`)
  - Bug in <span class="title-ref">.assert\_index\_equal</span> with `exact=True` not raising when comparing <span class="title-ref">CategoricalIndex</span> instances with `Int64Index` and `RangeIndex` categories (`41263`)
  - Bug in <span class="title-ref">DataFrame.equals</span>, <span class="title-ref">Series.equals</span>, and <span class="title-ref">Index.equals</span> with object-dtype containing `np.datetime64("NaT")` or `np.timedelta64("NaT")` (`39650`)
  - Bug in <span class="title-ref">show\_versions</span> where console JSON output was not proper JSON (`39701`)
  - pandas can now compile on z/OS when using [xlc](https://www.ibm.com/products/xl-cpp-compiler-zos) (`35826`)
  - Bug in <span class="title-ref">pandas.util.hash\_pandas\_object</span> not recognizing `hash_key`, `encoding` and `categorize` when the input object type is a <span class="title-ref">DataFrame</span> (`41404`)

## Contributors

<div class="contributors">

v1.2.5..v1.3.0

</div>

---

v1.3.1.md

---

# What's new in 1.3.1 (July 25, 2021)

These are the changes in pandas 1.3.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - pandas could not be built on PyPy (`42355`)
  - <span class="title-ref">DataFrame</span> constructed with an older version of pandas could not be unpickled (`42345`)
  - Performance regression in constructing a <span class="title-ref">DataFrame</span> from a dictionary of dictionaries (`42248`)
  - Fixed regression in <span class="title-ref">DataFrame.agg</span> dropping values when the DataFrame had an Extension Array dtype, a duplicate index, and `axis=1` (`42380`)
  - Fixed regression in <span class="title-ref">DataFrame.astype</span> changing the order of noncontiguous data (`42396`)
  - Performance regression in <span class="title-ref">DataFrame</span> in reduction operations requiring casting such as <span class="title-ref">DataFrame.mean</span> on integer data (`38592`)
  - Performance regression in <span class="title-ref">DataFrame.to\_dict</span> and <span class="title-ref">Series.to\_dict</span> when `orient` argument one of "records", "dict", or "split" (`42352`)
  - Fixed regression in indexing with a `list` subclass incorrectly raising `TypeError` (`42433`, `42461`)
  - Fixed regression in <span class="title-ref">DataFrame.isin</span> and <span class="title-ref">Series.isin</span> raising `TypeError` with nullable data containing at least one missing value (`42405`)
  - Regression in <span class="title-ref">concat</span> between objects with bool dtype and integer dtype casting to object instead of to integer (`42092`)
  - Bug in <span class="title-ref">Series</span> constructor not accepting a `dask.Array` (`38645`)
  - Fixed regression for `SettingWithCopyWarning` displaying incorrect stacklevel (`42570`)
  - Fixed regression for <span class="title-ref">merge\_asof</span> raising `KeyError` when one of the `by` columns is in the index (`34488`)
  - Fixed regression in <span class="title-ref">to\_datetime</span> returning pd.NaT for inputs that produce duplicated values, when `cache=True` (`42259`)
  - Fixed regression in <span class="title-ref">SeriesGroupBy.value\_counts</span> that resulted in an `IndexError` when called on a Series with one row (`42618`)

## Bug fixes

  - Fixed bug in <span class="title-ref">DataFrame.transpose</span> dropping values when the DataFrame had an Extension Array dtype and a duplicate index (`42380`)
  - Fixed bug in <span class="title-ref">DataFrame.to\_xml</span> raising `KeyError` when called with `index=False` and an offset index (`42458`)
  - Fixed bug in <span class="title-ref">.Styler.set\_sticky</span> not handling index names correctly for single index columns case (`42537`)
  - Fixed bug in <span class="title-ref">DataFrame.copy</span> failing to consolidate blocks in the result (`42579`)

## Contributors

<div class="contributors">

v1.3.0..v1.3.1

</div>

---

v1.3.2.md

---

# What's new in 1.3.2 (August 15, 2021)

These are the changes in pandas 1.3.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Performance regression in <span class="title-ref">DataFrame.isin</span> and <span class="title-ref">Series.isin</span> for nullable data types (`42714`)
  - Regression in updating values of <span class="title-ref">Series</span> using boolean index, created by using <span class="title-ref">DataFrame.pop</span> (`42530`)
  - Regression in <span class="title-ref">DataFrame.from\_records</span> with empty records (`42456`)
  - Fixed regression in <span class="title-ref">DataFrame.shift</span> where `TypeError` occurred when shifting DataFrame created by concatenation of slices and fills with values (`42719`)
  - Regression in <span class="title-ref">DataFrame.agg</span> when the `func` argument returned lists and `axis=1` (`42727`)
  - Regression in <span class="title-ref">DataFrame.drop</span> does nothing if <span class="title-ref">MultiIndex</span> has duplicates and indexer is a tuple or list of tuples (`42771`)
  - Fixed regression where <span class="title-ref">read\_csv</span> raised a `ValueError` when parameters `names` and `prefix` were both set to `None` (`42387`)
  - Fixed regression in comparisons between <span class="title-ref">Timestamp</span> object and `datetime64` objects outside the implementation bounds for nanosecond `datetime64` (`42794`)
  - Fixed regression in <span class="title-ref">.Styler.highlight\_min</span> and <span class="title-ref">.Styler.highlight\_max</span> where `pandas.NA` was not successfully ignored (`42650`)
  - Fixed regression in <span class="title-ref">concat</span> where `copy=False` was not honored in `axis=1` Series concatenation (`42501`)
  - Regression in <span class="title-ref">Series.nlargest</span> and <span class="title-ref">Series.nsmallest</span> with nullable integer or float dtype (`42816`)
  - Fixed regression in <span class="title-ref">Series.quantile</span> with <span class="title-ref">Int64Dtype</span> (`42626`)
  - Fixed regression in <span class="title-ref">Series.groupby</span> and <span class="title-ref">DataFrame.groupby</span> where supplying the `by` argument with a Series named with a tuple would incorrectly raise (`42731`)

## Bug fixes

  - Bug in <span class="title-ref">read\_excel</span> modifies the dtypes dictionary when reading a file with duplicate columns (`42462`)
  - 1D slices over extension types turn into N-dimensional slices over ExtensionArrays (`42430`)
  - Fixed bug in <span class="title-ref">Series.rolling</span> and <span class="title-ref">DataFrame.rolling</span> not calculating window bounds correctly for the first row when `center=True` and `window` is an offset that covers all the rows (`42753`)
  - <span class="title-ref">.Styler.hide\_columns</span> now hides the index name header row as well as column headers (`42101`)
  - <span class="title-ref">.Styler.set\_sticky</span> has amended CSS to control the column/index names and ensure the correct sticky positions (`42537`)
  - Bug in de-serializing datetime indexes in PYTHONOPTIMIZED mode (`42866`)

## Contributors

<div class="contributors">

v1.3.1..v1.3.2

</div>

---

v1.3.3.md

---

# What's new in 1.3.3 (September 12, 2021)

These are the changes in pandas 1.3.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame</span> constructor failing to broadcast for defined <span class="title-ref">Index</span> and len one list of <span class="title-ref">Timestamp</span> (`42810`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span> incorrectly raising in some cases (`42390`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> where `nan` values were dropped even with `dropna=False` (`43205`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> which were failing with `pandas.NA` (`42849`)
  - Fixed regression in <span class="title-ref">merge</span> where `on` columns with `ExtensionDtype` or `bool` data types were cast to `object` in `right` and `outer` merge (`40073`)
  - Fixed regression in <span class="title-ref">RangeIndex.where</span> and <span class="title-ref">RangeIndex.putmask</span> raising `AssertionError` when result did not represent a <span class="title-ref">RangeIndex</span> (`43240`)
  - Fixed regression in <span class="title-ref">read\_parquet</span> where the `fastparquet` engine would not work properly with fastparquet 0.7.0 (`43075`)
  - Fixed regression in <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> raising `ValueError` when setting array as cell value (`43422`)
  - Fixed regression in <span class="title-ref">is\_list\_like</span> where objects with `__iter__` set to `None` would be identified as iterable (`43373`)
  - Fixed regression in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> raising error for slice of <span class="title-ref">DatetimeIndex</span> when index is non monotonic (`43223`)
  - Fixed regression in <span class="title-ref">.Resampler.aggregate</span> when used after column selection would raise if `func` is a list of aggregation functions (`42905`)
  - Fixed regression in <span class="title-ref">DataFrame.corr</span> where Kendall correlation would produce incorrect results for columns with repeated values (`43401`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby</span> where aggregation on columns with object types dropped results on those columns (`42395`, `43108`)
  - Fixed regression in <span class="title-ref">Series.fillna</span> raising `TypeError` when filling `float` `Series` with list-like fill value having a dtype which couldn't cast lostlessly (like `float32` filled with `float64`) (`43424`)
  - Fixed regression in <span class="title-ref">read\_csv</span> raising `AttributeError` when the file handle is an `tempfile.SpooledTemporaryFile` object (`43439`)
  - Fixed performance regression in <span class="title-ref">core.window.ewm.ExponentialMovingWindow.mean</span> (`42333`)

## Performance improvements

  - Performance improvement for <span class="title-ref">DataFrame.\_\_setitem\_\_</span> when the key or value is not a <span class="title-ref">DataFrame</span>, or key is not list-like (`43274`)

## Bug fixes

  - Fixed bug in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.DataFrameGroupBy.transform</span> with `engine="numba"` where `index` data was not being correctly passed into `func` (`43133`)

## Contributors

<div class="contributors">

v1.3.2..v1.3.3

</div>

---

v1.3.4.md

---

# What's new in 1.3.4 (October 17, 2021)

These are the changes in pandas 1.3.4. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.convert\_dtypes</span> incorrectly converts byte strings to strings (`43183`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.SeriesGroupBy.agg</span> were failing silently with mixed data types along `axis=1` and <span class="title-ref">MultiIndex</span> (`43209`)
  - Fixed regression in <span class="title-ref">merge</span> with integer and `NaN` keys failing with `outer` merge (`43550`)
  - Fixed regression in <span class="title-ref">DataFrame.corr</span> raising `ValueError` with `method="spearman"` on 32-bit platforms (`43588`)
  - Fixed performance regression in <span class="title-ref">MultiIndex.equals</span> (`43549`)
  - Fixed performance regression in <span class="title-ref">.DataFrameGroupBy.first</span>, <span class="title-ref">.SeriesGroupBy.first</span>, <span class="title-ref">.DataFrameGroupBy.last</span>, and <span class="title-ref">.SeriesGroupBy.last</span> with <span class="title-ref">StringDtype</span> (`41596`)
  - Fixed regression in <span class="title-ref">Series.cat.reorder\_categories</span> failing to update the categories on the `Series` (`43232`)
  - Fixed regression in <span class="title-ref">Series.cat.categories</span> setter failing to update the categories on the `Series` (`43334`)
  - Fixed regression in <span class="title-ref">read\_csv</span> raising `UnicodeDecodeError` exception when `memory_map=True` (`43540`)
  - Fixed regression in <span class="title-ref">DataFrame.explode</span> raising `AssertionError` when `column` is any scalar which is not a string (`43314`)
  - Fixed regression in <span class="title-ref">Series.aggregate</span> attempting to pass `args` and `kwargs` multiple times to the user supplied `func` in certain cases (`43357`)
  - Fixed regression when iterating over a <span class="title-ref">DataFrame.groupby.rolling</span> object causing the resulting DataFrames to have an incorrect index if the input groupings were not sorted (`43386`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby.rolling.cov</span> and <span class="title-ref">DataFrame.groupby.rolling.corr</span> computing incorrect results if the input groupings were not sorted (`43386`)

## Bug fixes

  - Fixed bug in <span class="title-ref">pandas.DataFrame.groupby.rolling</span> and <span class="title-ref">pandas.api.indexers.FixedForwardWindowIndexer</span> leading to segfaults and window endpoints being mixed across groups (`43267`)
  - Fixed bug in <span class="title-ref">.DataFrameGroupBy.mean</span> and <span class="title-ref">.SeriesGroupBy.mean</span> with datetimelike values including `NaT` values returning incorrect results (`43132`)
  - Fixed bug in <span class="title-ref">Series.aggregate</span> not passing the first `args` to the user supplied `func` in certain cases (`43357`)
  - Fixed memory leaks in <span class="title-ref">Series.rolling.quantile</span> and <span class="title-ref">Series.rolling.median</span> (`43339`)

## Other

  - The minimum version of Cython needed to compile pandas is now `0.29.24` (`43729`)

## Contributors

<div class="contributors">

v1.3.3..v1.3.4

</div>

---

v1.3.5.md

---

# What's new in 1.3.5 (December 12, 2021)

These are the changes in pandas 1.3.5. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">Series.equals</span> when comparing floats with dtype object to None (`44190`)
  - Fixed regression in <span class="title-ref">merge\_asof</span> raising error when array was supplied as join key (`42844`)
  - Fixed regression when resampling <span class="title-ref">DataFrame</span> with <span class="title-ref">DateTimeIndex</span> with empty groups and `uint8`, `uint16` or `uint32` columns incorrectly raising `RuntimeError` (`43329`)
  - Fixed regression in creating a <span class="title-ref">DataFrame</span> from a timezone-aware <span class="title-ref">Timestamp</span> scalar near a Daylight Savings Time transition (`42505`)
  - Fixed performance regression in <span class="title-ref">read\_csv</span> (`44106`)
  - Fixed regression in <span class="title-ref">Series.duplicated</span> and <span class="title-ref">Series.drop\_duplicates</span> when Series has <span class="title-ref">Categorical</span> dtype with boolean categories (`44351`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.sum</span> and <span class="title-ref">.SeriesGroupBy.sum</span> with `timedelta64[ns]` dtype containing `NaT` failing to treat that value as NA (`42659`)
  - Fixed regression in <span class="title-ref">.RollingGroupby.cov</span> and <span class="title-ref">.RollingGroupby.corr</span> when `other` had the same shape as each group would incorrectly return superfluous groups in the result (`42915`)

## Contributors

<div class="contributors">

v1.3.4..v1.3.5|HEAD

</div>

---

v1.4.0.md

---

# What's new in 1.4.0 (January 22, 2022)

These are the changes in pandas 1.4.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### Improved warning messages

Previously, warning messages may have pointed to lines within the pandas library. Running the script `setting_with_copy_warning.py`

`` `python     import pandas as pd      df = pd.DataFrame({'a': [1, 2, 3]})     df[:2].loc[:, 'a'] = 5  with pandas 1.3 resulted in::      .../site-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning:     A value is trying to be set on a copy of a slice from a DataFrame.  This made it difficult to determine where the warning was being generated from. ``\` Now pandas will inspect the call stack, reporting the first line outside of the pandas library that gave rise to the warning. The output of the above script is now:

    setting_with_copy_warning.py:4: SettingWithCopyWarning:
    A value is trying to be set on a copy of a slice from a DataFrame.

### Index can hold arbitrary ExtensionArrays

Until now, passing a custom <span class="title-ref">ExtensionArray</span> to `pd.Index` would cast the array to `object` dtype. Now <span class="title-ref">Index</span> can directly hold arbitrary ExtensionArrays (`43930`).

*Previous behavior*:

<div class="ipython">

python

arr = pd.array(\[1, 2, pd.NA\]) idx = pd.Index(arr)

</div>

In the old behavior, `idx` would be object-dtype:

*Previous behavior*:

`` `ipython    In [1]: idx    Out[1]: Index([1, 2, <NA>], dtype='object')  With the new behavior, we keep the original dtype:  *New behavior*:  .. ipython:: python     idx  One exception to this is ``SparseArray`, which will continue to cast to numpy`\` dtype until pandas 2.0. At that point it will retain its dtype like other ExtensionArrays.

### Styler

<span class="title-ref">.Styler</span> has been further developed in 1.4.0. The following general enhancements have been made:

>   - Styling and formatting of indexes has been added, with <span class="title-ref">.Styler.apply\_index</span>, <span class="title-ref">.Styler.applymap\_index</span> and <span class="title-ref">.Styler.format\_index</span>. These mirror the signature of the methods already used to style and format data values, and work with both HTML, LaTeX and Excel format (`41893`, `43101`, `41993`, `41995`)
>   - The new method <span class="title-ref">.Styler.hide</span> deprecates <span class="title-ref">.Styler.hide\_index</span> and <span class="title-ref">.Styler.hide\_columns</span> (`43758`)
>   - The keyword arguments `level` and `names` have been added to <span class="title-ref">.Styler.hide</span> (and implicitly to the deprecated methods <span class="title-ref">.Styler.hide\_index</span> and <span class="title-ref">.Styler.hide\_columns</span>) for additional control of visibility of MultiIndexes and of Index names (`25475`, `43404`, `43346`)
>   - The <span class="title-ref">.Styler.export</span> and <span class="title-ref">.Styler.use</span> have been updated to address all of the added functionality from v1.2.0 and v1.3.0 (`40675`)
>   - Global options under the category `pd.options.styler` have been extended to configure default `Styler` properties which address formatting, encoding, and HTML and LaTeX rendering. Note that formerly `Styler` relied on `display.html.use_mathjax`, which has now been replaced by `styler.html.mathjax` (`41395`)
>   - Validation of certain keyword arguments, e.g. `caption` (`43368`)
>   - Various bug fixes as recorded below

Additionally there are specific enhancements to the HTML specific rendering:

>   - <span class="title-ref">.Styler.bar</span> introduces additional arguments to control alignment and display (`26070`, `36419`), and it also validates the input arguments `width` and `height` (`42511`)
>   - <span class="title-ref">.Styler.to\_html</span> introduces keyword arguments `sparse_index`, `sparse_columns`, `bold_headers`, `caption`, `max_rows` and `max_columns` (`41946`, `43149`, `42972`)
>   - <span class="title-ref">.Styler.to\_html</span> omits CSSStyle rules for hidden table elements as a performance enhancement (`43619`)
>   - Custom CSS classes can now be directly specified without string replacement (`43686`)
>   - Ability to render hyperlinks automatically via a new `hyperlinks` formatting keyword argument (`45058`)

There are also some LaTeX specific enhancements:

>   - <span class="title-ref">.Styler.to\_latex</span> introduces keyword argument `environment`, which also allows a specific "longtable" entry through a separate jinja2 template (`41866`)
>   - Naive sparsification is now possible for LaTeX without the necessity of including the multirow package (`43369`)
>   - *cline* support has been added for <span class="title-ref">MultiIndex</span> row sparsification through a keyword argument (`45138`)

### Multi-threaded CSV reading with a new CSV Engine based on pyarrow

<span class="title-ref">pandas.read\_csv</span> now accepts `engine="pyarrow"` (requires at least `pyarrow` 1.0.1) as an argument, allowing for faster csv parsing on multicore machines with pyarrow installed. See the \[I/O docs \</user\_guide/io\>\](I/O docs \</user\_guide/io\>.md) for more info. (`23697`, `43706`)

### Rank function for rolling and expanding windows

Added `rank` function to <span class="title-ref">Rolling</span> and <span class="title-ref">Expanding</span>. The new function supports the `method`, `ascending`, and `pct` flags of <span class="title-ref">DataFrame.rank</span>. The `method` argument supports `min`, `max`, and `average` ranking methods. Example:

<div class="ipython">

python

s = pd.Series(\[1, 4, 2, 3, 5, 3\]) s.rolling(3).rank()

s.rolling(3).rank(method="max")

</div>

### Groupby positional indexing

It is now possible to specify positional ranges relative to the ends of each group.

Negative arguments for <span class="title-ref">.DataFrameGroupBy.head</span>, <span class="title-ref">.SeriesGroupBy.head</span>, <span class="title-ref">.DataFrameGroupBy.tail</span>, and <span class="title-ref">.SeriesGroupBy.tail</span> now work correctly and result in ranges relative to the end and start of each group, respectively. Previously, negative arguments returned empty frames.

<div class="ipython">

python

  - df = pd.DataFrame(\[\["g", "g0"\], \["g", "g1"\], \["g", "g2"\], \["g", "g3"\],  
    \["h", "h0"\], \["h", "h1"\]\], columns=\["A", "B"\])

df.groupby("A").head(-1)

</div>

<span class="title-ref">.DataFrameGroupBy.nth</span> and <span class="title-ref">.SeriesGroupBy.nth</span> now accept a slice or list of integers and slices.

<div class="ipython">

python

df.groupby("A").nth(slice(1, -1)) df.groupby("A").nth(\[slice(None, 1), slice(-1, None)\])

</div>

<span class="title-ref">.DataFrameGroupBy.nth</span> and <span class="title-ref">.SeriesGroupBy.nth</span> now accept index notation.

<div class="ipython">

python

df.groupby("A").nth\[1, -1\] df.groupby("A").nth\[1:-1\] df.groupby("A").nth\[:1, -1:\]

</div>

### DataFrame.from\_dict and DataFrame.to\_dict have new `'tight'` option

A new `'tight'` dictionary format that preserves <span class="title-ref">MultiIndex</span> entries and names is now available with the <span class="title-ref">DataFrame.from\_dict</span> and <span class="title-ref">DataFrame.to\_dict</span> methods and can be used with the standard `json` library to produce a tight representation of <span class="title-ref">DataFrame</span> objects (`4889`).

<div class="ipython">

python

  - df = pd.DataFrame.from\_records(  
    \[\[1, 3\], \[2, 4\]\], index=pd.MultiIndex.from\_tuples(\[("a", "b"), ("a", "c")\], names=\["n1", "n2"\]), columns=pd.MultiIndex.from\_tuples(\[("x", 1), ("y", 2)\], names=\["z1", "z2"\]),

) df df.to\_dict(orient='tight')

</div>

### Other enhancements

  - <span class="title-ref">concat</span> will preserve the `attrs` when it is the same for all objects and discard the `attrs` when they are different (`41828`)
  - <span class="title-ref">DataFrameGroupBy</span> operations with `as_index=False` now correctly retain `ExtensionDtype` dtypes for columns being grouped on (`41373`)
  - Add support for assigning values to `by` argument in <span class="title-ref">DataFrame.plot.hist</span> and <span class="title-ref">DataFrame.plot.box</span> (`15079`)
  - <span class="title-ref">Series.sample</span>, <span class="title-ref">DataFrame.sample</span>, <span class="title-ref">.DataFrameGroupBy.sample</span>, and <span class="title-ref">.SeriesGroupBy.sample</span> now accept a `np.random.Generator` as input to `random_state`. A generator will be more performant, especially with `replace=False` (`38100`)
  - <span class="title-ref">Series.ewm</span> and <span class="title-ref">DataFrame.ewm</span> now support a `method` argument with a `'table'` option that performs the windowing operation over an entire <span class="title-ref">DataFrame</span>. See \[Window Overview \<window.overview\>\](\#window-overview-\<window.overview\>) for performance and functional benefits (`42273`)
  - <span class="title-ref">.DataFrameGroupBy.cummin</span>, <span class="title-ref">.SeriesGroupBy.cummin</span>, <span class="title-ref">.DataFrameGroupBy.cummax</span>, and <span class="title-ref">.SeriesGroupBy.cummax</span> now support the argument `skipna` (`34047`)
  - <span class="title-ref">read\_table</span> now supports the argument `storage_options` (`39167`)
  - <span class="title-ref">DataFrame.to\_stata</span> and <span class="title-ref">StataWriter</span> now accept the keyword only argument `value_labels` to save labels for non-categorical columns (`38454`)
  - Methods that relied on hashmap based algos such as <span class="title-ref">DataFrameGroupBy.value\_counts</span>, <span class="title-ref">DataFrameGroupBy.count</span> and <span class="title-ref">factorize</span> ignored imaginary component for complex numbers (`17927`)
  - Add <span class="title-ref">Series.str.removeprefix</span> and <span class="title-ref">Series.str.removesuffix</span> introduced in Python 3.9 to remove pre-/suffixes from string-type <span class="title-ref">Series</span> (`36944`)
  - Attempting to write into a file in missing parent directory with <span class="title-ref">DataFrame.to\_csv</span>, <span class="title-ref">DataFrame.to\_html</span>, <span class="title-ref">DataFrame.to\_excel</span>, <span class="title-ref">DataFrame.to\_feather</span>, <span class="title-ref">DataFrame.to\_parquet</span>, <span class="title-ref">DataFrame.to\_stata</span>, <span class="title-ref">DataFrame.to\_json</span>, <span class="title-ref">DataFrame.to\_pickle</span>, and <span class="title-ref">DataFrame.to\_xml</span> now explicitly mentions missing parent directory, the same is true for <span class="title-ref">Series</span> counterparts (`24306`)
  - Indexing with `.loc` and `.iloc` now supports `Ellipsis` (`37750`)
  - <span class="title-ref">IntegerArray.all</span> , <span class="title-ref">IntegerArray.any</span>, <span class="title-ref">FloatingArray.any</span>, and <span class="title-ref">FloatingArray.all</span> use Kleene logic (`41967`)
  - Added support for nullable boolean and integer types in <span class="title-ref">DataFrame.to\_stata</span>, <span class="title-ref">\~pandas.io.stata.StataWriter</span>, <span class="title-ref">\~pandas.io.stata.StataWriter117</span>, and <span class="title-ref">\~pandas.io.stata.StataWriterUTF8</span> (`40855`)
  - <span class="title-ref">DataFrame.\_\_pos\_\_</span> and <span class="title-ref">DataFrame.\_\_neg\_\_</span> now retain `ExtensionDtype` dtypes (`43883`)
  - The error raised when an optional dependency can't be imported now includes the original exception, for easier investigation (`43882`)
  - Added <span class="title-ref">.ExponentialMovingWindow.sum</span> (`13297`)
  - <span class="title-ref">Series.str.split</span> now supports a `regex` argument that explicitly specifies whether the pattern is a regular expression. Default is `None` (`43563`, `32835`, `25549`)
  - <span class="title-ref">DataFrame.dropna</span> now accepts a single label as `subset` along with array-like (`41021`)
  - Added <span class="title-ref">DataFrameGroupBy.value\_counts</span> (`43564`)
  - <span class="title-ref">read\_csv</span> now accepts a `callable` function in `on_bad_lines` when `engine="python"` for custom handling of bad lines (`5686`)
  - <span class="title-ref">ExcelWriter</span> argument `if_sheet_exists="overlay"` option added (`40231`)
  - <span class="title-ref">read\_excel</span> now accepts a `decimal` argument that allow the user to specify the decimal point when parsing string columns to numeric (`14403`)
  - <span class="title-ref">.DataFrameGroupBy.mean</span>, <span class="title-ref">.SeriesGroupBy.mean</span>, <span class="title-ref">.DataFrameGroupBy.std</span>, <span class="title-ref">.SeriesGroupBy.std</span>, <span class="title-ref">.DataFrameGroupBy.var</span>, <span class="title-ref">.SeriesGroupBy.var</span>, <span class="title-ref">.DataFrameGroupBy.sum</span>, and <span class="title-ref">.SeriesGroupBy.sum</span> now support [Numba](http://numba.pydata.org/) execution with the `engine` keyword (`43731`, `44862`, `44939`)
  - <span class="title-ref">Timestamp.isoformat</span> now handles the `timespec` argument from the base `datetime` class (`26131`)
  - <span class="title-ref">NaT.to\_numpy</span> `dtype` argument is now respected, so `np.timedelta64` can be returned (`44460`)
  - New option `display.max_dir_items` customizes the number of columns added to <span class="title-ref">Dataframe.\_\_dir\_\_</span> and suggested for tab completion (`37996`)
  - Added "Juneteenth National Independence Day" to `USFederalHolidayCalendar` (`44574`)
  - <span class="title-ref">.Rolling.var</span>, <span class="title-ref">.Expanding.var</span>, <span class="title-ref">.Rolling.std</span>, and <span class="title-ref">.Expanding.std</span> now support [Numba](http://numba.pydata.org/) execution with the `engine` keyword (`44461`)
  - <span class="title-ref">Series.info</span> has been added, for compatibility with <span class="title-ref">DataFrame.info</span> (`5167`)
  - Implemented <span class="title-ref">IntervalArray.min</span> and <span class="title-ref">IntervalArray.max</span>, as a result of which `min` and `max` now work for <span class="title-ref">IntervalIndex</span>, <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> with `IntervalDtype` (`44746`)
  - <span class="title-ref">UInt64Index.map</span> now retains `dtype` where possible (`44609`)
  - <span class="title-ref">read\_json</span> can now parse unsigned long long integers (`26068`)
  - <span class="title-ref">DataFrame.take</span> now raises a `TypeError` when passed a scalar for the indexer (`42875`)
  - <span class="title-ref">is\_list\_like</span> now identifies duck-arrays as list-like unless `.ndim == 0` (`35131`)
  - <span class="title-ref">ExtensionDtype</span> and <span class="title-ref">ExtensionArray</span> are now (de)serialized when exporting a <span class="title-ref">DataFrame</span> with <span class="title-ref">DataFrame.to\_json</span> using `orient='table'` (`20612`, `44705`)
  - Add support for [Zstandard](http://facebook.github.io/zstd/) compression to <span class="title-ref">DataFrame.to\_pickle</span>/<span class="title-ref">read\_pickle</span> and friends (`43925`)
  - <span class="title-ref">DataFrame.to\_sql</span> now returns an `int` of the number of written rows (`23998`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### Inconsistent date string parsing

The `dayfirst` option of <span class="title-ref">to\_datetime</span> isn't strict, and this can lead to surprising behavior:

<div class="ipython" data-okwarning="">

python

pd.to\_datetime(\["31-12-2021"\], dayfirst=False)

</div>

Now, a warning will be raised if a date string cannot be parsed accordance to the given `dayfirst` value when the value is a delimited date string (e.g. `31-12-2012`).

### Ignoring dtypes in concat with empty or all-NA columns

<div class="note">

<div class="title">

Note

</div>

This behaviour change has been reverted in pandas 1.4.3.

</div>

When using <span class="title-ref">concat</span> to concatenate two or more <span class="title-ref">DataFrame</span> objects, if one of the DataFrames was empty or had all-NA values, its dtype was *sometimes* ignored when finding the concatenated dtype. These are now consistently *not* ignored (`43507`).

`` `ipython     In [3]: df1 = pd.DataFrame({"bar": [pd.Timestamp("2013-01-01")]}, index=range(1))     In [4]: df2 = pd.DataFrame({"bar": np.nan}, index=range(1, 2))     In [5]: res = pd.concat([df1, df2])  Previously, the float-dtype in ``df2`would be ignored so the result dtype`<span class="title-ref"> would be </span><span class="title-ref">datetime64\[ns\]</span><span class="title-ref">. As a result, the </span><span class="title-ref">np.nan</span><span class="title-ref"> would be cast to </span><span class="title-ref">NaT</span>\`.

*Previous behavior*:

`` `ipython     In [6]: res     Out[6]:              bar     0 2013-01-01     1        NaT  Now the float-dtype is respected. Since the common dtype for these DataFrames is ``<span class="title-ref"> object, the </span><span class="title-ref">np.nan</span>\` is retained.

*New behavior*:

`` `ipython     In [6]: res     Out[6]:                        bar     0  2013-01-01 00:00:00     1                  NaN    .. _whatsnew_140.notable_bug_fixes.value_counts_and_mode_do_not_coerce_to_nan:  Null-values are no longer coerced to NaN-value in value_counts and mode ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">Series.value\_counts</span> and <span class="title-ref">Series.mode</span> no longer coerce `None`, `NaT` and other null-values to a NaN-value for `np.object_`-dtype. This behavior is now consistent with `unique`, `isin` and others (`42688`).

<div class="ipython">

python

s = pd.Series(\[True, None, pd.NaT, None, pd.NaT, None\]) res = s.value\_counts(dropna=False)

</div>

Previously, all null-values were replaced by a NaN-value.

*Previous behavior*:

`` `ipython     In [3]: res     Out[3]:     NaN     5     True    1     dtype: int64  Now null-values are no longer mangled.  *New behavior*:  .. ipython:: python      res  .. _whatsnew_140.notable_bug_fixes.read_csv_mangle_dup_cols:  mangle_dupe_cols in read_csv no longer renames unique columns conflicting with target names ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">read\_csv</span> no longer renames unique column labels which conflict with the target names of duplicated columns. Already existing columns are skipped, i.e. the next available index is used for the target column name (`14704`).

<div class="ipython">

python

import io

data = "a,a,a.1n1,2,3" res = pd.read\_csv(io.StringIO(data))

</div>

Previously, the second column was called `a.1`, while the third column was also renamed to `a.1.1`.

*Previous behavior*:

`` `ipython     In [3]: res     Out[3]:         a  a.1  a.1.1     0   1    2      3  Now the renaming checks if ``a.1`already exists when changing the name of the`<span class="title-ref"> second column and jumps this index. The second column is instead renamed to </span><span class="title-ref">a.2</span>\`.

*New behavior*:

<div class="ipython">

python

res

</div>

### unstack and pivot\_table no longer raises ValueError for result that would exceed int32 limit

Previously <span class="title-ref">DataFrame.pivot\_table</span> and <span class="title-ref">DataFrame.unstack</span> would raise a `ValueError` if the operation could produce a result with more than `2**31 - 1` elements. This operation now raises a <span class="title-ref">errors.PerformanceWarning</span> instead (`26314`).

*Previous behavior*:

`` `ipython     In [3]: df = DataFrame({"ind1": np.arange(2 ** 16), "ind2": np.arange(2 ** 16), "count": 0})     In [4]: df.pivot_table(index="ind1", columns="ind2", values="count", aggfunc="count")     ValueError: Unstacked DataFrame is too big, causing int32 overflow  *New behavior*:  .. code-block:: python      In [4]: df.pivot_table(index="ind1", columns="ind2", values="count", aggfunc="count")     PerformanceWarning: The following operation may generate 4294967296 cells in the resulting pandas object.  .. ---------------------------------------------------------------------------  .. _whatsnew_140.notable_bug_fixes.groupby_apply_mutation:  groupby.apply consistent transform detection ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> are designed to be flexible, allowing users to perform aggregations, transformations, filters, and use it with user-defined functions that might not fall into any of these categories. As part of this, apply will attempt to detect when an operation is a transform, and in such a case, the result will have the same index as the input. In order to determine if the operation is a transform, pandas compares the input's index to the result's and determines if it has been mutated. Previously in pandas 1.3, different code paths used different definitions of "mutated": some would use Python's `is` whereas others would test only up to equality.

This inconsistency has been removed, pandas now tests up to equality.

<div class="ipython">

python

  - def func(x):  
    return x.copy()

df = pd.DataFrame({'a': \[1, 2\], 'b': \[3, 4\], 'c': \[5, 6\]}) df

</div>

*Previous behavior*:

`` `ipython     In [3]: df.groupby(['a']).apply(func)     Out[3]:          a  b  c     a     1 0  1  3  5     2 1  2  4  6      In [4]: df.set_index(['a', 'b']).groupby(['a']).apply(func)     Out[4]:          c     a b     1 3  5     2 4  6  In the examples above, the first uses a code path where pandas uses ``is`and`<span class="title-ref"> determines that </span><span class="title-ref">func</span><span class="title-ref"> is not a transform whereas the second tests up to equality and determines that </span><span class="title-ref">func</span>\` is a transform. In the first case, the result's index is not the same as the input's.

*New behavior*:

`` `ipython     In [5]: df.groupby(['a']).apply(func)     Out[5]:        a  b  c     0  1  3  5     1  2  4  6      In [6]: df.set_index(['a', 'b']).groupby(['a']).apply(func)     Out[6]:          c     a b     1 3  5     2 4  6  Now in both cases it is determined that ``func`is a transform. In each case,`\` the result has the same index as the input.

## Backwards incompatible API changes

### Increased minimum version for Python

pandas 1.4.0 supports Python 3.8 and higher.

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated. If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.18.5</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytz</td>
<td>2020.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.8.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.3.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.7.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>6.0</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>mypy (dev)</td>
<td>0.930</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.8.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>fastparquet</td>
<td>0.4.0</td>
<td></td>
</tr>
<tr class="odd">
<td>fsspec</td>
<td>0.7.4</td>
<td></td>
</tr>
<tr class="even">
<td>gcsfs</td>
<td>0.6.0</td>
<td></td>
</tr>
<tr class="odd">
<td>lxml</td>
<td>4.5.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>3.3.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>numba</td>
<td>0.50.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>openpyxl</td>
<td>3.0.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pandas-gbq</td>
<td>0.14.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pyarrow</td>
<td>1.0.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pymysql</td>
<td>0.10.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytables</td>
<td>3.6.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>s3fs</td>
<td>0.4.0</td>
<td></td>
</tr>
<tr class="even">
<td>scipy</td>
<td>1.4.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>sqlalchemy</td>
<td>1.4.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>tabulate</td>
<td>0.8.7</td>
<td></td>
</tr>
<tr class="odd">
<td>xarray</td>
<td>0.15.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xlrd</td>
<td>2.0.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xlsxwriter</td>
<td>1.2.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xlwt</td>
<td>1.3.0</td>
<td></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - <span class="title-ref">Index.get\_indexer\_for</span> no longer accepts keyword arguments (other than `target`); in the past these would be silently ignored if the index was not unique (`42310`)

  - Change in the position of the `min_rows` argument in <span class="title-ref">DataFrame.to\_string</span> due to change in the docstring (`44304`)

  - Reduction operations for <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> now raising a `ValueError` when `None` is passed for `skipna` (`44178`)

  - <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_html</span> no longer raising an error when one of the header rows consists only of `Unnamed:` columns (`13054`)

  - Changed the `name` attribute of several holidays in `USFederalHolidayCalendar` to match [official federal holiday names](https://www.opm.gov/policy-data-oversight/pay-leave/federal-holidays/) specifically:
    
    >   - "New Year's Day" gains the possessive apostrophe
    >   - "Presidents Day" becomes "Washington's Birthday"
    >   - "Martin Luther King Jr. Day" is now "Birthday of Martin Luther King, Jr."
    >   - "July 4th" is now "Independence Day"
    >   - "Thanksgiving" is now "Thanksgiving Day"
    >   - "Christmas" is now "Christmas Day"
    >   - Added "Juneteenth National Independence Day"

## Deprecations

### Deprecated Int64Index, UInt64Index & Float64Index

<span class="title-ref">Int64Index</span>, <span class="title-ref">UInt64Index</span> and <span class="title-ref">Float64Index</span> have been deprecated in favor of the base <span class="title-ref">Index</span> class and will be removed in pandas 2.0 (`43028`).

For constructing a numeric index, you can use the base <span class="title-ref">Index</span> class instead specifying the data type (which will also work on older pandas releases):

`` `python     # replace     pd.Int64Index([1, 2, 3])     # with     pd.Index([1, 2, 3], dtype="int64")  For checking the data type of an index object, you can replace ``isinstance`  `<span class="title-ref"> checks with checking the </span><span class="title-ref">dtype</span>\`:

`` `python     # replace     isinstance(idx, pd.Int64Index)     # with     idx.dtype == "int64"  Currently, in order to maintain backward compatibility, calls to `Index` ``<span class="title-ref"> will continue to return \`Int64Index</span>, <span class="title-ref">UInt64Index</span> and <span class="title-ref">Float64Index</span> when given numeric data, but in the future, an <span class="title-ref">Index</span> will be returned.

*Current behavior*:

`` `ipython     In [1]: pd.Index([1, 2, 3], dtype="int32")     Out [1]: Int64Index([1, 2, 3], dtype='int64')     In [1]: pd.Index([1, 2, 3], dtype="uint64")     Out [1]: UInt64Index([1, 2, 3], dtype='uint64')  *Future behavior*:  .. code-block:: ipython      In [3]: pd.Index([1, 2, 3], dtype="int32")     Out [3]: Index([1, 2, 3], dtype='int32')     In [4]: pd.Index([1, 2, 3], dtype="uint64")     Out [4]: Index([1, 2, 3], dtype='uint64')   .. _whatsnew_140.deprecations.frame_series_append:  Deprecated DataFrame.append and Series.append ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">DataFrame.append</span> and <span class="title-ref">Series.append</span> have been deprecated and will be removed in a future version. Use <span class="title-ref">pandas.concat</span> instead (`35407`).

*Deprecated syntax*

`` `ipython     In [1]: pd.Series([1, 2]).append(pd.Series([3, 4])     Out [1]:     <stdin>:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.     0    1     1    2     0    3     1    4     dtype: int64      In [2]: df1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))     In [3]: df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))     In [4]: df1.append(df2)     Out [4]:     <stdin>:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.        A  B     0  1  2     1  3  4     0  5  6     1  7  8  *Recommended syntax*  .. ipython:: python      pd.concat([pd.Series([1, 2]), pd.Series([3, 4])])      df1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))     df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))     pd.concat([df1, df2])   .. _whatsnew_140.deprecations.other:  Other Deprecations ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^ - Deprecated \`Index.is\_type\_compatible</span> (`42113`) - Deprecated `method` argument in <span class="title-ref">Index.get\_loc</span>, use `index.get_indexer([label], method=...)` instead (`42269`) - Deprecated treating integer keys in <span class="title-ref">Series.\_\_setitem\_\_</span> as positional when the index is a <span class="title-ref">Float64Index</span> not containing the key, a <span class="title-ref">IntervalIndex</span> with no entries containing the key, or a <span class="title-ref">MultiIndex</span> with leading <span class="title-ref">Float64Index</span> level not containing the key (`33469`) - Deprecated treating `numpy.datetime64` objects as UTC times when passed to the <span class="title-ref">Timestamp</span> constructor along with a timezone. In a future version, these will be treated as wall-times. To retain the old behavior, use `Timestamp(dt64).tz_localize("UTC").tz_convert(tz)` (`24559`) - Deprecated ignoring missing labels when indexing with a sequence of labels on a level of a <span class="title-ref">MultiIndex</span> (`42351`) - Creating an empty <span class="title-ref">Series</span> without a `dtype` will now raise a more visible `FutureWarning` instead of a `DeprecationWarning` (`30017`) - Deprecated the `kind` argument in <span class="title-ref">Index.get\_slice\_bound</span>, <span class="title-ref">Index.slice\_indexer</span>, and <span class="title-ref">Index.slice\_locs</span>; in a future version passing `kind` will raise (`42857`) - Deprecated dropping of nuisance columns in <span class="title-ref">Rolling</span>, <span class="title-ref">Expanding</span>, and <span class="title-ref">EWM</span> aggregations (`42738`) - Deprecated <span class="title-ref">Index.reindex</span> with a non-unique <span class="title-ref">Index</span> (`42568`) - Deprecated <span class="title-ref">.Styler.render</span> in favor of <span class="title-ref">.Styler.to\_html</span> (`42140`) - Deprecated <span class="title-ref">.Styler.hide\_index</span> and <span class="title-ref">.Styler.hide\_columns</span> in favor of <span class="title-ref">.Styler.hide</span> (`43758`) - Deprecated passing in a string column label into `times` in <span class="title-ref">DataFrame.ewm</span> (`43265`) - Deprecated the `include_start` and `include_end` arguments in <span class="title-ref">DataFrame.between\_time</span>; in a future version passing `include_start` or `include_end` will raise (`40245`) - Deprecated the `squeeze` argument to <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_table</span>, and <span class="title-ref">read\_excel</span>. Users should squeeze the <span class="title-ref">DataFrame</span> afterwards with `.squeeze("columns")` instead (`43242`) - Deprecated the `index` argument to <span class="title-ref">SparseArray</span> construction (`23089`) - Deprecated the `closed` argument in <span class="title-ref">date\_range</span> and <span class="title-ref">bdate\_range</span> in favor of `inclusive` argument; In a future version passing `closed` will raise (`40245`) - Deprecated <span class="title-ref">.Rolling.validate</span>, <span class="title-ref">.Expanding.validate</span>, and <span class="title-ref">.ExponentialMovingWindow.validate</span> (`43665`) - Deprecated silent dropping of columns that raised a `TypeError` in <span class="title-ref">Series.transform</span> and <span class="title-ref">DataFrame.transform</span> when used with a dictionary (`43740`) - Deprecated silent dropping of columns that raised a `TypeError`, `DataError`, and some cases of `ValueError` in <span class="title-ref">Series.aggregate</span>, <span class="title-ref">DataFrame.aggregate</span>, <span class="title-ref">Series.groupby.aggregate</span>, and <span class="title-ref">DataFrame.groupby.aggregate</span> when used with a list (`43740`) - Deprecated casting behavior when setting timezone-aware value(s) into a timezone-aware <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> column when the timezones do not match. Previously this cast to object dtype. In a future version, the values being inserted will be converted to the series or column's existing timezone (`37605`) - Deprecated casting behavior when passing an item with mismatched-timezone to <span class="title-ref">DatetimeIndex.insert</span>, <span class="title-ref">DatetimeIndex.putmask</span>, <span class="title-ref">DatetimeIndex.where</span> <span class="title-ref">DatetimeIndex.fillna</span>, <span class="title-ref">Series.mask</span>, <span class="title-ref">Series.where</span>, <span class="title-ref">Series.fillna</span>, <span class="title-ref">Series.shift</span>, <span class="title-ref">Series.replace</span>, <span class="title-ref">Series.reindex</span> (and <span class="title-ref">DataFrame</span> column analogues). In the past this has cast to object `dtype`. In a future version, these will cast the passed item to the index or series's timezone (`37605`, `44940`) - Deprecated the `prefix` keyword argument in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span>, in a future version the argument will be removed (`43396`) - Deprecated passing non boolean argument to `sort` in <span class="title-ref">concat</span> (`41518`) - Deprecated passing arguments as positional for <span class="title-ref">read\_fwf</span> other than `filepath_or_buffer` (`41485`) - Deprecated passing arguments as positional for <span class="title-ref">read\_xml</span> other than `path_or_buffer` (`45133`) - Deprecated passing `skipna=None` for <span class="title-ref">DataFrame.mad</span> and <span class="title-ref">Series.mad</span>, pass `skipna=True` instead (`44580`) - Deprecated the behavior of <span class="title-ref">to\_datetime</span> with the string "now" with `utc=False`; in a future version this will match `Timestamp("now")`, which in turn matches <span class="title-ref">Timestamp.now</span> returning the local time (`18705`) - Deprecated <span class="title-ref">DateOffset.apply</span>, use `offset + other` instead (`44522`) - Deprecated parameter `names` in <span class="title-ref">Index.copy</span> (`44916`) - A deprecation warning is now shown for <span class="title-ref">DataFrame.to\_latex</span> indicating the arguments signature may change and emulate more the arguments to <span class="title-ref">.Styler.to\_latex</span> in future versions (`44411`) - Deprecated behavior of <span class="title-ref">concat</span> between objects with bool-dtype and numeric-dtypes; in a future version these will cast to object dtype instead of coercing bools to numeric values (`39817`) - Deprecated <span class="title-ref">Categorical.replace</span>, use <span class="title-ref">Series.replace</span> instead (`44929`) - Deprecated passing `set` or `dict` as indexer for <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span>, <span class="title-ref">DataFrame.loc.\_\_getitem\_\_</span>, <span class="title-ref">Series.loc.\_\_setitem\_\_</span>, <span class="title-ref">Series.loc.\_\_getitem\_\_</span>, <span class="title-ref">DataFrame.\_\_getitem\_\_</span>, <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_setitem\_\_</span> (`42825`) - Deprecated <span class="title-ref">Index.\_\_getitem\_\_</span> with a bool key; use `index.values[key]` to get the old behavior (`44051`) - Deprecated downcasting column-by-column in <span class="title-ref">DataFrame.where</span> with integer-dtypes (`44597`) - Deprecated <span class="title-ref">DatetimeIndex.union\_many</span>, use <span class="title-ref">DatetimeIndex.union</span> instead (`44091`) - Deprecated <span class="title-ref">.Groupby.pad</span> in favor of <span class="title-ref">.Groupby.ffill</span> (`33396`) - Deprecated <span class="title-ref">.Groupby.backfill</span> in favor of <span class="title-ref">.Groupby.bfill</span> (`33396`) - Deprecated <span class="title-ref">.Resample.pad</span> in favor of <span class="title-ref">.Resample.ffill</span> (`33396`) - Deprecated <span class="title-ref">.Resample.backfill</span> in favor of <span class="title-ref">.Resample.bfill</span> (`33396`) - Deprecated `numeric_only=None` in <span class="title-ref">DataFrame.rank</span>; in a future version `numeric_only` must be either `True` or `False` (the default) (`45036`) - Deprecated the behavior of <span class="title-ref">Timestamp.utcfromtimestamp</span>, in the future it will return a timezone-aware UTC <span class="title-ref">Timestamp</span> (`22451`) - Deprecated <span class="title-ref">NaT.freq</span> (`45071`) - Deprecated behavior of <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> construction when passed float-dtype data containing `NaN` and an integer dtype ignoring the dtype argument; in a future version this will raise (`40110`) - Deprecated the behaviour of <span class="title-ref">Series.to\_frame</span> and <span class="title-ref">Index.to\_frame</span> to ignore the `name` argument when `name=None`. Currently, this means to preserve the existing name, but in the future explicitly passing `name=None` will set `None` as the name of the column in the resulting DataFrame (`44212`)

## Performance improvements

  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.sample</span> and <span class="title-ref">.SeriesGroupBy.sample</span>, especially when `weights` argument provided (`34483`)
  - Performance improvement when converting non-string arrays to string arrays (`34483`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> for user-defined functions (`41598`)
  - Performance improvement in constructing <span class="title-ref">DataFrame</span> objects (`42631`, `43142`, `43147`, `43307`, `43144`, `44826`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.shift</span> and <span class="title-ref">.SeriesGroupBy.shift</span> when `fill_value` argument is provided (`26615`)
  - Performance improvement in <span class="title-ref">DataFrame.corr</span> for `method=pearson` on data without missing values (`40956`)
  - Performance improvement in some <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> operations (`42992`, `43578`)
  - Performance improvement in <span class="title-ref">read\_stata</span> (`43059`, `43227`)
  - Performance improvement in <span class="title-ref">read\_sas</span> (`43333`)
  - Performance improvement in <span class="title-ref">to\_datetime</span> with `uint` dtypes (`42606`)
  - Performance improvement in <span class="title-ref">to\_datetime</span> with `infer_datetime_format` set to `True` (`43901`)
  - Performance improvement in <span class="title-ref">Series.sparse.to\_coo</span> (`42880`)
  - Performance improvement in indexing with a <span class="title-ref">UInt64Index</span> (`43862`)
  - Performance improvement in indexing with a <span class="title-ref">Float64Index</span> (`43705`)
  - Performance improvement in indexing with a non-unique <span class="title-ref">Index</span> (`43792`)
  - Performance improvement in indexing with a listlike indexer on a <span class="title-ref">MultiIndex</span> (`43370`)
  - Performance improvement in indexing with a <span class="title-ref">MultiIndex</span> indexer on another <span class="title-ref">MultiIndex</span> (`43370`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> (`43469`, `43725`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.count</span> and <span class="title-ref">.SeriesGroupBy.count</span> (`43730`, `43694`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.any</span>, <span class="title-ref">.SeriesGroupBy.any</span>, <span class="title-ref">.DataFrameGroupBy.all</span>, and <span class="title-ref">.SeriesGroupBy.all</span> (`43675`, `42841`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.std</span> and <span class="title-ref">.SeriesGroupBy.std</span> (`43115`, `43576`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.cumsum</span> and <span class="title-ref">.SeriesGroupBy.cumsum</span> (`43309`)
  - <span class="title-ref">SparseArray.min</span> and <span class="title-ref">SparseArray.max</span> no longer require converting to a dense array (`43526`)
  - Indexing into a <span class="title-ref">SparseArray</span> with a `slice` with `step=1` no longer requires converting to a dense array (`43777`)
  - Performance improvement in <span class="title-ref">SparseArray.take</span> with `allow_fill=False` (`43654`)
  - Performance improvement in <span class="title-ref">.Rolling.mean</span>, <span class="title-ref">.Expanding.mean</span>, <span class="title-ref">.Rolling.sum</span>, <span class="title-ref">.Expanding.sum</span>, <span class="title-ref">.Rolling.max</span>, <span class="title-ref">.Expanding.max</span>, <span class="title-ref">.Rolling.min</span> and <span class="title-ref">.Expanding.min</span> with `engine="numba"` (`43612`, `44176`, `45170`)
  - Improved performance of <span class="title-ref">pandas.read\_csv</span> with `memory_map=True` when file encoding is UTF-8 (`43787`)
  - Performance improvement in <span class="title-ref">RangeIndex.sort\_values</span> overriding <span class="title-ref">Index.sort\_values</span> (`43666`)
  - Performance improvement in <span class="title-ref">RangeIndex.insert</span> (`43988`)
  - Performance improvement in <span class="title-ref">Index.insert</span> (`43953`)
  - Performance improvement in <span class="title-ref">DatetimeIndex.tolist</span> (`43823`)
  - Performance improvement in <span class="title-ref">DatetimeIndex.union</span> (`42353`)
  - Performance improvement in <span class="title-ref">Series.nsmallest</span> (`43696`)
  - Performance improvement in <span class="title-ref">DataFrame.insert</span> (`42998`)
  - Performance improvement in <span class="title-ref">DataFrame.dropna</span> (`43683`)
  - Performance improvement in <span class="title-ref">DataFrame.fillna</span> (`43316`)
  - Performance improvement in <span class="title-ref">DataFrame.values</span> (`43160`)
  - Performance improvement in <span class="title-ref">DataFrame.select\_dtypes</span> (`42611`)
  - Performance improvement in <span class="title-ref">DataFrame</span> reductions (`43185`, `43243`, `43311`, `43609`)
  - Performance improvement in <span class="title-ref">Series.unstack</span> and <span class="title-ref">DataFrame.unstack</span> (`43335`, `43352`, `42704`, `43025`)
  - Performance improvement in <span class="title-ref">Series.to\_frame</span> (`43558`)
  - Performance improvement in <span class="title-ref">Series.mad</span> (`43010`)
  - Performance improvement in <span class="title-ref">merge</span> (`43332`)
  - Performance improvement in <span class="title-ref">to\_csv</span> when index column is a datetime and is formatted (`39413`)
  - Performance improvement in <span class="title-ref">to\_csv</span> when <span class="title-ref">MultiIndex</span> contains a lot of unused levels (`37484`)
  - Performance improvement in <span class="title-ref">read\_csv</span> when `index_col` was set with a numeric column (`44158`)
  - Performance improvement in <span class="title-ref">concat</span> (`43354`)
  - Performance improvement in <span class="title-ref">SparseArray.\_\_getitem\_\_</span> (`23122`)
  - Performance improvement in constructing a <span class="title-ref">DataFrame</span> from array-like objects like a `Pytorch` tensor (`44616`)

## Bug fixes

### Categorical

  - Bug in setting dtype-incompatible values into a <span class="title-ref">Categorical</span> (or `Series` or `DataFrame` backed by `Categorical`) raising `ValueError` instead of `TypeError` (`41919`)
  - Bug in <span class="title-ref">Categorical.searchsorted</span> when passing a dtype-incompatible value raising `KeyError` instead of `TypeError` (`41919`)
  - Bug in <span class="title-ref">Categorical.astype</span> casting datetimes and <span class="title-ref">Timestamp</span> to int for dtype `object` (`44930`)
  - Bug in <span class="title-ref">Series.where</span> with `CategoricalDtype` when passing a dtype-incompatible value raising `ValueError` instead of `TypeError` (`41919`)
  - Bug in <span class="title-ref">Categorical.fillna</span> when passing a dtype-incompatible value raising `ValueError` instead of `TypeError` (`41919`)
  - Bug in <span class="title-ref">Categorical.fillna</span> with a tuple-like category raising `ValueError` instead of `TypeError` when filling with a non-category tuple (`41919`)

### Datetimelike

  - Bug in <span class="title-ref">DataFrame</span> constructor unnecessarily copying non-datetimelike 2D object arrays (`39272`)
  - Bug in <span class="title-ref">to\_datetime</span> with `format` and `pandas.NA` was raising `ValueError` (`42957`)
  - <span class="title-ref">to\_datetime</span> would silently swap `MM/DD/YYYY` and `DD/MM/YYYY` formats if the given `dayfirst` option could not be respected - now, a warning is raised in the case of delimited date strings (e.g. `31-12-2012`) (`12585`)
  - Bug in <span class="title-ref">date\_range</span> and <span class="title-ref">bdate\_range</span> do not return right bound when `start` = `end` and set is closed on one side (`43394`)
  - Bug in inplace addition and subtraction of <span class="title-ref">DatetimeIndex</span> or <span class="title-ref">TimedeltaIndex</span> with <span class="title-ref">DatetimeArray</span> or <span class="title-ref">TimedeltaArray</span> (`43904`)
  - Bug in calling `np.isnan`, `np.isfinite`, or `np.isinf` on a timezone-aware <span class="title-ref">DatetimeIndex</span> incorrectly raising `TypeError` (`43917`)
  - Bug in constructing a <span class="title-ref">Series</span> from datetime-like strings with mixed timezones incorrectly partially-inferring datetime values (`40111`)
  - Bug in addition of a <span class="title-ref">Tick</span> object and a `np.timedelta64` object incorrectly raising instead of returning <span class="title-ref">Timedelta</span> (`44474`)
  - `np.maximum.reduce` and `np.minimum.reduce` now correctly return <span class="title-ref">Timestamp</span> and <span class="title-ref">Timedelta</span> objects when operating on <span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span>, or <span class="title-ref">Index</span> with `datetime64[ns]` or `timedelta64[ns]` dtype (`43923`)
  - Bug in adding a `np.timedelta64` object to a <span class="title-ref">BusinessDay</span> or <span class="title-ref">CustomBusinessDay</span> object incorrectly raising (`44532`)
  - Bug in <span class="title-ref">Index.insert</span> for inserting `np.datetime64`, `np.timedelta64` or `tuple` into <span class="title-ref">Index</span> with `dtype='object'` with negative loc adding `None` and replacing existing value (`44509`)
  - Bug in <span class="title-ref">Timestamp.to\_pydatetime</span> failing to retain the `fold` attribute (`45087`)
  - Bug in <span class="title-ref">Series.mode</span> with `DatetimeTZDtype` incorrectly returning timezone-naive and `PeriodDtype` incorrectly raising (`41927`)
  - Fixed regression in <span class="title-ref">\~Series.reindex</span> raising an error when using an incompatible fill value with a datetime-like dtype (or not raising a deprecation warning for using a `datetime.date` as fill value) (`42921`)
  - Bug in <span class="title-ref">DateOffset</span> addition with <span class="title-ref">Timestamp</span> where `offset.nanoseconds` would not be included in the result (`43968`, `36589`)
  - Bug in <span class="title-ref">Timestamp.fromtimestamp</span> not supporting the `tz` argument (`45083`)
  - Bug in <span class="title-ref">DataFrame</span> construction from dict of <span class="title-ref">Series</span> with mismatched index dtypes sometimes raising depending on the ordering of the passed dict (`44091`)
  - Bug in <span class="title-ref">Timestamp</span> hashing during some DST transitions caused a segmentation fault (`33931` and `40817`)

### Timedelta

  - Bug in division of all-`NaT` <span class="title-ref">TimeDeltaIndex</span>, <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> column with object-dtype array like of numbers failing to infer the result as timedelta64-dtype (`39750`)
  - Bug in floor division of `timedelta64[ns]` data with a scalar returning garbage values (`44466`)
  - Bug in <span class="title-ref">Timedelta</span> now properly taking into account any nanoseconds contribution of any kwarg (`43764`, `45227`)

### Time Zones

  - Bug in <span class="title-ref">to\_datetime</span> with `infer_datetime_format=True` failing to parse zero UTC offset (`Z`) correctly (`41047`)
  - Bug in <span class="title-ref">Series.dt.tz\_convert</span> resetting index in a <span class="title-ref">Series</span> with <span class="title-ref">CategoricalIndex</span> (`43080`)
  - Bug in `Timestamp` and `DatetimeIndex` incorrectly raising a `TypeError` when subtracting two timezone-aware objects with mismatched timezones (`31793`)

### Numeric

  - Bug in floor-dividing a list or tuple of integers by a <span class="title-ref">Series</span> incorrectly raising (`44674`)
  - Bug in <span class="title-ref">DataFrame.rank</span> raising `ValueError` with `object` columns and `method="first"` (`41931`)
  - Bug in <span class="title-ref">DataFrame.rank</span> treating missing values and extreme values as equal (for example `np.nan` and `np.inf`), causing incorrect results when `na_option="bottom"` or `na_option="top` used (`41931`)
  - Bug in `numexpr` engine still being used when the option `compute.use_numexpr` is set to `False` (`32556`)
  - Bug in <span class="title-ref">DataFrame</span> arithmetic ops with a subclass whose <span class="title-ref">\_constructor</span> attribute is a callable other than the subclass itself (`43201`)
  - Bug in arithmetic operations involving <span class="title-ref">RangeIndex</span> where the result would have the incorrect `name` (`43962`)
  - Bug in arithmetic operations involving <span class="title-ref">Series</span> where the result could have the incorrect `name` when the operands having matching NA or matching tuple names (`44459`)
  - Bug in division with `IntegerDtype` or `BooleanDtype` array and NA scalar incorrectly raising (`44685`)
  - Bug in multiplying a <span class="title-ref">Series</span> with `FloatingDtype` with a timedelta-like scalar incorrectly raising (`44772`)

### Conversion

  - Bug in <span class="title-ref">UInt64Index</span> constructor when passing a list containing both positive integers small enough to cast to int64 and integers too large to hold in int64 (`42201`)
  - Bug in <span class="title-ref">Series</span> constructor returning 0 for missing values with dtype `int64` and `False` for dtype `bool` (`43017`, `43018`)
  - Bug in constructing a <span class="title-ref">DataFrame</span> from a <span class="title-ref">PandasArray</span> containing <span class="title-ref">Series</span> objects behaving differently than an equivalent `np.ndarray` (`43986`)
  - Bug in <span class="title-ref">IntegerDtype</span> not allowing coercion from string dtype (`25472`)
  - Bug in <span class="title-ref">to\_datetime</span> with `arg:xr.DataArray` and `unit="ns"` specified raises `TypeError` (`44053`)
  - Bug in <span class="title-ref">DataFrame.convert\_dtypes</span> not returning the correct type when a subclass does not overload <span class="title-ref">\_constructor\_sliced</span> (`43201`)
  - Bug in <span class="title-ref">DataFrame.astype</span> not propagating `attrs` from the original <span class="title-ref">DataFrame</span> (`44414`)
  - Bug in <span class="title-ref">DataFrame.convert\_dtypes</span> result losing `columns.names` (`41435`)
  - Bug in constructing a `IntegerArray` from pyarrow data failing to validate dtypes (`44891`)
  - Bug in <span class="title-ref">Series.astype</span> not allowing converting from a `PeriodDtype` to `datetime64` dtype, inconsistent with the <span class="title-ref">PeriodIndex</span> behavior (`45038`)

### Strings

  - Bug in checking for `string[pyarrow]` dtype incorrectly raising an `ImportError` when pyarrow is not installed (`44276`)

### Interval

  - Bug in <span class="title-ref">Series.where</span> with `IntervalDtype` incorrectly raising when the `where` call should not replace anything (`44181`)

### Indexing

  - Bug in <span class="title-ref">Series.rename</span> with <span class="title-ref">MultiIndex</span> and `level` is provided (`43659`)
  - Bug in <span class="title-ref">DataFrame.truncate</span> and <span class="title-ref">Series.truncate</span> when the object's <span class="title-ref">Index</span> has a length greater than one but only one unique value (`42365`)
  - Bug in <span class="title-ref">Series.loc</span> and <span class="title-ref">DataFrame.loc</span> with a <span class="title-ref">MultiIndex</span> when indexing with a tuple in which one of the levels is also a tuple (`27591`)
  - Bug in <span class="title-ref">Series.loc</span> with a <span class="title-ref">MultiIndex</span> whose first level contains only `np.nan` values (`42055`)
  - Bug in indexing on a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with a <span class="title-ref">DatetimeIndex</span> when passing a string, the return type depended on whether the index was monotonic (`24892`)
  - Bug in indexing on a <span class="title-ref">MultiIndex</span> failing to drop scalar levels when the indexer is a tuple containing a datetime-like string (`42476`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> and <span class="title-ref">Series.sort\_values</span> when passing an ascending value, failed to raise or incorrectly raising `ValueError` (`41634`)
  - Bug in updating values of <span class="title-ref">pandas.Series</span> using boolean index, created by using <span class="title-ref">pandas.DataFrame.pop</span> (`42530`)
  - Bug in <span class="title-ref">Index.get\_indexer\_non\_unique</span> when index contains multiple `np.nan` (`35392`)
  - Bug in <span class="title-ref">DataFrame.query</span> did not handle the degree sign in a backticked column name, such as \`Temp(Â°C)\`, used in an expression to query a <span class="title-ref">DataFrame</span> (`42826`)
  - Bug in <span class="title-ref">DataFrame.drop</span> where the error message did not show missing labels with commas when raising `KeyError` (`42881`)
  - Bug in <span class="title-ref">DataFrame.query</span> where method calls in query strings led to errors when the `numexpr` package was installed (`22435`)
  - Bug in <span class="title-ref">DataFrame.nlargest</span> and <span class="title-ref">Series.nlargest</span> where sorted result did not count indexes containing `np.nan` (`28984`)
  - Bug in indexing on a non-unique object-dtype <span class="title-ref">Index</span> with an NA scalar (e.g. `np.nan`) (`43711`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> incorrectly writing into an existing column's array rather than setting a new array when the new dtype and the old dtype match (`43406`)
  - Bug in setting floating-dtype values into a <span class="title-ref">Series</span> with integer dtype failing to set inplace when those values can be losslessly converted to integers (`44316`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> with object dtype when setting an array with matching size and dtype='datetime64\[ns\]' or dtype='timedelta64\[ns\]' incorrectly converting the datetime/timedeltas to integers (`43868`)
  - Bug in <span class="title-ref">DataFrame.sort\_index</span> where `ignore_index=True` was not being respected when the index was already sorted (`43591`)
  - Bug in <span class="title-ref">Index.get\_indexer\_non\_unique</span> when index contains multiple `np.datetime64("NaT")` and `np.timedelta64("NaT")` (`43869`)
  - Bug in setting a scalar <span class="title-ref">Interval</span> value into a <span class="title-ref">Series</span> with `IntervalDtype` when the scalar's sides are floats and the values' sides are integers (`44201`)
  - Bug when setting string-backed <span class="title-ref">Categorical</span> values that can be parsed to datetimes into a <span class="title-ref">DatetimeArray</span> or <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> column backed by <span class="title-ref">DatetimeArray</span> failing to parse these strings (`44236`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> with an integer dtype other than `int64` setting with a `range` object unnecessarily upcasting to `int64` (`44261`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> with a boolean mask indexer setting a listlike value of length 1 incorrectly broadcasting that value (`44265`)
  - Bug in <span class="title-ref">Series.reset\_index</span> not ignoring `name` argument when `drop` and `inplace` are set to `True` (`44575`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> and <span class="title-ref">DataFrame.iloc.\_\_setitem\_\_</span> with mixed dtypes sometimes failing to operate in-place (`44345`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_getitem\_\_</span> incorrectly raising `KeyError` when selecting a single column with a boolean key (`44322`).
  - Bug in setting <span class="title-ref">DataFrame.iloc</span> with a single `ExtensionDtype` column and setting 2D values e.g. `df.iloc[:] = df.values` incorrectly raising (`44514`)
  - Bug in setting values with <span class="title-ref">DataFrame.iloc</span> with a single `ExtensionDtype` column and a tuple of arrays as the indexer (`44703`)
  - Bug in indexing on columns with `loc` or `iloc` using a slice with a negative step with `ExtensionDtype` columns incorrectly raising (`44551`)
  - Bug in <span class="title-ref">DataFrame.loc.\_\_setitem\_\_</span> changing dtype when indexer was completely `False` (`37550`)
  - Bug in <span class="title-ref">IntervalIndex.get\_indexer\_non\_unique</span> returning boolean mask instead of array of integers for a non unique and non monotonic index (`44084`)
  - Bug in <span class="title-ref">IntervalIndex.get\_indexer\_non\_unique</span> not handling targets of `dtype` 'object' with NaNs correctly (`44482`)
  - Fixed regression where a single column `np.matrix` was no longer coerced to a 1d `np.ndarray` when added to a <span class="title-ref">DataFrame</span> (`42376`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> with a <span class="title-ref">CategoricalIndex</span> of integers treating lists of integers as positional indexers, inconsistent with the behavior with a single scalar integer (`15470`, `14865`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> when setting floats or integers into integer-dtype <span class="title-ref">Series</span> failing to upcast when necessary to retain precision (`45121`)
  - Bug in <span class="title-ref">DataFrame.iloc.\_\_setitem\_\_</span> ignores axis argument (`45032`)

### Missing

  - Bug in <span class="title-ref">DataFrame.fillna</span> with `limit` and no `method` ignores `axis='columns'` or `axis = 1` (`40989`, `17399`)
  - Bug in <span class="title-ref">DataFrame.fillna</span> not replacing missing values when using a dict-like `value` and duplicate column names (`43476`)
  - Bug in constructing a <span class="title-ref">DataFrame</span> with a dictionary `np.datetime64` as a value and `dtype='timedelta64[ns]'`, or vice-versa, incorrectly casting instead of raising (`44428`)
  - Bug in <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> with `inplace=True` not writing to the underlying array(s) in-place (`44749`)
  - Bug in <span class="title-ref">Index.fillna</span> incorrectly returning an unfilled <span class="title-ref">Index</span> when NA values are present and `downcast` argument is specified. This now raises `NotImplementedError` instead; do not pass `downcast` argument (`44873`)
  - Bug in <span class="title-ref">DataFrame.dropna</span> changing <span class="title-ref">Index</span> even if no entries were dropped (`41965`)
  - Bug in <span class="title-ref">Series.fillna</span> with an object-dtype incorrectly ignoring `downcast="infer"` (`44241`)

### MultiIndex

  - Bug in <span class="title-ref">MultiIndex.get\_loc</span> where the first level is a <span class="title-ref">DatetimeIndex</span> and a string key is passed (`42465`)
  - Bug in <span class="title-ref">MultiIndex.reindex</span> when passing a `level` that corresponds to an `ExtensionDtype` level (`42043`)
  - Bug in <span class="title-ref">MultiIndex.get\_loc</span> raising `TypeError` instead of `KeyError` on nested tuple (`42440`)
  - Bug in <span class="title-ref">MultiIndex.union</span> setting wrong `sortorder` causing errors in subsequent indexing operations with slices (`44752`)
  - Bug in <span class="title-ref">MultiIndex.putmask</span> where the other value was also a <span class="title-ref">MultiIndex</span> (`43212`)
  - Bug in <span class="title-ref">MultiIndex.dtypes</span> duplicate level names returned only one dtype per name (`45174`)

### I/O

  - Bug in <span class="title-ref">read\_excel</span> attempting to read chart sheets from .xlsx files (`41448`)
  - Bug in <span class="title-ref">json\_normalize</span> where `errors=ignore` could fail to ignore missing values of `meta` when `record_path` has a length greater than one (`41876`)
  - Bug in <span class="title-ref">read\_csv</span> with multi-header input and arguments referencing column names as tuples (`42446`)
  - Bug in <span class="title-ref">read\_fwf</span>, where difference in lengths of `colspecs` and `names` was not raising `ValueError` (`40830`)
  - Bug in <span class="title-ref">Series.to\_json</span> and <span class="title-ref">DataFrame.to\_json</span> where some attributes were skipped when serializing plain Python objects to JSON (`42768`, `33043`)
  - Column headers are dropped when constructing a <span class="title-ref">DataFrame</span> from a sqlalchemy's `Row` object (`40682`)
  - Bug in unpickling an <span class="title-ref">Index</span> with object dtype incorrectly inferring numeric dtypes (`43188`)
  - Bug in <span class="title-ref">read\_csv</span> where reading multi-header input with unequal lengths incorrectly raised `IndexError` (`43102`)
  - Bug in <span class="title-ref">read\_csv</span> raising `ParserError` when reading file in chunks and some chunk blocks have fewer columns than header for `engine="c"` (`21211`)
  - Bug in <span class="title-ref">read\_csv</span>, changed exception class when expecting a file path name or file-like object from `OSError` to `TypeError` (`43366`)
  - Bug in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_fwf</span> ignoring all `skiprows` except first when `nrows` is specified for `engine='python'` (`44021`, `10261`)
  - Bug in <span class="title-ref">read\_csv</span> keeping the original column in object format when `keep_date_col=True` is set (`13378`)
  - Bug in <span class="title-ref">read\_json</span> not handling non-numpy dtypes correctly (especially `category`) (`21892`, `33205`)
  - Bug in <span class="title-ref">json\_normalize</span> where multi-character `sep` parameter is incorrectly prefixed to every key (`43831`)
  - Bug in <span class="title-ref">json\_normalize</span> where reading data with missing multi-level metadata would not respect `errors="ignore"` (`44312`)
  - Bug in <span class="title-ref">read\_csv</span> used second row to guess implicit index if `header` was set to `None` for `engine="python"` (`22144`)
  - Bug in <span class="title-ref">read\_csv</span> not recognizing bad lines when `names` were given for `engine="c"` (`22144`)
  - Bug in <span class="title-ref">read\_csv</span> with `float_precision="round_trip"` which did not skip initial/trailing whitespace (`43713`)
  - Bug when Python is built without the lzma module: a warning was raised at the pandas import time, even if the lzma capability isn't used (`43495`)
  - Bug in <span class="title-ref">read\_csv</span> not applying dtype for `index_col` (`9435`)
  - Bug in dumping/loading a <span class="title-ref">DataFrame</span> with `yaml.dump(frame)` (`42748`)
  - Bug in <span class="title-ref">read\_csv</span> raising `ValueError` when `names` was longer than `header` but equal to data rows for `engine="python"` (`38453`)
  - Bug in <span class="title-ref">ExcelWriter</span>, where `engine_kwargs` were not passed through to all engines (`43442`)
  - Bug in <span class="title-ref">read\_csv</span> raising `ValueError` when `parse_dates` was used with <span class="title-ref">MultiIndex</span> columns (`8991`)
  - Bug in <span class="title-ref">read\_csv</span> not raising an `ValueError` when `\n` was specified as `delimiter` or `sep` which conflicts with `lineterminator` (`43528`)
  - Bug in <span class="title-ref">to\_csv</span> converting datetimes in categorical <span class="title-ref">Series</span> to integers (`40754`)
  - Bug in <span class="title-ref">read\_csv</span> converting columns to numeric after date parsing failed (`11019`)
  - Bug in <span class="title-ref">read\_csv</span> not replacing `NaN` values with `np.nan` before attempting date conversion (`26203`)
  - Bug in <span class="title-ref">read\_csv</span> raising `AttributeError` when attempting to read a .csv file and infer index column dtype from an nullable integer type (`44079`)
  - Bug in <span class="title-ref">to\_csv</span> always coercing datetime columns with different formats to the same format (`21734`)
  - <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span> with `compression` set to `'zip'` no longer create a zip file containing a file ending with ".zip". Instead, they try to infer the inner file name more smartly (`39465`)
  - Bug in <span class="title-ref">read\_csv</span> where reading a mixed column of booleans and missing values to a float type results in the missing values becoming 1.0 rather than NaN (`42808`, `34120`)
  - Bug in <span class="title-ref">to\_xml</span> raising error for `pd.NA` with extension array dtype (`43903`)
  - Bug in <span class="title-ref">read\_csv</span> when passing simultaneously a parser in `date_parser` and `parse_dates=False`, the parsing was still called (`44366`)
  - Bug in <span class="title-ref">read\_csv</span> not setting name of <span class="title-ref">MultiIndex</span> columns correctly when `index_col` is not the first column (`38549`)
  - Bug in <span class="title-ref">read\_csv</span> silently ignoring errors when failing to create a memory-mapped file (`44766`)
  - Bug in <span class="title-ref">read\_csv</span> when passing a `tempfile.SpooledTemporaryFile` opened in binary mode (`44748`)
  - Bug in <span class="title-ref">read\_json</span> raising `ValueError` when attempting to parse json strings containing "://" (`36271`)
  - Bug in <span class="title-ref">read\_csv</span> when `engine="c"` and `encoding_errors=None` which caused a segfault (`45180`)
  - Bug in <span class="title-ref">read\_csv</span> an invalid value of `usecols` leading to an unclosed file handle (`45384`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> fix memory leak (`43877`)

### Period

  - Bug in adding a <span class="title-ref">Period</span> object to a `np.timedelta64` object incorrectly raising `TypeError` (`44182`)
  - Bug in <span class="title-ref">PeriodIndex.to\_timestamp</span> when the index has `freq="B"` inferring `freq="D"` for its result instead of `freq="B"` (`44105`)
  - Bug in <span class="title-ref">Period</span> constructor incorrectly allowing `np.timedelta64("NaT")` (`44507`)
  - Bug in <span class="title-ref">PeriodIndex.to\_timestamp</span> giving incorrect values for indexes with non-contiguous data (`44100`)
  - Bug in <span class="title-ref">Series.where</span> with `PeriodDtype` incorrectly raising when the `where` call should not replace anything (`45135`)

### Plotting

  - When given non-numeric data, <span class="title-ref">DataFrame.boxplot</span> now raises a `ValueError` rather than a cryptic `KeyError` or `ZeroDivisionError`, in line with other plotting functions like <span class="title-ref">DataFrame.hist</span> (`43480`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">SeriesGroupBy.apply</span> where passing an unrecognized string argument failed to raise `TypeError` when the underlying `Series` is empty (`42021`)
  - Bug in <span class="title-ref">Series.rolling.apply</span>, <span class="title-ref">DataFrame.rolling.apply</span>, <span class="title-ref">Series.expanding.apply</span> and <span class="title-ref">DataFrame.expanding.apply</span> with `engine="numba"` where `*args` were being cached with the user passed function (`42287`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.max</span>, <span class="title-ref">.SeriesGroupBy.max</span>, <span class="title-ref">.DataFrameGroupBy.min</span>, and <span class="title-ref">.SeriesGroupBy.min</span> with nullable integer dtypes losing precision (`41743`)
  - Bug in <span class="title-ref">DataFrame.groupby.rolling.var</span> would calculate the rolling variance only on the first group (`42442`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.shift</span> and <span class="title-ref">.SeriesGroupBy.shift</span> that would return the grouping columns if `fill_value` was not `None` (`41556`)
  - Bug in <span class="title-ref">SeriesGroupBy.nlargest</span> and <span class="title-ref">SeriesGroupBy.nsmallest</span> would have an inconsistent index when the input <span class="title-ref">Series</span> was sorted and `n` was greater than or equal to all group sizes (`15272`, `16345`, `29129`)
  - Bug in <span class="title-ref">pandas.DataFrame.ewm</span>, where non-float64 dtypes were silently failing (`42452`)
  - Bug in <span class="title-ref">pandas.DataFrame.rolling</span> operation along rows (`axis=1`) incorrectly omits columns containing `float16` and `float32` (`41779`)
  - Bug in <span class="title-ref">Resampler.aggregate</span> did not allow the use of Named Aggregation (`32803`)
  - Bug in <span class="title-ref">Series.rolling</span> when the <span class="title-ref">Series</span> `dtype` was `Int64` (`43016`)
  - Bug in <span class="title-ref">DataFrame.rolling.corr</span> when the <span class="title-ref">DataFrame</span> columns was a <span class="title-ref">MultiIndex</span> (`21157`)
  - Bug in <span class="title-ref">DataFrame.groupby.rolling</span> when specifying `on` and calling `__getitem__` would subsequently return incorrect results (`43355`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> with time-based <span class="title-ref">Grouper</span> objects incorrectly raising `ValueError` in corner cases where the grouping vector contains a `NaT` (`43500`, `43515`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.mean</span> and <span class="title-ref">.SeriesGroupBy.mean</span> failing with `complex` dtype (`43701`)
  - Bug in <span class="title-ref">Series.rolling</span> and <span class="title-ref">DataFrame.rolling</span> not calculating window bounds correctly for the first row when `center=True` and index is decreasing (`43927`)
  - Bug in <span class="title-ref">Series.rolling</span> and <span class="title-ref">DataFrame.rolling</span> for centered datetimelike windows with uneven nanosecond (`43997`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.mean</span> and <span class="title-ref">.SeriesGroupBy.mean</span> raising `KeyError` when column was selected at least twice (`44924`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.nth</span> and <span class="title-ref">.SeriesGroupBy.nth</span> failing on `axis=1` (`43926`)
  - Bug in <span class="title-ref">Series.rolling</span> and <span class="title-ref">DataFrame.rolling</span> not respecting right bound on centered datetime-like windows, if the index contain duplicates (`3944`)
  - Bug in <span class="title-ref">Series.rolling</span> and <span class="title-ref">DataFrame.rolling</span> when using a <span class="title-ref">pandas.api.indexers.BaseIndexer</span> subclass that returned unequal start and end arrays would segfault instead of raising a `ValueError` (`44470`)
  - Bug in <span class="title-ref">Groupby.nunique</span> not respecting `observed=True` for `categorical` grouping columns (`45128`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.head</span>, <span class="title-ref">.SeriesGroupBy.head</span>, <span class="title-ref">.DataFrameGroupBy.tail</span>, and <span class="title-ref">.SeriesGroupBy.tail</span> not dropping groups with `NaN` when `dropna=True` (`45089`)
  - Bug in <span class="title-ref">GroupBy.\_\_iter\_\_</span> after selecting a subset of columns in a <span class="title-ref">GroupBy</span> object, which returned all columns instead of the chosen subset (`44821`)
  - Bug in <span class="title-ref">Groupby.rolling</span> when non-monotonic data passed, fails to correctly raise `ValueError` (`43909`)
  - Bug where grouping by a <span class="title-ref">Series</span> that has a `categorical` data type and length unequal to the axis of grouping raised `ValueError` (`44179`)

### Reshaping

  - Improved error message when creating a <span class="title-ref">DataFrame</span> column from a multi-dimensional <span class="title-ref">numpy.ndarray</span> (`42463`)
  - Bug in <span class="title-ref">concat</span> creating <span class="title-ref">MultiIndex</span> with duplicate level entries when concatenating a <span class="title-ref">DataFrame</span> with duplicates in <span class="title-ref">Index</span> and multiple keys (`42651`)
  - Bug in <span class="title-ref">pandas.cut</span> on <span class="title-ref">Series</span> with duplicate indices and non-exact <span class="title-ref">pandas.CategoricalIndex</span> (`42185`, `42425`)
  - Bug in <span class="title-ref">DataFrame.append</span> failing to retain dtypes when appended columns do not match (`43392`)
  - Bug in <span class="title-ref">concat</span> of `bool` and `boolean` dtypes resulting in `object` dtype instead of `boolean` dtype (`42800`)
  - Bug in <span class="title-ref">crosstab</span> when inputs are categorical <span class="title-ref">Series</span>, there are categories that are not present in one or both of the <span class="title-ref">Series</span>, and `margins=True`. Previously the margin value for missing categories was `NaN`. It is now correctly reported as 0 (`43505`)
  - Bug in <span class="title-ref">concat</span> would fail when the `objs` argument all had the same index and the `keys` argument contained duplicates (`43595`)
  - Bug in <span class="title-ref">concat</span> which ignored the `sort` parameter (`43375`)
  - Bug in <span class="title-ref">merge</span> with <span class="title-ref">MultiIndex</span> as column index for the `on` argument returning an error when assigning a column internally (`43734`)
  - Bug in <span class="title-ref">crosstab</span> would fail when inputs are lists or tuples (`44076`)
  - Bug in <span class="title-ref">DataFrame.append</span> failing to retain `index.name` when appending a list of <span class="title-ref">Series</span> objects (`44109`)
  - Fixed metadata propagation in <span class="title-ref">Dataframe.apply</span> method, consequently fixing the same issue for <span class="title-ref">Dataframe.transform</span>, <span class="title-ref">Dataframe.nunique</span> and <span class="title-ref">Dataframe.mode</span> (`28283`)
  - Bug in <span class="title-ref">concat</span> casting levels of <span class="title-ref">MultiIndex</span> to float if all levels only consist of missing values (`44900`)
  - Bug in <span class="title-ref">DataFrame.stack</span> with `ExtensionDtype` columns incorrectly raising (`43561`)
  - Bug in <span class="title-ref">merge</span> raising `KeyError` when joining over differently named indexes with on keywords (`45094`)
  - Bug in <span class="title-ref">Series.unstack</span> with object doing unwanted type inference on resulting columns (`44595`)
  - Bug in <span class="title-ref">MultiIndex.join</span> with overlapping `IntervalIndex` levels (`44096`)
  - Bug in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> results is different `dtype` based on `regex` parameter (`44864`)
  - Bug in <span class="title-ref">DataFrame.pivot</span> with `index=None` when the <span class="title-ref">DataFrame</span> index was a <span class="title-ref">MultiIndex</span> (`23955`)

### Sparse

  - Bug in <span class="title-ref">DataFrame.sparse.to\_coo</span> raising `AttributeError` when column names are not unique (`29564`)
  - Bug in <span class="title-ref">SparseArray.max</span> and <span class="title-ref">SparseArray.min</span> raising `ValueError` for arrays with 0 non-null elements (`43527`)
  - Bug in <span class="title-ref">DataFrame.sparse.to\_coo</span> silently converting non-zero fill values to zero (`24817`)
  - Bug in <span class="title-ref">SparseArray</span> comparison methods with an array-like operand of mismatched length raising `AssertionError` or unclear `ValueError` depending on the input (`43863`)
  - Bug in <span class="title-ref">SparseArray</span> arithmetic methods `floordiv` and `mod` behaviors when dividing by zero not matching the non-sparse <span class="title-ref">Series</span> behavior (`38172`)
  - Bug in <span class="title-ref">SparseArray</span> unary methods as well as <span class="title-ref">SparseArray.isna</span> doesn't recalculate indexes (`44955`)

### ExtensionArray

  - Bug in <span class="title-ref">array</span> failing to preserve <span class="title-ref">PandasArray</span> (`43887`)
  - NumPy ufuncs `np.abs`, `np.positive`, `np.negative` now correctly preserve dtype when called on ExtensionArrays that implement `__abs__, __pos__, __neg__`, respectively. In particular this is fixed for <span class="title-ref">TimedeltaArray</span> (`43899`, `23316`)
  - NumPy ufuncs `np.minimum.reduce` `np.maximum.reduce`, `np.add.reduce`, and `np.prod.reduce` now work correctly instead of raising `NotImplementedError` on <span class="title-ref">Series</span> with `IntegerDtype` or `FloatDtype` (`43923`, `44793`)
  - NumPy ufuncs with `out` keyword are now supported by arrays with `IntegerDtype` and `FloatingDtype` (`45122`)
  - Avoid raising `PerformanceWarning` about fragmented <span class="title-ref">DataFrame</span> when using many columns with an extension dtype (`44098`)
  - Bug in <span class="title-ref">IntegerArray</span> and <span class="title-ref">FloatingArray</span> construction incorrectly coercing mismatched NA values (e.g. `np.timedelta64("NaT")`) to numeric NA (`44514`)
  - Bug in <span class="title-ref">BooleanArray.\_\_eq\_\_</span> and <span class="title-ref">BooleanArray.\_\_ne\_\_</span> raising `TypeError` on comparison with an incompatible type (like a string). This caused <span class="title-ref">DataFrame.replace</span> to sometimes raise a `TypeError` if a nullable boolean column was included (`44499`)
  - Bug in <span class="title-ref">array</span> incorrectly raising when passed a `ndarray` with `float16` dtype (`44715`)
  - Bug in calling `np.sqrt` on <span class="title-ref">BooleanArray</span> returning a malformed <span class="title-ref">FloatingArray</span> (`44715`)
  - Bug in <span class="title-ref">Series.where</span> with `ExtensionDtype` when `other` is a NA scalar incompatible with the <span class="title-ref">Series</span> dtype (e.g. `NaT` with a numeric dtype) incorrectly casting to a compatible NA value (`44697`)
  - Bug in <span class="title-ref">Series.replace</span> where explicitly passing `value=None` is treated as if no `value` was passed, and `None` not being in the result (`36984`, `19998`)
  - Bug in <span class="title-ref">Series.replace</span> with unwanted downcasting being done in no-op replacements (`44498`)
  - Bug in <span class="title-ref">Series.replace</span> with `FloatDtype`, `string[python]`, or `string[pyarrow]` dtype not being preserved when possible (`33484`, `40732`, `31644`, `41215`, `25438`)

### Styler

  - Bug in <span class="title-ref">.Styler</span> where the `uuid` at initialization maintained a floating underscore (`43037`)
  - Bug in <span class="title-ref">.Styler.to\_html</span> where the `Styler` object was updated if the `to_html` method was called with some args (`43034`)
  - Bug in <span class="title-ref">.Styler.copy</span> where `uuid` was not previously copied (`40675`)
  - Bug in <span class="title-ref">Styler.apply</span> where functions which returned <span class="title-ref">Series</span> objects were not correctly handled in terms of aligning their index labels (`13657`, `42014`)
  - Bug when rendering an empty <span class="title-ref">DataFrame</span> with a named <span class="title-ref">Index</span> (`43305`)
  - Bug when rendering a single level <span class="title-ref">MultiIndex</span> (`43383`)
  - Bug when combining non-sparse rendering and <span class="title-ref">.Styler.hide\_columns</span> or <span class="title-ref">.Styler.hide\_index</span> (`43464`)
  - Bug setting a table style when using multiple selectors in <span class="title-ref">.Styler</span> (`44011`)
  - Bugs where row trimming and column trimming failed to reflect hidden rows (`43703`, `44247`)

### Other

  - Bug in <span class="title-ref">DataFrame.astype</span> with non-unique columns and a <span class="title-ref">Series</span> `dtype` argument (`44417`)
  - Bug in <span class="title-ref">CustomBusinessMonthBegin.\_\_add\_\_</span> (<span class="title-ref">CustomBusinessMonthEnd.\_\_add\_\_</span>) not applying the extra `offset` parameter when beginning (end) of the target month is already a business day (`41356`)
  - Bug in <span class="title-ref">RangeIndex.union</span> with another `RangeIndex` with matching (even) `step` and starts differing by strictly less than `step / 2` (`44019`)
  - Bug in <span class="title-ref">RangeIndex.difference</span> with `sort=None` and `step<0` failing to sort (`44085`)
  - Bug in <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> with `value=None` and ExtensionDtypes (`44270`, `37899`)
  - Bug in <span class="title-ref">FloatingArray.equals</span> failing to consider two arrays equal if they contain `np.nan` values (`44382`)
  - Bug in <span class="title-ref">DataFrame.shift</span> with `axis=1` and `ExtensionDtype` columns incorrectly raising when an incompatible `fill_value` is passed (`44564`)
  - Bug in <span class="title-ref">DataFrame.shift</span> with `axis=1` and `periods` larger than `len(frame.columns)` producing an invalid <span class="title-ref">DataFrame</span> (`44978`)
  - Bug in <span class="title-ref">DataFrame.diff</span> when passing a NumPy integer object instead of an `int` object (`44572`)
  - Bug in <span class="title-ref">Series.replace</span> raising `ValueError` when using `regex=True` with a <span class="title-ref">Series</span> containing `np.nan` values (`43344`)
  - Bug in <span class="title-ref">DataFrame.to\_records</span> where an incorrect `n` was used when missing names were replaced by `level_n` (`44818`)
  - Bug in <span class="title-ref">DataFrame.eval</span> where `resolvers` argument was overriding the default resolvers (`34966`)
  - <span class="title-ref">Series.\_\_repr\_\_</span> and <span class="title-ref">DataFrame.\_\_repr\_\_</span> no longer replace all null-values in indexes with "NaN" but use their real string-representations. "NaN" is used only for `float("nan")` (`45263`)

## Contributors

<div class="contributors">

v1.3.5..v1.4.0

</div>

---

v1.4.1.md

---

# What's new in 1.4.1 (February 12, 2022)

These are the changes in pandas 1.4.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Regression in <span class="title-ref">Series.mask</span> with `inplace=True` and `PeriodDtype` and an incompatible `other` coercing to a common dtype instead of raising (`45546`)
  - Regression in <span class="title-ref">.assert\_frame\_equal</span> not respecting `check_flags=False` (`45554`)
  - Regression in <span class="title-ref">DataFrame.loc</span> raising `ValueError` when indexing (getting values) on a <span class="title-ref">MultiIndex</span> with one level (`45779`)
  - Regression in <span class="title-ref">Series.fillna</span> with `downcast=False` incorrectly downcasting `object` dtype (`45603`)
  - Regression in <span class="title-ref">api.types.is\_bool\_dtype</span> raising an `AttributeError` when evaluating a categorical <span class="title-ref">Series</span> (`45615`)
  - Regression in <span class="title-ref">DataFrame.iat</span> setting values leading to not propagating correctly in subsequent lookups (`45684`)
  - Regression when setting values with <span class="title-ref">DataFrame.loc</span> losing <span class="title-ref">Index</span> name if <span class="title-ref">DataFrame</span> was empty before (`45621`)
  - Regression in <span class="title-ref">\~Index.join</span> with overlapping <span class="title-ref">IntervalIndex</span> raising an `InvalidIndexError` (`45661`)
  - Regression when setting values with <span class="title-ref">Series.loc</span> raising with all `False` indexer and <span class="title-ref">Series</span> on the right hand side (`45778`)
  - Regression in <span class="title-ref">read\_sql</span> with a DBAPI2 connection that is not an instance of `sqlite3.Connection` incorrectly requiring SQLAlchemy be installed (`45660`)
  - Regression in <span class="title-ref">DateOffset</span> when constructing with an integer argument with no keywords (e.g. `pd.DateOffset(n)`) would behave like `datetime.timedelta(days=0)` (`45643`, `45890`)

## Bug fixes

  - Fixed segfault in <span class="title-ref">DataFrame.to\_json</span> when dumping tz-aware datetimes in Python 3.10 (`42130`)
  - Stopped emitting unnecessary `FutureWarning` in <span class="title-ref">DataFrame.sort\_values</span> with sparse columns (`45618`)
  - Fixed window aggregations in <span class="title-ref">DataFrame.rolling</span> and <span class="title-ref">Series.rolling</span> to skip over unused elements (`45647`)
  - Fixed builtin highlighters in <span class="title-ref">.Styler</span> to be responsive to `NA` with nullable dtypes (`45804`)
  - Bug in <span class="title-ref">\~Rolling.apply</span> with `axis=1` raising an erroneous `ValueError` (`45912`)

## Other

  - Reverted performance speedup of <span class="title-ref">DataFrame.corr</span> for `method=pearson` to fix precision regression (`45640`, `42761`)

## Contributors

<div class="contributors">

v1.4.0..v1.4.1

</div>

---

v1.4.2.md

---

# What's new in 1.4.2 (April 2, 2022)

These are the changes in pandas 1.4.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.drop</span> and <span class="title-ref">Series.drop</span> when <span class="title-ref">Index</span> had extension dtype and duplicates (`45860`)
  - Fixed regression in <span class="title-ref">read\_csv</span> killing python process when invalid file input was given for `engine="c"` (`45957`)
  - Fixed memory performance regression in <span class="title-ref">Series.fillna</span> when called on a <span class="title-ref">DataFrame</span> column with `inplace=True` (`46149`)
  - Provided an alternative solution for passing custom Excel formats in <span class="title-ref">.Styler.to\_excel</span>, which was a regression based on stricter CSS validation. Examples available in the documentation for <span class="title-ref">.Styler.format</span> (`46152`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> when a replacement value was also a target for replacement (`46306`)
  - Fixed regression in <span class="title-ref">DataFrame.replace</span> when the replacement value was explicitly `None` when passed in a dictionary to `to_replace` (`45601`, `45836`)
  - Fixed regression when setting values with <span class="title-ref">DataFrame.loc</span> losing <span class="title-ref">MultiIndex</span> names if <span class="title-ref">DataFrame</span> was empty before (`46317`)
  - Fixed regression when rendering boolean datatype columns with <span class="title-ref">.Styler</span> (`46384`)
  - Fixed regression in <span class="title-ref">Groupby.rolling</span> with a frequency window that would raise a `ValueError` even if the datetimes within each group were monotonic (`46061`)

## Bug fixes

  - Fix some cases for subclasses that define their `_constructor` properties as general callables (`46018`)
  - Fixed "longtable" formatting in <span class="title-ref">.Styler.to\_latex</span> when `column_format` is given in extended format (`46037`)
  - Fixed incorrect rendering in <span class="title-ref">.Styler.format</span> with `hyperlinks="html"` when the url contains a colon or other special characters (`46389`)
  - Improved error message in <span class="title-ref">.Rolling</span> when `window` is a frequency and `NaT` is in the rolling axis (`46087`)

## Contributors

<div class="contributors">

v1.4.1..v1.4.2

</div>

---

v1.4.3.md

---

# What's new in 1.4.3 (June 23, 2022)

These are the changes in pandas 1.4.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Behavior of `concat` with empty or all-NA DataFrame columns

The behavior change in version 1.4.0 to stop ignoring the data type of empty or all-NA columns with float or object dtype in <span class="title-ref">concat</span> (\[whatsnew\_140.notable\_bug\_fixes.concat\_with\_empty\_or\_all\_na\](\#whatsnew\_140.notable\_bug\_fixes.concat\_with\_empty\_or\_all\_na)) has been reverted (`45637`).

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.replace</span> when the replacement value was explicitly `None` when passed in a dictionary to `to_replace` also casting other columns to object dtype even when there were no values to replace (`46634`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_csv</span> raising error when <span class="title-ref">DataFrame</span> contains extension dtype categorical column (`46297`, `46812`)
  - Fixed regression in representation of `dtypes` attribute of <span class="title-ref">MultiIndex</span> (`46900`)
  - Fixed regression when setting values with <span class="title-ref">DataFrame.loc</span> updating <span class="title-ref">RangeIndex</span> when index was set as new column and column was updated afterwards (`47128`)
  - Fixed regression in <span class="title-ref">DataFrame.fillna</span> and <span class="title-ref">DataFrame.update</span> creating a copy when updating inplace (`47188`)
  - Fixed regression in <span class="title-ref">DataFrame.nsmallest</span> led to wrong results when the sorting column has `np.nan` values (`46589`)
  - Fixed regression in <span class="title-ref">read\_fwf</span> raising `ValueError` when `widths` was specified with `usecols` (`46580`)
  - Fixed regression in <span class="title-ref">concat</span> not sorting columns for mixed column names (`47127`)
  - Fixed regression in <span class="title-ref">.Groupby.transform</span> and <span class="title-ref">.Groupby.agg</span> failing with `engine="numba"` when the index was a <span class="title-ref">MultiIndex</span> (`46867`)
  - Fixed regression in `NaN` comparison for <span class="title-ref">Index</span> operations where the same object was compared (`47105`)
  - Fixed regression is <span class="title-ref">.Styler.to\_latex</span> and <span class="title-ref">.Styler.to\_html</span> where `buf` failed in combination with `encoding` (`47053`)
  - Fixed regression in <span class="title-ref">read\_csv</span> with `index_col=False` identifying first row as index names when `header=None` (`46955`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.agg</span> when used with list-likes or dict-likes and `axis=1` that would give incorrect results; now raises `NotImplementedError` (`46995`)
  - Fixed regression in <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">DataFrame.rolling</span> when used with list-likes or dict-likes and `axis=1` that would raise an unintuitive error message; now raises `NotImplementedError` (`46904`)
  - Fixed regression in <span class="title-ref">testing.assert\_index\_equal</span> when `check_order=False` and <span class="title-ref">Index</span> has extension or object dtype (`47207`)
  - Fixed regression in <span class="title-ref">read\_excel</span> returning ints as floats on certain input sheets (`46988`)
  - Fixed regression in <span class="title-ref">DataFrame.shift</span> when `axis` is `columns` and `fill_value` is absent, `freq` is ignored (`47039`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_json</span> causing a segmentation violation when <span class="title-ref">DataFrame</span> is created with an `index` parameter of the type <span class="title-ref">PeriodIndex</span> (`46683`)

## Bug fixes

  - Bug in <span class="title-ref">pandas.eval</span>, <span class="title-ref">DataFrame.eval</span> and <span class="title-ref">DataFrame.query</span> where passing empty `local_dict` or `global_dict` was treated as passing `None` (`47084`)
  - Most I/O methods no longer suppress `OSError` and `ValueError` when closing file handles (`47136`)
  - Improving error message raised by <span class="title-ref">DataFrame.from\_dict</span> when passing an invalid `orient` parameter (`47450`)

## Other

  - The minimum version of Cython needed to compile pandas is now `0.29.30` (`41935`)

## Contributors

<div class="contributors">

v1.4.2..v1.4.3

</div>

---

v1.4.4.md

---

# What's new in 1.4.4 (August 31, 2022)

These are the changes in pandas 1.4.4. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.fillna</span> not working on a <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> (`47649`)
  - Fixed regression in taking NULL <span class="title-ref">objects</span> from a <span class="title-ref">DataFrame</span> causing a segmentation violation. These NULL values are created by <span class="title-ref">numpy.empty\_like</span> (`46848`)
  - Fixed regression in <span class="title-ref">concat</span> materializing the <span class="title-ref">Index</span> during sorting even if the <span class="title-ref">Index</span> was already sorted (`47501`)
  - Fixed regression in <span class="title-ref">concat</span> or <span class="title-ref">merge</span> handling of all-NaN ExtensionArrays with custom attributes (`47762`)
  - Fixed regression in calling bitwise numpy ufuncs (for example, `np.bitwise_and`) on Index objects (`46769`)
  - Fixed regression in <span class="title-ref">cut</span> when using a `datetime64` IntervalIndex as bins (`46218`)
  - Fixed regression in <span class="title-ref">DataFrame.select\_dtypes</span> where `include="number"` included <span class="title-ref">BooleanDtype</span> (`46870`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> raising error when indexing with a `NamedTuple` (`48124`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> not updating the cache correctly after values were set (`47867`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> not aligning index in some cases when setting a <span class="title-ref">DataFrame</span> (`47578`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> setting a length-1 array like value to a single value in the DataFrame (`46268`)
  - Fixed regression when slicing with <span class="title-ref">DataFrame.loc</span> with <span class="title-ref">DatetimeIndex</span> with a <span class="title-ref">.DateOffset</span> object for its `freq` (`46671`)
  - Fixed regression in setting `None` or non-string value into a `string`-dtype Series using a mask (`47628`)
  - Fixed regression in updating a DataFrame column through Series `__setitem__` (using chained assignment) not updating column values inplace and using too much memory (`47172`)
  - Fixed regression in <span class="title-ref">DataFrame.select\_dtypes</span> returning a view on the original DataFrame (`48090`)
  - Fixed regression using custom Index subclasses (for example, used in xarray) with <span class="title-ref">\~DataFrame.reset\_index</span> or <span class="title-ref">Index.insert</span> (`47071`)
  - Fixed regression in <span class="title-ref">\~Index.intersection</span> when the <span class="title-ref">DatetimeIndex</span> has dates crossing daylight savings time (`46702`)
  - Fixed regression in <span class="title-ref">merge</span> throwing an error when passing a <span class="title-ref">Series</span> with a multi-level name (`47946`)
  - Fixed regression in <span class="title-ref">DataFrame.eval</span> creating a copy when updating inplace (`47449`)
  - Fixed regression where getting a row using <span class="title-ref">DataFrame.iloc</span> with <span class="title-ref">SparseDtype</span> would raise (`46406`)

## Bug fixes

  - The `FutureWarning` raised when passing arguments (other than `filepath_or_buffer`) as positional in <span class="title-ref">read\_csv</span> is now raised at the correct stacklevel (`47385`)
  - Bug in <span class="title-ref">DataFrame.to\_sql</span> when `method` was a `callable` that did not return an `int` and would raise a `TypeError` (`46891`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.value\_counts</span> where `subset` had no effect (`46383`)
  - Bug when getting values with <span class="title-ref">DataFrame.loc</span> with a list of keys causing an internal inconsistency that could lead to a disconnect between `frame.at[x, y]` vs `frame[y].loc[x]` (`22372`)
  - Bug in the <span class="title-ref">Series.dt.strftime</span> accessor return a float instead of object dtype Series for all-NaT input, which also causes a spurious deprecation warning (`45858`)

## Other

  - The minimum version of Cython needed to compile pandas is now `0.29.32` (`47978`)

## Contributors

<div class="contributors">

v1.4.3..v1.4.4|HEAD

</div>

---

v1.5.0.md

---

# What's new in 1.5.0 (September 19, 2022)

These are the changes in pandas 1.5.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### `pandas-stubs`

The `pandas-stubs` library is now supported by the pandas development team, providing type stubs for the pandas API. Please visit <https://github.com/pandas-dev/pandas-stubs> for more information.

We thank VirtusLab and Microsoft for their initial, significant contributions to `pandas-stubs`

### Native PyArrow-backed ExtensionArray

With [Pyarrow](https://arrow.apache.org/docs/python/index.html) installed, users can now create pandas objects that are backed by a `pyarrow.ChunkedArray` and `pyarrow.DataType`.

The `dtype` argument can accept a string of a [pyarrow data type](https://arrow.apache.org/docs/python/api/datatypes.html) with `pyarrow` in brackets e.g. `"int64[pyarrow]"` or, for pyarrow data types that take parameters, a <span class="title-ref">ArrowDtype</span> initialized with a `pyarrow.DataType`.

<div class="ipython">

python

import pyarrow as pa ser\_float = pd.Series(\[1.0, 2.0, None\], dtype="float32\[pyarrow\]") ser\_float

list\_of\_int\_type = pd.ArrowDtype([pa.list]()(pa.int64())) ser\_list = pd.Series(\[\[1, 2\], \[3, None\]\], dtype=list\_of\_int\_type) ser\_list

ser\_list.take(\[1, 0\]) ser\_float \* 5 ser\_float.mean() ser\_float.dropna()

</div>

Most operations are supported and have been implemented using [pyarrow compute](https://arrow.apache.org/docs/python/api/compute.html) functions. We recommend installing the latest version of PyArrow to access the most recently implemented compute functions.

\> **Warning** \> This feature is experimental, and the API can change in a future release without warning.

### DataFrame interchange protocol implementation

pandas now implement the DataFrame interchange API spec. See the full details on the API at <https://data-apis.org/dataframe-protocol/latest/index.html>

The protocol consists of two parts:

  - New method <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> which produces the interchange object. It effectively "exports" the pandas dataframe as an interchange object so any other library which has the protocol implemented can "import" that dataframe without knowing anything about the producer except that it makes an interchange object.
  - New function <span class="title-ref">pandas.api.interchange.from\_dataframe</span> which can take an arbitrary interchange object from any conformant library and construct a pandas DataFrame out of it.

### Styler

The most notable development is the new method <span class="title-ref">.Styler.concat</span> which allows adding customised footer rows to visualise additional calculations on the data, e.g. totals and counts etc. (`43875`, `46186`)

Additionally there is an alternative output method <span class="title-ref">.Styler.to\_string</span>, which allows using the Styler's formatting methods to create, for example, CSVs (`44502`).

A new feature <span class="title-ref">.Styler.relabel\_index</span> is also made available to provide full customisation of the display of index or column headers (`47864`)

Minor feature improvements are:

>   - Adding the ability to render `border` and `border-{side}` CSS properties in Excel (`42276`)
>   - Making keyword arguments consist: <span class="title-ref">.Styler.highlight\_null</span> now accepts `color` and deprecates `null_color` although this remains backwards compatible (`45907`)

### Control of index with `group_keys` in <span class="title-ref">DataFrame.resample</span>

The argument `group_keys` has been added to the method <span class="title-ref">DataFrame.resample</span>. As with <span class="title-ref">DataFrame.groupby</span>, this argument controls the whether each group is added to the index in the resample when <span class="title-ref">.Resampler.apply</span> is used.

<div class="warning">

<div class="title">

Warning

</div>

Not specifying the `group_keys` argument will retain the previous behavior and emit a warning if the result will change by specifying `group_keys=False`. In a future version of pandas, not specifying `group_keys` will default to the same behavior as `group_keys=False`.

</div>

`` `ipython     In [11]: df = pd.DataFrame(        ....:     {'a': range(6)},        ....:     index=pd.date_range("2021-01-01", periods=6, freq="8H")        ....: )        ....:      In [12]: df.resample("D", group_keys=True).apply(lambda x: x)     Out[12]:                                     a     2021-01-01 2021-01-01 00:00:00  0                2021-01-01 08:00:00  1                2021-01-01 16:00:00  2     2021-01-02 2021-01-02 00:00:00  3                2021-01-02 08:00:00  4                2021-01-02 16:00:00  5      In [13]: df.resample("D", group_keys=False).apply(lambda x: x)     Out[13]:                          a     2021-01-01 00:00:00  0     2021-01-01 08:00:00  1     2021-01-01 16:00:00  2     2021-01-02 00:00:00  3     2021-01-02 08:00:00  4     2021-01-02 16:00:00  5  Previously, the resulting index would depend upon the values returned by ``apply`,`\` as seen in the following example.

`` `ipython     In [1]: # pandas 1.3     In [2]: df.resample("D").apply(lambda x: x)     Out[2]:                          a     2021-01-01 00:00:00  0     2021-01-01 08:00:00  1     2021-01-01 16:00:00  2     2021-01-02 00:00:00  3     2021-01-02 08:00:00  4     2021-01-02 16:00:00  5      In [3]: df.resample("D").apply(lambda x: x.reset_index())     Out[3]:                                index  a     2021-01-01 0 2021-01-01 00:00:00  0                1 2021-01-01 08:00:00  1                2 2021-01-01 16:00:00  2     2021-01-02 0 2021-01-02 00:00:00  3                1 2021-01-02 08:00:00  4                2 2021-01-02 16:00:00  5  .. _whatsnew_150.enhancements.from_dummies:  from_dummies ``\` ^^^^^^^^^^^^

Added new function <span class="title-ref">\~pandas.from\_dummies</span> to convert a dummy coded <span class="title-ref">DataFrame</span> into a categorical <span class="title-ref">DataFrame</span>.

<div class="ipython">

python

import pandas as pd

  - df = pd.DataFrame({"col1\_a": \[1, 0, 1\], "col1\_b": \[0, 1, 0\],  
    "col2\_a": \[0, 1, 0\], "col2\_b": \[1, 0, 0\], "col2\_c": \[0, 0, 1\]})

pd.from\_dummies(df, sep="\_")

</div>

### Writing to ORC files

The new method <span class="title-ref">DataFrame.to\_orc</span> allows writing to ORC files (`43864`).

This functionality depends the [pyarrow](http://arrow.apache.org/docs/python/) library. For more details, see \[the IO docs on ORC \<io.orc\>\](\#the-io-docs-on-orc-\<io.orc\>).

\> **Warning** \> \* It is *highly recommended* to install pyarrow using conda due to some issues occurred by pyarrow. \* <span class="title-ref">\~pandas.DataFrame.to\_orc</span> requires pyarrow\>=7.0.0. \* <span class="title-ref">\~pandas.DataFrame.to\_orc</span> is not supported on Windows yet, you can find valid environments on \[install optional dependencies \<install.warn\_orc\>\](\#install-optional-dependencies-\<install.warn\_orc\>). \* For supported dtypes please refer to [supported ORC features in Arrow](https://arrow.apache.org/docs/cpp/orc.html#data-types). \* Currently timezones in datetime columns are not preserved when a dataframe is converted into ORC files.

`` `python     df = pd.DataFrame(data={"col1": [1, 2], "col2": [3, 4]})     df.to_orc("./out.orc")  .. _whatsnew_150.enhancements.tar:  Reading directly from TAR archives ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

I/O methods like <span class="title-ref">read\_csv</span> or <span class="title-ref">DataFrame.to\_json</span> now allow reading and writing directly on TAR archives (`44787`).

`` `python    df = pd.read_csv("./movement.tar.gz")    # ...    df.to_csv("./out.tar.gz")  This supports ``.tar`,`.tar.gz`,`.tar.bz`and`.tar.xz2`archives.`<span class="title-ref"> The used compression method is inferred from the filename. If the compression method cannot be inferred, use the </span><span class="title-ref">compression</span>\` argument:

`` `python    df = pd.read_csv(some_file_obj, compression={"method": "tar", "mode": "r:gz"}) # noqa F821  ( ``mode`being one of`tarfile.open`'s modes: https://docs.python.org/3/library/tarfile.html#tarfile.open)   .. _whatsnew_150.enhancements.read_xml_dtypes:  read_xml now supports`dtype`,`converters`, and`parse\_dates`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Similar to other IO methods, <span class="title-ref">pandas.read\_xml</span> now supports assigning specific dtypes to columns, apply converter methods, and parse dates (`43567`).

<div class="ipython">

python

from io import StringIO xml\_dates = """\<?xml version='1.0' encoding='utf-8'?\> \<data\> \<row\> \<shape\>square\</shape\> \<degrees\>00360\</degrees\> \<sides\>4.0\</sides\> \<date\>2020-01-01\</date\> \</row\> \<row\> \<shape\>circle\</shape\> \<degrees\>00360\</degrees\> \<sides/\> \<date\>2021-01-01\</date\> \</row\> \<row\> \<shape\>triangle\</shape\> \<degrees\>00180\</degrees\> \<sides\>3.0\</sides\> \<date\>2022-01-01\</date\> \</row\> \</data\>"""

  - df = pd.read\_xml(  
    StringIO(xml\_dates), dtype={'sides': 'Int64'}, converters={'degrees': str}, parse\_dates=\['date'\]

) df df.dtypes

</div>

### read\_xml now supports large XML using `iterparse`

For very large XML files that can range in hundreds of megabytes to gigabytes, <span class="title-ref">pandas.read\_xml</span> now supports parsing such sizeable files using [lxml's iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk) and [etree's iterparse]() which are memory-efficient methods to iterate through XML trees and extract specific elements and attributes without holding entire tree in memory (`45442`).

`` `ipython     In [1]: df = pd.read_xml(     ...      "/path/to/downloaded/enwikisource-latest-pages-articles.xml",     ...      iterparse = {"page": ["title", "ns", "id"]})     ...  )     df     Out[2]:                                                          title   ns        id     0                                       Gettysburg Address    0     21450     1                                                Main Page    0     42950     2                            Declaration by United Nations    0      8435     3             Constitution of the United States of America    0      8435     4                     Declaration of Independence (Israel)    0     17858     ...                                                    ...  ...       ...     3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649     3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649     3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649     3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291     3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450      [3578765 rows x 3 columns] ``\` .. \_\`etree's iterparse\`: <https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse>

### Copy on Write

A new feature `copy_on_write` was added (`46958`). Copy on write ensures that any DataFrame or Series derived from another in any way always behaves as a copy. Copy on write disallows updating any other object than the object the method was applied to.

Copy on write can be enabled through:

`` `python     pd.set_option("mode.copy_on_write", True)     pd.options.mode.copy_on_write = True  Alternatively, copy on write can be enabled locally through:  .. code-block:: python      with pd.option_context("mode.copy_on_write", True):         ...  Without copy on write, the parent `DataFrame` is updated when updating a child ``<span class="title-ref"> \`DataFrame</span> that was derived from this <span class="title-ref">DataFrame</span>.

<div class="ipython">

python

df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": 1}) view = df\["foo"\] view.iloc\[0\] df

</div>

With copy on write enabled, df won't be updated anymore:

<div class="ipython">

python

  - with pd.option\_context("mode.copy\_on\_write", True):  
    df = pd.DataFrame({"foo": \[1, 2, 3\], "bar": 1}) view = df\["foo"\] view.iloc\[0\] df

</div>

A more detailed explanation can be found [here](https://phofl.github.io/cow-introduction.html).

### Other enhancements

  - <span class="title-ref">Series.map</span> now raises when `arg` is dict but `na_action` is not either `None` or `'ignore'` (`46588`)
  - <span class="title-ref">MultiIndex.to\_frame</span> now supports the argument `allow_duplicates` and raises on duplicate labels if it is missing or False (`45245`)
  - <span class="title-ref">.StringArray</span> now accepts array-likes containing nan-likes (`None`, `np.nan`) for the `values` parameter in its constructor in addition to strings and <span class="title-ref">pandas.NA</span>. (`40839`)
  - Improved the rendering of `categories` in <span class="title-ref">CategoricalIndex</span> (`45218`)
  - <span class="title-ref">DataFrame.plot</span> will now allow the `subplots` parameter to be a list of iterables specifying column groups, so that columns may be grouped together in the same subplot (`29688`).
  - <span class="title-ref">to\_numeric</span> now preserves float64 arrays when downcasting would generate values not representable in float32 (`43693`)
  - <span class="title-ref">Series.reset\_index</span> and <span class="title-ref">DataFrame.reset\_index</span> now support the argument `allow_duplicates` (`44410`)
  - <span class="title-ref">.DataFrameGroupBy.min</span>, <span class="title-ref">.SeriesGroupBy.min</span>, <span class="title-ref">.DataFrameGroupBy.max</span>, and <span class="title-ref">.SeriesGroupBy.max</span> now supports [Numba](https://numba.pydata.org/) execution with the `engine` keyword (`45428`)
  - <span class="title-ref">read\_csv</span> now supports `defaultdict` as a `dtype` parameter (`41574`)
  - <span class="title-ref">DataFrame.rolling</span> and <span class="title-ref">Series.rolling</span> now support a `step` parameter with fixed-length windows (`15354`)
  - Implemented a `bool`-dtype <span class="title-ref">Index</span>, passing a bool-dtype array-like to `pd.Index` will now retain `bool` dtype instead of casting to `object` (`45061`)
  - Implemented a complex-dtype <span class="title-ref">Index</span>, passing a complex-dtype array-like to `pd.Index` will now retain complex dtype instead of casting to `object` (`45845`)
  - <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> with <span class="title-ref">IntegerDtype</span> now supports bitwise operations (`34463`)
  - Add `milliseconds` field support for <span class="title-ref">.DateOffset</span> (`43371`)
  - <span class="title-ref">DataFrame.where</span> tries to maintain dtype of <span class="title-ref">DataFrame</span> if fill value can be cast without loss of precision (`45582`)
  - <span class="title-ref">DataFrame.reset\_index</span> now accepts a `names` argument which renames the index names (`6878`)
  - <span class="title-ref">concat</span> now raises when `levels` is given but `keys` is None (`46653`)
  - <span class="title-ref">concat</span> now raises when `levels` contains duplicate values (`46653`)
  - Added `numeric_only` argument to <span class="title-ref">DataFrame.corr</span>, <span class="title-ref">DataFrame.corrwith</span>, <span class="title-ref">DataFrame.cov</span>, <span class="title-ref">DataFrame.idxmin</span>, <span class="title-ref">DataFrame.idxmax</span>, <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.DataFrameGroupBy.var</span>, <span class="title-ref">.SeriesGroupBy.var</span>, <span class="title-ref">.DataFrameGroupBy.std</span>, <span class="title-ref">.SeriesGroupBy.std</span>, <span class="title-ref">.DataFrameGroupBy.sem</span>, <span class="title-ref">.SeriesGroupBy.sem</span>, and <span class="title-ref">.DataFrameGroupBy.quantile</span> (`46560`)
  - A <span class="title-ref">errors.PerformanceWarning</span> is now thrown when using `string[pyarrow]` dtype with methods that don't dispatch to `pyarrow.compute` methods (`42613`, `46725`)
  - Added `validate` argument to <span class="title-ref">DataFrame.join</span> (`46622`)
  - Added `numeric_only` argument to <span class="title-ref">.Resampler.sum</span>, <span class="title-ref">.Resampler.prod</span>, <span class="title-ref">.Resampler.min</span>, <span class="title-ref">.Resampler.max</span>, <span class="title-ref">.Resampler.first</span>, and <span class="title-ref">.Resampler.last</span> (`46442`)
  - `times` argument in <span class="title-ref">.ExponentialMovingWindow</span> now accepts `np.timedelta64` (`47003`)
  - <span class="title-ref">.DataError</span>, <span class="title-ref">.SpecificationError</span>, `SettingWithCopyError`, `SettingWithCopyWarning`, <span class="title-ref">.NumExprClobberingError</span>, <span class="title-ref">.UndefinedVariableError</span>, <span class="title-ref">.IndexingError</span>, <span class="title-ref">.PyperclipException</span>, <span class="title-ref">.PyperclipWindowsException</span>, <span class="title-ref">.CSSWarning</span>, <span class="title-ref">.PossibleDataLossError</span>, <span class="title-ref">.ClosedFileError</span>, <span class="title-ref">.IncompatibilityWarning</span>, <span class="title-ref">.AttributeConflictWarning</span>, <span class="title-ref">.DatabaseError</span>, <span class="title-ref">.PossiblePrecisionLoss</span>, <span class="title-ref">.ValueLabelTypeMismatch</span>, <span class="title-ref">.InvalidColumnName</span>, and <span class="title-ref">.CategoricalConversionWarning</span> are now exposed in `pandas.errors` (`27656`)
  - Added `check_like` argument to <span class="title-ref">testing.assert\_series\_equal</span> (`47247`)
  - Add support for <span class="title-ref">.DataFrameGroupBy.ohlc</span> and <span class="title-ref">.SeriesGroupBy.ohlc</span> for extension array dtypes (`37493`)
  - Allow reading compressed SAS files with <span class="title-ref">read\_sas</span> (e.g., `.sas7bdat.gz` files)
  - <span class="title-ref">pandas.read\_html</span> now supports extracting links from table cells (`13141`)
  - <span class="title-ref">DatetimeIndex.astype</span> now supports casting timezone-naive indexes to `datetime64[s]`, `datetime64[ms]`, and `datetime64[us]`, and timezone-aware indexes to the corresponding `datetime64[unit, tzname]` dtypes (`47579`)
  - <span class="title-ref">Series</span> reducers (e.g. `min`, `max`, `sum`, `mean`) will now successfully operate when the dtype is numeric and `numeric_only=True` is provided; previously this would raise a `NotImplementedError` (`47500`)
  - <span class="title-ref">RangeIndex.union</span> now can return a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Int64Index</span> if the resulting values are equally spaced (`47557`, `43885`)
  - <span class="title-ref">DataFrame.compare</span> now accepts an argument `result_names` to allow the user to specify the result's names of both left and right DataFrame which are being compared. This is by default `'self'` and `'other'` (`44354`)
  - <span class="title-ref">DataFrame.quantile</span> gained a `method` argument that can accept `table` to evaluate multi-column quantiles (`43881`)
  - <span class="title-ref">Interval</span> now supports checking whether one interval is contained by another interval (`46613`)
  - Added `copy` keyword to <span class="title-ref">Series.set\_axis</span> and <span class="title-ref">DataFrame.set\_axis</span> to allow user to set axis on a new object without necessarily copying the underlying data (`47932`)
  - The method <span class="title-ref">.ExtensionArray.factorize</span> accepts `use_na_sentinel=False` for determining how null values are to be treated (`46601`)
  - The `Dockerfile` now installs a dedicated `pandas-dev` virtual environment for pandas development instead of using the `base` environment (`48427`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### Using `dropna=True` with `groupby` transforms

A transform is an operation whose result has the same size as its input. When the result is a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span>, it is also required that the index of the result matches that of the input. In pandas 1.4, using <span class="title-ref">.DataFrameGroupBy.transform</span> or <span class="title-ref">.SeriesGroupBy.transform</span> with null values in the groups and `dropna=True` gave incorrect results. Demonstrated by the examples below, the incorrect results either contained incorrect values, or the result did not have the same index as the input.

<div class="ipython">

python

df = pd.DataFrame({'a': \[1, 1, np.nan\], 'b': \[2, 3, 4\]})

</div>

*Old behavior*:

`` `ipython     In [3]: # Value in the last row should be np.nan             df.groupby('a', dropna=True).transform('sum')     Out[3]:        b     0  5     1  5     2  5      In [3]: # Should have one additional row with the value np.nan             df.groupby('a', dropna=True).transform(lambda x: x.sum())     Out[3]:        b     0  5     1  5      In [3]: # The value in the last row is np.nan interpreted as an integer             df.groupby('a', dropna=True).transform('ffill')     Out[3]:                          b     0                    2     1                    3     2 -9223372036854775808      In [3]: # Should have one additional row with the value np.nan             df.groupby('a', dropna=True).transform(lambda x: x)     Out[3]:        b     0  2     1  3  *New behavior*:  .. ipython:: python      df.groupby('a', dropna=True).transform('sum')     df.groupby('a', dropna=True).transform(lambda x: x.sum())     df.groupby('a', dropna=True).transform('ffill')     df.groupby('a', dropna=True).transform(lambda x: x)  .. _whatsnew_150.notable_bug_fixes.to_json_incorrectly_localizing_naive_timestamps:  Serializing tz-naive Timestamps with to_json() with ``iso\_dates=True`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

<span class="title-ref">DataFrame.to\_json</span>, <span class="title-ref">Series.to\_json</span>, and <span class="title-ref">Index.to\_json</span> would incorrectly localize DatetimeArrays/DatetimeIndexes with tz-naive Timestamps to UTC. (`38760`)

Note that this patch does not fix the localization of tz-aware Timestamps to UTC upon serialization. (Related issue `12997`)

*Old Behavior*

`` `ipython     In [32]: index = pd.date_range(        ....:     start='2020-12-28 00:00:00',        ....:     end='2020-12-28 02:00:00',        ....:     freq='1H',        ....: )        ....:      In [33]: a = pd.Series(        ....:     data=range(3),        ....:     index=index,        ....: )        ....:      In [4]: from io import StringIO      In [5]: a.to_json(date_format='iso')     Out[5]: '{"2020-12-28T00:00:00.000Z":0,"2020-12-28T01:00:00.000Z":1,"2020-12-28T02:00:00.000Z":2}'      In [6]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ="series").index == a.index     Out[6]: array([False, False, False])  *New Behavior*  .. code-block:: ipython      In [34]: from io import StringIO      In [35]: a.to_json(date_format='iso')     Out[35]: '{"2020-12-28T00:00:00.000Z":0,"2020-12-28T01:00:00.000Z":1,"2020-12-28T02:00:00.000Z":2}'      # Roundtripping now works     In [36]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ="series").index == a.index     Out[36]: array([ True,  True,  True])  .. _whatsnew_150.notable_bug_fixes.groupby_value_counts_categorical:  DataFrameGroupBy.value_counts with non-grouping categorical columns and ``observed=True`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Calling <span class="title-ref">.DataFrameGroupBy.value\_counts</span> with `observed=True` would incorrectly drop non-observed categories of non-grouping columns (`46357`).

`` `ipython     In [6]: df = pd.DataFrame(["a", "b", "c"], dtype="category").iloc[0:2]     In [7]: df     Out[7]:        0     0  a     1  b  *Old Behavior*  .. code-block:: ipython      In [8]: df.groupby(level=0, observed=True).value_counts()     Out[8]:     0  a    1     1  b    1     dtype: int64   *New Behavior*  .. code-block:: ipython      In [9]: df.groupby(level=0, observed=True).value_counts()     Out[9]:     0  a    1     1  a    0        b    1     0  b    0        c    0     1  c    0     dtype: int64  .. --------------------------------------------------------------------------- ``\` .. \_whatsnew\_150.api\_breaking:

## Backwards incompatible API changes

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated. If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.20.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>mypy (dev)</td>
<td>0.971</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.9.3</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>blosc</td>
<td>1.21.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>bottleneck</td>
<td>1.3.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>fsspec</td>
<td>2021.07.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>hypothesis</td>
<td>6.13.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>gcsfs</td>
<td>2021.07.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>jinja2</td>
<td>3.0.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>lxml</td>
<td>4.6.3</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>numba</td>
<td>0.53.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>numexpr</td>
<td>2.7.3</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>openpyxl</td>
<td>3.0.7</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pandas-gbq</td>
<td>0.15.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>psycopg2</td>
<td>2.8.6</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pymysql</td>
<td>1.0.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pyreadstat</td>
<td>1.1.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pyxlsb</td>
<td>1.0.8</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>s3fs</td>
<td>2021.08.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>scipy</td>
<td>1.7.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>sqlalchemy</td>
<td>1.4.16</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>tabulate</td>
<td>0.8.9</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xarray</td>
<td>0.19.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xlsxwriter</td>
<td>1.4.3</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.9.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>blosc</td>
<td>1.21.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>bottleneck</td>
<td>1.3.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>brotlipy</td>
<td>0.7.0</td>
<td></td>
</tr>
<tr class="odd">
<td>fastparquet</td>
<td>0.4.0</td>
<td></td>
</tr>
<tr class="even">
<td>fsspec</td>
<td>2021.08.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>html5lib</td>
<td>1.1</td>
<td></td>
</tr>
<tr class="even">
<td>hypothesis</td>
<td>6.13.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>gcsfs</td>
<td>2021.08.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>jinja2</td>
<td>3.0.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>lxml</td>
<td>4.6.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>3.3.2</td>
<td></td>
</tr>
<tr class="odd">
<td>numba</td>
<td>0.53.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>numexpr</td>
<td>2.7.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>odfpy</td>
<td>1.4.1</td>
<td></td>
</tr>
<tr class="even">
<td>openpyxl</td>
<td>3.0.7</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pandas-gbq</td>
<td>0.15.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>psycopg2</td>
<td>2.8.6</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pyarrow</td>
<td>1.0.1</td>
<td></td>
</tr>
<tr class="even">
<td>pymysql</td>
<td>1.0.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pyreadstat</td>
<td>1.1.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytables</td>
<td>3.6.1</td>
<td></td>
</tr>
<tr class="odd">
<td>python-snappy</td>
<td>0.6.0</td>
<td></td>
</tr>
<tr class="even">
<td>pyxlsb</td>
<td>1.0.8</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>s3fs</td>
<td>2021.08.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>scipy</td>
<td>1.7.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>sqlalchemy</td>
<td>1.4.16</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>tabulate</td>
<td>0.8.9</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>tzdata</td>
<td>2022a</td>
<td></td>
</tr>
<tr class="even">
<td>xarray</td>
<td>0.19.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xlrd</td>
<td>2.0.1</td>
<td></td>
</tr>
<tr class="even">
<td>xlsxwriter</td>
<td>1.4.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xlwt</td>
<td>1.3.0</td>
<td></td>
</tr>
<tr class="even">
<td>zstandard</td>
<td>0.15.2</td>
<td></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - BigQuery I/O methods <span class="title-ref">read\_gbq</span> and <span class="title-ref">DataFrame.to\_gbq</span> default to `auth_local_webserver = True`. Google has deprecated the `auth_local_webserver = False` ["out of band" (copy-paste) flow](https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob). The `auth_local_webserver = False` option is planned to stop working in October 2022. (`46312`)
  - <span class="title-ref">read\_json</span> now raises `FileNotFoundError` (previously `ValueError`) when input is a string ending in `.json`, `.json.gz`, `.json.bz2`, etc. but no such file exists. (`29102`)
  - Operations with <span class="title-ref">Timestamp</span> or <span class="title-ref">Timedelta</span> that would previously raise `OverflowError` instead raise `OutOfBoundsDatetime` or `OutOfBoundsTimedelta` where appropriate (`47268`)
  - When <span class="title-ref">read\_sas</span> previously returned `None`, it now returns an empty <span class="title-ref">DataFrame</span> (`47410`)
  - <span class="title-ref">DataFrame</span> constructor raises if `index` or `columns` arguments are sets (`47215`)

## Deprecations

\> **Warning** \> In the next major version release, 2.0, several larger API changes are being considered without a formal deprecation such as making the standard library [zoneinfo](https://docs.python.org/3/library/zoneinfo.html) the default timezone implementation instead of `pytz`, having the <span class="title-ref">Index</span> support all data types instead of having multiple subclasses (<span class="title-ref">CategoricalIndex</span>, <span class="title-ref">Int64Index</span>, etc.), and more. The changes under consideration are logged in [this GitHub issue](https://github.com/pandas-dev/pandas/issues/44823), and any feedback or concerns are welcome.

### Label-based integer slicing on a Series with an Int64Index or RangeIndex

In a future version, integer slicing on a <span class="title-ref">Series</span> with a <span class="title-ref">Int64Index</span> or <span class="title-ref">RangeIndex</span> will be treated as *label-based*, not positional. This will make the behavior consistent with other <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_setitem\_\_</span> behaviors (`45162`).

For example:

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3, 4, 5\], index=\[2, 3, 5, 7, 11\])

</div>

In the old behavior, `ser[2:4]` treats the slice as positional:

*Old behavior*:

`` `ipython     In [3]: ser[2:4]     Out[3]:     5    3     7    4     dtype: int64  In a future version, this will be treated as label-based:  *Future behavior*:  .. code-block:: ipython      In [4]: ser.loc[2:4]     Out[4]:     2    1     3    2     dtype: int64  To retain the old behavior, use ``series.iloc\[i:j\]`. To get the future behavior,`<span class="title-ref"> use </span><span class="title-ref">series.loc\[i:j\]</span>\`.

Slicing on a <span class="title-ref">DataFrame</span> will not be affected.

### <span class="title-ref">ExcelWriter</span> attributes

All attributes of <span class="title-ref">ExcelWriter</span> were previously documented as not public. However some third party Excel engines documented accessing `ExcelWriter.book` or `ExcelWriter.sheets`, and users were utilizing these and possibly other attributes. Previously these attributes were not safe to use; e.g. modifications to `ExcelWriter.book` would not update `ExcelWriter.sheets` and conversely. In order to support this, pandas has made some attributes public and improved their implementations so that they may now be safely used. (`45572`)

The following attributes are now public and considered safe to access.

>   - `book`
>   - `check_extension`
>   - `close`
>   - `date_format`
>   - `datetime_format`
>   - `engine`
>   - `if_sheet_exists`
>   - `sheets`
>   - `supported_extensions`

The following attributes have been deprecated. They now raise a `FutureWarning` when accessed and will be removed in a future version. Users should be aware that their usage is considered unsafe, and can lead to unexpected results.

>   - `cur_sheet`
>   - `handles`
>   - `path`
>   - `save`
>   - `write_cells`

See the documentation of <span class="title-ref">ExcelWriter</span> for further details.

### Using `group_keys` with transformers in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span>

In previous versions of pandas, if it was inferred that the function passed to <span class="title-ref">.DataFrameGroupBy.apply</span> or <span class="title-ref">.SeriesGroupBy.apply</span> was a transformer (i.e. the resulting index was equal to the input index), the `group_keys` argument of <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> was ignored and the group keys would never be added to the index of the result. In the future, the group keys will be added to the index when the user specifies `group_keys=True`.

As `group_keys=True` is the default value of <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span>, not specifying `group_keys` with a transformer will raise a `FutureWarning`. This can be silenced and the previous behavior retained by specifying `group_keys=False`.

### Inplace operation when setting values with `loc` and `iloc`

Most of the time setting values with <span class="title-ref">DataFrame.iloc</span> attempts to set values inplace, only falling back to inserting a new array if necessary. There are some cases where this rule is not followed, for example when setting an entire column from an array with different dtype:

<div class="ipython">

python

df = pd.DataFrame({'price': \[11.1, 12.2\]}, index=\['book1', 'book2'\]) original\_prices = df\['price'\] new\_prices = np.array(\[98, 99\])

</div>

*Old behavior*:

`` `ipython     In [3]: df.iloc[:, 0] = new_prices     In [4]: df.iloc[:, 0]     Out[4]:     book1    98     book2    99     Name: price, dtype: int64     In [5]: original_prices     Out[5]:     book1    11.1     book2    12.2     Name: price, float: 64  This behavior is deprecated. In a future version, setting an entire column with ``\` iloc will attempt to operate inplace.

*Future behavior*:

`` `ipython     In [3]: df.iloc[:, 0] = new_prices     In [4]: df.iloc[:, 0]     Out[4]:     book1    98.0     book2    99.0     Name: price, dtype: float64     In [5]: original_prices     Out[5]:     book1    98.0     book2    99.0     Name: price, dtype: float64  To get the old behavior, use `DataFrame.__setitem__` directly:  .. code-block:: ipython      In [3]: df[df.columns[0]] = new_prices     In [4]: df.iloc[:, 0]     Out[4]     book1    98     book2    99     Name: price, dtype: int64     In [5]: original_prices     Out[5]:     book1    11.1     book2    12.2     Name: price, dtype: float64  To get the old behaviour when ``df.columns`is not unique and you want to`<span class="title-ref"> change a single column by index, you can use \`DataFrame.isetitem</span>, which has been added in pandas 1.5:

`` `ipython     In [3]: df_with_duplicated_cols = pd.concat([df, df], axis='columns')     In [3]: df_with_duplicated_cols.isetitem(0, new_prices)     In [4]: df_with_duplicated_cols.iloc[:, 0]     Out[4]:     book1    98     book2    99     Name: price, dtype: int64     In [5]: original_prices     Out[5]:     book1    11.1     book2    12.2     Name: 0, dtype: float64  .. _whatsnew_150.deprecations.numeric_only_default: ``numeric\_only`default value`\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Across the <span class="title-ref">DataFrame</span>, <span class="title-ref">.DataFrameGroupBy</span>, and <span class="title-ref">.Resampler</span> operations such as `min`, `sum`, and `idxmax`, the default value of the `numeric_only` argument, if it exists at all, was inconsistent. Furthermore, operations with the default value `None` can lead to surprising results. (`46560`)

`` `ipython     In [1]: df = pd.DataFrame({"a": [1, 2], "b": ["x", "y"]})      In [2]: # Reading the next line without knowing the contents of df, one would             # expect the result to contain the products for both columns a and b.             df[["a", "b"]].prod()     Out[2]:     a    2     dtype: int64  To avoid this behavior, the specifying the value ``numeric\_only=None`has been`<span class="title-ref"> deprecated, and will be removed in a future version of pandas. In the future, all operations with a </span><span class="title-ref">numeric\_only</span><span class="title-ref"> argument will default to </span><span class="title-ref">False</span><span class="title-ref">. Users should either call the operation only with columns that can be operated on, or specify </span><span class="title-ref">numeric\_only=True</span>\` to operate only on Boolean, integer, and float columns.

In order to support the transition to the new behavior, the following methods have gained the `numeric_only` argument.

  - <span class="title-ref">DataFrame.corr</span>
  - <span class="title-ref">DataFrame.corrwith</span>
  - <span class="title-ref">DataFrame.cov</span>
  - <span class="title-ref">DataFrame.idxmin</span>
  - <span class="title-ref">DataFrame.idxmax</span>
  - <span class="title-ref">.DataFrameGroupBy.cummin</span>
  - <span class="title-ref">.DataFrameGroupBy.cummax</span>
  - <span class="title-ref">.DataFrameGroupBy.idxmin</span>
  - <span class="title-ref">.DataFrameGroupBy.idxmax</span>
  - <span class="title-ref">.DataFrameGroupBy.var</span>
  - <span class="title-ref">.DataFrameGroupBy.std</span>
  - <span class="title-ref">.DataFrameGroupBy.sem</span>
  - <span class="title-ref">.DataFrameGroupBy.quantile</span>
  - <span class="title-ref">.Resampler.mean</span>
  - <span class="title-ref">.Resampler.median</span>
  - <span class="title-ref">.Resampler.sem</span>
  - <span class="title-ref">.Resampler.std</span>
  - <span class="title-ref">.Resampler.var</span>
  - <span class="title-ref">DataFrame.rolling</span> operations
  - <span class="title-ref">DataFrame.expanding</span> operations
  - <span class="title-ref">DataFrame.ewm</span> operations

### Other Deprecations

  - Deprecated the keyword `line_terminator` in <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span>, use `lineterminator` instead; this is for consistency with <span class="title-ref">read\_csv</span> and the standard library 'csv' module (`9568`)
  - Deprecated behavior of <span class="title-ref">SparseArray.astype</span>, <span class="title-ref">Series.astype</span>, and <span class="title-ref">DataFrame.astype</span> with <span class="title-ref">SparseDtype</span> when passing a non-sparse `dtype`. In a future version, this will cast to that non-sparse dtype instead of wrapping it in a <span class="title-ref">SparseDtype</span> (`34457`)
  - Deprecated behavior of <span class="title-ref">DatetimeIndex.intersection</span> and <span class="title-ref">DatetimeIndex.symmetric\_difference</span> (`union` behavior was already deprecated in version 1.3.0) with mixed time zones; in a future version both will be cast to UTC instead of object dtype (`39328`, `45357`)
  - Deprecated <span class="title-ref">DataFrame.iteritems</span>, <span class="title-ref">Series.iteritems</span>, <span class="title-ref">HDFStore.iteritems</span> in favor of <span class="title-ref">DataFrame.items</span>, <span class="title-ref">Series.items</span>, <span class="title-ref">HDFStore.items</span> (`45321`)
  - Deprecated <span class="title-ref">Series.is\_monotonic</span> and <span class="title-ref">Index.is\_monotonic</span> in favor of <span class="title-ref">Series.is\_monotonic\_increasing</span> and <span class="title-ref">Index.is\_monotonic\_increasing</span> (`45422`, `21335`)
  - Deprecated behavior of <span class="title-ref">DatetimeIndex.astype</span>, <span class="title-ref">TimedeltaIndex.astype</span>, <span class="title-ref">PeriodIndex.astype</span> when converting to an integer dtype other than `int64`. In a future version, these will convert to exactly the specified dtype (instead of always `int64`) and will raise if the conversion overflows (`45034`)
  - Deprecated the `__array_wrap__` method of DataFrame and Series, rely on standard numpy ufuncs instead (`45451`)
  - Deprecated treating float-dtype data as wall-times when passed with a timezone to <span class="title-ref">Series</span> or <span class="title-ref">DatetimeIndex</span> (`45573`)
  - Deprecated the behavior of <span class="title-ref">Series.fillna</span> and <span class="title-ref">DataFrame.fillna</span> with `timedelta64[ns]` dtype and incompatible fill value; in a future version this will cast to a common dtype (usually object) instead of raising, matching the behavior of other dtypes (`45746`)
  - Deprecated the `warn` parameter in <span class="title-ref">infer\_freq</span> (`45947`)
  - Deprecated allowing non-keyword arguments in <span class="title-ref">.ExtensionArray.argsort</span> (`46134`)
  - Deprecated treating all-bool `object`-dtype columns as bool-like in <span class="title-ref">DataFrame.any</span> and <span class="title-ref">DataFrame.all</span> with `bool_only=True`, explicitly cast to bool instead (`46188`)
  - Deprecated behavior of method <span class="title-ref">DataFrame.quantile</span>, attribute `numeric_only` will default False. Including datetime/timedelta columns in the result (`7308`).
  - Deprecated <span class="title-ref">Timedelta.freq</span> and <span class="title-ref">Timedelta.is\_populated</span> (`46430`)
  - Deprecated <span class="title-ref">Timedelta.delta</span> (`46476`)
  - Deprecated passing arguments as positional in <span class="title-ref">DataFrame.any</span> and <span class="title-ref">Series.any</span> (`44802`)
  - Deprecated passing positional arguments to <span class="title-ref">DataFrame.pivot</span> and <span class="title-ref">pivot</span> except `data` (`30228`)
  - Deprecated the methods <span class="title-ref">DataFrame.mad</span>, <span class="title-ref">Series.mad</span>, and the corresponding groupby methods (`11787`)
  - Deprecated positional arguments to <span class="title-ref">Index.join</span> except for `other`, use keyword-only arguments instead of positional arguments (`46518`)
  - Deprecated positional arguments to <span class="title-ref">StringMethods.rsplit</span> and <span class="title-ref">StringMethods.split</span> except for `pat`, use keyword-only arguments instead of positional arguments (`47423`)
  - Deprecated indexing on a timezone-naive <span class="title-ref">DatetimeIndex</span> using a string representing a timezone-aware datetime (`46903`, `36148`)
  - Deprecated allowing `unit="M"` or `unit="Y"` in <span class="title-ref">Timestamp</span> constructor with a non-round float value (`47267`)
  - Deprecated the `display.column_space` global configuration option (`7576`)
  - Deprecated the argument `na_sentinel` in <span class="title-ref">factorize</span>, <span class="title-ref">Index.factorize</span>, and <span class="title-ref">.ExtensionArray.factorize</span>; pass `use_na_sentinel=True` instead to use the sentinel `-1` for NaN values and `use_na_sentinel=False` instead of `na_sentinel=None` to encode NaN values (`46910`)
  - Deprecated <span class="title-ref">.DataFrameGroupBy.transform</span> not aligning the result when the UDF returned DataFrame (`45648`)
  - Clarified warning from <span class="title-ref">to\_datetime</span> when delimited dates can't be parsed in accordance to specified `dayfirst` argument (`46210`)
  - Emit warning from <span class="title-ref">to\_datetime</span> when delimited dates can't be parsed in accordance to specified `dayfirst` argument even for dates where leading zero is omitted (e.g. `31/1/2001`) (`47880`)
  - Deprecated <span class="title-ref">Series</span> and <span class="title-ref">Resampler</span> reducers (e.g. `min`, `max`, `sum`, `mean`) raising a `NotImplementedError` when the dtype is non-numric and `numeric_only=True` is provided; this will raise a `TypeError` in a future version (`47500`)
  - Deprecated <span class="title-ref">Series.rank</span> returning an empty result when the dtype is non-numeric and `numeric_only=True` is provided; this will raise a `TypeError` in a future version (`47500`)
  - Deprecated argument `errors` for <span class="title-ref">Series.mask</span>, <span class="title-ref">Series.where</span>, <span class="title-ref">DataFrame.mask</span>, and <span class="title-ref">DataFrame.where</span> as `errors` had no effect on this methods (`47728`)
  - Deprecated arguments `*args` and `**kwargs` in <span class="title-ref">Rolling</span>, <span class="title-ref">Expanding</span>, and <span class="title-ref">ExponentialMovingWindow</span> ops. (`47836`)
  - Deprecated the `inplace` keyword in <span class="title-ref">Categorical.set\_ordered</span>, <span class="title-ref">Categorical.as\_ordered</span>, and <span class="title-ref">Categorical.as\_unordered</span> (`37643`)
  - Deprecated setting a categorical's categories with `cat.categories = ['a', 'b', 'c']`, use <span class="title-ref">Categorical.rename\_categories</span> instead (`37643`)
  - Deprecated unused arguments `encoding` and `verbose` in <span class="title-ref">Series.to\_excel</span> and <span class="title-ref">DataFrame.to\_excel</span> (`47912`)
  - Deprecated the `inplace` keyword in <span class="title-ref">DataFrame.set\_axis</span> and <span class="title-ref">Series.set\_axis</span>, use `obj = obj.set_axis(..., copy=False)` instead (`48130`)
  - Deprecated producing a single element when iterating over a <span class="title-ref">DataFrameGroupBy</span> or a <span class="title-ref">SeriesGroupBy</span> that has been grouped by a list of length 1; A tuple of length one will be returned instead (`42795`)
  - Fixed up warning message of deprecation of <span class="title-ref">MultiIndex.lesort\_depth</span> as public method, as the message previously referred to <span class="title-ref">MultiIndex.is\_lexsorted</span> instead (`38701`)
  - Deprecated the `sort_columns` argument in <span class="title-ref">DataFrame.plot</span> and <span class="title-ref">Series.plot</span> (`47563`).
  - Deprecated positional arguments for all but the first argument of <span class="title-ref">DataFrame.to\_stata</span> and <span class="title-ref">read\_stata</span>, use keyword arguments instead (`48128`).
  - Deprecated the `mangle_dupe_cols` argument in <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_fwf</span>, <span class="title-ref">read\_table</span> and <span class="title-ref">read\_excel</span>. The argument was never implemented, and a new argument where the renaming pattern can be specified will be added instead (`47718`)
  - Deprecated allowing `dtype='datetime64'` or `dtype=np.datetime64` in <span class="title-ref">Series.astype</span>, use "datetime64\[ns\]" instead (`47844`)

## Performance improvements

  - Performance improvement in <span class="title-ref">DataFrame.corrwith</span> for column-wise (axis=0) Pearson and Spearman correlation when other is a <span class="title-ref">Series</span> (`46174`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> for some user-defined DataFrame -\> Series functions (`45387`)
  - Performance improvement in <span class="title-ref">DataFrame.duplicated</span> when subset consists of only one column (`45236`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.diff</span> and <span class="title-ref">.SeriesGroupBy.diff</span> (`16706`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> when broadcasting values for user-defined functions (`45708`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> for user-defined functions when only a single group exists (`44977`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> when grouping on a non-unique unsorted index (`46527`)
  - Performance improvement in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> for tuple-based indexing of a <span class="title-ref">MultiIndex</span> (`45681`, `46040`, `46330`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.var</span> and <span class="title-ref">.SeriesGroupBy.var</span> with `ddof` other than one (`48152`)
  - Performance improvement in <span class="title-ref">DataFrame.to\_records</span> when the index is a <span class="title-ref">MultiIndex</span> (`47263`)
  - Performance improvement in <span class="title-ref">MultiIndex.values</span> when the MultiIndex contains levels of type DatetimeIndex, TimedeltaIndex or ExtensionDtypes (`46288`)
  - Performance improvement in <span class="title-ref">merge</span> when left and/or right are empty (`45838`)
  - Performance improvement in <span class="title-ref">DataFrame.join</span> when left and/or right are empty (`46015`)
  - Performance improvement in <span class="title-ref">DataFrame.reindex</span> and <span class="title-ref">Series.reindex</span> when target is a <span class="title-ref">MultiIndex</span> (`46235`)
  - Performance improvement when setting values in a pyarrow backed string array (`46400`)
  - Performance improvement in <span class="title-ref">factorize</span> (`46109`)
  - Performance improvement in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> constructors for extension dtype scalars (`45854`)
  - Performance improvement in <span class="title-ref">read\_excel</span> when `nrows` argument provided (`32727`)
  - Performance improvement in <span class="title-ref">.Styler.to\_excel</span> when applying repeated CSS formats (`47371`)
  - Performance improvement in <span class="title-ref">MultiIndex.is\_monotonic\_increasing</span> (`47458`)
  - Performance improvement in <span class="title-ref">BusinessHour</span> `str` and `repr` (`44764`)
  - Performance improvement in datetime arrays string formatting when one of the default strftime formats `"%Y-%m-%d %H:%M:%S"` or `"%Y-%m-%d %H:%M:%S.%f"` is used. (`44764`)
  - Performance improvement in <span class="title-ref">Series.to\_sql</span> and <span class="title-ref">DataFrame.to\_sql</span> (<span class="title-ref">SQLiteTable</span>) when processing time arrays. (`44764`)
  - Performance improvement to <span class="title-ref">read\_sas</span> (`47404`)
  - Performance improvement in `argmax` and `argmin` for <span class="title-ref">arrays.SparseArray</span> (`34197`)

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">.Categorical.view</span> not accepting integer dtypes (`25464`)
  - Bug in <span class="title-ref">.CategoricalIndex.union</span> when the index's categories are integer-dtype and the index contains `NaN` values incorrectly raising instead of casting to `float64` (`45362`)
  - Bug in <span class="title-ref">concat</span> when concatenating two (or more) unordered <span class="title-ref">CategoricalIndex</span> variables, whose categories are permutations, yields incorrect index values (`24845`)

### Datetimelike

  - Bug in <span class="title-ref">DataFrame.quantile</span> with datetime-like dtypes and no rows incorrectly returning `float64` dtype instead of retaining datetime-like dtype (`41544`)
  - Bug in <span class="title-ref">to\_datetime</span> with sequences of `np.str_` objects incorrectly raising (`32264`)
  - Bug in <span class="title-ref">Timestamp</span> construction when passing datetime components as positional arguments and `tzinfo` as a keyword argument incorrectly raising (`31929`)
  - Bug in <span class="title-ref">Index.astype</span> when casting from object dtype to `timedelta64[ns]` dtype incorrectly casting `np.datetime64("NaT")` values to `np.timedelta64("NaT")` instead of raising (`45722`)
  - Bug in <span class="title-ref">.SeriesGroupBy.value\_counts</span> index when passing categorical column (`44324`)
  - Bug in <span class="title-ref">DatetimeIndex.tz\_localize</span> localizing to UTC failing to make a copy of the underlying data (`46460`)
  - Bug in <span class="title-ref">DatetimeIndex.resolution</span> incorrectly returning "day" instead of "nanosecond" for nanosecond-resolution indexes (`46903`)
  - Bug in <span class="title-ref">Timestamp</span> with an integer or float value and `unit="Y"` or `unit="M"` giving slightly-wrong results (`47266`)
  - Bug in <span class="title-ref">.DatetimeArray</span> construction when passed another <span class="title-ref">.DatetimeArray</span> and `freq=None` incorrectly inferring the freq from the given array (`47296`)
  - Bug in <span class="title-ref">to\_datetime</span> where `OutOfBoundsDatetime` would be thrown even if `errors=coerce` if there were more than 50 rows (`45319`)
  - Bug when adding a <span class="title-ref">DateOffset</span> to a <span class="title-ref">Series</span> would not add the `nanoseconds` field (`47856`)

### Timedelta

  - Bug in <span class="title-ref">astype\_nansafe</span> astype("timedelta64\[ns\]") fails when np.nan is included (`45798`)
  - Bug in constructing a <span class="title-ref">Timedelta</span> with a `np.timedelta64` object and a `unit` sometimes silently overflowing and returning incorrect results instead of raising `OutOfBoundsTimedelta` (`46827`)
  - Bug in constructing a <span class="title-ref">Timedelta</span> from a large integer or float with `unit="W"` silently overflowing and returning incorrect results instead of raising `OutOfBoundsTimedelta` (`47268`)

### Time Zones

  - Bug in <span class="title-ref">Timestamp</span> constructor raising when passed a `ZoneInfo` tzinfo object (`46425`)

### Numeric

  - Bug in operations with array-likes with `dtype="boolean"` and <span class="title-ref">NA</span> incorrectly altering the array in-place (`45421`)
  - Bug in arithmetic operations with nullable types without <span class="title-ref">NA</span> values not matching the same operation with non-nullable types (`48223`)
  - Bug in `floordiv` when dividing by `IntegerDtype` `0` would return `0` instead of `inf` (`48223`)
  - Bug in division, `pow` and `mod` operations on array-likes with `dtype="boolean"` not being like their `np.bool_` counterparts (`46063`)
  - Bug in multiplying a <span class="title-ref">Series</span> with `IntegerDtype` or `FloatingDtype` by an array-like with `timedelta64[ns]` dtype incorrectly raising (`45622`)
  - Bug in <span class="title-ref">mean</span> where the optional dependency `bottleneck` causes precision loss linear in the length of the array. `bottleneck` has been disabled for <span class="title-ref">mean</span> improving the loss to log-linear but may result in a performance decrease. (`42878`)

### Conversion

  - Bug in <span class="title-ref">DataFrame.astype</span> not preserving subclasses (`40810`)
  - Bug in constructing a <span class="title-ref">Series</span> from a float-containing list or a floating-dtype ndarray-like (e.g. `dask.Array`) and an integer dtype raising instead of casting like we would with an `np.ndarray` (`40110`)
  - Bug in <span class="title-ref">Float64Index.astype</span> to unsigned integer dtype incorrectly casting to `np.int64` dtype (`45309`)
  - Bug in <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> from floating dtype to unsigned integer dtype failing to raise in the presence of negative values (`45151`)
  - Bug in <span class="title-ref">array</span> with `FloatingDtype` and values containing float-castable strings incorrectly raising (`45424`)
  - Bug when comparing string and datetime64ns objects causing `OverflowError` exception. (`45506`)
  - Bug in metaclass of generic abstract dtypes causing <span class="title-ref">DataFrame.apply</span> and <span class="title-ref">Series.apply</span> to raise for the built-in function `type` (`46684`)
  - Bug in <span class="title-ref">DataFrame.to\_records</span> returning inconsistent numpy types if the index was a <span class="title-ref">MultiIndex</span> (`47263`)
  - Bug in <span class="title-ref">DataFrame.to\_dict</span> for `orient="list"` or `orient="index"` was not returning native types (`46751`)
  - Bug in <span class="title-ref">DataFrame.apply</span> that returns a <span class="title-ref">DataFrame</span> instead of a <span class="title-ref">Series</span> when applied to an empty <span class="title-ref">DataFrame</span> and `axis=1` (`39111`)
  - Bug when inferring the dtype from an iterable that is *not* a NumPy `ndarray` consisting of all NumPy unsigned integer scalars did not result in an unsigned integer dtype (`47294`)
  - Bug in <span class="title-ref">DataFrame.eval</span> when pandas objects (e.g. `'Timestamp'`) were column names (`44603`)

### Strings

  - Bug in <span class="title-ref">str.startswith</span> and <span class="title-ref">str.endswith</span> when using other series as parameter \_[pat](). Now raises `TypeError` (`3485`)
  - Bug in <span class="title-ref">Series.str.zfill</span> when strings contain leading signs, padding '0' before the sign character rather than after as `str.zfill` from standard library (`20868`)

### Interval

  - Bug in <span class="title-ref">IntervalArray.\_\_setitem\_\_</span> when setting `np.nan` into an integer-backed array raising `ValueError` instead of `TypeError` (`45484`)
  - Bug in <span class="title-ref">IntervalDtype</span> when using datetime64\[ns, tz\] as a dtype string (`46999`)

### Indexing

  - Bug in <span class="title-ref">DataFrame.iloc</span> where indexing a single row on a <span class="title-ref">DataFrame</span> with a single ExtensionDtype column gave a copy instead of a view on the underlying data (`45241`)
  - Bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> returning copy when <span class="title-ref">DataFrame</span> has duplicated columns even if a unique column is selected (`45316`, `41062`)
  - Bug in <span class="title-ref">Series.align</span> does not create <span class="title-ref">MultiIndex</span> with union of levels when both MultiIndexes intersections are identical (`45224`)
  - Bug in setting a NA value (`None` or `np.nan`) into a <span class="title-ref">Series</span> with int-based <span class="title-ref">IntervalDtype</span> incorrectly casting to object dtype instead of a float-based <span class="title-ref">IntervalDtype</span> (`45568`)
  - Bug in indexing setting values into an `ExtensionDtype` column with `df.iloc[:, i] = values` with `values` having the same dtype as `df.iloc[:, i]` incorrectly inserting a new array instead of setting in-place (`33457`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> with a non-integer <span class="title-ref">Index</span> when using an integer key to set a value that cannot be set inplace where a `ValueError` was raised instead of casting to a common dtype (`45070`)
  - Bug in <span class="title-ref">DataFrame.loc</span> not casting `None` to `NA` when setting value as a list into <span class="title-ref">DataFrame</span> (`47987`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> when setting incompatible values into a `PeriodDtype` or `IntervalDtype` <span class="title-ref">Series</span> raising when indexing with a boolean mask but coercing when indexing with otherwise-equivalent indexers; these now consistently coerce, along with <span class="title-ref">Series.mask</span> and <span class="title-ref">Series.where</span> (`45768`)
  - Bug in <span class="title-ref">DataFrame.where</span> with multiple columns with datetime-like dtypes failing to downcast results consistent with other dtypes (`45837`)
  - Bug in <span class="title-ref">isin</span> upcasting to `float64` with unsigned integer dtype and list-like argument without a dtype (`46485`)
  - Bug in <span class="title-ref">Series.loc.\_\_setitem\_\_</span> and <span class="title-ref">Series.loc.\_\_getitem\_\_</span> not raising when using multiple keys without using a <span class="title-ref">MultiIndex</span> (`13831`)
  - Bug in <span class="title-ref">Index.reindex</span> raising `AssertionError` when `level` was specified but no <span class="title-ref">MultiIndex</span> was given; level is ignored now (`35132`)
  - Bug when setting a value too large for a <span class="title-ref">Series</span> dtype failing to coerce to a common type (`26049`, `32878`)
  - Bug in <span class="title-ref">loc.\_\_setitem\_\_</span> treating `range` keys as positional instead of label-based (`45479`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> casting extension array dtypes to object when setting with a scalar key and <span class="title-ref">DataFrame</span> as value (`46896`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> when setting a scalar to a nullable pandas dtype would not raise a `TypeError` if the scalar could not be cast (losslessly) to the nullable type (`45404`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> when setting `boolean` dtype values containing `NA` incorrectly raising instead of casting to `boolean` dtype (`45462`)
  - Bug in <span class="title-ref">Series.loc</span> raising with boolean indexer containing `NA` when <span class="title-ref">Index</span> did not match (`46551`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> where setting <span class="title-ref">NA</span> into a numeric-dtype <span class="title-ref">Series</span> would incorrectly upcast to object-dtype rather than treating the value as `np.nan` (`44199`)
  - Bug in <span class="title-ref">DataFrame.loc</span> when setting values to a column and right hand side is a dictionary (`47216`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> with `datetime64[ns]` dtype, an all-`False` boolean mask, and an incompatible value incorrectly casting to `object` instead of retaining `datetime64[ns]` dtype (`45967`)
  - Bug in <span class="title-ref">Index.\_\_getitem\_\_</span> raising `ValueError` when indexer is from boolean dtype with `NA` (`45806`)
  - Bug in <span class="title-ref">Series.\_\_setitem\_\_</span> losing precision when enlarging <span class="title-ref">Series</span> with scalar (`32346`)
  - Bug in <span class="title-ref">Series.mask</span> with `inplace=True` or setting values with a boolean mask with small integer dtypes incorrectly raising (`45750`)
  - Bug in <span class="title-ref">DataFrame.mask</span> with `inplace=True` and `ExtensionDtype` columns incorrectly raising (`45577`)
  - Bug in getting a column from a DataFrame with an object-dtype row index with datetime-like values: the resulting Series now preserves the exact object-dtype Index from the parent DataFrame (`42950`)
  - Bug in <span class="title-ref">DataFrame.\_\_getattribute\_\_</span> raising `AttributeError` if columns have `"string"` dtype (`46185`)
  - Bug in <span class="title-ref">DataFrame.compare</span> returning all `NaN` column when comparing extension array dtype and numpy dtype (`44014`)
  - Bug in <span class="title-ref">DataFrame.where</span> setting wrong values with `"boolean"` mask for numpy dtype (`44014`)
  - Bug in indexing on a <span class="title-ref">DatetimeIndex</span> with a `np.str_` key incorrectly raising (`45580`)
  - Bug in <span class="title-ref">CategoricalIndex.get\_indexer</span> when index contains `NaN` values, resulting in elements that are in target but not present in the index to be mapped to the index of the NaN element, instead of -1 (`45361`)
  - Bug in setting large integer values into <span class="title-ref">Series</span> with `float32` or `float16` dtype incorrectly altering these values instead of coercing to `float64` dtype (`45844`)
  - Bug in <span class="title-ref">Series.asof</span> and <span class="title-ref">DataFrame.asof</span> incorrectly casting bool-dtype results to `float64` dtype (`16063`)
  - Bug in <span class="title-ref">NDFrame.xs</span>, <span class="title-ref">DataFrame.iterrows</span>, <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">DataFrame.iloc</span> not always propagating metadata (`28283`)
  - Bug in <span class="title-ref">DataFrame.sum</span> min\_count changes dtype if input contains NaNs (`46947`)
  - Bug in <span class="title-ref">IntervalTree</span> that lead to an infinite recursion. (`46658`)
  - Bug in <span class="title-ref">PeriodIndex</span> raising `AttributeError` when indexing on `NA`, rather than putting `NaT` in its place. (`46673`)
  - Bug in <span class="title-ref">DataFrame.at</span> would allow the modification of multiple columns (`48296`)

### Missing

  - Bug in <span class="title-ref">Series.fillna</span> and <span class="title-ref">DataFrame.fillna</span> with `downcast` keyword not being respected in some cases where there are no NA values present (`45423`)
  - Bug in <span class="title-ref">Series.fillna</span> and <span class="title-ref">DataFrame.fillna</span> with <span class="title-ref">IntervalDtype</span> and incompatible value raising instead of casting to a common (usually object) dtype (`45796`)
  - Bug in <span class="title-ref">Series.map</span> not respecting `na_action` argument if mapper is a `dict` or <span class="title-ref">Series</span> (`47527`)
  - Bug in <span class="title-ref">DataFrame.interpolate</span> with object-dtype column not returning a copy with `inplace=False` (`45791`)
  - Bug in <span class="title-ref">DataFrame.dropna</span> allows to set both `how` and `thresh` incompatible arguments (`46575`)
  - Bug in <span class="title-ref">DataFrame.fillna</span> ignored `axis` when <span class="title-ref">DataFrame</span> is single block (`47713`)

### MultiIndex

  - Bug in <span class="title-ref">DataFrame.loc</span> returning empty result when slicing a <span class="title-ref">MultiIndex</span> with a negative step size and non-null start/stop values (`46156`)
  - Bug in <span class="title-ref">DataFrame.loc</span> raising when slicing a <span class="title-ref">MultiIndex</span> with a negative step size other than -1 (`46156`)
  - Bug in <span class="title-ref">DataFrame.loc</span> raising when slicing a <span class="title-ref">MultiIndex</span> with a negative step size and slicing a non-int labeled index level (`46156`)
  - Bug in <span class="title-ref">Series.to\_numpy</span> where multiindexed Series could not be converted to numpy arrays when an `na_value` was supplied (`45774`)
  - Bug in <span class="title-ref">MultiIndex.equals</span> not commutative when only one side has extension array dtype (`46026`)
  - Bug in <span class="title-ref">MultiIndex.from\_tuples</span> cannot construct Index of empty tuples (`45608`)

### I/O

  - Bug in <span class="title-ref">DataFrame.to\_stata</span> where no error is raised if the <span class="title-ref">DataFrame</span> contains `-np.inf` (`45350`)
  - Bug in <span class="title-ref">read\_excel</span> results in an infinite loop with certain `skiprows` callables (`45585`)
  - Bug in <span class="title-ref">DataFrame.info</span> where a new line at the end of the output is omitted when called on an empty <span class="title-ref">DataFrame</span> (`45494`)
  - Bug in <span class="title-ref">read\_csv</span> not recognizing line break for `on_bad_lines="warn"` for `engine="c"` (`41710`)
  - Bug in <span class="title-ref">DataFrame.to\_csv</span> not respecting `float_format` for `Float64` dtype (`45991`)
  - Bug in <span class="title-ref">read\_csv</span> not respecting a specified converter to index columns in all cases (`40589`)
  - Bug in <span class="title-ref">read\_csv</span> interpreting second row as <span class="title-ref">Index</span> names even when `index_col=False` (`46569`)
  - Bug in <span class="title-ref">read\_parquet</span> when `engine="pyarrow"` which caused partial write to disk when column of unsupported datatype was passed (`44914`)
  - Bug in <span class="title-ref">DataFrame.to\_excel</span> and <span class="title-ref">ExcelWriter</span> would raise when writing an empty DataFrame to a `.ods` file (`45793`)
  - Bug in <span class="title-ref">read\_csv</span> ignoring non-existing header row for `engine="python"` (`47400`)
  - Bug in <span class="title-ref">read\_excel</span> raising uncontrolled `IndexError` when `header` references non-existing rows (`43143`)
  - Bug in <span class="title-ref">read\_html</span> where elements surrounding `<br>` were joined without a space between them (`29528`)
  - Bug in <span class="title-ref">read\_csv</span> when data is longer than header leading to issues with callables in `usecols` expecting strings (`46997`)
  - Bug in Parquet roundtrip for Interval dtype with `datetime64[ns]` subtype (`45881`)
  - Bug in <span class="title-ref">read\_excel</span> when reading a `.ods` file with newlines between xml elements (`45598`)
  - Bug in <span class="title-ref">read\_parquet</span> when `engine="fastparquet"` where the file was not closed on error (`46555`)
  - <span class="title-ref">DataFrame.to\_html</span> now excludes the `border` attribute from `<table>` elements when `border` keyword is set to `False`.
  - Bug in <span class="title-ref">read\_sas</span> with certain types of compressed SAS7BDAT files (`35545`)
  - Bug in <span class="title-ref">read\_excel</span> not forward filling <span class="title-ref">MultiIndex</span> when no names were given (`47487`)
  - Bug in <span class="title-ref">read\_sas</span> returned `None` rather than an empty DataFrame for SAS7BDAT files with zero rows (`18198`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> using wrong missing value with extension arrays in <span class="title-ref">MultiIndex</span> (`47986`)
  - Bug in <span class="title-ref">StataWriter</span> where value labels were always written with default encoding (`46750`)
  - Bug in <span class="title-ref">StataWriterUTF8</span> where some valid characters were removed from variable names (`47276`)
  - Bug in <span class="title-ref">DataFrame.to\_excel</span> when writing an empty dataframe with <span class="title-ref">MultiIndex</span> (`19543`)
  - Bug in <span class="title-ref">read\_sas</span> with RLE-compressed SAS7BDAT files that contain 0x40 control bytes (`31243`)
  - Bug in <span class="title-ref">read\_sas</span> that scrambled column names (`31243`)
  - Bug in <span class="title-ref">read\_sas</span> with RLE-compressed SAS7BDAT files that contain 0x00 control bytes (`47099`)
  - Bug in <span class="title-ref">read\_parquet</span> with `use_nullable_dtypes=True` where `float64` dtype was returned instead of nullable `Float64` dtype (`45694`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> where `PeriodDtype` would not make the serialization roundtrip when read back with <span class="title-ref">read\_json</span> (`44720`)
  - Bug in <span class="title-ref">read\_xml</span> when reading XML files with Chinese character tags and would raise `XMLSyntaxError` (`47902`)

### Period

  - Bug in subtraction of <span class="title-ref">Period</span> from <span class="title-ref">.PeriodArray</span> returning wrong results (`45999`)
  - Bug in <span class="title-ref">Period.strftime</span> and <span class="title-ref">PeriodIndex.strftime</span>, directives `%l` and `%u` were giving wrong results (`46252`)
  - Bug in inferring an incorrect `freq` when passing a string to <span class="title-ref">Period</span> microseconds that are a multiple of 1000 (`46811`)
  - Bug in constructing a <span class="title-ref">Period</span> from a <span class="title-ref">Timestamp</span> or `np.datetime64` object with non-zero nanoseconds and `freq="ns"` incorrectly truncating the nanoseconds (`46811`)
  - Bug in adding `np.timedelta64("NaT", "ns")` to a <span class="title-ref">Period</span> with a timedelta-like freq incorrectly raising `IncompatibleFrequency` instead of returning `NaT` (`47196`)
  - Bug in adding an array of integers to an array with <span class="title-ref">PeriodDtype</span> giving incorrect results when `dtype.freq.n > 1` (`47209`)
  - Bug in subtracting a <span class="title-ref">Period</span> from an array with <span class="title-ref">PeriodDtype</span> returning incorrect results instead of raising `OverflowError` when the operation overflows (`47538`)

### Plotting

  - Bug in <span class="title-ref">DataFrame.plot.barh</span> that prevented labeling the x-axis and `xlabel` updating the y-axis label (`45144`)
  - Bug in <span class="title-ref">DataFrame.plot.box</span> that prevented labeling the x-axis (`45463`)
  - Bug in <span class="title-ref">DataFrame.boxplot</span> that prevented passing in `xlabel` and `ylabel` (`45463`)
  - Bug in <span class="title-ref">DataFrame.boxplot</span> that prevented specifying `vert=False` (`36918`)
  - Bug in <span class="title-ref">DataFrame.plot.scatter</span> that prevented specifying `norm` (`45809`)
  - Fix showing "None" as ylabel in <span class="title-ref">Series.plot</span> when not setting ylabel (`46129`)
  - Bug in <span class="title-ref">DataFrame.plot</span> that led to xticks and vertical grids being improperly placed when plotting a quarterly series (`47602`)
  - Bug in <span class="title-ref">DataFrame.plot</span> that prevented setting y-axis label, limits and ticks for a secondary y-axis (`47753`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">DataFrame.resample</span> ignoring `closed="right"` on <span class="title-ref">TimedeltaIndex</span> (`45414`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> fails when `func="size"` and the input DataFrame has multiple columns (`27469`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.size</span> and <span class="title-ref">.DataFrameGroupBy.transform</span> with `func="size"` produced incorrect results when `axis=1` (`45715`)
  - Bug in <span class="title-ref">.ExponentialMovingWindow.mean</span> with `axis=1` and `engine='numba'` when the <span class="title-ref">DataFrame</span> has more columns than rows (`46086`)
  - Bug when using `engine="numba"` would return the same jitted function when modifying `engine_kwargs` (`46086`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> fails when `axis=1` and `func` is `"first"` or `"last"` (`45986`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cumsum</span> with `skipna=False` giving incorrect results (`46216`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.sum</span>, <span class="title-ref">.SeriesGroupBy.sum</span>, <span class="title-ref">.DataFrameGroupBy.prod</span>, <span class="title-ref">.SeriesGroupBy.prod, :meth:</span>.DataFrameGroupBy.cumsum\`, and <span class="title-ref">.SeriesGroupBy.cumsum</span> with integer dtypes losing precision (`37493`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cumsum</span> and <span class="title-ref">.SeriesGroupBy.cumsum</span> with `timedelta64[ns]` dtype failing to recognize `NaT` as a null value (`46216`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cumsum</span> and <span class="title-ref">.SeriesGroupBy.cumsum</span> with integer dtypes causing overflows when sum was bigger than maximum of dtype (`37493`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cummin</span>, <span class="title-ref">.SeriesGroupBy.cummin</span>, <span class="title-ref">.DataFrameGroupBy.cummax</span> and <span class="title-ref">.SeriesGroupBy.cummax</span> with nullable dtypes incorrectly altering the original data in place (`46220`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> raising error when `None` is in first level of <span class="title-ref">MultiIndex</span> (`47348`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cummax</span> and <span class="title-ref">.SeriesGroupBy.cummax</span> with `int64` dtype with leading value being the smallest possible int64 (`46382`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.cumprod</span> and <span class="title-ref">.SeriesGroupBy.cumprod</span> `NaN` influences calculation in different columns with `skipna=False` (`48064`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.max</span> and <span class="title-ref">.SeriesGroupBy.max</span> with empty groups and `uint64` dtype incorrectly raising `RuntimeError` (`46408`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> would fail when `func` was a string and args or kwargs were supplied (`46479`)
  - Bug in <span class="title-ref">SeriesGroupBy.apply</span> would incorrectly name its result when there was a unique group (`46369`)
  - Bug in <span class="title-ref">.Rolling.sum</span> and <span class="title-ref">.Rolling.mean</span> would give incorrect result with window of same values (`42064`, `46431`)
  - Bug in <span class="title-ref">.Rolling.var</span> and <span class="title-ref">.Rolling.std</span> would give non-zero result with window of same values (`42064`)
  - Bug in <span class="title-ref">.Rolling.skew</span> and <span class="title-ref">.Rolling.kurt</span> would give NaN with window of same values (`30993`)
  - Bug in <span class="title-ref">.Rolling.var</span> would segfault calculating weighted variance when window size was larger than data size (`46760`)
  - Bug in <span class="title-ref">Grouper.\_\_repr\_\_</span> where `dropna` was not included. Now it is (`46754`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> gives ValueError when center=True, axis=1 and win\_type is specified (`46135`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.describe</span> and <span class="title-ref">.SeriesGroupBy.describe</span> produces inconsistent results for empty datasets (`41575`)
  - Bug in <span class="title-ref">DataFrame.resample</span> reduction methods when used with `on` would attempt to aggregate the provided column (`47079`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> would not respect `dropna=False` when the input DataFrame/Series had a NaN values in a <span class="title-ref">MultiIndex</span> (`46783`)
  - Bug in <span class="title-ref">DataFrameGroupBy.resample</span> raises `KeyError` when getting the result from a key list which misses the resample key (`47362`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> would lose index columns when the DataFrame is empty for transforms, like fillna (`47787`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> with `dropna=False` and `sort=False` would put any null groups at the end instead the order that they are encountered (`46584`)

### Reshaping

  - Bug in <span class="title-ref">concat</span> between a <span class="title-ref">Series</span> with integer dtype and another with <span class="title-ref">CategoricalDtype</span> with integer categories and containing `NaN` values casting to object dtype instead of `float64` (`45359`)
  - Bug in <span class="title-ref">get\_dummies</span> that selected object and categorical dtypes but not string (`44965`)
  - Bug in <span class="title-ref">DataFrame.align</span> when aligning a <span class="title-ref">MultiIndex</span> to a <span class="title-ref">Series</span> with another <span class="title-ref">MultiIndex</span> (`46001`)
  - Bug in concatenation with `IntegerDtype`, or `FloatingDtype` arrays where the resulting dtype did not mirror the behavior of the non-nullable dtypes (`46379`)
  - Bug in <span class="title-ref">concat</span> losing dtype of columns when `join="outer"` and `sort=True` (`47329`)
  - Bug in <span class="title-ref">concat</span> not sorting the column names when `None` is included (`47331`)
  - Bug in <span class="title-ref">concat</span> with identical key leads to error when indexing <span class="title-ref">MultiIndex</span> (`46519`)
  - Bug in <span class="title-ref">pivot\_table</span> raising `TypeError` when `dropna=True` and aggregation column has extension array dtype (`47477`)
  - Bug in <span class="title-ref">merge</span> raising error for `how="cross"` when using `FIPS` mode in ssl library (`48024`)
  - Bug in <span class="title-ref">DataFrame.join</span> with a list when using suffixes to join DataFrames with duplicate column names (`46396`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> with `sort=False` results in sorted index (`17041`)
  - Bug in <span class="title-ref">concat</span> when `axis=1` and `sort=False` where the resulting Index was a <span class="title-ref">Int64Index</span> instead of a <span class="title-ref">RangeIndex</span> (`46675`)
  - Bug in <span class="title-ref">wide\_to\_long</span> raises when `stubnames` is missing in columns and `i` contains string dtype column (`46044`)
  - Bug in <span class="title-ref">DataFrame.join</span> with categorical index results in unexpected reordering (`47812`)

### Sparse

  - Bug in <span class="title-ref">Series.where</span> and <span class="title-ref">DataFrame.where</span> with `SparseDtype` failing to retain the array's `fill_value` (`45691`)
  - Bug in <span class="title-ref">SparseArray.unique</span> fails to keep original elements order (`47809`)

### ExtensionArray

  - Bug in <span class="title-ref">IntegerArray.searchsorted</span> and <span class="title-ref">FloatingArray.searchsorted</span> returning inconsistent results when acting on `np.nan` (`45255`)

### Styler

  - Bug when attempting to apply styling functions to an empty DataFrame subset (`45313`)
  - Bug in <span class="title-ref">CSSToExcelConverter</span> leading to `TypeError` when border color provided without border style for `xlsxwriter` engine (`42276`)
  - Bug in <span class="title-ref">Styler.set\_sticky</span> leading to white text on white background in dark mode (`46984`)
  - Bug in <span class="title-ref">Styler.to\_latex</span> causing `UnboundLocalError` when `clines="all;data"` and the `DataFrame` has no rows. (`47203`)
  - Bug in <span class="title-ref">Styler.to\_excel</span> when using `vertical-align: middle;` with `xlsxwriter` engine (`30107`)
  - Bug when applying styles to a DataFrame with boolean column labels (`47838`)

### Metadata

  - Fixed metadata propagation in <span class="title-ref">DataFrame.melt</span> (`28283`)
  - Fixed metadata propagation in <span class="title-ref">DataFrame.explode</span> (`28283`)

### Other

  - Bug in <span class="title-ref">.assert\_index\_equal</span> with `names=True` and `check_order=False` not checking names (`47328`)

## Contributors

<div class="contributors">

v1.4.4..v1.5.0

</div>

---

v1.5.1.md

---

# What's new in 1.5.1 (October 19, 2022)

These are the changes in pandas 1.5.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Behavior of `groupby` with categorical groupers (`48645`)

In versions of pandas prior to 1.5, `groupby` with `dropna=False` would still drop NA values when the grouper was a categorical dtype. A fix for this was attempted in 1.5, however it introduced a regression where passing `observed=False` and `dropna=False` to `groupby` would result in only observed categories. It was found that the patch fixing the `dropna=False` bug is incompatible with `observed=False`, and decided that the best resolution is to restore the correct `observed=False` behavior at the cost of reintroducing the `dropna=False` bug.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "x": pd.Categorical(\[1, None\], categories=\[1, 2, 3\]), "y": \[3, 4\],
    
    }

) df

</div>

*1.5.0 behavior*:

`` `ipython    In [3]: # Correct behavior, NA values are not dropped            df.groupby("x", observed=True, dropna=False).sum()    Out[3]:         y    x    1    3    NaN  4      In [4]: # Incorrect behavior, only observed categories present            df.groupby("x", observed=False, dropna=False).sum()    Out[4]:         y    x    1    3    NaN  4   *1.5.1 behavior*:  .. ipython:: python     # Incorrect behavior, NA values are dropped    df.groupby("x", observed=True, dropna=False).sum()     # Correct behavior, unobserved categories present (NA values still dropped)    df.groupby("x", observed=False, dropna=False).sum()  .. _whatsnew_151.regressions:  Fixed regressions ``<span class="title-ref"> \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~ - Fixed Regression in \`Series.\_\_setitem\_\_</span> casting `None` to `NaN` for object dtype (`48665`) - Fixed Regression in <span class="title-ref">DataFrame.loc</span> when setting values as a <span class="title-ref">DataFrame</span> with all `True` indexer (`48701`) - Regression in <span class="title-ref">.read\_csv</span> causing an `EmptyDataError` when using an UTF-8 file handle that was already read from (`48646`) - Regression in <span class="title-ref">to\_datetime</span> when `utc=True` and `arg` contained timezone naive and aware arguments raised a `ValueError` (`48678`) - Fixed regression in <span class="title-ref">DataFrame.loc</span> raising `FutureWarning` when setting an empty <span class="title-ref">DataFrame</span> (`48480`) - Fixed regression in <span class="title-ref">DataFrame.describe</span> raising `TypeError` when result contains `NA` (`48778`) - Fixed regression in <span class="title-ref">DataFrame.plot</span> ignoring invalid `colormap` for `kind="scatter"` (`48726`) - Fixed regression in <span class="title-ref">MultiIndex.values</span> resetting `freq` attribute of underlying <span class="title-ref">Index</span> object (`49054`) - Fixed performance regression in <span class="title-ref">factorize</span> when `na_sentinel` is not `None` and `sort=False` (`48620`) - Fixed regression causing an `AttributeError` during warning emitted if the provided table name in <span class="title-ref">DataFrame.to\_sql</span> and the table name actually used in the database do not match (`48733`) - Fixed regression in <span class="title-ref">to\_datetime</span> when `arg` was a date string with nanosecond and `format` contained `%f` would raise a `ValueError` (`48767`) - Fixed regression in <span class="title-ref">testing.assert\_frame\_equal</span> raising for <span class="title-ref">MultiIndex</span> with <span class="title-ref">Categorical</span> and `check_like=True` (`48975`) - Fixed regression in <span class="title-ref">DataFrame.fillna</span> replacing wrong values for `datetime64[ns]` dtype and `inplace=True` (`48863`) - Fixed <span class="title-ref">.DataFrameGroupBy.size</span> not returning a Series when `axis=1` (`48738`) - Fixed Regression in <span class="title-ref">.DataFrameGroupBy.apply</span> when user defined function is called on an empty dataframe (`47985`) - Fixed regression in <span class="title-ref">DataFrame.apply</span> when passing non-zero `axis` via keyword argument (`48656`) - Fixed regression in <span class="title-ref">Series.groupby</span> and <span class="title-ref">DataFrame.groupby</span> when the grouper is a nullable data type (e.g. <span class="title-ref">Int64</span>) or a PyArrow-backed string array, contains null values, and `dropna=False` (`48794`) - Fixed performance regression in <span class="title-ref">Series.isin</span> with mismatching dtypes (`49162`) - Fixed regression in <span class="title-ref">DataFrame.to\_parquet</span> raising when file name was specified as `bytes` (`48944`) - Fixed regression in <span class="title-ref">ExcelWriter</span> where the `book` attribute could no longer be set; however setting this attribute is now deprecated and this ability will be removed in a future version of pandas (`48780`) - Fixed regression in <span class="title-ref">DataFrame.corrwith</span> when computing correlation on tied data with `method="spearman"` (`48826`)

## Bug fixes

  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> not falling back to positional for integer keys and boolean <span class="title-ref">Index</span> (`48653`)
  - Bug in <span class="title-ref">DataFrame.to\_hdf</span> raising `AssertionError` with boolean index (`48667`)
  - Bug in <span class="title-ref">testing.assert\_index\_equal</span> for extension arrays with non matching `NA` raising `ValueError` (`48608`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> raising unexpected `FutureWarning` when setting datetime column as index (`48683`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> emitting unnecessary `FutureWarning` when called on <span class="title-ref">DataFrame</span> with boolean sparse columns (`48784`)
  - Bug in <span class="title-ref">.arrays.ArrowExtensionArray</span> with a comparison operator to an invalid object would not raise a `NotImplementedError` (`48833`)

## Other

  - Avoid showing deprecated signatures when introspecting functions with warnings about arguments becoming keyword-only (`48692`)

## Contributors

<div class="contributors">

v1.5.0..v1.5.1

</div>

---

v1.5.2.md

---

# What's new in 1.5.2 (November 21, 2022)

These are the changes in pandas 1.5.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">MultiIndex.join</span> for extension array dtypes (`49277`)
  - Fixed regression in <span class="title-ref">Series.replace</span> raising `RecursionError` with numeric dtype and when specifying `value=None` (`45725`)
  - Fixed regression in arithmetic operations for <span class="title-ref">DataFrame</span> with <span class="title-ref">MultiIndex</span> columns with different dtypes (`49769`)
  - Fixed regression in <span class="title-ref">DataFrame.plot</span> preventing <span class="title-ref">\~matplotlib.colors.Colormap</span> instance from being passed using the `colormap` argument if Matplotlib 3.6+ is used (`49374`)
  - Fixed regression in <span class="title-ref">date\_range</span> returning an invalid set of periods for `CustomBusinessDay` frequency and `start` date with timezone (`49441`)
  - Fixed performance regression in groupby operations (`49676`)
  - Fixed regression in <span class="title-ref">Timedelta</span> constructor returning object of wrong type when subclassing `Timedelta` (`49579`)

## Bug fixes

  - Bug in the Copy-on-Write implementation losing track of views in certain chained indexing cases (`48996`)
  - Fixed memory leak in <span class="title-ref">.Styler.to\_excel</span> (`49751`)

## Other

  - Reverted `color` as an alias for `c` and `size` as an alias for `s` in function <span class="title-ref">DataFrame.plot.scatter</span> (`49732`)

## Contributors

<div class="contributors">

v1.5.1..v1.5.2

</div>

---

v1.5.3.md

---

# What's new in 1.5.3 (January 18, 2023)

These are the changes in pandas 1.5.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed performance regression in <span class="title-ref">Series.isin</span> when `values` is empty (`49839`)
  - Fixed regression in <span class="title-ref">DataFrame.memory\_usage</span> showing unnecessary `FutureWarning` when <span class="title-ref">DataFrame</span> is empty (`50066`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.transform</span> when used with `as_index=False` (`49834`)
  - Enforced reversion of `color` as an alias for `c` and `size` as an alias for `s` in function <span class="title-ref">DataFrame.plot.scatter</span> (`49732`)
  - Fixed regression in <span class="title-ref">.SeriesGroupBy.apply</span> setting a `name` attribute on the result if the result was a <span class="title-ref">DataFrame</span> (`49907`)
  - Fixed performance regression in setting with the <span class="title-ref">\~DataFrame.at</span> indexer (`49771`)
  - Fixed regression in <span class="title-ref">to\_datetime</span> raising `ValueError` when parsing array of `float` containing `np.nan` (`50237`)

## Bug fixes

  - Bug in the Copy-on-Write implementation losing track of views when indexing a <span class="title-ref">DataFrame</span> with another <span class="title-ref">DataFrame</span> (`50630`)
  - Bug in <span class="title-ref">.Styler.to\_excel</span> leading to error when unrecognized `border-style` (e.g. `"hair"`) provided to Excel writers (`48649`)
  - Bug in <span class="title-ref">Series.quantile</span> emitting warning from NumPy when <span class="title-ref">Series</span> has only `NA` values (`50681`)
  - Bug when chaining several <span class="title-ref">.Styler.concat</span> calls, only the last styler was concatenated (`49207`)
  - Fixed bug when instantiating a <span class="title-ref">DataFrame</span> subclass inheriting from `typing.Generic` that triggered a `UserWarning` on python 3.11 (`49649`)
  - Bug in <span class="title-ref">pivot\_table</span> with NumPy 1.24 or greater when the <span class="title-ref">DataFrame</span> columns has nested elements (`50342`)
  - Bug in <span class="title-ref">pandas.testing.assert\_series\_equal</span> (and equivalent `assert_` functions) when having nested data and using numpy \>= 1.25 (`50360`)

## Other

\> **Note** \> If you are using <span class="title-ref">DataFrame.to\_sql</span>, <span class="title-ref">read\_sql</span>, <span class="title-ref">read\_sql\_table</span>, or <span class="title-ref">read\_sql\_query</span> with SQLAlchemy 1.4.46 or greater, you may see a `sqlalchemy.exc.RemovedIn20Warning`. These warnings can be safely ignored for the SQLAlchemy 1.4.x releases as pandas works toward compatibility with SQLAlchemy 2.0.

  - Reverted deprecation (`45324`) of behavior of <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_setitem\_\_</span> slicing with an integer <span class="title-ref">Index</span>; this will remain positional (`49612`)
  - A `FutureWarning` raised when attempting to set values inplace with <span class="title-ref">DataFrame.loc</span> or <span class="title-ref">DataFrame.iloc</span> has been changed to a `DeprecationWarning` (`48673`)

## Contributors

<div class="contributors">

v1.5.2..v1.5.3

</div>

---

v2.0.0.md

---

# What's new in 2.0.0 (April 3, 2023)

These are the changes in pandas 2.0.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### Installing optional dependencies with pip extras

When installing pandas using pip, sets of optional dependencies can also be installed by specifying extras.

`` `bash   pip install "pandas[performance, aws]>=2.0.0"  The available extras, found in the [installation guide<install.dependencies>](#installation-guide<install.dependencies>), are ``<span class="title-ref"> </span><span class="title-ref">\[all, performance, computation, fss, aws, gcp, excel, parquet, feather, hdf5, spss, postgresql, mysql, sql-other, html, xml, plot, output\_formatting, clipboard, compression, test\]</span><span class="title-ref"> (:issue:\`39164</span>).

### <span class="title-ref">Index</span> can now hold numpy numeric dtypes

It is now possible to use any numpy numeric dtype in a <span class="title-ref">Index</span> (`42717`).

Previously it was only possible to use `int64`, `uint64` & `float64` dtypes:

`` `ipython     In [1]: pd.Index([1, 2, 3], dtype=np.int8)     Out[1]: Int64Index([1, 2, 3], dtype="int64")     In [2]: pd.Index([1, 2, 3], dtype=np.uint16)     Out[2]: UInt64Index([1, 2, 3], dtype="uint64")     In [3]: pd.Index([1, 2, 3], dtype=np.float32)     Out[3]: Float64Index([1.0, 2.0, 3.0], dtype="float64")  `Int64Index`, `UInt64Index` & `Float64Index` were deprecated in pandas ``<span class="title-ref"> version 1.4 and have now been removed. Instead \`Index</span> should be used directly, and can it now take all numpy numeric dtypes, i.e. `int8`/ `int16`/`int32`/`int64`/`uint8`/`uint16`/`uint32`/`uint64`/`float32`/`float64` dtypes:

<div class="ipython">

python

pd.Index(\[1, 2, 3\], dtype=np.int8) pd.Index(\[1, 2, 3\], dtype=np.uint16) pd.Index(\[1, 2, 3\], dtype=np.float32)

</div>

The ability for <span class="title-ref">Index</span> to hold the numpy numeric dtypes has meant some changes in pandas functionality. In particular, operations that previously were forced to create 64-bit indexes, can now create indexes with lower bit sizes, e.g. 32-bit indexes.

Below is a possibly non-exhaustive list of changes:

1.  Instantiating using a numpy numeric array now follows the dtype of the numpy array. Previously, all indexes created from numpy numeric arrays were forced to 64-bit. Now, for example, `Index(np.array([1, 2, 3]))` will be `int32` on 32-bit systems, where it previously would have been `int64` even on 32-bit systems. Instantiating <span class="title-ref">Index</span> using a list of numbers will still return 64bit dtypes, e.g. `Index([1, 2, 3])` will have a `int64` dtype, which is the same as previously.

2.  The various numeric datetime attributes of <span class="title-ref">DatetimeIndex</span> (<span class="title-ref">\~DatetimeIndex.day</span>, <span class="title-ref">\~DatetimeIndex.month</span>, <span class="title-ref">\~DatetimeIndex.year</span> etc.) were previously in of dtype `int64`, while they were `int32` for <span class="title-ref">arrays.DatetimeArray</span>. They are now `int32` on <span class="title-ref">DatetimeIndex</span> also:
    
    <div class="ipython">
    
    python
    
    idx = pd.date\_range(start='1/1/2018', periods=3, freq='ME') idx.array.year idx.year
    
    </div>

3.  Level dtypes on Indexes from <span class="title-ref">Series.sparse.from\_coo</span> are now of dtype `int32`, the same as they are on the `rows`/`cols` on a scipy sparse matrix. Previously they were of dtype `int64`.
    
    <div class="ipython">
    
    python
    
    from scipy import sparse A = sparse.coo\_matrix( (\[3.0, 1.0, 2.0\], (\[1, 0, 0\], \[0, 2, 3\])), shape=(3, 4) ) ser = pd.Series.sparse.from\_coo(A) ser.index.dtypes
    
    </div>

4.  <span class="title-ref">Index</span> cannot be instantiated using a float16 dtype. Previously instantiating an <span class="title-ref">Index</span> using dtype `float16` resulted in a <span class="title-ref">Float64Index</span> with a `float64` dtype. It now raises a `NotImplementedError`:
    
    <div class="ipython" data-okexcept="">
    
    python
    
    pd.Index(\[1, 2, 3\], dtype=np.float16)
    
    </div>

### Argument `dtype_backend`, to return pyarrow-backed or numpy-backed nullable dtypes

The following functions gained a new keyword `dtype_backend` (`36712`)

  - <span class="title-ref">read\_csv</span>
  - <span class="title-ref">read\_clipboard</span>
  - <span class="title-ref">read\_fwf</span>
  - <span class="title-ref">read\_excel</span>
  - <span class="title-ref">read\_html</span>
  - <span class="title-ref">read\_xml</span>
  - <span class="title-ref">read\_json</span>
  - <span class="title-ref">read\_sql</span>
  - <span class="title-ref">read\_sql\_query</span>
  - <span class="title-ref">read\_sql\_table</span>
  - <span class="title-ref">read\_parquet</span>
  - <span class="title-ref">read\_orc</span>
  - <span class="title-ref">read\_feather</span>
  - <span class="title-ref">read\_spss</span>
  - <span class="title-ref">to\_numeric</span>
  - <span class="title-ref">DataFrame.convert\_dtypes</span>
  - <span class="title-ref">Series.convert\_dtypes</span>

When this option is set to `"numpy_nullable"` it will return a <span class="title-ref">DataFrame</span> that is backed by nullable dtypes.

When this keyword is set to `"pyarrow"`, then these functions will return pyarrow-backed nullable <span class="title-ref">ArrowDtype</span> DataFrames (`48957`, `49997`):

  - <span class="title-ref">read\_csv</span>
  - <span class="title-ref">read\_clipboard</span>
  - <span class="title-ref">read\_fwf</span>
  - <span class="title-ref">read\_excel</span>
  - <span class="title-ref">read\_html</span>
  - <span class="title-ref">read\_xml</span>
  - <span class="title-ref">read\_json</span>
  - <span class="title-ref">read\_sql</span>
  - <span class="title-ref">read\_sql\_query</span>
  - <span class="title-ref">read\_sql\_table</span>
  - <span class="title-ref">read\_parquet</span>
  - <span class="title-ref">read\_orc</span>
  - <span class="title-ref">read\_feather</span>
  - <span class="title-ref">read\_spss</span>
  - <span class="title-ref">to\_numeric</span>
  - <span class="title-ref">DataFrame.convert\_dtypes</span>
  - <span class="title-ref">Series.convert\_dtypes</span>

<div class="ipython">

python

import io data = io.StringIO("""a,b,c,d,e,f,g,h,i 1,2.5,True,a,,,,, 3,4.5,False,b,6,7.5,True,a, """) df = pd.read\_csv(data, dtype\_backend="pyarrow") df.dtypes

data.seek(0) df\_pyarrow = pd.read\_csv(data, dtype\_backend="pyarrow", engine="pyarrow") df\_pyarrow.dtypes

</div>

### Copy-on-Write improvements

  - A new lazy copy mechanism that defers the copy until the object in question is modified was added to the methods listed in \[Copy-on-Write optimizations \<copy\_on\_write.optimizations\>\](\#copy-on-write-optimizations-\<copy\_on\_write.optimizations\>). These methods return views when Copy-on-Write is enabled, which provides a significant performance improvement compared to the regular execution (`49473`).

  - Accessing a single column of a DataFrame as a Series (e.g. `df["col"]`) now always returns a new object every time it is constructed when Copy-on-Write is enabled (not returning multiple times an identical, cached Series object). This ensures that those Series objects correctly follow the Copy-on-Write rules (`49450`)

  - The <span class="title-ref">Series</span> constructor will now create a lazy copy (deferring the copy until a modification to the data happens) when constructing a Series from an existing Series with the default of `copy=False` (`50471`)

  - The <span class="title-ref">DataFrame</span> constructor will now create a lazy copy (deferring the copy until a modification to the data happens) when constructing from an existing <span class="title-ref">DataFrame</span> with the default of `copy=False` (`51239`)

  - The <span class="title-ref">DataFrame</span> constructor, when constructing a DataFrame from a dictionary of Series objects and specifying `copy=False`, will now use a lazy copy of those Series objects for the columns of the DataFrame (`50777`)

  - The <span class="title-ref">DataFrame</span> constructor, when constructing a DataFrame from a <span class="title-ref">Series</span> or <span class="title-ref">Index</span> and specifying `copy=False`, will now respect Copy-on-Write.

  - The <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> constructors, when constructing from a NumPy array, will now copy the array by default to avoid mutating the <span class="title-ref">DataFrame</span> / <span class="title-ref">Series</span> when mutating the array. Specify `copy=False` to get the old behavior. When setting `copy=False` pandas does not guarantee correct Copy-on-Write behavior when the NumPy array is modified after creation of the <span class="title-ref">DataFrame</span> / <span class="title-ref">Series</span>.

  - The <span class="title-ref">DataFrame.from\_records</span> will now respect Copy-on-Write when called with a <span class="title-ref">DataFrame</span>.

  - Trying to set values using chained assignment (for example, `df["a"][1:3] = 0`) will now always raise a warning when Copy-on-Write is enabled. In this mode, chained assignment can never work because we are always setting into a temporary object that is the result of an indexing operation (getitem), which under Copy-on-Write always behaves as a copy. Thus, assigning through a chain can never update the original Series or DataFrame. Therefore, an informative warning is raised to the user to avoid silently doing nothing (`49467`)

  - <span class="title-ref">DataFrame.replace</span> will now respect the Copy-on-Write mechanism when `inplace=True`.

  - <span class="title-ref">DataFrame.transpose</span> will now respect the Copy-on-Write mechanism.

  - Arithmetic operations that can be inplace, e.g. `ser *= 2` will now respect the Copy-on-Write mechanism.

  - <span class="title-ref">DataFrame.\_\_getitem\_\_</span> will now respect the Copy-on-Write mechanism when the <span class="title-ref">DataFrame</span> has <span class="title-ref">MultiIndex</span> columns.

  -   - <span class="title-ref">Series.\_\_getitem\_\_</span> will now respect the Copy-on-Write mechanism when the  
        <span class="title-ref">Series</span> has a <span class="title-ref">MultiIndex</span>.

  - <span class="title-ref">Series.view</span> will now respect the Copy-on-Write mechanism.

Copy-on-Write can be enabled through one of

`` `python     pd.set_option("mode.copy_on_write", True)   .. code-block:: python      pd.options.mode.copy_on_write = True  Alternatively, copy on write can be enabled locally through:  .. code-block:: python      with pd.option_context("mode.copy_on_write", True):         ...  .. _whatsnew_200.enhancements.other:  Other enhancements ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^ - Added support for </span><span class="title-ref">str</span><span class="title-ref"> accessor methods when using \`ArrowDtype</span> with a `pyarrow.string` type (`50325`) - Added support for `dt` accessor methods when using <span class="title-ref">ArrowDtype</span> with a `pyarrow.timestamp` type (`50954`) - <span class="title-ref">read\_sas</span> now supports using `encoding='infer'` to correctly read and use the encoding specified by the sas file. (`48048`) - <span class="title-ref">.DataFrameGroupBy.quantile</span>, <span class="title-ref">.SeriesGroupBy.quantile</span> and <span class="title-ref">.DataFrameGroupBy.std</span> now preserve nullable dtypes instead of casting to numpy dtypes (`37493`) - <span class="title-ref">.DataFrameGroupBy.std</span>, <span class="title-ref">.SeriesGroupBy.std</span> now support datetime64, timedelta64, and <span class="title-ref">DatetimeTZDtype</span> dtypes (`48481`) - <span class="title-ref">Series.add\_suffix</span>, <span class="title-ref">DataFrame.add\_suffix</span>, <span class="title-ref">Series.add\_prefix</span> and <span class="title-ref">DataFrame.add\_prefix</span> support an `axis` argument. If `axis` is set, the default behaviour of which axis to consider can be overwritten (`47819`) - <span class="title-ref">.testing.assert\_frame\_equal</span> now shows the first element where the DataFrames differ, analogously to `pytest`'s output (`47910`) - Added `index` parameter to <span class="title-ref">DataFrame.to\_dict</span> (`46398`) - Added support for extension array dtypes in <span class="title-ref">merge</span> (`44240`) - Added metadata propagation for binary operators on <span class="title-ref">DataFrame</span> (`28283`) - Added `cumsum`, `cumprod`, `cummin` and `cummax` to the `ExtensionArray` interface via `_accumulate` (`28385`) - <span class="title-ref">.CategoricalConversionWarning</span>, <span class="title-ref">.InvalidComparison</span>, <span class="title-ref">.InvalidVersion</span>, <span class="title-ref">.LossySetitemError</span>, and <span class="title-ref">.NoBufferPresent</span> are now exposed in `pandas.errors` (`27656`) - Fix `test` optional\_extra by adding missing test package `pytest-asyncio` (`48361`) - <span class="title-ref">DataFrame.astype</span> exception message thrown improved to include column name when type conversion is not possible. (`47571`) - <span class="title-ref">date\_range</span> now supports a `unit` keyword ("s", "ms", "us", or "ns") to specify the desired resolution of the output index (`49106`) - <span class="title-ref">timedelta\_range</span> now supports a `unit` keyword ("s", "ms", "us", or "ns") to specify the desired resolution of the output index (`49824`) - <span class="title-ref">DataFrame.to\_json</span> now supports a `mode` keyword with supported inputs 'w' and 'a'. Defaulting to 'w', 'a' can be used when lines=True and orient='records' to append record oriented json lines to an existing json file. (`35849`) - Added `name` parameter to <span class="title-ref">IntervalIndex.from\_breaks</span>, <span class="title-ref">IntervalIndex.from\_arrays</span> and <span class="title-ref">IntervalIndex.from\_tuples</span> (`48911`) - Improve exception message when using <span class="title-ref">.testing.assert\_frame\_equal</span> on a <span class="title-ref">DataFrame</span> to include the column that is compared (`50323`) - Improved error message for <span class="title-ref">merge\_asof</span> when join-columns were duplicated (`50102`) - Added support for extension array dtypes to <span class="title-ref">get\_dummies</span> (`32430`) - Added <span class="title-ref">Index.infer\_objects</span> analogous to <span class="title-ref">Series.infer\_objects</span> (`50034`) - Added `copy` parameter to <span class="title-ref">Series.infer\_objects</span> and <span class="title-ref">DataFrame.infer\_objects</span>, passing `False` will avoid making copies for series or columns that are already non-object or where no better dtype can be inferred (`50096`) - <span class="title-ref">DataFrame.plot.hist</span> now recognizes `xlabel` and `ylabel` arguments (`49793`) - <span class="title-ref">Series.drop\_duplicates</span> has gained `ignore_index` keyword to reset index (`48304`) - <span class="title-ref">Series.dropna</span> and <span class="title-ref">DataFrame.dropna</span> has gained `ignore_index` keyword to reset index (`31725`) - Improved error message in <span class="title-ref">to\_datetime</span> for non-ISO8601 formats, informing users about the position of the first error (`50361`) - Improved error message when trying to align <span class="title-ref">DataFrame</span> objects (for example, in <span class="title-ref">DataFrame.compare</span>) to clarify that "identically labelled" refers to both index and columns (`50083`) - Added support for <span class="title-ref">Index.min</span> and <span class="title-ref">Index.max</span> for pyarrow string dtypes (`51397`) - Added <span class="title-ref">DatetimeIndex.as\_unit</span> and <span class="title-ref">TimedeltaIndex.as\_unit</span> to convert to different resolutions; supported resolutions are "s", "ms", "us", and "ns" (`50616`) - Added <span class="title-ref">Series.dt.unit</span> and <span class="title-ref">Series.dt.as\_unit</span> to convert to different resolutions; supported resolutions are "s", "ms", "us", and "ns" (`51223`) - Added new argument `dtype` to <span class="title-ref">read\_sql</span> to be consistent with <span class="title-ref">read\_sql\_query</span> (`50797`) - <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_table</span>, <span class="title-ref">read\_fwf</span> and <span class="title-ref">read\_excel</span> now accept `date_format` (`50601`) - <span class="title-ref">to\_datetime</span> now accepts `"ISO8601"` as an argument to `format`, which will match any ISO8601 string (but possibly not identically-formatted) (`50411`) - <span class="title-ref">to\_datetime</span> now accepts `"mixed"` as an argument to `format`, which will infer the format for each element individually (`50972`) - Added new argument `engine` to <span class="title-ref">read\_json</span> to support parsing JSON with pyarrow by specifying `engine="pyarrow"` (`48893`) - Added support for SQLAlchemy 2.0 (`40686`) - Added support for `decimal` parameter when `engine="pyarrow"` in <span class="title-ref">read\_csv</span> (`51302`) - <span class="title-ref">Index</span> set operations <span class="title-ref">Index.union</span>, <span class="title-ref">Index.intersection</span>, <span class="title-ref">Index.difference</span>, and <span class="title-ref">Index.symmetric\_difference</span> now support `sort=True`, which will always return a sorted result, unlike the default `sort=None` which does not sort in some cases (`25151`) - Added new escape mode "latex-math" to avoid escaping "$" in formatter (`50040`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### <span class="title-ref">.DataFrameGroupBy.cumsum</span> and <span class="title-ref">.DataFrameGroupBy.cumprod</span> overflow instead of lossy casting to float

In previous versions we cast to float when applying `cumsum` and `cumprod` which lead to incorrect results even if the result could be hold by `int64` dtype. Additionally, the aggregation overflows consistent with numpy and the regular <span class="title-ref">DataFrame.cumprod</span> and <span class="title-ref">DataFrame.cumsum</span> methods when the limit of `int64` is reached (`37493`).

*Old Behavior*

`` `ipython     In [1]: df = pd.DataFrame({"key": ["b"] * 7, "value": 625})     In [2]: df.groupby("key")["value"].cumprod()[5]     Out[2]: 5.960464477539062e+16  We return incorrect results with the 6th value.  *New Behavior*  .. ipython:: python      df = pd.DataFrame({"key": ["b"] * 7, "value": 625})     df.groupby("key")["value"].cumprod()  We overflow with the 7th value, but the 6th value is still correct.  .. _whatsnew_200.notable_bug_fixes.groupby_nth_filter:  `.DataFrameGroupBy.nth` and `.SeriesGroupBy.nth` now behave as filtrations ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions of pandas, <span class="title-ref">.DataFrameGroupBy.nth</span> and <span class="title-ref">.SeriesGroupBy.nth</span> acted as if they were aggregations. However, for most inputs `n`, they may return either zero or multiple rows per group. This means that they are filtrations, similar to e.g. <span class="title-ref">.DataFrameGroupBy.head</span>. pandas now treats them as filtrations (`13666`).

<div class="ipython">

python

df = pd.DataFrame({"a": \[1, 1, 2, 1, 2\], "b": \[np.nan, 2.0, 3.0, 4.0, 5.0\]}) gb = df.groupby("a")

</div>

*Old Behavior*

`` `ipython     In [5]: gb.nth(n=1)     Out[5]:        A    B     1  1  2.0     4  2  5.0  *New Behavior*  .. ipython:: python      gb.nth(n=1)  In particular, the index of the result is derived from the input by selecting ``<span class="title-ref"> the appropriate rows. Also, when </span><span class="title-ref">n</span><span class="title-ref"> is larger than the group, no rows instead of </span><span class="title-ref">NaN</span>\` is returned.

*Old Behavior*

`` `ipython     In [5]: gb.nth(n=3, dropna="any")     Out[5]:         B     A     1 NaN     2 NaN  *New Behavior*  .. ipython:: python      gb.nth(n=3, dropna="any")  .. --------------------------------------------------------------------------- ``\` .. \_whatsnew\_200.api\_breaking:

## Backwards incompatible API changes

### Construction with datetime64 or timedelta64 dtype with unsupported resolution

In past versions, when constructing a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> and passing a "datetime64" or "timedelta64" dtype with unsupported resolution (i.e. anything other than "ns"), pandas would silently replace the given dtype with its nanosecond analogue:

*Previous behavior*:

`` `ipython    In [5]: pd.Series(["2016-01-01"], dtype="datetime64[s]")    Out[5]:    0   2016-01-01    dtype: datetime64[ns]     In [6] pd.Series(["2016-01-01"], dtype="datetime64[D]")    Out[6]:    0   2016-01-01    dtype: datetime64[ns]  In pandas 2.0 we support resolutions "s", "ms", "us", and "ns". When passing ``\` a supported dtype (e.g. "datetime64\[s\]"), the result now has exactly the requested dtype:

*New behavior*:

<div class="ipython">

python

pd.Series(\["2016-01-01"\], dtype="datetime64\[s\]")

</div>

With an un-supported dtype, pandas now raises instead of silently swapping in a supported dtype:

*New behavior*:

<div class="ipython" data-okexcept="">

python

pd.Series(\["2016-01-01"\], dtype="datetime64\[D\]")

</div>

### Value counts sets the resulting name to `count`

In past versions, when running <span class="title-ref">Series.value\_counts</span>, the result would inherit the original object's name, and the result index would be nameless. This would cause confusion when resetting the index, and the column names would not correspond with the column values. Now, the result name will be `'count'` (or `'proportion'` if `normalize=True` was passed), and the index will be named after the original object (`49497`).

*Previous behavior*:

`` `ipython     In [8]: pd.Series(['quetzal', 'quetzal', 'elk'], name='animal').value_counts()      Out[2]:     quetzal    2     elk        1     Name: animal, dtype: int64  *New behavior*:  .. ipython:: python      pd.Series(['quetzal', 'quetzal', 'elk'], name='animal').value_counts()  Likewise for other ``value\_counts``methods (for example, `DataFrame.value_counts`).  .. _whatsnew_200.api_breaking.astype_to_unsupported_datetimelike:  Disallow astype conversion to non-supported datetime64/timedelta64 dtypes``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ In previous versions, converting a \`Series</span> or <span class="title-ref">DataFrame</span> from `datetime64[ns]` to a different `datetime64[X]` dtype would return with `datetime64[ns]` dtype instead of the requested dtype. In pandas 2.0, support is added for "datetime64\[s\]", "datetime64\[ms\]", and "datetime64\[us\]" dtypes, so converting to those dtypes gives exactly the requested dtype:

*Previous behavior*:

<div class="ipython">

python

idx = pd.date\_range("2016-01-01", periods=3) ser = pd.Series(idx)

</div>

*Previous behavior*:

`` `ipython    In [4]: ser.astype("datetime64[s]")    Out[4]:    0   2016-01-01    1   2016-01-02    2   2016-01-03    dtype: datetime64[ns]  With the new behavior, we get exactly the requested dtype:  *New behavior*:  .. ipython:: python     ser.astype("datetime64[s]")  For non-supported resolutions e.g. "datetime64[D]", we raise instead of silently ``\` ignoring the requested dtype:

*New behavior*:

<div class="ipython" data-okexcept="">

python

ser.astype("datetime64\[D\]")

</div>

For conversion from `timedelta64[ns]` dtypes, the old behavior converted to a floating point format.

*Previous behavior*:

<div class="ipython">

python

idx = pd.timedelta\_range("1 Day", periods=3) ser = pd.Series(idx)

</div>

*Previous behavior*:

`` `ipython    In [7]: ser.astype("timedelta64[s]")    Out[7]:    0     86400.0    1    172800.0    2    259200.0    dtype: float64     In [8]: ser.astype("timedelta64[D]")    Out[8]:    0    1.0    1    2.0    2    3.0    dtype: float64  The new behavior, as for datetime64, either gives exactly the requested dtype or raises:  *New behavior*:  .. ipython:: python    :okexcept:     ser.astype("timedelta64[s]")    ser.astype("timedelta64[D]")  .. _whatsnew_200.api_breaking.default_to_stdlib_tzinfos:  UTC and fixed-offset timezones default to standard-library tzinfo objects ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ In previous versions, the default </span><span class="title-ref">tzinfo</span><span class="title-ref"> object used to represent UTC was </span><span class="title-ref">pytz.UTC</span><span class="title-ref">. In pandas 2.0, we default to </span><span class="title-ref">datetime.timezone.utc</span><span class="title-ref"> instead. Similarly, for timezones represent fixed UTC offsets, we use </span><span class="title-ref">datetime.timezone</span><span class="title-ref"> objects instead of </span><span class="title-ref">pytz.FixedOffset</span><span class="title-ref"> objects. See (:issue:\`34916</span>)

*Previous behavior*:

`` `ipython    In [2]: ts = pd.Timestamp("2016-01-01", tz="UTC")    In [3]: type(ts.tzinfo)    Out[3]: pytz.UTC     In [4]: ts2 = pd.Timestamp("2016-01-01 04:05:06-07:00")    In [3]: type(ts2.tzinfo)    Out[5]: pytz._FixedOffset  *New behavior*:  .. ipython:: python     ts = pd.Timestamp("2016-01-01", tz="UTC")    type(ts.tzinfo)     ts2 = pd.Timestamp("2016-01-01 04:05:06-07:00")    type(ts2.tzinfo)  For timezones that are neither UTC nor fixed offsets, e.g. "US/Pacific", we ``<span class="title-ref"> continue to default to </span><span class="title-ref">pytz</span>\` objects.

### Empty DataFrames/Series will now default to have a `RangeIndex`

Before, constructing an empty (where `data` is `None` or an empty list-like argument) <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> without specifying the axes (`index=None`, `columns=None`) would return the axes as empty <span class="title-ref">Index</span> with object dtype.

Now, the axes return an empty <span class="title-ref">RangeIndex</span> (`49572`).

*Previous behavior*:

`` `ipython    In [8]: pd.Series().index    Out[8]:    Index([], dtype='object')     In [9] pd.DataFrame().axes    Out[9]:    [Index([], dtype='object'), Index([], dtype='object')]  *New behavior*:  .. ipython:: python     pd.Series().index    pd.DataFrame().axes  .. _whatsnew_200.api_breaking.to_latex:  DataFrame to LaTeX has a new render engine ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The existing <span class="title-ref">DataFrame.to\_latex</span> has been restructured to utilise the extended implementation previously available under <span class="title-ref">.Styler.to\_latex</span>. The arguments signature is similar, albeit `col_space` has been removed since it is ignored by LaTeX engines. This render engine also requires `jinja2` as a dependency which needs to be installed, since rendering is based upon jinja2 templates.

The pandas latex options below are no longer used and have been removed. The generic max rows and columns arguments remain but for this functionality should be replaced by the Styler equivalents. The alternative options giving similar functionality are indicated below:

  - `display.latex.escape`: replaced with `styler.format.escape`,
  - `display.latex.longtable`: replaced with `styler.latex.environment`,
  - `display.latex.multicolumn`, `display.latex.multicolumn_format` and `display.latex.multirow`: replaced with `styler.sparse.rows`, `styler.sparse.columns`, `styler.latex.multirow_align` and `styler.latex.multicol_align`,
  - `display.latex.repr`: replaced with `styler.render.repr`,
  - `display.max_rows` and `display.max_columns`: replace with `styler.render.max_rows`, `styler.render.max_columns` and `styler.render.max_elements`.

Note that due to this change some defaults have also changed:

  - `multirow` now defaults to *True*.
  - `multirow_align` defaults to *"r"* instead of *"l"*.
  - `multicol_align` defaults to *"r"* instead of *"l"*.
  - `escape` now defaults to *False*.

Note that the behaviour of `_repr_latex_` is also changed. Previously setting `display.latex.repr` would generate LaTeX only when using nbconvert for a JupyterNotebook, and not when the user is running the notebook. Now the `styler.render.repr` option allows control of the specific output within JupyterNotebooks for operations (not just on nbconvert). See `39911`.

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated. If installed, we now require:

<table style="width:83%;">
<colgroup>
<col style="width: 27%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mypy (dev)</td>
<td>1.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytest (dev)</td>
<td>7.0.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pytest-xdist (dev)</td>
<td>2.2.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>hypothesis (dev)</td>
<td>6.34.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>python-dateutil</td>
<td>2.8.2</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>tzdata</td>
<td>2022.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

<table style="width:64%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pyarrow</td>
<td>7.0.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>matplotlib</td>
<td>3.6.1</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>fastparquet</td>
<td>0.6.3</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xarray</td>
<td>0.21.0</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Datetimes are now parsed with a consistent format

In the past, <span class="title-ref">to\_datetime</span> guessed the format for each element independently. This was appropriate for some cases where elements had mixed date formats - however, it would regularly cause problems when users expected a consistent format but the function would switch formats between elements. As of version 2.0.0, parsing will use a consistent format, determined by the first non-NA value (unless the user specifies a format, in which case that is used).

*Old behavior*:

`` `ipython    In [1]: ser = pd.Series(['13-01-2000', '12-01-2000'])    In [2]: pd.to_datetime(ser)    Out[2]:    0   2000-01-13    1   2000-12-01    dtype: datetime64[ns]  *New behavior*:  .. ipython:: python     :okwarning:       ser = pd.Series(['13-01-2000', '12-01-2000'])      pd.to_datetime(ser)  Note that this affects `read_csv` as well.  If you still need to parse dates with inconsistent formats, you can use ``<span class="title-ref"> </span><span class="title-ref">format='mixed'</span><span class="title-ref"> (possibly alongside </span><span class="title-ref">dayfirst</span>\`) :

    ser = pd.Series(['13-01-2000', '12 January 2000'])
    pd.to_datetime(ser, format='mixed', dayfirst=True)

or, if your formats are all ISO8601 (but possibly not identically-formatted) :

    ser = pd.Series(['2020-01-01', '2020-01-01 03:00'])
    pd.to_datetime(ser, format='ISO8601')

### Other API changes

  - The `tz`, `nanosecond`, and `unit` keywords in the <span class="title-ref">Timestamp</span> constructor are now keyword-only (`45307`, `32526`)
  - Passing `nanoseconds` greater than 999 or less than 0 in <span class="title-ref">Timestamp</span> now raises a `ValueError` (`48538`, `48255`)
  - \`read\_csv\`: specifying an incorrect number of columns with `index_col` of now raises `ParserError` instead of `IndexError` when using the c parser.
  - Default value of `dtype` in <span class="title-ref">get\_dummies</span> is changed to `bool` from `uint8` (`45848`)
  - <span class="title-ref">DataFrame.astype</span>, <span class="title-ref">Series.astype</span>, and <span class="title-ref">DatetimeIndex.astype</span> casting datetime64 data to any of "datetime64\[s\]", "datetime64\[ms\]", "datetime64\[us\]" will return an object with the given resolution instead of coercing back to "datetime64\[ns\]" (`48928`)
  - <span class="title-ref">DataFrame.astype</span>, <span class="title-ref">Series.astype</span>, and <span class="title-ref">DatetimeIndex.astype</span> casting timedelta64 data to any of "timedelta64\[s\]", "timedelta64\[ms\]", "timedelta64\[us\]" will return an object with the given resolution instead of coercing to "float64" dtype (`48963`)
  - <span class="title-ref">DatetimeIndex.astype</span>, <span class="title-ref">TimedeltaIndex.astype</span>, <span class="title-ref">PeriodIndex.astype</span> <span class="title-ref">Series.astype</span>, <span class="title-ref">DataFrame.astype</span> with `datetime64`, `timedelta64` or <span class="title-ref">PeriodDtype</span> dtypes no longer allow converting to integer dtypes other than "int64", do `obj.astype('int64', copy=False).astype(dtype)` instead (`49715`)
  - <span class="title-ref">Index.astype</span> now allows casting from `float64` dtype to datetime-like dtypes, matching <span class="title-ref">Series</span> behavior (`49660`)
  - Passing data with dtype of "timedelta64\[s\]", "timedelta64\[ms\]", or "timedelta64\[us\]" to <span class="title-ref">TimedeltaIndex</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> constructors will now retain that dtype instead of casting to "timedelta64\[ns\]"; timedelta64 data with lower resolution will be cast to the lowest supported resolution "timedelta64\[s\]" (`49014`)
  - Passing `dtype` of "timedelta64\[s\]", "timedelta64\[ms\]", or "timedelta64\[us\]" to <span class="title-ref">TimedeltaIndex</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> constructors will now retain that dtype instead of casting to "timedelta64\[ns\]"; passing a dtype with lower resolution for <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> will be cast to the lowest supported resolution "timedelta64\[s\]" (`49014`)
  - Passing a `np.datetime64` object with non-nanosecond resolution to <span class="title-ref">Timestamp</span> will retain the input resolution if it is "s", "ms", "us", or "ns"; otherwise it will be cast to the closest supported resolution (`49008`)
  - Passing `datetime64` values with resolution other than nanosecond to <span class="title-ref">to\_datetime</span> will retain the input resolution if it is "s", "ms", "us", or "ns"; otherwise it will be cast to the closest supported resolution (`50369`)
  - Passing integer values and a non-nanosecond datetime64 dtype (e.g. "datetime64\[s\]") <span class="title-ref">DataFrame</span>, <span class="title-ref">Series</span>, or <span class="title-ref">Index</span> will treat the values as multiples of the dtype's unit, matching the behavior of e.g. `Series(np.array(values, dtype="M8[s]"))` (`51092`)
  - Passing a string in ISO-8601 format to <span class="title-ref">Timestamp</span> will retain the resolution of the parsed input if it is "s", "ms", "us", or "ns"; otherwise it will be cast to the closest supported resolution (`49737`)
  - The `other` argument in <span class="title-ref">DataFrame.mask</span> and <span class="title-ref">Series.mask</span> now defaults to `no_default` instead of `np.nan` consistent with <span class="title-ref">DataFrame.where</span> and <span class="title-ref">Series.where</span>. Entries will be filled with the corresponding NULL value (`np.nan` for numpy dtypes, `pd.NA` for extension dtypes). (`49111`)
  - Changed behavior of <span class="title-ref">Series.quantile</span> and <span class="title-ref">DataFrame.quantile</span> with <span class="title-ref">SparseDtype</span> to retain sparse dtype (`49583`)
  - When creating a <span class="title-ref">Series</span> with a object-dtype <span class="title-ref">Index</span> of datetime objects, pandas no longer silently converts the index to a <span class="title-ref">DatetimeIndex</span> (`39307`, `23598`)
  - <span class="title-ref">pandas.testing.assert\_index\_equal</span> with parameter `exact="equiv"` now considers two indexes equal when both are either a <span class="title-ref">RangeIndex</span> or <span class="title-ref">Index</span> with an `int64` dtype. Previously it meant either a <span class="title-ref">RangeIndex</span> or a <span class="title-ref">Int64Index</span> (`51098`)
  - <span class="title-ref">Series.unique</span> with dtype "timedelta64\[ns\]" or "datetime64\[ns\]" now returns <span class="title-ref">TimedeltaArray</span> or <span class="title-ref">DatetimeArray</span> instead of `numpy.ndarray` (`49176`)
  - <span class="title-ref">to\_datetime</span> and <span class="title-ref">DatetimeIndex</span> now allow sequences containing both `datetime` objects and numeric entries, matching <span class="title-ref">Series</span> behavior (`49037`, `50453`)
  - <span class="title-ref">pandas.api.types.is\_string\_dtype</span> now only returns `True` for array-likes with `dtype=object` when the elements are inferred to be strings (`15585`)
  - Passing a sequence containing `datetime` objects and `date` objects to <span class="title-ref">Series</span> constructor will return with `object` dtype instead of `datetime64[ns]` dtype, consistent with <span class="title-ref">Index</span> behavior (`49341`)
  - Passing strings that cannot be parsed as datetimes to <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> with `dtype="datetime64[ns]"` will raise instead of silently ignoring the keyword and returning `object` dtype (`24435`)
  - Passing a sequence containing a type that cannot be converted to <span class="title-ref">Timedelta</span> to <span class="title-ref">to\_timedelta</span> or to the <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> constructor with `dtype="timedelta64[ns]"` or to <span class="title-ref">TimedeltaIndex</span> now raises `TypeError` instead of `ValueError` (`49525`)
  - Changed behavior of <span class="title-ref">Index</span> constructor with sequence containing at least one `NaT` and everything else either `None` or `NaN` to infer `datetime64[ns]` dtype instead of `object`, matching <span class="title-ref">Series</span> behavior (`49340`)
  - <span class="title-ref">read\_stata</span> with parameter `index_col` set to `None` (the default) will now set the index on the returned <span class="title-ref">DataFrame</span> to a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Int64Index</span> (`49745`)
  - Changed behavior of <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, and <span class="title-ref">DataFrame</span> arithmetic methods when working with object-dtypes, the results no longer do type inference on the result of the array operations, use `result.infer_objects(copy=False)` to do type inference on the result (`49999`, `49714`)
  - Changed behavior of <span class="title-ref">Index</span> constructor with an object-dtype `numpy.ndarray` containing all-`bool` values or all-complex values, this will now retain object dtype, consistent with the <span class="title-ref">Series</span> behavior (`49594`)
  - Changed behavior of <span class="title-ref">Series.astype</span> from object-dtype containing `bytes` objects to string dtypes; this now does `val.decode()` on bytes objects instead of `str(val)`, matching <span class="title-ref">Index.astype</span> behavior (`45326`)
  - Added `"None"` to default `na_values` in <span class="title-ref">read\_csv</span> (`50286`)
  - Changed behavior of <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors when given an integer dtype and floating-point data that is not round numbers, this now raises `ValueError` instead of silently retaining the float dtype; do `Series(data)` or `DataFrame(data)` to get the old behavior, and `Series(data).astype(dtype)` or `DataFrame(data).astype(dtype)` to get the specified dtype (`49599`)
  - Changed behavior of <span class="title-ref">DataFrame.shift</span> with `axis=1`, an integer `fill_value`, and homogeneous datetime-like dtype, this now fills new columns with integer dtypes instead of casting to datetimelike (`49842`)
  - Files are now closed when encountering an exception in <span class="title-ref">read\_json</span> (`49921`)
  - Changed behavior of <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_json</span> & <span class="title-ref">read\_fwf</span>, where the index will now always be a <span class="title-ref">RangeIndex</span>, when no index is specified. Previously the index would be a <span class="title-ref">Index</span> with dtype `object` if the new DataFrame/Series has length 0 (`49572`)
  - <span class="title-ref">DataFrame.values</span>, <span class="title-ref">DataFrame.to\_numpy</span>, <span class="title-ref">DataFrame.xs</span>, <span class="title-ref">DataFrame.reindex</span>, <span class="title-ref">DataFrame.fillna</span>, and <span class="title-ref">DataFrame.replace</span> no longer silently consolidate the underlying arrays; do `df = df.copy()` to ensure consolidation (`49356`)
  - Creating a new DataFrame using a full slice on both axes with <span class="title-ref">\~DataFrame.loc</span> or <span class="title-ref">\~DataFrame.iloc</span> (thus, `df.loc[:, :]` or `df.iloc[:, :]`) now returns a new DataFrame (shallow copy) instead of the original DataFrame, consistent with other methods to get a full slice (for example `df.loc[:]` or `df[:]`) (`49469`)
  - The <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors will now return a shallow copy (i.e. share data, but not attributes) when passed a Series and DataFrame, respectively, and with the default of `copy=False` (and if no other keyword triggers a copy). Previously, the new Series or DataFrame would share the index attribute (e.g. `df.index = ...` would also update the index of the parent or child) (`49523`)
  - Disallow computing `cumprod` for <span class="title-ref">Timedelta</span> object; previously this returned incorrect values (`50246`)
  - <span class="title-ref">DataFrame</span> objects read from a <span class="title-ref">HDFStore</span> file without an index now have a <span class="title-ref">RangeIndex</span> instead of an `int64` index (`51076`)
  - Instantiating an <span class="title-ref">Index</span> with an numeric numpy dtype with data containing <span class="title-ref">NA</span> and/or <span class="title-ref">NaT</span> now raises a `ValueError`. Previously a `TypeError` was raised (`51050`)
  - Loading a JSON file with duplicate columns using `read_json(orient='split')` renames columns to avoid duplicates, as <span class="title-ref">read\_csv</span> and the other readers do (`50370`)
  - The levels of the index of the <span class="title-ref">Series</span> returned from `Series.sparse.from_coo` now always have dtype `int32`. Previously they had dtype `int64` (`50926`)
  - <span class="title-ref">to\_datetime</span> with `unit` of either "Y" or "M" will now raise if a sequence contains a non-round `float` value, matching the `Timestamp` behavior (`50301`)
  - The methods <span class="title-ref">Series.round</span>, <span class="title-ref">DataFrame.\_\_invert\_\_</span>, <span class="title-ref">Series.\_\_invert\_\_</span>, <span class="title-ref">DataFrame.swapaxes</span>, <span class="title-ref">DataFrame.first</span>, <span class="title-ref">DataFrame.last</span>, <span class="title-ref">Series.first</span>, <span class="title-ref">Series.last</span> and <span class="title-ref">DataFrame.align</span> will now always return new objects (`51032`)
  - <span class="title-ref">DataFrame</span> and <span class="title-ref">DataFrameGroupBy</span> aggregations (e.g. "sum") with object-dtype columns no longer infer non-object dtypes for their results, explicitly call `result.infer_objects(copy=False)` on the result to obtain the old behavior (`51205`, `49603`)
  - Division by zero with <span class="title-ref">ArrowDtype</span> dtypes returns `-inf`, `nan`, or `inf` depending on the numerator, instead of raising (`51541`)
  - Added <span class="title-ref">pandas.api.types.is\_any\_real\_numeric\_dtype</span> to check for real numeric dtypes (`51152`)
  - <span class="title-ref">\~arrays.ArrowExtensionArray.value\_counts</span> now returns data with <span class="title-ref">ArrowDtype</span> with `pyarrow.int64` type instead of `"Int64"` type (`51462`)
  - <span class="title-ref">factorize</span> and <span class="title-ref">unique</span> preserve the original dtype when passed numpy timedelta64 or datetime64 with non-nanosecond resolution (`48670`)

\> **Note** \> A current PDEP proposes the deprecation and removal of the keywords `inplace` and `copy` for all but a small subset of methods from the pandas API. The current discussion takes place at [here](https://github.com/pandas-dev/pandas/pull/51466). The keywords won't be necessary anymore in the context of Copy-on-Write. If this proposal is accepted, both keywords would be deprecated in the next release of pandas and removed in pandas 3.0.

## Deprecations

  - Deprecated parsing datetime strings with system-local timezone to `tzlocal`, pass a `tz` keyword or explicitly call `tz_localize` instead (`50791`)
  - Deprecated argument `infer_datetime_format` in <span class="title-ref">to\_datetime</span> and <span class="title-ref">read\_csv</span>, as a strict version of it is now the default (`48621`)
  - Deprecated behavior of <span class="title-ref">to\_datetime</span> with `unit` when parsing strings, in a future version these will be parsed as datetimes (matching unit-less behavior) instead of cast to floats. To retain the old behavior, cast strings to numeric types before calling <span class="title-ref">to\_datetime</span> (`50735`)
  - Deprecated <span class="title-ref">pandas.io.sql.execute</span> (`50185`)
  - <span class="title-ref">Index.is\_boolean</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_bool\_dtype</span> instead (`50042`)
  - <span class="title-ref">Index.is\_integer</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_integer\_dtype</span> instead (`50042`)
  - <span class="title-ref">Index.is\_floating</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_float\_dtype</span> instead (`50042`)
  - <span class="title-ref">Index.holds\_integer</span> has been deprecated. Use <span class="title-ref">pandas.api.types.infer\_dtype</span> instead (`50243`)
  - <span class="title-ref">Index.is\_numeric</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_any\_real\_numeric\_dtype</span> instead (`50042`,`51152`)
  - <span class="title-ref">Index.is\_categorical</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_categorical\_dtype</span> instead (`50042`)
  - <span class="title-ref">Index.is\_object</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_object\_dtype</span> instead (`50042`)
  - <span class="title-ref">Index.is\_interval</span> has been deprecated. Use <span class="title-ref">pandas.api.types.is\_interval\_dtype</span> instead (`50042`)
  - Deprecated argument `date_parser` in <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_table</span>, <span class="title-ref">read\_fwf</span>, and <span class="title-ref">read\_excel</span> in favour of `date_format` (`50601`)
  - Deprecated `all` and `any` reductions with `datetime64` and <span class="title-ref">DatetimeTZDtype</span> dtypes, use e.g. `(obj != pd.Timestamp(0), tz=obj.tz).all()` instead (`34479`)
  - Deprecated unused arguments `*args` and `**kwargs` in <span class="title-ref">Resampler</span> (`50977`)
  - Deprecated calling `float` or `int` on a single element <span class="title-ref">Series</span> to return a `float` or `int` respectively. Extract the element before calling `float` or `int` instead (`51101`)
  - Deprecated <span class="title-ref">Grouper.groups</span>, use <span class="title-ref">Groupby.groups</span> instead (`51182`)
  - Deprecated <span class="title-ref">Grouper.grouper</span>, use <span class="title-ref">Groupby.grouper</span> instead (`51182`)
  - Deprecated <span class="title-ref">Grouper.obj</span>, use <span class="title-ref">Groupby.obj</span> instead (`51206`)
  - Deprecated <span class="title-ref">Grouper.indexer</span>, use <span class="title-ref">Resampler.indexer</span> instead (`51206`)
  - Deprecated <span class="title-ref">Grouper.ax</span>, use <span class="title-ref">Resampler.ax</span> instead (`51206`)
  - Deprecated keyword `use_nullable_dtypes` in <span class="title-ref">read\_parquet</span>, use `dtype_backend` instead (`51853`)
  - Deprecated <span class="title-ref">Series.pad</span> in favor of <span class="title-ref">Series.ffill</span> (`33396`)
  - Deprecated <span class="title-ref">Series.backfill</span> in favor of <span class="title-ref">Series.bfill</span> (`33396`)
  - Deprecated <span class="title-ref">DataFrame.pad</span> in favor of <span class="title-ref">DataFrame.ffill</span> (`33396`)
  - Deprecated <span class="title-ref">DataFrame.backfill</span> in favor of <span class="title-ref">DataFrame.bfill</span> (`33396`)
  - Deprecated <span class="title-ref">\~pandas.io.stata.StataReader.close</span>. Use <span class="title-ref">\~pandas.io.stata.StataReader</span> as a context manager instead (`49228`)
  - Deprecated producing a scalar when iterating over a <span class="title-ref">.DataFrameGroupBy</span> or a <span class="title-ref">.SeriesGroupBy</span> that has been grouped by a `level` parameter that is a list of length 1; a tuple of length one will be returned instead (`51583`)

## Removal of prior version deprecations/changes

  - Removed <span class="title-ref">Int64Index</span>, <span class="title-ref">UInt64Index</span> and <span class="title-ref">Float64Index</span>. See also \[here \<whatsnew\_200.enhancements.index\_can\_hold\_numpy\_numeric\_dtypes\>\](\#here-\<whatsnew\_200.enhancements.index\_can\_hold\_numpy\_numeric\_dtypes\>) for more information (`42717`)
  - Removed deprecated <span class="title-ref">Timestamp.freq</span>, <span class="title-ref">Timestamp.freqstr</span> and argument `freq` from the <span class="title-ref">Timestamp</span> constructor and <span class="title-ref">Timestamp.fromordinal</span> (`14146`)
  - Removed deprecated <span class="title-ref">CategoricalBlock</span>, <span class="title-ref">Block.is\_categorical</span>, require datetime64 and timedelta64 values to be wrapped in <span class="title-ref">DatetimeArray</span> or <span class="title-ref">TimedeltaArray</span> before passing to <span class="title-ref">Block.make\_block\_same\_class</span>, require `DatetimeTZBlock.values` to have the correct ndim when passing to the <span class="title-ref">BlockManager</span> constructor, and removed the "fastpath" keyword from the <span class="title-ref">SingleBlockManager</span> constructor (`40226`, `40571`)
  - Removed deprecated global option `use_inf_as_null` in favor of `use_inf_as_na` (`17126`)
  - Removed deprecated module `pandas.core.index` (`30193`)
  - Removed deprecated alias `pandas.core.tools.datetimes.to_time`, import the function directly from `pandas.core.tools.times` instead (`34145`)
  - Removed deprecated alias `pandas.io.json.json_normalize`, import the function directly from `pandas.json_normalize` instead (`27615`)
  - Removed deprecated <span class="title-ref">Categorical.to\_dense</span>, use `np.asarray(cat)` instead (`32639`)
  - Removed deprecated <span class="title-ref">Categorical.take\_nd</span> (`27745`)
  - Removed deprecated <span class="title-ref">Categorical.mode</span>, use `Series(cat).mode()` instead (`45033`)
  - Removed deprecated <span class="title-ref">Categorical.is\_dtype\_equal</span> and <span class="title-ref">CategoricalIndex.is\_dtype\_equal</span> (`37545`)
  - Removed deprecated <span class="title-ref">CategoricalIndex.take\_nd</span> (`30702`)
  - Removed deprecated <span class="title-ref">Index.is\_type\_compatible</span> (`42113`)
  - Removed deprecated <span class="title-ref">Index.is\_mixed</span>, check `index.inferred_type` directly instead (`32922`)
  - Removed deprecated <span class="title-ref">pandas.api.types.is\_categorical</span>; use <span class="title-ref">pandas.api.types.is\_categorical\_dtype</span> instead (`33385`)
  - Removed deprecated <span class="title-ref">Index.asi8</span> (`37877`)
  - Enforced deprecation changing behavior when passing `datetime64[ns]` dtype data and timezone-aware dtype to <span class="title-ref">Series</span>, interpreting the values as wall-times instead of UTC times, matching <span class="title-ref">DatetimeIndex</span> behavior (`41662`)
  - Enforced deprecation changing behavior when applying a numpy ufunc on multiple non-aligned (on the index or columns) <span class="title-ref">DataFrame</span> that will now align the inputs first (`39239`)
  - Removed deprecated <span class="title-ref">DataFrame.\_AXIS\_NUMBERS</span>, <span class="title-ref">DataFrame.\_AXIS\_NAMES</span>, <span class="title-ref">Series.\_AXIS\_NUMBERS</span>, <span class="title-ref">Series.\_AXIS\_NAMES</span> (`33637`)
  - Removed deprecated <span class="title-ref">Index.to\_native\_types</span>, use `obj.astype(str)` instead (`36418`)
  - Removed deprecated <span class="title-ref">Series.iteritems</span>, <span class="title-ref">DataFrame.iteritems</span>, use `obj.items` instead (`45321`)
  - Removed deprecated <span class="title-ref">DataFrame.lookup</span> (`35224`)
  - Removed deprecated <span class="title-ref">Series.append</span>, <span class="title-ref">DataFrame.append</span>, use <span class="title-ref">concat</span> instead (`35407`)
  - Removed deprecated <span class="title-ref">Series.iteritems</span>, <span class="title-ref">DataFrame.iteritems</span> and <span class="title-ref">HDFStore.iteritems</span> use `obj.items` instead (`45321`)
  - Removed deprecated <span class="title-ref">DatetimeIndex.union\_many</span> (`45018`)
  - Removed deprecated `weekofyear` and `week` attributes of <span class="title-ref">DatetimeArray</span>, <span class="title-ref">DatetimeIndex</span> and `dt` accessor in favor of `isocalendar().week` (`33595`)
  - Removed deprecated <span class="title-ref">RangeIndex.\_start</span>, <span class="title-ref">RangeIndex.\_stop</span>, <span class="title-ref">RangeIndex.\_step</span>, use `start`, `stop`, `step` instead (`30482`)
  - Removed deprecated <span class="title-ref">DatetimeIndex.to\_perioddelta</span>, Use `dtindex - dtindex.to_period(freq).to_timestamp()` instead (`34853`)
  - Removed deprecated <span class="title-ref">.Styler.hide\_index</span> and <span class="title-ref">.Styler.hide\_columns</span> (`49397`)
  - Removed deprecated <span class="title-ref">.Styler.set\_na\_rep</span> and <span class="title-ref">.Styler.set\_precision</span> (`49397`)
  - Removed deprecated <span class="title-ref">.Styler.where</span> (`49397`)
  - Removed deprecated <span class="title-ref">.Styler.render</span> (`49397`)
  - Removed deprecated argument `col_space` in <span class="title-ref">DataFrame.to\_latex</span> (`47970`)
  - Removed deprecated argument `null_color` in <span class="title-ref">.Styler.highlight\_null</span> (`49397`)
  - Removed deprecated argument `check_less_precise` in <span class="title-ref">.testing.assert\_frame\_equal</span>, <span class="title-ref">.testing.assert\_extension\_array\_equal</span>, <span class="title-ref">.testing.assert\_series\_equal</span>, <span class="title-ref">.testing.assert\_index\_equal</span> (`30562`)
  - Removed deprecated `null_counts` argument in <span class="title-ref">DataFrame.info</span>. Use `show_counts` instead (`37999`)
  - Removed deprecated <span class="title-ref">Index.is\_monotonic</span>, and <span class="title-ref">Series.is\_monotonic</span>; use `obj.is_monotonic_increasing` instead (`45422`)
  - Removed deprecated <span class="title-ref">Index.is\_all\_dates</span> (`36697`)
  - Enforced deprecation disallowing passing a timezone-aware <span class="title-ref">Timestamp</span> and `dtype="datetime64[ns]"` to <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> constructors (`41555`)
  - Enforced deprecation disallowing passing a sequence of timezone-aware values and `dtype="datetime64[ns]"` to to <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> constructors (`41555`)
  - Enforced deprecation disallowing `numpy.ma.mrecords.MaskedRecords` in the <span class="title-ref">DataFrame</span> constructor; pass `"{name: data[name] for name in data.dtype.names}` instead (`40363`)
  - Enforced deprecation disallowing unit-less "datetime64" dtype in <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> (`47844`)
  - Enforced deprecation disallowing using `.astype` to convert a `datetime64[ns]` <span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span>, or <span class="title-ref">DatetimeIndex</span> to timezone-aware dtype, use `obj.tz_localize` or `ser.dt.tz_localize` instead (`39258`)
  - Enforced deprecation disallowing using `.astype` to convert a timezone-aware <span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span>, or <span class="title-ref">DatetimeIndex</span> to timezone-naive `datetime64[ns]` dtype, use `obj.tz_localize(None)` or `obj.tz_convert("UTC").tz_localize(None)` instead (`39258`)
  - Enforced deprecation disallowing passing non boolean argument to sort in <span class="title-ref">concat</span> (`44629`)
  - Removed Date parser functions <span class="title-ref">\~pandas.io.date\_converters.parse\_date\_time</span>, <span class="title-ref">\~pandas.io.date\_converters.parse\_date\_fields</span>, <span class="title-ref">\~pandas.io.date\_converters.parse\_all\_fields</span> and <span class="title-ref">\~pandas.io.date\_converters.generic\_parser</span> (`24518`)
  - Removed argument `index` from the <span class="title-ref">core.arrays.SparseArray</span> constructor (`43523`)
  - Remove argument `squeeze` from <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> (`32380`)
  - Removed deprecated `apply`, `apply_index`, `__call__`, `onOffset`, and `isAnchored` attributes from <span class="title-ref">DateOffset</span> (`34171`)
  - Removed `keep_tz` argument in <span class="title-ref">DatetimeIndex.to\_series</span> (`29731`)
  - Remove arguments `names` and `dtype` from <span class="title-ref">Index.copy</span> and `levels` and `codes` from <span class="title-ref">MultiIndex.copy</span> (`35853`, `36685`)
  - Remove argument `inplace` from <span class="title-ref">MultiIndex.set\_levels</span> and <span class="title-ref">MultiIndex.set\_codes</span> (`35626`)
  - Removed arguments `verbose` and `encoding` from <span class="title-ref">DataFrame.to\_excel</span> and <span class="title-ref">Series.to\_excel</span> (`47912`)
  - Removed argument `line_terminator` from <span class="title-ref">DataFrame.to\_csv</span> and <span class="title-ref">Series.to\_csv</span>, use `lineterminator` instead (`45302`)
  - Removed argument `inplace` from <span class="title-ref">DataFrame.set\_axis</span> and <span class="title-ref">Series.set\_axis</span>, use `obj = obj.set_axis(..., copy=False)` instead (`48130`)
  - Disallow passing positional arguments to <span class="title-ref">MultiIndex.set\_levels</span> and <span class="title-ref">MultiIndex.set\_codes</span> (`41485`)
  - Disallow parsing to Timedelta strings with components with units "Y", "y", or "M", as these do not represent unambiguous durations (`36838`)
  - Removed <span class="title-ref">MultiIndex.is\_lexsorted</span> and <span class="title-ref">MultiIndex.lexsort\_depth</span> (`38701`)
  - Removed argument `how` from <span class="title-ref">PeriodIndex.astype</span>, use <span class="title-ref">PeriodIndex.to\_timestamp</span> instead (`37982`)
  - Removed argument `try_cast` from <span class="title-ref">DataFrame.mask</span>, <span class="title-ref">DataFrame.where</span>, <span class="title-ref">Series.mask</span> and <span class="title-ref">Series.where</span> (`38836`)
  - Removed argument `tz` from <span class="title-ref">Period.to\_timestamp</span>, use `obj.to_timestamp(...).tz_localize(tz)` instead (`34522`)
  - Removed argument `sort_columns` in <span class="title-ref">DataFrame.plot</span> and <span class="title-ref">Series.plot</span> (`47563`)
  - Removed argument `is_copy` from <span class="title-ref">DataFrame.take</span> and <span class="title-ref">Series.take</span> (`30615`)
  - Removed argument `kind` from <span class="title-ref">Index.get\_slice\_bound</span>, <span class="title-ref">Index.slice\_indexer</span> and <span class="title-ref">Index.slice\_locs</span> (`41378`)
  - Removed arguments `prefix`, `squeeze`, `error_bad_lines` and `warn_bad_lines` from <span class="title-ref">read\_csv</span> (`40413`, `43427`)
  - Removed arguments `squeeze` from <span class="title-ref">read\_excel</span> (`43427`)
  - Removed argument `datetime_is_numeric` from <span class="title-ref">DataFrame.describe</span> and <span class="title-ref">Series.describe</span> as datetime data will always be summarized as numeric data (`34798`)
  - Disallow passing list `key` to <span class="title-ref">Series.xs</span> and <span class="title-ref">DataFrame.xs</span>, pass a tuple instead (`41789`)
  - Disallow subclass-specific keywords (e.g. "freq", "tz", "names", "closed") in the <span class="title-ref">Index</span> constructor (`38597`)
  - Removed argument `inplace` from <span class="title-ref">Categorical.remove\_unused\_categories</span> (`37918`)
  - Disallow passing non-round floats to <span class="title-ref">Timestamp</span> with `unit="M"` or `unit="Y"` (`47266`)
  - Remove keywords `convert_float` and `mangle_dupe_cols` from <span class="title-ref">read\_excel</span> (`41176`)
  - Remove keyword `mangle_dupe_cols` from <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> (`48137`)
  - Removed `errors` keyword from <span class="title-ref">DataFrame.where</span>, <span class="title-ref">Series.where</span>, <span class="title-ref">DataFrame.mask</span> and <span class="title-ref">Series.mask</span> (`47728`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_excel</span> except `io` and `sheet_name` (`34418`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.drop</span> and <span class="title-ref">Series.drop</span> except `labels` (`41486`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.fillna</span> and <span class="title-ref">Series.fillna</span> except `value` (`41485`)
  - Disallow passing non-keyword arguments to <span class="title-ref">StringMethods.split</span> and <span class="title-ref">StringMethods.rsplit</span> except for `pat` (`47448`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.set\_index</span> except `keys` (`41495`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Resampler.interpolate</span> except `method` (`41699`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.reset\_index</span> and <span class="title-ref">Series.reset\_index</span> except `level` (`41496`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.dropna</span> and <span class="title-ref">Series.dropna</span> (`41504`)
  - Disallow passing non-keyword arguments to <span class="title-ref">ExtensionArray.argsort</span> (`46134`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Categorical.sort\_values</span> (`47618`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Index.drop\_duplicates</span> and <span class="title-ref">Series.drop\_duplicates</span> (`41485`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.drop\_duplicates</span> except for `subset` (`41485`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.sort\_index</span> and <span class="title-ref">Series.sort\_index</span> (`41506`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.interpolate</span> and <span class="title-ref">Series.interpolate</span> except for `method` (`41510`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.any</span> and <span class="title-ref">Series.any</span> (`44896`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Index.set\_names</span> except for `names` (`41551`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Index.join</span> except for `other` (`46518`)
  - Disallow passing non-keyword arguments to <span class="title-ref">concat</span> except for `objs` (`41485`)
  - Disallow passing non-keyword arguments to <span class="title-ref">pivot</span> except for `data` (`48301`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.pivot</span> (`48301`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_html</span> except for `io` (`27573`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_json</span> except for `path_or_buf` (`27573`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_sas</span> except for `filepath_or_buffer` (`47154`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_stata</span> except for `filepath_or_buffer` (`48128`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_csv</span> except `filepath_or_buffer` (`41485`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_table</span> except `filepath_or_buffer` (`41485`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_fwf</span> except `filepath_or_buffer` (`44710`)
  - Disallow passing non-keyword arguments to <span class="title-ref">read\_xml</span> except for `path_or_buffer` (`45133`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Series.mask</span> and <span class="title-ref">DataFrame.mask</span> except `cond` and `other` (`41580`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.to\_stata</span> except for `path` (`48128`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.where</span> and <span class="title-ref">Series.where</span> except for `cond` and `other` (`41523`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Series.set\_axis</span> and <span class="title-ref">DataFrame.set\_axis</span> except for `labels` (`41491`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Series.rename\_axis</span> and <span class="title-ref">DataFrame.rename\_axis</span> except for `mapper` (`47587`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Series.clip</span> and <span class="title-ref">DataFrame.clip</span> except `lower` and `upper` (`41511`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Series.bfill</span>, <span class="title-ref">Series.ffill</span>, <span class="title-ref">DataFrame.bfill</span> and <span class="title-ref">DataFrame.ffill</span> (`41508`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.replace</span>, <span class="title-ref">Series.replace</span> except for `to_replace` and `value` (`47587`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.sort\_values</span> except for `by` (`41505`)
  - Disallow passing non-keyword arguments to <span class="title-ref">Series.sort\_values</span> (`41505`)
  - Disallow passing non-keyword arguments to <span class="title-ref">DataFrame.reindex</span> except for `labels` (`17966`)
  - Disallow <span class="title-ref">Index.reindex</span> with non-unique <span class="title-ref">Index</span> objects (`42568`)
  - Disallowed constructing <span class="title-ref">Categorical</span> with scalar `data` (`38433`)
  - Disallowed constructing <span class="title-ref">CategoricalIndex</span> without passing `data` (`38944`)
  - Removed <span class="title-ref">.Rolling.validate</span>, <span class="title-ref">.Expanding.validate</span>, and <span class="title-ref">.ExponentialMovingWindow.validate</span> (`43665`)
  - Removed <span class="title-ref">Rolling.win\_type</span> returning `"freq"` (`38963`)
  - Removed <span class="title-ref">Rolling.is\_datetimelike</span> (`38963`)
  - Removed the `level` keyword in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> aggregations; use `groupby` instead (`39983`)
  - Removed deprecated <span class="title-ref">Timedelta.delta</span>, <span class="title-ref">Timedelta.is\_populated</span>, and <span class="title-ref">Timedelta.freq</span> (`46430`, `46476`)
  - Removed deprecated <span class="title-ref">NaT.freq</span> (`45071`)
  - Removed deprecated <span class="title-ref">Categorical.replace</span>, use <span class="title-ref">Series.replace</span> instead (`44929`)
  - Removed the `numeric_only` keyword from <span class="title-ref">Categorical.min</span> and <span class="title-ref">Categorical.max</span> in favor of `skipna` (`48821`)
  - Changed behavior of <span class="title-ref">DataFrame.median</span> and <span class="title-ref">DataFrame.mean</span> with `numeric_only=None` to not exclude datetime-like columns THIS NOTE WILL BE IRRELEVANT ONCE `numeric_only=None` DEPRECATION IS ENFORCED (`29941`)
  - Removed <span class="title-ref">is\_extension\_type</span> in favor of <span class="title-ref">is\_extension\_array\_dtype</span> (`29457`)
  - Removed `.ExponentialMovingWindow.vol` (`39220`)
  - Removed <span class="title-ref">Index.get\_value</span> and <span class="title-ref">Index.set\_value</span> (`33907`, `28621`)
  - Removed <span class="title-ref">Series.slice\_shift</span> and <span class="title-ref">DataFrame.slice\_shift</span> (`37601`)
  - Remove <span class="title-ref">DataFrameGroupBy.pad</span> and <span class="title-ref">DataFrameGroupBy.backfill</span> (`45076`)
  - Remove `numpy` argument from <span class="title-ref">read\_json</span> (`30636`)
  - Disallow passing abbreviations for `orient` in <span class="title-ref">DataFrame.to\_dict</span> (`32516`)
  - Disallow partial slicing on an non-monotonic <span class="title-ref">DatetimeIndex</span> with keys which are not in Index. This now raises a `KeyError` (`18531`)
  - Removed `get_offset` in favor of <span class="title-ref">to\_offset</span> (`30340`)
  - Removed the `warn` keyword in <span class="title-ref">infer\_freq</span> (`45947`)
  - Removed the `include_start` and `include_end` arguments in <span class="title-ref">DataFrame.between\_time</span> in favor of `inclusive` (`43248`)
  - Removed the `closed` argument in <span class="title-ref">date\_range</span> and <span class="title-ref">bdate\_range</span> in favor of `inclusive` argument (`40245`)
  - Removed the `center` keyword in <span class="title-ref">DataFrame.expanding</span> (`20647`)
  - Removed the `truediv` keyword from <span class="title-ref">eval</span> (`29812`)
  - Removed the `method` and `tolerance` arguments in <span class="title-ref">Index.get\_loc</span>. Use `index.get_indexer([label], method=..., tolerance=...)` instead (`42269`)
  - Removed the `pandas.datetime` submodule (`30489`)
  - Removed the `pandas.np` submodule (`30296`)
  - Removed `pandas.util.testing` in favor of `pandas.testing` (`30745`)
  - Removed <span class="title-ref">Series.str.\_\_iter\_\_</span> (`28277`)
  - Removed `pandas.SparseArray` in favor of <span class="title-ref">arrays.SparseArray</span> (`30642`)
  - Removed `pandas.SparseSeries` and `pandas.SparseDataFrame`, including pickle support. (`30642`)
  - Enforced disallowing passing an integer `fill_value` to <span class="title-ref">DataFrame.shift</span> and <span class="title-ref">Series.shift</span><span class="title-ref"> with datetime64, timedelta64, or period dtypes (:issue:\`32591</span>)
  - Enforced disallowing a string column label into `times` in <span class="title-ref">DataFrame.ewm</span> (`43265`)
  - Enforced disallowing passing `True` and `False` into `inclusive` in <span class="title-ref">Series.between</span> in favor of `"both"` and `"neither"` respectively (`40628`)
  - Enforced disallowing using `usecols` with out of bounds indices for `read_csv` with `engine="c"` (`25623`)
  - Enforced disallowing the use of `**kwargs` in <span class="title-ref">.ExcelWriter</span>; use the keyword argument `engine_kwargs` instead (`40430`)
  - Enforced disallowing a tuple of column labels into <span class="title-ref">.DataFrameGroupBy.\_\_getitem\_\_</span> (`30546`)
  - Enforced disallowing missing labels when indexing with a sequence of labels on a level of a <span class="title-ref">MultiIndex</span>. This now raises a `KeyError` (`42351`)
  - Enforced disallowing setting values with `.loc` using a positional slice. Use `.loc` with labels or `.iloc` with positions instead (`31840`)
  - Enforced disallowing positional indexing with a `float` key even if that key is a round number, manually cast to integer instead (`34193`)
  - Enforced disallowing using a <span class="title-ref">DataFrame</span> indexer with `.iloc`, use `.loc` instead for automatic alignment (`39022`)
  - Enforced disallowing `set` or `dict` indexers in `__getitem__` and `__setitem__` methods (`42825`)
  - Enforced disallowing indexing on a <span class="title-ref">Index</span> or positional indexing on a <span class="title-ref">Series</span> producing multi-dimensional objects e.g. `obj[:, None]`, convert to numpy before indexing instead (`35141`)
  - Enforced disallowing `dict` or `set` objects in `suffixes` in <span class="title-ref">merge</span> (`34810`)
  - Enforced disallowing <span class="title-ref">merge</span> to produce duplicated columns through the `suffixes` keyword and already existing columns (`22818`)
  - Enforced disallowing using <span class="title-ref">merge</span> or <span class="title-ref">join</span> on a different number of levels (`34862`)
  - Enforced disallowing `value_name` argument in <span class="title-ref">DataFrame.melt</span> to match an element in the <span class="title-ref">DataFrame</span> columns (`35003`)
  - Enforced disallowing passing `showindex` into `**kwargs` in <span class="title-ref">DataFrame.to\_markdown</span> and <span class="title-ref">Series.to\_markdown</span> in favor of `index` (`33091`)
  - Removed setting Categorical.\_codes directly (`41429`)
  - Removed setting Categorical.categories directly (`47834`)
  - Removed argument `inplace` from <span class="title-ref">Categorical.add\_categories</span>, <span class="title-ref">Categorical.remove\_categories</span>, <span class="title-ref">Categorical.set\_categories</span>, <span class="title-ref">Categorical.rename\_categories</span>, <span class="title-ref">Categorical.reorder\_categories</span>, <span class="title-ref">Categorical.set\_ordered</span>, <span class="title-ref">Categorical.as\_ordered</span>, <span class="title-ref">Categorical.as\_unordered</span> (`37981`, `41118`, `41133`, `47834`)
  - Enforced <span class="title-ref">Rolling.count</span> with `min_periods=None` to default to the size of the window (`31302`)
  - Renamed `fname` to `path` in <span class="title-ref">DataFrame.to\_parquet</span>, <span class="title-ref">DataFrame.to\_stata</span> and <span class="title-ref">DataFrame.to\_feather</span> (`30338`)
  - Enforced disallowing indexing a <span class="title-ref">Series</span> with a single item list with a slice (e.g. `ser[[slice(0, 2)]]`). Either convert the list to tuple, or pass the slice directly instead (`31333`)
  - Changed behavior indexing on a <span class="title-ref">DataFrame</span> with a <span class="title-ref">DatetimeIndex</span> index using a string indexer, previously this operated as a slice on rows, now it operates like any other column key; use `frame.loc[key]` for the old behavior (`36179`)
  - Enforced the `display.max_colwidth` option to not accept negative integers (`31569`)
  - Removed the `display.column_space` option in favor of `df.to_string(col_space=...)` (`47280`)
  - Removed the deprecated method `mad` from pandas classes (`11787`)
  - Removed the deprecated method `tshift` from pandas classes (`11631`)
  - Changed behavior of empty data passed into <span class="title-ref">Series</span>; the default dtype will be `object` instead of `float64` (`29405`)
  - Changed the behavior of <span class="title-ref">DatetimeIndex.union</span>, <span class="title-ref">DatetimeIndex.intersection</span>, and <span class="title-ref">DatetimeIndex.symmetric\_difference</span> with mismatched timezones to convert to UTC instead of casting to object dtype (`39328`)
  - Changed the behavior of <span class="title-ref">to\_datetime</span> with argument "now" with `utc=False` to match `Timestamp("now")` (`18705`)
  - Changed the behavior of indexing on a timezone-aware <span class="title-ref">DatetimeIndex</span> with a timezone-naive `datetime` object or vice-versa; these now behave like any other non-comparable type by raising `KeyError` (`36148`)
  - Changed the behavior of <span class="title-ref">Index.reindex</span>, <span class="title-ref">Series.reindex</span>, and <span class="title-ref">DataFrame.reindex</span> with a `datetime64` dtype and a `datetime.date` object for `fill_value`; these are no longer considered equivalent to `datetime.datetime` objects so the reindex casts to object dtype (`39767`)
  - Changed behavior of <span class="title-ref">SparseArray.astype</span> when given a dtype that is not explicitly `SparseDtype`, cast to the exact requested dtype rather than silently using a `SparseDtype` instead (`34457`)
  - Changed behavior of <span class="title-ref">Index.ravel</span> to return a view on the original <span class="title-ref">Index</span> instead of a `np.ndarray` (`36900`)
  - Changed behavior of <span class="title-ref">Series.to\_frame</span> and <span class="title-ref">Index.to\_frame</span> with explicit `name=None` to use `None` for the column name instead of the index's name or default `0` (`45523`)
  - Changed behavior of <span class="title-ref">concat</span> with one array of `bool`-dtype and another of integer dtype, this now returns `object` dtype instead of integer dtype; explicitly cast the bool object to integer before concatenating to get the old behavior (`45101`)
  - Changed behavior of <span class="title-ref">DataFrame</span> constructor given floating-point `data` and an integer `dtype`, when the data cannot be cast losslessly, the floating point dtype is retained, matching <span class="title-ref">Series</span> behavior (`41170`)
  - Changed behavior of <span class="title-ref">Index</span> constructor when given a `np.ndarray` with object-dtype containing numeric entries; this now retains object dtype rather than inferring a numeric dtype, consistent with <span class="title-ref">Series</span> behavior (`42870`)
  - Changed behavior of <span class="title-ref">Index.\_\_and\_\_</span>, <span class="title-ref">Index.\_\_or\_\_</span> and <span class="title-ref">Index.\_\_xor\_\_</span> to behave as logical operations (matching <span class="title-ref">Series</span> behavior) instead of aliases for set operations (`37374`)
  - Changed behavior of <span class="title-ref">DataFrame</span> constructor when passed a list whose first element is a <span class="title-ref">Categorical</span>, this now treats the elements as rows casting to `object` dtype, consistent with behavior for other types (`38845`)
  - Changed behavior of <span class="title-ref">DataFrame</span> constructor when passed a `dtype` (other than int) that the data cannot be cast to; it now raises instead of silently ignoring the dtype (`41733`)
  - Changed the behavior of <span class="title-ref">Series</span> constructor, it will no longer infer a datetime64 or timedelta64 dtype from string entries (`41731`)
  - Changed behavior of <span class="title-ref">Timestamp</span> constructor with a `np.datetime64` object and a `tz` passed to interpret the input as a wall-time as opposed to a UTC time (`42288`)
  - Changed behavior of <span class="title-ref">Timestamp.utcfromtimestamp</span> to return a timezone-aware object satisfying `Timestamp.utcfromtimestamp(val).timestamp() == val` (`45083`)
  - Changed behavior of <span class="title-ref">Index</span> constructor when passed a `SparseArray` or `SparseDtype` to retain that dtype instead of casting to `numpy.ndarray` (`43930`)
  - Changed behavior of setitem-like operations (`__setitem__`, `fillna`, `where`, `mask`, `replace`, `insert`, fill\_value for `shift`) on an object with <span class="title-ref">DatetimeTZDtype</span> when using a value with a non-matching timezone, the value will be cast to the object's timezone instead of casting both to object-dtype (`44243`)
  - Changed behavior of <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span> constructors with floating-dtype data and a <span class="title-ref">DatetimeTZDtype</span>, the data are now interpreted as UTC-times instead of wall-times, consistent with how integer-dtype data are treated (`45573`)
  - Changed behavior of <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors with integer dtype and floating-point data containing `NaN`, this now raises `IntCastingNaNError` (`40110`)
  - Changed behavior of <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors with an integer `dtype` and values that are too large to losslessly cast to this dtype, this now raises `ValueError` (`41734`)
  - Changed behavior of <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors with an integer `dtype` and values having either `datetime64` or `timedelta64` dtypes, this now raises `TypeError`, use `values.view("int64")` instead (`41770`)
  - Removed the deprecated `base` and `loffset` arguments from <span class="title-ref">pandas.DataFrame.resample</span>, <span class="title-ref">pandas.Series.resample</span> and <span class="title-ref">pandas.Grouper</span>. Use `offset` or `origin` instead (`31809`)
  - Changed behavior of <span class="title-ref">Series.fillna</span> and <span class="title-ref">DataFrame.fillna</span> with `timedelta64[ns]` dtype and an incompatible `fill_value`; this now casts to `object` dtype instead of raising, consistent with the behavior with other dtypes (`45746`)
  - Change the default argument of `regex` for <span class="title-ref">Series.str.replace</span> from `True` to `False`. Additionally, a single character `pat` with `regex=True` is now treated as a regular expression instead of a string literal. (`36695`, `24804`)
  - Changed behavior of <span class="title-ref">DataFrame.any</span> and <span class="title-ref">DataFrame.all</span> with `bool_only=True`; object-dtype columns with all-bool values will no longer be included, manually cast to `bool` dtype first (`46188`)
  - Changed behavior of <span class="title-ref">DataFrame.max</span>, <span class="title-ref">DataFrame.min</span>, <span class="title-ref">DataFrame.mean</span>, <span class="title-ref">DataFrame.median</span>, <span class="title-ref">DataFrame.skew</span>, <span class="title-ref">DataFrame.kurt</span> with `axis=None` to return a scalar applying the aggregation across both axes (`45072`)
  - Changed behavior of comparison of a <span class="title-ref">Timestamp</span> with a `datetime.date` object; these now compare as un-equal and raise on inequality comparisons, matching the `datetime.datetime` behavior (`36131`)
  - Changed behavior of comparison of `NaT` with a `datetime.date` object; these now raise on inequality comparisons (`39196`)
  - Enforced deprecation of silently dropping columns that raised a `TypeError` in <span class="title-ref">Series.transform</span> and <span class="title-ref">DataFrame.transform</span> when used with a list or dictionary (`43740`)
  - Changed behavior of <span class="title-ref">DataFrame.apply</span> with list-like so that any partial failure will raise an error (`43740`)
  - Changed behaviour of <span class="title-ref">DataFrame.to\_latex</span> to now use the Styler implementation via <span class="title-ref">.Styler.to\_latex</span> (`47970`)
  - Changed behavior of <span class="title-ref">Series.\_\_setitem\_\_</span> with an integer key and a <span class="title-ref">Float64Index</span> when the key is not present in the index; previously we treated the key as positional (behaving like `series.iloc[key] = val`), now we treat it is a label (behaving like `series.loc[key] = val`), consistent with <span class="title-ref">Series.\_\_getitem\_\_</span><span class="title-ref"> behavior (:issue:\`33469</span>)
  - Removed `na_sentinel` argument from <span class="title-ref">factorize</span>, <span class="title-ref">.Index.factorize</span>, and <span class="title-ref">.ExtensionArray.factorize</span> (`47157`)
  - Changed behavior of <span class="title-ref">Series.diff</span> and <span class="title-ref">DataFrame.diff</span> with <span class="title-ref">ExtensionDtype</span> dtypes whose arrays do not implement `diff`, these now raise `TypeError` rather than casting to numpy (`31025`)
  - Enforced deprecation of calling numpy "ufunc"s on <span class="title-ref">DataFrame</span> with `method="outer"`; this now raises `NotImplementedError` (`36955`)
  - Enforced deprecation disallowing passing `numeric_only=True` to <span class="title-ref">Series</span> reductions (`rank`, `any`, `all`, ...) with non-numeric dtype (`47500`)
  - Changed behavior of <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> so that `group_keys` is respected even if a transformer is detected (`34998`)
  - Comparisons between a <span class="title-ref">DataFrame</span> and a <span class="title-ref">Series</span> where the frame's columns do not match the series's index raise `ValueError` instead of automatically aligning, do `left, right = left.align(right, axis=1, copy=False)` before comparing (`36795`)
  - Enforced deprecation `numeric_only=None` (the default) in DataFrame reductions that would silently drop columns that raised; `numeric_only` now defaults to `False` (`41480`)
  - Changed default of `numeric_only` to `False` in all DataFrame methods with that argument (`46096`, `46906`)
  - Changed default of `numeric_only` to `False` in <span class="title-ref">Series.rank</span> (`47561`)
  - Enforced deprecation of silently dropping nuisance columns in groupby and resample operations when `numeric_only=False` (`41475`)
  - Enforced deprecation of silently dropping nuisance columns in <span class="title-ref">Rolling</span>, <span class="title-ref">Expanding</span>, and <span class="title-ref">ExponentialMovingWindow</span> ops. This will now raise a <span class="title-ref">.errors.DataError</span> (`42834`)
  - Changed behavior in setting values with `df.loc[:, foo] = bar` or `df.iloc[:, foo] = bar`, these now always attempt to set values inplace before falling back to casting (`45333`)
  - Changed default of `numeric_only` in various <span class="title-ref">.DataFrameGroupBy</span> methods; all methods now default to `numeric_only=False` (`46072`)
  - Changed default of `numeric_only` to `False` in <span class="title-ref">.Resampler</span> methods (`47177`)
  - Using the method <span class="title-ref">.DataFrameGroupBy.transform</span> with a callable that returns DataFrames will align to the input's index (`47244`)
  - When providing a list of columns of length one to <span class="title-ref">DataFrame.groupby</span>, the keys that are returned by iterating over the resulting <span class="title-ref">DataFrameGroupBy</span> object will now be tuples of length one (`47761`)
  - Removed deprecated methods <span class="title-ref">ExcelWriter.write\_cells</span>, <span class="title-ref">ExcelWriter.save</span>, <span class="title-ref">ExcelWriter.cur\_sheet</span>, <span class="title-ref">ExcelWriter.handles</span>, <span class="title-ref">ExcelWriter.path</span> (`45795`)
  - The <span class="title-ref">ExcelWriter</span> attribute `book` can no longer be set; it is still available to be accessed and mutated (`48943`)
  - Removed unused `*args` and `**kwargs` in <span class="title-ref">Rolling</span>, <span class="title-ref">Expanding</span>, and <span class="title-ref">ExponentialMovingWindow</span> ops (`47851`)
  - Removed the deprecated argument `line_terminator` from <span class="title-ref">DataFrame.to\_csv</span> (`45302`)
  - Removed the deprecated argument `label` from <span class="title-ref">lreshape</span> (`30219`)
  - Arguments after `expr` in <span class="title-ref">DataFrame.eval</span> and <span class="title-ref">DataFrame.query</span> are keyword-only (`47587`)
  - Removed <span class="title-ref">Index.\_get\_attributes\_dict</span> (`50648`)
  - Removed <span class="title-ref">Series.\_\_array\_wrap\_\_</span> (`50648`)
  - Changed behavior of <span class="title-ref">.DataFrame.value\_counts</span> to return a <span class="title-ref">Series</span> with <span class="title-ref">MultiIndex</span> for any list-like(one element or not) but an <span class="title-ref">Index</span> for a single label (`50829`)

## Performance improvements

  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.median</span> and <span class="title-ref">.SeriesGroupBy.median</span> and <span class="title-ref">.DataFrameGroupBy.cumprod</span> for nullable dtypes (`37493`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.all</span>, <span class="title-ref">.DataFrameGroupBy.any</span>, <span class="title-ref">.SeriesGroupBy.all</span>, and <span class="title-ref">.SeriesGroupBy.any</span> for object dtype (`50623`)
  - Performance improvement in <span class="title-ref">MultiIndex.argsort</span> and <span class="title-ref">MultiIndex.sort\_values</span> (`48406`)
  - Performance improvement in <span class="title-ref">MultiIndex.size</span> (`48723`)
  - Performance improvement in <span class="title-ref">MultiIndex.union</span> without missing values and without duplicates (`48505`, `48752`)
  - Performance improvement in <span class="title-ref">MultiIndex.difference</span> (`48606`)
  - Performance improvement in <span class="title-ref">MultiIndex</span> set operations with sort=None (`49010`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.mean</span>, <span class="title-ref">.SeriesGroupBy.mean</span>, <span class="title-ref">.DataFrameGroupBy.var</span>, and <span class="title-ref">.SeriesGroupBy.var</span> for extension array dtypes (`37493`)
  - Performance improvement in <span class="title-ref">MultiIndex.isin</span> when `level=None` (`48622`, `49577`)
  - Performance improvement in <span class="title-ref">MultiIndex.putmask</span> (`49830`)
  - Performance improvement in <span class="title-ref">Index.union</span> and <span class="title-ref">MultiIndex.union</span> when index contains duplicates (`48900`)
  - Performance improvement in <span class="title-ref">Series.rank</span> for pyarrow-backed dtypes (`50264`)
  - Performance improvement in <span class="title-ref">Series.searchsorted</span> for pyarrow-backed dtypes (`50447`)
  - Performance improvement in <span class="title-ref">Series.fillna</span> for extension array dtypes (`49722`, `50078`)
  - Performance improvement in <span class="title-ref">Index.join</span>, <span class="title-ref">Index.intersection</span> and <span class="title-ref">Index.union</span> for masked and arrow dtypes when <span class="title-ref">Index</span> is monotonic (`50310`, `51365`)
  - Performance improvement for <span class="title-ref">Series.value\_counts</span> with nullable dtype (`48338`)
  - Performance improvement for <span class="title-ref">Series</span> constructor passing integer numpy array with nullable dtype (`48338`)
  - Performance improvement for <span class="title-ref">DatetimeIndex</span> constructor passing a list (`48609`)
  - Performance improvement in <span class="title-ref">merge</span> and <span class="title-ref">DataFrame.join</span> when joining on a sorted <span class="title-ref">MultiIndex</span> (`48504`)
  - Performance improvement in <span class="title-ref">to\_datetime</span> when parsing strings with timezone offsets (`50107`)
  - Performance improvement in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> for tuple-based indexing of a <span class="title-ref">MultiIndex</span> (`48384`)
  - Performance improvement for <span class="title-ref">Series.replace</span> with categorical dtype (`49404`)
  - Performance improvement for <span class="title-ref">MultiIndex.unique</span> (`48335`)
  - Performance improvement for indexing operations with nullable and arrow dtypes (`49420`, `51316`)
  - Performance improvement for <span class="title-ref">concat</span> with extension array backed indexes (`49128`, `49178`)
  - Performance improvement for <span class="title-ref">api.types.infer\_dtype</span> (`51054`)
  - Reduce memory usage of <span class="title-ref">DataFrame.to\_pickle</span>/<span class="title-ref">Series.to\_pickle</span> when using BZ2 or LZMA (`49068`)
  - Performance improvement for <span class="title-ref">\~arrays.StringArray</span> constructor passing a numpy array with type `np.str_` (`49109`)
  - Performance improvement in <span class="title-ref">\~arrays.IntervalArray.from\_tuples</span> (`50620`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray.factorize</span> (`49177`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray.\_\_setitem\_\_</span> (`50248`, `50632`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray</span> comparison methods when array contains NA (`50524`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray.to\_numpy</span> (`49973`, `51227`)
  - Performance improvement when parsing strings to <span class="title-ref">BooleanDtype</span> (`50613`)
  - Performance improvement in <span class="title-ref">DataFrame.join</span> when joining on a subset of a <span class="title-ref">MultiIndex</span> (`48611`)
  - Performance improvement for <span class="title-ref">MultiIndex.intersection</span> (`48604`)
  - Performance improvement in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> (`46267`)
  - Performance improvement in `var` and `std` for nullable dtypes (`48379`).
  - Performance improvement when iterating over pyarrow and nullable dtypes (`49825`, `49851`)
  - Performance improvements to <span class="title-ref">read\_sas</span> (`47403`, `47405`, `47656`, `48502`)
  - Memory improvement in <span class="title-ref">RangeIndex.sort\_values</span> (`48801`)
  - Performance improvement in <span class="title-ref">Series.to\_numpy</span> if `copy=True` by avoiding copying twice (`24345`)
  - Performance improvement in <span class="title-ref">Series.rename</span> with <span class="title-ref">MultiIndex</span> (`21055`)
  - Performance improvement in <span class="title-ref">DataFrameGroupBy</span> and <span class="title-ref">SeriesGroupBy</span> when `by` is a categorical type and `sort=False` (`48976`)
  - Performance improvement in <span class="title-ref">DataFrameGroupBy</span> and <span class="title-ref">SeriesGroupBy</span> when `by` is a categorical type and `observed=False` (`49596`)
  - Performance improvement in <span class="title-ref">read\_stata</span> with parameter `index_col` set to `None` (the default). Now the index will be a <span class="title-ref">RangeIndex</span> instead of <span class="title-ref">Int64Index</span> (`49745`)
  - Performance improvement in <span class="title-ref">merge</span> when not merging on the index - the new index will now be <span class="title-ref">RangeIndex</span> instead of <span class="title-ref">Int64Index</span> (`49478`)
  - Performance improvement in <span class="title-ref">DataFrame.to\_dict</span> and <span class="title-ref">Series.to\_dict</span> when using any non-object dtypes (`46470`)
  - Performance improvement in <span class="title-ref">read\_html</span> when there are multiple tables (`49929`)
  - Performance improvement in <span class="title-ref">Period</span> constructor when constructing from a string or integer (`38312`)
  - Performance improvement in <span class="title-ref">to\_datetime</span> when using `'%Y%m%d'` format (`17410`)
  - Performance improvement in <span class="title-ref">to\_datetime</span> when format is given or can be inferred (`50465`)
  - Performance improvement in <span class="title-ref">Series.median</span> for nullable dtypes (`50838`)
  - Performance improvement in <span class="title-ref">read\_csv</span> when passing <span class="title-ref">to\_datetime</span> lambda-function to `date_parser` and inputs have mixed timezone offsets (`35296`)
  - Performance improvement in <span class="title-ref">isna</span> and <span class="title-ref">isnull</span> (`50658`)
  - Performance improvement in <span class="title-ref">.SeriesGroupBy.value\_counts</span> with categorical dtype (`46202`)
  - Fixed a reference leak in <span class="title-ref">read\_hdf</span> (`37441`)
  - Fixed a memory leak in <span class="title-ref">DataFrame.to\_json</span> and <span class="title-ref">Series.to\_json</span> when serializing datetimes and timedeltas (`40443`)
  - Decreased memory usage in many <span class="title-ref">DataFrameGroupBy</span> methods (`51090`)
  - Performance improvement in <span class="title-ref">DataFrame.round</span> for an integer `decimal` parameter (`17254`)
  - Performance improvement in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> when using a large dict for `to_replace` (`6697`)
  - Memory improvement in <span class="title-ref">StataReader</span> when reading seekable files (`48922`)

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">Categorical.set\_categories</span> losing dtype information (`48812`)
  - Bug in <span class="title-ref">Series.replace</span> with categorical dtype when `to_replace` values overlap with new values (`49404`)
  - Bug in <span class="title-ref">Series.replace</span> with categorical dtype losing nullable dtypes of underlying categories (`49404`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> would reorder categories when used as a grouper (`48749`)
  - Bug in <span class="title-ref">Categorical</span> constructor when constructing from a <span class="title-ref">Categorical</span> object and `dtype="category"` losing ordered-ness (`49309`)
  - Bug in <span class="title-ref">.SeriesGroupBy.min</span>, <span class="title-ref">.SeriesGroupBy.max</span>, <span class="title-ref">.DataFrameGroupBy.min</span>, and <span class="title-ref">.DataFrameGroupBy.max</span> with unordered <span class="title-ref">CategoricalDtype</span> with no groups failing to raise `TypeError` (`51034`)

### Datetimelike

  - Bug in <span class="title-ref">pandas.infer\_freq</span>, raising `TypeError` when inferred on <span class="title-ref">RangeIndex</span> (`47084`)
  - Bug in <span class="title-ref">to\_datetime</span> incorrectly raising `OverflowError` with string arguments corresponding to large integers (`50533`)
  - Bug in <span class="title-ref">to\_datetime</span> was raising on invalid offsets with `errors='coerce'` and `infer_datetime_format=True` (`48633`)
  - Bug in <span class="title-ref">DatetimeIndex</span> constructor failing to raise when `tz=None` is explicitly specified in conjunction with timezone-aware `dtype` or data (`48659`)
  - Bug in subtracting a `datetime` scalar from <span class="title-ref">DatetimeIndex</span> failing to retain the original `freq` attribute (`48818`)
  - Bug in `pandas.tseries.holiday.Holiday` where a half-open date interval causes inconsistent return types from <span class="title-ref">USFederalHolidayCalendar.holidays</span> (`49075`)
  - Bug in rendering <span class="title-ref">DatetimeIndex</span> and <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> with timezone-aware dtypes with `dateutil` or `zoneinfo` timezones near daylight-savings transitions (`49684`)
  - Bug in <span class="title-ref">to\_datetime</span> was raising `ValueError` when parsing <span class="title-ref">Timestamp</span>, `datetime.datetime`, `datetime.date`, or `np.datetime64` objects when non-ISO8601 `format` was passed (`49298`, `50036`)
  - Bug in <span class="title-ref">to\_datetime</span> was raising `ValueError` when parsing empty string and non-ISO8601 format was passed. Now, empty strings will be parsed as <span class="title-ref">NaT</span>, for compatibility with how is done for ISO8601 formats (`50251`)
  - Bug in <span class="title-ref">Timestamp</span> was showing `UserWarning`, which was not actionable by users, when parsing non-ISO8601 delimited date strings (`50232`)
  - Bug in <span class="title-ref">to\_datetime</span> was showing misleading `ValueError` when parsing dates with format containing ISO week directive and ISO weekday directive (`50308`)
  - Bug in <span class="title-ref">Timestamp.round</span> when the `freq` argument has zero-duration (e.g. "0ns") returning incorrect results instead of raising (`49737`)
  - Bug in <span class="title-ref">to\_datetime</span> was not raising `ValueError` when invalid format was passed and `errors` was `'ignore'` or `'coerce'` (`50266`)
  - Bug in <span class="title-ref">DateOffset</span> was throwing `TypeError` when constructing with milliseconds and another super-daily argument (`49897`)
  - Bug in <span class="title-ref">to\_datetime</span> was not raising `ValueError` when parsing string with decimal date with format `'%Y%m%d'` (`50051`)
  - Bug in <span class="title-ref">to\_datetime</span> was not converting `None` to `NaT` when parsing mixed-offset date strings with ISO8601 format (`50071`)
  - Bug in <span class="title-ref">to\_datetime</span> was not returning input when parsing out-of-bounds date string with `errors='ignore'` and `format='%Y%m%d'` (`14487`)
  - Bug in <span class="title-ref">to\_datetime</span> was converting timezone-naive `datetime.datetime` to timezone-aware when parsing with timezone-aware strings, ISO8601 format, and `utc=False` (`50254`)
  - Bug in <span class="title-ref">to\_datetime</span> was throwing `ValueError` when parsing dates with ISO8601 format where some values were not zero-padded (`21422`)
  - Bug in <span class="title-ref">to\_datetime</span> was giving incorrect results when using `format='%Y%m%d'` and `errors='ignore'` (`26493`)
  - Bug in <span class="title-ref">to\_datetime</span> was failing to parse date strings `'today'` and `'now'` if `format` was not ISO8601 (`50359`)
  - Bug in <span class="title-ref">Timestamp.utctimetuple</span> raising a `TypeError` (`32174`)
  - Bug in <span class="title-ref">to\_datetime</span> was raising `ValueError` when parsing mixed-offset <span class="title-ref">Timestamp</span> with `errors='ignore'` (`50585`)
  - Bug in <span class="title-ref">to\_datetime</span> was incorrectly handling floating-point inputs within 1 `unit` of the overflow boundaries (`50183`)
  - Bug in <span class="title-ref">to\_datetime</span> with unit of "Y" or "M" giving incorrect results, not matching pointwise <span class="title-ref">Timestamp</span> results (`50870`)
  - Bug in <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> with datetime or timedelta dtypes incorrectly raising `ValueError` (`11312`)
  - Bug in <span class="title-ref">to\_datetime</span> was not returning input with `errors='ignore'` when input was out-of-bounds (`50587`)
  - Bug in <span class="title-ref">DataFrame.from\_records</span> when given a <span class="title-ref">DataFrame</span> input with timezone-aware datetime64 columns incorrectly dropping the timezone-awareness (`51162`)
  - Bug in <span class="title-ref">to\_datetime</span> was raising `decimal.InvalidOperation` when parsing date strings with `errors='coerce'` (`51084`)
  - Bug in <span class="title-ref">to\_datetime</span> with both `unit` and `origin` specified returning incorrect results (`42624`)
  - Bug in <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> when converting an object-dtype object containing timezone-aware datetimes or strings to `datetime64[ns]` incorrectly localizing as UTC instead of raising `TypeError` (`50140`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> with datetime or timedelta dtypes giving incorrect results for groups containing `NaT` (`51373`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> incorrectly raising with <span class="title-ref">PeriodDtype</span> or <span class="title-ref">DatetimeTZDtype</span> (`51373`)

### Timedelta

  - Bug in <span class="title-ref">to\_timedelta</span> raising error when input has nullable dtype `Float64` (`48796`)
  - Bug in <span class="title-ref">Timedelta</span> constructor incorrectly raising instead of returning `NaT` when given a `np.timedelta64("nat")` (`48898`)
  - Bug in <span class="title-ref">Timedelta</span> constructor failing to raise when passed both a <span class="title-ref">Timedelta</span> object and keywords (e.g. days, seconds) (`48898`)
  - Bug in <span class="title-ref">Timedelta</span> comparisons with very large `datetime.timedelta` objects incorrect raising `OutOfBoundsTimedelta` (`49021`)

### Timezones

  - Bug in <span class="title-ref">Series.astype</span> and <span class="title-ref">DataFrame.astype</span> with object-dtype containing multiple timezone-aware `datetime` objects with heterogeneous timezones to a <span class="title-ref">DatetimeTZDtype</span> incorrectly raising (`32581`)
  - Bug in <span class="title-ref">to\_datetime</span> was failing to parse date strings with timezone name when `format` was specified with `%Z` (`49748`)
  - Better error message when passing invalid values to `ambiguous` parameter in <span class="title-ref">Timestamp.tz\_localize</span> (`49565`)
  - Bug in string parsing incorrectly allowing a <span class="title-ref">Timestamp</span> to be constructed with an invalid timezone, which would raise when trying to print (`50668`)
  - Corrected TypeError message in <span class="title-ref">objects\_to\_datetime64ns</span> to inform that DatetimeIndex has mixed timezones (`50974`)

### Numeric

  - Bug in <span class="title-ref">DataFrame.add</span> cannot apply ufunc when inputs contain mixed DataFrame type and Series type (`39853`)
  - Bug in arithmetic operations on <span class="title-ref">Series</span> not propagating mask when combining masked dtypes and numpy dtypes (`45810`, `42630`)
  - Bug in <span class="title-ref">DataFrame.sem</span> and <span class="title-ref">Series.sem</span> where an erroneous `TypeError` would always raise when using data backed by an <span class="title-ref">ArrowDtype</span> (`49759`)
  - Bug in <span class="title-ref">Series.\_\_add\_\_</span> casting to object for list and masked <span class="title-ref">Series</span> (`22962`)
  - Bug in <span class="title-ref">\~arrays.ArrowExtensionArray.mode</span> where `dropna=False` was not respected when there was `NA` values (`50982`)
  - Bug in <span class="title-ref">DataFrame.query</span> with `engine="numexpr"` and column names are `min` or `max` would raise a `TypeError` (`50937`)
  - Bug in <span class="title-ref">DataFrame.min</span> and <span class="title-ref">DataFrame.max</span> with tz-aware data containing `pd.NaT` and `axis=1` would return incorrect results (`51242`)

### Conversion

  - Bug in constructing <span class="title-ref">Series</span> with `int64` dtype from a string list raising instead of casting (`44923`)
  - Bug in constructing <span class="title-ref">Series</span> with masked dtype and boolean values with `NA` raising (`42137`)
  - Bug in <span class="title-ref">DataFrame.eval</span> incorrectly raising an `AttributeError` when there are negative values in function call (`46471`)
  - Bug in <span class="title-ref">Series.convert\_dtypes</span> not converting dtype to nullable dtype when <span class="title-ref">Series</span> contains `NA` and has dtype `object` (`48791`)
  - Bug where any <span class="title-ref">ExtensionDtype</span> subclass with `kind="M"` would be interpreted as a timezone type (`34986`)
  - Bug in <span class="title-ref">.arrays.ArrowExtensionArray</span> that would raise `NotImplementedError` when passed a sequence of strings or binary (`49172`)
  - Bug in <span class="title-ref">Series.astype</span> raising `pyarrow.ArrowInvalid` when converting from a non-pyarrow string dtype to a pyarrow numeric type (`50430`)
  - Bug in <span class="title-ref">DataFrame.astype</span> modifying input array inplace when converting to `string` and `copy=False` (`51073`)
  - Bug in <span class="title-ref">Series.to\_numpy</span> converting to NumPy array before applying `na_value` (`48951`)
  - Bug in <span class="title-ref">DataFrame.astype</span> not copying data when converting to pyarrow dtype (`50984`)
  - Bug in <span class="title-ref">to\_datetime</span> was not respecting `exact` argument when `format` was an ISO8601 format (`12649`)
  - Bug in <span class="title-ref">TimedeltaArray.astype</span> raising `TypeError` when converting to a pyarrow duration type (`49795`)
  - Bug in <span class="title-ref">DataFrame.eval</span> and <span class="title-ref">DataFrame.query</span> raising for extension array dtypes (`29618`, `50261`, `31913`)
  - Bug in <span class="title-ref">Series</span> not copying data when created from <span class="title-ref">Index</span> and `dtype` is equal to `dtype` from <span class="title-ref">Index</span> (`52008`)

### Strings

  - Bug in <span class="title-ref">pandas.api.types.is\_string\_dtype</span> that would not return `True` for <span class="title-ref">StringDtype</span> or <span class="title-ref">ArrowDtype</span> with `pyarrow.string()` (`15585`)
  - Bug in converting string dtypes to "datetime64\[ns\]" or "timedelta64\[ns\]" incorrectly raising `TypeError` (`36153`)
  - Bug in setting values in a string-dtype column with an array, mutating the array as side effect when it contains missing values (`51299`)

### Interval

  - Bug in <span class="title-ref">IntervalIndex.is\_overlapping</span> incorrect output if interval has duplicate left boundaries (`49581`)
  - Bug in <span class="title-ref">Series.infer\_objects</span> failing to infer <span class="title-ref">IntervalDtype</span> for an object series of <span class="title-ref">Interval</span> objects (`50090`)
  - Bug in <span class="title-ref">Series.shift</span> with <span class="title-ref">IntervalDtype</span> and invalid null `fill_value` failing to raise `TypeError` (`51258`)

### Indexing

  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> raising when indexer is a <span class="title-ref">DataFrame</span> with `boolean` dtype (`47125`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> filling with wrong values when indexing columns and index for `uint` dtypes (`48184`)
  - Bug in <span class="title-ref">DataFrame.loc</span> when setting <span class="title-ref">DataFrame</span> with different dtypes coercing values to single dtype (`50467`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> where `None` was not returned when `by` is empty list and `inplace=True` (`50643`)
  - Bug in <span class="title-ref">DataFrame.loc</span> coercing dtypes when setting values with a list indexer (`49159`)
  - Bug in <span class="title-ref">Series.loc</span> raising error for out of bounds end of slice indexer (`50161`)
  - Bug in <span class="title-ref">DataFrame.loc</span> raising `ValueError` with all `False` `bool` indexer and empty object (`51450`)
  - Bug in <span class="title-ref">DataFrame.loc</span> raising `ValueError` with `bool` indexer and <span class="title-ref">MultiIndex</span> (`47687`)
  - Bug in <span class="title-ref">DataFrame.loc</span> raising `IndexError` when setting values for a pyarrow-backed column with a non-scalar indexer (`50085`)
  - Bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span>, <span class="title-ref">Series.\_\_getitem\_\_</span>, <span class="title-ref">DataFrame.\_\_setitem\_\_</span> and <span class="title-ref">Series.\_\_setitem\_\_</span> when indexing on indexes with extension float dtypes (<span class="title-ref">Float64</span> & <span class="title-ref">Float64</span>) or complex dtypes using integers (`51053`)
  - Bug in <span class="title-ref">DataFrame.loc</span> modifying object when setting incompatible value with an empty indexer (`45981`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> raising `ValueError` when right hand side is <span class="title-ref">DataFrame</span> with <span class="title-ref">MultiIndex</span> columns (`49121`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> casting dtype to `object` when <span class="title-ref">DataFrame</span> has single extension array column when re-indexing `columns` and `index` (`48190`)
  - Bug in <span class="title-ref">DataFrame.iloc</span> raising `IndexError` when indexer is a <span class="title-ref">Series</span> with numeric extension array dtype (`49521`)
  - Bug in <span class="title-ref">\~DataFrame.describe</span> when formatting percentiles in the resulting index showed more decimals than needed (`46362`)
  - Bug in <span class="title-ref">DataFrame.compare</span> does not recognize differences when comparing `NA` with value in nullable dtypes (`48939`)
  - Bug in <span class="title-ref">Series.rename</span> with <span class="title-ref">MultiIndex</span> losing extension array dtypes (`21055`)
  - Bug in <span class="title-ref">DataFrame.isetitem</span> coercing extension array dtypes in <span class="title-ref">DataFrame</span> to object (`49922`)
  - Bug in <span class="title-ref">Series.\_\_getitem\_\_</span> returning corrupt object when selecting from an empty pyarrow backed object (`51734`)
  - Bug in <span class="title-ref">BusinessHour</span> would cause creation of <span class="title-ref">DatetimeIndex</span> to fail when no opening hour was included in the index (`49835`)

### Missing

  - Bug in <span class="title-ref">Index.equals</span> raising `TypeError` when <span class="title-ref">Index</span> consists of tuples that contain `NA` (`48446`)
  - Bug in <span class="title-ref">Series.map</span> caused incorrect result when data has NaNs and defaultdict mapping was used (`48813`)
  - Bug in <span class="title-ref">NA</span> raising a `TypeError` instead of return <span class="title-ref">NA</span> when performing a binary operation with a `bytes` object (`49108`)
  - Bug in <span class="title-ref">DataFrame.update</span> with `overwrite=False` raising `TypeError` when `self` has column with `NaT` values and column not present in `other` (`16713`)
  - Bug in <span class="title-ref">Series.replace</span> raising `RecursionError` when replacing value in object-dtype <span class="title-ref">Series</span> containing `NA` (`47480`)
  - Bug in <span class="title-ref">Series.replace</span> raising `RecursionError` when replacing value in numeric <span class="title-ref">Series</span> with `NA` (`50758`)

### MultiIndex

  - Bug in <span class="title-ref">MultiIndex.get\_indexer</span> not matching `NaN` values (`29252`, `37222`, `38623`, `42883`, `43222`, `46173`, `48905`)
  - Bug in <span class="title-ref">MultiIndex.argsort</span> raising `TypeError` when index contains <span class="title-ref">NA</span> (`48495`)
  - Bug in <span class="title-ref">MultiIndex.difference</span> losing extension array dtype (`48606`)
  - Bug in <span class="title-ref">MultiIndex.set\_levels</span> raising `IndexError` when setting empty level (`48636`)
  - Bug in <span class="title-ref">MultiIndex.unique</span> losing extension array dtype (`48335`)
  - Bug in <span class="title-ref">MultiIndex.intersection</span> losing extension array (`48604`)
  - Bug in <span class="title-ref">MultiIndex.union</span> losing extension array (`48498`, `48505`, `48900`)
  - Bug in <span class="title-ref">MultiIndex.union</span> not sorting when sort=None and index contains missing values (`49010`)
  - Bug in <span class="title-ref">MultiIndex.append</span> not checking names for equality (`48288`)
  - Bug in <span class="title-ref">MultiIndex.symmetric\_difference</span> losing extension array (`48607`)
  - Bug in <span class="title-ref">MultiIndex.join</span> losing dtypes when <span class="title-ref">MultiIndex</span> has duplicates (`49830`)
  - Bug in <span class="title-ref">MultiIndex.putmask</span> losing extension array (`49830`)
  - Bug in <span class="title-ref">MultiIndex.value\_counts</span> returning a <span class="title-ref">Series</span> indexed by flat index of tuples instead of a <span class="title-ref">MultiIndex</span> (`49558`)

### I/O

  - Bug in <span class="title-ref">read\_sas</span> caused fragmentation of <span class="title-ref">DataFrame</span> and raised <span class="title-ref">.errors.PerformanceWarning</span> (`48595`)
  - Improved error message in <span class="title-ref">read\_excel</span> by including the offending sheet name when an exception is raised while reading a file (`48706`)
  - Bug when a pickling a subset PyArrow-backed data that would serialize the entire data instead of the subset (`42600`)
  - Bug in <span class="title-ref">read\_sql\_query</span> ignoring `dtype` argument when `chunksize` is specified and result is empty (`50245`)
  - Bug in <span class="title-ref">read\_csv</span> for a single-line csv with fewer columns than `names` raised <span class="title-ref">.errors.ParserError</span> with `engine="c"` (`47566`)
  - Bug in <span class="title-ref">read\_json</span> raising with `orient="table"` and `NA` value (`40255`)
  - Bug in displaying `string` dtypes not showing storage option (`50099`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> with `header=False` that printed the index name on the same line as the first row of the data (`49230`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> ignoring float formatter for extension arrays (`39336`)
  - Fixed memory leak which stemmed from the initialization of the internal JSON module (`49222`)
  - Fixed issue where <span class="title-ref">json\_normalize</span> would incorrectly remove leading characters from column names that matched the `sep` argument (`49861`)
  - Bug in <span class="title-ref">read\_csv</span> unnecessarily overflowing for extension array dtype when containing `NA` (`32134`)
  - Bug in <span class="title-ref">DataFrame.to\_dict</span> not converting `NA` to `None` (`50795`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> where it would segfault when failing to encode a string (`50307`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> with `na_rep` set when the <span class="title-ref">DataFrame</span> contains non-scalar data (`47103`)
  - Bug in <span class="title-ref">read\_xml</span> where file-like objects failed when iterparse is used (`50641`)
  - Bug in <span class="title-ref">read\_csv</span> when `engine="pyarrow"` where `encoding` parameter was not handled correctly (`51302`)
  - Bug in <span class="title-ref">read\_xml</span> ignored repeated elements when iterparse is used (`51183`)
  - Bug in <span class="title-ref">ExcelWriter</span> leaving file handles open if an exception occurred during instantiation (`51443`)
  - Bug in <span class="title-ref">DataFrame.to\_parquet</span> where non-string index or columns were raising a `ValueError` when `engine="pyarrow"` (`52036`)

### Period

  - Bug in <span class="title-ref">Period.strftime</span> and <span class="title-ref">PeriodIndex.strftime</span>, raising `UnicodeDecodeError` when a locale-specific directive was passed (`46319`)
  - Bug in adding a <span class="title-ref">Period</span> object to an array of <span class="title-ref">DateOffset</span> objects incorrectly raising `TypeError` (`50162`)
  - Bug in <span class="title-ref">Period</span> where passing a string with finer resolution than nanosecond would result in a `KeyError` instead of dropping the extra precision (`50417`)
  - Bug in parsing strings representing Week-periods e.g. "2017-01-23/2017-01-29" as minute-frequency instead of week-frequency (`50803`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.sum</span>, <span class="title-ref">.DataFrameGroupByGroupBy.cumsum</span>, <span class="title-ref">.DataFrameGroupByGroupBy.prod</span>, <span class="title-ref">.DataFrameGroupByGroupBy.cumprod</span> with <span class="title-ref">PeriodDtype</span> failing to raise `TypeError` (`51040`)
  - Bug in parsing empty string with <span class="title-ref">Period</span> incorrectly raising `ValueError` instead of returning `NaT` (`51349`)

### Plotting

  - Bug in <span class="title-ref">DataFrame.plot.hist</span>, not dropping elements of `weights` corresponding to `NaN` values in `data` (`48884`)
  - `ax.set_xlim` was sometimes raising `UserWarning` which users couldn't address due to `set_xlim` not accepting parsing arguments - the converter now uses <span class="title-ref">Timestamp</span> instead (`49148`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">.ExponentialMovingWindow</span> with `online` not raising a `NotImplementedError` for unsupported operations (`48834`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.sample</span> raises `ValueError` when the object is empty (`48459`)
  - Bug in <span class="title-ref">Series.groupby</span> raises `ValueError` when an entry of the index is equal to the name of the index (`48567`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.resample</span> produces inconsistent results when passing empty DataFrame (`47705`)
  - Bug in <span class="title-ref">.DataFrameGroupBy</span> and <span class="title-ref">.SeriesGroupBy</span> would not include unobserved categories in result when grouping by categorical indexes (`49354`)
  - Bug in <span class="title-ref">.DataFrameGroupBy</span> and <span class="title-ref">.SeriesGroupBy</span> would change result order depending on the input index when grouping by categoricals (`49223`)
  - Bug in <span class="title-ref">.DataFrameGroupBy</span> and <span class="title-ref">.SeriesGroupBy</span> when grouping on categorical data would sort result values even when used with `sort=False` (`42482`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.SeriesGroupBy.apply</span> with `as_index=False` would not attempt the computation without using the grouping keys when using them failed with a `TypeError` (`49256`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.describe</span> would describe the group keys (`49256`)
  - Bug in <span class="title-ref">.SeriesGroupBy.describe</span> with `as_index=False` would have the incorrect shape (`49256`)
  - Bug in <span class="title-ref">.DataFrameGroupBy</span> and <span class="title-ref">.SeriesGroupBy</span> with `dropna=False` would drop NA values when the grouper was categorical (`36327`)
  - Bug in <span class="title-ref">.SeriesGroupBy.nunique</span> would incorrectly raise when the grouper was an empty categorical and `observed=True` (`21334`)
  - Bug in <span class="title-ref">.SeriesGroupBy.nth</span> would raise when grouper contained NA values after subsetting from a <span class="title-ref">DataFrameGroupBy</span> (`26454`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> would not include a <span class="title-ref">.Grouper</span> specified by `key` in the result when `as_index=False` (`50413`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.value\_counts</span> would raise when used with a <span class="title-ref">.TimeGrouper</span> (`50486`)
  - Bug in <span class="title-ref">.Resampler.size</span> caused a wide <span class="title-ref">DataFrame</span> to be returned instead of a <span class="title-ref">Series</span> with <span class="title-ref">MultiIndex</span> (`46826`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> would raise incorrectly when grouper had `axis=1` for `"idxmin"` and `"idxmax"` arguments (`45986`)
  - Bug in <span class="title-ref">.DataFrameGroupBy</span> would raise when used with an empty DataFrame, categorical grouper, and `dropna=False` (`50634`)
  - Bug in <span class="title-ref">.SeriesGroupBy.value\_counts</span> did not respect `sort=False` (`50482`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.resample</span> raises `KeyError` when getting the result from a key list when resampling on time index (`50840`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> would raise incorrectly when grouper had `axis=1` for `"ngroup"` argument (`45986`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.describe</span> produced incorrect results when data had duplicate columns (`50806`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.agg</span> with `engine="numba"` failing to respect `as_index=False` (`51228`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.agg</span>, <span class="title-ref">.SeriesGroupBy.agg</span>, and <span class="title-ref">.Resampler.agg</span> would ignore arguments when passed a list of functions (`50863`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.ohlc</span> ignoring `as_index=False` (`51413`)
  - Bug in <span class="title-ref">DataFrameGroupBy.agg</span> after subsetting columns (e.g. `.groupby(...)[["a", "b"]]`) would not include groupings in the result (`51186`)

### Reshaping

  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> raising `TypeError` for nullable dtype and `margins=True` (`48681`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> and <span class="title-ref">Series.unstack</span> unstacking wrong level of <span class="title-ref">MultiIndex</span> when <span class="title-ref">MultiIndex</span> has mixed names (`48763`)
  - Bug in <span class="title-ref">DataFrame.melt</span> losing extension array dtype (`41570`)
  - Bug in <span class="title-ref">DataFrame.pivot</span> not respecting `None` as column name (`48293`)
  - Bug in <span class="title-ref">DataFrame.join</span> when `left_on` or `right_on` is or includes a <span class="title-ref">CategoricalIndex</span> incorrectly raising `AttributeError` (`48464`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> raising `ValueError` with parameter `margins=True` when result is an empty <span class="title-ref">DataFrame</span> (`49240`)
  - Clarified error message in <span class="title-ref">merge</span> when passing invalid `validate` option (`49417`)
  - Bug in <span class="title-ref">DataFrame.explode</span> raising `ValueError` on multiple columns with `NaN` values or empty lists (`46084`)
  - Bug in <span class="title-ref">DataFrame.transpose</span> with `IntervalDtype` column with `timedelta64[ns]` endpoints (`44917`)
  - Bug in <span class="title-ref">DataFrame.agg</span> and <span class="title-ref">Series.agg</span> would ignore arguments when passed a list of functions (`50863`)

### Sparse

  - Bug in <span class="title-ref">Series.astype</span> when converting a `SparseDtype` with `datetime64[ns]` subtype to `int64` dtype raising, inconsistent with the non-sparse behavior (`49631`,`50087`)
  - Bug in <span class="title-ref">Series.astype</span> when converting a from `datetime64[ns]` to `Sparse[datetime64[ns]]` incorrectly raising (`50082`)
  - Bug in <span class="title-ref">Series.sparse.to\_coo</span> raising `SystemError` when <span class="title-ref">MultiIndex</span> contains a `ExtensionArray` (`50996`)

### ExtensionArray

  - Bug in <span class="title-ref">Series.mean</span> overflowing unnecessarily with nullable integers (`48378`)
  - Bug in <span class="title-ref">Series.tolist</span> for nullable dtypes returning numpy scalars instead of python scalars (`49890`)
  - Bug in <span class="title-ref">Series.round</span> for pyarrow-backed dtypes raising `AttributeError` (`50437`)
  - Bug when concatenating an empty DataFrame with an ExtensionDtype to another DataFrame with the same ExtensionDtype, the resulting dtype turned into object (`48510`)
  - Bug in <span class="title-ref">array.PandasArray.to\_numpy</span> raising with `NA` value when `na_value` is specified (`40638`)
  - Bug in <span class="title-ref">api.types.is\_numeric\_dtype</span> where a custom <span class="title-ref">ExtensionDtype</span> would not return `True` if `_is_numeric` returned `True` (`50563`)
  - Bug in <span class="title-ref">api.types.is\_integer\_dtype</span>, <span class="title-ref">api.types.is\_unsigned\_integer\_dtype</span>, <span class="title-ref">api.types.is\_signed\_integer\_dtype</span>, <span class="title-ref">api.types.is\_float\_dtype</span> where a custom <span class="title-ref">ExtensionDtype</span> would not return `True` if `kind` returned the corresponding NumPy type (`50667`)
  - Bug in <span class="title-ref">Series</span> constructor unnecessarily overflowing for nullable unsigned integer dtypes (`38798`, `25880`)
  - Bug in setting non-string value into `StringArray` raising `ValueError` instead of `TypeError` (`49632`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> not honoring the default `copy=True` keyword in case of columns with ExtensionDtype (and as a result also selecting multiple columns with getitem (`[]`) didn't correctly result in a copy) (`51197`)
  - Bug in <span class="title-ref">\~arrays.ArrowExtensionArray</span> logical operations `&` and `|` raising `KeyError` (`51688`)

### Styler

  - Fix <span class="title-ref">\~pandas.io.formats.style.Styler.background\_gradient</span> for nullable dtype <span class="title-ref">Series</span> with `NA` values (`50712`)

### Metadata

  - Fixed metadata propagation in <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">DataFrame.cov</span> (`28283`)

### Other

  - Bug in incorrectly accepting dtype strings containing "\[pyarrow\]" more than once (`51548`)
  - Bug in <span class="title-ref">Series.searchsorted</span> inconsistent behavior when accepting <span class="title-ref">DataFrame</span> as parameter `value` (`49620`)
  - Bug in <span class="title-ref">array</span> failing to raise on <span class="title-ref">DataFrame</span> inputs (`51167`)

## Contributors

<div class="contributors">

v1.5.0rc0..v2.0.0

</div>

---

v2.0.1.md

---

# What's new in 2.0.1 (April 24, 2023)

These are the changes in pandas 2.0.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression for subclassed Series when constructing from a dictionary (`52445`)
  - Fixed regression in <span class="title-ref">.SeriesGroupBy.agg</span> failing when grouping with categorical data, multiple groupings, `as_index=False`, and a list of aggregations (`52760`)
  - Fixed regression in <span class="title-ref">DataFrame.pivot</span> changing <span class="title-ref">Index</span> name of input object (`52629`)
  - Fixed regression in <span class="title-ref">DataFrame.resample</span> raising on a DataFrame with no columns (`52484`)
  - Fixed regression in <span class="title-ref">DataFrame.sort\_values</span> not resetting index when <span class="title-ref">DataFrame</span> is already sorted and `ignore_index=True` (`52553`)
  - Fixed regression in <span class="title-ref">MultiIndex.isin</span> raising `TypeError` for `Generator` (`52568`)
  - Fixed regression in <span class="title-ref">Series.describe</span> showing `RuntimeWarning` for extension dtype <span class="title-ref">Series</span> with one element (`52515`)
  - Fixed regression when adding a new column to a <span class="title-ref">DataFrame</span> when the <span class="title-ref">DataFrame.columns</span> was a <span class="title-ref">RangeIndex</span> and the new key was hashable but not a scalar (`52652`)

## Bug fixes

  - Bug in <span class="title-ref">Series.dt.days</span> that would overflow `int32` number of days (`52391`)
  - Bug in <span class="title-ref">arrays.DatetimeArray</span> constructor returning an incorrect unit when passed a non-nanosecond numpy datetime array (`52555`)
  - Bug in <span class="title-ref">\~arrays.ArrowExtensionArray</span> with duration dtype overflowing when constructed from data containing numpy `NaT` (`52843`)
  - Bug in <span class="title-ref">Series.dt.round</span> when passing a `freq` of equal or higher resolution compared to the <span class="title-ref">Series</span> would raise a `ZeroDivisionError` (`52761`)
  - Bug in <span class="title-ref">Series.median</span> with <span class="title-ref">ArrowDtype</span> returning an approximate median (`52679`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was unnecessarily raising on categorical dtypes (`49889`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was unnecessarily raising on large string dtypes (`52795`)
  - Bug in <span class="title-ref">pandas.testing.assert\_series\_equal</span> where `check_dtype=False` would still raise for datetime or timedelta types with different resolutions (`52449`)
  - Bug in <span class="title-ref">read\_csv</span> casting PyArrow datetimes to NumPy when `dtype_backend="pyarrow"` and `parse_dates` is set causing a performance bottleneck in the process (`52546`)
  - Bug in <span class="title-ref">to\_datetime</span> and <span class="title-ref">to\_timedelta</span> when trying to convert numeric data with a <span class="title-ref">ArrowDtype</span> (`52425`)
  - Bug in <span class="title-ref">to\_numeric</span> with `errors='coerce'` and `dtype_backend='pyarrow'` with <span class="title-ref">ArrowDtype</span> data (`52588`)
  - Bug in <span class="title-ref">ArrowDtype.\_\_from\_arrow\_\_</span> not respecting if dtype is explicitly given (`52533`)
  - Bug in <span class="title-ref">DataFrame.describe</span> not respecting `ArrowDtype` in `include` and `exclude` (`52570`)
  - Bug in <span class="title-ref">DataFrame.max</span> and related casting different <span class="title-ref">Timestamp</span> resolutions always to nanoseconds (`52524`)
  - Bug in <span class="title-ref">Series.describe</span> not returning <span class="title-ref">ArrowDtype</span> with `pyarrow.float64` type with numeric data (`52427`)
  - Bug in <span class="title-ref">Series.dt.tz\_localize</span> incorrectly localizing timestamps with <span class="title-ref">ArrowDtype</span> (`52677`)
  - Bug in arithmetic between `np.datetime64` and `np.timedelta64` `NaT` scalars with units always returning nanosecond resolution (`52295`)
  - Bug in logical and comparison operations between <span class="title-ref">ArrowDtype</span> and numpy masked types (e.g. `"boolean"`) (`52625`)
  - Fixed bug in <span class="title-ref">merge</span> when merging with `ArrowDtype` one one and a NumPy dtype on the other side (`52406`)
  - Fixed segfault in <span class="title-ref">Series.to\_numpy</span> with `null[pyarrow]` dtype (`52443`)

## Other

  - <span class="title-ref">DataFrame</span> created from empty dicts had <span class="title-ref">\~DataFrame.columns</span> of dtype `object`. It is now a <span class="title-ref">RangeIndex</span> (`52404`)
  - <span class="title-ref">Series</span> created from empty dicts had <span class="title-ref">\~Series.index</span> of dtype `object`. It is now a <span class="title-ref">RangeIndex</span> (`52404`)
  - Implemented <span class="title-ref">Series.str.split</span> and <span class="title-ref">Series.str.rsplit</span> for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` (`52401`)
  - Implemented most `str` accessor methods for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` (`52401`)
  - Supplying a non-integer hashable key that tests `False` in <span class="title-ref">api.types.is\_scalar</span> now raises a `KeyError` for <span class="title-ref">RangeIndex.get\_loc</span>, like it does for <span class="title-ref">Index.get\_loc</span>. Previously it raised an `InvalidIndexError` (`52652`).

## Contributors

<div class="contributors">

v2.0.0..v2.0.1

</div>

---

v2.0.2.md

---

# What's new in 2.0.2 (May 29, 2023)

These are the changes in pandas 2.0.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed performance regression in <span class="title-ref">GroupBy.apply</span> (`53195`)
  - Fixed regression in <span class="title-ref">merge</span> on Windows when dtype is `np.intc` (`52451`)
  - Fixed regression in <span class="title-ref">read\_sql</span> dropping columns with duplicated column names (`53117`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> losing <span class="title-ref">MultiIndex</span> name when enlarging object (`53053`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_string</span> printing a backslash at the end of the first row of data, instead of headers, when the DataFrame doesn't fit the line width (`53054`)
  - Fixed regression in <span class="title-ref">MultiIndex.join</span> returning levels in wrong order (`53093`)

## Bug fixes

  - Bug in <span class="title-ref">.arrays.ArrowExtensionArray</span> incorrectly assigning `dict` instead of `list` for `.type` with `pyarrow.map_` and raising a `NotImplementedError` with `pyarrow.struct` (`53328`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was raising `IndexError` on empty categorical data (`53077`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was returning <span class="title-ref">DataFrame</span>'s of incorrect sizes when called on slices (`52824`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was unnecessarily raising on bitmasks (`49888`)
  - Bug in <span class="title-ref">merge</span> when merging on datetime columns on different resolutions (`53200`)
  - Bug in <span class="title-ref">read\_csv</span> raising `OverflowError` for `engine="pyarrow"` and `parse_dates` set (`53295`)
  - Bug in <span class="title-ref">to\_datetime</span> was inferring format to contain `"%H"` instead of `"%I"` if date contained "AM" / "PM" tokens (`53147`)
  - Bug in <span class="title-ref">to\_timedelta</span> was raising `ValueError` with `pandas.NA` (`52909`)
  - Bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> not preserving dtypes for <span class="title-ref">MultiIndex</span> partial keys (`51895`)
  - Bug in <span class="title-ref">DataFrame.convert\_dtypes</span> ignores `convert_*` keywords when set to False `dtype_backend="pyarrow"` (`52872`)
  - Bug in <span class="title-ref">DataFrame.convert\_dtypes</span> losing timezone for tz-aware dtypes and `dtype_backend="pyarrow"` (`53382`)
  - Bug in <span class="title-ref">DataFrame.sort\_values</span> raising for PyArrow `dictionary` dtype (`53232`)
  - Bug in <span class="title-ref">Series.describe</span> treating pyarrow-backed timestamps and timedeltas as categorical data (`53001`)
  - Bug in <span class="title-ref">Series.rename</span> not making a lazy copy when Copy-on-Write is enabled when a scalar is passed to it (`52450`)
  - Bug in <span class="title-ref">pd.array</span> raising for `NumPy` array and `pa.large_string` or `pa.large_binary` (`52590`)

## Other

  - Raised a better error message when calling <span class="title-ref">Series.dt.to\_pydatetime</span> with <span class="title-ref">ArrowDtype</span> with `pyarrow.date32` or `pyarrow.date64` type (`52812`)

## Contributors

<div class="contributors">

v2.0.1..v2.0.2

</div>

---

v2.0.3.md

---

# What's new in 2.0.3 (June 28, 2023)

These are the changes in pandas 2.0.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Bug in <span class="title-ref">Timestamp.weekday</span><span class="title-ref"> was returning incorrect results before </span><span class="title-ref">'0000-02-29'</span><span class="title-ref"> (:issue:\`53738</span>)
  - Fixed performance regression in merging on datetime-like columns (`53231`)
  - Fixed regression when <span class="title-ref">DataFrame.to\_string</span> creates extra space for string dtypes (`52690`)

## Bug fixes

  - Bug in <span class="title-ref">DataFrame.convert\_dtype</span> and <span class="title-ref">Series.convert\_dtype</span> when trying to convert <span class="title-ref">ArrowDtype</span> with `dtype_backend="nullable_numpy"` (`53648`)
  - Bug in <span class="title-ref">RangeIndex.union</span> when using `sort=True` with another <span class="title-ref">RangeIndex</span> (`53490`)
  - Bug in <span class="title-ref">Series.reindex</span> when expanding a non-nanosecond datetime or timedelta <span class="title-ref">Series</span> would not fill with `NaT` correctly (`53497`)
  - Bug in <span class="title-ref">read\_csv</span> when defining `dtype` with `bool[pyarrow]` for the `"c"` and `"python"` engines (`53390`)
  - Bug in <span class="title-ref">Series.str.split</span> and <span class="title-ref">Series.str.rsplit</span> with `expand=True` for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` (`53532`)
  - Bug in indexing methods (e.g. <span class="title-ref">DataFrame.\_\_getitem\_\_</span>) where taking the entire <span class="title-ref">DataFrame</span>/<span class="title-ref">Series</span> would raise an `OverflowError` when Copy on Write was enabled and the length of the array was over the maximum size a 32-bit integer can hold (`53616`)
  - Bug when constructing a <span class="title-ref">DataFrame</span> with columns of an <span class="title-ref">ArrowDtype</span> with a `pyarrow.dictionary` type that reindexes the data (`53617`)
  - Bug when indexing a <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> with an <span class="title-ref">Index</span> with a timestamp <span class="title-ref">ArrowDtype</span> would raise an `AttributeError` (`53644`)

## Other

## Contributors

<div class="contributors">

v2.0.2..v2.0.3

</div>

---

v2.1.0.md

---

# What's new in 2.1.0 (Aug 30, 2023)

These are the changes in pandas 2.1.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### PyArrow will become a required dependency with pandas 3.0

[PyArrow](https://arrow.apache.org/docs/python/index.html) will become a required dependency of pandas starting with pandas 3.0. This decision was made based on [PDEP 10](https://pandas.pydata.org/pdeps/0010-required-pyarrow-dependency.html).

This will enable more changes that are hugely beneficial to pandas users, including but not limited to:

  - inferring strings as PyArrow backed strings by default enabling a significant reduction of the memory footprint and huge performance improvements.
  - inferring more complex dtypes with PyArrow by default, like `Decimal`, `lists`, `bytes`, `structured data` and more.
  - Better interoperability with other libraries that depend on Apache Arrow.

We are collecting feedback on this decision [here](https://github.com/pandas-dev/pandas/issues/54466).

### Avoid NumPy object dtype for strings by default

Previously, all strings were stored in columns with NumPy object dtype by default. This release introduces an option `future.infer_string` that infers all strings as PyArrow backed strings with dtype `"string[pyarrow_numpy]"` instead. This is a new string dtype implementation that follows NumPy semantics in comparison operations and will return `np.nan` as the missing value indicator. Setting the option will also infer the dtype `"string"` as a <span class="title-ref">StringDtype</span> with storage set to `"pyarrow_numpy"`, ignoring the value behind the option `mode.string_storage`.

This option only works if PyArrow is installed. PyArrow backed strings have a significantly reduced memory footprint and provide a big performance improvement compared to NumPy object (`54430`).

The option can be enabled with:

`` `python     pd.options.future.infer_string = True  This behavior will become the default with pandas 3.0.  .. _whatsnew_210.enhancements.reduction_extension_dtypes:  DataFrame reductions preserve extension dtypes ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions of pandas, the results of DataFrame reductions (<span class="title-ref">DataFrame.sum</span> <span class="title-ref">DataFrame.mean</span> etc.) had NumPy dtypes, even when the DataFrames were of extension dtypes. pandas can now keep the dtypes when doing reductions over DataFrame columns with a common dtype (`52788`).

*Old Behavior*

`` `ipython     In [1]: df = pd.DataFrame({"a": [1, 1, 2, 1], "b": [np.nan, 2.0, 3.0, 4.0]}, dtype="Int64")     In [2]: df.sum()     Out[2]:     a    5     b    9     dtype: int64     In [3]: df = df.astype("int64[pyarrow]")     In [4]: df.sum()     Out[4]:     a    5     b    9     dtype: int64  *New Behavior*  .. ipython:: python      df = pd.DataFrame({"a": [1, 1, 2, 1], "b": [np.nan, 2.0, 3.0, 4.0]}, dtype="Int64")     df.sum()     df = df.astype("int64[pyarrow]")     df.sum()  Notice that the dtype is now a masked dtype and PyArrow dtype, respectively, while previously it was a NumPy integer dtype.  To allow DataFrame reductions to preserve extension dtypes, `.ExtensionArray._reduce` has gotten a new keyword parameter ``keepdims``. Calling `.ExtensionArray._reduce` with``keepdims=True`should return an array of length 1 along the reduction axis. In order to maintain backward compatibility, the parameter is not required, but will it become required in the future. If the parameter is not found in the signature, DataFrame reductions can not preserve extension dtypes. Also, if the parameter is not found, a`FutureWarning``will be emitted and type checkers like mypy may complain about the signature not being compatible with `.ExtensionArray._reduce`.  .. _whatsnew_210.enhancements.cow:  Copy-on-Write improvements``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^

  - <span class="title-ref">Series.transform</span> not respecting Copy-on-Write when `func` modifies <span class="title-ref">Series</span> inplace (`53747`)
  - Calling <span class="title-ref">Index.values</span> will now return a read-only NumPy array (`53704`)
  - Setting a <span class="title-ref">Series</span> into a <span class="title-ref">DataFrame</span> now creates a lazy instead of a deep copy (`53142`)
  - The <span class="title-ref">DataFrame</span> constructor, when constructing a DataFrame from a dictionary of Index objects and specifying `copy=False`, will now use a lazy copy of those Index objects for the columns of the DataFrame (`52947`)
  - A shallow copy of a Series or DataFrame (`df.copy(deep=False)`) will now also return a shallow copy of the rows/columns <span class="title-ref">Index</span> objects instead of only a shallow copy of the data, i.e. the index of the result is no longer identical (`df.copy(deep=False).index is df.index` is no longer True) (`53721`)
  - <span class="title-ref">DataFrame.head</span> and <span class="title-ref">DataFrame.tail</span> will now return deep copies (`54011`)
  - Add lazy copy mechanism to <span class="title-ref">DataFrame.eval</span> (`53746`)
  - Trying to operate inplace on a temporary column selection (for example, `df["a"].fillna(100, inplace=True)`) will now always raise a warning when Copy-on-Write is enabled. In this mode, operating inplace like this will never work, since the selection behaves as a temporary copy. This holds true for:
      - DataFrame.update / Series.update
      - DataFrame.fillna / Series.fillna
      - DataFrame.replace / Series.replace
      - DataFrame.clip / Series.clip
      - DataFrame.where / Series.where
      - DataFrame.mask / Series.mask
      - DataFrame.interpolate / Series.interpolate
      - DataFrame.ffill / Series.ffill
      - DataFrame.bfill / Series.bfill

### New <span class="title-ref">DataFrame.map</span> method and support for ExtensionArrays

The <span class="title-ref">DataFrame.map</span> been added and <span class="title-ref">DataFrame.applymap</span> has been deprecated. <span class="title-ref">DataFrame.map</span> has the same functionality as <span class="title-ref">DataFrame.applymap</span>, but the new name better communicates that this is the <span class="title-ref">DataFrame</span> version of <span class="title-ref">Series.map</span> (`52353`).

When given a callable, <span class="title-ref">Series.map</span> applies the callable to all elements of the <span class="title-ref">Series</span>. Similarly, <span class="title-ref">DataFrame.map</span> applies the callable to all elements of the <span class="title-ref">DataFrame</span>, while <span class="title-ref">Index.map</span> applies the callable to all elements of the <span class="title-ref">Index</span>.

Frequently, it is not desirable to apply the callable to nan-like values of the array and to avoid doing that, the `map` method could be called with `na_action="ignore"`, i.e. `ser.map(func, na_action="ignore")`. However, `na_action="ignore"` was not implemented for many <span class="title-ref">.ExtensionArray</span> and `Index` types and `na_action="ignore"` did not work correctly for any <span class="title-ref">.ExtensionArray</span> subclass except the nullable numeric ones (i.e. with dtype <span class="title-ref">Int64</span> etc.).

`na_action="ignore"` now works for all array types (`52219`, `51645`, `51809`, `51936`, `52033`; `52096`).

*Previous behavior*:

`` `ipython     In [1]: ser = pd.Series(["a", "b", np.nan], dtype="category")     In [2]: ser.map(str.upper, na_action="ignore")     NotImplementedError     In [3]: df = pd.DataFrame(ser)     In [4]: df.applymap(str.upper, na_action="ignore")  # worked for DataFrame          0     0    A     1    B     2  NaN     In [5]: idx = pd.Index(ser)     In [6]: idx.map(str.upper, na_action="ignore")     TypeError: CategoricalIndex.map() got an unexpected keyword argument 'na_action'  *New behavior*:  .. ipython:: python      ser = pd.Series(["a", "b", np.nan], dtype="category")     ser.map(str.upper, na_action="ignore")     df = pd.DataFrame(ser)     df.map(str.upper, na_action="ignore")     idx = pd.Index(ser)     idx.map(str.upper, na_action="ignore")  Also, note that `Categorical.map` implicitly has had its ``na\_action`set to`"ignore"`by default.`<span class="title-ref"> This has been deprecated and the default for \`Categorical.map</span> will change to `na_action=None`, consistent with all the other array types.

### New implementation of <span class="title-ref">DataFrame.stack</span>

pandas has reimplemented <span class="title-ref">DataFrame.stack</span>. To use the new implementation, pass the argument `future_stack=True`. This will become the only option in pandas 3.0.

The previous implementation had two main behavioral downsides.

1.  The previous implementation would unnecessarily introduce NA values into the result. The user could have NA values automatically removed by passing `dropna=True` (the default), but doing this could also remove NA values from the result that existed in the input. See the examples below.
2.  The previous implementation with `sort=True` (the default) would sometimes sort part of the resulting index, and sometimes not. If the input's columns are *not* a <span class="title-ref">MultiIndex</span>, then the resulting index would never be sorted. If the columns are a <span class="title-ref">MultiIndex</span>, then in most cases the level(s) in the resulting index that come from stacking the column level(s) would be sorted. In rare cases such level(s) would be sorted in a non-standard order, depending on how the columns were created.

The new implementation (`future_stack=True`) will no longer unnecessarily introduce NA values when stacking multiple levels and will never sort. As such, the arguments `dropna` and `sort` are not utilized and must remain unspecified when using `future_stack=True`. These arguments will be removed in the next major release.

<div class="ipython">

python

columns = pd.MultiIndex.from\_tuples(\[("B", "d"), ("A", "c")\]) df = pd.DataFrame(\[\[0, 2\], \[1, 3\]\], index=\["z", "y"\], columns=columns) df

</div>

In the previous version (`future_stack=False`), the default of `dropna=True` would remove unnecessarily introduced NA values but still coerce the dtype to `float64` in the process. In the new version, no NAs are introduced and so there is no coercion of the dtype.

<div class="ipython" data-okwarning="">

python

df.stack(\[0, 1\], future\_stack=False, dropna=True) df.stack(\[0, 1\], future\_stack=True)

</div>

If the input contains NA values, the previous version would drop those as well with `dropna=True` or introduce new NA values with `dropna=False`. The new version persists all values from the input.

<div class="ipython" data-okwarning="">

python

df = pd.DataFrame(\[\[0, 2\], \[np.nan, np.nan\]\], columns=columns) df df.stack(\[0, 1\], future\_stack=False, dropna=True) df.stack(\[0, 1\], future\_stack=False, dropna=False) df.stack(\[0, 1\], future\_stack=True)

</div>

### Other enhancements

  - <span class="title-ref">Series.ffill</span> and <span class="title-ref">Series.bfill</span> are now supported for objects with <span class="title-ref">IntervalDtype</span> (`54247`)
  - Added `filters` parameter to <span class="title-ref">read\_parquet</span> to filter out data, compatible with both `engines` (`53212`)
  - <span class="title-ref">.Categorical.map</span> and <span class="title-ref">CategoricalIndex.map</span> now have a `na_action` parameter. <span class="title-ref">.Categorical.map</span> implicitly had a default value of `"ignore"` for `na_action`. This has formally been deprecated and will be changed to `None` in the future. Also notice that <span class="title-ref">Series.map</span> has default `na_action=None` and calls to series with categorical data will now use `na_action=None` unless explicitly set otherwise (`44279`)
  - <span class="title-ref">api.extensions.ExtensionArray</span> now has a <span class="title-ref">\~api.extensions.ExtensionArray.map</span> method (`51809`)
  - <span class="title-ref">DataFrame.applymap</span> now uses the <span class="title-ref">\~api.extensions.ExtensionArray.map</span> method of underlying <span class="title-ref">api.extensions.ExtensionArray</span> instances (`52219`)
  - <span class="title-ref">MultiIndex.sort\_values</span> now supports `na_position` (`51612`)
  - <span class="title-ref">MultiIndex.sortlevel</span> and <span class="title-ref">Index.sortlevel</span> gained a new keyword `na_position` (`51612`)
  - <span class="title-ref">arrays.DatetimeArray.map</span>, <span class="title-ref">arrays.TimedeltaArray.map</span> and <span class="title-ref">arrays.PeriodArray.map</span> can now take a `na_action` argument (`51644`)
  - <span class="title-ref">arrays.SparseArray.map</span> now supports `na_action` (`52096`).
  - <span class="title-ref">pandas.read\_html</span> now supports the `storage_options` keyword when used with a URL, allowing users to add headers to the outbound HTTP request (`49944`)
  - Add <span class="title-ref">Index.diff</span> and <span class="title-ref">Index.round</span> (`19708`)
  - Add `"latex-math"` as an option to the `escape` argument of <span class="title-ref">.Styler</span> which will not escape all characters between `"\("` and `"\)"` during formatting (`51903`)
  - Add dtype of categories to `repr` information of <span class="title-ref">CategoricalDtype</span> (`52179`)
  - Adding `engine_kwargs` parameter to <span class="title-ref">read\_excel</span> (`52214`)
  - Classes that are useful for type-hinting have been added to the public API in the new submodule `pandas.api.typing` (`48577`)
  - Implemented <span class="title-ref">Series.dt.is\_month\_start</span>, <span class="title-ref">Series.dt.is\_month\_end</span>, <span class="title-ref">Series.dt.is\_year\_start</span>, <span class="title-ref">Series.dt.is\_year\_end</span>, <span class="title-ref">Series.dt.is\_quarter\_start</span>, <span class="title-ref">Series.dt.is\_quarter\_end</span>, <span class="title-ref">Series.dt.days\_in\_month</span>, <span class="title-ref">Series.dt.unit</span>, <span class="title-ref">Series.dt.normalize</span>, <span class="title-ref">Series.dt.day\_name</span>, <span class="title-ref">Series.dt.month\_name</span>, <span class="title-ref">Series.dt.tz\_convert</span> for <span class="title-ref">ArrowDtype</span> with `pyarrow.timestamp` (`52388`, `51718`)
  - <span class="title-ref">.DataFrameGroupBy.agg</span> and <span class="title-ref">.DataFrameGroupBy.transform</span> now support grouping by multiple keys when the index is not a <span class="title-ref">MultiIndex</span> for `engine="numba"` (`53486`)
  - <span class="title-ref">.SeriesGroupBy.agg</span> and <span class="title-ref">.DataFrameGroupBy.agg</span> now support passing in multiple functions for `engine="numba"` (`53486`)
  - <span class="title-ref">.SeriesGroupBy.transform</span> and <span class="title-ref">.DataFrameGroupBy.transform</span> now support passing in a string as the function for `engine="numba"` (`53579`)
  - <span class="title-ref">DataFrame.stack</span> gained the `sort` keyword to dictate whether the resulting <span class="title-ref">MultiIndex</span> levels are sorted (`15105`)
  - <span class="title-ref">DataFrame.unstack</span> gained the `sort` keyword to dictate whether the resulting <span class="title-ref">MultiIndex</span> levels are sorted (`15105`)
  - <span class="title-ref">Series.explode</span> now supports PyArrow-backed list types (`53602`)
  - <span class="title-ref">Series.str.join</span> now supports `ArrowDtype(pa.string())` (`53646`)
  - Add `validate` parameter to <span class="title-ref">Categorical.from\_codes</span> (`50975`)
  - Added <span class="title-ref">.ExtensionArray.interpolate</span> used by <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> (`53659`)
  - Added `engine_kwargs` parameter to <span class="title-ref">DataFrame.to\_excel</span> (`53220`)
  - Implemented <span class="title-ref">api.interchange.from\_dataframe</span> for <span class="title-ref">DatetimeTZDtype</span> (`54239`)
  - Implemented `__from_arrow__` on <span class="title-ref">DatetimeTZDtype</span> (`52201`)
  - Implemented `__pandas_priority__` to allow custom types to take precedence over <span class="title-ref">DataFrame</span>, <span class="title-ref">Series</span>, <span class="title-ref">Index</span>, or <span class="title-ref">.ExtensionArray</span> for arithmetic operations, \[see the developer guide \<extending.pandas\_priority\>\](\#see-the-developer-guide-\<extending.pandas\_priority\>) (`48347`)
  - Improve error message when having incompatible columns using <span class="title-ref">DataFrame.merge</span> (`51861`)
  - Improve error message when setting <span class="title-ref">DataFrame</span> with wrong number of columns through <span class="title-ref">DataFrame.isetitem</span> (`51701`)
  - Improved error handling when using <span class="title-ref">DataFrame.to\_json</span> with incompatible `index` and `orient` arguments (`52143`)
  - Improved error message when creating a DataFrame with empty data (0 rows), no index and an incorrect number of columns (`52084`)
  - Improved error message when providing an invalid `index` or `offset` argument to <span class="title-ref">.VariableOffsetWindowIndexer</span> (`54379`)
  - Let <span class="title-ref">DataFrame.to\_feather</span> accept a non-default <span class="title-ref">Index</span> and non-string column names (`51787`)
  - Added a new parameter `by_row` to <span class="title-ref">Series.apply</span> and <span class="title-ref">DataFrame.apply</span>. When set to `False` the supplied callables will always operate on the whole Series or DataFrame (`53400`, `53601`).
  - <span class="title-ref">DataFrame.shift</span> and <span class="title-ref">Series.shift</span> now allow shifting by multiple periods by supplying a list of periods (`44424`)
  - Groupby aggregations with `numba` (such as <span class="title-ref">.DataFrameGroupBy.sum</span>) now can preserve the dtype of the input instead of casting to `float64` (`44952`)
  - Improved error message when <span class="title-ref">.DataFrameGroupBy.agg</span> failed (`52930`)
  - Many read/[to]()\* functions, such as <span class="title-ref">DataFrame.to\_pickle</span> and <span class="title-ref">read\_csv</span>, support forwarding compression arguments to `lzma.LZMAFile` (`52979`)
  - Reductions <span class="title-ref">Series.argmax</span>, <span class="title-ref">Series.argmin</span>, <span class="title-ref">Series.idxmax</span>, <span class="title-ref">Series.idxmin</span>, <span class="title-ref">Index.argmax</span>, <span class="title-ref">Index.argmin</span>, <span class="title-ref">DataFrame.idxmax</span>, <span class="title-ref">DataFrame.idxmin</span> are now supported for object-dtype (`4279`, `18021`, `40685`, `43697`)
  - <span class="title-ref">DataFrame.to\_parquet</span> and <span class="title-ref">read\_parquet</span> will now write and read `attrs` respectively (`54346`)
  - <span class="title-ref">Index.all</span> and <span class="title-ref">Index.any</span> with floating dtypes and timedelta64 dtypes no longer raise `TypeError`, matching the <span class="title-ref">Series.all</span> and <span class="title-ref">Series.any</span> behavior (`54566`)
  - <span class="title-ref">Series.cummax</span>, <span class="title-ref">Series.cummin</span> and <span class="title-ref">Series.cumprod</span> are now supported for pyarrow dtypes with pyarrow version 13.0 and above (`52085`)
  - Added support for the DataFrame Consortium Standard (`54383`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> (`51722`)
  - PyArrow-backed integer dtypes now support bitwise operations (`54495`)

## Backwards incompatible API changes

### Increased minimum version for Python

pandas 2.1.0 supports Python 3.9 and higher.

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated. If installed, we now require:

<table style="width:88%;">
<colgroup>
<col style="width: 31%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.22.4</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>mypy (dev)</td>
<td>1.4.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>beautifulsoup4</td>
<td>4.11.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>bottleneck</td>
<td>1.3.4</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>dataframe-api-compat</td>
<td>0.1.7</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>fastparquet</td>
<td>0.8.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>fsspec</td>
<td>2022.05.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>hypothesis</td>
<td>6.46.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>gcsfs</td>
<td>2022.05.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>jinja2</td>
<td>3.1.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>lxml</td>
<td>4.8.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>numba</td>
<td>0.55.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>numexpr</td>
<td>2.8.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>openpyxl</td>
<td>3.0.10</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pandas-gbq</td>
<td>0.17.5</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>psycopg2</td>
<td>2.9.3</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pyreadstat</td>
<td>1.1.5</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pyqt5</td>
<td>5.15.6</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>pytables</td>
<td>3.7.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pytest</td>
<td>7.3.2</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>python-snappy</td>
<td>0.6.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>pyxlsb</td>
<td>1.0.9</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>s3fs</td>
<td>2022.05.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>scipy</td>
<td>1.8.1</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>sqlalchemy</td>
<td>1.4.36</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>tabulate</td>
<td>0.8.10</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>xarray</td>
<td>2022.03.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="even">
<td>xlsxwriter</td>
<td>3.0.3</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>zstandard</td>
<td>0.17.0</td>
<td></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version.

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - <span class="title-ref">arrays.PandasArray</span> has been renamed <span class="title-ref">.NumpyExtensionArray</span> and the attached dtype name changed from `PandasDtype` to `NumpyEADtype`; importing `PandasArray` still works until the next major version (`53694`)

## Deprecations

### Deprecated silent upcasting in setitem-like Series operations

PDEP-6: <https://pandas.pydata.org/pdeps/0006-ban-upcasting.html>

Setitem-like operations on Series (or DataFrame columns) which silently upcast the dtype are deprecated and show a warning. Examples of affected operations are:

  - `ser.fillna('foo', inplace=True)`
  - `ser.where(ser.isna(), 'foo', inplace=True)`
  - `ser.iloc[indexer] = 'foo'`
  - `ser.loc[indexer] = 'foo'`
  - `df.iloc[indexer, 0] = 'foo'`
  - `df.loc[indexer, 'a'] = 'foo'`
  - `ser[indexer] = 'foo'`

where `ser` is a <span class="title-ref">Series</span>, `df` is a <span class="title-ref">DataFrame</span>, and `indexer` could be a slice, a mask, a single value, a list or array of values, or any other allowed indexer.

In a future version, these will raise an error and you should cast to a common dtype first.

*Previous behavior*:

`` `ipython   In [1]: ser = pd.Series([1, 2, 3])    In [2]: ser   Out[2]:   0    1   1    2   2    3   dtype: int64    In [3]: ser[0] = 'not an int64'    In [4]: ser   Out[4]:   0    not an int64   1               2   2               3   dtype: object  *New behavior*:  .. code-block:: ipython    In [1]: ser = pd.Series([1, 2, 3])    In [2]: ser   Out[2]:   0    1   1    2   2    3   dtype: int64    In [3]: ser[0] = 'not an int64'   FutureWarning:     Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas.     Value 'not an int64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.    In [4]: ser   Out[4]:   0    not an int64   1               2   2               3   dtype: object  To retain the current behaviour, in the case above you could cast ``ser`to`object`dtype first:  .. ipython:: python    ser = pd.Series([1, 2, 3])   ser = ser.astype('object')   ser[0] = 'not an int64'   ser  Depending on the use-case, it might be more appropriate to cast to a different dtype.`<span class="title-ref"> In the following, for example, we cast to </span><span class="title-ref">float64</span>\`:

<div class="ipython">

python

ser = pd.Series(\[1, 2, 3\]) ser = ser.astype('float64') ser\[0\] = 1.1 ser

</div>

For further reading, please see <https://pandas.pydata.org/pdeps/0006-ban-upcasting.html>.

### Deprecated parsing datetimes with mixed time zones

Parsing datetimes with mixed time zones is deprecated and shows a warning unless user passes `utc=True` to <span class="title-ref">to\_datetime</span> (`50887`)

*Previous behavior*:

`` `ipython   In [7]: data = ["2020-01-01 00:00:00+06:00", "2020-01-01 00:00:00+01:00"]    In [8]:  pd.to_datetime(data, utc=False)   Out[8]:   Index([2020-01-01 00:00:00+06:00, 2020-01-01 00:00:00+01:00], dtype='object')  *New behavior*:  .. code-block:: ipython    In [9]: pd.to_datetime(data, utc=False)   FutureWarning:     In a future version of pandas, parsing datetimes with mixed time zones will raise     a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour     and silence this warning. To create a `Series` with mixed offsets and `object` dtype,     please use `apply` and `datetime.datetime.strptime`.   Index([2020-01-01 00:00:00+06:00, 2020-01-01 00:00:00+01:00], dtype='object')  In order to silence this warning and avoid an error in a future version of pandas, ``<span class="title-ref"> please specify </span><span class="title-ref">utc=True</span>\`:

<div class="ipython">

python

data = \["2020-01-01 00:00:00+06:00", "2020-01-01 00:00:00+01:00"\] pd.to\_datetime(data, utc=True)

</div>

To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`:

<div class="ipython">

python

import datetime as dt

data = \["2020-01-01 00:00:00+06:00", "2020-01-01 00:00:00+01:00"\] pd.Series(data).apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S%z'))

</div>

### Other Deprecations

  - Deprecated <span class="title-ref">.DataFrameGroupBy.dtypes</span>, check `dtypes` on the underlying object instead (`51045`)
  - Deprecated <span class="title-ref">DataFrame.\_data</span> and <span class="title-ref">Series.\_data</span>, use public APIs instead (`33333`)
  - Deprecated <span class="title-ref">concat</span> behavior when any of the objects being concatenated have length 0; in the past the dtypes of empty objects were ignored when determining the resulting dtype, in a future version they will not (`39122`)
  - Deprecated <span class="title-ref">.Categorical.to\_list</span>, use `obj.tolist()` instead (`51254`)
  - Deprecated <span class="title-ref">.DataFrameGroupBy.all</span> and <span class="title-ref">.DataFrameGroupBy.any</span> with datetime64 or <span class="title-ref">PeriodDtype</span> values, matching the <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> deprecations (`34479`)
  - Deprecated `axis=1` in <span class="title-ref">DataFrame.ewm</span>, <span class="title-ref">DataFrame.rolling</span>, <span class="title-ref">DataFrame.expanding</span>, transpose before calling the method instead (`51778`)
  - Deprecated `axis=1` in <span class="title-ref">DataFrame.groupby</span> and in <span class="title-ref">Grouper</span> constructor, do `frame.T.groupby(...)` instead (`51203`)
  - Deprecated `broadcast_axis` keyword in <span class="title-ref">Series.align</span> and <span class="title-ref">DataFrame.align</span>, upcast before calling `align` with `left = DataFrame({col: left for col in right.columns}, index=right.index)` (`51856`)
  - Deprecated `downcast` keyword in <span class="title-ref">Index.fillna</span> (`53956`)
  - Deprecated `fill_method` and `limit` keywords in <span class="title-ref">DataFrame.pct\_change</span>, <span class="title-ref">Series.pct\_change</span>, <span class="title-ref">.DataFrameGroupBy.pct\_change</span>, and <span class="title-ref">.SeriesGroupBy.pct\_change</span>, explicitly call e.g. <span class="title-ref">DataFrame.ffill</span> or <span class="title-ref">DataFrame.bfill</span> before calling `pct_change` instead (`53491`)
  - Deprecated `method`, `limit`, and `fill_axis` keywords in <span class="title-ref">DataFrame.align</span> and <span class="title-ref">Series.align</span>, explicitly call <span class="title-ref">DataFrame.fillna</span> or <span class="title-ref">Series.fillna</span> on the alignment results instead (`51856`)
  - Deprecated `quantile` keyword in <span class="title-ref">.Rolling.quantile</span> and <span class="title-ref">.Expanding.quantile</span>, renamed to `q` instead (`52550`)
  - Deprecated accepting slices in <span class="title-ref">DataFrame.take</span>, call `obj[slicer]` or pass a sequence of integers instead (`51539`)
  - Deprecated behavior of <span class="title-ref">DataFrame.idxmax</span>, <span class="title-ref">DataFrame.idxmin</span>, <span class="title-ref">Series.idxmax</span>, <span class="title-ref">Series.idxmin</span> in with all-NA entries or any-NA and `skipna=False`; in a future version these will raise `ValueError` (`51276`)
  - Deprecated explicit support for subclassing <span class="title-ref">Index</span> (`45289`)
  - Deprecated making functions given to <span class="title-ref">Series.agg</span> attempt to operate on each element in the <span class="title-ref">Series</span> and only operate on the whole <span class="title-ref">Series</span> if the elementwise operations failed. In the future, functions given to <span class="title-ref">Series.agg</span> will always operate on the whole <span class="title-ref">Series</span> only. To keep the current behavior, use <span class="title-ref">Series.transform</span> instead (`53325`)
  - Deprecated making the functions in a list of functions given to <span class="title-ref">DataFrame.agg</span> attempt to operate on each element in the <span class="title-ref">DataFrame</span> and only operate on the columns of the <span class="title-ref">DataFrame</span> if the elementwise operations failed. To keep the current behavior, use <span class="title-ref">DataFrame.transform</span> instead (`53325`)
  - Deprecated passing a <span class="title-ref">DataFrame</span> to <span class="title-ref">DataFrame.from\_records</span>, use <span class="title-ref">DataFrame.set\_index</span> or <span class="title-ref">DataFrame.drop</span> instead (`51353`)
  - Deprecated silently dropping unrecognized timezones when parsing strings to datetimes (`18702`)
  - Deprecated the `axis` keyword in <span class="title-ref">DataFrame.ewm</span>, <span class="title-ref">Series.ewm</span>, <span class="title-ref">DataFrame.rolling</span>, <span class="title-ref">Series.rolling</span>, <span class="title-ref">DataFrame.expanding</span>, <span class="title-ref">Series.expanding</span> (`51778`)
  - Deprecated the `axis` keyword in <span class="title-ref">DataFrame.resample</span>, <span class="title-ref">Series.resample</span> (`51778`)
  - Deprecated the `downcast` keyword in <span class="title-ref">Series.interpolate</span>, <span class="title-ref">DataFrame.interpolate</span>, <span class="title-ref">Series.fillna</span>, <span class="title-ref">DataFrame.fillna</span>, <span class="title-ref">Series.ffill</span>, <span class="title-ref">DataFrame.ffill</span>, <span class="title-ref">Series.bfill</span>, <span class="title-ref">DataFrame.bfill</span> (`40988`)
  - Deprecated the behavior of <span class="title-ref">concat</span> with both `len(keys) != len(objs)`, in a future version this will raise instead of truncating to the shorter of the two sequences (`43485`)
  - Deprecated the behavior of <span class="title-ref">Series.argsort</span> in the presence of NA values; in a future version these will be sorted at the end instead of giving -1 (`54219`)
  - Deprecated the default of `observed=False` in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span>; this will default to `True` in a future version (`43999`)
  - Deprecating pinning `group.name` to each group in <span class="title-ref">.SeriesGroupBy.aggregate</span> aggregations; if your operation requires utilizing the groupby keys, iterate over the groupby object instead (`41090`)
  - Deprecated the `axis` keyword in <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.fillna</span>, <span class="title-ref">.DataFrameGroupBy.take</span>, <span class="title-ref">.DataFrameGroupBy.skew</span>, <span class="title-ref">.DataFrameGroupBy.rank</span>, <span class="title-ref">.DataFrameGroupBy.cumprod</span>, <span class="title-ref">.DataFrameGroupBy.cumsum</span>, <span class="title-ref">.DataFrameGroupBy.cummax</span>, <span class="title-ref">.DataFrameGroupBy.cummin</span>, <span class="title-ref">.DataFrameGroupBy.pct\_change</span>, <span class="title-ref">.DataFrameGroupBy.diff</span>, <span class="title-ref">.DataFrameGroupBy.shift</span>, and <span class="title-ref">.DataFrameGroupBy.corrwith</span>; for `axis=1` operate on the underlying <span class="title-ref">DataFrame</span> instead (`50405`, `51046`)
  - Deprecated <span class="title-ref">.DataFrameGroupBy</span> with `as_index=False` not including groupings in the result when they are not columns of the DataFrame (`49519`)
  - Deprecated <span class="title-ref">is\_categorical\_dtype</span>, use `isinstance(obj.dtype, pd.CategoricalDtype)` instead (`52527`)
  - Deprecated <span class="title-ref">is\_datetime64tz\_dtype</span>, check `isinstance(dtype, pd.DatetimeTZDtype)` instead (`52607`)
  - Deprecated <span class="title-ref">is\_int64\_dtype</span>, check `dtype == np.dtype(np.int64)` instead (`52564`)
  - Deprecated <span class="title-ref">is\_interval\_dtype</span>, check `isinstance(dtype, pd.IntervalDtype)` instead (`52607`)
  - Deprecated <span class="title-ref">is\_period\_dtype</span>, check `isinstance(dtype, pd.PeriodDtype)` instead (`52642`)
  - Deprecated <span class="title-ref">is\_sparse</span>, check `isinstance(dtype, pd.SparseDtype)` instead (`52642`)
  - Deprecated <span class="title-ref">.Styler.applymap\_index</span>. Use the new <span class="title-ref">.Styler.map\_index</span> method instead (`52708`)
  - Deprecated <span class="title-ref">.Styler.applymap</span>. Use the new <span class="title-ref">.Styler.map</span> method instead (`52708`)
  - Deprecated <span class="title-ref">DataFrame.applymap</span>. Use the new <span class="title-ref">DataFrame.map</span> method instead (`52353`)
  - Deprecated <span class="title-ref">DataFrame.swapaxes</span> and <span class="title-ref">Series.swapaxes</span>, use <span class="title-ref">DataFrame.transpose</span> or <span class="title-ref">Series.transpose</span> instead (`51946`)
  - Deprecated `freq` parameter in <span class="title-ref">.PeriodArray</span> constructor, pass `dtype` instead (`52462`)
  - Deprecated allowing non-standard inputs in <span class="title-ref">take</span>, pass either a `numpy.ndarray`, <span class="title-ref">.ExtensionArray</span>, <span class="title-ref">Index</span>, or <span class="title-ref">Series</span> (`52981`)
  - Deprecated allowing non-standard sequences for <span class="title-ref">isin</span>, <span class="title-ref">value\_counts</span>, <span class="title-ref">unique</span>, <span class="title-ref">factorize</span>, case to one of `numpy.ndarray`, <span class="title-ref">Index</span>, <span class="title-ref">.ExtensionArray</span>, or <span class="title-ref">Series</span> before calling (`52986`)
  - Deprecated behavior of <span class="title-ref">DataFrame</span> reductions `sum`, `prod`, `std`, `var`, `sem` with `axis=None`, in a future version this will operate over both axes returning a scalar instead of behaving like `axis=0`; note this also affects numpy functions e.g. `np.sum(df)` (`21597`)
  - Deprecated behavior of <span class="title-ref">concat</span> when <span class="title-ref">DataFrame</span> has columns that are all-NA, in a future version these will not be discarded when determining the resulting dtype (`40893`)
  - Deprecated behavior of <span class="title-ref">Series.dt.to\_pydatetime</span>, in a future version this will return a <span class="title-ref">Series</span> containing python `datetime` objects instead of an `ndarray` of datetimes; this matches the behavior of other <span class="title-ref">Series.dt</span> properties (`20306`)
  - Deprecated logical operations (`|`, `&`, `^`) between pandas objects and dtype-less sequences (e.g. `list`, `tuple`), wrap a sequence in a <span class="title-ref">Series</span> or NumPy array before operating instead (`51521`)
  - Deprecated parameter `convert_type` in <span class="title-ref">Series.apply</span> (`52140`)
  - Deprecated passing a dictionary to <span class="title-ref">.SeriesGroupBy.agg</span>; pass a list of aggregations instead (`50684`)
  - Deprecated the `fastpath` keyword in <span class="title-ref">Categorical</span> constructor, use <span class="title-ref">Categorical.from\_codes</span> instead (`20110`)
  - Deprecated the behavior of <span class="title-ref">is\_bool\_dtype</span> returning `True` for object-dtype <span class="title-ref">Index</span> of bool objects (`52680`)
  - Deprecated the methods <span class="title-ref">Series.bool</span> and <span class="title-ref">DataFrame.bool</span> (`51749`)
  - Deprecated unused `closed` and `normalize` keywords in the <span class="title-ref">DatetimeIndex</span> constructor (`52628`)
  - Deprecated unused `closed` keyword in the <span class="title-ref">TimedeltaIndex</span> constructor (`52628`)
  - Deprecated logical operation between two non boolean <span class="title-ref">Series</span> with different indexes always coercing the result to bool dtype. In a future version, this will maintain the return type of the inputs (`52500`, `52538`)
  - Deprecated <span class="title-ref">Period</span> and <span class="title-ref">PeriodDtype</span> with `BDay` freq, use a <span class="title-ref">DatetimeIndex</span> with `BDay` freq instead (`53446`)
  - Deprecated <span class="title-ref">value\_counts</span>, use `pd.Series(obj).value_counts()` instead (`47862`)
  - Deprecated <span class="title-ref">Series.first</span> and <span class="title-ref">DataFrame.first</span>; create a mask and filter using `.loc` instead (`45908`)
  - Deprecated <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> for object-dtype (`53631`)
  - Deprecated <span class="title-ref">Series.last</span> and <span class="title-ref">DataFrame.last</span>; create a mask and filter using `.loc` instead (`53692`)
  - Deprecated allowing arbitrary `fill_value` in <span class="title-ref">SparseDtype</span>, in a future version the `fill_value` will need to be compatible with the `dtype.subtype`, either a scalar that can be held by that subtype or `NaN` for integer or bool subtypes (`23124`)
  - Deprecated allowing bool dtype in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span>, consistent with the <span class="title-ref">Series.quantile</span> and <span class="title-ref">DataFrame.quantile</span> behavior (`51424`)
  - Deprecated behavior of <span class="title-ref">.testing.assert\_series\_equal</span> and <span class="title-ref">.testing.assert\_frame\_equal</span> considering NA-like values (e.g. `NaN` vs `None` as equivalent) (`52081`)
  - Deprecated bytes input to <span class="title-ref">read\_excel</span>. To read a file path, use a string or path-like object (`53767`)
  - Deprecated constructing <span class="title-ref">.SparseArray</span> from scalar data, pass a sequence instead (`53039`)
  - Deprecated falling back to filling when `value` is not specified in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> with non-dict-like `to_replace` (`33302`)
  - Deprecated literal json input to <span class="title-ref">read\_json</span>. Wrap literal json string input in `io.StringIO` instead (`53409`)
  - Deprecated literal string input to <span class="title-ref">read\_xml</span>. Wrap literal string/bytes input in `io.StringIO` / `io.BytesIO` instead (`53767`)
  - Deprecated literal string/bytes input to <span class="title-ref">read\_html</span>. Wrap literal string/bytes input in `io.StringIO` / `io.BytesIO` instead (`53767`)
  - Deprecated option `mode.use_inf_as_na`, convert inf entries to `NaN` before instead (`51684`)
  - Deprecated parameter `obj` in <span class="title-ref">.DataFrameGroupBy.get\_group</span> (`53545`)
  - Deprecated positional indexing on <span class="title-ref">Series</span> with <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_setitem\_\_</span>, in a future version `ser[item]` will *always* interpret `item` as a label, not a position (`50617`)
  - Deprecated replacing builtin and NumPy functions in `.agg`, `.apply`, and `.transform`; use the corresponding string alias (e.g. `"sum"` for `sum` or `np.sum`) instead (`53425`)
  - Deprecated strings `T`, `t`, `L` and `l` denoting units in <span class="title-ref">to\_timedelta</span> (`52536`)
  - Deprecated the "method" and "limit" keywords in `.ExtensionArray.fillna`, implement `_pad_or_backfill` instead (`53621`)
  - Deprecated the `method` and `limit` keywords in <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> (`33302`)
  - Deprecated the `method` and `limit` keywords on <span class="title-ref">Series.fillna</span>, <span class="title-ref">DataFrame.fillna</span>, <span class="title-ref">.SeriesGroupBy.fillna</span>, <span class="title-ref">.DataFrameGroupBy.fillna</span>, and <span class="title-ref">.Resampler.fillna</span>, use `obj.bfill()` or `obj.ffill()` instead (`53394`)
  - Deprecated the behavior of <span class="title-ref">Series.\_\_getitem\_\_</span>, <span class="title-ref">Series.\_\_setitem\_\_</span>, <span class="title-ref">DataFrame.\_\_getitem\_\_</span>, <span class="title-ref">DataFrame.\_\_setitem\_\_</span> with an integer slice on objects with a floating-dtype index, in a future version this will be treated as *positional* indexing (`49612`)
  - Deprecated the use of non-supported datetime64 and timedelta64 resolutions with <span class="title-ref">pandas.array</span>. Supported resolutions are: "s", "ms", "us", "ns" resolutions (`53058`)
  - Deprecated values `"pad"`, `"ffill"`, `"bfill"`, `"backfill"` for <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span>, use `obj.ffill()` or `obj.bfill()` instead (`53581`)
  - Deprecated the behavior of <span class="title-ref">Index.argmax</span>, <span class="title-ref">Index.argmin</span>, <span class="title-ref">Series.argmax</span>, <span class="title-ref">Series.argmin</span> with either all-NAs and `skipna=True` or any-NAs and `skipna=False` returning -1; in a future version this will raise `ValueError` (`33941`, `33942`)
  - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_sql</span> except `name` and `con` (`54229`)
  - Deprecated silently ignoring `fill_value` when passing both `freq` and `fill_value` to <span class="title-ref">DataFrame.shift</span>, <span class="title-ref">Series.shift</span> and <span class="title-ref">.DataFrameGroupBy.shift</span>; in a future version this will raise `ValueError` (`53832`)

## Performance improvements

  - Performance improvement in <span class="title-ref">concat</span> with homogeneous `np.float64` or `np.float32` dtypes (`52685`)
  - Performance improvement in <span class="title-ref">factorize</span> for object columns not containing strings (`51921`)
  - Performance improvement in <span class="title-ref">read\_orc</span> when reading a remote URI file path (`51609`)
  - Performance improvement in <span class="title-ref">read\_parquet</span> and <span class="title-ref">DataFrame.to\_parquet</span> when reading a remote file with `engine="pyarrow"` (`51609`)
  - Performance improvement in <span class="title-ref">read\_parquet</span> on string columns when using `use_nullable_dtypes=True` (`47345`)
  - Performance improvement in <span class="title-ref">DataFrame.clip</span> and <span class="title-ref">Series.clip</span> (`51472`)
  - Performance improvement in <span class="title-ref">DataFrame.filter</span> when `items` is given (`52941`)
  - Performance improvement in <span class="title-ref">DataFrame.first\_valid\_index</span> and <span class="title-ref">DataFrame.last\_valid\_index</span> for extension array dtypes (`51549`)
  - Performance improvement in <span class="title-ref">DataFrame.where</span> when `cond` is backed by an extension dtype (`51574`)
  - Performance improvement in <span class="title-ref">MultiIndex.set\_levels</span> and <span class="title-ref">MultiIndex.set\_codes</span> when `verify_integrity=True` (`51873`)
  - Performance improvement in <span class="title-ref">MultiIndex.sortlevel</span> when `ascending` is a list (`51612`)
  - Performance improvement in <span class="title-ref">Series.combine\_first</span> (`51777`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray.fillna</span> when array does not contain nulls (`51635`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray.isna</span> when array has zero nulls or is all nulls (`51630`)
  - Performance improvement when parsing strings to `boolean[pyarrow]` dtype (`51730`)
  - Performance improvement when searching an <span class="title-ref">Index</span> sliced from other indexes (`51738`)
  - Performance improvement in <span class="title-ref">concat</span> (`52291`, `52290`)
  - <span class="title-ref">Period</span>'s default formatter (`period_format`) is now significantly (\~twice) faster. This improves performance of `str(Period)`, `repr(Period)`, and <span class="title-ref">.Period.strftime(fmt=None)</span>, as well as `.PeriodArray.strftime(fmt=None)`, `.PeriodIndex.strftime(fmt=None)` and `.PeriodIndex.format(fmt=None)`. `to_csv` operations involving <span class="title-ref">.PeriodArray</span> or <span class="title-ref">PeriodIndex</span> with default `date_format` are also significantly accelerated (`51459`)
  - Performance improvement accessing <span class="title-ref">arrays.IntegerArrays.dtype</span> & <span class="title-ref">arrays.FloatingArray.dtype</span> (`52998`)
  - Performance improvement for <span class="title-ref">.DataFrameGroupBy</span>/<span class="title-ref">.SeriesGroupBy</span> aggregations (e.g. <span class="title-ref">.DataFrameGroupBy.sum</span>) with `engine="numba"` (`53731`)
  - Performance improvement in <span class="title-ref">DataFrame</span> reductions with `axis=1` and extension dtypes (`54341`)
  - Performance improvement in <span class="title-ref">DataFrame</span> reductions with `axis=None` and extension dtypes (`54308`)
  - Performance improvement in <span class="title-ref">MultiIndex</span> and multi-column operations (e.g. <span class="title-ref">DataFrame.sort\_values</span>, <span class="title-ref">DataFrame.groupby</span>, <span class="title-ref">Series.unstack</span>) when index/column values are already sorted (`53806`)
  - Performance improvement in <span class="title-ref">Series</span> reductions (`52341`)
  - Performance improvement in <span class="title-ref">concat</span> when `axis=1` and objects have different indexes (`52541`)
  - Performance improvement in <span class="title-ref">concat</span> when the concatenation axis is a <span class="title-ref">MultiIndex</span> (`53574`)
  - Performance improvement in <span class="title-ref">merge</span> for PyArrow backed strings (`54443`)
  - Performance improvement in <span class="title-ref">read\_csv</span> with `engine="c"` (`52632`)
  - Performance improvement in <span class="title-ref">.ArrowExtensionArray.to\_numpy</span> (`52525`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.groups</span> (`53088`)
  - Performance improvement in <span class="title-ref">DataFrame.astype</span> when `dtype` is an extension dtype (`54299`)
  - Performance improvement in <span class="title-ref">DataFrame.iloc</span> when input is an single integer and dataframe is backed by extension dtypes (`54508`)
  - Performance improvement in <span class="title-ref">DataFrame.isin</span> for extension dtypes (`53514`)
  - Performance improvement in <span class="title-ref">DataFrame.loc</span> when selecting rows and columns (`53014`)
  - Performance improvement in <span class="title-ref">DataFrame.transpose</span> when transposing a DataFrame with a single PyArrow dtype (`54224`)
  - Performance improvement in <span class="title-ref">DataFrame.transpose</span> when transposing a DataFrame with a single masked dtype, e.g. <span class="title-ref">Int64</span> (`52836`)
  - Performance improvement in <span class="title-ref">Series.add</span> for PyArrow string and binary dtypes (`53150`)
  - Performance improvement in <span class="title-ref">Series.corr</span> and <span class="title-ref">Series.cov</span> for extension dtypes (`52502`)
  - Performance improvement in <span class="title-ref">Series.drop\_duplicates</span> for `ArrowDtype` (`54667`).
  - Performance improvement in <span class="title-ref">Series.ffill</span>, <span class="title-ref">Series.bfill</span>, <span class="title-ref">DataFrame.ffill</span>, <span class="title-ref">DataFrame.bfill</span> with PyArrow dtypes (`53950`)
  - Performance improvement in <span class="title-ref">Series.str.get\_dummies</span> for PyArrow-backed strings (`53655`)
  - Performance improvement in <span class="title-ref">Series.str.get</span> for PyArrow-backed strings (`53152`)
  - Performance improvement in <span class="title-ref">Series.str.split</span> with `expand=True` for PyArrow-backed strings (`53585`)
  - Performance improvement in <span class="title-ref">Series.to\_numpy</span> when dtype is a NumPy float dtype and `na_value` is `np.nan` (`52430`)
  - Performance improvement in <span class="title-ref">\~arrays.ArrowExtensionArray.astype</span> when converting from a PyArrow timestamp or duration dtype to NumPy (`53326`)
  - Performance improvement in various <span class="title-ref">MultiIndex</span> set and indexing operations (`53955`)
  - Performance improvement when doing various reshaping operations on <span class="title-ref">arrays.IntegerArray</span> & <span class="title-ref">arrays.FloatingArray</span> by avoiding doing unnecessary validation (`53013`)
  - Performance improvement when indexing with PyArrow timestamp and duration dtypes (`53368`)
  - Performance improvement when passing an array to <span class="title-ref">RangeIndex.take</span>, <span class="title-ref">DataFrame.loc</span>, or <span class="title-ref">DataFrame.iloc</span> and the DataFrame is using a RangeIndex (`53387`)

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">CategoricalIndex.remove\_categories</span> where ordered categories would not be maintained (`53935`).
  - Bug in <span class="title-ref">Series.astype</span> with `dtype="category"` for nullable arrays with read-only null value masks (`53658`)
  - Bug in <span class="title-ref">Series.map</span> , where the value of the `na_action` parameter was not used if the series held a <span class="title-ref">Categorical</span> (`22527`).

### Datetimelike

  - <span class="title-ref">DatetimeIndex.map</span> with `na_action="ignore"` now works as expected (`51644`)
  - <span class="title-ref">DatetimeIndex.slice\_indexer</span> now raises `KeyError` for non-monotonic indexes if either of the slice bounds is not in the index; this behaviour was previously deprecated but inconsistently handled (`53983`)
  - Bug in <span class="title-ref">DateOffset</span> which had inconsistent behavior when multiplying a <span class="title-ref">DateOffset</span> object by a constant (`47953`)
  - Bug in <span class="title-ref">date\_range</span> when `freq` was a <span class="title-ref">DateOffset</span> with `nanoseconds` (`46877`)
  - Bug in <span class="title-ref">to\_datetime</span> converting <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> containing <span class="title-ref">arrays.ArrowExtensionArray</span> of PyArrow timestamps to numpy datetimes (`52545`)
  - Bug in <span class="title-ref">.DatetimeArray.map</span> and <span class="title-ref">DatetimeIndex.map</span>, where the supplied callable operated array-wise instead of element-wise (`51977`)
  - Bug in <span class="title-ref">DataFrame.to\_sql</span> raising `ValueError` for PyArrow-backed date like dtypes (`53854`)
  - Bug in <span class="title-ref">Timestamp.date</span>, <span class="title-ref">Timestamp.isocalendar</span>, <span class="title-ref">Timestamp.timetuple</span>, and <span class="title-ref">Timestamp.toordinal</span> were returning incorrect results for inputs outside those supported by the Python standard library's datetime module (`53668`)
  - Bug in <span class="title-ref">Timestamp.round</span> with values close to the implementation bounds returning incorrect results instead of raising `OutOfBoundsDatetime` (`51494`)
  - Bug in constructing a <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> from a datetime or timedelta scalar always inferring nanosecond resolution instead of inferring from the input (`52212`)
  - Bug in constructing a <span class="title-ref">Timestamp</span> from a string representing a time without a date inferring an incorrect unit (`54097`)
  - Bug in constructing a <span class="title-ref">Timestamp</span> with `ts_input=pd.NA` raising `TypeError` (`45481`)
  - Bug in parsing datetime strings with weekday but no day e.g. "2023 Sept Thu" incorrectly raising `AttributeError` instead of `ValueError` (`52659`)
  - Bug in the repr for <span class="title-ref">Series</span> when dtype is a timezone aware datetime with non-nanosecond resolution raising `OutOfBoundsDatetime` (`54623`)

### Timedelta

  - Bug in <span class="title-ref">TimedeltaIndex</span> division or multiplication leading to `.freq` of "0 Days" instead of `None` (`51575`)
  - Bug in <span class="title-ref">Timedelta</span> with NumPy `timedelta64` objects not properly raising `ValueError` (`52806`)
  - Bug in <span class="title-ref">to\_timedelta</span> converting <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> containing <span class="title-ref">ArrowDtype</span> of `pyarrow.duration` to NumPy `timedelta64` (`54298`)
  - Bug in <span class="title-ref">Timedelta.\_\_hash\_\_</span>, raising an `OutOfBoundsTimedelta` on certain large values of second resolution (`54037`)
  - Bug in <span class="title-ref">Timedelta.round</span> with values close to the implementation bounds returning incorrect results instead of raising `OutOfBoundsTimedelta` (`51494`)
  - Bug in <span class="title-ref">TimedeltaIndex.map</span> with `na_action="ignore"` (`51644`)
  - Bug in <span class="title-ref">arrays.TimedeltaArray.map</span> and <span class="title-ref">TimedeltaIndex.map</span>, where the supplied callable operated array-wise instead of element-wise (`51977`)

### Timezones

  - Bug in <span class="title-ref">infer\_freq</span> that raises `TypeError` for `Series` of timezone-aware timestamps (`52456`)
  - Bug in <span class="title-ref">DatetimeTZDtype.base</span> that always returns a NumPy dtype with nanosecond resolution (`52705`)

### Numeric

  - Bug in <span class="title-ref">RangeIndex</span> setting `step` incorrectly when being the subtrahend with minuend a numeric value (`53255`)
  - Bug in <span class="title-ref">Series.corr</span> and <span class="title-ref">Series.cov</span> raising `AttributeError` for masked dtypes (`51422`)
  - Bug when calling <span class="title-ref">Series.kurt</span> and <span class="title-ref">Series.skew</span> on NumPy data of all zero returning a Python type instead of a NumPy type (`53482`)
  - Bug in <span class="title-ref">Series.mean</span>, <span class="title-ref">DataFrame.mean</span> with object-dtype values containing strings that can be converted to numbers (e.g. "2") returning incorrect numeric results; these now raise `TypeError` (`36703`, `44008`)
  - Bug in <span class="title-ref">DataFrame.corrwith</span> raising `NotImplementedError` for PyArrow-backed dtypes (`52314`)
  - Bug in <span class="title-ref">DataFrame.size</span> and <span class="title-ref">Series.size</span> returning 64-bit integer instead of a Python int (`52897`)
  - Bug in <span class="title-ref">DateFrame.dot</span> returning `object` dtype for <span class="title-ref">ArrowDtype</span> data (`53979`)
  - Bug in <span class="title-ref">Series.any</span>, <span class="title-ref">Series.all</span>, <span class="title-ref">DataFrame.any</span>, and <span class="title-ref">DataFrame.all</span> had the default value of `bool_only` set to `None` instead of `False`; this change should have no impact on users (`53258`)
  - Bug in <span class="title-ref">Series.corr</span> and <span class="title-ref">Series.cov</span> raising `AttributeError` for masked dtypes (`51422`)
  - Bug in <span class="title-ref">Series.median</span> and <span class="title-ref">DataFrame.median</span> with object-dtype values containing strings that can be converted to numbers (e.g. "2") returning incorrect numeric results; these now raise `TypeError` (`34671`)
  - Bug in <span class="title-ref">Series.sum</span> converting dtype `uint64` to `int64` (`53401`)

### Conversion

  - Bug in <span class="title-ref">DataFrame.style.to\_latex</span> and <span class="title-ref">DataFrame.style.to\_html</span> if the DataFrame contains integers with more digits than can be represented by floating point double precision (`52272`)
  - Bug in <span class="title-ref">array</span> when given a `datetime64` or `timedelta64` dtype with unit of "s", "us", or "ms" returning <span class="title-ref">.NumpyExtensionArray</span> instead of <span class="title-ref">.DatetimeArray</span> or <span class="title-ref">.TimedeltaArray</span> (`52859`)
  - Bug in <span class="title-ref">array</span> when given an empty list and no dtype returning <span class="title-ref">.NumpyExtensionArray</span> instead of <span class="title-ref">.FloatingArray</span> (`54371`)
  - Bug in <span class="title-ref">.ArrowDtype.numpy\_dtype</span> returning nanosecond units for non-nanosecond `pyarrow.timestamp` and `pyarrow.duration` types (`51800`)
  - Bug in <span class="title-ref">DataFrame.\_\_repr\_\_</span> incorrectly raising a `TypeError` when the dtype of a column is `np.record` (`48526`)
  - Bug in <span class="title-ref">DataFrame.info</span> raising `ValueError` when `use_numba` is set (`51922`)
  - Bug in <span class="title-ref">DataFrame.insert</span> raising `TypeError` if `loc` is `np.int64` (`53193`)
  - Bug in <span class="title-ref">HDFStore.select</span> loses precision of large int when stored and retrieved (`54186`)
  - Bug in <span class="title-ref">Series.astype</span> not supporting `object_` (`54251`)

### Strings

  - Bug in <span class="title-ref">Series.str</span> that did not raise a `TypeError` when iterated (`54173`)
  - Bug in `repr` for <span class="title-ref">DataFrame</span><span class="title-ref"> with string-dtype columns (:issue:\`54797</span>)

### Interval

  - <span class="title-ref">IntervalIndex.get\_indexer</span> and <span class="title-ref">IntervalIndex.get\_indexer\_nonunique</span> raising if `target` is read-only array (`53703`)
  - Bug in <span class="title-ref">IntervalDtype</span> where the object could be kept alive when deleted (`54184`)
  - Bug in <span class="title-ref">interval\_range</span> where a float `step` would produce incorrect intervals from floating point artifacts (`54477`)

### Indexing

  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> losing dtype when setting a <span class="title-ref">DataFrame</span> into duplicated columns (`53143`)
  - Bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> with a boolean mask and <span class="title-ref">DataFrame.putmask</span> with mixed non-numeric dtypes and a value other than `NaN` incorrectly raising `TypeError` (`53291`)
  - Bug in <span class="title-ref">DataFrame.iloc</span> when using `nan` as the only element (`52234`)
  - Bug in <span class="title-ref">Series.loc</span> casting <span class="title-ref">Series</span> to `np.dnarray` when assigning <span class="title-ref">Series</span> at predefined index of `object` dtype <span class="title-ref">Series</span> (`48933`)

### Missing

  - Bug in <span class="title-ref">DataFrame.interpolate</span> failing to fill across data when `method` is `"pad"`, `"ffill"`, `"bfill"`, or `"backfill"` (`53898`)
  - Bug in <span class="title-ref">DataFrame.interpolate</span> ignoring `inplace` when <span class="title-ref">DataFrame</span> is empty (`53199`)
  - Bug in <span class="title-ref">Series.idxmin</span>, <span class="title-ref">Series.idxmax</span>, <span class="title-ref">DataFrame.idxmin</span>, <span class="title-ref">DataFrame.idxmax</span> with a <span class="title-ref">DatetimeIndex</span> index containing `NaT` incorrectly returning `NaN` instead of `NaT` (`43587`)
  - Bug in <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> failing to raise on invalid `downcast` keyword, which can be only `None` or `"infer"` (`53103`)
  - Bug in <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> with complex dtype incorrectly failing to fill `NaN` entries (`53635`)

### MultiIndex

  - Bug in <span class="title-ref">MultiIndex.set\_levels</span> not preserving dtypes for <span class="title-ref">Categorical</span> (`52125`)
  - Bug in displaying a <span class="title-ref">MultiIndex</span> with a long element (`52960`)

### I/O

  - <span class="title-ref">DataFrame.to\_orc</span> now raising `ValueError` when non-default <span class="title-ref">Index</span> is given (`51828`)
  - <span class="title-ref">DataFrame.to\_sql</span> now raising `ValueError` when the name param is left empty while using SQLAlchemy to connect (`52675`)
  - Bug in <span class="title-ref">json\_normalize</span> could not parse metadata fields list type (`37782`)
  - Bug in <span class="title-ref">read\_csv</span> where it would error when `parse_dates` was set to a list or dictionary with `engine="pyarrow"` (`47961`)
  - Bug in <span class="title-ref">read\_csv</span> with `engine="pyarrow"` raising when specifying a `dtype` with `index_col` (`53229`)
  - Bug in <span class="title-ref">read\_hdf</span> not properly closing store after an `IndexError` is raised (`52781`)
  - Bug in <span class="title-ref">read\_html</span> where style elements were read into DataFrames (`52197`)
  - Bug in <span class="title-ref">read\_html</span> where tail texts were removed together with elements containing `display:none` style (`51629`)
  - Bug in <span class="title-ref">read\_sql\_table</span> raising an exception when reading a view (`52969`)
  - Bug in <span class="title-ref">read\_sql</span> when reading multiple timezone aware columns with the same column name (`44421`)
  - Bug in <span class="title-ref">read\_xml</span> stripping whitespace in string data (`53811`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> where `colspace` was incorrectly applied in case of multi index columns (`53885`)
  - Bug in <span class="title-ref">DataFrame.to\_html</span> where conversion for an empty <span class="title-ref">DataFrame</span> with complex dtype raised a `ValueError` (`54167`)
  - Bug in <span class="title-ref">DataFrame.to\_json</span> where <span class="title-ref">.DateTimeArray</span>/<span class="title-ref">.DateTimeIndex</span> with non nanosecond precision could not be serialized correctly (`53686`)
  - Bug when writing and reading empty Stata dta files where dtype information was lost (`46240`)
  - Bug where `bz2` was treated as a hard requirement (`53857`)

### Period

  - Bug in <span class="title-ref">PeriodDtype</span> constructor failing to raise `TypeError` when no argument is passed or when `None` is passed (`27388`)
  - Bug in <span class="title-ref">PeriodDtype</span> constructor incorrectly returning the same `normalize` for different <span class="title-ref">DateOffset</span> `freq` inputs (`24121`)
  - Bug in <span class="title-ref">PeriodDtype</span> constructor raising `ValueError` instead of `TypeError` when an invalid type is passed (`51790`)
  - Bug in <span class="title-ref">PeriodDtype</span> where the object could be kept alive when deleted (`54184`)
  - Bug in <span class="title-ref">read\_csv</span> not processing empty strings as a null value, with `engine="pyarrow"` (`52087`)
  - Bug in <span class="title-ref">read\_csv</span> returning `object` dtype columns instead of `float64` dtype columns with `engine="pyarrow"` for columns that are all null with `engine="pyarrow"` (`52087`)
  - Bug in <span class="title-ref">Period.now</span> not accepting the `freq` parameter as a keyword argument (`53369`)
  - Bug in <span class="title-ref">PeriodIndex.map</span> with `na_action="ignore"` (`51644`)
  - Bug in <span class="title-ref">arrays.PeriodArray.map</span> and <span class="title-ref">PeriodIndex.map</span>, where the supplied callable operated array-wise instead of element-wise (`51977`)
  - Bug in incorrectly allowing construction of <span class="title-ref">Period</span> or <span class="title-ref">PeriodDtype</span> with <span class="title-ref">CustomBusinessDay</span> freq; use <span class="title-ref">BusinessDay</span> instead (`52534`)

### Plotting

  - Bug in <span class="title-ref">Series.plot</span> when invoked with `color=None` (`51953`)
  - Fixed UserWarning in <span class="title-ref">DataFrame.plot.scatter</span> when invoked with `c="b"` (`53908`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.SeriesGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.SeriesGroupBy.idxmax</span> returns wrong dtype when used on an empty DataFrameGroupBy or SeriesGroupBy (`51423`)
  - Bug in <span class="title-ref">DataFrame.groupby.rank</span> on nullable datatypes when passing `na_option="bottom"` or `na_option="top"` (`54206`)
  - Bug in <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">Series.resample</span> in incorrectly allowing non-fixed `freq` when resampling on a <span class="title-ref">TimedeltaIndex</span> (`51896`)
  - Bug in <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">Series.resample</span> losing time zone when resampling empty data (`53664`)
  - Bug in <span class="title-ref">DataFrame.resample</span> and <span class="title-ref">Series.resample</span> where `origin` has no effect in resample when values are outside of axis (`53662`)
  - Bug in weighted rolling aggregations when specifying `min_periods=0` (`51449`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> where, when the index of the grouped <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> was a <span class="title-ref">DatetimeIndex</span>, <span class="title-ref">TimedeltaIndex</span> or <span class="title-ref">PeriodIndex</span>, and the `groupby` method was given a function as its first argument, the function operated on the whole index rather than each element of the index (`51979`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.agg</span> with lists not respecting `as_index=False` (`52849`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> causing an error to be raised when the input <span class="title-ref">DataFrame</span> was subset as a <span class="title-ref">DataFrame</span> after groupby (`[['a']]` and not `['a']`) and the given callable returned <span class="title-ref">Series</span> that were not all indexed the same (`52444`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.apply</span> raising a `TypeError` when selecting multiple columns and providing a function that returns `np.ndarray` results (`18930`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.groups</span> and <span class="title-ref">.SeriesGroupBy.groups</span> with a datetime key in conjunction with another key produced an incorrect number of group keys (`51158`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> may implicitly sort the result index with `sort=False` (`53009`)
  - Bug in <span class="title-ref">.SeriesGroupBy.size</span> where the dtype would be `np.int64` for data with <span class="title-ref">ArrowDtype</span> or masked dtypes (e.g. `Int64`) (`53831`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> with column selection on the resulting groupby object not returning names as tuples when grouping by a list consisting of a single element (`53500`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.var</span> and <span class="title-ref">.SeriesGroupBy.var</span> failing to raise `TypeError` when called with datetime64, timedelta64 or <span class="title-ref">PeriodDtype</span> values (`52128`, `53045`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.resample</span> with `kind="period"` raising `AttributeError` (`24103`)
  - Bug in <span class="title-ref">.Resampler.ohlc</span> with empty object returning a <span class="title-ref">Series</span> instead of empty <span class="title-ref">DataFrame</span> (`42902`)
  - Bug in <span class="title-ref">.SeriesGroupBy.count</span> and <span class="title-ref">.DataFrameGroupBy.count</span> where the dtype would be `np.int64` for data with <span class="title-ref">ArrowDtype</span> or masked dtypes (e.g. `Int64`) (`53831`)
  - Bug in <span class="title-ref">.SeriesGroupBy.nth</span> and <span class="title-ref">.DataFrameGroupBy.nth</span> after performing column selection when using `dropna="any"` or `dropna="all"` would not subset columns (`53518`)
  - Bug in <span class="title-ref">.SeriesGroupBy.nth</span> and <span class="title-ref">.DataFrameGroupBy.nth</span> raised after performing column selection when using `dropna="any"` or `dropna="all"` resulted in rows being dropped (`53518`)
  - Bug in <span class="title-ref">.SeriesGroupBy.sum</span> and <span class="title-ref">.DataFrameGroupBy.sum</span> summing `np.inf + np.inf` and `(-np.inf) + (-np.inf)` to `np.nan` instead of `np.inf` and `-np.inf` respectively (`53606`)
  - Bug in <span class="title-ref">Series.groupby</span> raising an error when grouped <span class="title-ref">Series</span> has a <span class="title-ref">DatetimeIndex</span> index and a <span class="title-ref">Series</span> with a name that is a month is given to the `by` argument (`48509`)

### Reshaping

  - Bug in <span class="title-ref">concat</span> coercing to `object` dtype when one column has `pa.null()` dtype (`53702`)
  - Bug in <span class="title-ref">crosstab</span> when `dropna=False` would not keep `np.nan` in the result (`10772`)
  - Bug in <span class="title-ref">melt</span> where the `variable` column would lose extension dtypes (`54297`)
  - Bug in <span class="title-ref">merge\_asof</span> raising `KeyError` for extension dtypes (`52904`)
  - Bug in <span class="title-ref">merge\_asof</span> raising `ValueError` for data backed by read-only ndarrays (`53513`)
  - Bug in <span class="title-ref">merge\_asof</span> with `left_index=True` or `right_index=True` with mismatched index dtypes giving incorrect results in some cases instead of raising `MergeError` (`53870`)
  - Bug in <span class="title-ref">merge</span> when merging on integer `ExtensionDtype` and float NumPy dtype raising `TypeError` (`46178`)
  - Bug in <span class="title-ref">DataFrame.agg</span> and <span class="title-ref">Series.agg</span> on non-unique columns would return incorrect type when dist-like argument passed in (`51099`)
  - Bug in <span class="title-ref">DataFrame.combine\_first</span> ignoring other's columns if `other` is empty (`53792`)
  - Bug in <span class="title-ref">DataFrame.idxmin</span> and <span class="title-ref">DataFrame.idxmax</span>, where the axis dtype would be lost for empty frames (`53265`)
  - Bug in <span class="title-ref">DataFrame.merge</span> not merging correctly when having `MultiIndex` with single level (`52331`)
  - Bug in <span class="title-ref">DataFrame.stack</span> losing extension dtypes when columns is a <span class="title-ref">MultiIndex</span> and frame contains mixed dtypes (`45740`)
  - Bug in <span class="title-ref">DataFrame.stack</span> sorting columns lexicographically (`53786`)
  - Bug in <span class="title-ref">DataFrame.transpose</span> inferring dtype for object column (`51546`)
  - Bug in <span class="title-ref">Series.combine\_first</span> converting `int64` dtype to `float64` and losing precision on very large integers (`51764`)
  - Bug when joining empty <span class="title-ref">DataFrame</span> objects, where the joined index would be a <span class="title-ref">RangeIndex</span> instead of the joined index type (`52777`)

### Sparse

  - Bug in <span class="title-ref">SparseDtype</span> constructor failing to raise `TypeError` when given an incompatible `dtype` for its subtype, which must be a NumPy dtype (`53160`)
  - Bug in <span class="title-ref">arrays.SparseArray.map</span> allowed the fill value to be included in the sparse values (`52095`)

### ExtensionArray

  - Bug in <span class="title-ref">.ArrowStringArray</span> constructor raises `ValueError` with dictionary types of strings (`54074`)
  - Bug in <span class="title-ref">DataFrame</span> constructor not copying <span class="title-ref">Series</span> with extension dtype when given in dict (`53744`)
  - Bug in <span class="title-ref">\~arrays.ArrowExtensionArray</span> converting pandas non-nanosecond temporal objects from non-zero values to zero values (`53171`)
  - Bug in <span class="title-ref">Series.quantile</span> for PyArrow temporal types raising `ArrowInvalid` (`52678`)
  - Bug in <span class="title-ref">Series.rank</span> returning wrong order for small values with `Float64` dtype (`52471`)
  - Bug in <span class="title-ref">Series.unique</span> for boolean `ArrowDtype` with `NA` values (`54667`)
  - Bug in <span class="title-ref">\~arrays.ArrowExtensionArray.\_\_iter\_\_</span> and <span class="title-ref">\~arrays.ArrowExtensionArray.\_\_getitem\_\_</span> returning python datetime and timedelta objects for non-nano dtypes (`53326`)
  - Bug in <span class="title-ref">\~arrays.ArrowExtensionArray.factorize</span> returning incorrect uniques for a `pyarrow.dictionary` type `pyarrow.chunked_array` with more than one chunk (`54844`)
  - Bug when passing an <span class="title-ref">ExtensionArray</span> subclass to `dtype` keywords. This will now raise a `UserWarning` to encourage passing an instance instead (`31356`, `54592`)
  - Bug where the <span class="title-ref">DataFrame</span> repr would not work when a column had an <span class="title-ref">ArrowDtype</span> with a `pyarrow.ExtensionDtype` (`54063`)
  - Bug where the `__from_arrow__` method of masked ExtensionDtypes (e.g. <span class="title-ref">Float64Dtype</span>, <span class="title-ref">BooleanDtype</span>) would not accept PyArrow arrays of type `pyarrow.null()` (`52223`)

### Styler

  - Bug in <span class="title-ref">.Styler.\_copy</span> calling overridden methods in subclasses of <span class="title-ref">.Styler</span> (`52728`)

### Metadata

  - Fixed metadata propagation in <span class="title-ref">DataFrame.max</span>, <span class="title-ref">DataFrame.min</span>, <span class="title-ref">DataFrame.prod</span>, <span class="title-ref">DataFrame.mean</span>, <span class="title-ref">Series.mode</span>, <span class="title-ref">DataFrame.median</span>, <span class="title-ref">DataFrame.sem</span>, <span class="title-ref">DataFrame.skew</span>, <span class="title-ref">DataFrame.kurt</span> (`28283`)
  - Fixed metadata propagation in <span class="title-ref">DataFrame.squeeze</span>, and <span class="title-ref">DataFrame.describe</span> (`28283`)
  - Fixed metadata propagation in <span class="title-ref">DataFrame.std</span> (`28283`)

### Other

  - Bug in <span class="title-ref">.FloatingArray.\_\_contains\_\_</span> with `NaN` item incorrectly returning `False` when `NaN` values are present (`52840`)
  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> raising for data of complex dtype when `NaN` values are present (`53627`)
  - Bug in <span class="title-ref">DatetimeIndex</span> where `repr` of index passed with time does not print time is midnight and non-day based freq(`53470`)
  - Bug in <span class="title-ref">.testing.assert\_frame\_equal</span> and <span class="title-ref">.testing.assert\_series\_equal</span> now throw assertion error for two unequal sets (`51727`)
  - Bug in <span class="title-ref">.testing.assert\_frame\_equal</span> checks category dtypes even when asked not to check index type (`52126`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was not respecting `allow_copy` argument (`54322`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> was raising during interchanging from non-pandas tz-aware data containing null values (`54287`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> when converting an empty DataFrame object (`53155`)
  - Bug in <span class="title-ref">from\_dummies</span> where the resulting <span class="title-ref">Index</span> did not match the original <span class="title-ref">Index</span> (`54300`)
  - Bug in <span class="title-ref">from\_dummies</span> where the resulting data would always be `object` dtype instead of the dtype of the columns (`54300`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.first</span>, <span class="title-ref">.DataFrameGroupBy.last</span>, <span class="title-ref">.SeriesGroupBy.first</span>, and <span class="title-ref">.SeriesGroupBy.last</span> where an empty group would return `np.nan` instead of the corresponding <span class="title-ref">.ExtensionArray</span> NA value (`39098`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> with casting the mean of ints back to an int (`16676`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> with a `fill_value` that should be inferred with a <span class="title-ref">ExtensionDtype</span> incorrectly inferring `object` dtype (`52586`)
  - Bug in <span class="title-ref">DataFrame.shift</span> with `axis=1` on a <span class="title-ref">DataFrame</span> with a single <span class="title-ref">ExtensionDtype</span> column giving incorrect results (`53832`)
  - Bug in <span class="title-ref">Index.sort\_values</span> when a `key` is passed (`52764`)
  - Bug in <span class="title-ref">Series.align</span>, <span class="title-ref">DataFrame.align</span>, <span class="title-ref">Series.reindex</span>, <span class="title-ref">DataFrame.reindex</span>, <span class="title-ref">Series.interpolate</span>, <span class="title-ref">DataFrame.interpolate</span>, incorrectly failing to raise with method="asfreq" (`53620`)
  - Bug in <span class="title-ref">Series.argsort</span> failing to raise when an invalid `axis` is passed (`54257`)
  - Bug in <span class="title-ref">Series.map</span> when giving a callable to an empty series, the returned series had `object` dtype. It now keeps the original dtype (`52384`)
  - Bug in <span class="title-ref">Series.memory\_usage</span> when `deep=True` throw an error with Series of objects and the returned value is incorrect, as it does not take into account GC corrections (`51858`)
  - Bug in <span class="title-ref">period\_range</span> the default behavior when freq was not passed as an argument was incorrect(`53687`)
  - Fixed incorrect `__name__` attribute of `pandas._libs.json` (`52898`)

## Contributors

<div class="contributors">

v2.0.3..v2.1.0

</div>

---

v2.1.1.md

---

# What's new in 2.1.1 (September 20, 2023)

These are the changes in pandas 2.1.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression in <span class="title-ref">concat</span> when <span class="title-ref">DataFrame</span> 's have two different extension dtypes (`54848`)
  - Fixed regression in <span class="title-ref">merge</span> when merging over a PyArrow string index (`54894`)
  - Fixed regression in <span class="title-ref">read\_csv</span> when `usecols` is given and `dtypes` is a dict for `engine="python"` (`54868`)
  - Fixed regression in <span class="title-ref">read\_csv</span> when `delim_whitespace` is True (`54918`, `54931`)
  - Fixed regression in <span class="title-ref">.GroupBy.get\_group</span> raising for `axis=1` (`54858`)
  - Fixed regression in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> raising `AssertionError` when setting a <span class="title-ref">Series</span> with a partial <span class="title-ref">MultiIndex</span> (`54875`)
  - Fixed regression in <span class="title-ref">DataFrame.filter</span> not respecting the order of elements for `filter` (`54980`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_sql</span> not roundtripping datetime columns correctly for sqlite (`54877`)
  - Fixed regression in <span class="title-ref">DataFrameGroupBy.agg</span> when aggregating a DataFrame with duplicate column names using a dictionary (`55006`)
  - Fixed regression in <span class="title-ref">MultiIndex.append</span> raising when appending overlapping <span class="title-ref">IntervalIndex</span> levels (`54934`)
  - Fixed regression in <span class="title-ref">Series.drop\_duplicates</span> for PyArrow strings (`54904`)
  - Fixed regression in <span class="title-ref">Series.interpolate</span> raising when `fill_value` was given (`54920`)
  - Fixed regression in <span class="title-ref">Series.value\_counts</span> raising for numeric data if `bins` was specified (`54857`)
  - Fixed regression in comparison operations for PyArrow backed columns not propagating exceptions correctly (`54944`)
  - Fixed regression when comparing a <span class="title-ref">Series</span> with `datetime64` dtype with `None` (`54870`)

## Bug fixes

  - Fixed bug for <span class="title-ref">ArrowDtype</span> raising `NotImplementedError` for fixed-size list (`55000`)
  - Fixed bug in <span class="title-ref">DataFrame.stack</span> with `future_stack=True` and columns a non-<span class="title-ref">MultiIndex</span> consisting of tuples (`54948`)
  - Fixed bug in <span class="title-ref">Series.dt.tz</span> with <span class="title-ref">ArrowDtype</span> where a string was returned instead of a `tzinfo` object (`55003`)
  - Fixed bug in <span class="title-ref">Series.pct\_change</span> and <span class="title-ref">DataFrame.pct\_change</span> showing unnecessary `FutureWarning` (`54981`)

## Other

  - Reverted the deprecation that disallowed <span class="title-ref">Series.apply</span> returning a <span class="title-ref">DataFrame</span> when the passed-in callable returns a <span class="title-ref">Series</span> object (`52116`)

## Contributors

<div class="contributors">

v2.1.0..v2.1.1

</div>

---

v2.1.2.md

---

# What's new in 2.1.2 (October 26, 2023)

These are the changes in pandas 2.1.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Deprecations

  - Reverted deprecation of `fill_method=None` in <span class="title-ref">DataFrame.pct\_change</span>, <span class="title-ref">Series.pct\_change</span>, <span class="title-ref">DataFrameGroupBy.pct\_change</span>, and <span class="title-ref">SeriesGroupBy.pct\_change</span>; the values `'backfill'`, `'bfill'`, `'pad'`, and `'ffill'` are still deprecated (`53491`)

## Fixed regressions

  - Fixed regression in <span class="title-ref">DataFrame.join</span> where result has missing values and dtype is arrow backed string (`55348`)
  - Fixed regression in <span class="title-ref">\~DataFrame.rolling</span> where non-nanosecond index or `on` column would produce incorrect results (`55026`, `55106`, `55299`)
  - Fixed regression in <span class="title-ref">DataFrame.resample</span> which was extrapolating back to `origin` when `origin` was outside its bounds (`55064`)
  - Fixed regression in <span class="title-ref">DataFrame.sort\_index</span> which was not sorting correctly when the index was a sliced <span class="title-ref">MultiIndex</span> (`55379`)
  - Fixed regression in <span class="title-ref">DataFrameGroupBy.agg</span> and <span class="title-ref">SeriesGroupBy.agg</span> where if the option `compute.use_numba` was set to True, groupby methods not supported by the numba engine would raise a `TypeError` (`55520`)
  - Fixed performance regression with wide DataFrames, typically involving methods where all columns were accessed individually (`55256`, `55245`)
  - Fixed regression in <span class="title-ref">merge\_asof</span> raising `TypeError` for `by` with datetime and timedelta dtypes (`55453`)
  - Fixed regression in <span class="title-ref">read\_parquet</span> when reading a file with a string column consisting of more than 2 GB of string data and using the `"string"` dtype (`55606`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_sql</span> not roundtripping datetime columns correctly for sqlite when using `detect_types` (`55554`)
  - Fixed regression in construction of certain DataFrame or Series subclasses (`54922`)

## Bug fixes

  - Fixed bug in <span class="title-ref">.DataFrameGroupBy</span> reductions not preserving object dtype when `infer_string` is set (`55620`)
  - Fixed bug in <span class="title-ref">.SeriesGroupBy.value\_counts</span> returning incorrect dtype for string columns (`55627`)
  - Fixed bug in <span class="title-ref">Categorical.equals</span> if other has arrow backed string dtype (`55364`)
  - Fixed bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> not inferring string dtype for zero-dimensional array with `infer_string=True` (`55366`)
  - Fixed bug in <span class="title-ref">DataFrame.idxmin</span> and <span class="title-ref">DataFrame.idxmax</span> raising for arrow dtypes (`55368`)
  - Fixed bug in <span class="title-ref">DataFrame.interpolate</span> raising incorrect error message (`55347`)
  - Fixed bug in <span class="title-ref">Index.insert</span> raising when inserting `None` into <span class="title-ref">Index</span> with `dtype="string[pyarrow_numpy]"` (`55365`)
  - Fixed bug in <span class="title-ref">Series.all</span> and <span class="title-ref">Series.any</span> not treating missing values correctly for `dtype="string[pyarrow_numpy]"` (`55367`)
  - Fixed bug in <span class="title-ref">Series.floordiv</span> for <span class="title-ref">ArrowDtype</span> (`55561`)
  - Fixed bug in <span class="title-ref">Series.mode</span> not sorting values for arrow backed string dtype (`55621`)
  - Fixed bug in <span class="title-ref">Series.rank</span> for `string[pyarrow_numpy]` dtype (`55362`)
  - Fixed bug in <span class="title-ref">Series.str.extractall</span> for <span class="title-ref">ArrowDtype</span> dtype being converted to object (`53846`)
  - Fixed bug where PDEP-6 warning about setting an item of an incompatible dtype was being shown when creating a new conditional column (`55025`)
  - Silence `Period[B]` warnings introduced by `53446` during normal plotting activity (`55138`)
  - Fixed bug in <span class="title-ref">Series</span> constructor not inferring string dtype when `NA` is the first value and `infer_string` is set (`  55655 `)

## Other

  - Fixed non-working installation of optional dependency group `output_formatting`. Replacing underscore `_` with a dash `-` fixes broken dependency resolution. A correct way to use now is `pip install pandas[output-formatting]`.
  - 
## Contributors

<div class="contributors">

v2.1.1..v2.1.2

</div>

---

v2.1.3.md

---

# What's new in 2.1.3 (November 10, 2023)

These are the changes in pandas 2.1.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed infinite recursion from operations that return a new object on some DataFrame subclasses (`55763`)

## Bug fixes

  - Bug in <span class="title-ref">DatetimeIndex.diff</span> raising `TypeError` (`55080`)
  - Bug in <span class="title-ref">Index.isin</span> raising for Arrow backed string and `None` value (`55821`)
  - Fix <span class="title-ref">read\_parquet</span> and <span class="title-ref">read\_feather</span> for [CVE-2023-47248](https://www.cve.org/CVERecord?id=CVE-2023-47248) (`55894`)

## Contributors

<div class="contributors">

v2.1.2..v2.1.3|HEAD

</div>

---

v2.1.4.md

---

# What's new in 2.1.4 (December 8, 2023)

These are the changes in pandas 2.1.4. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Fixed regressions

  - Fixed regression when trying to read a pickled pandas <span class="title-ref">DataFrame</span> from pandas 1.3 (`55137`)

## Bug fixes

  - Bug in <span class="title-ref">Series</span> constructor raising DeprecationWarning when `index` is a list of <span class="title-ref">Series</span> (`55228`)
  - Bug in <span class="title-ref">Series</span> when trying to cast date-like string inputs to <span class="title-ref">ArrowDtype</span> of `pyarrow.timestamp` (`56266`)
  - Bug in <span class="title-ref">Timestamp</span> construction with `ts_input="now"` or `ts_input="today"` giving a different unit from <span class="title-ref">Timestamp.now</span> or <span class="title-ref">Timestamp.today</span> (`55879`)
  - Bug in <span class="title-ref">Index.\_\_getitem\_\_</span> returning wrong result for Arrow dtypes and negative stepsize (`55832`)
  - Fixed bug in <span class="title-ref">read\_csv</span> not respecting object dtype when `infer_string` option is set (`56047`)
  - Fixed bug in <span class="title-ref">to\_numeric</span> converting to extension dtype for `string[pyarrow_numpy]` dtype (`56179`)
  - Fixed bug in <span class="title-ref">.DataFrameGroupBy.min</span> and <span class="title-ref">.DataFrameGroupBy.max</span> not preserving extension dtype for empty object (`55619`)
  - Fixed bug in <span class="title-ref">DataFrame.\_\_setitem\_\_</span> casting <span class="title-ref">Index</span> with object-dtype to PyArrow backed strings when `infer_string` option is set (`55638`)
  - Fixed bug in <span class="title-ref">DataFrame.to\_hdf</span> raising when columns have `StringDtype` (`55088`)
  - Fixed bug in <span class="title-ref">Index.insert</span> casting object-dtype to PyArrow backed strings when `infer_string` option is set (`55638`)
  - Fixed bug in <span class="title-ref">Series.\_\_ne\_\_</span> resulting in False for comparison between `NA` and string value for `dtype="string[pyarrow_numpy]"` (`56122`)
  - Fixed bug in <span class="title-ref">Series.mode</span> not keeping object dtype when `infer_string` is set (`56183`)
  - Fixed bug in <span class="title-ref">Series.reset\_index</span> not preserving object dtype when `infer_string` is set (`56160`)
  - Fixed bug in <span class="title-ref">Series.str.split</span> and <span class="title-ref">Series.str.rsplit</span> when `pat=None` for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` (`56271`)
  - Fixed bug in <span class="title-ref">Series.str.translate</span> losing object dtype when string option is set (`56152`)

## Contributors

<div class="contributors">

v2.1.3..v2.1.4

</div>

---

v2.2.0.md

---

# What's new in 2.2.0 (January 19, 2024)

These are the changes in pandas 2.2.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Upcoming changes in pandas 3.0

pandas 3.0 will bring two bigger changes to the default behavior of pandas.

### Copy-on-Write

The currently optional mode Copy-on-Write will be enabled by default in pandas 3.0. There won't be an option to keep the current behavior enabled. The new behavioral semantics are explained in the \[user guide about Copy-on-Write \<copy\_on\_write\>\](\#user-guide-about-copy-on-write-\<copy\_on\_write\>).

The new behavior can be enabled since pandas 2.0 with the following option:

`` `ipython    pd.options.mode.copy_on_write = True  This change brings different changes in behavior in how pandas operates with respect to ``\` copies and views. Some of these changes allow a clear deprecation, like the changes in chained assignment. Other changes are more subtle and thus, the warnings are hidden behind an option that can be enabled in pandas 2.2.

`` `ipython    pd.options.mode.copy_on_write = "warn"  This mode will warn in many different scenarios that aren't actually relevant to ``\` most queries. We recommend exploring this mode, but it is not necessary to get rid of all of these warnings. The \[migration guide \<copy\_on\_write.migration\_guide\>\](\#migration-guide-\<copy\_on\_write.migration\_guide\>) explains the upgrade process in more detail.

### Dedicated string data type (backed by Arrow) by default

Historically, pandas represented string columns with NumPy object data type. This representation has numerous problems, including slow performance and a large memory footprint. This will change in pandas 3.0. pandas will start inferring string columns as a new `string` data type, backed by Arrow, which represents strings contiguous in memory. This brings a huge performance and memory improvement.

Old behavior:

`` `ipython     In [1]: ser = pd.Series(["a", "b"])     Out[1]:     0    a     1    b     dtype: object  New behavior:   .. code-block:: ipython      In [1]: ser = pd.Series(["a", "b"])     Out[1]:     0    a     1    b     dtype: string  The string data type that is used in these scenarios will mostly behave as NumPy ``\` object would, including missing value semantics and general operations on these columns.

This change includes a few additional changes across the API:

  - Currently, specifying `dtype="string"` creates a dtype that is backed by Python strings which are stored in a NumPy array. This will change in pandas 3.0, this dtype will create an Arrow backed string column.
  - The column names and the Index will also be backed by Arrow strings.
  - PyArrow will become a required dependency with pandas 3.0 to accommodate this change.

This future dtype inference logic can be enabled with:

`` `ipython    pd.options.future.infer_string = True  .. _whatsnew_220.enhancements:  Enhancements ``\` \~\~\~\~\~\~\~\~\~\~\~\~

### ADBC Driver support in to\_sql and read\_sql

<span class="title-ref">read\_sql</span> and <span class="title-ref">\~DataFrame.to\_sql</span> now work with [Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html) drivers. Compared to traditional drivers used via SQLAlchemy, ADBC drivers should provide significant performance improvements, better type support and cleaner nullability handling.

`` `ipython    import adbc_driver_postgresql.dbapi as pg_dbapi     df = pd.DataFrame(        [            [1, 2, 3],            [4, 5, 6],        ],        columns=['a', 'b', 'c']    )    uri = "postgresql://postgres:postgres@localhost/postgres"    with pg_dbapi.connect(uri) as conn:        df.to_sql("pandas_table", conn, index=False)     # for round-tripping    with pg_dbapi.connect(uri) as conn:        df2 = pd.read_sql("pandas_table", conn)  The Arrow type system offers a wider array of types that can more closely match ``\` what databases like PostgreSQL can offer. To illustrate, note this (non-exhaustive) listing of types available in different databases and pandas backends:

<table style="width:96%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 33%" />
<col style="width: 23%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>numpy/pandas</th>
<th>arrow</th>
<th>postgres</th>
<th>sqlite</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>int16/Int16</td>
<td>int16</td>
<td>SMALLINT</td>
<td>INTEGER</td>
</tr>
<tr class="even">
<td>int32/Int32</td>
<td>int32</td>
<td>INTEGER</td>
<td>INTEGER</td>
</tr>
<tr class="odd">
<td>int64/Int64</td>
<td>int64</td>
<td>BIGINT</td>
<td>INTEGER</td>
</tr>
<tr class="even">
<td>float32</td>
<td>float32</td>
<td>REAL</td>
<td>REAL</td>
</tr>
<tr class="odd">
<td>float64</td>
<td>float64</td>
<td>DOUBLE PRECISION</td>
<td>REAL</td>
</tr>
<tr class="even">
<td>object</td>
<td>string</td>
<td>TEXT</td>
<td>TEXT</td>
</tr>
<tr class="odd">
<td>bool</td>
<td><code>bool_</code></td>
<td>BOOLEAN</td>
<td></td>
</tr>
<tr class="even">
<td>datetime64[ns]</td>
<td>timestamp(us)</td>
<td>TIMESTAMP</td>
<td></td>
</tr>
<tr class="odd">
<td>datetime64[ns,tz]</td>
<td>timestamp(us,tz)</td>
<td>TIMESTAMPTZ</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>date32</td>
<td>DATE</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>month_day_nano_interval</td>
<td>INTERVAL</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>binary</td>
<td>BINARY</td>
<td>BLOB</td>
</tr>
<tr class="odd">
<td></td>
<td>decimal128</td>
<td>DECIMAL[1]</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>list</td>
<td>ARRAY[2]</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>struct</td>
<td><dl>
<dt>COMPOSITE TYPE</dt>
<dd><p>[3]</p>
</dd>
</dl></td>
<td></td>
</tr>
</tbody>
</table>

**Footnotes**

If you are interested in preserving database types as best as possible throughout the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"` argument of <span class="title-ref">\~pandas.read\_sql</span>

`` `ipython    # for round-tripping    with pg_dbapi.connect(uri) as conn:        df2 = pd.read_sql("pandas_table", conn, dtype_backend="pyarrow")  This will prevent your data from being converted to the traditional pandas/NumPy ``\` type system, which often converts SQL types in ways that make them impossible to round-trip.

For a full list of ADBC drivers and their development status, see the [ADBC Driver Implementation Status](https://arrow.apache.org/adbc/current/driver/status.html) documentation.

### Create a pandas Series based on one or more conditions

The <span class="title-ref">Series.case\_when</span> function has been added to create a Series object based on one or more conditions. (`39154`)

<div class="ipython">

python

import pandas as pd

df = pd.DataFrame(dict(a=\[1, 2, 3\], b=\[4, 5, 6\])) default=pd.Series('default', index=df.index) default.case\_when( caselist=\[ (df.a == 1, 'first'), \# condition, replacement (df.a.gt(1) & df.b.eq(5), 'second'), \# condition, replacement \], )

</div>

### `to_numpy` for NumPy nullable and Arrow types converts to suitable NumPy dtype

`to_numpy` for NumPy nullable and Arrow types will now convert to a suitable NumPy dtype instead of `object` dtype for nullable and PyArrow backed extension dtypes.

*Old behavior:*

`` `ipython     In [1]: ser = pd.Series([1, 2, 3], dtype="Int64")     In [2]: ser.to_numpy()     Out[2]: array([1, 2, 3], dtype=object)  *New behavior:*  .. ipython:: python      ser = pd.Series([1, 2, 3], dtype="Int64")     ser.to_numpy()      ser = pd.Series([1, 2, 3], dtype="timestamp[ns][pyarrow]")     ser.to_numpy()  The default NumPy dtype (without any arguments) is determined as follows:  - float dtypes are cast to NumPy floats ``<span class="title-ref"> - integer dtypes without missing values are cast to NumPy integer dtypes - integer dtypes with missing values are cast to NumPy float dtypes and </span><span class="title-ref">NaN</span><span class="title-ref"> is used as missing value indicator - boolean dtypes without missing values are cast to NumPy bool dtype - boolean dtypes with missing values keep object dtype - datetime and timedelta types are cast to Numpy datetime64 and timedelta64 types respectively and </span><span class="title-ref">NaT</span>\` is used as missing value indicator

### Series.struct accessor for PyArrow structured data

The `Series.struct` accessor provides attributes and methods for processing data with `struct[pyarrow]` dtype Series. For example, <span class="title-ref">Series.struct.explode</span> converts PyArrow structured data to a pandas DataFrame. (`54938`)

<div class="ipython">

python

import pyarrow as pa series = pd.Series( \[ {"project": "pandas", "version": "2.2.0"}, {"project": "numpy", "version": "1.25.2"}, {"project": "pyarrow", "version": "13.0.0"}, \], dtype=pd.ArrowDtype( pa.struct(\[ ("project", pa.string()), ("version", pa.string()), \]) ), ) series.struct.explode()

</div>

Use <span class="title-ref">Series.struct.field</span> to index into a (possible nested) struct field.

<div class="ipython">

python

series.struct.field("project")

</div>

### Series.list accessor for PyArrow list data

The `Series.list` accessor provides attributes and methods for processing data with `list[pyarrow]` dtype Series. For example, <span class="title-ref">Series.list.\_\_getitem\_\_</span> allows indexing pyarrow lists in a Series. (`55323`)

<div class="ipython">

python

import pyarrow as pa series = pd.Series( \[ \[1, 2, 3\], \[4, 5\], \[6\], \], dtype=pd.ArrowDtype( [pa.list]()(pa.int64()) ), ) series.list\[0\]

</div>

### Calamine engine for <span class="title-ref">read\_excel</span>

The `calamine` engine was added to <span class="title-ref">read\_excel</span>. It uses `python-calamine`, which provides Python bindings for the Rust library [calamine](https://crates.io/crates/calamine). This engine supports Excel files (`.xlsx`, `.xlsm`, `.xls`, `.xlsb`) and OpenDocument spreadsheets (`.ods`) (`50395`).

There are two advantages of this engine:

1.  Calamine is often faster than other engines, some benchmarks show results up to 5x faster than 'openpyxl', 20x - 'odf', 4x - 'pyxlsb', and 1.5x - 'xlrd'. But, 'openpyxl' and 'pyxlsb' are faster in reading a few rows from large files because of lazy iteration over rows.
2.  Calamine supports the recognition of datetime in `.xlsb` files, unlike 'pyxlsb' which is the only other engine in pandas that can read `.xlsb` files.

`` `python    pd.read_excel("path_to_file.xlsb", engine="calamine")   For more, see [io.calamine](#io.calamine) in the user guide on IO tools.  .. _whatsnew_220.enhancements.other:  Other enhancements ``\` ^^^^^^^^^^^^^^^^^^

  - <span class="title-ref">\~DataFrame.to\_sql</span> with method parameter set to `multi` works with Oracle on the backend
  - <span class="title-ref">Series.attrs</span> / <span class="title-ref">DataFrame.attrs</span> now uses a deepcopy for propagating `attrs` (`54134`).
  - <span class="title-ref">get\_dummies</span> now returning extension dtypes `boolean` or `bool[pyarrow]` that are compatible with the input dtype (`56273`)
  - <span class="title-ref">read\_csv</span> now supports `on_bad_lines` parameter with `engine="pyarrow"` (`54480`)
  - <span class="title-ref">read\_sas</span> returns `datetime64` dtypes with resolutions better matching those stored natively in SAS, and avoids returning object-dtype in cases that cannot be stored with `datetime64[ns]` dtype (`56127`)
  - <span class="title-ref">read\_spss</span> now returns a <span class="title-ref">DataFrame</span> that stores the metadata in <span class="title-ref">DataFrame.attrs</span> (`54264`)
  - <span class="title-ref">tseries.api.guess\_datetime\_format</span> is now part of the public API (`54727`)
  - <span class="title-ref">DataFrame.apply</span> now allows the usage of numba (via `engine="numba"`) to JIT compile the passed function, allowing for potential speedups (`54666`)
  - <span class="title-ref">ExtensionArray.\_explode</span> interface method added to allow extension type implementations of the `explode` method (`54833`)
  - <span class="title-ref">ExtensionArray.duplicated</span> added to allow extension type implementations of the `duplicated` method (`55255`)
  - <span class="title-ref">Series.ffill</span>, <span class="title-ref">Series.bfill</span>, <span class="title-ref">DataFrame.ffill</span>, and <span class="title-ref">DataFrame.bfill</span> have gained the argument `limit_area`; 3rd party <span class="title-ref">.ExtensionArray</span> authors need to add this argument to the method `_pad_or_backfill` (`56492`)
  - Allow passing `read_only`, `data_only` and `keep_links` arguments to openpyxl using `engine_kwargs` of <span class="title-ref">read\_excel</span> (`55027`)
  - Implement <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> for <span class="title-ref">ArrowDtype</span> and masked dtypes (`56267`)
  - Implement masked algorithms for <span class="title-ref">Series.value\_counts</span> (`54984`)
  - Implemented <span class="title-ref">Series.dt</span> methods and attributes for <span class="title-ref">ArrowDtype</span> with `pyarrow.duration` type (`52284`)
  - Implemented <span class="title-ref">Series.str.extract</span> for <span class="title-ref">ArrowDtype</span> (`56268`)
  - Improved error message that appears in <span class="title-ref">DatetimeIndex.to\_period</span> with frequencies which are not supported as period frequencies, such as `"BMS"` (`56243`)
  - Improved error message when constructing <span class="title-ref">Period</span> with invalid offsets such as `"QS"` (`55785`)
  - The dtypes `string[pyarrow]` and `string[pyarrow_numpy]` now both utilize the `large_string` type from PyArrow to avoid overflow for long columns (`56259`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### <span class="title-ref">merge</span> and <span class="title-ref">DataFrame.join</span> now consistently follow documented sort behavior

In previous versions of pandas, <span class="title-ref">merge</span> and <span class="title-ref">DataFrame.join</span> did not always return a result that followed the documented sort behavior. pandas now follows the documented sort behavior in merge and join operations (`54611`, `56426`, `56443`).

As documented, `sort=True` sorts the join keys lexicographically in the resulting <span class="title-ref">DataFrame</span>. With `sort=False`, the order of the join keys depends on the join type (`how` keyword):

  - `how="left"`: preserve the order of the left keys
  - `how="right"`: preserve the order of the right keys
  - `how="inner"`: preserve the order of the left keys
  - `how="outer"`: sort keys lexicographically

One example with changing behavior is inner joins with non-unique left join keys and `sort=False`:

<div class="ipython">

python

left = pd.DataFrame({"a": \[1, 2, 1\]}) right = pd.DataFrame({"a": \[1, 2\]}) result = pd.merge(left, right, how="inner", on="a", sort=False)

</div>

*Old Behavior*

`` `ipython     In [5]: result     Out[5]:        a     0  1     1  1     2  2  *New Behavior*  .. ipython:: python      result  .. _whatsnew_220.notable_bug_fixes.multiindex_join_different_levels:  `merge` and `DataFrame.join` no longer reorder levels when levels differ ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions of pandas, <span class="title-ref">merge</span> and <span class="title-ref">DataFrame.join</span> would reorder index levels when joining on two indexes with different levels (`34133`).

<div class="ipython">

python

left = pd.DataFrame({"left": 1}, index=pd.MultiIndex.from\_tuples(\[("x", 1), ("x", 2)\], names=\["A", "B"\])) right = pd.DataFrame({"right": 2}, index=pd.MultiIndex.from\_tuples(\[(1, 1), (2, 2)\], names=\["B", "C"\])) left right result = left.join(right)

</div>

*Old Behavior*

`` `ipython     In [5]: result     Out[5]:            left  right     B A C     1 x 1     1      2     2 x 2     1      2  *New Behavior*  .. ipython:: python      result  .. _whatsnew_220.api_breaking.deps:  Increased minimum versions for dependencies ``<span class="title-ref"> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ For \`optional dependencies \<https://pandas.pydata.org/docs/getting\_started/install.html\></span>\_ the general recommendation is to use the latest version. Optional dependencies below the lowest tested version may still work but are not considered supported. The following table lists the optional dependencies that have had their minimum tested version increased.

| Package        | New Minimum Version |
| -------------- | ------------------- |
| beautifulsoup4 | 4.11.2              |
| blosc          | 1.21.3              |
| bottleneck     | 1.3.6               |
| fastparquet    | 2022.12.0           |
| fsspec         | 2022.11.0           |
| gcsfs          | 2022.11.0           |
| lxml           | 4.9.2               |
| matplotlib     | 3.6.3               |
| numba          | 0.56.4              |
| numexpr        | 2.8.4               |
| qtpy           | 2.3.0               |
| openpyxl       | 3.1.0               |
| psycopg2       | 2.9.6               |
| pyreadstat     | 1.2.0               |
| pytables       | 3.8.0               |
| pyxlsb         | 1.0.10              |
| s3fs           | 2022.11.0           |
| scipy          | 1.10.0              |
| sqlalchemy     | 2.0.0               |
| tabulate       | 0.9.0               |
| xarray         | 2022.12.0           |
| xlsxwriter     | 3.0.5               |
| zstandard      | 0.19.0              |
| pyqt5          | 5.15.8              |
| tzdata         | 2022.7              |

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### Other API changes

  - The hash values of nullable extension dtypes changed to improve the performance of the hashing operation (`56507`)
  - `check_exact` now only takes effect for floating-point dtypes in <span class="title-ref">testing.assert\_frame\_equal</span> and <span class="title-ref">testing.assert\_series\_equal</span>. In particular, integer dtypes are always checked exactly (`55882`)

## Deprecations

### Chained assignment

In preparation of larger upcoming changes to the copy / view behaviour in pandas 3.0 (\[copy\_on\_write\](\#copy\_on\_write), PDEP-7), we started deprecating *chained assignment*.

Chained assignment occurs when you try to update a pandas DataFrame or Series through two subsequent indexing operations. Depending on the type and order of those operations this currently does or does not work.

A typical example is as follows:

`` `python     df = pd.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})      # first selecting rows with a mask, then assigning values to a column     # -> this has never worked and raises a SettingWithCopyWarning     df[df["bar"] > 5]["foo"] = 100      # first selecting the column, and then assigning to a subset of that column     # -> this currently works     df["foo"][df["bar"] > 5] = 100  This second example of chained assignment currently works to update the original ``df`.`\` This will no longer work in pandas 3.0, and therefore we started deprecating this:

`` `python     >>> df["foo"][df["bar"] > 5] = 100     FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!     You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.     A typical example is when you are setting values in a column of a DataFrame, like:      df["col"][row_indexer] = value      Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.      See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy  You can fix this warning and ensure your code is ready for pandas 3.0 by removing ``<span class="title-ref"> the usage of chained assignment. Typically, this can be done by doing the assignment in a single step using for example </span><span class="title-ref">.loc</span>\`. For the example above, we can do:

`` `python     df.loc[df["bar"] > 5, "foo"] = 100  The same deprecation applies to inplace methods that are done in a chained manner, such as:  .. code-block:: python      >>> df["foo"].fillna(0, inplace=True)     FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.     The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.      For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.  When the goal is to update the column in the DataFrame ``df`, the alternative here is`<span class="title-ref"> to call the method on </span><span class="title-ref">df</span><span class="title-ref"> itself, such as </span><span class="title-ref">df.fillna({"foo": 0}, inplace=True)</span>\`.

See more details in the \[migration guide \<copy\_on\_write.migration\_guide\>\](\#migration-guide-\<copy\_on\_write.migration\_guide\>).

### Deprecate aliases `M`, `Q`, `Y`, etc. in favour of `ME`, `QE`, `YE`, etc. for offsets

Deprecated the following frequency aliases (`9586`):

| offsets                                                   | deprecated aliases                                                     | new aliases |
| --------------------------------------------------------- | ---------------------------------------------------------------------- | ----------- |
| <span class="title-ref">MonthEnd</span> | \`              | <span class="title-ref">M</span><span class="title-ref"> | </span>\`   | ME\`\`      |
| <span class="title-ref">BusinessMonthEnd</span> | \`      | <span class="title-ref">BM</span><span class="title-ref"> | </span>\`  | BME\`\`     |
| <span class="title-ref">SemiMonthEnd</span> | \`          | <span class="title-ref">SM</span><span class="title-ref"> | </span>\`  | SME\`\`     |
| <span class="title-ref">CustomBusinessMonthEnd</span>| \` | <span class="title-ref">CBM</span><span class="title-ref"> | </span>\` | CBME\`\`    |
| <span class="title-ref">QuarterEnd</span> | \`            | <span class="title-ref">Q</span><span class="title-ref"> | </span>\`   | QE\`\`      |
| <span class="title-ref">BQuarterEnd</span> | \`           | <span class="title-ref">BQ</span><span class="title-ref"> | </span>\`  | BQE\`\`     |
| <span class="title-ref">YearEnd</span> | \`               | <span class="title-ref">Y</span><span class="title-ref"> | </span>\`   | YE\`\`      |
| <span class="title-ref">BYearEnd</span> | \`              | <span class="title-ref">BY</span><span class="title-ref"> | </span>\`  | BYE\`\`     |

For example:

*Previous behavior*:

`` `ipython     In [8]: pd.date_range('2020-01-01', periods=3, freq='Q-NOV')     Out[8]:     DatetimeIndex(['2020-02-29', '2020-05-31', '2020-08-31'],                   dtype='datetime64[ns]', freq='Q-NOV')  *Future behavior*:  .. ipython:: python      pd.date_range('2020-01-01', periods=3, freq='QE-NOV')  .. _whatsnew_220.silent_downcasting:  Deprecated automatic downcasting ``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Deprecated the automatic downcasting of object dtype results in a number of methods. These would silently change the dtype in a hard to predict manner since the behavior was value dependent. Additionally, pandas is moving away from silent dtype changes (`54710`, `54261`).

These methods are:

  - <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span>
  - <span class="title-ref">DataFrame.fillna</span>, <span class="title-ref">Series.fillna</span>
  - <span class="title-ref">DataFrame.ffill</span>, <span class="title-ref">Series.ffill</span>
  - <span class="title-ref">DataFrame.bfill</span>, <span class="title-ref">Series.bfill</span>
  - <span class="title-ref">DataFrame.mask</span>, <span class="title-ref">Series.mask</span>
  - <span class="title-ref">DataFrame.where</span>, <span class="title-ref">Series.where</span>
  - <span class="title-ref">DataFrame.clip</span>, <span class="title-ref">Series.clip</span>

Explicitly call <span class="title-ref">DataFrame.infer\_objects</span> to replicate the current behavior in the future.

`` `ipython     result = result.infer_objects(copy=False)  Or explicitly cast all-round floats to ints using ``astype`.  Set the following option to opt into the future behavior:  .. code-block:: ipython      In [9]: pd.set_option("future.no_silent_downcasting", True)  Other Deprecations`<span class="title-ref"> ^^^^^^^^^^^^^^^^^^ - Changed \`Timedelta.resolution\_string</span> to return `h`, `min`, `s`, `ms`, `us`, and `ns` instead of `H`, `T`, `S`, `L`, `U`, and `N`, for compatibility with respective deprecations in frequency aliases (`52536`) - Deprecated <span class="title-ref">offsets.Day.delta</span>, <span class="title-ref">offsets.Hour.delta</span>, <span class="title-ref">offsets.Minute.delta</span>, <span class="title-ref">offsets.Second.delta</span>, <span class="title-ref">offsets.Milli.delta</span>, <span class="title-ref">offsets.Micro.delta</span>, <span class="title-ref">offsets.Nano.delta</span>, use `pd.Timedelta(obj)` instead (`55498`) - Deprecated <span class="title-ref">pandas.api.types.is\_interval</span> and <span class="title-ref">pandas.api.types.is\_period</span>, use `isinstance(obj, pd.Interval)` and `isinstance(obj, pd.Period)` instead (`55264`) - Deprecated <span class="title-ref">read\_gbq</span> and <span class="title-ref">DataFrame.to\_gbq</span>. Use `pandas_gbq.read_gbq` and `pandas_gbq.to_gbq` instead <https://pandas-gbq.readthedocs.io/en/latest/api.html> (`55525`) - Deprecated <span class="title-ref">.DataFrameGroupBy.fillna</span> and <span class="title-ref">.SeriesGroupBy.fillna</span>; use <span class="title-ref">.DataFrameGroupBy.ffill</span>, <span class="title-ref">.DataFrameGroupBy.bfill</span> for forward and backward filling or <span class="title-ref">.DataFrame.fillna</span> to fill with a single value (or the Series equivalents) (`55718`) - Deprecated <span class="title-ref">DateOffset.is\_anchored</span>, use `obj.n == 1` for non-Tick subclasses (for Tick this was always False) (`55388`) - Deprecated <span class="title-ref">DatetimeArray.\_\_init\_\_</span> and <span class="title-ref">TimedeltaArray.\_\_init\_\_</span>, use <span class="title-ref">array</span> instead (`55623`) - Deprecated <span class="title-ref">Index.format</span>, use `index.astype(str)` or `index.map(formatter)` instead (`55413`) - Deprecated <span class="title-ref">Series.ravel</span>, the underlying array is already 1D, so ravel is not necessary (`52511`) - Deprecated <span class="title-ref">Series.resample</span> and <span class="title-ref">DataFrame.resample</span> with a <span class="title-ref">PeriodIndex</span> (and the 'convention' keyword), convert to <span class="title-ref">DatetimeIndex</span> (with `.to_timestamp()`) before resampling instead (`53481`) - Deprecated <span class="title-ref">Series.view</span>, use <span class="title-ref">Series.astype</span> instead to change the dtype (`20251`) - Deprecated <span class="title-ref">offsets.Tick.is\_anchored</span>, use `False` instead (`55388`) - Deprecated `core.internals` members `Block`, `ExtensionBlock`, and `DatetimeTZBlock`, use public APIs instead (`55139`) - Deprecated `year`, `month`, `quarter`, `day`, `hour`, `minute`, and `second` keywords in the <span class="title-ref">PeriodIndex</span> constructor, use <span class="title-ref">PeriodIndex.from\_fields</span> instead (`55960`) - Deprecated accepting a type as an argument in <span class="title-ref">Index.view</span>, call without any arguments instead (`55709`) - Deprecated allowing non-integer `periods` argument in <span class="title-ref">date\_range</span>, <span class="title-ref">timedelta\_range</span>, <span class="title-ref">period\_range</span>, and <span class="title-ref">interval\_range</span> (`56036`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_clipboard</span> (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_csv</span> except `path_or_buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_dict</span> (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_excel</span> except `excel_writer` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_gbq</span> except `destination_table` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_hdf</span> except `path_or_buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_html</span> except `buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_json</span> except `path_or_buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_latex</span> except `buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_markdown</span> except `buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_parquet</span> except `path` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_pickle</span> except `path` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_string</span> except `buf` (`54229`) - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.to\_xml</span> except `path_or_buffer` (`54229`) - Deprecated allowing passing <span class="title-ref">BlockManager</span> objects to <span class="title-ref">DataFrame</span> or <span class="title-ref">SingleBlockManager</span> objects to <span class="title-ref">Series</span> (`52419`) - Deprecated behavior of <span class="title-ref">Index.insert</span> with an object-dtype index silently performing type inference on the result, explicitly call `result.infer_objects(copy=False)` for the old behavior instead (`51363`) - Deprecated casting non-datetimelike values (mainly strings) in <span class="title-ref">Series.isin</span> and <span class="title-ref">Index.isin</span> with `datetime64`, `timedelta64`, and <span class="title-ref">PeriodDtype</span> dtypes (`53111`) - Deprecated dtype inference in <span class="title-ref">Index</span>, <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors when giving a pandas input, call `.infer_objects` on the input to keep the current behavior (`56012`) - Deprecated dtype inference when setting a <span class="title-ref">Index</span> into a <span class="title-ref">DataFrame</span>, cast explicitly instead (`56102`) - Deprecated including the groups in computations when using <span class="title-ref">.DataFrameGroupBy.apply</span> and <span class="title-ref">.DataFrameGroupBy.resample</span>; pass `include_groups=False` to exclude the groups (`7155`) - Deprecated indexing an <span class="title-ref">Index</span> with a boolean indexer of length zero (`55820`) - Deprecated not passing a tuple to <span class="title-ref">.DataFrameGroupBy.get\_group</span> or <span class="title-ref">.SeriesGroupBy.get\_group</span> when grouping by a length-1 list-like (`25971`) - Deprecated string `AS` denoting frequency in <span class="title-ref">YearBegin</span> and strings `AS-DEC`, `AS-JAN`, etc. denoting annual frequencies with various fiscal year starts (`54275`) - Deprecated string `A` denoting frequency in <span class="title-ref">YearEnd</span> and strings `A-DEC`, `A-JAN`, etc. denoting annual frequencies with various fiscal year ends (`54275`) - Deprecated string `BAS` denoting frequency in <span class="title-ref">BYearBegin</span> and strings `BAS-DEC`, `BAS-JAN`, etc. denoting annual frequencies with various fiscal year starts (`54275`) - Deprecated string `BA` denoting frequency in <span class="title-ref">BYearEnd</span> and strings `BA-DEC`, `BA-JAN`, etc. denoting annual frequencies with various fiscal year ends (`54275`) - Deprecated strings `H`, `BH`, and `CBH` denoting frequencies in <span class="title-ref">Hour</span>, <span class="title-ref">BusinessHour</span>, <span class="title-ref">CustomBusinessHour</span> (`52536`) - Deprecated strings `H`, `S`, `U`, and `N` denoting units in <span class="title-ref">to\_timedelta</span> (`52536`) - Deprecated strings `H`, `T`, `S`, `L`, `U`, and `N` denoting units in <span class="title-ref">Timedelta</span> (`52536`) - Deprecated strings `T`, `S`, `L`, `U`, and `N` denoting frequencies in <span class="title-ref">Minute</span>, <span class="title-ref">Second</span>, <span class="title-ref">Milli</span>, <span class="title-ref">Micro</span>, <span class="title-ref">Nano</span> (`52536`) - Deprecated support for combining parsed datetime columns in <span class="title-ref">read\_csv</span> along with the `keep_date_col` keyword (`55569`) - Deprecated the <span class="title-ref">.DataFrameGroupBy.grouper</span> and <span class="title-ref">SeriesGroupBy.grouper</span>; these attributes will be removed in a future version of pandas (`56521`) - Deprecated the <span class="title-ref">.Grouping</span> attributes `group_index`, `result_index`, and `group_arraylike`; these will be removed in a future version of pandas (`56148`) - Deprecated the `delim_whitespace` keyword in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span>, use `sep="\\s+"` instead (`55569`) - Deprecated the `errors="ignore"` option in <span class="title-ref">to\_datetime</span>, <span class="title-ref">to\_timedelta</span>, and <span class="title-ref">to\_numeric</span>; explicitly catch exceptions instead (`54467`) - Deprecated the `fastpath` keyword in the <span class="title-ref">Series</span> constructor (`20110`) - Deprecated the `kind` keyword in <span class="title-ref">Series.resample</span> and <span class="title-ref">DataFrame.resample</span>, explicitly cast the object's `index` instead (`55895`) - Deprecated the `ordinal` keyword in <span class="title-ref">PeriodIndex</span>, use <span class="title-ref">PeriodIndex.from\_ordinals</span> instead (`55960`) - Deprecated the `unit` keyword in <span class="title-ref">TimedeltaIndex</span> construction, use <span class="title-ref">to\_timedelta</span> instead (`55499`) - Deprecated the `verbose` keyword in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> (`55569`) - Deprecated the behavior of <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> with <span class="title-ref">CategoricalDtype</span>; in a future version replace will change the values while preserving the categories. To change the categories, use `ser.cat.rename_categories` instead (`55147`) - Deprecated the behavior of <span class="title-ref">Series.value\_counts</span> and <span class="title-ref">Index.value\_counts</span> with object dtype; in a future version these will not perform dtype inference on the resulting <span class="title-ref">Index</span>, do `result.index = result.index.infer_objects()` to retain the old behavior (`56161`) - Deprecated the default of `observed=False` in <span class="title-ref">DataFrame.pivot\_table</span>; will be `True` in a future version (`56236`) - Deprecated the extension test classes `BaseNoReduceTests`, `BaseBooleanReduceTests`, and `BaseNumericReduceTests`, use `BaseReduceTests` instead (`54663`) - Deprecated the option `mode.data_manager` and the `ArrayManager`; only the `BlockManager` will be available in future versions (`55043`) - Deprecated the previous implementation of <span class="title-ref">DataFrame.stack</span>; specify `future_stack=True` to adopt the future version (`53515`) -

## Performance improvements

  - Performance improvement in <span class="title-ref">.testing.assert\_frame\_equal</span> and <span class="title-ref">.testing.assert\_series\_equal</span> (`55949`, `55971`)
  - Performance improvement in <span class="title-ref">concat</span> with `axis=1` and objects with unaligned indexes (`55084`)
  - Performance improvement in <span class="title-ref">get\_dummies</span> (`56089`)
  - Performance improvement in <span class="title-ref">merge</span> and <span class="title-ref">merge\_ordered</span> when joining on sorted ascending keys (`56115`)
  - Performance improvement in <span class="title-ref">merge\_asof</span> when `by` is not `None` (`55580`, `55678`)
  - Performance improvement in <span class="title-ref">read\_stata</span> for files with many variables (`55515`)
  - Performance improvement in <span class="title-ref">DataFrame.groupby</span> when aggregating pyarrow timestamp and duration dtypes (`55031`)
  - Performance improvement in <span class="title-ref">DataFrame.join</span> when joining on unordered categorical indexes (`56345`)
  - Performance improvement in <span class="title-ref">DataFrame.loc</span> and <span class="title-ref">Series.loc</span> when indexing with a <span class="title-ref">MultiIndex</span> (`56062`)
  - Performance improvement in <span class="title-ref">DataFrame.sort\_index</span> and <span class="title-ref">Series.sort\_index</span> when indexed by a <span class="title-ref">MultiIndex</span> (`54835`)
  - Performance improvement in <span class="title-ref">DataFrame.to\_dict</span> on converting DataFrame to dictionary (`50990`)
  - Performance improvement in <span class="title-ref">Index.difference</span> (`55108`)
  - Performance improvement in <span class="title-ref">Index.sort\_values</span> when index is already sorted (`56128`)
  - Performance improvement in <span class="title-ref">MultiIndex.get\_indexer</span> when `method` is not `None` (`55839`)
  - Performance improvement in <span class="title-ref">Series.duplicated</span> for pyarrow dtypes (`55255`)
  - Performance improvement in <span class="title-ref">Series.str.get\_dummies</span> when dtype is `"string[pyarrow]"` or `"string[pyarrow_numpy]"` (`56110`)
  - Performance improvement in <span class="title-ref">Series.str</span> methods (`55736`)
  - Performance improvement in <span class="title-ref">Series.value\_counts</span> and <span class="title-ref">Series.mode</span> for masked dtypes (`54984`, `55340`)
  - Performance improvement in <span class="title-ref">.DataFrameGroupBy.nunique</span> and <span class="title-ref">.SeriesGroupBy.nunique</span> (`55972`)
  - Performance improvement in <span class="title-ref">.SeriesGroupBy.idxmax</span>, <span class="title-ref">.SeriesGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.DataFrameGroupBy.idxmin</span> (`54234`)
  - Performance improvement when hashing a nullable extension array (`56507`)
  - Performance improvement when indexing into a non-unique index (`55816`)
  - Performance improvement when indexing with more than 4 keys (`54550`)
  - Performance improvement when localizing time to UTC (`55241`)

## Bug fixes

### Categorical

  - <span class="title-ref">Categorical.isin</span> raising `InvalidIndexError` for categorical containing overlapping <span class="title-ref">Interval</span> values (`34974`)
  - Bug in <span class="title-ref">CategoricalDtype.\_\_eq\_\_</span> returning `False` for unordered categorical data with mixed types (`55468`)
  - Bug when casting `pa.dictionary` to <span class="title-ref">CategoricalDtype</span> using a `pa.DictionaryArray` as categories (`56672`)

### Datetimelike

  - Bug in <span class="title-ref">DatetimeIndex</span> construction when passing both a `tz` and either `dayfirst` or `yearfirst` ignoring dayfirst/yearfirst (`55813`)
  - Bug in <span class="title-ref">DatetimeIndex</span> when passing an object-dtype ndarray of float objects and a `tz` incorrectly localizing the result (`55780`)
  - Bug in <span class="title-ref">Series.isin</span> with <span class="title-ref">DatetimeTZDtype</span> dtype and comparison values that are all `NaT` incorrectly returning all-`False` even if the series contains `NaT` entries (`56427`)
  - Bug in <span class="title-ref">concat</span> raising `AttributeError` when concatenating all-NA DataFrame with <span class="title-ref">DatetimeTZDtype</span> dtype DataFrame (`52093`)
  - Bug in <span class="title-ref">testing.assert\_extension\_array\_equal</span> that could use the wrong unit when comparing resolutions (`55730`)
  - Bug in <span class="title-ref">to\_datetime</span> and <span class="title-ref">DatetimeIndex</span> when passing a list of mixed-string-and-numeric types incorrectly raising (`55780`)
  - Bug in <span class="title-ref">to\_datetime</span> and <span class="title-ref">DatetimeIndex</span> when passing mixed-type objects with a mix of timezones or mix of timezone-awareness failing to raise `ValueError` (`55693`)
  - Bug in <span class="title-ref">.Tick.delta</span> with very large ticks raising `OverflowError` instead of `OutOfBoundsTimedelta` (`55503`)
  - Bug in <span class="title-ref">DatetimeIndex.shift</span> with non-nanosecond resolution incorrectly returning with nanosecond resolution (`56117`)
  - Bug in <span class="title-ref">DatetimeIndex.union</span> returning object dtype for tz-aware indexes with the same timezone but different units (`55238`)
  - Bug in <span class="title-ref">Index.is\_monotonic\_increasing</span> and <span class="title-ref">Index.is\_monotonic\_decreasing</span> always caching <span class="title-ref">Index.is\_unique</span> as `True` when first value in index is `NaT` (`55755`)
  - Bug in <span class="title-ref">Index.view</span> to a datetime64 dtype with non-supported resolution incorrectly raising (`55710`)
  - Bug in <span class="title-ref">Series.dt.round</span> with non-nanosecond resolution and `NaT` entries incorrectly raising `OverflowError` (`56158`)
  - Bug in <span class="title-ref">Series.fillna</span> with non-nanosecond resolution dtypes and higher-resolution vector values returning incorrect (internally-corrupted) results (`56410`)
  - Bug in <span class="title-ref">Timestamp.unit</span> being inferred incorrectly from an ISO8601 format string with minute or hour resolution and a timezone offset (`56208`)
  - Bug in `.astype` converting from a higher-resolution `datetime64` dtype to a lower-resolution `datetime64` dtype (e.g. `datetime64[us]->datetime64[ms]`) silently overflowing with values near the lower implementation bound (`55979`)
  - Bug in adding or subtracting a <span class="title-ref">Week</span> offset to a `datetime64` <span class="title-ref">Series</span>, <span class="title-ref">Index</span>, or <span class="title-ref">DataFrame</span> column with non-nanosecond resolution returning incorrect results (`55583`)
  - Bug in addition or subtraction of <span class="title-ref">BusinessDay</span> offset with `offset` attribute to non-nanosecond <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> column giving incorrect results (`55608`)
  - Bug in addition or subtraction of <span class="title-ref">DateOffset</span> objects with microsecond components to `datetime64` <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> columns with non-nanosecond resolution (`55595`)
  - Bug in addition or subtraction of very large <span class="title-ref">.Tick</span> objects with <span class="title-ref">Timestamp</span> or <span class="title-ref">Timedelta</span> objects raising `OverflowError` instead of `OutOfBoundsTimedelta` (`55503`)
  - Bug in creating a <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> with a non-nanosecond <span class="title-ref">DatetimeTZDtype</span> and inputs that would be out of bounds with nanosecond resolution incorrectly raising `OutOfBoundsDatetime` (`54620`)
  - Bug in creating a <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> with a non-nanosecond `datetime64` (or <span class="title-ref">DatetimeTZDtype</span>) from mixed-numeric inputs treating those as nanoseconds instead of as multiples of the dtype's unit (which would happen with non-mixed numeric inputs) (`56004`)
  - Bug in creating a <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> with a non-nanosecond `datetime64` dtype and inputs that would be out of bounds for a `datetime64[ns]` incorrectly raising `OutOfBoundsDatetime` (`55756`)
  - Bug in parsing datetime strings with nanosecond resolution with non-ISO8601 formats incorrectly truncating sub-microsecond components (`56051`)
  - Bug in parsing datetime strings with sub-second resolution and trailing zeros incorrectly inferring second or millisecond resolution (`55737`)
  - Bug in the results of <span class="title-ref">to\_datetime</span> with an floating-dtype argument with `unit` not matching the pointwise results of <span class="title-ref">Timestamp</span> (`56037`)
  - Fixed regression where <span class="title-ref">concat</span> would raise an error when concatenating `datetime64` columns with differing resolutions (`53641`)

### Timedelta

  - Bug in <span class="title-ref">Timedelta</span> construction raising `OverflowError` instead of `OutOfBoundsTimedelta` (`55503`)
  - Bug in rendering (`__repr__`) of <span class="title-ref">TimedeltaIndex</span> and <span class="title-ref">Series</span> with timedelta64 values with non-nanosecond resolution entries that are all multiples of 24 hours failing to use the compact representation used in the nanosecond cases (`55405`)

### Timezones

  - Bug in <span class="title-ref">AbstractHolidayCalendar</span> where timezone data was not propagated when computing holiday observances (`54580`)
  - Bug in <span class="title-ref">Timestamp</span> construction with an ambiguous value and a `pytz` timezone failing to raise `pytz.AmbiguousTimeError` (`55657`)
  - Bug in <span class="title-ref">Timestamp.tz\_localize</span> with `nonexistent="shift_forward` around UTC+0 during DST (`51501`)

### Numeric

  - Bug in <span class="title-ref">read\_csv</span> with `engine="pyarrow"` causing rounding errors for large integers (`52505`)
  - Bug in <span class="title-ref">Series.\_\_floordiv\_\_</span> and <span class="title-ref">Series.\_\_truediv\_\_</span> for <span class="title-ref">ArrowDtype</span> with integral dtypes raising for large divisors (`56706`)
  - Bug in <span class="title-ref">Series.\_\_floordiv\_\_</span> for <span class="title-ref">ArrowDtype</span> with integral dtypes raising for large values (`56645`)
  - Bug in <span class="title-ref">Series.pow</span> not filling missing values correctly (`55512`)
  - Bug in <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> matching float `0.0` with `False` and vice versa (`55398`)
  - Bug in <span class="title-ref">Series.round</span> raising for nullable boolean dtype (`55936`)

### Conversion

  - Bug in <span class="title-ref">DataFrame.astype</span> when called with `str` on unpickled array - the array might change in-place (`54654`)
  - Bug in <span class="title-ref">DataFrame.astype</span> where `errors="ignore"` had no effect for extension types (`54654`)
  - Bug in <span class="title-ref">Series.convert\_dtypes</span> not converting all NA column to `null[pyarrow]` (`55346`)
  - Bug in `` `DataFrame.loc ``<span class="title-ref"> was not throwing "incompatible dtype warning" (see \`PDEP6 \<https://pandas.pydata.org/pdeps/0006-ban-upcasting.html\></span>\_) when assigning a `Series` with a different dtype using a full column setter (e.g. `df.loc[:, 'a'] = incompatible_value`) (`39584`)

### Strings

  - Bug in <span class="title-ref">pandas.api.types.is\_string\_dtype</span> while checking object array with no elements is of the string dtype (`54661`)
  - Bug in <span class="title-ref">DataFrame.apply</span> failing when `engine="numba"` and columns or index have `StringDtype` (`56189`)
  - Bug in <span class="title-ref">DataFrame.reindex</span> not matching <span class="title-ref">Index</span> with `string[pyarrow_numpy]` dtype (`56106`)
  - Bug in <span class="title-ref">Index.str.cat</span> always casting result to object dtype (`56157`)
  - Bug in <span class="title-ref">Series.\_\_mul\_\_</span> for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` dtype and `string[pyarrow]` for the pyarrow backend (`51970`)
  - Bug in <span class="title-ref">Series.str.find</span> when `start < 0` for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` (`56411`)
  - Bug in <span class="title-ref">Series.str.fullmatch</span> when `dtype=pandas.ArrowDtype(pyarrow.string()))` allows partial matches when regex ends in literal //$ (`56652`)
  - Bug in <span class="title-ref">Series.str.replace</span> when `n < 0` for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` (`56404`)
  - Bug in <span class="title-ref">Series.str.startswith</span> and <span class="title-ref">Series.str.endswith</span> with arguments of type `tuple[str, ...]` for <span class="title-ref">ArrowDtype</span> with `pyarrow.string` dtype (`56579`)
  - Bug in <span class="title-ref">Series.str.startswith</span> and <span class="title-ref">Series.str.endswith</span> with arguments of type `tuple[str, ...]` for `string[pyarrow]` (`54942`)
  - Bug in comparison operations for `dtype="string[pyarrow_numpy]"` raising if dtypes can't be compared (`56008`)

### Interval

  - Bug in <span class="title-ref">Interval</span> `__repr__` not displaying UTC offsets for <span class="title-ref">Timestamp</span> bounds. Additionally the hour, minute and second components will now be shown (`55015`)
  - Bug in <span class="title-ref">IntervalIndex.factorize</span> and <span class="title-ref">Series.factorize</span> with <span class="title-ref">IntervalDtype</span> with datetime64 or timedelta64 intervals not preserving non-nanosecond units (`56099`)
  - Bug in <span class="title-ref">IntervalIndex.from\_arrays</span> when passed `datetime64` or `timedelta64` arrays with mismatched resolutions constructing an invalid `IntervalArray` object (`55714`)
  - Bug in <span class="title-ref">IntervalIndex.from\_tuples</span> raising if subtype is a nullable extension dtype (`56765`)
  - Bug in <span class="title-ref">IntervalIndex.get\_indexer</span> with datetime or timedelta intervals incorrectly matching on integer targets (`47772`)
  - Bug in <span class="title-ref">IntervalIndex.get\_indexer</span> with timezone-aware datetime intervals incorrectly matching on a sequence of timezone-naive targets (`47772`)
  - Bug in setting values on a <span class="title-ref">Series</span> with an <span class="title-ref">IntervalIndex</span> using a slice incorrectly raising (`54722`)

### Indexing

  - Bug in <span class="title-ref">DataFrame.loc</span> mutating a boolean indexer when <span class="title-ref">DataFrame</span> has a <span class="title-ref">MultiIndex</span> (`56635`)
  - Bug in <span class="title-ref">DataFrame.loc</span> when setting <span class="title-ref">Series</span> with extension dtype into NumPy dtype (`55604`)
  - Bug in <span class="title-ref">Index.difference</span> not returning a unique set of values when `other` is empty or `other` is considered non-comparable (`55113`)
  - Bug in setting <span class="title-ref">Categorical</span> values into a <span class="title-ref">DataFrame</span> with numpy dtypes raising `RecursionError` (`52927`)
  - Fixed bug when creating new column with missing values when setting a single string value (`56204`)

### Missing

  - Bug in <span class="title-ref">DataFrame.update</span> wasn't updating in-place for tz-aware datetime64 dtypes (`56227`)

### MultiIndex

  - Bug in <span class="title-ref">MultiIndex.get\_indexer</span> not raising `ValueError` when `method` provided and index is non-monotonic (`53452`)

### I/O

  - Bug in <span class="title-ref">read\_csv</span> where `engine="python"` did not respect `chunksize` arg when `skiprows` was specified (`56323`)
  - Bug in <span class="title-ref">read\_csv</span> where `engine="python"` was causing a `TypeError` when a callable `skiprows` and a chunk size was specified (`55677`)
  - Bug in <span class="title-ref">read\_csv</span> where `on_bad_lines="warn"` would write to `stderr` instead of raising a Python warning; this now yields a <span class="title-ref">.errors.ParserWarning</span> (`54296`)
  - Bug in <span class="title-ref">read\_csv</span> with `engine="pyarrow"` where `quotechar` was ignored (`52266`)
  - Bug in <span class="title-ref">read\_csv</span> with `engine="pyarrow"` where `usecols` wasn't working with a CSV with no headers (`54459`)
  - Bug in <span class="title-ref">read\_excel</span>, with `engine="xlrd"` (`xls` files) erroring when the file contains `NaN` or `Inf` (`54564`)
  - Bug in <span class="title-ref">read\_json</span> not handling dtype conversion properly if `infer_string` is set (`56195`)
  - Bug in <span class="title-ref">DataFrame.to\_excel</span>, with `OdsWriter` (`ods` files) writing Boolean/string value (`54994`)
  - Bug in <span class="title-ref">DataFrame.to\_hdf</span> and <span class="title-ref">read\_hdf</span> with `datetime64` dtypes with non-nanosecond resolution failing to round-trip correctly (`55622`)
  - Bug in <span class="title-ref">DataFrame.to\_stata</span> raising for extension dtypes (`54671`)
  - Bug in <span class="title-ref">\~pandas.read\_excel</span> with `engine="odf"` (`ods` files) when a string cell contains an annotation (`55200`)
  - Bug in <span class="title-ref">\~pandas.read\_excel</span> with an ODS file without cached formatted cell for float values (`55219`)
  - Bug where <span class="title-ref">DataFrame.to\_json</span> would raise an `OverflowError` instead of a `TypeError` with unsupported NumPy types (`55403`)

### Period

  - Bug in <span class="title-ref">PeriodIndex</span> construction when more than one of `data`, `ordinal` and `**fields` are passed failing to raise `ValueError` (`55961`)
  - Bug in <span class="title-ref">Period</span> addition silently wrapping around instead of raising `OverflowError` (`55503`)
  - Bug in casting from <span class="title-ref">PeriodDtype</span> with `astype` to `datetime64` or <span class="title-ref">DatetimeTZDtype</span> with non-nanosecond unit incorrectly returning with nanosecond unit (`55958`)

### Plotting

  - Bug in <span class="title-ref">DataFrame.plot.box</span> with `vert=False` and a Matplotlib `Axes` created with `sharey=True` (`54941`)
  - Bug in <span class="title-ref">DataFrame.plot.scatter</span> discarding string columns (`56142`)
  - Bug in <span class="title-ref">Series.plot</span> when reusing an `ax` object failing to raise when a `how` keyword is passed (`55953`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.SeriesGroupBy.idxmin</span>, and <span class="title-ref">.SeriesGroupBy.idxmax</span> would not retain <span class="title-ref">.Categorical</span> dtype when the index was a <span class="title-ref">.CategoricalIndex</span> that contained NA values (`54234`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.transform</span> and <span class="title-ref">.SeriesGroupBy.transform</span> when `observed=False` and `f="idxmin"` or `f="idxmax"` would incorrectly raise on unobserved categories (`54234`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.value\_counts</span> and <span class="title-ref">.SeriesGroupBy.value\_counts</span> could result in incorrect sorting if the columns of the DataFrame or name of the Series are integers (`55951`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.value\_counts</span> and <span class="title-ref">.SeriesGroupBy.value\_counts</span> would not respect `sort=False` in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> (`55951`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.value\_counts</span> and <span class="title-ref">.SeriesGroupBy.value\_counts</span> would sort by proportions rather than frequencies when `sort=True` and `normalize=True` (`55951`)
  - Bug in <span class="title-ref">DataFrame.asfreq</span> and <span class="title-ref">Series.asfreq</span> with a <span class="title-ref">DatetimeIndex</span> with non-nanosecond resolution incorrectly converting to nanosecond resolution (`55958`)
  - Bug in <span class="title-ref">DataFrame.ewm</span> when passed `times` with non-nanosecond `datetime64` or <span class="title-ref">DatetimeTZDtype</span> dtype (`56262`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> where grouping by a combination of `Decimal` and NA values would fail when `sort=True` (`54847`)
  - Bug in <span class="title-ref">DataFrame.groupby</span> for DataFrame subclasses when selecting a subset of columns to apply the function to (`56761`)
  - Bug in <span class="title-ref">DataFrame.resample</span> not respecting `closed` and `label` arguments for <span class="title-ref">\~pandas.tseries.offsets.BusinessDay</span> (`55282`)
  - Bug in <span class="title-ref">DataFrame.resample</span> when resampling on a <span class="title-ref">ArrowDtype</span> of `pyarrow.timestamp` or `pyarrow.duration` type (`55989`)
  - Bug in <span class="title-ref">DataFrame.resample</span> where bin edges were not correct for <span class="title-ref">\~pandas.tseries.offsets.BusinessDay</span> (`55281`)
  - Bug in <span class="title-ref">DataFrame.resample</span> where bin edges were not correct for <span class="title-ref">\~pandas.tseries.offsets.MonthBegin</span> (`55271`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> and <span class="title-ref">Series.rolling</span> where duplicate datetimelike indexes are treated as consecutive rather than equal with `closed='left'` and `closed='neither'` (`20712`)
  - Bug in <span class="title-ref">DataFrame.rolling</span> and <span class="title-ref">Series.rolling</span> where either the `index` or `on` column was <span class="title-ref">ArrowDtype</span> with `pyarrow.timestamp` type (`55849`)

### Reshaping

  - Bug in <span class="title-ref">concat</span> ignoring `sort` parameter when passed <span class="title-ref">DatetimeIndex</span> indexes (`54769`)
  - Bug in <span class="title-ref">concat</span> renaming <span class="title-ref">Series</span> when `ignore_index=False` (`15047`)
  - Bug in <span class="title-ref">merge\_asof</span> raising `TypeError` when `by` dtype is not `object`, `int64`, or `uint64` (`22794`)
  - Bug in <span class="title-ref">merge\_asof</span> raising incorrect error for string dtype (`56444`)
  - Bug in <span class="title-ref">merge\_asof</span> when using a <span class="title-ref">Timedelta</span> tolerance on a <span class="title-ref">ArrowDtype</span> column (`56486`)
  - Bug in <span class="title-ref">merge</span> not raising when merging datetime columns with timedelta columns (`56455`)
  - Bug in <span class="title-ref">merge</span> not raising when merging string columns with numeric columns (`56441`)
  - Bug in <span class="title-ref">merge</span> not sorting for new string dtype (`56442`)
  - Bug in <span class="title-ref">merge</span> returning columns in incorrect order when left and/or right is empty (`51929`)
  - Bug in <span class="title-ref">DataFrame.melt</span> where an exception was raised if `var_name` was not a string (`55948`)
  - Bug in <span class="title-ref">DataFrame.melt</span> where it would not preserve the datetime (`55254`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> where the row margin is incorrect when the columns have numeric names (`26568`)
  - Bug in <span class="title-ref">DataFrame.pivot</span> with numeric columns and extension dtype for data (`56528`)
  - Bug in <span class="title-ref">DataFrame.stack</span> with `future_stack=True` would not preserve NA values in the index (`56573`)

### Sparse

  - Bug in <span class="title-ref">arrays.SparseArray.take</span> when using a different fill value than the array's fill value (`55181`)

### Other

  - <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> did not support pyarrow large strings (`56702`)
  - Bug in <span class="title-ref">DataFrame.describe</span> when formatting percentiles in the resulting percentile 99.999% is rounded to 100% (`55765`)
  - Bug in <span class="title-ref">api.interchange.from\_dataframe</span> where it raised `NotImplementedError` when handling empty string columns (`56703`)
  - Bug in <span class="title-ref">cut</span> and <span class="title-ref">qcut</span> with `datetime64` dtype values with non-nanosecond units incorrectly returning nanosecond-unit bins (`56101`)
  - Bug in <span class="title-ref">cut</span> incorrectly allowing cutting of timezone-aware datetimes with timezone-naive bins (`54964`)
  - Bug in <span class="title-ref">infer\_freq</span> and <span class="title-ref">DatetimeIndex.inferred\_freq</span> with weekly frequencies and non-nanosecond resolutions (`55609`)
  - Bug in <span class="title-ref">DataFrame.apply</span> where passing `raw=True` ignored `args` passed to the applied function (`55009`)
  - Bug in <span class="title-ref">DataFrame.from\_dict</span> which would always sort the rows of the created <span class="title-ref">DataFrame</span>. (`55683`)
  - Bug in <span class="title-ref">DataFrame.sort\_index</span> when passing `axis="columns"` and `ignore_index=True` raising a `ValueError` (`56478`)
  - Bug in rendering `inf` values inside a <span class="title-ref">DataFrame</span> with the `use_inf_as_na` option enabled (`55483`)
  - Bug in rendering a <span class="title-ref">Series</span> with a <span class="title-ref">MultiIndex</span> when one of the index level's names is 0 not having that name displayed (`55415`)
  - Bug in the error message when assigning an empty <span class="title-ref">DataFrame</span> to a column (`55956`)
  - Bug when time-like strings were being cast to <span class="title-ref">ArrowDtype</span> with `pyarrow.time64` type (`56463`)
  - Fixed a spurious deprecation warning from `numba` \>= 0.58.0 when passing a numpy ufunc in <span class="title-ref">core.window.Rolling.apply</span> with `engine="numba"` (`55247`)

## Contributors

<div class="contributors">

v2.1.4..v2.2.0

</div>

1.  Not implemented as of writing, but theoretically possible

2.  Not implemented as of writing, but theoretically possible

3.  Not implemented as of writing, but theoretically possible

---

v2.2.1.md

---

# What's new in 2.2.1 (February 22, 2024)

These are the changes in pandas 2.2.1. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

  - Added `pyarrow` pip extra so users can install pandas and pyarrow with pip with `pip install pandas[pyarrow]` (`54466`)

## Fixed regressions

  - Fixed memory leak in <span class="title-ref">read\_csv</span> (`57039`)
  - Fixed performance regression in <span class="title-ref">Series.combine\_first</span> (`55845`)
  - Fixed regression causing overflow for near-minimum timestamps (`57150`)
  - Fixed regression in <span class="title-ref">concat</span> changing long-standing behavior that always sorted the non-concatenation axis when the axis was a <span class="title-ref">DatetimeIndex</span> (`57006`)
  - Fixed regression in <span class="title-ref">merge\_ordered</span> raising `TypeError` for `fill_method="ffill"` and `how="left"` (`57010`)
  - Fixed regression in <span class="title-ref">pandas.testing.assert\_series\_equal</span> defaulting to `check_exact=True` when checking the <span class="title-ref">Index</span> (`57067`)
  - Fixed regression in <span class="title-ref">read\_json</span> where an <span class="title-ref">Index</span> would be returned instead of a <span class="title-ref">RangeIndex</span> (`57429`)
  - Fixed regression in <span class="title-ref">wide\_to\_long</span> raising an `AttributeError` for string columns (`57066`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.SeriesGroupBy.idxmin</span>, <span class="title-ref">.SeriesGroupBy.idxmax</span> ignoring the `skipna` argument (`57040`)
  - Fixed regression in <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.SeriesGroupBy.idxmin</span>, <span class="title-ref">.SeriesGroupBy.idxmax</span> where values containing the minimum or maximum value for the dtype could produce incorrect results (`57040`)
  - Fixed regression in <span class="title-ref">CategoricalIndex.difference</span> raising `KeyError` when other contains null values other than NaN (`57318`)
  - Fixed regression in <span class="title-ref">DataFrame.groupby</span> raising `ValueError` when grouping by a <span class="title-ref">Series</span> in some cases (`57276`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> raising `IndexError` for non-unique, masked dtype indexes where result has more than 10,000 rows (`57027`)
  - Fixed regression in <span class="title-ref">DataFrame.loc</span> which was unnecessarily throwing "incompatible dtype warning" when expanding with partial row indexer and multiple columns (see [PDEP6](https://pandas.pydata.org/pdeps/0006-ban-upcasting.html)) (`56503`)
  - Fixed regression in <span class="title-ref">DataFrame.map</span> with `na_action="ignore"` not being respected for NumPy nullable and <span class="title-ref">ArrowDtypes</span> (`57316`)
  - Fixed regression in <span class="title-ref">DataFrame.merge</span> raising `ValueError` for certain types of 3rd-party extension arrays (`57316`)
  - Fixed regression in <span class="title-ref">DataFrame.query</span> with all `NaT` column with object dtype (`57068`)
  - Fixed regression in <span class="title-ref">DataFrame.shift</span> raising `AssertionError` for `axis=1` and empty <span class="title-ref">DataFrame</span> (`57301`)
  - Fixed regression in <span class="title-ref">DataFrame.sort\_index</span> not producing a stable sort for a index with duplicates (`57151`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_dict</span> with `orient='list'` and datetime or timedelta types returning integers (`54824`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_json</span> converting nullable integers to floats (`57224`)
  - Fixed regression in <span class="title-ref">DataFrame.to\_sql</span> when `method="multi"` is passed and the dialect type is not Oracle (`57310`)
  - Fixed regression in <span class="title-ref">DataFrame.transpose</span> with nullable extension dtypes not having F-contiguous data potentially causing exceptions when used (`57315`)
  - Fixed regression in <span class="title-ref">DataFrame.update</span> emitting incorrect warnings about downcasting (`57124`)
  - Fixed regression in <span class="title-ref">DataFrameGroupBy.idxmin</span>, <span class="title-ref">DataFrameGroupBy.idxmax</span>, <span class="title-ref">SeriesGroupBy.idxmin</span>, <span class="title-ref">SeriesGroupBy.idxmax</span> ignoring the `skipna` argument (`57040`)
  - Fixed regression in <span class="title-ref">DataFrameGroupBy.idxmin</span>, <span class="title-ref">DataFrameGroupBy.idxmax</span>, <span class="title-ref">SeriesGroupBy.idxmin</span>, <span class="title-ref">SeriesGroupBy.idxmax</span> where values containing the minimum or maximum value for the dtype could produce incorrect results (`57040`)
  - Fixed regression in <span class="title-ref">ExtensionArray.to\_numpy</span> raising for non-numeric masked dtypes (`56991`)
  - Fixed regression in <span class="title-ref">Index.join</span> raising `TypeError` when joining an empty index to a non-empty index containing mixed dtype values (`57048`)
  - Fixed regression in <span class="title-ref">Series.astype</span> introducing decimals when converting from integer with missing values to string dtype (`57418`)
  - Fixed regression in <span class="title-ref">Series.pct\_change</span> raising a `ValueError` for an empty <span class="title-ref">Series</span> (`57056`)
  - Fixed regression in <span class="title-ref">Series.to\_numpy</span> when dtype is given as float and the data contains NaNs (`57121`)
  - Fixed regression in addition or subtraction of <span class="title-ref">DateOffset</span> objects with millisecond components to `datetime64` <span class="title-ref">Index</span>, <span class="title-ref">Series</span>, or <span class="title-ref">DataFrame</span> (`57529`)

## Bug fixes

  - Fixed bug in <span class="title-ref">pandas.api.interchange.from\_dataframe</span> which was raising for Nullable integers (`55069`)
  - Fixed bug in <span class="title-ref">pandas.api.interchange.from\_dataframe</span> which was raising for empty inputs (`56700`)
  - Fixed bug in <span class="title-ref">pandas.api.interchange.from\_dataframe</span> which wasn't converting columns names to strings (`55069`)
  - Fixed bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> for empty <span class="title-ref">DataFrame</span> with Copy-on-Write enabled (`57130`)
  - Fixed bug in <span class="title-ref">PeriodIndex.asfreq</span> which was silently converting frequencies which are not supported as period frequencies instead of raising an error (`56945`)

## Other

\> **Note** \> The `DeprecationWarning` that was raised when pandas was imported without PyArrow being installed has been removed. This decision was made because the warning was too noisy for too many users and a lot of feedback was collected about the decision to make PyArrow a required dependency. Pandas is currently considering the decision whether or not PyArrow should be added as a hard dependency in 3.0. Interested users can follow the discussion [here](https://github.com/pandas-dev/pandas/issues/57073).

  - Added the argument `skipna` to <span class="title-ref">DataFrameGroupBy.first</span>, <span class="title-ref">DataFrameGroupBy.last</span>, <span class="title-ref">SeriesGroupBy.first</span>, and <span class="title-ref">SeriesGroupBy.last</span>; achieving `skipna=False` used to be available via <span class="title-ref">DataFrameGroupBy.nth</span>, but the behavior was changed in pandas 2.0.0 (`57019`)
  - Added the argument `skipna` to <span class="title-ref">Resampler.first</span>, <span class="title-ref">Resampler.last</span> (`57019`)

## Contributors

<div class="contributors">

v2.2.0..v2.2.1

</div>

---

v2.2.2.md

---

# What's new in 2.2.2 (April 10, 2024)

These are the changes in pandas 2.2.2. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Pandas 2.2.2 is now compatible with numpy 2.0

Pandas 2.2.2 is the first version of pandas that is generally compatible with the upcoming numpy 2.0 release, and wheels for pandas 2.2.2 will work with both numpy 1.x and 2.x.

One major caveat is that arrays created with numpy 2.0's new `StringDtype` will convert to `object` dtyped arrays upon <span class="title-ref">Series</span>/<span class="title-ref">DataFrame</span> creation. Full support for numpy 2.0's StringDtype is expected to land in pandas 3.0.

As usual please report any bugs discovered to our [issue tracker](https://github.com/pandas-dev/pandas/issues/new/choose)

## Fixed regressions

  - <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> was producing incorrect data buffers when the a column's type was a pandas nullable on with missing values (`56702`)
  - <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> was producing incorrect data buffers when the a column's type was a pyarrow nullable on with missing values (`57664`)
  - Avoid issuing a spurious `DeprecationWarning` when a custom <span class="title-ref">DataFrame</span> or <span class="title-ref">Series</span> subclass method is called (`57553`)
  - Fixed regression in precision of <span class="title-ref">to\_datetime</span> with string and `unit` input (`57051`)

## Bug fixes

  - <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> was producing incorrect data buffers when the column's type was nullable boolean (`55332`)
  - <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> was showing bytemask instead of bitmask for `'string[pyarrow]'` validity buffer (`57762`)
  - <span class="title-ref">DataFrame.\_\_dataframe\_\_</span> was showing non-null validity buffer (instead of `None`) `'string[pyarrow]'` without missing values (`57761`)
  - <span class="title-ref">DataFrame.to\_sql</span> was failing to find the right table when using the schema argument (`57539`)

## Other

  - 
## Contributors

<div class="contributors">

v2.2.1..v2.2.2

</div>

---

v2.2.3.md

---

# What's new in 2.2.3 (September 20, 2024)

These are the changes in pandas 2.2.3. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Pandas 2.2.3 is now compatible with Python 3.13

Pandas 2.2.3 is the first version of pandas that is generally compatible with the upcoming Python 3.13, and both wheels for free-threaded and normal Python 3.13 will be uploaded for this release.

As usual please report any bugs discovered to our [issue tracker](https://github.com/pandas-dev/pandas/issues/new/choose)

## Bug fixes

  - Bug in <span class="title-ref">eval</span> on <span class="title-ref">complex</span> including division `/` discards imaginary part. (`21374`)
  - Minor fixes for numpy 2.1 compatibility. (`59444`)

## Other

  - Missing licenses for 3rd party dependencies were added back into the wheels. (`58632`)

## Contributors

<div class="contributors">

v2.2.2..v2.2.3|HEAD

</div>

---

v2.3.0.md

---

# What's new in 2.3.0 (Month XX, 2024)

These are the changes in pandas 2.3.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Upcoming changes in pandas 3.0

## Enhancements

### enhancement1

### Other enhancements

  - The semantics for the `copy` keyword in `__array__` methods (i.e. called when using `np.array()` or `np.asarray()` on pandas objects) has been updated to work correctly with NumPy \>= 2 (`57739`)
  - The <span class="title-ref">\~Series.sum</span> reduction is now implemented for `StringDtype` columns (`59853`)
  - 
## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### notable\_bug\_fix1

## Deprecations

  - Deprecated allowing non-`bool` values for `na` in <span class="title-ref">.str.contains</span>, <span class="title-ref">.str.startswith</span>, and <span class="title-ref">.str.endswith</span> for dtypes that do not already disallow these (`59615`)
  - Deprecated the `"pyarrow_numpy"` storage option for <span class="title-ref">StringDtype</span> (`60152`)

## Performance improvements

  - 
  - 
## Bug fixes

### Categorical

  - 
  - 
### Datetimelike

  - 
  - 
### Timedelta

  - 
  - 
### Timezones

  - 
  - 
### Numeric

  - 
  - 
### Conversion

  - 
  - 
### Strings

  - Bug in <span class="title-ref">Series.rank</span> for <span class="title-ref">StringDtype</span> with `storage="pyarrow"` incorrectly returning integer results in case of `method="average"` and raising an error if it would truncate results (`59768`)
  - Bug in <span class="title-ref">Series.replace</span> with <span class="title-ref">StringDtype</span> when replacing with a non-string value was not upcasting to `object` dtype (`60282`)
  - Bug in <span class="title-ref">Series.str.replace</span> when `n < 0` for <span class="title-ref">StringDtype</span> with `storage="pyarrow"` (`59628`)
  - Bug in `ser.str.slice` with negative `step` with <span class="title-ref">ArrowDtype</span> and <span class="title-ref">StringDtype</span> with `storage="pyarrow"` giving incorrect results (`59710`)
  - Bug in the `center` method on <span class="title-ref">Series</span> and <span class="title-ref">Index</span> object `str` accessors with pyarrow-backed dtype not matching the python behavior in corner cases with an odd number of fill characters (`54792`)

### Interval

  - 
  - 
### Indexing

  - Fixed bug in <span class="title-ref">Index.get\_indexer</span> round-tripping through string dtype when `infer_string` is enabled (`55834`)
  - 
### Missing

  - 
  - 
### MultiIndex

  - 
  - 
### I/O

  - <span class="title-ref">DataFrame.to\_excel</span> was storing decimals as strings instead of numbers (`49598`)
  - 
### Period

  - 
  - 
### Plotting

  - 
  - 
### Groupby/resample/rolling

  - 
  - 
### Reshaping

  - 
  - 
### Sparse

  - 
  - 
### ExtensionArray

  - 
  - 
### Styler

  - 
  - 
### Other

  - Fixed usage of `inspect` when the optional dependencies `pyarrow` or `jinja2` are not installed (`60196`)
  - 
## Contributors

---

v3.0.0.md

---

# What's new in 3.0.0 (Month XX, 2024)

These are the changes in pandas 3.0.0. See \[release\](\#release) for a full changelog including other versions of pandas.

{{ header }}

## Enhancements

### Enhancement1

### Enhancement2

### Other enhancements

  - <span class="title-ref">pandas.api.typing.FrozenList</span> is available for typing the outputs of <span class="title-ref">MultiIndex.names</span>, <span class="title-ref">MultiIndex.codes</span> and <span class="title-ref">MultiIndex.levels</span> (`58237`)
  - <span class="title-ref">pandas.api.typing.SASReader</span> is available for typing the output of <span class="title-ref">read\_sas</span> (`55689`)
  - <span class="title-ref">DataFrame.to\_excel</span> now raises an `UserWarning` when the character count in a cell exceeds Excel's limitation of 32767 characters (`56954`)
  - <span class="title-ref">pandas.merge</span> now validates the `how` parameter input (merge type) (`59435`)
  - <span class="title-ref">read\_spss</span> now supports kwargs to be passed to pyreadstat (`56356`)
  - <span class="title-ref">read\_stata</span> now returns `datetime64` resolutions better matching those natively stored in the stata format (`55642`)
  - <span class="title-ref">DataFrame.agg</span> called with `axis=1` and a `func` which relabels the result index now raises a `NotImplementedError` (`58807`).
  - <span class="title-ref">Index.get\_loc</span> now accepts also subclasses of `tuple` as keys (`57922`)
  - <span class="title-ref">Styler.set\_tooltips</span> provides alternative method to storing tooltips by using title attribute of td elements. (`56981`)
  - Added missing parameter `weights` in <span class="title-ref">DataFrame.plot.kde</span> for the estimation of the PDF (`59337`)
  - Allow dictionaries to be passed to <span class="title-ref">pandas.Series.str.replace</span> via `pat` parameter (`51748`)
  - Support passing a <span class="title-ref">Series</span> input to <span class="title-ref">json\_normalize</span> that retains the <span class="title-ref">Series</span> <span class="title-ref">Index</span> (`51452`)
  - Support reading value labels from Stata 108-format (Stata 6) and earlier files (`58154`)
  - Users can globally disable any `PerformanceWarning` by setting the option `mode.performance_warnings` to `False` (`56920`)
  - <span class="title-ref">Styler.format\_index\_names</span> can now be used to format the index and column names (`48936` and `47489`)
  - <span class="title-ref">.errors.DtypeWarning</span> improved to include column names when mixed data types are detected (`58174`)
  - <span class="title-ref">Series</span> now supports the Arrow PyCapsule Interface for export (`59518`)
  - <span class="title-ref">DataFrame.to\_excel</span> argument `merge_cells` now accepts a value of `"columns"` to only merge <span class="title-ref">MultiIndex</span> column header header cells (`35384`)
  - <span class="title-ref">DataFrame.corrwith</span> now accepts `min_periods` as optional arguments, as in <span class="title-ref">DataFrame.corr</span> and <span class="title-ref">Series.corr</span> (`9490`)
  - <span class="title-ref">DataFrame.cummin</span>, <span class="title-ref">DataFrame.cummax</span>, <span class="title-ref">DataFrame.cumprod</span> and <span class="title-ref">DataFrame.cumsum</span> methods now have a `numeric_only` parameter (`53072`)
  - <span class="title-ref">DataFrame.ewm</span> now allows `adjust=False` when `times` is provided (`54328`)
  - <span class="title-ref">DataFrame.fillna</span> and <span class="title-ref">Series.fillna</span> can now accept `value=None`; for non-object dtype the corresponding NA value will be used (`57723`)
  - <span class="title-ref">DataFrame.pivot\_table</span> and <span class="title-ref">pivot\_table</span> now allow the passing of keyword arguments to `aggfunc` through `**kwargs` (`57884`)
  - <span class="title-ref">Series.cummin</span> and <span class="title-ref">Series.cummax</span> now supports <span class="title-ref">CategoricalDtype</span> (`52335`)
  - <span class="title-ref">Series.plot</span> now correctly handle the `ylabel` parameter for pie charts, allowing for explicit control over the y-axis label (`58239`)
  - <span class="title-ref">DataFrame.plot.scatter</span> argument `c` now accepts a column of strings, where rows with the same string are colored identically (`16827` and `16485`)
  - <span class="title-ref">DataFrameGroupBy.transform</span>, <span class="title-ref">SeriesGroupBy.transform</span>, <span class="title-ref">DataFrameGroupBy.agg</span>, <span class="title-ref">SeriesGroupBy.agg</span>, <span class="title-ref">RollingGroupby.apply</span>, <span class="title-ref">ExpandingGroupby.apply</span>, <span class="title-ref">Rolling.apply</span>, <span class="title-ref">Expanding.apply</span>, <span class="title-ref">DataFrame.apply</span> with `engine="numba"` now supports positional arguments passed as kwargs (`58995`)
  - <span class="title-ref">Series.map</span> can now accept kwargs to pass on to func (`59814`)
  - <span class="title-ref">pandas.concat</span> will raise a `ValueError` when `ignore_index=True` and `keys` is not `None` (`59274`)
  - <span class="title-ref">str.get\_dummies</span> now accepts a `dtype` parameter to specify the dtype of the resulting DataFrame (`47872`)
  - Multiplying two <span class="title-ref">DateOffset</span> objects will now raise a `TypeError` instead of a `RecursionError` (`59442`)
  - Restore support for reading Stata 104-format and enable reading 103-format dta files (`58554`)
  - Support passing a <span class="title-ref">Iterable\[Hashable\]</span> input to <span class="title-ref">DataFrame.drop\_duplicates</span> (`59237`)
  - Support reading Stata 102-format (Stata 1) dta files (`58978`)
  - Support reading Stata 110-format (Stata 7) dta files (`47176`)

## Notable bug fixes

These are bug fixes that might have notable behavior changes.

### Improved behavior in groupby for `observed=False`

A number of bugs have been fixed due to improved handling of unobserved groups (`55738`). All remarks in this section equally impact <span class="title-ref">.SeriesGroupBy</span>.

In previous versions of pandas, a single grouping with <span class="title-ref">.DataFrameGroupBy.apply</span> or <span class="title-ref">.DataFrameGroupBy.agg</span> would pass the unobserved groups to the provided function, resulting in `0` below.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "key1": pd.Categorical(list("aabb"), categories=list("abc")), "key2": \[1, 1, 1, 2\], "values": \[1, 2, 3, 4\],
    
    }

) df gb = df.groupby("key1", observed=False) gb\[\["values"\]\].apply(lambda x: x.sum())

</div>

However this was not the case when using multiple groupings, resulting in `NaN` below.

`` `ipython     In [1]: gb = df.groupby(["key1", "key2"], observed=False)     In [2]: gb[["values"]].apply(lambda x: x.sum())     Out[2]:                values     key1 key2     a    1        3.0          2        NaN     b    1        3.0          2        4.0     c    1        NaN          2        NaN  Now using multiple groupings will also pass the unobserved groups to the provided function.  .. ipython:: python      gb = df.groupby(["key1", "key2"], observed=False)     gb[["values"]].apply(lambda x: x.sum())  Similarly:  - In previous versions of pandas the method `.DataFrameGroupBy.sum` would result in ``0``for unobserved groups, but `.DataFrameGroupBy.prod`, `.DataFrameGroupBy.all`, and `.DataFrameGroupBy.any` would all result in NA values. Now these methods result in``1`,`True`, and`False`respectively.`<span class="title-ref"> - </span>.DataFrameGroupBy.groups\` did not include unobserved groups and now does.

These improvements also fixed certain bugs in groupby:

  - <span class="title-ref">.DataFrameGroupBy.agg</span> would fail when there are multiple groupings, unobserved groups, and `as_index=False` (`36698`)
  - <span class="title-ref">.DataFrameGroupBy.groups</span> with `sort=False` would sort groups; they now occur in the order they are observed (`56966`)
  - <span class="title-ref">.DataFrameGroupBy.nunique</span> would fail when there are multiple groupings, unobserved groups, and `as_index=False` (`52848`)
  - <span class="title-ref">.DataFrameGroupBy.sum</span> would have incorrect values when there are multiple groupings, unobserved groups, and non-numeric data (`43891`)
  - <span class="title-ref">.DataFrameGroupBy.value\_counts</span> would produce incorrect results when used with some categorical and some non-categorical groupings and `observed=False` (`56016`)

### notable\_bug\_fix2

## Backwards incompatible API changes

### Datetime resolution inference

Converting a sequence of strings, `datetime` objects, or `np.datetime64` objects to a `datetime64` dtype now performs inference on the appropriate resolution (AKA unit) for the output dtype. This affects <span class="title-ref">Series</span>, <span class="title-ref">DataFrame</span>, <span class="title-ref">Index</span>, <span class="title-ref">DatetimeIndex</span>, and <span class="title-ref">to\_datetime</span>.

Previously, these would always give nanosecond resolution:

`` `ipython     In [1]: dt = pd.Timestamp("2024-03-22 11:36").to_pydatetime()     In [2]: pd.to_datetime([dt]).dtype     Out[2]: dtype('<M8[ns]')     In [3]: pd.Index([dt]).dtype     Out[3]: dtype('<M8[ns]')     In [4]: pd.DatetimeIndex([dt]).dtype     Out[4]: dtype('<M8[ns]')     In [5]: pd.Series([dt]).dtype     Out[5]: dtype('<M8[ns]')  This now infers the unit microsecond unit "us" from the pydatetime object, matching the scalar `Timestamp` behavior.  .. ipython:: python      In [1]: dt = pd.Timestamp("2024-03-22 11:36").to_pydatetime()     In [2]: pd.to_datetime([dt]).dtype     In [3]: pd.Index([dt]).dtype     In [4]: pd.DatetimeIndex([dt]).dtype     In [5]: pd.Series([dt]).dtype  Similar when passed a sequence of ``np.datetime64``objects, the resolution of the passed objects will be retained (or for lower-than-second resolution, second resolution will be used).  When passing strings, the resolution will depend on the precision of the string, again matching the `Timestamp` behavior. Previously:  .. code-block:: ipython      In [2]: pd.to_datetime(["2024-03-22 11:43:01"]).dtype     Out[2]: dtype('<M8[ns]')     In [3]: pd.to_datetime(["2024-03-22 11:43:01.002"]).dtype     Out[3]: dtype('<M8[ns]')     In [4]: pd.to_datetime(["2024-03-22 11:43:01.002003"]).dtype     Out[4]: dtype('<M8[ns]')     In [5]: pd.to_datetime(["2024-03-22 11:43:01.002003004"]).dtype     Out[5]: dtype('<M8[ns]')  The inferred resolution now matches that of the input strings:  .. ipython:: python      In [2]: pd.to_datetime(["2024-03-22 11:43:01"]).dtype     In [3]: pd.to_datetime(["2024-03-22 11:43:01.002"]).dtype     In [4]: pd.to_datetime(["2024-03-22 11:43:01.002003"]).dtype     In [5]: pd.to_datetime(["2024-03-22 11:43:01.002003004"]).dtype  In cases with mixed-resolution inputs, the highest resolution is used:  .. code-block:: ipython      In [2]: pd.to_datetime([pd.Timestamp("2024-03-22 11:43:01"), "2024-03-22 11:43:01.002"]).dtype     Out[2]: dtype('<M8[ns]')  .. _whatsnew_300.api_breaking.value_counts_sorting:  Changed behavior in `DataFrame.value_counts` and `DataFrameGroupBy.value_counts` when``sort=False`  `\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions of pandas, <span class="title-ref">DataFrame.value\_counts</span> with `sort=False` would sort the result by row labels (as was documented). This was nonintuitive and inconsistent with <span class="title-ref">Series.value\_counts</span> which would maintain the order of the input. Now <span class="title-ref">DataFrame.value\_counts</span> will maintain the order of the input.

<div class="ipython">

python

  - df = pd.DataFrame(
    
      - {  
        "a": \[2, 2, 2, 2, 1, 1, 1, 1\], "b": \[2, 1, 3, 1, 2, 3, 1, 1\],
    
    }

) df

</div>

*Old behavior*

`` `ipython     In [3]: df.value_counts(sort=False)     Out[3]:     a  b     1  1    2        2    1        3    1     2  1    2        2    1        3    1     Name: count, dtype: int64  *New behavior*  .. ipython:: python      df.value_counts(sort=False)  This change also applies to `.DataFrameGroupBy.value_counts`. Here, there are two options for sorting: one ``sort``passed to `DataFrame.groupby` and one passed directly to `.DataFrameGroupBy.value_counts`. The former will determine whether to sort the groups, the latter whether to sort the counts. All non-grouping columns will maintain the order of the input *within groups*.  *Old behavior*  .. code-block:: ipython      In [5]: df.groupby("a", sort=True).value_counts(sort=False)     Out[5]:     a  b     1  1    2        2    1        3    1     2  1    2        2    1        3    1     dtype: int64  *New behavior*  .. ipython:: python      df.groupby("a", sort=True).value_counts(sort=False)  .. _whatsnew_300.api_breaking.deps:  Increased minimum version for Python``\` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 3.0.0 supports Python 3.10 and higher.

### Increased minimum versions for dependencies

Some minimum supported versions of dependencies were updated. If installed, we now require:

<table style="width:81%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Minimum Version</th>
<th>Required</th>
<th>Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>numpy</td>
<td>1.23.5</td>
<td><blockquote>
<p>X</p>
</blockquote></td>
<td><blockquote>
<p>X</p>
</blockquote></td>
</tr>
</tbody>
</table>

For [optional libraries](https://pandas.pydata.org/docs/getting_started/install.html) the general recommendation is to use the latest version. The following table lists the lowest version per library that is currently being tested throughout the development of pandas. Optional libraries below the lowest tested version may still work, but are not considered supported.

| Package                | New Minimum Version |
| ---------------------- | ------------------- |
| pytz                   | 2023.4              |
| fastparquet            | 2023.10.0           |
| adbc-driver-postgresql | 0.10.0              |
| mypy (dev)             | 1.9.0               |

See \[install.dependencies\](\#install.dependencies) and \[install.optional\_dependencies\](\#install.optional\_dependencies) for more.

### `pytz` now an optional dependency

pandas now uses :py`zoneinfo` from the standard library as the default timezone implementation when passing a timezone string to various methods. (`34916`)

*Old behavior:*

`` `ipython     In [1]: ts = pd.Timestamp(2024, 1, 1).tz_localize("US/Pacific")     In [2]: ts.tz     <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>  *New behavior:*  .. ipython:: python      ts = pd.Timestamp(2024, 1, 1).tz_localize("US/Pacific")     ts.tz ``pytz`timezone objects are still supported when passed directly, but they will no longer be returned by default`<span class="title-ref"> from string inputs. Moreover, </span><span class="title-ref">pytz</span><span class="title-ref"> is no longer a required dependency of pandas, but can be installed with the pip extra </span><span class="title-ref">pip install pandas\[timezone\]</span>\`.

Additionally, pandas no longer throws `pytz` exceptions for timezone operations leading to ambiguous or nonexistent times. These cases will now raise a `ValueError`.

### Other API changes

  - 3rd party `py.path` objects are no longer explicitly supported in IO methods. Use :py\`pathlib.Path\` objects instead (`57091`)
  - <span class="title-ref">read\_table</span>'s `parse_dates` argument defaults to `None` to improve consistency with <span class="title-ref">read\_csv</span> (`57476`)
  - All classes inheriting from builtin `tuple` (including types created with <span class="title-ref">collections.namedtuple</span>) are now hashed and compared as builtin `tuple` during indexing operations (`57922`)
  - Made `dtype` a required argument in <span class="title-ref">ExtensionArray.\_from\_sequence\_of\_strings</span> (`56519`)
  - Passing a <span class="title-ref">Series</span> input to <span class="title-ref">json\_normalize</span> will now retain the <span class="title-ref">Series</span> <span class="title-ref">Index</span>, previously output had a new <span class="title-ref">RangeIndex</span> (`51452`)
  - Removed <span class="title-ref">Index.sort</span> which always raised a `TypeError`. This attribute is not defined and will raise an `AttributeError` (`59283`)
  - Updated <span class="title-ref">DataFrame.to\_excel</span> so that the output spreadsheet has no styling. Custom styling can still be done using <span class="title-ref">Styler.to\_excel</span> (`54154`)
  - pickle and HDF (`.h5`) files created with Python 2 are no longer explicitly supported (`57387`)
  - pickled objects from pandas version less than `1.0.0` are no longer supported (`57155`)
  - when comparing the indexes in <span class="title-ref">testing.assert\_series\_equal</span>, check\_exact defaults to True if an <span class="title-ref">Index</span> is of integer dtypes. (`57386`)

## Deprecations

### Copy keyword

The `copy` keyword argument in the following methods is deprecated and will be removed in a future version:

  - <span class="title-ref">DataFrame.truncate</span> / <span class="title-ref">Series.truncate</span>
  - <span class="title-ref">DataFrame.tz\_convert</span> / <span class="title-ref">Series.tz\_convert</span>
  - <span class="title-ref">DataFrame.tz\_localize</span> / <span class="title-ref">Series.tz\_localize</span>
  - <span class="title-ref">DataFrame.infer\_objects</span> / <span class="title-ref">Series.infer\_objects</span>
  - <span class="title-ref">DataFrame.align</span> / <span class="title-ref">Series.align</span>
  - <span class="title-ref">DataFrame.astype</span> / <span class="title-ref">Series.astype</span>
  - <span class="title-ref">DataFrame.reindex</span> / <span class="title-ref">Series.reindex</span>
  - <span class="title-ref">DataFrame.reindex\_like</span> / <span class="title-ref">Series.reindex\_like</span>
  - <span class="title-ref">DataFrame.set\_axis</span> / <span class="title-ref">Series.set\_axis</span>
  - <span class="title-ref">DataFrame.to\_period</span> / <span class="title-ref">Series.to\_period</span>
  - <span class="title-ref">DataFrame.to\_timestamp</span> / <span class="title-ref">Series.to\_timestamp</span>
  - <span class="title-ref">DataFrame.rename</span> / <span class="title-ref">Series.rename</span>
  - <span class="title-ref">DataFrame.transpose</span>
  - <span class="title-ref">DataFrame.swaplevel</span>
  - <span class="title-ref">DataFrame.merge</span> / <span class="title-ref">pd.merge</span>

Copy-on-Write utilizes a lazy copy mechanism that defers copying the data until necessary. Use `.copy` to trigger an eager copy. The copy keyword has no effect starting with 3.0, so it can be safely removed from your code.

### Other Deprecations

  - Deprecated <span class="title-ref">core.internals.api.make\_block</span>, use public APIs instead (`56815`)
  - Deprecated <span class="title-ref">.DataFrameGroupby.corrwith</span> (`57158`)
  - Deprecated <span class="title-ref">Timestamp.utcfromtimestamp</span>, use `Timestamp.fromtimestamp(ts, "UTC")` instead (`56680`)
  - Deprecated <span class="title-ref">Timestamp.utcnow</span>, use `Timestamp.now("UTC")` instead (`56680`)
  - Deprecated allowing non-keyword arguments in <span class="title-ref">DataFrame.all</span>, <span class="title-ref">DataFrame.min</span>, <span class="title-ref">DataFrame.max</span>, <span class="title-ref">DataFrame.sum</span>, <span class="title-ref">DataFrame.prod</span>, <span class="title-ref">DataFrame.mean</span>, <span class="title-ref">DataFrame.median</span>, <span class="title-ref">DataFrame.sem</span>, <span class="title-ref">DataFrame.var</span>, <span class="title-ref">DataFrame.std</span>, <span class="title-ref">DataFrame.skew</span>, <span class="title-ref">DataFrame.kurt</span>, <span class="title-ref">Series.all</span>, <span class="title-ref">Series.min</span>, <span class="title-ref">Series.max</span>, <span class="title-ref">Series.sum</span>, <span class="title-ref">Series.prod</span>, <span class="title-ref">Series.mean</span>, <span class="title-ref">Series.median</span>, <span class="title-ref">Series.sem</span>, <span class="title-ref">Series.var</span>, <span class="title-ref">Series.std</span>, <span class="title-ref">Series.skew</span>, and <span class="title-ref">Series.kurt</span>. (`57087`)
  - Deprecated allowing non-keyword arguments in <span class="title-ref">Series.to\_markdown</span> except `buf`. (`57280`)
  - Deprecated allowing non-keyword arguments in <span class="title-ref">Series.to\_string</span> except `buf`. (`57280`)
  - Deprecated behavior of <span class="title-ref">.DataFrameGroupBy.groups</span> and <span class="title-ref">.SeriesGroupBy.groups</span>, in a future version `groups` by one element list will return tuple instead of scalar. (`58858`)
  - Deprecated behavior of <span class="title-ref">Series.dt.to\_pytimedelta</span>, in a future version this will return a <span class="title-ref">Series</span> containing python `datetime.timedelta` objects instead of an `ndarray` of timedelta; this matches the behavior of other <span class="title-ref">Series.dt</span> properties. (`57463`)
  - Deprecated lowercase strings `d`, `b` and `c` denoting frequencies in <span class="title-ref">Day</span>, <span class="title-ref">BusinessDay</span> and <span class="title-ref">CustomBusinessDay</span> in favour of `D`, `B` and `C` (`58998`)
  - Deprecated lowercase strings `w`, `w-mon`, `w-tue`, etc. denoting frequencies in <span class="title-ref">Week</span> in favour of `W`, `W-MON`, `W-TUE`, etc. (`58998`)
  - Deprecated parameter `method` in <span class="title-ref">DataFrame.reindex\_like</span> / <span class="title-ref">Series.reindex\_like</span> (`58667`)
  - Deprecated strings `w`, `d`, `MIN`, `MS`, `US` and `NS` denoting units in <span class="title-ref">Timedelta</span> in favour of `W`, `D`, `min`, `ms`, `us` and `ns` (`59051`)
  - Deprecated using `epoch` date format in <span class="title-ref">DataFrame.to\_json</span> and <span class="title-ref">Series.to\_json</span>, use `iso` instead. (`57063`)

## Removal of prior version deprecations/changes

### Enforced deprecation of aliases `M`, `Q`, `Y`, etc. in favour of `ME`, `QE`, `YE`, etc. for offsets

Renamed the following offset aliases (`57986`):

| offset                                                    | removed alias                                                          | new alias |
| --------------------------------------------------------- | ---------------------------------------------------------------------- | --------- |
| <span class="title-ref">MonthEnd</span> | \`              | <span class="title-ref">M</span><span class="title-ref"> | </span>\`   | ME\`\`    |
| <span class="title-ref">BusinessMonthEnd</span> | \`      | <span class="title-ref">BM</span><span class="title-ref"> | </span>\`  | BME\`\`   |
| <span class="title-ref">SemiMonthEnd</span> | \`          | <span class="title-ref">SM</span><span class="title-ref"> | </span>\`  | SME\`\`   |
| <span class="title-ref">CustomBusinessMonthEnd</span>| \` | <span class="title-ref">CBM</span><span class="title-ref"> | </span>\` | CBME\`\`  |
| <span class="title-ref">QuarterEnd</span> | \`            | <span class="title-ref">Q</span><span class="title-ref"> | </span>\`   | QE\`\`    |
| <span class="title-ref">BQuarterEnd</span> | \`           | <span class="title-ref">BQ</span><span class="title-ref"> | </span>\`  | BQE\`\`   |
| <span class="title-ref">YearEnd</span> | \`               | <span class="title-ref">Y</span><span class="title-ref"> | </span>\`   | YE\`\`    |
| <span class="title-ref">BYearEnd</span> | \`              | <span class="title-ref">BY</span><span class="title-ref"> | </span>\`  | BYE\`\`   |

### Other Removals

  - <span class="title-ref">.DataFrameGroupBy.idxmin</span>, <span class="title-ref">.DataFrameGroupBy.idxmax</span>, <span class="title-ref">.SeriesGroupBy.idxmin</span>, and <span class="title-ref">.SeriesGroupBy.idxmax</span> will now raise a `ValueError` when used with `skipna=False` and an NA value is encountered (`10694`)
  - <span class="title-ref">concat</span> no longer ignores empty objects when determining output dtypes (`39122`)
  - <span class="title-ref">concat</span> with all-NA entries no longer ignores the dtype of those entries when determining the result dtype (`40893`)
  - <span class="title-ref">read\_excel</span>, <span class="title-ref">read\_json</span>, <span class="title-ref">read\_html</span>, and <span class="title-ref">read\_xml</span> no longer accept raw string or byte representation of the data. That type of data must be wrapped in a :py\`StringIO\` or :py\`BytesIO\` (`53767`)
  - <span class="title-ref">to\_datetime</span> with a `unit` specified no longer parses strings into floats, instead parses them the same way as without `unit` (`50735`)
  - <span class="title-ref">DataFrame.groupby</span> with `as_index=False` and aggregation methods will no longer exclude from the result the groupings that do not arise from the input (`49519`)
  - <span class="title-ref">ExtensionArray.\_reduce</span> now requires a `keepdims: bool = False` parameter in the signature (`52788`)
  - <span class="title-ref">Series.dt.to\_pydatetime</span> now returns a <span class="title-ref">Series</span> of :py\`datetime.datetime\` objects (`52459`)
  - <span class="title-ref">SeriesGroupBy.agg</span> no longer pins the name of the group to the input passed to the provided `func` (`51703`)
  - All arguments except `name` in <span class="title-ref">Index.rename</span> are now keyword only (`56493`)
  - All arguments except the first `path`-like argument in IO writers are now keyword only (`54229`)
  - Changed behavior of <span class="title-ref">Series.\_\_getitem\_\_</span> and <span class="title-ref">Series.\_\_setitem\_\_</span> to always treat integer keys as labels, never as positional, consistent with <span class="title-ref">DataFrame</span> behavior (`50617`)
  - Changed behavior of <span class="title-ref">Series.\_\_getitem\_\_</span>, <span class="title-ref">Series.\_\_setitem\_\_</span>, <span class="title-ref">DataFrame.\_\_getitem\_\_</span>, <span class="title-ref">DataFrame.\_\_setitem\_\_</span> with an integer slice on objects with a floating-dtype index. This is now treated as *positional* indexing (`49612`)
  - Disallow a callable argument to <span class="title-ref">Series.iloc</span> to return a `tuple` (`53769`)
  - Disallow allowing logical operations (`||`, `&`, `^`) between pandas objects and dtype-less sequences (e.g. `list`, `tuple`); wrap the objects in <span class="title-ref">Series</span>, <span class="title-ref">Index</span>, or `np.array` first instead (`52264`)
  - Disallow automatic casting to object in <span class="title-ref">Series</span> logical operations (`&`, `^`, `||`) between series with mismatched indexes and dtypes other than `object` or `bool` (`52538`)
  - Disallow calling <span class="title-ref">Series.replace</span> or <span class="title-ref">DataFrame.replace</span> without a `value` and with non-dict-like `to_replace` (`33302`)
  - Disallow constructing a <span class="title-ref">arrays.SparseArray</span> with scalar data (`53039`)
  - Disallow indexing an <span class="title-ref">Index</span> with a boolean indexer of length zero, it now raises `ValueError` (`55820`)
  - Disallow non-standard (`np.ndarray`, <span class="title-ref">Index</span>, <span class="title-ref">ExtensionArray</span>, or <span class="title-ref">Series</span>) to <span class="title-ref">isin</span>, <span class="title-ref">unique</span>, <span class="title-ref">factorize</span> (`52986`)
  - Disallow passing a pandas type to <span class="title-ref">Index.view</span> (`55709`)
  - Disallow units other than "s", "ms", "us", "ns" for datetime64 and timedelta64 dtypes in <span class="title-ref">array</span> (`53817`)
  - Removed "freq" keyword from <span class="title-ref">PeriodArray</span> constructor, use "dtype" instead (`52462`)
  - Removed 'fastpath' keyword in <span class="title-ref">Categorical</span> constructor (`20110`)
  - Removed 'kind' keyword in <span class="title-ref">Series.resample</span> and <span class="title-ref">DataFrame.resample</span> (`58125`)
  - Removed `Block`, `DatetimeTZBlock`, `ExtensionBlock`, `create_block_manager_from_blocks` from `pandas.core.internals` and `pandas.core.internals.api` (`55139`)
  - Removed alias <span class="title-ref">arrays.PandasArray</span> for <span class="title-ref">arrays.NumpyExtensionArray</span> (`53694`)
  - Removed deprecated "method" and "limit" keywords from <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> (`53492`)
  - Removed extension test classes `BaseNoReduceTests`, `BaseNumericReduceTests`, `BaseBooleanReduceTests` (`54663`)
  - Removed the "closed" and "normalize" keywords in <span class="title-ref">DatetimeIndex.\_\_new\_\_</span> (`52628`)
  - Removed the deprecated `delim_whitespace` keyword in <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span>, use `sep=r"\s+"` instead (`55569`)
  - Require <span class="title-ref">SparseDtype.fill\_value</span> to be a valid value for the <span class="title-ref">SparseDtype.subtype</span> (`53043`)
  - Stopped automatically casting non-datetimelike values (mainly strings) in <span class="title-ref">Series.isin</span> and <span class="title-ref">Index.isin</span> with `datetime64`, `timedelta64`, and <span class="title-ref">PeriodDtype</span> dtypes (`53111`)
  - Stopped performing dtype inference in <span class="title-ref">Index</span>, <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> constructors when given a pandas object (<span class="title-ref">Series</span>, <span class="title-ref">Index</span>, <span class="title-ref">ExtensionArray</span>), call `.infer_objects` on the input to keep the current behavior (`56012`)
  - Stopped performing dtype inference when setting a <span class="title-ref">Index</span> into a <span class="title-ref">DataFrame</span> (`56102`)
  - Stopped performing dtype inference with in <span class="title-ref">Index.insert</span> with object-dtype index; this often affects the index/columns that result when setting new entries into an empty <span class="title-ref">Series</span> or <span class="title-ref">DataFrame</span> (`51363`)
  - Removed the "closed" and "unit" keywords in <span class="title-ref">TimedeltaIndex.\_\_new\_\_</span> (`52628`, `55499`)
  - All arguments in <span class="title-ref">Index.sort\_values</span> are now keyword only (`56493`)
  - All arguments in <span class="title-ref">Series.to\_dict</span> are now keyword only (`56493`)
  - Changed the default value of `na_action` in <span class="title-ref">Categorical.map</span> to `None` (`51645`)
  - Changed the default value of `observed` in <span class="title-ref">DataFrame.groupby</span> and <span class="title-ref">Series.groupby</span> to `True` (`51811`)
  - Enforce deprecation in <span class="title-ref">testing.assert\_series\_equal</span> and <span class="title-ref">testing.assert\_frame\_equal</span> with object dtype and mismatched null-like values, which are now considered not-equal (`18463`)
  - Enforce banning of upcasting in in-place setitem-like operations (`59007`) (see [PDEP6](https://pandas.pydata.org/pdeps/0006-ban-upcasting.html))
  - Enforced deprecation `all` and `any` reductions with `datetime64`, <span class="title-ref">DatetimeTZDtype</span>, and <span class="title-ref">PeriodDtype</span> dtypes (`58029`)
  - Enforced deprecation disallowing `float` "periods" in <span class="title-ref">date\_range</span>, <span class="title-ref">period\_range</span>, <span class="title-ref">timedelta\_range</span>, <span class="title-ref">interval\_range</span>, (`56036`)
  - Enforced deprecation disallowing parsing datetimes with mixed time zones unless user passes `utc=True` to <span class="title-ref">to\_datetime</span> (`57275`)
  - Enforced deprecation in <span class="title-ref">Series.value\_counts</span> and <span class="title-ref">Index.value\_counts</span> with object dtype performing dtype inference on the `.index` of the result (`56161`)
  - Enforced deprecation of <span class="title-ref">.DataFrameGroupBy.get\_group</span> and <span class="title-ref">.SeriesGroupBy.get\_group</span> allowing the `name` argument to be a non-tuple when grouping by a list of length 1 (`54155`)
  - Enforced deprecation of <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> for object-dtype (`57820`)
  - Enforced deprecation of <span class="title-ref">offsets.Tick.delta</span>, use `pd.Timedelta(obj)` instead (`55498`)
  - Enforced deprecation of `axis=None` acting the same as `axis=0` in the DataFrame reductions `sum`, `prod`, `std`, `var`, and `sem`, passing `axis=None` will now reduce over both axes; this is particularly the case when doing e.g. `numpy.sum(df)` (`21597`)
  - Enforced deprecation of `core.internals` member `DatetimeTZBlock` (`58467`)
  - Enforced deprecation of `date_parser` in <span class="title-ref">read\_csv</span>, <span class="title-ref">read\_table</span>, <span class="title-ref">read\_fwf</span>, and <span class="title-ref">read\_excel</span> in favour of `date_format` (`50601`)
  - Enforced deprecation of `keep_date_col` keyword in <span class="title-ref">read\_csv</span> (`55569`)
  - Enforced deprecation of `quantile` keyword in <span class="title-ref">.Rolling.quantile</span> and <span class="title-ref">.Expanding.quantile</span>, renamed to `q` instead. (`52550`)
  - Enforced deprecation of argument `infer_datetime_format` in <span class="title-ref">read\_csv</span>, as a strict version of it is now the default (`48621`)
  - Enforced deprecation of combining parsed datetime columns in <span class="title-ref">read\_csv</span> in `parse_dates` (`55569`)
  - Enforced deprecation of non-standard (`np.ndarray`, <span class="title-ref">ExtensionArray</span>, <span class="title-ref">Index</span>, or <span class="title-ref">Series</span>) argument to <span class="title-ref">api.extensions.take</span> (`52981`)
  - Enforced deprecation of parsing system timezone strings to `tzlocal`, which depended on system timezone, pass the 'tz' keyword instead (`50791`)
  - Enforced deprecation of passing a dictionary to <span class="title-ref">SeriesGroupBy.agg</span> (`52268`)
  - Enforced deprecation of string `AS` denoting frequency in <span class="title-ref">YearBegin</span> and strings `AS-DEC`, `AS-JAN`, etc. denoting annual frequencies with various fiscal year starts (`57793`)
  - Enforced deprecation of string `A` denoting frequency in <span class="title-ref">YearEnd</span> and strings `A-DEC`, `A-JAN`, etc. denoting annual frequencies with various fiscal year ends (`57699`)
  - Enforced deprecation of string `BAS` denoting frequency in <span class="title-ref">BYearBegin</span> and strings `BAS-DEC`, `BAS-JAN`, etc. denoting annual frequencies with various fiscal year starts (`57793`)
  - Enforced deprecation of string `BA` denoting frequency in <span class="title-ref">BYearEnd</span> and strings `BA-DEC`, `BA-JAN`, etc. denoting annual frequencies with various fiscal year ends (`57793`)
  - Enforced deprecation of strings `H`, `BH`, and `CBH` denoting frequencies in <span class="title-ref">Hour</span>, <span class="title-ref">BusinessHour</span>, <span class="title-ref">CustomBusinessHour</span> (`59143`)
  - Enforced deprecation of strings `H`, `BH`, and `CBH` denoting units in <span class="title-ref">Timedelta</span> (`59143`)
  - Enforced deprecation of strings `T`, `L`, `U`, and `N` denoting frequencies in <span class="title-ref">Minute</span>, <span class="title-ref">Milli</span>, <span class="title-ref">Micro</span>, <span class="title-ref">Nano</span> (`57627`)
  - Enforced deprecation of strings `T`, `L`, `U`, and `N` denoting units in <span class="title-ref">Timedelta</span> (`57627`)
  - Enforced deprecation of the behavior of <span class="title-ref">concat</span> when `len(keys) != len(objs)` would truncate to the shorter of the two. Now this raises a `ValueError` (`43485`)
  - Enforced deprecation of the behavior of <span class="title-ref">DataFrame.replace</span> and <span class="title-ref">Series.replace</span> with <span class="title-ref">CategoricalDtype</span> that would introduce new categories. (`58270`)
  - Enforced deprecation of the behavior of <span class="title-ref">Series.argsort</span> in the presence of NA values (`58232`)
  - Enforced deprecation of values "pad", "ffill", "bfill", and "backfill" for <span class="title-ref">Series.interpolate</span> and <span class="title-ref">DataFrame.interpolate</span> (`57869`)
  - Enforced deprecation removing <span class="title-ref">Categorical.to\_list</span>, use `obj.tolist()` instead (`51254`)
  - Enforced silent-downcasting deprecation for \[all relevant methods \<whatsnew\_220.silent\_downcasting\>\](\#all-relevant-methods-\<whatsnew\_220.silent\_downcasting\>) (`54710`)
  - In <span class="title-ref">DataFrame.stack</span>, the default value of `future_stack` is now `True`; specifying `False` will raise a `FutureWarning` (`55448`)
  - Iterating over a <span class="title-ref">.DataFrameGroupBy</span> or <span class="title-ref">.SeriesGroupBy</span> will return tuples of length 1 for the groups when grouping by `level` a list of length 1 (`50064`)
  - Methods `apply`, `agg`, and `transform` will no longer replace NumPy functions (e.g. `np.sum`) and built-in functions (e.g. `min`) with the equivalent pandas implementation; use string aliases (e.g. `"sum"` and `"min"`) if you desire to use the pandas implementation (`53974`)
  - Passing both `freq` and `fill_value` in <span class="title-ref">DataFrame.shift</span> and <span class="title-ref">Series.shift</span> and <span class="title-ref">.DataFrameGroupBy.shift</span> now raises a `ValueError` (`54818`)
  - Removed <span class="title-ref">.DataFrameGroupBy.quantile</span> and <span class="title-ref">.SeriesGroupBy.quantile</span> supporting bool dtype (`53975`)
  - Removed <span class="title-ref">DateOffset.is\_anchored</span> and <span class="title-ref">offsets.Tick.is\_anchored</span> (`56594`)
  - Removed `DataFrame.applymap`, `Styler.applymap` and `Styler.applymap_index` (`52364`)
  - Removed `DataFrame.bool` and `Series.bool` (`51756`)
  - Removed `DataFrame.first` and `DataFrame.last` (`53710`)
  - Removed `DataFrame.swapaxes` and `Series.swapaxes` (`51946`)
  - Removed `DataFrameGroupBy.grouper` and `SeriesGroupBy.grouper` (`56521`)
  - Removed `DataFrameGroupby.fillna` and `SeriesGroupBy.fillna`<span class="title-ref"> (:issue:\`55719</span>)
  - Removed `Index.format`, use <span class="title-ref">Index.astype</span> with `str` or <span class="title-ref">Index.map</span> with a `formatter` function instead (`55439`)
  - Removed `Resample.fillna` (`55719`)
  - Removed `Series.__int__` and `Series.__float__`. Call `int(Series.iloc[0])` or `float(Series.iloc[0])` instead. (`51131`)
  - Removed `Series.ravel` (`56053`)
  - Removed `Series.view` (`56054`)
  - Removed `StataReader.close` (`49228`)
  - Removed `_data` from <span class="title-ref">DataFrame</span>, <span class="title-ref">Series</span>, <span class="title-ref">.arrays.ArrowExtensionArray</span> (`52003`)
  - Removed `axis` argument from <span class="title-ref">DataFrame.groupby</span>, <span class="title-ref">Series.groupby</span>, <span class="title-ref">DataFrame.rolling</span>, <span class="title-ref">Series.rolling</span>, <span class="title-ref">DataFrame.resample</span>, and <span class="title-ref">Series.resample</span> (`51203`)
  - Removed `axis` argument from all groupby operations (`50405`)
  - Removed `convert_dtype` from <span class="title-ref">Series.apply</span> (`52257`)
  - Removed `method`, `limit` `fill_axis` and `broadcast_axis` keywords from <span class="title-ref">DataFrame.align</span> (`51968`)
  - Removed `pandas.api.types.is_interval` and `pandas.api.types.is_period`, use `isinstance(obj, pd.Interval)` and `isinstance(obj, pd.Period)` instead (`55264`)
  - Removed `pandas.io.sql.execute` (`50185`)
  - Removed `pandas.value_counts`, use <span class="title-ref">Series.value\_counts</span> instead (`53493`)
  - Removed `read_gbq` and `DataFrame.to_gbq`. Use `pandas_gbq.read_gbq` and `pandas_gbq.to_gbq` instead <https://pandas-gbq.readthedocs.io/en/latest/api.html> (`55525`)
  - Removed `use_nullable_dtypes` from <span class="title-ref">read\_parquet</span> (`51853`)
  - Removed `year`, `month`, `quarter`, `day`, `hour`, `minute`, and `second` keywords in the <span class="title-ref">PeriodIndex</span> constructor, use <span class="title-ref">PeriodIndex.from\_fields</span> instead (`55960`)
  - Removed argument `limit` from <span class="title-ref">DataFrame.pct\_change</span>, <span class="title-ref">Series.pct\_change</span>, <span class="title-ref">.DataFrameGroupBy.pct\_change</span>, and <span class="title-ref">.SeriesGroupBy.pct\_change</span>; the argument `method` must be set to `None` and will be removed in a future version of pandas (`53520`)
  - Removed deprecated argument `obj` in <span class="title-ref">.DataFrameGroupBy.get\_group</span> and <span class="title-ref">.SeriesGroupBy.get\_group</span> (`53545`)
  - Removed deprecated behavior of <span class="title-ref">Series.agg</span> using <span class="title-ref">Series.apply</span> (`53325`)
  - Removed deprecated keyword `method` on <span class="title-ref">Series.fillna</span>, <span class="title-ref">DataFrame.fillna</span> (`57760`)
  - Removed option `mode.use_inf_as_na`, convert inf entries to `NaN` before instead (`51684`)
  - Removed support for <span class="title-ref">DataFrame</span> in <span class="title-ref">DataFrame.from\_records</span>(`51697`)
  - Removed support for `errors="ignore"` in <span class="title-ref">to\_datetime</span>, <span class="title-ref">to\_timedelta</span> and <span class="title-ref">to\_numeric</span> (`55734`)
  - Removed support for `slice` in <span class="title-ref">DataFrame.take</span> (`51539`)
  - Removed the `ArrayManager` (`55043`)
  - Removed the `fastpath` argument from the <span class="title-ref">Series</span> constructor (`55466`)
  - Removed the `is_boolean`, `is_integer`, `is_floating`, `holds_integer`, `is_numeric`, `is_categorical`, `is_object`, and `is_interval` attributes of <span class="title-ref">Index</span> (`50042`)
  - Removed the `ordinal` keyword in <span class="title-ref">PeriodIndex</span>, use <span class="title-ref">PeriodIndex.from\_ordinals</span> instead (`55960`)
  - Removed unused arguments `*args` and `**kwargs` in <span class="title-ref">Resampler</span> methods (`50977`)
  - Unrecognized timezones when parsing strings to datetimes now raises a `ValueError` (`51477`)
  - Removed the <span class="title-ref">Grouper</span> attributes `ax`, `groups`, `indexer`, and `obj` (`51206`, `51182`)
  - Removed deprecated keyword `verbose` on <span class="title-ref">read\_csv</span> and <span class="title-ref">read\_table</span> (`56556`)
  - Removed the `method` keyword in `ExtensionArray.fillna`, implement `ExtensionArray._pad_or_backfill` instead (`53621`)
  - Removed the attribute `dtypes` from <span class="title-ref">.DataFrameGroupBy</span> (`51997`)
  - Enforced deprecation of `argmin`, `argmax`, `idxmin`, and `idxmax` returning a result when `skipna=False` and an NA value is encountered or all values are NA values; these operations will now raise in such cases (`33941`, `51276`)

## Performance improvements

  - Eliminated circular reference in to original pandas object in accessor attributes (e.g. <span class="title-ref">Series.str</span>). However, accessor instantiation is no longer cached (`47667`, `41357`)
  - <span class="title-ref">Categorical.categories</span> returns a <span class="title-ref">RangeIndex</span> columns instead of an <span class="title-ref">Index</span> if the constructed `values` was a `range`. (`57787`)
  - <span class="title-ref">DataFrame</span> returns a <span class="title-ref">RangeIndex</span> columns when possible when `data` is a `dict` (`57943`)
  - <span class="title-ref">Series</span> returns a <span class="title-ref">RangeIndex</span> index when possible when `data` is a `dict` (`58118`)
  - <span class="title-ref">concat</span> returns a <span class="title-ref">RangeIndex</span> column when possible when `objs` contains <span class="title-ref">Series</span> and <span class="title-ref">DataFrame</span> and `axis=0` (`58119`)
  - <span class="title-ref">concat</span> returns a <span class="title-ref">RangeIndex</span> level in the <span class="title-ref">MultiIndex</span> result when `keys` is a `range` or <span class="title-ref">RangeIndex</span> (`57542`)
  - <span class="title-ref">RangeIndex.append</span> returns a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when appending values that could continue the <span class="title-ref">RangeIndex</span> (`57467`)
  - <span class="title-ref">Series.str.extract</span> returns a <span class="title-ref">RangeIndex</span> columns instead of an <span class="title-ref">Index</span> column when possible (`57542`)
  - <span class="title-ref">Series.str.partition</span> with <span class="title-ref">ArrowDtype</span> returns a <span class="title-ref">RangeIndex</span> columns instead of an <span class="title-ref">Index</span> column when possible (`57768`)
  - Performance improvement in <span class="title-ref">DataFrame</span> when `data` is a `dict` and `columns` is specified (`24368`)
  - Performance improvement in <span class="title-ref">MultiIndex</span> when setting <span class="title-ref">MultiIndex.names</span> doesn't invalidate all cached operations (`59578`)
  - Performance improvement in <span class="title-ref">DataFrame.join</span> for sorted but non-unique indexes (`56941`)
  - Performance improvement in <span class="title-ref">DataFrame.join</span> when left and/or right are non-unique and `how` is `"left"`, `"right"`, or `"inner"` (`56817`)
  - Performance improvement in <span class="title-ref">DataFrame.join</span> with `how="left"` or `how="right"` and `sort=True` (`56919`)
  - Performance improvement in <span class="title-ref">DataFrame.to\_csv</span> when `index=False` (`59312`)
  - Performance improvement in <span class="title-ref">DataFrameGroupBy.ffill</span>, <span class="title-ref">DataFrameGroupBy.bfill</span>, <span class="title-ref">SeriesGroupBy.ffill</span>, and <span class="title-ref">SeriesGroupBy.bfill</span> (`56902`)
  - Performance improvement in <span class="title-ref">Index.join</span> by propagating cached attributes in cases where the result matches one of the inputs (`57023`)
  - Performance improvement in <span class="title-ref">Index.take</span> when `indices` is a full range indexer from zero to length of index (`56806`)
  - Performance improvement in <span class="title-ref">Index.to\_frame</span> returning a <span class="title-ref">RangeIndex</span> columns of a <span class="title-ref">Index</span> when possible. (`58018`)
  - Performance improvement in <span class="title-ref">MultiIndex.\_engine</span> to use smaller dtypes if possible (`58411`)
  - Performance improvement in <span class="title-ref">MultiIndex.equals</span> for equal length indexes (`56990`)
  - Performance improvement in <span class="title-ref">MultiIndex.memory\_usage</span> to ignore the index engine when it isn't already cached. (`58385`)
  - Performance improvement in <span class="title-ref">RangeIndex.\_\_getitem\_\_</span> with a boolean mask or integers returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when possible. (`57588`)
  - Performance improvement in <span class="title-ref">RangeIndex.append</span> when appending the same index (`57252`)
  - Performance improvement in <span class="title-ref">RangeIndex.argmin</span> and <span class="title-ref">RangeIndex.argmax</span> (`57823`)
  - Performance improvement in <span class="title-ref">RangeIndex.insert</span> returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when the <span class="title-ref">RangeIndex</span> is empty. (`57833`)
  - Performance improvement in <span class="title-ref">RangeIndex.round</span> returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when possible. (`57824`)
  - Performance improvement in <span class="title-ref">RangeIndex.searchsorted</span> (`58376`)
  - Performance improvement in <span class="title-ref">RangeIndex.to\_numpy</span> when specifying an `na_value` (`58376`)
  - Performance improvement in <span class="title-ref">RangeIndex.value\_counts</span> (`58376`)
  - Performance improvement in <span class="title-ref">RangeIndex.join</span> returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when possible. (`57651`, `57752`)
  - Performance improvement in <span class="title-ref">RangeIndex.reindex</span> returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when possible. (`57647`, `57752`)
  - Performance improvement in <span class="title-ref">RangeIndex.take</span> returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when possible. (`57445`, `57752`)
  - Performance improvement in <span class="title-ref">merge</span> if hash-join can be used (`57970`)
  - Performance improvement in <span class="title-ref">CategoricalDtype.update\_dtype</span> when `dtype` is a <span class="title-ref">CategoricalDtype</span> with non `None` categories and ordered (`59647`)
  - Performance improvement in <span class="title-ref">DataFrame.astype</span> when converting to extension floating dtypes, e.g. "Float64" (`60066`)
  - Performance improvement in <span class="title-ref">to\_hdf</span> avoid unnecessary reopenings of the HDF5 file to speedup data addition to files with a very large number of groups . (`58248`)
  - Performance improvement in `DataFrameGroupBy.__len__` and `SeriesGroupBy.__len__` (`57595`)
  - Performance improvement in indexing operations for string dtypes (`56997`)
  - Performance improvement in unary methods on a <span class="title-ref">RangeIndex</span> returning a <span class="title-ref">RangeIndex</span> instead of a <span class="title-ref">Index</span> when possible. (`57825`)

## Bug fixes

### Categorical

  - Bug in <span class="title-ref">Series.apply</span> where `nan` was ignored for <span class="title-ref">CategoricalDtype</span> (`59938`)
  - 
### Datetimelike

  - Bug in <span class="title-ref">is\_year\_start</span> where a DateTimeIndex constructed via a date\_range with frequency 'MS' wouldn't have the correct year or quarter start attributes (`57377`)
  - Bug in <span class="title-ref">DataFrame</span> raising `ValueError` when `dtype` is `timedelta64` and `data` is a list containing `None` (`60064`)
  - Bug in <span class="title-ref">Timestamp</span> constructor failing to raise when `tz=None` is explicitly specified in conjunction with timezone-aware `tzinfo` or data (`48688`)
  - Bug in <span class="title-ref">date\_range</span> where the last valid timestamp would sometimes not be produced (`56134`)
  - Bug in <span class="title-ref">date\_range</span> where using a negative frequency value would not include all points between the start and end values (`56147`)
  - Bug in <span class="title-ref">tseries.api.guess\_datetime\_format</span> would fail to infer time format when "%Y" == "%H%M" (`57452`)
  - Bug in <span class="title-ref">tseries.frequencies.to\_offset</span> would fail to parse frequency strings starting with "LWOM" (`59218`)
  - Bug in <span class="title-ref">Dataframe.agg</span> with df with missing values resulting in IndexError (`58810`)
  - Bug in <span class="title-ref">DatetimeIndex.is\_year\_start</span> and <span class="title-ref">DatetimeIndex.is\_quarter\_start</span> does not raise on Custom business days frequencies bigger then "1C" (`58664`)
  - Bug in <span class="title-ref">DatetimeIndex.is\_year\_start</span> and <span class="title-ref">DatetimeIndex.is\_quarter\_start</span> returning `False` on double-digit frequencies (`58523`)
  - Bug in <span class="title-ref">DatetimeIndex.union</span> and <span class="title-ref">DatetimeIndex.intersection</span> when `unit` was non-nanosecond (`59036`)
  - Bug in <span class="title-ref">Series.dt.microsecond</span> producing incorrect results for pyarrow backed <span class="title-ref">Series</span>. (`59154`)
  - Bug in <span class="title-ref">to\_datetime</span> not respecting dayfirst if an uncommon date string was passed. (`58859`)
  - Bug in <span class="title-ref">to\_datetime</span> reports incorrect index in case of any failure scenario. (`58298`)
  - Bug in setting scalar values with mismatched resolution into arrays with non-nanosecond `datetime64`, `timedelta64` or <span class="title-ref">DatetimeTZDtype</span> incorrectly truncating those scalars (`56410`)

### Timedelta

  - Accuracy improvement in <span class="title-ref">Timedelta.to\_pytimedelta</span> to round microseconds consistently for large nanosecond based Timedelta (`57841`)
  - Bug in <span class="title-ref">DataFrame.cumsum</span> which was raising `IndexError` if dtype is `timedelta64[ns]` (`57956`)

### Timezones

  - 
  - 
### Numeric

  - Bug in <span class="title-ref">DataFrame.quantile</span> where the column type was not preserved when `numeric_only=True` with a list-like `q` produced an empty result (`59035`)
  - Bug in `np.matmul` with <span class="title-ref">Index</span> inputs raising a `TypeError` (`57079`)

### Conversion

  - Bug in <span class="title-ref">DataFrame.astype</span> not casting `values` for Arrow-based dictionary dtype correctly (`58479`)
  - Bug in <span class="title-ref">DataFrame.update</span> bool dtype being converted to object (`55509`)
  - Bug in <span class="title-ref">Series.astype</span> might modify read-only array inplace when casting to a string dtype (`57212`)
  - Bug in <span class="title-ref">Series.reindex</span> not maintaining `float32` type when a `reindex` introduces a missing value (`45857`)

### Strings

  - Bug in <span class="title-ref">Series.value\_counts</span> would not respect `sort=False` for series having `string` dtype (`55224`)
  - 
### Interval

  - <span class="title-ref">Index.is\_monotonic\_decreasing</span>, <span class="title-ref">Index.is\_monotonic\_increasing</span>, and <span class="title-ref">Index.is\_unique</span> could incorrectly be `False` for an `Index` created from a slice of another `Index`. (`57911`)
  - Bug in <span class="title-ref">interval\_range</span> where start and end numeric types were always cast to 64 bit (`57268`)
  - 
### Indexing

  - Bug in <span class="title-ref">DataFrame.\_\_getitem\_\_</span> returning modified columns when called with `slice` in Python 3.12 (`57500`)
  - Bug in <span class="title-ref">DataFrame.from\_records</span> throwing a `ValueError` when passed an empty list in `index` (`58594`)
  - 
### Missing

  - Bug in <span class="title-ref">DataFrame.fillna</span> and <span class="title-ref">Series.fillna</span> that would ignore the `limit` argument on <span class="title-ref">.ExtensionArray</span> dtypes (`58001`)
  - 
### MultiIndex

  - <span class="title-ref">DataFrame.loc</span> with `axis=0` and <span class="title-ref">MultiIndex</span> when setting a value adds extra columns (`58116`)
  - <span class="title-ref">DataFrame.melt</span> would not accept multiple names in `var_name` when the columns were a <span class="title-ref">MultiIndex</span> (`58033`)
  - <span class="title-ref">MultiIndex.insert</span> would not insert NA value correctly at unified location of index -1 (`59003`)
  - <span class="title-ref">MultiIndex.get\_level\_values</span> accessing a <span class="title-ref">DatetimeIndex</span> does not carry the frequency attribute along (`58327`, `57949`)
  - 
### I/O

  - Bug in <span class="title-ref">DataFrame</span> and <span class="title-ref">Series</span> `repr` of :py\`collections.abc.Mapping\`<span class="title-ref"> elements. (:issue:\`57915</span>)
  - Bug in <span class="title-ref">.DataFrame.to\_json</span> when `"index"` was a value in the <span class="title-ref">DataFrame.column</span> and <span class="title-ref">Index.name</span> was `None`. Now, this will fail with a `ValueError` (`58925`)
  - Bug in <span class="title-ref">DataFrame.\_repr\_html\_</span> which ignored the `"display.float_format"` option (`59876`)
  - Bug in <span class="title-ref">DataFrame.from\_records</span> where `columns` parameter with numpy structured array was not reordering and filtering out the columns (`59717`)
  - Bug in <span class="title-ref">DataFrame.to\_dict</span> raises unnecessary `UserWarning` when columns are not unique and `orient='tight'`. (`58281`)
  - Bug in <span class="title-ref">DataFrame.to\_excel</span> when writing empty <span class="title-ref">DataFrame</span> with <span class="title-ref">MultiIndex</span> on both axes (`57696`)
  - Bug in <span class="title-ref">DataFrame.to\_stata</span> when writing <span class="title-ref">DataFrame</span> and ``byteorder=`big``<span class="title-ref">. (:issue:\`58969</span>)
  - Bug in <span class="title-ref">DataFrame.to\_stata</span> when writing more than 32,000 value labels. (`60107`)
  - Bug in <span class="title-ref">DataFrame.to\_string</span> that raised `StopIteration` with nested DataFrames. (`16098`)
  - Bug in <span class="title-ref">HDFStore.get</span> was failing to save data of dtype datetime64\[s\] correctly (`59004`)
  - Bug in <span class="title-ref">read\_csv</span> causing segmentation fault when `encoding_errors` is not a string. (`59059`)
  - Bug in <span class="title-ref">read\_csv</span> raising `TypeError` when `index_col` is specified and `na_values` is a dict containing the key `None`. (`57547`)
  - Bug in <span class="title-ref">read\_csv</span> raising `TypeError` when `nrows` and `iterator` are specified without specifying a `chunksize`. (`59079`)
  - Bug in <span class="title-ref">read\_csv</span> where the order of the `na_values` makes an inconsistency when `na_values` is a list non-string values. (`59303`)
  - Bug in <span class="title-ref">read\_excel</span> raising `ValueError` when passing array of boolean values when `dtype="boolean"`. (`58159`)
  - Bug in <span class="title-ref">read\_json</span> not validating the `typ` argument to not be exactly `"frame"` or `"series"` (`59124`)
  - Bug in <span class="title-ref">read\_json</span> where extreme value integers in string format were incorrectly parsed as a different integer number (`20608`)
  - Bug in <span class="title-ref">read\_stata</span> raising `KeyError` when input file is stored in big-endian format and contains strL data. (`58638`)
  - Bug in <span class="title-ref">read\_stata</span> where extreme value integers were incorrectly interpreted as missing for format versions 111 and prior (`58130`)
  - Bug in <span class="title-ref">read\_stata</span> where the missing code for double was not recognised for format versions 105 and prior (`58149`)
  - Bug in <span class="title-ref">set\_option</span> where setting the pandas option `display.html.use_mathjax` to `False` has no effect (`59884`)
  - Bug in <span class="title-ref">to\_excel</span> where <span class="title-ref">MultiIndex</span> columns would be merged to a single row when `merge_cells=False` is passed (`60274`)

### Period

  - Fixed error message when passing invalid period alias to <span class="title-ref">PeriodIndex.to\_timestamp</span> (`58974`)
  - 
### Plotting

  - Bug in <span class="title-ref">.DataFrameGroupBy.boxplot</span> failed when there were multiple groupings (`14701`)
  - Bug in <span class="title-ref">DataFrame.plot.bar</span> with `stacked=True` where labels on stacked bars with zero-height segments were incorrectly positioned at the base instead of the label position of the previous segment (`59429`)
  - Bug in <span class="title-ref">DataFrame.plot.line</span> raising `ValueError` when set both color and a `dict` style (`59461`)
  - Bug in <span class="title-ref">DataFrame.plot</span> that causes a shift to the right when the frequency multiplier is greater than one. (`57587`)
  - Bug in <span class="title-ref">Series.plot</span> with `kind="pie"` with <span class="title-ref">ArrowDtype</span> (`59192`)

### Groupby/resample/rolling

  - Bug in <span class="title-ref">.DataFrameGroupBy.\_\_len\_\_</span> and <span class="title-ref">.SeriesGroupBy.\_\_len\_\_</span> would raise when the grouping contained NA values and `dropna=False` (`58644`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.any</span> that returned True for groups where all Timedelta values are NaT. (`59712`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.groups</span> and <span class="title-ref">.SeriesGroupby.groups</span> that would not respect groupby argument `dropna` (`55919`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.median</span> where nat values gave an incorrect result. (`57926`)
  - Bug in <span class="title-ref">.DataFrameGroupBy.quantile</span> when `interpolation="nearest"` is inconsistent with <span class="title-ref">DataFrame.quantile</span> (`47942`)
  - Bug in <span class="title-ref">.Resampler.interpolate</span> on a <span class="title-ref">DataFrame</span> with non-uniform sampling and/or indices not aligning with the resulting resampled index would result in wrong interpolation (`21351`)
  - Bug in <span class="title-ref">DataFrame.ewm</span> and <span class="title-ref">Series.ewm</span> when passed `times` and aggregation functions other than mean (`51695`)
  - Bug in <span class="title-ref">DataFrameGroupBy.agg</span> that raises `AttributeError` when there is dictionary input and duplicated columns, instead of returning a DataFrame with the aggregation of all duplicate columns. (`55041`)
  - Bug in <span class="title-ref">DataFrameGroupBy.apply</span> that was returning a completely empty DataFrame when all return values of `func` were `None` instead of returning an empty DataFrame with the original columns and dtypes. (`57775`)
  - Bug in <span class="title-ref">DataFrameGroupBy.apply</span> with `as_index=False` that was returning <span class="title-ref">MultiIndex</span> instead of returning <span class="title-ref">Index</span>. (`58291`)
  - Bug in <span class="title-ref">DataFrameGroupBy.cumsum</span> and <span class="title-ref">DataFrameGroupBy.cumprod</span> where `numeric_only` parameter was passed indirectly through kwargs instead of passing directly. (`58811`)
  - Bug in <span class="title-ref">DataFrameGroupBy.cumsum</span> where it did not return the correct dtype when the label contained `None`. (`58811`)
  - Bug in <span class="title-ref">DataFrameGroupby.transform</span> and <span class="title-ref">SeriesGroupby.transform</span> with a reducer and `observed=False` that coerces dtype to float when there are unobserved categories. (`55326`)
  - Bug in <span class="title-ref">Rolling.apply</span> where the applied function could be called on fewer than `min_period` periods if `method="table"`. (`58868`)
  - Bug in <span class="title-ref">Series.resample</span> could raise when the the date range ended shortly before a non-existent time. (`58380`)

### Reshaping

  - Bug in <span class="title-ref">qcut</span> where values at the quantile boundaries could be incorrectly assigned (`59355`)
  - Bug in <span class="title-ref">DataFrame.join</span> inconsistently setting result index name (`55815`)
  - Bug in <span class="title-ref">DataFrame.join</span> when a <span class="title-ref">DataFrame</span> with a <span class="title-ref">MultiIndex</span> would raise an `AssertionError` when <span class="title-ref">MultiIndex.names</span> contained `None`. (`58721`)
  - Bug in <span class="title-ref">DataFrame.merge</span> where merging on a column containing only `NaN` values resulted in an out-of-bounds array access (`59421`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> producing incorrect results when `sort=False` (`54987`, `55516`)
  - Bug in <span class="title-ref">DataFrame.merge</span> when merging two <span class="title-ref">DataFrame</span> on `intc` or `uintc` types on Windows (`60091`, `58713`)
  - Bug in <span class="title-ref">DataFrame.pivot\_table</span> incorrectly subaggregating results when called without an `index` argument (`58722`)
  - Bug in <span class="title-ref">DataFrame.unstack</span> producing incorrect results when manipulating empty <span class="title-ref">DataFrame</span> with an <span class="title-ref">ExtentionDtype</span> (`59123`)

### Sparse

  - Bug in <span class="title-ref">SparseDtype</span> for equal comparison with na fill value. (`54770`)
  - Bug in <span class="title-ref">DataFrame.sparse.from\_spmatrix</span> which hard coded an invalid `fill_value` for certain subtypes. (`59063`)
  - Bug in <span class="title-ref">DataFrame.sparse.to\_dense</span> which ignored subclassing and always returned an instance of <span class="title-ref">DataFrame</span> (`59913`)

### ExtensionArray

  - Bug in <span class="title-ref">.arrays.ArrowExtensionArray.\_\_setitem\_\_</span> which caused wrong behavior when using an integer array with repeated values as a key (`58530`)
  - Bug in <span class="title-ref">api.types.is\_datetime64\_any\_dtype</span> where a custom <span class="title-ref">ExtensionDtype</span> would return `False` for array-likes (`57055`)
  - Bug in comparison between object with <span class="title-ref">ArrowDtype</span> and incompatible-dtyped (e.g. string vs bool) incorrectly raising instead of returning all-`False` (for `==`) or all-`True` (for `!=`) (`59505`)
  - Bug in various <span class="title-ref">DataFrame</span> reductions for pyarrow temporal dtypes returning incorrect dtype when result was null (`59234`)

### Styler

  - 
### Other

  - Bug in <span class="title-ref">DataFrame</span> when passing a `dict` with a NA scalar and `columns` that would always return `np.nan` (`57205`)
  - Bug in <span class="title-ref">eval</span> on <span class="title-ref">ExtensionArray</span> on including division `/` failed with a `TypeError`. (`58748`)
  - Bug in <span class="title-ref">eval</span> where the names of the <span class="title-ref">Series</span> were not preserved when using `engine="numexpr"`. (`10239`)
  - Bug in <span class="title-ref">eval</span> with `engine="numexpr"` returning unexpected result for float division. (`59736`)
  - Bug in <span class="title-ref">to\_numeric</span> raising `TypeError` when `arg` is a <span class="title-ref">Timedelta</span> or <span class="title-ref">Timestamp</span> scalar. (`59944`)
  - Bug in <span class="title-ref">unique</span> on <span class="title-ref">Index</span> not always returning <span class="title-ref">Index</span> (`57043`)
  - Bug in <span class="title-ref">DataFrame.apply</span> where passing `engine="numba"` ignored `args` passed to the applied function (`58712`)
  - Bug in <span class="title-ref">DataFrame.eval</span> and <span class="title-ref">DataFrame.query</span> which caused an exception when using NumPy attributes via `@` notation, e.g., `df.eval("@np.floor(a)")`. (`58041`)
  - Bug in <span class="title-ref">DataFrame.eval</span> and <span class="title-ref">DataFrame.query</span> which did not allow to use `tan` function. (`55091`)
  - Bug in <span class="title-ref">DataFrame.query</span> where using duplicate column names led to a `TypeError`. (`59950`)
  - Bug in <span class="title-ref">DataFrame.query</span> which raised an exception or produced incorrect results when expressions contained backtick-quoted column names containing the hash character `#`, backticks, or characters that fall outside the ASCII range (U+0001..U+007F). (`59285`) (`49633`)
  - Bug in <span class="title-ref">DataFrame.shift</span> where passing a `freq` on a DataFrame with no columns did not shift the index correctly. (`60102`)
  - Bug in <span class="title-ref">DataFrame.sort\_index</span> when passing `axis="columns"` and `ignore_index=True` and `ascending=False` not returning a <span class="title-ref">RangeIndex</span> columns (`57293`)
  - Bug in <span class="title-ref">DataFrame.transform</span> that was returning the wrong order unless the index was monotonically increasing. (`57069`)
  - Bug in <span class="title-ref">DataFrame.where</span> where using a non-bool type array in the function would return a `ValueError` instead of a `TypeError` (`56330`)
  - Bug in <span class="title-ref">Index.sort\_values</span> when passing a key function that turns values into tuples, e.g. `key=natsort.natsort_key`, would raise `TypeError` (`56081`)
  - Bug in <span class="title-ref">Series.diff</span> allowing non-integer values for the `periods` argument. (`56607`)
  - Bug in <span class="title-ref">Series.dt</span> methods in <span class="title-ref">ArrowDtype</span> that were returning incorrect values. (`57355`)
  - Bug in <span class="title-ref">Series.rank</span> that doesn't preserve missing values for nullable integers when `na_option='keep'`. (`56976`)
  - Bug in <span class="title-ref">Series.replace</span> and <span class="title-ref">DataFrame.replace</span> inconsistently replacing matching instances when `regex=True` and missing values are present. (`56599`)
  - Bug in <span class="title-ref">read\_csv</span> where chained fsspec TAR file and `compression="infer"` fails with `tarfile.ReadError` (`60028`)
  - Bug in Dataframe Interchange Protocol implementation was returning incorrect results for data buffers' associated dtype, for string and datetime columns (`54781`)
  - Bug in `Series.list` methods not preserving the original <span class="title-ref">Index</span>. (`58425`)

<!-- end list -->

  - 
  - 
## Contributors

---

AUTHORS.md

---

About the Copyright Holders
===========================

*   Copyright (c) 2008-2011 AQR Capital Management, LLC

    AQR Capital Management began pandas development in 2008. Development was
    led by Wes McKinney. AQR released the source under this license in 2009.
*   Copyright (c) 2011-2012, Lambda Foundry, Inc.

    Wes is now an employee of Lambda Foundry, and remains the pandas project
    lead.
*   Copyright (c) 2011-2012, PyData Development Team

    The PyData Development Team is the collection of developers of the PyData
    project. This includes all of the PyData sub-projects, including pandas. The
    core team that coordinates development on GitHub can be found here:
    https://github.com/pydata.

Full credits for pandas contributors can be found in the documentation.

Our Copyright Policy
====================

PyData uses a shared copyright model. Each contributor maintains copyright
over their contributions to PyData. However, it is important to note that
these contributions are typically only changes to the repositories. Thus,
the PyData source code, in its entirety, is not the copyright of any single
person or institution. Instead, it is the collective copyright of the
entire PyData Development Team. If individual contributors want to maintain
a record of what changes/contributions they have specific copyright on,
they should indicate their copyright in the commit message of the change
when they commit the change to one of the PyData repositories.

With this in mind, the following banner should be used in any source code
file to indicate the copyright and license terms:

```
#-----------------------------------------------------------------------------
# Copyright (c) 2012, PyData Development Team
# All rights reserved.
#
# Distributed under the terms of the BSD Simplified License.
#
# The full license is in the LICENSE file, distributed with this software.
#-----------------------------------------------------------------------------
```

Other licenses can be found in the LICENSES directory.

License
=======

pandas is distributed under a 3-clause ("Simplified" or "New") BSD
license. Parts of NumPy, SciPy, numpydoc, bottleneck, which all have
BSD-compatible licenses, are included. Their licenses follow the pandas
license.


---

README.md

---

<picture align="center">
  <source media="(prefers-color-scheme: dark)" srcset="https://pandas.pydata.org/static/img/pandas_white.svg">
  <img alt="Pandas Logo" src="https://pandas.pydata.org/static/img/pandas.svg">
</picture>

-----------------

# pandas: powerful Python data analysis toolkit

| | |
| --- | --- |
| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |
| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |
| Meta | [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134) [![License - BSD 3-Clause](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE) [![Slack](https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) |


## What is it?

**pandas** is a Python package that provides fast, flexible, and expressive data
structures designed to make working with "relational" or "labeled" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, **real world** data analysis in Python. Additionally, it has
the broader goal of becoming **the most powerful and flexible open source data
analysis / manipulation tool available in any language**. It is already well on
its way towards this goal.

## Table of Contents

- [Main Features](#main-features)
- [Where to get it](#where-to-get-it)
- [Dependencies](#dependencies)
- [Installation from sources](#installation-from-sources)
- [License](#license)
- [Documentation](#documentation)
- [Background](#background)
- [Getting Help](#getting-help)
- [Discussion and Development](#discussion-and-development)
- [Contributing to pandas](#contributing-to-pandas)

## Main Features
Here are just a few of the things that pandas does well:

  - Easy handling of [**missing data**][missing-data] (represented as
    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data
  - Size mutability: columns can be [**inserted and
    deleted**][insertion-deletion] from DataFrame and higher dimensional
    objects
  - Automatic and explicit [**data alignment**][alignment]: objects can
    be explicitly aligned to a set of labels, or the user can simply
    ignore the labels and let `Series`, `DataFrame`, etc. automatically
    align the data for you in computations
  - Powerful, flexible [**group by**][groupby] functionality to perform
    split-apply-combine operations on data sets, for both aggregating
    and transforming data
  - Make it [**easy to convert**][conversion] ragged,
    differently-indexed data in other Python and NumPy data structures
    into DataFrame objects
  - Intelligent label-based [**slicing**][slicing], [**fancy
    indexing**][fancy-indexing], and [**subsetting**][subsetting] of
    large data sets
  - Intuitive [**merging**][merging] and [**joining**][joining] data
    sets
  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of
    data sets
  - [**Hierarchical**][mi] labeling of axes (possible to have multiple
    labels per tick)
  - Robust IO tools for loading data from [**flat files**][flat-files]
    (CSV and delimited), [**Excel files**][excel], [**databases**][db],
    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]
  - [**Time series**][timeseries]-specific functionality: date range
    generation and frequency conversion, moving window statistics,
    date shifting and lagging


   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion
   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures
   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine
   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe
   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges
   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced
   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing
   [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging
   [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index
   [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex
   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files
   [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files
   [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries
   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables
   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality

## Where to get it
The source code is currently hosted on GitHub at:
https://github.com/pandas-dev/pandas

Binary installers for the latest released version are available at the [Python
Package Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://anaconda.org/conda-forge/pandas).

```sh
# conda
conda install -c conda-forge pandas
```

```sh
# or PyPI
pip install pandas
```

The list of changes to pandas between each release can be found
[here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html). For full
details, see the commit logs at https://github.com/pandas-dev/pandas.

## Dependencies
- [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)
- [python-dateutil - Provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)
- [pytz - Brings the Olson tz database into Python which allows accurate and cross platform timezone calculations](https://github.com/stub42/pytz)

See the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.

## Installation from sources
To install pandas from source you need [Cython](https://cython.org/) in addition to the normal
dependencies above. Cython can be installed from PyPI:

```sh
pip install cython
```

In the `pandas` directory (same one where you found this file after
cloning the git repo), execute:

```sh
pip install .
```

or for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):


```sh
python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true
```

See the full instructions for [installing from source](https://pandas.pydata.org/docs/dev/development/contributing_environment.html).

## License
[BSD 3](LICENSE)

## Documentation
The official documentation is hosted on [PyData.org](https://pandas.pydata.org/pandas-docs/stable/).

## Background
Work on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and
has been under active development since then.

## Getting Help

For usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).
Further, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).

## Discussion and Development
Most development discussions take place on GitHub in this repo, via the [GitHub issue tracker](https://github.com/pandas-dev/pandas/issues).

Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.

There are also frequent [community meetings](https://pandas.pydata.org/docs/dev/development/community.html#community-meeting) for project maintainers open to the community as well as monthly [new contributor meetings](https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting) to help support new contributors.

Additional information on the communication channels can be found on the [contributor community](https://pandas.pydata.org/docs/development/community.html) page.

## Contributing to pandas

[![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)

All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.

A detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.

If you are simply looking to start working with the pandas codebase, navigate to the [GitHub "issues" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.

You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).

Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking â€˜this can be improvedâ€™...you can do something about it!

Feel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Slack](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack).

As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/.github/blob/master/CODE_OF_CONDUCT.md)

<hr>

[Go to Top](#table-of-contents)
