content.md

---

# Roadmap & NumPy enhancement proposals

This page provides an overview of development priorities for NumPy. Specifically, it contains a roadmap with a higher-level overview, as well as NumPy Enhancement Proposals (NEPs)â€”suggested changes to the libraryâ€”in various stages of discussion or completion (see [NEP 0](nep-0000)).

## Roadmap

<div class="toctree" data-maxdepth="1">

Index \<index\> The Scope of NumPy \<scope\> Current roadmap \<roadmap\> Wish list \<<https://github.com/numpy/numpy/issues?q=is%3Aopen+is%3Aissue+label%3A%2223+-+Wish+List%22>\>

</div>

---

index.md

---

# Roadmap & NumPy enhancement proposals

This page provides an overview of development priorities for NumPy. Specifically, it contains a roadmap with a higher-level overview, as well as NumPy Enhancement Proposals (NEPs)â€”suggested changes to the libraryâ€”in various stages of discussion or completion (see [NEP 0](nep-0000)).

## Roadmap

<div class="toctree" data-maxdepth="1">

The Scope of NumPy \<scope\> Current roadmap \<roadmap\> Wish list \<<https://github.com/numpy/numpy/issues?q=is%3Aopen+is%3Aissue+label%3A%2223+-+Wish+List%22>\>

</div>

## NumPy enhancement proposals (NEPs)

<div class="toctree" data-maxdepth="2">

meta provisional accepted open finished deferred rejected

</div>

---

nep-0000.md

---

# NEP 0 â€” Purpose and process

  - Author  
    Jarrod Millman \<<millman@berkeley.edu>\>

  - Status  
    Active

  - Type  
    Process

  - Created  
    2017-12-11

## What is a NEP?

NEP stands for NumPy Enhancement Proposal. A NEP is a design document providing information to the NumPy community, or describing a new feature for NumPy or its processes or environment. The NEP should provide a concise technical specification of the feature and a rationale for the feature.

We intend NEPs to be the primary mechanisms for proposing major new features, for collecting community input on an issue, and for documenting the design decisions that have gone into NumPy. The NEP author is responsible for building consensus within the community and documenting dissenting opinions.

Because the NEPs are maintained as text files in a versioned repository, their revision history is the historical record of the feature proposal\[1\].

### Types

There are three kinds of NEPs:

1.  A **Standards Track** NEP describes a new feature or implementation for NumPy.
2.  An **Informational** NEP describes a NumPy design issue, or provides general guidelines or information to the Python community, but does not propose a new feature. Informational NEPs do not necessarily represent a NumPy community consensus or recommendation, so users and implementers are free to ignore Informational NEPs or follow their advice.
3.  A **Process** NEP describes a process surrounding NumPy, or proposes a change to (or an event in) a process. Process NEPs are like Standards Track NEPs but apply to areas other than the NumPy language itself. They may propose an implementation, but not to NumPy's codebase; they require community consensus. Examples include procedures, guidelines, changes to the decision-making process, and changes to the tools or environment used in NumPy development. Any meta-NEP is also considered a Process NEP.

## NEP workflow

The NEP process begins with a new idea for NumPy. It is highly recommended that a single NEP contain a single key proposal or new idea. Small enhancements or patches often don't need a NEP and can be injected into the NumPy development workflow with a pull request to the NumPy [repo](https://github.com/numpy/numpy). The more focused the NEP, the more successful it tends to be. If in doubt, split your NEP into several well-focused ones.

Each NEP must have a champion---someone who writes the NEP using the style and format described below, shepherds the discussions in the appropriate forums, and attempts to build community consensus around the idea. The NEP champion (a.k.a. Author) should first attempt to ascertain whether the idea is suitable for a NEP. Posting to the numpy-discussion [mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion) is the best way to go about doing this.

The proposal should be submitted as a draft NEP via a [GitHub pull request](https://github.com/numpy/numpy/pulls) to the `doc/neps` directory with the name `nep-<n>.rst` where `<n>` is an appropriately assigned four-digit number (e.g., `nep-0000.rst`). The draft must use the \[nep-template\](nep-template.md) file.

Once the PR for the NEP is in place, a post should be made to the mailing list containing the sections up to "Backward compatibility", with the purpose of limiting discussion there to usage and impact. Discussion on the pull request will have a broader scope, also including details of implementation.

At the earliest convenience, the PR should be merged (regardless of whether it is accepted during discussion). Additional PRs may be made by the Author to update or expand the NEP, or by maintainers to set its status, discussion URL, etc.

Standards Track NEPs consist of two parts, a design document and a reference implementation. It is generally recommended that at least a prototype implementation be co-developed with the NEP, as ideas that sound good in principle sometimes turn out to be impractical when subjected to the test of implementation. Often it makes sense for the prototype implementation to be made available as PR to the NumPy repo (making sure to appropriately mark the PR as a WIP).

### Review and resolution

NEPs are discussed on the mailing list. The possible paths of the status of NEPs are as follows:

![image](nep-0000.png)

All NEPs should be created with the `Draft` status.

Eventually, after discussion, there may be a consensus that the NEP should be accepted â€“ see the next section for details. At this point the status becomes `Accepted`.

Once a NEP has been `Accepted`, the reference implementation must be completed. When the reference implementation is complete and incorporated into the main source code repository, the status will be changed to `Final`.

To allow gathering of additional design and interface feedback before committing to long term stability for a language feature or standard library API, a NEP may also be marked as "Provisional". This is short for "Provisionally Accepted", and indicates that the proposal has been accepted for inclusion in the reference implementation, but additional user feedback is needed before the full design can be considered "Final". Unlike regular accepted NEPs, provisionally accepted NEPs may still be Rejected or Withdrawn even after the related changes have been included in a Python release.

Wherever possible, it is considered preferable to reduce the scope of a proposal to avoid the need to rely on the "Provisional" status (e.g. by deferring some features to later NEPs), as this status can lead to version compatibility challenges in the wider NumPy ecosystem.

A NEP can also be assigned status `Deferred`. The NEP author or a core developer can assign the NEP this status when no progress is being made on the NEP.

A NEP can also be `Rejected`. Perhaps after all is said and done it was not a good idea. It is still important to have a record of this fact. The `Withdrawn` status is similar---it means that the NEP author themselves has decided that the NEP is actually a bad idea, or has accepted that a competing proposal is a better alternative.

When a NEP is `Accepted`, `Rejected`, or `Withdrawn`, the NEP should be updated accordingly. In addition to updating the status field, at the very least the `Resolution` header should be added with a link to the relevant thread in the mailing list archives.

NEPs can also be `Superseded` by a different NEP, rendering the original obsolete. The `Replaced-By` and `Replaces` headers containing references to the original and new NEPs, like `[NEP#number](#nep#number)` should be added respectively.

Process NEPs may also have a status of `Active` if they are never meant to be completed, e.g. NEP 0 (this NEP).

### How a NEP becomes accepted

A NEP is `Accepted` by consensus of all interested contributors. We need a concrete way to tell whether consensus has been reached. When you think a NEP is ready to accept, send an email to the numpy-discussion mailing list with a subject like:

> Proposal to accept NEP \#\<number\>: \<title\>

In the body of your email, you should:

  - link to the latest version of the NEP,
  - briefly describe any major points of contention and how they were resolved,
  - include a sentence like: "If there are no substantive objections within 7 days from this email, then the NEP will be accepted; see NEP 0 for more details."

For an example, see: <https://mail.python.org/pipermail/numpy-discussion/2018-June/078345.html>

After you send the email, you should make sure to link to the email thread from the `Discussion` section of the NEP, so that people can find it later.

Generally the NEP author will be the one to send this email, but anyone can do it â€“ the important thing is to make sure that everyone knows when a NEP is on the verge of acceptance, and give them a final chance to respond. If there's some special reason to extend this final comment period beyond 7 days, then that's fine, just say so in the email. You shouldn't do less than 7 days, because sometimes people are travelling or similar and need some time to respond.

In general, the goal is to make sure that the community has consensus, not provide a rigid policy for people to try to game. When in doubt, err on the side of asking for more feedback and looking for opportunities to compromise.

If the final comment period passes without any substantive objections, then the NEP can officially be marked `Accepted`. You should send a followup email notifying the list (celebratory emoji optional but encouraged ðŸŽ‰âœ¨), and then update the NEP by setting its `:Status:` to `Accepted`, and its `:Resolution:` header to a link to your followup email.

If there *are* substantive objections, then the NEP remains in `Draft` state, discussion continues as normal, and it can be proposed for acceptance again later once the objections are resolved.

In unusual cases, the [NumPy Steering Council](https://docs.scipy.org/doc/numpy/dev/governance/governance.html) may be asked to decide whether a controversial NEP is `Accepted`.

### Maintenance

In general, Standards track NEPs are no longer modified after they have reached the Final state as the code and project documentation are considered the ultimate reference for the implemented feature. However, finalized Standards track NEPs may be updated as needed.

Process NEPs may be updated over time to reflect changes to development practices and other details. The precise process followed in these cases will depend on the nature and purpose of the NEP being updated.

## Format and template

NEPs are UTF-8 encoded text files using the [reStructuredText](http://docutils.sourceforge.net/rst.html) format. Please see the \[nep-template\](nep-template.md) file and the [reStructuredTextPrimer](http://www.sphinx-doc.org/en/stable/rest.html) for more information. We use [Sphinx](http://www.sphinx-doc.org/en/stable/) to convert NEPs to HTML for viewing on the web \[2\].

### Header Preamble

Each NEP must begin with a header preamble. The headers must appear in the following order. Headers marked with `*` are optional. All other headers are required.

`` `rst     :Author: <list of authors' real names and optionally, email addresses>     :Status: <Draft | Active | Accepted | Deferred | Rejected |              Withdrawn | Final | Superseded>     :Type: <Standards Track | Process>     :Created: <date created on, in dd-mmm-yyyy format>   * :Requires: <nep numbers>   * :NumPy-Version: <version number>   * :Replaces: <nep number>   * :Replaced-By: <nep number>   * :Resolution: <url>  The Author header lists the names, and optionally the email addresses ``\` of all the authors of the NEP. The format of the Author header value must be

`` `rst     Random J. User <address@dom.ain>  if the email address is included, and just  .. code-block:: rst      Random J. User  if the address is not given.  If there are multiple authors, each should be on ``\` a separate line.

## Discussion

  - <https://mail.python.org/pipermail/numpy-discussion/2017-December/077481.html>

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  This historical record is available by the normal git commands for retrieving older revisions, and can also be browsed on [GitHub](https://github.com/numpy/numpy/tree/main/doc/neps).

2.  The URL for viewing NEPs on the web is <https://www.numpy.org/neps/>.

---

nep-0001-npy-format.md

---

# NEP 1 â€” A simple file format for NumPy arrays

  - Author  
    Robert Kern \<<robert.kern@gmail.com>\>

  - Status  
    Final

  - Created  
    20-Dec-2007

## Abstract

We propose a standard binary file format (NPY) for persisting a single arbitrary NumPy array on disk. The format stores all of the shape and dtype information necessary to reconstruct the array correctly even on another machine with a different architecture. The format is designed to be as simple as possible while achieving its limited goals. The implementation is intended to be pure Python and distributed as part of the main numpy package.

## Rationale

A lightweight, omnipresent system for saving NumPy arrays to disk is a frequent need. Python in general has pickle \[1\] for saving most Python objects to disk. This often works well enough with NumPy arrays for many purposes, but it has a few drawbacks:

  - Dumping or loading a pickle file require the duplication of the data in memory. For large arrays, this can be a showstopper.
  - The array data is not directly accessible through memory-mapping. Now that numpy has that capability, it has proved very useful for loading large amounts of data (or more to the point: avoiding loading large amounts of data when you only need a small part).

Both of these problems can be addressed by dumping the raw bytes to disk using ndarray.tofile() and numpy.fromfile(). However, these have their own problems:

  - The data which is written has no information about the shape or dtype of the array.
  - It is incapable of handling object arrays.

The NPY file format is an evolutionary advance over these two approaches. Its design is mostly limited to solving the problems with pickles and tofile()/fromfile(). It does not intend to solve more complicated problems for which more complicated formats like HDF5 \[2\] are a better solution.

## Use cases

  - Neville Newbie has just started to pick up Python and NumPy. He has not installed many packages, yet, nor learned the standard library, but he has been playing with NumPy at the interactive prompt to do small tasks. He gets a result that he wants to save.
  - Annie Analyst has been using large nested record arrays to represent her statistical data. She wants to convince her R-using colleague, David Doubter, that Python and NumPy are awesome by sending him her analysis code and data. She needs the data to load at interactive speeds. Since David does not use Python usually, needing to install large packages would turn him off.
  - Simon Seismologist is developing new seismic processing tools. One of his algorithms requires large amounts of intermediate data to be written to disk. The data does not really fit into the industry-standard SEG-Y schema, but he already has a nice record-array dtype for using it internally.
  - Polly Parallel wants to split up a computation on her multicore machine as simply as possible. Parts of the computation can be split up among different processes without any communication between processes; they just need to fill in the appropriate portion of a large array with their results. Having several child processes memory-mapping a common array is a good way to achieve this.

## Requirements

The format MUST be able to:

  - Represent all NumPy arrays including nested record arrays and object arrays.
  - Represent the data in its native binary form.
  - Be contained in a single file.
  - Support Fortran-contiguous arrays directly.
  - Store all of the necessary information to reconstruct the array including shape and dtype on a machine of a different architecture. Both little-endian and big-endian arrays must be supported and a file with little-endian numbers will yield a little-endian array on any machine reading the file. The types must be described in terms of their actual sizes. For example, if a machine with a 64-bit C "long int" writes out an array with "long ints", a reading machine with 32-bit C "long ints" will yield an array with 64-bit integers.
  - Be reverse engineered. Datasets often live longer than the programs that created them. A competent developer should be able to create a solution in his preferred programming language to read most NPY files that he has been given without much documentation.
  - Allow memory-mapping of the data.
  - Be read from a filelike stream object instead of an actual file. This allows the implementation to be tested easily and makes the system more flexible. NPY files can be stored in ZIP files and easily read from a ZipFile object.
  - Store object arrays. Since general Python objects are complicated and can only be reliably serialized by pickle (if at all), many of the other requirements are waived for files containing object arrays. Files with object arrays do not have to be mmapable since that would be technically impossible. We cannot expect the pickle format to be reverse engineered without knowledge of pickle. However, one should at least be able to read and write object arrays with the same generic interface as other arrays.
  - Be read and written using APIs provided in the numpy package itself without any other libraries. The implementation inside numpy may be in C if necessary.

The format explicitly *does not* need to:

  - Support multiple arrays in a file. Since we require filelike objects to be supported, one could use the API to build an ad hoc format that supported multiple arrays. However, solving the general problem and use cases is beyond the scope of the format and the API for numpy.
  - Fully handle arbitrary subclasses of numpy.ndarray. Subclasses will be accepted for writing, but only the array data will be written out. A regular numpy.ndarray object will be created upon reading the file. The API can be used to build a format for a particular subclass, but that is out of scope for the general NPY format.

## Format specification: version 1.0

The first 6 bytes are a magic string: exactly "x93NUMPY".

The next 1 byte is an unsigned byte: the major version number of the file format, e.g. x01.

The next 1 byte is an unsigned byte: the minor version number of the file format, e.g. x00. Note: the version of the file format is not tied to the version of the numpy package.

The next 2 bytes form a little-endian unsigned short int: the length of the header data HEADER\_LEN.

The next HEADER\_LEN bytes form the header data describing the array's format. It is an ASCII string which contains a Python literal expression of a dictionary. It is terminated by a newline ('n') and padded with spaces ('x20') to make the total length of the magic string + 4 + HEADER\_LEN be evenly divisible by 16 for alignment purposes.

The dictionary contains three keys:

>   - "descr" : dtype.descr  
>     An object that can be passed as an argument to the numpy.dtype() constructor to create the array's dtype.
> 
>   - "fortran\_order" : bool  
>     Whether the array data is Fortran-contiguous or not. Since Fortran-contiguous arrays are a common form of non-C-contiguity, we allow them to be written directly to disk for efficiency.
> 
>   - "shape" : tuple of int  
>     The shape of the array.

For repeatability and readability, this dictionary is formatted using pprint.pformat() so the keys are in alphabetic order.

Following the header comes the array data. If the dtype contains Python objects (i.e. dtype.hasobject is True), then the data is a Python pickle of the array. Otherwise the data is the contiguous (either C- or Fortran-, depending on fortran\_order) bytes of the array. Consumers can figure out the number of bytes by multiplying the number of elements given by the shape (noting that shape=() means there is 1 element) by dtype.itemsize.

## Format specification: version 2.0

The version 1.0 format only allowed the array header to have a total size of 65535 bytes. This can be exceeded by structured arrays with a large number of columns. The version 2.0 format extends the header size to 4 GiB. <span class="title-ref">numpy.save</span> will automatically save in 2.0 format if the data requires it, else it will always use the more compatible 1.0 format.

The description of the fourth element of the header therefore has become:

> The next 4 bytes form a little-endian unsigned int: the length of the header data HEADER\_LEN.

## Conventions

We recommend using the ".npy" extension for files following this format. This is by no means a requirement; applications may wish to use this file format but use an extension specific to the application. In the absence of an obvious alternative, however, we suggest using ".npy".

For a simple way to combine multiple arrays into a single file, one can use ZipFile to contain multiple ".npy" files. We recommend using the file extension ".npz" for these archives.

## Alternatives

The author believes that this system (or one along these lines) is about the simplest system that satisfies all of the requirements. However, one must always be wary of introducing a new binary format to the world.

HDF5 \[2\] is a very flexible format that should be able to represent all of NumPy's arrays in some fashion. It is probably the only widely-used format that can faithfully represent all of NumPy's array features. It has seen substantial adoption by the scientific community in general and the NumPy community in particular. It is an excellent solution for a wide variety of array storage problems with or without NumPy.

HDF5 is a complicated format that more or less implements a hierarchical filesystem-in-a-file. This fact makes satisfying some of the Requirements difficult. To the author's knowledge, as of this writing, there is no application or library that reads or writes even a subset of HDF5 files that does not use the canonical libhdf5 implementation. This implementation is a large library that is not always easy to build. It would be infeasible to include it in numpy.

It might be feasible to target an extremely limited subset of HDF5. Namely, there would be only one object in it: the array. Using contiguous storage for the data, one should be able to implement just enough of the format to provide the same metadata that the proposed format does. One could still meet all of the technical requirements like mmapability.

We would accrue a substantial benefit by being able to generate files that could be read by other HDF5 software. Furthermore, by providing the first non-libhdf5 implementation of HDF5, we would be able to encourage more adoption of simple HDF5 in applications where it was previously infeasible because of the size of the library. The basic work may encourage similar dead-simple implementations in other languages and further expand the community.

The remaining concern is about reverse engineerability of the format. Even the simple subset of HDF5 would be very difficult to reverse engineer given just a file by itself. However, given the prominence of HDF5, this might not be a substantial concern.

In conclusion, we are going forward with the design laid out in this document. If someone writes code to handle the simple subset of HDF5 that would be useful to us, we may consider a revision of the file format.

## Implementation

The version 1.0 implementation was first included in the 1.0.5 release of numpy, and remains available. The version 2.0 implementation was first included in the 1.9.0 release of numpy.

Specifically, the file format.py in this directory implements the format as described here.

> <https://github.com/numpy/numpy/blob/main/numpy/lib/format.py>

## References

\[1\] <https://docs.python.org/library/pickle.html>

\[2\] <https://support.hdfgroup.org/HDF5/>

## Copyright

This document has been placed in the public domain.

---

nep-0002-warnfix.md

---

# NEP 2 â€” A proposal to build numpy without warning with a big set of warning flags

  - Author  
    David Cournapeau

  - Contact  
    <david@ar.media.kyoto-u.ac.jp>

  - Date  
    2008-09-04

  - Status  
    Deferred

## Executive summary

When building numpy and scipy, we are limited to a quite restricted set of warning compilers, thus missing a large class of potential bugs which could be detected with stronger warning flags. The goal of this NEP is present the various methods used to clean the code and implement some policy to make numpy buildable with a bigger set of warning flags, while keeping the build warnings free.

## Warning flags

Each compiler detects a different set of potential errors. The baseline will be gcc -Wall -W -Wextra. Ideally, a complete set would be nice:

`` `bash   -W -Wall -Wextra -Wstrict-prototypes -Wmissing-prototypes -Waggregate-return   -Wcast-align -Wcast-qual -Wnested-externs -Wshadow -Wbad-function-cast   -Wwrite-strings "  Intel compiler, VS with ``/W3 /Wall`, Sun compilers have extra warnings too.  Kind of warnings`\` ================

C Python extension code tends to naturally generate a lot of spurious warnings. The goal is to have some facilities to tag some typical C-Python code so that the compilers do not generate warnings in those cases; the tag process has to be clean, readable, and be robust. In particular, it should not make the code more obscure or worse, break working code.

### Unused parameter

This one appears often: any python-callable C function takes two arguments, of which the first is not used for functions (only for methods). One way to solve it is to tag the function argument with a macro NPY\_UNUSED. This macro uses compiler specific code to tag the variable, and mangle it such as it is not possible to use it accidentally once it is tagged.

The code to apply compiler specific option could be:

``` c
#if defined(__GNUC__)
        #define __COMP_NPY_UNUSED __attribute__ ((__unused__))
# elif defined(__ICC)
        #define __COMP_NPY_UNUSED __attribute__ ((__unused__))
#else
        #define __COMP_NPY_UNUSED
#endif
```

The variable mangling would be:

``` c
#define NPY_UNUSED(x) (__NPY_UNUSED_TAGGED ## x) __COMP_NPY_UNUSED
```

When applied to a variable, one would get:

``` c
int foo(int * NPY_UNUSED(dummy))
```

expanded to:

``` c
int foo(int * __NPY_UNUSED_TAGGEDdummy __COMP_NPY_UNUSED)
```

Thus avoiding any accidental use of the variable. The mangling is pure C, and thus portable. The per-variable warning disabling is compiler specific.

### Signed/unsigned comparison

More tricky: not always clear what to do

### Half-initialized structures

Just put the elements with NULL in it.

---

nep-0003-math_config_clean.md

---

# NEP 3 â€” Cleaning the math configuration of numpy.core

  - Author  
    David Cournapeau

  - Contact  
    <david@ar.media.kyoto-u.ac.jp>

  - Date  
    2008-09-04

  - Status  
    Deferred

## Executive summary

Before building numpy.core, we use some configuration tests to gather some information about available math functions. Over the years, the configuration became convoluted, to the point it became difficult to support new platforms easily.

The goal of this proposal is to clean the configuration of the math capabilities for easier maintenance.

## Current problems

Currently, the math configuration mainly test for some math functions, and configure numpy accordingly. But instead of testing each desired function independently, the current system has been developed more as workarounds particular platform oddities, using platform implicit knowledge. This is against the normal philosophy of testing for capabilities only, which is the autoconf philosophy, which showed the path toward portability (on Unix at least) \[1\] This causes problems because modifying or adding configuration on existing platforms break the implicit assumption, without a clear solution.

For example, on windows, when numpy is built with mingw, it would be nice to enforce the configuration sizeof(long double) == sizeof(double) because mingw uses the MS runtime, and the MS runtime does not support long double. Unfortunately, doing so breaks the mingw math function detection, because of the implicit assumption that mingw has a configuration sizeof(long double) \!= sizeof(double).

Another example is the testing for set of functions using only one function: if expf is found, it is assumed that all basic float functions are available. Instead, each function should be tested independently (expf, sinf, etc...).

## Requirements

  - We have two strong requirements:
    
      - it should not break any currently supported platform
      - it should not make the configuration much slower (1-2 seconds are acceptable)

## Proposal

We suggest to break any implicit assumption, and test each math function independently from each other, as usually done by autoconf. Since testing for a vast set of functions can be time consuming, we will use a scheme similar to AC\_CHECK\_FUNCS\_ONCE in autoconf, that is test for a set of function at once, and only in the case it breaks, do the per function check. When the first check works, it should be as fast as the current scheme, except that the assumptions are explicitly checked (all functions implied by HAVE\_LONGDOUBLE\_FUNCS would be checked together, for example).

## Issues

Static vs non static ? For basic functions, shall we define them static or not ?

## License

This document has been placed in the public domain.

\[1\]: Autobook here

---

nep-0004-datetime-proposal3.md

---

# NEP 4 â€” A (third) proposal for implementing some date/time types in NumPy

  - Author  
    Francesc Alted i Abad

  - Contact  
    <faltet@pytables.com>

  - Author  
    Ivan Vilata i Balaguer

  - Contact  
    <ivan@selidor.net>

  - Date  
    2008-07-30

  - Status  
    Deferred

## Executive summary

A date/time mark is something very handy to have in many fields where one has to deal with data sets. While Python has several modules that define a date/time type (like the integrated `datetime`\[1\] or `mx.DateTime`\[2\]), NumPy has a lack of them.

In this document, we are proposing the addition of a series of date/time types to fill this gap. The requirements for the proposed types are two-folded: 1) they have to be fast to operate with and 2) they have to be as compatible as possible with the existing `datetime` module that comes with Python.

## Types proposed

To start with, it is virtually impossible to come up with a single date/time type that fills the needs of every case of use. So, after pondering about different possibilities, we have stuck with *two* different types, namely `datetime64` and `timedelta64` (these names are preliminary and can be changed), that can have different time units so as to cover different needs.

<div class="important">

<div class="title">

Important

</div>

the time unit is conceived here as metadata that *complements* a date/time dtype, *without changing the base type*. It provides information about the *meaning* of the stored numbers, not about their *structure*.

</div>

Now follows a detailed description of the proposed types.

### `datetime64`

It represents a time that is absolute (i.e. not relative). It is implemented internally as an `int64` type. The internal epoch is the POSIX epoch (see\[3\]). Like POSIX, the representation of a date doesn't take leap seconds into account.

In time unit *conversions* and time *representations* (but not in other time computations), the value -2\**63 (0x8000000000000000) is interpreted as an invalid or unknown date,*Not a Time\* or *NaT*. See the section on time unit conversions for more information.

#### Time units

It accepts different time units, each of them implying a different time span. The table below describes the time units supported with their corresponding time spans.

<table>
<thead>
<tr class="header">
<th>Time unit</th>
<th>Time span (years)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Code Meaning</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>======== ================</td>
<td>==========================</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Y year</p>
</blockquote></td>
<td><blockquote>
<p>[9.2e18 BC, 9.2e18 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>M month</p>
</blockquote></td>
<td><blockquote>
<p>[7.6e17 BC, 7.6e17 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>W week</p>
</blockquote></td>
<td><blockquote>
<p>[1.7e17 BC, 1.7e17 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>B business day</p>
</blockquote></td>
<td><blockquote>
<p>[3.5e16 BC, 3.5e16 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>D day</p>
</blockquote></td>
<td><blockquote>
<p>[2.5e16 BC, 2.5e16 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>h hour</p>
</blockquote></td>
<td><blockquote>
<p>[1.0e15 BC, 1.0e15 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>m minute</p>
</blockquote></td>
<td><blockquote>
<p>[1.7e13 BC, 1.7e13 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>s second</p>
</blockquote></td>
<td><blockquote>
<p>[ 2.9e9 BC, 2.9e9 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>ms millisecond</p>
</blockquote></td>
<td><blockquote>
<p>[ 2.9e6 BC, 2.9e6 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>us microsecond</p>
</blockquote></td>
<td><blockquote>
<p>[290301 BC, 294241 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>c# ticks (100ns)</p>
</blockquote></td>
<td><blockquote>
<p>[ 2757 BC, 31197 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>ns nanosecond</p>
</blockquote></td>
<td><blockquote>
<p>[ 1678 AD, 2262 AD]</p>
</blockquote></td>
</tr>
</tbody>
</table>

The value of an absolute date is thus *an integer number of units of the chosen time unit* passed since the internal epoch. When working with business days, Saturdays and Sundays are simply ignored from the count (i.e. day 3 in business days is not Saturday 1970-01-03, but Monday 1970-01-05).

#### Building a `datetime64` dtype

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation:

    dtype('datetime64[us]')

Using the short string notation:

    dtype('M8[us]')

The default is microseconds if no time unit is specified. Thus, 'M8' is equivalent to 'M8\[us\]'

#### Setting and getting values

The objects with this dtype can be set in a series of ways:

    t = numpy.ones(3, dtype='M8[s]')
    t[0] = 1199164176    # assign to July 30th, 2008 at 17:31:00
    t[1] = datetime.datetime(2008, 7, 30, 17, 31, 01) # with datetime module
    t[2] = '2008-07-30T17:31:02'    # with ISO 8601

And can be get in different ways too:

    str(t[0])  -->  2008-07-30T17:31:00
    repr(t[1]) -->  datetime64(1199164177, 's')
    str(t[0].item()) --> 2008-07-30 17:31:00  # datetime module object
    repr(t[0].item()) --> datetime.datetime(2008, 7, 30, 17, 31)  # idem
    str(t)  -->  [2008-07-30T17:31:00  2008-07-30T17:31:01  2008-07-30T17:31:02]
    repr(t)  -->  array([1199164176, 1199164177, 1199164178],
                        dtype='datetime64[s]')

#### Comparisons

The comparisons will be supported too:

    numpy.array(['1980'], 'M8[Y]') == numpy.array(['1979'], 'M8[Y]')
    --> [False]

or by applying broadcasting:

    numpy.array(['1979', '1980'], 'M8[Y]') == numpy.datetime64('1980', 'Y')
    --> [False, True]

The next should work too:

    numpy.array(['1979', '1980'], 'M8[Y]') == '1980-01-01'
    --> [False, True]

because the right hand expression can be broadcasted into an array of 2 elements of dtype 'M8\[Y\]'.

#### Compatibility issues

This will be fully compatible with the `datetime` class of the `datetime` module of Python only when using a time unit of microseconds. For other time units, the conversion process will lose precision or will overflow as needed. The conversion from/to a `datetime` object doesn't take leap seconds into account.

### `timedelta64`

It represents a time that is relative (i.e. not absolute). It is implemented internally as an `int64` type.

In time unit *conversions* and time *representations* (but not in other time computations), the value -2\**63 (0x8000000000000000) is interpreted as an invalid or unknown time,*Not a Time\* or *NaT*. See the section on time unit conversions for more information.

#### Time units

It accepts different time units, each of them implying a different time span. The table below describes the time units supported with their corresponding time spans.

<table>
<thead>
<tr class="header">
<th>Time unit</th>
<th>Time span</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Code Meaning</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>======== ================</td>
<td>==========================</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Y year</p>
</blockquote></td>
<td><blockquote>
<p>+- 9.2e18 years</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>M month</p>
</blockquote></td>
<td><blockquote>
<p>+- 7.6e17 years</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>W week</p>
</blockquote></td>
<td><blockquote>
<p>+- 1.7e17 years</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>B business day</p>
</blockquote></td>
<td><blockquote>
<p>+- 3.5e16 years</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>D day</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.5e16 years</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>h hour</p>
</blockquote></td>
<td><blockquote>
<p>+- 1.0e15 years</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>m minute</p>
</blockquote></td>
<td><blockquote>
<p>+- 1.7e13 years</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>s second</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e12 years</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>ms millisecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e9 years</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>us microsecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e6 years</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>c# ticks (100ns)</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e4 years</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>ns nanosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 292 years</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>ps picosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 106 days</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>fs femtosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.6 hours</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>as attosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 9.2 seconds</p>
</blockquote></td>
</tr>
</tbody>
</table>

The value of a time delta is thus *an integer number of units of the chosen time unit*.

#### Building a `timedelta64` dtype

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation:

    dtype('timedelta64[us]')

Using the short string notation:

    dtype('m8[us]')

The default is micro-seconds if no default is specified: 'm8' is equivalent to 'm8\[us\]'

#### Setting and getting values

The objects with this dtype can be set in a series of ways:

    t = numpy.ones(3, dtype='m8[ms]')
    t[0] = 12    # assign to 12 ms
    t[1] = datetime.timedelta(0, 0, 13000)   # 13 ms
    t[2] = '0:00:00.014'    # 14 ms

And can be get in different ways too:

    str(t[0])  -->  0:00:00.012
    repr(t[1]) -->  timedelta64(13, 'ms')
    str(t[0].item()) --> 0:00:00.012000   # datetime module object
    repr(t[0].item()) --> datetime.timedelta(0, 0, 12000)  # idem
    str(t)     -->  [0:00:00.012  0:00:00.014  0:00:00.014]
    repr(t)    -->  array([12, 13, 14], dtype="timedelta64[ms]")

#### Comparisons

The comparisons will be supported too:

    numpy.array([12, 13, 14], 'm8[ms]') == numpy.array([12, 13, 13], 'm8[ms]')
    --> [True, True, False]

or by applying broadcasting:

    numpy.array([12, 13, 14], 'm8[ms]') == numpy.timedelta64(13, 'ms')
    --> [False, True, False]

The next should work too:

    numpy.array([12, 13, 14], 'm8[ms]') == '0:00:00.012'
    --> [True, False, False]

because the right hand expression can be broadcasted into an array of 3 elements of dtype 'm8\[ms\]'.

#### Compatibility issues

This will be fully compatible with the `timedelta` class of the `datetime` module of Python only when using a time unit of microseconds. For other units, the conversion process will lose precision or will overflow as needed.

## Examples of use

Here it is an example of use for the `datetime64`:

    In [5]: numpy.datetime64(42, 'us')
    Out[5]: datetime64(42, 'us')
    
    In [6]: print numpy.datetime64(42, 'us')
    1970-01-01T00:00:00.000042  # representation in ISO 8601 format
    
    In [7]: print numpy.datetime64(367.7, 'D')  # decimal part is lost
    1971-01-02  # still ISO 8601 format
    
    In [8]: numpy.datetime('2008-07-18T12:23:18', 'm')  # from ISO 8601
    Out[8]: datetime64(20273063, 'm')
    
    In [9]: print numpy.datetime('2008-07-18T12:23:18', 'm')
    Out[9]: 2008-07-18T12:23
    
    In [10]: t = numpy.zeros(5, dtype="datetime64[ms]")
    
    In [11]: t[0] = datetime.datetime.now()  # setter in action
    
    In [12]: print t
    [2008-07-16T13:39:25.315  1970-01-01T00:00:00.000
     1970-01-01T00:00:00.000  1970-01-01T00:00:00.000
     1970-01-01T00:00:00.000]
    
    In [13]: repr(t)
    Out[13]: array([267859210457, 0, 0, 0, 0], dtype="datetime64[ms]")
    
    In [14]: t[0].item()     # getter in action
    Out[14]: datetime.datetime(2008, 7, 16, 13, 39, 25, 315000)
    
    In [15]: print t.dtype
    dtype('datetime64[ms]')

And here it goes an example of use for the `timedelta64`:

    In [5]: numpy.timedelta64(10, 'us')
    Out[5]: timedelta64(10, 'us')
    
    In [6]: print numpy.timedelta64(10, 'us')
    0:00:00.000010
    
    In [7]: print numpy.timedelta64(3600.2, 'm')  # decimal part is lost
    2 days, 12:00
    
    In [8]: t1 = numpy.zeros(5, dtype="datetime64[ms]")
    
    In [9]: t2 = numpy.ones(5, dtype="datetime64[ms]")
    
    In [10]: t = t2 - t1
    
    In [11]: t[0] = datetime.timedelta(0, 24)  # setter in action
    
    In [12]: print t
    [0:00:24.000  0:00:01.000  0:00:01.000  0:00:01.000  0:00:01.000]
    
    In [13]: print repr(t)
    Out[13]: array([24000, 1, 1, 1, 1], dtype="timedelta64[ms]")
    
    In [14]: t[0].item()     # getter in action
    Out[14]: datetime.timedelta(0, 24)
    
    In [15]: print t.dtype
    dtype('timedelta64[s]')

## Operating with date/time arrays

### `datetime64` vs `datetime64`

The only arithmetic operation allowed between absolute dates is the subtraction:

    In [10]: numpy.ones(3, "M8[s]") - numpy.zeros(3, "M8[s]")
    Out[10]: array([1, 1, 1], dtype=timedelta64[s])

But not other operations:

    In [11]: numpy.ones(3, "M8[s]") + numpy.zeros(3, "M8[s]")
    TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'numpy.ndarray'

Comparisons between absolute dates are allowed.

#### Casting rules

When operating (basically, only the subtraction will be allowed) two absolute times with different unit times, the outcome would be to raise an exception. This is because the ranges and time-spans of the different time units can be very different, and it is not clear at all what time unit will be preferred for the user. For example, this should be allowed:

    >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[Y]")
    array([1, 1, 1], dtype="timedelta64[Y]")

But the next should not:

    >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[ns]")
    raise numpy.IncompatibleUnitError  # what unit to choose?

### `datetime64` vs `timedelta64`

It will be possible to add and subtract relative times from absolute dates:

    In [10]: numpy.zeros(5, "M8[Y]") + numpy.ones(5, "m8[Y]")
    Out[10]: array([1971, 1971, 1971, 1971, 1971], dtype=datetime64[Y])
    
    In [11]: numpy.ones(5, "M8[Y]") - 2 * numpy.ones(5, "m8[Y]")
    Out[11]: array([1969, 1969, 1969, 1969, 1969], dtype=datetime64[Y])

But not other operations:

    In [12]: numpy.ones(5, "M8[Y]") * numpy.ones(5, "m8[Y]")
    TypeError: unsupported operand type(s) for *: 'numpy.ndarray' and 'numpy.ndarray'

#### Casting rules

In this case the absolute time should have priority for determining the time unit of the outcome. That would represent what the people wants to do most of the times. For example, this would allow to do:

    >>> series = numpy.array(['1970-01-01', '1970-02-01', '1970-09-01'],
    dtype='datetime64[D]')
    >>> series2 = series + numpy.timedelta(1, 'Y')  # Add 2 relative years
    >>> series2
    array(['1972-01-01', '1972-02-01', '1972-09-01'],
    dtype='datetime64[D]')  # the 'D'ay time unit has been chosen

### `timedelta64` vs `timedelta64`

Finally, it will be possible to operate with relative times as if they were regular int64 dtypes *as long as* the result can be converted back into a `timedelta64`:

    In [10]: numpy.ones(3, 'm8[us]')
    Out[10]: array([1, 1, 1], dtype="timedelta64[us]")
    
    In [11]: (numpy.ones(3, 'm8[M]') + 2) ** 3
    Out[11]: array([27, 27, 27], dtype="timedelta64[M]")

But:

    In [12]: numpy.ones(5, 'm8') + 1j
    TypeError: the result cannot be converted into a ``timedelta64``

#### Casting rules

When combining two `timedelta64` dtypes with different time units the outcome will be the shorter of both ("keep the precision" rule). For example:

    In [10]: numpy.ones(3, 'm8[s]') + numpy.ones(3, 'm8[m]')
    Out[10]: array([61, 61, 61],  dtype="timedelta64[s]")

However, due to the impossibility to know the exact duration of a relative year or a relative month, when these time units appear in one of the operands, the operation will not be allowed:

    In [11]: numpy.ones(3, 'm8[Y]') + numpy.ones(3, 'm8[D]')
    raise numpy.IncompatibleUnitError  # how to convert relative years to days?

In order to being able to perform the above operation a new NumPy function, called `change_timeunit` is proposed. Its signature will be:

    change_timeunit(time_object, new_unit, reference)

where 'time\_object' is the time object whose unit is to be changed, 'new\_unit' is the desired new time unit, and 'reference' is an absolute date (NumPy datetime64 scalar) that will be used to allow the conversion of relative times in case of using time units with an uncertain number of smaller time units (relative years or months cannot be expressed in days).

With this, the above operation can be done as follows:

    In [10]: t_years = numpy.ones(3, 'm8[Y]')
    
    In [11]: t_days = numpy.change_timeunit(t_years, 'D', '2001-01-01')
    
    In [12]: t_days + numpy.ones(3, 'm8[D]')
    Out[12]: array([366, 366, 366],  dtype="timedelta64[D]")

## dtype vs time units conversions

For changing the date/time dtype of an existing array, we propose to use the `.astype()` method. This will be mainly useful for changing time units.

For example, for absolute dates:

    In[10]: t1 = numpy.zeros(5, dtype="datetime64[s]")
    
    In[11]: print t1
    [1970-01-01T00:00:00  1970-01-01T00:00:00  1970-01-01T00:00:00
     1970-01-01T00:00:00  1970-01-01T00:00:00]
    
    In[12]: print t1.astype('datetime64[D]')
    [1970-01-01  1970-01-01  1970-01-01  1970-01-01  1970-01-01]

For relative times:

    In[10]: t1 = numpy.ones(5, dtype="timedelta64[s]")
    
    In[11]: print t1
    [1 1 1 1 1]
    
    In[12]: print t1.astype('timedelta64[ms]')
    [1000 1000 1000 1000 1000]

Changing directly from/to relative to/from absolute dtypes will not be supported:

    In[13]: numpy.zeros(5, dtype="datetime64[s]").astype('timedelta64')
    TypeError: data type cannot be converted to the desired type

Business days have the peculiarity that they do not cover a continuous line of time (they have gaps at weekends). Thus, when converting from any ordinary time to business days, it can happen that the original time is not representable. In that case, the result of the conversion is *Not a Time* (*NaT*):

    In[10]: t1 = numpy.arange(5, dtype="datetime64[D]")
    
    In[11]: print t1
    [1970-01-01  1970-01-02  1970-01-03  1970-01-04  1970-01-05]
    
    In[12]: t2 = t1.astype("datetime64[B]")
    
    In[13]: print t2  # 1970 begins in a Thursday
    [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

When converting back to ordinary days, NaT values are left untouched (this happens in all time unit conversions):

    In[14]: t3 = t2.astype("datetime64[D]")
    
    In[13]: print t3
    [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

## Final considerations

### Why the `origin` metadata disappeared

During the discussion of the date/time dtypes in the NumPy list, the idea of having an `origin` metadata that complemented the definition of the absolute `datetime64` was initially found to be useful.

However, after thinking more about this, we found that the combination of an absolute `datetime64` with a relative `timedelta64` does offer the same functionality while removing the need for the additional `origin` metadata. This is why we have removed it from this proposal.

### Operations with mixed time units

Whenever an operation between two time values of the same dtype with the same unit is accepted, the same operation with time values of different units should be possible (e.g. adding a time delta in seconds and one in microseconds), resulting in an adequate time unit. The exact semantics of this kind of operations is defined int the "Casting rules" subsections of the "Operating with date/time arrays" section.

Due to the peculiarities of business days, it is most probable that operations mixing business days with other time units will not be allowed.

### Why there is not a `quarter` time unit?

This proposal tries to focus on the most common used set of time units to operate with, and the `quarter` can be considered more of a derived unit. Besides, the use of a `quarter` normally requires that it can start at whatever month of the year, and as we are not including support for a time `origin` metadata, this is not a viable venue here. Finally, if we were to add the `quarter` then people should expect to find a `biweekly`, `semester` or `biyearly` just to put some examples of other derived units, and we find this a bit too overwhelming for this proposal purposes.

1.  <https://docs.python.org/library/datetime.html>

2.  <https://www.egenix.com/products/python/mxBase/mxDateTime>

3.  <https://en.wikipedia.org/wiki/Unix_time>

---

nep-0005-generalized-ufuncs.md

---

# NEP 5 â€” Generalized universal functions

  - Status  
    Final

There is a general need for looping over not only functions on scalars but also over functions on vectors (or arrays), as explained on <http://scipy.org/scipy/numpy/wiki/GeneralLoopingFunctions>. We propose to realize this concept by generalizing the universal functions (ufuncs), and provide a C implementation that adds \~500 lines to the numpy code base. In current (specialized) ufuncs, the elementary function is limited to element-by-element operations, whereas the generalized version supports "sub-array" by "sub-array" operations. The Perl vector library PDL provides a similar functionality and its terms are re-used in the following.

Each generalized ufunc has information associated with it that states what the "core" dimensionality of the inputs is, as well as the corresponding dimensionality of the outputs (the element-wise ufuncs have zero core dimensions). The list of the core dimensions for all arguments is called the "signature" of a ufunc. For example, the ufunc numpy.add has signature `(),()->()` defining two scalar inputs and one scalar output.

Another example is (see the GeneralLoopingFunctions page) the function `inner1d(a,b)` with a signature of `(i),(i)->()`. This applies the inner product along the last axis of each input, but keeps the remaining indices intact. For example, where `a` is of shape `(3,5,N)` and `b` is of shape `(5,N)`, this will return an output of shape `(3,5)`. The underlying elementary function is called 3\*5 times. In the signature, we specify one core dimension `(i)` for each input and zero core dimensions `()` for the output, since it takes two 1-d arrays and returns a scalar. By using the same name `i`, we specify that the two corresponding dimensions should be of the same size (or one of them is of size 1 and will be broadcasted).

The dimensions beyond the core dimensions are called "loop" dimensions. In the above example, this corresponds to `(3,5)`.

The usual numpy "broadcasting" rules apply, where the signature determines how the dimensions of each input/output object are split into core and loop dimensions:

1.  While an input array has a smaller dimensionality than the corresponding number of core dimensions, 1's are prepended to its shape.
2.  The core dimensions are removed from all inputs and the remaining dimensions are broadcasted; defining the loop dimensions.
3.  The output is given by the loop dimensions plus the output core dimensions.

## Definitions

  - Elementary Function  
    Each ufunc consists of an elementary function that performs the most basic operation on the smallest portion of array arguments (e.g. adding two numbers is the most basic operation in adding two arrays). The ufunc applies the elementary function multiple times on different parts of the arrays. The input/output of elementary functions can be vectors; e.g., the elementary function of inner1d takes two vectors as input.

  - Signature  
    A signature is a string describing the input/output dimensions of the elementary function of a ufunc. See section below for more details.

  - Core Dimension  
    The dimensionality of each input/output of an elementary function is defined by its core dimensions (zero core dimensions correspond to a scalar input/output). The core dimensions are mapped to the last dimensions of the input/output arrays.

  - Dimension Name  
    A dimension name represents a core dimension in the signature. Different dimensions may share a name, indicating that they are of the same size (or are broadcastable).

  - Dimension Index  
    A dimension index is an integer representing a dimension name. It enumerates the dimension names according to the order of the first occurrence of each name in the signature.

## Details of signature

The signature defines "core" dimensionality of input and output variables, and thereby also defines the contraction of the dimensions. The signature is represented by a string of the following format:

  - Core dimensions of each input or output array are represented by a list of dimension names in parentheses, `(i_1,...,i_N)`; a scalar input/output is denoted by `()`. Instead of `i_1`, `i_2`, etc, one can use any valid Python variable name.
  - Dimension lists for different arguments are separated by `","`. Input/output arguments are separated by `"->"`.
  - If one uses the same dimension name in multiple locations, this enforces the same size (or broadcastable size) of the corresponding dimensions.

The formal syntax of signatures is as follows:

    <Signature>            ::= <Input arguments> "->" <Output arguments>
    <Input arguments>      ::= <Argument list>
    <Output arguments>     ::= <Argument list>
    <Argument list>        ::= nil | <Argument> | <Argument> "," <Argument list>
    <Argument>             ::= "(" <Core dimension list> ")"
    <Core dimension list>  ::= nil | <Dimension name> |
                               <Dimension name> "," <Core dimension list>
    <Dimension name>       ::= valid Python variable name

Notes:

1.  All quotes are for clarity.
2.  Core dimensions that share the same name must be broadcastable, as the two `i` in our example above. Each dimension name typically corresponding to one level of looping in the elementary function's implementation.
3.  White spaces are ignored.

Here are some examples of signatures:

|              |                      |                                                                                                 |
| ------------ | -------------------- | ----------------------------------------------------------------------------------------------- |
| add          | `(),()->()`          |                                                                                                 |
| inner1d      | `(i),(i)->()`        |                                                                                                 |
| sum1d        | `(i)->()`            |                                                                                                 |
| dot2d        | `(m,n),(n,p)->(m,p)` | matrix multiplication                                                                           |
| outer\_inner | `(i,t),(j,t)->(i,j)` | inner over the last dimension, outer over the second to last, and loop/broadcast over the rest. |

## C-API for implementing elementary functions

The current interface remains unchanged, and `PyUFunc_FromFuncAndData` can still be used to implement (specialized) ufuncs, consisting of scalar elementary functions.

One can use `PyUFunc_FromFuncAndDataAndSignature` to declare a more general ufunc. The argument list is the same as `PyUFunc_FromFuncAndData`, with an additional argument specifying the signature as C string.

Furthermore, the callback function is of the same type as before, `void (*foo)(char **args, intp *dimensions, intp *steps, void *func)`. When invoked, `args` is a list of length `nargs` containing the data of all input/output arguments. For a scalar elementary function, `steps` is also of length `nargs`, denoting the strides used for the arguments. `dimensions` is a pointer to a single integer defining the size of the axis to be looped over.

For a non-trivial signature, `dimensions` will also contain the sizes of the core dimensions as well, starting at the second entry. Only one size is provided for each unique dimension name and the sizes are given according to the first occurrence of a dimension name in the signature.

The first `nargs` elements of `steps` remain the same as for scalar ufuncs. The following elements contain the strides of all core dimensions for all arguments in order.

For example, consider a ufunc with signature `(i,j),(i)->()`. In this case, `args` will contain three pointers to the data of the input/output arrays `a`, `b`, `c`. Furthermore, `dimensions` will be `[N, I, J]` to define the size of `N` of the loop and the sizes `I` and `J` for the core dimensions `i` and `j`. Finally, `steps` will be `[a_N, b_N, c_N, a_i, a_j, b_i]`, containing all necessary strides.

---

nep-0006-newbugtracker.md

---

# NEP 6 â€” Replacing Trac with a different bug tracker

  - Author  
    David Cournapeau, Stefan van der Walt

  - Status  
    Deferred

Some release managers of both numpy and scipy are becoming more and more dissatisfied with the current development workflow, in particular for bug tracking. This document is a tentative to explain some problematic scenario, current trac limitations, and what can be done about it.

## Scenario

### New release

The workflow for a release is roughly as follows:

>   - find all known regressions from last release, and fix them
>     
>     >   - get an idea of all bugs reported since last release
>     >   - triage bugs in regressions/blocker issues/etc..., and assign them in the according roadmap, subpackage and maintainers
> 
>   - pinging subpackage maintainers

Most of those tasks are quite inefficient in the current trac as used on scipy:

>   - it is hard to keep track of issues. In particular, every time one goes to trac, we don't really know what's new from what's not. If you think of issues as emails, the current situation would be like not having read/unread feature.
>   - Batch handling of issues: changing characteristics of several issues at the same time is difficult, because the only available UI is web-based. Command-line based UI are much more efficient for this kind of scenario

More generally, making useful reports is very awkward with the currently deployed trac. Trac 0.11 may solve of those problems, but it has to be much better than the actually deployed version on scipy website. Finding issues with patches, old patches, etc... and making reports has to be much more streamlined that it is now.

### Subcomponent maintainer

Say you are the maintainer of scipy.foo, then you are mostly interested in getting bugs concerning scipy.foo only. But it should be easy for the general team to follow your work - it should also be easy for casual users (e.g. not developers) to follow some new features development pace.

### Review, newcoming code

The goal is simple: make the bar as low as possible, and make sure people know what to do at every step to contribute to numpy or scipy:

>   - Right now, patches languish for too long in trac. Of course, lack of time is one big reason; but the process of following new contributes could be made much simpler
>   - It should be possible to be pinged only for reviews one a subset of numpy/scipy.
>   - It should be possible for people interested in the patches to follow its progression. Comments, but also 'mini' timelines could be useful, particularly for massive issues (massive from a coding POV).

## Current trac limitation

Note: by trac, we mean the currently deployed one. Some more recent versions may solve some of the issues.

>   - Multi-project support: we have three trac instances, one for scipy, one for numpy, one for scikits. Creating accounts, maintaining and updating each of them is a maintenance burden. Nobody likes to do this kind of work, so anything which can reduce the burden is a plus. Also, it happens quite frequently that a bug against numpy is filled on scipy trac and vice and versa. You have to handle this manually, currently.
>   - Clients not based on the web-ui. This can be made through the xmlrpc plugin + some clients. In particular, something like <http://tracexplorer.devjavu.com/> can be interesting for people who like IDE. At least one person expressed his desire to have as much integration as possible with Eclipse.
>   - Powerful queries: it should be possible to quickly find issues between two releases, the new issues from a given date, issues with patch, issues waiting for reviews, etc... The issues data have to be customizable, because most bug-tracker do not support things like review, etc... so we need to handle this ourselves (through tags, etc...)
>   - Marking issues as read/unread. It should also be possible for any user to 'mask' issues to ignore them.
>   - ticket dependency. This is quite helpful in my experience for big features which can be split into several issues. Roadmap can only be created by trac admin, and they are kind of heavy-weight.

## Possible candidates

### Updated trac + plugins

Pros:

>   - Same system
>   - In python, so we can hack it if we want

Cons:

>   - Trac is aimed at being basic, and extended with plugins. But most plugins are broken, or not up to date. The information on which plugins are mature is not easily available.
>   - At least the scipy.org trac was slow, and needed to be restarted constantly. This is simply not acceptable.

### Redmine

Pros:

>   - Support most features (except xmlrpc ?). Multi-project, etc...
>   - (subjective): I (cdavid) find the out-of-the-box experience with redmine much more enjoyable. More information is available easily, less clicks, more streamlined. See <http://www.redmine.org/wiki/redmine/TheyAreUsingRedmine> for examples
>   - Conversion scripts from trac (no experience with it yet for numpy/scipy).
>   - Community seems friendly and gets a lof of features done

Cons:

>   - new system, less mature ?
>   - in Ruby: since we are a python project, most of dev are familiar with python.
>   - Wiki integration, etc... ?

Unknown:

>   - xmlrpc API
>   - performances
>   - maintenance cost

### Roundup

TODO

---

nep-0007-datetime-proposal.md

---

# NEP 7 â€” A proposal for implementing some date/time types in NumPy

  - Author  
    Travis Oliphant

  - Contact  
    <oliphant@enthought.com>

  - Date  
    2009-06-09

  - Status  
    Final

Revised only slightly from the third proposal by

  - Author  
    Francesc Alted i Abad

  - Contact  
    <faltet@pytables.com>

  - Author  
    Ivan Vilata i Balaguer

  - Contact  
    <ivan@selidor.net>

  - Date  
    2008-07-30

## Executive summary

A date/time mark is something very handy to have in many fields where one has to deal with data sets. While Python has several modules that define a date/time type (like the integrated `datetime`\[1\] or `mx.DateTime`\[2\]), NumPy has a lack of them.

We are proposing the addition of date/time types to fill this gap. The requirements for the proposed types are two-fold: 1) they have to be fast to operate with and 2) they have to be as compatible as possible with the existing `datetime` module that comes with Python.

## Types proposed

It is virtually impossible to come up with a single date/time type that fills the needs of every use case. As a result, we propose two general date-time types: 1) `timedelta64` -- a relative time and 2) `datetime64` -- an absolute time.

Each of these times are represented internally as 64-bit signed integers that refer to a particular unit (hour, minute, microsecond, etc.). There are several pre-defined units as well as the ability to create rational multiples of these units. A representation is also supported such that the stored date-time integer can encode both the number of a particular unit as well as a number of sequential events tracked for each unit.

The `datetime64` represents an absolute time. Internally it is represented as the number of time units between the intended time and the epoch (12:00am on January 1, 1970 --- POSIX time including its lack of leap seconds).

## Time units

The 64-bit integer time can represent several different basic units as well as derived units. The basic units are listed in the following table:

<table>
<thead>
<tr class="header">
<th>Time unit</th>
<th>Time span</th>
<th>Time span (years)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Code Meaning</p>
</blockquote></td>
<td><blockquote>
<p>Relative Time</p>
</blockquote></td>
<td><blockquote>
<p>Absolute Time</p>
</blockquote></td>
</tr>
<tr class="even">
<td>======== ================</td>
<td>=======================</td>
<td>==========================</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Y year</p>
</blockquote></td>
<td><blockquote>
<p>+- 9.2e18 years</p>
</blockquote></td>
<td><blockquote>
<p>[9.2e18 BC, 9.2e18 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>M month</p>
</blockquote></td>
<td><blockquote>
<p>+- 7.6e17 years</p>
</blockquote></td>
<td><blockquote>
<p>[7.6e17 BC, 7.6e17 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>W week</p>
</blockquote></td>
<td><blockquote>
<p>+- 1.7e17 years</p>
</blockquote></td>
<td><blockquote>
<p>[1.7e17 BC, 1.7e17 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>B business day</p>
</blockquote></td>
<td><blockquote>
<p>+- 3.5e16 years</p>
</blockquote></td>
<td><blockquote>
<p>[3.5e16 BC, 3.5e16 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>D day</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.5e16 years</p>
</blockquote></td>
<td><blockquote>
<p>[2.5e16 BC, 2.5e16 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>h hour</p>
</blockquote></td>
<td><blockquote>
<p>+- 1.0e15 years</p>
</blockquote></td>
<td><blockquote>
<p>[1.0e15 BC, 1.0e15 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>m minute</p>
</blockquote></td>
<td><blockquote>
<p>+- 1.7e13 years</p>
</blockquote></td>
<td><blockquote>
<p>[1.7e13 BC, 1.7e13 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>s second</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e12 years</p>
</blockquote></td>
<td><blockquote>
<p>[ 2.9e9 BC, 2.9e9 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>ms millisecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e9 years</p>
</blockquote></td>
<td><blockquote>
<p>[ 2.9e6 BC, 2.9e6 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>us microsecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.9e6 years</p>
</blockquote></td>
<td><blockquote>
<p>[290301 BC, 294241 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>ns nanosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 292 years</p>
</blockquote></td>
<td><blockquote>
<p>[ 1678 AD, 2262 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>ps picosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 106 days</p>
</blockquote></td>
<td><blockquote>
<p>[ 1969 AD, 1970 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>fs femtosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 2.6 hours</p>
</blockquote></td>
<td><blockquote>
<p>[ 1969 AD, 1970 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>as attosecond</p>
</blockquote></td>
<td><blockquote>
<p>+- 9.2 seconds</p>
</blockquote></td>
<td><blockquote>
<p>[ 1969 AD, 1970 AD]</p>
</blockquote></td>
</tr>
</tbody>
</table>

A time unit is specified by a string consisting of a base-type given in the above table

Besides these basic code units, the user can create derived units consisting of multiples of any basic unit: 100ns, 3M, 15m, etc.

A limited number of divisions of any basic unit can be used to create multiples of a higher-resolution unit provided the divisor can be divided evenly into the number of higher-resolution units available. For example: Y/4 is just short-hand for -\> (12M)/4 -\> 3M and Y/4 will be represented after creation as 3M. The first lower unit found to have an even divisor will be chosen (up to 3 lower units). The following standardized definitions are used in this specific case to find acceptable divisors

| Code | Interpreted as       |
| ---- | -------------------- |
| Y    | 12M, 52W, 365D       |
| M    | 4W, 30D, 720h        |
| W    | 5B, 7D, 168h, 10080m |
| B    | 24h, 1440m, 86400s   |
| D    | 24h, 1440m, 86400s   |
| h    | 60m, 3600s           |
| m    | 60s, 60000ms         |

s, ms, us, ns, ps, fs (use 1000 and 1000000 of the next two available lower units respectively).

Finally, a date-time data-type can be created with support for tracking sequential events within a basic unit: \[D\]//100, \[Y\]//4 (notice the required brackets). These `modulo` event units provide the following interpretation to the date-time integer:

>   - the divisor is the number of events in each period
>   - the (integer) quotient is the integer number representing the base units
>   - the remainder is the particular event in the period.

Modulo event-units can be combined with any derived units, but brackets are required. Thus \[100ns\]//50 which allows recording 50 events for every 100ns so that 0 represents the first event in the first 100ns tick, 1 represents the second event in the first 100ns tick, while 50 represents the first event in the second 100ns tick, and 51 represents the second event in the second 100ns tick.

To fully specify a date-time type, the time unit string must be combined with either the string for a datetime64 ('M8') or a timedelta64 ('m8') using brackets '\[\]'. Therefore, a fully-specified string representing a date-time dtype is 'M8\[Y\]' or (for a more complicated example) 'M8\[7s/9\]//5'.

If a time unit is not specified, then it defaults to \[us\]. Thus 'M8' is equivalent to 'M8\[us\]' (except when modulo event-units are desired --i.e. you cannot specify 'M8\[us\]//5' as 'M8//5' or as '//5'

## `datetime64`

This dtype represents a time that is absolute (i.e. not relative). It is implemented internally as an `int64` type. The integer represents units from the internal POSIX epoch (see\[3\]). Like POSIX, the representation of a date doesn't take leap seconds into account.

In time unit *conversions* and time *representations* (but not in other time computations), the value -2\**63 (0x8000000000000000) is interpreted as an invalid or unknown date,*Not a Time\* or *NaT*. See the section on time unit conversions for more information.

The value of an absolute date is thus *an integer number of units of the chosen time unit* passed since the epoch. If the integer is a negative number, then the magnitude of the integer represents the number of units prior to the epoch. When working with business days, Saturdays and Sundays are simply ignored from the count (i.e. day 3 in business days is not Saturday 1970-01-03, but Monday 1970-01-05).

### Building a `datetime64` dtype

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation:

    dtype('datetime64[us]')

Using the short string notation:

    dtype('M8[us]')

If a time unit is not specified, then it defaults to \[us\]. Thus 'M8' is equivalent to 'M8\[us\]'.

### Setting and getting values

The objects with this dtype can be set in a series of ways:

    t = numpy.ones(3, dtype='M8[s]')
    t[0] = 1199164176    # assign to July 30th, 2008 at 17:31:00
    t[1] = datetime.datetime(2008, 7, 30, 17, 31, 01) # with datetime module
    t[2] = '2008-07-30T17:31:02'    # with ISO 8601

And can be get in different ways too:

    str(t[0])  -->  2008-07-30T17:31:00
    repr(t[1]) -->  datetime64(1199164177, 's')
    str(t[0].item()) --> 2008-07-30 17:31:00  # datetime module object
    repr(t[0].item()) --> datetime.datetime(2008, 7, 30, 17, 31)  # idem
    str(t)  -->  [2008-07-30T17:31:00  2008-07-30T17:31:01  2008-07-30T17:31:02]
    repr(t)  -->  array([1199164176, 1199164177, 1199164178],
                        dtype='datetime64[s]')

### Comparisons

The comparisons will be supported too:

    numpy.array(['1980'], 'M8[Y]') == numpy.array(['1979'], 'M8[Y]')
    --> [False]

including applying broadcasting:

    numpy.array(['1979', '1980'], 'M8[Y]') == numpy.datetime64('1980', 'Y')
    --> [False, True]

The following should also work:

    numpy.array(['1979', '1980'], 'M8[Y]') == '1980-01-01'
    --> [False, True]

because the right hand expression can be broadcasted into an array of 2 elements of dtype 'M8\[Y\]'.

### Compatibility issues

This will be fully compatible with the `datetime` class of the `datetime` module of Python only when using a time unit of microseconds. For other time units, the conversion process will lose precision or will overflow as needed. The conversion from/to a `datetime` object doesn't take leap seconds into account.

## `timedelta64`

It represents a time that is relative (i.e. not absolute). It is implemented internally as an `int64` type.

In time unit *conversions* and time *representations* (but not in other time computations), the value -2\**63 (0x8000000000000000) is interpreted as an invalid or unknown time,*Not a Time\* or *NaT*. See the section on time unit conversions for more information.

The value of a time delta is *an integer number of units of the chosen time unit*.

### Building a `timedelta64` dtype

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation:

    dtype('timedelta64[us]')

Using the short string notation:

    dtype('m8[us]')

If a time unit is not specified, then a default of \[us\] is assumed. Thus 'm8' and 'm8\[us\]' are equivalent.

### Setting and getting values

The objects with this dtype can be set in a series of ways:

    t = numpy.ones(3, dtype='m8[ms]')
    t[0] = 12    # assign to 12 ms
    t[1] = datetime.timedelta(0, 0, 13000)   # 13 ms
    t[2] = '0:00:00.014'    # 14 ms

And can be get in different ways too:

    str(t[0])  -->  0:00:00.012
    repr(t[1]) -->  timedelta64(13, 'ms')
    str(t[0].item()) --> 0:00:00.012000   # datetime module object
    repr(t[0].item()) --> datetime.timedelta(0, 0, 12000)  # idem
    str(t)     -->  [0:00:00.012  0:00:00.014  0:00:00.014]
    repr(t)    -->  array([12, 13, 14], dtype="timedelta64[ms]")

### Comparisons

The comparisons will be supported too:

    numpy.array([12, 13, 14], 'm8[ms]') == numpy.array([12, 13, 13], 'm8[ms]')
    --> [True, True, False]

or by applying broadcasting:

    numpy.array([12, 13, 14], 'm8[ms]') == numpy.timedelta64(13, 'ms')
    --> [False, True, False]

The following should work too:

    numpy.array([12, 13, 14], 'm8[ms]') == '0:00:00.012'
    --> [True, False, False]

because the right hand expression can be broadcasted into an array of 3 elements of dtype 'm8\[ms\]'.

### Compatibility issues

This will be fully compatible with the `timedelta` class of the `datetime` module of Python only when using a time unit of microseconds. For other units, the conversion process will lose precision or will overflow as needed.

## Examples of use

Here is an example of use for the `datetime64`:

    In [5]: numpy.datetime64(42, 'us')
    Out[5]: datetime64(42, 'us')
    
    In [6]: print numpy.datetime64(42, 'us')
    1970-01-01T00:00:00.000042  # representation in ISO 8601 format
    
    In [7]: print numpy.datetime64(367.7, 'D')  # decimal part is lost
    1971-01-02  # still ISO 8601 format
    
    In [8]: numpy.datetime('2008-07-18T12:23:18', 'm')  # from ISO 8601
    Out[8]: datetime64(20273063, 'm')
    
    In [9]: print numpy.datetime('2008-07-18T12:23:18', 'm')
    Out[9]: 2008-07-18T12:23
    
    In [10]: t = numpy.zeros(5, dtype="datetime64[ms]")
    
    In [11]: t[0] = datetime.datetime.now()  # setter in action
    
    In [12]: print t
    [2008-07-16T13:39:25.315  1970-01-01T00:00:00.000
     1970-01-01T00:00:00.000  1970-01-01T00:00:00.000
     1970-01-01T00:00:00.000]
    
    In [13]: repr(t)
    Out[13]: array([267859210457, 0, 0, 0, 0], dtype="datetime64[ms]")
    
    In [14]: t[0].item()     # getter in action
    Out[14]: datetime.datetime(2008, 7, 16, 13, 39, 25, 315000)
    
    In [15]: print t.dtype
    dtype('datetime64[ms]')

And here it goes an example of use for the `timedelta64`:

    In [5]: numpy.timedelta64(10, 'us')
    Out[5]: timedelta64(10, 'us')
    
    In [6]: print numpy.timedelta64(10, 'us')
    0:00:00.000010
    
    In [7]: print numpy.timedelta64(3600.2, 'm')  # decimal part is lost
    2 days, 12:00
    
    In [8]: t1 = numpy.zeros(5, dtype="datetime64[ms]")
    
    In [9]: t2 = numpy.ones(5, dtype="datetime64[ms]")
    
    In [10]: t = t2 - t1
    
    In [11]: t[0] = datetime.timedelta(0, 24)  # setter in action
    
    In [12]: print t
    [0:00:24.000  0:00:01.000  0:00:01.000  0:00:01.000  0:00:01.000]
    
    In [13]: print repr(t)
    Out[13]: array([24000, 1, 1, 1, 1], dtype="timedelta64[ms]")
    
    In [14]: t[0].item()     # getter in action
    Out[14]: datetime.timedelta(0, 24)
    
    In [15]: print t.dtype
    dtype('timedelta64[s]')

## Operating with date/time arrays

### `datetime64` vs `datetime64`

The only arithmetic operation allowed between absolute dates is subtraction:

    In [10]: numpy.ones(3, "M8[s]") - numpy.zeros(3, "M8[s]")
    Out[10]: array([1, 1, 1], dtype=timedelta64[s])

But not other operations:

    In [11]: numpy.ones(3, "M8[s]") + numpy.zeros(3, "M8[s]")
    TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'numpy.ndarray'

Comparisons between absolute dates are allowed.

#### Casting rules

When operating (basically, only the subtraction will be allowed) two absolute times with different unit times, the outcome would be to raise an exception. This is because the ranges and time-spans of the different time units can be very different, and it is not clear at all what time unit will be preferred for the user. For example, this should be allowed:

    >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[Y]")
    array([1, 1, 1], dtype="timedelta64[Y]")

But the next should not:

    >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[ns]")
    raise numpy.IncompatibleUnitError  # what unit to choose?

### `datetime64` vs `timedelta64`

It will be possible to add and subtract relative times from absolute dates:

    In [10]: numpy.zeros(5, "M8[Y]") + numpy.ones(5, "m8[Y]")
    Out[10]: array([1971, 1971, 1971, 1971, 1971], dtype=datetime64[Y])
    
    In [11]: numpy.ones(5, "M8[Y]") - 2 * numpy.ones(5, "m8[Y]")
    Out[11]: array([1969, 1969, 1969, 1969, 1969], dtype=datetime64[Y])

But not other operations:

    In [12]: numpy.ones(5, "M8[Y]") * numpy.ones(5, "m8[Y]")
    TypeError: unsupported operand type(s) for *: 'numpy.ndarray' and 'numpy.ndarray'

#### Casting rules

In this case the absolute time should have priority for determining the time unit of the outcome. That would represent what the people wants to do most of the times. For example, this would allow to do:

    >>> series = numpy.array(['1970-01-01', '1970-02-01', '1970-09-01'],
    dtype='datetime64[D]')
    >>> series2 = series + numpy.timedelta(1, 'Y')  # Add 2 relative years
    >>> series2
    array(['1972-01-01', '1972-02-01', '1972-09-01'],
    dtype='datetime64[D]')  # the 'D'ay time unit has been chosen

### `timedelta64` vs `timedelta64`

Finally, it will be possible to operate with relative times as if they were regular int64 dtypes *as long as* the result can be converted back into a `timedelta64`:

    In [10]: numpy.ones(3, 'm8[us]')
    Out[10]: array([1, 1, 1], dtype="timedelta64[us]")
    
    In [11]: (numpy.ones(3, 'm8[M]') + 2) ** 3
    Out[11]: array([27, 27, 27], dtype="timedelta64[M]")

But:

    In [12]: numpy.ones(5, 'm8') + 1j
    TypeError: the result cannot be converted into a ``timedelta64``

#### Casting rules

When combining two `timedelta64` dtypes with different time units the outcome will be the shorter of both ("keep the precision" rule). For example:

    In [10]: numpy.ones(3, 'm8[s]') + numpy.ones(3, 'm8[m]')
    Out[10]: array([61, 61, 61],  dtype="timedelta64[s]")

However, due to the impossibility to know the exact duration of a relative year or a relative month, when these time units appear in one of the operands, the operation will not be allowed:

    In [11]: numpy.ones(3, 'm8[Y]') + numpy.ones(3, 'm8[D]')
    raise numpy.IncompatibleUnitError  # how to convert relative years to days?

In order to being able to perform the above operation a new NumPy function, called `change_timeunit` is proposed. Its signature will be:

    change_timeunit(time_object, new_unit, reference)

where 'time\_object' is the time object whose unit is to be changed, 'new\_unit' is the desired new time unit, and 'reference' is an absolute date (NumPy datetime64 scalar) that will be used to allow the conversion of relative times in case of using time units with an uncertain number of smaller time units (relative years or months cannot be expressed in days).

With this, the above operation can be done as follows:

    In [10]: t_years = numpy.ones(3, 'm8[Y]')
    
    In [11]: t_days = numpy.change_timeunit(t_years, 'D', '2001-01-01')
    
    In [12]: t_days + numpy.ones(3, 'm8[D]')
    Out[12]: array([366, 366, 366],  dtype="timedelta64[D]")

## dtype vs time units conversions

For changing the date/time dtype of an existing array, we propose to use the `.astype()` method. This will be mainly useful for changing time units.

For example, for absolute dates:

    In[10]: t1 = numpy.zeros(5, dtype="datetime64[s]")
    
    In[11]: print t1
    [1970-01-01T00:00:00  1970-01-01T00:00:00  1970-01-01T00:00:00
     1970-01-01T00:00:00  1970-01-01T00:00:00]
    
    In[12]: print t1.astype('datetime64[D]')
    [1970-01-01  1970-01-01  1970-01-01  1970-01-01  1970-01-01]

For relative times:

    In[10]: t1 = numpy.ones(5, dtype="timedelta64[s]")
    
    In[11]: print t1
    [1 1 1 1 1]
    
    In[12]: print t1.astype('timedelta64[ms]')
    [1000 1000 1000 1000 1000]

Changing directly from/to relative to/from absolute dtypes will not be supported:

    In[13]: numpy.zeros(5, dtype="datetime64[s]").astype('timedelta64')
    TypeError: data type cannot be converted to the desired type

Business days have the peculiarity that they do not cover a continuous line of time (they have gaps at weekends). Thus, when converting from any ordinary time to business days, it can happen that the original time is not representable. In that case, the result of the conversion is *Not a Time* (*NaT*):

    In[10]: t1 = numpy.arange(5, dtype="datetime64[D]")
    
    In[11]: print t1
    [1970-01-01  1970-01-02  1970-01-03  1970-01-04  1970-01-05]
    
    In[12]: t2 = t1.astype("datetime64[B]")
    
    In[13]: print t2  # 1970 begins in a Thursday
    [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

When converting back to ordinary days, NaT values are left untouched (this happens in all time unit conversions):

    In[14]: t3 = t2.astype("datetime64[D]")
    
    In[13]: print t3
    [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

## Necessary changes to NumPy

In order to facilitate the addition of the date-time data-types a few changes to NumPy were made:

### Addition of metadata to dtypes

All data-types now have a metadata dictionary. It can be set using the metadata keyword during construction of the object.

Date-time data-types will place the word "\_\_frequency\_\_" in the meta-data dictionary containing a 4-tuple with the following parameters.

  - (basic unit string (str),  
    number of multiples (int), number of sub-divisions (int), number of events (int)).

Simple time units like 'D' for days will thus be specified by ('D', 1, 1, 1) in the "\_\_frequency\_\_" key of the metadata. More complicated time units (like '\[2W/5\]//50') will be indicated by ('D', 2, 5, 50).

The "\_\_frequency\_\_" key is reserved for metadata and cannot be set with a dtype constructor.

### Ufunc interface extension

ufuncs that have datetime and timedelta arguments can use the Python API during ufunc calls (to raise errors).

There is a new ufunc C-API call to set the data for a particular function pointer (for a particular set of data-types) to be the list of arrays passed in to the ufunc.

### Array interface extensions

The array interface is extended to both handle datetime and timedelta typestr (including extended notation).

In addition, the typestr element of the \_\_array\_interface\_\_ can be a tuple as long as the version string is 4. The tuple is ('typestr', metadata dictionary).

This extension to the typestr concept extends to the descr portion of the \_\_array\_interface\_\_. Thus, the second element in the tuple of a list of tuples describing a data-format can itself be a tuple of ('typestr', metadata dictionary).

## Final considerations

### Why the fractional time and events: \[3Y/12\]//50

It is difficult to come up with enough units to satisfy every need. For example, in C\# on Windows the fundamental tick of time is 100ns. Multiple of basic units are simple to handle. Divisors of basic units are harder to handle arbitrarily, but it is common to mentally think of a month as 1/12 of a year, or a day as 1/7 of a week. Therefore, the ability to specify a unit in terms of a fraction of a "larger" unit was implemented.

The event notion (//50) was added to solve a use-case of a commercial sponsor of this NEP. The idea is to allow timestamp to carry both event number and timestamp information. The remainder carries the event number information, while the quotient carries the timestamp information.

### Why the `origin` metadata disappeared

During the discussion of the date/time dtypes in the NumPy list, the idea of having an `origin` metadata that complemented the definition of the absolute `datetime64` was initially found to be useful.

However, after thinking more about this, we found that the combination of an absolute `datetime64` with a relative `timedelta64` does offer the same functionality while removing the need for the additional `origin` metadata. This is why we have removed it from this proposal.

### Operations with mixed time units

Whenever an operation between two time values of the same dtype with the same unit is accepted, the same operation with time values of different units should be possible (e.g. adding a time delta in seconds and one in microseconds), resulting in an adequate time unit. The exact semantics of this kind of operations is defined int the "Casting rules" subsections of the "Operating with date/time arrays" section.

Due to the peculiarities of business days, it is most probable that operations mixing business days with other time units will not be allowed.

1.  <https://docs.python.org/library/datetime.html>

2.  <https://www.egenix.com/products/python/mxBase/mxDateTime>

3.  <https://en.wikipedia.org/wiki/Unix_time>

---

nep-0008-groupby_additions.md

---

# NEP 8 â€” A proposal for adding groupby functionality to NumPy

  - Author  
    Travis Oliphant

  - Contact  
    <oliphant@enthought.com>

  - Date  
    2010-04-27

  - Status  
    Deferred

## Executive summary

NumPy provides tools for handling data and doing calculations in much the same way as relational algebra allows. However, the common group-by functionality is not easily handled. The reduce methods of NumPy's ufuncs are a natural place to put this groupby behavior. This NEP describes two additional methods for ufuncs (reduceby and reducein) and two additional functions (segment and edges) which can help add this functionality.

## Example use case

Suppose you have a NumPy structured array containing information about the number of purchases at several stores over multiple days. To be clear, the structured array data-type is:

    dt = [('year', i2), ('month', i1), ('day', i1), ('time', float),
        ('store', i4), ('SKU', 'S6'), ('number', i4)]

Suppose there is a 1-d NumPy array of this data-type and you would like to compute various statistics (max, min, mean, sum, etc.) on the number of products sold, by product, by month, by store, etc.

Currently, this could be done by using reduce methods on the number field of the array, coupled with in-place sorting, unique with return\_inverse=True and bincount, etc. However, for such a common data-analysis need, it would be nice to have standard and more direct ways to get the results.

## Ufunc methods proposed

It is proposed to add two new reduce-style methods to the ufuncs: reduceby and reducein. The reducein method is intended to be a simpler to use version of reduceat, while the reduceby method is intended to provide group-by capability on reductions.

reducein:

    <ufunc>.reducein(arr, indices, axis=0, dtype=None, out=None)
    
    Perform a local reduce with slices specified by pairs of indices.
    
    The reduction occurs along the provided axis, using the provided
    data-type to calculate intermediate results, storing the result into
    the array out (if provided).
    
    The indices array provides the start and end indices for the
    reduction.  If the length of the indices array is odd, then the
    final index provides the beginning point for the final reduction
    and the ending point is the end of arr.
    
    This generalizes along the given axis, the behavior:
    
    [<ufunc>.reduce(arr[indices[2*i]:indices[2*i+1]])
            for i in range(len(indices)/2)]
    
    This assumes indices is of even length
    
    Example:
       >>> a = [0,1,2,4,5,6,9,10]
       >>> add.reducein(a,[0,3,2,5,-2])
       [3, 11, 19]
    
       Notice that sum(a[0:3]) = 3; sum(a[2:5]) = 11; and sum(a[-2:]) = 19

reduceby:

    <ufunc>.reduceby(arr, by, dtype=None, out=None)
    
    Perform a reduction in arr over unique non-negative integers in by.

    Let N=arr.ndim and M=by.ndim.  Then, by.shape[:N] == arr.shape.
    In addition, let I be an N-length index tuple, then by[I]
    contains the location in the output array for the reduction to
    be stored.  Notice that if N == M, then by[I] is a non-negative
    integer, while if N < M, then by[I] is an array of indices into
    the output array.
    
    The reduction is computed on groups specified by unique indices
    into the output array. The index is either the single
    non-negative integer if N == M or if N < M, the entire
    (M-N+1)-length index by[I] considered as a whole.

## Functions proposed

  - segment
  - edges

---

nep-0009-structured_array_extensions.md

---

# NEP 9 â€” Structured array extensions

  - Status  
    Deferred

<!-- end list -->

1.  Create with-style context that makes "named-columns" available as names in the namespace.

>   - with np.columns(array):  
>     price = unit \* quantity

2.  Allow structured arrays to be sliced by their column (i.e. one additional indexing option for structured arrays) so that a\[:4, 'foo':'bar'\] would be allowed.

---

nep-0010-new-iterator-ufunc.md

---

# NEP 10 â€” Optimizing iterator/UFunc performance

  - Author  
    Mark Wiebe \<<mwwiebe@gmail.com>\>

  - Content-Type  
    text/x-rst

  - Created  
    25-Nov-2010

  - Status  
    Final

## Table of contents

<div class="contents">

</div>

## Abstract

This NEP proposes to replace the NumPy iterator and multi-iterator with a single new iterator, designed to be more flexible and allow for more cache-friendly data access. The new iterator also subsumes much of the core ufunc functionality, making it easy to get the current ufunc benefits in contexts which don't precisely fit the ufunc mold. Key benefits include:

  - automatic reordering to find a cache-friendly access pattern
  - standard and customizable broadcasting
  - automatic type/byte-order/alignment conversions
  - optional buffering to minimize conversion memory usage
  - optional output arrays, with automatic allocation when unsupplied
  - automatic output or common type selection

A large fraction of this iterator design has already been implemented with promising results. Construction overhead is slightly greater (a.flat: 0.5 us, nditer(a): 1.4 us and broadcast(a,b): 1.4 us, nditer(\[a,b\]): 2.2 us), but, as shown in an example, it is already possible to improve on the performance of the built-in NumPy mechanisms in pure Python code together with the iterator. One example rewrites np.add, getting a four times improvement with some Fortran-contiguous arrays, and another improves image compositing code from 1.4s to 180ms.

The implementation attempts to take into account the design decisions made in the NumPy 2.0 refactor, to make its future integration into libndarray relatively simple.

## Motivation

NumPy defaults to returning C-contiguous arrays from UFuncs. This can result in extremely poor memory access patterns when dealing with data that is structured differently. A simple timing example illustrates this with a more than eight times performance hit from adding Fortran-contiguous arrays together. All timings are done using NumPy 2.0dev (Nov 22, 2010) on an Athlon 64 X2 4200+, with a 64-bit OS.:

    In [1]: import numpy as np
    In [2]: a = np.arange(1000000,dtype=np.float32).reshape(10,10,10,10,10,10)
    In [3]: b, c, d = a.copy(), a.copy(), a.copy()
    
    In [4]: timeit a+b+c+d
    10 loops, best of 3: 28.5 ms per loop
    
    In [5]: timeit a.T+b.T+c.T+d.T
    1 loops, best of 3: 237 ms per loop
    
    In [6]: timeit a.T.ravel('A')+b.T.ravel('A')+c.T.ravel('A')+d.T.ravel('A')
    10 loops, best of 3: 29.6 ms per loop

In this case, it is simple to recover the performance by switching to a view of the memory, adding, then reshaping back. To further examine the problem and see how it isnâ€™t always as trivial to work around, letâ€™s consider simple code for working with image buffers in NumPy.

### Image compositing example

For a more realistic example, consider an image buffer. Images are generally stored in a Fortran-contiguous order, and the colour channel can be treated as either a structured 'RGB' type or an extra dimension of length three. The resulting memory layout is neither C-nor Fortran-contiguous, but is easy to work with directly in NumPy, because of the flexibility of the ndarray. This appears ideal, because it makes the memory layout compatible with typical C or C++ image code, while simultaneously giving natural access in Python. Getting the color of pixel (x,y) is just â€˜image\[x,y\]â€™.

The performance of this layout in NumPy turns out to be very poor. Here is code which creates two black images, and does an â€˜overâ€™ compositing operation on them.:

    In [9]: image1 = np.zeros((1080,1920,3), dtype=np.float32).swapaxes(0,1)
    In [10]: alpha1 = np.zeros((1080,1920,1), dtype=np.float32).swapaxes(0,1)
    In [11]: image2 = np.zeros((1080,1920,3), dtype=np.float32).swapaxes(0,1)
    In [12]: alpha2 = np.zeros((1080,1920,1), dtype=np.float32).swapaxes(0,1)
    In [13]: def composite_over(im1, al1, im2, al2):
       ....:     return (im1 + (1-al1)*im2, al1 + (1-al1)*al2)
    
    In [14]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 3.51 s per loop

If we give up the convenient layout, and use the C-contiguous default, the performance is about seven times better.:

    In [16]: image1 = np.zeros((1080,1920,3), dtype=np.float32)
    In [17]: alpha1 = np.zeros((1080,1920,1), dtype=np.float32)
    In [18]: image2 = np.zeros((1080,1920,3), dtype=np.float32)
    In [19]: alpha2 = np.zeros((1080,1920,1), dtype=np.float32)
    
    In [20]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 581 ms per loop

But this is not all, since it turns out that broadcasting the alpha channel is exacting a performance price as well. If we use an alpha channel with 3 values instead of one, we get:

    In [21]: image1 = np.zeros((1080,1920,3), dtype=np.float32)
    In [22]: alpha1 = np.zeros((1080,1920,3), dtype=np.float32)
    In [23]: image2 = np.zeros((1080,1920,3), dtype=np.float32)
    In [24]: alpha2 = np.zeros((1080,1920,3), dtype=np.float32)
    
    In [25]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 313 ms per loop

For a final comparison, letâ€™s see how it performs when we use one-dimensional arrays to ensure just a single loop does the calculation.:

    In [26]: image1 = np.zeros((1080*1920*3), dtype=np.float32)
    In [27]: alpha1 = np.zeros((1080*1920*3), dtype=np.float32)
    In [28]: image2 = np.zeros((1080*1920*3), dtype=np.float32)
    In [29]: alpha2 = np.zeros((1080*1920*3), dtype=np.float32)
    
    In [30]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 312 ms per loop

To get a reference performance number, I implemented this simple operation straightforwardly in C (careful to use the same compile options as NumPy). If I emulated the memory allocation and layout of the Python code, the performance was roughly 0.3 seconds, very much in line with NumPyâ€™s performance. Combining the operations into one pass reduced the time to roughly 0.15 seconds.

A slight variation of this example is to use a single memory block with four channels (1920,1080,4) instead of separate image and alpha. This is more typical in image processing applications, and hereâ€™s how that looks with a C-contiguous layout.:

    In [31]: image1 = np.zeros((1080,1920,4), dtype=np.float32)
    In [32]: image2 = np.zeros((1080,1920,4), dtype=np.float32)
    In [33]: def composite_over(im1, im2):
       ....:     ret = (1-im1[:,:,-1])[:,:,np.newaxis]*im2
       ....:     ret += im1
       ....:     return ret
    
    In [34]: timeit composite_over(image1,image2)
    1 loops, best of 3: 481 ms per loop

To see the improvements that implementation of the new iterator as proposed can produce, go to the example continued after the proposed API, near the bottom of the document.

## Improving cache-coherency

In order to get the best performance from UFunc calls, the pattern of memory reads should be as regular as possible. Modern CPUs attempt to predict the memory read/write pattern and fill the cache ahead of time. The most predictable pattern is for all the inputs and outputs to be sequentially processed in the same order.

I propose that by default, the memory layout of the UFunc outputs be as close to that of the inputs as possible. Whenever there is an ambiguity or a mismatch, it defaults to a C-contiguous layout.

To understand how to accomplish this, we first consider the strides of all the inputs after the shapes have been normalized for broadcasting. By determining whether a set of strides are compatible and/or ambiguous, we can determine an output memory layout which maximizes coherency.

In broadcasting, the input shapes are first transformed to broadcast shapes by prepending singular dimensions, then the broadcast strides are created, where any singular dimensionâ€™s stride is set to zero.

Strides may be negative as well, and in certain cases this can be normalized to fit the following discussion. If all the strides for a particular axis are negative or zero, the strides for that dimension can be negated after adjusting the base data pointers appropriately.

Here's an example of how three inputs with C-contiguous layouts result in broadcast strides. To simplify things, the examples use an itemsize of 1.

|                    |          |         |         |
| ------------------ | -------- | ------- | ------- |
| Input shapes:      | (5,3,7)  | (5,3,1) | (1,7)   |
| Broadcast shapes:  | (5,3,7)  | (5,3,1) | (1,1,7) |
| Broadcast strides: | (21,7,1) | (3,1,0) | (0,0,1) |

*Compatible Strides* - A set of strides are compatible if there exists a permutation of the axes such that the strides are decreasing for every stride in the set, excluding entries that are zero.

The example above satisfies the definition with the identity permutation. In the motivation image example, the strides are slightly different if we separate the colour and alpha information or not. The permutation which demonstrates compatibility here is the transposition (0,1).

|                               |                       |                       |
| ----------------------------- | --------------------- | --------------------- |
| Input/Broadcast shapes:       | Image (1920, 1080, 3) | Alpha (1920, 1080, 1) |
| Broadcast strides (separate): | (3,5760,1)            | (1,1920,0)            |
| Broadcast strides (together): | (4,7680,1)            | (4,7680,0)            |

*Ambiguous Strides* - A set of compatible strides are ambiguous if more than one permutation of the axes exists such that the strides are decreasing for every stride in the set, excluding entries that are zero.

This typically occurs when every axis has a 0-stride somewhere in the set of strides. The simplest example is in two dimensions, as follows.

|                    |       |       |
| ------------------ | ----- | ----- |
| Broadcast shapes:  | (1,3) | (5,1) |
| Broadcast strides: | (0,1) | (1,0) |

There may, however, be unambiguous compatible strides without a single input forcing the entire layout, as in this example:

|                    |         |         |
| ------------------ | ------- | ------- |
| Broadcast shapes:  | (1,3,4) | (5,3,1) |
| Broadcast strides: | (0,4,1) | (3,1,0) |

In the face of ambiguity, we have a choice to either completely throw away the fact that the strides are compatible, or try to resolve the ambiguity by adding an additional constraint. I think the appropriate choice is to resolve it by picking the memory layout closest to C-contiguous, but still compatible with the input strides.

### Output layout selection algorithm

The output ndarray memory layout we would like to produce is as follows:

|                                 |                                               |
| ------------------------------- | --------------------------------------------- |
| Consistent/Unambiguous strides: | The single consistent layout                  |
| Consistent/Ambiguous strides:   | The consistent layout closest to C-contiguous |
| Inconsistent strides:           | C-contiguous                                  |

Here is pseudo-code for an algorithm to compute the permutation for the output layout.:

    perm = range(ndim) # Identity, i.e. C-contiguous
    # Insertion sort, ignoring 0-strides
    # Note that the sort must be stable, and 0-strides may
    # be reordered if necessary, but should be moved as little
    # as possible.
    for i0 = 1 to ndim-1:
        # ipos is where perm[i0] will get inserted
        ipos = i0
        j0 = perm[i0]
        for i1 = i0-1 to 0:
            j1 = perm[i1]
            ambig, shouldswap = True, False
            # Check whether any strides are ordered wrong
            for strides in broadcast_strides:
                if strides[j0] != 0 and strides[j1] != 0:
                    if strides[j0] > strides[j1]:
                        # Only set swap if it's still ambiguous.
                        if ambig:
                            shouldswap = True
                    else:
                        # Set swap even if it's not ambiguous,
                        # because not swapping is the choice
                        # for conflicts as well.
                        shouldswap = False
                    ambig = False
            # If there was an unambiguous comparison, either shift ipos
            # to i1 or stop looking for the comparison
            if not ambig:
                if shouldswap:
                    ipos = i1
                else:
                    break
        # Insert perm[i0] into the right place
        if ipos != i0:
           for i1 = i0-1 to ipos:
             perm[i1+1] = perm[i1]
           perm[ipos] = j0
    # perm is now the closest consistent ordering to C-contiguous
    return perm

## Coalescing dimensions

In many cases, the memory layout allows for the use of a one-dimensional loop instead of tracking multiple coordinates within the iterator. The existing code already exploits this when the data is C-contiguous, but since we're reordering the axes, we can apply this optimization more generally.

Once the iteration strides have been sorted to be monotonically decreasing, any dimensions which could be coalesced are side by side. If for all the operands, incrementing by strides\[i+1\] shape\[i+1\] times is the same as incrementing by strides\[i\], or strides\[i+1\]\*shape\[i+1\] == strides\[i\], dimensions i and i+1 can be coalesced into a single dimension.

Here is pseudo-code for coalescing.:

    # Figure out which pairs of dimensions can be coalesced
    can_coalesce = [False]*ndim
    for strides, shape in zip(broadcast_strides, broadcast_shape):
        for i = 0 to ndim-2:
            if strides[i+1]*shape[i+1] == strides[i]:
                can_coalesce[i] = True
    # Coalesce the types
    new_ndim = ndim - count_nonzero(can_coalesce)
    for strides, shape in zip(broadcast_strides, broadcast_shape):
        j = 0
        for i = 0 to ndim-1:
            # Note that can_coalesce[ndim-1] is always False, so
            # there is no out-of-bounds access here.
            if can_coalesce[i]:
                shape[i+1] = shape[i]*shape[i+1]
            else:
                strides[j] = strides[i]
                shape[j] = shape[i]
                j += 1

## Inner loop specialization

Specialization is handled purely by the inner loop function, so this optimization is independent of the others. Some specialization is already done, like for the reduce operation. The idea is mentioned in <http://projects.scipy.org/numpy/wiki/ProjectIdeas>, â€œuse intrinsics (SSE-instructions) to speed up low-level loops in NumPy.â€

Here are some possibilities for two-argument functions, covering the important cases of add/subtract/multiply/divide.

  - The first or second argument is a single value (i.e. a 0 stride value) and does not alias the output. arr = arr + 1; arr = 1 + arr
      - Can load the constant once instead of reloading it from memory every time
  - The strides match the size of the data type. C- or Fortran-contiguous data, for example
      - Can do a simple loop without using strides
  - The strides match the size of the data type, and they are both 16-byte aligned (or differ from 16-byte aligned by the same offset)
      - Can use SSE to process multiple values at once
  - The first input and the output are the same single value (i.e. a reduction operation).
      - This is already specialized for many UFuncs in the existing code

The above cases are not generally mutually exclusive, for example a constant argument may be combined with SSE when the strides match the data type size, and reductions can be optimized with SSE as well.

## Implementation details

Except for inner loop specialization, the discussed optimizations significantly affect ufunc\_object.c and the PyArrayIterObject/PyArrayMultiIterObject used to do the broadcasting. In general, it should be possible to emulate the current behavior where it is desired, but I believe the default should be to produce and manipulate memory layouts which will give the best performance.

To support the new cache-friendly behavior, we introduce a new option â€˜Kâ€™ (for â€œkeepâ€) for any `order=` parameter.

The proposed â€˜order=â€™ flags become as follows:

|     |                                                                                                                                         |
| --- | --------------------------------------------------------------------------------------------------------------------------------------- |
| â€˜Câ€™ | C-contiguous layout                                                                                                                     |
| â€˜Fâ€™ | Fortran-contiguous layout                                                                                                               |
| â€˜Aâ€™ | â€˜Fâ€™ if the input(s) have a Fortran-contiguous layout, â€˜Câ€™ otherwise (â€œAny Contiguousâ€)                                                  |
| â€˜Kâ€™ | a layout equivalent to â€˜Câ€™ followed by some permutation of the axes, as close to the layout of the input(s) as possible (â€œKeep Layoutâ€) |

Or as an enum:

`` `c     /* For specifying array memory layout or iteration order */     typedef enum {             /* Fortran order if inputs are all Fortran, C otherwise */             NPY_ANYORDER=-1,             /* C order */             NPY_CORDER=0,             /* Fortran order */             NPY_FORTRANORDER=1,             /* An order as close to the inputs as possible */             NPY_KEEPORDER=2     } NPY_ORDER;   Perhaps a good strategy is to first implement the capabilities discussed ``<span class="title-ref"> here without changing the defaults. Once they are implemented and well-tested, the defaults can change from </span><span class="title-ref">order='C'</span><span class="title-ref"> to </span><span class="title-ref">order='K'</span><span class="title-ref"> everywhere appropriate. UFuncs additionally should gain an </span><span class="title-ref">order=</span>\` parameter to control the layout of their output(s).

The iterator can do automatic casting, and I have created a sequence of progressively more permissive casting rules. Perhaps for 2.0, NumPy could adopt this enum as its preferred way of dealing with casting.

`` `c     /* For specifying allowed casting in operations which support it */     typedef enum {             /* Only allow identical types */             NPY_NO_CASTING=0,             /* Allow identical and byte swapped types */             NPY_EQUIV_CASTING=1,             /* Only allow safe casts */             NPY_SAFE_CASTING=2,             /* Allow safe casts and casts within the same kind */             NPY_SAME_KIND_CASTING=3,             /* Allow any casts */             NPY_UNSAFE_CASTING=4     } NPY_CASTING;  Iterator rewrite ``\` ================

Based on an analysis of the code, it appears that refactoring the existing iteration objects to implement these optimizations is prohibitively difficult. Additionally, some usage of the iterator requires modifying internal values or flags, so code using the iterator would have to change anyway. Thus we propose creating a new iterator object which subsumes the existing iterator functionality and expands it to account for the optimizations.

High level goals for the replacement iterator include:

  - Small memory usage and a low number of memory allocations.
  - Simple cases (like flat arrays) should have very little overhead.
  - Combine single and multiple iteration into one object.

Capabilities that should be provided to user code:

  - Iterate in C, Fortran, or â€œFastestâ€ (default) order.
  - Track a C-style or Fortran-style flat index if requested (existing iterator always tracks a C-style index). This can be done independently of the iteration order.
  - Track the coordinates if requested (the existing iterator requires manually changing an internal iterator flag to guarantee this).
  - Skip iteration of the last internal dimension so that it can be processed with an inner loop.
  - Jump to a specific coordinate in the array.
  - Iterate an arbitrary subset of axes (to support, for example, reduce with multiple axes at once).
  - Ability to automatically allocate output parameters if a NULL input is provided, These outputs should have a memory layout matching the iteration order, and are the mechanism for the `order='K'` support.
  - Automatic copying and/or buffering of inputs which do not satisfy type/byte-order/alignment requirements. The caller's iteration inner loop should be the same no matter what buffering or copying is done.

Notes for implementation:

  - User code must never touch the inside of the iterator. This allows for drastic changes of the internal memory layout in the future, if higher-performance implementation strategies are found.
  - Use a function pointer instead of a macro for iteration. This way, specializations can be created for the common cases, like when ndim is small, for different flag settings, and when the number of arrays iterated is small. Also, an iteration pattern can be prescribed that makes a copy of the function pointer first to allow the compiler to keep the function pointer in a register.
  - Dynamically create the memory layout, to minimize the number of cache lines taken up by the iterator (for LP64, sizeof(PyArrayIterObject) is about 2.5KB, and a binary operation like plus needs three of these for the Multi-Iterator).
  - Isolate the C-API object from Python reference counting, so that it can be used naturally from C. The Python object then becomes a wrapper around the C iterator. This is analogous to the PEP 3118 design separation of Py\_buffer and memoryview.

### Proposed iterator memory layout

The following struct describes the iterator memory. All items are packed together, which means that different values of the flags, ndim, and niter will produce slightly different layouts.

`` `c     struct {         /* Flags indicate what optimizations have been applied, and          * affect the layout of this struct. */         uint32 itflags;         /* Number of iteration dimensions.  If FLAGS_HASCOORDS is set,          * it matches the creation ndim, otherwise it may be smaller.  */         uint16 ndim;         /* Number of objects being iterated.  This is fixed at creation time. */         uint16 niter;          /* The number of times the iterator will iterate */         intp itersize;          /* The permutation is only used when FLAGS_HASCOORDS is set,          * and is placed here so its position depends on neither ndim          * nor niter. */         intp perm[ndim];          /* The data types of all the operands */         PyArray_Descr *dtypes[niter];         /* Backups of the starting axisdata 'ptr' values, to support Reset */         char *resetdataptr[niter];         /* Backup of the starting index value, to support Reset */         npy_intp resetindex;          /* When the iterator is destroyed, Py_XDECREF is called on all            these objects */         PyObject *objects[niter];          /* Flags indicating read/write status and buffering          * for each operand. */         uint8 opitflags[niter];         /* Padding to make things intp-aligned again */         uint8 padding[];          /* If some or all of the inputs are being buffered */         #if (flags&FLAGS_BUFFERED)         struct buffer_data {             /* The size of the buffer, and which buffer we're on.              * the i-th iteration has i = buffersize*bufferindex+pos              */             intp buffersize;             /* For tracking position inside the buffer */             intp size, pos;             /* The strides for the pointers */             intp stride[niter];             /* Pointers to the data for the current iterator position.              * The buffer_data.value ptr[i] equals either              * axis_data[0].ptr[i] or buffer_data.buffers[i] depending              * on whether copying to the buffer was necessary.              */             char* ptr[niter];             /* Functions to do the copyswap and casting necessary */             transferfn_t readtransferfn[niter];             void *readtransferdata[niter];             transferfn_t writetransferfn[niter];             void *writetransferdata[niter];             /* Pointers to the allocated buffers for operands              * which the iterator determined needed buffering              */             char *buffers[niter];         };         #endif /* FLAGS_BUFFERED */          /* Data per axis, starting with the most-frequently          * updated, and in decreasing order after that. */         struct axis_data {             /* The shape of this axis */             intp shape;             /* The current coordinate along this axis */             intp coord;             /* The operand and index strides for this axis */             intp stride[niter];             #if (flags&FLAGS_HASINDEX)                 intp indexstride;             #endif             /* The operand pointers and index values for this axis */             char* ptr[niter];             #if (flags&FLAGS_HASINDEX)                 intp index;             #endif         }[ndim];     };  The array of axis_data structs is ordered to be in increasing rapidity ``<span class="title-ref"> of increment updates. If the </span><span class="title-ref">perm</span>\` is the identity, this means itâ€™s reversed from the C-order. This is done so data items touched most often are closest to the beginning of the struct, where the common properties are, resulting in increased cache coherency. It also simplifies the iternext call, while making getcoord and related functions slightly more complicated.

### Proposed iterator API

The existing iterator API includes functions like PyArrayIter\_Check, PyArray\_Iter\* and [PyArray\_ITER]()*. The multi-iterator array includes PyArray\_MultiIter*, PyArray\_Broadcast, and PyArray\_RemoveSmallest. The new iterator design replaces all of this functionality with a single object and associated API. One goal of the new API is that all uses of the existing iterator should be replaceable with the new iterator without significant effort.

The C-API naming convention chosen is based on the one in the numpy-refactor branch, where libndarray has the array named `NpyArray` and functions named `NpyArray_*`. The iterator is named `NpyIter` and functions are named `NpyIter_*`.

The Python exposure has the iterator named `np.nditer`. One possible release strategy for this iterator would be to release a 1.X (1.6?) version with the iterator added, but not used by the NumPy code. Then, 2.0 can be release with it fully integrated. If this strategy is chosen, the naming convention and API should be finalized as much as possible before the 1.X release. The name `np.iter` can't be used because it conflicts with the Python built-in `iter`. I would suggest the name `np.nditer` within Python, as it is currently unused.

In addition to the performance goals set out for the new iterator, it appears the API can be refactored to better support some common NumPy programming idioms.

By moving some functionality currently in the UFunc code into the iterator, it should make it easier for extension code which wants to emulate UFunc behavior in cases which don't quite fit the UFunc paradigm. In particular, emulating the UFunc buffering behavior is not a trivial enterprise.

#### Old -\> new iterator API conversion

For the regular iterator:

|                            |                                                                                     |
| -------------------------- | ----------------------------------------------------------------------------------- |
| `PyArray_IterNew`          | `NpyIter_New`                                                                       |
| `PyArray_IterAllButAxis`   | `NpyIter_New` + `axes` parameter **or** Iterator flag `NPY_ITER_NO_INNER_ITERATION` |
| `PyArray_BroadcastToShape` | **NOT SUPPORTED** (but could be, if needed)                                         |
| `PyArrayIter_Check`        | Will need to add this in Python exposure                                            |
| `PyArray_ITER_RESET`       | `NpyIter_Reset`                                                                     |
| `PyArray_ITER_NEXT`        | Function pointer from `NpyIter_GetIterNext`                                         |
| `PyArray_ITER_DATA`        | `NpyIter_GetDataPtrArray`                                                           |
| `PyArray_ITER_GOTO`        | `NpyIter_GotoCoords`                                                                |
| `PyArray_ITER_GOTO1D`      | `NpyIter_GotoIndex`                                                                 |
| `PyArray_ITER_NOTDONE`     | Return value of `iternext` function pointer                                         |

For the multi-iterator:

|                             |                                                |
| --------------------------- | ---------------------------------------------- |
| `PyArray_MultiIterNew`      | `NpyIter_MultiNew`                             |
| `PyArray_MultiIter_RESET`   | `NpyIter_Reset`                                |
| `PyArray_MultiIter_NEXT`    | Function pointer from `NpyIter_GetIterNext`    |
| `PyArray_MultiIter_DATA`    | `NpyIter_GetDataPtrArray`                      |
| `PyArray_MultiIter_NEXTi`   | **NOT SUPPORTED** (always lock-step iteration) |
| `PyArray_MultiIter_GOTO`    | `NpyIter_GotoCoords`                           |
| `PyArray_MultiIter_GOTO1D`  | `NpyIter_GotoIndex`                            |
| `PyArray_MultiIter_NOTDONE` | Return value of `iternext` function pointer    |
| `PyArray_Broadcast`         | Handled by `NpyIter_MultiNew`                  |
| `PyArray_RemoveSmallest`    | Iterator flag `NPY_ITER_NO_INNER_ITERATION`    |

For other API calls:

|                               |                                       |
| ----------------------------- | ------------------------------------- |
| `PyArray_ConvertToCommonType` | Iterator flag `NPY_ITER_COMMON_DTYPE` |

#### Iterator pointer type

The iterator structure is internally generated, but a type is still needed to provide warnings and/or errors when the wrong type is passed to the API. We do this with a typedef of an incomplete struct

`typedef struct NpyIter_InternalOnly NpyIter;`

#### Construction and destruction

`NpyIter* NpyIter_New(PyArrayObject* op, npy_uint32 flags, NPY_ORDER order, NPY_CASTING casting, PyArray_Descr* dtype, npy_intp a_ndim, npy_intp *axes, npy_intp buffersize)`

> Creates an iterator for the given numpy array object `op`.
> 
> Flags that may be passed in `flags` are any combination of the global and per-operand flags documented in `NpyIter_MultiNew`, except for `NPY_ITER_ALLOCATE`.
> 
> Any of the `NPY_ORDER` enum values may be passed to `order`. For efficient iteration, `NPY_KEEPORDER` is the best option, and the other orders enforce the particular iteration pattern.
> 
> Any of the `NPY_CASTING` enum values may be passed to `casting`. The values include `NPY_NO_CASTING`, `NPY_EQUIV_CASTING`, `NPY_SAFE_CASTING`, `NPY_SAME_KIND_CASTING`, and `NPY_UNSAFE_CASTING`. To allow the casts to occur, copying or buffering must also be enabled.
> 
> If `dtype` isn't `NULL`, then it requires that data type. If copying is allowed, it will make a temporary copy if the data is castable. If `UPDATEIFCOPY` is enabled, it will also copy the data back with another cast upon iterator destruction.
> 
> If `a_ndim` is greater than zero, `axes` must also be provided. In this case, `axes` is an `a_ndim`-sized array of `op`'s axes. A value of -1 in `axes` means `newaxis`. Within the `axes` array, axes may not be repeated.
> 
> If `buffersize` is zero, a default buffer size is used, otherwise it specifies how big of a buffer to use. Buffers which are powers of 2 such as 512 or 1024 are recommended.
> 
> Returns NULL if there is an error, otherwise returns the allocated iterator.
> 
> To make an iterator similar to the old iterator, this should work.
> 
> `` `c     iter = NpyIter_New(op, NPY_ITER_READWRITE,                         NPY_CORDER, NPY_NO_CASTING, NULL, 0, NULL);  If you want to edit an array with aligned ``double\`\` code, but the order doesn't matter, you would use this.
> 
> ``` c
> dtype = PyArray_DescrFromType(NPY_DOUBLE);
> iter = NpyIter_New(op, NPY_ITER_READWRITE |
>                     NPY_ITER_BUFFERED |
>                     NPY_ITER_NBO,
>                     NPY_ITER_ALIGNED,
>                     NPY_KEEPORDER,
>                     NPY_SAME_KIND_CASTING,
>                     dtype, 0, NULL);
> Py_DECREF(dtype);
> ```

`NpyIter* NpyIter_MultiNew(npy_intp niter, PyArrayObject** op, npy_uint32 flags, NPY_ORDER order, NPY_CASTING casting, npy_uint32 *op_flags, PyArray_Descr** op_dtypes, npy_intp oa_ndim, npy_intp **op_axes, npy_intp buffersize)`

> Creates an iterator for broadcasting the `niter` array objects provided in `op`.
> 
> For normal usage, use 0 for `oa_ndim` and NULL for `op_axes`. See below for a description of these parameters, which allow for custom manual broadcasting as well as reordering and leaving out axes.
> 
> Any of the `NPY_ORDER` enum values may be passed to `order`. For efficient iteration, `NPY_KEEPORDER` is the best option, and the other orders enforce the particular iteration pattern. When using `NPY_KEEPORDER`, if you also want to ensure that the iteration is not reversed along an axis, you should pass the flag `NPY_ITER_DONT_NEGATE_STRIDES`.
> 
> Any of the `NPY_CASTING` enum values may be passed to `casting`. The values include `NPY_NO_CASTING`, `NPY_EQUIV_CASTING`, `NPY_SAFE_CASTING`, `NPY_SAME_KIND_CASTING`, and `NPY_UNSAFE_CASTING`. To allow the casts to occur, copying or buffering must also be enabled.
> 
> If `op_dtypes` isn't `NULL`, it specifies a data type or `NULL` for each `op[i]`.
> 
> The parameter `oa_ndim`, when non-zero, specifies the number of dimensions that will be iterated with customized broadcasting. If it is provided, `op_axes` must also be provided. These two parameters let you control in detail how the axes of the operand arrays get matched together and iterated. In `op_axes`, you must provide an array of `niter` pointers to `oa_ndim`-sized arrays of type `npy_intp`. If an entry in `op_axes` is NULL, normal broadcasting rules will apply. In `op_axes[j][i]` is stored either a valid axis of `op[j]`, or -1 which means `newaxis`. Within each `op_axes[j]` array, axes may not be repeated. The following example is how normal broadcasting applies to a 3-D array, a 2-D array, a 1-D array and a scalar.
> 
> ``` c
> npy_intp oa_ndim = 3;               /* # iteration axes */
> npy_intp op0_axes[] = {0, 1, 2};    /* 3-D operand */
> npy_intp op1_axes[] = {-1, 0, 1};   /* 2-D operand */
> npy_intp op2_axes[] = {-1, -1, 0};  /* 1-D operand */
> npy_intp op3_axes[] = {-1, -1, -1}  /* 0-D (scalar) operand */
> npy_intp *op_axes[] = {op0_axes, op1_axes, op2_axes, op3_axes};
> ```
> 
> If `buffersize` is zero, a default buffer size is used, otherwise it specifies how big of a buffer to use. Buffers which are powers of 2 such as 512 or 1024 are recommended.
> 
> Returns NULL if there is an error, otherwise returns the allocated iterator.
> 
> Flags that may be passed in `flags`, applying to the whole iterator, are:
> 
> > `NPY_ITER_C_INDEX`, `NPY_ITER_F_INDEX`
> > 
> > > Causes the iterator to track an index matching C or Fortran order. These options are mutually exclusive.
> > 
> > `NPY_ITER_COORDS`
> > 
> > > Causes the iterator to track array coordinates. This prevents the iterator from coalescing axes to produce bigger inner loops.
> > 
> > `NPY_ITER_NO_INNER_ITERATION`
> > 
> > > Causes the iterator to skip iteration of the innermost loop, allowing the user of the iterator to handle it.
> > > 
> > > This flag is incompatible with `NPY_ITER_C_INDEX`, `NPY_ITER_F_INDEX`, and `NPY_ITER_COORDS`.
> > 
> > `NPY_ITER_DONT_NEGATE_STRIDES`
> > 
> > > This only affects the iterator when NPY\_KEEPORDER is specified for the order parameter. By default with NPY\_KEEPORDER, the iterator reverses axes which have negative strides, so that memory is traversed in a forward direction. This disables this step. Use this flag if you want to use the underlying memory-ordering of the axes, but don't want an axis reversed. This is the behavior of `numpy.ravel(a, order='K')`, for instance.
> > 
> > `NPY_ITER_COMMON_DTYPE`
> > 
> > > Causes the iterator to convert all the operands to a common data type, calculated based on the ufunc type promotion rules. The flags for each operand must be set so that the appropriate casting is permitted, and copying or buffering must be enabled.
> > > 
> > > If the common data type is known ahead of time, don't use this flag. Instead, set the requested dtype for all the operands.
> > 
> > `NPY_ITER_REFS_OK`
> > 
> > > Indicates that arrays with reference types (object arrays or structured arrays containing an object type) may be accepted and used in the iterator. If this flag is enabled, the caller must be sure to check whether `NpyIter_IterationNeedsAPI(iter)` is true, in which case it may not release the GIL during iteration.
> > 
> > `NPY_ITER_ZEROSIZE_OK`
> > 
> > > Indicates that arrays with a size of zero should be permitted. Since the typical iteration loop does not naturally work with zero-sized arrays, you must check that the IterSize is non-zero before entering the iteration loop.
> > 
> > `NPY_ITER_REDUCE_OK`
> > 
> > > Permits writeable operands with a dimension with zero stride and size greater than one. Note that such operands must be read/write.
> > > 
> > > When buffering is enabled, this also switches to a special buffering mode which reduces the loop length as necessary to not trample on values being reduced.
> > > 
> > > Note that if you want to do a reduction on an automatically allocated output, you must use `NpyIter_GetOperandArray` to get its reference, then set every value to the reduction unit before doing the iteration loop. In the case of a buffered reduction, this means you must also specify the flag `NPY_ITER_DELAY_BUFALLOC`, then reset the iterator after initializing the allocated operand to prepare the buffers.
> > 
> > `NPY_ITER_RANGED`
> > 
> > > Enables support for iteration of sub-ranges of the full `iterindex` range `[0, NpyIter_IterSize(iter))`. Use the function `NpyIter_ResetToIterIndexRange` to specify a range for iteration.
> > > 
> > > This flag can only be used with `NPY_ITER_NO_INNER_ITERATION` when `NPY_ITER_BUFFERED` is enabled. This is because without buffering, the inner loop is always the size of the innermost iteration dimension, and allowing it to get cut up would require special handling, effectively making it more like the buffered version.
> > 
> > `NPY_ITER_BUFFERED`
> > 
> > > Causes the iterator to store buffering data, and use buffering to satisfy data type, alignment, and byte-order requirements. To buffer an operand, do not specify the `NPY_ITER_COPY` or `NPY_ITER_UPDATEIFCOPY` flags, because they will override buffering. Buffering is especially useful for Python code using the iterator, allowing for larger chunks of data at once to amortize the Python interpreter overhead.
> > > 
> > > If used with `NPY_ITER_NO_INNER_ITERATION`, the inner loop for the caller may get larger chunks than would be possible without buffering, because of how the strides are laid out.
> > > 
> > > Note that if an operand is given the flag `NPY_ITER_COPY` or `NPY_ITER_UPDATEIFCOPY`, a copy will be made in preference to buffering. Buffering will still occur when the array was broadcast so elements need to be duplicated to get a constant stride.
> > > 
> > > In normal buffering, the size of each inner loop is equal to the buffer size, or possibly larger if `NPY_ITER_GROWINNER` is specified. If `NPY_ITER_REDUCE_OK` is enabled and a reduction occurs, the inner loops may become smaller depending on the structure of the reduction.
> > 
> > `NPY_ITER_GROWINNER`
> > 
> > > When buffering is enabled, this allows the size of the inner loop to grow when buffering isn't necessary. This option is best used if you're doing a straight pass through all the data, rather than anything with small cache-friendly arrays of temporary values for each inner loop.
> > 
> > `NPY_ITER_DELAY_BUFALLOC`
> > 
> > > When buffering is enabled, this delays allocation of the buffers until one of the `NpyIter_Reset*` functions is called. This flag exists to avoid wasteful copying of buffer data when making multiple copies of a buffered iterator for multi-threaded iteration.
> > > 
> > > Another use of this flag is for setting up reduction operations. After the iterator is created, and a reduction output is allocated automatically by the iterator (be sure to use READWRITE access), its value may be initialized to the reduction unit. Use `NpyIter_GetOperandArray` to get the object. Then, call `NpyIter_Reset` to allocate and fill the buffers with their initial values.
> 
> Flags that may be passed in `op_flags[i]`, where `0 <= i < niter`:
> 
> > `NPY_ITER_READWRITE`, `NPY_ITER_READONLY`, `NPY_ITER_WRITEONLY`
> > 
> > > Indicate how the user of the iterator will read or write to `op[i]`. Exactly one of these flags must be specified per operand.
> > 
> > `NPY_ITER_COPY`
> > 
> > > Allow a copy of `op[i]` to be made if it does not meet the data type or alignment requirements as specified by the constructor flags and parameters.
> > 
> > `NPY_ITER_UPDATEIFCOPY`
> > 
> > > Triggers `NPY_ITER_COPY`, and when an array operand is flagged for writing and is copied, causes the data in a copy to be copied back to `op[i]` when the iterator is destroyed.
> > > 
> > > If the operand is flagged as write-only and a copy is needed, an uninitialized temporary array will be created and then copied to back to `op[i]` on destruction, instead of doing the unnecessary copy operation.
> > 
> > `NPY_ITER_NBO`, `NPY_ITER_ALIGNED`, `NPY_ITER_CONTIG`
> > 
> > > Causes the iterator to provide data for `op[i]` that is in native byte order, aligned according to the dtype requirements, contiguous, or any combination.
> > > 
> > > By default, the iterator produces pointers into the arrays provided, which may be aligned or unaligned, and with any byte order. If copying or buffering is not enabled and the operand data doesn't satisfy the constraints, an error will be raised.
> > > 
> > > The contiguous constraint applies only to the inner loop, successive inner loops may have arbitrary pointer changes.
> > > 
> > > If the requested data type is in non-native byte order, the NBO flag overrides it and the requested data type is converted to be in native byte order.
> > 
> > `NPY_ITER_ALLOCATE`
> > 
> > > This is for output arrays, and requires that the flag `NPY_ITER_WRITEONLY` be set. If `op[i]` is NULL, creates a new array with the final broadcast dimensions, and a layout matching the iteration order of the iterator.
> > > 
> > > When `op[i]` is NULL, the requested data type `op_dtypes[i]` may be NULL as well, in which case it is automatically generated from the dtypes of the arrays which are flagged as readable. The rules for generating the dtype are the same is for UFuncs. Of special note is handling of byte order in the selected dtype. If there is exactly one input, the input's dtype is used as is. Otherwise, if more than one input dtypes are combined together, the output will be in native byte order.
> > > 
> > > After being allocated with this flag, the caller may retrieve the new array by calling `NpyIter_GetOperandArray` and getting the i-th object in the returned C array. The caller must call Py\_INCREF on it to claim a reference to the array.
> > 
> > `NPY_ITER_NO_SUBTYPE`
> > 
> > > For use with `NPY_ITER_ALLOCATE`, this flag disables allocating an array subtype for the output, forcing it to be a straight ndarray.
> > > 
> > > TODO: Maybe it would be better to introduce a function `NpyIter_GetWrappedOutput` and remove this flag?
> > 
> > `NPY_ITER_NO_BROADCAST`
> > 
> > > Ensures that the input or output matches the iteration dimensions exactly.
> > 
> > `NPY_ITER_WRITEABLE_REFERENCES`
> > 
> > > By default, the iterator fails on creation if the iterator has a writeable operand where the data type involves Python references. Adding this flag indicates that the code using the iterator is aware of this possibility and handles it correctly.

`NpyIter *NpyIter_Copy(NpyIter *iter)`

> Makes a copy of the given iterator. This function is provided primarily to enable multi-threaded iteration of the data.
> 
> *TODO*: Move this to a section about multithreaded iteration.
> 
> The recommended approach to multithreaded iteration is to first create an iterator with the flags `NPY_ITER_NO_INNER_ITERATION`, `NPY_ITER_RANGED`, `NPY_ITER_BUFFERED`, `NPY_ITER_DELAY_BUFALLOC`, and possibly `NPY_ITER_GROWINNER`. Create a copy of this iterator for each thread (minus one for the first iterator). Then, take the iteration index range `[0, NpyIter_GetIterSize(iter))` and split it up into tasks, for example using a TBB parallel\_for loop. When a thread gets a task to execute, it then uses its copy of the iterator by calling `NpyIter_ResetToIterIndexRange` and iterating over the full range.
> 
> When using the iterator in multi-threaded code or in code not holding the Python GIL, care must be taken to only call functions which are safe in that context. `NpyIter_Copy` cannot be safely called without the Python GIL, because it increments Python references. The `Reset*` and some other functions may be safely called by passing in the `errmsg` parameter as non-NULL, so that the functions will pass back errors through it instead of setting a Python exception.

`int NpyIter_UpdateIter(NpyIter *iter, npy_intp i, npy_uint32 op_flags, NPY_CASTING casting, PyArray_Descr *dtype)` **UNIMPLEMENTED**

> Updates the i-th operand within the iterator to possibly have a new data type or more restrictive flag attributes. A use-case for this is to allow the automatic allocation to determine an output data type based on the standard NumPy type promotion rules, then use this function to convert the inputs and possibly the automatic output to a different data type during processing.
> 
> This operation can only be done if `NPY_ITER_COORDS` was passed as a flag to the iterator. If coordinates are not needed, call the function `NpyIter_RemoveCoords()` once no more calls to `NpyIter_UpdateIter` are needed.
> 
> If the i-th operand has already been copied, an error is thrown. To avoid this, leave all the flags out except the read/write indicators for any operand that later has `NpyIter_UpdateIter` called on it.
> 
> The flags that may be passed in `op_flags` are `NPY_ITER_COPY`, `NPY_ITER_UPDATEIFCOPY`, `NPY_ITER_NBO`, `NPY_ITER_ALIGNED`, `NPY_ITER_CONTIG`.

`int NpyIter_RemoveAxis(NpyIter *iter, npy_intp axis)`

> Removes an axis from iteration. This requires that `NPY_ITER_COORDS` was set for iterator creation, and does not work if buffering is enabled or an index is being tracked. This function also resets the iterator to its initial state.
> 
> This is useful for setting up an accumulation loop, for example. The iterator can first be created with all the dimensions, including the accumulation axis, so that the output gets created correctly. Then, the accumulation axis can be removed, and the calculation done in a nested fashion.
> 
> **WARNING**: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again\!
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`int NpyIter_RemoveCoords(NpyIter *iter)`

> If the iterator has coordinates, this strips support for them, and does further iterator optimizations that are possible if coordinates are not needed. This function also resets the iterator to its initial state.
> 
> **WARNING**: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again\!
> 
> After calling this function, `NpyIter_HasCoords(iter)` will return false.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`int NpyIter_RemoveInnerLoop(NpyIter *iter)`

> If UpdateIter/RemoveCoords was used, you may want to specify the flag `NPY_ITER_NO_INNER_ITERATION`. This flag is not permitted together with `NPY_ITER_COORDS`, so this function is provided to enable the feature after `NpyIter_RemoveCoords` is called. This function also resets the iterator to its initial state.
> 
> **WARNING**: This function changes the internal logic of the iterator. Any cached functions or pointers from the iterator must be retrieved again\!
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`int NpyIter_Deallocate(NpyIter *iter)`

> Deallocates the iterator object. This additionally frees any copies made, triggering UPDATEIFCOPY behavior where necessary.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`int NpyIter_Reset(NpyIter *iter, char **errmsg)`

> Resets the iterator back to its initial state, at the beginning of the iteration range.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`. If errmsg is non-NULL, no Python exception is set when `NPY_FAIL` is returned. Instead, \*errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.

`int NpyIter_ResetToIterIndexRange(NpyIter *iter, npy_intp istart, npy_intp iend, char **errmsg)`

> Resets the iterator and restricts it to the `iterindex` range `[istart, iend)`. See `NpyIter_Copy` for an explanation of how to use this for multi-threaded iteration. This requires that the flag `NPY_ITER_RANGED` was passed to the iterator constructor.
> 
> If you want to reset both the `iterindex` range and the base pointers at the same time, you can do the following to avoid extra buffer copying (be sure to add the return code error checks when you copy this code).
> 
> ``` c
> /* Set to a trivial empty range */
> NpyIter_ResetToIterIndexRange(iter, 0, 0);
> /* Set the base pointers */
> NpyIter_ResetBasePointers(iter, baseptrs);
> /* Set to the desired range */
> NpyIter_ResetToIterIndexRange(iter, istart, iend);
> ```
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`. If errmsg is non-NULL, no Python exception is set when `NPY_FAIL` is returned. Instead, \*errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.

`int NpyIter_ResetBasePointers(NpyIter *iter, char **baseptrs, char **errmsg)`

> Resets the iterator back to its initial state, but using the values in `baseptrs` for the data instead of the pointers from the arrays being iterated. This functions is intended to be used, together with the `op_axes` parameter, by nested iteration code with two or more iterators.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`. If errmsg is non-NULL, no Python exception is set when `NPY_FAIL` is returned. Instead, \*errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.
> 
> *TODO*: Move the following into a special section on nested iterators.
> 
> Creating iterators for nested iteration requires some care. All the iterator operands must match exactly, or the calls to `NpyIter_ResetBasePointers` will be invalid. This means that automatic copies and output allocation should not be used haphazardly. It is possible to still use the automatic data conversion and casting features of the iterator by creating one of the iterators with all the conversion parameters enabled, then grabbing the allocated operands with the `NpyIter_GetOperandArray` function and passing them into the constructors for the rest of the iterators.
> 
> **WARNING**: When creating iterators for nested iteration, the code must not use a dimension more than once in the different iterators. If this is done, nested iteration will produce out-of-bounds pointers during iteration.
> 
> **WARNING**: When creating iterators for nested iteration, buffering can only be applied to the innermost iterator. If a buffered iterator is used as the source for `baseptrs`, it will point into a small buffer instead of the array and the inner iteration will be invalid.
> 
> The pattern for using nested iterators is as follows:
> 
> ``` c
> NpyIter *iter1, *iter1;
> NpyIter_IterNext_Fn iternext1, iternext2;
> char **dataptrs1;
> 
> /*
>  * With the exact same operands, no copies allowed, and
>  * no axis in op_axes used both in iter1 and iter2.
>  * Buffering may be enabled for iter2, but not for iter1.
>  */
> iter1 = ...; iter2 = ...;
> 
> iternext1 = NpyIter_GetIterNext(iter1);
> iternext2 = NpyIter_GetIterNext(iter2);
> dataptrs1 = NpyIter_GetDataPtrArray(iter1);
> 
> do {
>     NpyIter_ResetBasePointers(iter2, dataptrs1);
>     do {
>         /* Use the iter2 values */
>     } while (iternext2(iter2));
> } while (iternext1(iter1));
> ```

`int NpyIter_GotoCoords(NpyIter *iter, npy_intp *coords)`

> Adjusts the iterator to point to the `ndim` coordinates pointed to by `coords`. Returns an error if coordinates are not being tracked, the coordinates are out of bounds, or inner loop iteration is disabled.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`int NpyIter_GotoIndex(NpyIter *iter, npy_intp index)`

> Adjusts the iterator to point to the `index` specified. If the iterator was constructed with the flag `NPY_ITER_C_INDEX`, `index` is the C-order index, and if the iterator was constructed with the flag `NPY_ITER_F_INDEX`, `index` is the Fortran-order index. Returns an error if there is no index being tracked, the index is out of bounds, or inner loop iteration is disabled.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`npy_intp NpyIter_GetIterSize(NpyIter *iter)`

> Returns the number of elements being iterated. This is the product of all the dimensions in the shape.

`npy_intp NpyIter_GetReduceBlockSizeFactor(NpyIter *iter)` **UNIMPLEMENTED**

> This provides a factor that must divide into the blocksize used for ranged iteration to safely multithread a reduction. If the iterator has no reduction, it returns 1.
> 
> When using ranged iteration to multithread a reduction, there are two possible ways to do the reduction:
> 
> If there is a big reduction to a small output, make a temporary array initialized to the reduction unit for each thread, then have each thread reduce into its temporary. When that is complete, combine the temporaries together. You can detect this case by observing that `NpyIter_GetReduceBlockSizeFactor` returns a large value, for instance half or a third of `NpyIter_GetIterSize`. You should also check that the output is small just to be sure.
> 
> If there are many small reductions to a big output, and the reduction dimensions are inner dimensions, `NpyIter_GetReduceBlockSizeFactor` will return a small number, and as long as the block size you choose for multithreading is `NpyIter_GetReduceBlockSizeFactor(iter)*n` for some `n`, the operation will be safe.
> 
> The bad case is when the a reduction dimension is the outermost loop in the iterator. For example, if you have a C-order array with shape (3,1000,1000), and you reduce on dimension 0, `NpyIter_GetReduceBlockSizeFactor` will return a size equal to `NpyIter_GetIterSize` for `NPY_KEEPORDER` or `NPY_CORDER` iteration orders. While it is bad for the CPU cache, perhaps in the future another order possibility could be provided, maybe `NPY_REDUCEORDER`, which pushes the reduction axes to the inner loop, but otherwise is the same as `NPY_KEEPORDER`.

`npy_intp NpyIter_GetIterIndex(NpyIter *iter)`

> Gets the `iterindex` of the iterator, which is an index matching the iteration order of the iterator.

`void NpyIter_GetIterIndexRange(NpyIter *iter, npy_intp *istart, npy_intp *iend)`

> Gets the `iterindex` sub-range that is being iterated. If `NPY_ITER_RANGED` was not specified, this always returns the range `[0, NpyIter_IterSize(iter))`.

`int NpyIter_GotoIterIndex(NpyIter *iter, npy_intp iterindex)`

> Adjusts the iterator to point to the `iterindex` specified. The IterIndex is an index matching the iteration order of the iterator. Returns an error if the `iterindex` is out of bounds, buffering is enabled, or inner loop iteration is disabled.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`int NpyIter_HasInnerLoop(NpyIter *iter)`

> Returns 1 if the iterator handles the inner loop, or 0 if the caller needs to handle it. This is controlled by the constructor flag `NPY_ITER_NO_INNER_ITERATION`.

`int NpyIter_HasCoords(NpyIter *iter)`

> Returns 1 if the iterator was created with the `NPY_ITER_COORDS` flag, 0 otherwise.

`int NpyIter_HasIndex(NpyIter *iter)`

> Returns 1 if the iterator was created with the `NPY_ITER_C_INDEX` or `NPY_ITER_F_INDEX` flag, 0 otherwise.

`int NpyIter_IsBuffered(NpyIter *iter)`

> Returns 1 if the iterator was created with the `NPY_ITER_BUFFERED` flag, 0 otherwise.

`int NpyIter_IsGrowInner(NpyIter *iter)`

> Returns 1 if the iterator was created with the `NPY_ITER_GROWINNER` flag, 0 otherwise.

`npy_intp NpyIter_GetBufferSize(NpyIter *iter)`

> If the iterator is buffered, returns the size of the buffer being used, otherwise returns 0.

`npy_intp NpyIter_GetNDim(NpyIter *iter)`

> Returns the number of dimensions being iterated. If coordinates were not requested in the iterator constructor, this value may be smaller than the number of dimensions in the original objects.

`npy_intp NpyIter_GetNIter(NpyIter *iter)`

> Returns the number of objects being iterated.

`npy_intp *NpyIter_GetAxisStrideArray(NpyIter *iter, npy_intp axis)`

> Gets the array of strides for the specified axis. Requires that the iterator be tracking coordinates, and that buffering not be enabled.
> 
> This may be used when you want to match up operand axes in some fashion, then remove them with `NpyIter_RemoveAxis` to handle their processing manually. By calling this function before removing the axes, you can get the strides for the manual processing.
> 
> Returns `NULL` on error.

`int NpyIter_GetShape(NpyIter *iter, npy_intp *outshape)`

> Returns the broadcast shape of the iterator in `outshape`. This can only be called on an iterator which supports coordinates.
> 
> Returns `NPY_SUCCEED` or `NPY_FAIL`.

`PyArray_Descr **NpyIter_GetDescrArray(NpyIter *iter)`

> This gives back a pointer to the `niter` data type Descrs for the objects being iterated. The result points into `iter`, so the caller does not gain any references to the Descrs.
> 
> This pointer may be cached before the iteration loop, calling `iternext` will not change it.

`PyObject **NpyIter_GetOperandArray(NpyIter *iter)`

> This gives back a pointer to the `niter` operand PyObjects that are being iterated. The result points into `iter`, so the caller does not gain any references to the PyObjects.

`PyObject *NpyIter_GetIterView(NpyIter *iter, npy_intp i)`

> This gives back a reference to a new ndarray view, which is a view into the i-th object in the array `NpyIter_GetOperandArray()`, whose dimensions and strides match the internal optimized iteration pattern. A C-order iteration of this view is equivalent to the iterator's iteration order.
> 
> For example, if an iterator was created with a single array as its input, and it was possible to rearrange all its axes and then collapse it into a single strided iteration, this would return a view that is a one-dimensional array.

`void NpyIter_GetReadFlags(NpyIter *iter, char *outreadflags)`

> Fills `niter` flags. Sets `outreadflags[i]` to 1 if `op[i]` can be read from, and to 0 if not.

`void NpyIter_GetWriteFlags(NpyIter *iter, char *outwriteflags)`

> Fills `niter` flags. Sets `outwriteflags[i]` to 1 if `op[i]` can be written to, and to 0 if not.

Functions for iteration `` ` ----------------------- ``NpyIter\_IterNext\_Fn NpyIter\_GetIterNext(NpyIter *iter, char*\*errmsg)`Returns a function pointer for iteration.  A specialized version     of the function pointer may be calculated by this function     instead of being stored in the iterator structure. Thus, to     get good performance, it is required that the function pointer     be saved in a variable rather than retrieved for each loop iteration.      Returns NULL if there is an error.  If errmsg is non-NULL,     no Python exception is set when`NPY\_FAIL`is returned.     Instead, \*errmsg is set to an error message.  When errmsg is     non-NULL, the function may be safely called without holding     the Python GIL.      The typical looping construct is as follows:`\`c NpyIter\_IterNext\_Fn iternext = NpyIter\_GetIterNext(iter, NULL); char \*\*dataptr = NpyIter\_GetDataPtrArray(iter);

>   - do {  
>     /\* use the addresses dataptr\[0\], ... dataptr\[niter-1\] \*/
> 
> } while(iternext(iter));
> 
> When `NPY_ITER_NO_INNER_ITERATION` is specified, the typical inner loop construct is as follows:
> 
> ``` c
> ```
> 
> NpyIter\_IterNext\_Fn iternext = NpyIter\_GetIterNext(iter, NULL); char \**dataptr = NpyIter\_GetDataPtrArray(iter); npy\_intp*stride = NpyIter\_GetInnerStrideArray(iter); npy\_intp \*size\_ptr = NpyIter\_GetInnerLoopSizePtr(iter), size; npy\_intp iiter, niter = NpyIter\_GetNIter(iter);
> 
>   - do {
>     
>       - size = *size\_ptr; while (size--) { /* use the addresses dataptr\[0\], ... dataptr\[niter-1\] \*/
>         
>           - for (iiter = 0; iiter \< niter; ++iiter) {  
>             dataptr\[iiter\] += stride\[iiter\];
>         
>         }
>     
>     }
> 
> } while (iternext());
> 
> Observe that we are using the dataptr array inside the iterator, not copying the values to a local temporary. This is possible because when `iternext()` is called, these pointers will be overwritten with fresh values, not incrementally updated.
> 
> If a compile-time fixed buffer is being used (both flags `NPY_ITER_BUFFERED` and `NPY_ITER_NO_INNER_ITERATION`), the inner size may be used as a signal as well. The size is guaranteed to become zero when `iternext()` returns false, enabling the following loop construct. Note that if you use this construct, you should not pass `NPY_ITER_GROWINNER` as a flag, because it will cause larger sizes under some circumstances:
> 
> ``` c
> ```
> 
> /\* The constructor should have buffersize passed as this value \*/ \#define FIXED\_BUFFER\_SIZE 1024
> 
> NpyIter\_IterNext\_Fn iternext = NpyIter\_GetIterNext(iter, NULL); char \**dataptr = NpyIter\_GetDataPtrArray(iter); npy\_intp*stride = NpyIter\_GetInnerStrideArray(iter); npy\_intp \*size\_ptr = NpyIter\_GetInnerLoopSizePtr(iter), size; npy\_intp i, iiter, niter = NpyIter\_GetNIter(iter);
> 
> /\* One loop with a fixed inner size */ size =*size\_ptr; while (size == FIXED\_BUFFER\_SIZE) { /\* \* This loop could be manually unrolled by a factor \* which divides into FIXED\_BUFFER\_SIZE */ for (i = 0; i \< FIXED\_BUFFER\_SIZE; ++i) { /* use the addresses dataptr\[0\], ... dataptr\[niter-1\] */ for (iiter = 0; iiter \< niter; ++iiter) { dataptr\[iiter\] += stride\[iiter\]; } } iternext(); size =*size\_ptr; }
> 
>   - /\* Finish-up loop with variable inner size */ if (size \> 0) do { size =*size\_ptr;
>     
>       - while (size--) {  
>         /\* use the addresses dataptr\[0\], ... dataptr\[niter-1\] \*/ for (iiter = 0; iiter \< niter; ++iiter) { dataptr\[iiter\] += stride\[iiter\]; }
>     
>     }
> 
> } while (iternext());

`NpyIter_GetCoords_Fn NpyIter_GetGetCoords(NpyIter *iter, char **errmsg)`

> Returns a function pointer for getting the coordinates of the iterator. Returns NULL if the iterator does not support coordinates. It is recommended that this function pointer be cached in a local variable before the iteration loop.
> 
> Returns NULL if there is an error. If errmsg is non-NULL, no Python exception is set when `NPY_FAIL` is returned. Instead, \*errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.

`char **NpyIter_GetDataPtrArray(NpyIter *iter)`

> This gives back a pointer to the `niter` data pointers. If `NPY_ITER_NO_INNER_ITERATION` was not specified, each data pointer points to the current data item of the iterator. If no inner iteration was specified, it points to the first data item of the inner loop.
> 
> This pointer may be cached before the iteration loop, calling `iternext` will not change it. This function may be safely called without holding the Python GIL.

`npy_intp *NpyIter_GetIndexPtr(NpyIter *iter)`

> This gives back a pointer to the index being tracked, or NULL if no index is being tracked. It is only usable if one of the flags `NPY_ITER_C_INDEX` or `NPY_ITER_F_INDEX` were specified during construction.

When the flag `NPY_ITER_NO_INNER_ITERATION` is used, the code `` ` needs to know the parameters for doing the inner loop.  These functions provide that information. ``npy\_intp *NpyIter\_GetInnerStrideArray(NpyIter*iter)`Returns a pointer to an array of the`niter`strides,     one for each iterated object, to be used by the inner loop.      This pointer may be cached before the iteration loop, calling`iternext`will not change it. This function may be safely     called without holding the Python GIL.`npy\_intp\* NpyIter\_GetInnerLoopSizePtr(NpyIter \*iter)`Returns a pointer to the number of iterations the     inner loop should execute.      This address may be cached before the iteration loop, calling`iternext`will not change it.  The value itself may change during     iteration, in particular if buffering is enabled.  This function     may be safely called without holding the Python GIL.`void NpyIter\_GetInnerFixedStrideArray(NpyIter *iter, npy\_intp*out\_strides)`Gets an array of strides which are fixed, or will not change during     the entire iteration.  For strides that may change, the value     NPY_MAX_INTP is placed in the stride.      Once the iterator is prepared for iteration (after a reset if`NPY\_DELAY\_BUFALLOC`was used), call this to get the strides     which may be used to select a fast inner loop function.  For example,     if the stride is 0, that means the inner loop can always load its     value into a variable once, then use the variable throughout the loop,     or if the stride equals the itemsize, a contiguous version for that     operand may be used.      This function may be safely called without holding the Python GIL.  Examples --------  A copy function using the iterator.  The`order`parameter is used to control the memory layout of the allocated result.  If the input is a reference type, this function will fail. To fix this, the code must be changed to specially handle writeable references, and add`NPY\_ITER\_WRITEABLE\_REFERENCES`to the flags:`\`c /\* NOTE: This code has not been compiled/tested */ PyObject*CopyArray(PyObject *arr, NPY\_ORDER order) { NpyIter*iter; NpyIter\_IterNext\_Fn iternext; PyObject *op\[2\],*ret; npy\_uint32 flags; npy\_uint32 op\_flags\[2\]; npy\_intp itemsize, *innersizeptr, innerstride; char*\*dataptrarray;

>   - /\*  
>     \* No inner iteration - inner loop is handled by CopyArray code \*/
> 
> flags = NPY\_ITER\_NO\_INNER\_ITERATION; /\* \* Tell the constructor to automatically allocate the output. \* The data type of the output will match that of the input. \*/ op\[0\] = arr; op\[1\] = NULL; op\_flags\[0\] = NPY\_ITER\_READONLY; op\_flags\[1\] = NPY\_ITER\_WRITEONLY | NPY\_ITER\_ALLOCATE;
> 
> /\* Construct the iterator \*/ iter = NpyIter\_MultiNew(2, op, flags, order, NPY\_NO\_CASTING, op\_flags, NULL, 0, NULL); if (iter == NULL) { return NULL; }
> 
>   - /\*
>     
>       - Make a copy of the iternext function pointer and
>     
>     \* a few other variables the inner loop needs. \*/
> 
> iternext = NpyIter\_GetIterNext(iter); innerstride = NpyIter\_GetInnerStrideArray(iter)\[0\]; itemsize = NpyIter\_GetDescrArray(iter)\[0\]-\>elsize; /\* \* The inner loop size and data pointers may change during the \* loop, so just cache the addresses. \*/ innersizeptr = NpyIter\_GetInnerLoopSizePtr(iter); dataptrarray = NpyIter\_GetDataPtrArray(iter);
> 
>   - /\*
>     
>       - Note that because the iterator allocated the output,
>       - it matches the iteration order and is packed tightly,
>     
>     \* so we don't need to check it like the input. \*/
> 
>   - if (innerstride == itemsize) {
>     
>       - do {
>         
>           - memcpy(dataptrarray\[1\], dataptrarray\[0\],  
>             itemsize \* (\*innersizeptr));
>     
>     } while (iternext(iter));
> 
>   - } else {
>     
>       - /\* Should specialize this further based on item size... */ npy\_intp i; do { npy\_intp size =*innersizeptr;  
>         char *src = dataaddr\[0\],*dst = dataaddr\[1\]; for(i = 0; i \< size; i++, src += innerstride, dst += itemsize) { memcpy(dst, src, itemsize); }
>     
>     } while (iternext(iter));
> 
> }
> 
> /\* Get the result from the iterator object array \*/ ret = NpyIter\_GetOperandArray(iter)\[1\]; Py\_INCREF(ret);
> 
>   - if (NpyIter\_Deallocate(iter) \!= NPY\_SUCCEED) {  
>     Py\_DECREF(ret); return NULL;
> 
> }
> 
> return ret; }

Python lambda UFunc example `` ` ---------------------------  To show how the new iterator allows the definition of efficient UFunc-like functions in pure Python, we demonstrate the function ``luf`, which makes a lambda-expression act like a UFunc.  This is very similar to the`numexpr`library, but only takes a few lines of code.  First, here is the definition of the`luf`function.::      def luf(lamdaexpr, *args, **kwargs):         """Lambda UFunc              e.g.             c = luf(lambda i,j:i+j, a, b, order='K',                                 casting='safe', buffersize=8192)              c = np.empty(...)             luf(lambda i,j:i+j, a, b, out=c, order='K',                                 casting='safe', buffersize=8192)         """          nargs = len(args)         op = args + (kwargs.get('out',None),)         it = np.nditer(op, ['buffered','no_inner_iteration'],                 [['readonly','nbo_aligned']]*nargs +                                 [['writeonly','allocate','no_broadcast']],                 order=kwargs.get('order','K'),                 casting=kwargs.get('casting','safe'),                 buffersize=kwargs.get('buffersize',0))         while not it.finished:             it[-1] = lamdaexpr(*it[:-1])             it.iternext()          return it.operands[-1]  Then, by using`luf`instead of straight Python expressions, we can gain some performance from better cache behavior.::      In [2]: a = np.random.random((50,50,50,10))     In [3]: b = np.random.random((50,50,1,10))     In [4]: c = np.random.random((50,50,50,1))      In [5]: timeit 3*a+b-(a/c)     1 loops, best of 3: 138 ms per loop      In [6]: timeit luf(lambda a,b,c:3*a+b-(a/c), a, b, c)     10 loops, best of 3: 60.9 ms per loop      In [7]: np.all(3*a+b-(a/c) == luf(lambda a,b,c:3*a+b-(a/c), a, b, c))     Out[7]: True   Python addition example -----------------------  The iterator has been mostly written and exposed to Python.  To see how it behaves, let's see what we can do with the np.add ufunc. Even without changing the core of NumPy, we will be able to use the iterator to make a faster add function.  The Python exposure supplies two iteration interfaces, one which follows the Python iterator protocol, and another which mirrors the C-style do-while pattern.  The native Python approach is better in most cases, but if you need the iterator's coordinates or index, use the C-style pattern.  Here is how we might write an`iter\_add`function, using the Python iterator protocol.::      def iter_add_py(x, y, out=None):         addop = np.add          it = np.nditer([x,y,out], [],                     [['readonly'],['readonly'],['writeonly','allocate']])          for (a, b, c) in it:             addop(a, b, c)          return it.operands[2]  Here is the same function, but following the C-style pattern.::      def iter_add(x, y, out=None):         addop = np.add          it = np.nditer([x,y,out], [],                     [['readonly'],['readonly'],['writeonly','allocate']])          while not it.finished:             addop(it[0], it[1], it[2])             it.iternext()          return it.operands[2]  Some noteworthy points about this function:  * Cache np.add as a local variable to reduce namespace lookups * Inputs are readonly, output is writeonly, and will be allocated   automatically if it is None. * Uses np.add's out parameter to avoid an extra copy.  Let's create some test variables, and time this function as well as the built-in np.add.::      In [1]: a = np.arange(1000000,dtype='f4').reshape(100,100,100)     In [2]: b = np.arange(10000,dtype='f4').reshape(1,100,100)     In [3]: c = np.arange(10000,dtype='f4').reshape(100,100,1)      In [4]: timeit iter_add(a, b)     1 loops, best of 3: 7.03 s per loop      In [5]: timeit np.add(a, b)     100 loops, best of 3: 6.73 ms per loop  At a thousand times slower, this is clearly not very good.  One feature of the iterator, designed to help speed up the inner loops, is the flag`no\_inner\_iteration`.  This is the same idea as the old iterator's`PyArray\_IterAllButAxis`, but slightly smarter.  Let's modify`iter\_add`to use this feature.::      def iter_add_noinner(x, y, out=None):         addop = np.add          it = np.nditer([x,y,out], ['no_inner_iteration'],                     [['readonly'],['readonly'],['writeonly','allocate']])          for (a, b, c) in it:             addop(a, b, c)          return it.operands[2]  The performance improves dramatically.::      In[6]: timeit iter_add_noinner(a, b)     100 loops, best of 3: 7.1 ms per loop  The performance is basically as good as the built-in function!  It turns out this is because the iterator was able to coalesce the last two dimensions, resulting in 100 adds of 10000 elements each.  If the inner loop doesn't become as large, the performance doesn't improve as dramatically.  Let's use`c`instead of`b`to see how this works.::      In[7]: timeit iter_add_noinner(a, c)     10 loops, best of 3: 76.4 ms per loop  It's still a lot better than seven seconds, but still over ten times worse than the built-in function.  Here, the inner loop has 100 elements, and it's iterating 10000 times.  If we were coding in C, our performance would already be as good as the built-in performance, but in Python there is too much overhead.  This leads us to another feature of the iterator, its ability to give us views of the iterated memory.  The views it gives us are structured so that processing them in C-order, like the built-in NumPy code does, gives the same access order as the iterator itself.  Effectively, we are using the iterator to solve for a good memory access pattern, then using other NumPy machinery to efficiently execute it.  Let's modify`iter\_add\`\` once again.:

    def iter_add_itview(x, y, out=None):
        it = np.nditer([x,y,out], [],
                    [['readonly'],['readonly'],['writeonly','allocate']])
    
        (a, b, c) = it.itviews
        np.add(a, b, c)
    
        return it.operands[2]

Now the performance pretty closely matches the built-in function's.:

    In [8]: timeit iter_add_itview(a, b)
    100 loops, best of 3: 6.18 ms per loop
    
    In [9]: timeit iter_add_itview(a, c)
    100 loops, best of 3: 6.69 ms per loop

Let us now step back to a case similar to the original motivation for the new iterator. Here are the same calculations in Fortran memory order instead Of C memory order.:

    In [10]: a = np.arange(1000000,dtype='f4').reshape(100,100,100).T
    In [12]: b = np.arange(10000,dtype='f4').reshape(100,100,1).T
    In [11]: c = np.arange(10000,dtype='f4').reshape(1,100,100).T
    
    In [39]: timeit np.add(a, b)
    10 loops, best of 3: 34.3 ms per loop
    
    In [41]: timeit np.add(a, c)
    10 loops, best of 3: 31.6 ms per loop
    
    In [44]: timeit iter_add_itview(a, b)
    100 loops, best of 3: 6.58 ms per loop
    
    In [43]: timeit iter_add_itview(a, c)
    100 loops, best of 3: 6.33 ms per loop

As you can see, the performance of the built-in function dropped significantly, but our newly-written add function maintained essentially the same performance. As one final test, let's try several adds chained together.:

    In [4]: timeit np.add(np.add(np.add(a,b), c), a)
    1 loops, best of 3: 99.5 ms per loop
    
    In [9]: timeit iter_add_itview(iter_add_itview(iter_add_itview(a,b), c), a)
    10 loops, best of 3: 29.3 ms per loop

Also, just to check that it's doing the same thing,:

    In [22]: np.all(
       ....: iter_add_itview(iter_add_itview(iter_add_itview(a,b), c), a) ==
       ....: np.add(np.add(np.add(a,b), c), a)
       ....: )
    
    Out[22]: True

#### Image compositing example revisited

For motivation, we had an example that did an 'over' composite operation on two images. Now let's see how we can write the function with the new iterator.

Here is one of the original functions, for reference, and some random image data.:

    In [5]: rand1 = np.random.random(1080*1920*4).astype(np.float32)
    In [6]: rand2 = np.random.random(1080*1920*4).astype(np.float32)
    In [7]: image1 = rand1.reshape(1080,1920,4).swapaxes(0,1)
    In [8]: image2 = rand2.reshape(1080,1920,4).swapaxes(0,1)
    
    In [3]: def composite_over(im1, im2):
      ....:     ret = (1-im1[:,:,-1])[:,:,np.newaxis]*im2
      ....:     ret += im1
      ....:     return ret
    
    In [4]: timeit composite_over(image1,image2)
    1 loops, best of 3: 1.39 s per loop

Here's the same function, rewritten to use a new iterator. Note how easy it was to add an optional output parameter.:

    In [5]: def composite_over_it(im1, im2, out=None, buffersize=4096):
      ....:     it = np.nditer([im1, im1[:,:,-1], im2, out],
      ....:                     ['buffered','no_inner_iteration'],
      ....:                     [['readonly']]*3+[['writeonly','allocate']],
      ....:                     op_axes=[None,[0,1,np.newaxis],None,None],
      ....:                     buffersize=buffersize)
      ....:     while not it.finished:
      ....:         np.multiply(1-it[1], it[2], it[3])
      ....:         it[3] += it[0]
      ....:         it.iternext()
      ....:     return it.operands[3]
    
    In [6]: timeit composite_over_it(image1, image2)
    1 loops, best of 3: 197 ms per loop

A big speed improvement, over even the best previous attempt using straight NumPy and a C-order array\! By playing with the buffer size, we can see how the speed improves until we hit the limits of the CPU cache in the inner loop.:

    In [7]: timeit composite_over_it(image1, image2, buffersize=2**7)
    1 loops, best of 3: 1.23 s per loop
    
    In [8]: timeit composite_over_it(image1, image2, buffersize=2**8)
    1 loops, best of 3: 699 ms per loop
    
    In [9]: timeit composite_over_it(image1, image2, buffersize=2**9)
    1 loops, best of 3: 418 ms per loop
    
    In [10]: timeit composite_over_it(image1, image2, buffersize=2**10)
    1 loops, best of 3: 287 ms per loop
    
    In [11]: timeit composite_over_it(image1, image2, buffersize=2**11)
    1 loops, best of 3: 225 ms per loop
    
    In [12]: timeit composite_over_it(image1, image2, buffersize=2**12)
    1 loops, best of 3: 194 ms per loop
    
    In [13]: timeit composite_over_it(image1, image2, buffersize=2**13)
    1 loops, best of 3: 180 ms per loop
    
    In [14]: timeit composite_over_it(image1, image2, buffersize=2**14)
    1 loops, best of 3: 192 ms per loop
    
    In [15]: timeit composite_over_it(image1, image2, buffersize=2**15)
    1 loops, best of 3: 280 ms per loop
    
    In [16]: timeit composite_over_it(image1, image2, buffersize=2**16)
    1 loops, best of 3: 328 ms per loop
    
    In [17]: timeit composite_over_it(image1, image2, buffersize=2**17)
    1 loops, best of 3: 345 ms per loop

And finally, to double check that it's working, we can compare the two functions.:

    In [18]: np.all(composite_over(image1, image2) ==
        ...:        composite_over_it(image1, image2))
    Out[18]: True

#### Image compositing with NumExpr

As a test of the iterator, numexpr has been enhanced to allow use of the iterator instead of its internal broadcasting code. First, let's implement the composite operation with numexpr.:

    In [22]: def composite_over_ne(im1, im2, out=None):
       ....:     ima = im1[:,:,-1][:,:,np.newaxis]
       ....:     return ne.evaluate("im1+(1-ima)*im2")
    
    In [23]: timeit composite_over_ne(image1,image2)
    1 loops, best of 3: 1.25 s per loop

This beats the straight NumPy operation, but isn't very good. Switching to the iterator version of numexpr, we get a big improvement over the straight Python function using the iterator. Note that this is on a dual core machine.:

    In [29]: def composite_over_ne_it(im1, im2, out=None):
       ....:     ima = im1[:,:,-1][:,:,np.newaxis]
       ....:     return ne.evaluate_iter("im1+(1-ima)*im2")
    
    In [30]: timeit composite_over_ne_it(image1,image2)
    10 loops, best of 3: 67.2 ms per loop
    
    In [31]: ne.set_num_threads(1)
    In [32]: timeit composite_over_ne_it(image1,image2)
    10 loops, best of 3: 91.1 ms per loop

---

nep-0011-deferred-ufunc-evaluation.md

---

# NEP 11 â€” Deferred UFunc evaluation

  - Author  
    Mark Wiebe \<<mwwiebe@gmail.com>\>

  - Content-Type  
    text/x-rst

  - Created  
    30-Nov-2010

  - Status  
    Deferred

## Abstract

This NEP describes a proposal to add deferred evaluation to NumPy's UFuncs. This will allow Python expressions like "a\[:\] = b + c + d + e" to be evaluated in a single pass through all the variables at once, with no temporary arrays. The resulting performance will likely be comparable to the *numexpr* library, but with a more natural syntax.

This idea has some interaction with UFunc error handling and the UPDATEIFCOPY flag, affecting the design and implementation, but the result allows for the usage of deferred evaluation with minimal effort from the Python user's perspective.

## Motivation

NumPy's style of UFunc execution causes suboptimal performance for large expressions, because multiple temporaries are allocated and the inputs are swept through in multiple passes. The *numexpr* library can outperform NumPy for such large expressions, by doing the execution in small cache-friendly blocks, and evaluating the whole expression per element. This results in one sweep through each input, which is significantly better for the cache.

For an idea of how to get this kind of behavior in NumPy without changing the Python code, consider the C++ technique of expression templates. These can be used to quite arbitrarily rearrange expressions using vectors or other data structures, example:

`` `cpp     A = B + C + D;  can be transformed into something equivalent to:  .. code-block:: cpp      for(i = 0; i < A.size; ++i) {         A[i] = B[i] + C[i] + D[i];     }  This is done by returning a proxy object that knows how to calculate ``<span class="title-ref"> the result instead of returning the actual object. With modern C++ optimizing compilers, the resulting machine code is often the same as hand-written loops. For an example of this, see the \`Blitz++ Library \<http://www.oonumerics.org/blitz/docs/blitz\_3.html\></span>\_. A more recently created library for helping write expression templates is [Boost Proto](http://beta.boost.org/doc/libs/1_44_0/doc/html/proto.html).

By using the same idea of returning a proxy object in Python, we can accomplish the same thing dynamically. The return object is an ndarray without its buffer allocated, and with enough knowledge to calculate itself when needed. When a "deferred array" is finally evaluated, we can use the expression tree made up of all the operand deferred arrays, effectively creating a single new UFunc to evaluate on the fly.

## Example Python code

Here's how it might be used in NumPy.:

    # a, b, c are large ndarrays
    
    with np.deferredstate(True):
    
        d = a + b + c
        # Now d is a 'deferred array,' a, b, and c are marked READONLY
        # similar to the existing UPDATEIFCOPY mechanism.
    
        print d
        # Since the value of d was required, it is evaluated so d becomes
        # a regular ndarray and gets printed.
    
        d[:] = a*b*c
        # Here, the automatically combined "ufunc" that computes
        # a*b*c effectively gets an out= parameter, so no temporary
        # arrays are needed whatsoever.
    
        e = a+b+c*d
        # Now e is a 'deferred array,' a, b, c, and d are marked READONLY
    
        d[:] = a
        # d was marked readonly, but the assignment could see that
        # this was due to it being a deferred expression operand.
        # This triggered the deferred evaluation so it could assign
        # the value of a to d.

There may be some surprising behavior, though.:

    with np.deferredstate(True):
    
        d = a + b + c
        # d is deferred
    
        e[:] = d
        f[:] = d
        g[:] = d
        # d is still deferred, and its deferred expression
        # was evaluated three times, once for each assignment.
        # This could be detected, with d being converted to
        # a regular ndarray the second time it is evaluated.

I believe the usage that should be recommended in the documentation is to leave the deferred state at its default, except when evaluating a large expression that can benefit from it.:

    # calculations
    
    with np.deferredstate(True):
        x = <big expression>
    
    # more calculations

This will avoid surprises which would be cause by always keeping deferred usage True, like floating point warnings or exceptions at surprising times when deferred expression are used later. User questions like "Why does my print statement throw a divide by zero error?" can hopefully be avoided by recommending this approach.

## Proposed deferred evaluation API

For deferred evaluation to work, the C API needs to be aware of its existence, and be able to trigger evaluation when necessary. The ndarray would gain two new flag.

> `NPY_ISDEFERRED`
> 
> > Indicates the expression evaluation for this ndarray instance has been deferred.
> 
> `NPY_DEFERRED_WASWRITEABLE`
> 
> > Can only be set when `PyArray_GetDeferredUsageCount(arr) > 0`. It indicates that when `arr` was first used in a deferred expression, it was a writeable array. If this flag is set, calling `PyArray_CalculateAllDeferred()` will make `arr` writeable again.

<div class="note">

<div class="title">

Note

</div>

QUESTION

Should NPY\_DEFERRED and NPY\_DEFERRED\_WASWRITEABLE be visible to Python, or should accessing the flags from python trigger PyArray\_CalculateAllDeferred if necessary?

</div>

The API would be expanded with a number of functions.

`int PyArray_CalculateAllDeferred()`

> This function forces all currently deferred calculations to occur.
> 
> For example, if the error state is set to ignore all, and np.seterr({all='raise'}), this would change what happens to already deferred expressions. Thus, all the existing deferred arrays should be evaluated before changing the error state.

`int PyArray_CalculateDeferred(PyArrayObject* arr)`

> If 'arr' is a deferred array, allocates memory for it and evaluates the deferred expression. If 'arr' is not a deferred array, simply returns success. Returns NPY\_SUCCESS or NPY\_FAILURE.

`int PyArray_CalculateDeferredAssignment(PyArrayObject* arr, PyArrayObject* out)`

> If 'arr' is a deferred array, evaluates the deferred expression into 'out', and 'arr' remains a deferred array. If 'arr' is not a deferred array, copies its value into out. Returns NPY\_SUCCESS or NPY\_FAILURE.

`int PyArray_GetDeferredUsageCount(PyArrayObject* arr)`

> Returns a count of how many deferred expressions use this array as an operand.

The Python API would be expanded as follows.

> `numpy.setdeferred(state)`
> 
> > Enables or disables deferred evaluation. True means to always use deferred evaluation. False means to never use deferred evaluation. None means to use deferred evaluation if the error handling state is set to ignore everything. At NumPy initialization, the deferred state is None.
> > 
> > Returns the previous deferred state.

`numpy.getdeferred()`

> Returns the current deferred state.

`numpy.deferredstate(state)`

> A context manager for deferred state handling, similar to `numpy.errstate`.

### Error handling

Error handling is a thorny issue for deferred evaluation. If the NumPy error state is {all='ignore'}, it might be reasonable to introduce deferred evaluation as the default, however if a UFunc can raise an error, it would be very strange for the later 'print' statement to throw the exception instead of the actual operation which caused the error.

What may be a good approach is to by default enable deferred evaluation only when the error state is set to ignore all, but allow user control with 'setdeferred' and 'getdeferred' functions. True would mean always use deferred evaluation, False would mean never use it, and None would mean use it only when safe (i.e. the error state is set to ignore all).

### Interaction with UPDATEIFCOPY

The `NPY_UPDATEIFCOPY` documentation states:

> The data area represents a (well-behaved) copy whose information should be transferred back to the original when this array is deleted.
> 
> This is a special flag that is set if this array represents a copy made because a user required certain flags in PyArray\_FromAny and a copy had to be made of some other array (and the user asked for this flag to be set in such a situation). The base attribute then points to the â€œmisbehavedâ€ array (which is set read\_only). When the array with this flag set is deallocated, it will copy its contents back to the â€œmisbehavedâ€ array (casting if necessary) and will reset the â€œmisbehavedâ€ array to NPY\_WRITEABLE. If the â€œmisbehavedâ€ array was not NPY\_WRITEABLE to begin with then PyArray\_FromAny would have returned an error because NPY\_UPDATEIFCOPY would not have been possible.

The current implementation of UPDATEIFCOPY assumes that it is the only mechanism mucking with the writeable flag in this manner. These mechanisms must be aware of each other to work correctly. Here's an example of how they might go wrong:

1.  Make a temporary copy of 'arr' with UPDATEIFCOPY ('arr' becomes read only)
2.  Use 'arr' in a deferred expression (deferred usage count becomes one, NPY\_DEFERRED\_WASWRITEABLE is **not** set, since 'arr' is read only)
3.  Destroy the temporary copy, causing 'arr' to become writeable
4.  Writing to 'arr' destroys the value of the deferred expression

To deal with this issue, we make these two states mutually exclusive.

  - Usage of UPDATEIFCOPY checks the `NPY_DEFERRED_WASWRITEABLE` flag, and if it's set, calls `PyArray_CalculateAllDeferred` to flush all deferred calculation before proceeding.
  - The ndarray gets a new flag `NPY_UPDATEIFCOPY_TARGET` indicating the array will be updated and made writeable at some point in the future. If the deferred evaluation mechanism sees this flag in any operand, it triggers immediate evaluation.

### Other implementation details

When a deferred array is created, it gets references to all the operands of the UFunc, along with the UFunc itself. The 'DeferredUsageCount' is incremented for each operand, and later gets decremented when the deferred expression is calculated or the deferred array is destroyed.

A global list of weak references to all the deferred arrays is tracked, in order of creation. When `PyArray_CalculateAllDeferred` gets called, the newest deferred array is calculated first. This may release references to other deferred arrays contained in the deferred expression tree, which then never have to be calculated.

### Further optimization

Instead of conservatively disabling deferred evaluation when any errors are not set to 'ignore', each UFunc could give a set of possible errors it generates. Then, if all those errors are set to 'ignore', deferred evaluation could be used even if other errors are not set to ignore.

Once the expression tree is explicitly stored, it is possible to do transformations on it. For example add(add(a,b),c) could be transformed into add3(a,b,c), or add(multiply(a,b),c) could become fma(a,b,c) using the CPU fused multiply-add instruction where available.

While I've framed deferred evaluation as just for UFuncs, it could be extended to other functions, such as dot(). For example, chained matrix multiplications could be reordered to minimize the size of intermediates, or peep-hole style optimizer passes could search for patterns that match optimized BLAS/other high performance library calls.

For operations on really large arrays, integrating a JIT like LLVM into this system might be a big benefit. The UFuncs and other operations would provide bitcode, which could be inlined together and optimized by the LLVM optimizers, then executed. In fact, the iterator itself could also be represented in bitcode, allowing LLVM to consider the entire iteration while doing its optimization.

---

nep-0012-missing-data.md

---

# NEP 12 â€” Missing data functionality in NumPy

  - Author  
    Mark Wiebe \<<mwwiebe@gmail.com>\>

  - Copyright  
    Copyright 2011 by Enthought, Inc

  - License  
    CC By-SA 3.0 (<https://creativecommons.org/licenses/by-sa/3.0/>)

  - Date  
    2011-06-23

  - Status  
    Deferred

## Table of contents

<div class="contents">

</div>

## Abstract

Users interested in dealing with missing data within NumPy are generally pointed to the masked array subclass of the ndarray, known as 'numpy.ma'. This class has a number of users who depend strongly on its capabilities, but people who are accustomed to the deep integration of the missing data placeholder "NA" in the R project and others who find the programming interface challenging or inconsistent tend not to use it.

This NEP proposes to integrate a mask-based missing data solution into NumPy, with an additional bitpattern-based missing data solution that can be implemented concurrently or later integrating seamlessly with the mask-based solution.

The mask-based solution and the bitpattern-based solutions in this proposal offer the exact same missing value abstraction, with several differences in performance, memory overhead, and flexibility.

The mask-based solution is more flexible, supporting all behaviors of the bitpattern-based solution, but leaving the hidden values untouched whenever an element is masked.

The bitpattern-based solution requires less memory, is bit-level compatible with the 64-bit floating point representation used in R, but does not preserve the hidden values and in fact requires stealing at least one bit pattern from the underlying dtype to represent the missing value NA.

Both solutions are generic in the sense that they can be used with custom data types very easily, with no effort in the case of the masked solution, and with the requirement that a bit pattern to sacrifice be chosen in the case of the bitpattern solution.

## Definition of missing data

In order to be able to develop an intuition about what computation will be done by various NumPy functions, a consistent conceptual model of what a missing element means must be applied. Ferreting out the behaviors people need or want when they are working with "missing data" seems to be tricky, but I believe that it boils down to two different ideas, each of which is internally self-consistent.

One of them, the "unknown yet existing data" interpretation, can be applied rigorously to all computations, while the other makes sense for some statistical operations like standard deviation but not for linear algebra operations like matrix product. Thus, making "unknown yet existing data" be the default interpretation is superior, providing a consistent model across all computations, and for those operations where the other interpretation makes sense, an optional parameter "skipna=" can be added.

For people who want the other interpretation to be default, a mechanism proposed elsewhere for customizing subclass ufunc behavior with a \_[numpy\_ufunc]() member function would allow a subclass with a different default to be created.

### Unknown yet existing data (NA)

This is the approach taken in the R project, defining a missing element as something which does have a valid value which isn't known, or is NA (not available). This proposal adopts this behavior as the default for all operations involving missing values.

In this interpretation, nearly any computation with a missing input produces a missing output. For example, 'sum(a)' would produce a missing value if 'a' contained just one missing element. When the output value does not depend on one of the inputs, it is reasonable to output a value that is not NA, such as logical\_and(NA, False) == False.

Some more complex arithmetic operations, such as matrix products, are well defined with this interpretation, and the result should be the same as if the missing values were NaNs. Actually implementing such things to the theoretical limit is probably not worth it, and in many cases either raising an exception or returning all missing values may be preferred to doing precise calculations.

### Data that doesn't exist or is being skipped (IGNORE)

Another useful interpretation is that the missing elements should be treated as if they didn't exist in the array, and the operation should do its best to interpret what that means according to the data that's left. In this case, 'mean(a)' would compute the mean of just the values that are available, adjusting both the sum and count it uses based on which values are missing. To be consistent, the mean of an array of all missing values must produce the same result as the mean of a zero-sized array without missing value support.

This kind of data can arise when conforming sparsely sampled data into a regular sampling pattern, and is a useful interpretation to use when attempting to get best-guess answers for many statistical queries.

In R, many functions take a parameter "na.rm=T" which means to treat the data as if the NA values are not part of the data set. This proposal defines a standard parameter "skipna=True" for this same purpose.

## Implementation techniques for missing values

In addition to there being two different interpretations of missing values, there are two different commonly used implementation techniques for missing values. While there are some differing default behaviors between existing implementations of the techniques, I believe that the design choices made in a new implementation must be made based on their merits, not by rote copying of previous designs.

Both masks and bitpatterns have different strong and weak points, depending on the application context. This NEP thus proposes to implement both. To enable the writing of generic "missing value" code which does not have to worry about whether the arrays it is using have taken one or the other approach, the missing value semantics will be identical for the two implementations.

### Bit patterns signalling missing values (bitpattern)

One or more patterns of bits, for example a NaN with a particular payload, are chosen to represent the missing value placeholder NA.

A consequence of this approach is that assigning NA changes the bits holding the value, so that value is gone.

Additionally, for some types such as integers, a good and proper value must be sacrificed to enable this functionality.

### Boolean masks signalling missing values (mask)

A mask is a parallel array of booleans, either one byte per element or one bit per element, allocated alongside the existing array data. In this NEP, the convention is chosen that True means the element is valid (unmasked), and False means the element is NA.

By taking care when writing any C algorithm that works with values and masks together, it is possible to have the memory for a value that is masked never be written to. This feature allows multiple simultaneous views of the same data with different choices of what is missing, a feature requested by many people on the mailing list.

This approach places no limitations on the values of the underlying data type, it may take on any binary pattern without affecting the NA behavior.

## Glossary of terms

Because the above discussions of the different concepts and their relationships are tricky to understand, here are more succinct definitions of the terms used in this NEP.

  - NA (Not Available/Propagate)  
    A placeholder for a value which is unknown to computations. That value may be temporarily hidden with a mask, may have been lost due to hard drive corruption, or gone for any number of reasons. For sums and products this means to produce NA if any of the inputs are NA. This is the same as NA in the R project.

  - IGNORE (Ignore/Skip)  
    A placeholder which should be treated by computations as if no value does or could exist there. For sums, this means act as if the value were zero, and for products, this means act as if the value were one. It's as if the array were compressed in some fashion to not include that element.

  - bitpattern  
    A technique for implementing either NA or IGNORE, where a particular set of bit patterns are chosen from all the possible bit patterns of the value's data type to signal that the element is NA or IGNORE.

  - mask  
    A technique for implementing either NA or IGNORE, where a boolean or enum array parallel to the data array is used to signal which elements are NA or IGNORE.

  - numpy.ma  
    The existing implementation of a particular form of masked arrays, which is part of the NumPy codebase.

  - Python API  
    All the interface mechanisms that are exposed to Python code for using missing values in NumPy. This API is designed to be Pythonic and fit into the way NumPy works as much as possible.

  - C API  
    All the implementation mechanisms exposed for CPython extensions written in C that want to support NumPy missing value support. This API is designed to be as natural as possible in C, and is usually prioritizes flexibility and high performance.

## Missing values as seen in Python

### Working with missing values

NumPy will gain a global singleton called numpy.NA, similar to None, but with semantics reflecting its status as a missing value. In particular, trying to treat it as a boolean will raise an exception, and comparisons with it will produce numpy.NA instead of True or False. These basics are adopted from the behavior of the NA value in the R project. To dig deeper into the ideas, <https://en.wikipedia.org/wiki/Ternary_logic#Kleene_logic> provides a starting point.

For example,:

    >>> np.array([1.0, 2.0, np.NA, 7.0], maskna=True)
    array([1., 2., NA, 7.], maskna=True)
    >>> np.array([1.0, 2.0, np.NA, 7.0], dtype='NA')
    array([1., 2., NA, 7.], dtype='NA[<f8]')
    >>> np.array([1.0, 2.0, np.NA, 7.0], dtype='NA[f4]')
    array([1., 2., NA, 7.], dtype='NA[<f4]')

produce arrays with values \[1.0, 2.0, \<inaccessible\>, 7.0\] / mask \[Exposed, Exposed, Hidden, Exposed\], and values \[1.0, 2.0, \<NA bitpattern\>, 7.0\] for the masked and NA dtype versions respectively.

The np.NA singleton may accept a dtype= keyword parameter, indicating that it should be treated as an NA of a particular data type. This is also a mechanism for preserving the dtype in a NumPy scalar-like fashion. Here's what this looks like:

    >>> np.sum(np.array([1.0, 2.0, np.NA, 7.0], maskna=True))
    NA(dtype='<f8')
    >>> np.sum(np.array([1.0, 2.0, np.NA, 7.0], dtype='NA[f8]'))
    NA(dtype='NA[<f8]')

Assigning a value to an array always causes that element to not be NA, transparently unmasking it if necessary. Assigning numpy.NA to the array masks that element or assigns the NA bitpattern for the particular dtype. In the mask-based implementation, the storage behind a missing value may never be accessed in any way, other than to unmask it by assigning its value.

To test if a value is missing, the function "np.isna(arr\[0\])" will be provided. One of the key reasons for the NumPy scalars is to allow their values into dictionaries.

All operations which write to masked arrays will not affect the value unless they also unmask that value. This allows the storage behind masked elements to still be relied on if they are still accessible from another view which doesn't have them masked. For example, the following was run on the missingdata work-in-progress branch:

    >>> a = np.array([1,2])
    >>> b = a.view(maskna=True)
    >>> b
    array([1, 2], maskna=True)
    >>> b[0] = np.NA
    >>> b
    array([NA, 2], maskna=True)
    >>> a
    array([1, 2])
    >>> # The underlying number 1 value in 'a[0]' was untouched

Copying values between the mask-based implementation and the bitpattern implementation will transparently do the correct thing, turning the bitpattern into a masked value, or a masked value into the bitpattern where appropriate. The one exception is if a valid value in a masked array happens to have the NA bitpattern, copying this value to the NA form of the dtype will cause it to become NA as well.

When operations are done between arrays with NA dtypes and masked arrays, the result will be masked arrays. This is because in some cases the NA dtypes cannot represent all the values in the masked array, so going to masked arrays is the only way to preserve all aspects of the data.

If np.NA or masked values are copied to an array without support for missing values enabled, an exception will be raised. Adding a mask to the target array would be problematic, because then having a mask would be a "viral" property consuming extra memory and reducing performance in unexpected ways.

By default, the string "NA" will be used to represent missing values in str and repr outputs. A global configuration will allow this to be changed, exactly extending the way nan and inf are treated. The following works in the current draft implementation:

    >>> a = np.arange(6, maskna=True)
    >>> a[3] = np.NA
    >>> a
    array([0, 1, 2, NA, 4, 5], maskna=True)
    >>> np.set_printoptions(nastr='blah')
    >>> a
    array([0, 1, 2, blah, 4, 5], maskna=True)

For floating point numbers, Inf and NaN are separate concepts from missing values. If a division by zero occurs in an array with default missing value support, an unmasked Inf or NaN will be produced. To mask those values, a further 'a\[np.logical\_not(a.isfinite(a))\] = np.NA' can achieve that. For the bitpattern approach, the parameterized dtype('NA\[f8,InfNan\]') described in a later section can be used to get these semantics without the extra manipulation.

A manual loop through a masked array like:

    >>> a = np.arange(5., maskna=True)
    >>> a[3] = np.NA
    >>> a
    array([ 0.,  1.,  2., NA,  4.], maskna=True)
    >>> for i in range(len(a)):
    ...     a[i] = np.log(a[i])
    ...
    __main__:2: RuntimeWarning: divide by zero encountered in log
    >>> a
    array([       -inf,  0.        ,  0.69314718, NA,  1.38629436], maskna=True)

works even with masked values, because 'a\[i\]' returns an NA object with a data type associated, that can be treated properly by the ufuncs.

### Accessing a boolean mask

The mask used to implement missing data in the masked approach is not accessible from Python directly. This is partially due to differing opinions on whether True in the mask should mean "missing" or "not missing" Additionally, exposing the mask directly would preclude a potential space optimization, where a bit-level instead of a byte-level mask is used to get a factor of eight memory usage improvement.

To access a mask directly, there are two functions provided. They work equivalently for both arrays with masks and NA bit patterns, so they are specified in terms of NA and available values instead of masked and unmasked values. The functions are 'np.isna' and 'np.isavail', which test for NA or available values respectively.

### Creating NA-masked arrays

The usual way to create an array with an NA mask is to pass the keyword parameter maskna=True to one of the constructors. Most functions that create a new array take this parameter, and produce an NA-masked array with all its elements exposed when the parameter is set to True.

There are also two flags which indicate and control the nature of the mask used in masked arrays. These flags can be used to add a mask, or ensure the mask isn't a view into another array's mask.

First is 'arr.flags.maskna', which is True for all masked arrays and may be set to True to add a mask to an array which does not have one.

Second is 'arr.flags.ownmaskna', which is True if the array owns the memory to the mask, and False if the array has no mask, or has a view into the mask of another array. If this is set to True in a masked array, the array will create a copy of the mask so that further modifications to the mask will not affect the original mask from which the view was taken.

### Na-masks when constructing from lists

The initial design of NA-mask construction was to make all construction fully explicit. This turns out to be unwieldy when working interactively with NA-masked arrays, and having an object array be created instead of an NA-masked array can be very surprising.

Because of this, the design has been changed to enable an NA-mask whenever creating an array from lists which have an NA object in them. There could be some debate of whether one should create NA-masks or NA-bitpatterns by default, but due to the time constraints it was only feasible to tackle NA-masks, and extending the NA-mask support more fully throughout NumPy seems much more reasonable than starting another system and ending up with two incomplete systems.

### Mask implementation details

The memory ordering of the mask will always match the ordering of the array it is associated with. A Fortran-style array will have a Fortran-style mask, etc.

When a view of an array with a mask is taken, the view will have a mask which is also a view of the mask in the original array. This means unmasking values in views will also unmask them in the original array, and if a mask is added to an array, it will not be possible to ever remove that mask except to create a new array copying the data but not the mask.

It is still possible to temporarily treat an array with a mask without giving it one, by first creating a view of the array and then adding a mask to that view. A data set can be viewed with multiple different masks simultaneously, by creating multiple views, and giving each view a mask.

### New ndarray methods

New functions added to the numpy namespace are:

    np.isna(arr) [IMPLEMENTED]
        Returns a boolean array with True wherever the array is masked
        or matches the NA bitpattern, and False elsewhere
    
    np.isavail(arr)
        Returns a boolean array with False wherever the array is masked
        or matches the NA bitpattern, and True elsewhere

New functions added to the ndarray are:

    arr.copy(..., replacena=np.NA)
        Modification to the copy function which replaces NA values,
        either masked or with the NA bitpattern, with the 'replacena='
        parameter supplied. When 'replacena' isn't NA, the copied
        array is unmasked and has the 'NA' part stripped from the
        parameterized dtype ('NA[f8]' becomes just 'f8').
    
        The default for replacena is chosen to be np.NA instead of None,
        because it may be desirable to replace NA with None in an
        NA-masked object array.
    
        For future multi-NA support, 'replacena' could accept a dictionary
        mapping the NA payload to the value to substitute for that
        particular NA. NAs with payloads not appearing in the dictionary
        would remain as NA unless a 'default' key was also supplied.
    
        Both the parameter to replacena and the values in the dictionaries
        can be either scalars or arrays which get broadcast onto 'arr'.
    
    arr.view(maskna=True) [IMPLEMENTED]
        This is a shortcut for
        >>> a = arr.view()
        >>> a.flags.maskna = True
    
    arr.view(ownmaskna=True) [IMPLEMENTED]
        This is a shortcut for
        >>> a = arr.view()
        >>> a.flags.maskna = True
        >>> a.flags.ownmaskna = True

### Element-wise ufuncs with missing values

As part of the implementation, ufuncs and other operations will have to be extended to support masked computation. Because this is a useful feature in general, even outside the context of a masked array, in addition to working with masked arrays ufuncs will take an optional 'where=' parameter which allows the use of boolean arrays to choose where a computation should be done.:

    >>> np.add(a, b, out=b, where=(a > threshold))

A benefit of having this 'where=' parameter is that it provides a way to temporarily treat an object with a mask without ever creating a masked array object. In the example above, this would only do the add for the array elements with True in the 'where' clause, and neither 'a' nor 'b' need to be masked arrays.

If the 'out' parameter isn't specified, use of the 'where=' parameter will produce an array with a mask as the result, with missing values for everywhere the 'where' clause had the value False.

For boolean operations, the R project special cases logical\_and and logical\_or so that logical\_and(NA, False) is False, and logical\_or(NA, True) is True. On the other hand, 0 \* NA isn't 0, but here the NA could represent Inf or NaN, in which case 0 \* the backing value wouldn't be 0 anyway.

For NumPy element-wise ufuncs, the design won't support this ability for the mask of the output to depend simultaneously on the mask and the value of the inputs. The NumPy 1.6 nditer, however, makes it fairly easy to write standalone functions which look and feel just like ufuncs, but deviate from their behavior. The functions logical\_and and logical\_or can be moved into standalone function objects which are backwards compatible with the current ufuncs.

### Reduction ufuncs with missing values

Reduction operations like 'sum', 'prod', 'min', and 'max' will operate consistently with the idea that a masked value exists, but its value is unknown.

An optional parameter 'skipna=' will be added to those functions which can interpret it appropriately to do the operation as if just the unmasked values existed.

With 'skipna=True', when all the input values are masked, 'sum' and 'prod' will produce the additive and multiplicative identities respectively, while 'min' and 'max' will produce masked values. Statistics operations which require a count, like 'mean' and 'std' will also use the unmasked value counts for their calculations if 'skipna=True', and produce masked values when all the inputs are masked.

Some examples:

    >>> a = np.array([1., 3., np.NA, 7.], maskna=True)
    >>> np.sum(a)
    array(NA, dtype='<f8', maskna=True)
    >>> np.sum(a, skipna=True)
    11.0
    >>> np.mean(a)
    NA(dtype='<f8')
    >>> np.mean(a, skipna=True)
    3.6666666666666665
    
    >>> a = np.array([np.NA, np.NA], dtype='f8', maskna=True)
    >>> np.sum(a, skipna=True)
    0.0
    >>> np.max(a, skipna=True)
    array(NA, dtype='<f8', maskna=True)
    >>> np.mean(a)
    NA(dtype='<f8')
    >>> np.mean(a, skipna=True)
    /home/mwiebe/virtualenvs/dev/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars
      return mean(axis, dtype, out)
    nan

The functions 'np.any' and 'np.all' require some special consideration, just as logical\_and and logical\_or do. Maybe the best way to describe their behavior is through a series of examples:

    >>> np.any(np.array([False, False, False], maskna=True))
    False
    >>> np.any(np.array([False, np.NA, False], maskna=True))
    NA
    >>> np.any(np.array([False, np.NA, True], maskna=True))
    True
    
    >>> np.all(np.array([True, True, True], maskna=True))
    True
    >>> np.all(np.array([True, np.NA, True], maskna=True))
    NA
    >>> np.all(np.array([False, np.NA, True], maskna=True))
    False

Since 'np.any' is the reduction for 'np.logical\_or', and 'np.all' is the reduction for 'np.logical\_and', it makes sense for them to have a 'skipna=' parameter like the other similar reduction functions.

### Parameterized NA data types

A masked array isn't the only way to deal with missing data, and some systems deal with the problem by defining a special "NA" value, for data which is missing. This is distinct from NaN floating point values, which are the result of bad floating point calculation values, but many people use NaNs for this purpose.

In the case of IEEE floating point values, it is possible to use a particular NaN value, of which there are many, for "NA", distinct from NaN. For signed integers, a reasonable approach would be to use the minimum storable value, which doesn't have a corresponding positive value. For unsigned integers, the maximum storage value seems most reasonable.

With the goal of providing a general mechanism, a parameterized type mechanism for this is much more attractive than creating separate nafloat32, nafloat64, naint64, nauint64, etc dtypes. If this is viewed as an alternative way of treating the mask except without value preservation, this parameterized type can work together with the mask in a special way to produce a value + mask combination on the fly, and use the exact same computational infrastructure as the masked array system. This allows one to avoid the need to write special case code for each ufunc and for each na\* dtype, something that is hard to avoid when building a separate independent dtype implementation for each na\* dtype.

Reliable conversions with the NA bitpattern preserved across primitive types requires consideration as well. Even in the simple case of double -\> float, where this is supported by hardware, the NA value will get lost because the NaN payload is typically not preserved. The ability to have different bit masks specified for the same underlying type also needs to convert properly. With a well-defined interface converting to/from a (value,flag) pair, this becomes straightforward to support generically.

This approach also provides some opportunities for some subtle variations with IEEE floats. By default, one exact bit-pattern, a silent NaN with a payload that won't be generated by hardware floating point operations, would be used. The choice R has made could be this default.

Additionally, it might be nice to sometimes treat all NaNs as missing values. This requires a slightly more complex mapping to convert the floating point values into mask/value combinations, and converting back would always produce the default NaN used by NumPy. Finally, treating both NaNs and Infs as missing values would be just a slight variation of the NaN version.

Strings require a slightly different handling, because they may be any size. One approach is to use a one-character signal consisting of one of the first 32 ASCII/unicode values. There are many possible values to use here, like 0x15 'Negative Acknowledgement' or 0x10 'Data Link Escape'.

The Object dtype has an obvious signal, the np.NA singleton itself. Any dtype with object semantics won't be able to have this customized, since specifying bit patterns applies only to plain binary data, not data with object semantics of construction and destructions.

Struct dtypes are more of a core primitive dtype, in the same fashion that this parameterized NA-capable dtype is. It won't be possible to put these as the parameter for the parameterized NA-dtype.

The dtype names would be parameterized similar to how the datetime64 is parameterized by the metadata unit. What name to use may require some debate, but "NA" seems like a reasonable choice. With the default missing value bit-pattern, these dtypes would look like np.dtype('NA\[float32\]'), np.dtype('NA\[f8\]'), or np.dtype('NA\[i64\]').

To override the bit pattern that signals a missing value, a raw value in the format of a hexadecimal unsigned integer can be given, and in the above special cases for floating point, special strings can be provided. The defaults for some cases, written explicitly in this form, are then:

    np.dtype('NA[?,0x02]')
    np.dtype('NA[i4,0x80000000]')
    np.dtype('NA[u4,0xffffffff]')
    np.dtype('NA[f4,0x7f8007a2')
    np.dtype('NA[f8,0x7ff00000000007a2') (R-compatible bitpattern)
    np.dtype('NA[S16,0x15]') (using the NAK character as the signal).
    
    np.dtype('NA[f8,NaN]') (for any NaN)
    np.dtype('NA[f8,InfNaN]') (for any NaN or Inf)

When no parameter is specified a flexible NA dtype is created, which itself cannot hold values, but will conform to the input types in functions like 'np.astype'. The dtype 'f8' maps to 'NA\[f8\]', and \[('a', 'f4'), ('b', 'i4')\] maps to \[('a', 'NA\[f4\]'), ('b', 'NA\[i4\]')\]. Thus, to view the memory of an 'f8' array 'arr' with 'NA\[f8\]', you can say arr.view(dtype='NA').

### Future expansion to multi-NA payloads

The packages SAS and Stata both support multiple different "NA" values. This allows one to specify different reasons for why a value, for example homework that wasn't done because the dog ate it or the student was sick. In these packages, the different NA values have a linear ordering which specifies how different NA values combine together.

In the sections on C implementation details, the mask has been designed so that a mask with a payload is a strict superset of the NumPy boolean type, and the boolean type has a payload of just zero. Different payloads combine with the 'min' operation.

The important part of future-proofing the design is making sure the C ABI-level choices and the Python API-level choices have a natural transition to multi-NA support. Here is one way multi-NA support could look:

    >>> a = np.array([np.NA(1), 3, np.NA(2)], maskna='multi')
    >>> np.sum(a)
    NA(1, dtype='<i4')
    >>> np.sum(a[1:])
    NA(2, dtype='<i4')
    >>> b = np.array([np.NA, 2, 5], maskna=True)
    >>> a + b
    array([NA(0), 5, NA(2)], maskna='multi')

The design of this NEP does not distinguish between NAs that come from an NA mask or NAs that come from an NA dtype. Both of these get treated equivalently in computations, with masks dominating over NA dtypes.:

    >>> a = np.array([np.NA, 2, 5], maskna=True)
    >>> b = np.array([1, np.NA, 7], dtype='NA')
    >>> a + b
    array([NA, NA, 12], maskna=True)

The multi-NA approach allows one to distinguish between these NAs, through assigning different payloads to the different types. If we extend the 'skipna=' parameter to accept a list of payloads in addition to True/False, one could do this:

    >>> a = np.array([np.NA(1), 2, 5], maskna='multi')
    >>> b = np.array([1, np.NA(0), 7], dtype='NA[f4,multi]')
    >>> a + b
    array([NA(1), NA(0), 12], maskna='multi')
    >>> np.sum(a, skipna=0)
    NA(1, dtype='<i4')
    >>> np.sum(a, skipna=1)
    7
    >>> np.sum(b, skipna=0)
    8
    >>> np.sum(b, skipna=1)
    NA(0, dtype='<f4')
    >>> np.sum(a+b, skipna=(0,1))
    12

### Differences with numpy.ma

The computational model that numpy.ma uses does not strictly adhere to either the NA or the IGNORE model. This section exhibits some examples of how these differences affect simple computations. This information will be very important for helping users navigate between the systems, so a summary probably should be put in a table in the documentation.:

    >>> a = np.random.random((3, 2))
    >>> mask = [[False, True], [True, True], [False, False]]
    >>> b1 = np.ma.masked_array(a, mask=mask)
    >>> b2 = a.view(maskna=True)
    >>> b2[mask] = np.NA
    
    >>> b1
    masked_array(data =
     [[0.110804969841 --]
     [-- --]
     [0.955128477746 0.440430735546]],
                 mask =
     [[False  True]
     [ True  True]
     [False False]],
           fill_value = 1e+20)
    >>> b2
    array([[0.110804969841, NA],
           [NA, NA],
           [0.955128477746, 0.440430735546]],
           maskna=True)
    
    >>> b1.mean(axis=0)
    masked_array(data = [0.532966723794 0.440430735546],
                 mask = [False False],
           fill_value = 1e+20)
    
    >>> b2.mean(axis=0)
    array([NA, NA], dtype='<f8', maskna=True)
    >>> b2.mean(axis=0, skipna=True)
    array([0.532966723794 0.440430735546], maskna=True)

For functions like np.mean, when 'skipna=True', the behavior for all NAs is consistent with an empty array:

    >>> b1.mean(axis=1)
    masked_array(data = [0.110804969841 -- 0.697779606646],
                 mask = [False  True False],
           fill_value = 1e+20)
    
    >>> b2.mean(axis=1)
    array([NA, NA, 0.697779606646], maskna=True)
    >>> b2.mean(axis=1, skipna=True)
    RuntimeWarning: invalid value encountered in double_scalars
    array([0.110804969841, nan, 0.697779606646], maskna=True)
    
    >>> np.mean([])
    RuntimeWarning: invalid value encountered in double_scalars
    nan

In particular, note that numpy.ma generally skips masked values, except returns masked when all the values are masked, while the 'skipna=' parameter returns zero when all the values are NA, to be consistent with the result of np.sum(\[\]):

    >>> b1[1]
    masked_array(data = [-- --],
                 mask = [ True  True],
           fill_value = 1e+20)
    >>> b2[1]
    array([NA, NA], dtype='<f8', maskna=True)
    >>> b1[1].sum()
    masked
    >>> b2[1].sum()
    NA(dtype='<f8')
    >>> b2[1].sum(skipna=True)
    0.0
    
    >>> np.sum([])
    0.0

### Boolean indexing

Indexing using a boolean array containing NAs does not have a consistent interpretation according to the NA abstraction. For example:

    >>> a = np.array([1, 2])
    >>> mask = np.array([np.NA, True], maskna=True)
    >>> a[mask]
    What should happen here?

Since the NA represents a valid but unknown value, and it is a boolean, it has two possible underlying values:

    >>> a[np.array([True, True])]
    array([1, 2])
    >>> a[np.array([False, True])]
    array([2])

The thing which changes is the length of the output array, nothing which itself can be substituted for NA. For this reason, at least initially, NumPy will raise an exception for this case.

Another possibility is to add an inconsistency, and follow the approach R uses. That is, to produce the following:

    >>> a[mask]
    array([NA, 2], maskna=True)

If, in user testing, this is found necessary for pragmatic reasons, the feature should be added even though it is inconsistent.

### PEP 3118

PEP 3118 doesn't have any mask mechanism, so arrays with masks will not be accessible through this interface. Similarly, it doesn't support the specification of dtypes with NA or IGNORE bitpatterns, so the parameterized NA dtypes will also not be accessible through this interface.

If NumPy did allow access through PEP 3118, this would circumvent the missing value abstraction in a very damaging way. Other libraries would try to use masked arrays, and silently get access to the data without also getting access to the mask or being aware of the missing value abstraction the mask and data together are following.

### Cython

Cython uses PEP 3118 to work with NumPy arrays, so currently it will simply refuse to work with them as described in the "PEP 3118" section.

In order to properly support NumPy missing values, Cython will need to be modified in some fashion to add this support. Likely the best way to do this will be to include it with supporting np.nditer, which is most likely going to have an enhancement to make writing missing value algorithms easier.

### Hard masks

The numpy.ma implementation has a "hardmask" feature, which prevents values from ever being unmasked by assigning a value. This would be an internal array flag, named something like 'arr.flags.hardmask'.

If the hardmask feature is implemented, boolean indexing could return a hardmasked array instead of a flattened array with the arbitrary choice of C-ordering as it currently does. While this improves the abstraction of the array significantly, it is not a compatible change.

### Shared masks

One feature of numpy.ma is called 'shared masks'.

<https://docs.scipy.org/doc/numpy/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray.sharedmask>

This feature cannot be supported by a masked implementation of missing values without directly violating the missing value abstraction. If the same mask memory is shared between two arrays 'a' and 'b', assigning a value to a masked element in 'a' will simultaneously unmask the element with matching index in 'b'. Because this isn't at the same time assigning a valid value to that element in 'b', this has violated the abstraction. For this reason, shared masks will not be supported by the mask-based missing value implementation.

This is slightly different from what happens when taking a view of an array with masked missing value support, where a view of both the mask and the data are taken simultaneously. The result is two views which share the same mask memory and the same data memory, which still preserves the missing value abstraction.

### Interaction with pre-existing C API usage

Making sure existing code using the C API, whether it's written in C, C++, or Cython, does something reasonable is an important goal of this implementation. The general strategy is to make existing code which does not explicitly tell numpy it supports NA masks fail with an exception saying so. There are a few different access patterns people use to get ahold of the numpy array data, here we examine a few of them to see what numpy can do. These examples are found from doing google searches of numpy C API array access.

#### NumPy documentation - how to extend NumPy

<https://docs.scipy.org/doc/numpy/user/c-info.how-to-extend.html#dealing-with-array-objects>

This page has a section "Dealing with array objects" which has some advice for how to access numpy arrays from C. When accepting arrays, the first step it suggests is to use PyArray\_FromAny or a macro built on that function, so code following this advice will properly fail when given an NA-masked array it doesn't know how to handle.

The way this is handled is that PyArray\_FromAny requires a special flag, NPY\_ARRAY\_ALLOWNA, before it will allow NA-masked arrays to flow through.

<https://docs.scipy.org/doc/numpy/reference/c-api.array.html#NPY_ARRAY_ALLOWNA>

Code which does not follow this advice, and instead just calls PyArray\_Check() to verify it is an ndarray and checks some flags, will silently produce incorrect results. This style of code does not provide any opportunity for numpy to say "hey, this array is special", so also is not compatible with future ideas of lazy evaluation, derived dtypes, etc.

#### Tutorial from Cython website

<http://docs.cython.org/src/tutorial/numpy.html>

This tutorial gives a convolution example, and all the examples fail with Python exceptions when given inputs that contain NA values.

Before any Cython type annotation is introduced, the code functions just as equivalent Python would in the interpreter.

When the type information is introduced, it is done via numpy.pxd which defines a mapping between an ndarray declaration and PyArrayObject \*. Under the hood, this maps to \_\_Pyx\_ArgTypeTest, which does a direct comparison of Py\_TYPE(obj) against the PyTypeObject for the ndarray.

Then the code does some dtype comparisons, and uses regular python indexing to access the array elements. This python indexing still goes through the Python API, so the NA handling and error checking in numpy still can work like normal and fail if the inputs have NAs which cannot fit in the output array. In this case it fails when trying to convert the NA into an integer to set in the output.

The next version of the code introduces more efficient indexing. This operates based on Python's buffer protocol. This causes Cython to call \_\_Pyx\_GetBufferAndValidate, which calls \_\_Pyx\_GetBuffer, which calls PyObject\_GetBuffer. This call gives numpy the opportunity to raise an exception if the inputs are arrays with NA-masks, something not supported by the Python buffer protocol.

#### Numerical Python - JPL website

<http://dsnra.jpl.nasa.gov/software/Python/numpydoc/numpy-13.html>

This document is from 2001, so does not reflect recent numpy, but it is the second hit when searching for "numpy c api example" on google.

There first example, heading "A simple example", is in fact already invalid for recent numpy even without the NA support. In particular, if the data is misaligned or in a different byteorder, it may crash or produce incorrect results.

The next thing the document does is introduce PyArray\_ContiguousFromObject, which gives numpy an opportunity to raise an exception when NA-masked arrays are used, so the later code will raise exceptions as desired.

## C implementation details

The first version to implement is the array masks, because it is the more general approach. The mask itself is an array, but since it is intended to never be directly accessible from Python, it won't be a full ndarray itself. The mask always has the same shape as the array it is attached to, so it doesn't need its own shape. For an array with a struct dtype, however, the mask will have a different dtype than just a straight bool, so it does need its own dtype. This gives us the following additions to the PyArrayObject:

``` c
/*
 * Descriptor for the mask dtype.
 *   If no mask: NULL
 *   If mask   : bool/uint8/structured dtype of mask dtypes
 */
PyArray_Descr *maskna_dtype;
/*
 * Raw data buffer for mask. If the array has the flag
 * NPY_ARRAY_OWNMASKNA enabled, it owns this memory and
 * must call PyArray_free on it when destroyed.
 */
npy_mask *maskna_data;
/*
 * Just like dimensions and strides point into the same memory
 * buffer, we now just make the buffer 3x the nd instead of 2x
 * and use the same buffer.
 */
npy_intp *maskna_strides;
```

These fields can be accessed through the inline functions:

``` c
PyArray_Descr *
PyArray_MASKNA_DTYPE(PyArrayObject *arr);

npy_mask *
PyArray_MASKNA_DATA(PyArrayObject *arr);

npy_intp *
PyArray_MASKNA_STRIDES(PyArrayObject *arr);

npy_bool
PyArray_HASMASKNA(PyArrayObject *arr);
```

There are 2 or 3 flags which must be added to the array flags, both for requesting NA masks and for testing for them:

``` c
NPY_ARRAY_MASKNA
NPY_ARRAY_OWNMASKNA
/* To possibly add in a later revision */
NPY_ARRAY_HARDMASKNA
```

To allow the easy detection of NA support, and whether an array has any missing values, we add the following functions:

  - PyDataType\_HasNASupport(PyArray\_Descr\* dtype)  
    Returns true if this is an NA dtype, or a struct dtype where every field has NA support.

  - PyArray\_HasNASupport(PyArrayObject\* obj)  
    Returns true if the array dtype has NA support, or the array has an NA mask.

  - PyArray\_ContainsNA(PyArrayObject\* obj)  
    Returns false if the array has no NA support. Returns true if the array has NA support AND there is an NA anywhere in the array.

  - int PyArray\_AllocateMaskNA(PyArrayObject\* arr, npy\_bool ownmaskna, npy\_bool multina)  
    Allocates an NA mask for the array, ensuring ownership if requested and using NPY\_MASK instead of NPY\_BOOL for the dtype if multina is True.

### Mask binary format

The format of the mask itself is designed to indicate whether an element is masked or not, as well as contain a payload so that multiple different NAs with different payloads can be used in the future. Initially, we will simply use the payload 0.

The mask has type npy\_uint8, and bit 0 is used to indicate whether a value is masked. If ((m&0x01) == 0), the element is masked, otherwise it is unmasked. The rest of the bits are the payload, which is (m\>\>1). The convention for combining masks with payloads is that smaller payloads propagate. This design gives 128 payload values to masked elements, and 128 payload values to unmasked elements.

The big benefit of this approach is that npy\_bool also works as a mask, because it takes on the values 0 for False and 1 for True. Additionally, the payload for npy\_bool, which is always zero, dominates over all the other possible payloads.

Since the design involves giving the mask its own dtype, we can distinguish between masking with a single NA value (npy\_bool mask), and masking with multi-NA (npy\_uint8 mask). Initial implementations will just support the npy\_bool mask.

An idea that was discarded is to allow the combination of masks + payloads to be a simple 'min' operation. This can be done by putting the payload in bits 0 through 6, so that the payload is (m&0x7f), and using bit 7 for the masking flag, so ((m&0x80) == 0) means the element is masked. The fact that this makes masks completely different from booleans, instead of a strict superset, is the primary reason this choice was discarded.

## C iterator API changes: iteration with masks

For iteration and computation with masks, both in the context of missing values and when the mask is used like the 'where=' parameter in ufuncs, extending the nditer is the most natural way to expose this functionality.

Masked operations need to work with casting, alignment, and anything else which causes values to be copied into a temporary buffer, something which is handled nicely by the nditer but difficult to do outside that context.

First we describe iteration designed for use of masks outside the context of missing values, then the features which include missing value support.

### Iterator mask features

We add several new per-operand flags:

  - NPY\_ITER\_WRITEMASKED  
    Indicates that any copies done from a buffer to the array are masked. This is necessary because READWRITE mode could destroy data if a float array was being treated like an int array, so copying to the buffer and back would truncate to integers. No similar flag is provided for reading, because it may not be possible to know the mask ahead of time, and copying everything into the buffer will never destroy data.
    
    The code using the iterator should only write to values which are not masked by the mask specified, otherwise the result will be different depending on whether buffering is enabled or not.

  - NPY\_ITER\_ARRAYMASK  
    Indicates that this array is a boolean mask to use when copying any WRITEMASKED argument from a buffer back to the array. There can be only one such mask, and there cannot also be a virtual mask.
    
    As a special case, if the flag NPY\_ITER\_USE\_MASKNA is specified at the same time, the mask for the operand is used instead of the operand itself. If the operand has no mask but is based on an NA dtype, that mask exposed by the iterator converts into the NA bitpattern when copying from the buffer to the array.

  - NPY\_ITER\_VIRTUAL  
    Indicates that this operand is not an array, but rather created on the fly for the inner iteration code. This allocates enough buffer space for the code to read/write data, but does not have an actual array backing the data. When combined with NPY\_ITER\_ARRAYMASK, allows for creating a "virtual mask", specifying which values are unmasked without ever creating a full mask array.

### Iterator NA-array features

We add several new per-operand flags:

  - NPY\_ITER\_USE\_MASKNA  
    If the operand has an NA dtype, an NA mask, or both, this adds a new virtual operand to the end of the operand list which iterates over the mask for the particular operand.

  - NPY\_ITER\_IGNORE\_MASKNA  
    If an operand has an NA mask, by default the iterator will raise an exception unless NPY\_ITER\_USE\_MASKNA is specified. This flag disables that check, and is intended for cases where one has first checked that all the elements in the array are not NA using the PyArray\_ContainsNA function.
    
    If the dtype is an NA dtype, this also strips the NA-ness from the dtype, showing a dtype that does not support NA.

## Rejected alternative

### Parameterized data type which adds additional memory for the NA flag

Another alternative to having a separate mask added to the array is to introduced a parameterized type, which takes a primitive dtype as an argument. The dtype "i8" would turn into "maybe\[i8\]", and a byte flag would be appended to the dtype to indicate whether the value was NA or not.

This approach adds memory overhead greater or equal to keeping a separate mask, but has better locality. To keep the dtype aligned, an 'i8' would need to have 16 bytes to retain proper alignment, a 100% overhead compared to 12.5% overhead for a separately kept mask.

## Acknowledgments

In addition to feedback from Travis Oliphant and others at Enthought, this NEP has been revised based on a great deal of feedback from the NumPy-Discussion mailing list. The people participating in the discussion are:

  - Nathaniel Smith

  - Robert Kern

  - Charles Harris

  - Gael Varoquaux

  - Eric Firing

  - Keith Goodman

  - Pierre GM

  - Christopher Barker

  - Josef Perktold

  - Ben Root

  - Laurent Gautier

  - Neal Becker

  - Bruce Southey

  - Matthew Brett

  - Wes McKinney

  - LluÃ­s

  - Olivier Delalleau

  - Alan G Isaac

  - 5.  Antero Tammi

  - Jason Grout

  - Dag Sverre Seljebotn

  - Joe Harrington

  - Gary Strangman

  - Chris Jordan-Squire

  - Peter

I apologize if I missed anyone.

---

nep-0013-ufunc-overrides.md

---

# NEP 13 â€” A mechanism for overriding Ufuncs

<div class="currentmodule">

numpy

</div>

  - Author  
    Blake Griffith

  - Contact  
    <blake.g@utexas.edu>

  - Date  
    2013-07-10

  - Author  
    Pauli Virtanen

  - Author  
    Nathaniel Smith

  - Author  
    Marten van Kerkwijk

  - Author  
    Stephan Hoyer

  - Date  
    2017-03-31

  - Status  
    Final

  - Updated  
    2023-02-19

  - Author  
    Roy Smart

## Executive summary

NumPy's universal functions (ufuncs) currently have some limited functionality for operating on user defined subclasses of <span class="title-ref">ndarray</span> using `__array_prepare__` and `__array_wrap__` \[1\], and there is little to no support for arbitrary objects. e.g. SciPy's sparse matrices\[2\]\[3\].

Here we propose adding a mechanism to override ufuncs based on the ufunc checking each of it's arguments for a `__array_ufunc__` method. On discovery of `__array_ufunc__` the ufunc will hand off the operation to the method.

This covers some of the same ground as Travis Oliphant's proposal to retro-fit NumPy with multi-methods\[4\], which would solve the same problem. The mechanism here follows more closely the way Python enables classes to override `__mul__` and other binary operations. It also specifically addresses how binary operators and ufuncs should interact. (Note that in earlier iterations, the override was called `__numpy_ufunc__`. An implementation was made, but had not quite the right behaviour, hence the change in name.)

The `__array_ufunc__` as described below requires that any corresponding Python binary operations (`__mul__` et al.) should be implemented in a specific way and be compatible with NumPy's ndarray semantics. Objects that do not satisfy this cannot override any NumPy ufuncs. We do not specify a future-compatible path by which this requirement can be relaxed --- any changes here require corresponding changes in 3rd party code.

## Motivation

The current machinery for dispatching Ufuncs is generally agreed to be insufficient. There have been lengthy discussions and other proposed solutions\[5\],\[6\].

Using ufuncs with subclasses of <span class="title-ref">ndarray</span> is limited to `__array_prepare__` and `__array_wrap__` to prepare the output arguments, but these don't allow you to for example change the shape or the data of the arguments. Trying to ufunc things that don't subclass <span class="title-ref">ndarray</span> is even more difficult, as the input arguments tend to be cast to object arrays, which ends up producing surprising results.

Take this example of ufuncs interoperability with sparse matrices.:

    In [1]: import numpy as np
    import scipy.sparse as sp
    
    a = np.random.randint(5, size=(3,3))
    b = np.random.randint(5, size=(3,3))
    
    asp = sp.csr_matrix(a)
    bsp = sp.csr_matrix(b)
    
    In [2]: a, b
    Out[2]:(array([[0, 4, 4],
                   [1, 3, 2],
                   [1, 3, 1]]),
            array([[0, 1, 0],
                   [0, 0, 1],
                   [4, 0, 1]]))
    
    In [3]: np.multiply(a, b) # The right answer
    Out[3]: array([[0, 4, 0],
                   [0, 0, 2],
                   [4, 0, 1]])
    
    In [4]: np.multiply(asp, bsp).todense() # calls __mul__ which does matrix multi
    Out[4]: matrix([[16,  0,  8],
                    [ 8,  1,  5],
                    [ 4,  1,  4]], dtype=int64)
    
    In [5]: np.multiply(a, bsp) # Returns NotImplemented to user, bad!
    Out[5]: NotImplemented

Returning `NotImplemented` to user should not happen. Moreover:

    In [6]: np.multiply(asp, b)
    Out[6]: array([[ <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>],
                       [ <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>],
                       [ <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>]], dtype=object)

Here, it appears that the sparse matrix was converted to an object array scalar, which was then multiplied with all elements of the `b` array. However, this behavior is more confusing than useful, and having a <span class="title-ref">TypeError</span> would be preferable.

This proposal will *not* resolve the issue with scipy.sparse matrices, which have multiplication semantics incompatible with NumPy arrays. However, the aim is to enable writing other custom array types that have strictly ndarray compatible semantics.

## Proposed interface

The standard array class <span class="title-ref">ndarray</span> gains an `__array_ufunc__` method and objects can override Ufuncs by overriding this method (if they are <span class="title-ref">ndarray</span> subclasses) or defining their own. The method signature is:

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs)

Here:

  - *ufunc* is the ufunc object that was called.
  - *method* is a string indicating how the Ufunc was called, either `"__call__"` to indicate it was called directly, or one of its methods: `"reduce"`, `"accumulate"`, `"reduceat"`, `"outer"`, or `"at"`.
  - *inputs* is a tuple of the input arguments to the `ufunc`
  - *kwargs* contains any optional or keyword arguments passed to the function. This includes any `out` arguments, which are always contained in a tuple.

Hence, the arguments are normalized: only the required input arguments (`inputs`) are passed on as positional arguments, all the others are passed on as a dict of keyword arguments (`kwargs`). In particular, if there are output arguments, positional are otherwise, that are not `None`, they are passed on as a tuple in the `out` keyword argument (even for the `reduce`, `accumulate`, and `reduceat` methods where in all current cases only a single output makes sense).

The function dispatch proceeds as follows:

  - If one of the input, output, or `where` arguments implements `__array_ufunc__`, it is executed instead of the ufunc.
  - If more than one of the arguments implements `__array_ufunc__`, they are tried in the following order: subclasses before superclasses, inputs before outputs, outputs before `where`, otherwise left to right.
  - The first `__array_ufunc__` method returning something else than `NotImplemented` determines the return value of the Ufunc.
  - If all `__array_ufunc__` methods of the input arguments return `NotImplemented`, a <span class="title-ref">TypeError</span> is raised.
  - If a `__array_ufunc__` method raises an error, the error is propagated immediately.
  - If none of the input arguments had an `__array_ufunc__` method, the execution falls back on the default ufunc behaviour.

In the above, there is one proviso: if a class has an `__array_ufunc__` attribute but it is identical to `ndarray.__array_ufunc__`, the attribute is ignored. This happens for instances of <span class="title-ref">ndarray</span> and for <span class="title-ref">ndarray</span> subclasses that did not override their inherited `__array_ufunc__` implementation.

### Type casting hierarchy

The Python operator override mechanism gives much freedom in how to write the override methods, and it requires some discipline in order to achieve predictable results. Here, we discuss an approach for understanding some of the implications, which can provide input in the design.

It is useful to maintain a clear idea of what types can be "upcast" to others, possibly indirectly (e.g. indirect A-\>B-\>C is implemented but direct A-\>C not). If the implementations of `__array_ufunc__` follow a coherent type casting hierarchy, it can be used to understand results of operations.

Type casting can be expressed as a [graph](https://en.wikipedia.org/wiki/Graph_theory) defined as follows:

> For each `__array_ufunc__` method, draw directed edges from each possible input type to each possible output type.
> 
> That is, in each case where `y = x.__array_ufunc__(a, b, c, ...)` does something else than returning `NotImplemented` or raising an error, draw edges `type(a) -> type(y)`, `type(b) -> type(y)`, ...

If the resulting graph is *acyclic*, it defines a coherent type casting hierarchy (unambiguous partial ordering between types). In this case, operations involving multiple types generally predictably produce result of the "highest" type, or raise a <span class="title-ref">TypeError</span>. See examples at the end of this section.

If the graph has cycles, the `__array_ufunc__` type casting is not well-defined, and things such as `type(multiply(a, b)) != type(multiply(b, a))` or `type(add(a, add(b, c))) != type(add(add(a, b), c))` are not excluded (and then probably always possible).

If the type casting hierarchy is well defined, for each class A, all other classes that define `__array_ufunc__` belong to exactly one of three groups:

  - *Above A*: the types that A can be (indirectly) upcast to in ufuncs.
  - *Below A*: the types that can be (indirectly) upcast to A in ufuncs.
  - *Incompatible*: neither above nor below A; types for which no (indirect) upcasting is possible.

Note that the legacy behaviour of NumPy ufuncs is to try to convert unknown objects to <span class="title-ref">ndarray</span> via <span class="title-ref">np.asarray</span>. This is equivalent to placing <span class="title-ref">ndarray</span> above these objects in the graph. Since we above defined <span class="title-ref">ndarray</span> to return <span class="title-ref">NotImplemented</span> for classes with custom `__array_ufunc__`, this puts <span class="title-ref">ndarray</span> below such classes in the type hierarchy, allowing the operations to be overridden.

In view of the above, binary ufuncs describing transitive operations should aim to define a well-defined casting hierarchy. This is likely also a sensible approach to all ufuncs --- exceptions to this should consider carefully if any surprising behavior results.

<div class="admonition">

Example

Type casting hierarchy.

![image](nep0013_image1.png)

The `__array_ufunc__` of type A can handle ndarrays returning C, B can handle ndarray and D returning B, and C can handle A and B returning C, but not ndarrays or D. The result is a directed acyclic graph, and defines a type casting hierarchy, with relations `C > A`, `C > ndarray`, `C > B > ndarray`, `C > B > D`. The type A is incompatible with B, D, ndarray, and D is incompatible with A and ndarray. Ufunc expressions involving these classes should produce results of the highest type involved or raise a <span class="title-ref">TypeError</span>.

</div>

<div class="admonition">

Example

One-cycle in the `__array_ufunc__` graph.

![image](nep0013_image2.png)

In this case, the `__array_ufunc__` relations have a cycle of length 1, and a type casting hierarchy does not exist. Binary operations are not commutative: `type(a + b) is A` but `type(b + a) is B`.

</div>

<div class="admonition">

Example

Longer cycle in the `__array_ufunc__` graph.

![image](nep0013_image3.png)

In this case, the `__array_ufunc__` relations have a longer cycle, and a type casting hierarchy does not exist. Binary operations are still commutative, but type transitivity is lost: `type(a + (b + c)) is A` but `type((a + b) + c) is C`.

</div>

### Subclass hierarchies

Generally, it is desirable to mirror the class hierarchy in the ufunc type casting hierarchy. The recommendation is that an `__array_ufunc__` implementation of a class should generally return <span class="title-ref">NotImplemented</span> unless the inputs are instances of the same class or superclasses. This guarantees that in the type casting hierarchy, superclasses are below, subclasses above, and other classes are incompatible. Exceptions to this need to check they respect the implicit type casting hierarchy.

\> **Note** \> Note that type casting hierarchy and class hierarchy are here defined to go the "opposite" directions. It would in principle also be consistent to have `__array_ufunc__` handle also instances of subclasses. In this case, the "subclasses first" dispatch rule would ensure a relatively similar outcome. However, the behavior is then less explicitly specified.

Subclasses can be easily constructed if methods consistently use <span class="title-ref">super</span> to pass through the class hierarchy\[7\]. To support this, <span class="title-ref">ndarray</span> has its own `__array_ufunc__` method, equivalent to:

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        # Cannot handle items that have __array_ufunc__ (other than our own).
        outputs = kwargs.get('out', ())
        objs = inputs + outputs
        if "where" in kwargs:
            objs = objs + (kwargs["where"], )
        for item in objs:
            if (hasattr(item, '__array_ufunc__') and
                    type(item).__array_ufunc__ is not ndarray.__array_ufunc__):
                return NotImplemented
    
        # If we didn't have to support legacy behaviour (__array_prepare__,
        # __array_wrap__, etc.), we might here convert python floats,
        # lists, etc, to arrays with
        # items = [np.asarray(item) for item in inputs]
        # and then start the right iterator for the given method.
        # However, we do have to support legacy, so call back into the ufunc.
        # Its arguments are now guaranteed not to have __array_ufunc__
        # overrides, and it will do the coercion to array for us.
        return getattr(ufunc, method)(*items, **kwargs)

Note that, as a special case, the ufunc dispatch mechanism does not call this <span class="title-ref">ndarray.\_\_array\_ufunc\_\_</span> method, even for <span class="title-ref">ndarray</span> subclasses if they have not overridden the default <span class="title-ref">ndarray</span> implementation. As a consequence, calling <span class="title-ref">ndarray.\_\_array\_ufunc\_\_</span> will not result to a nested ufunc dispatch cycle.

The use of <span class="title-ref">super</span> should be particularly useful for subclasses of <span class="title-ref">ndarray</span> that only add an attribute like a unit. In their <span class="title-ref">\_\_array\_ufunc\_\_</span> implementation, such classes can do possible adjustment of the arguments relevant to their own class, and pass on to the superclass implementation using <span class="title-ref">super</span> until the ufunc is actually done, and then do possible adjustments of the outputs.

In general, custom implementations of <span class="title-ref">\_\_array\_ufunc\_\_</span> should avoid nested dispatch cycles, where one not just calls the ufunc via `getattr(ufunc, method)(*items, **kwargs)`, but catches possible exceptions, etc. As always, there may be exceptions. For instance, for a class like <span class="title-ref">MaskedArray</span>, which only cares that whatever it contains is an <span class="title-ref">ndarray</span> subclass, a reimplementation with `__array_ufunc__` may well be more easily done by directly applying the ufunc to its data, and then adjusting the mask. Indeed, one can think of this as part of the class determining whether it can handle the other argument (i.e., where in the type hierarchy it sits). In this case, one should return `NotImplemented` if the trial fails. So, the implementation would be something like:

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        # for simplicity, outputs are ignored here.
        unmasked_items = tuple((item.data if isinstance(item, MaskedArray)
                                else item) for item in inputs)
        try:
            unmasked_result = getattr(ufunc, method)(*unmasked_items, **kwargs)
        except TypeError:
            return NotImplemented
        # for simplicity, ignore that unmasked_result could be a tuple
        # or a scalar.
        if not isinstance(unmasked_result, np.ndarray):
            return NotImplemented
        # now combine masks and view as MaskedArray instance
        ...

As a specific example, consider a quantity and a masked array class which both override `__array_ufunc__`, with specific instances `q` and `ma`, where the latter contains a regular array. Executing `np.multiply(q, ma)`, the ufunc will first dispatch to `q.__array_ufunc__`, which returns `NotImplemented` (since the quantity class turns itself into an array and calls <span class="title-ref">super</span>, which passes on to `ndarray.__array_ufunc__`, which sees the override on `ma`). Next, `ma.__array_ufunc__` gets a chance. It does not know quantity, and if it were to just return `NotImplemented` as well, an <span class="title-ref">TypeError</span> would result. But in our sample implementation, it uses `getattr(ufunc, method)` to, effectively, evaluate `np.multiply(q, ma.data)`. This again will pass to `q.__array_ufunc__`, but this time, since `ma.data` is a regular array, it will return a result that is also a quantity. Since this is a subclass of <span class="title-ref">ndarray</span>, `ma.__array_ufunc__` can turn this into a masked array and thus return a result (obviously, if it was not a array subclass, it could still return `NotImplemented`).

Note that in the context of the type hierarchy discussed above this is a somewhat tricky example, since <span class="title-ref">MaskedArray</span> has a strange position: it is above all subclasses of <span class="title-ref">ndarray</span>, in that it can cast them to its own type, but it does not itself know how to interact with them in ufuncs.

### Turning Ufuncs off

For some classes, Ufuncs make no sense, and, like for some other special methods such as `__hash__` and `__iter__`\[8\], one can indicate Ufuncs are not available by setting `__array_ufunc__` to `None`. If a Ufunc is called on any operand that sets `__array_ufunc__ = None`, it will unconditionally raise <span class="title-ref">TypeError</span>.

In the type casting hierarchy, this makes it explicit that the type is incompatible relative to <span class="title-ref">ndarray</span>.

### Behavior in combination with Python's binary operations

The Python operator override mechanism in <span class="title-ref">ndarray</span> is coupled to the `__array_ufunc__` mechanism. For the special methods calls such as `ndarray.__mul__(self, other)` that Python calls for implementing binary operations such as `*` and `+`, NumPy's <span class="title-ref">ndarray</span> implements the following behavior:

  - If `other.__array_ufunc__ is None`, <span class="title-ref">ndarray</span> returns `NotImplemented`. Control reverts to Python, which in turn will try calling a corresponding reflexive method on `other` (e.g., `other.__rmul__`), if present.
  - If the `__array_ufunc__` attribute is absent on `other` and `other.__array_priority__ > self.__array_priority__`, <span class="title-ref">ndarray</span> also returns `NotImplemented` (and the logic proceeds as in the previous case). This ensures backwards compatibility with old versions of NumPy.
  - Otherwise, <span class="title-ref">ndarray</span> unilaterally calls the corresponding Ufunc. Ufuncs never return `NotImplemented`, so **reflexive methods such as** `other.__rmul__` **cannot be used to override arithmetic with NumPy arrays if** `__array_ufunc__` **is set** to any value other than `None`. Instead, their behavior needs to be changed by implementing `__array_ufunc__` in a fashion consistent with the corresponding Ufunc, e.g., `np.multiply`. See \[neps.ufunc-overrides.list-of-operators\](\#neps.ufunc-overrides.list-of-operators) for a list of affected operators and their corresponding ufuncs.

A class wishing to modify the interaction with <span class="title-ref">ndarray</span> in binary operations therefore has two options:

1.  Implement `__array_ufunc__` and follow NumPy semantics for Python binary operations (see below).
2.  Set `__array_ufunc__ = None`, and implement Python binary operations freely. In this case, ufuncs called on this argument will raise <span class="title-ref">TypeError</span> (see \[neps.ufunc-overrides.turning-ufuncs-off\](\#neps.ufunc-overrides.turning-ufuncs-off)).

### Recommendations for implementing binary operations

For most numerical classes, the easiest way to override binary operations is thus to define `__array_ufunc__` and override the corresponding Ufunc. The class can then, like <span class="title-ref">ndarray</span> itself, define the binary operators in terms of Ufuncs. Here, one has to take some care to ensure that one allows for other classes to indicate they are not compatible, i.e., implementations should be something like:

    def _disables_array_ufunc(obj):
        try:
            return obj.__array_ufunc__ is None
        except AttributeError:
            return False
    
    class ArrayLike:
        ...
        def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
            ...
            return result
    
        # Option 1: call ufunc directly
        def __mul__(self, other):
            if _disables_array_ufunc(other):
                return NotImplemented
            return np.multiply(self, other)
    
        def __rmul__(self, other):
            if _disables_array_ufunc(other):
                return NotImplemented
            return np.multiply(other, self)
    
        def __imul__(self, other):
            return np.multiply(self, other, out=(self,))
    
        # Option 2: call into one's own __array_ufunc__
        def __mul__(self, other):
            return self.__array_ufunc__(np.multiply, '__call__', self, other)
    
        def __rmul__(self, other):
            return self.__array_ufunc__(np.multiply, '__call__', other, self)
    
        def __imul__(self, other):
            result = self.__array_ufunc__(np.multiply, '__call__', self, other,
                                          out=(self,))
            if result is NotImplemented:
                raise TypeError(...)

To see why some care is necessary, consider another class `other` that does not know how to deal with arrays and ufuncs, and thus has set `__array_ufunc__` to `None`, but does know how to do multiplication:

    class MyObject:
        __array_ufunc__ = None
        def __init__(self, value):
            self.value = value
        def __repr__(self):
            return "MyObject({!r})".format(self.value)
        def __mul__(self, other):
            return MyObject(1234)
        def __rmul__(self, other):
            return MyObject(4321)

For either option above, we get the expected result:

    mine = MyObject(0)
    arr = ArrayLike([0])
    
    mine * arr    # -> MyObject(1234)
    mine *= arr   # -> MyObject(1234)
    arr * mine    # -> MyObject(4321)
    arr *= mine   # -> TypeError

Here, in the first and second example, `mine.__mul__(arr)` gets called and the result arrives immediately. In the third example, first `arr.__mul__(mine)` is called. In option (1), the check on `mine.__array_ufunc__ is None` will succeed and thus `NotImplemented` is returned, which causes `mine.__rmul__(arg)` to be executed. In option (2), it is presumably inside `arr.__array_ufunc__` that it becomes clear that the other argument cannot be dealt with, and again `NotImplemented` is returned, causing control to pass to `mine.__rmul__`.

For the fourth example, with the in-place operators, we have here followed <span class="title-ref">ndarray</span> and ensure we never return `NotImplemented`, but rather raise a <span class="title-ref">TypeError</span>. In option (1) this happens indirectly: we pass to `np.multiply`, which in turn immediately raises <span class="title-ref">TypeError</span>, because one of its operands (`out[0]`) disables Ufuncs. In option (2), we pass directly to `arr.__array_ufunc__`, which will return `NotImplemented`, which we catch.

<div class="note">

<div class="title">

Note

</div>

the reason for not allowing in-place operations to return `NotImplemented` is that these cannot generically be replaced by a simple reverse operation: most array operations assume the contents of the instance are changed in-place, and do not expect a new instance. Also, what would `ndarr[:] *= mine` imply? Assuming it means `ndarr[:] = ndarr[:] * mine`, as python does by default if the `ndarr.__imul__` were to return `NotImplemented`, is likely to be wrong.

</div>

Now consider what would happen if we had not added checks. For option (1), the relevant case is if we had not checked whether `__array_func__` was set to `None`. In the third example, `arr.__mul__(mine)` is called, and without the check, this would go to `np.multiply(arr, mine)`. This tries `arr.__array_ufunc__`, which returns `NotImplemented` and sees that `mine.__array_ufunc__ is None`, so a <span class="title-ref">TypeError</span> is raised.

For option (2), the relevant example is the fourth, with `arr *= mine`: if we had let the `NotImplemented` pass, python would have replaced this with `arr = mine.__rmul__(arr)`, which is not wanted.

Because the semantics of Ufunc overrides and Python's binary operations are nearly identical, in most cases options (1) and (2) will yield the same result with the same implementation of `__array_ufunc__`. One exception is the order in which implementations are tried when the second argument is a subclass of the first argument, due to a Python bug\[9\] expected to be fixed in Python 3.7.

In general, we recommend adopting option (1), which is the option most similar to that used by <span class="title-ref">ndarray</span> itself. Note that option (1) is viral, in the sense that any other class that wishes to support binary operations with your class now must also follow these rules for supporting binary arithmetic with <span class="title-ref">ndarray</span> (i.e., they must either implement `__array_ufunc__` or set it to `None`). We believe this is a good thing, because it ensures the consistency of ufuncs and arithmetic on all objects that support them.

To make implementing such array-like classes easier, the mixin class <span class="title-ref">\~numpy.lib.mixins.NDArrayOperatorsMixin</span> provides option (1) style overrides for all binary operators with corresponding Ufuncs. Classes that wish to implement `__array_ufunc__` for compatible versions of NumPy but that also need to support binary arithmetic with NumPy arrays on older versions should ensure that `__array_ufunc__` can also be used to implement all binary operations they support.

Finally, we note that we had extensive discussion about whether it might make more sense to ask classes like `MyObject` to implement a full `__array_ufunc__`\[10\]. In the end, allowing classes to opt out was preferred, and the above reasoning led us to agree on a similar implementation for <span class="title-ref">ndarray</span> itself. The opt-out mechanism requires disabling Ufuncs so a class cannot define a Ufuncs to return a different result than the corresponding binary operations (i.e., if `np.add(x, y)` is defined, it should match `x + y`). Our goal was to simplify the dispatch logic for binary operations with NumPy arrays as much as possible, by making it possible to use Python's dispatch rules or NumPy's dispatch rules, but not some mixture of both at the same time.

### List of operators and NumPy Ufuncs

Here is a full list of Python binary operators and the corresponding NumPy Ufuncs used by <span class="title-ref">ndarray</span> and \`\~numpy.lib.mixins.NDArrayOperatorsMixin\`:

| Symbol | Operator             | NumPy Ufunc(s)                                |
| ------ | -------------------- | --------------------------------------------- |
| `<`    | `lt`                 | <span class="title-ref">less</span>           |
| `<=`   | `le`                 | <span class="title-ref">less\_equal</span>    |
| `==`   | `eq`                 | <span class="title-ref">equal</span>          |
| `!=`   | `ne`                 | <span class="title-ref">not\_equal</span>     |
| `>`    | `gt`                 | <span class="title-ref">greater</span>        |
| `>=`   | `ge`                 | <span class="title-ref">greater\_equal</span> |
| `+`    | `add`                | <span class="title-ref">add</span>            |
| `-`    | `sub`                | <span class="title-ref">subtract</span>       |
| `*`    | `mul`                | <span class="title-ref">multiply</span>       |
| `/`    | `truediv` (Python 3) | <span class="title-ref">true\_divide</span>   |
| `/`    | `div` (Python 2)     | <span class="title-ref">divide</span>         |
| `//`   | `floordiv`           | <span class="title-ref">floor\_divide</span>  |
| `%`    | `mod`                | <span class="title-ref">remainder</span>      |
| NA     | `divmod`             | <span class="title-ref">divmod</span>         |
| `**`   | `pow`                | <span class="title-ref">power</span>\[11\]    |
| `<<`   | `lshift`             | <span class="title-ref">left\_shift</span>    |
| `>>`   | `rshift`             | <span class="title-ref">right\_shift</span>   |
| `&`    | `and_`               | <span class="title-ref">bitwise\_and</span>   |
| `^`    | `xor_`               | <span class="title-ref">bitwise\_xor</span>   |
| `\|`   | `or_`                | <span class="title-ref">bitwise\_or</span>    |
| `@`    | `matmul`             | Not yet implemented as a ufunc\[12\]          |

And here is the list of unary operators:

| Symbol | Operator | NumPy Ufunc(s)                                |
| ------ | -------- | --------------------------------------------- |
| `-`    | `neg`    | <span class="title-ref">negative</span>       |
| `+`    | `pos`    | <span class="title-ref">positive</span>\[13\] |
| NA     | `abs`    | <span class="title-ref">absolute</span>       |
| `~`    | `invert` | <span class="title-ref">invert</span>         |

### Future extensions to other functions

Some NumPy functions could be implemented as (generalized) Ufunc, in which case it would be possible for them to be overridden by the `__array_ufunc__` method. A prime candidate is <span class="title-ref">\~numpy.matmul</span>, which currently is not a Ufunc, but could be relatively easily be rewritten as a (set of) generalized Ufuncs. The same may happen with functions such as <span class="title-ref">\~numpy.median</span>, <span class="title-ref">\~numpy.min</span>, and <span class="title-ref">\~numpy.argsort</span>.

1.  <http://docs.python.org/doc/numpy/user/basics.subclassing.html>

2.  <https://github.com/scipy/scipy/issues/2123>

3.  <https://github.com/scipy/scipy/issues/1569>

4.  <https://technicaldiscovery.blogspot.com/2013/07/thoughts-after-scipy-2013-and-specific.html>

5.  <https://mail.python.org/pipermail/numpy-discussion/2011-June/056945.html>

6.  <https://github.com/numpy/numpy/issues/5844>

7.  <https://rhettinger.wordpress.com/2011/05/26/super-considered-super/>

8.  <https://docs.python.org/3/reference/datamodel.html#specialnames>

9.  <https://bugs.python.org/issue30140>

10. <https://github.com/numpy/numpy/issues/5844>

11. class :<span class="title-ref">ndarray</span> takes short cuts for `__pow__` for the cases where the power equals `1` (<span class="title-ref">positive</span>), `-1` (<span class="title-ref">reciprocal</span>), `2` (<span class="title-ref">square</span>), `0` (an otherwise private `_ones_like` ufunc), and `0.5` (<span class="title-ref">sqrt</span>), and the array is float or complex (or integer for square).

12. Because NumPy's <span class="title-ref">matmul</span> is not a ufunc, it is [currently not possible](https://github.com/numpy/numpy/issues/9028) to override `numpy_array @ other` with `other` taking precedence if `other` implements `__array_func__`.

13. <span class="title-ref">ndarray</span> currently does a copy instead of using this ufunc.

---

nep-0014-dropping-python2.7-proposal.md

---

# NEP 14 â€” Plan for dropping Python 2.7 support

  - Status  
    Final

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2017-November/077419.html>

The Python core team plans to stop supporting Python 2 in 2020. The NumPy project has supported both Python 2 and Python 3 in parallel since 2010, and has found that supporting Python 2 is an increasing burden on our limited resources; thus, we plan to eventually drop Python 2 support as well. Now that we're entering the final years of community-supported Python 2, the NumPy project wants to clarify our plans, with the goal of to helping our downstream ecosystem make plans and accomplish the transition with as little disruption as possible.

Our current plan is as follows.

Until **December 31, 2018**, all NumPy releases will fully support both Python2 and Python3.

Starting on **January 1, 2019**, any new feature releases will support only Python3.

The last Python2 supporting release will be designated as a long term support (LTS) release, meaning that we will continue to merge bug fixes and make bug fix releases for a longer period than usual. Specifically, it will be supported by the community until **December 31, 2019**.

On **January 1, 2020** we will raise a toast to Python2, and community support for the last Python2 supporting release will come to an end. However, it will continue to be available on PyPI indefinitely, and if any commercial vendors wish to extend the LTS support past this point then we are open to letting them use the LTS branch in the official NumPy repository to coordinate that.

If you are a NumPy user who requires ongoing Python2 support in 2020 or later, then please contact your vendor. If you are a vendor who wishes to continue to support NumPy on Python2 in 2020+, please get in touch; ideally we'd like you to get involved in maintaining the LTS before it actually hits end of life so that we can make a clean handoff.

To minimize disruption, running `pip install numpy` on Python 2 will continue to give the last working release in perpetuity, but after January 1, 2019 it may not contain the latest features, and after January 1, 2020 it may not contain the latest bug fixes.

For more information on the scientific Python ecosystem's transition to Python3 only, see the [python3-statement](https://python3statement.github.io/).

For more information on porting your code to run on Python 3, see the [python3-howto](https://docs.python.org/3/howto/pyporting.html).

---

nep-0015-merge-multiarray-umath.md

---

# NEP 15 â€” Merging multiarray and umath

  - Author  
    Nathaniel J. Smith \<<njs@pobox.com>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2018-02-22

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-June/078345.html>

## Abstract

Let's merge `numpy.core.multiarray` and `numpy.core.umath` into a single extension module, and deprecate `np.set_numeric_ops`.

## Background

Currently, numpy's core C code is split between two separate extension modules.

`numpy.core.multiarray` is built from `numpy/core/src/multiarray/*.c`, and contains the core array functionality (in particular, the `ndarray` object).

`numpy.core.umath` is built from `numpy/core/src/umath/*.c`, and contains the ufunc machinery.

These two modules each expose their own separate C API, accessed via `import_multiarray()` and `import_umath()` respectively. The idea is that they're supposed to be independent modules, with `multiarray` as a lower-level layer with `umath` built on top. In practice this has turned out to be problematic.

First, the layering isn't perfect: when you write `ndarray + ndarray`, this invokes `ndarray.__add__`, which then calls the ufunc `np.add`. This means that `ndarray` needs to know about ufuncs â€“ so instead of a clean layering, we have a circular dependency. To solve this, `multiarray` exports a somewhat terrifying function called `set_numeric_ops`. The bootstrap procedure each time you `import numpy` is:

1.  `multiarray` and its `ndarray` object are loaded, but arithmetic operations on ndarrays are broken.
2.  `umath` is loaded.
3.  `set_numeric_ops` is used to monkeypatch all the methods like `ndarray.__add__` with objects from `umath`.

In addition, `set_numeric_ops` is exposed as a public API, `np.set_numeric_ops`.

Furthermore, even when this layering does work, it ends up distorting the shape of our public ABI. In recent years, the most common reason for adding new functions to `multiarray`'s "public" ABI is not that they really need to be public or that we expect other projects to use them, but rather just that we need to call them from `umath`. This is extremely unfortunate, because it makes our public ABI unnecessarily large, and since we can never remove things from it then this creates an ongoing maintenance burden. The way C works, you can have internal API that's visible to everything inside the same extension module, or you can have a public API that everyone can use; you can't (easily) have an API that's visible to multiple extension modules inside numpy, but not to external users.

We've also increasingly been putting utility code into `numpy/core/src/private/`, which now contains a bunch of files which are `#include`d twice, once into `multiarray` and once into `umath`. This is pretty gross, and is purely a workaround for these being separate C extensions. The `npymath` library is also included in both extension modules.

## Proposed changes

This NEP proposes three changes:

1.  We should start building `numpy/core/src/multiarray/*.c` and `numpy/core/src/umath/*.c` together into a single extension module.
2.  Instead of `set_numeric_ops`, we should use some new, private API to set up `ndarray.__add__` and friends.
3.  We should deprecate, and eventually remove, `np.set_numeric_ops`.

## Non-proposed changes

We don't necessarily propose to throw away the distinction between multiarray/ and umath/ in terms of our source code organization: internal organization is useful\! We just want to build them together into a single extension module. Of course, this does open the door for potential future refactorings, which we can then evaluate based on their merits as they come up.

It also doesn't propose that we break the public C ABI. We should continue to provide `import_multiarray()` and `import_umath()` functions â€“ it's just that now both ABIs will ultimately be loaded from the same C library. Due to how `import_multiarray()` and `import_umath()` are written, we'll also still need to have modules called `numpy.core.multiarray` and `numpy.core.umath`, and they'll need to continue to export `_ARRAY_API` and `_UFUNC_API` objects â€“ but we can make one or both of these modules be tiny shims that simply re-export the magic API object from where-ever it's actually defined. (See `numpy/core/code_generators/generate_{numpy,ufunc}_api.py` for details of how these imports work.)

## Backward compatibility

The only compatibility break is the deprecation of `np.set_numeric_ops`.

## Rejected alternatives

### Preserve `set_numeric_ops` for monkeypatching

In discussing this NEP, one additional use case was raised for `set_numeric_ops`: if you have an optimized vector math library (e.g. Intel's MKL VML, Sleef, or Yeppp), then `set_numeric_ops` can be used to monkeypatch numpy to use these operations instead of numpy's built-in vector operations. But, even if we grant that this is a great idea, using `set_numeric_ops` isn't actually the best way to do it. All `set_numeric_ops` allows you to do is take over Python's syntactic operators (`+`, `*`, etc.) on ndarrays; it doesn't let you affect operations called via other APIs (e.g., `np.add`), or operations that don't have built-in syntax (e.g., `np.exp`). Also, you have to reimplement the whole ufunc machinery, instead of just the core loop. On the other hand, the [PyUFunc\_ReplaceLoopBySignature](https://docs.scipy.org/doc/numpy/reference/c-api.ufunc.html#c.PyUFunc_ReplaceLoopBySignature) API â€“ which was added in 2006 â€“ allows replacement of the inner loops of arbitrary ufuncs. This is both simpler and more powerful â€“ e.g. replacing the inner loop of `np.add` means your code will automatically be used for both `ndarray + ndarray` as well as direct calls to `np.add`. So this doesn't seem like a good reason to not deprecate `set_numeric_ops`.

## Discussion

  - <https://mail.python.org/pipermail/numpy-discussion/2018-March/077764.html>
  - <https://mail.python.org/pipermail/numpy-discussion/2018-June/078345.html>

## Copyright

This document has been placed in the public domain.

---

nep-0016-abstract-array.md

---

# NEP 16 â€” An abstract base class for identifying "duck arrays"

  - Author  
    Nathaniel J. Smith \<<njs@pobox.com>\>

  - Status  
    Withdrawn

  - Type  
    Standards Track

  - Created  
    2018-03-06

  - Resolution  
    <https://github.com/numpy/numpy/pull/12174>

\> **Note** \> This NEP has been withdrawn in favor of the protocol based approach described in [NEP 22](nep-0022-ndarray-duck-typing-overview.html)

## Abstract

We propose to add an abstract base class `AbstractArray` so that third-party classes can declare their ability to "quack like" an `ndarray`, and an `asabstractarray` function that performs similarly to `asarray` except that it passes through `AbstractArray` instances unchanged.

## Detailed description

Many functions, in NumPy and in third-party packages, start with some code like:

    def myfunc(a, b):
        a = np.asarray(a)
        b = np.asarray(b)
        ...

This ensures that `a` and `b` are `np.ndarray` objects, so `myfunc` can carry on assuming that they'll act like ndarrays both semantically (at the Python level), and also in terms of how they're stored in memory (at the C level). But many of these functions only work with arrays at the Python level, which means that they don't actually need `ndarray` objects *per se*: they could work just as well with any Python object that "quacks like" an ndarray, such as sparse arrays, dask's lazy arrays, or xarray's labeled arrays.

However, currently, there's no way for these libraries to express that their objects can quack like an ndarray, and there's no way for functions like `myfunc` to express that they'd be happy with anything that quacks like an ndarray. The purpose of this NEP is to provide those two features.

Sometimes people suggest using `np.asanyarray` for this purpose, but unfortunately its semantics are exactly backwards: it guarantees that the object it returns uses the same memory layout as an `ndarray`, but tells you nothing at all about its semantics, which makes it essentially impossible to use safely in practice. Indeed, the two `ndarray` subclasses distributed with NumPy â€“ `np.matrix` and `np.ma.masked_array` â€“ do have incompatible semantics, and if they were passed to a function like `myfunc` that doesn't check for them as a special-case, then it may silently return incorrect results.

### Declaring that an object can quack like an array

There are two basic approaches we could use for checking whether an object quacks like an array. We could check for a special attribute on the class:

    def quacks_like_array(obj):
        return bool(getattr(type(obj), "__quacks_like_array__", False))

Or, we could define an [abstract base class (ABC)](https://docs.python.org/3/library/collections.abc.html):

    def quacks_like_array(obj):
        return isinstance(obj, AbstractArray)

If you look at how ABCs work, this is essentially equivalent to keeping a global set of types that have been declared to implement the `AbstractArray` interface, and then checking it for membership.

Between these, the ABC approach seems to have a number of advantages:

  - It's Python's standard, "one obvious way" of doing this.
  - ABCs can be introspected (e.g. `help(np.AbstractArray)` does something useful).
  - ABCs can provide useful mixin methods.
  - ABCs integrate with other features like mypy type-checking, `functools.singledispatch`, etc.

One obvious thing to check is whether this choice affects speed. Using the attached benchmark script on a CPython 3.7 prerelease (revision c4d77a661138d, self-compiled, no PGO), on a Thinkpad T450s running Linux, we find:

    np.asarray(ndarray_obj)      330 ns
    np.asarray([])              1400 ns
    
    Attribute check, success      80 ns
    Attribute check, failure      80 ns
    
    ABC, success via subclass    340 ns
    ABC, success via register()  700 ns
    ABC, failure                 370 ns

Notes:

  - The first two lines are included to put the other lines in context.

  - This used 3.7 because both `getattr` and ABCs are receiving substantial optimizations in this release, and it's more representative of the long-term future of Python. (Failed `getattr` doesn't necessarily construct an exception object anymore, and ABCs were reimplemented in C.)

  - The "success" lines refer to cases where `quacks_like_array` would return True. The "failure" lines are cases where it would return False.

  - The first measurement for ABCs is subclasses defined like:
    
        class MyArray(AbstractArray):
            ...
    
    The second is for subclasses defined like:
    
        class MyArray:
            ...
        
        AbstractArray.register(MyArray)
    
    I don't know why there's such a large difference between these.

In practice, either way we'd only do the full test after first checking for well-known types like `ndarray`, `list`, etc. [This is how NumPy currently checks for other double-underscore attributes](https://github.com/numpy/numpy/blob/main/numpy/core/src/private/get_attr_string.h) and the same idea applies here to either approach. So these numbers won't affect the common case, just the case where we actually have an `AbstractArray`, or else another third-party object that will end up going through `__array__` or `__array_interface__` or end up as an object array.

So in summary, using an ABC will be slightly slower than using an attribute, but this doesn't affect the most common paths, and the magnitude of slowdown is fairly small (\~250 ns on an operation that already takes longer than that). Furthermore, we can potentially optimize this further (e.g. by keeping a tiny LRU cache of types that are known to be AbstractArray subclasses, on the assumption that most code will only use one or two of these types at a time), and it's very unclear that this even matters â€“ if the speed of `asarray` no-op pass-throughs were a bottleneck that showed up in profiles, then probably we would have made them faster already\! (It would be trivial to fast-path this, but we don't.)

Given the semantic and usability advantages of ABCs, this seems like an acceptable trade-off.

### Specification of `asabstractarray`

Given `AbstractArray`, the definition of `asabstractarray` is simple:

    def asabstractarray(a, dtype=None):
        if isinstance(a, AbstractArray):
            if dtype is not None and dtype != a.dtype:
                return a.astype(dtype)
            return a
        return asarray(a, dtype=dtype)

Things to note:

  - `asarray` also accepts an `order=` argument, but we don't include that here because it's about details of memory representation, and the whole point of this function is that you use it to declare that you don't care about details of memory representation.

  - Using the `astype` method allows the `a` object to decide how to implement casting for its particular type.

  - For strict compatibility with `asarray`, we skip calling `astype` when the dtype is already correct. Compare:
    
        >>> a = np.arange(10)
        
        # astype() always returns a view:
        >>> a.astype(a.dtype) is a
        False
        
        # asarray() returns the original object if possible:
        >>> np.asarray(a, dtype=a.dtype) is a
        True

### What exactly are you promising if you inherit from `AbstractArray`?

This will presumably be refined over time. The ideal of course is that your class should be indistinguishable from a real `ndarray`, but nothing enforces that except the expectations of users. In practice, declaring that your class implements the `AbstractArray` interface simply means that it will start passing through `asabstractarray`, and so by subclassing it you're saying that if some code works for `ndarray`s but breaks for your class, then you're willing to accept bug reports on that.

To start with, we should declare `__array_ufunc__` to be an abstract method, and add the `NDArrayOperatorsMixin` methods as mixin methods.

Declaring `astype` as an `@abstractmethod` probably makes sense as well, since it's used by `asabstractarray`. We might also want to go ahead and add some basic attributes like `ndim`, `shape`, `dtype`.

Adding new abstract methods will be a bit tricky, because ABCs enforce these at subclass time; therefore, simply adding a new <span class="title-ref">@abstractmethod</span> will be a backwards compatibility break. If this becomes a problem then we can use some hacks to implement an <span class="title-ref">@upcoming\_abstractmethod</span> decorator that only issues a warning if the method is missing, and treat it like a regular deprecation cycle. (In this case, the thing we'd be deprecating is "support for abstract arrays that are missing feature X".)

### Naming

The name of the ABC doesn't matter too much, because it will only be referenced rarely and in relatively specialized situations. The name of the function matters a lot, because most existing instances of `asarray` should be replaced by this, and in the future it's what everyone should be reaching for by default unless they have a specific reason to use `asarray` instead. This suggests that its name really should be *shorter* and *more memorable* than `asarray`... which is difficult. I've used `asabstractarray` in this draft, but I'm not really happy with it, because it's too long and people are unlikely to start using it by habit without endless exhortations.

One option would be to actually change `asarray`'s semantics so that *it* passes through `AbstractArray` objects unchanged. But I'm worried that there may be a lot of code out there that calls `asarray` and then passes the result into some C function that doesn't do any further type checking (because it knows that its caller has already used `asarray`). If we allow `asarray` to return `AbstractArray` objects, and then someone calls one of these C wrappers and passes it an `AbstractArray` object like a sparse array, then they'll get a segfault. Right now, in the same situation, `asarray` will instead invoke the object's `__array__` method, or use the buffer interface to make a view, or pass through an array with object dtype, or raise an error, or similar. Probably none of these outcomes are actually desirable in most cases, so maybe making it a segfault instead would be OK? But it's dangerous given that we don't know how common such code is. OTOH, if we were starting from scratch then this would probably be the ideal solution.

We can't use `asanyarray` or `array`, since those are already taken.

Any other ideas? `np.cast`, `np.coerce`?

## Implementation

1.  Rename `NDArrayOperatorsMixin` to `AbstractArray` (leaving behind an alias for backwards compatibility) and make it an ABC.
2.  Add `asabstractarray` (or whatever we end up calling it), and probably a C API equivalent.
3.  Begin migrating NumPy internal functions to using `asabstractarray` where appropriate.

## Backward compatibility

This is purely a new feature, so there are no compatibility issues. (Unless we decide to change the semantics of `asarray` itself.)

## Rejected alternatives

One suggestion that has come up is to define multiple abstract classes for different subsets of the array interface. Nothing in this proposal stops either NumPy or third-parties from doing this in the future, but it's very difficult to guess ahead of time which subsets would be useful. Also, "the full ndarray interface" is something that existing libraries are written to expect (because they work with actual ndarrays) and test (because they test with actual ndarrays), so it's by far the easiest place to start.

## Links to discussion

  - <https://mail.python.org/pipermail/numpy-discussion/2018-March/077767.html>

## Appendix: benchmark script

<div class="literalinclude">

nep-0016-benchmark.py

</div>

## Copyright

This document has been placed in the public domain.

---

nep-0017-split-out-maskedarray.md

---

# NEP 17 â€” Split out masked arrays

  - Author  
    StÃ©fan van der Walt \<<stefanv@berkeley.edu>\>

  - Status  
    Rejected

  - Type  
    Standards Track

  - Created  
    2018-03-22

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-May/078026.html>

## Abstract

This NEP proposes removing MaskedArray functionality from NumPy, and publishing it as a stand-alone package.

## Detailed description

MaskedArrays are a sub-class of the NumPy `ndarray` that adds masking capabilities, i.e. the ability to ignore or hide certain array values during computation.

While historically convenient to distribute this class inside of NumPy, improved packaging has made it possible to distribute it separately without difficulty.

Motivations for this move include:

>   - Focus: the NumPy package should strive to only include the <span class="title-ref">ndarray</span> object, and the essential utilities needed to manipulate such arrays.
>   - Complexity: the MaskedArray implementation is non-trivial, and imposes a significant maintenance burden.
>   - Compatibility: MaskedArray objects, being subclasses\[1\] of <span class="title-ref">ndarrays</span>, often cause complications when being used with other packages. Fixing these issues is outside the scope of NumPy development.

This NEP proposes a deprecation pathway through which MaskedArrays would still be accessible to users, but no longer as part of the core package.

## Implementation

Currently, a MaskedArray is created as follows:

    from numpy import ma
    ma.array([1, 2, 3], mask=[True, False, True])

This will return an array where the values 1 and 3 are masked (no longer visible to operations such as <span class="title-ref">np.sum</span>).

We propose refactoring the <span class="title-ref">np.ma</span> subpackage into a new pip-installable library called <span class="title-ref">maskedarray</span>\[2\], which would be used in a similar fashion:

    import maskedarray as ma
    ma.array([1, 2, 3], mask=[True, False, True])

For two releases of NumPy, <span class="title-ref">maskedarray</span> would become a NumPy dependency, and would expose MaskedArrays under the existing name, <span class="title-ref">np.ma</span>. If imported as <span class="title-ref">np.ma</span>, a <span class="title-ref">NumpyDeprecationWarning</span> will be raised, describing the impending deprecation with instructions on how to modify code to use <span class="title-ref">maskedarray</span>.

After two releases, <span class="title-ref">np.ma</span> will be removed entirely. In order to obtain <span class="title-ref">np.ma</span>, a user will install it via <span class="title-ref">pip install</span> or via their package manager. Subsequently, <span class="title-ref">importing maskedarray</span> on a version of NumPy that includes it integrally will raise an <span class="title-ref">ImportError</span>.

### Documentation

NumPy's internal documentation refers explicitly to MaskedArrays in certain places, e.g. \`ndarray.concatenate\`:

\> When one or more of the arrays to be concatenated is a MaskedArray, \> this function will return a MaskedArray object instead of an ndarray, \> but the input masks are *not* preserved. In cases where a MaskedArray \> is expected as input, use the ma.concatenate function from the masked \> array module instead.

Such documentation will be removed, since the expectation is that users of <span class="title-ref">maskedarray</span> will use methods from that package to operate on MaskedArrays.

#### Other appearances

Explicit MaskedArray support will be removed from:

  - <span class="title-ref">numpygenfromtext</span>
  - <span class="title-ref">numpy.libmerge\_arrays</span>, <span class="title-ref">numpy.lib.stack\_arrays</span>

## Backward compatibility

For two releases of NumPy, apart from a deprecation notice, there will be no user visible changes. Thereafter, <span class="title-ref">np.ma</span> will no longer be available (instead, MaskedArrays will live in the <span class="title-ref">maskedarray</span> package).

Note also that new PEPs on array-like objects may eventually provide better support for MaskedArrays than is currently available.

## Alternatives

After a lively discussion on the mailing list:

  - There is support (and active interest in) making a better *new* masked array class.
  - The new class should be a consumer of the external NumPy API with no special status (unlike today where there are hacks across the codebase to support it)
  - <span class="title-ref">MaskedArray</span> will stay where it is, at least until the new masked array class materializes and has been tried in the wild.

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  Subclassing ndarray, <https://docs.scipy.org/doc/numpy/user/basics.subclassing.html>

2.  PyPI: maskedarray, <https://pypi.org/project/maskedarray/>

---

nep-0018-array-function-protocol.md

---

# NEP 18 â€” A dispatch mechanism for NumPy's high level array functions

  - Author  
    Stephan Hoyer \<<shoyer@google.com>\>

  - Author  
    Matthew Rocklin \<<mrocklin@gmail.com>\>

  - Author  
    Marten van Kerkwijk \<<mhvk@astro.utoronto.ca>\>

  - Author  
    Hameer Abbasi \<<hameerabbasi@yahoo.com>\>

  - Author  
    Eric Wieser \<<wieser.eric@gmail.com>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2018-05-29

  - Updated  
    2019-05-25

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-August/078493.html>

## Abstract

We propose the `__array_function__` protocol, to allow arguments of NumPy functions to define how that function operates on them. This will allow using NumPy as a high level API for efficient multi-dimensional array operations, even with array implementations that differ greatly from `numpy.ndarray`.

## Detailed description

NumPy's high level ndarray API has been implemented several times outside of NumPy itself for different architectures, such as for GPU arrays (CuPy), Sparse arrays (scipy.sparse, pydata/sparse) and parallel arrays (Dask array) as well as various NumPy-like implementations in the deep learning frameworks, like TensorFlow and PyTorch.

Similarly there are many projects that build on top of the NumPy API for labeled and indexed arrays (XArray), automatic differentiation (Autograd, Tangent), masked arrays (numpy.ma), physical units (astropy.units, pint, unyt), etc. that add additional functionality on top of the NumPy API. Most of these project also implement a close variation of NumPy's level high API.

We would like to be able to use these libraries together, for example we would like to be able to place a CuPy array within XArray, or perform automatic differentiation on Dask array code. This would be easier to accomplish if code written for NumPy ndarrays could also be used by other NumPy-like projects.

For example, we would like for the following code example to work equally well with any NumPy-like array object:

``` python
def f(x):
    y = np.tensordot(x, x.T)
    return np.mean(np.exp(y))
```

Some of this is possible today with various protocol mechanisms within NumPy.

  - The `np.exp` function checks the `__array_ufunc__` protocol
  - The `.T` method works using Python's method dispatch
  - The `np.mean` function explicitly checks for a `.mean` method on the argument

However other functions, like `np.tensordot` do not dispatch, and instead are likely to coerce to a NumPy array (using the `__array__`) protocol, or err outright. To achieve enough coverage of the NumPy API to support downstream projects like XArray and autograd we want to support *almost all* functions within NumPy, which calls for a more reaching protocol than just `__array_ufunc__`. We would like a protocol that allows arguments of a NumPy function to take control and divert execution to another function (for example a GPU or parallel implementation) in a way that is safe and consistent across projects.

## Implementation

We propose adding support for a new protocol in NumPy, `__array_function__`.

This protocol is intended to be a catch-all for NumPy functionality that is not covered by the `__array_ufunc__` protocol for universal functions (like `np.exp`). The semantics are very similar to `__array_ufunc__`, except the operation is specified by an arbitrary callable object rather than a ufunc instance and method.

A prototype implementation can be found in [this notebook](https://nbviewer.jupyter.org/gist/shoyer/1f0a308a06cd96df20879a1ddb8f0006).

\> **Warning** \> The `__array_function__` protocol, and its use on particular functions, is *experimental*. We plan to retain an interface that makes it possible to override NumPy functions, but the way to do so for particular functions **can and will change** with little warning. If such reduced backwards compatibility guarantees are not accepted to you, do not rely upon overrides of NumPy functions for non-NumPy arrays. See "Non-goals" below for more details.

\> **Note** \> Dispatch with the `__array_function__` protocol has been implemented but is not yet enabled by default:

>   - In NumPy 1.16, you need to set the environment variable `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1` before importing NumPy to test NumPy function overrides.
>   - In NumPy 1.17, the protocol will be enabled by default, but can be disabled with `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0`.
>   - Eventually, expect to `__array_function__` to always be enabled.

### The interface

We propose the following signature for implementations of `__array_function__`:

  - `` `python     def __array_function__(self, func, types, args, kwargs)  - ``func`is an arbitrary callable exposed by NumPy's public API,    which was called in the form`func(*args,*\*kwargs)`.`<span class="title-ref"> - </span><span class="title-ref">types</span><span class="title-ref"> is a \`collection \<https://docs.python.org/3/library/collections.abc.html\#collections.abc.Collection\></span>\_  
    of unique argument types from the original NumPy function call that implement `__array_function__`.

  - \- The tuple `args` and dict `kwargs` are directly passed on from the  
    original call.

Unlike `__array_ufunc__`, there are no high-level guarantees about the type of `func`, or about which of `args` and `kwargs` may contain objects implementing the array API.

As a convenience for `__array_function__` implementers, `types` provides all argument types with an `'__array_function__'` attribute. This allows implementers to quickly identify cases where they should defer to `__array_function__` implementations on other arguments. The type of `types` is intentionally vague: `frozenset` would most closely match intended use, but we may use `tuple` instead for performance reasons. In any case, `__array_function__` implementations should not rely on the iteration order of `types`, which would violate a well-defined "Type casting hierarchy" (as described in \[NEP-13 \<NEP13\>\](\#nep-13-\<nep13\>)).

### Example for a project implementing the NumPy API

Most implementations of `__array_function__` will start with two checks:

1.  Is the given function something that we know how to overload?
2.  Are all arguments of a type that we know how to handle?

If these conditions hold, `__array_function__` should return the result from calling its implementation for `func(*args, **kwargs)`. Otherwise, it should return the sentinel value `NotImplemented`, indicating that the function is not implemented by these types. This is preferable to raising `TypeError` directly, because it gives *other* arguments the opportunity to define the operations.

There are no general requirements on the return value from `__array_function__`, although most sensible implementations should probably return array(s) with the same type as one of the function's arguments. If/when Python gains [typing support for protocols](https://www.python.org/dev/peps/pep-0544/) and NumPy adds static type annotations, the `@overload` implementation for `SupportsArrayFunction` will indicate a return type of `Any`.

It may also be convenient to define a custom decorators (`implements` below) for registering `__array_function__` implementations.

``` python
HANDLED_FUNCTIONS = {}

class MyArray:
    def __array_function__(self, func, types, args, kwargs):
        if func not in HANDLED_FUNCTIONS:
            return NotImplemented
        # Note: this allows subclasses that don't override
        # __array_function__ to handle MyArray objects
        if not all(issubclass(t, MyArray) for t in types):
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)

def implements(numpy_function):
    """Register an __array_function__ implementation for MyArray objects."""
    def decorator(func):
        HANDLED_FUNCTIONS[numpy_function] = func
        return func
    return decorator

@implements(np.concatenate)
def concatenate(arrays, axis=0, out=None):
    ...  # implementation of concatenate for MyArray objects

@implements(np.broadcast_to)
def broadcast_to(array, shape):
    ...  # implementation of broadcast_to for MyArray objects
```

Note that it is not required for `__array_function__` implementations to include *all* of the corresponding NumPy function's optional arguments (e.g., `broadcast_to` above omits the irrelevant `subok` argument). Optional arguments are only passed in to `__array_function__` if they were explicitly used in the NumPy function call.

\> **Note** \> Just like the case for builtin special methods like `__add__`, properly written `__array_function__` methods should always return `NotImplemented` when an unknown type is encountered. Otherwise, it will be impossible to correctly override NumPy functions from another object if the operation also includes one of your objects.

### Necessary changes within the NumPy codebase itself

This will require two changes within the NumPy codebase:

1.  A function to inspect available inputs, look for the `__array_function__` attribute on those inputs, and call those methods appropriately until one succeeds. This needs to be fast in the common all-NumPy case, and have acceptable performance (no worse than linear time) even if the number of overloaded inputs is large (e.g., as might be the case for <span class="title-ref">np.concatenate</span>).
    
    This is one additional function of moderate complexity.

2.  Calling this function within all relevant NumPy functions.
    
    This affects many parts of the NumPy codebase, although with very low complexity.

#### Finding and calling the right `__array_function__`

Given a NumPy function, `*args` and `**kwargs` inputs, we need to search through `*args` and `**kwargs` for all appropriate inputs that might have the `__array_function__` attribute. Then we need to select among those possible methods and execute the right one. Negotiating between several possible implementations can be complex.

##### Finding arguments

Valid arguments may be directly in the `*args` and `**kwargs`, such as in the case for `np.tensordot(left, right, out=out)`, or they may be nested within lists or dictionaries, such as in the case of `np.concatenate([x, y, z])`. This can be problematic for two reasons:

1.  Some functions are given long lists of values, and traversing them might be prohibitively expensive.
2.  Some functions may have arguments that we don't want to inspect, even if they have the `__array_function__` method.

To resolve these issues, NumPy functions should explicitly indicate which of their arguments may be overloaded, and how these arguments should be checked. As a rule, this should include all arguments documented as either `array_like` or `ndarray`.

We propose to do so by writing "dispatcher" functions for each overloaded NumPy function:

  - These functions will be called with the exact same arguments that were passed into the NumPy function (i.e., `dispatcher(*args, **kwargs)`), and should return an iterable of arguments to check for overrides.
  - Dispatcher functions are required to share the exact same positional, optional and keyword-only arguments as their corresponding NumPy functions. Otherwise, valid invocations of a NumPy function could result in an error when calling its dispatcher.
  - Because default *values* for keyword arguments do not have `__array_function__` attributes, by convention we set all default argument values to `None`. This reduces the likelihood of signatures falling out of sync, and minimizes extraneous information in the dispatcher. The only exception should be cases where the argument value in some way effects dispatching, which should be rare.

An example of the dispatcher for `np.concatenate` may be instructive:

``` python
def _concatenate_dispatcher(arrays, axis=None, out=None):
    for array in arrays:
        yield array
    if out is not None:
        yield out
```

The concatenate dispatcher is written as generator function, which allows it to potentially include the value of the optional `out` argument without needing to create a new sequence with the (potentially long) list of objects to be concatenated.

##### Trying `__array_function__` methods until the right one works

Many arguments may implement the `__array_function__` protocol. Some of these may decide that, given the available inputs, they are unable to determine the correct result. How do we call the right one? If several are valid then which has precedence?

For the most part, the rules for dispatch with `__array_function__` match those for `__array_ufunc__` (see \[NEP-13 \<NEP13\>\](\#nep-13-\<nep13\>)). In particular:

  - NumPy will gather implementations of `__array_function__` from all specified inputs and call them in order: subclasses before superclasses, and otherwise left to right. Note that in some edge cases involving subclasses, this differs slightly from the [current behavior](https://bugs.python.org/issue30140) of Python.
  - Implementations of `__array_function__` indicate that they can handle the operation by returning any value other than `NotImplemented`.
  - If all `__array_function__` methods return `NotImplemented`, NumPy will raise `TypeError`.

If no `__array_function__` methods exist, NumPy will default to calling its own implementation, intended for use on NumPy arrays. This case arises, for example, when all array-like arguments are Python numbers or lists. (NumPy arrays do have a `__array_function__` method, given below, but it always returns `NotImplemented` if any argument other than a NumPy array subclass implements `__array_function__`.)

One deviation from the current behavior of `__array_ufunc__` is that NumPy will only call `__array_function__` on the *first* argument of each unique type. This matches Python's [rule for calling reflected methods](https://docs.python.org/3/reference/datamodel.html#object.__ror__), and this ensures that checking overloads has acceptable performance even when there are a large number of overloaded arguments. To avoid long-term divergence between these two dispatch protocols, we should [also update](https://github.com/numpy/numpy/issues/11306) `__array_ufunc__` to match this behavior.

##### The `__array_function__` method on `numpy.ndarray`

The use cases for subclasses with `__array_function__` are the same as those with `__array_ufunc__`, so `numpy.ndarray` also defines a `__array_function__` method:

``` python
def __array_function__(self, func, types, args, kwargs):
    if not all(issubclass(t, ndarray) for t in types):
        # Defer to any non-subclasses that implement __array_function__
        return NotImplemented

    # Use NumPy's private implementation without __array_function__
    # dispatching
    return func._implementation(*args, **kwargs)
```

This method matches NumPy's dispatching rules, so for most part it is possible to pretend that `ndarray.__array_function__` does not exist. The private `_implementation` attribute, defined below in the `array_function_dispatch` decorator, allows us to avoid the special cases for NumPy arrays that were needed in the `__array_ufunc__` protocol.

The `__array_function__` protocol always calls subclasses before superclasses, so if any `ndarray` subclasses are involved in an operation, they will get the chance to override it, just as if any other argument overrides `__array_function__`. But the default behavior in an operation that combines a base NumPy array and a subclass is different: if the subclass returns `NotImplemented`, NumPy's implementation of the function will be called instead of raising an exception. This is appropriate since subclasses are [expected to be substitutable](https://en.wikipedia.org/wiki/Liskov_substitution_principle).

We still caution authors of subclasses to exercise caution when relying upon details of NumPy's internal implementations. It is not always possible to write a perfectly substitutable ndarray subclass, e.g., in cases involving the creation of new arrays, not least because NumPy makes use of internal optimizations specialized to base NumPy arrays, e.g., code written in C. Even if NumPy's implementation happens to work today, it may not work in the future. In these cases, your recourse is to re-implement top-level NumPy functions via `__array_function__` on your subclass.

#### Changes within NumPy functions

Given a function defining the above behavior, for now call it `implement_array_function`, we now need to call that function from within every relevant NumPy function. This is a pervasive change, but of fairly simple and innocuous code that should complete quickly and without effect if no arguments implement the `__array_function__` protocol.

To achieve this, we define a `array_function_dispatch` decorator to rewrite NumPy functions. The basic implementation is as follows:

``` python
def array_function_dispatch(dispatcher, module=None):
    """Wrap a function for dispatch with the __array_function__ protocol."""
    def decorator(implementation):
        @functools.wraps(implementation)
        def public_api(*args, **kwargs):
            relevant_args = dispatcher(*args, **kwargs)
            return implement_array_function(
                implementation, public_api, relevant_args, args, kwargs)
        if module is not None:
            public_api.__module__ = module
        # for ndarray.__array_function__
        public_api._implementation = implementation
        return public_api
    return decorator

# example usage
def _broadcast_to_dispatcher(array, shape, subok=None):
    return (array,)

@array_function_dispatch(_broadcast_to_dispatcher, module='numpy')
def broadcast_to(array, shape, subok=False):
    ...  # existing definition of np.broadcast_to
```

Using a decorator is great\! We don't need to change the definitions of existing NumPy functions, and only need to write a few additional lines for the dispatcher function. We could even reuse a single dispatcher for families of functions with the same signature (e.g., `sum` and `prod`). For such functions, the largest change could be adding a few lines to the docstring to note which arguments are checked for overloads.

It's particularly worth calling out the decorator's use of `functools.wraps`:

  - This ensures that the wrapped function has the same name and docstring as the wrapped NumPy function.
  - On Python 3, it also ensures that the decorator function copies the original function signature, which is important for introspection based tools such as auto-complete.
  - Finally, it ensures that the wrapped function [can be pickled](http://gael-varoquaux.info/programming/decoration-in-python-done-right-decorating-and-pickling.html).

The example usage illustrates several best practices for writing dispatchers relevant to NumPy contributors:

  - We passed the `module` argument, which in turn sets the `__module__` attribute on the generated function. This is for the benefit of better error messages, here for errors raised internally by NumPy when no implementation is found, e.g., `TypeError: no implementation found for 'numpy.broadcast_to'`. Setting `__module__` to the canonical location in NumPy's public API encourages users to use NumPy's public API for identifying functions in `__array_function__`.

  - The dispatcher is a function that returns a tuple, rather than an equivalent (and equally valid) generator using `yield`:
    
    ``` python
    # example usage
    def broadcast_to(array, shape, subok=None):
        yield array
    ```
    
    This is no accident: NumPy's implementation of dispatch for `__array_function__` is fastest when dispatcher functions return a builtin sequence type (`tuple` or `list`).
    
    On a related note, it's perfectly fine for dispatchers to return arguments even if in some cases you *know* that they cannot have an `__array_function__` method. This can arise for functions with default arguments (e.g., `None`) or complex signatures. NumPy's dispatching logic sorts out these cases very quickly, so it generally is not worth the trouble of parsing them on your own.

\> **Note** \> The code for `array_function_dispatch` above has been updated from the original version of this NEP to match the actual [implementation in NumPy](https://github.com/numpy/numpy/blob/e104f03ac8f65ae5b92a9b413b0fa639f39e6de2/numpy/core/overrides.py).

### Extensibility

An important virtue of this approach is that it allows for adding new optional arguments to NumPy functions without breaking code that already relies on `__array_function__`.

This is not a theoretical concern. NumPy's older, haphazard implementation of overrides *within* functions like `np.sum()` necessitated some awkward gymnastics when we decided to add new optional arguments, e.g., the new `keepdims` argument is only passed in cases where it is used:

``` python
def sum(array, ..., keepdims=np._NoValue):
    kwargs = {}
    if keepdims is not np._NoValue:
        kwargs['keepdims'] = keepdims
    return array.sum(..., **kwargs)
```

For `__array_function__` implementers, this also means that it is possible to implement even existing optional arguments incrementally, and only in cases where it makes sense. For example, a library implementing immutable arrays would not be required to explicitly include an unsupported `out` argument in the function signature. This can be somewhat onerous to implement properly, e.g.,

``` python
def my_sum(array, ..., out=None):
    if out is not None:
        raise TypeError('out argument is not supported')
    ...
```

We thus avoid encouraging the tempting shortcut of adding catch-all `**ignored_kwargs` to the signatures of functions called by NumPy, which fails silently for misspelled or ignored arguments.

### Performance

Performance is always a concern with NumPy, even though NumPy users have already prioritized usability over pure speed with their choice of the Python language itself. It's important that this new `__array_function__` protocol not impose a significant cost in the typical case of NumPy functions acting on NumPy arrays.

Our [microbenchmark results](https://nbviewer.jupyter.org/gist/shoyer/1f0a308a06cd96df20879a1ddb8f0006) show that a pure Python implementation of the override machinery described above adds roughly 2-3 microseconds of overhead to each NumPy function call without any overloaded arguments. For context, typical NumPy functions on small arrays have a runtime of 1-10 microseconds, mostly determined by what fraction of the function's logic is written in C. For example, one microsecond is about the difference in speed between the `ndarray.sum()` method (1.6 us) and `numpy.sum()` function (2.6 us).

Fortunately, we expect significantly less overhead with a C implementation of `implement_array_function`, which is where the bulk of the runtime is. This would leave the `array_function_dispatch` decorator and dispatcher function on their own adding about 0.5 microseconds of overhead, for perhaps \~1 microsecond of overhead in the typical case.

In our view, this level of overhead is reasonable to accept for code written in Python. We're pretty sure that the vast majority of NumPy users aren't concerned about performance differences measured in microsecond(s) on NumPy functions, because it's difficult to do *anything* in Python in less than a microsecond.

### Use outside of NumPy

Nothing about this protocol that is particular to NumPy itself. Should we encourage use of the same `__array_function__` protocol third-party libraries for overloading non-NumPy functions, e.g., for making array-implementation generic functionality in SciPy?

This would offer significant advantages (SciPy wouldn't need to invent its own dispatch system) and no downsides that we can think of, because every function that dispatches with `__array_function__` already needs to be explicitly recognized. Libraries like Dask, CuPy, and Autograd already wrap a limited subset of SciPy functionality (e.g., `scipy.linalg`) similarly to how they wrap NumPy.

If we want to do this, we should expose at least the decorator `array_function_dispatch()` and possibly also the lower level `implement_array_function()` as part of NumPy's public API.

## Non-goals

We are aiming for basic strategy that can be relatively mechanistically applied to almost all functions in NumPy's API in a relatively short period of time, the development cycle of a single NumPy release.

We hope to get both the `__array_function__` protocol and all specific overloads right on the first try, but our explicit aim here is to get something that mostly works (and can be iterated upon), rather than to wait for an optimal implementation. The price of moving fast is that for now **this protocol should be considered strictly experimental**. We reserve the right to change the details of this protocol and how specific NumPy functions use it at any time in the future -- even in otherwise bug-fix only releases of NumPy. In practice, once initial issues with `__array_function__` are worked out, we will use abbreviated deprecation cycles as short as a single major NumPy release (e.g., as little as four months).

In particular, we don't plan to write additional NEPs that list all specific functions to overload, with exactly how they should be overloaded. We will leave this up to the discretion of committers on individual pull requests, trusting that they will surface any controversies for discussion by interested parties.

However, we already know several families of functions that should be explicitly exclude from `__array_function__`. These will need their own protocols:

  - universal functions, which already have their own protocol.
  - `array` and `asarray`, because they are explicitly intended for coercion to actual `numpy.ndarray` object.
  - dispatch for methods of any kind, e.g., methods on `np.random.RandomState` objects.

We also expect that the mechanism for overriding specific functions that will initially use the `__array_function__` protocol can and will change in the future. As a concrete example of how we expect to break behavior in the future, some functions such as `np.where` are currently not NumPy universal functions, but conceivably could become universal functions in the future. When/if this happens, we will change such overloads from using `__array_function__` to the more specialized `__array_ufunc__`.

## Backward compatibility

This proposal does not change existing semantics, except for those arguments that currently have `__array_function__` attributes, which should be rare.

## Alternatives

### Specialized protocols

We could (and should) continue to develop protocols like `__array_ufunc__` for cohesive subsets of NumPy functionality.

As mentioned above, if this means that some functions that we overload with `__array_function__` should switch to a new protocol instead, that is explicitly OK for as long as `__array_function__` retains its experimental status.

Switching to a new protocol should use an abbreviated version of NumPy's normal deprecation cycle:

  - For a single major release, after checking for any new protocols, NumPy should still check for `__array_function__` methods that implement the given function. If any argument returns a value other than `NotImplemented` from `__array_function__`, a descriptive `FutureWarning` should be issued.
  - In the next major release, the checks for `__array_function__` will be removed.

### Separate namespace

A separate namespace for overloaded functions is another possibility, either inside or outside of NumPy.

This has the advantage of alleviating any possible concerns about backwards compatibility and would provide the maximum freedom for quick experimentation. In the long term, it would provide a clean abstraction layer, separating NumPy's high level API from default implementations on `numpy.ndarray` objects.

The downsides are that this would require an explicit opt-in from all existing code, e.g., `import numpy.api as np`, and in the long term would result in the maintenance of two separate NumPy APIs. Also, many functions from `numpy` itself are already overloaded (but inadequately), so confusion about high vs. low level APIs in NumPy would still persist.

Alternatively, a separate namespace, e.g., `numpy.array_only`, could be created for a non-overloaded version of NumPy's high level API, for cases where performance with NumPy arrays is a critical concern. This has most of the same downsides as the separate namespace.

### Multiple dispatch

An alternative to our suggestion of the `__array_function__` protocol would be implementing NumPy's core functions as [multi-methods](https://en.wikipedia.org/wiki/Multiple_dispatch). Although one of us wrote a [multiple dispatch library](https://github.com/mrocklin/multipledispatch) for Python, we don't think this approach makes sense for NumPy in the near term.

The main reason is that NumPy already has a well-proven dispatching mechanism with `__array_ufunc__`, based on Python's own dispatching system for arithmetic, and it would be confusing to add another mechanism that works in a very different way. This would also be more invasive change to NumPy itself, which would need to gain a multiple dispatch implementation.

It is possible that multiple dispatch implementation for NumPy's high level API could make sense in the future. Fortunately, `__array_function__` does not preclude this possibility, because it would be straightforward to write a shim for a default `__array_function__` implementation in terms of multiple dispatch.

### Implementations in terms of a limited core API

The internal implementation of some NumPy functions is extremely simple. For example:

  - `np.stack()` is implemented in only a few lines of code by combining indexing with `np.newaxis`, `np.concatenate` and the `shape` attribute.
  - `np.mean()` is implemented internally in terms of `np.sum()`, `np.divide()`, `.astype()` and `.shape`.

This suggests the possibility of defining a minimal "core" ndarray interface, and relying upon it internally in NumPy to implement the full API. This is an attractive option, because it could significantly reduce the work required for new array implementations.

However, this also comes with several downsides:

1.  The details of how NumPy implements a high-level function in terms of overloaded functions now becomes an implicit part of NumPy's public API. For example, refactoring `stack` to use `np.block()` instead of `np.concatenate()` internally would now become a breaking change.
2.  Array libraries may prefer to implement high level functions differently than NumPy. For example, a library might prefer to implement a fundamental operations like `mean()` directly rather than relying on `sum()` followed by division. More generally, it's not clear yet what exactly qualifies as core functionality, and figuring this out could be a large project.
3.  We don't yet have an overloading system for attributes and methods on array objects, e.g., for accessing `.dtype` and `.shape`. This should be the subject of a future NEP, but until then we should be reluctant to rely on these properties.

Given these concerns, we think it's valuable to support explicit overloading of nearly every public function in NumPy's API. This does not preclude the future possibility of rewriting NumPy functions in terms of simplified core functionality with `__array_function__` and a protocol and/or base class for ensuring that arrays expose methods and properties like `numpy.ndarray`. However, to work well this would require the possibility of implementing *some* but not all functions with `__array_function__`, e.g., as described in the next section.

### Partial implementation of NumPy's API

With the current design, classes that implement `__array_function__` to overload at least one function implicitly declare an intent to implement the entire NumPy API. It's not possible to implement *only* `np.concatenate()` on a type, but fall back to NumPy's default behavior of casting with `np.asarray()` for all other functions.

This could present a backwards compatibility concern that would discourage libraries from adopting `__array_function__` in an incremental fashion. For example, currently most numpy functions will implicitly convert `pandas.Series` objects into NumPy arrays, behavior that assuredly many pandas users rely on. If pandas implemented `__array_function__` only for `np.concatenate`, unrelated NumPy functions like `np.nanmean` would suddenly break on pandas objects by raising TypeError.

Even libraries that reimplement most of NumPy's public API sometimes rely upon using utility functions from NumPy without a wrapper. For example, both CuPy and JAX simply [use an alias](https://github.com/numpy/numpy/issues/12974) to `np.result_type`, which already supports duck-types with a `dtype` attribute.

With `__array_ufunc__`, it's possible to alleviate this concern by casting all arguments to numpy arrays and re-calling the ufunc, but the heterogeneous function signatures supported by `__array_function__` make it impossible to implement this generic fallback behavior for `__array_function__`.

We considered three possible ways to resolve this issue, but none were entirely satisfactory:

1.  Change the meaning of all arguments returning `NotImplemented` from `__array_function__` to indicate that all arguments should be coerced to NumPy arrays and the operation should be retried. However, many array libraries (e.g., scipy.sparse) really don't want implicit conversions to NumPy arrays, and often avoid implementing `__array__` for exactly this reason. Implicit conversions can result in silent bugs and performance degradation.
    
    Potentially, we could enable this behavior only for types that implement `__array__`, which would resolve the most problematic cases like scipy.sparse. But in practice, a large fraction of classes that present a high level API like NumPy arrays already implement `__array__`. This would preclude reliable use of NumPy's high level API on these objects.

2.  Use another sentinel value of some sort, e.g., `np.NotImplementedButCoercible`, to indicate that a class implementing part of NumPy's higher level array API is coercible as a fallback. If all arguments return `NotImplementedButCoercible`, arguments would be coerced and the operation would be retried.
    
    Unfortunately, correct behavior after encountering `NotImplementedButCoercible` is not always obvious. Particularly challenging is the "mixed" case where some arguments return `NotImplementedButCoercible` and others return `NotImplemented`. Would dispatching be retried after only coercing the "coercible" arguments? If so, then conceivably we could end up looping through the dispatching logic an arbitrary number of times. Either way, the dispatching rules would definitely get more complex and harder to reason about.

3.  Allow access to NumPy's implementation of functions, e.g., in the form of a publicly exposed `__skip_array_function__` attribute on the NumPy functions. This would allow for falling back to NumPy's implementation by using `func.__skip_array_function__` inside `__array_function__` methods, and could also potentially be used to be used to avoid the overhead of dispatching. However, it runs the risk of potentially exposing details of NumPy's implementations for NumPy functions that do not call `np.asarray()` internally. See [this note](https://mail.python.org/pipermail/numpy-discussion/2019-May/079541.html) for a summary of the full discussion.

These solutions would solve real use cases, but at the cost of additional complexity. We would like to gain experience with how `__array_function__` is actually used before making decisions that would be difficult to roll back.

### A magic decorator that inspects type annotations

In principle, Python 3 type annotations contain sufficient information to automatically create most `dispatcher` functions. It would be convenient to use these annotations to dispense with the need for manually writing dispatchers, e.g.,

``` python
@array_function_dispatch
def broadcast_to(array: ArrayLike
                 shape: Tuple[int, ...],
                 subok: bool = False):
    ...  # existing definition of np.broadcast_to
```

This would require some form of automatic code generation, either at compile or import time.

We think this is an interesting possible extension to consider in the future. We don't think it makes sense to do so now, because code generation involves tradeoffs and NumPy's experience with type annotations is still [quite limited](https://github.com/numpy/numpy-stubs). Even if NumPy was Python 3 only (which will happen \[sometime in 2019 \<NEP14\>\](\#sometime-in-2019-\<nep14\>)), we aren't ready to annotate NumPy's codebase directly yet.

### Support for implementation-specific arguments

We could allow `__array_function__` implementations to add their own optional keyword arguments by including `**ignored_kwargs` in dispatcher functions, e.g.,

``` python
def _concatenate_dispatcher(arrays, axis=None, out=None, **ignored_kwargs):
    ...  # same implementation of _concatenate_dispatcher as above
```

Implementation-specific arguments are somewhat common in libraries that otherwise emulate NumPy's higher level API (e.g., `dask.array.sum()` adds `split_every` and `tensorflow.reduce_sum()` adds `name`). Supporting them in NumPy would be particularly useful for libraries that implement new high-level array functions on top of NumPy functions, e.g.,

``` python
def mean_squared_error(x, y, **kwargs):
    return np.mean((x - y) ** 2, **kwargs)
```

Otherwise, we would need separate versions of `mean_squared_error` for each array implementation in order to pass implementation-specific arguments to `mean()`.

We wouldn't allow adding optional positional arguments, because these are reserved for future use by NumPy itself, but conflicts between keyword arguments should be relatively rare.

However, this flexibility would come with a cost. In particular, it implicitly adds `**kwargs` to the signature for all wrapped NumPy functions without actually including it (because we use `functools.wraps`). This means it is unlikely to work well with static analysis tools, which could report invalid arguments. Likewise, there is a price in readability: these optional arguments won't be included in the docstrings for NumPy functions.

It's not clear that this tradeoff is worth it, so we propose to leave this out for now. Adding implementation-specific arguments will require using those libraries directly.

### Other possible choices for the protocol

The array function `__array_function__` includes only two arguments, `func` and `types`, that provide information about the context of the function call.

`func` is part of the protocol because there is no way to avoid it: implementations need to be able to dispatch by matching a function to NumPy's public API.

`types` is included because we can compute it almost for free as part of collecting `__array_function__` implementations to call in `implement_array_function`. We also think it will be used by many `__array_function__` methods, which otherwise would need to extract this information themselves. It would be equivalently easy to provide single instances of each type, but providing only types seemed cleaner.

Taking this even further, it was suggested that `__array_function__` should be a `classmethod`. We agree that it would be a little cleaner to remove the redundant `self` argument, but feel that this minor clean-up would not be worth breaking from the precedence of `__array_ufunc__`.

There are two other arguments that we think *might* be important to pass to `__array_ufunc__` implementations:

  - Access to the non-dispatched implementation (i.e., before wrapping with `array_function_dispatch`) in `ndarray.__array_function__` would allow us to drop special case logic for that method from `implement_array_function`.
  - Access to the `dispatcher` function passed into `array_function_dispatch()` would allow `__array_function__` implementations to determine the list of "array-like" arguments in a generic way by calling `dispatcher(*args, **kwargs)`. This *could* be useful for `__array_function__` implementations that dispatch based on the value of an array attribute (e.g., `dtype` or `units`) rather than directly on the array type.

We have left these out for now, because we don't know that they are necessary. If we want to include them in the future, the easiest way to do so would be to update the `array_function_dispatch` decorator to add them as function attributes.

### Callable objects generated at runtime

NumPy has some APIs that define callable objects *dynamically*, such as `vectorize` and methods on `random.RandomState` object. Examples can also be found in other core libraries in the scientific Python stack, e.g., distribution objects in scipy.stats and model objects in scikit-learn. It would be nice to be able to write overloads for such callables, too. This presents a challenge for the `__array_function__` protocol, because unlike the case for functions there is no public object in the `numpy` namespace to pass into the `func` argument.

We could potentially handle this by establishing an alternative convention for how the `func` argument could be inspected, e.g., by using `func.__self__` to obtain the class object and `func.__func__` to return the unbound function object. However, some caution is in order, because this would immesh what are currently implementation details as a permanent features of the interface, such as the fact that `vectorize` is implemented as a class rather than closure, or whether a method is implemented directly or using a descriptor.

Given the complexity and the limited use cases, we are also deferring on this issue for now, but we are confident that `__array_function__` could be expanded to accommodate these use cases in the future if need be.

## Discussion

Various alternatives to this proposal were discussed in a few GitHub issues:

1.  [pydata/sparse \#1](https://github.com/pydata/sparse/issues/1)
2.  [numpy/numpy \#11129](https://github.com/numpy/numpy/issues/11129)

Additionally it was the subject of [a blogpost](http://matthewrocklin.com/blog/work/2018/05/27/beyond-numpy). Following this it was discussed at a [NumPy developer sprint](https://scisprints.github.io/#may-numpy-developer-sprint) at the [UC Berkeley Institute for Data Science (BIDS)](https://bids.berkeley.edu/).

Detailed discussion of this proposal itself can be found on the [the mailing list](https://mail.python.org/pipermail/numpy-discussion/2018-June/078127.html) and relevant pull requests ([1](https://github.com/numpy/numpy/pull/11189), [2](https://github.com/numpy/numpy/pull/11303#issuecomment-396638175), [3](https://github.com/numpy/numpy/pull/11374))

## Copyright

This document has been placed in the public domain.

---

nep-0019-rng-policy.md

---

# NEP 19 â€” Random number generator policy

  - Author  
    Robert Kern \<<robert.kern@gmail.com>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2018-05-24

  - Updated  
    2019-05-21

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-July/078380.html>

## Abstract

For the past decade, NumPy has had a strict backwards compatibility policy for the number stream of all of its random number distributions. Unlike other numerical components in `numpy`, which are usually allowed to return different results when modified if the results remain correct, we have obligated the random number distributions to always produce the exact same numbers in every version. The objective of our stream-compatibility guarantee was to provide exact reproducibility for simulations across numpy versions in order to promote reproducible research. However, this policy has made it very difficult to enhance any of the distributions with faster or more accurate algorithms. After a decade of experience and improvements in the surrounding ecosystem of scientific software, we believe that there are now better ways to achieve these objectives. We propose relaxing our strict stream-compatibility policy to remove the obstacles that are in the way of accepting contributions to our random number generation capabilities.

## The status quo

Our current policy, in full:

> A fixed seed and a fixed series of calls to `RandomState` methods using the same parameters will always produce the same results up to roundoff error except when the values were incorrect. Incorrect values will be fixed and the NumPy version in which the fix was made will be noted in the relevant docstring. Extension of existing parameter ranges and the addition of new parameters is allowed as long the previous behavior remains unchanged.

This policy was first instated in Nov 2008 (in essence; the full set of weasel words grew over time) in response to a user wanting to be sure that the simulations that formed the basis of their scientific publication could be reproduced years later, exactly, with whatever version of `numpy` that was current at the time. We were keen to support reproducible research, and it was still early in the life of `numpy.random`. We had not seen much cause to change the distribution methods all that much.

We also had not thought very thoroughly about the limits of what we really could promise (and by â€œweâ€ in this section, we really mean Robert Kern, letâ€™s be honest). Despite all of the weasel words, our policy overpromises compatibility. The same version of `numpy` built on different platforms, or just in a different way could cause changes in the stream, with varying degrees of rarity. The biggest is that the `.multivariate_normal()` method relies on `numpy.linalg` functions. Even on the same platform, if one links `numpy` with a different LAPACK, `.multivariate_normal()` may well return completely different results. More rarely, building on a different OS or CPU can cause differences in the stream. We use C `long` integers internally for integer distribution (it seemed like a good idea at the time), and those can vary in size depending on the platform. Distribution methods can overflow their internal C `longs` at different breakpoints depending on the platform and cause all of the random variate draws that follow to be different.

And even if all of that is controlled, our policy still does not provide exact guarantees across versions. We still do apply bug fixes when correctness is at stake. And even if we didnâ€™t do that, any nontrivial program does more than just draw random numbers. They do computations on those numbers, transform those with numerical algorithms from the rest of `numpy`, which is not subject to so strict a policy. Trying to maintain stream-compatibility for our random number distributions does not help reproducible research for these reasons.

The standard practice now for bit-for-bit reproducible research is to pin all of the versions of code of your software stack, possibly down to the OS itself. The landscape for accomplishing this is much easier today than it was in 2008. We now have `pip`. We now have virtual machines. Those who need to reproduce simulations exactly now can (and ought to) do so by using the exact same version of `numpy`. We do not need to maintain stream-compatibility across `numpy` versions to help them.

Our stream-compatibility guarantee has hindered our ability to make improvements to `numpy.random`. Several first-time contributors have submitted PRs to improve the distributions, usually by implementing a faster, or more accurate algorithm than the one that is currently there. Unfortunately, most of them would have required breaking the stream to do so. Blocked by our policy, and our inability to work around that policy, many of those contributors simply walked away.

## Implementation

Work on a proposed new Pseudo Random Number Generator (PRNG) subsystem is already underway in the [randomgen](https://github.com/bashtage/randomgen) project. The specifics of the new design are out of scope for this NEP and up for much discussion, but we will discuss general policies that will guide the evolution of whatever code is adopted. We will also outline just a few of the requirements that such a new system must have to support the policy proposed in this NEP.

First, we will maintain API source compatibility just as we do with the rest of `numpy`. If we *must* make a breaking change, we will only do so with an appropriate deprecation period and warnings.

Second, breaking stream-compatibility in order to introduce new features or improve performance will be *allowed* with *caution*. Such changes will be considered features, and as such will be no faster than the standard release cadence of features (i.e. on `X.Y` releases, never `X.Y.Z`). Slowness will not be considered a bug for this purpose. Correctness bug fixes that break stream-compatibility can happen on bugfix releases, per usual, but developers should consider if they can wait until the next feature release. We encourage developers to strongly weight userâ€™s pain from the break in stream-compatibility against the improvements. One example of a worthwhile improvement would be to change algorithms for a significant increase in performance, for example, moving from the [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) method of Gaussian variate generation to the faster [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm). An example of a discouraged improvement would be tweaking the Ziggurat tables just a little bit for a small performance improvement.

Any new design for the random subsystem will provide a choice of different core uniform PRNG algorithms. A promising design choice is to make these core uniform PRNGs their own lightweight objects with a minimal set of methods ([randomgen](https://github.com/bashtage/randomgen) calls them â€œBitGeneratorsâ€). The broader set of non-uniform distributions will be its own class that holds a reference to one of these core uniform PRNG objects and simply delegates to the core uniform PRNG object when it needs uniform random numbers ([randomgen](https://github.com/bashtage/randomgen) calls this the Generator). To borrow an example from [randomgen](https://github.com/bashtage/randomgen), the class `MT19937` is a BitGenerator that implements the classic Mersenne Twister algorithm. The class `Generator` wraps around the BitGenerator to provide all of the non-uniform distribution methods:

    # This is not the only way to instantiate this object.
    # This is just handy for demonstrating the delegation.
    >>> bg = MT19937(seed)
    >>> rg = Generator(bg)
    >>> x = rg.standard_normal(10)

We will be more strict about a select subset of methods on these BitGenerator objects. They MUST guarantee stream-compatibility for a specified set of methods which are chosen to make it easier to compose them to build other distributions and which are needed to abstract over the implementation details of the variety of BitGenerator algorithms. Namely,

>   - `.bytes()`
>   - `integers()` (formerly `.random_integers()`)
>   - `random()` (formerly `.random_sample()`)

The distributions class (`Generator`) SHOULD have all of the same distribution methods as `RandomState` with close-enough function signatures such that almost all code that currently works with `RandomState` instances will work with `Generator` instances (ignoring the precise stream values). Some variance will be allowed for integer distributions: in order to avoid some of the cross-platform problems described above, these SHOULD be rewritten to work with `uint64` numbers on all platforms.

### Supporting Unit Tests

Because we did make a strong stream-compatibility guarantee early in numpyâ€™s life, reliance on stream-compatibility has grown beyond reproducible simulations. One use case that remains for stream-compatibility across numpy versions is to use pseudorandom streams to generate test data in unit tests. With care, many of the cross-platform instabilities can be avoided in the context of small unit tests.

The new PRNG subsystem MUST provide a second, legacy distributions class that uses the same implementations of the distribution methods as the current version of `numpy.random.RandomState`. The methods of this class will have strict stream-compatibility guarantees, even stricter than the current policy. It is intended that this class will no longer be modified, except to keep it working when numpy internals change. All new development should go into the primary distributions class. Bug fixes that change the stream SHALL NOT be made to `RandomState`; instead, buggy distributions should be made to warn when they are buggy. The purpose of `RandomState` will be documented as providing certain fixed functionality for backwards compatibility and stable numbers for the limited purpose of unit testing, and not making whole programs reproducible across numpy versions.

This legacy distributions class MUST be accessible under the name `numpy.random.RandomState` for backwards compatibility. All current ways of instantiating `numpy.random.RandomState` with a given state should instantiate the Mersenne Twister BitGenerator with the same state. The legacy distributions class MUST be capable of accepting other BitGenerators. The purpose here is to ensure that one can write a program with a consistent BitGenerator state with a mixture of libraries that may or may not have upgraded from `RandomState`. Instances of the legacy distributions class MUST respond `True` to `isinstance(rg, numpy.random.RandomState)` because there is current utility code that relies on that check. Similarly, old pickles of `numpy.random.RandomState` instances MUST unpickle correctly.

### `numpy.random.*`

The preferred best practice for getting reproducible pseudorandom numbers is to instantiate a generator object with a seed and pass it around. The implicit global `RandomState` behind the `numpy.random.*` convenience functions can cause problems, especially when threads or other forms of concurrency are involved. Global state is always problematic. We categorically recommend avoiding using the convenience functions when reproducibility is involved.

That said, people do use them and use `numpy.random.seed()` to control the state underneath them. It can be hard to categorize and count API usages consistently and usefully, but a very common usage is in unit tests where many of the problems of global state are less likely.

This NEP does not propose removing these functions or changing them to use the less-stable `Generator` distribution implementations. Future NEPs might.

Specifically, the initial release of the new PRNG subsystem SHALL leave these convenience functions as aliases to the methods on a global `RandomState` that is initialized with a Mersenne Twister BitGenerator object. A call to `numpy.random.seed()` will be forwarded to that BitGenerator object. In addition, the global `RandomState` instance MUST be accessible in this initial release by the name `numpy.random.mtrand._rand`: Robert Kern long ago promised `scikit-learn` that this name would be stable. Whoops.

In order to allow certain workarounds, it MUST be possible to replace the BitGenerator underneath the global `RandomState` with any other BitGenerator object (we leave the precise API details up to the new subsystem). Calling `numpy.random.seed()` thereafter SHOULD just pass the given seed to the current BitGenerator object and not attempt to reset the BitGenerator to the Mersenne Twister. The set of `numpy.random.*` convenience functions SHALL remain the same as they currently are. They SHALL be aliases to the `RandomState` methods and not the new less-stable distributions class (`Generator`, in the examples above). Users who want to get the fastest, best distributions can follow best practices and instantiate generator objects explicitly.

This NEP does not propose that these requirements remain in perpetuity. After we have experience with the new PRNG subsystem, we can and should revisit these issues in future NEPs.

## Alternatives

### Versioning

For a long time, we considered that the way to allow algorithmic improvements while maintaining the stream was to apply some form of versioning. That is, every time we make a stream change in one of the distributions, we increment some version number somewhere. `numpy.random` would keep all past versions of the code, and there would be a way to get the old versions.

We will not be doing this. If one needs to get the exact bit-for-bit results from a given version of `numpy`, whether one uses random numbers or not, one should use the exact version of `numpy`.

Proposals of how to do RNG versioning varied widely, and we will not exhaustively list them here. We spent years going back and forth on these designs and were not able to find one that sufficed. Let that time lost, and more importantly, the contributors that we lost while we dithered, serve as evidence against the notion.

Concretely, adding in versioning makes maintenance of `numpy.random` difficult. Necessarily, we would be keeping lots of versions of the same code around. Adding a new algorithm safely would still be quite hard.

But most importantly, versioning is fundamentally difficult to *use* correctly. We want to make it easy and straightforward to get the latest, fastest, best versions of the distribution algorithms; otherwise, what's the point? The way to make that easy is to make the latest the default. But the default will necessarily change from release to release, so the userâ€™s code would need to be altered anyway to specify the specific version that one wants to replicate.

Adding in versioning to maintain stream-compatibility would still only provide the same level of stream-compatibility that we currently do, with all of the limitations described earlier. Given that the standard practice for such needs is to pin the release of `numpy` as a whole, versioning `RandomState` alone is superfluous.

### `StableRandom`

A previous version of this NEP proposed to leave `RandomState` completely alone for a deprecation period and build the new subsystem alongside with new names. To satisfy the unit testing use case, it proposed introducing a small distributions class nominally called `StableRandom`. It would have provided a small subset of distribution methods that were considered most useful in unit testing, but not the full set such that it would be too likely to be used outside of the testing context.

During discussion about this proposal, it became apparent that there was no satisfactory subset. At least some projects used a fairly broad selection of the `RandomState` methods in unit tests.

Downstream project owners would have been forced to modify their code to accommodate the new PRNG subsystem. Some modifications might be simply mechanical, but the bulk of the work would have been tedious churn for no positive improvement to the downstream project, just avoiding being broken.

Furthermore, under this old proposal, we would have had a quite lengthy deprecation period where `RandomState` existed alongside the new system of BitGenerator and Generator classes. Leaving the implementation of `RandomState` fixed meant that it could not use the new BitGenerator state objects. Developing programs that use a mixture of libraries that have and have not upgraded would require managing two sets of PRNG states. This would notionally have been time-limited, but we intended the deprecation to be very long.

The current proposal solves all of these problems. All current usages of `RandomState` will continue to work in perpetuity, though some may be discouraged through documentation. Unit tests can continue to use the full complement of `RandomState` methods. Mixed `RandomState/Generator` code can safely share the common BitGenerator state. Unmodified `RandomState` code can make use of the new features of alternative BitGenerator-like settable streams.

## Discussion

  - [NEP discussion](https://mail.python.org/pipermail/numpy-discussion/2018-June/078126.html)
  - [Earlier discussion](https://mail.python.org/pipermail/numpy-discussion/2018-January/077608.html)

## Copyright

This document has been placed in the public domain.

---

nep-0020-gufunc-signature-enhancement.md

---

# NEP 20 â€” Expansion of generalized universal function signatures

  - Author  
    Marten van Kerkwijk \<<mhvk@astro.utoronto.ca>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2018-06-10

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-April/077959.html>, <https://mail.python.org/pipermail/numpy-discussion/2018-May/078078.html>

<div class="note">

<div class="title">

Note

</div>

The proposal to add fixed (i) and flexible (ii) dimensions was accepted, while that to add broadcastable (iii) ones was deferred.

</div>

## Abstract

Generalized universal functions are, as their name indicates, generalization of universal functions: they operate on non-scalar elements. Their signature describes the structure of the elements they operate on, with names linking dimensions of the operands that should be the same. Here, it is proposed to extend the signature to allow the signature to indicate that a dimension (i) has fixed size; (ii) can be absent; and (iii) can be broadcast.

## Detailed description

Each part of the proposal is driven by specific needs\[1\].

1.  Fixed-size dimensions. Code working with spatial vectors often explicitly is for 2 or 3-dimensional space (e.g., the code from the [Standards Of Fundamental Astronomy](http://www.iausofa.org/), which the author hopes to wrap using gufuncs for astropy\[2\]). The signature should be able to indicate that. E.g., the signature of a function that converts a polar angle to a two-dimensional cartesian unit vector would currently have to be `()->(n)`, with there being no way to indicate that `n` has to equal 2. Indeed, this signature is particularly annoying since without putting in an output argument, the current gufunc wrapper code fails because it cannot determine `n`. Similarly, the signature for an cross product of two 3-dimensional vectors has to be `(n),(n)->(n)`, with again no way to indicate that `n` has to equal 3. Hence, the proposal here to allow one to give numerical values in addition to variable names. Thus, angle to two-dimensional unit vector would be `()->(2)`; two angles to three-dimensional unit vector `(),()->(3)`; and that for the cross product of two three-dimensional vectors would be `(3),(3)->(3)`.

2.  Possibly missing dimensions. This part is almost entirely driven by the wish to wrap `matmul` in a gufunc. `matmul` stands for matrix multiplication, and if it did only that, it could be covered with the signature `(m,n),(n,p)->(m,p)`. However, it has special cases for when a dimension is missing, allowing either argument to be treated as a single vector, with the function thus becoming, effectively, vector-matrix, matrix-vector, or vector-vector multiplication (but with no broadcasting). To support this, it is suggested to allow postfixing a dimension name with a question mark to indicate that the dimension does not necessarily have to be present.
    
    With this addition, the signature for `matmul` can be expressed as `(m?,n),(n,p?)->(m?,p?)`. This indicates that if, e.g., the second operand has only one dimension, for the purposes of the elementary function it will be treated as if that input has core shape `(n, 1)`, and the output has the corresponding core shape of `(m, 1)`. The actual output array, however, has the flexible dimension removed, i.e., it will have shape `(..., m)`. Similarly, if both arguments have only a single dimension, the inputs will be presented as having shapes `(1, n)` and `(n, 1)` to the elementary function, and the output as `(1, 1)`, while the actual output array returned will have shape `()`. In this way, the signature allows one to use a single elementary function for four related but different signatures, `(m,n),(n,p)->(m,p)`, `(n),(n,p)->(p)`, `(m,n),(n)->(m)` and `(n),(n)->()`.

3.  Dimensions that can be broadcast. For some applications, broadcasting between operands makes sense. For instance, an `all_equal` function that compares vectors in arrays could have a signature `(n),(n)->()`, but this forces both operands to be arrays, while it would be useful also to check that, e.g., all parts of a vector are constant (maybe zero). The proposal is to allow the implementer of a gufunc to indicate that a dimension can be broadcast by post-fixing the dimension name with `|1`. Hence, the signature for `all_equal` would become `(n|1),(n|1)->()`. The signature seems handy more generally for "chained ufuncs"; e.g., another application might be in a putative ufunc implementing `sumproduct`.
    
    Another example that arose in the discussion, is of a weighted mean, which might look like `weighted_mean(y, sigma[, axis, ...])`, returning the mean and its uncertainty. With a signature of `(n),(n)->(),()`, one would be forced to always give as many sigmas as there are data points, while broadcasting would allow one to give a single sigma for all points (which is still useful to calculate the uncertainty on the mean).

## Implementation

The proposed changes have all been implemented\[3\],\[4\],\[5\]. These PRs extend the ufunc structure with two new fields, each of size equal to the number of distinct dimensions, with `core_dim_sizes` holding possibly fixed sizes, and `core_dim_flags` holding flags indicating whether a dimension can be missing or broadcast. To ensure we can distinguish between this new version and previous versions, an unused entry `reserved1` is repurposed as a version number.

In the implementation, care is taken that to the elementary function flagged dimensions are not treated any differently than non-flagged ones: for instance, sizes of fixed-size dimensions are still passed on to the elementary function (but the loop can now count on that size being equal to the fixed one given in the signature).

An implementation detail to be decided upon is whether it might be handy to have a summary of all flags. This could possibly be stored in `core_enabled` (which currently is a bool), with non-zero continuing to indicate a gufunc, but specific flags indicating whether or not a gufunc uses fixed, flexible, or broadcastable dimensions.

With the above, the formal definition of the syntax would become\[6\]:

    <Signature>            ::= <Input arguments> "->" <Output arguments>
    <Input arguments>      ::= <Argument list>
    <Output arguments>     ::= <Argument list>
    <Argument list>        ::= nil | <Argument> | <Argument> "," <Argument list>
    <Argument>             ::= "(" <Core dimension list> ")"
    <Core dimension list>  ::= nil | <Core dimension> |
                               <Core dimension> "," <Core dimension list>
    <Core dimension>       ::= <Dimension name> <Dimension modifier>
    <Dimension name>       ::= valid Python variable name | valid integer
    <Dimension modifier>   ::= nil | "|1" | "?"

1.  All quotes are for clarity.
2.  Unmodified core dimensions that share the same name must have the same size. Each dimension name typically corresponds to one level of looping in the elementary function's implementation.
3.  White spaces are ignored.
4.  An integer as a dimension name freezes that dimension to the value.
5.  If a name if suffixed with the `|1` modifier, it is allowed to broadcast against other dimensions with the same name. All input dimensions must share this modifier, while no output dimensions should have it.
6.  If the name is suffixed with the `?` modifier, the dimension is a core dimension only if it exists on all inputs and outputs that share it; otherwise it is ignored (and replaced by a dimension of size 1 for the elementary function).

Examples of signatures\[7\]:

|                          |                                                                                                 |
| ------------------------ | ----------------------------------------------------------------------------------------------- |
| Signature                | Possible use                                                                                    |
| `(),()->()`              | Addition                                                                                        |
| `(i)->()`                | Sum over last axis                                                                              |
| `(i\|1),(i\|1)->()`      | Test for equality along axis, allowing comparison with a scalar                                 |
| `(i),(i)->()`            | inner vector product                                                                            |
| `(m,n),(n,p)->(m,p)`     | matrix multiplication                                                                           |
| `(n),(n,p)->(p)`         | vector-matrix multiplication                                                                    |
| `(m,n),(n)->(m)`         | matrix-vector multiplication                                                                    |
| `(m?,n),(n,p?)->(m?,p?)` | all four of the above at once, except vectors cannot have loop dimensions (ie, like `matmul`)   |
| `(3),(3)->(3)`           | cross product for 3-vectors                                                                     |
| `(i,t),(j,t)->(i,j)`     | inner over the last dimension, outer over the second to last, and loop/broadcast over the rest. |

## Backward compatibility

One possible worry is the change in ufunc structure. For most applications, which call `PyUFunc_FromDataAndSignature`, this is entirely transparent. Furthermore, by repurposing `reserved1` as a version number, code compiled against older versions of numpy will continue to work (though one will get a warning upon import of that code with a newer version of numpy), except if code explicitly changes the `reserved1` entry.

## Alternatives

It was suggested instead of extending the signature, to have multiple dispatch, so that, e.g., `matmul` would simply have the multiple signatures it supports, i.e., instead of `(m?,n),(n,p?)->(m?,p?)` one would have `(m,n),(n,p)->(m,p) | (n),(n,p)->(p) | (m,n),(n)->(m) | (n),(n)->()`. A disadvantage of this is that the developer now has to make sure that the elementary function can deal with these different signatures. Furthermore, the expansion quickly becomes cumbersome. For instance, for the `all_equal` signature of `(n|1),(n|1)->()`, one would have to have five entries: `(n),(n)->() | (n),(1)->() | (1),(n)->() | (n),()->() | (),(n)->()`. For signatures like `(m|1,n|1,o|1),(m|1,n|1,o|1)->()` (from the `cube_equal` test case in\[8\]), it is not even worth writing out the expansion.

For broadcasting, the alternative suffix of `^` was suggested (as broadcasting can be thought of as increasing the size of the array). This seems less clear. Furthermore, it was wondered whether it should not just be an all-or-nothing flag. This could be the case, though given the postfix for flexible dimensions, arguably another postfix is clearer (as is the implementation).

## Discussion

The proposals here were discussed at fair length on the mailing list\[9\], \[10\]. The main points of contention were whether the use cases were sufficiently strong. In particular, for frozen dimensions, it was argued that checks on the right number could be put in loop selection code. This seems much less clear for no benefit.

For broadcasting, the lack of examples of elementary functions that might need it was noted, with it being questioned whether something like `all_equal` was best done with a gufunc rather than as a special method on `np.equal`. One counter-argument to this would be that there is an actual PR for `all_equal`\[11\]. Another that even if one were to use a method, it would be good to be able to express their signature (just as is possible at least for `reduce` and `accumulate`).

A final argument was that we were making the gufuncs too complex. This arguably holds for the dimensions that can be omitted, but that also has the strongest use case. The frozen dimensions has a very simple implementation and its meaning is obvious. The ability to broadcast is simple too, once the flexible dimensions are supported.

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  Identified needs and suggestions for the implementation are not all by the author. In particular, the suggestion for fixed dimensions and initial implementation was by Jaime Frio ([gh-5015](https://github.com/numpy/numpy/pull/5015)), the suggestion of `?` to indicate dimensions can be omitted was by Nathaniel Smith, and the initial implementation of that by Matti Picus ([gh-11132](https://github.com/numpy/numpy/pull/11132)).

2.  [wrap ERFA functions in gufuncs](https://github.com/astropy/astropy/pull/7502) ([ERFA](https://github.com/liberfa/erfa)) is the less stringently licensed version of [Standards Of Fundamental Astronomy](http://www.iausofa.org/)

3.  [fixed-size and flexible dimensions](https://github.com/numpy/numpy/pull/11175)

4.  [broadcastable dimensions](https://github.com/numpy/numpy/pull/11179)

5.  [use in matmul](https://github.com/numpy/numpy/pull/11133)

6.  [broadcastable dimensions](https://github.com/numpy/numpy/pull/11179)

7.  [broadcastable dimensions](https://github.com/numpy/numpy/pull/11179)

8.  [broadcastable dimensions](https://github.com/numpy/numpy/pull/11179)

9.  Discusses implementations for `matmul`: <https://mail.python.org/pipermail/numpy-discussion/2018-May/077972.html>, <https://mail.python.org/pipermail/numpy-discussion/2018-May/078021.html>

10. Broadcasting: <https://mail.python.org/pipermail/numpy-discussion/2018-May/078078.html>

11. [Logical gufuncs](https://github.com/numpy/numpy/pull/8528) (includes `all_equal`)

---

nep-0021-advanced-indexing.md

---

# NEP 21 â€” Simplified and explicit advanced indexing

  - Author  
    Sebastian Berg

  - Author  
    Stephan Hoyer \<<shoyer@google.com>\>

  - Status  
    Deferred

  - Type  
    Standards Track

  - Created  
    2015-08-27

## Abstract

NumPy's "advanced" indexing support for indexing array with other arrays is one of its most powerful and popular features. Unfortunately, the existing rules for advanced indexing with multiple array indices are typically confusing to both new, and in many cases even old, users of NumPy. Here we propose an overhaul and simplification of advanced indexing, including two new "indexer" attributes `oindex` and `vindex` to facilitate explicit indexing.

## Background

### Existing indexing operations

NumPy arrays currently support a flexible range of indexing operations:

  - "Basic" indexing involving only slices, integers, `np.newaxis` and ellipsis (`...`), e.g., `x[0, :3, np.newaxis]` for selecting the first element from the 0th axis, the first three elements from the 1st axis and inserting a new axis of size 1 at the end. Basic indexing always return a view of the indexed array's data.
  - "Advanced" indexing, also called "fancy" indexing, includes all cases where arrays are indexed by other arrays. Advanced indexing always makes a copy:
      - "Boolean" indexing by boolean arrays, e.g., `x[x > 0]` for selecting positive elements.
      - "Vectorized" indexing by one or more integer arrays, e.g., `x[[0, 1]]` for selecting the first two elements along the first axis. With multiple arrays, vectorized indexing uses broadcasting rules to combine indices along multiple dimensions. This allows for producing a result of arbitrary shape with arbitrary elements from the original arrays.
      - "Mixed" indexing involving any combinations of the other advancing types. This is no more powerful than vectorized indexing, but is sometimes more convenient.

For clarity, we will refer to these existing rules as "legacy indexing". This is only a high-level summary; for more details, see NumPy's documentation and <span class="title-ref">Examples</span> below.

### Outer indexing

One broadly useful class of indexing operations is not supported:

  - "Outer" or orthogonal indexing treats one-dimensional arrays equivalently to slices for determining output shapes. The rule for outer indexing is that the result should be equivalent to independently indexing along each dimension with integer or boolean arrays as if both the indexed and indexing arrays were one-dimensional. This form of indexing is familiar to many users of other programming languages such as MATLAB, Fortran and R.

The reason why NumPy omits support for outer indexing is that the rules for outer and vectorized conflict. Consider indexing a 2D array by two 1D integer arrays, e.g., `x[[0, 1], [0, 1]]`:

  - Outer indexing is equivalent to combining multiple integer indices with `itertools.product()`. The result in this case is another 2D array with all combinations of indexed elements, e.g., `np.array([[x[0, 0], x[0, 1]], [x[1, 0], x[1, 1]]])`
  - Vectorized indexing is equivalent to combining multiple integer indices with `zip()`. The result in this case is a 1D array containing the diagonal elements, e.g., `np.array([x[0, 0], x[1, 1]])`.

This difference is a frequent stumbling block for new NumPy users. The outer indexing model is easier to understand, and is a natural generalization of slicing rules. But NumPy instead chose to support vectorized indexing, because it is strictly more powerful.

It is always possible to emulate outer indexing by vectorized indexing with the right indices. To make this easier, NumPy includes utility objects and functions such as `np.ogrid` and `np.ix_`, e.g., `x[np.ix_([0, 1], [0, 1])]`. However, there are no utilities for emulating fully general/mixed outer indexing, which could unambiguously allow for slices, integers, and 1D boolean and integer arrays.

### Mixed indexing

NumPy's existing rules for combining multiple types of indexing in the same operation are quite complex, involving a number of edge cases.

One reason why mixed indexing is particularly confusing is that at first glance the result works deceptively like outer indexing. Returning to our example of a 2D array, both `x[:2, [0, 1]]` and `x[[0, 1], :2]` return 2D arrays with axes in the same order as the original array.

However, as soon as two or more non-slice objects (including integers) are introduced, vectorized indexing rules apply. The axes introduced by the array indices are at the front, unless all array indices are consecutive, in which case NumPy deduces where the user "expects" them to be. Consider indexing a 3D array `arr` with shape `(X, Y, Z)`:

1.  `arr[:, [0, 1], 0]` has shape `(X, 2)`.
2.  `arr[[0, 1], 0, :]` has shape `(2, Z)`.
3.  `arr[0, :, [0, 1]]` has shape `(2, Y)`, not `(Y, 2)`\!

These first two cases are intuitive and consistent with outer indexing, but this last case is quite surprising, even to many highly experienced NumPy users.

Mixed cases involving multiple array indices are also surprising, and only less problematic because the current behavior is so useless that it is rarely encountered in practice. When a boolean array index is mixed with another boolean or integer array, boolean array is converted to integer array indices (equivalent to `np.nonzero()`) and then broadcast. For example, indexing a 2D array of size `(2, 2)` like `x[[True, False], [True, False]]` produces a 1D vector with shape `(1,)`, not a 2D sub-matrix with shape `(1, 1)`.

Mixed indexing seems so tricky that it is tempting to say that it never should be used. However, it is not easy to avoid, because NumPy implicitly adds full slices if there are fewer indices than the full dimensionality of the indexed array. This means that indexing a 2D array like <span class="title-ref">x\[\[0, 1\]\]</span><span class="title-ref"> is equivalent to </span><span class="title-ref">x\[\[0, 1\], :\]</span>\`. These cases are not surprising, but they constrain the behavior of mixed indexing.

### Indexing in other Python array libraries

Indexing is a useful and widely recognized mechanism for accessing multi-dimensional array data, so it is no surprise that many other libraries in the scientific Python ecosystem also support array indexing.

Unfortunately, the full complexity of NumPy's indexing rules mean that it is both challenging and undesirable for other libraries to copy its behavior in all of its nuance. The only full implementation of NumPy-style indexing is NumPy itself. This includes projects like dask.array and h5py, which support *most* types of array indexing in some form, and otherwise attempt to copy NumPy's API exactly.

Vectorized indexing in particular can be challenging to implement with array storage backends not based on NumPy. In contrast, indexing by 1D arrays along at least one dimension in the style of outer indexing is much more achievable. This has led many libraries (including dask and h5py) to attempt to define a safe subset of NumPy-style indexing that is equivalent to outer indexing, e.g., by only allowing indexing with an array along at most one dimension. However, this is quite challenging to do correctly in a general enough way to be useful. For example, the current versions of dask and h5py both handle mixed indexing in case 3 above inconsistently with NumPy. This is quite likely to lead to bugs.

These inconsistencies, in addition to the broader challenge of implementing every type of indexing logic, make it challenging to write high-level array libraries like xarray or dask.array that can interchangeably index many types of array storage. In contrast, explicit APIs for outer and vectorized indexing in NumPy would provide a model that external libraries could reliably emulate, even if they don't support every type of indexing.

## High level changes

Inspired by multiple "indexer" attributes for controlling different types of indexing behavior in pandas, we propose to:

1.  Introduce `arr.oindex[indices]` which allows array indices, but uses outer indexing logic.
2.  Introduce `arr.vindex[indices]` which use the current "vectorized"/broadcasted logic but with two differences from legacy indexing:
      - Boolean indices are not supported. All indices must be integers, integer arrays or slices.
      - The integer index result dimensions are always the first axes of the result array. No transpose is done, even for a single integer array index.
3.  Plain indexing on arrays will start to give warnings and eventually errors in cases where one of the explicit indexers should be preferred:
      - First, in all cases where legacy and outer indexing would give different results.
      - Later, potentially in all cases involving an integer array.

These constraints are sufficient for making indexing generally consistent with expectations and providing a less surprising learning curve with `oindex`.

Note that all things mentioned here apply both for assignment as well as subscription.

Understanding these details is *not* easy. The <span class="title-ref">Examples</span> section in the discussion gives code examples. And the hopefully easier <span class="title-ref">Motivational Example</span> provides some motivational use-cases for the general ideas and is likely a good start for anyone not intimately familiar with advanced indexing.

## Detailed description

### Proposed rules

From the three problems noted above some expectations for NumPy can be deduced:

1.  There should be a prominent outer/orthogonal indexing method such as `arr.oindex[indices]`.
2.  Considering how confusing vectorized/fancy indexing can be, it should be possible to be made more explicitly (e.g. `arr.vindex[indices]`).
3.  A new `arr.vindex[indices]` method, would not be tied to the confusing transpose rules of fancy indexing, which is for example needed for the simple case of a single advanced index. Thus, no transposing should be done. The axes created by the integer array indices are always inserted at the front, even for a single index.
4.  Boolean indexing is conceptionally outer indexing. Broadcasting together with other advanced indices in the manner of legacy indexing is generally not helpful or well defined. A user who wishes the "`nonzero`" plus broadcast behaviour can thus be expected to do this manually. Thus, `vindex` does not need to support boolean index arrays.
5.  An `arr.legacy_index` attribute should be implemented to support legacy indexing. This gives a simple way to update existing codebases using legacy indexing, which will make the deprecation of plain indexing behavior easier. The longer name `legacy_index` is intentionally chosen to be explicit and discourage its use in new code.
6.  Plain indexing `arr[...]` should return an error for ambiguous cases. For the beginning, this probably means cases where `arr[ind]` and `arr.oindex[ind]` return different results give deprecation warnings. This includes every use of vectorized indexing with multiple integer arrays. Due to the transposing behaviour, this means that`arr[0, :, index_arr]` will be deprecated, but `arr[:, 0, index_arr]` will not for the time being.
7.  To ensure that existing subclasses of <span class="title-ref">ndarray</span> that override indexing do not inadvertently revert to default behavior for indexing attributes, these attribute should have explicit checks that disable them if `__getitem__` or `__setitem__` has been overridden.

Unlike plain indexing, the new indexing attributes are explicitly aimed at higher dimensional indexing, several additional changes should be implemented:

  - The indexing attributes will enforce exact dimension and indexing match. This means that no implicit ellipsis (`...`) will be added. Unless an ellipsis is present the indexing expression will thus only work for an array with a specific number of dimensions. This makes the expression more explicit and safeguards against wrong dimensionality of arrays. There should be no implications for "duck typing" compatibility with builtin Python sequences, because Python sequences only support a limited form of "basic indexing" with integers and slices.
  - The current plain indexing allows for the use of non-tuples for multi-dimensional indexing such as `arr[[slice(None), 2]]`. This creates some inconsistencies and thus the indexing attributes should only allow plain python tuples for this purpose. (Whether or not this should be the case for plain indexing is a different issue.)
  - The new attributes should not use getitem to implement setitem, since it is a cludge and not useful for vectorized indexing. (not implemented yet)

### Open Questions

  - The names `oindex`, `vindex` and `legacy_index` are just suggestions at the time of writing this, another name NumPy has used for something like `oindex` is `np.ix_`. See also below.
  - `oindex` and `vindex` could always return copies, even when no array operation occurs. One argument for allowing a view return is that this way `oindex` can be used as a general index replacement. However, there is one argument for returning copies. It is possible for `arr.vindex[array_scalar, ...]`, where `array_scalar` should be a 0-D array but is not, since 0-D arrays tend to be converted. Copying always "fixes" this possible inconsistency.
  - The final state to morph plain indexing in is not fixed in this PEP. It is for example possible that <span class="title-ref">arr\[index\]</span><span class="title-ref"> will be equivalent to </span><span class="title-ref">arr.oindex</span>\` at some point in the future. Since such a change will take years, it seems unnecessary to make specific decisions at this time.
  - The proposed changes to plain indexing could be postponed indefinitely or not taken in order to not break or force major fixes to existing code bases.

### Alternative Names

Possible names suggested (more suggestions will be added).

|                |               |          |
| -------------- | ------------- | -------- |
| **Orthogonal** | oindex        | oix      |
| **Vectorized** | vindex        | vix      |
| **Legacy**     | legacy\_index | l/findex |

### Subclasses

Subclasses are a bit problematic in the light of these changes. There are some possible solutions for this. For most subclasses (those which do not provide `__getitem__` or `__setitem__`) the special attributes should just work. Subclasses that *do* provide it must be updated accordingly and should preferably not subclass `oindex` and `vindex`.

All subclasses will inherit the attributes, however, the implementation of `__getitem__` on these attributes should test `subclass.__getitem__ is ndarray.__getitem__`. If not, the subclass has special handling for indexing and `NotImplementedError` should be raised, requiring that the indexing attributes is also explicitly overwritten. Likewise, implementations of `__setitem__` should check to see if `__setitem__` is overridden.

A further question is how to facilitate implementing the special attributes. Also there is the weird functionality where `__setitem__` calls `__getitem__` for non-advanced indices. It might be good to avoid it for the new attributes, but on the other hand, that may make it even more confusing.

To facilitate implementations we could provide functions similar to `operator.itemgetter` and `operator.setitem` for the attributes. Possibly a mixin could be provided to help implementation. These improvements are not essential to the initial implementation, so they are saved for future work.

## Implementation

Implementation would start with writing special indexing objects available through `arr.oindex`, `arr.vindex`, and `arr.legacy_index` to allow these indexing operations. Also, we would need to start to deprecate those plain index operations which are not ambiguous. Furthermore, the NumPy code base will need to use the new attributes and tests will have to be adapted.

## Backward compatibility

As a new feature, no backward compatibility issues with the new `vindex` and `oindex` attributes would arise.

To facilitate backwards compatibility as much as possible, we expect a long deprecation cycle for legacy indexing behavior and propose the new `legacy_index` attribute.

Some forward compatibility issues with subclasses that do not specifically implement the new methods may arise.

## Alternatives

NumPy may not choose to offer these different type of indexing methods, or choose to only offer them through specific functions instead of the proposed notation above.

We don't think that new functions are a good alternative, because indexing notation `[]` offer some syntactic advantages in Python (i.e., direct creation of slice objects) compared to functions.

A more reasonable alternative would be write new wrapper objects for alternative indexing with functions rather than methods (e.g., `np.oindex(arr)[indices]` instead of `arr.oindex[indices]`). Functionally, this would be equivalent, but indexing is such a common operation that we think it is important to minimize syntax and worth implementing it directly on <span class="title-ref">ndarray</span> objects themselves. Indexing attributes also define a clear interface that is easier for alternative array implementations to copy, notwithstanding ongoing efforts to make it easier to override NumPy functions\[1\].

## Discussion

The original discussion about vectorized vs outer/orthogonal indexing arose on the NumPy mailing list:

>   - <https://mail.python.org/pipermail/numpy-discussion/2015-April/072550.html>

Some discussion can be found on the original pull request for this NEP:

>   - <https://github.com/numpy/numpy/pull/6256>

Python implementations of the indexing operations can be found at:

>   - <https://github.com/numpy/numpy/pull/5749>
>   - <https://gist.github.com/shoyer/c700193625347eb68fee4d1f0dc8c0c8>

### Examples

Since the various kinds of indexing is hard to grasp in many cases, these examples hopefully give some more insights. Note that they are all in terms of shape. In the examples, all original dimensions have 5 or more elements, advanced indexing inserts smaller dimensions. These examples may be hard to grasp without working knowledge of advanced indexing as of NumPy 1.9.

Example array:

    >>> arr = np.ones((5, 6, 7, 8))

## Legacy fancy indexing

Note that the same result can be achieved with `arr.legacy_index`, but the "future error" will still work in this case.

Single index is transposed (this is the same for all indexing types):

    >>> arr[[0], ...].shape
    (1, 6, 7, 8)
    >>> arr[:, [0], ...].shape
    (5, 1, 7, 8)

Multiple indices are transposed *if* consecutive:

    >>> arr[:, [0], [0], :].shape  # future error
    (5, 1, 8)
    >>> arr[:, [0], :, [0]].shape  # future error
    (1, 5, 7)

It is important to note that a scalar *is* integer array index in this sense (and gets broadcasted with the other advanced index):

    >>> arr[:, [0], 0, :].shape
    (5, 1, 8)
    >>> arr[:, [0], :, 0].shape  # future error (scalar is "fancy")
    (1, 5, 7)

Single boolean index can act on multiple dimensions (especially the whole array). It has to match (as of 1.10. a deprecation warning) the dimensions. The boolean index is otherwise identical to (multiple consecutive) integer array indices:

    >>> # Create boolean index with one True value for the last two dimensions:
    >>> bindx = np.zeros((7, 8), dtype=np.bool_)
    >>> bindx[0, 0] = True
    >>> arr[:, 0, bindx].shape
    (5, 1)
    >>> arr[0, :, bindx].shape
    (1, 6)

The combination with anything that is not a scalar is confusing, e.g.:

    >>> arr[[0], :, bindx].shape  # bindx result broadcasts with [0]
    (1, 6)
    >>> arr[:, [0, 1], bindx].shape  # IndexError

## Outer indexing

Multiple indices are "orthogonal" and their result axes are inserted at the same place (they are not broadcasted):

    >>> arr.oindex[:, [0], [0, 1], :].shape
    (5, 1, 2, 8)
    >>> arr.oindex[:, [0], :, [0, 1]].shape
    (5, 1, 7, 2)
    >>> arr.oindex[:, [0], 0, :].shape
    (5, 1, 8)
    >>> arr.oindex[:, [0], :, 0].shape
    (5, 1, 7)

Boolean indices results are always inserted where the index is:

    >>> # Create boolean index with one True value for the last two dimensions:
    >>> bindx = np.zeros((7, 8), dtype=np.bool_)
    >>> bindx[0, 0] = True
    >>> arr.oindex[:, 0, bindx].shape
    (5, 1)
    >>> arr.oindex[0, :, bindx].shape
    (6, 1)

Nothing changed in the presence of other advanced indices since:

    >>> arr.oindex[[0], :, bindx].shape
    (1, 6, 1)
    >>> arr.oindex[:, [0, 1], bindx].shape
    (5, 2, 1)

## Vectorized/inner indexing

Multiple indices are broadcasted and iterated as one like fancy indexing, but the new axes are always inserted at the front:

    >>> arr.vindex[:, [0], [0, 1], :].shape
    (2, 5, 8)
    >>> arr.vindex[:, [0], :, [0, 1]].shape
    (2, 5, 7)
    >>> arr.vindex[:, [0], 0, :].shape
    (1, 5, 8)
    >>> arr.vindex[:, [0], :, 0].shape
    (1, 5, 7)

Boolean indices results are always inserted where the index is, exactly as in `oindex` given how specific they are to the axes they operate on:

    >>> # Create boolean index with one True value for the last two dimensions:
    >>> bindx = np.zeros((7, 8), dtype=np.bool_)
    >>> bindx[0, 0] = True
    >>> arr.vindex[:, 0, bindx].shape
    (5, 1)
    >>> arr.vindex[0, :, bindx].shape
    (6, 1)

But other advanced indices are again transposed to the front:

    >>> arr.vindex[[0], :, bindx].shape
    (1, 6, 1)
    >>> arr.vindex[:, [0, 1], bindx].shape
    (2, 5, 1)

### Motivational Example

Imagine having a data acquisition software storing `D` channels and `N` datapoints along the time. She stores this into an `(N, D)` shaped array. During data analysis, we needs to fetch a pool of channels, for example to calculate a mean over them.

This data can be faked using:

    >>> arr = np.random.random((100, 10))

Now one may remember indexing with an integer array and find the correct code:

    >>> group = arr[:, [2, 5]]
    >>> mean_value = arr.mean()

However, assume that there were some specific time points (first dimension of the data) that need to be specially considered. These time points are already known and given by:

    >>> interesting_times = np.array([1, 5, 8, 10], dtype=np.intp)

Now to fetch them, we may try to modify the previous code:

    >>> group_at_it = arr[interesting_times, [2, 5]]
    IndexError: Ambiguous index, use `.oindex` or `.vindex`

An error such as this will point to read up the indexing documentation. This should make it clear, that `oindex` behaves more like slicing. So, out of the different methods it is the obvious choice (for now, this is a shape mismatch, but that could possibly also mention `oindex`):

    >>> group_at_it = arr.oindex[interesting_times, [2, 5]]

Now of course one could also have used `vindex`, but it is much less obvious how to achieve the right thing\!:

    >>> reshaped_times = interesting_times[:, np.newaxis]
    >>> group_at_it = arr.vindex[reshaped_times, [2, 5]]

One may find, that for example our data is corrupt in some places. So, we need to replace these values by zero (or anything else) for these times. The first column may for example give the necessary information, so that changing the values becomes easy remembering boolean indexing:

    >>> bad_data = arr[:, 0] > 0.5
    >>> arr[bad_data, :] = 0  # (corrupts further examples)

Again, however, the columns may need to be handled more individually (but in groups), and the `oindex` attribute works well:

    >>> arr.oindex[bad_data, [2, 5]] = 0

Note that it would be very hard to do this using legacy fancy indexing. The only way would be to create an integer array first:

    >>> bad_data_indx = np.nonzero(bad_data)[0]
    >>> bad_data_indx_reshaped = bad_data_indx[:, np.newaxis]
    >>> arr[bad_data_indx_reshaped, [2, 5]]

In any case we can use only `oindex` to do all of this without getting into any trouble or confused by the whole complexity of advanced indexing.

But, some new features are added to the data acquisition. Different sensors have to be used depending on the times. Let us assume we already have created an array of indices:

    >>> correct_sensors = np.random.randint(10, size=(100, 2))

Which lists for each time the two correct sensors in an `(N, 2)` array.

A first try to achieve this may be `arr[:, correct_sensors]` and this does not work. It should be clear quickly that slicing cannot achieve the desired thing. But hopefully users will remember that there is `vindex` as a more powerful and flexible approach to advanced indexing. One may, if trying `vindex` randomly, be confused about:

    >>> new_arr = arr.vindex[:, correct_sensors]

which is neither the same, nor the correct result (see transposing rules)\! This is because slicing works still the same in `vindex`. However, reading the documentation and examples, one can hopefully quickly find the desired solution:

    >>> rows = np.arange(len(arr))
    >>> rows = rows[:, np.newaxis]  # make shape fit with correct_sensors
    >>> new_arr = arr.vindex[rows, correct_sensors]

At this point we have left the straight forward world of `oindex` but can do random picking of any element from the array. Note that in the last example a method such as mentioned in the `Related Questions` section could be more straight forward. But this approach is even more flexible, since `rows` does not have to be a simple `arange`, but could be `interesting_times`:

    >>> interesting_times = np.array([0, 4, 8, 9, 10])
    >>> correct_sensors_at_it = correct_sensors[interesting_times, :]
    >>> interesting_times_reshaped = interesting_times[:, np.newaxis]
    >>> new_arr_it = arr[interesting_times_reshaped, correct_sensors_at_it]

Truly complex situation would arise now if you would for example pool `L` experiments into an array shaped `(L, N, D)`. But for `oindex` this should not result into surprises. `vindex`, being more powerful, will quite certainly create some confusion in this case but also cover pretty much all eventualities.

## Copyright

This document is placed under the CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\[2\].

## References and footnotes

1.  e.g., see NEP 18, <http://www.numpy.org/neps/nep-0018-array-function-protocol.html>

2.  To the extent possible under law, the person who associated CC0 with this work has waived all copyright and related or neighboring rights to this work. The CC0 license may be found at <https://creativecommons.org/publicdomain/zero/1.0/>

---

nep-0022-ndarray-duck-typing-overview.md

---

# NEP 22 â€” Duck typing for NumPy arrays â€“ high level overview

  - Author  
    Stephan Hoyer \<<shoyer@google.com>\>, Nathaniel J. Smith \<<njs@pobox.com>\>

  - Status  
    Final

  - Type  
    Informational

  - Created  
    2018-03-22

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-September/078752.html>

## Abstract

We outline a high-level vision for how NumPy will approach handling â€œduck arraysâ€. This is an Informational-class NEP; it doesnâ€™t prescribe full details for any particular implementation. In brief, we propose developing a number of new protocols for defining implementations of multi-dimensional arrays with high-level APIs matching NumPy.

## Detailed description

Traditionally, NumPyâ€™s `ndarray` objects have provided two things: a high level API for expression operations on homogeneously-typed, arbitrary-dimensional, array-structured data, and a concrete implementation of the API based on strided in-RAM storage. The API is powerful, fairly general, and used ubiquitously across the scientific Python stack. The concrete implementation, on the other hand, is suitable for a wide range of uses, but has limitations: as data sets grow and NumPy becomes used in a variety of new environments, there are increasingly cases where the strided in-RAM storage strategy is inappropriate, and users find they need sparse arrays, lazily evaluated arrays (as in dask), compressed arrays (as in blosc), arrays stored in GPU memory, arrays stored in alternative formats such as Arrow, and so forth â€“ yet users still want to work with these arrays using the familiar NumPy APIs, and reuse existing code with minimal (ideally zero) porting overhead. As a working shorthand, we call these â€œduck arraysâ€, by analogy with Pythonâ€™s â€œduck typingâ€: a â€œduck arrayâ€ is a Python object which â€œquacks likeâ€ a numpy array in the sense that it has the same or similar Python API, but doesnâ€™t share the C-level implementation.

This NEP doesnâ€™t propose any specific changes to NumPy or other projects; instead, it gives an overview of how we hope to extend NumPy to support a robust ecosystem of projects implementing and relying upon its high level API.

### Terminology

â€œDuck arrayâ€ works fine as a placeholder for now, but itâ€™s pretty jargony and may confuse new users, so we may want to pick something else for the actual API functions. Unfortunately, â€œarray-likeâ€ is already taken for the concept of â€œanything that can be coerced into an arrayâ€ (including e.g. list objects), and â€œanyarrayâ€ is already taken for the concept of â€œsomething that shares ndarrayâ€™s implementation, but has different semanticsâ€, which is the opposite of a duck array (e.g., np.matrix is an â€œanyarrayâ€, but is not a â€œduck arrayâ€). This is a classic bike-shed so for now weâ€™re just using â€œduck arrayâ€. Some possible options though include: arrayish, pseudoarray, nominalarray, ersatzarray, arraymimic, ...

### General approach

At a high level, duck array support requires working through each of the API functions provided by NumPy, and figuring out how it can be extended to work with duck array objects. In some cases this is easy (e.g., methods/attributes on ndarray itself); in other cases itâ€™s more difficult. Here are some principles weâ€™ve found useful so far:

#### Principle 1: focus on â€œfullâ€ duck arrays, but donâ€™t rule out â€œpartialâ€ duck arrays

We can distinguish between two classes:

  - â€œfullâ€ duck arrays, which aspire to fully implement np.ndarrayâ€™s Python-level APIs and work essentially anywhere that np.ndarray works
  - â€œpartialâ€ duck arrays, which intentionally implement only a subset of np.ndarrayâ€™s API.

Full duck arrays are, well, kind of boring. They have exactly the same semantics as ndarray, with differences being restricted to under-the-hood decisions about how the data is actually stored. The kind of people that are excited about making numpy more extensible are also, unsurprisingly, excited about changing or extending numpyâ€™s semantics. So thereâ€™s been a lot of discussion of how to best support partial duck arrays. We've been guilty of this ourself.

At this point though, we think the best general strategy is to focus our efforts primarily on supporting full duck arrays, and only worry about partial duck arrays as much as we need to make sure we don't accidentally rule them out for no reason.

Why focus on full duck arrays? Several reasons:

First, there are lots of very clear use cases. Potential consumers of the full duck array interface include almost every package that uses numpy (scipy, sklearn, astropy, ...), and in particular packages that provide array-wrapping-classes that handle multiple types of arrays, such as xarray and dask.array. Potential implementers of the full duck array interface include: distributed arrays, sparse arrays, masked arrays, arrays with units (unless they switch to using dtypes), labeled arrays, and so forth. Clear use cases lead to good and relevant APIs.

Second, the Anna Karenina principle applies here: full duck arrays are all alike, but every partial duck array is partial in its own way:

  - `xarray.DataArray` is mostly a duck array, but has incompatible broadcasting semantics.
  - `xarray.Dataset` wraps multiple arrays in one object; it still implements some array interfaces like `__array_ufunc__`, but certainly not all of them.
  - `pandas.Series` has methods with similar behavior to numpy, but unique null-skipping behavior.
  - scipyâ€™s `LinearOperator`s support matrix multiplication and nothing else
  - h5py and similar libraries for accessing array storage have objects that support numpy-like slicing and conversion into a full array, but not computation.
  - Some classes may be similar to ndarray, but without supporting the full indexing semantics.

And so forth.

Despite our best attempts, we haven't found any clear, unique way of slicing up the ndarray API into a hierarchy of related types that captures these distinctions; in fact, itâ€™s unlikely that any single person even understands all the distinctions. And this is important, because we have a *lot* of APIs that we need to add duck array support to (both in numpy and in all the projects that depend on numpy\!). By definition, these already work for `ndarray`, so hopefully getting them to work for full duck arrays shouldnâ€™t be so hard, since by definition full duck arrays act like `ndarray`. Itâ€™d be very cumbersome to have to go through each function and identify the exact subset of the ndarray API that it needs, then figure out which partial array types can/should support it. Once we have things working for full duck arrays, we can go back later and refine the APIs needed further as needed. Focusing on full duck arrays allows us to start making progress immediately.

In the future, it might be useful to identify specific use cases for duck arrays and standardize narrower interfaces targeted just at those use cases. For example, it might make sense to have a standard â€œarray loaderâ€ interface that file access libraries like h5py, netcdf, pydap, zarr, ... all implement, to make it easy to switch between these libraries. But thatâ€™s something that we can do as we go, and it doesnâ€™t necessarily have to involve the NumPy devs at all. For an example of what this might look like, see the documentation for [dask.array.from\_array](http://dask.pydata.org/en/latest/array-api.html#dask.array.from_array).

#### Principle 2: take advantage of duck typing

`ndarray` has a very large API surface area:

    In [1]: len(set(dir(np.ndarray)) - set(dir(object)))
    Out[1]: 138

And this is a huge **under**estimate, because there are also many free-standing functions in NumPy and other libraries which currently use the NumPy C API and thus only work on `ndarray` objects. In type theory, a type is defined by the operations you can perform on an object; thus, the actual type of `ndarray` includes not just its methods and attributes, but *all* of these functions. For duck arrays to be successful, theyâ€™ll need to implement a large proportion of the `ndarray` API â€“ but not all of it. (For example, `dask.array.Array` does not provide an equivalent to the `ndarray.ptp` method, presumably because no-one has ever noticed or cared about its absence. But this doesnâ€™t seem to have stopped people from using dask.)

This means that realistically, we canâ€™t hope to define the whole duck array API up front, or that anyone will be able to implement it all in one go; this will be an incremental process. It also means that even the so-called â€œfullâ€ duck array interface is somewhat fuzzily defined at the borders; there are parts of the `np.ndarray` API that duck arrays wonâ€™t have to implement, but we arenâ€™t entirely sure what those are.

And ultimately, it isnâ€™t really up to the NumPy developers to define what does or doesnâ€™t qualify as a duck array. If we want scikit-learn functions to work on dask arrays (for example), then thatâ€™s going to require negotiation between those two projects to discover incompatibilities, and when an incompatibility is discovered it will be up to them to negotiate who should change and how. The NumPy project can provide technical tools and general advice to help resolve these disagreements, but we canâ€™t force one group or another to take responsibility for any given bug.

Therefore, even though weâ€™re focusing on â€œfullâ€ duck arrays, we *donâ€™t* attempt to define a normative â€œarray ABCâ€ â€“ maybe this will be useful someday, but right now, itâ€™s not. And as a convenient side-effect, the lack of a normative definition leaves partial duck arrays room to experiment.

But, we do provide some more detailed advice for duck array implementers and consumers below.

#### Principle 3: focus on protocols

Historically, numpy has had lots of success at interoperating with third-party objects by defining *protocols*, like `__array__` (asks an arbitrary object to convert itself into an array), `__array_interface__` (a precursor to Pythonâ€™s buffer protocol), and `__array_ufunc__` (allows third-party objects to support ufuncs like `np.exp`).

[NEP 16](https://github.com/numpy/numpy/pull/10706) took a different approach: we need a duck-array equivalent of `asarray`, and it proposed to do this by defining a version of `asarray` that would let through objects which implemented a new AbstractArray ABC. As noted above, we now think that trying to define an ABC is a bad idea for other reasons. But when this NEP was discussed on the mailing list, we realized that even on its own merits, this idea is not so great. A better approach is to define a *method* that can be called on an arbitrary object to ask it to convert itself into a duck array, and then define a version of `asarray` that calls this method.

This is strictly more powerful: if an object is already a duck array, it can simply `return self`. It allows more correct semantics: NEP 16 assumed that `asarray(obj, dtype=X)` is the same as `asarray(obj).astype(X)`, but this isnâ€™t true. And it supports more use cases: if h5py supported sparse arrays, it might want to provide an object which is not itself a sparse array, but which can be automatically converted into a sparse array. See NEP \<XX, to be written\> for full details.

The protocol approach is also more consistent with core Python conventions: for example, see the `__iter__` method for coercing objects to iterators, or the `__index__` protocol for safe integer coercion. And finally, focusing on protocols leaves the door open for partial duck arrays, which can pick and choose which subset of the protocols they want to participate in, each of which have well-defined semantics.

Conclusion: protocols are one honking great idea â€“ letâ€™s do more of those.

#### Principle 4: reuse existing methods when possible

Itâ€™s tempting to try to define cleaned up versions of ndarray methods with a more minimal interface to allow for easier implementation. For example, `__array_reshape__` could drop some of the strange arguments accepted by `reshape` and `__array_basic_getitem__` could drop all the [strange edge cases](http://www.numpy.org/neps/nep-0021-advanced-indexing.html) of NumPyâ€™s advanced indexing.

But as discussed above, we donâ€™t really know what APIs we need for duck-typing ndarray. We would inevitably end up with a very long list of new special methods. In contrast, existing methods like `reshape` and `__getitem__` have the advantage of already being widely used/exercised by libraries that use duck arrays, and in practice, any serious duck array type is going to have to implement them anyway.

#### Principle 5: make it easy to do the right thing

Making duck arrays work well is going to be a community effort. Documentation helps, but only goes so far. We want to make it easy to implement duck arrays that do the right thing.

One way NumPy can help is by providing mixin classes for implementing large groups of related functionality at once. `NDArrayOperatorsMixin` is a good example: it allows for implementing arithmetic operators implicitly via the `__array_ufunc__` method. Itâ€™s not complete, and weâ€™ll want more helpers like that (e.g. for reductions).

(We initially thought that the importance of these mixins might be an argument for providing an array ABC, since thatâ€™s the standard way to do mixins in modern Python. But in discussion around NEP 16 we realized that partial duck arrays also wanted to take advantage of these mixins in some cases, so even if we did have an array ABC then the mixins would still need some sort of separate existence. So never mind that argument.)

### Tentative duck array guidelines

As a general rule, libraries using duck arrays should insist upon the minimum possible requirements, and libraries implementing duck arrays should provide as complete of an API as possible. This will ensure maximum compatibility. For example, users should prefer to rely on `.transpose()` rather than `.swapaxes()` (which can be implemented in terms of transpose), but duck array authors should ideally implement both.

If you are trying to implement a duck array, then you should strive to implement everything. You certainly need `.shape`, `.ndim` and `.dtype`, but also your dtype attribute should actually be a `numpy.dtype` object, weird fancy indexing edge cases should ideally work, etc. Only details related to NumPyâ€™s specific `np.ndarray` implementation (e.g., `strides`, `data`, `view`) are explicitly out of scope.

### A (very) rough sketch of future plans

The proposals discussed so far â€“ `__array_ufunc__` and some kind of `asarray` protocol â€“ are clearly necessary but not sufficient for full duck typing support. We expect the need for additional protocols to support (at least) these features:

  - **Concatenating** duck arrays, which would be used internally by other array combining methods like stack/vstack/hstack. The implementation of concatenate will need to be negotiated among the list of array arguments. We expect to use an `__array_concatenate__` protocol like `__array_ufunc__` instead of multiple dispatch.
  - **Ufunc-like functions** that currently arenâ€™t ufuncs. Many NumPy functions like median, percentile, sort, where and clip could be written as generalized ufuncs but currently arenâ€™t. Either these functions should be written as ufuncs, or we should consider adding another generic wrapper mechanism that works similarly to ufuncs but makes fewer guarantees about how the implementation is done.
  - **Random number generation** with duck arrays, e.g., `np.random.randn()`. For example, we might want to add new APIs like `random_like()` for generating new arrays with a matching shape *and* type â€“ though we'll need to look at some real examples of how these functions are used to figure out what would be helpful.
  - **Miscellaneous other functions** such as `np.einsum`, `np.zeros_like`, and `np.broadcast_to` that donâ€™t fall into any of the above categories.
  - **Checking mutability** on duck arrays, which would imply that they support assignment with `__setitem__` and the out argument to ufuncs. Many otherwise fine duck arrays are not easily mutable (for example, because they use some kinds of sparse or compressed storage, or are in read-only shared memory), and it turns out that frequently-used code like the default implementation of `np.mean` needs to check this (to decide whether it can reuse temporary arrays).

We intentionally do not describe exactly how to add support for these types of duck arrays here. These will be the subject of future NEPs.

## Copyright

This document has been placed in the public domain.

---

nep-0023-backwards-compatibility.md

---

# NEP 23 â€” Backwards compatibility and deprecation policy

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Status  
    Active

  - Type  
    Process

  - Created  
    2018-07-14

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2021-January/081423.html>

## Abstract

In this NEP we describe NumPy's approach to backwards compatibility, its deprecation and removal policy, and the trade-offs and decision processes for individual cases where breaking backwards compatibility is considered.

## Motivation and scope

NumPy has a very large user base. Those users rely on NumPy being stable and the code they write that uses NumPy functionality to keep working. NumPy is also actively maintained and improved -- and sometimes improvements require, or are made easier, by breaking backwards compatibility. Finally, there are trade-offs in stability for existing users vs. avoiding errors or having a better user experience for new users. These competing needs often give rise to long debates and delay accepting or rejecting contributions. This NEP tries to address that by providing a policy as well as examples and rationales for when it is or isn't a good idea to break backwards compatibility.

In addition, this NEP can serve as documentation for users about how the NumPy project treats backwards compatibility, and the speed at which they can expect changes to be made.

In scope for this NEP are:

  - Principles of NumPy's approach to backwards compatibility.
  - How to deprecate functionality, and when to remove already deprecated functionality.
  - Decision making process for deprecations and removals.
  - How to ensure that users are well informed about any change.

Out of scope are:

  - Making concrete decisions about deprecations of particular functionality.
  - NumPy's versioning scheme.

## General principles

When considering proposed changes that are backwards incompatible, the main principles the NumPy developers use when making a decision are:

1.  Changes need to benefit more than they harm users.
2.  NumPy is widely used, so breaking changes should be assumed by default to be harmful.
3.  Decisions should be based on how they affect users and downstream packages and should be based on usage data where possible. It does not matter whether this use contradicts the documentation or best practices.
4.  The possibility of an incorrect result is worse than an error or even crash.

When assessing the costs of proposed changes, keep in mind that most users do not read the mailing list, do not notice deprecation warnings, and sometimes wait more than one or two years before upgrading from their old version. And that NumPy has millions of users, so "no one will do or use this" is likely incorrect.

Benefits of proposed changes can include improved functionality, usability and performance, as well as lower maintenance cost and improved future extensibility.

Fixes for clear bugs are exempt from this backwards compatibility policy. However, in case of serious impact on users even bug fixes may have to be delayed for one or more releases. For example, if a downstream library would no longer build or would give incorrect results.

### Strategies related to deprecations

#### Impact assessment

Getting hard data on the impact of a deprecation of often difficult. Strategies that can be used to assess such impact include:

  - Use a code search engine (\[1\],\[2\]) or static (\[3\]) or dynamic (\[4\]) code analysis tools to determine where and how the functionality is used.
  - Test prominent downstream libraries against a development build of NumPy containing the proposed change to get real-world data on its impact.
  - Make a change on the main branch and revert it before release if it causes problems. We encourage other packages to test against NumPy's main branch and if that's too burdensome, then at least to test pre-releases. This often turns up issues quickly.

#### Alternatives to deprecations

If the impact is unclear or significant, it is often good to consider alternatives to deprecations. For example, discouraging use in documentation only, or moving the documentation for the functionality to a less prominent place or even removing it completely. Commenting on open issues related to it that they are low-prio or labeling them as "wontfix" will also be a signal to users, and reduce the maintenance effort needing to be spent.

## Implementing deprecations and removals

Deprecation warnings are necessary in all cases where functionality will eventually be removed. If there is no intent to remove functionality, then it should not be deprecated. A "please don't use this for new code" in the documentation or other type of warning should be used instead, and the documentation can be organized such that the preferred alternative is more prominently shown.

Deprecations:

  - shall include the version number of the release in which the functionality was deprecated.
  - shall include information on alternatives to the deprecated functionality, or a reason for the deprecation if no clear alternative is available. Note that release notes can include longer messages if needed.
  - shall use `DeprecationWarning` by default, and `VisibleDeprecation` for changes that need attention again after already having been deprecated or needing extra attention for some reason.
  - shall be listed in the release notes of the release where the deprecation is first present.
  - shall not be introduced in micro (bug fix) releases.
  - shall set a `stacklevel`, so the warning appears to come from the correct place.
  - shall be mentioned in the documentation for the functionality. A `.. deprecated::` directive can be used for this.

Examples of good deprecation warnings (also note standard form of the comments above the warning, helps when grepping):

`` `python     # NumPy 1.15.0, 2018-09-02     warnings.warn('np.asscalar(a) is deprecated since NumPy 1.16.0, use '                   'a.item() instead', DeprecationWarning, stacklevel=3)      # NumPy 1.15.0, 2018-02-10     warnings.warn("Importing from numpy.testing.utils is deprecated "                   "since 1.15.0, import from numpy.testing instead.",                   DeprecationWarning, stacklevel=2)      # NumPy 1.14.0, 2017-07-14     warnings.warn(         "Reading unicode strings without specifying the encoding "         "argument is deprecated since NumPy 1.14.0. Set the encoding, "         "use None for the system default.",         np.VisibleDeprecationWarning, stacklevel=2)  .. code-block:: C          /* DEPRECATED 2020-05-13, NumPy 1.20 */         if (PyErr_WarnFormat(PyExc_DeprecationWarning, 1,                 matrix_deprecation_msg, ufunc->name, "first") < 0) {             return NULL;         }  Removal of deprecated functionality:  - shall be done after at least 2 releases assuming the current 6-monthly   release cycle; if that changes, there shall be at least 1 year between   deprecation and removal. ``\` - shall be listed in the release notes of the release where the removal happened. - can be done in any minor, but not bugfix, release.

For backwards incompatible changes that aren't "deprecate and remove" but for which code will start behaving differently, a `FutureWarning` should be used. Release notes, mentioning version number and using `stacklevel` should be done in the same way as for deprecation warnings. A `.. versionchanged::` directive shall be used in the documentation after the behaviour change was made to indicate when the behavior changed:

`` `python     def argsort(self, axis=np._NoValue, ...):         """         Parameters         ----------         axis : int, optional             Axis along which to sort. If None, the default, the flattened array             is used.              ..  versionchanged:: 1.13.0                 Previously, the default was documented to be -1, but that was                 in error. At some future date, the default will change to -1, as                 originally intended.                 Until then, the axis should be given explicitly when ``arr.ndim \> 1`, to avoid a FutureWarning.         """         ...         warnings.warn(             "In the future the default for argsort will be axis=-1, not the "             "current None, to match its documentation and np.argsort. "             "Explicitly pass -1 or None to silence this warning.",             MaskedArrayFutureWarning, stacklevel=3)   Decision making`\` ---------------

In concrete cases where this policy needs to be applied, decisions are made according to the [NumPy governance model](https://docs.scipy.org/doc/numpy/dev/governance/index.html).

All deprecations must be proposed on the mailing list in order to give everyone with an interest in NumPy development a chance to comment. Removal of deprecated functionality does not need discussion on the mailing list.

### Functionality with more strict deprecation policies

  - `numpy.random` has its own backwards compatibility policy with additional requirements on top of the ones in this NEP, see [NEP 19](http://www.numpy.org/neps/nep-0019-rng-policy.html).
  - The file format of `.npy` and `.npz` files is strictly versioned independent of the NumPy version; existing format versions must remain backwards compatible even if a newer format version is introduced.

## Example cases

We now discuss a few concrete examples from NumPy's history to illustrate typical issues and trade-offs.

**Changing the behavior of a function**

`np.histogram` is probably the most infamous example. First, a new keyword `new=False` was introduced, this was then switched over to None one release later, and finally it was removed again. Also, it has a `normed` keyword that had behavior that could be considered either suboptimal or broken (depending on ones opinion on the statistics). A new keyword `density` was introduced to replace it; `normed` started giving `DeprecationWarning` only in v.1.15.0. Evolution of `histogram`:

    def histogram(a, bins=10, range=None, normed=False):  # v1.0.0
    
    def histogram(a, bins=10, range=None, normed=False, weights=None, new=False):  #v1.1.0
    
    def histogram(a, bins=10, range=None, normed=False, weights=None, new=None):  #v1.2.0
    
    def histogram(a, bins=10, range=None, normed=False, weights=None):  #v1.5.0
    
    def histogram(a, bins=10, range=None, normed=False, weights=None, density=None):  #v1.6.0
    
    def histogram(a, bins=10, range=None, normed=None, weights=None, density=None):  #v1.15.0
        # v1.15.0 was the first release where `normed` started emitting
        # DeprecationWarnings

The `new` keyword was planned from the start to be temporary. Such a plan forces users to change their code more than once, which is almost never the right thing to do. Instead, a better approach here would have been to deprecate `histogram` and introduce a new function `hist` in its place.

**Disallowing indexing with floats**

Indexing an array with floats is asking for something ambiguous, and can be a sign of a bug in user code. After some discussion, it was deemed a good idea to deprecate indexing with floats. This was first tried for the v1.8.0 release, however in pre-release testing it became clear that this would break many libraries that depend on NumPy. Therefore it was reverted before release, to give those libraries time to fix their code first. It was finally introduced for v1.11.0 and turned into a hard error for v1.12.0.

This change was disruptive, however it did catch real bugs in, e.g., SciPy and scikit-learn. Overall the change was worth the cost, and introducing it in the main branch first to allow testing, then removing it again before a release, is a useful strategy.

Similar deprecations that also look like good examples of cleanups/improvements:

  - removing deprecated boolean indexing (in 2016, see [gh-8312](https://github.com/numpy/numpy/pull/8312))
  - deprecating truth testing on empty arrays (in 2017, see [gh-9718](https://github.com/numpy/numpy/pull/9718))

**Removing the financial functions**

The financial functions (e.g. `np.pmt`) had short non-descriptive names, were present in the main NumPy namespace, and didn't really fit well within NumPy's scope. They were added in 2008 after [a discussion](https://mail.python.org/pipermail/numpy-discussion/2008-April/032353.html) on the mailing list where opinion was divided (but a majority in favor). The financial functions didn't cause a lot of overhead, however there were still multiple issues and PRs a year for them which cost maintainer time to deal with. And they cluttered up the `numpy` namespace. Discussion on removing them was discussed in 2013 (gh-2880, rejected) and in 2019 (\[NEP32\](\#nep32), accepted without significant complaints).

Given that they were clearly outside of NumPy's scope, moving them to a separate `numpy-financial` package and removing them from NumPy after a deprecation period made sense. That also gave users an easy way to update their code by doing <span class="title-ref">pip install numpy-financial</span>.

## Alternatives

**Being more aggressive with deprecations.**

The goal of being more aggressive is to allow NumPy to move forward faster. This would avoid others inventing their own solutions (often in multiple places), as well as be a benefit to users without a legacy code base. We reject this alternative because of the place NumPy has in the scientific Python ecosystem - being fairly conservative is required in order to not increase the extra maintenance for downstream libraries and end users to an unacceptable level.

## Discussion

  - [Mailing list discussion on the first version of this NEP in 2018](https://mail.python.org/pipermail/numpy-discussion/2018-July/078432.html)
  - [Mailing list discussion on the Dec 2020 update of this NEP](https://mail.python.org/pipermail/numpy-discussion/2020-December/081358.html)
  - [PR with review comments on the Dec 2020 update of this NEP](https://github.com/numpy/numpy/pull/18097)

## References and footnotes

  - [Issue requesting semantic versioning](https://github.com/numpy/numpy/issues/10156)
  - [PEP 387 - Backwards Compatibility Policy](https://www.python.org/dev/peps/pep-0387/)

## Copyright

This document has been placed in the public domain.

1.  <https://searchcode.com/>

2.  <https://sourcegraph.com/search>

3.  <https://github.com/Quansight-Labs/python-api-inspect>

4.  <https://github.com/data-apis/python-record-api>

---

nep-0024-missing-data-2.md

---

# NEP 24 â€” Missing data functionality - alternative 1 to NEP 12

  - Author  
    Nathaniel J. Smith \<<njs@pobox.com>\>, Matthew Brett \<<matthew.brett@gmail.com>\>

  - Status  
    Deferred

  - Type  
    Standards Track

  - Created  
    2011-06-30

## Abstract

*Context: this NEP was written as an alternative to NEP 12, which at the time of writing had an implementation that was merged into the NumPy main branch.*

The principle of this NEP is to separate the APIs for masking and for missing values, according to

  - The current implementation of masked arrays (NEP 12)
  - This proposal.

This discussion is only of the API, and not of the implementation.

## Detailed description

### Rationale

The purpose of this NEP is to define two interfaces -- one for handling 'missing values', and one for handling 'masked arrays'.

An ordinary value is something like an integer or a floating point number. A *missing* value is a placeholder for an ordinary value that is for some reason unavailable. For example, in working with statistical data, we often build tables in which each row represents one item, and each column represents properties of that item. For instance, we might take a group of people and for each one record height, age, education level, and income, and then stick these values into a table. But then we discover that our research assistant screwed up and forgot to record the age of one of our individuals. We could throw out the rest of their data as well, but this would be wasteful; even such an incomplete row is still perfectly usable for some analyses (e.g., we can compute the correlation of height and income). The traditional way to handle this would be to stick some particular meaningless value in for the missing data, e.g., recording this person's age as 0. But this is very error prone; we may later forget about these special values while running other analyses, and discover to our surprise that babies have higher incomes than teenagers. (In this case, the solution would be to just leave out all the items where we have no age recorded, but this isn't a general solution; many analyses require something more clever to handle missing values.) So instead of using an ordinary value like 0, we define a special "missing" value, written "NA" for "not available".

Therefore, missing values have the following properties: Like any other value, they must be supported by your array's dtype -- you can't store a floating point number in an array with dtype=int32, and you can't store an NA in it either. You need an array with dtype=NAint32 or something (exact syntax to be determined). Otherwise, they act exactly like any other values. In particular, you can apply arithmetic functions and so forth to them. By default, any function which takes an NA as an argument always returns an NA as well, regardless of the values of the other arguments. This ensures that if we try to compute the correlation of income with age, we will get "NA", meaning "given that some of the entries could be anything, the answer could be anything as well". This reminds us to spend a moment thinking about how we should rephrase our question to be more meaningful. And as a convenience for those times when you do decide that you just want the correlation between the known ages and income, then you can enable this behavior by adding a single argument to your function call.

For floating point computations, NAs and NaNs have (almost?) identical behavior. But they represent different things -- NaN an invalid computation like 0/0, NA a value that is not available -- and distinguishing between these things is useful because in some situations they should be treated differently. (For example, an imputation procedure should replace NAs with imputed values, but probably should leave NaNs alone.) And anyway, we can't use NaNs for integers, or strings, or booleans, so we need NA anyway, and once we have NA support for all these types, we might as well support it for floating point too for consistency.

A masked array is, conceptually, an ordinary rectangular numpy array, which has had an arbitrarily-shaped mask placed over it. The result is, essentially, a non-rectangular view of a rectangular array. In principle, anything you can accomplish with a masked array could also be accomplished by explicitly keeping a regular array and a boolean mask array and using numpy indexing to combine them for each operation, but combining them into a single structure is much more convenient when you need to perform complex operations on the masked view of an array, while still being able to manipulate the mask in the usual ways. Therefore, masks are preserved through indexing, and functions generally treat masked-out values as if they were not even part of the array in the first place. (Maybe this is a good heuristic: a length-4 array in which the last value has been masked out behaves just like an ordinary length-3 array, so long as you don't change the mask.) Except, of course, that you are free to manipulate the mask in arbitrary ways whenever you like; it's just a standard numpy array.

There are some simple situations where one could use either of these tools to get the job done -- or other tools entirely, like using designated surrogate values (age=0), separate mask arrays, etc. But missing values are designed to be particularly helpful in situations where the missingness is an intrinsic feature of the data -- where there's a specific value that **should** exist, if it did exist we'd it'd mean something specific, but it **doesn't**. Masked arrays are designed to be particularly helpful in situations where we just want to temporarily ignore some data that does exist, or generally when we need to work with data that has a non-rectangular shape (e.g., if you make some measurement at each point on a grid laid over a circular agar dish, then the points that fall outside the dish aren't missing measurements, they're just meaningless).

### Initialization

First, missing values can be set and be displayed as `np.NA, NA`:

    >>> np.array([1.0, 2.0, np.NA, 7.0], dtype='NA[f8]')
    array([1., 2., NA, 7.], dtype='NA[<f8]')

As the initialization is not ambiguous, this can be written without the NA dtype:

    >>> np.array([1.0, 2.0, np.NA, 7.0])
    array([1., 2., NA, 7.], dtype='NA[<f8]')

Masked values can be set and be displayed as `np.IGNORE, IGNORE`:

    >>> np.array([1.0, 2.0, np.IGNORE, 7.0], masked=True)
    array([1., 2., IGNORE, 7.], masked=True)

As the initialization is not ambiguous, this can be written without `masked=True`:

    >>> np.array([1.0, 2.0, np.IGNORE, 7.0])
    array([1., 2., IGNORE, 7.], masked=True)

### Ufuncs

By default, NA values propagate:

    >>> na_arr = np.array([1.0, 2.0, np.NA, 7.0])
    >>> np.sum(na_arr)
    NA('float64')

unless the `skipna` flag is set:

    >>> np.sum(na_arr, skipna=True)
    10.0

By default, masking does not propagate:

    >>> masked_arr = np.array([1.0, 2.0, np.IGNORE, 7.0])
    >>> np.sum(masked_arr)
    10.0

unless the `propmask` flag is set:

    >>> np.sum(masked_arr, propmask=True)
    IGNORE

An array can be masked, and contain NA values:

    >>> both_arr = np.array([1.0, 2.0, np.IGNORE, np.NA, 7.0])

In the default case, the behavior is obvious:

    >>> np.sum(both_arr)
    NA('float64')

It's also obvious what to do with `skipna=True`:

    >>> np.sum(both_arr, skipna=True)
    10.0
    >>> np.sum(both_arr, skipna=True, propmask=True)
    IGNORE

To break the tie between NA and MSK, NAs propagate harder:

    >>> np.sum(both_arr, propmask=True)
    NA('float64')

### Assignment

is obvious in the NA case:

    >>> arr = np.array([1.0, 2.0, 7.0])
    >>> arr[2] = np.NA
    TypeError('dtype does not support NA')
    >>> na_arr = np.array([1.0, 2.0, 7.0], dtype='NA[f8]')
    >>> na_arr[2] = np.NA
    >>> na_arr
    array([1., 2., NA], dtype='NA[<f8]')

Direct assignment in the masked case is magic and confusing, and so happens only via the mask:

    >>> masked_array = np.array([1.0, 2.0, 7.0], masked=True)
    >>> masked_arr[2] = np.NA
    TypeError('dtype does not support NA')
    >>> masked_arr[2] = np.IGNORE
    TypeError('float() argument must be a string or a number')
    >>> masked_arr.visible[2] = False
    >>> masked_arr
    array([1., 2., IGNORE], masked=True)

## Copyright

This document has been placed in the public domain.

---

nep-0025-missing-data-3.md

---

# NEP 25 â€” NA support via special dtypes

  - Author  
    Nathaniel J. Smith \<<njs@pobox.com>\>

  - Status  
    Deferred

  - Type  
    Standards Track

  - Created  
    2011-07-08

## Abstract

*Context: this NEP was written as an additional alternative to NEP 12 (NEP 24 is another alternative), which at the time of writing had an implementation that was merged into the NumPy main branch.*

To try and make more progress on the whole missing values/masked arrays/... debate, it seems useful to have a more technical discussion of the pieces which we *can* agree on. This is the second, which attempts to nail down the details of how NAs can be implemented using special dtype's.

### Rationale

An ordinary value is something like an integer or a floating point number. A missing value is a placeholder for an ordinary value that is for some reason unavailable. For example, in working with statistical data, we often build tables in which each row represents one item, and each column represents properties of that item. For instance, we might take a group of people and for each one record height, age, education level, and income, and then stick these values into a table. But then we discover that our research assistant screwed up and forgot to record the age of one of our individuals. We could throw out the rest of their data as well, but this would be wasteful; even such an incomplete row is still perfectly usable for some analyses (e.g., we can compute the correlation of height and income). The traditional way to handle this would be to stick some particular meaningless value in for the missing data,e.g., recording this person's age as 0. But this is very error prone; we may later forget about these special values while running other analyses, and discover to our surprise that babies have higher incomes than teenagers. (In this case, the solution would be to just leave out all the items where we have no age recorded, but this isn't a general solution; many analyses require something more clever to handle missing values.) So instead of using an ordinary value like 0, we define a special "missing" value, written "NA" for "not available".

There are several possible ways to represent such a value in memory. For instance, we could reserve a specific value (like 0, or a particular NaN, or the smallest negative integer) and then ensure that this value is treated specially by all arithmetic and other operations on our array. Another option would be to add an additional mask array next to our main array, use this to indicate which values should be treated as NA, and then extend our array operations to check this mask array whenever performing computations. Each implementation approach has various strengths and weaknesses, but here we focus on the former (value-based) approach exclusively and leave the possible addition of the latter to future discussion. The core advantages of this approach are (1) it adds no additional memory overhead, (2) it is straightforward to store and retrieve such arrays to disk using existing file storage formats, (3) it allows binary compatibility with R arrays including NA values, (4) it is compatible with the common practice of using NaN to indicate missingness when working with floating point numbers, (5) the dtype is already a place where "weird things can happen" -- there are a wide variety of dtypes that don't act like ordinary numbers (including structs, Python objects, fixed-length strings, ...), so code that accepts arbitrary NumPy arrays already has to be prepared to handle these (even if only by checking for them and raising an error). Therefore adding yet more new dtypes has less impact on extension authors than if we change the ndarray object itself.

The basic semantics of NA values are as follows. Like any other value, they must be supported by your array's dtype -- you can't store a floating point number in an array with dtype=int32, and you can't store an NA in it either. You need an array with dtype=NAint32 or something (exact syntax to be determined). Otherwise, NA values act exactly like any other values. In particular, you can apply arithmetic functions and so forth to them. By default, any function which takes an NA as an argument always returns an NA as well, regardless of the values of the other arguments. This ensures that if we try to compute the correlation of income with age, we will get "NA", meaning "given that some of the entries could be anything, the answer could be anything as well". This reminds us to spend a moment thinking about how we should rephrase our question to be more meaningful. And as a convenience for those times when you do decide that you just want the correlation between the known ages and income, then you can enable this behavior by adding a single argument to your function call.

For floating point computations, NAs and NaNs have (almost?) identical behavior. But they represent different things -- NaN an invalid computation like 0/0, NA a value that is not available -- and distinguishing between these things is useful because in some situations they should be treated differently. (For example, an imputation procedure should replace NAs with imputed values, but probably should leave NaNs alone.) And anyway, we can't use NaNs for integers, or strings, or booleans, so we need NA anyway, and once we have NA support for all these types, we might as well support it for floating point too for consistency.

## General strategy

NumPy already has a general mechanism for defining new dtypes and slotting them in so that they're supported by ndarrays, by the casting machinery, by ufuncs, and so on. In principle, we could implement NA-dtypes just using these existing interfaces. But we don't want to do that, because defining all those new ufunc loops etc. from scratch would be a huge hassle, especially since the basic functionality needed is the same in all cases. So we need some generic functionality for NAs -- but it would be better not to bake this in as a single set of special "NA types", since users may well want to define new custom dtypes that have their own NA values, and have them integrate well the rest of the NA machinery. Our strategy, therefore, is to avoid the [mid-layer mistake](https://lwn.net/Articles/336262/) by exposing some code for generic NA handling in different situations, which dtypes can selectively use or not as they choose.

  - Some example use cases:
    
    1.  We want to define a dtype that acts exactly like an int32, except that the most negative value is treated as NA.
    2.  We want to define a parametrized dtype to represent [categorical data](http://mail.scipy.org/pipermail/numpy-discussion/2010-August/052401.html), and the bit-pattern to be used for NA depends on the number of categories defined, so our code needs to play an active role handling it rather than simply deferring to the standard machinery.
    3.  We want to define a dtype that acts like an length-10 string and supports NAs. Since our string may hold arbitrary binary values, we want to actually allocate 11 bytes for it, with the first byte a flag indicating whether this string is NA and the rest containing the string content.
    4.  We want to define a dtype that allows multiple different types of NA data, which print differently and can be distinguished by the new ufunc that we define called `is_na_of_type(...)`, but otherwise takes advantage of the generic NA machinery for most operations.

## dtype C-level API extensions

The [PyArray\_Descr](http://docs.scipy.org/doc/numpy/reference/c-api.types-and-structures.html#PyArray_Descr) struct gains the following new fields:

``` c
void * NA_value;
PyArray_Descr * NA_extends;
int NA_extends_offset;
```

The following new flag values are defined:

``` c
NPY_NA_AUTO_ARRFUNCS
NPY_NA_AUTO_CAST
NPY_NA_AUTO_UFUNC
NPY_NA_AUTO_UFUNC_CHECKED
NPY_NA_AUTO_ALL /* the above flags OR'ed together */
```

The [PyArray\_ArrFuncs](http://docs.scipy.org/doc/numpy/reference/c-api.types-and-structures.html#PyArray_ArrFuncs) struct gains the following new fields:

``` c
void (*isna)(void * src, void * dst, npy_intp n, void * arr);
void (*clearna)(void * data, npy_intp n, void * arr);
```

We add at least one new convenience macro:

``` c
#define NPY_NA_SUPPORTED(dtype) ((dtype)->f->isna != NULL)
```

The general idea is that anywhere where we used to call a dtype-specific function pointer, the code will be modified to instead:

> 1.  Check for whether the relevant `NPY_NA_AUTO_...` bit is enabled, the NA\_extends field is non-NULL, and the function pointer we wanted to call is NULL.
> 2.  If these conditions are met, then use `isna` to identify which entries in the array are NA, and handle them appropriately. Then look up whatever function we were *going* to call using this dtype on the `NA_extends` dtype instead, and use that to handle the non-NA elements.

For more specifics, see following sections.

Note that if `NA_extends` points to a parametrized dtype, then the dtype object it points to must be fully specified. For example, if it is a string dtype, it must have a non-zero `elsize` field.

In order to handle the case where the NA information is stored in a field next to the <span class="title-ref">real' data, the </span><span class="title-ref">NA\_extends\_offset</span><span class="title-ref"> field is set to a non-zero value; it must point to the location within each element of this dtype where some data of the </span><span class="title-ref">NA\_extends</span>\` dtype is found. For example, if we have are storing 10-byte strings with an NA indicator byte at the beginning, then we have:

``` c
elsize == 11
NA_extends_offset == 1
NA_extends->elsize == 10
```

When delegating to the `NA_extends` dtype, we offset our data pointer by `NA_extends_offset` (while keeping our strides the same) so that it sees an array of data of the expected type (plus some superfluous padding). This is basically the same mechanism that record dtypes use, IIUC, so it should be pretty well-tested.

When delegating to a function that cannot handle "misbehaved" source data (see the `PyArray_ArrFuncs` documentation for details), then we need to check for alignment issues before delegating (especially with a non-zero `NA_extends_offset`). If there's a problem, when we need to "clean up" the source data first, using the usual mechanisms for handling misaligned data. (Of course, we should usually set up our dtypes so that there aren't any alignment issues, but someone screws that up, or decides that reduced memory usage is more important to them then fast inner loops, then we should still handle that gracefully, as we do now.)

The `NA_value` and `clearna` fields are used for various sorts of casting. `NA_value` is a bit-pattern to be used when, for example, assigning from np.NA. `clearna` can be a no-op if `elsize` and `NA_extends->elsize` are the same, but if they aren't then it should clear whatever auxiliary NA storage this dtype uses, so that none of the specified array elements are NA.

### Core dtype functions

The following functions are defined in `PyArray_ArrFuncs`. The special behavior described here is enabled by the NPY\_NA\_AUTO\_ARRFUNCS bit in the dtype flags, and only enabled if the given function field is *not* filled in.

`getitem`: Calls `isna`. If `isna` returns true, returns np.NA. Otherwise, delegates to the `NA_extends` dtype.

`setitem`: If the input object is `np.NA`, then runs `memcpy(self->NA_value, data, arr->dtype->elsize);`. Otherwise, calls `clearna`, and then delegates to the `NA_extends` dtype.

`copyswapn`, `copyswap`: FIXME: Not sure whether there's any special handling to use for these?

`compare`: FIXME: how should this handle NAs? R's sort function *discards* NAs, which doesn't seem like a good option.

`argmax`: FIXME: what is this used for? If it's the underlying implementation for np.max, then it really needs some way to get a skipna argument. If not, then the appropriate semantics depends on what it's supposed to accomplish...

`dotfunc`: QUESTION: is it actually guaranteed that everything has the same dtype? FIXME: same issues as for `argmax`.

`scanfunc`: This one's ugly. We may have to explicitly override it in all of our special dtypes, because assuming that we want the option of, say, having the token "NA" represent an NA value in a text file, we need some way to check whether that's there before delegating. But `ungetc` is only guaranteed to let us put back 1 character, and we need 2 (or maybe 3 if we actually check for "NA "). The other option would be to read to the next delimiter, check whether we have an NA, and if not then delegate to `fromstr` instead of `scanfunc`, but according to the current API, each dtype might in principle use a totally different rule for defining "the next delimiter". So... any ideas? (FIXME)

`fromstr`: Easy -- check for "NA ", if present then assign `NA_value`, otherwise call `clearna` and delegate.

`nonzero`: FIXME: again, what is this used for? (It seems redundant with using the casting machinery to cast to bool.) Probably it needs to be modified so that it can return NA, though...

`fill`: Use `isna` to check if either of the first two values is NA. If so, then fill the rest of the array with `NA_value`. Otherwise, call `clearna` and then delegate.

`fillwithvalue`: Guess this can just delegate?

`sort`, `argsort`: These should probably arrange to sort NAs to a particular place in the array (either the front or the back -- any opinions?)

`scalarkind`: FIXME: I have no idea what this does.

`castdict`, `cancastscalarkindto`, `cancastto`: See section on casting below.

### Casting

FIXME: this really needs attention from an expert on NumPy's casting rules. But I can't seem to find the docs that explain how casting loops are looked up and decided between (e.g., if you're casting from dtype A to dtype B, which dtype's loops are used?), so I can't go into details. But those details are tricky and they matter...

But the general idea is, if you have a dtype with `NPY_NA_AUTO_CAST` set, then the following conversions are automatically allowed:

>   - Casting from the underlying type to the NA-type: this is performed by the
>   - usual `clearna` + potentially-strided copy dance. Also, `isna` is
>   - called to check that none of the regular values have been accidentally
>   - converted into NA; if so, then an error is raised.
>   - Casting from the NA-type to the underlying type: allowed in principle, but if `isna` returns true for any of the values that are to be converted, then again, an error is raised. (If you want to get around this, use `np.view(array_with_NAs, dtype=float)`.)
>   - Casting between the NA-type and other types that do not support NA: this is allowed if the underlying type is allowed to cast to the other type, and is performed by combining a cast to or from the underlying type (using the above rules) with a cast to or from the other type (using the underlying type's rules).
>   - Casting between the NA-type and other types that do support NA: if the other type has NPY\_NA\_AUTO\_CAST set, then we use the above rules plus the usual dance with `isna` on one array being converted to `NA_value` elements in the other. If only one of the arrays has NPY\_NA\_AUTO\_CAST set, then it's assumed that that dtype knows what it's doing, and we don't do any magic. (But this is one of the things that I'm not sure makes sense, as per my caveat above.)

### Ufuncs

All ufuncs gain an additional optional keyword argument, `skipNA=`, which defaults to False.

If `skipNA == True`, then the ufunc machinery *unconditionally* calls `isna` for any dtype where NPY\_NA\_SUPPORTED(dtype) is true, and then acts as if any values for which isna returns True were masked out in the `where=` argument (see miniNEP 1 for the behavior of `where=`). If a `where=` argument is also given, then it acts as if the `isna` values had be ANDed out of the `where=` mask, though it does not actually modify the mask. Unlike the other changes below, this is performed *unconditionally* for any dtype which has an `isna` function defined; the NPY\_NA\_AUTO\_UFUNC flag is *not* checked.

If NPY\_NA\_AUTO\_UFUNC is set, then ufunc loop lookup is modified so that whenever it checks for the existence of a loop on the current dtype, and does not find one, then it also checks for a loop on the `NA_extends` dtype. If that loop is found, then it uses it in the normal way, with the exceptions that (1) it is only called for values which are not NA according to `isna`, (2) if the output array has NPY\_NA\_AUTO\_UFUNC set, then `clearna` is called on it before calling the ufunc loop, (3) pointer offsets are adjusted by `NA_extends_offset` before calling the ufunc loop. In addition, if NPY\_NA\_AUTO\_UFUNC\_CHECK is set, then after evaluating the ufunc loop we call `isna` on the *output* array, and if there are any NAs in the output which were not in the input, then we raise an error. (The intention of this is to catch cases where, say, we represent NA using the most-negative integer, and then someone's arithmetic overflows to create such a value by accident.)

FIXME: We should go into more detail here about how NPY\_NA\_AUTO\_UFUNC works when there are multiple input arrays, of which potentially some have the flag set and some do not.

### Printing

FIXME: There should be some sort of mechanism by which values which are NA are automatically repr'ed as NA, but I don't really understand how NumPy printing works, so I'll let someone else fill in this section.

### Indexing

Scalar indexing like `a[12]` goes via the `getitem` function, so according to the proposal as described above, if a dtype delegates `getitem`, then scalar indexing on NAs will return the object `np.NA`. (If it doesn't delegate `getitem`, of course, then it can return whatever it wants.)

This seems like the simplest approach, but an alternative would be to add a special case to scalar indexing, where if an `NPY_NA_AUTO_INDEX` flag were set, then it would call `isna` on the specified element. If this returned false, it would call `getitem` as usual; otherwise, it would return a 0-d array containing the specified element. The problem with this is that it breaks expressions like `if a[i] is np.NA: ...`. (Of course, there is nothing nearly so convenient as that for NaN values now, but then, NaN values don't have their own global singleton.) So for now we stick to scalar indexing just returning `np.NA`, but this can be revisited if anyone objects.

## Python API for generic NA support

NumPy will gain a global singleton called `numpy.NA`, similar to None, but with semantics reflecting its status as a missing value. In particular, trying to treat it as a boolean will raise an exception, and comparisons with it will produce `numpy.NA` instead of True or False. These basics are adopted from the behavior of the NA value in the R project. To dig deeper into the ideas, <http://en.wikipedia.org/wiki/Ternary_logic#Kleene_logic> provides a starting point.

Most operations on `np.NA` (e.g., `__add__`, `__mul__`) are overridden to unconditionally return `np.NA`.

The automagic dtype detection used for expressions like `np.asarray([1, 2, 3])`, `np.asarray([1.0, 2.0. 3.0])` will be extended to recognize the `np.NA` value, and use it to automatically switch to a built-in NA-enabled dtype (which one being determined by the other elements in the array). A simple `np.asarray([np.NA])` will use an NA-enabled float64 dtype (which is analogous to what you get from `np.asarray([])`). Note that this means that expressions like `np.log(np.NA)` will work: first `np.NA` will be coerced to a 0-d NA-float array, and then `np.log` will be called on that.

Python-level dtype objects gain the following new fields:

``` python
NA_supported
NA_value
```

`NA_supported` is a boolean which simply exposes the value of the `NPY_NA_SUPPORTED` flag; it should be true if this dtype allows for NAs, false otherwise. \[FIXME: would it be better to just key this off the existence of the `isna` function? Even if a dtype decides to implement all other NA handling itself, it still has to define `isna` in order to make `skipNA=` work correctly.\]

`NA_value` is a 0-d array of the given dtype, and its sole element contains the same bit-pattern as the dtype's underlying `NA_value` field. This makes it possible to determine the default bit-pattern for NA values for this type (e.g., with `np.view(mydtype.NA_value, dtype=int8)`).

We *do not* expose the `NA_extends` and `NA_extends_offset` values at the Python level, at least for now; they're considered an implementation detail (and it's easier to expose them later if they're needed then unexpose them if they aren't).

Two new ufuncs are defined: `np.isNA` returns a logical array, with true values where-ever the dtype's `isna` function returned true. `np.isnumber` is only defined for numeric dtypes, and returns True for all elements which are not NA, and for which `np.isfinite` would return True.

## Builtin NA dtypes

The above describes the generic machinery for NA support in dtypes. It's flexible enough to handle all sorts of situations, but we also want to define a few generally useful NA-supporting dtypes that are available by default.

For each built-in dtype, we define an associated NA-supporting dtype, as follows:

  - floats: the associated dtype uses a specific NaN bit-pattern to indicate NA (chosen for R compatibility)
  - complex: we do whatever R does (FIXME: look this up -- two NA floats, probably?)
  - signed integers: the most-negative signed value is used as NA (chosen for R compatibility)
  - unsigned integers: the most-positive value is used as NA (no R compatibility possible).
  - strings: the first byte (or, in the case of unicode strings, first 4 bytes) is used as a flag to indicate NA, and the rest of the data gives the actual string. (no R compatibility possible)
  - objects: Two options (FIXME): either we don't include an NA-ful version, or we use np.NA as the NA bit pattern.
  - boolean: we do whatever R does (FIXME: look this up -- 0 == FALSE, 1 == TRUE, 2 == NA?)

Each of these dtypes is trivially defined using the above machinery, and are what are automatically used by the automagic type inference machinery (for `np.asarray([True, np.NA, False])`, etc.).

They can also be accessed via a new function `np.withNA`, which takes a regular dtype (or an object that can be coerced to a dtype, like 'float') and returns one of the above dtypes. Ideally `withNA` should also take some optional arguments that let you describe which values you want to count as NA, etc., but I'll leave that for a future draft (FIXME).

FIXME: If `d` is one of the above dtypes, then should `d.type` return?

The NEP also contains a proposal for a somewhat elaborate domain-specific-language for describing NA dtypes. I'm not sure how great an idea that is. (I have a bias against using strings as data structures, and find the already existing strings confusing enough as it is -- also, apparently the NEP version of NumPy uses strings like 'f8' when printing dtypes, while my NumPy uses object names like 'float64', so I'm not sure what's going on there. `withNA(float64, arg1=value1)` seems like a more pleasant way to print a dtype than "NA\[f8,value1\]", at least to me.) But if people want it, then cool.

### Type hierarchy

FIXME: how should we do subtype checks, etc., for NA dtypes? What does `issubdtype(withNA(float), float)` return? How about `issubdtype(withNA(float), np.floating)`?

### Serialization

### Copyright

This document has been placed in the public domain.

---

nep-0026-missing-data-summary.md

---

# NEP 26 â€” Summary of missing data NEPs and discussion

  - Author  
    Mark Wiebe \<<mwwiebe@gmail.com>\>, Nathaniel J. Smith \<<njs@pobox.com>\>

  - Status  
    Deferred

  - Type  
    Standards Track

  - Created  
    2012-04-22

*Context*: this NEP was written as summary of the large number of discussions and proposals (\[NEP12\](\#nep12), \[NEP24\](\#nep24), \[NEP25\](\#nep25)), regarding missing data functionality.

The debate about how NumPy should handle missing data, a subject with many preexisting approaches, requirements, and conventions, has been long and contentious. There has been more than one proposal for how to implement support into NumPy, and there is a testable implementation which is merged into NumPy's current main. The vast number of emails and differing points of view has made it difficult for interested parties to understand the issues and be comfortable with the direction NumPy is going.

Here is our (Mark and Nathaniel's) attempt to summarize the problem, proposals, and points of agreement/disagreement in a single place, to help the community move towards consensus.

## The NumPy developers' problem

For this discussion, "missing data" means array elements which can be indexed (e.g. A\[3\] in an array A with shape (5,)), but have, in some sense, no value.

It does not refer to compressed or sparse storage techniques where the value for A\[3\] is not actually stored in memory, but still has a well-defined value like 0.

This is still vague, and to create an actual implementation, it is necessary to answer such questions as:

  - What values are computed when doing element-wise ufuncs.
  - What values are computed when doing reductions.
  - Whether the storage for an element gets overwritten when marking that value missing.
  - Whether computations resulting in NaN automatically treat in the same way as a missing value.
  - Whether one interacts with missing values using a placeholder object (e.g. called "NA" or "masked"), or through a separate boolean array.
  - Whether there is such a thing as an array object that cannot hold missing array elements.
  - How the (C and Python) API is expressed, in terms of dtypes, masks, and other constructs.
  - If we decide to answer some of these questions in multiple ways, then that creates the question of whether that requires multiple systems, and if so how they should interact.

There's clearly a very large space of missing-data APIs that *could* be implemented. There is likely at least one user, somewhere, who would find any possible implementation to be just the thing they need to solve some problem. On the other hand, much of NumPy's power and clarity comes from having a small number of orthogonal concepts, such as strided arrays, flexible indexing, broadcasting, and ufuncs, and we'd like to preserve that simplicity.

There has been dissatisfaction among several major groups of NumPy users about the existing status quo of missing data support. In particular, neither the numpy.ma component nor use of floating-point NaNs as a missing data signal fully satisfy the performance requirements and ease of use for these users. The example of R, where missing data is treated via an NA placeholder and is deeply integrated into all computation, is where many of these users point to indicate what functionality they would like. Doing a deep integration of missing data like in R must be considered carefully, it must be clear it is not being done in a way which sacrifices existing performance or functionality.

Our problem is, how can we choose some incremental additions to NumPy that will make a large class of users happy, be reasonably elegant, complement the existing design, and that we're comfortable we won't regret being stuck with in the long term.

## Prior art

So a major (maybe *the* major) problem is figuring out how ambitious the project to add missing data support to NumPy should be, and which kinds of problems are in scope. Let's start with the best understood situation where "missing data" comes into play:

### "Statistical missing data"

In statistics, social science, etc., "missing data" is a term of art referring to a specific (but extremely common and important) situation: we have tried to gather some measurements according to some scheme, but some of these measurements are missing. For example, if we have a table listing the height, age, and income of a number of individuals, but one person did not provide their income, then we need some way to represent this:

    Person | Height | Age | Income
    ------------------------------
       1   |   63   | 25  | 15000
       2   |   58   | 32  | <missing>
       3   |   71   | 45  | 30000

The traditional way is to record that income as, say, "-99", and document this in the README along with the data set. Then, you have to remember to check for and handle such incomes specially; if you forget, you'll get superficially reasonable but completely incorrect results, like calculating the average income on this data set as 14967. If you're in one of these fields, then such missing-ness is routine and inescapable, and if you use the "-99" approach then it's a pitfall you have to remember to check for explicitly on literally *every* calculation you ever do. This is, obviously, an unpleasant way to live.

Let's call this situation the "statistical missing data" situation, just to have a convenient handle for it. (As mentioned, practitioners just call this "missing data", and what to do about it is literally an entire sub-field of statistics; if you google "missing data" then every reference is on how to handle it.) NumPy isn't going to do automatic imputation or anything like that, but it could help a great deal by providing some standard way to at least represent data which is missing in this sense.

The main prior art for how this could be done comes from the S/S+/R family of languages. Their strategy is, for each type they support, to define a special value called "NA". (For ints this is INT\_MAX, for floats it's a special NaN value that's distinguishable from other NaNs, ...) Then, they arrange that in computations, this value has a special semantics that we will call "NA semantics".

### NA semantics

The idea of NA semantics is that any computations involving NA values should be consistent with what would have happened if we had known the correct value.

For example, let's say we want to compute the mean income, how might we do this? One way would be to just ignore the missing entry, and compute the mean of the remaining entries. This gives us (15000 + 30000)/2, or 22500.

Is this result consistent with discovering the income of person 2? Let's say we find out that person 2's income is 50000. This means the correct answer is (15000 + 50000 + 30000)/3, or 31666.67, indicating clearly that it is not consistent. Therefore, the mean income is NA, i.e. a specific number whose value we are unable to compute.

This motivates the following rules, which are how R implements NA:

  - Assignment:  
    NA values are understood to represent specific unknown values, and thus should have value-like semantics with respect to assignment and other basic data manipulation operations. Code which does not actually look at the values involved should work the same regardless of whether some of them are missing. For example, one might write:
    
        income[:] = income[np.argsort(height)]
    
    to perform an in-place sort of the `income` array, and know that the shortest person's income would end up being first. It turns out that the shortest person's income is not known, so the array should end up being `[NA, 15000, 30000]`, but there's nothing special about NAness here.

  - Propagation:  
    In the example above, we concluded that an operation like `mean` should produce NA when one of its data values was NA. If you ask me, "what is 3 plus x?", then my only possible answer is "I don't know what x is, so I don't know what 3 + x is either". NA means "I don't know", so 3 + NA is NA.
    
    This is important for safety when analyzing data: missing data often requires special handling for correctness -- the fact that you are missing information might mean that something you wanted to compute cannot actually be computed, and there are whole books written on how to compensate in various situations. Plus, it's easy to not realize that you have missing data, and write code that assumes you have all the data. Such code should not silently produce the wrong answer.
    
    There is an important exception to characterizing this as propagation, in the case of boolean values. Consider the calculation:
    
        v = np.any([False, False, NA, True])
    
    If we strictly propagate, `v` will become NA. However, no matter whether we place True or False into the third array position, `v` will then get the value True. The answer to the question "Is the result True consistent with later discovering the value that was missing?" is yes, so it is reasonable to not propagate here, and instead return the value True. This is what R does:
    
        > any(c(F, F, NA, T))
        [1] TRUE
        > any(c(F, F, NA, F))
        [1] NA

  - Other:  
    NaN and NA are conceptually distinct. 0.0/0.0 is not a mysterious, unknown value -- it's defined to be NaN by IEEE floating point, Not a Number. NAs are numbers (or strings, or whatever), just unknown ones. Another small but important difference is that in Python, `if NaN: ...` treats NaN as True (NaN is "truthy"); but `if NA: ...` would be an error.
    
    In R, all reduction operations implement an alternative semantics, activated by passing a special argument (`na.rm=TRUE` in R). `sum(a)` means "give me the sum of all the values" (which is NA if some of the values are NA); `sum(a, na.rm=True)` means "give me the sum of all the non-NA values".

### Other prior art

Once we move beyond the "statistical missing data" case, the correct behavior for missing data becomes less clearly defined. There are many cases where specific elements are singled out to be treated specially or excluded from computations, and these could often be conceptualized as involving 'missing data' in some sense.

In image processing, it's common to use a single image together with one or more boolean masks to e.g. composite subsets of an image. As Joe Harrington pointed out on the list, in the context of processing astronomical images, it's also common to generalize to a floating-point valued mask, or alpha channel, to indicate degrees of "missingness". We think this is out of scope for the present design, but it is an important use case, and ideally NumPy should support natural ways of manipulating such data.

After R, numpy.ma is probably the most mature source of experience on missing-data-related APIs. Its design is quite different from R; it uses different semantics -- reductions skip masked values by default and NaNs convert to masked -- and it uses a different storage strategy via a separate mask. While it seems to be generally considered sub-optimal for general use, it's hard to pin down whether this is because the API is immature but basically good, or the API is fundamentally broken, or the API is great but the code should be faster, or what. We looked at some of those users to try and get a better idea.

Matplotlib is perhaps the best known package to rely on numpy.ma. It seems to use it in two ways. One is as a way for users to indicate what data is missing when passing it to be graphed. (Other ways are also supported, e.g., passing in NaN values gives the same result.) In this regard, matplotlib treats np.ma.masked and NaN values in the same way that R's plotting routines handle NA and NaN values. For these purposes, matplotlib doesn't really care what semantics or storage strategy is used for missing data.

Internally, matplotlib uses numpy.ma arrays to store and pass around separately computed boolean masks containing 'validity' information for each input array in a cheap and non-destructive fashion. Mark's impression from some shallow code review is that mostly it works directly with the data and mask attributes of the masked arrays, not extensively using the particular computational semantics of numpy.ma. So, for this usage they do rely on the non-destructive mask-based storage, but this doesn't say much about what semantics are needed.

Paul Hobson [posted some code](https://mail.scipy.org/pipermail/numpy-discussion/2012-April/061743.html) on the list that uses numpy.ma for storing arrays of contaminant concentration measurements. Here the mask indicates whether the corresponding number represents an actual measurement, or just the estimated detection limit for a concentration which was too small to detect. Nathaniel's impression from reading through this code is that it also mostly uses the .data and .mask attributes in preference to performing operations on the MaskedArray directly.

So, these examples make it clear that there is demand for a convenient way to keep a data array and a mask array (or even a floating point array) bundled up together and "aligned". But they don't tell us much about what semantics the resulting object should have with respect to ufuncs and friends.

## Semantics, storage, API, oh my\!

We think it's useful to draw a clear line between use cases, semantics, and storage. Use cases are situations that users encounter, regardless of what NumPy does; they're the focus of the previous section. When we say *semantics*, we mean the result of different operations as viewed from the Python level without regard to the underlying implementation.

*NA semantics* are the ones described above and used by R:

    1 + NA = NA
    sum([1, 2, NA]) = NA
    NA | False = NA
    NA | True = True

With `na.rm=TRUE` or `skipNA=True`, this switches to:

    1 + NA = illegal # in R, only reductions take na.rm argument
    sum([1, 2, NA], skipNA=True) = 3

There's also been discussion of what we'll call *ignore semantics*. These are somewhat underdefined:

    sum([1, 2, IGNORED]) = 3
    # Several options here:
    1 + IGNORED = 1
    #  or
    1 + IGNORED = <leaves output array untouched>
    #  or
    1 + IGNORED = IGNORED

The numpy.ma semantics are:

    sum([1, 2, masked]) = 3
    1 + masked = masked

If either NA or ignore semantics are implemented with masks, then there is a choice of what should be done to the value in the storage for an array element which gets assigned a missing value. Three possibilities are:

  - Leave that memory untouched (the choice made in the NEP).
  - Do the calculation with the values independently of the mask (perhaps the most useful option for Paul Hobson's use-case above).
  - Copy whatever value is stored behind the input missing value into the output (this is what numpy.ma does. Even that is ambiguous in the case of `masked + masked` -- in this case numpy.ma copies the value stored behind the leftmost masked value).

When we talk about *storage*, we mean the debate about whether missing values should be represented by designating a particular value of the underlying data-type (the *bitpattern dtype* option, as used in R), or by using a separate *mask* stored alongside the data itself.

For mask-based storage, there is also an important question about what the API looks like for accessing the mask, modifying the mask, and "peeking behind" the mask.

## Designs that have been proposed

One option is to just copy R, by implementing a mechanism whereby dtypes can arrange for certain bitpatterns to be given NA semantics.

One option is to copy numpy.ma closely, but with a more optimized implementation. (Or to simply optimize the existing implementation.)

One option is that described in <span class="title-ref">NEP12</span>, for which an implementation of mask-based missing data exists. This system is roughly:

  - There is both bitpattern and mask-based missing data, and both have identical interoperable NA semantics.
  - Masks are modified by assigning np.NA or values to array elements. The way to peek behind the mask or to unmask values is to keep a view of the array that shares the data pointer but not the mask pointer.
  - Mark would like to add a way to access and manipulate the mask more directly, to be used in addition to this view-based API.
  - If an array has both a bitpattern dtype and a mask, then assigning np.NA writes to the mask, rather than to the array itself. Writing a bitpattern NA to an array which supports both requires accessing the data by "peeking under the mask".

Another option is that described in <span class="title-ref">NEP24</span>, which is to implement bitpattern dtypes with NA semantics for the "statistical missing data" use case, and to also implement a totally independent API for masked arrays with ignore semantics and all mask manipulation done explicitly through a .mask attribute.

Another option would be to define a minimalist aligned array container that holds multiple arrays and that can be used to pass them around together. It would support indexing (to help with the common problem of wanting to subset several arrays together without their becoming unaligned), but all arithmetic etc. would be done by accessing the underlying arrays directly via attributes. The "prior art" discussion above suggests that something like this holding a .data and a .mask array might actually be solve a number of people's problems without requiring any major architectural changes to NumPy. This is similar to a structured array, but with each field in a separately stored array instead of packed together.

Several people have suggested that there should be a single system that has multiple missing values that each have different semantics, e.g., a MISSING value that has NA semantics, and a separate IGNORED value that has ignored semantics.

None of these options are necessarily exclusive.

## The debate

We both are dubious of using ignored semantics as a default missing data behavior. **Nathaniel** likes NA semantics because he is most interested in the "statistical missing data" use case, and NA semantics are exactly right for that. **Mark** isn't as interested in that use case in particular, but he likes the NA computational abstraction because it is unambiguous and well-defined in all cases, and has a lot of existing experience to draw from.

What **Nathaniel** thinks, overall:

  - The "statistical missing data" use case is clear and compelling; the other use cases certainly deserve our attention, but it's hard to say what they *are* exactly yet, or even if the best way to support them is by extending the ndarray object.

  - The "statistical missing data" use case is best served by an R-style system that uses bitpattern storage to implement NA semantics. The main advantage of bitpattern storage for this use case is that it avoids the extra memory and speed overhead of storing and checking a mask (especially for the common case of floating point data, where some tricks with NaNs allow us to effectively hardware-accelerate most NA operations). These concerns alone appears to make a mask-based implementation unacceptable to many NA users, particularly in areas like neuroscience (where memory is tight) or financial modeling (where milliseconds are critical). In addition, the bit-pattern approach is less confusing conceptually (e.g., assignment really is just assignment, no magic going on behind the curtain), and it's possible to have in-memory compatibility with R for inter-language calls via rpy2. The main disadvantage of the bitpattern approach is the need to give up a value to represent NA, but this is not an issue for the most important data types (float, bool, strings, enums, objects); really, only integers are affected. And even for integers, giving up a value doesn't really matter for statistical problems. (Occupy Wall Street notwithstanding, no-one's income is 2\*\*63 - 1. And if it were, we'd be switching to floats anyway to avoid overflow.)

  - Adding new dtypes requires some cooperation with the ufunc and casting machinery, but doesn't require any architectural changes or violations of NumPy's current orthogonality.

  - His impression from the mailing list discussion, esp. the ["what can we agree on?" thread](http://thread.gmane.org/gmane.comp.python.numeric.general/49485%3E), is that many numpy.ma users specifically like the combination of masked storage, the mask being easily accessible through the API, and ignored semantics. He could be wrong, of course. But he cannot remember seeing anybody besides Mark advocate for the specific combination of masked storage and NA semantics, which makes him nervous.
    
    \_\_ <http://thread.gmane.org/gmane.comp.python.numeric.general/46704>

  - Also, he personally is not very happy with the idea of having two storage implementations that are almost-but-not-quite identical at the Python level. While there likely are people who would like to temporarily pretend that certain data is "statistically missing data" without making a copy of their array, it's not at all clear that they outnumber the people who would like to use bitpatterns and masks simultaneously for distinct purposes. And honestly he'd like to be able to just ignore masks if he wants and stick to bitpatterns, which isn't possible if they're coupled together tightly in the API. So he would say the jury is still very much out on whether this aspect of the NEP design is an advantage or a disadvantage. (Certainly he's never heard of any R users complaining that they really wish they had an option of making a different trade-off here.)

  - R's NA support is a <span class="title-ref">headline feature</span>\_\_ and its target audience consider it a compelling advantage over other platforms like Matlab or Python. Working with statistical missing data is very painful without platform support.
    
    \_\_ <http://www.sr.bham.ac.uk/~ajrs/R/why_R.html>

  - By comparison, we clearly have much more uncertainty about the use cases that require a mask-based implementation, and it doesn't seem like people will suffer too badly if they are forced for now to settle for using NumPy's excellent mask-based indexing, the new where= support, and even numpy.ma.

  - Therefore, bitpatterns with NA semantics seem to meet the criteria of making a large class of users happy, in an elegant way, that fits into the original design, and where we can have reasonable certainty that we understand the problem and use cases well enough that we'll be happy with them in the long run. But no mask-based storage proposal does, yet.

What **Mark** thinks, overall:

  - The idea of using NA semantics by default for missing data, inspired by the "statistical missing data" problem, is better than all the other default behaviors which were considered. This applies equally to the bitpattern and the masked approach.
  - For NA-style functionality to get proper support by all NumPy features and eventually all third-party libraries, it needs to be in the core. How to correctly and efficiently handle missing data differs by algorithm, and if thinking about it is required to fully support NumPy, NA support will be broader and higher quality.
  - At the same time, providing two different missing data interfaces, one for masks and one for bitpatterns, requires NumPy developers and third-party NumPy plugin developers to separately consider the question of what to do in either case, and do two additional implementations of their code. This complicates their job, and could lead to inconsistent support for missing data.
  - Providing the ability to work with both masks and bitpatterns through the same C and Python programming interface makes missing data support cleanly orthogonal with all other NumPy features.
  - There are many trade-offs of memory usage, performance, correctness, and flexibility between masks and bitpatterns. Providing support for both approaches allows users of NumPy to choose the approach which is most compatible with their way of thinking, or has characteristics which best match their use-case. Providing them through the same interface further allows them to try both with minimal effort, and choose the one which performs better or uses the least memory for their programs.
  - Memory Usage
      - With bitpatterns, less memory is used for storing a single array containing some NAs.
      - With masks, less memory is used for storing multiple arrays that are identical except for the location of their NAs. (In this case a single data array can be re-used with multiple mask arrays; bitpattern NAs would need to copy the whole data array.)
  - Performance
      - With bitpatterns, the floating point type can use native hardware operations, with nearly correct behavior. For fully correct floating point behavior and with other types, code must be written which specially tests for equality with the missing-data bitpattern.
      - With masks, there is always the overhead of accessing mask memory and testing its truth value. The implementation that currently exists has no performance tuning, so it is only good to judge a minimum performance level. Optimal mask-based code is in general going to be slower than optimal bitpattern-based code.
  - Correctness
      - Bitpattern integer types must sacrifice a valid value to represent NA. For larger integer types, there are arguments that this is ok, but for 8-bit types there is no reasonable choice. In the floating point case, if the performance of native floating point operations is chosen, there is a small inconsistency that NaN+NA and NA+NaN are different.
      - With masks, it works correctly in all cases.
  - Generality
      - The bitpattern approach can work in a fully general way only when there is a specific value which can be given up from the data type. For IEEE floating point, a NaN is an obvious choice, and for booleans represented as a byte, there are plenty of choices. For integers, a valid value must be sacrificed to use this approach. Third-party dtypes which plug into NumPy will also have to make a bitpattern choice to support this system, something which may not always be possible.
      - The mask approach works universally with all data types.

## Recommendations for moving forward

**Nathaniel** thinks we should:

  - Go ahead and implement bitpattern NAs.
  - *Don't* implement masked arrays in the core -- or at least, not yet. Instead, we should focus on figuring out how to implement them out-of-core, so that people can try out different approaches without us committing to any one approach. And so new prototypes can be released more quickly than the NumPy release cycle. And anyway, we're going to have to figure out how to experiment with such changes out-of-core if NumPy is to continue to evolve without forking -- might as well do it now. The existing code can live in the main branch, be disabled, or live its own branch -- it'll still be there once we know what we're doing.

**Mark** thinks we should:

  - The existing code should remain as is, with a global run-time experimental flag added which disables NA support by default.

A more detailed rationale for this recommendation is:

  - A solid preliminary NA-mask implementation is currently in NumPy main. This implementation has been extensively tested against scipy and other third-party packages, and has been in main in a stable state for a significant amount of time.
  - This implementation integrates deeply with the core, providing an interface which is usable in the same way R's NA support is. It provides a compelling, user-friendly answer to R's NA support.
  - The missing data NEP provides a plan for adding bitpattern-based dtype support of NAs, which will operate through the same interface but allow for the same performance/correctness tradeoffs that R has made.
  - Making it very easy for users to try out this implementation, which has reasonable feature coverage and performance characteristics, is the best way to get more concrete feedback about how NumPy's missing data support should look.

Because of its preliminary state, the existing implementation is marked as experimental in the NumPy documentation. It would be good for this to remain marked as experimental until it is more fleshed out, for example supporting struct and array dtypes and with a fuller set of NumPy operations.

I think the code should stay as it is, except to add a run-time global NumPy flag, perhaps numpy.experimental.maskna, which defaults to False and can be toggled to True. In its default state, any NA feature usage would raise an "ExperimentalError" exception, a measure which would prevent it from being accidentally used and communicate its experimental status very clearly.

The <span class="title-ref">ABI issues</span>\_\_ seem very tricky to deal with effectively in the 1.x series of releases, but I believe that with proper implementation-hiding in a 2.0 release, evolving the software to support various other ABI ideas that have been discussed is feasible. This is the approach I like best.

**Nathaniel** notes in response that he doesn't really have any objection to shipping experimental APIs in the main numpy distribution *if* we're careful to make sure that they don't "leak out" in a way that leaves us stuck with them. And in principle some sort of "this violates your warranty" global flag could be a way to do that. (In fact, this might also be a useful strategy for the kinds of changes that he favors, of adding minimal hooks to enable us to build prototypes more easily -- we could have some "rapid prototyping only" hooks that let prototype hacks get deeper access to NumPy's internals than we were otherwise ready to support.)

But, he wants to point out two things. First, it seems like we still have fundamental questions to answer about the NEP design, like whether masks should have NA semantics or ignore semantics, and there are already plans to majorly change how NEP masks are exposed and accessed. So he isn't sure what we'll learn by asking for feedback on the NEP code in its current state.

And second, given the concerns about their causing (minor) ABI issues, it's not clear that we could really prevent them from leaking out. (He looks forward to 2.0 too, but we're not there yet.) So maybe it would be better if they weren't present in the C API at all, and the hoops required for testers were instead something like, 'we have included a hacky pure-Python prototype accessible by typing "import numpy.experimental.donttrythisathome.NEP" and would welcome feedback'?

If so, then he should mention that he did implement a horribly klugy, pure Python implementation of the NEP API that works with NumPy 1.6.1. This was mostly as an experiment to see how possible such prototyping was and to test out a possible ufunc override mechanism, but if there's interest, the module is available here: <https://github.com/njsmith/numpyNEP>

It passes the maskna test-suite, with some minor issues described in a big comment at the top.

**Mark** responds:

I agree that it's important to be careful when adding new features to NumPy, but I also believe it is essential that the project have forward development momentum. A project like NumPy requires developers to write code for advancement to occur, and obstacles that impede the writing of code discourage existing developers from contributing more, and potentially scare away developers who are thinking about joining in.

All software projects, both open source and closed source, must balance between short-term practicality and long-term planning. In the case of the missing data development, there was a short-term resource commitment to tackle this problem, which is quite immense in scope. If there isn't a high likelihood of getting a contribution into NumPy that concretely advances towards a solution, I expect that individuals and companies interested in doing such work will have a much harder time justifying a commitment of their resources. For a project which is core to so many other libraries, only relying on the good will of selfless volunteers would mean that NumPy could more easily be overtaken by another project.

In the case of the existing NA contribution at issue, how we resolve this disagreement represents a decision about how NumPy's developers, contributors, and users should interact. If we create a document describing a dispute resolution process, how do we design it so that it doesn't introduce a large burden and excessive uncertainty on developers that could prevent them from productively contributing code?

If we go this route of writing up a decision process which includes such a dispute resolution mechanism, I think the meat of it should be a roadmap that potential contributors and developers can follow to gain influence over NumPy. NumPy development needs broad support beyond code contributions, and tying influence in the project to contributions seems to me like it would be a good way to encourage people to take on tasks like bug triaging/management, continuous integration/build server administration, and the myriad other tasks that help satisfy the project's needs. No specific meritocratic, democratic, consensus-striving system will satisfy everyone, but the vigour of the discussions around governance and process indicate that something at least a little bit more formal than the current status quo is necessary.

In conclusion, I would like the NumPy project to prioritize movement towards a more flexible and modular ABI/API, balanced with strong backwards-compatibility constraints and feature additions that individuals, universities, and companies want to contribute. I do not believe keeping the NA code in 1.7 as it is, with the small additional measure of requiring it to be enabled by an experimental flag, poses a risk of long-term ABI troubles. The greater risk I see is a continuing lack of developers contributing to the project, and I believe backing out this code because these worries would create a risk of reducing developer contribution.

### References and footnotes

\[NEP12\](\#nep12) describes Mark's NA-semantics/mask implementation/view based mask handling API.

\[NEP24\](\#nep24) ("the alterNEP") was Nathaniel's initial attempt at separating MISSING and IGNORED handling into bit-patterns versus masks, though there's a bunch he would change about the proposal at this point.

\[NEP25\](\#nep25) ("miniNEP 2") was a later attempt by Nathaniel to sketch out an implementation strategy for NA dtypes.

A further discussion overview page can be found at: <https://github.com/njsmith/numpy/wiki/NA-discussion-status>

### Copyright

This document has been placed in the public domain.

---

nep-0027-zero-rank-arrarys.md

---

# NEP 27 â€” Zero rank arrays

  - Author  
    Alexander Belopolsky (sasha), transcribed Matt Picus \<<matti.picus@gmail.com>\>

  - Status  
    Final

  - Type  
    Informational

  - Created  
    2006-06-10

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2018-October/078824.html>

\> **Note** \> NumPy has both zero rank arrays and scalars. This design document, adapted from a [2006 wiki entry](https://web.archive.org/web/20100503065506/http://projects.scipy.org:80/numpy/wiki/ZeroRankArray), describes what zero rank arrays are and why they exist. It was transcribed 2018-10-13 into a NEP and links were updated. The pull request sparked [a lively discussion](https://github.com/numpy/numpy/pull/12166) about the continued need for zero rank arrays and scalars in NumPy.

> Some of the information here is dated, for instance indexing of 0-D arrays now is now implemented and does not error.

## Zero-rank arrays

Zero-rank arrays are arrays with shape=(). For example:

> \>\>\> x = array(1) \>\>\> x.shape ()

## Zero-rank arrays and array scalars

Array scalars are similar to zero-rank arrays in many aspects:

    >>> int_(1).shape
    ()

They even print the same:

    >>> print int_(1)
    1
    >>> print array(1)
    1

However there are some important differences:

  - Array scalars are immutable
  - Array scalars have different python type for different data types

## Motivation for array scalars

NumPy's design decision to provide 0-d arrays and array scalars in addition to native python types goes against one of the fundamental python design principles that there should be only one obvious way to do it. In this section we will try to explain why it is necessary to have three different ways to represent a number.

There were several numpy-discussion threads:

  - [rank-0 arrays](https://mail.python.org/pipermail/numpy-discussion/2002-September/001600.html) in a 2002 mailing list thread.
  - Thoughts about zero dimensional arrays vs Python scalars in a [2005 mailing list thread](https://sourceforge.net/p/numpy/mailman/message/11299166)\]

It has been suggested several times that NumPy just use rank-0 arrays to represent scalar quantities in all case. Pros and cons of converting rank-0 arrays to scalars were summarized as follows:

  - Pros:
      - Some cases when Python expects an integer (the most dramatic is when slicing and indexing a sequence: \_PyEval\_SliceIndex in ceval.c) it will not try to convert it to an integer first before raising an error. Therefore it is convenient to have 0-dim arrays that are integers converted for you by the array object.
      - No risk of user confusion by having two types that are nearly but not exactly the same and whose separate existence can only be explained by the history of Python and NumPy development.
      - No problems with code that does explicit typechecks `(isinstance(x, float)` or `type(x) == types.FloatType)`. Although explicit typechecks are considered bad practice in general, there are a couple of valid reasons to use them.
      - No creation of a dependency on Numeric in pickle files (though this could also be done by a special case in the pickling code for arrays)
  - Cons:
      - It is difficult to write generic code because scalars do not have the same methods and attributes as arrays. (such as `.type` or `.shape`). Also Python scalars have different numeric behavior as well.
      - This results in a special-case checking that is not pleasant. Fundamentally it lets the user believe that somehow multidimensional homogeneous arrays are something like Python lists (which except for Object arrays they are not).

NumPy implements a solution that is designed to have all the pros and none of the cons above.

> Create Python scalar types for all of the 21 types and also inherit from the three that already exist. Define equivalent methods and attributes for these Python scalar types.

## The need for zero-rank arrays

Once the idea to use zero-rank arrays to represent scalars was rejected, it was natural to consider whether zero-rank arrays can be eliminated altogether. However there are some important use cases where zero-rank arrays cannot be replaced by array scalars. See also [A case for rank-0 arrays](https://mail.python.org/pipermail/numpy-discussion/2006-February/006384.html) from February 2006.

  - Output arguments:
    
        >>> y = int_(5)
        >>> add(5,5,x)
        array(10)
        >>> x
        array(10)
        >>> add(5,5,y)
        Traceback (most recent call last):
             File "<stdin>", line 1, in ?
        TypeError: return arrays must be of ArrayType

  - Shared data:
    
        >>> x = array([1,2])
        >>> y = x[1:2]
        >>> y.shape = ()
        >>> y
        array(2)
        >>> x[1] = 20
        >>> y
        array(20)

## Indexing of zero-rank arrays

As of NumPy release 0.9.3, zero-rank arrays do not support any indexing:

    >>> x[...]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: 0-d arrays can't be indexed.

On the other hand there are several cases that make sense for rank-zero arrays.

### Ellipsis and empty tuple

Alexander started a [Jan 2006 discussion](https://mail.python.org/pipermail/numpy-discussion/2006-January/005579.html) on scipy-dev with the following proposal:

> ... it may be reasonable to allow `a[...]`. This way ellipsis can be interpreted as any number of `:` s including zero. Another subscript operation that makes sense for scalars would be `a[...,newaxis]` or even `a[{newaxis, }* ..., {newaxis,}*]`, where `{newaxis,}*` stands for any number of comma-separated newaxis tokens. This will allow one to use ellipsis in generic code that would work on any numpy type.

Francesc Altet supported the idea of `[...]` on zero-rank arrays and [suggested](https://mail.python.org/pipermail/numpy-discussion/2006-January/005572.html) that `[()]` be supported as well.

Francesc's proposal was:

    In [65]: type(numpy.array(0)[...])
    Out[65]: <type 'numpy.ndarray'>
    
    In [66]: type(numpy.array(0)[()])   # Indexing a la numarray
    Out[66]: <type 'int32_arrtype'>
    
    In [67]: type(numpy.array(0).item())  # already works
    Out[67]: <type 'int'>

There is a consensus that for a zero-rank array `x`, both `x[...]` and `x[()]` should be valid, but the question remains on what should be the type of the result - zero rank ndarray or `x.dtype`?

  - (Alexander)  
    First, whatever choice is made for `x[...]` and `x[()]` they should be the same because `...` is just syntactic sugar for "as many <span class="title-ref">:</span> as necessary", which in the case of zero rank leads to `... = (:,)*0 = ()`. Second, rank zero arrays and numpy scalar types are interchangeable within numpy, but numpy scalars can be use in some python constructs where ndarrays can't. For example:
    
        >>> (1,)[array(0)]
        Traceback (most recent call last):
          File "<stdin>", line 1, in ?
        TypeError: tuple indices must be integers
        >>> (1,)[int32(0)]
        1

Since most if not all numpy function automatically convert zero-rank arrays to scalars on return, there is no reason for `[...]` and `[()]` operations to be different.

See SVN changeset 1864 (which became git commit [9024ff0](https://github.com/numpy/numpy/commit/9024ff0dc052888b5922dde0f3e615607a9e99d7)) for implementation of `x[...]` and `x[()]` returning numpy scalars.

See SVN changeset 1866 (which became git commit [743d922](https://github.com/numpy/numpy/commit/743d922bf5893acf00ac92e823fe12f460726f90)) for implementation of `x[...] = v` and `x[()] = v`

### Increasing rank with newaxis

Everyone who commented liked this feature, so as of SVN changeset 1871 (which became git commit [b32744e](https://github.com/numpy/numpy/commit/b32744e3fc5b40bdfbd626dcc1f72907d77c01c4)) any number of ellipses and newaxis tokens can be placed as a subscript argument for a zero-rank array. For example:

    >>> x = array(1)
    >>> x[newaxis,...,newaxis,...]
    array([[1]])

It is not clear why more than one ellipsis should be allowed, but this is the behavior of higher rank arrays that we are trying to preserve.

### Refactoring

Currently all indexing on zero-rank arrays is implemented in a special `if (nd == 0)` branch of code that used to always raise an index error. This ensures that the changes do not affect any existing usage (except, the usage that relies on exceptions). On the other hand part of motivation for these changes was to make behavior of ndarrays more uniform and this should allow to eliminate `if (nd == 0)` checks altogether.

## Copyright

The original document appeared on the scipy.org wiki, with no Copyright notice, and its [history](https://web.archive.org/web/20100503065506/http://projects.scipy.org:80/numpy/wiki/ZeroRankArray?action=history) attributes it to sasha.

---

nep-0028-website-redesign.md

---

# NEP 28 â€” numpy.org website redesign

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Author  
    Joe LaChance \<<joe@boldmetrics.com>\>

  - Author  
    Shekhar Rajak \<<shekharrajak.1994@gmail.com>\>

  - Status  
    Final

  - Type  
    Informational

  - Created  
    2019-07-16

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2019-August/079889.html>

## Abstract

NumPy is the fundamental library for numerical and scientific computing with Python. It is used by millions and has a large team of maintainers and contributors. Despite that, its [numpy.org](http://numpy.org) website has never received the attention it needed and deserved. We hope and intend to change that soon. This document describes ideas and requirements for how to design a replacement for the current website, to better serve the needs of our diverse community.

At a high level, what we're aiming for is:

  - a modern, clean look
  - an easy to deploy static site
  - a structure that's easy to navigate
  - content that addresses all types of stakeholders
  - Possible multilingual translations / i18n

This website serves a couple of roles:

  - it's the entry point to the project for new users
  - it should link to the documentation (which is hosted separately, now on <http://docs.scipy.org/> and in the near future on <http://numpy.org/doc>).
  - it should address various aspects of the project (e.g. what NumPy is and why you'd want to use it, community, project organization, funding, relationship with NumFOCUS and possibly other organizations)
  - it should link out to other places, so every type of stakeholder (beginning and advanced user, educators, packagers, funders, etc.) can find their way

## Motivation and scope

The current numpy.org website has almost no content and its design is poor. This affects many users, who come there looking for information. It also affects many other aspects of the NumPy project, from finding new contributors to fundraising.

The scope of the proposed redesign is the top-level numpy.org site, which now contains only a couple of pages and may contain on the order of ten pages after the redesign. Changing the documentation (user guide, reference guide, and some other pages in the NumPy Manual) is out of scope for this proposal.

## Detailed description

### User Experience

Besides the NumPy logo, there is little that can or needs to be kept from the current website. We will rely to a large extent on ideas and proposals by the designer(s) of the new website.

As reference points we can use the [Jupyter website](https://jupyter.org/), which is probably the best designed site in our ecosystem, and the [QuantEcon](https://quantecon.org) and [Julia](https://julialang.org) sites which are well-designed too.

### The Website

A static site is a must. There are many high-quality static site generators. The current website uses Sphinx, however that is not the best choice - it's hard to theme and results in sites that are too text-heavy due to Sphinx' primary aim being documentation.

The following should be considered when choosing a static site generator:

1.  *How widely used is it?* This is important when looking for help maintaining or improving the site. More popular frameworks are usually also better maintained, so less chance of bugs or obsolescence.
2.  *Ease of deployment.* Most generators meet this criterion, however things like built-in support for GitHub Pages helps.
3.  *Preferences of who implements the new site.* Everyone has their own preferences. And it's a significant amount of work to build a new site. So we should take the opinion of those doing the work into account.

#### Traffic

The current site receives on the order of 500,000 unique visitors per month. With a redesigned site and relevant content, there is potential for visitor counts to reach 5-6 million -- a similar level as [scipy.org](http://scipy.org) or [matplotlib.org](http://matplotlib.org) --or more.

#### Possible options for static site generators

1.  *Jekyll.* This is a well maintained option with 855 Github contributors, with contributions within the last month. Jekyll is written in Ruby, and has a simple CLI interface. Jekyll also has a large directory of [themes](https://jekyllthemes.io), although a majority cost money. There are several themes ([serif](https://jekyllthemes.io/theme/serif), [uBuild](https://jekyllthemes.io/theme/ubuild-jekyll-theme), [Just The Docs](https://jekyllthemes.io/theme/just-the-docs)) that are appropriate and free. Most themes are likely responsive for mobile, and that should be a requirement. Jekyll uses a combination of liquid templating and YAML to render HTML, and content is written in Markdown. i18n functionality is not native to Jekyll, but can be added easily. One nice benefit of Jekyll is that it can be run automatically by GitHub Pages, so deployment via a CI system doesn't need to be implemented.
2.  *Hugo.* This is another well maintained option with 554 contributors, with contributions within the last month. Hugo is written in Go, and similar to Jekyll, has a simple to use CLI interface to generate static sites. Again, similar to Jekyll, Hugo has a large directory of [themes](https://themes.gohugo.io). These themes appear to be free, unlike some of Jekyll's themes. ([Sample landing page theme](https://themes.gohugo.io/hugo-hero-theme), [docs theme](https://themes.gohugo.io/hugo-whisper-theme)). Hugo uses Jade as its templating language, and content is also written in Markdown. i18n functionality is native to Hugo.
3.  *Docusaurus.* Docusaurus is a responsive static site generator made by Facebook. Unlike the previous options, Docusaurus doesn't come with themes, and thus we would not want to use this for our landing page. This is an excellent docs option written in React. Docusaurus natively has support for i18n (via [Crowdin](https://crowdin.com/pricing#annual)), document versioning, and document search.

Both Jekyll and Hugo are excellent options that should be supported into the future and are good choices for NumPy. Docusaurus has several bonus features such as versioning and search that Jekyll and Hugo don't have, but is likely a poor candidate for a landing page - it could be a good option for a high-level docs site later on though.

### Deployment

There is no need for running a server, and doing so is in our experience a significant drain on the time of maintainers.

1.  *Netlify.* Using netlify is free until 100GB of bandwidth is used. Additional bandwidth costs $20/100GB. They support a global CDN system, which will keep load times quick for users in other regions. Netlify also has Github integration, which will allow for easy deployment. When a pull request is merged, Netlify will automatically deploy the changes. DNS is simple, and HTTPS is also supported.
2.  *Github Pages.* Github Pages also has a 100GB bandwidth limit, and is unclear if additional bandwidth can be purchased. It is also unclear where sites are deployed, and should be assumed sites aren't deployed globally. Github Pages has an easy to use CI & DNS, similar to Netlify. HTTPS is supported.
3.  *Cloudflare.* An excellent option, additional CI is likely needed for the same ease of deployment.

All of the above options are appropriate for the NumPy site based on current traffic. Updating to a new deployment strategy, if needed, is a minor amount of work compared to developing the website itself. If a provider such as Cloudflare is chosen, additional CI may be required, such as CircleCI, to have a similar deployment to GitHub Pages or Netlify.

### Analytics

It's beneficial to maintainers to know how many visitors are coming to numpy.org. Google Analytics offers visitor counts and locations. This will help to support and deploy more strategically, and help maintainers understand where traffic is coming from.

Google Analytics is free. A script, provided by Google, must be added to the home page.

### Website Structure

We aim to keep the first version of the new website small in terms of amount of content. New pages can be added later on, it's more important right now to get the site design right and get some essential information up. Note that in the second half of 2019 we expect to get 1 or 2 tech writers involved in the project via Google Season of Docs. They will likely help improve the content and organization of that content.

We propose the following structure:

0.  Front page: essentials of what NumPy is (compare e.g. jupyter.org), one or a couple key user stories (compare e.g. julialang.org)
1.  Install
2.  Documentation
3.  Array computing
4.  Community
5.  Learning
6.  About Us
7.  Contribute
8.  Donate

There may be a few other pages, e.g. a page on performance, that are linked from one of the main pages.

### Stakeholder Content

This should have as little content as possible *within the site*. Somewhere on the site we should link out to content that's specific to:

  - beginning users (quickstart, tutorial)
  - advanced users
  - educators
  - packagers
  - package authors that depend on NumPy
  - funders (governance, roadmap)

### Translation (multilingual / i18n)

NumPy has users all over the world. Most of those users are not native English speakers, and many don't speak English well or at all. Therefore having content in multiple languages is potentially addressing a large unmet need. It would likely also help make the NumPy project more diverse and welcoming.

On the other hand, there are good reasons why few projects have a multi-lingual site. It's potentially a lot of extra work. Extra work for maintainers is costly - they're already struggling to keep up with the work load. Therefore we have to very carefully consider whether a multi-lingual site is feasible and weight costs and benefits.

We start with an assertion: maintaining translations of all documentation, or even the whole user guide, as part of the NumPy project is not feasible. One simply has to look at the volume of our documentation and the frequency with which we change it to realize that that's the case. Perhaps it will be feasible though to translate just the top-level pages of the website. Those do not change very often, and it will be a limited amount of content (order of magnitude 5-10 pages of text).

We propose the following requirements for adding a language:

  - The language must have a dedicated maintainer
  - There must be a way to validate content changes (e.g. a second maintainer/reviewer, or high quality language support in a freely available machine translation tool)
  - The language must have a reasonable size target audience (to be assessed by the NumPy maintainers)

Furthermore we propose a policy for when to remove support for a language again (preferably by hiding it rather than deleting content). This may be done when the language no longer has a maintainer, and coverage of translations falls below an acceptable threshold (say 80%).

Benefits of having translations include:

  - Better serve many existing and potential users
  - Potentially attract a culturally and geographically more diverse set of contributors

The tradeoffs are:

  - Cost of maintaining a more complex code base
  - Cost of making decisions about whether or not to add a new language
  - Higher cost to making content changes, creates work for language maintainers
  - Any content change should be rolled out with enough delay to have translations in place

Can we define a small enough set of pages and content that it makes sense to do this? Probably yes.

Is there an easy to use tool to maintain translations and add them to the website? To be discussed - it needs investigating, and may depend on the choice of static site generator. One potential option is [Crowdin](https://crowdin.com/pricing#annual), which is free for open source projects.

### Style and graphic design

Beyond the "a modern, clean look" goal we choose to not specify too much. A designer may have much better ideas than the authors of this proposal, hence we will work with the designer(s) during the implementation phase.

The NumPy logo could use a touch-up. The logo widely recognized and its colors and design are good, however the look-and-feel is perhaps a little dated.

### Other aspects

A search box would be nice to have. The Sphinx documentation already has a search box, however a search box on the main site which provides search results for the docs, the website, and perhaps other domains that are relevant for NumPy would make sense.

## Backward compatibility

Given a static site generator is chosen, we will migrate away from Sphinx for numpy.org (the website, *not including the docs*). The current deployment can be preserved until a future deprecation date is decided (potentially based on the comfort level of our new site).

All site generators listed above have visibility into the HTML and Javascript that is generated, and can continue to be maintained in the event a given project ceases to be maintained.

## Alternatives

Alternatives we considered for the overall design of the website:

1.  *Update current site.* A new Sphinx theme could be chosen. This would likely take the least amount of resources initially, however, Sphinx does not have the features we are looking for moving forward such as i18n, responsive design, and a clean, modern look. Note that updating the docs Sphinx theme is likely still a good idea - it's orthogonal to this NEP though.
2.  *Create custom site.* This would take the most amount of resources, and is likely to have additional benefit in comparison to a static site generator. All features would be able to be added at the cost of developer time.

## Discussion

  - Pull request for this NEP (with a good amount of discussion): <https://github.com/numpy/numpy/pull/14032>
  - Email about NEP for review: <https://mail.python.org/pipermail/numpy-discussion/2019-July/079856.html>
  - Proposal to accept this NEP: <https://mail.python.org/pipermail/numpy-discussion/2019-August/079889.html>

## References and footnotes

## Copyright

This document has been placed in the public domain.

---

nep-0029-deprecation_policy.md

---

# NEP 29 â€” Recommend Python and NumPy version support as a community policy standard

  - Author  
    Thomas A Caswell \<<tcaswell@gmail.com>\>, Andreas Mueller, Brian Granger, Madicken Munk, Ralf Gommers, Matt Haberland \<<mhaberla@calpoly.edu>\>, Matthias Bussonnier \<<bussonniermatthias@gmail.com>\>, Stefan van der Walt \<<stefanv@berkeley.edu>\>

  - Status  
    Final

  - Type  
    Informational

  - Created  
    2019-07-13

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2019-October/080128.html>

<div class="note">

<div class="title">

Note

</div>

This NEP is superseded by the scientific python ecosystem coordination guideline [SPEC 0 â€” Minimum Supported Versions](https://scientific-python.org/specs/spec-0000/).

</div>

## Abstract

This NEP recommends that all projects across the Scientific Python ecosystem adopt a common "time window-based" policy for support of Python and NumPy versions. Standardizing a recommendation for project support of minimum Python and NumPy versions will improve downstream project planning.

This is an unusual NEP in that it offers recommendations for community-wide policy and not for changes to NumPy itself. Since a common place for SPEEPs (Scientific Python Ecosystem Enhancement Proposals) does not exist and given NumPy's central role in the ecosystem, a NEP provides a visible place to document the proposed policy.

This NEP is being put forward by maintainers of Matplotlib, scikit-learn, IPython, Jupyter, yt, SciPy, NumPy, and scikit-image.

## Detailed description

For the purposes of this NEP we assume semantic versioning and define:

  - *major version*  
    A release that changes the first number (e.g. X.0.0)

  - *minor version*  
    A release that changes the second number (e.g 1.Y.0)

  - *patch version*  
    A release that changes the third number (e.g. 1.1.Z)

When a project releases a new major or minor version, we recommend that they support at least all minor versions of Python introduced and released in the prior 42 months *from the anticipated release date* with a minimum of 2 minor versions of Python, and all minor versions of NumPy released in the prior 24 months *from the anticipated release date* with a minimum of 3 minor versions of NumPy.

Consider the following timeline:

    Jan 16      Jan 17      Jan 18      Jan 19      Jan 20
    |           |           |           |           |
    +++++|+++++++++++|+++++++++++|+++++++++++|+++++++++++|++++++++++++
    |              |                  |               |
    py 3.5.0       py 3.6.0           py 3.7.0        py 3.8.0
    |-----------------------------------------> Feb19
         |-----------------------------------------> Dec19
                   |-----------------------------------------> Nov20

It shows the 42 month support windows for Python. A project with a major or minor version release in February 2019 should support Python 3.5 and newer, a project with a major or minor version released in December 2019 should support Python 3.6 and newer, and a project with a major or minor version release in November 2020 should support Python 3.7 and newer.

When this NEP was drafted the Python release cadence was 18 months so a 42 month window ensured that there would always be at least two minor versions of Python in the window. The window was extended 6 months beyond the anticipated two-release interval for Python to provide resilience against small fluctuations / delays in its release schedule.

The Python release cadence increased in [PEP 0602](https://peps.python.org/pep-0602/), with releases now every 12 months, so there will be 3-4 Python releases in the support window at any time. However, PEP 0602 does not decrease the support window of Python (18 months of regular full bug-fix releases and 42 months of as-needed source-only releases). Thus, we do not expect our users to upgrade Python faster, and our 42 month support window will cover the same portion of the upstream support of any given Python release.

Because Python minor version support is based only on historical release dates, a 42 month time window, and a planned project release date, one can predict with high confidence when a project will be able to drop any given minor version of Python. This, in turn, could save months of unnecessary maintenance burden.

If a project releases immediately after a minor version of Python drops out of the support window, there will inevitably be some mismatch in supported versionsâ€”but this situation should only last until other projects in the ecosystem make releases.

Otherwise, once a project does a minor or major release, it is guaranteed that there will be a stable release of all other projects that, at the source level, support the same set of Python versions supported by the new release.

If there is a Python 4 or a NumPy 2 this policy will have to be reviewed in light of the community's and projects' best interests.

### Support Table

| Date         | Python | NumPy |
| ------------ | ------ | ----- |
| Jan 07, 2020 | 3.6+   | 1.15+ |
| Jun 23, 2020 | 3.7+   | 1.15+ |
| Jul 23, 2020 | 3.7+   | 1.16+ |
| Jan 13, 2021 | 3.7+   | 1.17+ |
| Jul 26, 2021 | 3.7+   | 1.18+ |
| Dec 22, 2021 | 3.7+   | 1.19+ |
| Dec 26, 2021 | 3.8+   | 1.19+ |
| Jun 21, 2022 | 3.8+   | 1.20+ |
| Jan 31, 2023 | 3.8+   | 1.21+ |
| Apr 14, 2023 | 3.9+   | 1.21+ |
| Jun 23, 2023 | 3.9+   | 1.22+ |
| Jan 01, 2024 | 3.9+   | 1.23+ |
| Apr 05, 2024 | 3.10+  | 1.23+ |
| Jun 22, 2024 | 3.10+  | 1.24+ |
| Dec 18, 2024 | 3.10+  | 1.25+ |
| Apr 04, 2025 | 3.11+  | 1.25+ |
| Apr 24, 2026 | 3.12+  | 1.25+ |

### Drop Schedule

    On next release, drop support for Python 3.5 (initially released on Sep 13, 2015)
    On Jan 07, 2020 drop support for NumPy 1.14 (initially released on Jan 06, 2018)
    On Jun 23, 2020 drop support for Python 3.6 (initially released on Dec 23, 2016)
    On Jul 23, 2020 drop support for NumPy 1.15 (initially released on Jul 23, 2018)
    On Jan 13, 2021 drop support for NumPy 1.16 (initially released on Jan 13, 2019)
    On Jul 26, 2021 drop support for NumPy 1.17 (initially released on Jul 26, 2019)
    On Dec 22, 2021 drop support for NumPy 1.18 (initially released on Dec 22, 2019)
    On Dec 26, 2021 drop support for Python 3.7 (initially released on Jun 27, 2018)
    On Jun 21, 2022 drop support for NumPy 1.19 (initially released on Jun 20, 2020)
    On Jan 31, 2023 drop support for NumPy 1.20 (initially released on Jan 31, 2021)
    On Apr 14, 2023 drop support for Python 3.8 (initially released on Oct 14, 2019)
    On Jun 23, 2023 drop support for NumPy 1.21 (initially released on Jun 22, 2021)
    On Jan 01, 2024 drop support for NumPy 1.22 (initially released on Dec 31, 2021)
    On Apr 05, 2024 drop support for Python 3.9 (initially released on Oct 05, 2020)
    On Jun 22, 2024 drop support for NumPy 1.23 (initially released on Jun 22, 2022)
    On Dec 18, 2024 drop support for NumPy 1.24 (initially released on Dec 18, 2022)
    On Apr 04, 2025 drop support for Python 3.10 (initially released on Oct 04, 2021)
    On Apr 24, 2026 drop support for Python 3.11 (initially released on Oct 24, 2022)

## Implementation

We suggest that all projects adopt the following language into their development guidelines:

> This project supports:
> 
>   - All minor versions of Python released 42 months prior to the project, and at minimum the two latest minor versions.
>   - All minor versions of `numpy` released in the 24 months prior to the project, and at minimum the last three minor versions.
> 
> In `setup.py`, the `python_requires` variable should be set to the minimum supported version of Python. All supported minor versions of Python should be in the test matrix and have binary artifacts built for the release.
> 
> Minimum Python and NumPy version support should be adjusted upward on every major and minor release, but never on a patch release.

## Backward compatibility

No backward compatibility issues.

## Alternatives

### Ad-Hoc version support

A project could, on every release, evaluate whether to increase the minimum version of Python supported. As a major downside, an ad-hoc approach makes it hard for downstream users to predict what the future minimum versions will be. As there is no objective threshold to when the minimum version should be dropped, it is easy for these version support discussions to devolve into [bike shedding](https://en.wikipedia.org/wiki/Wikipedia:Avoid_Parkinson%27s_bicycle-shed_effect) and acrimony.

### All CPython supported versions

The CPython supported versions of Python are listed in the Python Developers Guide and the Python PEPs. Supporting these is a very clear and conservative approach. However, it means that there exists a four year lag between when a new features is introduced into the language and when a project is able to use it. Additionally, for projects with compiled extensions this requires building many binary artifacts for each release.

For the case of NumPy, many projects carry workarounds to bugs that are fixed in subsequent versions of NumPy. Being proactive about increasing the minimum version of NumPy allows downstream packages to carry fewer version-specific patches.

### Default version on Linux distribution

The policy could be to support the version of Python that ships by default in the latest Ubuntu LTS or CentOS/RHEL release. However, we would still have to standardize across the community which distribution to follow.

By following the versions supported by major Linux distributions, we are giving up technical control of our projects to external organizations that may have different motivations and concerns than we do.

### N minor versions of Python

Given the current release cadence of the Python, the proposed time (42 months) is roughly equivalent to "the last two" Python minor versions. However, if Python changes their release cadence substantially, any rule based solely on the number of minor releases may need to be changed to remain sensible.

A more fundamental problem with a policy based on number of Python releases is that it is hard to predict when support for a given minor version of Python will be dropped as that requires correctly predicting the release schedule of Python for the next 3-4 years. A time-based rule, in contrast, only depends on past events and the length of the support window.

### Time window from the X.Y.1 Python release

This is equivalent to a few month longer support window from the X.Y.0 release. This is because X.Y.1 bug-fix release is typically a few months after the X.Y.0 release, thus a N month window from X.Y.1 is roughly equivalent to a N+3 month from X.Y.0.

The X.Y.0 release is naturally a special release. If we were to anchor the window on X.Y.1 we would then have the discussion of why not X.Y.M?

## Discussion

## References and footnotes

Code to generate support and drop schedule tables :

    from datetime import datetime, timedelta
    
    data = """Jan 15, 2017: NumPy 1.12
    Sep 13, 2015: Python 3.5
    Dec 23, 2016: Python 3.6
    Jun 27, 2018: Python 3.7
    Jun 07, 2017: NumPy 1.13
    Jan 06, 2018: NumPy 1.14
    Jul 23, 2018: NumPy 1.15
    Jan 13, 2019: NumPy 1.16
    Jul 26, 2019: NumPy 1.17
    Oct 14, 2019: Python 3.8
    Dec 22, 2019: NumPy 1.18
    Jun 20, 2020: NumPy 1.19
    Oct 05, 2020: Python 3.9
    Jan 30, 2021: NumPy 1.20
    Jun 22, 2021: NumPy 1.21
    Oct 04, 2021: Python 3.10
    Dec 31, 2021: NumPy 1.22
    Jun 22, 2022: NumPy 1.23
    Oct 24, 2022: Python 3.11
    Dec 18, 2022: NumPy 1.24
    """
    
    releases = []
    
    plus42 = timedelta(days=int(365*3.5 + 1))
    plus24 = timedelta(days=int(365*2 + 1))
    
    for line in data.splitlines():
        date, project_version = line.split(':')
        project, version = project_version.strip().split(' ')
        release = datetime.strptime(date, '%b %d, %Y')
        if project.lower() == 'numpy':
            drop = release + plus24
        else:
            drop = release + plus42
        releases.append((drop, project, version, release))
    
    releases = sorted(releases, key=lambda x: x[0])

    py_major,py_minor = sorted([int(x) for x in r[2].split('.')] for r in releases if r[1] == 'Python')[-1]
    minpy = f"{py_major}.{py_minor+1}+"
    
    num_major,num_minor = sorted([int(x) for x in r[2].split('.')] for r in releases if r[1] == 'NumPy')[-1]
    minnum = f"{num_major}.{num_minor+1}+"
    
    toprint_drop_dates = ['']
    toprint_support_table = []
    for d, p, v, r in releases[::-1]:
        df = d.strftime('%b %d, %Y')
        toprint_drop_dates.append(
            f'On {df} drop support for {p} {v} '
            f'(initially released on {r.strftime("%b %d, %Y")})')
        toprint_support_table.append(f'{df} {minpy:<6} {minnum:<5}')
        if p.lower() == 'numpy':
            minnum = v+'+'
        else:
            minpy = v+'+'
    print("On next release, drop support for Python 3.5 (initially released on Sep 13, 2015)")
    for e in toprint_drop_dates[-4::-1]:
        print(e)
    
    print('============ ====== =====')
    print('Date         Python NumPy')
    print('------------ ------ -----')
    for e in toprint_support_table[-4::-1]:
        print(e)
    print('============ ====== =====')

## Copyright

This document has been placed in the public domain.

---

nep-0030-duck-array-protocol.md

---

# NEP 30 â€” Duck typing for NumPy arrays - implementation

  - Author  
    Peter Andreas Entschev \<<pentschev@nvidia.com>\>

  - Author  
    Stephan Hoyer \<<shoyer@google.com>\>

  - Status  
    Superseded

  - Replaced-By  
    \[NEP56\](\#nep56)

  - Type  
    Standards Track

  - Created  
    2019-07-31

  - Updated  
    2019-07-31

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/message/Z6AA5CL47NHBNEPTFWYOTSUVSRDGHYPN/>

## Abstract

We propose the `__duckarray__` protocol, following the high-level overview described in NEP 22, allowing downstream libraries to return arrays of their defined types, in contrast to `np.asarray`, that coerces those `array_like` objects to NumPy arrays.

## Detailed description

NumPy's API, including array definitions, is implemented and mimicked in countless other projects. By definition, many of those arrays are fairly similar in how they operate to the NumPy standard. The introduction of `__array_function__` allowed dispatching of functions implemented by several of these projects directly via NumPy's API. This introduces a new requirement, returning the NumPy-like array itself, rather than forcing a coercion into a pure NumPy array.

For the purpose above, NEP 22 introduced the concept of duck typing to NumPy arrays. The suggested solution described in the NEP allows libraries to avoid coercion of a NumPy-like array to a pure NumPy array where necessary, while still allowing that NumPy-like array libraries that do not wish to implement the protocol to coerce arrays to a pure NumPy array via `np.asarray`.

### Usage Guidance

Code that uses `np.duckarray` is meant for supporting other ndarray-like objects that "follow the NumPy API". That is an ill-defined concept at the moment --every known library implements the NumPy API only partly, and many deviate intentionally in at least some minor ways. This cannot be easily remedied, so for users of `np.duckarray` we recommend the following strategy: check if the NumPy functionality used by the code that follows your use of `np.duckarray` is present in Dask, CuPy and Sparse. If so, it's reasonable to expect any duck array to work here. If not, we suggest you indicate in your docstring what kinds of duck arrays are accepted, or what properties they need to have.

To exemplify the usage of duck arrays, suppose one wants to take the `mean()` of an array-like object `arr`. Using NumPy to achieve that, one could write `np.asarray(arr).mean()` to achieve the intended result. If `arr` is not a NumPy array, this would create an actual NumPy array in order to call `.mean()`. However, if the array is an object that is compliant with the NumPy API (either in full or partially) such as a CuPy, Sparse or a Dask array, then that copy would have been unnecessary. On the other hand, if one were to use the new `__duckarray__` protocol: `np.duckarray(arr).mean()`, and `arr` is an object compliant with the NumPy API, it would simply be returned rather than coerced into a pure NumPy array, avoiding unnecessary copies and potential loss of performance.

## Implementation

The implementation idea is fairly straightforward, requiring a new function `duckarray` to be introduced in NumPy, and a new method `__duckarray__` in NumPy-like array classes. The new `__duckarray__` method shall return the downstream array-like object itself, such as the `self` object, while the `__array__` method raises `TypeError`. Alternatively, the `__array__` method could create an actual NumPy array and return that.

The new NumPy `duckarray` function can be implemented as follows:

``` python
def duckarray(array_like):
    if hasattr(array_like, '__duckarray__'):
        return array_like.__duckarray__()
    return np.asarray(array_like)
```

### Example for a project implementing NumPy-like arrays

Now consider a library that implements a NumPy-compatible array class called `NumPyLikeArray`, this class shall implement the methods described above, and a complete implementation would look like the following:

``` python
class NumPyLikeArray:
    def __duckarray__(self):
        return self

    def __array__(self):
        raise TypeError("NumPyLikeArray can not be converted to a NumPy "
                         "array. You may want to use np.duckarray() instead.")
```

The implementation above exemplifies the simplest case, but the overall idea is that libraries will implement a `__duckarray__` method that returns the original object, and an `__array__` method that either creates and returns an appropriate NumPy array, or raises a`TypeError` to prevent unintentional use as an object in a NumPy array (if `np.asarray` is called on an arbitrary object that does not implement `__array__`, it will create a NumPy array scalar).

In case of existing libraries that don't already implement `__array__` but would like to use duck array typing, it is advised that they introduce both `__array__` and`__duckarray__` methods.

## Usage

An example of how the `__duckarray__` protocol could be used to write a `stack` function based on `concatenate`, and its produced outcome, can be seen below. The example here was chosen not only to demonstrate the usage of the `duckarray` function, but also to demonstrate its dependency on the NumPy API, demonstrated by checks on the array's `shape` attribute. Note that the example is merely a simplified version of NumPy's actual implementation of `stack` working on the first axis, and it is assumed that Dask has implemented the `__duckarray__` method.

``` python
def duckarray_stack(arrays):
    arrays = [np.duckarray(arr) for arr in arrays]

    shapes = {arr.shape for arr in arrays}
    if len(shapes) != 1:
        raise ValueError('all input arrays must have the same shape')

    expanded_arrays = [arr[np.newaxis, ...] for arr in arrays]
    return np.concatenate(expanded_arrays, axis=0)

dask_arr = dask.array.arange(10)
np_arr = np.arange(10)
np_like = list(range(10))

duckarray_stack((dask_arr, dask_arr))   # Returns dask.array
duckarray_stack((dask_arr, np_arr))     # Returns dask.array
duckarray_stack((dask_arr, np_like))    # Returns dask.array
```

In contrast, using only `np.asarray` (at the time of writing of this NEP, this is the usual method employed by library developers to ensure arrays are NumPy-like) has a different outcome:

``` python
def asarray_stack(arrays):
    arrays = [np.asanyarray(arr) for arr in arrays]

    # The remaining implementation is the same as that of
    # ``duckarray_stack`` above

asarray_stack((dask_arr, dask_arr))     # Returns np.ndarray
asarray_stack((dask_arr, np_arr))       # Returns np.ndarray
asarray_stack((dask_arr, np_like))      # Returns np.ndarray
```

## Backward compatibility

This proposal does not raise any backward compatibility issues within NumPy, given that it only introduces a new function. However, downstream libraries that opt to introduce the `__duckarray__` protocol may choose to remove the ability of coercing arrays back to a NumPy array via `np.array` or `np.asarray` functions, preventing unintended effects of coercion of such arrays back to a pure NumPy array (as some libraries already do, such as CuPy and Sparse), but still leaving libraries not implementing the protocol with the choice of utilizing `np.duckarray` to promote `array_like` objects to pure NumPy arrays.

## Previous proposals and discussion

The duck typing protocol proposed here was described in a high level in \[NEP 22 \<NEP22\>\](\#nep-22-\<nep22\>).

Additionally, longer discussions about the protocol and related proposals took place in [numpy/numpy \#13831](https://github.com/numpy/numpy/issues/13831)

## Copyright

This document has been placed in the public domain.

---

nep-0031-uarray.md

---

# NEP 31 â€” Context-local and global overrides of the NumPy API

  - Author  
    Hameer Abbasi \<<habbasi@quansight.com>\>

  - Author  
    Ralf Gommers \<<rgommers@quansight.com>\>

  - Author  
    Peter Bell \<<pbell@quansight.com>\>

  - Status  
    Superseded

  - Replaced-By  
    \[NEP56\](\#nep56)

  - Type  
    Standards Track

  - Created  
    2019-08-22

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/message/Z6AA5CL47NHBNEPTFWYOTSUVSRDGHYPN/>

## Abstract

This NEP proposes to make all of NumPy's public API overridable via an extensible backend mechanism.

Acceptance of this NEP means NumPy would provide global and context-local overrides in a separate namespace, as well as a dispatch mechanism similar to NEP-18\[1\]. First experiences with `__array_function__` show that it is necessary to be able to override NumPy functions that *do not take an array-like argument*, and hence aren't overridable via `__array_function__`. The most pressing need is array creation and coercion functions, such as `numpy.zeros` or `numpy.asarray`; see e.g. NEP-30\[2\].

This NEP proposes to allow, in an opt-in fashion, overriding any part of the NumPy API. It is intended as a comprehensive resolution to NEP-22\[3\], and obviates the need to add an ever-growing list of new protocols for each new type of function or object that needs to become overridable.

## Motivation and scope

The primary end-goal of this NEP is to make the following possible:

``` python
# On the library side
import numpy.overridable as unp

def library_function(array):
    array = unp.asarray(array)
    # Code using unumpy as usual
    return array

# On the user side:
import numpy.overridable as unp
import uarray as ua
import dask.array as da

ua.register_backend(da) # Can be done within Dask itself

library_function(dask_array)  # works and returns dask_array

with unp.set_backend(da):
    library_function([1, 2, 3, 4])  # actually returns a Dask array.
```

Here, `backend` can be any compatible object defined either by NumPy or an external library, such as Dask or CuPy. Ideally, it should be the module `dask.array` or `cupy` itself.

These kinds of overrides are useful for both the end-user as well as library authors. End-users may have written or wish to write code that they then later speed up or move to a different implementation, say PyData/Sparse. They can do this simply by setting a backend. Library authors may also wish to write code that is portable across array implementations, for example `sklearn` may wish to write code for a machine learning algorithm that is portable across array implementations while also using array creation functions.

This NEP takes a holistic approach: It assumes that there are parts of the API that need to be overridable, and that these will grow over time. It provides a general framework and a mechanism to avoid a design of a new protocol each time this is required. This was the goal of `uarray`: to allow for overrides in an API without needing the design of a new protocol.

This NEP proposes the following: That `unumpy`\[4\] becomes the recommended override mechanism for the parts of the NumPy API not yet covered by `__array_function__` or `__array_ufunc__`, and that `uarray` is vendored into a new namespace within NumPy to give users and downstream dependencies access to these overrides. This vendoring mechanism is similar to what SciPy decided to do for making `scipy.fft` overridable (see\[5\]).

The motivation behind `uarray` is manyfold: First, there have been several attempts to allow dispatch of parts of the NumPy API, including (most prominently), the `__array_ufunc__` protocol in NEP-13\[6\], and the `__array_function__` protocol in NEP-18\[7\], but this has shown the need for further protocols to be developed, including a protocol for coercion (see \[8\],\[9\]). The reasons these overrides are needed have been extensively discussed in the references, and this NEP will not attempt to go into the details of why these are needed; but in short: It is necessary for library authors to be able to coerce arbitrary objects into arrays of their own types, such as CuPy needing to coerce to a CuPy array, for example, instead of a NumPy array. In simpler words, one needs things like `np.asarray(...)` or an alternative to "just work" and return duck-arrays.

## Usage and impact

This NEP allows for global and context-local overrides, as well as automatic overrides a-la `__array_function__`.

Here are some use-cases this NEP would enable, besides the first one stated in the motivation section:

The first is allowing alternate dtypes to return their respective arrays.

``` python
# Returns an XND array
x = unp.ones((5, 5), dtype=xnd_dtype) # Or torch dtype
```

The second is allowing overrides for parts of the API. This is to allow alternate and/or optimized implementations for `np.linalg`, BLAS, and `np.random`.

``` python
import numpy as np
import pyfftw # Or mkl_fft

# Makes pyfftw the default for FFT
np.set_global_backend(pyfftw)

# Uses pyfftw without monkeypatching
np.fft.fft(numpy_array)

with np.set_backend(pyfftw) # Or mkl_fft, or numpy
    # Uses the backend you specified
    np.fft.fft(numpy_array)
```

This will allow an official way for overrides to work with NumPy without monkeypatching or distributing a modified version of NumPy.

Here are a few other use-cases, implied but not already stated:

``` python
data = da.from_zarr('myfile.zarr')
# result should still be dask, all things being equal
result = library_function(data)
result.to_zarr('output.zarr')
```

This second one would work if `magic_library` was built on top of `unumpy`.

``` python
from dask import array as da
from magic_library import pytorch_predict

data = da.from_zarr('myfile.zarr')
# normally here one would use e.g. data.map_overlap
result = pytorch_predict(data)
result.to_zarr('output.zarr')
```

There are some backends which may depend on other backends, for example xarray depending on <span class="title-ref">numpy.fft</span>, and transforming a time axis into a frequency axis, or Dask/xarray holding an array other than a NumPy array inside it. This would be handled in the following manner inside code:

    with ua.set_backend(cupy), ua.set_backend(dask.array):
        # Code that has distributed GPU arrays here

## Backward compatibility

There are no backward incompatible changes proposed in this NEP.

## Detailed description

### Proposals

The only change this NEP proposes at its acceptance, is to make `unumpy` the officially recommended way to override NumPy, along with making some submodules overridable by default via `uarray`. `unumpy` will remain a separate repository/package (which we propose to vendor to avoid a hard dependency, and use the separate `unumpy` package only if it is installed, rather than depend on for the time being). In concrete terms, `numpy.overridable` becomes an alias for `unumpy`, if available with a fallback to the a vendored version if not. `uarray` and `unumpy` and will be developed primarily with the input of duck-array authors and secondarily, custom dtype authors, via the usual GitHub workflow. There are a few reasons for this:

  - Faster iteration in the case of bugs or issues.
  - Faster design changes, in the case of needed functionality.
  - `unumpy` will work with older versions of NumPy as well.
  - The user and library author opt-in to the override process, rather than breakages happening when it is least expected. In simple terms, bugs in `unumpy` mean that `numpy` remains unaffected.
  - For `numpy.fft`, `numpy.linalg` and `numpy.random`, the functions in the main namespace will mirror those in the `numpy.overridable` namespace. The reason for this is that there may exist functions in the in these submodules that need backends, even for `numpy.ndarray` inputs.

#### Advantages of `unumpy` over other solutions

`unumpy` offers a number of advantages over the approach of defining a new protocol for every problem encountered: Whenever there is something requiring an override, `unumpy` will be able to offer a unified API with very minor changes. For example:

  - `ufunc` objects can be overridden via their `__call__`, `reduce` and other methods.
  - Other functions can be overridden in a similar fashion.
  - `np.asduckarray` goes away, and becomes `np.overridable.asarray` with a backend set.
  - The same holds for array creation functions such as `np.zeros`, `np.empty` and so on.

This also holds for the future: Making something overridable would require only minor changes to `unumpy`.

Another promise `unumpy` holds is one of default implementations. Default implementations can be provided for any multimethod, in terms of others. This allows one to override a large part of the NumPy API by defining only a small part of it. This is to ease the creation of new duck-arrays, by providing default implementations of many functions that can be easily expressed in terms of others, as well as a repository of utility functions that help in the implementation of duck-arrays that most duck-arrays would require. This would allow us to avoid designing entire protocols, e.g., a protocol for stacking and concatenating would be replaced by simply implementing `stack` and/or `concatenate` and then providing default implementations for everything else in that class. The same applies for transposing, and many other functions for which protocols haven't been proposed, such as `isin` in terms of `in1d`, `setdiff1d` in terms of `unique`, and so on.

It also allows one to override functions in a manner which `__array_function__` simply cannot, such as overriding `np.einsum` with the version from the `opt_einsum` package, or Intel MKL overriding FFT, BLAS or `ufunc` objects. They would define a backend with the appropriate multimethods, and the user would select them via a `with` statement, or registering them as a backend.

The last benefit is a clear way to coerce to a given backend (via the `coerce` keyword in `ua.set_backend`), and a protocol for coercing not only arrays, but also `dtype` objects and `ufunc` objects with similar ones from other libraries. This is due to the existence of actual, third party dtype packages, and their desire to blend into the NumPy ecosystem (see\[10\]). This is a separate issue compared to the C-level dtype redesign proposed in\[11\], it's about allowing third-party dtype implementations to work with NumPy, much like third-party array implementations. These can provide features such as, for example, units, jagged arrays or other such features that are outside the scope of NumPy.

#### Mixing NumPy and `unumpy` in the same file

Normally, one would only want to import only one of `unumpy` or `numpy`, you would import it as `np` for familiarity. However, there may be situations where one wishes to mix NumPy and the overrides, and there are a few ways to do this, depending on the user's style:

    from numpy import overridable as unp
    import numpy as np

or:

    import numpy as np
    
    # Use unumpy via np.overridable

### Duck-array coercion

There are inherent problems about returning objects that are not NumPy arrays from `numpy.array` or `numpy.asarray`, particularly in the context of C/C++ or Cython code that may get an object with a different memory layout than the one it expects. However, we believe this problem may apply not only to these two functions but all functions that return NumPy arrays. For this reason, overrides are opt-in for the user, by using the submodule `numpy.overridable` rather than `numpy`. NumPy will continue to work unaffected by anything in `numpy.overridable`.

If the user wishes to obtain a NumPy array, there are two ways of doing it:

1.  Use `numpy.asarray` (the non-overridable version).
2.  Use `numpy.overridable.asarray` with the NumPy backend set and coercion enabled

### Aliases outside of the `numpy.overridable` namespace

All functionality in `numpy.random`, `numpy.linalg` and `numpy.fft` will be aliased to their respective overridable versions inside `numpy.overridable`. The reason for this is that there are alternative implementations of RNGs (`mkl-random`), linear algebra routines (`eigen`, `blis`) and FFT routines (`mkl-fft`, `pyFFTW`) that need to operate on `numpy.ndarray` inputs, but still need the ability to switch behaviour.

This is different from monkeypatching in a few different ways:

  - The caller-facing signature of the function is always the same, so there is at least the loose sense of an API contract. Monkeypatching does not provide this ability.
  - There is the ability of locally switching the backend.
  - It has been [suggested](https://mail.python.org/archives/list/numpy-discussion@python.org/message/PS7EN3CRT6XERNTCN56MAYOXFFFEC55G/) that the reason that 1.17 hasn't landed in the Anaconda defaults channel is due to the incompatibility between monkeypatching and `__array_function__`, as monkeypatching would bypass the protocol completely.
  - Statements of the form `from numpy import x; x` and `np.x` would have different results depending on whether the import was made before or after monkeypatching happened.

All this isn't possible at all with `__array_function__` or `__array_ufunc__`.

It has been formally realized (at least in part) that a backend system is needed for this, in the [NumPy roadmap](https://numpy.org/neps/roadmap.html#other-functionality).

For `numpy.random`, it's still necessary to make the C-API fit the one proposed in \[NEP-19 \<NEP19\>\](\#nep-19-\<nep19\>). This is impossible for <span class="title-ref">mkl-random</span>, because then it would need to be rewritten to fit that framework. The guarantees on stream compatibility will be the same as before, but if there's a backend that affects `numpy.random` set, we make no guarantees about stream compatibility, and it is up to the backend author to provide their own guarantees.

### Providing a way for implicit dispatch

It has been suggested that the ability to dispatch methods which do not take a dispatchable is needed, while guessing that backend from another dispatchable.

As a concrete example, consider the following:

``` python
with unumpy.determine_backend(array_like, np.ndarray):
    unumpy.arange(len(array_like))
```

While this does not exist yet in `uarray`, it is trivial to add it. The need for this kind of code exists because one might want to have an alternative for the proposed `*_like` functions, or the `like=` keyword argument. The need for these exists because there are functions in the NumPy API that do not take a dispatchable argument, but there is still the need to select a backend based on a different dispatchable.

### The need for an opt-in module

The need for an opt-in module is realized because of a few reasons:

  - There are parts of the API (like <span class="title-ref">numpy.asarray</span>) that simply cannot be overridden due to incompatibility concerns with C/Cython extensions, however, one may want to coerce to a duck-array using `asarray` with a backend set.
  - There are possible issues around an implicit option and monkeypatching, such as those mentioned above.

NEP 18 notes that this may require maintenance of two separate APIs. However, this burden may be lessened by, for example, parameterizing all tests over `numpy.overridable` separately via a fixture. This also has the side-effect of thoroughly testing it, unlike `__array_function__`. We also feel that it provides an opportunity to separate the NumPy API contract properly from the implementation.

### Benefits to end-users and mixing backends

Mixing backends is easy in `uarray`, one only has to do:

``` python
# Explicitly say which backends you want to mix
ua.register_backend(backend1)
ua.register_backend(backend2)
ua.register_backend(backend3)

# Freely use code that mixes backends here.
```

The benefits to end-users extend beyond just writing new code. Old code (usually in the form of scripts) can be easily ported to different backends by a simple import switch and a line adding the preferred backend. This way, users may find it easier to port existing code to GPU or distributed computing.

## Related work

### Other override mechanisms

  - NEP-18, the `__array_function__` protocol.\[12\]
  - NEP-13, the `__array_ufunc__` protocol.\[13\]
  - NEP-30, the `__duck_array__` protocol.\[14\]

### Existing NumPy-like array implementations

  - Dask: <https://dask.org/>
  - CuPy: <https://cupy.chainer.org/>
  - PyData/Sparse: <https://sparse.pydata.org/>
  - Xnd: <https://xnd.readthedocs.io/>
  - Astropy's Quantity: <https://docs.astropy.org/en/stable/units/>

### Existing and potential consumers of alternative arrays

  - Dask: <https://dask.org/>
  - scikit-learn: <https://scikit-learn.org/>
  - xarray: <https://xarray.pydata.org/>
  - TensorLy: <http://tensorly.org/>

### Existing alternate dtype implementations

  - `ndtypes`: <https://ndtypes.readthedocs.io/en/latest/>
  - Datashape: <https://datashape.readthedocs.io>
  - Plum: <https://plum-py.readthedocs.io/>

### Alternate implementations of parts of the NumPy API

  - `mkl_random`: <https://github.com/IntelPython/mkl_random>
  - `mkl_fft`: <https://github.com/IntelPython/mkl_fft>
  - `bottleneck`: <https://github.com/pydata/bottleneck>
  - `opt_einsum`: <https://github.com/dgasmith/opt_einsum>

## Implementation

The implementation of this NEP will require the following steps:

  - Implementation of `uarray` multimethods corresponding to the NumPy API, including classes for overriding `dtype`, `ufunc` and `array` objects, in the `unumpy` repository, which are usually very easy to create.
  - Moving backends from `unumpy` into the respective array libraries.

Maintenance can be eased by testing over `{numpy, unumpy}` via parameterized tests. If a new argument is added to a method, the corresponding argument extractor and replacer will need to be updated within `unumpy`.

A lot of argument extractors can be re-used from the existing implementation of the `__array_function__` protocol, and the replacers can be usually re-used across many methods.

For the parts of the namespace which are going to be overridable by default, the main method will need to be renamed and hidden behind a `uarray` multimethod.

Default implementations are usually seen in the documentation using the words "equivalent to", and thus, are easily available.

### `uarray` Primer

**Note:** *This section will not attempt to go into too much detail about uarray, that is the purpose of the uarray documentation.*\[15\] *However, the NumPy community will have input into the design of uarray, via the issue tracker.*

`unumpy` is the interface that defines a set of overridable functions (multimethods) compatible with the numpy API. To do this, it uses the `uarray` library. `uarray` is a general purpose tool for creating multimethods that dispatch to one of multiple different possible backend implementations. In this sense, it is similar to the `__array_function__` protocol but with the key difference that the backend is explicitly installed by the end-user and not coupled into the array type.

Decoupling the backend from the array type gives much more flexibility to end-users and backend authors. For example, it is possible to:

  - override functions not taking arrays as arguments
  - create backends out of source from the array type
  - install multiple backends for the same array type

This decoupling also means that `uarray` is not constrained to dispatching over array-like types. The backend is free to inspect the entire set of function arguments to determine if it can implement the function e.g. `dtype` parameter dispatching.

#### Defining backends

`uarray` consists of two main protocols: `__ua_convert__` and `__ua_function__`, called in that order, along with `__ua_domain__`. `__ua_convert__` is for conversion and coercion. It has the signature `(dispatchables, coerce)`, where `dispatchables` is an iterable of `ua.Dispatchable` objects and `coerce` is a boolean indicating whether or not to force the conversion. `ua.Dispatchable` is a simple class consisting of three simple values: `type`, `value`, and `coercible`. `__ua_convert__` returns an iterable of the converted values, or `NotImplemented` in the case of failure.

`__ua_function__` has the signature `(func, args, kwargs)` and defines the actual implementation of the function. It receives the function and its arguments. Returning `NotImplemented` will cause a move to the default implementation of the function if one exists, and failing that, the next backend.

Here is what will happen assuming a `uarray` multimethod is called:

1.  We canonicalise the arguments so any arguments without a default are placed in `*args` and those with one are placed in `**kwargs`.
2.  We check the list of backends.
    1.  If it is empty, we try the default implementation.
3.  We check if the backend's `__ua_convert__` method exists. If it exists:
    1.  We pass it the output of the dispatcher, which is an iterable of `ua.Dispatchable` objects.
    2.  We feed this output, along with the arguments, to the argument replacer. `NotImplemented` means we move to 3 with the next backend.
    3.  We store the replaced arguments as the new arguments.
4.  We feed the arguments into `__ua_function__`, and return the output, and exit if it isn't `NotImplemented`.
5.  If the default implementation exists, we try it with the current backend.
6.  On failure, we move to 3 with the next backend. If there are no more backends, we move to 7.
7.  We raise a `ua.BackendNotImplementedError`.

#### Defining overridable multimethods

To define an overridable function (a multimethod), one needs a few things:

1.  A dispatcher that returns an iterable of `ua.Dispatchable` objects.
2.  A reverse dispatcher that replaces dispatchable values with the supplied ones.
3.  A domain.
4.  Optionally, a default implementation, which can be provided in terms of other multimethods.

As an example, consider the following:

    import uarray as ua
    
    def full_argreplacer(args, kwargs, dispatchables):
        def full(shape, fill_value, dtype=None, order='C'):
            return (shape, fill_value), dict(
                dtype=dispatchables[0],
                order=order
            )
    
        return full(*args, **kwargs)
    
    @ua.create_multimethod(full_argreplacer, domain="numpy")
    def full(shape, fill_value, dtype=None, order='C'):
        return (ua.Dispatchable(dtype, np.dtype),)

A large set of examples can be found in the `unumpy` repository,\[16\]. This simple act of overriding callables allows us to override:

  - Methods
  - Properties, via `fget` and `fset`
  - Entire objects, via `__get__`.

#### Examples for NumPy

A library that implements a NumPy-like API will use it in the following manner (as an example):

    import numpy.overridable as unp
    _ua_implementations = {}
    
    __ua_domain__ = "numpy"
    
    def __ua_function__(func, args, kwargs):
        fn = _ua_implementations.get(func, None)
        return fn(*args, **kwargs) if fn is not None else NotImplemented
    
    def implements(ua_func):
        def inner(func):
            _ua_implementations[ua_func] = func
            return func
    
        return inner
    
    @implements(unp.asarray)
    def asarray(a, dtype=None, order=None):
        # Code here
        # Either this method or __ua_convert__ must
        # return NotImplemented for unsupported types,
        # Or they shouldn't be marked as dispatchable.
    
    # Provides a default implementation for ones and zeros.
    @implements(unp.full)
    def full(shape, fill_value, dtype=None, order='C'):
        # Code here

## Alternatives

The current alternative to this problem is a combination of NEP-18\[17\], NEP-13\[18\] and NEP-30\[19\] plus adding more protocols (not yet specified) in addition to it. Even then, some parts of the NumPy API will remain non-overridable, so it's a partial alternative.

The main alternative to vendoring `unumpy` is to simply move it into NumPy completely and not distribute it as a separate package. This would also achieve the proposed goals, however we prefer to keep it a separate package for now, for reasons already stated above.

The third alternative is to move `unumpy` into the NumPy organisation and develop it as a NumPy project. This will also achieve the said goals, and is also a possibility that can be considered by this NEP. However, the act of doing an extra `pip install` or `conda install` may discourage some users from adopting this method.

An alternative to requiring opt-in is mainly to *not* override `np.asarray` and `np.array`, and making the rest of the NumPy API surface overridable, instead providing `np.duckarray` and `np.asduckarray` as duck-array friendly alternatives that used the respective overrides. However, this has the downside of adding a minor overhead to NumPy calls.

## Discussion

  - `uarray` blogpost: <https://labs.quansight.org/blog/2019/07/uarray-update-api-changes-overhead-and-comparison-to-__array_function__/>
  - The discussion section of \[NEP18\](\#nep18)
  - \[NEP22\](\#nep22)
  - Dask issue \#4462: <https://github.com/dask/dask/issues/4462>
  - PR \#13046: <https://github.com/numpy/numpy/pull/13046>
  - Dask issue \#4883: <https://github.com/dask/dask/issues/4883>
  - Issue \#13831: <https://github.com/numpy/numpy/issues/13831>
  - Discussion PR 1: <https://github.com/hameerabbasi/numpy/pull/3>
  - Discussion PR 2: <https://github.com/hameerabbasi/numpy/pull/4>
  - Discussion PR 3: <https://github.com/numpy/numpy/pull/14389>

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  \[NEP18\](\#nep18)

2.  \[NEP30\](\#nep30)

3.  \[NEP22\](\#nep22)

4.  unumpy: NumPy, but implementation-independent: <https://unumpy.readthedocs.io>

5.  <http://scipy.github.io/devdocs/fft.html#backend-control>

6.  \[NEP13\](\#nep13)

7.  \[NEP18\](\#nep18)

8.  Reply to Adding to the non-dispatched implementation of NumPy methods: <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/5GUDMALWDIRHITG5YUOCV343J66QSX3U/#5GUDMALWDIRHITG5YUOCV343J66QSX3U>

9.  \[NEP30\](\#nep30)

10. Custom Dtype/Units discussion: <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/RZYCVT6C3F7UDV6NA6FEV4MC5FKS6RDA/#RZYCVT6C3F7UDV6NA6FEV4MC5FKS6RDA>

11. The epic dtype cleanup plan: <https://github.com/numpy/numpy/issues/2899>

12. \[NEP18\](\#nep18)

13. \[NEP22\](\#nep22)

14. \[NEP30\](\#nep30)

15. uarray, A general dispatch mechanism for Python: <https://uarray.readthedocs.io>

16. unumpy: NumPy, but implementation-independent: <https://unumpy.readthedocs.io>

17. \[NEP18\](\#nep18)

18. \[NEP13\](\#nep13)

19. \[NEP30\](\#nep30)

---

nep-0032-remove-financial-functions.md

---

# NEP 32 â€” Remove the financial functions from NumPy

  - Author  
    Warren Weckesser \<<warren.weckesser@gmail.com>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2019-08-30

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2019-September/080074.html>

## Abstract

We propose deprecating and ultimately removing the financial functions\[1\] from NumPy. The functions will be moved to an independent repository, and provided to the community as a separate package with the name `numpy_financial`.

## Motivation and scope

The NumPy financial functions\[2\] are the 10 functions `fv`, `ipmt`, `irr`, `mirr`, `nper`, `npv`, `pmt`, `ppmt`, `pv` and `rate`. The functions provide elementary financial calculations such as future value, net present value, etc. These functions were added to NumPy in 2008\[3\].

In May, 2009, a request by Joe Harrington to add a function called `xirr` to the financial functions triggered a long thread about these functions\[4\]. One important point that came up in that thread is that a "real" financial library must be able to handle real dates. The NumPy financial functions do not work with actual dates or calendars. The preference for a more capable library independent of NumPy was expressed several times in that thread.

In June, 2009, D. L. Goldsmith expressed concerns about the correctness of the implementations of some of the financial functions\[5\]. It was suggested then to move the financial functions out of NumPy to an independent package.

In a GitHub issue in 2013\[6\], Nathaniel Smith suggested moving the financial functions from the top-level namespace to `numpy.financial`. He also suggested giving the functions better names. Responses at that time included the suggestion to deprecate them and move them from NumPy to a separate package. This issue is still open.

Later in 2013\[7\], it was suggested on the mailing list that these functions be removed from NumPy.

The arguments for the removal of these functions from NumPy:

  - They are too specialized for NumPy.
  - They are not actually useful for "real world" financial calculations, because they do not handle real dates and calendars.
  - The definition of "correctness" for some of these functions seems to be a matter of convention, and the current NumPy developers do not have the background to judge their correctness.
  - There has been little interest among past and present NumPy developers in maintaining these functions.

The main arguments for keeping the functions in NumPy are:

  - Removing these functions will be disruptive for some users. Current users will have to add the new `numpy_financial` package to their dependencies, and then modify their code to use the new package.
  - The functions provided, while not "industrial strength", are apparently similar to functions provided by spreadsheets and some calculators. Having them available in NumPy makes it easier for some developers to migrate their software to Python and NumPy.

It is clear from comments in the mailing list discussions and in the GitHub issues that many current NumPy developers believe the benefits of removing the functions outweigh the costs. For example, from\[8\]:

    The financial functions should probably be part of a separate package
    -- Charles Harris
    
    If there's a better package we can point people to we could just deprecate
    them and then remove them entirely... I'd be fine with that too...
    -- Nathaniel Smith
    
    +1 to deprecate them. If no other package exists, it can be created if
    someone feels the need for that.
    -- Ralf Gommers
    
    I feel pretty strongly that we should deprecate these. If nobody on numpyâ€™s
    core team is interested in maintaining them, then it is purely a drag on
    development for NumPy.
    -- Stephan Hoyer

And from the 2013 mailing list discussion, about removing the functions from NumPy:

    I am +1 as well, I don't think they should have been included in the first
    place.
    -- David Cournapeau

But not everyone was in favor of removal:

    The fin routines are tiny and don't require much maintenance once
    written.  If we made an effort (putting up pages with examples of common
    financial calculations and collecting those under a topical web page,
    then linking to that page from various places and talking it up), I
    would think they could attract users looking for a free way to play with
    financial scenarios.  [...]
    So, I would say we keep them.  If ours are not the best, we should bring
    them up to snuff.
    -- Joe Harrington

For an idea of the maintenance burden of the financial functions, one can look for all the GitHub issues\[9\] and pull requests\[10\] that have the tag `component: numpy.lib.financial`.

One method for measuring the effect of removing these functions is to find all the packages on GitHub that use them. Such a search can be performed with the `python-api-inspect` service\[11\]. A search for all uses of the NumPy financial functions finds just eight repositories. (See the comments in\[12\] for the actual SQL query.)

## Implementation

  - Create a new Python package, `numpy_financial`, to be maintained in the top-level NumPy github organization. This repository will contain the definitions and unit tests for the financial functions. The package will be added to PyPI so it can be installed with `pip`.
  - Deprecate the financial functions in the `numpy` namespace, beginning in NumPy version 1.18. Remove the financial functions from NumPy version 1.20.

## Backward compatibility

The removal of these functions breaks backward compatibility, as explained earlier. The effects are mitigated by providing the `numpy_financial` library.

## Alternatives

The following alternatives were mentioned in\[13\]:

  - *Maintain the functions as they are (i.e. do nothing).* A review of the history makes clear that this is not the preference of many NumPy developers. A recurring comment is that the functions simply do not belong in NumPy. When that sentiment is combined with the history of bug reports and the ongoing questions about the correctness of the functions, the conclusion is that the cleanest solution is deprecation and removal.
  - *Move the functions from the \`\`numpy\`\` namespace to \`\`numpy.financial\`\`.* This was the initial suggestion in\[14\]. Such a change does not address the maintenance issues, and doesn't change the misfit that many developers see between these functions and NumPy. It causes disruption for the current users of these functions without addressing what many developers see as the fundamental problem.

## Discussion

Links to past mailing list discussions, and to relevant GitHub issues and pull requests, have already been given. The announcement of this NEP was made on the NumPy-Discussion mailing list on 3 September 2019\[15\], and on the PyData mailing list on 8 September 2019\[16\]. The formal proposal to accept the NEP was made on 19 September 2019\[17\]; a notification was also sent to PyData (same thread as\[18\]). There have been no substantive objections.

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  Financial functions, <https://numpy.org/doc/1.17/reference/routines.financial.html>

2.  Financial functions, <https://numpy.org/doc/1.17/reference/routines.financial.html>

3.  NumPy-Discussion mailing list, "Simple financial functions for NumPy", <https://mail.python.org/pipermail/numpy-discussion/2008-April/032353.html>

4.  NumPy-Discussion mailing list, "add xirr to numpy financial functions?", <https://mail.python.org/pipermail/numpy-discussion/2009-May/042645.html>

5.  NumPy-Discussion mailing list, "Definitions of pv, fv, nper, pmt, and rate", <https://mail.python.org/pipermail/numpy-discussion/2009-June/043188.html>

6.  Get financial functions out of main namespace, <https://github.com/numpy/numpy/issues/2880>

7.  NumPy-Discussion mailing list, "Deprecation of financial routines", <https://mail.python.org/pipermail/numpy-discussion/2013-August/067409.html>

8.  Get financial functions out of main namespace, <https://github.com/numpy/numpy/issues/2880>

9.  `component: numpy.lib.financial` issues, <https://github.com/numpy/numpy/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22component%3A+numpy.lib.financial%22+>

10. `component: numpy.lib.financial` pull requests, <https://github.com/numpy/numpy/pulls?utf8=%E2%9C%93&q=is%3Apr+label%3A%22component%3A+numpy.lib.financial%22+>

11. Quansight-Labs/python-api-inspect, <https://github.com/Quansight-Labs/python-api-inspect/>

12. Get financial functions out of main namespace, <https://github.com/numpy/numpy/issues/2880>

13. Get financial functions out of main namespace, <https://github.com/numpy/numpy/issues/2880>

14. Get financial functions out of main namespace, <https://github.com/numpy/numpy/issues/2880>

15. NumPy-Discussion mailing list, "NEP 32: Remove the financial functions from NumPy" <https://mail.python.org/pipermail/numpy-discussion/2019-September/079965.html>

16. PyData mailing list (<pydata@googlegroups.com>), "NumPy proposal to remove the financial functions. <https://mail.google.com/mail/u/0/h/1w0mjgixc4rpe/?&th=16d5c38be45f77c4&q=nep+32&v=c&s=q>

17. NumPy-Discussion mailing list, "Proposal to accept NEP 32: Remove the financial functions from NumPy" <https://mail.python.org/pipermail/numpy-discussion/2019-September/080074.html>

18. PyData mailing list (<pydata@googlegroups.com>), "NumPy proposal to remove the financial functions. <https://mail.google.com/mail/u/0/h/1w0mjgixc4rpe/?&th=16d5c38be45f77c4&q=nep+32&v=c&s=q>

---

nep-0034-infer-dtype-is-object.md

---

# NEP 34 â€” Disallow inferring `dtype=object` from sequences

  - Author  
    Matti Picus

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2019-10-10

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2019-October/080200.html>

## Abstract

When users create arrays with sequences-of-sequences, they sometimes err in matching the lengths of the nested [sequences](https://docs.python.org/3.7/glossary.html#term-sequence), commonly called "ragged arrays". Here we will refer to them as ragged nested sequences. Creating such arrays via `np.array([<ragged_nested_sequence>])` with no `dtype` keyword argument will today default to an `object`-dtype array. Change the behaviour to raise a `ValueError` instead.

## Motivation and scope

Users who specify lists-of-lists when creating a <span class="title-ref">numpy.ndarray</span> via `np.array` may mistakenly pass in lists of different lengths. Currently we accept this input and automatically create an array with `dtype=object`. This can be confusing, since it is rarely what is desired. Changing the automatic dtype detection to never return `object` for ragged nested sequences (defined as a recursive sequence of sequences, where not all the sequences on the same level have the same length) will force users who actually wish to create `object` arrays to specify that explicitly. Note that `lists`, `tuples`, and `nd.ndarrays` are all sequences\[1\]. See for instance [issue 5303](https://github.com/numpy/numpy/issues/5303).

## Usage and impact

After this change, array creation with ragged nested sequences must explicitly define a dtype:

> \>\>\> np.array(\[\[1, 2\], \[1\]\]) ValueError: cannot guess the desired dtype from the input
> 
> \>\>\> np.array(\[\[1, 2\], \[1\]\], dtype=object) \# succeeds, with no change from current behaviour

The deprecation will affect any call that internally calls `np.asarray`. For instance, the `assert_equal` family of functions calls `np.asarray`, so users will have to change code like:

    np.assert_equal(a, [[1, 2], 3])

to:

    np.assert_equal(a, np.array([[1, 2], 3], dtype=object))

## Detailed description

To explicitly set the shape of the object array, since it is sometimes hard to determine what shape is desired, one could use:

> \>\>\> arr = np.empty(correct\_shape, dtype=object) \>\>\> arr\[...\] = values

We will also reject mixed sequences of non-sequence and sequence, for instance all of these will be rejected:

> \>\>\> arr = np.array(\[np.arange(10), \[10\]\]) \>\>\> arr = np.array(\[\[range(3), range(3), range(3)\], \[range(3), 0, 0\]\])

## Related work

[PR 14341](https://github.com/numpy/numpy/pull/14341) tried to raise an error when ragged nested sequences were specified with a numeric dtype `np.array, [[1], [2, 3]], dtype=int)` but failed due to false-positives, for instance `np.array([1, np.array([5])], dtype=int)`.

## Implementation

The code to be changed is inside `PyArray_GetArrayParamsFromObject` and the internal `discover_dimensions` function. The first implementation in [PR 14794](https://github.com/numpy/numpy/pull/14794) caused a number of downstream library failures and was reverted before the release of 1.18. Subsequently downstream libraries fixed the places they were using ragged arrays. The reimplementation became [PR 15119](https://github.com/numpy/numpy/pull/15119) which was merged for the 1.19 release.

## Backward compatibility

Anyone depending on creating object arrays from ragged nested sequences will need to modify their code. There will be a deprecation period during which the current behaviour will emit a `DeprecationWarning`.

## Alternatives

  - We could continue with the current situation.
  - It was also suggested to add a kwarg `depth` to array creation, or perhaps to add another array creation API function `ragged_array_object`. The goal was to eliminate the ambiguity in creating an object array from `array([[1, 2], [1]], dtype=object)`: should the returned array have a shape of `(1,)`, or `(2,)`? This NEP does not deal with that issue, and only deprecates the use of `array` with no `dtype=object` for ragged nested sequences. Users of ragged nested sequences may face another deprecation cycle in the future. Rationale: we expect that there are very few users who intend to use ragged arrays like that, this was never intended as a use case of NumPy arrays. Users are likely better off with [another library](https://github.com/scikit-hep/awkward-array) or just using list of lists.
  - It was also suggested to deprecate all automatic creation of `object`-dtype arrays, which would require adding an explicit `dtype=object` for something like `np.array([Decimal(10), Decimal(10)])`. This too is out of scope for the current NEP. Rationale: it's harder to asses the impact of this larger change, we're not sure how many users this may impact.

## Discussion

Comments to [issue 5303](https://github.com/numpy/numpy/issues/5303) indicate this is unintended behaviour as far back as 2014. Suggestions to change it have been made in the ensuing years, but none have stuck. The WIP implementation in [PR 14794](https://github.com/numpy/numpy/pull/14794) seems to point to the viability of this approach.

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  `np.ndarrays` are not recursed into, rather their shape is used directly. This will not emit warnings:
    
        ragged = np.array([[1], [1, 2, 3]], dtype=object)
        np.array([ragged, ragged]) # no dtype needed

---

nep-0035-array-creation-dispatch-with-array-function.md

---

# NEP 35 â€” Array creation dispatching with \_\_array\_function\_\_

  - Author  
    Peter Andreas Entschev \<<pentschev@nvidia.com>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2019-10-15

  - Updated  
    2020-11-06

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2021-May/081761.html>

## Abstract

We propose the introduction of a new keyword argument `like=` to all array creation functions to address one of the shortcomings of `__array_function__`, as described by NEP 18\[1\]. The `like=` keyword argument will create an instance of the argument's type, enabling direct creation of non-NumPy arrays. The target array type must implement the `__array_function__` protocol.

## Motivation and scope

Many libraries implement the NumPy API, such as Dask for graph computing, CuPy for GPGPU computing, xarray for N-D labeled arrays, etc. Underneath, they have adopted the `__array_function__` protocol which allows NumPy to understand and treat downstream objects as if they are the native `numpy.ndarray` object. Hence the community while using various libraries still benefits from a unified NumPy API. This not only brings great convenience for standardization but also removes the burden of learning a new API and rewriting code for every new object. In more technical terms, this mechanism of the protocol is called a "dispatcher", which is the terminology we use from here onwards when referring to that.

``` python
x = dask.array.arange(5)    # Creates dask.array
np.diff(x)                  # Returns dask.array
```

Note above how we called Dask's implementation of `diff` via the NumPy namespace by calling `np.diff`, and the same would apply if we had a CuPy array or any other array from a library that adopts `__array_function__`. This allows writing code that is agnostic to the implementation library, thus users can write their code once and still be able to use different array implementations according to their needs.

Obviously, having a protocol in-place is useful if the arrays are created elsewhere and let NumPy handle them. But still these arrays have to be started in their native library and brought back. Instead if it was possible to create these objects through NumPy API then there would be an almost complete experience, all using NumPy syntax. For example, say we have some CuPy array `cp_arr`, and want a similar CuPy array with identity matrix. We could still write the following:

``` python
x = cupy.identity(3)
```

Instead, the better way would be using to only use the NumPy API, this could now be achieved with:

``` python
x = np.identity(3, like=cp_arr)
```

As if by magic, `x` will also be a CuPy array, as NumPy was capable to infer that from the type of `cp_arr`. Note that this last step would not be possible without `like=`, as it would be impossible for the NumPy to know the user expects a CuPy array based only on the integer input.

The new `like=` keyword proposed is solely intended to identify the downstream library where to dispatch and the object is used only as reference, meaning that no modifications, copies or processing will be performed on that object.

We expect that this functionality will be mostly useful to library developers, allowing them to create new arrays for internal usage based on arrays passed by the user, preventing unnecessary creation of NumPy arrays that will ultimately lead to an additional conversion into a downstream array type.

Support for Python 2.7 has been dropped since NumPy 1.17, therefore we make use of the keyword-only argument standard described in PEP-3102\[2\] to implement `like=`, thus preventing it from being passed by position.

## Usage and impact

NumPy users who don't use other arrays from downstream libraries can continue to use array creation routines without a `like=` argument. Using `like=np.ndarray` will work as if no array was passed via that argument. However, this will incur additional checks that will negatively impact performance.

To understand the intended use for `like=`, and before we move to more complex cases, consider the following illustrative example consisting only of NumPy and CuPy arrays:

``` python
import numpy as np
import cupy

def my_pad(arr, padding):
    padding = np.array(padding, like=arr)
    return np.concatenate((padding, arr, padding))

my_pad(np.arange(5), [-1, -1])    # Returns np.ndarray
my_pad(cupy.arange(5), [-1, -1])  # Returns cupy.core.core.ndarray
```

Note in the `my_pad` function above how `arr` is used as a reference to dictate what array type padding should have, before concatenating the arrays to produce the result. On the other hand, if `like=` wasn't used, the NumPy case would still work, but CuPy wouldn't allow this kind of automatic conversion, ultimately raising a `TypeError: Only cupy arrays can be concatenated` exception.

Now we should look at how a library like Dask could benefit from `like=`. Before we understand that, it's important to understand a bit about Dask basics and how it ensures correctness with `__array_function__`. Note that Dask can perform computations on different sorts of objects, like dataframes, bags and arrays, here we will focus strictly on arrays, which are the objects we can use `__array_function__` with.

Dask uses a graph computing model, meaning it breaks down a large problem in many smaller problems and merges their results to reach the final result. To break the problem down into smaller ones, Dask also breaks arrays into smaller arrays that it calls "chunks". A Dask array can thus consist of one or more chunks and they may be of different types. However, in the context of `__array_function__`, Dask only allows chunks of the same type; for example, a Dask array can be formed of several NumPy arrays or several CuPy arrays, but not a mix of both.

To avoid mismatched types during computation, Dask keeps an attribute `_meta` as part of its array throughout computation: this attribute is used to both predict the output type at graph creation time, and to create any intermediary arrays that are necessary within some function's computation. Going back to our previous example, we can use `_meta` information to identify what kind of array we would use for padding, as seen below:

``` python
import numpy as np
import cupy
import dask.array as da
from dask.array.utils import meta_from_array

def my_dask_pad(arr, padding):
    padding = np.array(padding, like=meta_from_array(arr))
    return np.concatenate((padding, arr, padding))

# Returns dask.array<concatenate, shape=(9,), dtype=int64, chunksize=(5,), chunktype=numpy.ndarray>
my_dask_pad(da.arange(5), [-1, -1])

# Returns dask.array<concatenate, shape=(9,), dtype=int64, chunksize=(5,), chunktype=cupy.ndarray>
my_dask_pad(da.from_array(cupy.arange(5)), [-1, -1])
```

Note how `chunktype` in the return value above changes from `numpy.ndarray` in the first `my_dask_pad` call to `cupy.ndarray` in the second. We have also renamed the function to `my_dask_pad` in this example with the intent to make it clear that this is how Dask would implement such functionality, should it need to do so, as it requires Dask's internal tools that are not of much use elsewhere.

To enable proper identification of the array type we use Dask's utility function `meta_from_array`, which was introduced as part of the work to support `__array_function__`, allowing Dask to handle `_meta` appropriately. Readers can think of `meta_from_array` as a special function that just returns the type of the underlying Dask array, for example:

``` python
np_arr = da.arange(5)
cp_arr = da.from_array(cupy.arange(5))

meta_from_array(np_arr)  # Returns a numpy.ndarray
meta_from_array(cp_arr)  # Returns a cupy.ndarray
```

Since the value returned by `meta_from_array` is a NumPy-like array, we can just pass that directly into the `like=` argument.

The `meta_from_array` function is primarily targeted at the library's internal usage to ensure chunks are created with correct types. Without the `like=` argument, it would be impossible to ensure `my_pad` creates a padding array with a type matching that of the input array, which would cause a `TypeError` exception to be raised by CuPy, as discussed above would happen to the CuPy case alone. Combining Dask's internal handling of meta arrays and the proposed `like=` argument, it now becomes possible to handle cases involving creation of non-NumPy arrays, which is likely the heaviest limitation Dask currently faces from the `__array_function__` protocol.

## Backward compatibility

This proposal does not raise any backward compatibility issues within NumPy, given that it only introduces a new keyword argument to existing array creation functions with a default `None` value, thus not changing current behavior.

## Detailed description

The introduction of the `__array_function__` protocol allowed downstream library developers to use NumPy as a dispatching API. However, the protocol did not -- and did not intend to -- address the creation of arrays by downstream libraries, preventing those libraries from using such important functionality in that context.

The purpose of this NEP is to address that shortcoming in a simple and straightforward way: introduce a new `like=` keyword argument, similar to how the `empty_like` family of functions work. When array creation functions receive such an argument, they will trigger the `__array_function__` protocol, and call the downstream library's own array creation function implementation. The `like=` argument, as its own name suggests, shall be used solely for the purpose of identifying where to dispatch. In contrast to the way `__array_function__` has been used so far (the first argument identifies the target downstream library), and to avoid breaking NumPy's API with regards to array creation, the new `like=` keyword shall be used for the purpose of dispatching.

Downstream libraries will benefit from the `like=` argument without any changes to their API, given the argument only needs to be implemented by NumPy. It's still allowed that downstream libraries include the `like=` argument, as it can be useful in some cases, please refer to \[neps.like-kwarg.implementation\](\#neps.like-kwarg.implementation) for details on those cases. It will still be required that downstream libraries implement the `__array_function__` protocol, as described by NEP 18\[3\], and appropriately introduce the argument to their calls to NumPy array creation functions, as exemplified in \[neps.like-kwarg.usage-and-impact\](\#neps.like-kwarg.usage-and-impact).

## Related work

Other NEPs have been written to address parts of `__array_function__` protocol's limitation, such as the introduction of the `__duckarray__` protocol in NEP 30\[4\], and the introduction of an overriding mechanism called `uarray` by NEP 31\[5\].

## Implementation

The implementation requires introducing a new `like=` keyword to all existing array creation functions of NumPy. As examples of functions that would add this new argument (but not limited to) we can cite those taking array-like objects such as `array` and `asarray`, functions that create arrays based on numerical inputs such as `range` and `identity`, as well as the `empty` family of functions, even though that may be redundant, since specializations for those already exist with the naming format `empty_like`. As of the writing of this NEP, a complete list of array creation functions can be found in\[6\].

This newly proposed keyword shall be removed by the `__array_function__` mechanism from the keyword dictionary before dispatching. The purpose for this is twofold:

1.  Simplifies adoption of array creation by those libraries already opting-in to implement the `__array_function__` protocol, thus removing the requirement to explicitly opt-in for all array creation functions; and
2.  Most downstream libraries will have no use for the keyword argument, and those that do may accomplish so by capturing `self` from `__array_function__`.

Downstream libraries thus do not require to include the `like=` keyword to their array creation APIs. In some cases (e.g., Dask), having the `like=` keyword can be useful, as it would allow the implementation to identify array internals. As an example, Dask could benefit from the reference array to identify its chunk type (e.g., NumPy, CuPy, Sparse), and thus create a new Dask array backed by the same chunk type, something that's not possible unless Dask can read the reference array's attributes.

### Function Dispatching

There are two different cases to dispatch: Python functions, and C functions. To permit `__array_function__` dispatching, one possible implementation is to decorate Python functions with `overrides.array_function_dispatch`, but C functions have a different requirement, which we shall describe shortly.

The example below shows a suggestion on how the `asarray` could be decorated with `overrides.array_function_dispatch`:

``` python
def _asarray_decorator(a, dtype=None, order=None, *, like=None):
    return (like,)

@set_module('numpy')
@array_function_dispatch(_asarray_decorator)
def asarray(a, dtype=None, order=None, *, like=None):
    return array(a, dtype, copy=False, order=order)
```

Note in the example above that the implementation remains unchanged, the only difference is the decoration, which uses the new `_asarray_decorator` function to instruct the `__array_function__` protocol to dispatch if `like` is not `None`.

We will now look at a C function example, and since `asarray` is anyway a specialization of `array`, we will use the latter as an example now. As `array` is a C function, currently all NumPy does regarding its Python source is to import the function and adjust its `__module__` to `numpy`. The function will now be decorated with a specialization of `overrides.array_function_from_dispatcher`, which shall take care of adjusting the module too.

``` python
array_function_nodocs_from_c_func_and_dispatcher = functools.partial(
    overrides.array_function_from_dispatcher,
    module='numpy', docs_from_dispatcher=False, verify=False)

@array_function_nodocs_from_c_func_and_dispatcher(_multiarray_umath.array)
def array(a, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,
          like=None):
    return (like,)
```

There are two downsides to the implementation above for C functions:

1.  It creates another Python function call; and
2.  To follow current implementation standards, documentation should be attached directly to the Python source code.

The first version of this proposal suggested the implementation above as one viable solution for NumPy functions implemented in C. However, due to the downsides pointed out above we have decided to discard any changes on the Python side and resolve those issues with a pure-C implementation. Please refer to \[7\] for details.

### Reading the Reference Array Downstream

As stated in the beginning of \[neps.like-kwarg.implementation\](\#neps.like-kwarg.implementation) section, `like=` is not propagated to the downstream library, nevertheless, it's still possible to access it. This requires some changes in the downstream library's `__array_function__` definition, where the `self` attribute is in practice that passed via `like=`. This is the case because we use `like=` as the dispatching array, unlike other compute functions covered by NEP-18 that usually dispatch on the first positional argument.

An example of such use is to create a new Dask array while preserving its backend type:

``` python
# Returns dask.array<array, shape=(3,), dtype=int64, chunksize=(3,), chunktype=cupy.ndarray>
np.asarray([1, 2, 3], like=da.array(cp.array(())))

# Returns a cupy.ndarray
type(np.asarray([1, 2, 3], like=da.array(cp.array(()))).compute())
```

Note how above the array is backed by `chunktype=cupy.ndarray`, and the resulting array after computing it is also a `cupy.ndarray`. If Dask did not use the `like=` argument via the `self` attribute from `__array_function__`, the example above would be backed by `numpy.ndarray` instead:

``` python
# Returns dask.array<array, shape=(3,), dtype=int64, chunksize=(3,), chunktype=numpy.ndarray>
np.asarray([1, 2, 3], like=da.array(cp.array(())))

# Returns a numpy.ndarray
type(np.asarray([1, 2, 3], like=da.array(cp.array(()))).compute())
```

Given the library would need to rely on `self` attribute from `__array_function__` to dispatch the function with the correct reference array, we suggest one of two alternatives:

1.  Introduce a list of functions in the downstream library that do support the `like=` argument and pass `like=self` when calling the function; or
2.  Inspect whether the function's signature and verify whether it includes the `like=` argument. Note that this may incur in a higher performance penalty and assumes introspection is possible, which may not be if the function is a C function.

To make things clearer, let's take a look at how suggestion 2 could be implemented in Dask. The current relevant part of `__array_function__` definition in Dask is seen below:

``` python
def __array_function__(self, func, types, args, kwargs):
    # Code not relevant for this example here

    # Dispatch ``da_func`` (da.asarray, for example) with *args and **kwargs
    da_func(*args, **kwargs)
```

And this is how the updated code would look like:

``` python
def __array_function__(self, func, types, args, kwargs):
    # Code not relevant for this example here

    # Inspect ``da_func``'s  signature and store keyword-only arguments
    import inspect
    kwonlyargs = inspect.getfullargspec(da_func).kwonlyargs

    # If ``like`` is contained in ``da_func``'s signature, add ``like=self``
    # to the kwargs dictionary.
    if 'like' in kwonlyargs:
        kwargs['like'] = self

    # Dispatch ``da_func`` (da.asarray, for example) with args and kwargs.
    # Here, kwargs contain ``like=self`` if the function's signature does too.
    da_func(*args, **kwargs)
```

## Alternatives

Recently a new protocol to replace `__array_function__` entirely was proposed by NEP 37\[8\], which would require considerable rework by downstream libraries that adopt `__array_function__` already, because of that we still believe the `like=` argument is beneficial for NumPy and downstream libraries. However, that proposal wouldn't necessarily be considered a direct alternative to the present NEP, as it would replace NEP 18 entirely, upon which this builds. Discussion on details about this new proposal and why that would require rework by downstream libraries is beyond the scope of the present proposal.

## Discussion

  - [Further discussion on implementation and the NEP's content](https://mail.python.org/pipermail/numpy-discussion/2020-August/080919.html)
  - [Decision to release an experimental implementation in NumPy 1.20.0](https://mail.python.org/pipermail/numpy-discussion/2020-November/081193.html)

## References

## Copyright

This document has been placed in the public domain.

1.  \[NEP18\](\#nep18).

2.  [PEP 3102 â€” Keyword-Only Arguments](https://www.python.org/dev/peps/pep-3102/).

3.  \[NEP18\](\#nep18).

4.  \[NEP30\](\#nep30).

5.  \[NEP31\](\#nep31).

6.  [Array creation routines](https://docs.scipy.org/doc/numpy-1.17.0/reference/routines.array-creation.html).

7.  [Implementation's pull request on GitHub](https://github.com/numpy/numpy/pull/16935)

8.  \[NEP37\](\#nep37).

---

nep-0036-fair-play.md

---

# NEP 36 â€” Fair play

  - Author  
    StÃ©fan van der Walt \<<stefanv@berkeley.edu>\>

  - Status  
    Active

  - Type  
    Informational

  - Created  
    2019-10-24

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2021-June/081890.html>

## Abstract

This document sets out Rules of Play for companies and outside developers that engage with the NumPy project. It covers:

  - Restrictions on use of the NumPy name
  - How and whether to publish a modified distribution
  - How to make us aware of patched versions

Companies and developers will know after reading this NEP what kinds of behavior the community would like to see, and which we consider troublesome, bothersome, and unacceptable.

## Motivation

Every so often, we learn of NumPy versions modified and circulated by outsiders. These patched versions can cause problems for the NumPy community (see, e.g.,\[1\] and\[2\]). When issues like these arise, our developers waste time identifying the problematic release, locating alterations, and determining an appropriate course of action.

In addition, packages on the Python Packaging Index are sometimes named such that users assume they are sanctioned or maintained by NumPy. We wish to reduce the number of such incidents.

During a community call on [October 16th, 2019](https://github.com/numpy/archive/blob/main/status_meetings/status-2019-10-16.md) the community resolved to draft guidelines to address these matters.

## Scope

This document aims to define a minimal set of rules that, when followed, will be considered good-faith efforts in line with the expectations of the NumPy developers.

Our hope is that developers who feel they need to modify NumPy will first consider contributing to the project, or use one of several existing mechanisms for extending our APIs and for operating on externally defined array objects.

When in doubt, please [talk to us first](https://numpy.org/community/). We may suggest an alternative; at minimum, we'll be prepared.

## Fair play rules

1.  Do not reuse the NumPy name for projects not developed by the NumPy community.
    
    At time of writing, there are only a handful of `numpy`-named packages developed by the community, including `numpy`, `numpy-financial`, and `unumpy`. We ask that external packages not include the phrase `numpy`, i.e., avoid names such as `mycompany_numpy`.
    
    To be clear, this rule only applies to modules (package names); it is perfectly acceptable to have a *submodule* of your own library named `mylibrary.numpy`.
    
    NumPy is a trademark owned by NumFOCUS.

2.  Do not republish modified versions of NumPy.
    
    Modified versions of NumPy make it very difficult for the developers to address bug reports, since we typically do not know which parts of NumPy have been modified.
    
    If you have to break this rule (and we implore you not to\!), then make it clear in the `__version__` tag that you have modified NumPy, e.g.:
    
        >>> print(np.__version__)
        '1.17.2+mycompany.15`
    
    We understand that minor patches are often required to make a library work inside of a distribution. E.g., Debian may patch NumPy so that it searches for optimized BLAS libraries in the correct locations. This is acceptable, but we ask that no substantive changes are made.

3.  Do not extend or modify NumPy's API.
    
    If you absolutely have to break rule two, please do not add additional functions to the namespace, or modify the API of existing functions. NumPy's API is already quite large, and we are working hard to reduce it where feasible. Having additional functions exposed in distributed versions is confusing for users and developers alike.

4.  *DO* use official mechanism to engage with the API.
    
    Protocols such as \[\_\_array\_ufunc\_\_ \<NEP13\>\](\#\_\_array\_ufunc\_\_-\<nep13\>) and \[\_\_array\_function\_\_ \<NEP18\>\](\#\_\_array\_function\_\_-\<nep18\>) were designed to help external packages interact more easily with NumPy. E.g., the latter allows objects from foreign libraries to pass through NumPy. We actively encourage using any of these "officially sanctioned" mechanisms for overriding or interacting with NumPy.
    
    If these mechanisms are deemed insufficient, please start a discussion on the mailing list before monkeypatching NumPy.

## Questions and answers

**Q:** We would like to distribute an optimized version of NumPy that utilizes special instructions for our company's CPU. You recommend against that, so what are we to do?

**A:** Please consider including the patches required in the official NumPy repository. Not only do we encourage such contributions, but we already have optimized loops for some platforms available.

**Q:** We would like to ship a much faster version of FFT than NumPy provides, but NumPy has no mechanism for overriding its FFT routines. How do we proceed?

**A:** There are two solutions that we approve of: let the users install your optimizations using a piece of code, such as:

    from my_company_accel import patch_numpy_fft
    patch_numpy_fft()

or have your distribution automatically perform the above, but print a message to the terminal clearly stating what is happening:

    We are now patching NumPy for optimal performance under MyComp
    Special Platform.  Please direct all bug reports to
    https://mycomp.com/numpy-bugs

If you require additional mechanisms for overriding code, please discuss this with the development team on the mailing list.

**Q:** We would like to distribute NumPy with faster linear algebra routines. Are we allowed to do this?

**A:** Yes, this is explicitly supported by linking to a different version of BLAS.

## Discussion

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  In December 2018, a [bug report](https://github.com/numpy/numpy/issues/12515) was filed against <span class="title-ref">np.erf</span> -- a function that didn't exist in the NumPy distribution. It came to light that a company had published a NumPy version with an extended API footprint. After several months of discussion, the company agreed to make its patches public, and we added a label to the NumPy issue tracker to identify issues pertaining to that distribution.

2.  After a security issue (CVE-2019-6446) was filed against NumPy, distributions put in their own fixes, most often by changing a default keyword value. As a result the NumPy API was inconsistent across distributions.

---

nep-0037-array-module.md

---

# NEP 37 â€” A dispatch protocol for NumPy-like modules

  - Author  
    Stephan Hoyer \<<shoyer@google.com>\>

  - Author  
    Hameer Abbasi

  - Author  
    Sebastian Berg

  - Status  
    Superseded

  - Replaced-By  
    \[NEP56\](\#nep56)

  - Type  
    Standards Track

  - Created  
    2019-12-29

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/message/Z6AA5CL47NHBNEPTFWYOTSUVSRDGHYPN/>

## Abstract

NEP-18's `__array_function__` has been a mixed success. Some projects (e.g., dask, CuPy, xarray, sparse, Pint, MXNet) have enthusiastically adopted it. Others (e.g., JAX) have been more reluctant. Here we propose a new protocol, `__array_module__`, that we expect could eventually subsume most use-cases for `__array_function__`. The protocol requires explicit adoption by both users and library authors, which ensures backwards compatibility, and is also significantly simpler than `__array_function__`, both of which we expect will make it easier to adopt.

## Why `__array_function__` hasn't been enough

There are two broad ways in which \[NEP-18 \<NEP18\>\](\#nep-18-\<nep18\>) has fallen short of its goals:

1.  **Backwards compatibility concerns**. <span class="title-ref">\_\_array\_function\_\_</span> has significant implications for libraries that use it:
      - [JAX](https://github.com/google/jax/issues/1565) has been reluctant to implement `__array_function__` in part because it is concerned about breaking existing code: users expect NumPy functions like `np.concatenate` to return NumPy arrays. This is a fundamental limitation of the `__array_function__` design, which we chose to allow overriding the existing `numpy` namespace. Libraries like Dask and CuPy have looked at and accepted the backwards incompatibility impact of `__array_function__`; it would still have been better for them if that impact didn't exist.
        
        Note that projects like [PyTorch](https://github.com/pytorch/pytorch/issues/22402) and [scipy.sparse](https://github.com/scipy/scipy/issues/10362) have also not adopted `__array_function__` yet, because they don't have a NumPy-compatible API or semantics. In the case of PyTorch, that is likely to be added in the future. `scipy.sparse` is in the same situation as `numpy.matrix`: its semantics are not compatible with `numpy.ndarray` and therefore adding `__array_function__` (except to return `NotImplemented` perhaps) is not a healthy idea.
    
      - `__array_function__` currently requires an "all or nothing" approach to implementing NumPy's API. There is no good pathway for **incremental adoption**, which is particularly problematic for established projects for which adopting `__array_function__` would result in breaking changes.
2.  **Limitations on what can be overridden.** `__array_function__` has some important gaps, most notably array creation and coercion functions:
      - **Array creation** routines (e.g., `np.arange` and those in `np.random`) need some other mechanism for indicating what type of arrays to create. \[NEP 35 \<NEP35\>\](\#nep-35-\<nep35\>) proposed adding optional `like=` arguments to functions without existing array arguments. However, we still lack any mechanism to override methods on objects, such as those needed by `np.random.RandomState`.
      - **Array conversion** can't reuse the existing coercion functions like `np.asarray`, because `np.asarray` sometimes means "convert to an exact `np.ndarray`" and other times means "convert to something \_[like]() a NumPy array." This led to the \[NEP 30 \<NEP30\>\](\#nep-30-\<nep30\>) proposal for a separate `np.duckarray` function, but this still does not resolve how to cast one duck array into a type matching another duck array.

Other maintainability concerns that were raised include:

  - It is no longer possible to use **aliases to NumPy functions** within modules that support overrides. For example, both CuPy and JAX set `result_type = np.result_type` and now have to wrap use of `np.result_type` in their own `result_type` function instead.
  - Implementing **fall-back mechanisms** for unimplemented NumPy functions by using NumPy's implementation is hard to get right (but see the [version from dask](https://github.com/dask/dask/pull/5043)), because `__array_function__` does not present a consistent interface. Converting all arguments of array type requires recursing into generic arguments of the form `*args, **kwargs`.

## `get_array_module` and the `__array_module__` protocol

We propose a new user-facing mechanism for dispatching to a duck-array implementation, `numpy.get_array_module`. `get_array_module` performs the same type resolution as `__array_function__` and returns a module with an API promised to match the standard interface of `numpy` that can implement operations on all provided array types.

The protocol itself is both simpler and more powerful than `__array_function__`, because it doesn't need to worry about actually implementing functions. We believe it resolves most of the maintainability and functionality limitations of `__array_function__`.

The new protocol is opt-in, explicit and with local control; see \[appendix-design-choices\](\#appendix-design-choices) for discussion on the importance of these design features.

### The array module contract

Modules returned by `get_array_module`/`__array_module__` should make a best effort to implement NumPy's core functionality on new array types(s). Unimplemented functionality should simply be omitted (e.g., accessing an unimplemented function should raise `AttributeError`). In the future, we anticipate codifying a protocol for requesting restricted subsets of `numpy`; see \[requesting-restricted-subsets\](\#requesting-restricted-subsets) for more details.

### How to use `get_array_module`

Code that wants to support generic duck arrays should explicitly call `get_array_module` to determine an appropriate array module from which to call functions, rather than using the `numpy` namespace directly. For example:

``` python
# calls the appropriate version of np.something for x and y
module = np.get_array_module(x, y)
module.something(x, y)
```

Both array creation and array conversion are supported, because dispatching is handled by `get_array_module` rather than via the types of function arguments. For example, to use random number generation functions or methods, we can simply pull out the appropriate submodule:

``` python
def duckarray_add_random(array):
    module = np.get_array_module(array)
    noise = module.random.randn(*array.shape)
    return array + noise
```

We can also write the duck-array `stack` function from \[NEP 30 \<NEP30\>\](\#nep-30-\<nep30\>), without the need for a new `np.duckarray` function:

``` python
def duckarray_stack(arrays):
    module = np.get_array_module(*arrays)
    arrays = [module.asarray(arr) for arr in arrays]
    shapes = {arr.shape for arr in arrays}
    if len(shapes) != 1:
        raise ValueError('all input arrays must have the same shape')
    expanded_arrays = [arr[module.newaxis, ...] for arr in arrays]
    return module.concatenate(expanded_arrays, axis=0)
```

By default, `get_array_module` will return the `numpy` module if no arguments are arrays. This fall-back can be explicitly controlled by providing the `module` keyword-only argument. It is also possible to indicate that an exception should be raised instead of returning a default array module by setting `module=None`.

### How to implement `__array_module__`

Libraries implementing a duck array type that want to support `get_array_module` need to implement the corresponding protocol, `__array_module__`. This new protocol is based on Python's dispatch protocol for arithmetic, and is essentially a simpler version of `__array_function__`.

Only one argument is passed into `__array_module__`, a Python collection of unique array types passed into `get_array_module`, i.e., all arguments with an `__array_module__` attribute.

The special method should either return a namespace with an API matching `numpy`, or `NotImplemented`, indicating that it does not know how to handle the operation:

``` python
class MyArray:
    def __array_module__(self, types):
        if not all(issubclass(t, MyArray) for t in types):
            return NotImplemented
        return my_array_module
```

#### Returning custom objects from `__array_module__`

`my_array_module` will typically, but need not always, be a Python module. Returning a custom objects (e.g., with functions implemented via `__getattr__`) may be useful for some advanced use cases.

For example, custom objects could allow for partial implementations of duck array modules that fall-back to NumPy (although this is not recommended in general because such fall-back behavior can be error prone):

``` python
class MyArray:
    def __array_module__(self, types):
        if all(issubclass(t, MyArray) for t in types):
            return ArrayModule()
        else:
            return NotImplemented

class ArrayModule:
    def __getattr__(self, name):
        import base_module
        return getattr(base_module, name, getattr(numpy, name))
```

#### Subclassing from `numpy.ndarray`

All of the same guidance about well-defined type casting hierarchies from NEP-18 still applies. `numpy.ndarray` itself contains a matching implementation of `__array_module__`, which is convenient for subclasses:

``` python
class ndarray:
    def __array_module__(self, types):
        if all(issubclass(t, ndarray) for t in types):
            return numpy
        else:
            return NotImplemented
```

### NumPy's internal machinery

The type resolution rules of `get_array_module` follow the same model as Python and NumPy's existing dispatch protocols: subclasses are called before super-classes, and otherwise left to right. `__array_module__` is guaranteed to be called only a single time on each unique type.

The actual implementation of <span class="title-ref">get\_array\_module</span> will be in C, but should be equivalent to this Python code:

``` python
def get_array_module(*arrays, default=numpy):
    implementing_arrays, types = _implementing_arrays_and_types(arrays)
    if not implementing_arrays and default is not None:
        return default
    for array in implementing_arrays:
        module = array.__array_module__(types)
        if module is not NotImplemented:
            return module
    raise TypeError("no common array module found")

def _implementing_arrays_and_types(relevant_arrays):
    types = []
    implementing_arrays = []
    for array in relevant_arrays:
        t = type(array)
        if t not in types and hasattr(t, '__array_module__'):
            types.append(t)
            # Subclasses before superclasses, otherwise left to right
            index = len(implementing_arrays)
            for i, old_array in enumerate(implementing_arrays):
                if issubclass(t, type(old_array)):
                    index = i
                    break
            implementing_arrays.insert(index, array)
    return implementing_arrays, types
```

## Relationship with `__array_ufunc__` and `__array_function__`

### These older protocols have distinct use-cases and should remain

`__array_module__` is intended to resolve limitations of `__array_function__`, so it is natural to consider whether it could entirely replace `__array_function__`. This would offer dual benefits: (1) simplifying the user-story about how to override NumPy and (2) removing the slowdown associated with checking for dispatch when calling every NumPy function.

However, `__array_module__` and `__array_function__` are pretty different from a user perspective: it requires explicit calls to `get_array_function`, rather than simply reusing original `numpy` functions. This is probably fine for *libraries* that rely on duck-arrays, but may be frustratingly verbose for interactive use.

Some of the dispatching use-cases for `__array_ufunc__` are also solved by `__array_module__`, but not all of them. For example, it is still useful to be able to define non-NumPy ufuncs (e.g., from Numba or SciPy) in a generic way on non-NumPy arrays (e.g., with dask.array).

Given their existing adoption and distinct use cases, we don't think it makes sense to remove or deprecate `__array_function__` and `__array_ufunc__` at this time.

### Mixin classes to implement `__array_function__` and `__array_ufunc__`

Despite the user-facing differences, `__array_module__` and a module implementing NumPy's API still contain sufficient functionality needed to implement dispatching with the existing duck array protocols.

For example, the following mixin classes would provide sensible defaults for these special methods in terms of `get_array_module` and `__array_module__`:

``` python
class ArrayUfuncFromModuleMixin:

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        arrays = inputs + kwargs.get('out', ())
        try:
            array_module = np.get_array_module(*arrays)
        except TypeError:
            return NotImplemented

        try:
            # Note this may have false positive matches, if ufunc.__name__
            # matches the name of a ufunc defined by NumPy. Unfortunately
            # there is no way to determine in which module a ufunc was
            # defined.
            new_ufunc = getattr(array_module, ufunc.__name__)
        except AttributeError:
            return NotImplemented

        try:
            callable = getattr(new_ufunc, method)
        except AttributeError:
            return NotImplemented

        return callable(*inputs, **kwargs)

class ArrayFunctionFromModuleMixin:

    def __array_function__(self, func, types, args, kwargs):
        array_module = self.__array_module__(types)
        if array_module is NotImplemented:
            return NotImplemented

        # Traverse submodules to find the appropriate function
        modules = func.__module__.split('.')
        assert modules[0] == 'numpy'
        for submodule in modules[1:]:
            module = getattr(module, submodule, None)
        new_func = getattr(module, func.__name__, None)
        if new_func is None:
            return NotImplemented

        return new_func(*args, **kwargs)
```

To make it easier to write duck arrays, we could also add these mixin classes into `numpy.lib.mixins` (but the examples above may suffice).

## Alternatives considered

### Naming

We like the name `__array_module__` because it mirrors the existing `__array_function__` and `__array_ufunc__` protocols. Another reasonable choice could be `__array_namespace__`.

It is less clear what the NumPy function that calls this protocol should be called (`get_array_module` in this proposal). Some possible alternatives: `array_module`, `common_array_module`, `resolve_array_module`, `get_namespace`, `get_numpy`, `get_numpylike_module`, `get_duck_array_module`.

### Requesting restricted subsets of NumPy's API

Over time, NumPy has accumulated a very large API surface, with over 600 attributes in the top level `numpy` module alone. It is unlikely that any duck array library could or would want to implement all of these functions and classes, because the frequently used subset of NumPy is much smaller.

We think it would be useful exercise to define "minimal" subset(s) of NumPy's API, omitting rarely used or non-recommended functionality. For example, minimal NumPy might include `stack`, but not the other stacking functions `column_stack`, `dstack`, `hstack` and `vstack`. This could clearly indicate to duck array authors and users what functionality is core and what functionality they can skip.

Support for requesting a restricted subset of NumPy's API would be a natural feature to include in `get_array_function` and `__array_module__`, e.g.,

``` python
# array_module is only guaranteed to contain "minimal" NumPy
array_module = np.get_array_module(*arrays, request='minimal')
```

To facilitate testing with NumPy and use with any valid duck array library, NumPy itself would return restricted versions of the `numpy` module when `get_array_module` is called only on NumPy arrays. Omitted functions would simply not exist.

Unfortunately, we have not yet figured out what these restricted subsets should be, so it doesn't make sense to do this yet. When/if we do, we could either add new keyword arguments to `get_array_module` or add new top level functions, e.g., `get_minimal_array_module`. We would also need to add either a new protocol patterned off of `__array_module__` (e.g., `__array_module_minimal__`), or could add an optional second argument to `__array_module__` (catching errors with `try`/`except`).

### A new namespace for implicit dispatch

Instead of supporting overrides in the main <span class="title-ref">numpy</span> namespace with `__array_function__`, we could create a new opt-in namespace, e.g., `numpy.api`, with versions of NumPy functions that support dispatching. These overrides would need new opt-in protocols, e.g., `__array_function_api__` patterned off of `__array_function__`.

This would resolve the biggest limitations of `__array_function__` by being opt-in and would also allow for unambiguously overriding functions like `asarray`, because `np.api.asarray` would always mean "convert an array-like object." But it wouldn't solve all the dispatching needs met by `__array_module__`, and would leave us with supporting a considerably more complex protocol both for array users and implementers.

We could potentially implement such a new namespace *via* the `__array_module__` protocol. Certainly some users would find this convenient, because it is slightly less boilerplate. But this would leave users with a confusing choice: when should they use <span class="title-ref">get\_array\_module</span> vs. <span class="title-ref">np.api.something</span>. Also, we would have to add and maintain a whole new module, which is considerably more expensive than merely adding a function.

### Dispatching on both types and arrays instead of only types

Instead of supporting dispatch only via unique array types, we could also support dispatch via array objects, e.g., by passing an `arrays` argument as part of the `__array_module__` protocol. This could potentially be useful for dispatch for arrays with metadata, such provided by Dask and Pint, but would impose costs in terms of type safety and complexity.

For example, a library that supports arrays on both CPUs and GPUs might decide on which device to create a new arrays from functions like `ones` based on input arguments:

``` python
class Array:
    def __array_module__(self, types, arrays):
        useful_arrays = tuple(a in arrays if isinstance(a, Array))
        if not useful_arrays:
            return NotImplemented
        prefer_gpu = any(a.prefer_gpu for a in useful_arrays)
        return ArrayModule(prefer_gpu)

class ArrayModule:
    def __init__(self, prefer_gpu):
        self.prefer_gpu = prefer_gpu

    def __getattr__(self, name):
        import base_module
        base_func = getattr(base_module, name)
        return functools.partial(base_func, prefer_gpu=self.prefer_gpu)
```

This might be useful, but it's not clear if we really need it. Pint seems to get along OK without any explicit array creation routines (favoring multiplication by units, e.g., `np.ones(5) * ureg.m`), and for the most part Dask is also OK with existing `__array_function__` style overrides (e.g., favoring `np.ones_like` over `np.ones`). Choosing whether to place an array on the CPU or GPU could be solved by [making array creation lazy](https://github.com/google/jax/pull/1668).

## Appendix: design choices for API overrides

There is a large range of possible design choices for overriding NumPy's API. Here we discuss three major axes of the design decision that guided our design for `__array_module__`.

### Opt-in vs. opt-out for users

The `__array_ufunc__` and `__array_function__` protocols provide a mechanism for overriding NumPy functions *within NumPy's existing namespace*. This means that users need to explicitly opt-out if they do not want any overridden behavior, e.g., by casting arrays with `np.asarray()`.

In theory, this approach lowers the barrier for adopting these protocols in user code and libraries, because code that uses the standard NumPy namespace is automatically compatible. But in practice, this hasn't worked out. For example, most well-maintained libraries that use NumPy follow the best practice of casting all inputs with `np.asarray()`, which they would have to explicitly relax to use `__array_function__`. Our experience has been that making a library compatible with a new duck array type typically requires at least a small amount of work to accommodate differences in the data model and operations that can be implemented efficiently.

These opt-out approaches also considerably complicate backwards compatibility for libraries that adopt these protocols, because by opting in as a library they also opt-in their users, whether they expect it or not. For winning over libraries that have been unable to adopt `__array_function__`, an opt-in approach seems like a must.

### Explicit vs. implicit choice of implementation

Both `__array_ufunc__` and `__array_function__` have implicit control over dispatching: the dispatched functions are determined via the appropriate protocols in every function call. This generalizes well to handling many different types of objects, as evidenced by its use for implementing arithmetic operators in Python, but it has an important downside for **readability**: it is not longer immediately evident to readers of code what happens when a function is called, because the function's implementation could be overridden by any of its arguments.

The **speed** implications are:

  - When using a *duck-array type*, `get_array_module` means type checking only needs to happen once inside each function that supports duck typing, whereas with `__array_function__` it happens every time a NumPy function is called. Obvious it's going to depend on the function, but if a typical duck-array supporting function calls into other NumPy functions 3-5 times this is a factor of 3-5x more overhead.
  - When using *NumPy arrays*, `get_array_module` is one extra call per function (`__array_function__` overhead remains the same), which means a small amount of extra overhead.

Explicit and implicit choice of implementations are not mutually exclusive options. Indeed, most implementations of NumPy API overrides via `__array_function__` that we are familiar with (namely, Dask, CuPy and Sparse, but not Pint) also include an explicit way to use their version of NumPy's API by importing a module directly (`dask.array`, `cupy` or `sparse`, respectively).

### Local vs. non-local vs. global control

The final design axis is how users control the choice of API:

  - **Local control**, as exemplified by multiple dispatch and Python protocols for arithmetic, determines which implementation to use either by checking types or calling methods on the direct arguments of a function.
  - **Non-local control** such as [np.errstate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.errstate.html) overrides behavior with global-state via function decorators or context-managers. Control is determined hierarchically, via the inner-most context.
  - **Global control** provides a mechanism for users to set default behavior, either via function calls or configuration files. For example, matplotlib allows setting a global choice of plotting backend.

Local control is generally considered a best practice for API design, because control flow is entirely explicit, which makes it the easiest to understand. Non-local and global control are occasionally used, but generally either due to ignorance or a lack of better alternatives.

In the case of duck typing for NumPy's public API, we think non-local or global control would be mistakes, mostly because they **don't compose well**. If one library sets/needs one set of overrides and then internally calls a routine that expects another set of overrides, the resulting behavior may be very surprising. Higher order functions are especially problematic, because the context in which functions are evaluated may not be the context in which they are defined.

One class of override use cases where we think non-local and global control are appropriate is for choosing a backend system that is guaranteed to have an entirely consistent interface, such as a faster alternative implementation of `numpy.fft` on NumPy arrays. However, these are out of scope for the current proposal, which is focused on duck arrays.

---

nep-0038-SIMD-optimizations.md

---

# NEP 38 â€” Using SIMD optimization instructions for performance

  - Author  
    Sayed Adel, Matti Picus, Ralf Gommers

  - Status  
    Final

  - Type  
    Standards

  - Created  
    2019-11-25

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/PVWJ74UVBRZ5ZWF6MDU7EUSJXVNILAQB/#PVWJ74UVBRZ5ZWF6MDU7EUSJXVNILAQB>

## Abstract

While compilers are getting better at using hardware-specific routines to optimize code, they sometimes do not produce optimal results. Also, we would like to be able to copy binary optimized C-extension modules from one machine to another with the same base architecture (x86, ARM, or PowerPC) but with different capabilities without recompiling.

We have a mechanism in the ufunc machinery to [build alternative loops](https://github.com/numpy/numpy/blob/v1.17.4/numpy/core/code_generators/generate_umath.py#L50) indexed by CPU feature name. At import (in `InitOperators`), the loop function that matches the run-time CPU info [is chosen](https://github.com/numpy/numpy/blob/v1.17.4/numpy/core/code_generators/generate_umath.py#L1038) from the candidates.This NEP proposes a mechanism to build on that for many more features and architectures. The steps proposed are to:

  - Establish a set of well-defined, architecture-agnostic, universal intrinsics which capture features available across architectures.
  - Capture these universal intrinsics in a set of C macros and use the macros to build code paths for sets of features from the baseline up to the maximum set of features available on that architecture. Offer these as a limited number of compiled alternative code paths.
  - At runtime, discover which CPU features are available, and choose from among the possible code paths accordingly.

## Motivation and scope

Traditionally NumPy has depended on compilers to generate optimal code specifically for the target architecture. However few users today compile NumPy locally for their machines. Most use the binary packages which must provide run-time support for the lowest-common denominator CPU architecture. Thus NumPy cannot take advantage of more advanced features of their CPU processors, since they may not be available on all users' systems.

Traditionally, CPU features have been exposed through [intrinsics](https://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-intrinsics) which are compiler-specific instructions that map directly to assembly instructions. Recently there were discussions about the effectiveness of adding more intrinsics (e.g., [gh-11113](https://github.com/numpy/numpy/pull/11113) for AVX optimizations for floats). In the past, architecture-specific code was added to NumPy for [fast avx512 routines](https://github.com/numpy/numpy/pulls?q=is%3Apr+avx512+is%3Aclosed) in various ufuncs, using the mechanism described above to choose the best loop for the architecture. However the code is not generic and does not generalize to other architectures.

Recently, OpenCV moved to using [universal intrinsics](https://docs.opencv.org/master/df/d91/group__core__hal__intrin.html) in the Hardware Abstraction Layer (HAL) which provided a nice abstraction for common shared Single Instruction Multiple Data (SIMD) constructs. This NEP proposes a similar mechanism for NumPy. There are three stages to using the mechanism:

  - Infrastructure is provided in the code for abstract intrinsics. The ufunc machinery will be extended using sets of these abstract intrinsics, so that a single ufunc will be expressed as a set of loops, going from a minimal to a maximal set of possibly available intrinsics.
  - At compile time, compiler macros and CPU detection are used to turn the abstract intrinsics into concrete intrinsic calls. Any intrinsics not available on the platform, either because the CPU does not support them (and so cannot be tested) or because the abstract intrinsic does not have a parallel concrete intrinsic on the platform will not error, rather the corresponding loop will not be produced and added to the set of possibilities.
  - At runtime, the CPU detection code will further limit the set of loops available, and the optimal one will be chosen for the ufunc.

The current NEP proposes only to use the runtime feature detection and optimal loop selection mechanism for ufuncs. Future NEPS may propose other uses for the proposed solution.

The ufunc machinery already has the ability to select an optimal loop for specifically available CPU features at runtime, currently used for `avx2`, `fma` and `avx512f` loops (in the generated `__umath_generated.c` file); universal intrinsics would extend the generated code to include more loop variants.

## Usage and impact

The end user will be able to get a list of intrinsics available for their platform and compiler. Optionally, the user may be able to specify which of the loops available at runtime will be used, perhaps via an environment variable to enable benchmarking the impact of the different loops. There should be no direct impact to naive end users, the results of all the loops should be identical to within a small number (1-3?) ULPs. On the other hand, users with more powerful machines should notice a significant performance boost.

### Binary releases - wheels on PyPI and conda packages

The binaries released by this process will be larger since they include all possible loops for the architecture. Some packagers may prefer to limit the number of loops in order to limit the size of the binaries, we would hope they would still support a wide range of families of architectures. Note this problem already exists in the Intel MKL offering, where the binary package includes an extensive set of alternative shared objects (DLLs) for various CPU alternatives.

### Source builds

See "Detailed Description" below. A source build where the packager knows details of the target machine could theoretically produce a smaller binary by choosing to compile only the loops needed by the target via command line arguments.

### How to run benchmarks to assess performance benefits

Adding more code which use intrinsics will make the code harder to maintain. Therefore, such code should only be added if it yields a significant performance benefit. Assessing this performance benefit can be nontrivial. To aid with this, the implementation for this NEP will add a way to select which instruction sets can be used at *runtime* via environment variables. (name TBD). This ability is critical for CI code verification.

### Diagnostics

A new dictionary `__cpu_features__` will be available to python. The keys are the available features, the value is a boolean whether the feature is available or not. Various new private C functions will be used internally to query available features. These might be exposed via specific c-extension modules for testing.

### Workflow for adding a new CPU architecture-specific optimization

NumPy will always have a baseline C implementation for any code that may be a candidate for SIMD vectorization. If a contributor wants to add SIMD support for some architecture (typically the one of most interest to them), this comment is the beginning of a tutorial on how to do so: <https://github.com/numpy/numpy/pull/13516#issuecomment-558859638>

<div id="tradeoffs">

As of this moment, NumPy has a number of `avx512f` and `avx2` and `fma` SIMD loops for many ufuncs. These would likely be the first candidates to be ported to universal intrinsics. The expectation is that the new implementation may cause a regression in benchmarks, but not increase the size of the binary. If the regression is not minimal, we may choose to keep the X86-specific code for that platform and use the universal intrinsic code for other platforms.

</div>

Any new PRs to implement ufuncs using intrinsics will be expected to use the universal intrinsics. If it can be demonstrated that the use of universal intrinsics is too awkward or is not performant enough, platform specific code may be accepted as well. In rare cases, a single-platform only PR may be accepted, but it would have to be examined within the framework of preferring a solution using universal intrinsics.

The subjective criteria for accepting new loops are:

  - correctness: the new code must not decrease accuracy by more than 1-3 ULPs even at edge points in the algorithm.
  - code bloat: both source code size and especially binary size of the compiled wheel.
  - maintainability: how readable is the code
  - performance: benchmarks must show a significant performance boost

#### Adding a new intrinsic

If a contributor wants to use a platform-specific SIMD instruction that is not yet supported as a universal intrinsic, then:

1.  It should be added as a universal intrinsic for all platforms
2.  If it does not have an equivalent instruction on other platforms (e.g. `_mm512_mask_i32gather_ps` in `AVX512`), then no universal intrinsic should be added and a platform-specific `ufunc` or a short helper function should be written instead. If such a helper function is used, it must be wrapped with the feature macros, and a reasonable non-intrinsic fallback to be used by default.

We expect (2) to be the exception. The contributor and maintainers should consider whether that single-platform intrinsic is worth it compared to using the best available universal intrinsic based implementation.

### Reuse by other projects

It would be nice if the universal intrinsics would be available to other libraries like SciPy or Astropy that also build ufuncs, but that is not an explicit goal of the first implementation of this NEP.

## Backward compatibility

There should be no impact on backwards compatibility.

## Detailed description

The CPU-specific are mapped to universal intrinsics which are similar for all x86 SIMD variants, ARM SIMD variants etc. For example, the NumPy universal intrinsic `npyv_load_u32` maps to:

  - `vld1q_u32` for ARM based NEON
  - `_mm256_loadu_si256` for x86 based AVX2
  - `_mm512_loadu_si512` for x86 based AVX-512

Anyone writing a SIMD loop will use the `npyv_load_u32` macro instead of the architecture specific intrinsic. The code also supplies guard macros for compilation and runtime, so that the proper loops can be chosen.

Two new build options are available to `runtests.py` and `setup.py`: `--cpu-baseline` and `--cpu-dispatch`. The absolute minimum required features to compile are defined by `--cpu-baseline`. For instance, on `x86_64` this defaults to `SSE3`. The minimum features will be enabled if the compiler support it. The set of additional intrinsics that can be detected and used as sets of requirements to dispatch on are set by `--cpu-dispatch`. For instance, on `x86_64` this defaults to `[SSSE3, SSE41, POPCNT, SSE42, AVX, F16C, XOP, FMA4, FMA3, AVX2, AVX512F, AVX512CD, AVX512_KNL, AVX512_KNM, AVX512_SKX, AVX512_CLX, AVX512_CNL, AVX512_ICL]`. These features are all mapped to a c-level boolean array `npy__cpu_have`, and a c-level convenience function `npy_cpu_have(int feature_id)` queries this array, and the results are stored in `__cpu_features__` at runtime.

When importing the ufuncs, the available compiled loops' required features are matched to the ones discovered. The loop with the best match is marked to be called by the ufunc.

## Related work

  - [Pixman](https://gitlab.freedesktop.org/pixman) is the library used by Cairo and X to manipulate pixels. It uses a technique like the one described here to fill a structure with function pointers at runtime. These functions are similar to ufunc loops.
  - [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. It is a higher level-abstraction than the intrinsics discussed here.
  - [xsimd](https://xsimd.readthedocs.io/en/latest/) is a header-only C++ library for x86 and ARM that implements the mathematical functions used in the algorithms of `boost.SIMD`.
  - [Simd](https://github.com/ermig1979/Simd) is a high-level image processing and machine learning library with optimizations for different platforms.
  - OpenCV used to have the one-implementation-per-architecture design, but more recently moved to a design that is quite similar to what is proposed in this NEP. The top-level [dispatch code](https://github.com/opencv/opencv/blob/4.1.2/modules/core/src/arithm.dispatch.cpp) includes a [generic header](https://github.com/opencv/opencv/blob/4.1.2/modules/core/src/arithm.simd.hpp) that is [specialized at compile time](https://github.com/opencv/opencv/blob/4.1.2/modules/core/CMakeLists.txt#L3-#L13) by the CMakefile system.
  - [VOLK](https://www.libvolk.org/doxygen/index.html) is a GPL3 library used by gnuradio and others to abstract SIMD intrinsics. They offer a set of high-level operations which have been optimized for each architecture.
  - The C++ Standards Committee has proposed [class templates](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0214r8.pdf) for portable SIMD programming via vector types, and [namespaces](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/n4808.pdf) for the templates.

## Implementation

Current PRs:

  - [gh-13421 improve runtime detection of CPU features](https://github.com/numpy/numpy/pull/13421)
  - [gh-13516: enable multi-platform SIMD compiler optimizations](https://github.com/numpy/numpy/pull/13516)

The compile-time and runtime code infrastructure are supplied by the first PR. The second adds a demonstration of use of the infrastructure for a loop. Once the NEP is approved, more work is needed to write loops using the mechanisms provided by the NEP.

## Alternatives

A proposed alternative in [gh-13516](https://github.com/numpy/numpy/pull/13516) is to implement loops for each CPU architecture separately by hand, without trying to abstract common patterns in the SIMD intrinsics (e.g., have <span class="title-ref">loops.avx512.c.src</span>, <span class="title-ref">loops.avx2.c.src</span>, <span class="title-ref">loops.sse.c.src</span>, <span class="title-ref">loops.vsx.c.src</span>, <span class="title-ref">loops.neon.c.src</span>, etc.). This is more similar to what PIXMAX does. There's a lot of duplication here though, and the manual code duplication requires a champion who will be dedicated to implementing and maintaining that platform's loop code.

## Discussion

Most of the discussion took place on the PR [gh-15228](https://github.com/numpy/numpy/pull/15228) to accept this NEP. Discussion on the mailing list mentioned [VOLK](https://www.libvolk.org/doxygen/index.html) which was added to the section on related work. The question of maintainability also was raised both on the mailing list and in [gh-15228](https://github.com/numpy/numpy/pull/15228) and resolved as follows:

  - If contributors want to leverage a specific SIMD instruction, will they be expected to add software implementation of this instruction for all other architectures too? (see the [new-intrinsics](#new-intrinsics) part of the workflow).
  - On whom does the burden lie to verify the code and benchmarks for all architectures? What happens if adding a universal ufunc in place of architecture-specific code helps one architecture but harms performance on another? (answered in the [tradeoffs](#tradeoffs) part of the workflow).

## References and footnotes

## Copyright

This document has been placed in the public domain.\[1\]

1.  Each NEP must either be explicitly labeled as placed in the public domain (see this NEP as an example) or licensed under the [Open Publication License](https://www.opencontent.org/openpub/).

---

nep-0040-legacy-datatype-impl.md

---

# NEP 40 â€” Legacy datatype implementation in NumPy

  - title  
    Legacy Datatype Implementation in NumPy

  - Author  
    Sebastian Berg

  - Status  
    Final

  - Type  
    Informational

  - Created  
    2019-07-17

\> **Note** \> This NEP is first in a series:

>   - NEP 40 (this document) explains the shortcomings of NumPy's dtype implementation.
>   - \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) gives an overview of our proposed replacement.
>   - \[NEP 42 \<NEP42\>\](\#nep-42-\<nep42\>) describes the new design's datatype-related APIs.
>   - \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) describes the new design's API for universal functions.

## Abstract

As a preparation to further NumPy enhancement proposals 41, 42, and 43. This NEP details the current status of NumPy datatypes as of NumPy 1.18. It describes some of the technical aspects and concepts that motivated the other proposals. For more general information most readers should begin by reading \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) and use this document only as a reference or for additional details.

## Detailed description

This section describes some central concepts and provides a brief overview of the current implementation of dtypes as well as a discussion. In many cases subsections will be split roughly to first describe the current implementation and then follow with an "Issues and Discussion" section.

### Parametric datatypes

Some datatypes are inherently *parametric*. All `np.flexible` scalar types are attached to parametric datatypes (string, bytes, and void). The class `np.flexible` for scalars is a superclass for the data types of variable length (string, bytes, and void). This distinction is similarly exposed by the C-Macros `PyDataType_ISFLEXIBLE` and `PyTypeNum_ISFLEXIBLE`. This flexibility generalizes to the set of values which can be represented inside the array. For instance, `"S8"` can represent longer strings than `"S4"`. The parametric string datatype thus also limits the values inside the array to a subset (or subtype) of all values which can be represented by string scalars.

The basic numerical datatypes are not flexible (do not inherit from `np.flexible`). `float64`, `float32`, etc. do have a byte order, but the described values are unaffected by it, and it is always possible to cast them to the native, canonical representation without any loss of information.

The concept of flexibility can be generalized to parametric datatypes. For example the private `PyArray_AdaptFlexibleDType` function also accepts the naive datetime dtype as input to find the correct time unit. The datetime dtype is thus parametric not in the size of its storage, but instead in what the stored value represents. Currently `np.can_cast("datetime64[s]", "datetime64[ms]", casting="safe")` returns true, although it is unclear that this is desired or generalizes to possible future data types such as physical units.

Thus we have data types (mainly strings) with the properties that:

1.  Casting is not always safe (`np.can_cast("S8", "S4")`)
2.  Array coercion should be able to discover the exact dtype, such as for `np.array(["str1", 12.34], dtype="S")` where NumPy discovers the resulting dtype as `"S5"`. (If the dtype argument is omitted the behaviour is currently ill defined [\[gh-15327\]](#gh-15327).) A form similar to `dtype="S"` is `dtype="datetime64"` which can discover the unit: `np.array(["2017-02"], dtype="datetime64")`.

This notion highlights that some datatypes are more complex than the basic numerical ones, which is evident in the complicated output type discovery of universal functions.

### Value based casting

Casting is typically defined between two types: A type is considered to cast safely to a second type when the second type can represent all values of the first without loss of information. NumPy may inspect the actual value to decide whether casting is safe or not.

This is useful for example in expressions such as:

    arr = np.array([1, 2, 3], dtype="int8")
    result = arr + 5
    assert result.dtype == np.dtype("int8")
    # If the value is larger, the result will change however:
    result = arr + 500
    assert result.dtype == np.dtype("int16")

In this expression, the python value (which originally has no datatype) is represented as an `int8` or `int16` (the smallest possible data type).

NumPy currently does this even for NumPy scalars and zero-dimensional arrays, so that replacing `5` with `np.int64(5)` or `np.array(5, dtype="int64")` in the above expression will lead to the same results, and thus ignores the existing datatype. The same logic also applies to floating-point scalars, which are allowed to lose precision. The behavior is not used when both inputs are scalars, so that `5 + np.int8(5)` returns the default integer size (32 or 64-bit) and not an `np.int8`.

While the behaviour is defined in terms of casting and exposed by `np.result_type` it is mainly important for universal functions (such as `np.add` in the above examples). Universal functions currently rely on safe casting semantics to decide which loop should be used, and thus what the output datatype will be.

#### Issues and discussion

There appears to be some agreement that the current method is not desirable for values that have a datatype, but may be useful for pure python integers or floats as in the first example. However, any change of the datatype system and universal function dispatching must initially fully support the current behavior. A main difficulty is that for example the value `156` can be represented by `np.uint8` and `np.int16`. The result depends on the "minimal" representation in the context of the conversion (for ufuncs the context may depend on the loop order).

### The object datatype

The object datatype currently serves as a generic fallback for any value which is not otherwise representable. However, due to not having a well-defined type, it has some issues, for example when an array is filled with Python sequences:

    >>> l = [1, [2]]
    >>> np.array(l, dtype=np.object_)
    array([1, list([2])], dtype=object)  # a 1d array
    
    >>> a = np.empty((), dtype=np.object_)
    >>> a[...] = l
    ValueError: assignment to 0-d array  # ???
    >>> a[()] = l
    >>> a
    array(list([1, [2]]), dtype=object)

Without a well-defined type, functions such as `isnan()` or `conjugate()` do not necessarily work, but can work for a <span class="title-ref">decimal.Decimal</span>. To improve this situation it seems desirable to make it easy to create `object` dtypes that represent a specific Python datatype and stores its object inside the array in the form of pointer to python `PyObject`. Unlike most datatypes, Python objects require garbage collection. This means that additional methods to handle references and visit all objects must be defined. In practice, for most use-cases it is sufficient to limit the creation of such datatypes so that all functionality related to Python C-level references is private to NumPy.

Creating NumPy datatypes that match builtin Python objects also creates a few problems that require more thoughts and discussion. These issues do not need to solved right away:

  - NumPy currently returns *scalars* even for array input in some cases, in most cases this works seamlessly. However, this is only true because the NumPy scalars behave much like NumPy arrays, a feature that general Python objects do not have.
  - Seamless integration probably requires that `np.array(scalar)` finds the correct DType automatically since some operations (such as indexing) return the scalar instead of a 0D array. This is problematic if multiple users independently decide to implement for example a DType for `decimal.Decimal`.

### Current `dtype` implementation

Currently `np.dtype` is a Python class with its instances being the `np.dtype(">float64")`, etc. instances. To set the actual behaviour of these instances, a prototype instance is stored globally and looked up based on the `dtype.typenum`. The singleton is used where possible. Where required it is copied and modified, for instance to change endianness.

Parametric datatypes (strings, void, datetime, and timedelta) must store additional information such as string lengths, fields, or datetime units --new instances of these types are created instead of relying on a singleton. All current datatypes within NumPy further support setting a metadata field during creation which can be set to an arbitrary dictionary value, but seems rarely used in practice (one recent and prominent user is h5py).

Many datatype-specific functions are defined within a C structure called :c`PyArray_ArrFuncs`, which is part of each `dtype` instance and has a similarity to Python's `PyNumberMethods`. For user-defined datatypes this structure is exposed to the user, making ABI-compatible changes impossible. This structure holds important information such as how to copy or cast, and provides space for pointers to functions, such as comparing elements, converting to bool, or sorting. Since some of these functions are vectorized operations, operating on more than one element, they fit the model of ufuncs and do not need to be defined on the datatype in the future. For example the `np.clip` function was previously implemented using `PyArray_ArrFuncs` and is now implemented as a ufunc.

#### Discussion and issues

A further issue with the current implementation of the functions on the dtype is that, unlike methods, they are not passed an instance of the dtype when called. Instead, in many cases, the array which is being operated on is passed in and typically only used to extract the datatype again. A future API should likely stop passing in the full array object. Since it will be necessary to fall back to the old definitions for backward compatibility, the array object may not be available. However, passing a "fake" array in which mainly the datatype is defined is probably a sufficient workaround (see backward compatibility; alignment information may sometimes also be desired).

Although not extensively used outside of NumPy itself, the currently `PyArray_Descr` is a public structure. This is especially also true for the `PyArray_ArrFuncs` structure stored in the `f` field. Due to compatibility they may need to remain supported for a very long time, with the possibility of replacing them by functions that dispatch to a newer API.

However, in the long run access to these structures will probably have to be deprecated.

### NumPy scalars and type hierarchy

As a side note to the above datatype implementation: unlike the datatypes, the NumPy scalars currently **do** provide a type hierarchy, consisting of abstract types such as `np.inexact` (see figure below). In fact, some control flow within NumPy currently uses `issubclass(a.dtype.type, np.inexact)`.

<div id="nep-0040_dtype-hierarchy">

![**Figure:** Hierarchy of NumPy scalar types reproduced from the reference documentation. Some aliases such as `np.intp` are excluded. Datetime and timedelta are not shown.](nep-0040_dtype-hierarchy.png)

</div>

NumPy scalars try to mimic zero-dimensional arrays with a fixed datatype. For the numerical (and unicode) datatypes, they are further limited to native byte order.

### Current implementation of casting

One of the main features which datatypes need to support is casting between one another using `arr.astype(new_dtype, casting="unsafe")`, or during execution of ufuncs with different types (such as adding integer and floating point numbers).

Casting tables determine whether it is possible to cast from one specific type to another. However, generic casting rules cannot handle the parametric dtypes such as strings. The logic for parametric datatypes is defined mainly in `PyArray_CanCastTo` and currently cannot be customized for user defined datatypes.

The actual casting has two distinct parts:

1.  `copyswap`/`copyswapn` are defined for each dtype and can handle byte-swapping for non-native byte orders as well as unaligned memory.
2.  The generic casting code is provided by C functions which know how to cast aligned and contiguous memory from one dtype to another (both in native byte order). These C-level functions can be registered to cast aligned and contiguous memory from one dtype to another. The function may be provided with both arrays (although the parameter is sometimes `NULL` for scalars). NumPy will ensure that these functions receive native byte order input. The current implementation stores the functions either in a C-array on the datatype which is cast, or in a dictionary when casting to a user defined datatype.

Generally NumPy will thus perform casting as chain of the three functions `in_copyswapn -> castfunc -> out_copyswapn` using (small) buffers between these steps.

The above multiple functions are wrapped into a single function (with metadata) that handles the cast and is used for example during the buffered iteration used by ufuncs. This is the mechanism that is always used for user defined datatypes. For most dtypes defined within NumPy itself, more specialized code is used to find a function to do the actual cast (defined by the private `PyArray_GetDTypeTransferFunction`). This mechanism replaces most of the above mechanism and provides much faster casts for example when the inputs are not contiguous in memory. However, it cannot be extended by user defined datatypes.

Related to casting, we currently have a `PyArray_EquivTypes` function which indicate that a *view* is sufficient (and thus no cast is necessary). This function is used multiple places and should probably be part of a redesigned casting API.

### DType handling in universal functions

Universal functions are implemented as instances of the `numpy.UFunc` class with an ordered-list of datatype-specific (based on the dtype typecode character, not datatype instances) implementations, each with a signature and a function pointer. This list of implementations can be seen with `ufunc.types` where all implementations are listed with their C-style typecode signatures. For example:

    >>> np.add.types
    [...,
     'll->l',
     ...,
     'dd->d',
     ...]

Each of these signatures is associated with a single inner-loop function defined in C, which does the actual calculation, and may be called multiple times.

The main step in finding the correct inner-loop function is to call a :c`PyUFunc_TypeResolutionFunc` which retrieves the input dtypes from the provided input arrays and will determine the full type signature (including output dtype) to be executed.

By default the `TypeResolver` is implemented by searching all of the implementations listed in `ufunc.types` in order and stopping if all inputs can be safely cast to fit the signature. This means that if long (`l`) and double (`d`) arrays are added, numpy will find that the `'dd->d'` definition works (long can safely cast to double) and uses that.

In some cases this is not desirable. For example the `np.isnat` universal function has a `TypeResolver` which rejects integer inputs instead of allowing them to be cast to float. In principle, downstream projects can currently use their own non-default `TypeResolver`, since the corresponding C-structure necessary to do this is public. The only project known to do this is Astropy, which is willing to switch to a new API if NumPy were to remove the possibility to replace the TypeResolver.

For user defined datatypes, the dispatching logic is similar, although separately implemented and limited (see discussion below).

#### Issues and discussion

It is currently only possible for user defined functions to be found/resolved if any of the inputs (or the outputs) has the user datatype, since it uses the <span class="title-ref">OO-\>O</span> signature. For example, given that a ufunc loop to implement `fraction_divide(int, int) -> Fraction` has been implemented, the call `fraction_divide(4, 5)` (with no specific output dtype) will fail because the loop that includes the user datatype `Fraction` (as output) can only be found if any of the inputs is already a `Fraction`. `fraction_divide(4, 5, dtype=Fraction)` can be made to work, but is inconvenient.

Typically, dispatching is done by finding the first loop that matches. A match is defined as: all inputs (and possibly outputs) can be cast safely to the signature typechars (see also the current implementation section). However, in some cases safe casting is problematic and thus explicitly not allowed. For example the `np.isnat` function is currently only defined for datetime and timedelta, even though integers are defined to be safely castable to timedelta. If this was not the case, calling `np.isnat(np.array("NaT", "timedelta64").astype("int64"))` would currently return true, although the integer input array has no notion of "not a time". If a universal function, such as most functions in `scipy.special`, is only defined for `float32` and `float64` it will currently automatically cast a `float16` silently to `float32` (similarly for any integer input). This ensures successful execution, but may lead to a change in the output dtype when support for new data types is added to a ufunc. When a `float16` loop is added, the output datatype will currently change from `float32` to `float16` without a warning.

In general the order in which loops are registered is important. However, this is only reliable if all loops are added when the ufunc is first defined. Additional loops added when a new user datatypes is imported must not be sensitive to the order in which imports occur.

There are two main approaches to better define the type resolution for user defined types:

1.  Allow for user dtypes to directly influence the loop selection. For example they may provide a function which return/select a loop when there is no exact matching loop available.
2.  Define a total ordering of all implementations/loops, probably based on "safe casting" semantics, or semantics similar to that.

While option 2 may be less complex to reason about it remains to be seen whether it is sufficient for all (or most) use cases.

### Adjustment of parametric output DTypes in UFuncs

A second step necessary for parametric dtypes is currently performed within the `TypeResolver`: the datetime and timedelta datatypes have to decide on the correct parameter for the operation and output array. This step also needs to double check that all casts can be performed safely, which by default means that they are "same kind" casts.

#### Issues and discussion

Fixing the correct output dtype is currently part of the type resolution. However, it is a distinct step and should probably be handled as such after the actual type/loop resolution has occurred.

As such this step may move from the dispatching step (described above) to the implementation-specific code described below.

### DType-specific implementation of the UFunc

Once the correct implementation/loop is found, UFuncs currently call a single *inner-loop function* which is written in C. This may be called multiple times to do the full calculation and it has little or no information about the current context. It also has a void return value.

#### Issues and discussion

Parametric datatypes may require passing additional information to the inner-loop function to decide how to interpret the data. This is the reason why currently no universal functions for `string` dtypes exist (although technically possible within NumPy itself). Note that it is currently possible to pass in the input array objects (which in turn hold the datatypes when no casting is necessary). However, the full array information should not be required and currently the arrays are passed in before any casting occurs. The feature is unused within NumPy and no known user exists.

Another issue is the error reporting from within the inner-loop function. There exist currently two ways to do this:

1.  by setting a Python exception
2.  using the CPU floating point error flags.

Both of these are checked before returning to the user. However, many integer functions currently can set neither of these errors, so that checking the floating point error flags is unnecessary overhead. On the other hand, there is no way to stop the iteration or pass out error information which does not use the floating point flags or requires to hold the Python global interpreter lock (GIL).

It seems necessary to provide more control to authors of inner loop functions. This means allowing users to pass in and out information from the inner-loop function more easily, while *not* providing the input array objects. Most likely this will involve:

  - Allowing the execution of additional code before the first and after the last inner-loop call.
  - Returning an integer value from the inner-loop to allow stopping the iteration early and possibly propagate error information.
  - Possibly, to allow specialized inner-loop selections. For example currently `matmul` and many reductions will execute optimized code for certain inputs. It may make sense to allow selecting such optimized loops beforehand. Allowing this may also help to bring casting (which uses this heavily) and ufunc implementations closer.

The issues surrounding the inner-loop functions have been discussed in some detail in the github issue [gh-12518](https://github.com/numpy/numpy/issues/12518) .

Reductions use an "identity" value. This is currently defined once per ufunc, regardless of the ufunc dtype signature. For example `0` is used for `sum`, or `math.inf` for `min`. This works well for numerical datatypes, but is not always appropriate for other dtypes. In general it should be possible to provide a dtype-specific identity to the ufunc reduction.

### Datatype discovery during array coercion

When calling `np.array(...)` to coerce a general Python object to a NumPy array, all objects need to be inspected to find the correct dtype. The input to `np.array()` are potentially nested Python sequences which hold the final elements as generic Python objects. NumPy has to unpack all the nested sequences and then inspect the elements. The final datatype is found by iterating over all elements which will end up in the array and:

1.  discovering the dtype of the single element:
      - from array (or array like) or NumPy scalar using `element.dtype`
      - using `isinstance(..., float)` for known Python types (note that these rules mean that subclasses are *currently* valid).
      - special rule for void datatypes to coerce tuples.
2.  Promoting the current dtype with the next elements dtype using `np.promote_types`.
3.  If strings are found, the whole process is restarted (see also [\[gh-15327\]](#gh-15327)), in a similar manner as if `dtype="S"` was given (see below).

If `dtype=...` is given, this dtype is used unmodified, unless it is an unspecific *parametric dtype instance* which means "S0", "V0", "U0", "datetime64", and "timdelta64". These are thus flexible datatypes without length 0 â€“ considered to be unsized â€“ and datetimes or timedelta without a unit attached ("generic unit").

In future DType class hierarchy, these may be represented by the class rather than a special instance, since these special instances should not normally be attached to an array.

If such a *parametric dtype instance* is provided for example using `dtype="S"` `PyArray_AdaptFlexibleDType` is called and effectively inspects all values using DType specific logic. That is:

  - Strings will use `str(element)` to find the length of most elements
  - Datetime64 is capable of coercing from strings and guessing the correct unit.

#### Discussion and issues

It seems probable that during normal discovery, the `isinstance` should rather be strict `type(element) is desired_type` checks. Further, the current `AdaptFlexibleDType` logic should be made available to user DTypes and not be a secondary step, but instead replace, or be part of, the normal discovery.

## Related issues

`np.save` currently translates all user-defined dtypes to void dtypes. This means they cannot be stored using the `npy` format. This is not an issue for the python pickle protocol, although it may require some thought if we wish to ensure that such files can be loaded securely without the possibility of executing malicious code (i.e. without the `allow_pickle=True` keyword argument).

The additional existence of masked arrays and especially masked datatypes within Pandas has interesting implications for interoperability. Since mask information is often stored separately, its handling requires support by the container (array) object. NumPy itself does not provide such support, and is not expected to add it in the foreseeable future. However, if such additions to the datatypes within NumPy would improve interoperability they could be considered even if they are not used by NumPy itself.

## Related work

  - Julia types are an interesting blueprint for a type hierarchy, and define abstract and concrete types [\[julia-types\]](#julia-types).
  - In Julia promotion can occur based on abstract types. If a promoter is defined, it will cast the inputs and then Julia can then retry to find an implementation with the new values [\[julia-promotion\]](#julia-promotion).
  - `xnd-project` (<https://github.com/xnd-project>) with ndtypes and gumath
      - The `xnd-project` is similar to NumPy and defines data types as well as the possibility to extend them. A major difference is that it does not use promotion/casting within the ufuncs, but instead requires explicit definition of `int32 + float64 -> float64` loops.

## Discussion

There have been many discussions about the current state and what a future datatype system may look like. The full list of these discussion is long and some are lost to time, the following provides a subset for more recent ones:

  - Draft NEP by Stephan Hoyer after a developer meeting (was updated on the next developer meeting) <https://hackmd.io/6YmDt_PgSVORRNRxHyPaNQ>
  - List of related documents gathered previously here <https://hackmd.io/UVOtgj1wRZSsoNQCjkhq1g> (TODO: Reduce to the most important ones):
      - <https://github.com/numpy/numpy/pull/12630> Matti Picus draft NEP, discusses the technical side of subclassing more from the side of `ArrFunctions`
    
      - <https://hackmd.io/ok21UoAQQmOtSVk6keaJhw> and <https://hackmd.io/s/ryTFaOPHE> (2019-04-30) Proposals for subclassing implementation approach.
    
      - Discussion about the calling convention of ufuncs and need for more powerful UFuncs: <https://github.com/numpy/numpy/issues/12518>
    
      - 2018-11-30 developer meeting notes: <https://github.com/BIDS-numpy/docs/blob/master/meetings/2018-11-30-dev-meeting.md> and subsequent draft for an NEP: <https://hackmd.io/6YmDt_PgSVORRNRxHyPaNQ>
        
        BIDS Meeting on November 30, 2018 and document by Stephan Hoyer about what numpy should provide and thoughts of how to get there. Meeting with Eric Wieser, Matti Picus, Charles Harris, Tyler Reddy, StÃ©fan van der Walt, and Travis Oliphant.
    
      - SciPy 2018 brainstorming session with summaries of use cases: <https://github.com/numpy/numpy/wiki/Dtype-Brainstorming>
        
        Also lists some requirements and some ideas on implementations

## References

## Copyright

This document has been placed in the public domain.

<div id="citations">

  - <span id="gh-15327" class="citation-label">gh-15327</span>  
    <https://github.com/numpy/numpy/issues/12518>

  - <span id="julia-promotion" class="citation-label">julia-promotion</span>  
    <https://docs.julialang.org/en/v1/manual/conversion-and-promotion/>

  - <span id="julia-types" class="citation-label">julia-types</span>  
    <https://docs.julialang.org/en/v1/manual/types/index.html#Abstract-Types-1>

</div>

---

nep-0041-improved-dtype-support.md

---

# NEP 41 â€” First step towards a new datatype system

  - title  
    First step towards a new Datatype System

  - Author  
    Sebastian Berg

  - Author  
    StÃ©fan van der Walt

  - Author  
    Matti Picus

  - Status  
    Accepted

  - Type  
    Standard Track

  - Created  
    2020-02-03

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2020-April/080573.html> and <https://mail.python.org/pipermail/numpy-discussion/2020-March/080495.html>

\> **Note** \> This NEP is second in a series:

>   - \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>) explains the shortcomings of NumPy's dtype implementation.
>   - NEP 41 (this document) gives an overview of our proposed replacement.
>   - \[NEP 42 \<NEP42\>\](\#nep-42-\<nep42\>) describes the new design's datatype-related APIs.
>   - \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) describes the new design's API for universal functions.

## Abstract

\[Datatypes \<arrays.dtypes\>\](\#datatypes-\<arrays.dtypes\>) in NumPy describe how to interpret each element in arrays. NumPy provides `int`, `float`, and `complex` numerical types, as well as string, datetime, and structured datatype capabilities. The growing Python community, however, has need for more diverse datatypes. Examples are datatypes with unit information attached (such as meters) or categorical datatypes (fixed set of possible values). However, the current NumPy datatype API is too limited to allow the creation of these.

This NEP is the first step to enable such growth; it will lead to a simpler development path for new datatypes. In the long run the new datatype system will also support the creation of datatypes directly from Python rather than C. Refactoring the datatype API will improve maintainability and facilitate development of both user-defined external datatypes, as well as new features for existing datatypes internal to NumPy.

## Motivation and scope

<div class="seealso">

The user impact section includes examples of what kind of new datatypes will be enabled by the proposed changes in the long run. It may thus help to read these section out of order.

</div>

### Motivation

One of the main issues with the current API is the definition of typical functions such as addition and multiplication for parametric datatypes (see also \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>)) which require additional steps to determine the output type. For example when adding two strings of length 4, the result is a string of length 8, which is different from the input. Similarly, a datatype which embeds a physical unit must calculate the new unit information: dividing a distance by a time results in a speed. A related difficulty is that the \[current casting rules \<ufuncs.casting\>\](\#current-casting-rules-\<ufuncs.casting\>) -- the conversion between different datatypes --cannot describe casting for such parametric datatypes implemented outside of NumPy.

This additional functionality for supporting parametric datatypes introduces increased complexity within NumPy itself, and furthermore is not available to external user-defined datatypes. In general the concerns of different datatypes are not well-encapsulated. This burden is exacerbated by the exposure of internal C structures, limiting the addition of new fields (for example to support new sorting methods [\[new\_sort\]](#new_sort)).

Currently there are many factors which limit the creation of new user-defined datatypes:

  - Creating casting rules for parametric user-defined dtypes is either impossible or so complex that it has never been attempted.
  - Type promotion, e.g. the operation deciding that adding float and integer values should return a float value, is very valuable for numeric datatypes but is limited in scope for user-defined and especially parametric datatypes.
  - Much of the logic (e.g. promotion) is written in single functions instead of being split as methods on the datatype itself.
  - In the current design datatypes cannot have methods that do not generalize to other datatypes. For example a unit datatype cannot have a `.to_si()` method to easily find the datatype which would represent the same values in SI units.

The large need to solve these issues has driven the scientific community to create work-arounds in multiple projects implementing physical units as an array-like class instead of a datatype, which would generalize better across multiple array-likes (Dask, pandas, etc.). Already, Pandas has made a push into the same direction with its extension arrays [\[pandas\_extension\_arrays\]](#pandas_extension_arrays) and undoubtedly the community would be best served if such new features could be common between NumPy, Pandas, and other projects.

### Scope

The proposed refactoring of the datatype system is a large undertaking and thus is proposed to be split into various phases, roughly:

  - Phase I: Restructure and extend the datatype infrastructure (This NEP 41)
  - Phase II: Incrementally define or rework API (Detailed largely in NEPs 42/43)
  - Phase III: Growth of NumPy and Scientific Python Ecosystem capabilities.

For a more detailed accounting of the various phases, see "Plan to Approach the Full Refactor" in the Implementation section below. This NEP proposes to move ahead with the necessary creation of new dtype subclasses (Phase I), and start working on implementing current functionality. Within the context of this NEP all development will be fully private API or use preliminary underscored names which must be changed in the future. Most of the internal and public API choices are part of a second Phase and will be discussed in more detail in the following NEPs 42 and 43. The initial implementation of this NEP will have little or no effect on users, but provides the necessary ground work for incrementally addressing the full rework.

The implementation of this NEP and the following, implied large rework of how datatypes are defined in NumPy is expected to create small incompatibilities (see backward compatibility section). However, a transition requiring large code adaption is not anticipated and not within scope.

Specifically, this NEP makes the following design choices which are discussed in more details in the detailed description section:

1.  Each datatype will be an instance of a subclass of `np.dtype`, with most of the datatype-specific logic being implemented as special methods on the class. In the C-API, these correspond to specific slots. In short, for `f = np.dtype("f8")`, `isinstance(f, np.dtype)` will remain true, but `type(f)` will be a subclass of `np.dtype` rather than just `np.dtype` itself. The `PyArray_ArrFuncs` which are currently stored as a pointer on the instance (as `PyArray_Descr->f`), should instead be stored on the class as typically done in Python. In the future these may correspond to python side dunder methods. Storage information such as itemsize and byteorder can differ between different dtype instances (e.g. "S3" vs. "S8") and will remain part of the instance. This means that in the long run the current lowlevel access to dtype methods will be removed (see `PyArray_ArrFuncs` in \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>)).
2.  The current NumPy scalars will *not* change, they will not be instances of datatypes. This will also be true for new datatypes, scalars will not be instances of a dtype (although `isinstance(scalar, dtype)` may be made to return `True` when appropriate).

Detailed technical decisions to follow in NEP 42.

Further, the public API will be designed in a way that is extensible in the future:

3.  All new C-API functions provided to the user will hide implementation details as much as possible. The public API should be an identical, but limited, version of the C-API used for the internal NumPy datatypes.

The datatype system may be targeted to work with NumPy arrays, for example by providing strided-loops, but should avoid direct interactions with the array-object (typically <span class="title-ref">np.ndarray</span> instances). Instead, the design principle will be that the array-object is a consumer of the datatype. While only a guiding principle, this may allow splitting the datatype system or even the NumPy datatypes into their own project which NumPy depends on.

The changes to the datatype system in Phase II must include a large refactor of the UFunc machinery, which will be further defined in NEP 43:

4.  To enable all of the desired functionality for new user-defined datatypes, the UFunc machinery will be changed to replace the current dispatching and type resolution system. The old system should be *mostly* supported as a legacy version for some time.

Additionally, as a general design principle, the addition of new user-defined datatypes will *not* change the behaviour of programs. For example `common_dtype(a, b)` must not be `c` unless `a` or `b` know that `c` exists.

## User impact

The current ecosystem has very few user-defined datatypes using NumPy, the two most prominent being: `rational` and `quaternion`. These represent fairly simple datatypes which are not strongly impacted by the current limitations. However, we have identified a need for datatypes such as:

  - bfloat16, used in deep learning
  - categorical types
  - physical units (such as meters)
  - datatypes for tracing/automatic differentiation
  - high, fixed precision math
  - specialized integer types such as int2, int24
  - new, better datetime representations
  - extending e.g. integer dtypes to have a sentinel NA value
  - geometrical objects [\[pygeos\]](#pygeos)

Some of these are partially solved; for example unit capability is provided in `astropy.units`, `unyt`, or `pint`, as <span class="title-ref">numpy.ndarray</span> subclasses. Most of these datatypes, however, simply cannot be reasonably defined right now. An advantage of having such datatypes in NumPy is that they should integrate seamlessly with other array or array-like packages such as Pandas, `xarray` [\[xarray\_dtype\_issue\]](#xarray_dtype_issue), or `Dask`.

The long term user impact of implementing this NEP will be to allow both the growth of the whole ecosystem by having such new datatypes, as well as consolidating implementation of such datatypes within NumPy to achieve better interoperability.

### Examples

The following examples represent future user-defined datatypes we wish to enable. These datatypes are not part the NEP and choices (e.g. choice of casting rules) are possibilities we wish to enable and do not represent recommendations.

#### Simple numerical types

Mainly used where memory is a consideration, lower-precision numeric types such as [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) are common in other computational frameworks. For these types the definitions of things such as `np.common_type` and `np.can_cast` are some of the most important interfaces. Once they support `np.common_type`, it is (for the most part) possible to find the correct ufunc loop to call, since most ufuncs -- such as add -- effectively only require `np.result_type`:

    >>> np.add(arr1, arr2).dtype == np.result_type(arr1, arr2)

and <span class="title-ref">\~numpy.result\_type</span> is largely identical to <span class="title-ref">\~numpy.common\_type</span>.

#### Fixed, high precision math

Allowing arbitrary precision or higher precision math is important in simulations. For instance `mpmath` defines a precision:

    >>> import mpmath as mp
    >>> print(mp.dps)  # the current (default) precision
    15

NumPy should be able to construct a native, memory-efficient array from a list of `mpmath.mpf` floating point objects:

    >>> arr_15_dps = np.array(mp.arange(3))  # (mp.arange returns a list)
    >>> print(arr_15_dps)  # Must find the correct precision from the objects:
    array(['0.0', '1.0', '2.0'], dtype=mpf[dps=15])

We should also be able to specify the desired precision when creating the datatype for an array. Here, we use `np.dtype[mp.mpf]` to find the DType class (the notation is not part of this NEP), which is then instantiated with the desired parameter. This could also be written as `MpfDType` class:

    >>> arr_100_dps = np.array([1, 2, 3], dtype=np.dtype[mp.mpf](dps=100))
    >>> print(arr_15_dps + arr_100_dps)
    array(['0.0', '2.0', '4.0'], dtype=mpf[dps=100])

The `mpf` datatype can decide that the result of the operation should be the higher precision one of the two, so uses a precision of 100. Furthermore, we should be able to define casting, for example as in:

    >>> np.can_cast(arr_15_dps.dtype, arr_100_dps.dtype, casting="safe")
    True
    >>> np.can_cast(arr_100_dps.dtype, arr_15_dps.dtype, casting="safe")
    False  # loses precision
    >>> np.can_cast(arr_100_dps.dtype, arr_100_dps.dtype, casting="same_kind")
    True

Casting from float is a probably always at least a `same_kind` cast, but in general, it is not safe:

    >>> np.can_cast(np.float64, np.dtype[mp.mpf](dps=4), casting="safe")
    False

since a float64 has a higher precision than the `mpf` datatype with `dps=4`.

Alternatively, we can say that:

    >>> np.common_type(np.dtype[mp.mpf](dps=5), np.dtype[mp.mpf](dps=10))
    np.dtype[mp.mpf](dps=10)

And possibly even:

    >>> np.common_type(np.dtype[mp.mpf](dps=5), np.float64)
    np.dtype[mp.mpf](dps=16)  # equivalent precision to float64 (I believe)

since `np.float64` can be cast to a `np.dtype[mp.mpf](dps=16)` safely.

#### Categoricals

Categoricals are interesting in that they can have fixed, predefined values, or can be dynamic with the ability to modify categories when necessary. The fixed categories (defined ahead of time) is the most straight forward categorical definition. Categoricals are *hard*, since there are many strategies to implement them, suggesting NumPy should only provide the scaffolding for user-defined categorical types. For instance:

    >>> cat = Categorical(["eggs", "spam", "toast"])
    >>> breakfast = array(["eggs", "spam", "eggs", "toast"], dtype=cat)

could store the array very efficiently, since it knows that there are only 3 categories. Since a categorical in this sense knows almost nothing about the data stored in it, few operations makes, sense, although equality does:

> \>\>\> breakfast2 = array(\["eggs", "eggs", "eggs", "eggs"\], dtype=cat) \>\>\> breakfast == breakfast2 array\[True, False, True, False\])

The categorical datatype could work like a dictionary: no two items names can be equal (checked on dtype creation), so that the equality operation above can be performed very efficiently. If the values define an order, the category labels (internally integers) could be ordered the same way to allow efficient sorting and comparison.

Whether or not casting is defined from one categorical with less to one with strictly more values defined, is something that the Categorical datatype would need to decide. Both options should be available.

#### Unit on the datatype

There are different ways to define Units, depending on how the internal machinery would be organized, one way is to have a single Unit datatype for every existing numerical type. This will be written as `Unit[float64]`, the unit itself is part of the DType instance `Unit[float64]("m")` is a `float64` with meters attached:

    >>> from astropy import units
    >>> meters = np.array([1, 2, 3], dtype=np.float64) * units.m  # meters
    >>> print(meters)
    array([1.0, 2.0, 3.0], dtype=Unit[float64]("m"))

Note that units are a bit tricky. It is debatable, whether:

    >>> np.array([1.0, 2.0, 3.0], dtype=Unit[float64]("m"))

should be valid syntax (coercing the float scalars without a unit to meters). Once the array is created, math will work without any issue:

    >>> meters / (2 * unit.seconds)
    array([0.5, 1.0, 1.5], dtype=Unit[float64]("m/s"))

Casting is not valid from one unit to the other, but can be valid between different scales of the same dimensionality (although this may be "unsafe"):

    >>> meters.astype(Unit[float64]("s"))
    TypeError: Cannot cast meters to seconds.
    >>> meters.astype(Unit[float64]("km"))
    >>> # Convert to centimeter-gram-second (cgs) units:
    >>> meters.astype(meters.dtype.to_cgs())

The above notation is somewhat clumsy. Functions could be used instead to convert between units. There may be ways to make these more convenient, but those must be left for future discussions:

    >>> units.convert(meters, "km")
    >>> units.to_cgs(meters)

There are some open questions. For example, whether additional methods on the array object could exist to simplify some of the notions, and how these would percolate from the datatype to the `ndarray`.

The interaction with other scalars would likely be defined through:

    >>> np.common_type(np.float64, Unit)
    Unit[np.float64](dimensionless)

Ufunc output datatype determination can be more involved than for simple numerical dtypes since there is no "universal" output type:

    >>> np.multiply(meters, seconds).dtype != np.result_type(meters, seconds)

In fact `np.result_type(meters, seconds)` must error without context of the operation being done. This example highlights how the specific ufunc loop (loop with known, specific DTypes as inputs), has to be able to make certain decisions before the actual calculation can start.

## Implementation

### Plan to approach the full refactor

To address these issues in NumPy and enable new datatypes, multiple development stages are required:

  - Phase I: Restructure and extend the datatype infrastructure (This NEP)
      - Organize Datatypes like normal Python classes \[<span class="title-ref">PR 15508</span>\]\_
  - Phase II: Incrementally define or rework API
      - Incrementally define all necessary functionality through methods and properties on the DType (NEP 42):
          - The properties of the class hierarchy and DType class itself, including methods not covered by the following, most central, points.
          - The functionality that will support dtype casting using `arr.astype()` and casting related operations such as `np.common_type`.
          - The implementation of item access and storage, and the way shape and dtype are determined when creating an array with `np.array()`
          - Create a public C-API to define new DTypes.
      - Restructure how universal functions work (NEP 43), to allow extending a <span class="title-ref">\~numpy.ufunc</span> such as `np.add` for user-defined datatypes such as Units:
          - Refactor how the low-level C functions are organized to make it extensible and flexible enough for complicated DTypes such as Units.
          - Implement registration and efficient lookup for these low-level C functions as defined by the user.
          - Define how promotion will be used to implement behaviour when casting is required. For example `np.float64(3) + np.int32(3)` promotes the `int32` to a `float64`.
  - Phase III: Growth of NumPy and Scientific Python Ecosystem capabilities:
      - Cleanup of legacy behaviour where it is considered buggy or undesirable.
      - Provide a path to define new datatypes from Python.
      - Assist the community in creating types such as Units or Categoricals
      - Allow strings to be used in functions such as `np.equal` or `np.add`.
      - Remove legacy code paths within NumPy to improve long term maintainability

This document serves as a basis for phase I and provides the vision and motivation for the full project. Phase I does not introduce any new user-facing features, but is concerned with the necessary conceptual cleanup of the current datatype system. It provides a more "pythonic" datatype Python type object, with a clear class hierarchy.

The second phase is the incremental creation of all APIs necessary to define fully featured datatypes and reorganization of the NumPy datatype system. This phase will thus be primarily concerned with defining an, initially preliminary, stable public API.

Some of the benefits of a large refactor may only become evident after the full deprecation of the current legacy implementation (i.e. larger code removals). However, these steps are necessary for improvements to many parts of the core NumPy API, and are expected to make the implementation generally easier to understand.

The following figure illustrates the proposed design at a high level, and roughly delineates the components of the overall design. Note that this NEP only regards Phase I (shaded area), the rest encompasses Phase II and the design choices are up for discussion, however, it highlights that the DType datatype class is the central, necessary concept:

![image](nep-0041-mindmap.svg)

### First steps directly related to this NEP

The required changes necessary to NumPy are large and touch many areas of the code base but many of these changes can be addressed incrementally.

To enable an incremental approach we will start by creating a C defined `PyArray_DTypeMeta` class with its instances being the `DType` classes, subclasses of `np.dtype`. This is necessary to add the ability of storing custom slots on the DType in C. This `DTypeMeta` will be implemented first to then enable incremental restructuring of current code.

The addition of `DType` will then enable addressing other changes incrementally, some of which may begin before the settling the full internal API:

1.  New machinery for array coercion, with the goal of enabling user DTypes with appropriate class methods.
2.  The replacement or wrapping of the current casting machinery.
3.  Incremental redefinition of the current `PyArray_ArrFuncs` slots into DType method slots.

At this point, no or only very limited new public API will be added and the internal API is considered to be in flux. Any new public API may be set up give warnings and will have leading underscores to indicate that it is not finalized and can be changed without warning.

## Backward compatibility

While the actual backward compatibility impact of implementing Phase I and II are not yet fully clear, we anticipate, and accept the following changes:

  - **Python API**:
      - `type(np.dtype("f8"))` will be a subclass of `np.dtype`, while right now `type(np.dtype("f8")) is np.dtype`. Code should use `isinstance` checks, and in very rare cases may have to be adapted to use it.
  - **C-API**:
      - In old versions of NumPy `PyArray_DescrCheck` is a macro which uses `type(dtype) is np.dtype`. When compiling against an old NumPy version, the macro may have to be replaced with the corresponding `PyObject_IsInstance` call. (If this is a problem, we could backport fixing the macro)
      - The UFunc machinery changes will break *limited* parts of the current implementation. Replacing e.g. the default `TypeResolver` is expected to remain supported for a time, although optimized masked inner loop iteration (which is not even used *within* NumPy) will no longer be supported.
      - All functions currently defined on the dtypes, such as `PyArray_Descr->f->nonzero`, will be defined and accessed differently. This means that in the long run lowlevel access code will have to be changed to use the new API. Such changes are expected to be necessary in very few project.
  - **dtype implementers (C-API)**:
      - The array which is currently provided to some functions (such as cast functions), will no longer be provided. For example `PyArray_Descr->f->nonzero` or `PyArray_Descr->f->copyswapn`, may instead receive a dummy array object with only some fields (mainly the dtype), being valid. At least in some code paths, a similar mechanism is already used.
      - The `scalarkind` slot and registration of scalar casting will be removed/ignored without replacement. It currently allows partial value-based casting. The `PyArray_ScalarKind` function will continue to work for builtin types, but will not be used internally and be deprecated.
      - Currently user dtypes are defined as instances of `np.dtype`. The creation works by the user providing a prototype instance. NumPy will need to modify at least the type during registration. This has no effect for either `rational` or `quaternion` and mutation of the structure seems unlikely after registration.

Since there is a fairly large API surface concerning datatypes, further changes or the limitation certain function to currently existing datatypes is likely to occur. For example functions which use the type number as input should be replaced with functions taking DType classes instead. Although public, large parts of this C-API seem to be used rarely, possibly never, by downstream projects.

## Detailed description

This section details the design decisions covered by this NEP. The subsections correspond to the list of design choices presented in the Scope section.

### Datatypes as Python classes (1)

The current NumPy datatypes are not full scale python classes. They are instead (prototype) instances of a single `np.dtype` class. Changing this means that any special handling, e.g. for `datetime` can be moved to the Datetime DType class instead, away from monolithic general code (e.g. current `PyArray_AdjustFlexibleDType`).

The main consequence of this change with respect to the API is that special methods move from the dtype instances to methods on the new DType class. This is the typical design pattern used in Python. Organizing these methods and information in a more Pythonic way provides a solid foundation for refining and extending the API in the future. The current API cannot be extended due to how it is exposed publicly. This means for example that the methods currently stored in `PyArray_ArrFuncs` on each datatype (see \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>)) will be defined differently in the future and deprecated in the long run.

The most prominent visible side effect of this will be that `type(np.dtype(np.float64))` will not be `np.dtype` anymore. Instead it will be a subclass of `np.dtype` meaning that `isinstance(np.dtype(np.float64), np.dtype)` will remain true. This will also add the ability to use `isinstance(dtype, np.dtype[float64])` thus removing the need to use `dtype.kind`, `dtype.char`, or `dtype.type` to do this check.

With the design decision of DTypes as full-scale Python classes, the question of subclassing arises. Inheritance, however, appears problematic and a complexity best avoided (at least initially) for container datatypes. Further, subclasses may be more interesting for interoperability for example with GPU backends (CuPy) storing additional methods related to the GPU rather than as a mechanism to define new datatypes. A class hierarchy does provides value, and one can be achieved by allowing the creation of *abstract* datatypes. An example for an abstract datatype would be the datatype equivalent of `np.floating`, representing any floating point number. These can serve the same purpose as Python's abstract base classes.

This NEP chooses to duplicate the scalar hierarchy fully or in part. The main reason is to uncouple the implementation of the DType and scalar. To add a DType to NumPy, in theory the scalar will not need to be modified or know about NumPy. Also note that the categorical DType as currently implemented in pandas does not have a scalar correspondence making it less straight forward to rely on scalars to implement behaviour. While DType and Scalar describe the same concept/type (e.g. an <span class="title-ref">int64</span>), it seems practical to split out the information and functionality necessary for numpy into the DType class.

#### The dtype instances provide parameters and storage options

From a computer science point of view a type defines the *value space* (all possible values its instances can take) and their *behaviour*. As proposed in this NEP, the DType class defines value space and behaviour. The `dtype` instance can be seen as part of the value, so that the typical Python `instance` corresponds to `dtype + element` (where *element* is the data stored in the array). An alternative view would be to define value space and behaviour on the `dtype` instances directly. These two options are presented in the following figure and compared to similar Python implementation patterns:

![image](nep-0041-type-sketch-no-fonts.svg)

The difference is in how parameters, such as string length or the datetime units (`ms`, `ns`, ...), and storage options, such as byte-order, are handled. When implementing a Python (scalar) `type` parameters, for example the datetimes unit, will be stored in the instance. This is the design NEP 42 tries to mimic, however, the parameters are now part of the dtype instance, meaning that part of the data stored in the instance is shared by all array elements. As mentioned previously, this means that the Python `instance` corresponds to the `dtype + element` stored in a NumPy array.

An more advanced approach in Python is to use a class factory and an abstract base class (ABC). This allows moving the parameter into the dynamically created `type` and behaviour implementation may be specific to those parameters. An alternative approach might use this model and implemented behaviour directly on the `dtype` instance.

We believe that the version as proposed here is easier to work with and understand. Python class factories are not commonly used and NumPy does not use code specialized for dtype parameters or byte-orders. Making such specialization easier to implement such specialization does not seem to be a priority. One result of this choice is that some DTypes may only have a singleton instance if they have no parameters or storage variation. However, all of the NumPy dtypes require dynamically created instances due to allowing metadata to be attached.

### Scalars should not be instances of the datatypes (2)

For simple datatypes such as `float64` (see also below), it seems tempting that the instance of a `np.dtype("float64")` can be the scalar. This idea may be even more appealing due to the fact that scalars, rather than datatypes, currently define a useful type hierarchy.

However, we have specifically decided against this for a number of reasons. First, the new datatypes described herein would be instances of DType classes. Making these instances themselves classes, while possible, adds additional complexity that users need to understand. It would also mean that scalars must have storage information (such as byteorder) which is generally unnecessary and currently is not used. Second, while the simple NumPy scalars such as `float64` may be such instances, it should be possible to create datatypes for Python objects without enforcing NumPy as a dependency. However, Python objects that do not depend on NumPy cannot be instances of a NumPy DType. Third, there is a mismatch between the methods and attributes which are useful for scalars and datatypes. For instance `to_float()` makes sense for a scalar but not for a datatype and `newbyteorder` is not useful on a scalar (or has a different meaning).

Overall, it seem rather than reducing the complexity, i.e. by merging the two distinct type hierarchies, making scalars instances of DTypes would increase the complexity of both the design and implementation.

A possible future path may be to instead simplify the current NumPy scalars to be much simpler objects which largely derive their behaviour from the datatypes.

### C-API for creating new datatypes (3)

The current C-API with which users can create new datatypes is limited in scope, and requires use of "private" structures. This means the API is not extensible: no new members can be added to the structure without losing binary compatibility. This has already limited the inclusion of new sorting methods into NumPy [\[new\_sort\]](#new_sort).

The new version shall thus replace the current `PyArray_ArrFuncs` structure used to define new datatypes. Datatypes that currently exist and are defined using these slots will be supported during a deprecation period.

The most likely solution is to hide the implementation from the user and thus make it extensible in the future is to model the API after Python's stable API [\[PEP-384\]](#PEP-384):

`` `C     static struct PyArrayMethodDef slots[] = {         {NPY_dt_method, method_implementation},         ...,         {0, NULL}     }      typedef struct{       PyTypeObject *typeobj;  /* type of python scalar */       ...;       PyType_Slot *slots;     } PyArrayDTypeMeta_Spec;      PyObject* PyArray_InitDTypeMetaFromSpec(             PyArray_DTypeMeta *user_dtype, PyArrayDTypeMeta_Spec *dtype_spec);  The C-side slots should be designed to mirror Python side methods ``<span class="title-ref"> such as </span><span class="title-ref">dtype.\_\_dtype\_method\_\_</span>\`, although the exposure to Python is a later step in the implementation to reduce the complexity of the initial implementation.

### C-API changes to the UFunc machinery (4)

Proposed changes to the UFunc machinery will be part of NEP 43. However, the following changes will be necessary (see \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>) for a detailed description of the current implementation and its issues):

  - The current UFunc type resolution must be adapted to allow better control for user-defined dtypes as well as resolve current inconsistencies.
  - The inner-loop used in UFuncs must be expanded to include a return value. Further, error reporting must be improved, and passing in dtype-specific information enabled. This requires the modification of the inner-loop function signature and addition of new hooks called before and after the inner-loop is used.

An important goal for any changes to the universal functions will be to allow the reuse of existing loops. It should be easy for a new units datatype to fall back to existing math functions after handling the unit related computations.

## Discussion

See \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>) for a list of previous meetings and discussions.

Additional discussion around this specific NEP has occurred on both the mailing list and the pull request:

  - [Mailing list discussion](https://mail.python.org/pipermail/numpy-discussion/2020-March/080481.html)
  - [NEP 41 pull request](https://github.com/numpy/numpy/pull/15506)
  - [Pull request thread on Dtype hierarchy and Scalars](https://github.com/numpy/numpy/pull/15506#discussion_r390016298)

## References

## Copyright

This document has been placed in the public domain.

## Acknowledgments

The effort to create new datatypes for NumPy has been discussed for several years in many different contexts and settings, making it impossible to list everyone involved. We would like to thank especially Stephan Hoyer, Nathaniel Smith, and Eric Wieser for repeated in-depth discussion about datatype design. We are very grateful for the community input in reviewing and revising this NEP and would like to thank especially Ross Barnowski and Ralf Gommers.

<div id="citations">

  - <span id="PEP-384" class="citation-label">PEP-384</span>  
    <https://www.python.org/dev/peps/pep-0384/>

  - <span id="new_sort" class="citation-label">new\_sort</span>  
    <https://github.com/numpy/numpy/pull/12945>

  - <span id="pandas_extension_arrays" class="citation-label">pandas\_extension\_arrays</span>  
    <https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extension-types>

  - <span id="pygeos" class="citation-label">pygeos</span>  
    <https://github.com/caspervdw/pygeos>

  - <span id="xarray_dtype_issue" class="citation-label">xarray\_dtype\_issue</span>  
    <https://github.com/pydata/xarray/issues/1262>

</div>

---

nep-0042-new-dtypes.md

---

# NEP 42 â€” New and extensible DTypes

  - title  
    New and extensible DTypes

  - Author  
    Sebastian Berg

  - Author  
    Ben Nathanson

  - Author  
    Marten van Kerkwijk

  - Status  
    Accepted

  - Type  
    Standard

  - Created  
    2019-07-17

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2020-October/081038.html>

\> **Note** \> This NEP is third in a series:

>   - \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>) explains the shortcomings of NumPy's dtype implementation.
>   - \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) gives an overview of our proposed replacement.
>   - NEP 42 (this document) describes the new design's datatype-related APIs.
>   - \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) describes the new design's API for universal functions.

## Abstract

NumPy's dtype architecture is monolithic -- each dtype is an instance of a single class. There's no principled way to expand it for new dtypes, and the code is difficult to read and maintain.

As \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) explains, we are proposing a new architecture that is modular and open to user additions. dtypes will derive from a new `DType` class serving as the extension point for new types. `np.dtype("float64")` will return an instance of a `Float64` class, a subclass of root class `np.dtype`.

This NEP is one of two that lay out the design and API of this new architecture. This NEP addresses dtype implementation; \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) addresses universal functions.

\> **Note** \> Details of the private and external APIs may change to reflect user comments and implementation constraints. The underlying principles and choices should not change significantly.

## Motivation and scope

Our goal is to allow user code to create fully featured dtypes for a broad variety of uses, from physical units (such as meters) to domain-specific representations of geometric objects. \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) describes a number of these new dtypes and their benefits.

Any design supporting dtypes must consider:

  - How shape and dtype are determined when an array is created
  - How array elements are stored and accessed
  - The rules for casting dtypes to other dtypes

In addition:

  - We want dtypes to comprise a class hierarchy open to new types and to subhierarchies, as motivated in \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>).

And to provide this,

  - We need to define a user API.

All these are the subjects of this NEP.

  - The class hierarchy, its relation to the Python scalar types, and its important attributes are described in [nep42\_DType class](#nep42_DType%20class).
  - The functionality that will support dtype casting is described in [Casting](#casting).
  - The implementation of item access and storage, and the way shape and dtype are determined when creating an array, are described in \[nep42\_array\_coercion\](\#nep42\_array\_coercion).
  - The functionality for users to define their own DTypes is described in [Public C-API](#public-c-api).

The API here and in \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) is entirely on the C side. A Python-side version will be proposed in a future NEP. A future Python API is expected to be similar, but provide a more convenient API to reuse the functionality of existing DTypes. It could also provide shorthands to create structured DTypes similar to Python's [dataclasses](https://docs.python.org/3.8/library/dataclasses.html).

## Backward compatibility

The disruption is expected to be no greater than that of a typical NumPy release.

  - The main issues are noted in \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) and will mostly affect heavy users of the NumPy C-API.
  - Eventually we will want to deprecate the API currently used for creating user-defined dtypes.
  - Small, rarely noticed inconsistencies are likely to change. Examples:
      - `np.array(np.nan, dtype=np.int64)` behaves differently from `np.array([np.nan], dtype=np.int64)` with the latter raising an error. This may require identical results (either both error or both succeed).
      - `np.array([array_like])` sometimes behaves differently from `np.array([np.array(array_like)])`
      - array operations may or may not preserve dtype metadata
  - Documentation that describes the internal structure of dtypes will need to be updated.

The new code must pass NumPy's regular test suite, giving some assurance that the changes are compatible with existing code.

## Usage and impact

We believe the few structures in this section are sufficient to consolidate NumPy's present functionality and also to support complex user-defined DTypes.

The rest of the NEP fills in details and provides support for the claim.

Again, though Python is used for illustration, the implementation is a C API only; a future NEP will tackle the Python API.

After implementing this NEP, creating a DType will be possible by implementing the following outlined DType base class, that is further described in [nep42\_DType class](#nep42_DType%20class):

``` python
class DType(np.dtype):
    type : type        # Python scalar type
    parametric : bool  # (may be indicated by superclass)

    @property
    def canonical(self) -> bool:
        raise NotImplementedError

    def ensure_canonical(self : DType) -> DType:
        raise NotImplementedError
```

For casting, a large part of the functionality is provided by the "methods" stored in `_castingimpl`

``` python
@classmethod
def common_dtype(cls : DTypeMeta, other : DTypeMeta) -> DTypeMeta:
    raise NotImplementedError

def common_instance(self : DType, other : DType) -> DType:
    raise NotImplementedError

# A mapping of "methods" each detailing how to cast to another DType
# (further specified at the end of the section)
_castingimpl = {}
```

For array-coercion, also part of casting:

``` python
def __dtype_setitem__(self, item_pointer, value):
    raise NotImplementedError

def __dtype_getitem__(self, item_pointer, base_obj) -> object:
    raise NotImplementedError

@classmethod
def __discover_descr_from_pyobject__(cls, obj : object) -> DType:
    raise NotImplementedError

# initially private:
@classmethod
def _known_scalar_type(cls, obj : object) -> bool:
    raise NotImplementedError
```

Other elements of the casting implementation is the `CastingImpl`:

``` python
casting = Union["safe", "same_kind", "unsafe"]

class CastingImpl:
    # Object describing and performing the cast
    casting : casting

    def resolve_descriptors(self, Tuple[DTypeMeta], Tuple[DType|None] : input) -> (casting, Tuple[DType]):
        raise NotImplementedError

    # initially private:
    def _get_loop(...) -> lowlevel_C_loop:
        raise NotImplementedError
```

which describes the casting from one DType to another. In \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) this `CastingImpl` object is used unchanged to support universal functions. Note that the name `CastingImpl` here will be generically called `ArrayMethod` to accommodate both casting and universal functions.

## Definitions

<div class="glossary">

  - dtype  
    The dtype *instance*; this is the object attached to a numpy array.

  - DType  
    Any subclass of the base type `np.dtype`.

  - coercion  
    Conversion of Python types to NumPy arrays and values stored in a NumPy array.

  - cast  
    Conversion of an array to a different dtype.

  - parametric type  
    A dtype whose representation can change based on a parameter value, like a string dtype with a length parameter. All members of the current `flexible` dtype class are parametric. See \[NEP 40 \<parametric-datatype-discussion\>\](\#nep-40-\<parametric-datatype-discussion\>).

  - promotion  
    Finding a dtype that can perform an operation on a mix of dtypes without loss of information.

  - safe cast  
    A cast is safe if no information is lost when changing type.

</div>

On the C level we use `descriptor` or `descr` to mean *dtype instance*. In the proposed C-API, these terms will distinguish dtype instances from DType classes.

<div class="note">

<div class="title">

Note

</div>

NumPy has an existing class hierarchy for scalar types, as seen \[in the figure \<nep-0040\_dtype-hierarchy\>\](\#in-the-figure-\<nep-0040\_dtype-hierarchy\>) of \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>), and the new DType hierarchy will resemble it. The types are used as an attribute of the single dtype class in the current NumPy; they're not dtype classes. They neither harm nor help this work.

</div>

## The DType class

This section reviews the structure underlying the proposed DType class, including the type hierarchy and the use of abstract DTypes.

### Class getter

To create a DType instance from a scalar type users now call `np.dtype` (for instance, `np.dtype(np.int64)`). Sometimes it is also necessary to access the underlying DType class; this comes up in particular with type hinting because the "type" of a DType instance is the DType class. Taking inspiration from type hinting, we propose the following getter syntax:

    np.dtype[np.int64]

to get the DType class corresponding to a scalar type. The notation works equally well with built-in and user-defined DTypes.

This getter eliminates the need to create an explicit name for every DType, crowding the `np` namespace; the getter itself signifies the type. It also opens the possibility of making `np.ndarray` generic over DType class using annotations like:

    np.ndarray[np.dtype[np.float64]]

The above is fairly verbose, so it is possible that we will include aliases like:

    Float64 = np.dtype[np.float64]

in `numpy.typing`, thus keeping annotations concise but still avoiding crowding the `np` namespace as discussed above. For a user-defined DType:

    class UserDtype(dtype): ...

one can do `np.ndarray[UserDtype]`, keeping annotations concise in that case without introducing boilerplate in NumPy itself. For a user-defined scalar type:

    class UserScalar(generic): ...

we would need to add a typing overload to `dtype`:

    @overload
    __new__(cls, dtype: Type[UserScalar], ...) -> UserDtype

to allow `np.dtype[UserScalar]`.

The initial implementation probably will return only concrete (not abstract) DTypes.

*This item is still under review.*

### Hierarchy and abstract classes

We will use abstract classes as building blocks of our extensible DType class hierarchy.

1.  Abstract classes are inherited cleanly, in principle allowing checks like `isinstance(np.dtype("float64"), np.inexact)`.
2.  Abstract classes allow a single piece of code to handle a multiplicity of input types. Code written to accept Complex objects can work with numbers of any precision; the precision of the results is determined by the precision of the arguments.
3.  There's room for user-created families of DTypes. We can envision an abstract `Unit` class for physical units, with a concrete subclass like `Float64Unit`. Calling `Unit(np.float64, "m")` (`m` for meters) would be equivalent to `Float64Unit("m")`.
4.  The implementation of universal functions in \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) may require a class hierarchy.

**Example:** A NumPy `Categorical` class would be a match for pandas `Categorical` objects, which can contain integers or general Python objects. NumPy needs a DType that it can assign a Categorical to, but it also needs DTypes like `CategoricalInt64` and `CategoricalObject` such that `common_dtype(CategoricalInt64, String)` raises an error, but `common_dtype(CategoricalObject, String)` returns an `object` DType. In our scheme, `Categorical` is an abstract type with `CategoricalInt64` and `CategoricalObject` subclasses.

Rules for the class structure, illustrated \[below \<nep42\_hierarchy\_figure\>\](\#below-\<nep42\_hierarchy\_figure\>):

1.  Abstract DTypes cannot be instantiated. Instantiating an abstract DType raises an error, or perhaps returns an instance of a concrete subclass. Raising an error will be the default behavior and may be required initially.
2.  While abstract DTypes may be superclasses, they may also act like Python's abstract base classes (ABC) allowing registration instead of subclassing. It may be possible to simply use or inherit from Python ABCs.
3.  Concrete DTypes may not be subclassed. In the future this might be relaxed to allow specialized implementations such as a GPU float64 subclassing a NumPy float64.

The [Julia language](https://docs.julialang.org/en/v1/manual/types/#man-abstract-types-1) has a similar prohibition against subclassing concrete types. For example methods such as the later `__common_instance__` or `__common_dtype__` cannot work for a subclass unless they were designed very carefully. It helps avoid unintended vulnerabilities to implementation changes that result from subclassing types that were not written to be subclassed. We believe that the DType API should rather be extended to simplify wrapping of existing functionality.

The DType class requires C-side storage of methods and additional information, to be implemented by a `DTypeMeta` class. Each `DType` class is an instance of `DTypeMeta` with a well-defined and extensible interface; end users ignore it.

### Miscellaneous methods and attributes

This section collects definitions in the DType class that are not used in casting and array coercion, which are described in detail below.

  - Existing dtype methods (<span class="title-ref">numpy.dtype</span>) and C-side fields are preserved.
  - `DType.type` replaces `dtype.type`. Unless a use case arises, `dtype.type` will be deprecated. This indicates a Python scalar type which represents the same values as the DType. This is the same type as used in the proposed [Class getter](#class-getter) and for [DType discovery during array coercion](#dtype-discovery-during-array-coercion). (This can may also be set for abstract DTypes, this is necessary for array coercion.)
  - A new `self.canonical` property generalizes the notion of byte order to indicate whether data has been stored in a default/canonical way. For existing code, "canonical" will just signify native byte order, but it can take on new meanings in new DTypes -- for instance, to distinguish a complex-conjugated instance of Complex which stores `real - imag` instead of `real + imag`. The ISNBO ("is native byte order") flag might be repurposed as the canonical flag.
  - Support is included for parametric DTypes. A DType will be deemed parametric if it inherits from ParametricDType.
  - DType methods may resemble or even reuse existing Python slots. Thus Python special slots are off-limits for user-defined DTypes (for instance, defining `Unit("m") > Unit("cm")`), since we may want to develop a meaning for these operators that is common to all DTypes.
  - Sorting functions are moved to the DType class. They may be implemented by defining a method `dtype_get_sort_function(self, sortkind="stable") -> sortfunction` that must return `NotImplemented` if the given `sortkind` is not known.
  - Functions that cannot be removed are implemented as special methods. Many of these were previously defined part of the :c`PyArray_ArrFuncs` slot of the dtype instance (`PyArray_Descr *`) and include functions such as `nonzero`, `fill` (used for `np.arange`), and `fromstr` (used to parse text files). These old methods will be deprecated and replacements following the new design principles added. The API is not defined here. Since these methods can be deprecated and renamed replacements added, it is acceptable if these new methods have to be modified.
  - Use of `kind` for non-built-in types is discouraged in favor of `isinstance` checks. `kind` will return the `__qualname__` of the object to ensure uniqueness for all DTypes. On the C side, `kind` and `char` are set to `\0` (NULL character). While `kind` will be discouraged, the current `np.issubdtype` may remain the preferred method for this type of check.
  - A method `ensure_canonical(self) -> dtype` returns a new dtype (or `self`) with the `canonical` flag set.
  - Since NumPy's approach is to provide functionality through unfuncs, functions like sorting that will be implemented in DTypes might eventually be reimplemented as generalized ufuncs.

## Casting

We review here the operations related to casting arrays:

  - Finding the "common dtype," returned by <span class="title-ref">numpy.promote\_types</span> and <span class="title-ref">numpy.result\_type</span>
  - The result of calling <span class="title-ref">numpy.can\_cast</span>

We show how casting arrays with `astype(new_dtype)` will be implemented.

### <span class="title-ref">Common DType</span> operations

When input types are mixed, a first step is to find a DType that can hold the result without loss of information -- a "common DType."

Array coercion and concatenation both return a common dtype instance. Most universal functions use the common DType for dispatching, though they might not use it for a result (for instance, the result of a comparison is always bool).

We propose the following implementation:

  - For two DType classes:
    
        __common_dtype__(cls, other : DTypeMeta) -> DTypeMeta
    
    Returns a new DType, often one of the inputs, which can represent values of both input DTypes. This should usually be minimal: the common DType of `Int16` and `Uint16` is `Int32` and not `Int64`. `__common_dtype__` may return NotImplemented to defer to other and, like Python operators, subclasses take precedence (their `__common_dtype__` method is tried first).

  - For two instances of the same DType:
    
        __common_instance__(self: SelfT, other : SelfT) -> SelfT
    
    For nonparametric built-in dtypes, this returns a canonicalized copy of `self`, preserving metadata. For nonparametric user types, this provides a default implementation.

  - For instances of different DTypes, for example `>float64` and `S8`, the operation is done in three steps:
    
    1.  `Float64.__common_dtype__(type(>float64), type(S8))` returns `String` (or defers to `String.__common_dtype__`).
    2.  The casting machinery (explained in detail below) provides the information that `">float64"` casts to `"S32"`
    3.  `String.__common_instance__("S8", "S32")` returns the final `"S32"`.

The benefit of this handoff is to reduce duplicated code and keep concerns separate. DType implementations don't need to know how to cast, and the results of casting can be extended to new types, such as a new string encoding.

This means the implementation will work like this:

    def common_dtype(DType1, DType2):
        common_dtype = type(dtype1).__common_dtype__(type(dtype2))
        if common_dtype is NotImplemented:
            common_dtype = type(dtype2).__common_dtype__(type(dtype1))
            if common_dtype is NotImplemented:
                raise TypeError("no common dtype")
        return common_dtype
    
    def promote_types(dtype1, dtype2):
        common = common_dtype(type(dtype1), type(dtype2))
    
        if type(dtype1) is not common:
            # Find what dtype1 is cast to when cast to the common DType
            # by using the CastingImpl as described below:
            castingimpl = get_castingimpl(type(dtype1), common)
            safety, (_, dtype1) = castingimpl.resolve_descriptors(
                    (common, common), (dtype1, None))
            assert safety == "safe"  # promotion should normally be a safe cast
    
        if type(dtype2) is not common:
            # Same as above branch for dtype1.
    
        if dtype1 is not dtype2:
            return common.__common_instance__(dtype1, dtype2)

Some of these steps may be optimized for nonparametric DTypes.

Since the type returned by `__common_dtype__` is not necessarily one of the two arguments, it's not equivalent to NumPy's "safe" casting. Safe casting works for `np.promote_types(int16, int64)`, which returns `int64`, but fails for:

    np.promote_types("int64", "float32") -> np.dtype("float64")

It is the responsibility of the DType author to ensure that the inputs can be safely cast to the `__common_dtype__`.

Exceptions may apply. For example, casting `int32` to a (long enough) string is at least at this time considered "safe". However `np.promote_types(int32, String)` will *not* be defined.

**Example:**

`object` always chooses `object` as the common DType. For `datetime64` type promotion is defined with no other datatype, but if someone were to implement a new higher precision datetime, then:

    HighPrecisionDatetime.__common_dtype__(np.dtype[np.datetime64])

would return `HighPrecisionDatetime`, and the casting implementation, as described below, may need to decide how to handle the datetime unit.

**Alternatives:**

  - We're pushing the decision on common DTypes to the DType classes. Suppose instead we could turn to a universal algorithm based on safe casting, imposing a total order on DTypes and returning the first type that both arguments could cast to safely.
    
    It would be difficult to devise a reasonable total order, and it would have to accept new entries. Beyond that, the approach is flawed because importing a type can change the behavior of a program. For example, a program requiring the common DType of `int16` and `uint16` would ordinarily get the built-in type `int32` as the first match; if the program adds `import int24`, the first match becomes `int24` and the smaller type might make the program overflow for the first time.\[1\]

  - A more flexible common DType could be implemented in the future where `__common_dtype__` relies on information from the casting logic. Since `__commond_dtype__` is a method a such a default implementation could be added at a later time.

  - The three-step handling of differing dtypes could, of course, be coalesced. It would lose the value of splitting in return for a possibly faster execution. But few cases would benefit. Most cases, such as array coercion, involve a single Python type (and thus dtype).

### The cast operation

Casting is perhaps the most complex and interesting DType operation. It is much like a typical universal function on arrays, converting one input to a new output, with two distinctions:

  - Casting always requires an explicit output datatype.
  - The NumPy iterator API requires access to functions that are lower-level than what universal functions currently need.

Casting can be complex and may not implement all details of each input datatype (such as non-native byte order or unaligned access). So a complex type conversion might entail 3 steps:

1.  The input datatype is normalized and prepared for the cast.
2.  The cast is performed.
3.  The result, which is in a normalized form, is cast to the requested form (non-native byte order).

Further, NumPy provides different casting kinds or safety specifiers:

  - `equivalent`, allowing only byte-order changes
  - `safe`, requiring a type large enough to preserve value
  - `same_kind`, requiring a safe cast or one within a kind, like float64 to float32
  - `unsafe`, allowing any data conversion

and in some cases a cast may be just a view.

We need to support the two current signatures of `arr.astype`:

  - For DTypes: `arr.astype(np.String)`
      - current spelling `arr.astype("S")`
      - `np.String` can be an abstract DType
  - For dtypes: `arr.astype(np.dtype("S8"))`

We also have two signatures of `np.can_cast`:

  - Instance to class: `np.can_cast(dtype, DType, "safe")`
  - Instance to instance: `np.can_cast(dtype, other_dtype, "safe")`

On the Python level `dtype` is overloaded to mean class or instance.

A third `can_cast` signature, `np.can_cast(DType, OtherDType, "safe")`,may be used internally but need not be exposed to Python.

During DType creation, DTypes will be able to pass a list of `CastingImpl` objects, which can define casting to and from the DType.

One of them should define the cast between instances of that DType. It can be omitted if the DType has only a single implementation and is nonparametric.

Each `CastingImpl` has a distinct DType signature:

> `CastingImpl[InputDtype, RequestedDtype]`

and implements the following methods and attributes:

  - To report safeness,
    
    `resolve_descriptors(self, Tuple[DTypeMeta], Tuple[DType|None] : input) -> casting, Tuple[DType]`.
    
    The `casting` output reports safeness (safe, unsafe, or same-kind), and the tuple is used for more multistep casting, as in the example below.

  - To get a casting function,
    
    `get_loop(...) -> function_to_handle_cast (signature to be decided)`
    
    returns a low-level implementation of a strided casting function ("transfer function") capable of performing the cast.
    
    Initially the implementation will be *private*, and users will only be able to provide strided loops with the signature.

  - For performance, a `casting` attribute taking a value of `equivalent`, `safe`, `unsafe`, or `same-kind`.

**Performing a cast**

<div id="nep42_cast_figure">

![](casting_flow.svg)

</div>

The above figure illustrates a multistep cast of an `int24` with a value of `42` to a string of length 20 (`"S20"`).

We've picked an example where the implementer has only provided limited functionality: a function to cast an `int24` to an `S8` string (which can hold all 24-bit integers). This means multiple conversions are needed.

The full process is:

1.  Call
    
    `CastingImpl[Int24, String].resolve_descriptors((Int24, String), (int24, "S20"))`.
    
    This provides the information that `CastingImpl[Int24, String]` only implements the cast of `int24` to `"S8"`.

2.  Since `"S8"` does not match `"S20"`, use
    
    `CastingImpl[String, String].get_loop()`
    
    to find the transfer (casting) function to convert an `"S8"` into an `"S20"`

3.  Fetch the transfer function to convert an `int24` to an `"S8"` using
    
    `CastingImpl[Int24, String].get_loop()`

4.  Perform the actual cast using the two transfer functions:
    
    `int24(42) -> S8("42") -> S20("42")`.
    
    `resolve_descriptors` allows the implementation for
    
    `np.array(42, dtype=int24).astype(String)`
    
    to call
    
    `CastingImpl[Int24, String].resolve_descriptors((Int24, String), (int24, None))`.
    
    In this case the result of `(int24, "S8")` defines the correct cast:
    
    `np.array(42, dtype=int24).astype(String) == np.array("42", dtype="S8")`.

**Casting safety**

To compute `np.can_cast(int24, "S20", casting="safe")`, only the `resolve_descriptors` function is required and is called in the same way as in \[the figure describing a cast \<nep42\_cast\_figure\>\](\#the-figure-describing-a-cast-\<nep42\_cast\_figure\>).

In this case, the calls to `resolve_descriptors`, will also provide the information that `int24 -> "S8"` as well as `"S8" -> "S20"` are safe casts, and thus also the `int24 -> "S20"` is a safe cast.

In some cases, no cast is necessary. For example, on most Linux systems `np.dtype("long")` and `np.dtype("longlong")` are different dtypes but are both 64-bit integers. In this case, the cast can be performed using `long_arr.view("longlong")`. The information that a cast is a view will be handled by an additional flag. Thus the `casting` can have the 8 values in total: the original 4 of `equivalent`, `safe`, `unsafe`, and `same-kind`, plus `equivalent+view`, `safe+view`, `unsafe+view`, and `same-kind+view`. NumPy currently defines `dtype1 == dtype2` to be True only if byte order matches. This functionality can be replaced with the combination of "equivalent" casting and the "view" flag.

(For more information on the `resolve_descriptors` signature see the \[nep42\_C-API\](\#nep42\_c-api) section below and \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>).)

**Casting between instances of the same DType**

To keep down the number of casting steps, CastingImpl must be capable of any conversion between all instances of this DType.

In general the DType implementer must include `CastingImpl[DType, DType]` unless there is only a singleton instance.

**General multistep casting**

We could implement certain casts, such as `int8` to `int24`, even if the user provides only an `int16 -> int24` cast. This proposal does not provide that, but future work might find such casts dynamically, or at least allow `resolve_descriptors` to return arbitrary `dtypes`.

If `CastingImpl[Int8, Int24].resolve_descriptors((Int8, Int24), (int8, int24))` returns `(int16, int24)`, the actual casting process could be extended to include the `int8 -> int16` cast. This adds a step.

**Example:**

The implementation for casting integers to datetime would generally say that this cast is unsafe (because it is always an unsafe cast). Its `resolve_descriptors` function may look like:

    def resolve_descriptors(self, DTypes, given_dtypes):
       from_dtype, to_dtype = given_dtypes
       from_dtype = from_dtype.ensure_canonical()  # ensure not byte-swapped
       if to_dtype is None:
           raise TypeError("Cannot convert to a NumPy datetime without a unit")
       to_dtype = to_dtype.ensure_canonical()  # ensure not byte-swapped
    
       # This is always an "unsafe" cast, but for int64, we can represent
       # it by a simple view (if the dtypes are both canonical).
       # (represented as C-side flags here).
       safety_and_view = NPY_UNSAFE_CASTING | _NPY_CAST_IS_VIEW
       return safety_and_view, (from_dtype, to_dtype)

\> **Note** \> While NumPy currently defines integer-to-datetime casts, with the possible exception of the unit-less `timedelta64` it may be better to not define these casts at all. In general we expect that user defined DTypes will be using custom methods such as `unit.drop_unit(arr)` or `arr *     unit.seconds`.

**Alternatives:**

  - Our design objectives are:
    
      - Minimize the number of DType methods and avoid code duplication.
      - Mirror the implementation of universal functions.

  - The decision to use only the DType classes in the first step of finding the correct `CastingImpl` in addition to defining `CastingImpl.casting`, allows to retain the current default implementation of `__common_dtype__` for existing user defined dtypes, which could be expanded in the future.

  - The split into multiple steps may seem to add complexity rather than reduce it, but it consolidates the signatures of `np.can_cast(dtype, DTypeClass)` and `np.can_cast(dtype, other_dtype)`.
    
    Further, the API guarantees separation of concerns for user DTypes. The user `Int24` dtype does not have to handle all string lengths if it does not wish to do so. Further, an encoding added to the `String` DType would not affect the overall cast. The `resolve_descriptors` function can keep returning the default encoding and the `CastingImpl[String, String]` can take care of any necessary encoding changes.

  - The main alternative is moving most of the information that is here pushed into the `CastingImpl` directly into methods on the DTypes. But this obscures the similarity between casting and universal functions. It does reduce indirection, as noted below.

  - An earlier proposal defined two methods `__can_cast_to__(self, other)` to dynamically return `CastingImpl`. This removes the requirement to define all possible casts at DType creation (of one of the involved DTypes).
    
    Such an API could be added later. It resembles Python's `__getattr__` in providing additional control over attribute lookup.

**Notes:**

`CastingImpl` is used as a name in this NEP to clarify that it implements all functionality related to a cast. It is meant to be identical to the `ArrayMethod` proposed in NEP 43 as part of restructuring ufuncs to handle new DTypes. All type definitions are expected to be named `ArrayMethod`.

The way dispatching works for `CastingImpl` is planned to be limited initially and fully opaque. In the future, it may or may not be moved into a special UFunc, or behave more like a universal function.

### Coercion to and from Python objects

When storing a single value in an array or taking it out, it is necessary to coerce it -- that is, convert it -- to and from the low-level representation inside the array.

Coercion is slightly more complex than typical casts. One reason is that a Python object could itself be a 0-dimensional array or scalar with an associated DType.

Coercing to and from Python scalars requires two to three methods that largely correspond to the current definitions:

1.  `__dtype_setitem__(self, item_pointer, value)`
2.  `__dtype_getitem__(self, item_pointer, base_obj) -> object`; `base_obj` is for memory management and usually ignored; it points to an object owning the data. Its only role is to support structured datatypes with subarrays within NumPy, which currently return views into the array. The function returns an equivalent Python scalar (i.e. typically a NumPy scalar).
3.  `__dtype_get_pyitem__(self, item_pointer, base_obj) -> object` (initially hidden for new-style user-defined datatypes, may be exposed on user request). This corresponds to the `arr.item()` method also used by `arr.tolist()` and returns Python floats, for example, instead of NumPy floats.

(The above is meant for C-API. A Python-side API would have to use byte buffers or similar to implement this, which may be useful for prototyping.)

When a certain scalar has a known (different) dtype, NumPy may in the future use casting instead of `__dtype_setitem__`.

A user datatype is (initially) expected to implement `__dtype_setitem__` for its own `DType.type` and all basic Python scalars it wishes to support (e.g. `int` and `float`). In the future a function `known_scalar_type` may be made public to allow a user dtype to signal which Python scalars it can store directly.

**Implementation:** The pseudocode implementation for setting a single item in an array from an arbitrary Python object `value` is (some functions here are defined later):

    def PyArray_Pack(dtype, item_pointer, value):
        DType = type(dtype)
        if DType.type is type(value) or DType.known_scalartype(type(value)):
            return dtype.__dtype_setitem__(item_pointer, value)
    
        # The dtype cannot handle the value, so try casting:
        arr = np.array(value)
        if arr.dtype is object or arr.ndim != 0:
            # not a numpy or user scalar; try using the dtype after all:
            return dtype.__dtype_setitem__(item_pointer, value)
    
         arr.astype(dtype)
         item_pointer.write(arr[()])

where the call to `np.array()` represents the dtype discovery and is not actually performed.

**Example:** Current `datetime64` returns `np.datetime64` scalars and can be assigned from `np.datetime64`. However, the datetime `__dtype_setitem__` also allows assignment from date strings ("2016-05-01") or Python integers. Additionally the datetime `__dtype_get_pyitem__` function actually returns a Python `datetime.datetime` object (most of the time).

**Alternatives:** This functionality could also be implemented as a cast to and from the `object` dtype. However, coercion is slightly more complex than typical casts. One reason is that in general a Python object could itself be a zero-dimensional array or scalar with an associated DType. Such an object has a DType, and the correct cast to another DType is already defined:

    np.array(np.float32(4), dtype=object).astype(np.float64)

is identical to:

    np.array(4, dtype=np.float32).astype(np.float64)

Implementing the first `object` to `np.float64` cast explicitly, would require the user to take to duplicate or fall back to existing casting functionality.

It is certainly possible to describe the coercion to and from Python objects using the general casting machinery, but the `object` dtype is special and important enough to be handled by NumPy using the presented methods.

**Further issues and discussion:**

  - The `__dtype_setitem__` function duplicates some code, such as coercion from a string.
    
    `datetime64` allows assignment from string, but the same conversion also occurs for casting from the string dtype to `datetime64`.
    
    We may in the future expose the `known_scalartype` function to allow the user to implement such duplication.
    
    For example, NumPy would normally use
    
    `np.array(np.string_("2019")).astype(datetime64)`
    
    but `datetime64` could choose to use its `__dtype_setitem__` instead for performance reasons.

  - There is an issue about how subclasses of scalars should be handled. We anticipate to stop automatically detecting the dtype for `np.array(float64_subclass)` to be float64. The user can still provide `dtype=np.float64`. However, the above automatic casting using `np.array(scalar_subclass).astype(requested_dtype)` will fail. In many cases, this is not an issue, since the Python `__float__` protocol can be used instead. But in some cases, this will mean that subclasses of Python scalars will behave differently.

\> **Note** \> *Example:* `np.complex256` should not use `__float__` in its `__dtype_setitem__` method in the future unless it is a known floating point type. If the scalar is a subclass of a different high precision floating point type (e.g. `np.float128`) then this currently loses precision without notifying the user. In that case `np.array(float128_subclass(3), dtype=np.complex256)` may fail unless the `float128_subclass` is first converted to the `np.float128` base class.

### DType discovery during array coercion

An important step in the use of NumPy arrays is creation of the array from collections of generic Python objects.

**Motivation:** Although the distinction is not clear currently, there are two main needs:

    np.array([1, 2, 3, 4.])

needs to guess the correct dtype based on the Python objects inside. Such an array may include a mix of datatypes, as long as they can be promoted. A second use case is when users provide the output DType class, but not the specific DType instance:

    np.array([object(), None], dtype=np.dtype[np.string_])  # (or `dtype="S"`)

In this case the user indicates that `object()` and `None` should be interpreted as strings. The need to consider the user provided DType also arises for a future `Categorical`:

    np.array([1, 2, 1, 1, 2], dtype=Categorical)

which must interpret the numbers as unique categorical values rather than integers.

There are three further issues to consider:

1.  It may be desirable to create datatypes associated with normal Python scalars (such as `datetime.datetime`) that do not have a `dtype` attribute already.
2.  In general, a datatype could represent a sequence, however, NumPy currently assumes that sequences are always collections of elements (the sequence cannot be an element itself). An example would be a `vector` DType.
3.  An array may itself contain arrays with a specific dtype (even general Python objects). For example: `np.array([np.array(None, dtype=object)], dtype=np.String)` poses the issue of how to handle the included array.

Some of these difficulties arise because finding the correct shape of the output array and finding the correct datatype are closely related.

**Implementation:** There are two distinct cases above:

1.  The user has provided no dtype information.
2.  The user provided a DType class -- as represented, for example, by `"S"` representing a string of any length.

In the first case, it is necessary to establish a mapping from the Python type(s) of the constituent elements to the DType class. Once the DType class is known, the correct dtype instance needs to be found. In the case of strings, this requires to find the string length.

These two cases shall be implemented by leveraging two pieces of information:

1.  `DType.type`: The current type attribute to indicate which Python scalar type is associated with the DType class (this is a *class* attribute that always exists for any datatype and is not limited to array coercion).
2.  `__discover_descr_from_pyobject__(cls, obj) -> dtype`: A classmethod that returns the correct descriptor given the input object. Note that only parametric DTypes have to implement this. For nonparametric DTypes using the default instance will always be acceptable.

The Python scalar type which is already associated with a DType through the `DType.type` attribute maps from the DType to the Python scalar type. At registration time, a DType may choose to allow automatically discover for this Python scalar type. This requires a lookup in the opposite direction, which will be implemented using global a mapping (dictionary-like) of:

    known_python_types[type] = DType

Correct garbage collection requires additional care. If both the Python scalar type (`pytype`) and `DType` are created dynamically, they will potentially be deleted again. To allow this, it must be possible to make the above mapping weak. This requires that the `pytype` holds a reference of `DType` explicitly. Thus, in addition to building the global mapping, NumPy will store the `DType` as `pytype.__associated_array_dtype__` in the Python type. This does *not* define the mapping and should *not* be accessed directly. In particular potential inheritance of the attribute does not mean that NumPy will use the superclasses `DType` automatically. A new `DType` must be created for the subclass.

\> **Note** \> Python integers do not have a clear/concrete NumPy type associated right now. This is because during array coercion NumPy currently finds the first type capable of representing their value in the list of <span class="title-ref">long</span>, <span class="title-ref">unsigned long</span>, <span class="title-ref">int64</span>, <span class="title-ref">unsigned int64</span>, and <span class="title-ref">object</span> (on many machines <span class="title-ref">long</span> is 64 bit).

> Instead they will need to be implemented using an `AbstractPyInt`. This DType class can then provide `__discover_descr_from_pyobject__` and return the actual dtype which is e.g. `np.dtype("int64")`. For dispatching/promotion in ufuncs, it will also be necessary to dynamically create `AbstractPyInt[value]` classes (creation can be cached), so that they can provide the current value based promotion functionality provided by `np.result_type(python_integer, array)`\[2\] .

To allow for a DType to accept inputs as scalars that are not basic Python types or instances of `DType.type`, we use `known_scalar_type` method. This can allow discovery of a `vector` as a scalar (element) instead of a sequence (for the command `np.array(vector, dtype=VectorDType)`) even when `vector` is itself a sequence or even an array subclass. This will *not* be public API initially, but may be made public at a later time.

**Example:** The current datetime DType requires a `__discover_descr_from_pyobject__` which returns the correct unit for string inputs. This allows it to support:

    np.array(["2020-01-02", "2020-01-02 11:24"], dtype="M8")

By inspecting the date strings. Together with the common dtype operation, this allows it to automatically find that the datetime64 unit should be "minutes".

**NumPy internal implementation:** The implementation to find the correct dtype will work similar to the following pseudocode:

    def find_dtype(array_like):
        common_dtype = None
        for element in array_like:
            # default to object dtype, if unknown
            DType = known_python_types.get(type(element), np.dtype[object])
            dtype = DType.__discover_descr_from_pyobject__(element)
    
            if common_dtype is None:
                common_dtype = dtype
            else:
                common_dtype = np.promote_types(common_dtype, dtype)

In practice, the input to `np.array()` is a mix of sequences and array-like objects, so that deciding what is an element requires to check whether it is a sequence. The full algorithm (without user provided dtypes) thus looks more like:

    def find_dtype_recursive(array_like, dtype=None):
        """
        Recursively find the dtype for a nested sequences (arrays are not
        supported here).
        """
        DType = known_python_types.get(type(element), None)
    
        if DType is None and is_array_like(array_like):
            # Code for a sequence, an array_like may have a DType we
            # can use directly:
            for element in array_like:
                dtype = find_dtype_recursive(element, dtype=dtype)
            return dtype
    
        elif DType is None:
            DType = np.dtype[object]
    
        # dtype discovery and promotion as in `find_dtype` above

If the user provides `DType`, then this DType will be tried first, and the `dtype` may need to be cast before the promotion is performed.

**Limitations:** The motivational point 3. of a nested array `np.array([np.array(None, dtype=object)], dtype=np.String)` is currently (sometimes) supported by inspecting all elements of the nested array. User DTypes will implicitly handle these correctly if the nested array is of `object` dtype. In some other cases NumPy will retain backward compatibility for existing functionality only. NumPy uses such functionality to allow code such as:

    >>> np.array([np.array(["2020-05-05"], dtype="S")], dtype=np.datetime64)
    array([['2020-05-05']], dtype='datetime64[D]')

which discovers the datetime unit `D` (days). This possibility will not be accessible to user DTypes without an intermediate cast to `object` or a custom function.

The use of a global type map means that an error or warning has to be given if two DTypes wish to map to the same Python type. In most cases user DTypes should only be implemented for types defined within the same library to avoid the potential for conflicts. It will be the DType implementor's responsibility to be careful about this and use avoid registration when in doubt.

**Alternatives:**

  - Instead of a global mapping, we could rely on the scalar attribute `scalar.__associated_array_dtype__`. This only creates a difference in behavior for subclasses, and the exact implementation can be undefined initially. Scalars will be expected to derive from a NumPy scalar. In principle NumPy could, for a time, still choose to rely on the attribute.

  - An earlier proposal for the `dtype` discovery algorithm used a two-pass approach, first finding the correct `DType` class and only then discovering the parametric `dtype` instance. It was rejected as needlessly complex. But it would have enabled value-based promotion in universal functions, allowing:
    
        np.add(np.array([8], dtype="uint8"), [4])
    
    to return a `uint8` result (instead of `int16`), which currently happens for:
    
        np.add(np.array([8], dtype="uint8"), 4)
    
    (note the list `[4]` instead of scalar `4`). This is not a feature NumPy currently has or desires to support.

**Further issues and discussion:** It is possible to create a DType such as Categorical, array, or vector which can only be used if `dtype=DType` is provided. Such DTypes cannot roundtrip correctly. For example:

    np.array(np.array(1, dtype=Categorical)[()])

will result in an integer array. To get the original `Categorical` array `dtype=Categorical` will need to be passed explicitly. This is a general limitation, but round-tripping is always possible if `dtype=original_arr.dtype` is passed.

## Public C-API

### DType creation

To create a new DType the user will need to define the methods and attributes outlined in the [Usage and impact](#usage-and-impact) section and detailed throughout this proposal.

In addition, some methods similar to those in :c`PyArray_ArrFuncs` will be needed for the slots struct below.

As mentioned in \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>), the interface to define this DType class in C is modeled after [PEP 384](http://www.python.org/dev/peps/pep-0384/ "PEP 384"): Slots and some additional information will be passed in a slots struct and identified by `ssize_t` integers:

    static struct PyArrayMethodDef slots[] = {
        {NPY_dt_method, method_implementation},
        ...,
        {0, NULL}
    }
    
    typedef struct{
      PyTypeObject *typeobj;    /* type of python scalar or NULL */
      int flags                 /* flags, including parametric and abstract */
      /* NULL terminated CastingImpl; is copied and references are stolen */
      CastingImpl *castingimpls[];
      PyType_Slot *slots;
      PyTypeObject *baseclass;  /* Baseclass or NULL */
    } PyArrayDTypeMeta_Spec;
    
    PyObject* PyArray_InitDTypeMetaFromSpec(PyArrayDTypeMeta_Spec *dtype_spec);

All of this is passed by copying.

**TODO:** The DType author should be able to define new methods for the DType, up to defining a full object, and, in the future, possibly even extending the `PyArrayDTypeMeta_Type` struct. We have to decide what to make available initially. A solution may be to allow inheriting only from an existing class: `class MyDType(np.dtype, MyBaseclass)`. If `np.dtype` is first in the method resolution order, this also prevents an undesirable override of slots like `==`.

The `slots` will be identified by names which are prefixed with `NPY_dt_` and are:

  - `is_canonical(self) -> {0, 1}`
  - `ensure_canonical(self) -> dtype`
  - `default_descr(self) -> dtype` (return must be native and should normally be a singleton)
  - `setitem(self, char *item_ptr, PyObject *value) -> {-1, 0}`
  - `getitem(self, char *item_ptr, PyObject (base_obj) -> object or NULL`
  - `discover_descr_from_pyobject(cls, PyObject) -> dtype or NULL`
  - `common_dtype(cls, other) -> DType, NotImplemented, or NULL`
  - `common_instance(self, other) -> dtype or NULL`

Where possible, a default implementation will be provided if the slot is omitted or set to `NULL`. Nonparametric dtypes do not have to implement:

  - `discover_descr_from_pyobject` (uses `default_descr` instead)
  - `common_instance` (uses `default_descr` instead)
  - `ensure_canonical` (uses `default_descr` instead).

Sorting is expected to be implemented using:

  - `get_sort_function(self, NPY_SORTKIND sort_kind) -> {out_sortfunction, NotImplemented, NULL}`.

For convenience, it will be sufficient if the user implements only:

  - `compare(self, char *item_ptr1, char *item_ptr2, int *res) -> {-1, 0, 1}`

**Limitations:** The `PyArrayDTypeMeta_Spec` struct is clumsy to extend (for instance, by adding a version tag to the `slots` to indicate a new, longer version). We could use a function to provide the struct; it would require memory management but would allow ABI-compatible extension (the struct is freed again when the DType is created).

### CastingImpl

The external API for `CastingImpl` will be limited initially to defining:

  - `casting` attribute, which can be one of the supported casting kinds. This is the safest cast possible. For example, casting between two NumPy strings is of course "safe" in general, but may be "same kind" in a specific instance if the second string is shorter. If neither type is parametric the `resolve_descriptors` must use it.
  - `resolve_descriptors(PyArrayMethodObject *self, PyArray_DTypeMeta *DTypes[2], PyArray_Descr *dtypes_in[2], PyArray_Descr *dtypes_out[2], NPY_CASTING *casting_out) -> int {0, -1}` The out dtypes must be set correctly to dtypes which the strided loop (transfer function) can handle. Initially the result must have instances of the same DType class as the `CastingImpl` is defined for. The `casting` will be set to `NPY_EQUIV_CASTING`, `NPY_SAFE_CASTING`, `NPY_UNSAFE_CASTING`, or `NPY_SAME_KIND_CASTING`. A new, additional flag, `_NPY_CAST_IS_VIEW`, can be set to indicate that no cast is necessary and a view is sufficient to perform the cast. The cast should return `-1` when an error occurred. If a cast is not possible (but no error occurred), a `-1` result should be returned *without* an error set. *This point is under consideration, we may use \`\`-1\`\` to indicate a general error, and use a different return value for an impossible cast.* This means that it is *not* possible to inform the user about why a cast is impossible.
  - `strided_loop(char **args, npy_intp *dimensions, npy_intp *strides, ...) -> int {0, -1}` (signature will be fully defined in \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>))

This is identical to the proposed API for ufuncs. The additional `...` part of the signature will include information such as the two `dtype`s. More optimized loops are in use internally, and will be made available to users in the future (see notes).

Although verbose, the API will mimic the one for creating a new DType:

`` `C     typedef struct{       int flags;                  /* e.g. whether the cast requires the API */       int nin, nout;              /* Number of Input and outputs (always 1) */       NPY_CASTING casting;        /* The "minimal casting level" */       PyArray_DTypeMeta *dtypes;  /* input and output DType class */       /* NULL terminated slots defining the methods */       PyType_Slot *slots;     } PyArrayMethod_Spec;  The focus differs between casting and general ufuncs.  For example, for casts ``<span class="title-ref"> </span><span class="title-ref">nin == nout == 1</span><span class="title-ref"> is always correct, while for ufuncs </span><span class="title-ref">casting</span><span class="title-ref"> is expected to be usually </span>"no"\`.

**Notes:** We may initially allow users to define only a single loop. Internally NumPy optimizes far more, and this should be made public incrementally in one of two ways:

  - Allow multiple versions, such as:
      - contiguous inner loop
      - strided inner loop
      - scalar inner loop
  - Or, more likely, expose the `get_loop` function which is passed additional information, such as the fixed strides (similar to our internal API).
  - The casting level denotes the minimal guaranteed casting level and can be `-1` if the cast may be impossible. For most non-parametric casts, this value will be the casting level. NumPy may skip the `resolve_descriptors` call for `np.can_cast()` when the result is `True` based on this level.

The example does not yet include setup and error handling. Since these are similar to the UFunc machinery, they will be defined in \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>) and then incorporated identically into casting.

The slots/methods used will be prefixed with `NPY_meth_`.

**Alternatives:**

  - Aside from name changes and signature tweaks, there seem to be few alternatives to the above structure. The proposed API using `*_FromSpec` function is a good way to achieve a stable and extensible API. The slots design is extensible and can be changed without breaking binary compatibility. Convenience functions can still be provided to allow creation with less code.
  - One downside is that compilers cannot warn about function-pointer incompatibilities.

## Implementation

Steps for implementation are outlined in the Implementation section of \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>). In brief, we first will rewrite the internals of casting and array coercion. After that, the new public API will be added incrementally. We plan to expose it in a preliminary state initially to gain experience. All functionality currently implemented on the dtypes will be replaced systematically as new features are added.

## Alternatives

The space of possible implementations is large, so there have been many discussions, conceptions, and design documents. These are listed in \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>). Alternatives were also been discussed in the relevant sections above.

## References

## Copyright

This document has been placed in the public domain.

1.  To be clear, the program is broken: It should not have stored a value in the common DType that was below the lowest int16 or above the highest uint16. It avoided overflow earlier by an accident of implementation. Nonetheless, we insist that program behavior not be altered just by importing a type.

2.  NumPy currently inspects the value to allow the operations:
    
    np.array(\[1\], dtype=np.uint8) + 1 np.array(\[1.2\], dtype=np.float32) + 1.
    
    to return a `uint8` or `float32` array respectively. This is further described in the documentation for <span class="title-ref">numpy.result\_type</span>.

---

nep-0043-extensible-ufuncs.md

---

# NEP 43 â€” Enhancing the extensibility of UFuncs

  - title  
    Enhancing the Extensibility of UFuncs

  - Author  
    Sebastian Berg

  - Status  
    Draft

  - Type  
    Standard

  - Created  
    2020-06-20

\> **Note** \> This NEP is fourth in a series:

>   - \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>) explains the shortcomings of NumPy's dtype implementation.
>   - \[NEP 41 \<NEP41\>\](\#nep-41-\<nep41\>) gives an overview of our proposed replacement.
>   - \[NEP 42 \<NEP42\>\](\#nep-42-\<nep42\>) describes the new design's datatype-related APIs.
>   - NEP 43 (this document) describes the new design's API for universal functions.

## Abstract

The previous NEP 42 proposes the creation of new DTypes which can be defined by users outside of NumPy itself. The implementation of NEP 42 will enable users to create arrays with a custom dtype and stored values. This NEP outlines how NumPy will operate on arrays with custom dtypes in the future. The most important functions operating on NumPy arrays are the so called "universal functions" (ufunc) which include all math functions, such as `np.add`, `np.multiply`, and even `np.matmul`. These ufuncs must operate efficiently on multiple arrays with different datatypes.

This NEP proposes to expand the design of ufuncs. It makes a new distinction between the ufunc which can operate on many different dtypes such as floats or integers, and a new `ArrayMethod` which defines the efficient operation for specific dtypes.

\> **Note** \> Details of the private and external APIs may change to reflect user comments and implementation constraints. The underlying principles and choices should not change significantly.

## Motivation and scope

The goal of this NEP is to extend universal functions support the new DType system detailed in NEPs 41 and 42. While the main motivation is enabling new user-defined DTypes, this will also significantly simplify defining universal functions for NumPy strings or structured DTypes. Until now, these DTypes are not supported by any of NumPy's functions (such as `np.add` or `np.equal`), due to difficulties arising from their parametric nature (compare NEP 41 and 42), e.g. the string length.

Functions on arrays must handle a number of distinct steps which are described in more detail in section "[Steps involved in a UFunc call]()". The most important ones are:

  - Organizing all functionality required to define a ufunc call for specific DTypes. This is often called the "inner-loop".
  - Deal with input for which no exact matching implementation is found. For example when `int32` and `float64` are added, the `int32` is cast to `float64`. This requires a distinct "promotion" step.

After organizing and defining these, we need to:

  - Define the user API for customizing both of the above points.
  - Allow convenient reuse of existing functionality. For example a DType representing physical units, such as meters, should be able to fall back to NumPy's existing math implementations.

This NEP details how these requirements will be achieved in NumPy:

  - All DTyper-specific functionality currently part of the ufunc definition will be defined as part of a new [ArrayMethod](#arraymethod) object. This `ArrayMethod` object will be the new, preferred, way to describe any function operating on arrays.
  - Ufuncs will dispatch to the `ArrayMethod` and potentially use promotion to find the correct `ArrayMethod` to use. This will be described in the [Promotion and dispatching](#promotion-and-dispatching) section.

A new C-API will be outlined in each section. A future Python API is expected to be very similar and the C-API is presented in terms of Python code for readability.

The NEP proposes a large, but necessary, refactor of the NumPy ufunc internals. This modernization will not affect end users directly and is not only a necessary step for new DTypes, but in itself a maintenance effort which is expected to help with future improvements to the ufunc machinery.

While the most important restructure proposed is the new `ArrayMethod` object, the largest long-term consideration is the API choice for promotion and dispatching.

## Backwards compatibility

The general backwards compatibility issues have also been listed previously in NEP 41.

The vast majority of users should not see any changes beyond those typical for NumPy releases. There are three main users or use-cases impacted by the proposed changes:

1.  The Numba package uses direct access to the NumPy C-loops and modifies the NumPy ufunc struct directly for its own purposes.
2.  Astropy uses its own "type resolver", meaning that a default switch over from the existing type resolution to a new default Promoter requires care.
3.  It is currently possible to register loops for dtype *instances*. This is theoretically useful for structured dtypes and is a resolution step happening *after* the DType resolution step proposed here.

This NEP will try hard to maintain backward compatibility as much as possible. However, both of these projects have signaled willingness to adapt to breaking changes.

The main reason why NumPy will be able to provide backward compatibility is that:

  - Existing inner-loops can be wrapped, adding an indirection to the call but maintaining full backwards compatibility. The `get_loop` function can, in this case, search the existing inner-loop functions (which are stored on the ufunc directly) in order to maintain full compatibility even with potential direct structure access.
  - Legacy type resolvers can be called as a fallback (potentially caching the result). The resolver may need to be called twice (once for the DType resolution and once for the `resolve_descriptor` implementation).
  - The fallback to the legacy type resolver should in most cases handle loops defined for such structured dtype instances. This is because if there is no other `np.Void` implementation, the legacy fallback will retain the old behaviour at least initially.

The masked type resolvers specifically will *not* remain supported, but has no known users (including NumPy itself, which only uses the default version).

Further, no compatibility attempt will be made for *calling* as opposed to providing either the normal or the masked type resolver. As NumPy will use it only as a fallback. There are no known users of this (undocumented) possibility.

While the above changes potentially break some workflows, we believe that the long-term improvements vastly outweigh this. Further, packages such as astropy and Numba are capable of adapting so that end-users may need to update their libraries but not their code.

## Usage and impact

This NEP restructures how operations on NumPy arrays are defined both within NumPy and for external implementers. The NEP mainly concerns those who either extend ufuncs for custom DTypes or create custom ufuncs. It does not aim to finalize all potential use-cases, but rather restructure NumPy to be extensible and allow addressing new issues or feature requests as they arise.

### Overview and end user API

To give an overview of how this NEP proposes to structure ufuncs, the following describes the potential exposure of the proposed restructure to the end user.

Universal functions are much like a Python method defined on the DType of the array when considering a ufunc with only a single input:

    res = np.positive(arr)

could be implemented (conceptually) as:

    positive_impl = arr.dtype.positive
    res = positive_impl(arr)

However, unlike methods, `positive_impl` is not stored on the dtype itself. It is rather the implementation of `np.positive` for a specific DType. Current NumPy partially exposes this "choice of implementation" using the `dtype` (or more exact `signature`) attribute in universal functions, although these are rarely used:

    np.positive(arr, dtype=np.float64)

forces NumPy to use the `positive_impl` written specifically for the Float64 DType.

This NEP makes the distinction more explicit, by creating a new object to represent `positive_impl`:

    positive_impl = np.positive.resolve_impl((type(arr.dtype), None))
    # The `None` represents the output DType which is automatically chosen.

While the creation of a `positive_impl` object and the `resolve_impl` method is part of this NEP, the following code:

    res = positive_impl(arr)

may not be implemented initially and is not central to the redesign.

In general NumPy universal functions can take many inputs. This requires looking up the implementation by considering all of them and makes ufuncs "multi-methods" with respect to the input DTypes:

    add_impl = np.add.resolve_impl((type(arr1.dtype), type(arr2.dtype), None))

This NEP defines how `positive_impl` and `add_impl` will be represented as a new `ArrayMethod` which can be implemented outside of NumPy. Further, it defines how `resolve_impl` will implement and solve dispatching and promotion.

The reasons for this split may be more clear after reviewing the [Steps involved in a UFunc call]() section.

### Defining a new ufunc implementation

The following is a mock-up of how a new implementation, in this case to define string equality, will be added to a ufunc.

`` `python     class StringEquality(BoundArrayMethod):         nin = 1         nout = 1         # DTypes are stored on the BoundArrayMethod and not on the internal         # ArrayMethod, to reference cyles.         DTypes = (String, String, Bool)          def resolve_descriptors(self: ArrayMethod, DTypes, given_descrs):             """The strided loop supports all input string dtype instances             and always returns a boolean. (String is always native byte order.)              Defining this function is not necessary, since NumPy can provide             it by default.              The `self` argument here refers to the unbound array method, so             that DTypes are passed in explicitly.             """             assert isinstance(given_descrs[0], DTypes[0])             assert isinstance(given_descrs[1], DTypes[1])             assert given_descrs[2] is None or isinstance(given_descrs[2], DTypes[2])              out_descr = given_descrs[2]  # preserve input (e.g. metadata)             if given_descrs[2] is None:                 out_descr = DTypes[2]()              # The operation is always "no" casting (most ufuncs are)             return (given_descrs[0], given_descrs[1], out_descr), "no"          def strided_loop(context, dimensions, data, strides, innerloop_data):             """The 1-D strided loop, similar to those used in current ufuncs"""             # dimensions: Number of loop items and core dimensions             # data: Pointers to the array data.             # strides: strides to iterate all elements             n = dimensions[0]  # number of items to loop over             num_chars1 = context.descriptors[0].itemsize             num_chars2 = context.descriptors[1].itemsize              # C code using the above information to compare the strings in             # both arrays.  In particular, this loop requires the `num_chars1`             # and `num_chars2`.  Information which is currently not easily             # available.      np.equal.register_impl(StringEquality)     del StringEquality  # may be deleted.   This definition will be sufficient to create a new loop, and the ``<span class="title-ref"> structure allows for expansion in the future; something that is already required to implement casting within NumPy itself. We use </span><span class="title-ref">BoundArrayMethod</span><span class="title-ref"> and a </span><span class="title-ref">context</span>\` structure here. These are described and motivated in details later. Briefly:

  - `context` is a generalization of the `self` that Python passes to its methods.
  - `BoundArrayMethod` is equivalent to the Python distinction that `class.method` is a method, while `class().method` returns a "bound" method.

### Customizing dispatching and Promotion

Finding the correct implementation when `np.positive.resolve_impl()` is called is largely an implementation detail. But, in some cases it may be necessary to influence this process when no implementation matches the requested DTypes exactly:

`` `python     np.multiple.resolve_impl((Timedelta64, Int8, None))  will not have an exact match, because NumPy only has an implementation for ``<span class="title-ref"> multiplying </span><span class="title-ref">Timedelta64</span><span class="title-ref"> with </span><span class="title-ref">Int64</span>\`. In simple cases, NumPy will use a default promotion step to attempt to find the correct implementation, but to implement the above step, we will allow the following:

`` `python     def promote_timedelta_integer(ufunc, dtypes):         new_dtypes = (Timdelta64, Int64, dtypes[-1])         # Resolve again, using Int64:         return ufunc.resolve_impl(new_dtypes)      np.multiple.register_promoter(         (Timedelta64, Integer, None), promote_timedelta_integer)  Where ``Integer`is an abstract DType (compare NEP 42).   .. _steps_of_a_ufunc_call:  ****************************************************************************`\` Steps involved in a UFunc call \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

Before going into more detailed API choices, it is helpful to review the steps involved in a call to a universal function in NumPy.

A UFunc call is split into the following steps:

1.  Handle `__array_ufunc__` protocol:
      - For array-likes such as a Dask arrays, NumPy can defer the operation. This step is performed first, and unaffected by this NEP (compare \[NEP18\](\#nep18)).
2.  Promotion and dispatching
      - Given the DTypes of all inputs, find the correct implementation. E.g. an implementation for `float64`, `int64` or a user-defined DType.
      - When no exact implementation exists, *promotion* has to be performed. For example, adding a `float32` and a `float64` is implemented by first casting the `float32` to `float64`.
3.  Parametric `dtype` resolution:
      - In general, whenever an output DType is parametric the parameters have to be found (resolved).
      - For example, if a loop adds two strings, it is necessary to define the correct output (and possibly input) dtypes. `S5 + S4 -> S9`, while an `upper` function has the signature `S5 -> S5`.
      - When they are not parametric, a default implementation is provided which fills in the default dtype instances (ensuring for example native byte order).
4.  Preparing the iteration:
      - This step is largely handled by `NpyIter` internally (the iterator).
      - Allocate all outputs and temporary buffers necessary to perform casts. This *requires* the dtypes as resolved in step 3.
      - Find the best iteration order, which includes information to efficiently implement broadcasting. For example, adding a single value to an array repeats the same value.
5.  Setup and fetch the C-level function:
      - If necessary, allocate temporary working space.
      - Find the C-implemented, light weight, inner-loop function. Finding the inner-loop function can allow specialized implementations in the future. For example casting currently optimizes contiguous casts and reductions have optimizations that are currently handled inside the inner-loop function itself.
      - Signal whether the inner-loop requires the Python API or whether the GIL may be released (to allow threading).
      - Clear floating point exception flags.
6.  Perform the actual calculation:
      - Run the DType specific inner-loop function.
      - The inner-loop may require access to additional data, such as dtypes or additional data set in the previous step.
      - The inner-loop function may be called an undefined number of times.
7.  Finalize:
      - Free any temporary working space allocated in step 5.
      - Check for floating point exception flags.
      - Return the result.

The `ArrayMethod` provides a concept to group steps 3 to 6 and partially 7. However, implementers of a new ufunc or `ArrayMethod` usually do not need to customize the behaviour in steps 4 or 6 which NumPy can and does provide. For the `ArrayMethod` implementer, the central steps to customize are step 3 and step 5. These provide the custom inner-loop function and potentially inner-loop specific setup. Further customization is possible and anticipated as future extensions.

Step 2. is promotion and dispatching and will be restructured with new API to allow customization of the process where necessary.

Step 1 is listed for completeness and is unaffected by this NEP.

The following sketch provides an overview of step 2 to 6 with an emphasize of how dtypes are handled and which parts are customizable ("Registered") and which are handled by NumPy:

![](nep43-sketch.svg)

## ArrayMethod

A central proposal of this NEP is the creation of the `ArrayMethod` as an object describing each implementation specific to a given set of DTypes. We use the `class` syntax to describe the information required to create a new `ArrayMethod` object:

`` `python     class ArrayMethod:         name: str  # Name, mainly useful for debugging          # Casting safety information (almost always "safe", necessary to         # unify casting and universal functions)         casting: Casting = "no"          # More general flags:         flags: int          def resolve_descriptors(self,                 Tuple[DTypeMeta], Tuple[DType|None]: given_descrs) -> Casting, Tuple[DType]:             """Returns the safety of the operation (casting safety) and the             """             # A default implementation can be provided for non-parametric             # output dtypes.             raise NotImplementedError          @staticmethod         def get_loop(Context : context, strides, ...) -> strided_loop_function, flags:             """Returns the low-level C (strided inner-loop) function which             performs the actual operation.              This method may initially private, users will be able to provide             a set of optimized inner-loop functions instead:              * `strided_inner_loop`             * `contiguous_inner_loop`             * `unaligned_strided_loop`             * ...             """             raise NotImplementedError          @staticmethod         def strided_inner_loop(                 Context : context, data, dimensions, strides, innerloop_data):             """The inner-loop (equivalent to the current ufunc loop)             which is returned by the default `get_loop()` implementation."""             raise NotImplementedError  With ``Context``providing mostly static information about the function call:  .. code-block:: python      class Context:         # The ArrayMethod object itself:         ArrayMethod : method          # Information about the caller, e.g. the ufunc, such as `np.add`:         callable : caller = None         # The number of input arguments:         int : nin = 1         # The number of output arguments:         int : nout = 1         # The actual dtypes instances the inner-loop operates on:         Tuple[DType] : descriptors          # Any additional information required. In the future, this will         # generalize or duplicate things currently stored on the ufunc:         #  - The ufunc signature of generalized ufuncs         #  - The identity used for reductions  And``flags`stored properties, for whether:  * the`ArrayMethod`supports unaligned input and output arrays`\` \* the inner-loop function requires the Python API (GIL) \* NumPy has to check the floating point error CPU flags.

*Note: More information is expected to be added as necessary.*

### The call `Context`

The "context" object is analogous to Python's `self` that is passed to all methods. To understand why the "context" object is necessary and its internal structure, it is helpful to remember that a Python method can be written in the following way (see also the [documentation of \_\_get\_\_](https://docs.python.org/3.8/reference/datamodel.html#object.__get__)):

`` `python     class BoundMethod:         def __init__(self, instance, method):             self.instance = instance             self.method = method          def __call__(self, *args, **kwargs):             return self.method.function(self.instance, *args, **kwargs)       class Method:         def __init__(self, function):             self.function = function          def __get__(self, instance, owner=None):             assert instance is not None  # unsupported here             return BoundMethod(instance, self)               With which the following ``method1`and`method2`below, behave identically:  .. code-block:: python      def function(self):         print(self)      class MyClass:         def method1(self):             print(self)          method2 = Method(function)  And both will print the same result:  .. code-block:: python      >>> myinstance = MyClass()     >>> myinstance.method1()     <__main__.MyClass object at 0x7eff65436d00>     >>> myinstance.method2()     <__main__.MyClass object at 0x7eff65436d00>  Here`self.instance`would be all information passed on by`Context`.`<span class="title-ref"> The </span><span class="title-ref">Context</span>\` is a generalization and has to pass additional information:

  - Unlike a method which operates on a single class instance, the `ArrayMethod` operates on many input arrays and thus multiple dtypes.
  - The `__call__` of the `BoundMethod` above contains only a single call to a function. But an `ArrayMethod` has to call `resolve_descriptors` and later pass on that information to the inner-loop function.
  - A Python function has no state except that defined by its outer scope. Within C, `Context` is able to provide additional state if necessary.

Just as Python requires the distinction of a method and a bound method, NumPy will have a `BoundArrayMethod`. This stores all of the constant information that is part of the `Context`, such as:

  - the `DTypes`
  - the number of input and output arguments
  - the ufunc signature (specific to generalized ufuncs, compare \[NEP20\](\#nep20)).

Fortunately, most users and even ufunc implementers will not have to worry about these internal details; just like few Python users need to know about the `__get__` dunder method. The `Context` object or C-structure provides all necessary data to the fast C-functions and NumPy API creates the new `ArrayMethod` or `BoundArrayMethod` as required.

### ArrayMethod specifications

These specifications provide a minimal initial C-API, which shall be expanded in the future, for example to allow specialized inner-loops.

Briefly, NumPy currently relies on strided inner-loops and this will be the only allowed method of defining a ufunc initially. We expect the addition of a `setup` function or exposure of `get_loop` in the future.

UFuncs require the same information as casting, giving the following definitions (see also \[NEP 42 \<NEP42\>\](\#nep-42-\<nep42\>) `CastingImpl`):

  - A new structure to be passed to the resolve function and inner-loop:
    
    ``` c
    typedef struct {
        PyObject *caller;  /* The ufunc object */
        PyArrayMethodObject *method;
    
        int nin, nout;
    
        PyArray_DTypeMeta **dtypes;
        /* Operand descriptors, filled in by resolve_desciptors */
        PyArray_Descr **descriptors;
    
        void *reserved;  // For Potential in threading (Interpreter state)
    } PyArrayMethod_Context
    ```
    
    This structure may be appended to include additional information in future versions of NumPy and includes all constant loop metadata.
    
    We could version this structure, although it may be simpler to version the `ArrayMethod` itself.

  - Similar to casting, ufuncs may need to find the correct loop dtype or indicate that a loop is only capable of handling certain instances of the involved DTypes (e.g. only native byteorder). This is handled by a `resolve_descriptors` function (identical to the `resolve_descriptors` of `CastingImpl`):
    
    ``` c
    NPY_CASTING
    resolve_descriptors(
            PyArrayMethodObject *self,
            PyArray_DTypeMeta *dtypes,
            PyArray_Descr *given_dtypes[nin+nout],
            PyArray_Descr *loop_dtypes[nin+nout]);
    ```
    
    The function fills `loop_dtypes` based on the given `given_dtypes`. This requires filling in the descriptor of the output(s). Often also the input descriptor(s) have to be found, e.g. to ensure native byteorder when needed by the inner-loop.
    
    In most cases an `ArrayMethod` will have non-parametric output DTypes so that a default implementation can be provided.

  - An additional `void *user_data` will usually be typed to extend the existing `NpyAuxData *` struct:
    
    ``` c
    struct {
        NpyAuxData_FreeFunc *free;
        NpyAuxData_CloneFunc *clone;
        /* To allow for a bit of expansion without breaking the ABI */
       void *reserved[2];
    } NpyAuxData;
    ```
    
    This struct is currently mainly used for the NumPy internal casting machinery and as of now both `free` and `clone` must be provided, although this could be relaxed.
    
    Unlike NumPy casts, the vast majority of ufuncs currently do not require this additional scratch-space, but may need simple flagging capability for example for implementing warnings (see Error and Warning Handling below). To simplify this NumPy will pass a single zero initialized `npy_intp *` when `user_data` is not set. *Note that it would be possible to pass this as part of Context.*

  - The optional `get_loop` function will not be public initially, to avoid finalizing the API which requires design choices also with casting:
    
    `` `C       innerloop *       get_loop(           PyArrayMethod_Context *context,           int aligned, int move_references,           npy_intp *strides,           PyArray_StridedUnaryOp **out_loop,           NpyAuxData **innerloop_data,           NPY_ARRAYMETHOD_FLAGS *flags); ``NPY\_ARRAYMETHOD\_FLAGS`can indicate whether the Python API is required and floating point errors must be checked.`move\_references\`\` is used internally for NumPy casting at this time.

  - The inner-loop function:
    
    ``` c
    int inner_loop(PyArrayMethod_Context *context, ..., void *innerloop_data);
    ```
    
    Will have the identical signature to current inner-loops with the following changes:
    
      - A return value to indicate an error when returning `-1` instead of `0`. When returning `-1` a Python error must be set.
      - The new first argument `PyArrayMethod_Context *` is used to pass in potentially required information about the ufunc or descriptors in a convenient way.
      - The `void *innerloop_data` will be the `NpyAuxData **innerloop_data` as set by `get_loop`. If `get_loop` does not set `innerloop_data` an `npy_intp *` is passed instead (see [Error Handling](#error-handling) below for the motivation).
    
    *Note:* Since `get_loop` is expected to be private, the exact implementation of `innerloop_data` can be modified until final exposure.

Creation of a new `BoundArrayMethod` will use a `PyArrayMethod_FromSpec()` `` ` function.  A shorthand will allow direct registration to a ufunc using ``PyUFunc\_AddImplementationFromSpec()`.  The specification is expected to contain the following (this may extend in the future)::      typedef struct {         const char *name;  /* Generic name, mainly for debugging */         int nin, nout;         NPY_CASTING casting;         NPY_ARRAYMETHOD_FLAGS flags;         PyArray_DTypeMeta **dtypes;         PyType_Slot *slots;     } PyArrayMethod_Spec;  .. highlight:: python  Discussion and alternatives ===========================  The above split into an`ArrayMethod`and`Context`and the additional requirement of a`BoundArrayMethod`is a necessary split mirroring the implementation of methods and bound methods in Python.  One reason for this requirement is that it allows storing the`ArrayMethod`object in many cases without holding references to the`DTypes`which may be important if DTypes are created (and deleted) dynamically. (This is a complex topic, which does not have a complete solution in current Python, but the approach solves the issue with respect to casting.)  There seem to be no alternatives to this structure.  Separating the DType-specific steps from the general ufunc dispatching and promotion is absolutely necessary to allow future extension and flexibility. Furthermore, it allows unifying casting and ufuncs.  Since the structure of`ArrayMethod`and`BoundArrayMethod`will be opaque and can be extended, there are few long-term design implications aside from the choice of making them Python objects.`resolve\_descriptors`-----------------------  The`resolve\_descriptors`method is possibly the main innovation of this NEP and it is central also in the implementation of casting in NEP 42.  By ensuring that every`ArrayMethod`provides`resolve\_descriptors``we define a unified, clear API for step 3 in `Steps involved in a UFunc call`_. This step is required to allocate output arrays and has to happen before casting can be prepared.  While the returned casting-safety (``NPY\_CASTING`) will almost always be "no" for universal functions, including it has two big advantages:  *`-1`indicates that an error occurred. If a Python error is set, it will   be raised.  If no Python error is set this will be considered an "impossible"   cast and a custom error will be set. (This distinction is important for the`np.can\_cast()`function, which should raise the first one and return`False`in the second case, it is not noteworthy for typical ufuncs).   *This point is under consideration, we may use -1 to indicate   a general error, and use a different return value for an impossible cast.* * Returning the casting safety is central to NEP 42 for casting and   allows the unmodified use of`ArrayMethod`there. * There may be a future desire to implement fast but unsafe implementations.   For example for`int64 + int64 -\> int32`which is unsafe from a casting   perspective. Currently, this would use`int64 + int64 -\> int64`and then   cast to`int32`. An implementation that skips the cast would   have to signal that it effectively includes the "same-kind" cast and is   thus not considered "no".`get\_loop`method -------------------  Currently, NumPy ufuncs typically only provide a single strided loop, so that the`get\_loop`method may seem unnecessary. For this reason we plan for`get\_loop`to be a private function initially.  However,`get\_loop`is required for casting where specialized loops are used even beyond strided and contiguous loops. Thus, the`get\_loop`function must be a full replacement for the internal`PyArray\_GetDTypeTransferFunction`.  In the future,`get\_loop`may be made public or a new`setup`function be exposed to allow more control, for example to allow allocating working memory. Further, we could expand`get\_loop`and allow the`ArrayMethod`implementer to also control the outer iteration and not only the 1-D inner-loop.   Extending the inner-loop signature ----------------------------------  Extending the inner-loop signature is another central and necessary part of the NEP.  **Passing in the Context:**  Passing in the`Context`potentially allows for the future extension of the signature by adding new fields to the context struct. Furthermore it provides direct access to the dtype instances which the inner-loop operates on. This is necessary information for parametric dtypes since for example comparing two strings requires knowing the length of both strings. The`Context`can also hold potentially useful information such as the original`ufunc`, which can be helpful when reporting errors.  In principle passing in Context is not necessary, as all information could be included in`innerloop\_data`and set up in the`get\_loop`function. In this NEP we propose passing the struct to simplify creation of loops for parametric DTypes.  **Passing in user data:**  The current casting implementation uses the existing`NpyAuxData \*`to pass in additional data as defined by`get\_loop`. There are certainly alternatives to the use of this structure, but it provides a simple solution, which is already used in NumPy and public API.`NpyAyxData *\`\` is a light weight, allocated structure and since it already exists in NumPy for this purpose, it seems a natural choice. To simplify some use-cases (see "Error Handling" below), we will pass a \`\`npy\_intp*innerloop\_data = 0`instead when`innerloop\_data`is not provided.  *Note:* Since`get\_loop`is expected to be private initially we can gain experience with`innerloop\_data`before exposing it as public API.  **Return value:**  The return value to indicate an error is an important, but currently missing feature in NumPy. The error handling is further complicated by the way CPUs signal floating point errors. Both are discussed in the next section.  Error handling """"""""""""""  .. highlight:: c  We expect that future inner-loops will generally set Python errors as soon as an error is found. This is complicated when the inner-loop is run without locking the GIL.  In this case the function will have to lock the GIL, set the Python error and return`-1`to indicate an error occurred:::      int     inner_loop(PyArrayMethod_Context *context, ..., void *innerloop_data)     {         NPY_ALLOW_C_API_DEF          for (npy_intp i = 0; i < N; i++) {             /* calculation */              if (error_occurred) {                 NPY_ALLOW_C_API;                 PyErr_SetString(PyExc_ValueError,                     "Error occurred inside inner_loop.");                 NPY_DISABLE_C_API                 return -1;             }         }         return 0;     }  Floating point errors are special, since they require checking the hardware state which is too expensive if done within the inner-loop function itself. Thus, NumPy will handle these if flagged by the`ArrayMethod`. An`ArrayMethod`should never cause floating point error flags to be set if it flags that these should not be checked. This could interfere when calling multiple functions; in particular when casting is necessary.  An alternative solution would be to allow setting the error only at the later finalization step when NumPy will also check the floating point error flags.  We decided against this pattern at this time. It seems more complex and generally unnecessary. While safely grabbing the GIL in the loop may require passing in an additional`PyThreadState`or`PyInterpreterState`in the future (for subinterpreter support), this is acceptable and can be anticipated. Setting the error at a later point would add complexity: for instance if an operation is paused (which can currently happen for casting in particular), the error check needs to run explicitly ever time this happens.  We expect that setting errors immediately is the easiest and most convenient solution and more complex solution may be possible future extensions.  Handling *warnings* is slightly more complex: A warning should be given exactly once for each function call (i.e. for the whole array) even if naively it would be given many times. To simplify such a use case, we will pass in`npy\_intp \*innerloop\_data = 0`by default which can be used to store flags (or other simple persistent data). For instance, we could imagine an integer multiplication loop which warns when an overflow occurred::      int     integer_multiply(PyArrayMethod_Context *context, ..., npy_intp *innerloop_data)     {         int overflow;         NPY_ALLOW_C_API_DEF          for (npy_intp i = 0; i < N; i++) {             *out = multiply_integers(*in1, *in2, &overflow);              if (overflow && !*innerloop_data) {                 NPY_ALLOW_C_API;                 if (PyErr_Warn(PyExc_UserWarning,                         "Integer overflow detected.") < 0) {                     NPY_DISABLE_C_API                     return -1;                 }                 *innerloop_data = 1;                 NPY_DISABLE_C_API         }         return 0;     }  *TODO:* The idea of passing an`npy\_intp`scratch space when`innerloop\_data`is not set seems convenient, but I am uncertain about it, since I am not aware of any similar prior art.  This "scratch space" could also be part of the`context`in principle.  .. highlight:: python  Reusing existing loops/implementations ======================================  For many DTypes the above definition for adding additional C-level loops will be sufficient and require no more than a single strided loop implementation and if the loop works with parametric DTypes, the`resolve\_descriptors`function *must* additionally be provided.  However, in some use-cases it is desirable to call back to an existing implementation. In Python, this could be achieved by simply calling into the original ufunc.  For better performance in C, and for large arrays, it is desirable to reuse an existing`ArrayMethod`as directly as possible, so that its inner-loop function can be used directly without additional overhead. We will thus allow to create a new, wrapping,`ArrayMethod`from an existing`ArrayMethod`.  This wrapped`ArrayMethod`will have two additional methods:  *`view\_inputs(Tuple\[DType\]: input\_descr) -\> Tuple\[DType\]`replacing the   user input descriptors with descriptors matching the wrapped loop.   It must be possible to *view* the inputs as the output.   For example for`Unit\[Float64\]("m") + Unit\[Float32\]("km")`this will   return`float64 + int32`. The original`resolve\_descriptors`will   convert this to`float64 + float64`.  *`wrap\_outputs(Tuple\[DType\]: input\_descr) -\> Tuple\[DType\]`replacing the   resolved descriptors with the desired actual loop descriptors.   The original`resolve\_descriptors`function will be called between these   two calls, so that the output descriptors may not be set in the first call.   In the above example it will use the`float64`as returned (which might   have changed the byte-order), and further resolve the physical unit making   the final signature::        Unit[Float64]("m") + Unit[Float64]("m") -> Unit[Float64]("m")    the UFunc machinery will take care of casting the "km" input to "m".   The`view\_inputs`method allows passing the correct inputs into the original`resolve\_descriptors`function, while`wrap\_outputs`ensures the correct descriptors are used for output allocation and input buffering casts.  An important use-case for this is that of an abstract Unit DType with subclasses for each numeric dtype (which could be dynamically created)::      Unit[Float64]("m")     # with Unit[Float64] being the concrete DType:     isinstance(Unit[Float64], Unit)  # is True  Such a`Unit\[Float64\]("m")`instance has a well-defined signature with respect to type promotion. The author of the`Unit`DType can implement most necessary logic by wrapping the existing math functions and using the two additional methods above. Using the *promotion* step, this will allow to create a register a single promoter for the abstract`Unit`DType with the`ufunc`. The promoter can then add the wrapped concrete`ArrayMethod`dynamically at promotion time, and NumPy can cache (or store it) after the first call.  **Alternative use-case:**  A different use-case is that of a`Unit(float64, "m")`DType, where the numerical type is part of the DType parameter. This approach is possible, but will require a custom`ArrayMethod`which wraps existing loops. It must also always require two steps of dispatching (one to the`Unit`DType and a second one for the numerical type).  Furthermore, the efficient implementation will require the ability to fetch and reuse the inner-loop function from another`ArrayMethod`. (Which is probably necessary for users like Numba, but it is uncertain whether it should be a common pattern and it cannot be accessible from Python itself.)   .. _promotion_and_dispatching:  ************************* Promotion and dispatching *************************  NumPy ufuncs are multi-methods in the sense that they operate on (or with) multiple DTypes at once. While the input (and output) dtypes are attached to NumPy arrays, the`ndarray`type itself does not carry the information of which function to apply to the data.  For example, given the input::      int_arr = np.array([1, 2, 3], dtype=np.int64)     float_arr = np.array([1, 2, 3], dtype=np.float64)     np.add(int_arr, float_arr)  has to find the correct`ArrayMethod`to perform the operation. Ideally, there is an exact match defined, e.g. for`np.add(int\_arr, int\_arr)`the`ArrayMethod\[Int64, Int64, out=Int64\]`matches exactly and can be used. However, for`np.add(int\_arr, float\_arr)`there is no direct match, requiring a promotion step.  Promotion and dispatching process =================================  In general the`ArrayMethod`is found by searching for an exact match of all input DTypes. The output dtypes should *not* affect calculation, but if multiple registered`ArrayMethod`\ s match exactly, the output DType will be used to find the better match. This will allow the current distinction for`np.equal`loops which define both`Object, Object -\> Bool`(default) and`Object, Object -\> Object`.  Initially, an`ArrayMethod`will be defined for *concrete* DTypes only and since these cannot be subclassed an exact match is guaranteed. In the future we expect that`ArrayMethod`\ s can also be defined for *abstract* DTypes. In which case the best match is found as detailed below.  **Promotion:**  If a matching`ArrayMethod`exists, dispatching is straight forward. However, when it does not, additional definitions are required to implement this "promotion":  * By default any UFunc has a promotion which uses the common DType of all   inputs and dispatches a second time.  This is well-defined for most   mathematical functions, but can be disabled or customized if necessary.   For instances`int32 + float64`tries again using`float64 + float64`which is the common DType.  * Users can *register* new Promoters just as they can register a   new`ArrayMethod`.  These will use abstract DTypes to allow matching   a large variety of signatures.   The return value of a promotion function shall be a new`ArrayMethod`or`NotImplemented`.  It must be consistent over multiple calls with   the same input to allow caching of the result.  The signature of a promotion function would be::      promoter(np.ufunc: ufunc, Tuple[DTypeMeta]: DTypes): -> Union[ArrayMethod, NotImplemented]  Note that DTypes may include the output's DType, however, normally the output DType will *not* affect which`ArrayMethod`is chosen.  In most cases, it should not be necessary to add a custom promotion function. An example which requires this is multiplication with a unit: in NumPy`timedelta64`can be multiplied with most integers, but NumPy only defines a loop (`ArrayMethod`) for`timedelta64 \* int64`so that multiplying with`int32`would fail.  To allow this, the following promoter can be registered for`(Timedelta64, Integral, None)`::      def promote(ufunc, DTypes):         res = list(DTypes)         try:             res[1] = np.common_dtype(DTypes[1], Int64)         except TypeError:             return NotImplemented          # Could check that res[1] is actually Int64         return ufunc.resolve_impl(tuple(res))  In this case, just as a`Timedelta64 \* int64`and`int64 \* timedelta64`  `ArrayMethod`is necessary, a second promoter will have to be registered to handle the case where the integer is passed first.  **Dispatching rules for ArrayMethod and Promoters:**  Promoter and`ArrayMethod`are discovered by finding the best match as defined by the DType class hierarchy. The best match is defined if:  * The signature matches for all input DTypes, so that`issubclass(input\_DType, registered\_DType)`returns true. * No other promoter or`ArrayMethod`is more precise in any input:`issubclass(other\_DType, this\_DType)`is true (this may include if both   are identical). * This promoter or`ArrayMethod`is more precise in at least one input or   output DType.  It will be an error if`NotImplemented`is returned or if two promoters match the input equally well. When an existing promoter is not precise enough for new functionality, a new promoter has to be added. To ensure that this promoter takes precedence it may be necessary to define new abstract DTypes as more precise subclasses of existing ones.  The above rules enable specialization if an output is supplied or the full loop is specified.  This should not typically be necessary, but allows resolving`np.logic\_or`, etc. which have both`Object, Object -\> Bool`and`Object, Object -\> Object`loops (using the first by default).   Discussion and alternatives ===========================  Instead of resolving and returning a new implementation, we could also return a new set of DTypes to use for dispatching.  This works, however, it has the disadvantage that it is impossible to dispatch to a loop defined on a different ufunc or to dynamically create a new`ArrayMethod`.   **Rejected Alternatives:**  In the above the promoters use a multiple dispatching style type resolution while the current UFunc machinery uses the first "safe" loop (see also [NEP 40 <NEP40>](#nep-40-<nep40>)) in an ordered hierarchy.  While the "safe" casting rule is not restrictive enough, we could imagine using a new "promote" casting rule, or the common-DType logic to find the best matching loop by upcasting the inputs as necessary.  One downside to this approach is that upcasting alone allows upcasting the result beyond what is expected by users: Currently (which will remain supported as a fallback) any ufunc which defines only a float64 loop will also work for float16 and float32 by *upcasting*::      >>> from scipy.special import erf     >>> erf(np.array([4.], dtype=np.float16))  # float16     array([1.], dtype=float32)  with a float32 result.  It is impossible to change the`erf`function to return a float16 result without changing the result of following code. In general, we argue that automatic upcasting should not occur in cases where a less precise loop can be defined, *unless* the ufunc author does this intentionally using a promotion.  This consideration means that upcasting has to be limited by some additional method.  *Alternative 1:*  Assuming general upcasting is not intended, a rule must be defined to limit upcasting the input from`float16 -\> float32`either using generic logic on the DTypes or the UFunc itself (or a combination of both). The UFunc cannot do this easily on its own, since it cannot know all possible DTypes which register loops. Consider the two examples:  First (should be rejected):  * Input:`float16 \* float16`* Existing loop:`float32 \* float32`Second (should be accepted):  * Input:`timedelta64 \* int32`* Existing loop:`timedelta64 \* int16`This requires either:  1. The`timedelta64`to somehow signal that the`int64`upcast is    always supported if it is involved in the operation. 2. The`float32 \* float32`loop to reject upcasting.  Implementing the first approach requires signaling that upcasts are acceptable in the specific context.  This would require additional hooks and may not be simple for complex DTypes.  For the second approach in most cases a simple`np.common\_dtype`rule will work for initial dispatching, however, even this is only clearly the case for homogeneous loops. This option will require adding a function to check whether the input is a valid upcast to each loop individually, which seems problematic. In many cases a default could be provided (homogeneous signature).  *Alternative 2:*  An alternative "promotion" step is to ensure that the *output* DType matches with the loop after first finding the correct output DType. If the output DTypes are known, finding a safe loop becomes easy. In the majority of cases this works, the correct output dtype is just::      np.common_dtype(*input_DTypes)  or some fixed DType (e.g. Bool for logical functions).  However, it fails for example in the`timedelta64 \* int32`case above since there is a-priori no way to know that the "expected" result type of this output is indeed`timedelta64`(`np.common\_dtype(Datetime64, Int32)`fails). This requires some additional knowledge of the timedelta64 precision being int64. Since a ufunc can have an arbitrary number of (relevant) inputs it would thus at least require an additional`\_\_promoted\_dtypes\_\_`method on`Datetime64`(and all DTypes).  A further limitation is shown by masked DTypes.  Logical functions do not have a boolean result when masked are involved, which would thus require the original ufunc author to anticipate masked DTypes in this scheme. Similarly, some functions defined for complex values will return real numbers while others return complex numbers.  If the original author did not anticipate complex numbers, the promotion may be incorrect for a later added complex loop.   We believe that promoters, while allowing for an huge theoretical complexity, are the best solution:  1. Promotion allows for dynamically adding new loops. E.g. it is possible    to define an abstract Unit DType, which dynamically creates classes to    wrap other existing DTypes.  Using a single promoter, this DType can    dynamically wrap existing`ArrayMethod`enabling it to find the correct    loop in a single lookup instead of two. 2. The promotion logic will usually err on the safe side: A newly-added    loop cannot be misused unless a promoter is added as well. 3. They put the burden of carefully thinking of whether the logic is correct    on the programmer adding new loops to a UFunc.  (Compared to Alternative 2) 4. In case of incorrect existing promotion, writing a promoter to restrict    or refine a generic rule is possible.  In general a promotion rule should    never return an *incorrect* promotion, but if it the existing promotion    logic fails or is incorrect for a newly-added loop, the loop can add a    new promoter to refine the logic.  The option of having each loop verify that no upcast occurred is probably the best alternative, but does not include the ability to dynamically adding new loops.  The main downsides of general promoters is that they allow a possible very large complexity. A third-party library *could* add incorrect promotions to NumPy, however, this is already possible by adding new incorrect loops. In general we believe we can rely on downstream projects to use this power and complexity carefully and responsibly.   *************** User guidelines ***************  In general adding a promoter to a UFunc must be done very carefully. A promoter should never affect loops which can be reasonably defined by other datatypes.  Defining a hypothetical`erf(UnitFloat16)`loop must not lead to`erf(float16)`. In general a promoter should fulfill the following requirements:  * Be conservative when defining a new promotion rule. An incorrect result   is a much more dangerous error than an unexpected error. * One of the (abstract) DTypes added should typically match specifically with a   DType (or family of DTypes) defined by your project.   Never add promotion rules which go beyond normal common DType rules!   It is *not* reasonable to add a loop for`int16 + uint16 -\> int24`if   you write an`int24`dtype. The result of this operation was already   defined previously as`int32`and will be used with this assumption. * A promoter (or loop) should never affect existing loop results.   This includes adding faster but less precise loops/promoters to replace   existing ones. * Try to stay within a clear, linear hierarchy for all promotion (and casting)   related logic. NumPy itself breaks this logic for integers and floats   (they are not strictly linear, since int64 cannot promote to float32). * Loops and promoters can be added by any project, which could be:    * The project defining the ufunc   * The project defining the DType   * A third-party project    Try to find out which is the best project to add the loop.  If neither   the project defining the ufunc nor the project defining the DType add the   loop, issues with multiple definitions (which are rejected) may arise   and care should be taken that the loop behaviour is always more desirable   than an error.  In some cases exceptions to these rules may make sense, however, in general we ask you to use extreme caution and when in doubt create a new UFunc instead.  This clearly notifies the users of differing rules. When in doubt, ask on the NumPy mailing list or issue tracker!   ************** Implementation **************  Implementation of this NEP will entail a large refactor and restructuring of the current ufunc machinery (as well as casting).  The implementation unfortunately will require large maintenance of the UFunc machinery, since both the actual UFunc loop calls, as well as the initial dispatching steps have to be modified.  In general, the correct`ArrayMethod\`\`, also those returned by a promoter, will be cached (or stored) inside a hashtable for efficient lookup.

## Discussion

There is a large space of possible implementations with many discussions in various places, as well as initial thoughts and design documents. These are listed in the discussion of \[NEP 40 \<NEP40\>\](\#nep-40-\<nep40\>) and not repeated here for brevity.

A long discussion which touches many of these points and points towards similar solutions can be found in [the github issue 12518 "What should be the calling convention for ufunc inner loop signatures?"](https://github.com/numpy/numpy/issues/12518)

## References

Please see NEP 40 and 41 for more discussion and references.

## Copyright

This document has been placed in the public domain.

---

nep-0044-restructuring-numpy-docs.md

---

# NEP 44 â€” Restructuring the NumPy documentation

  - Author  
    Ralf Gommers

  - Author  
    Melissa MendonÃ§a

  - Author  
    Mars Lee

  - Status  
    Accepted

  - Type  
    Process

  - Created  
    2020-02-11

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2020-March/080467.html>

## Abstract

This document proposes a restructuring of the NumPy Documentation, both in form and content, with the goal of making it more organized and discoverable for beginners and experienced users.

## Motivation and scope

See [here](https://numpy.org/devdocs/) for the front page of the latest docs. The organization is quite confusing and illogical (e.g. user and developer docs are mixed). We propose the following:

  - Reorganizing the docs into the four categories mentioned in\[1\], namely *Tutorials*, *How Tos*, *Reference Guide* and *Explanations* (more about this below).
  - Creating dedicated sections for Tutorials and How-Tos, including orientation on how to create new content;
  - Adding an Explanations section for key concepts and techniques that require deeper descriptions, some of which will be rearranged from the Reference Guide.

## Usage and impact

The documentation is a fundamental part of any software project, especially open source projects. In the case of NumPy, many beginners might feel demotivated by the current structure of the documentation, since it is difficult to discover what to learn (unless the user has a clear view of what to look for in the Reference docs, which is not always the case).

Looking at the results of a "NumPy Tutorial" search on any search engine also gives an idea of the demand for this kind of content. Having official high-level documentation written using up-to-date content and techniques will certainly mean more users (and developers/contributors) are involved in the NumPy community.

## Backward compatibility

The restructuring will effectively demand a complete rewrite of links and some of the current content. Input from the community will be useful for identifying key links and pages that should not be broken.

## Detailed description

As discussed in the article\[2\], there are four categories of doc content:

  - Tutorials
  - How-to guides
  - Explanations
  - Reference guide

We propose to use those categories as the ones we use (for writing and reviewing) whenever we add a new documentation section.

The reasoning for this is that it is clearer both for developers/documentation writers and to users where each piece of information should go, and the scope and tone of each document. For example, if explanations are mixed with basic tutorials, beginners might be overwhelmed and alienated. On the other hand, if the reference guide contains basic how-tos, it might be difficult for experienced users to find the information they need, quickly.

Currently, there are many blogs and tutorials on the internet about NumPy or using NumPy. One of the issues with this is that if users search for this information they may end up in an outdated (unofficial) tutorial before they find the current official documentation. This can be especially confusing, especially for beginners. Having a better infrastructure for the documentation also aims to solve this problem by giving users high-level, up-to-date official documentation that can be easily updated.

### Status and ideas of each type of doc content

#### Reference guide

NumPy has a quite complete reference guide. All functions are documented, most have examples, and most are cross-linked well with *See Also* sections. Further improving the reference guide is incremental work that can be done (and is being done) by many people. There are, however, many explanations in the reference guide. These can be moved to a more dedicated Explanations section on the docs.

#### How-to guides

NumPy does not have many how-to's. The subclassing and array ducktyping section may be an example of a how-to. Others that could be added are:

  - Parallelization (controlling BLAS multithreading with `threadpoolctl`, using multiprocessing, random number generation, etc.)
  - Storing and loading data (`.npy`/`.npz` format, text formats, Zarr, HDF5, Bloscpack, etc.)
  - Performance (memory layout, profiling, use with Numba, Cython, or Pythran)
  - Writing generic code that works with NumPy, Dask, CuPy, pydata/sparse, etc.

#### Explanations

There is a reasonable amount of content on fundamental NumPy concepts such as indexing, vectorization, broadcasting, (g)ufuncs, and dtypes. This could be organized better and clarified to ensure it's really about explaining the concepts and not mixed with tutorial or how-to like content.

There are few explanations about anything other than those fundamental NumPy concepts.

Some examples of concepts that could be expanded:

  - Copies vs. Views;
  - BLAS and other linear algebra libraries;
  - Fancy indexing.

In addition, there are many explanations in the Reference Guide, which should be moved to this new dedicated Explanations section.

#### Tutorials

There's a lot of scope for writing better tutorials. We have a new *NumPy for absolute beginners tutorial*\[3\] (GSoD project of Anne Bonner). In addition we need a number of tutorials addressing different levels of experience with Python and NumPy. This could be done using engaging data sets, ideas or stories. For example, curve fitting with polynomials and functions in `numpy.linalg` could be done with the Keeling curve (decades worth of CO2 concentration in air measurements) rather than with synthetic random data.

Ideas for tutorials (these capture the types of things that make sense, they're not necessarily the exact topics we propose to implement):

  - Conway's game of life with only NumPy (note: already in [Nicolas Rougier's book](https://www.labri.fr/perso/nrougier/from-python-to-numpy/#the-game-of-life))
  - Using masked arrays to deal with missing data in time series measurements
  - Using Fourier transforms to analyze the Keeling curve data, and extrapolate it.
  - Geospatial data (e.g. lat/lon/time to create maps for every year via a stacked array, like [gridMet data](http://www.climatologylab.org/gridmet.html))
  - Using text data and dtypes (e.g. use speeches from different people, shape `(n_speech, n_sentences, n_words)`)

The *Preparing to Teach* document\[4\] from the Software Carpentry Instructor Training materials is a nice summary of how to write effective lesson plans (and tutorials would be very similar). In addition to adding new tutorials, we also propose a *How to write a tutorial* document, which would help users contribute new high-quality content to the documentation.

##### Data sets

Using interesting data in the NumPy docs requires giving all users access to that data, either inside NumPy or in a separate package. The former is not the best idea, since it's hard to do without increasing the size of NumPy significantly.

Whenever possible, documentation pages should use examples from the `scipy.datasets` package.

## Related work

Some examples of documentation organization in other projects:

  - [Documentation for Jupyter](https://jupyter.org/documentation)
  - [Documentation for Python](https://docs.python.org/3/)
  - [Documentation for TensorFlow](https://www.tensorflow.org/learn)

These projects make the intended audience for each part of the documentation more explicit, as well as previewing some of the content in each section.

## Implementation

Currently, the [documentation for NumPy](https://numpy.org/devdocs/) can be confusing, especially for beginners. Our proposal is to reorganize the docs in the following structure:

  -   - For users:
        
          - Absolute Beginners Tutorial
          - main Tutorials section
          - How Tos for common tasks with NumPy
          - Reference Guide (API Reference)
          - Explanations
          - F2Py Guide
          - Glossary

  -   - For developers/contributors:
        
          - Contributor's Guide
          - Under-the-hood docs
          - Building and extending the documentation
          - Benchmarking
          - NumPy Enhancement Proposals

  -   - Meta information
        
          - Reporting bugs
          - Release Notes
          - About NumPy
          - License

### Ideas for follow-up

Besides rewriting the current documentation to some extent, it would be ideal to have a technical infrastructure that would allow more contributions from the community. For example, if Jupyter Notebooks could be submitted as-is as tutorials or How-Tos, this might create more contributors and broaden the NumPy community.

Similarly, if people could download some of the documentation in Notebook format, this would certainly mean people would use less outdated material for learning NumPy.

It would also be interesting if the new structure for the documentation makes translations easier.

## Discussion

Discussion around this NEP can be found on the NumPy mailing list:

  - <https://mail.python.org/pipermail/numpy-discussion/2020-February/080419.html>

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  [DiÃ¡taxis - A systematic framework for technical documentation authoring](https://diataxis.fr/)

2.  [DiÃ¡taxis - A systematic framework for technical documentation authoring](https://diataxis.fr/)

3.  [NumPy for absolute beginners Tutorial](https://numpy.org/devdocs/user/absolute_beginners.html) by Anne Bonner

4.  [Preparing to Teach](https://carpentries.github.io/instructor-training/15-lesson-study/index.html) (from the [Software Carpentry](https://software-carpentry.org/) Instructor Training materials)

---

nep-0045-c_style_guide.md

---

# NEP 45 â€” C style guide

  - Author  
    Charles Harris \<<charlesr.harris@gmail.com>\>

  - Status  
    Active

  - Type  
    Process

  - Created  
    2012-02-26

  - Resolution  
    <https://github.com/numpy/numpy/issues/11911>

## Abstract

This document gives coding conventions for the C code comprising the C implementation of NumPy.

## Motivation and scope

The NumPy C coding conventions are based on Python [PEP 7 -- Style Guide for C Code](https://www.python.org/dev/peps/pep-0007) by Guido van Rossum with a few added strictures.

Because the NumPy conventions are very close to those in PEP 7, that PEP is used as a template with the NumPy additions and variations in the appropriate spots.

## Usage and impact

There are many C coding conventions and it must be emphasized that the primary goal of the NumPy conventions isn't to choose the "best," about which there is certain to be disagreement, but to achieve uniformity.

Two good reasons to break a particular rule:

1.  When applying the rule would make the code less readable, even for someone who is used to reading code that follows the rules.
2.  To be consistent with surrounding code that also breaks it (maybe for historic reasons) -- although this is also an opportunity to clean up someone else's mess.

## Backward compatibility

No impact.

## Detailed description

### C dialect

  - Use C99 (that is, the standard defined by ISO/IEC 9899:1999).

  - Don't use GCC extensions (for instance, don't write multi-line strings without trailing backslashes). Preferably break long strings up onto separate lines like so:
    
    ``` c
    "blah blah"
    "blah blah"
    ```
    
    This will work with MSVC, which otherwise chokes on very long strings.

  - All function declarations and definitions must use full prototypes (that is, specify the types of all arguments).

  - No compiler warnings with major compilers (gcc, VC++, a few others).

<div class="note">

<div class="title">

Note

</div>

NumPy still produces compiler warnings that need to be addressed.

</div>

### Code layout

  - Use 4-space indents and no tabs at all.

  - No line should be longer than 80 characters. If this and the previous rule together don't give you enough room to code, your code is too complicated -- consider using subroutines.

  - No line should end in whitespace. If you think you need significant trailing whitespace, think again; somebody's editor might delete it as a matter of routine.

  - Function definition style: function name in column 1, outermost curly braces in column 1, blank line after local variable declarations:
    
    ``` c
    static int
    extra_ivars(PyTypeObject *type, PyTypeObject *base)
    {
        int t_size = PyType_BASICSIZE(type);
        int b_size = PyType_BASICSIZE(base);
    
        assert(t_size >= b_size); /* type smaller than base! */
        ...
        return 1;
    }
    ```
    
    If the transition to C++ goes through it is possible that this form will be relaxed so that short class methods meant to be inlined can have the return type on the same line as the function name. However, that is yet to be determined.

  - Code structure: one space between keywords like `if`, `for` and the following left parenthesis; no spaces inside the parenthesis; braces around all `if` branches, and no statements on the same line as the `if`. They should be formatted as shown:
    
    ``` c
    if (mro != NULL) {
        one_line_statement;
    }
    else {
        ...
    }

    for (i = 0; i < n; i++) {
        one_line_statement;
    }

    while (isstuff) {
        dostuff;
    }

    do {
        stuff;
    } while (isstuff);

    switch (kind) {
        /* Boolean kind */
        case 'b':
            return 0;
        /* Unsigned int kind */
        case 'u':
            ...
        /* Anything else */
        default:
            return 3;
    }
    ```

  - The return statement should *not* get redundant parentheses:
    
    ``` c
    return Py_None; /* correct */
    return(Py_None); /* incorrect */
    ```

  - Function and macro call style: `foo(a, b, c)`, no space before the open paren, no spaces inside the parens, no spaces before commas, one space after each comma.

  - Always put spaces around the assignment, Boolean, and comparison operators. In expressions using a lot of operators, add spaces around the outermost (lowest priority) operators.

  - Breaking long lines: If you can, break after commas in the outermost argument list. Always indent continuation lines appropriately: :
    
    ``` c
    PyErr_SetString(PyExc_TypeError,
            "Oh dear, you messed up.");
    ```
    
    Here appropriately means at least a double indent (8 spaces). It isn't necessary to line everything up with the opening parenthesis of the function call.

  - When you break a long expression at a binary operator, the operator goes at the end of the previous line, for example: :
    
    ``` c
    if (type > tp_dictoffset != 0 &&
            base > tp_dictoffset == 0 &&
            type > tp_dictoffset == b_size &&
            (size_t)t_size == b_size + sizeof(PyObject *)) {
        return 0;
    }
    ```
    
    Note that the terms in the multi-line Boolean expression are indented so as to make the beginning of the code block clearly visible.

  - Put blank lines around functions, structure definitions, and major sections inside functions.

  - Comments go before the code they describe. Multi-line comments should be like so: :
    
    ``` c
    /*
     * This would be a long
     * explanatory comment.
     */
    ```
    
    Trailing comments should be used sparingly. Instead of :
    
    ``` c
    if (yes) { // Success!
    ```
    
    do :
    
    ``` c
    if (yes) {
        // Success!
    ```

  - All functions and global variables should be declared static when they aren't needed outside the current compilation unit.

  - Declare external functions and variables in a header file.

### Naming conventions

  - There has been no consistent prefix for NumPy public functions, but they all begin with a prefix of some sort, followed by an underscore, and are in camel case: `PyArray_DescrAlignConverter`, `NpyIter_GetIterNext`. In the future the names should be of the form `Npy*_PublicFunction`, where the star is something appropriate.
  - Public Macros should have a `NPY_` prefix and then use upper case, for example, `NPY_DOUBLE`.
  - Private functions should be lower case with underscores, for example: `array_real_get`. Single leading underscores should not be used, but some current function names violate that rule due to historical accident.

\> **Note** \> Functions whose names begin with a single underscore should be renamed at some point.

### Function documentation

NumPy doesn't have a C function documentation standard at this time, but needs one. Most NumPy functions are not documented in the code, and that should change. One possibility is Doxygen with a plugin so that the same NumPy style used for Python functions can also be used for documenting C functions, see the files in `doc/cdoc/`.

## Related work

Based on Van Rossum and Warsaw, `7`

## Discussion

<https://github.com/numpy/numpy/issues/11911> recommended that this proposal, which originated as `doc/C_STYLE_GUIDE.rst`, be turned into an NEP.

## Copyright

This document has been placed in the public domain.

---

nep-0046-sponsorship-guidelines.md

---

# NEP 46 â€” NumPy sponsorship guidelines

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Status  
    Active

  - Type  
    Process

  - Created  
    2020-12-27

  - Resolution  
    <https://mail.python.org/pipermail/numpy-discussion/2021-January/081424.html>

## Abstract

This NEP provides guidelines on how the NumPy project will acknowledge financial and in-kind support.

## Motivation and scope

In the past few years, the NumPy project has gotten significant financial support, as well as dedicated work time for maintainers to work on NumPy. There is a need to acknowledge that support - it's the right thing to do, it's helpful when looking for new funding, and funders and organizations expect or require it, Furthermore, having a clear policy for how NumPy acknowledges support is helpful when searching for new support. Finally, this policy may help set reasonable expectations for potential funders.

This NEP is aimed at both the NumPy community - who can use it as a guideline when looking for support on behalf of the project and when acknowledging existing support - and at past, current and prospective sponsors, who often want or need to know what they get in return for their support other than a healthier NumPy.

The scope of this proposal includes:

  - direct financial support, employers providing paid time for NumPy maintainers and regular contributors, and in-kind support such as free hardware resources or services,
  - where and how NumPy acknowledges support (e.g., logo placement on the website),
  - the amount and duration of support which leads to acknowledgement, and
  - who in the NumPy project is responsible for sponsorship related topics, and how to contact them.

## How NumPy will acknowledge support

There will be two different ways to acknowledge financial and in-kind support: one to recognize significant active support, and another one to recognize support received in the past and smaller amounts of support.

Entities who fall under "significant active supporter" we'll call Sponsor. The minimum level of support given to NumPy to be considered a Sponsor are:

  - $30,000/yr for unrestricted financial contributions (e.g., donations)
  - $60,000/yr for financial contributions for a particular purpose (e.g., grants)
  - $100,000/yr for in-kind contributions (e.g., time for employees to contribute)

We define support being active as:

  - for a one-off donation: it was received within the previous 12 months,
  - for recurring or financial or in-kind contributions: they should be ongoing.

After support moves from "active" to "inactive" status, the acknowledgement will be left in its place for at least another 6 months. If appropriate, the funding team can discuss opportunities for renewal with the sponsor. After those 6 months, acknowledgement may be moved to the historical overview. The exact timing of this move is at the discretion of the funding team, because there may be reasons to keep it in the more prominent place for longer.

The rationale for the above funding levels is that unrestricted financial contributions are typically the most valuable for the project, and the hardest to obtain. The opposite is true for in-kind contributions. The dollar value of the levels also reflect that NumPy's needs have grown to the point where we need multiple paid developers in order to effectively support our user base and continue to move the project forward. Financial support at or above these levels is needed to be able to make a significant difference.

Sponsors will get acknowledged through:

  - a small logo displayed on the front page of the NumPy website
  - prominent logo placement on <https://numpy.org/about/>
  - logos displayed in talks about NumPy by maintainers
  - announcements of the sponsorship on the NumPy mailing list and the numpy-team Twitter account

In addition to Sponsors, we already have the concept of Institutional Partner (defined in NumPy's [governance document](https://numpy.org/devdocs/dev/governance/index.html)), for entities who employ a NumPy maintainer and let them work on NumPy as part of their official duties. The governance document doesn't currently define a minimum amount of paid maintainer time needed to be considered for partnership. Therefore we propose that level here, roughly in line with the sponsorship levels:

  - 6 person-months/yr of paid work time for one or more NumPy maintainers or regular contributors to any NumPy team or activity

Institutional Partners get the same benefits as Sponsors, in addition to what is specified in the NumPy governance document.

Finally, a new page on the website (<https://numpy.org/funding/>, linked from the About page) will be added to acknowledge all current and previous sponsors, partners, and any other entities and individuals who provided $5,000 or more of financial or in-kind support. This page will include relevant details of support (dates, amounts, names, and purpose); no logos will be used on this page. Such support, if provided for a specific enhancements or fix, may be acknowledged in the appropriate release note snippet. The rationale for the $5,000 minimum level is to keep the amount of work maintaining the page reasonable; the level is the equivalent of, e.g., one GSoC or a person-week's worth of engineering time in a Western country, which seems like a reasonable lower limit.

## Implementation

The following content changes need to be made:

  - Add a section with small logos towards the bottom of the [numpy.org](https://numpy.org/) website.
  - Create a full list of historical and current support and deploy it to <https://numpy.org/funding>.
  - Update the NumPy governance document for changes to Institutional Partner eligibility requirements and benefits.
  - Update <https://numpy.org/about> with details on how to get in touch with the NumPy project about sponsorship related matters (see next section).

### NumPy Funding Team

At the moment NumPy has only one official body, the Steering Council, and no good way to get in touch with either that body or any person or group responsible for funding and sponsorship related matters. The way this is typically done now is to somehow find the personal email of a maintainer, and email them in private. There is a need to organize this more transparently - a potential sponsor isn't likely to inquire through the mailing list, nor is it easy for a potential sponsor to know if they're reaching out to the right person in private.

<https://numpy.org/about/> already says that NumPy has a "funding and grants" team. However that is not the case. We propose to organize this team, name team members on it, and add the names of those team members plus a dedicated email address for the team to the About page.

## Status before this proposal

### Acknowledgement of support

At the time of writing (Dec 2020), the logos of the four largest financial sponsors and two institutional partners are displayed on <https://numpy.org/about/>. The [Nature paper about NumPy](https://www.nature.com/articles/s41586-020-2649-2) mentions some early funding. No comprehensive list of received funding and in-kind support is published anywhere.

Decisions on which logos to list on the website have been made mostly by the website team. Decisions on which entities to recognize as Institutional Partner have been made by the NumPy Steering Council.

### NumPy governance, decision-making, and financial oversight

*This section is meant as context for the reader, to help put the rest of this NEP in perspective, and perhaps answer questions the reader has when reading this as a potential sponsor.*

NumPy has a formal governance structure defined in [this governance document](https://numpy.org/devdocs/dev/governance/index.html)). Decisions are made by consensus among all active participants in a discussion (typically on the mailing list), and if consensus cannot be reached then the Steering Council takes the decision (also by consensus).

NumPy is a sponsored project of NumFOCUS, a US-based 501(c)3 nonprofit. NumFOCUS administers NumPy funds, and ensures they are spent in accordance with its mission and nonprofit status. In practice, NumPy has a NumFOCUS subcommittee (with its members named in the NumPy governance document) who can authorize financial transactions. Those transactions, for example paying a contractor for a particular activity or deliverable, are decided on by the NumPy Steering Council.

## Alternatives

*Tiered sponsorship levels.* We considered using tiered sponsorship levels, and rejected this alternative because it would be more complex, and not necessarily communicate the right intent - the minimum levels are for us to determine how to acknowledge support that we receive, not a commercial value proposition. Entities typically will support NumPy because they rely on the project or want to help advance it, and not to get brand awareness through logo placement.

*Listing all donations*. Note that in the past we have received many smaller donations, mostly from individuals through NumFOCUS. It would be great to list all of those contributions, but given the way we receive information on those donations right now, that would be quite labor-intensive. If we manage to move to a more suitable platform, such as [Open Collective](https://opencollective.com/), in the future, we should reconsider listing all individual donations.

## Related work

Here we provide a few examples of how other projects handle sponsorship guidelines and acknowledgements.

*Scikit-learn* has a narrow banner with logos at the bottom of <https://scikit-learn.org>, and a list of present funding and past sponsors at <https://scikit-learn.org/stable/about.html#funding>. Plus a separate section "Infrastructure support" at the bottom of that same About page.

*Jupyter* has logos of sponsors and institutional partners in two sections on <https://jupyter.org/about>. Some subprojects have separate approaches, for example sponsors are listed (by using the [all-contributors](https://github.com/all-contributors/all-contributors) bot) in the README for [jupyterlab-git](https://github.com/jupyterlab/jupyterlab-git). For a discussion from Jan 2020 on that, see [here](https://discourse.jupyter.org/t/ideas-for-recognizing-developer-contributions-by-companies-institutes/3178).

*NumFOCUS* has a large banner with sponsor logos on its front page at <https://numfocus.org>, and a full page with sponsors at different sponsorship levels listed at <https://numfocus.org/sponsors>. They also have a [Corporate Sponsorship Prospectus](https://numfocus.org/blog/introducing-our-newest-corporate-sponsorship-prospectus), which includes a lot of detail on both sponsorship levels and benefits, as well as how that helps NumFOCUS-affiliated projects (including NumPy).

## Discussion

  - [Mailing list thread discussing this NEP](https://mail.python.org/pipermail/numpy-discussion/2020-December/081353.html)
  - [PR with review of the NEP draft](https://github.com/numpy/numpy/pull/18084)

## References and footnotes

  - [Inside NumPy: preparing for the next decade](https://github.com/numpy/archive/blob/main/content/inside_numpy_presentation_SciPy2019.pdf) presentation at SciPy'19 discussing the impact of the first NumPy grant.
  - [Issue](https://github.com/numpy/numpy/issues/13393) and [email](https://mail.python.org/pipermail/numpy-discussion/2019-April/079371.html) where IBM offered a $5,000 bounty for VSX SIMD support
  - [JupyterLab Corporate Engagement and Contribution Guide](https://github.com/jupyterlab/jupyterlab/blob/master/CORPORATE.md)

## Copyright

This document has been placed in the public domain.

---

nep-0047-array-api-standard.md

---

# NEP 47 â€” Adopting the array API standard

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Author  
    Stephan Hoyer \<<shoyer@gmail.com>\>

  - Author  
    Aaron Meurer \<<asmeurer@gmail.com>\>

  - Status  
    Superseded

  - Replaced-By  
    \[NEP56\](\#nep56)

  - Type  
    Standards Track

  - Created  
    2021-01-21

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/message/Z6AA5CL47NHBNEPTFWYOTSUVSRDGHYPN/>

\> **Note** \> This NEP was implemented and released under an experimental label (it emitted a warning on import) in NumPy 1.22.0-1.26.x. It was removed before NumPy 2.0.0, which is when NumPy gained support for the array API standard in its main namespace (see \[NEP56\](\#nep56)). The code for `numpy.array_api` was moved to a standalone package: [array-api-strict](https://github.com/data-apis/array-api-strict). For a detailed overview of the differences between the last version of the `numpy.array_api` module with `numpy`, see [this table in the 1.26.x docs](https://numpy.org/doc/1.26/reference/array_api.html#table-of-differences-between-numpy-array-api-and-numpy).

## Abstract

We propose to adopt the [Python array API standard](https://data-apis.github.io/array-api/latest), developed by the [Consortium for Python Data API Standards](https://data-apis.org/). Implementing this as a separate new namespace in NumPy will allow authors of libraries which depend on NumPy as well as end users to write code that is portable between NumPy and all other array/tensor libraries that adopt this standard.

\> **Note** \> We expect that this NEP will remain in a draft state for quite a while. Given the large scope we don't expect to propose it for acceptance any time soon; instead, we want to solicit feedback on both the high-level design and implementation, and learn what needs describing better in this NEP or changing in either the implementation or the array API standard itself.

## Motivation and scope

Python users have a wealth of choice for libraries and frameworks for numerical computing, data science, machine learning, and deep learning. New frameworks pushing forward the state of the art in these fields are appearing every year. One unintended consequence of all this activity and creativity has been fragmentation in multidimensional array (a.k.a. tensor) libraries -which are the fundamental data structure for these fields. Choices include NumPy, Tensorflow, PyTorch, Dask, JAX, CuPy, MXNet, and others.

The APIs of each of these libraries are largely similar, but with enough differences that itâ€™s quite difficult to write code that works with multiple (or all) of these libraries. The array API standard aims to address that issue, by specifying an API for the most common ways arrays are constructed and used. The proposed API is quite similar to NumPy's API, and deviates mainly in places where (a) NumPy made design choices that are inherently not portable to other implementations, and (b) where other libraries consistently deviated from NumPy on purpose because NumPy's design turned out to have issues or unnecessary complexity.

For a longer discussion on the purpose of the array API standard we refer to the [Purpose and Scope section of the array API standard](https://data-apis.github.io/array-api/latest/purpose_and_scope.html) and the two blog posts announcing the formation of the Consortium\[1\] and the release of the first draft version of the standard for community review\[2\].

The scope of this NEP includes:

  - Adopting the 2021 version of the array API standard
  - Adding a separate namespace, tentatively named `numpy.array_api`
  - Changes needed/desired outside of the new namespace, for example new dunder methods on the `ndarray` object
  - Implementation choices, and differences between functions in the new namespace with those in the main `numpy` namespace
  - A new array object conforming to the array API standard
  - Maintenance effort and testing strategy
  - Impact on NumPy's total exposed API surface and on other future and under-discussion design choices
  - Relation to existing and proposed NumPy array protocols (`__array_ufunc__`, `__array_function__`, `__array_module__`).
  - Required improvements to existing NumPy functionality

Out of scope for this NEP are:

  - Changes in the array API standard itself. Those are likely to come up during review of this NEP, but should be upstreamed as needed and this NEP subsequently updated.

## Usage and impact

*This section will be fleshed out later, for now we refer to the use cases given in* [the array API standard Use Cases section](https://data-apis.github.io/array-api/latest/use_cases.html)

In addition to those use cases, the new namespace contains functionality that is widely used and supported by many array libraries. As such, it is a good set of functions to teach to newcomers to NumPy and recommend as "best practice". That contrasts with NumPy's main namespace, which contains many functions and objects that have been superseded or we consider mistakes - but that we can't remove because of backwards compatibility reasons.

The usage of the `numpy.array_api` namespace by downstream libraries is intended to enable them to consume multiple kinds of arrays, *without having to have a hard dependency on all of those array libraries*:

![image](nep-0047-library-dependencies.png)

### Adoption in downstream libraries

The prototype implementation of the `array_api` namespace will be used with SciPy, scikit-learn, and other libraries of interest that depend on NumPy, in order to get more experience with the design and find out if any important parts are missing.

The pattern to support multiple array libraries is intended to be something like:

    def somefunc(x, y):
        # Retrieves standard namespace. Raises if x and y have different
        # namespaces.  See Appendix for possible get_namespace implementation
        xp = get_namespace(x, y)
        out = xp.mean(x, axis=0) + 2*xp.std(y, axis=0)
        return out

The `get_namespace` call is effectively the library author opting in to using the standard API namespace, and thereby explicitly supporting all conforming array libraries.

#### The `asarray` / `asanyarray` pattern

Many existing libraries use the same `asarray` (or `asanyarray`) pattern as NumPy itself does; accepting any object that can be coerced into a `np.ndarray`. We consider this design pattern problematic - keeping in mind the Zen of Python, *"explicit is better than implicit"*, as well as the pattern being historically problematic in the SciPy ecosystem for `ndarray` subclasses and with over-eager object creation. All other array/tensor libraries are more strict, and that works out fine in practice. We would advise authors of new libraries to avoid the `asarray` pattern. Instead they should either accept just NumPy arrays or, if they want to support multiple kinds of arrays, check if the incoming array object supports the array API standard by checking for `__array_namespace__` as shown in the example above.

Existing libraries can do such a check as well, and only call `asarray` if the check fails. This is very similar to the `__duckarray__` idea in \[NEP30\](\#nep30).

### Adoption in application code

The new namespace can be seen by end users as a cleaned up and slimmed down version of NumPy's main namespace. Encouraging end users to use this namespace like:

    import numpy.array_api as xp
    
    x = xp.linspace(0, 2*xp.pi, num=100)
    y = xp.cos(x)

seems perfectly reasonable, and potentially beneficial - users get offered only one function for each purpose (the one we consider best-practice), and they then write code that is more easily portable to other libraries.

## Backward compatibility

No deprecations or removals of existing NumPy APIs or other backwards incompatible changes are proposed.

## High-level design

The array API standard consists of approximately 120 objects, all of which have a direct NumPy equivalent. This figure shows what is included at a high level:

![image](nep-0047-scope-of-array-API.png)

The most important changes compared to what NumPy currently offers are:

  - A new array object, `numpy.array_api.Array` which:
    
    >   - is a thin pure Python (non-subclass) wrapper around `np.ndarray`,
    >   - conforms to the casting rules and indexing behavior specified by the standard,
    >   - does not have methods other than dunder methods,
    >   - does not support the full range of NumPy indexing behavior (see \[indexing\](\#indexing) below),
    >   - does not have distinct scalar objects, only 0-D arrays,
    >   - cannot be constructed directly. Instead array construction functions like `asarray()` should be used.

  - Functions in the `array_api` namespace:
    
    >   - do not accept `array_like` inputs, only `numpy.array_api` array objects, with Python scalars only being supported in dunder operators on the array object,
    >   - do not support `__array_ufunc__` and `__array_function__`,
    >   - use positional-only and keyword-only parameters in their signatures,
    >   - have inline type annotations,
    >   - may have minor changes to signatures and semantics of individual functions compared to their equivalents already present in NumPy,
    >   - only support dtype literals, not format strings or other ways of specifying dtypes,
    >   - generally may only support a restricted set of dtypes compared to their NumPy counterparts.

  - [DLPack](https://github.com/dmlc/dlpack) support will be added to NumPy,

  - New syntax for "device support" will be added, through a `.device` attribute on the new array object, and `device=` keywords in array creation functions in the `array_api` namespace,

  - Casting rules will differ from those NumPy currently has. Output dtypes can be derived from input dtypes (i.e. no value-based casting), and 0-D arrays are treated like \>=1-D arrays. Cross-kind casting (e.g., int to float) is not allowed.

  - Not all dtypes NumPy has are part of the standard. Only boolean, signed and unsigned integers, and floating-point dtypes up to `float64` are supported. Complex dtypes are expected to be added in the next version of the standard. Extended precision, string, void, object and datetime dtypes, as well as structured dtypes, are not included.

Improvements to existing NumPy functionality that are needed include:

  - Add support for stacks of matrices to some functions in `numpy.linalg` that are currently missing such support.
  - Add the `keepdims` keyword to `np.argmin` and `np.argmax`.
  - Add a "never copy" mode to `np.asarray`.
  - Add smallest\_normal to `np.finfo()`.
  - [DLPack](https://github.com/dmlc/dlpack) support.

Additionally, the `numpy.array_api` implementation was chosen to be a *minimal* implementation of the array API standard. This means that it not only conforms to all the requirements of the array API, but it explicitly does not include any APIs or behaviors not explicitly required by it. The standard itself does not require implementations to be so restrictive, but doing this with the NumPy array API implementation will allow it to become a canonical implementation of the array API standard. Anyone who wants to make use of the array API standard can use the NumPy implementation and be sure that their code is not making use of behaviors that will not be in other conforming implementations.

In particular, this means

  - `numpy.array_api` will only include those functions that are listed in the standard. This also applies to methods on the `Array` object,
  - Functions will only accept input dtypes that are required by the standard (e.g., transcendental functions like `cos` will not accept integer dtypes because the standard only requires them to accept floating-point dtypes),
  - Type promotion will only occur for combinations of dtypes required by the standard (see the \[dtypes-and-casting-rules\](\#dtypes-and-casting-rules) section below),
  - Indexing is limited to a subset of possible index types (see \[indexing\](\#indexing) below).

### Functions in the `array_api` namespace

Let's start with an example of a function implementation that shows the most important differences with the equivalent function in the main namespace:

    def matmul(x1: Array, x2: Array, /) -> Array:
        """
        Array API compatible wrapper for :py`np.matmul <numpy.matmul>`.
        See its docstring for more information.
        """
        if x1.dtype not in _numeric_dtypes or x2.dtype not in _numeric_dtypes:
            raise TypeError("Only numeric dtypes are allowed in matmul")
    
        # Call result type here just to raise on disallowed type combinations
        _result_type(x1.dtype, x2.dtype)
    
        return Array._new(np.matmul(x1._array, x2._array))

This function does not accept `array_like` inputs, only `numpy.array_api.Array`. There are multiple reasons for this. Other array libraries all work like this. Requiring the user to do coercion of Python scalars, lists, generators, or other foreign objects explicitly results in a cleaner design with less unexpected behavior. It is higher-performance---less overhead from `asarray` calls. Static typing is easier. Subclasses will work as expected. And the slight increase in verbosity because users have to explicitly coerce to `ndarray` on rare occasions seems like a small price to pay.

This function does not support `__array_ufunc__` nor `__array_function__`. These protocols serve a similar purpose as the array API standard module itself, but through a different mechanisms. Because only `Array` instances are accepted, dispatching via one of these protocols isn't useful anymore.

This function uses positional-only parameters in its signature. This makes code more portable---writing, for instance, `max(a=a, ...)` is no longer valid, hence if other libraries call the first parameter `input` rather than `a`, that is fine. Note that NumPy already uses positional-only arguments for functions that are ufuncs. The rationale for keyword-only parameters (not shown in the above example) is two-fold: clarity of end user code, and it being easier to extend the signature in the future without worrying about the order of keywords.

This function has inline type annotations. Inline annotations are far easier to maintain than separate stub files. And because the types are simple, this will not result in a large amount of clutter with type aliases or unions like in the current stub files NumPy has.

This function only accepts numeric dtypes (i.e., not `bool`). It also does not allow the input dtypes to be of different kinds (the internal `_result_type()` function will raise `TypeError` on cross-kind type combinations like `_result_type(int32, float64)`). This allows the implementation to be minimal. Preventing combinations that work in NumPy but are not required by the array API specification lets users of the submodule know they are not relying on NumPy specific behavior that may not be present in array API conforming implementations from other libraries.

### DLPack support for zero-copy data interchange

The ability to convert one kind of array into another kind is valuable, and indeed necessary when downstream libraries want to support multiple kinds of arrays. This requires a well-specified data exchange protocol. NumPy already supports two of these, namely the buffer protocol (i.e., PEP 3118), and the `__array_interface__` (Python side) / `__array_struct__` (C side) protocol. Both work similarly, letting the "producer" describe how the data is laid out in memory so the "consumer" can construct its own kind of array with a view on that data.

DLPack works in a very similar way. The main reasons to prefer DLPack over the options already present in NumPy are:

1.  DLPack is the only protocol with device support (e.g., GPUs using CUDA or ROCm drivers, or OpenCL devices). NumPy is CPU-only, but other array libraries are not. Having one protocol per device isn't tenable, hence device support is a must.
2.  Widespread support. DLPack has the widest adoption of all protocols. Only NumPy is missing support, and the experiences of other libraries with it are positive. This contrasts with the protocols NumPy does support, which are used very little---when other libraries want to interoperate with NumPy, they typically use the (more limited, and NumPy-specific) `__array__` protocol.

Adding support for DLPack to NumPy entails:

  - Adding a `ndarray.__dlpack__()` method which returns a `dlpack` C structure wrapped in a `PyCapsule`.
  - Adding a `np.from_dlpack(obj)` function, where `obj` supports `__dlpack__()`, and returns an `ndarray`.

DLPack is currently a \~200 LoC header, and is meant to be included directly, so no external dependency is needed. Implementation should be straightforward.

### Syntax for device support

NumPy itself is CPU-only, so it clearly doesn't have a need for device support. However, other libraries (e.g. TensorFlow, PyTorch, JAX, MXNet) support multiple types of devices: CPU, GPU, TPU, and more exotic hardware. To write portable code on systems with multiple devices, it's often necessary to create new arrays on the same device as some other array, or to check that two arrays live on the same device. Hence syntax for that is needed.

The array object will have a `.device` attribute which enables comparing devices of different arrays (they only should compare equal if both arrays are from the same library and it's the same hardware device). Furthermore, `device=` keywords in array creation functions are needed. For example:

    def empty(shape: Union[int, Tuple[int, ...]], /, *,
              dtype: Optional[dtype] = None,
              device: Optional[device] = None) -> Array:
        """
        Array API compatible wrapper for :py`np.empty <numpy.empty>`.
        """
        if device not in ["cpu", None]:
            raise ValueError(f"Unsupported device {device!r}")
        return Array._new(np.empty(shape, dtype=dtype))

The implementation for NumPy is as simple as setting the device attribute to the string `"cpu"` and raising an exception if array creation functions encounter any other value.

### Dtypes and casting rules

The supported dtypes in this namespace are boolean, 8/16/32/64-bit signed and unsigned integer, and 32/64-bit floating-point dtypes. These will be added to the namespace as dtype literals with the expected names (e.g., `bool`, `uint16`, `float64`).

The most obvious omissions are the complex dtypes. The rationale for the lack of complex support in the first version of the array API standard is that several libraries (PyTorch, MXNet) are still in the process of adding support for complex dtypes. The next version of the standard is expected to include `complex64` and `complex128` (see [this issue](https://github.com/data-apis/array-api/issues/102) for more details).

Specifying dtypes to functions, e.g. via the `dtype=` keyword, is expected to only use the dtype literals. Format strings, Python builtin dtypes, or string representations of the dtype literals are not accepted. This will improve readability and portability of code at little cost. Furthermore, no behavior is expected of these dtype literals themselves other than basic equality comparison. In particular, since the array API does not have scalar objects, syntax like `float32(0.0)` is not allowed (a 0-D array can be created with `asarray(0.0, dtype=float32)`).

Casting rules are only defined between different dtypes of the same kind (i.e., boolean to boolean, integer to integer, or floating-point to floating-point). This also means omitting integer-uint64 combinations that would upcast to float64 in NumPy. The rationale for this is that mixed-kind (e.g., integer to floating-point) casting behaviors differ between libraries.

![image](nep-0047-casting-rules-lattice.png)

*Type promotion diagram. Promotion between any two types is given by their join on this lattice. Only the types of participating arrays matter, not their values. Dashed lines indicate that behavior for Python scalars is undefined on overflow. The Python scalars themselves are only allowed in operators on the array object, not inside of functions. Boolean, integer and floating-point dtypes are not connected, indicating mixed-kind promotion is undefined (for the NumPy implementation, these raise an exception).*

The most important difference between the casting rules in NumPy and in the array API standard is how scalars and 0-dimensional arrays are handled. In the standard, array scalars do not exist and 0-dimensional arrays follow the same casting rules as higher-dimensional arrays. Furthermore, there is no value-based casting in the standard. The result type of an operation can be predicted entirely from its input arrays' dtypes, regardless of their shapes or values. Python scalars are only allowed in dunder operations (like `__add__`), and only if they are of the same kind as the array dtype. They always cast to the dtype of the array, regardless of value. Overflow behavior is undefined.

See the [Type Promotion Rules section of the array API standard](https://data-apis.github.io/array-api/latest/API_specification/type_promotion.html) for more details.

In the implementation, this means

  - Ensuring any operation that would produce an scalar object in NumPy is converted to a 0-D array in the `Array` constructor,
  - Checking for combinations that would apply value-based casting and ensuring they promote to the correct type. This can be achieved, e.g., by manually broadcasting 0-D inputs (preventing them from participating in value-based casting), or by explicitly passing the `signature` argument to the underlying ufunc,
  - In dunder operator methods, manually converting Python scalar inputs to 0-D arrays of the matching dtype if they are the same kind, and raising otherwise. For scalars out of bounds of the given dtype (for which the behavior is undefined by the spec), the behavior of `np.array(scalar, dtype=dtype)` is used (either cast or raise OverflowError).

### Indexing

An indexing expression that would return a scalar with `ndarray`, e.g. `arr_2d[0, 0]`, will return a 0-D array with the new `Array` object. There are several reasons for this: array scalars are largely considered a design mistake which no other array library copied; it works better for non-CPU libraries (typically arrays can live on the device, scalars live on the host); and it's simply a more consistent design. To get a Python scalar out of a 0-D array, one can use the builtin for the type, e.g. `float(arr_0d)`.

The other [indexing modes in the standard](https://data-apis.github.io/array-api/latest/API_specification/indexing.html) do work largely the same as they do for `numpy.ndarray`. One noteworthy difference is that clipping in slice indexing (e.g., `a[:n]` where `n` is larger than the size of the first axis) is unspecified behavior, because that kind of check can be expensive on accelerators.

The standard omits advanced indexing (indexing by an integer array), and boolean indexing is limited to a single n-D boolean array. This is due to those indexing modes not being suitable for all types of arrays or JIT compilation. Furthermore, some advanced NumPy indexing semantics, such as the semantics for mixing advanced and non-advanced indices in a single index, are considered design mistakes in NumPy. The absence of these more advanced index types does not seem to be problematic; if a user or library author wants to use them, they can do so through zero-copy conversion to `numpy.ndarray`. This will signal correctly to whomever reads the code that it is then NumPy-specific rather than portable to all conforming array types.

Being a minimal implementation, `numpy.array_api` will explicitly disallow slices with clipped bounds, advanced indexing, and boolean indices mixed with other indices.

### The array object

The array object in the standard does not have methods other than dunder methods. It also does not allow direct construction, preferring instead array construction methods like `asarray`. The rationale for that is that not all array libraries have methods on their array object (e.g., TensorFlow does not). It also provides only a single way of doing something, rather than have functions and methods that are effectively duplicate.

Mixing operations that may produce views (e.g., indexing, `nonzero`) in combination with mutation (e.g., item or slice assignment) is [explicitly documented in the standard to not be supported](https://data-apis.github.io/array-api/latest/design_topics/copies_views_and_mutation.html). This cannot easily be prohibited in the array object itself; instead this will be guidance to the user via documentation.

The standard current does not prescribe a name for the array object itself. We propose to name it `Array`. This uses proper PEP 8 capitalization for a class, and does not conflict with any existing NumPy class names.\[3\] Note that the actual name of the array class does not actually matter that much as it is not itself included in the top-level namespace, and cannot be directly constructed.

## Implementation

A prototype of the `array_api` namespace can be found in <https://github.com/numpy/numpy/pull/18585>. The docstring in its `__init__.py` has several important notes about implementation details. The code for the wrapper functions also contains `# Note:` comments everywhere there is a difference with the NumPy API. The implementation is entirely in pure Python, and consists primarily of wrapper classes/functions that pass through to the corresponding NumPy functions after applying input validation and any changed behavior. One important part that is not implemented yet is [DLPack](https://github.com/dmlc/dlpack) support, as its implementation in `np.ndarray` is still in progress (<https://github.com/numpy/numpy/pull/19083>).

The `numpy.array_api` module is considered experimental. This means that importing it will issue a `UserWarning`. The alternative to this was naming the module `numpy._array_api`, but the warning was chosen instead so that it does not become necessary to rename the module in the future, potentially breaking user code. The module also requires Python 3.8 or greater due to extensive use of the positional-only argument syntax.

The experimental nature of the module also means that it is not yet mentioned anywhere in the NumPy documentation, outside of its module docstring and this NEP. Documentation for the implementation is itself a challenging problem. Presently every docstring in the implementation simply references the underlying NumPy function it implements. However, this is not ideal, as the underlying NumPy function may have different behavior from the corresponding function in the array API, for instance, additional keyword arguments that are not present in the array API. It has been suggested that documentation may be pulled directly from the spec itself, but support for this would require making some technical changes to the way the spec is written, and so the current implementation does not yet make any attempt to do this.

The array API specification is accompanied by an in-progress [official test suite](https://github.com/data-apis/array-api-tests), which is designed to test conformance of any library to the array API specification. The tests included with the implementation will therefore be minimal, as the majority of the behavior will be verified by this test suite. The tests in NumPy itself for the `array_api` submodule will only include testing for behavior not covered by the array API test suite, for instance, tests that the implementation is minimal and properly rejects things like disallowed type combinations. A CI job will be added to the array API test suite repository to regularly test it against the NumPy implementation. The array API test suite is designed to be vendored if libraries wish to do that, but this idea was rejected for NumPy because the time taken by it is significant relative to the existing NumPy test suite, and because the test suite is itself still a work in progress.

### The dtype objects

We must be able to compare dtypes for equality, and expressions like these must be possible:

    np.array_api.some_func(..., dtype=x.dtype)

The above implies it would be nice to have `np.array_api.float32 == np.array_api.ndarray(...).dtype`.

Dtypes should not be assumed to have a class hierarchy by users, however we are free to implement it with a class hierarchy if that's convenient. We considered the following options to implement dtype objects:

1.  Alias dtypes to those in the main namespace, e.g., `np.array_api.float32 = np.float32`.
2.  Make the dtypes instances of `np.dtype`, e.g., `np.array_api.float32 = np.dtype(np.float32)`.
3.  Create new singleton classes with only the required methods/attributes (currently just `__eq__`).

It seems like (2) would be easiest from the perspective of interacting with functions outside the main namespace and (3) would adhere best to the standard. (2) does not prevent users from accessing NumPy-specific attributes of the dtype objects like (3) would, although unlike (1), it does disallow creating scalar objects like `float32(0.0)`. (2) also keeps only one object per dtype---with (1), `arr.dtype` would be still be a dtype instance. The implementation currently uses (2).

TBD: the standard does not yet have a good way to inspect properties of a dtype, to ask questions like "is this an integer dtype?". Perhaps this is easy enough to do for users, like so:

    def _get_dtype(dt_or_arr):
        return dt_or_arr.dtype if hasattr(dt_or_arr, 'dtype') else dt_or_arr
    
    def is_floating(dtype_or_array):
        dtype = _get_dtype(dtype_or_array)
        return dtype in (float32, float64)
    
    def is_integer(dtype_or_array):
        dtype = _get_dtype(dtype_or_array)
        return dtype in (uint8, uint16, uint32, uint64, int8, int16, int32, int64)

However it could make sense to add to the standard. Note that NumPy itself currently does not have a great for asking such questions, see [gh-17325](https://github.com/numpy/numpy/issues/17325).

### Feedback from downstream library authors

TODO - this can only be done after trying out some use cases

Leo Fang (CuPy): *"My impression is for CuPy we could simply take this new array object and s/numpy/cupy"*

## Related work

\[NEP37\](\#nep37) contains a similar mechanism to retrieve a NumPy-like namespace. In fact, NEP 37 inspired the (slightly simpler) mechanism in the array API standard.

Other libraries have adopted large parts of NumPy's API, made changes where necessary, and documented deviations. See for example [the jax.numpy documentation](https://jax.readthedocs.io/en/latest/jax.numpy.html) and [Difference between CuPy and NumPy](https://docs.cupy.dev/en/stable/reference/difference.html). The array API standard was constructed with the help of such comparisons, only between many array libraries rather than only between NumPy and one other library.

## Alternatives

It was proposed to have the NumPy array API implementation as a separate library from NumPy. This was rejected because keeping it separate will make it less likely for people to review it, and including it in NumPy itself as an experimental submodule will make it easier for end users and library authors who already depend on NumPy to access the implementation.

## Appendix - a possible `get_namespace` implementation

The `get_namespace` function mentioned in the \[adoption-application-code\](\#adoption-application-code) section can be implemented like:

    def get_namespace(*xs):
        # `xs` contains one or more arrays, or possibly Python scalars (accepting
        # those is a matter of taste, but doesn't seem unreasonable).
        namespaces = {
            x.__array_namespace__() if hasattr(x, '__array_namespace__') else None for x in xs if not isinstance(x, (bool, int, float, complex))
        }
    
        if not namespaces:
            # one could special-case np.ndarray above or use np.asarray here if
            # older numpy versions need to be supported.
            raise ValueError("Unrecognized array input")
    
        if len(namespaces) != 1:
            raise ValueError(f"Multiple namespaces for array inputs: {namespaces}")
    
        xp, = namespaces
        if xp is None:
            raise ValueError("The input is not a supported array type")
    
        return xp

## Discussion

  - [First discussion on the mailing list about the array API standard](https://mail.python.org/pipermail/numpy-discussion/2020-November/081181.html)
  - [Discussion of NEP 47 on the mailing list](https://mail.python.org/pipermail/numpy-discussion/2021-February/081530.html)
  - [PR \#18585 implementing numpy.array\_api](https://github.com/numpy/numpy/pull/18585)

## References and footnotes

## Copyright

This document has been placed in the public domain.\[4\]

1.  <https://data-apis.org/blog/announcing_the_consortium/>

2.  <https://data-apis.org/blog/array_api_standard_release/>

3.  <https://github.com/numpy/numpy/pull/18585#discussion_r641370294>

4.  <https://data-apis.org/blog/announcing_the_consortium/>

---

nep-0048-spending-project-funds.md

---

# NEP 48 â€” Spending NumPy project funds

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Author  
    Inessa Pawson \<<inessa@albuscode.org>\>

  - Author  
    Stefan van der Walt \<<stefanv@berkeley.edu>\>

  - Status  
    Active

  - Type  
    Informational

  - Created  
    2021-02-07

  - Resolution

## Abstract

The NumPy project has historically never received significant **unrestricted** funding. However, that is starting to change. This NEP aims to provide guidance about spending NumPy project unrestricted funds by formulating a set of principles about *what* to pay for and *who* to pay. It will also touch on how decisions regarding spending funds get made, how funds get administered, and transparency around these topics.

## Motivation and scope

NumPy is a fiscally sponsored project of NumFOCUS, a 501(c)(3) nonprofit organization headquartered in Austin, TX. Therefore, for all legal and accounting matters the NumPy project has to follow the rules and regulations for US nonprofits. All nonprofit donations are classified into two categories: **unrestricted funds** which may be used for any legal purpose appropriate to the organization and **restricted funds**, monies set aside for a particular purpose (e.g., project, educational program, etc.).

For the detailed timeline of NumPy funding refer to \[numpy-funding-history\](\#numpy-funding-history).

Since its inception and until 2020, the NumPy project has only spent on the order of $10,000 USD of funds that were not restricted to a particular program. Project income of this type has been relying on donations from individuals and, from mid 2019, recurring monthly contributions from Tidelift. By the end of 2020, the Tidelift contributions increased to $3,000/month, and there's also a potential for an increase of donations and grants going directly to the project. Having a clear set of principles around how to use these funds will facilitate spending them fairly and effectively. Additionally, it will make it easier to solicit donations and other contributions.

A key assumption this NEP makes is that NumPy remains a largely volunteer-driven project, and that the project funds are not enough to employ maintainers full-time. If funding increases to the point where that assumption is no longer true, this NEP should be updated.

In scope for this NEP are:

  - Principles of spending project funds: what to pay for, and who to pay.
  - Describing how NumPy's funds get administered.
  - Describing how decisions to spend funds get proposed and made.

Out of scope for this NEP are:

  - Making any decisions about spending project funds on a specific project or activity.
  - Principles for spending funds that are intended for NumPy development, but don't fall in the category of NumPy unrestricted funds. This includes most of the grant funding, which is usually earmarked for certain activities/deliverables and goes to an Institutional Partner rather than directly to the NumPy project, and companies or institutions funding specific features. *Rationale: As a project, we have no direct control over how this work gets executed (at least formally, until issues or PRs show up). In some cases, we may not even know the contributions were funded or done by an employee on work time. (Whether that's the case or not should not change how we approach a contribution). For grants though, we do expect the research/project leader and funded team to align their work with the needs of NumPy and be receptive to feedback from other NumPy maintainers and contributors.*

## Principles of spending project funds

NumPy will likely always be a project with many times more volunteer contributors than funded people. Therefore having those funded people operate in ways that attract more volunteers and enhance their participation experience is critical. That key principle motivates many of the more detailed principles given below for what to pay for and whom to pay.

The approach for spending funds will be:

  - first figure out what we want to fund,
  - then look for a great candidate,
  - after that's settled, determine a fair compensation level.

The next sections go into detail on each of these three points.

### What to pay for

1.  Pay for things that are important *and* otherwise won't get done. *Rationale: there is way more to be done than there are funds to do all those things. So count on interested volunteers or external sponsored work to do many of those things.*
2.  Plan for sustainability. Don't rely on money always being there.
3.  Consider potential positive benefits for NumPy maintainers and contributors, maintainers of other projects, end users, and other stakeholders like packagers and educators.
4.  Think broadly. There's more to a project than code: websites, documentation, community building, governance - it's all important.
5.  For proposed funded work, include paid time for others to review your work if such review is expected to be significant effort - do not just increase the load on volunteer maintainers. *Rationale: we want the effect of spending funds to be positive for everyone, not just for the people getting paid. This is also a matter of fairness.*

When considering development work, principle (1) implies that priority should be giving to (a) the most boring/painful tasks that no one likes doing, and to necessary structural changes to the code base that are too large to be done by a volunteer in a reasonable amount of time.

There are also many tasks, activities, and projects outside of development work that are important and could enhance the project or community - think of, for example, user surveys, translations, outreach, dedicated mentoring of newcomers, community organizing, website improvements, and administrative tasks.

Time of people to perform tasks is also not the only thing that funds can be used for: expenses for in-person developer meetings or sprints, hosted hardware for benchmarking or development work, and CI or other software services could all be good candidates to spend funds on.

### Whom to pay

1.  All else being equal, give preference to existing maintainers/contributors.
2.  When looking outside of the current team, consider this an opportunity to make the project more diverse.
3.  Pay attention to the following when considering paying someone:
      - the necessary technical or domain-specific skills to execute the tasks,
      - communication and self-management skills,
      - experience contributing to and working with open source projects.

It will likely depend on the project/tasks whether there's already a clear best candidate within the NumPy team, or whether we look for new people to get involved. Before making any decisions, the decision makers (according to the NumPy governance document - currently that's the Steering Council) should think about whether an opportunity should be advertised to give a wider group of people a chance to apply for it.

### Compensating fairly

\> **Note** \> This section on compensating fairly will be considered *Draft* even if this NEP as a whole is accepted. Once we have applied the approach outlined here at least 2-3 times and we are happy with it, will we remove this note and consider this section *Accepted*.

Paying people fairly is a difficult topic, especially when it comes to distributed teams. Therefore, we will only offer some guidance here. Final decisions will always have to be considered and approved by the group of people that bears this responsibility (according to the current NumPy governance structure, this would be the NumPy Steering Council).

Discussions on remote employee compensation tend to be dominated by two narratives: "pay local market rates" and "same work -- same pay".

We consider them both extreme:

  - "Same work -- same pay" is unfair to people living in locations with a higher cost of living. For example, the average rent for a single family apartment can differ by a large factor (from a few hundred dollars to thousands of dollars per month).
  - "Pay local market rates" bakes in existing inequalities between countries and makes fixed-cost items like a development machine or a holiday trip abroad relatively harder to afford in locations where market rates are lower.

We seek to find a middle ground between these two extremes.

Useful points of reference include companies like GitLab and Buffer who are transparent about their remuneration policies (\[1\],\[2\]), Google Summer of Code stipends (\[3\]), other open source projects that manage their budget in a transparent manner (e.g., Babel and Webpack on Open Collective (\[4\],\[5\])), and standard salary comparison sites.

Since NumPy is a not-for-profit project, we also looked to the nonprofit sector for guidelines on remuneration policies and compensation levels. Our findings show that most smaller non-profits tend to pay a median salary/wage. We recognize merit in this approach: applying candidates are likely to have a genuine interest in open source, rather than to be motivated purely by financial incentives.

Considering all of the above, we will use the following guidelines for determining compensation:

1.  Aim to compensate people appropriately, up to a level that's expected for senior engineers or other professionals as applicable.
2.  Establish a compensation cap of $125,000 USD that cannot be exceeded even for the residents from the most expensive/competitive locations (\[6\]).
3.  For equivalent work and seniority, a pay differential between locations should never be more than 2x. For example, if we pay $110,000 USD to a senior-level developer from New York, for equivalent work a senior-level developer from South-East Asia should be paid at least $55,000 USD. To compare locations, we will use [Numbeo Cost of Living calculator](https://www.numbeo.com/cost-of-living/) (or its equivalent).

Some other considerations:

  - Often, compensated work is offered for a limited amount of hours or fixed term. In those cases, consider compensation equivalent to a remuneration package that comes with permanent employment (e.g., one month of work should be compensated by at most 1/12th of a full-year salary + benefits).
  - When comparing rates, an individual contractor should typically make 20% more than someone who is employed since they have to take care of their benefits and accounting on their own.
  - Some people may be happy with one-off payments towards a particular deliverable (e.g., "triage all open issues for label X for $x,xxx"). This should be compensated at a lower rate compared to an individual contractor. Or they may motivate lower amounts for another reason (e.g., "I want to receive $x,xxx to hire a cleaner or pay for childcare, to free up time for work on open source).
  - When funding someone's time through their employer, that employer may want to set the compensation level based on its internal rules (e.g., overhead rates). Small deviations from the guidelines in this NEP may be needed in such cases, however they should be within reason.
  - It's entirely possible that another strategy rather than paying people for their time on certain tasks may turn out to be more effective. Anything that helps the project and community grow and improve is worth considering.
  - Transparency helps. If everyone involved is comfortable sharing their compensation levels with the rest of the team (or better make it public), it's least likely to be way off the mark for fairness.

We highly recommend that the individuals involved in decision-making about hiring and compensation peruse the content of the References section of this NEP. It offers a lot of helpful advice on this topic.

## Defining fundable activities and projects

We'd like to have a broader set of fundable ideas that we will prioritize with input from NumPy team members and the wider community. All ideas will be documented on a single wiki page. Anyone may propose an idea. Only members of a NumPy team may edit the wiki page.

Each listed idea must meet the following requirements:

1.  It must be clearly scoped: its description must explain the importance to the project, referencing the NumPy Roadmap if possible, the items to pay for or activities and deliverables, and why it should be a funded activity (see \[section-what-to-pay-for\](\#section-what-to-pay-for)).
2.  It must contain the following metadata: title, cost, time duration or effort estimate, and (if known) names of the team member(s) to execute or coordinate.
3.  It must have an assigned priority (low, medium, or high). This discussion can originate at a NumPy community meeting or on the mailing list. However, it must be finalized on the mailing list allowing everyone to weigh in.

If a proposed idea has been assigned a high priority level, a decision on allocating funding for it will be made on the private NumPy Steering Council mailing list. *Rationale: these will often involve decisions about individuals, which is typically hard to do in public. This is the current practice that seems to be working well.*

Sometimes, it may be practical to make a single funding decision ad-hoc (e.g., "Here's a great opportunity plus the right person to execute it right nowâ€). However, this approach to decision-making should be used rarely.

## Strategy for spending/saving funds

There is an expectation from NumPy individual, corporate, and institutional donors that the funds will be used for the benefit of the project and the community. Therefore, we should spend available funds, thoughtfully, strategically, and fairly, as they come in. For emergencies, we should keep a $10,000 - $15,000 USD reserve which could cover, for example, a year of CI and hosting services, 1-2 months of full-time maintenance work, or contracting a consultant for a specific need.

## How project funds get administered

We will first summarize how administering of funds works today, and then discuss how to make this process more efficient and transparent.

Currently, the project funds are held by NumFOCUS in a dedicated account. NumFOCUS has a small accounting team, which produces an account overview as a set of spreadsheets on a monthly basis. These land in a shared drive, typically with about a one month delay (e.g., the balance and transactions for February are available at the end of March), where a few NumPy team members can access them. Expense claims and invoices are submitted through the NumFOCUS website. Those then show up in another spreadsheet, where a NumPy team member must review and approve each of them before payments are made. Following NumPy bylaws, the NumFOCUS finance subcommittee, consisting of five people, meets every six months to review all the project related transactions. (In practice, there have been so few transactions that we skipped some of these meetings.)

The existing process is time-consuming and error-prone. More transparency and automation are desirable.

### Transparency about project funds and in decision making

**To discuss: do we want full transparency by publishing our accounts, transparency to everyone on a NumPy team, or some other level?**

Ralf: I'd personally like it to be fully transparent, like through Open Collective, so the whole community can see current balance, income and expenses paid out at any moment in time. Moving to Open Collective is nontrivial, however we can publish the data elsewhere for now if we'd want to. *Note: Google Season of Docs this year requires having an Open Collective account, so this is likely to happen soon enough.*

Stefan/Inessa: at least a summary overview should be fully public, and all transactions should be visible to the Steering Council. Full transparency of all transactions is probably fine, but not necessary.

*The options here may be determined by the accounting system and amount of effort required.*

## NumPy funding â€“ history and current status

The NumPy project received its first major funding in 2017. For an overview of the early history of NumPy (and SciPy), including some institutions sponsoring time for their employees or contractors to work on NumPy, see\[7\] and\[8\]. To date, NumPy has received four grants:

  - Two grants, from the Alfred P. Sloan Foundation and the Gordon and Betty Moore Foundation respectively, of about $1.3M combined to the Berkeley Institute of Data Science. Work performed during the period 2017-2020; PI StÃ©fan van der Walt.
  - Two grants from the Chan Zuckerberg Foundation to NumFOCUS, for a combined amount of $335k. Work performed during the period 2020-2021; PI's Ralf Gommers (first grant) and Melissa MendonÃ§a (second grant).

From 2012 onwards NumPy has been a fiscally sponsored project of NumFOCUS. Note that fiscal sponsorship doesn't mean NumPy gets funding, rather that it can receive funds under the umbrella of a nonprofit. See [NumFOCUS Project Support](https://numfocus.org/projects-overview) for more details.

Only since 2017 has the NumPy website displayed a "Donate" button, and since 2019 the NumPy repositories have had the GitHub Sponsors button. Before that, it was possible to donate to NumPy on the NumFOCUS website. The sum total of donations from individuals to NumPy for 2017-2020 was about $6,100.

From May 2019 onwards, Tidelift has supported NumPy financially as part of its "managed open source" business model. From May 2019 till July 2020 this was $1,000/month, and it started steadily growing after that to about $3,000/month (as of Feb 2021).

Finally, there has been other incidental project income, for example, some book royalties from Packt Publishing, GSoC mentoring fees from Google, and merchandise sales revenue through the NumFOCUS web shop. All of these were small (two or three figure) amounts.

This brings the total amount of project income which did not already have a spending target to about $35,000. Most of that is recent, from Tidelift. Over the past 1.5 years we spent about $10,000 for work on the new NumPy website and Sphinx theme. Those spending decisions were made by the NumPy Steering Council and announced on the mailing list.

That leaves about $25,000 in available funds at the time of writing, and that amount is currently growing at a rate of about $3,000/month.

## Related work

See references. We assume that other open source projects have also developed guidelines on spending project funds. However, we were unable to find any examples at the time of writing.

## Alternatives

*Alternative spending strategy*: not having cash reserves. The rationale being that NumPy is important enough that in a real emergency some person or entity will likely jump in to help out. This is not a responsible approach to financial stewardship of the project though. Hence, we decided against it.

## Discussion

## References and footnotes

  - Nadia Eghbal, "Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure", 2016
  - Nadia Eghbal, "Working in Public: The Making and Maintenance of Open Source", 2020
  - <https://github.com/nayafia/lemonade-stand>
  - Daniel Oberhaus, ["The Internet Was Built on the Free Labor of Open Source Developers. Is That Sustainable?"](https://www.vice.com/en/article/43zak3/the-internet-was-built-on-the-free-labor-of-open-source-developers-is-that-sustainable), 2019
  - David Heinemeier Hansson, ["The perils of mixing open source and money"](https://dhh.dk/2013/the-perils-of-mixing-open-source-and-money.html), 2013
  - Danny Crichton, ["Open source sustainability"](https://techcrunch.com/2018/06/23/open-source-sustainability/?guccounter=1), 2018
  - Nadia Eghbal, "Rebuilding the Cathedral", <https://www.youtube.com/watch?v=VS6IpvTWwkQ>, 2017
  - Nadia Eghbal, "Where money meets open source", <https://www.youtube.com/watch?v=bjAinwgvQqc&t=246s>, 2017
  - Eileen Uchitelle, ""The unbearable vulnerability of open source", <https://www.youtube.com/watch?v=VdwO3LQ56oM>, 2017 (the inverted triangle, open source is a funnel)
  - Dries Buytaert, "Balancing Makers and Takers to scale and sustain Open Source", <https://dri.es/balancing-makers-and-takers-to-scale-and-sustain-open-source>, 2019
  - Safia Abdalla, "Beyond Maintenance", <https://increment.com/open-source/beyond-maintenance/>, 2019
  - Xavier Damman, "Money and Open Source Communities", <https://blog.opencollective.com/money-and-open-source-communities/>, 2016
  - Aseem Sood, "Let's talk about money", <https://blog.opencollective.com/lets-talk-about-money/>, 2017
  - Alanna Irving, "Has your open source community raised money? Here's how to spend it.", <https://blog.opencollective.com/has-your-open-source-community-raised-money-heres-how-to-spend-it/>, 2017
  - Alanna Irving, "Funding open source, how Webpack reached $400k+/year", <https://blog.opencollective.com/funding-open-source-how-webpack-reached-400k-year/>, 2017
  - Alanna Irving, "Babel's rise to financial sustainability", <https://blog.opencollective.com/babels-rise-to-financial-sustainability/>, 2019
  - Devon Zuegel, "The city guide to open source", <https://www.youtube.com/watch?v=80KTVu6GGSE>, 2020 + blog: <https://increment.com/open-source/the-city-guide-to-open-source/>

GitHub Sponsors:

  - <https://github.blog/2019-05-23-announcing-github-sponsors-a-new-way-to-contribute-to-open-source/>
  - <https://github.blog/2020-05-12-github-sponsors-is-out-of-beta-for-sponsored-organizations/>
  - <https://blog.opencollective.com/on-github-sponsors/>, 2019
  - <https://blog.opencollective.com/double-the-love/>, 2020
  - <https://blog.opencollective.com/github-sponsors-for-companies-open-source-collective-for-people/>

## Copyright

This document has been placed in the public domain.

1.  <https://remote.com/blog/remote-compensation>

2.  <https://about.gitlab.com/company/culture/all-remote/compensation/#how-do-you-decide-how-much-to-pay-people>

3.  <https://developers.google.com/open-source/gsoc/help/student-stipends>

4.  Jurgen Appelo, "Compensation: what is fair?", <https://blog.agilityscales.com/compensation-what-is-fair-38a65a822c29>, 2016

5.  Project Include, "Compensating fairly", <https://projectinclude.org/compensating_fairly>

6.  This cap is derived from comparing with compensation levels at other open source projects (e.g., Babel, Webpack, Drupal - all in the $100,000 -- $125,000 range) and Partner Institutions.

7.  Pauli Virtanen et al., "SciPy 1.0: fundamental algorithms for scientific computing in Python", <https://www.nature.com/articles/s41592-019-0686-2>, 2020

8.  Charles Harris et al., "Array programming with NumPy", <https://www.nature.com/articles/s41586-020-2649-2>, 2020

---

nep-0049.md

---

# NEP 49 â€” Data allocation strategies

  - Author  
    Matti Picus

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2021-04-18

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/YZ3PNTXZUT27B6ITFAD3WRSM3T3SRVK4/#PKYXCTG4R5Q6LIRZC4SEWLNBM6GLRF26>

## Abstract

The `numpy.ndarray` requires additional memory allocations to hold `numpy.ndarray.strides`, `numpy.ndarray.shape` and `numpy.ndarray.data` attributes. These attributes are specially allocated after creating the python object in `__new__` method.

This NEP proposes a mechanism to override the memory management strategy used for `ndarray->data` with user-provided alternatives. This allocation holds the data and can be very large. As accessing this data often becomes a performance bottleneck, custom allocation strategies to guarantee data alignment or pinning allocations to specialized memory hardware can enable hardware-specific optimizations. The other allocations remain unchanged.

## Motivation and scope

Users may wish to override the internal data memory routines with ones of their own. Two such use-cases are to ensure data alignment and to pin certain allocations to certain NUMA cores. This desire for alignment was discussed multiple times on the mailing list [in 2005](https://numpy-discussion.scipy.narkive.com/MvmMkJcK/numpy-arrays-data-allocation-and-simd-alignement), and in [issue 5312](https://github.com/numpy/numpy/issues/5312) in 2014, which led to [PR 5457](https://github.com/numpy/numpy/pull/5457) and more mailing list discussions [here](https://mail.python.org/archives/list/numpy-discussion@python.org/thread/YPC5BGPUMKT2MLBP6O3FMPC35LFM2CCH/#YPC5BGPUMKT2MLBP6O3FMPC35LFM2CCH) [and here](https://mail.python.org/archives/list/numpy-discussion@python.org/thread/IQK3EPIIRE3V4BPNAMJ2ZST3NUG2MK2A/#IQK3EPIIRE3V4BPNAMJ2ZST3NUG2MK2A). In a comment on the issue [from 2017](https://github.com/numpy/numpy/issues/5312#issuecomment-315234656), a user described how 64-byte alignment improved performance by 40x.

Also related is [issue 14177](https://github.com/numpy/numpy/issues/14177) around the use of `madvise` and huge pages on Linux.

Various tracing and profiling libraries like [filprofiler](https://github.com/pythonspeed/filprofiler/blob/master/design/allocator-overrides.md) or [electric fence](https://github.com/boundarydevices/efence) override `malloc`.

The long CPython discussion of [BPO 18835](https://bugs.python.org/issue18835) began with discussing the need for `PyMem_Alloc32` and `PyMem_Alloc64`. The early conclusion was that the cost (of wasted padding) vs. the benefit of aligned memory is best left to the user, but then evolves into a discussion of various proposals to deal with memory allocations, including [PEP 445](https://www.python.org/dev/peps/pep-0445/) [memory interfaces](https://docs.python.org/3/c-api/memory.html#customize-memory-allocators) to `PyTraceMalloc_Track` which apparently was explicitly added for NumPy.

Allowing users to implement different strategies via the NumPy C-API will enable exploration of this rich area of possible optimizations. The intention is to create a flexible enough interface without burdening normative users.

## Usage and impact

The new functions can only be accessed via the NumPy C-API. An example is included later in this NEP. The added `struct` will increase the size of the `ndarray` object. It is a necessary price to pay for this approach. We can be reasonably sure that the change in size will have a minimal impact on end-user code because NumPy version 1.20 already changed the object size.

The implementation preserves the use of `PyTraceMalloc_Track` to track allocations already present in NumPy.

## Backward compatibility

The design will not break backward compatibility. Projects that were assigning to the `ndarray->data` pointer were already breaking the current memory management strategy and should restore `ndarray->data` before calling `Py_DECREF`. As mentioned above, the change in size should not impact end-users.

## Detailed description

### High level design

Users who wish to change the NumPy data memory management routines will use :c\`PyDataMem\_SetHandler\`, which uses a :c`PyDataMem_Handler` structure to hold pointers to functions used to manage the data memory. In order to allow lifetime management of the `context`, the structure is wrapped in a `PyCapsule`.

Since a call to `PyDataMem_SetHandler` will change the default functions, but that function may be called during the lifetime of an `ndarray` object, each `ndarray` will carry with it the `PyDataMem_Handler`-wrapped PyCapsule used at the time of its instantiation, and these will be used to reallocate or free the data memory of the instance. Internally NumPy may use `memcpy` or `memset` on the pointer to the data memory.

The name of the handler will be exposed on the python level via a `numpy.core.multiarray.get_handler_name(arr)` function. If called as `numpy.core.multiarray.get_handler_name()` it will return the name of the handler that will be used to allocate data for the next new <span class="title-ref">ndarrray</span>.

The version of the handler will be exposed on the python level via a `numpy.core.multiarray.get_handler_version(arr)` function. If called as `numpy.core.multiarray.get_handler_version()` it will return the version of the handler that will be used to allocate data for the next new <span class="title-ref">ndarrray</span>.

The version, currently 1, allows for future enhancements to the `PyDataMemAllocator`. If fields are added, they must be added to the end.

### NumPy C-API functions

`PyDataMem_Handler` thread safety and lifetime `` ` ================================================ The active handler is stored in the current :py`~contextvars.Context` via a :py`~contextvars.ContextVar`. This ensures it can be configured both per-thread and per-async-coroutine.  There is currently no lifetime management of ``PyDataMem\_Handler``. The user of `PyDataMem_SetHandler` must ensure that the argument remains alive for as long as any objects allocated with it, and while it is the active handler. In practice, this means the handler must be immortal.  As an implementation detail, currently this``ContextVar`contains a`PyCapsule`object storing a pointer to a`PyDataMem\_Handler`with no destructor, but this should not be relied upon.  Sample code ===========  This code adds a 64-byte header to each`data`pointer and stores information about the allocation in the header. Before calling`free`, a check ensures the`sz`argument is correct.`\`c \#define NPY\_NO\_DEPRECATED\_API NPY\_1\_7\_API\_VERSION \#include \<numpy/arrayobject.h\> NPY\_NO\_EXPORT void \*

>   - typedef struct {  
>     void *(*malloc)(size\_t); void *(*calloc)(size\_t, size\_t); void *(*realloc)(void *, size\_t); void (*free)(void \*);
> 
> } Allocator;
> 
> NPY\_NO\_EXPORT void \* shift\_alloc(Allocator *ctx, size\_t sz) { char*real = (char *)ctx-\>malloc(sz + 64); if (real == NULL) { return NULL; } snprintf(real, 64, "originally allocated %ld", (unsigned long)sz); return (void*)(real + 64); }
> 
> NPY\_NO\_EXPORT void \* shift\_zero(Allocator *ctx, size\_t sz, size\_t cnt) { char*real = (char *)ctx-\>calloc(sz + 64, cnt); if (real == NULL) { return NULL; } snprintf(real, 64, "originally allocated %ld via zero", (unsigned long)sz); return (void*)(real + 64); }
> 
> NPY\_NO\_EXPORT void shift\_free(Allocator *ctx, void* p, npy\_uintp sz) { if (p == NULL) { return ; } char *real = (char*)p - 64; if (strncmp(real, "originally allocated", 20) \!= 0) { fprintf(stdout, "uh-oh, unmatched shift\_free, " "no appropriate prefix\\n"); /\* Make C runtime crash by calling free on the wrong address */ ctx-\>free((char*)p + 10); /\* ctx-\>free(real); */ } else { npy\_uintp i = (npy\_uintp)atoi(real +20); if (i \!= sz) { fprintf(stderr, "uh-oh, unmatched shift\_free" "(ptr, %ld) but allocated %ld\\n", sz, i); /* This happens when the shape has a 0, only print \*/ ctx-\>free(real); } else { ctx-\>free(real); } } }
> 
> NPY\_NO\_EXPORT void \* shift\_realloc(Allocator *ctx, void* p, npy\_uintp sz) { if (p \!= NULL) { char *real = (char*)p - 64; if (strncmp(real, "originally allocated", 20) \!= 0) { fprintf(stdout, "uh-oh, unmatched shift\_realloc\\n"); return realloc(p, sz); } return (void *)((char*)ctx-\>realloc(real, sz + 64) + 64); } else { char *real = (char*)ctx-\>realloc(p, sz + 64); if (real == NULL) { return NULL; } snprintf(real, 64, "originally allocated " "%ld via realloc", (unsigned long)sz); return (void \*)(real + 64); } }
> 
>   - static Allocator new\_handler\_ctx = {  
>     malloc, calloc, realloc, free
> 
> };
> 
>   - static PyDataMem\_Handler new\_handler = {  
>     "secret\_data\_allocator", 1, { \&new\_handler\_ctx, shift\_alloc, /\* malloc */ shift\_zero, /* calloc */ shift\_realloc, /* realloc */ shift\_free /* free \*/ }
> 
> };

Related work `` ` ------------  This NEP is being tracked by the pnumpy_ project and a `comment in the PR`_ mentions use in orchestrating FPGA DMAs.  Implementation --------------  This NEP has been implemented in `PR  17582`_.  Alternatives ------------  These were discussed in `issue 17467`_. `PR 5457`_  and `PR 5470`_ proposed a global interface for specifying aligned allocations. ``PyArray\_malloc\_aligned``and friends were added to NumPy with the `numpy.random` module API refactor. and are used there for performance.  `PR 390`_ had two parts: expose``[PyDataMem]()\*`via the NumPy C-API, and a hook mechanism. The PR was merged with no example code for using these features.  Discussion ----------  The discussion on the mailing list led to the`PyDataMemAllocator`struct with a`context``field like :c:type:`PyMemAllocatorEx` but with a different signature for``free\`\`.

## References and footnotes

## Copyright

This document has been placed in the public domain.\[1\]

1.  Each NEP must either be explicitly labeled as placed in the public domain (see this NEP as an example) or licensed under the [Open Publication License](https://www.opencontent.org/openpub/).

---

nep-0050-scalar-promotion.md

---

# NEP 50 â€” Promotion rules for Python scalars

  - Author  
    Sebastian Berg

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2021-05-25

## Abstract

Since NumPy 1.7, promotion rules use so-called "safe casting" which relies on inspection of the values involved. This helped identify a number of edge cases for users, but was complex to implement and also made behavior hard to predict.

There are two kinds of confusing results:

1.  Value-based promotion means that the value, for example of a Python integer, can determine output type as found by `np.result_type`:
    
        np.result_type(np.int8, 1) == np.int8
        np.result_type(np.int8, 255) == np.int16
    
    This logic arises because `1` can be represented by a `uint8` or `int8` while `255` cannot be represented by an `int8` but only by by a `uint8` or `int16`.
    
    This also holds when working with 0-D arrays (so-called "scalar arrays"):
    
        int64_0d_array = np.array(1, dtype=np.int64)
        np.result_type(np.int8, int64_0d_array) == np.int8
    
    Where the fact that `int64_0d_array` has an `int64` dtype has no influence on the resulting dtype. The `dtype=np.int64` is effectively ignored in this example since only its value matters.

2.  For a Python `int`, `float`, or `complex` the value is inspected as previously shown. But surprisingly *not* when the NumPy object is a 0-D array or NumPy scalar:
    
        np.result_type(np.array(1, dtype=np.uint8), 1) == np.int64
        np.result_type(np.int8(1), 1) == np.int64
    
    The reason is that value-based promotion is disabled when all objects are scalars or 0-D arrays. NumPy thus returns the same type as `np.array(1)`, which is usually an `int64` (this depends on the system).

Note that the examples apply also to operations like multiplication, addition, comparisons, and their corresponding functions like `np.multiply`.

This NEP proposes to refactor the behaviour around two guiding principles:

1.  Values must never influence result type.
2.  NumPy scalars and 0-D arrays should behave consistently with their N-D counterparts.

We propose to remove all value-based logic and add special handling for Python scalars to preserve some convenient behaviors. Python scalars will be considered "weakly" typed. When a NumPy array/scalar is combined with a Python scalar, it will be converted to the NumPy dtype, such that:

    np.array([1, 2, 3], dtype=np.uint8) + 1  # returns a uint8 array
    np.array([1, 2, 3], dtype=np.float32) + 2.  # returns a float32 array

There will be no dependence on the Python value itself.

The proposed changes also apply to `np.can_cast(100, np.int8)`, however, we expect that the behaviour in functions (promotion) will, in practice, be far more important than the casting change itself.

\> **Note** \> As of the NumPy 1.24.x series, NumPy has preliminary and limited support to test this proposal.

> It is further necessary to set the following environment variable:
> 
>     export NPY_PROMOTION_STATE=weak
> 
> Valid values are `weak`, `weak_and_warn`, and `legacy`. Note that `weak_and_warn` implements the optional warnings proposed in this NEP and is expected to be *very* noisy. We recommend starting using the `weak` option and use `weak_and_warn` mainly to understand a specific observed change in behaviour.
> 
> The following additional API exists:
> 
>   - `np._set_promotion_state()` and `np._get_promotion_state()` which is equivalent to the environment variable. (Not thread/context safe.)
>   - `with np._no_nep50_warning():` allows to suppress warnings when `weak_and_warn` promotion is used. (Thread and context safe.)
> 
> At this time overflow warnings on integer power are missing. Further, `np.can_cast` fails to give warnings in the `weak_and_warn` mode. Its behavior with respect to Python scalar input may still be in flux (this should affect very few users).

### Schema of the new proposed promotion rules

After the change, the promotions in NumPy will follow the schema below. Promotion always occurs along the green lines: from left to right within their kind and to a higher kind only when necessary. The result kind is always the largest kind of the inputs. Note that `float32` has a lower precision than `int32` or `uint32` and is thus sorted slightly to the left in the schematic. This is because `float32` cannot represent all `int32` values exactly. However, for practical reasons, NumPy allows promoting `int64` to `float64` effectively considering them to have the same precision.

The Python scalars are inserted at the very left of each "kind" and the Python integer does not distinguish signed and unsigned. NumPy promotion thus uses the following, ordered, kind categories:

  - <span class="title-ref">boolean</span>
  - \`integral\`: signed or unsigned integers
  - \`inexact\`: floating point numbers and complex floating point numbers

When promoting a Python scalar with a dtype of lower kind category (<span class="title-ref">boolean \< integral \< inexact</span>) with a higher one, we use the minimum/default precision: that is `float64`, `complex128` or `int64` (`int32` is used on some systems, e.g. windows).

![](nep-0050-promotion-no-fonts.svg)

See the next section for examples which clarify the proposed behavior. Further examples with a comparison to the current behavior can be found in the table below.

### Examples of new behaviour

To make interpretation of above text and figure easier, we provide a few examples of the new behaviour. Below, the Python integer has no influence on the result type:

    np.uint8(1) + 1 == np.uint8(2)
    np.int16(2) + 2 == np.int16(4)

In the following the Python `float` and `complex` are "inexact", but the NumPy value is integral, so we use at least `float64`/`complex128`:

    np.uint16(3) + 3.0 == np.float64(6.0)
    np.int16(4) + 4j == np.complex128(4+4j)

But this does not happen for `float` to `complex` promotions, where `float32` and `complex64` have the same precision:

    np.float32(5) + 5j == np.complex64(5+5j)

Note that the schematic omits `bool`. It is set below "integral", so that the following hold:

    np.bool_(True) + 1 == np.int64(2)
    True + np.uint8(2) == np.uint8(3)

Note that while this NEP uses simple operators as example, the rules described generally apply to all of NumPy operations.

### Table comparing new and old behaviour

The following table lists relevant changes and unchanged behaviours. Please see the [Old implementation](#Old%20implementation) for a detailed explanation of the rules that lead to the "Old result", and the following sections for the rules detailing the new. The backwards compatibility section discusses how these changes are likely to impact users.

Note the important distinction between a 0-D array like `array(2)` and arrays that are not 0-D, such as `array([2])`.

<table>
<caption>Table of changed behaviours</caption>
<colgroup>
<col style="width: 45%" />
<col style="width: 27%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th>Expression</th>
<th>Old result</th>
<th>New result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>uint8(1) + 2</code></td>
<td><code>int64(3)</code></td>
<td><code>uint8(3)</code> <a href="#T1" class="citation">[T1]</a></td>
</tr>
<tr class="even">
<td><p><code>array([1], uint8) + int64(1)</code> or</p>
<p><code>array([1], uint8) + array(1, int64)</code></p></td>
<td><p><code>array([2], uint8)</code></p></td>
<td><p><code>array([2], int64)</code> <a href="#T2" class="citation">[T2]</a></p></td>
</tr>
<tr class="odd">
<td><p><code>array([1.], float32) + float64(1.)</code> or</p>
<p><code>array([1.], float32) + array(1., float64)</code></p></td>
<td><p><code>array([2.], float32)</code></p></td>
<td><p><code>array([2.], float64)</code></p></td>
</tr>
<tr class="even">
<td><code>array([1], uint8) + 1</code></td>
<td><code>array([2], uint8)</code></td>
<td><em>unchanged</em></td>
</tr>
<tr class="odd">
<td><code>array([1], uint8) + 200</code></td>
<td><code>array([201], np.uint8)</code></td>
<td><em>unchanged</em></td>
</tr>
<tr class="even">
<td><code>array([100], uint8) + 200</code></td>
<td><code>array([ 44], uint8)</code></td>
<td><em>unchanged</em> <a href="#T3" class="citation">[T3]</a></td>
</tr>
<tr class="odd">
<td><code>array([1], uint8) + 300</code></td>
<td><code>array([301], uint16)</code></td>
<td><em>Exception</em> <a href="#T4" class="citation">[T4]</a></td>
</tr>
<tr class="even">
<td><code>uint8(1) + 300</code></td>
<td><code>int64(301)</code></td>
<td><em>Exception</em> <a href="#T5" class="citation">[T5]</a></td>
</tr>
<tr class="odd">
<td><code>uint8(100) + 200</code></td>
<td><code>int64(300)</code></td>
<td><code>uint8(44)</code> <em>and</em> <code>RuntimeWarning</code> <a href="#T6" class="citation">[T6]</a></td>
</tr>
<tr class="even">
<td><code>float32(1) + 3e100</code></td>
<td><code>float64(3e100)</code></td>
<td><code>float32(Inf)</code> <em>and</em> <code>RuntimeWarning</code> <a href="#T7" class="citation">[T7]</a></td>
</tr>
<tr class="odd">
<td><code>array([1.0], float32) + 1e-14 == 1.0</code> <a href="#T8" class="citation">[T8]</a></td>
<td><code>array([True])</code></td>
<td><em>unchanged</em></td>
</tr>
<tr class="even">
<td><code>array(1.0, float32) + 1e-14 == 1.0</code> <a href="#T8" class="citation">[T8]</a></td>
<td><code>False</code></td>
<td><code>True</code></td>
</tr>
<tr class="odd">
<td><code>array([1.], float32) + 3</code></td>
<td><code>array([4.], float32)</code></td>
<td><em>unchanged</em></td>
</tr>
<tr class="even">
<td><code>array([1.], float32) + int64(3)</code></td>
<td><code>array([4.], float32)</code></td>
<td><code>array([4.], float64)</code> <a href="#T9" class="citation">[T9]</a></td>
</tr>
<tr class="odd">
<td><code>(3j + array(3, complex64)).dtype</code></td>
<td><code>complex128</code></td>
<td><code>complex64</code> <a href="#T10" class="citation">[T10]</a></td>
</tr>
<tr class="even">
<td><code>(float32(1) + 1j)).dtype</code></td>
<td><code>complex128</code></td>
<td><code>complex64</code> <a href="#T11" class="citation">[T11]</a></td>
</tr>
<tr class="odd">
<td><code>(int32(1) + 5j).dtype</code></td>
<td><code>complex128</code></td>
<td><em>unchanged</em> <a href="#T12" class="citation">[T12]</a></td>
</tr>
</tbody>
</table>

## Motivation and scope

The motivation for changing the behaviour with respect to inspecting the value of Python scalars and NumPy scalars/0-D arrays is three-fold:

1.  The special handling of NumPy scalars/0-D arrays as well as the value inspection can be very surprising to users,
2.  The value-inspection logic is much harder to explain and implement. It is further harder to make it available to user-defined DTypes through \[NEP 42 \<NEP42\>\](\#nep-42-\<nep42\>). Currently, this leads to a dual implementation of a new and an old (value sensitive) system. Fixing this will greatly simplify the internal logic and make results more consistent.
3.  It largely aligns with the choice of other projects like <span class="title-ref">JAX</span> and <span class="title-ref">data-apis.org</span> (see also <span class="title-ref">Related Work</span>).

We believe that the proposal of "weak" Python scalars will help users by providing a clear mental model for which datatype an operation will result in. This model fits well with the preservation of array precisions that NumPy currently often follows, and also uses for in-place operations:

    arr += value

Preserves precision as long as "kind" boundaries are not crossed (otherwise an error is raised).

While some users will potentially miss the value inspecting behavior, even for those cases where it seems useful it quickly leads to surprises. This may be expected:

    np.array([100], dtype=np.uint8) + 1000 == np.array([1100], dtype=np.uint16)

But the following will then be a surprise:

    np.array([100], dtype=np.uint8) + 200 == np.array([44], dtype=np.uint8)

Considering that the proposal aligns with the behavior of in-place operands and avoids the surprising switch in behavior that only sometimes avoids overflow in the result, we believe that the proposal follows the "principle of least surprise".

## Usage and impact

This NEP is expected to be implemented with **no** transition period that warns for all changes. Such a transition period would create many (often harmless) warnings which would be difficult to silence. We expect that most users will benefit long term from the clearer promotion rules and that few are directly (negatively) impacted by the change. However, certain usage patterns may lead to problematic changes, these are detailed in the backwards compatibility section.

The solution to this will be an *optional* warning mode capable of notifying users of potential changes in behavior. This mode is expected to generate many harmless warnings, but provide a way to systematically vet code and track down changes if problems are observed.

### Impact on `can_cast`

<span class="title-ref">can\_cast</span> will never inspect the value anymore. So that the following results are expected to change from `True` to `False`:

    np.can_cast(np.int64(100), np.uint8)
    np.can_cast(np.array(100, dtype=np.int64), np.uint8)
    np.can_cast(100, np.uint8)

We expect that the impact of this change will be small compared to that of the following changes.

\> **Note** \> The last example where the input is a Python scalar \_[may]() be preserved since `100` can be represented by a `uint8`.

### Impact on operators and functions involving NumPy arrays or scalars

The main impact on operations not involving Python scalars (`float`, `int`, `complex`) will be that operations on 0-D arrays and NumPy scalars will never depend on their values. This removes currently surprising cases. For example:

    np.arange(10, dtype=np.uint8) + np.int64(1)
    # and:
    np.add(np.arange(10, dtype=np.uint8), np.int64(1))

Will return an `int64` array in the future because the type of `np.int64(1)` is strictly honoured. Currently a `uint8` array is returned.

### Impact on operators involving Python `int`, `float`, and `complex`

This NEP attempts to preserve the convenience of the old behaviour when working with literal values. The current value-based logic had some nice properties when "untyped", literal Python scalars are involved:

    np.arange(10, dtype=np.int8) + 1  # returns an int8 array
    np.array([1., 2.], dtype=np.float32) * 3.5  # returns a float32 array

But led to surprises when it came to "unrepresentable" values:

    np.arange(10, dtype=np.int8) + 256  # returns int16
    np.array([1., 2.], dtype=np.float32) * 1e200  # returns float64

The proposal is to preserve this behaviour for the most part. This is achieved by considering Python `int`, `float`, and `complex` to be "weakly" typed in operations. However, to avoid surprises, we plan to make conversion to the new type more strict: The results will be unchanged in the first two examples, but in the second one, it will change the following way:

    np.arange(10, dtype=np.int8) + 256  # raises a TypeError
    np.array([1., 2.], dtype=np.float32) * 1e200  # warning and returns infinity

The second one warns because `np.float32(1e200)` overflows to infinity. It will then continue to do the calculation with `inf` as usual.

<div class="admonition">

Behaviour in other libraries

Overflowing in the conversion rather than raising an error is a choice; it is one that is the default in most C setups (similar to NumPy C can be set up to raise an error due to the overflow, however). It is also for example the behaviour of `pytorch` 1.10.

</div>

#### Particular behavior of Python integers

The NEPs promotion rules stated in terms of the resulting dtype which is typically also the operation dtype (in terms of result precision). This leads to what may seem like exceptions for Python integers: While `uint8(3) + 1000` must be rejected because operating in `uint8` is not possible, `uint8(3) / 1000` returns a `float64` and can convert both inputs to `float64` to find the result.

In practice this means that arbitrary Python integer values are accepted in the following cases:

  - All comparisons (`==`, `<`, etc.) between NumPy and Python integers are always well defined.
  - Unary functions like `np.sqrt` that give a floating point result can and will convert the Python integer to a float.
  - Division of integers returns floating point by casting input to `float64`.

Note that there may be additional functions where these exceptions could be applied but are not. In these cases it should be considered an improvement to allow them, but when the user impact is low we may not do so for simplicity.

## Backward compatibility

In general, code which only uses the default dtypes float64, or int32/int64 or more precise ones should not be affected.

However, the proposed changes will modify results in quite a few cases where 0-D or scalar values (with non-default dtypes) are mixed. In many cases, these will be bug-fixes, however, there are certain changes which may be problematic to the end-user.

The most important possible failure is probably the following example:

    arr = np.arange(100, dtype=np.uint8)  # storage array with low precision
    value = arr[10]
    
    # calculation continues with "value" without considering where it came from
    value * 100

Where previously the `value * 100` would cause an up-cast to `int32`/`int64` (because value is a scalar). The new behaviour will preserve the lower precision unless explicitly dealt with (just as if `value` was an array). This can lead to integer overflows and thus incorrect results beyond precision. In many cases this may be silent, although NumPy usually gives warnings for the scalar operators.

Similarly, if the storage array is `float32` a calculation may retain the lower `float32` precision rather than use the default `float64`.

Further issues can occur. For example:

  - Floating point comparisons, especially equality, may change when mixing precisions:
    
        np.float32(1/3) == 1/3  # was False, will be True.

  - Certain operations are expected to start failing:
    
        np.array([1], np.uint8) * 1000
        np.array([1], np.uint8) == 1000  # possibly also
    
    to protect users in cases where previous value-based casting led to an upcast. (Failures occur when converting `1000` to a `uint8`.)

  - Floating point overflow may occur in odder cases:
    
        np.float32(1e-30) * 1e50  # will return ``inf`` and a warning
    
    Because `np.float32(1e50)` returns `inf`. Previously, this would return a double precision result even if the `1e50` was not a 0-D array

In other cases, increased precision may occur. For example:

    np.multiple(float32_arr, 2.)
    float32_arr * np.float64(2.)

Will both return a float64 rather than `float32`. This improves precision but slightly changes results and uses double the memory.

### Changes due to the integer "ladder of precision"

When creating an array from a Python integer, NumPy will try the following types in order, with the result depending on the value:

    long (usually int64) â†’ int64 â†’ uint64 -> object

which is subtly different from the promotion described above.

This NEP currently does not include changing this ladder (although it may be suggested in a separate document). However, in mixed operations, this ladder will be ignored, since the value will be ignored. This means, that operations will never silently use the `object` dtype:

    np.array([3]) + 2**100  # Will error

The user will have to write one of:

    np.array([3]) + np.array(2**100)
    np.array([3]) + np.array(2**100, dtype=object)

As such implicit conversion to `object` should be rare and the work-around is clear, we expect that the backwards compatibility concerns are fairly small.

## Detailed description

The following provides some additional details on the current "value based" promotion logic, and then on the "weak scalar" promotion and how it is handled internally.

### Old implementation of "values based" promotion

This section reviews how the current value-based logic works in practice, please see the following section for examples on how it can be useful.

When NumPy sees a "scalar" value, which can be a Python int, float, complex, a NumPy scalar or an array:

    1000  # Python scalar
    int32(1000)  # NumPy scalar
    np.array(1000, dtype=int64)  # zero dimensional

Or the float/complex equivalents, NumPy will ignore the precision of the dtype and find the smallest possible dtype that can hold the value. That is, it will try the following dtypes:

  - Integral: `uint8`, `int8`, `uint16`, `int16`, `uint32`, `int32`, `uint64`, `int64`.
  - Floating: `float16`, `float32`, `float64`, `longdouble`.
  - Complex: `complex64`, `complex128`, `clongdouble`.

Note that e.g. for the integer value of `10`, the smallest dtype can be *either* `uint8` or `int8`.

NumPy never applied this rule when all arguments are scalar values:

    np.int64(1) + np.int32(2) == np.int64(3)

For integers, whether a value fits is decided precisely by whether it can be represented by the dtype. For float and complex, the a dtype is considered sufficient if:

  - `float16`: `-65000 < value < 65000` (or NaN/Inf)
  - `float32`: `-3.4e38 < value < 3.4e38` (or NaN/Inf)
  - `float64`: `-1.7e308 < value < 1.7e308` (or Nan/Inf)
  - `longdouble`: (largest range, so no limit)

for complex these bounds were applied to the real and imaginary component. These values roughly correspond to `np.finfo(np.float32).max`. (NumPy did never force the use of `float64` for a value of `float32(3.402e38)` though, but it will for a Python value of `3.402e38`.)

### State of the current "value based" promotion

Before we can propose alternatives to the current datatype system, it is helpful to review how "value based promotion" is used and can be useful. Value based promotion allows for the following code to work:

    # Create uint8 array, as this is sufficient:
    uint8_arr = np.array([1, 2, 3], dtype=np.uint8)
    result = uint8_arr + 4
    result.dtype == np.uint8
    
    result = uint8_arr * (-1)
    result.dtype == np.int16  # upcast as little as possible.

Where especially the first part can be useful: The user knows that the input is an integer array with a specific precision. Considering that plain `+ 4` retaining the previous datatype is intuitive. Replacing this example with `np.float32` is maybe even more clear, as float will rarely have overflows. Without this behaviour, the above example would require writing `np.uint8(4)` and lack of the behaviour would make the following surprising:

    result = np.array([1, 2, 3], dtype=np.float32) * 2.
    result.dtype == np.float32

where lack of a special case would cause `float64` to be returned.

It is important to note that the behaviour also applies to universal functions and zero dimensional arrays:

    # This logic is also used for ufuncs:
    np.add(uint8_arr, 4).dtype == np.uint8
    # And even if the other array is explicitly typed:
    np.add(uint8_arr, np.array(4, dtype=np.int64)).dtype == np.uint8 

To review, if we replace `4` with `[4]` to make it one dimensional, the result will be different:

    # This logic is also used for ufuncs:
    np.add(uint8_arr, [4]).dtype == np.int64  # platform dependent
    # And even if the other array is explicitly typed:
    np.add(uint8_arr, np.array([4], dtype=np.int64)).dtype == np.int64

### Proposed weak promotion

This proposal uses a "weak scalar" logic. This means that Python `int`, `float`, and `complex` are not assigned one of the typical dtypes, such as float64 or int64. Rather, they are assigned a special abstract DType, similar to the "scalar" hierarchy names: Integral, Floating, ComplexFloating.

When promotion occurs (as it does for ufuncs if no exact loop matches), the other DType is able to decide how to regard the Python scalar. E.g. a `UInt16` promoting with an `Integral` will give `UInt16`.

\> **Note** \> A default will most likely be provided in the future for user-defined DTypes. Most likely this will end up being the default integer/float, but in principle more complex schemes could be implemented.

At no time is the value used to decide the result of this promotion. The value is only considered when it is converted to the new dtype; this may raise an error.

## Related work

Different Python projects that fill a similar space to NumPy prefer the weakly typed Python scalars as proposed in this NEP. Details of these may differ or be unspecified though:

  - [JAX promotion](https://jax.readthedocs.io/en/latest/type_promotion.html) also uses the weak-scalar concept. However, it makes use of it also for most functions. JAX further stores the "weak-type" information on the array: `jnp.array(1)` remains weakly typed.
  - [data-apis.org](https://data-apis.org/array-api/latest/API_specification/type_promotion.html) also suggests this weak-scalar logic for the Python scalars.

## Implementation

Implementing this NEP requires some additional machinery to be added to all binary operators (or ufuncs), so that they attempt to use the "weak" logic if possible. There are two possible approaches to this:

1.  The binary operator simply tries to call `np.result_type()` if this situation arises and converts the Python scalar to the result-type (if defined).
2.  The binary operator indicates that an input was a Python scalar, and the ufunc dispatching/promotion machinery is used for the rest (see \[NEP 42 \<NEP42\>\](\#nep-42-\<nep42\>)). This allows more flexibility, but requires some additional logic in the ufunc machinery.

<div class="note">

<div class="title">

Note

</div>

As of now, it is not quite clear which approach is better, either will give fairly equivalent results and 1. could be extended by 2. in the future if necessary.

</div>

It further requires removing all current special value-based code paths.

Unintuitively, a larger step in the implementation may be to implement a solution to allow an error to be raised in the following example:

    np.arange(10, dtype=np.uint8) + 1000

Even though `np.uint8(1000)` returns the same value as `np.uint8(232)`.

\> **Note** \> See alternatives, we may yet decide that this silent overflow is acceptable or at least a separate issue.

## Alternatives

There are several design axes where different choices are possible. The below sections outline these.

### Use strongly-typed scalars or a mix of both

The simplest solution to the value-based promotion/casting issue would be to use strongly typed Python scalars, i.e. Python floats are considered double precision and Python integers are always considered the same as the default integer dtype.

This would be the simplest solution, however, it would lead to many upcasts when working with arrays of `float32` or `int16`, etc. The solution for these cases would be to rely on in-place operations. We currently believe that while less dangerous, this change would affect many users and would be surprising more often than not (although expectations differ widely).

In principle, the weak vs. strong behaviour need not be uniform. It would also be possible to make Python floats use the weak behaviour, but Python integers use the strong one, since integer overflows are far more surprising.

### Do not use weak scalar logic in functions

One alternative to this NEPs proposal is to narrow the use of weak types to Python operators.

This has advantages and disadvantages:

  - The main advantage is that limiting it to Python operators means that these "weak" types/dtypes are clearly ephemeral to short Python statements.
  - A disadvantage is that `np.multiply` and `*` are less interchangeable.
  - Using "weak" promotion only for operators means that libraries do not have to worry about whether they want to "remember" that an input was a Python scalar initially. On the other hand, it would add a the need for slightly different (or additional) logic for Python operators. (Technically, probably as a flag to the ufunc dispatching mechanism to toggle the weak logic.)
  - `__array_ufunc__` is often used on its own to provide Python operator support for array-likes implementing it. If operators are special, these array-likes may need a mechanism to match NumPy (e.g. a kwarg to ufuncs to enable weak promotion.)

### NumPy scalars could be special

Many users expect that NumPy scalars should be different from NumPy arrays, in that `np.uint8(3) + 3` should return an `int64` (or Python integer), when `uint8_arr + 3` preserves the `uint8` dtype.

This alternative would be very close to the current behaviour for NumPy scalars but it would cement a distinction between arrays and scalars (NumPy arrays are "stronger" than Python scalars, but NumPy scalars are not).

Such a distinction is very much possible, however, at this time NumPy will often (and silently) convert 0-D arrays to scalars. It may thus make sense, to only consider this alternative if we also change this silent conversion (sometimes referred to as "decay") behaviour.

### Handling conversion of scalars when unsafe

Cases such as:

    np.arange(10, dtype=np.uint8) + 1000

should raise an error as per this NEP. This could be relaxed to give a warning or even ignore the "unsafe" conversion which (on all relevant hardware) would lead to `np.uint8(1000) == np.uint8(232)` being used.

### Allowing weakly typed arrays

One problem with having weakly typed Python scalars, but not weakly typed arrays is that in many cases `np.asarray()` is called indiscriminately on inputs. To solve this issue JAX will consider the result of `np.asarray(1)` also to be weakly typed. There are, however, two difficulties with this:

1.  JAX noticed that it can be confusing that:
    
        np.broadcast_to(np.asarray(1), (100, 100))
    
    is a non 0-D array that "inherits" the weak typing.\[1\]

2.  Unlike JAX tensors, NumPy arrays are mutable, so assignment may need to cause it to be strongly typed?

A flag will likely be useful as an implementation detail (e.g. in ufuncs), however, as of now we do not expect to have this as user API. The main reason is that such a flag may be surprising for users if it is passed out as a result from a function, rather than used only very localized.

<div class="admonition">

TODO

Before accepting the NEP it may be good to discuss this issue further. Libraries may need clearer patterns to "propagate" the "weak" type, this could just be an `np.asarray_or_literal()` to preserve Python scalars, or a pattern of calling `np.result_type()` before `np.asarray()`.

</div>

### Keep using value-based logic for Python scalars

Some of the main issues with the current logic arise, because we apply it to NumPy scalars and 0-D arrays, rather than the application to Python scalars. We could thus consider to keep inspecting the value for Python scalars.

We reject this idea on the grounds that it will not remove the surprises given earlier:

    np.uint8(100) + 1000 == np.uint16(1100)
    np.uint8(100) + 200 == np.uint8(44)

And adapting the precision based on the result value rather than the input value might be possible for scalar operations, but is not feasible for array operations. This is because array operations need to allocate the result array before performing the calculation.

## Discussion

  - <https://github.com/numpy/numpy/issues/2878>
  - <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/R7D65SNGJW4PD6V7N3CEI4NJUHU6QP2I/#RB3JLIYJITVO3BWUPGLN4FJUUIKWKZIW>
  - <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/NA3UBE3XAUTXFYBX6HPIOCNCTNF3PWSZ/#T5WAYQPRMI5UCK7PKPCE3LGK7AQ5WNGH>
  - Poll about the desired future behavior: <https://discuss.scientific-python.org/t/poll-future-numpy-behavior-when-mixing-arrays-numpy-scalars-and-python-scalars/202>

## References and footnotes

## Copyright

This document has been placed in the public domain.\[2\]

<div id="citations">

  - <span id="T1" class="citation-label">T1</span>  
    New behaviour honours the dtype of the `uint8` scalar.

  - <span id="T10" class="citation-label">T10</span>  
    The new behavior is consistent between `array(3, complex64)` and `array([3], complex64)`: the dtype of the result is that of the array argument.

  - <span id="T11" class="citation-label">T11</span>  
    The new behavior uses the complex dtype of the precision compatible with the array argument, `float32`.

  - <span id="T12" class="citation-label">T12</span>  
    Since the array kind is integer, the result uses the default complex precision, which is `complex128`.

  - <span id="T2" class="citation-label">T2</span>  
    Current NumPy ignores the precision of 0-D arrays or NumPy scalars when combined with arrays.

  - <span id="T3" class="citation-label">T3</span>  
    Current NumPy ignores the precision of 0-D arrays or NumPy scalars when combined with arrays.

  - <span id="T4" class="citation-label">T4</span>  
    Old behaviour uses `uint16` because `300` does not fit `uint8`, new behaviour raises an error for the same reason.

  - <span id="T5" class="citation-label">T5</span>  
    `300` cannot be converted to `uint8`.

  - <span id="T6" class="citation-label">T6</span>  
    One of the most dangerous changes maybe. Retaining the type leads to overflow. A `RuntimeWarning` indicating overflow is given for the NumPy scalars.

  - <span id="T7" class="citation-label">T7</span>  
    `np.float32(3e100)` overflows to infinity with a warning.

  - <span id="T8" class="citation-label">T8</span>  
    `1 + 1e-14` loses precision when done in float32 but not in float64. The old behavior was casting the scalar argument to float32 or float64 differently depending on the dimensionality of the array; with the new behavior the computation is always done in the array precision (float32 in this case).

  - <span id="T9" class="citation-label">T9</span>  
    NumPy promotes `float32` and `int64` to `float64`. The old behaviour ignored the `int64` here.

</div>

1.  <https://github.com/numpy/numpy/pull/21103/files#r814188019>

2.  Each NEP must either be explicitly labeled as placed in the public domain (see this NEP as an example) or licensed under the [Open Publication License](https://www.opencontent.org/openpub/).

---

nep-0051-scalar-representation.md

---

# NEP 51 â€” Changing the representation of NumPy scalars

  - Author  
    Sebastian Berg

  - Status  
    Accepted

  - Type  
    Standards Track

  - Created  
    2022-09-13

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/message/U2A4RCJSXMK7GG23MA5QMRG4KQYFMO2S/>

## Abstract

NumPy has scalar objects ("NumPy scalar") representing a single value corresponding to a NumPy DType. The representation of these currently matches that of the Python builtins, giving:

    >>> np.float32(3.0)
    3.0

In this NEP we propose to change the representation to include the NumPy scalar type information. Changing the above example to:

    >>> np.float32(3.0)
    np.float32(3.0)

We expect that this change will help users distinguish the NumPy scalars from the Python builtin types and clarify their behavior.

The distinction between NumPy scalars and Python builtins will further become more important for users once \[NEP 50 \<NEP50\>\](\#nep-50-\<nep50\>) is adopted.

These changes do lead to smaller incompatible and infrastructure changes related to array printing.

## Motivation and scope

This NEP proposes to change the representation of the following NumPy scalars types to distinguish them from the Python scalars:

  - `np.bool_`
  - `np.uint8`, `np.int8`, and all other integer scalars
  - `np.float16`, `np.float32`, `np.float64`, `np.longdouble`
  - `np.complex64`, `np.complex128`, `np.clongdouble`
  - `np.str_`, `np.bytes_`
  - `np.void` (structured dtypes)

Additionally, the representation of the remaining NumPy scalars will be adapted to print as `np.` rather than `numpy.`:

  - `np.datetime64` and `np.timedelta64`
  - `np.void` (unstructured version)

The NEP does not propose to change how these scalars print â€“ only their representation (`__repr__`) will be changed. Further, array representation will not be affected since it already includes the `dtype=` when necessary.

The main motivation behind the change is that the Python numerical types behave differently from the NumPy scalars. For example numbers with lower precision (e.g. `uint8` or `float16`) should be used with care and users should be aware when they are working with them. All NumPy integers can experience overflow which Python integers do not. These differences will be exacerbated when adopting \[NEP 50 \<NEP50\>\](\#nep-50-\<nep50\>) because the lower precision NumPy scalar will be preserved more often. Even `np.float64`, which is very similar to Python's `float` and inherits from it, does behave differently for example when dividing by zero.

Another common source of confusion are the NumPy booleans. Python programmers sometimes write `obj is True` and will surprised when an object that shows as `True` fails to pass the test. It is much easier to understand this behavior when the value is shown as `np.True_`.

Not only do we expect the change to help users better understand and be reminded of the differences between NumPy and Python scalars, but we also believe that the awareness will greatly help debugging.

## Usage and impact

Most user code should not be impacted by the change, but users will now often see NumPy values shown as:

    np.True_
    np.float64(3.0)
    np.int64(34)

and so on. This will also mean that documentation and output in Jupyter notebook cells will often show the type information intact.

`np.longdouble` and `np.clongdouble` will print with single quotes:

    np.longdouble('3.0')

to allow round-tripping. Additionally to this change, `float128` will now always be printed as `longdouble` since the old name gives a false impression of precision.

## Backward compatibility

We expect that most workflows will not be affected as only printing changes. In general we believe that informing users about the type they are working with outweighs the need for adapting printing in some instances.

The NumPy test suite includes code such as `decimal.Decimal(repr(scalar))`. This code needs to be modified to use the `str()`.

An exception to this are downstream libraries with documentation and especially documentation testing. Since the representation of many values will change, in many cases the documentation will have to be updated. This is expected to require larger documentation fixups in the mid-term.

It may be necessary to adopt tools for doctest testing to allow approximate value checking for the new representation.

### Changes to `arr.tofile()`

`arr.tofile()` currently stores values as `repr(arr.item())` when in text mode. This is not always ideal since that may include a conversion to Python. One issue is that this would start saving longdouble as `np.longdouble('3.1')` which is clearly not desired. We expect that this method is rarely used for object arrays. For string arrays, using the `repr` also leads to storing `"string"` or `b"string"` which seems rarely desired.

The proposal is to change the default (back) to use `str` rather than `repr`. If `repr` is desired, users will have to pass `fmt=%r`.

## Detailed description

This NEP proposes to change the representation for NumPy scalars to:

  - `np.True_` and `np.False_` for booleans (their singleton instances)
  - `np.scalar(<value>)`, i.e. `np.float64(3.0)` for all numerical dtypes.
  - The value for `np.longdouble` and `np.clongdouble` will be given in quotes: `np.longdouble('3.0')`. This ensures that it can always roundtrip correctly and matches the way that `decimal.Decimal` behaves. For these two the size-based name such as `float128` will not be used as the actual size is platform-dependent and therefore misleading.
  - `np.str_("string")` and `np.bytes_(b"byte_string")` for string dtypes.
  - `np.void((3, 5), dtype=[('a', '<i8'), ('b', 'u1')])` (similar to arrays) for structured types. This will be valid syntax to recreate the scalar.

Unlike arrays, the scalar representation should round-trip correctly, so longdouble values will be quoted and other values never be truncated.

In some places (i.e. masked arrays, void and record scalars) we will want to print the representation without the type. For example:

    np.void(('3.0',), dtype=[('a', 'f16')])  # longdouble

should print the 3.0 with quotes (to ensure round-tripping), but not repeat the full `np.longdouble('3.0')` as the dtype includes the longdouble information. To allow this, a new semi-public `np.core.array_print.get_formatter()` will be introduced to expand the current functionality (see Implementation).

### Effects on masked arrays and records

Some other parts of NumPy will indirectly be changed. Masked arrays `fill_value` will be adapted to only include the full scalar information such as `fill_value=np.float64(1e20)` when the dtype of the array mismatches. For longdouble (with matching dtype), it will be printed as `fill_value='3.1'` including the quotes which (in principle but likely not in practice) ensure round-tripping. It should be noted that for strings it is typical for the dtypes to mismatch in the string length. So that strings will usually be printed as `np.str_("N/A")`.

The `np.record` scalar will be aligned with `np.void` and print identically to it (except the name itself). For example as: `np.record((3, 5), dtype=[('a', '<i8'), ('b', 'u1')])`

### Details about `longdouble` and `clongdouble`

For `longdouble` and `clongdouble` values such as:

    np.sqrt(np.longdouble(2.))

may not roundtrip unless quoted as strings (as the conversion to a Python float would lose precision). This NEP proposes to use a single quote similar to Python's decimal which prints as `Decimal('3.0')`

`longdouble` can have different precision and storage sizes varying from 8 to 16 bytes. However, even if `float128` is correct because the number is stored as 128 bits, it normally does not have 128 bit precision. (`clongdouble` is the same, but with twice the storage size.)

This NEP thus includes the proposal of changing the name of `longdouble` to always print as `longdouble` and never `float128` or `float96`. It does not include deprecating the `np.float128` alias. However, such a deprecation may occur independently of the NEP.

### Integer scalar type name and instance representation

One detail is that due to NumPy scalar types being based on the C types, NumPy sometimes distinguishes them, for example on most 64 bit systems (not windows):

    >>> np.longlong
    numpy.longlong
    >>> np.longlong(3)
    np.int64(3)

The proposal will lead to the `longlong` name for the type while using the `int64` form for the scalar. This choice is made since `int64` is generally the more useful information for users, but the type name itself must be precise.

## Related work

A PR to only change the representation of booleans was previously made [here](https://github.com/numpy/numpy/pull/17592).

The implementation is (at the time of writing) largely finished and can be found [here](https://github.com/numpy/numpy/pull/22449)

## Implementation

<div class="note">

<div class="title">

Note

</div>

This part has *not* been implemented in the [initial PR](https://github.com/numpy/numpy/pull/22449). A similar change will be required to fix certain cases in printing and allow fully correct printing e.g. of structured scalars which include longdoubles. A similar solution is also expected to be necessary in the future to allow custom DTypes to correctly print.

</div>

The new representations can be mostly implemented on the scalar types with the largest changes needed in the test suite.

The proposed changes for void scalars and masked `fill_value` makes it necessary to expose the scalar representation without the type.

We propose introducing the semi-public API:

    np.core.arrayprint.get_formatter(*,
            data=None, dtype=None, fmt=None, options=None)

to replace the current internal `_get_formatting_func`. This will allow two things compared to the old function:

  - `data` may be `None` (if `dtype` is passed) allowing to not pass multiple values that will be printed/formatted later.

  - `fmt=` will allow passing on format strings to a DType-specific element formatter in the future. For now, `get_formatter()` will accept `repr` or `str` (the singletons not strings) to format the elements without type information (`'3.1'` rather than `np.longdouble('3.1')`). The implementation ensures that formatting matches except for the type information.
    
    The empty format string will print identically to `str()` (with possibly extra padding when data is passed).

`get_formatter()` is expected to query a user DType's method in the future allowing customized formatting for all DTypes.

Making `get_formatter` public allows it to be used for `np.record` and masked arrays. Currently, the formatters themselves seem semi-public; using a single entry-point will hopefully provide a clear API for formatting NumPy values.

The large part for the scalar representation changes had previously been done by Ganesh Kathiresan in\[1\].

## Alternatives

Different representations can be considered: alternatives include spelling `np.` as `numpy.` or dropping the `np.` part from the numerical scalars. We believe that using `np.` is sufficiently clear, concise, and does allow copy pasting the representation. Using only `float64(3.0)` without the `np.` prefix is more concise but contexts may exists where the NumPy dependency is not fully clear and the name could clash with other libraries.

For booleans an alternative would be to use `np.bool_(True)` or `bool_(True)`. However, NumPy boolean scalars are singletons and the proposed formatting is more concise. Alternatives for booleans were also discussed previously in\[2\].

For the string scalars, the confusion is generally less pronounced. It may be reasonable to defer changing these.

### Non-finite values

The proposal does not allow copy pasting `nan` and `inf` values. They could be represented by `np.float64('nan')` or `np.float64(np.nan)` instead. This is more concise and Python also uses `nan` and `inf` rather than allowing copy-pasting by showing it as `float('nan')`. Arguably, it would be a smaller addition in NumPy, where the will already be always printed.

### Alternatives for the new `get_formatter()`

When `fmt=` is passed, and specifically for the main use (in this NEP) to format to a `repr` or `str`. It would also be possible to use a ufunc or a direct formatting function rather than wrapping it into a `` `get_formatter() `` which relies on instantiating a formatter class for the DType.

This NEP does not preclude creating a ufunc or making a special path. However, NumPy array formatting commonly looks at all values to be formatted in order to add padding for alignment or give uniform exponential output. In this case `data=` is passed and used in preparation. This form of formatting (unlike the scalar case where `data=None` would be desired) is unfortunately fundamentally incompatible with UFuncs.

The use of the singleton `str` and `repr` ensures that future formatting strings like `f"{arr:r}"` are not in any way limited by using `"r"` or `"s"` instead.

## Discussion

  - An discussion on this changed happened in the mailing list thread: <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/7GLGFHTZHJ6KQPOLMVY64OM6IC6KVMYI/>
  - There was a previous issue\[3\] and PR\[4\] to change only the representation of the NumPy booleans. The PR was later updated to change the representation of all (or at least most) NumPy scalars.

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  <https://github.com/numpy/numpy/pull/17592>

2.  <https://github.com/numpy/numpy/issues/12950>

3.  <https://github.com/numpy/numpy/issues/12950>

4.  <https://github.com/numpy/numpy/pull/17592>

---

nep-0052-python-api-cleanup.md

---

# NEP 52 â€” Python API cleanup for NumPy 2.0

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Author  
    StÃ©fan van der Walt \<<stefanv@berkeley.edu>\>

  - Author  
    Nathan Goldbaum \<<ngoldbaum@quansight.com>\>

  - Author  
    Mateusz SokÃ³Å‚ \<<msokol@quansight.com>\>

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2023-03-28

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/QLMPFTWA67DXE3JCUQT2RIRLQ44INS4F/>

## Abstract

We propose to clean up NumPy's Python API for the NumPy 2.0 release. This includes a more clearly defined split between what is public and what is private, and reducing the size of the main namespace by removing aliases and functions that have better alternatives. Furthermore, each function is meant to be accessible from only one place, so all duplicates also need to be dropped.

## Motivation and scope

NumPy has a large API surface that evolved organically over many years:

``` python
>>> objects_in_api = [s for s in dir(np) if not s.startswith('_')]
>>> len(objects_in_api)
562
>>> modules = [s for s in objects_in_api if inspect.ismodule(eval(f'np.{s}'))]
>>> modules
['char', 'compat', 'ctypeslib', 'emath', 'fft', 'lib', 'linalg', 'ma', 'math', 'polynomial', 'random', 'rec', 'testing', 'version']
>>> len(modules)
14
```

The above doesn't even include items that are public but have been hidden from `__dir__`. A particularly problematic example of that is `np.core`, which is technically private but heavily used in practice. For a full overview of what's considered public, private or a bit in between, see <https://github.com/numpy/numpy/blob/main/numpy/tests/test_public_api.py>.

The size of the API and the lacking definition of its boundaries incur significant costs:

  - **Users find it hard to disambiguate between similarly named functions.**
    
    Looking for functions with tab completion in IPython, a notebook, or an IDE is a challenge. E.g., type `np.<TAB>` and look at the first six items offered: two ufuncs (`abs`, `add`), one alias (`absolute`), and three functions that are not intended for end-users (`add_docstring`, `add_newdoc`, `add_newdoc_ufunc`). As a result, the learning curve for NumPy is steeper than it has to be.

  - **Libraries that mimic the NumPy API face significant implementation barriers.**
    
    For maintainers of NumPy API-compatible array libraries (Dask, CuPy, JAX, PyTorch, TensorFlow, cuNumeric, etc.) and compilers/transpilers (Numba, Pythran, Cython, etc.) there is an implementation cost to each object in the namespace. In practice, no other library has full support for the entire NumPy API, partly because it is so hard to know what to include when faced with a slew of aliases and legacy objects.

  - **Teaching NumPy is more complicated than it needs to be.**
    
    Similarly, a larger API is confusing to learners, who not only have to *find* functions but have to choose *which* functions to use.

  - **Developers are hesitant to grow the API surface.**
    
    This happens even when the changes are warranted, because they are aware of the above concerns.

The scope of this NEP includes:

  - Deprecating or removing functionality that is too niche for NumPy, not well-designed, superseded by better alternatives, an unnecessary alias, or otherwise a candidate for removal.
  - Clearly separating public from private NumPy API by use of underscores.
  - Restructuring the NumPy namespaces to be easier to understand and navigate.

Out of scope for this NEP are:

  - Introducing new functionality or performance enhancements.

## Usage and impact

A key principle of this API refactor is to ensure that, when code has been adapted to the changes and is 2.0-compatible, that code then *also* works with NumPy `1.2x.x`. This keeps the burden on users and downstream library maintainers low by not having to carry duplicate code which switches on the NumPy major version number.

## Backward compatibility

As mentioned above, while the new (or cleaned up, NumPy 2.0) API should be backward compatible, there is no guarantee of forward compatibility from 1.25.X to 2.0. Code will have to be updated to account for deprecated, moved, or removed functions/classes, as well as for more strictly enforced private APIs.

In order to make it easier to adopt the changes in this NEP, we will:

1.  Provide a transition guide that lists each API change and its replacement.
2.  Explicitly flag all expired attributes with a meaningful `AttributeError` that points out to the new place or recommends an alternative.
3.  Provide a script to automate the migration wherever possible. This will be similar to `tools/replace_old_macros.sed` (which adapts code for a previous C API naming scheme change). This will be `sed` (or equivalent) based rather than attempting AST analysis, so it won't cover everything.

## Detailed description

### Cleaning up the main namespace

We expect to reduce the main namespace by a large number of entries, on the order of 100. Here is a representative set of examples:

  - `np.inf` and `np.nan` have 8 aliases between them, of which most can be removed.
  - A collection of random and undocumented functions (e.g., `byte_bounds`, `disp`, `safe_eval`, `who`) listed in [gh-12385](https://github.com/numpy/numpy/issues/12385) can be deprecated and removed.
  - All `*sctype` functions can be deprecated and removed, they (see [gh-17325](https://github.com/numpy/numpy/issues/17325), [gh-12334](https://github.com/numpy/numpy/issues/12334), and other issues for `maximum_sctype` and related functions).
  - The `np.compat` namespace, used during the Python 2 to 3 transition, will be removed.
  - Functions that are narrow in scope, with very few public use-cases, will be removed. These will have to be identified manually and by issue triage.

New namespaces are introduced for warnings/exceptions (`np.exceptions`) and for dtype-related functionality (`np.dtypes`). NumPy 2.0 is a good opportunity to populate these submodules from the main namespace.

Functionality that is widely used but has a preferred alternative may either be deprecated (with the deprecation message pointing out what to use instead) or be hidden by not including it in `__dir__`. In case of hiding, a `.. legacy::` directory may be used to mark such functionality in the documentation.

A test will be added to ensure limited future growth of all namespaces; i.e., every new entry will need to be explicitly added to an allow-list.

### Cleaning up the submodule structure

We will clean up the NumPy submodule structure, so it is easier to navigate. When this was discussed before (see [MAINT: Hide internals of np.lib to only show submodules](https://github.com/numpy/numpy/pull/18447)) there was already rough consensus on that - however it was hard to pull off in a minor release.

A basic principle we will adhere to is "one function, one location". Functions that are exposed in more than one namespace (e.g., many functions are present in `numpy` and `numpy.lib`) need to find a single home.

We will reorganize the API reference guide along main and submodule namespaces, and only within the main namespace use the current subdivision along functionality groupings. Also by "mainstream" and special-purpose namespaces:

    # Regular/recommended user-facing namespaces for general use. Present these
    # as the primary set of namespaces to the users.
    numpy
    numpy.exceptions
    numpy.fft
    numpy.linalg
    numpy.polynomial
    numpy.random
    numpy.testing
    numpy.typing
    
    # Special-purpose namespaces. Keep these, but document them in a separate
    # grouping in the reference guide and explain their purpose.
    numpy.array_api
    numpy.ctypeslib
    numpy.emath
    numpy.f2py  # only a couple of public functions, like `compile` and `get_include`
    numpy.lib.stride_tricks
    numpy.lib.npyio
    numpy.rec
    numpy.dtypes
    numpy.array_utils
    
    # Legacy (prefer not to use, there are better alternatives and/or this code
    # is deprecated or isn't reliable). This will be a third grouping in the
    # reference guide; it's still there, but de-emphasized and the problems
    # with it or better alternatives are explained in the docs.
    numpy.char
    numpy.distutils
    numpy.ma
    numpy.matlib
    
    # To remove
    numpy.compat
    numpy.core  # rename to _core
    numpy.doc
    numpy.math
    numpy.version  # rename to _version
    numpy.matrixlib
    
    # To clean out or somehow deal with: everything in `numpy.lib`

\> **Note** \> TBD: will we preserve `np.lib` or not? It only has a couple of unique functions/objects, like `Arrayterator` (a candidate for removal), `NumPyVersion`, and the `stride_tricks`, `mixins` and `format` subsubmodules. `numpy.lib` itself is not a coherent namespace, and does not even have a reference guide page.

We will make all submodules available lazily, so that users don't have to type `import numpy.xxx` but can use `import numpy as np; np.xxx.*`, while at the same time not negatively impacting the overhead of `import numpy`. This has been very helpful for teaching scikit-image and SciPy, and it resolves a potential issue for Spyder users because Spyder already makes all submodules available - so code using the above import pattern then works in Spyder but not outside it.

### Reducing the number of ways to select dtypes

The many dtype classes, instances, aliases and ways to select them are one of the larger usability problems in the NumPy API. E.g.:

``` python
>>> # np.intp is different, but compares equal too
>>> np.int64 == np.int_ == np.dtype('i8') == np.sctypeDict['i8']
True
>>> np.float64 == np.double == np.float_ == np.dtype('f8') == np.sctypeDict['f8']
True
### Really?
>>> np.clongdouble == np.clongfloat == np.longcomplex == np.complex256
True
```

These aliases can go: <https://numpy.org/devdocs/reference/arrays.scalars.html#other-aliases>

All one-character type code strings and related routines like `mintypecode` will be marked as legacy.

To discuss:

  - move *all* dtype-related classes to `np.dtypes`?
  - canonical way to compare/select dtypes: `np.isdtype` (new, xref array API NEP), leaving `np.issubdtype` for the more niche use of numpy's dtype class hierarchy, and hide most other stuff.
  - possibly remove `float96`/`float128`? they're aliases that may not exist, and are too easy to shoot yourself in the foot with.

### Cleaning up the niche methods on `numpy.ndarray`

The `ndarray` object has a lot of attributes and methods, some of which are too niche to be that prominent, all that does is distract the average user. E.g.:

  - `.itemset` (already discouraged)
  - `.newbyteorder` (too niche)
  - `.ptp` (niche, use `np.ptp` function instead)

## API changes considered and rejected

For some functions and submodules it turned out that removing them would cause too much disruption or would require an amount of work disproportional to the actual gain. We arrived at this conclusion for such items:

  - Removing business day functions: `np.busday_count`, `np.busday_offset`, `np.busdaycalendar`.
  - Removing `np.nan*` functions and introducing new `nan_mode` argument to the related base functions.
  - Hiding histogram functions in the `np.histograms` submodule.
  - Hiding `c_`, `r_` and `s_` in the `np.lib.index_tricks` submodule.
  - Functions that looked niche but are present in the Array API (for example `np.can_cast`).
  - Removing `.repeat` and `.ctypes` from `ndarray` object.

## Related work

A clear split between public and private API was recently established as part of SciPy 1.8.0 (2021), see [tracking issue scipy\#14360](https://github.com/scipy/scipy/issues/14360). The results were beneficial, and the impact on users relatively modest.

## Implementation

The implementation has been split over many different PRs, each touching on a single API or a set of related APIs. Here's a sample of the most impactful PRs:

  - [gh-24634: Rename numpy/core to numpy/\_core](https://github.com/numpy/numpy/pull/24634)
  - [gh-24357: Cleaning numpy/\_\_init\_\_.py and main namespace - Part 2](https://github.com/numpy/numpy/pull/24357)
  - [gh-24376: Cleaning numpy/\_\_init\_\_.py and main namespace - Part 3](https://github.com/numpy/numpy/pull/24376)

The complete list of cleanup work done in the 2.0 release can be found by searching a dedicated label:

  - [Numpy 2.0 API Changes:](https://github.com/numpy/numpy/labels/Numpy%202.0%20API%20Changes)

Some PRs has already been merged and shipped with the <span class="title-ref">1.25.0</span> release. For example, deprecating non-preferred aliases:

  - [gh-23302: deprecate np.round\_; add round/min/max to the docs](https://github.com/numpy/numpy/pull/23302)
  - [gh-23314: deprecate product/cumproduct/sometrue/alltrue](https://github.com/numpy/numpy/pull/23314)

Hiding or removing objects that are accidentally made public or not even NumPy objects at all:

  - [gh-21403: remove some names from main numpy namespace](https://github.com/numpy/numpy/pull/21403)

Creation of new namespaces to make it easier to navigate the module structure:

  - [gh-22644: Add new np.exceptions namespace for errors and warnings](https://github.com/numpy/numpy/pull/22644)

## Alternatives

## Discussion

  - [gh-23999: Tracking issue for the NEP 52](https://github.com/numpy/numpy/issues/23999)
  - [gh-24306: Overhaul of the main namespace](https://github.com/numpy/numpy/issues/24306)
  - [gh-24507: Overhaul of the np.lib namespace](https://github.com/numpy/numpy/issues/24507)

## References and footnotes

## Copyright

This document has been placed in the public domain.

---

nep-0053-c-abi-evolution.md

---

# NEP 53 â€” Evolving the NumPy C-API for NumPy 2.0

  - Author  
    Sebastian Berg \<<sebastianb@nvidia.com>\>

  - Status  
    Draft

  - Type  
    Standard

  - Created  
    2022-04-10

## Abstract

The NumPy C-API is used in downstream projects (often through Cython) to extend NumPy functionality. Supporting these packages generally means that it is slow to evolve our C-API and some changes are not possible in a normal NumPy release because NumPy must guarantee backwards compatibility: A downstream package compiled against an old NumPy version (e.g. 1.17) will generally work with a new NumPy version (e.g. 1.25).

A NumPy 2.0 release allows to *partially* break this promise: We can accept that a SciPy version compiled with NumPy 1.17 (e.g. SciPy 1.10) will *not* work with NumPy 2.0. However, it must still be easy to create a single SciPy binary that is compatible with both NumPy 1.x and NumPy 2.0.

Given these constraints this NEP outlines a path forward to allow large changes to our C-API. Similar to Python API changes proposed for NumPy 2.0 the NEP aims to allow changes to an extend that *most* downstream packages are expected to need no or only minor code changes.

The implementation of this NEP consists would consist of two steps:

1.  As part of a general improvement, starting with NumPy 1.25 building with NumPy will by default export an older API version to allow backwards compatible builds with the newest available NumPy version. (New API is not available unless opted-in.)
2.  The NumPy 2.0 will:
      - require recompilation of downstream packages against NumPy 2.0 to be compatible with NumPy 2.0.
      - need a `numpy2_compat` as a dependency when running on NumPy 1.x.
      - require some downstream code changes to adapt to changed API.

## Motivation and scope

The NumPy API conists of more than 300 functions and numerous macros. Many of these are outdated: some were only ever used within NumPy, exist only for compatibility with NumPy's predecessors, or have no or only a single known downstream user (i.e. SciPy).

Further, many structs used by NumPy have always been public making it impossible to change them outside of a major release. Some changes have been planned for years and were the reason for `NPY_NO_DEPRECATED_API` and further deprecations as explained in \[c\_api\_deprecations\](\#c\_api\_deprecations).

While we probably have little reason to change the layout of the array struct (`PyArrayObject_fields`) for example the development and improvement of dtypes would be made easier by changing the <span class="title-ref">PyArray\_Descr</span> struct.

This NEP proposes a few concrete changes to our C-API mainly as examples. However, more changes will be handled on a case-by-case basis, and we do not aim to provide a full list of changes in this NEP.

### Adding state is out of scope

New developments such as CPython's support for subinterpreters and the HPy API may require the NumPy C-API to evolve in a way that may require (or at least prefer) state to be passed in.

As of now, we do not aim to include changes for this here. We cannot expect users to do large code updates to pass in for example an `HPy` context to many NumPy functions.

While we could introduce a second API for this purpose in NumPy 2.0, we expect that this is unnecessary and that the provisions introduced here:

  - the ability to compile with the newest NumPy version but be compatible with older versions,
  - and the possibility of updating a `numpy2_compat` package.

should allow to add such an API also in a minor release.

## Usage and impact

### Backwards compatible builds

Backwards compatible builds will be described in more details in the documentation. Briefly, we will allow users to use a definition like:

    #define NPY_TARGET_VERSION NPY_1_22_API_VERSION

to select the version they wish to compile for (lowest version to be compatible with). By default the backwards compatibility will be such that the resulting binary is compatible with the oldest NumPy version which supports the same version of Python: NumPy 1.19.x was the first to support Python 3.9 and NumPy 1.25 supports Python 3.9 or greater, so NumPy 1.25 defaults to 1.19 compatibility. Thus, users of *new* API may be required to add the define, but users of who want to be compatible with older versions need not do anything unless they wish to have exceptionally long compatibility.

The API additions in the past years were so limited that such a change should be necessary at most for a hand-full of users worldwide.

This mechanism is much the same as the [Python limited API](https://docs.python.org/3/c-api/stable.html) since NumPy's C-API has a similar need for ABI stability.

### Breaking the C-API and changing the ABI

NumPy has too many functions, many of which are aliases. The following lists *examples* of things we plan to remove and users will have to adapt to be compatible with NumPy 2.0:

  - `PyArray_Mean` and `PyArray_Std` are untested implementation similar to `arr.mean()` and `arr.std()`. We are planning on removing these as they can be replaced with method calls relatively easily.
  - The `MapIter` set of API functions (and struct) allows to implement advanced indexing like semantics downstream. There was a single *historic* known user of this (theano) and the use-case would be faster and easier to implement in a different way. The API is complicated, requires reaching deep into NumPy to be useful and its exposure makes the implementation more difficult. Unless new important use cases are found, we propose to remove it.

An example for an ABI change is to change the layout of `PyArray_Descr` (the struct of `np.dtype` instances) to allow a larger maximum itemsize and new flags (useful for future custom user DTypes). For this specific change, users who access the structs fields directly will have to change their code. A downstream search shows that this should not be very common, the main impacts are:

  - Access of the `descr->elsize` field (and others) would have to be replaced with a macro's like `PyDataType_ITEMSIZE(descr)` (NumPy may include a version check when needed).
  - Implementers of user defined dtypes, will have to change a few lines of code and luckily, there are very few of such user defined dtypes. (The details are that we rename the struct to `PyArray_DescrProto` for the static definition and fetch the actual instance from NumPy explicitly.)

A last example is increasing `NPY_MAXDIMS` to `64`. `NPY_MAXDIMS` is mainly used to statically allocate scratch space:

    func(PyArrayObject *arr) { 
        npy_intp shape[NPY_MAXDIMS];
        /* Work with a shape or strides from the array */
    }

If NumPy changed it to 64 in a minor release, this would lead to undefined behavior if the code was compiled with `NPY_MAXDIMS=32` but a 40 dimensional array is passed in. But the larger value is also a correct maximum on previous versions of NumPy making it generally safe for NumPy 2.0 change. (One can imagine code that wants to know the actual runtime value. We have not seen such code in practice, but it would need to be adjusted.)

### Impact on Cython users

Cython users may use the NumPy C-API via `cimport numpy as cnp`. Due to the uncertainty of Cython development, there are two scenarios for impact on Cython users.

If Cython 3 can be relied on, Cython users would be impacted *less* than C-API users, because Cython 3 allows us to hide struct layout changes (i.e. changes to `PyArray_Descr`). If this is not the case and we must support Cython 0.29.x (which is the historic branch before Cython 3), then Cython users will also have to use a function/macro like `PyDataType_ITEMSIZE()` (or use the Python object). This is unfortunately less typical in Cython code, but also unlikely to be a common pattern for dtype struct fields/attributes.

A further impact is that some future API additions such as new classes may need to placed in a distinct `.pyd` file to avoid Cython generating code that would fail on older NumPy versions.

### End-user and packaging impact

Packaging in a way that is compatible with NumPy 2.0 will require a recompilation of downstream libraries that rely on the NumPy C-API. This may take some time, although hopefully the process will start before NumPy 2.0 is itself released.

Further, to allow bigger changes more easily in NumPy 2.0, we expect to create a `numpy2_compat` package. When a library is build with NumPy 2.0 but wants to support NumPy 1.x it will have to depend on `numpy2_compat`. End-users should not need to be aware of this dependency and an informative error can be raised when the module is missing.

### Some new API can be backported

One large advantage of allowing users to compile with the newest version of NumPy is that in some cases we will be able to backport new API. Some new API functions can be written in terms of old ones or included directly.

\> **Note** \> It may be possible to make functions public that were present but private in NumPy 1.x public via the compatible `numpy2_compat` package.

This means that at some new API additions could be made available to downstreams users faster. They would require a new NumPy version for *compilation* but their wheels can be backwards compatible with earlier versions.

## Implementation

The first part of implementation (allowing building for an earlier API version) is very straight forward since the NumPy C-API evolved slowly for many years. Some struct fields will be hidden by default and functions introduced in a more recent version will be marked and hidden unless the user opted in to a newer API version. An implementation can be found in the [PR 23528](https://github.com/numpy/numpy/pull/23528).

The second part is mainly about identifying and implementing the desired changes in a way that backwards compatibility will not be broken and API breaks remain manageable for downstream libraries. Every change we do must have a brief note on how to adapt to the API change (i.e. alternative functions).

### NumPy 2 compatibility and API table changes

To allow changing the API table, NumPy 2.0 would ship a different table than NumPy 1.x (a table is a list of functions and symbols).

For compatibility we would need to translate the 1.x table to the 2.0 table. This could be done in headers only in theory, but this seems unwieldy. We thus propose to add a `numpy2_compat` package. This package's main purpose would be to provide a translation of the 1.x table to the 2.x one in a single place (filling in any necessary blanks).

Introducing this package solves the "transition" issue because it allows a user to:

  - Install a SciPy version that is compatible with 2.0 and 1.x
  - and keep using NumPy 1.x because of other packages they are using are not yet compatible.

The import of `numpy2_compat` (and an error when it is missing) will be inserted by the NumPy headers as part of the `import_array()` call.

## Alternatives

There are always possibilities to decide not to do certain changes (e.g. due to downstream users noting their continued need for it). For example, the function `PyArray_Mean` could be replaced by one to call `array.mean()` if necessary.

The NEP proposes to allow larger changes to our API table by introducing a compatibility package `numpy2_compat`. We could do many changes without introducing such a package.

The default API version could be chosen to be older or as the current one. An older version would be aimed at libraries who want a larger compatibility than NEP 29 suggests. Choosing the current would default to removing unnecessary compatibility shims for users who do not distribute wheels. The suggested default chooses to favors libraries that distribute wheels and wish a compatibility range similar to NEP 29. This is because compatibility shims should be light-weight and we expect few libraries require a longer compatibility.

## Backward compatibility

As mentioned above backwards compatibility is achieved by:

1.  Forcing downstream to recompile with NumPy 2.0
2.  Providing a `numpy2_compat` library.

But relies on users to adapt to changed C-API as described in the Usage and Impact section.

## Discussion

  - <https://github.com/numpy/numpy/issues/5888> brought up previously that it would be helpful to allow exporting of an older API version in our headers. This was never implemented, instead we relied on [oldest-support-numpy](https://github.com/scipy/oldest-supported-numpy).
  - A first draft of this proposal was presented at the NumPy 2.0 planning meeting 2023-04-03.

## References and footnotes

## Copyright

This document has been placed in the public domain.\[1\]

1.  Each NEP must either be explicitly labeled as placed in the public domain (see this NEP as an example) or licensed under the [Open Publication License](https://www.opencontent.org/openpub/).

---

nep-0054-simd-cpp-highway.md

---

# NEP 54 â€” SIMD infrastructure evolution: adopting Google Highway when moving to C++?

  - Author  
    Sayed Adel, Jan Wassenberg, Matti Picus, Ralf Gommers, Chris Sidebottom

  - Status  
    Draft

  - Type  
    Standards Track

  - Created  
    2023-07-06

  - Resolution  
    TODO

## Abstract

We are moving the SIMD intrinsic framework, Universal Intrinsics, from C to C++. We have also moved to Meson as the build system. The Google Highway intrinsics project is proposing we use Highway instead of our Universal Intrinsics as described in \[NEP 38 \<NEP38\>\](\#nep-38-\<nep38\>). This is a complex and multi-faceted decision - this NEP is an attempt to describe the trade-offs involved and what would need to be done.

## Motivation and Scope

We want to refactor the C-based Universal Intrinsics (see \[NEP 38 \<NEP38\>\](\#nep-38 \<nep38\>)) to C++. This work was ongoing for some time, and Google's Highway was suggested as an alternative, which was already written in C++ and had support for scalable SVE and other reusable components (such as VQSort).

The move from C to C++ is motivated by (a) code readability and ease of development, (b) the need to add support for sizeless SIMD instructions (e.g., ARM's SVE, RISC-V's RVV).

As an example of the readability improvement, here is a typical line of C code from our current C universal intrinsics framework:

    // The @name@ is the numpy-specific templating in .c.src files
    npyv_@sfx@  a5 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 4);

This will change (as implemented in PR [gh-21057](https://github.com/numpy/numpy/pull/21057)) to:

``` C++
auto a5 = Load(src1 + nlanes * 4);
```

If the above C++ code were to use Highway under the hood it would look quite similar, it uses similarly understandable names as `Load` for individual portable intrinsics.

The `@sfx` in the C version above is the template variable for type identifiers, e.g.: `#sfx = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#`. Explicit use of bitsize-encoded types like this won't work for sizeless SIMD instruction sets. With C++ this is easier to handle; PR [gh-21057](https://github.com/numpy/numpy/pull/21057) shows how and contains more complete examples of what the C++ code will look like.

The scope of this NEP includes discussing most relevant aspects of adopting Google Highway to replace our current Universal Intrinsics framework, including but not limited to:

  - Maintainability, domain expertise availability, ease of onboarding new contributor, and other social aspects,
  - Key technical differences and constraints that may impact NumPy's internal design or performance,
  - Build system related aspects,
  - Release timing related aspects.

Out of scope (at least for now) is revisiting other aspects of our current SIMD support strategy:

  - accuracy vs. performance trade-offs when adding SIMD support to a function
  - use of SVML and x86-simd-sort (and possibly its equivalents for aarch64)
  - pulling in individual bits or algorithms of Highway (as in [gh-24018](https://github.com/numpy/numpy/pull/24018)) or SLEEF (as discussed in that same PR)

## Usage and Impact

N/A - there will be no significant user-visible changes.

## Backward compatibility

There will be no changes in user-facing Python or C APIs: all the methods to control compilation and runtime CPU feature selection should remain, although there may be some changes due to moving to C++ without regards to the Highway/Universal Intrinsics choice.

The naming of the CPU features in Highway is different from that of the Universal Intrinsics (see "Supported features/targets" below)

On Windows, MSVC may have to be avoided, as a result of Highway's use of pragmas which are less well supported by MSVC. This means that we likely have to build our wheels with clang-cl or Mingw-w64. Both of those should work - we merged clang-cl support a while back (see [gh-20866](https://github.com/numpy/numpy/pull/20866)), and SciPy builds with Mingw-w64. It may however impact other redistributors or end users who build from source on Windows.

In response to the earlier discussions around this NEP, Highway is now dual-licensed as Apache 2 / BSD-3.

## High-level considerations

\> **Note** \> Currently this section attempts to cover each topic separately, and comparing the future use of a NumPy-specific C++ implementation vs. use of Google Highway with our own numerical routines on top of that. It does not (yet) assume a decision or proposed decision is made. Hence this NEP is not "this is proposed" with another option in the Alternatives section, but rather a side-by-side comparison.

### Development effort and long-term maintainability

Moving to Highway is likely to be a significant development effort. Longer-term, this will hopefully be offset by Highway itself having more maintainer bandwidth to deal with ongoing issues in compiler support and adding new platforms.

Highway being used by other projects, like Chromium and [JPEG XL](https://github.com/libjxl/libjxl) (see [this more complete list](https://google.github.io/highway/en/master/README.html#examples) in the Highway documentation), does imply that there is likely to be a benefit of a wider range of testing and bug reporting/fixing.

One concern is that new instructions may have to be added, and that that is often best done as part of the process of developing the numerical kernel that needs the instruction. This will be a little more clumsy if the instruction lives in Highway which is a git submodule inside the NumPy repo - there will be a need to implement a temporary/generic version first, and then update the submodule after upstreaming the new intrinsic.

Documentation-wise, Highway would be a clear win. NumPy's [CPU/SIMD Optimizations](https://numpy.org/doc/1.25/reference/simd/) docs are fairly sparse compared to [the Highway docs](https://google.github.io/highway/).

### Migration strategy - can it be gradual?

This is a story of two halves. Moving to Highway's statically dispatched intrinsics could be done gradually, as already seen in PR [gh-24018](https://github.com/numpy/numpy/pull/24018). However, adopting Highway's way of performing runtime dispatching has to be done in one go - we can't (or shouldn't) have two ways of doing that.

### Highway policies for compiler and platform support

When adding new instructions, Highway has a policy that they must be implemented in a way that fairly balances across CPU architectures.

Regarding the support status and whether all currently-supported architectures will remain supported, Jan stated that Highway can commit to the following:

1.  If it cross-compiles with Clang and can be tested via standard QEMU, it can go into Highway's CI.
2.  If it cross-compiles via clang/gcc and can be tested with a new QEMU (possibly with extra flags), then it can be support via manual testing before each Highway release.
3.  Existing targets will remain supported as long as they compile/run in QEMU.

Highway is not subject to Google's "no longer supported" strategy (or, as written in its README, *This is not an officially supported Google product*). That is not a bad thing; it means that it is less likely to go unsupported due to a Google business decision about the project. Quite a few well-known open source projects under the `google` GitHub org state this, e.g. [JAX](https://github.com/google/jax) and [tcmalloc](https://github.com/google/tcmalloc).

### Supported features/targets

Both frameworks support a large set of platforms and SIMD instruction sets, as well as generic scalar/fallback versions. The main differences right now are:

  - NumPy supports IBM Z-system (s390x, VX/VXE/VXE2) while Highway supports Z14, Z15.
  - Highway supports ARM SVE/SVE2 and RISC-V RVV (sizeless instructions), while NumPy does not.
      - The groundwork for sizeless SIMD support in NumPy has been done in [gh-21057](https://github.com/numpy/numpy/pull/21057), however SVE/SVE2 and RISC-V are not yet implemented there.

There is also a difference in the granularity of instruction set groups: NumPy supports a more granular set of architectures than Highway. See the list of targets for Highway [here](https://github.com/google/highway/#targets) (it's roughly per CPU family) and for NumPy [here](https://numpy.org/doc/1.25/reference/simd/build-options.html#supported-features) (roughly per SIMD instruction set). Hence with Highway we'd lose some granularity - but that is probably fine, we don't really need this level of granularity, and there isn't much evidence that users explicitly play with this to squeeze out the last bit of performance for their own CPU.

### Compilation strategy for multiple targets and runtime dispatching

Highway compiles once while using preprocessing tricks to generate multiple stanzas for each CPU feature within the same compilation unit (see the `foreach_target.h` usage and dynamic dispatch docs for how that is done). Universal Intrinsics generate multiple compilation units, one for each CPU feature group, and compiles multiple times, linking them all together (with different names) for runtime dispatch. The Highway technique may not work reliably on MSVC, the Universal Intrinsic technique does work on MSVC.

Which one is more robust? The experts disagree. Jan thinks that the Highway approach is more robust and in particular avoids the linker pulling in functions with too-new instructions into the final binary. Sayed thinks that the current NumPy approach (also used by OpenCV) is more robust, and in particular is less likely to run into compiler-specific bugs or catch them earlier. Both agree the meson build system allows specifying object link order, which produces more consistent builds. However that does tie NumPy to meson.

Matti and Ralf think the current build strategy is working well for NumPy and the advantages of changing the build and runtime dispatch, with possible unknown instabilities outweighs the advantages that adopting Highway's dynamic dispatch may bring.

Our experience of the past four years says that bugs with "invalid instruction" type crashes are invariably due to issues with feature detection - most often because users are running under emulation, and sometimes because there are actual issues with our CPU feature detection code. There is little evidence we're aware of of the linker pulling in a function which is compiled multiple times for different architectures and picking the one with unsupported instructions. To ensure to avoid the issue, it's advisable to keep numerical kernels inside the source code and refrain from defining non-inlined functions within cache-able objects.

### C++ refactoring considerations

We want to move from C to C++, which will naturally involve a significant amount of refactoring, for two main reasons:

  - get rid of the NumPy-specific templating language for more expressive C++
  - this would make using sizeless intrinsics (like for SVE) easier.

In addition, we see the following considerations:

  - If we use Highway, we would need to switch the C++ wrappers from universal intrinsics to Highway. On the other hand, the work to move to C++ is not complete.
  - If we use Highway, we'd need to rewrite existing kernels using Highway intrinsics. But again, moving to C++ requires touching all those kernels anyway.
  - One concern regarding Highway was whether it is possible to obtain a function pointer for an architecture-specific function instead of calling that function directly. This so that we can be sure that calling 1-D inner loop many times for a single Python API invocation does not incur the dispatching overhead many times. This was investigated: this can be done with Highway too.
  - A second concern was whether it's possible with Highway to allow the user at runtime to select or disable dispatching to certain instruction sets. This is possible.
  - Use of tags in Highway's C++ implementation reduces code duplication but the added templating makes C-level testing and tracing more complicated.

### The `_simd` unit testing module

Rewriting the `_simd testing` module to use C++ was done very recently in PR [gh-24069](https://github.com/numpy/numpy/pull/24069). It depends on the main PR for the move to C++, [gh-21057](https://github.com/numpy/numpy/pull/21057). It allows one to access the C++ intrinsics with almost the same signature, but from Python. This is a great way not only for testing, but also for designing new SIMD kernels.

It may be possible to add a similar testing and prototyping feature to Highway (which uses plain `googletest`), however currently the NumPy way is quite a bit nicer.

### Math routines

Math or numerical routines are written at a higher level of abstraction than the universal intrinsics that are the main focus of this NEP. Highway has only a limited number of math routines, and they are not precise enough for NumPy's needs. So either way, NumPy's existing routines (which use universal intrinsics) will stay, and if we go the Highway route they'll simply have to use Highway primitives internally. We could still use Highway sorting routines. If we do accept lower-precision routines (via a user-supplied choice, i.e. extending `errstate` to allow a precision option), we could use Highway-native routines.

There may be other libraries that have numerical routines that can be reused in NumPy (e.g., from SLEEF, or perhaps from JPEG XL or some other Highway-using libraries). There may be a small benefit here, but likely it doesn't matter too much.

### Supported and missing intrinsics

Some specific intrinsics that NumPy needs may be missing from Highway. Similarly, some intrinsics that NumPy needs to implement routines are already implemented in Highway and are missing from NumPy.

Highway has more instructions that NumPy's universal intrinsics, so it's possible that some future needs for NumPy kernels may already be met there.

Either way, we will always have to implement intrinsics in either solution.

## Related Work

  - [Google Highway](https://github.com/google/highway/)
  - [Xsimd](https://github.com/xtensor-stack/xsimd)
  - OpenCV's SIMD framework ([API reference](https://docs.opencv.org/4.x/df/d91/group__core__hal__intrin.html), [docs](https://github.com/opencv/opencv/wiki/CPU-optimizations-build-options))
  - [std::experimental::simd](https://en.cppreference.com/w/cpp/experimental/simd/simd)
  - See the Related Work section in \[NEP38\](\#nep38) for more related work (as of 2019)

## Implementation

TODO

## Alternatives

Use Google Highway for dynamic dispatch. Other alternatives include: do nothing and stay with C universal intrinsics, use [Xsimd](https://github.com/xtensor-stack/xsimd) as the SIMD framework (less comprehensive than Highway - no SVE or PowerPC support for example), or use/vendor [SLEEF](https://sleef.org/) (a good library, but inconsistently maintained). Neither of these alternatives seems appealing.

## Discussion

## References and Footnotes

## Copyright

This document has been placed in the public domain.\[1\]

1.  Each NEP must either be explicitly labeled as placed in the public domain (see this NEP as an example) or licensed under the [Open Publication License](https://www.opencontent.org/openpub/).

---

nep-0055-string_dtype.md

---

# NEP 55 â€” Add a UTF-8 variable-width string DType to NumPy

  - Author  
    Nathan Goldbaum \<<ngoldbaum@quansight.com>\>

  - Author  
    Warren Weckesser

  - Author  
    Marten van Kerkwijk

  - Status  
    Final

  - Type  
    Standards Track

  - Created  
    2023-06-29

  - Updated  
    2024-01-18

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/Y5CIKBZKMIOWSRYLJ64WV6DKM37QR76B/>

## Abstract

We propose adding a new string data type to NumPy where each item in the array is an arbitrary length UTF-8 encoded string. This will enable performance, memory usage, and usability improvements for NumPy users, including:

  - Memory savings for workflows that currently use fixed-width strings and store primarily ASCII data or a mix of short and long strings in a single NumPy array.
  - Downstream libraries and users will be able to move away from object arrays currently used as a substitute for variable-length string arrays, unlocking performance improvements by avoiding passes over the data outside of NumPy and allowing use of fast GIL-releasing C casts and string ufuncs for string operations.
  - A more intuitive user-facing API for working with arrays of Python strings, without a need to think about the in-memory array representation.

## Motivation and scope

First, we will describe how the current state of support for string or string-like data in NumPy arose. Next, we will summarize the last major previous discussion about this topic. Finally, we will describe the scope of the proposed changes to NumPy as well as changes that are explicitly out of scope of this proposal.

### History of string support in Numpy

Support in NumPy for textual data evolved organically in response to early user needs and then changes in the Python ecosystem.

Support for strings was added to NumPy to support users of the NumArray `chararray` type. Remnants of this are still visible in the NumPy API: string-related functionality lives in `np.char`, to support the `np.char.chararray` class. This class is not formally deprecated, but has a had comment in the module docstring suggesting to use string dtypes instead since NumPy 1.4.

NumPy's `bytes_` DType was originally used to represent the Python 2 `str` type before Python 3 support was added to NumPy. The bytes DType makes the most sense when it is used to represent Python 2 strings or other null-terminated byte sequences. However, ignoring trailing nulls means the `bytes_` DType is only suitable for fixed-width bytestreams that do not contain trailing nulls, so it is a possibly problematic match for generic bytestreams where trailing nulls need to round-trip through a NumPy string.

The `unicode` DType was added to support the Python 2 `unicode` type. It stores data in 32-bit UCS-4 codepoints (e.g. a UTF-32 encoding), which makes for a straightforward implementation, but is inefficient for storing text that can be represented well using a one-byte ASCII or Latin-1 encoding. This was not a problem in Python 2, where ASCII or mostly-ASCII text could use the `str` DType.

With the arrival of Python 3 support in NumPy, the string DTypes were largely left alone due to backward compatibility concerns, although the unicode DType became the default DType for `str` data and the old `string` DType was renamed the `bytes_` DType. This change left NumPy with the sub-optimal situation of shipping a data type originally intended for null-terminated bytestrings as the data type for *all* python `bytes` data, and a default string type with an in-memory representation that consumes four times as much memory than what is needed for data that can be represented well by a one-byte ASCII or Latin-1 encoding.

### Problems with fixed-width strings

Both existing string DTypes represent fixed-width sequences, allowing storage of the string data in the array buffer. This avoids adding out-of-band storage to NumPy, however, it makes for an awkward user interface for many use cases. In particular, the maximum string size must be inferred by NumPy or estimated by the user before loading the data into a NumPy array or selecting an output DType for string operations. In the worst case, this requires an expensive pass over the full dataset to calculate the maximum length of an array element. It also wastes memory when array elements have varying lengths. Pathological cases where an array stores many short strings and a few very long strings are particularly bad for wasting memory.

Downstream usage of string data in NumPy arrays has proven out the need for a variable-width string data type. In practice, many downstream libraries avoid using fixed-width strings due to usability issues and instead employ `object` arrays for storing strings. In particular, Pandas has explicitly deprecated support for NumPy fixed-width strings, coerces NumPy fixed-width string arrays to either `object` string arrays or `PyArrow`-backed string arrays, and in the future will switch to only supporting string data via `PyArrow`, which has native support for UTF-8 encoded variable-width string arrays\[1\].

## Previous discussions

The project last publicly discussed this topic in depth in 2017, when Julian Taylor proposed a fixed-width text data type parameterized by an encoding \[2\]. This started a wide-ranging discussion about pain points for working with string data in NumPy and possible ways forward.

The discussion highlighted two use-cases that the current support for strings does a poor job of handling\[3\]\[4\]\[5\]:

  - Loading or memory-mapping scientific datasets with unknown encoding,
  - Working with "a NumPy array of python strings" in a manner that allows transparent conversion between NumPy arrays and Python strings, including support for missing strings. The `object` DType partially satisfies this need, albeit with a cost of slow performance and no type checking.

As a result of this discussion, improving support for string data was added to the NumPy project roadmap\[6\], with an explicit call-out to add a DType better suited to memory-mapping bytes with any or no encoding, and a variable-width string DType that supports missing data to replace usages of object string arrays.

## Proposed work

This NEP proposes adding `StringDType`, a DType that stores variable-width heap-allocated strings in Numpy arrays, to replace downstream usages of the `object` DType for string data. This work will heavily leverage recent improvements in NumPy to improve support for user-defined DTypes, so we will also necessarily be working on the data type internals in NumPy. In particular, we propose to:

  - Add a new variable-length string DType to NumPy, targeting NumPy 2.0.
  - Work out issues related to adding a DType implemented using the experimental DType API to NumPy itself.
  - Support for a user-provided missing data sentinel.
  - Exposing string ufuncs in a new `np.strings` namespace for functions and types related to string support, enabling a migration path for a future deprecation of `np.char`.

The following is out of scope for this work:

  - Changing DType inference for string data.
  - Adding a DType for memory-mapping text in unknown encodings or a DType that attempts to fix issues with the `bytes_` DType.
  - Fully agreeing on the semantics of a missing data sentinels or adding a missing data sentinel to NumPy itself.
  - Implement SIMD optimizations for string operations.
  - An update to the `npy` and `npz` file formats to allow storage of arbitrary-length sidecar data.

While we're explicitly ruling out implementing these items as part of this work, adding a new string DType helps set up future work that does implement some of these items.

If implemented this NEP will make it easier to add a new fixed-width text DType in the future by moving string operations into a long-term supported namespace and improving the internal infrastructure in NumPy for handling strings. We are also proposing a memory layout that should be amenable to SIMD optimization in some cases, increasing the payoff for writing string operations as SIMD-optimized ufuncs in the future.

While we are not proposing adding a missing data sentinel to NumPy, we are proposing adding support for an optional, user-provided missing data sentinel, so this does move NumPy a little closer to officially supporting missing data. We are attempting to avoid resolving the disagreement described in \[NEP 26\<NEP26\>\](\#nep-26\<nep26\>) and this proposal does not require or preclude adding a missing data sentinel or bitflag-based missing data support to `ndarray` in the future.

## Usage and impact

The DType is intended as a drop-in replacement for object string arrays. This means that we intend to support as many downstream usages of object string arrays as possible, including all supported NumPy functionality. Pandas is the obvious first user, and substantial work has already occurred to add support in a fork of Pandas. `scikit-learn` also uses object string arrays and will be able to migrate to a DType with guarantees that the arrays contains only strings. Both h5py\[7\] and PyTables\[8\] will be able to add first-class support for variable-width UTF-8 encoded string datasets in HDF5. String data are heavily used in machine-learning workflows and downstream machine learning libraries will be able to leverage this new DType.

Users who wish to load string data into NumPy and leverage NumPy features like fancy advanced indexing will have a natural choice that offers substantial memory savings over fixed-width unicode strings and better validation guarantees and overall integration with NumPy than object string arrays. Moving to a first-class string DType also removes the need to acquire the GIL during string operations, unlocking future optimizations that are impossible with object string arrays.

### Performance

Here we briefly describe preliminary performance measurements of the prototype version of `StringDType` we have implemented outside of NumPy using the experimental DType API. All benchmarks in this section were performed on a Dell XPS 13 9380 running Ubuntu 22.04 and Python 3.11.3 compiled using pyenv. NumPy, Pandas, and the `StringDType` prototype were all compiled with meson release builds.

Currently, the `StringDType` prototype has comparable performance with object arrays and fixed-width string arrays. One exception is array creation from python strings, performance is somewhat slower than object arrays and comparable to fixed-width unicode arrays:

    In [1]: from stringdtype import StringDType
    
    In [2]: import numpy as np
    
    In [3]: data = [str(i) * 10 for i in range(100_000)]
    
    In [4]: %timeit arr_object = np.array(data, dtype=object)
    3.15 ms Â± 74.4 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
    
    In [5]: %timeit arr_stringdtype = np.array(data, dtype=StringDType())
    8.8 ms Â± 12.7 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
    
    In [6]: %timeit arr_strdtype = np.array(data, dtype=str)
    11.6 ms Â± 57.8 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

In this example, object DTypes are substantially faster because the objects in the `data` list can be directly interned in the array, while `StrDType` and `StringDType` need to copy the string data and `StringDType` needs to convert the data to UTF-8 and perform additional heap allocations outside the array buffer. In the future, if Python moves to a UTF-8 internal representation for strings, the string loading performance of `StringDType` should improve.

String operations have similar performance:

    In [7]: %timeit np.array([s.capitalize() for s in data], dtype=object)
    31.6 ms Â± 728 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
    
    In [8]: %timeit np.char.capitalize(arr_stringdtype)
    41.5 ms Â± 84.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
    
    In [9]: %timeit np.char.capitalize(arr_strdtype)
    47.6 ms Â± 386 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

The poor performance here is a reflection of the slow iterator-based implementation of operations in `np.char`. When we finish rewriting these operations as ufuncs, we will unlock substantial performance improvements. Using the example of the `add` ufunc, which we have implemented for the `StringDType` prototype:

    In [10]: %timeit arr_object + arr_object
    10.1 ms Â± 400 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
    
    In [11]: %timeit arr_stringdtype + arr_stringdtype
    3.64 ms Â± 258 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
    
    In [12]: %timeit np.char.add(arr_strdtype, arr_strdtype)
    17.7 ms Â± 245 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

As described below, we have already updated a fork of Pandas to use a prototype version of `StringDType`. This demonstrates the performance improvements available when data are already loaded into a NumPy array and are passed to a third-party library. Currently Pandas attempts to coerce all `str` data to `object` DType by default, and has to check and sanitize existing `object` arrays that are passed in. This requires a copy or pass over the data made unnecessary by first-class support for variable-width strings in both NumPy and Pandas:

    In [13]: import pandas as pd
    
    In [14]: %timeit pd.Series(arr_stringdtype)
    18.8 Âµs Â± 164 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)

If we force Pandas to use object string arrays, which was the default until very recently, we see the substantial performance penalty of a pass over the data outside of NumPy:

    In [15]: %timeit pd.Series(arr_object, dtype='string[python]')
    907 Âµs Â± 67 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each

Pandas switched to PyArrow-backed string arrays by default specifically to avoid this and other performance costs associated with object string arrays.

## Backward compatibility

We are not proposing a change to DType inference for python strings and do not expect to see any impacts on existing usages of NumPy.

## Detailed description

Here we provide a detailed description of the version of `StringDType` we would like to include in NumPy. This is mostly identical to the prototype, but has a few differences that are impossible to implement in a DType that lives outside of NumPy.

First, we describe the Python API for instantiating `StringDType` instances. Next, we will describe the missing data handling support and support for strict string type checking for array elements. We next discuss the cast and ufunc implementations we will define and discuss our plan for a new `np.strings` namespace to directly expose string ufuncs in the Python API. Finally, we provide an overview of the C API we would like to expose and the details of the memory layout and heap allocation strategy we have chosen for the initial implementation.

### Python API for `StringDType`

The new DType will be accessible via the `np.dtypes` namespace:

> \>\>\> from numpy.dtypes import StringDType \>\>\> dt = StringDType() \>\>\> dt numpy.dtypes.StringDType()

In addition, we propose reserving the character `"T"` (short for text) for usage with `np.dtype`, so the above would be identical to:

> \>\>\> np.dtype("T") numpy.dtypes.StringDType()

`StringDType` can be used out of the box to represent strings of arbitrary length in a NumPy array:

> \>\>\> data = \["this is a very long string", "short string"\] \>\>\> arr = np.array(data, dtype=StringDType()) \>\>\> arr array(\['this is a very long string', 'short string'\], dtype=StringDType())

Note that unlike fixed-width strings, `StringDType` is not parameterized by the maximum length of an array element, arbitrarily long or short strings can live in the same array without needing to reserve storage for padding bytes in the short strings.

The `StringDType` class will be a synonym for the default `StringDType` instance when the class is passed as a `dtype` argument in the NumPy Python API. We have already converted most of the API surface to work like this, but there are still a few spots that have not yet been converted and it's likely third-party code has not been converted, so we will not emphasize this in the docs. Emphasizing that `StringDType` is a class and `StringDType()` is an instance is a more forward-looking API that the rest of the NumPy DType API can move towards now that DType classes are importable from the `np.dtypes` namespace, so we will include an explicit instantiation of a `StringDType` object in the documentation even if it is not strictly necessary.

We propose associating the python `str` builtin as the DType's scalar type:

> \>\>\> StringDType.type \<class 'str'\>

While this does create an API wart in that the mapping from builtin DType classes to scalars in NumPy will no longer be one-to-one (the `unicode` DType's scalar type is `str`), this avoids needing to define, optimize, or maintain a `str` subclass for this purpose or other hacks to maintain this one-to-one mapping. To maintain backward compatibility, the DType detected for a list of python strings will remain a fixed-width unicode string.

As described below, `StringDType` supports two parameters that can adjust the runtime behavior of the DType. We will not attempt to support parameters for the dtype via a character code. If users need an instance of the DType that does not use the default parameters, they will need to instantiate an instance of the DType using the DType class.

We will also extend the `NPY_TYPES` enum in the C API with an `NPY_VSTRING` entry (there is already an `NPY_STRING` entry). This should not interfere with legacy user-defined DTypes since the integer type numbers for these data types begin at 256. In principle there is still room for hundreds more builtin DTypes in the integer range available in the `NPY_TYPES` enum.

In principle we do not need to reserve a character code and there is a desire to move away from character codes. However, a substantial amount of downstream code relies on checking DType character codes to discriminate between builtin NumPy DTypes, and we think it would harm adoption to require users to refactor their DType-handling code if they want to use `StringDType`.

We also hope that in the future we might be able to add a new fixed-width text version of `StringDType` that can re-use the `"T"` character code with length or encoding modifiers. This will allow a migration to a more flexible text dtype for use with structured arrays and other use-cases with a fixed-width string is a better fit than a variable-width string.

### Missing Data Support

Missing data can be represented using a sentinel:

> \>\>\> dt = StringDType(na\_object=np.nan) \>\>\> arr = np.array(\["hello", nan, "world"\], dtype=dt) \>\>\> arr array(\['hello', nan, 'world'\], dtype=StringDType(na\_object=nan)) \>\>\> arr\[1\] nan \>\>\> np.isnan(arr\[1\]) True \>\>\> np.isnan(arr) array(\[False, True, False\]) \>\>\> np.empty(3, dtype=dt) array(\['', '', ''\])

We only propose supporting user-provided sentinels. By default, empty arrays will be populated with empty strings:

> \>\>\> np.empty(3, dtype=StringDType()) array(\['', '', ''\], dtype=StringDType())

By only supporting user-provided missing data sentinels, we avoid resolving exactly how NumPy itself should support missing data and the correct semantics of the missing data object, leaving that up to users to decide. However, we *do* detect whether the user is providing a NaN-like missing data value, a string missing data value, or neither. We explain how we handle these cases below.

A cautious reader may be worried about the complexity of needing to handle three different categories of missing data sentinel. The complexity here is reflective of the flexibility of object arrays and the downstream usage patterns we've found. Some users want comparisons with the sentinel to error, so they use `None`. Others want comparisons to succeed and have some kind of meaningful ordering, so they use some arbitrary, hopefully unique string. Other users want to use something that acts like NaN in comparisons and arithmetic or is literally NaN so that NumPy operations that specifically look for exactly NaN work and there isn't a need to rewrite missing data handling outside of NumPy. We believe it is possible to support all this, but it requires a bit of hopefully manageable complexity.

#### NaN-like Sentinels

A NaN-like sentinel returns itself as the result of arithmetic operations. This includes the python `nan` float and the Pandas missing data sentinel `pd.NA`. We choose to make NaN-like sentinels inherit these behaviors in operations, so the result of addition is the sentinel:

> \>\>\> dt = StringDType(na\_object=np.nan) \>\>\> arr = np.array(\["hello", np.nan, "world"\], dtype=dt) \>\>\> arr + arr array(\['hellohello', nan, 'worldworld'\], dtype=StringDType(na\_object=nan))

We also chose to make a NaN-like sentinel sort to the end of the array, following the behavior of sorting an array containing `nan`.

> \>\>\> np.sort(arr) array(\['hello', 'world', nan\], dtype=StringDType(na\_object=nan))

#### String Sentinels

A string missing data value is an instance of `str` or subtype of `str`.

Operations will use the sentinel value directly for missing entries. This is the primary usage of this pattern we've found in downstream code, where a missing data sentinel like `"__nan__"` is passed to a low-level sorting or partitioning algorithm.

#### Other Sentinels

Any other python object will raise errors in operations or comparisons, just as `None` does as a missing data sentinel for object arrays currently:

> \>\>\> dt = StringDType(na\_object=None) \>\>\> np.sort(np.array(\["hello", None, "world"\], dtype=dt)) ValueError: Cannot compare null that is not a string or NaN-like value

Since comparisons need to raise an error, and the NumPy comparison API has no way to signal value-based errors during a sort without holding the GIL, sorting arrays that use arbitrary missing data sentinels will hold the GIL. We may also attempt to relax this restriction by refactoring NumPy's comparison and sorting implementation to allow value-based error propagation during a sort operation.

#### Implications for DType Inference

If, in the future, we decide to break backward compatibility to make `StringDType` the default DType for `str` data, the support for arbitrary objects as missing data sentinels may seem to pose a problem for implementing DType inference. However, given that initial support for this DType will require using the DType directly and will not be able to rely on NumPy to infer the DType, we do not think this will be a major problem for downstream users of the missing data feature. To use `StringDType`, they will need to update their code to explicitly specify a DType when an array is created, so if NumPy changes DType inference in the future, their code will not change behavior and there will never be a need for missing data sentinels to participate in DType inference.

### Coercing non-strings

By default, non-string data are coerced to strings:

> \>\>\> np.array(\[1, object(), 3.4\], dtype=StringDType()) array(\['1', '\<object object at 0x7faa2497dde0\>', '3.4'\], dtype=StringDType())

If this behavior is not desired, an instance of the DType can be created that disables string coercion:

> \>\>\> np.array(\[1, object(), 3.4\], dtype=StringDType(coerce=False)) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> ValueError: StringDType only allows string data when string coercion is disabled

This allows strict data validation in the same pass over the data NumPy uses to create the array without a need for downstream libraries to implement their own string validation in a separate, expensive, pass over the input array-like. We have chosen not to make this the default behavior to follow NumPy fixed-width strings, which coerce non-strings.

### Casts, ufunc support, and string manipulation functions

A full set of round-trip casts to the builtin NumPy DTypes will be available. In addition, we will add implementations for the comparison operators as well as an `add` loop that accepts two string arrays, `multiply` loops that accept string and integer arrays, an `isnan` loop, and implementations for the `str_len`, `isalpha`, `isdecimal`, `isdigit`, `isnumeric`, `isspace`, `find`, `rfind`, `count`, `strip`, `lstrip`, `rstrip`, and `replace` string ufuncs that will be newly available in NumPy 2.0.

The `isnan` ufunc will return `True` for entries that are NaN-like sentinels and `False` otherwise. Comparisons will sort data in order of unicode code point, as is currently implemented for the fixed-width unicode DType. In the future NumPy or a downstream library may add locale-aware sorting, case folding, and normalization for NumPy unicode strings arrays, but we are not proposing adding these features at this time.

Two `StringDType` instances are considered equal if they are created with the same `na_object` and `coerce` parameter. For ufuncs that accept more than one string argument we also introduce the concept of "compatible" `StringDType` instances. We allow distinct DType instances to be used in ufunc operations together if have the same `na_object` or if only one or the other DType has an `na_object` explicitly set. We do not consider string coercion for determining whether instances are compatible, although if the result of the operation is a string, the result will inherit the stricter string coercion setting of the original operands.

This notion of "compatible" instances will be enforced in the `resolve_descriptors` function of binary ufuncs. This choice makes it easier to work with non-default `StringDType` instances, because python strings are coerced to the default `StringDType` instance, so the following idiomatic expression is allowed:

    >>> arr = np.array(["hello", "world"], dtype=StringDType(na_object=None))
    >>> arr + "!"
    array(['hello!', 'world!'], dtype=StringDType(na_object=None))

If we only considered equality of `StringDType` instances, this would be an error, making for an awkward user experience. If the operands have distinct `na_object` settings, NumPy will raise an error because the choice for the result DType is ambiguous:

    >>> arr + np.array("!", dtype=StringDType(na_object=""))
    TypeError: Cannot find common instance for incompatible dtype instances

### `np.strings` namespace

String operations will be available in a `np.strings` namespace that will be populated with string ufuncs:

> \>\>\> np.strings.upper((np.array(\["hello", "world"\], dtype=StringDType()) array(\['HELLO', 'WORLD'\], dtype=StringDType()) \>\>\> isinstance(np.strings.upper, np.ufunc) True

We feel `np.strings` is a more intuitive name than `np.char`, and eventually will replace `np.char` once the minimum NumPy version supported by downstream libraries per [SPEC-0](https://scientific-python.org/specs/spec-0000/) is new enough that they can safely switch to `np.strings` without needing any logic conditional on the NumPy version.

### Serialization

Since string data are stored outside the array buffer, serialization to the `npy` format would requires a format revision to support storing variable-width sidecare data. Rather than doing this as part of this effort, we do not plan on supporting serialization to the `npy` or `npz` format without specifying `allow_pickle=True`.

This is a continuation of the current situation with object string arrays, which can only be saved to an `npy` file using the `allow_pickle=True` option.

In the future we may decide to add support for this, but care should be taken to not break parsers outside of NumPy that may not be maintained.

### C API for `StringDType`

The goal of the C API is to hide details of how string data are stored on the heap from the user and provide a thread-safe interface for reading and writing strings stored in `StringDType` arrays. To accomplish this, we have decided to split strings into two different *packed* and *unpacked* representations. A packed string lives directly in the array buffer and may contain either the string data for a sufficiently short string or metadata for a heap allocation where the characters of the string are stored. An unpacked string exposes the size of the string in bytes and a `char *` pointer to the string data.

To access the unpacked string data for a string stored in a numpy array, a user must call a function to load the packed string into an unpacked string or call another function to pack an unpacked string into an array. These operations require both a pointer to an array entry and a reference to an allocator struct. The allocator manages the bookkeeping needed to store the string data on the heap. Centralizing this bookkeeping in the allocator means we have the freedom to change the underlying allocation strategy. We also ensure thread safety by guarding access to the allocator with a mutex.

Below we describe this design in more detail, enumerating the types and functions we would like to add to the C API. In the \[next section \<memory\>\](\#next-section-\<memory\>) we describe the memory layout and heap allocation strategy we plan to implement using this API.

#### The `PyArray_StringDType` and `PyArray_StringDTypeObject` structs

We will publicly expose structs for the `StringDType` metaclass and a struct for the type of `StringDType` instances. The former `PyArray_StringDType` will be available in the C API in the same way as other `PyArray_DTypeMeta` instances for writing ufunc and cast loops. In addition, we will make the following struct public:

`` `C    struct PyArray_StringDTypeObject {        PyArray_Descr base;        // The object representing a null value        PyObject *na_object;        // Flag indicating whether or not to coerce arbitrary objects to strings        char coerce;        // Flag indicating the na object is NaN-like        char has_nan_na;        // Flag indicating the na object is a string        char has_string_na;        // If nonzero, indicates that this instance is owned by an array already        char array_owned;        // The string data to use when a default string is needed        npy_static_string default_string;        // The name of the missing data object, if any        npy_static_string na_name;        // the allocator should only be directly accessed after        // acquiring the allocator_lock and the lock should        // be released immediately after the allocator is        // no longer needed        npy_string_allocator *allocator;    }  Making this definition public eases future integration with other dtypes.  String and Allocator Types ``\` ++++++++++++++++++++++++++

Unpacked strings are represented in the C API with the `npy_static_string` type, which will be publicly exposed with the following definition:

`` `C    struct npy_static_string {        size_t size;        const char *buf;    };  Where ``size`is the size, in bytes, of the string and`buf`is a const`<span class="title-ref"> pointer to the beginning of a UTF-8 encoded bytestream containing string data. This is a \*read-only\* view onto the string, we will not expose a public interface for modifying these strings. We do not append a trailing null character to the byte stream, so users attempting to pass the </span><span class="title-ref">buf</span>\` field to an API expecting a C string must create a copy with a trailing null. In the future we may decide to always write a trailing null byte if the need to copy into a null-terminated buffer proves to be cost-prohibitive for downstream users of the C API.

In addition, we will expose two opaque structs, `npy_packed_static_string` and `npy_string_allocator`. Each entry in `StringDType` NumPy array will store the contents of an `npy_packed_static_string`; a packed representation of a string. The string data are stored either directly in the packed string or on the heap, in an allocation managed by a separate `npy_string_allocator` struct attached to the descriptor instance associated with the array. The precise layout of the packed string and the strategy used to allocate data on the heap will not be publicly exposed and users should not depend on these details.

#### New C API Functions

The C API functions we plan to expose fall into two categories: functions for acquiring and releasing the allocator lock and functions for loading and packing strings.

##### Acquiring and Releasing Allocators

The main interface for acquiring and releasing the allocator is the following pair of static inline functions:

`` `c    static inline npy_string_allocator *    NpyString_acquire_allocator(PyArray_StringDTypeObject *descr)     static inline void    NpyString_release_allocator(npy_string_allocator *allocator)  The first function acquires the allocator lock attached to the descriptor ``<span class="title-ref"> instance and returns a pointer to the allocator associated with the descriptor. The allocator can then be used by that thread to load existing packed strings or pack new strings into the array. Once the operation requiring the allocator is finished, the allocator lock must then be released. Use of the allocator after calling </span><span class="title-ref">NpyString\_release\_allocator</span>\` may lead to data races or memory corruption.

There are also cases when it is convenient to simultaneously work with several allocators. For example, the `add` ufunc takes two string arrays and produces a third string array. This means the ufunc loop needs three allocators to be able to load the strings for each operand and pack the result into the output array. This is also made more tricky by the fact that input and output operands need not be distinct objects and operands can share allocators by virtue of being the same array. In principle we could require users to acquire and release locks inside of a ufunc loop, but that would add a large performance overhead compared to acquiring all three allocators in the loop setup and releasing them simultaneously after the end of the loop.

To handle these situations, we will also expose variants of both functions that take an arbitrary number of descriptors and allocators (`NpyString_acquire_allocators`, and `NpyString_release_allocators`). Exposing these functions makes it straightforward to write code that works simultaneously with more than one allocator. The naive approach that simply calls `NpyString_acquire_allocator` and `NpyString_release_allocator` multiple times will cause undefined behavior by attempting to acquire the same lock more than once in the same thread when ufunc operands share descriptors. The multiple-descriptor variants check for identical descriptors before trying to acquire locks, avoiding the undefined behavior. To do the correct thing, the user will only need to choose the variant to acquire or release allocators that accepts the same number of descriptors as the number they need to work with.

##### Packing and Loading Strings

Accessing strings is mediated by the following function:

`` `c    int NpyString_load(        npy_string_allocator *allocator,        const npy_packed_static_string *packed_string,        npy_static_string *unpacked_string)  This function returns -1 on error, which can happen if there is a threading bug ``<span class="title-ref"> or corruption preventing access to a heap allocation. On success it can either return 1 or 0. If it returns 1, this indicates that the contents of the packed string are the null string, and special logic for handling null strings can happen in this case. If the function returns 0, this indicates the contents of the </span><span class="title-ref">packed\_string</span><span class="title-ref"> can be read from the </span><span class="title-ref">unpacked\_string</span>\`.

Packing strings can happen via one of these functions:

`` `c    int NpyString_pack(        npy_string_allocator *allocator,        npy_packed_static_string *packed_string,        const char *buf, size_t size)     int NpyString_pack_null(        npy_string_allocator *allocator,        npy_packed_static_string *packed_string)  The first function packs the contents of the first ``size`elements of`buf`  `<span class="title-ref"> into </span><span class="title-ref">packed\_string</span><span class="title-ref">. The second function packs the null string into </span><span class="title-ref">packed\_string</span><span class="title-ref">. Both functions invalidate any previous heap allocation associated with the packed string and old unpacked representations that are still in scope are invalid after packing a string. Both functions return 0 on success and -1 on failure, for example if </span><span class="title-ref">malloc</span>\` fails.

#### Example C API Usage

##### Loading a String

Say we are writing a ufunc implementation for `StringDType`. If we are given `const char *buf` pointer to the beginning of a `StringDType` array entry, and a `PyArray_Descr *` pointer to the array descriptor, one can access the underlying string data like so:

`` `C    npy_string_allocator *allocator = NpyString_acquire_allocator(            (PyArray_StringDTypeObject *)descr);     npy_static_string sdata = {0, NULL};    npy_packed_static_string *packed_string = (npy_packed_static_string *)buf;    int is_null = 0;     is_null = NpyString_load(allocator, packed_string, &sdata);     if (is_null == -1) {        // failed to load string, set error        return -1;    }    else if (is_null) {        // handle missing string        // sdata->buf is NULL        // sdata->size is 0    }    else {        // sdata->buf is a pointer to the beginning of a string        // sdata->size is the size of the string    }    NpyString_release_allocator(allocator);  Packing a String ``\` ^^^^^^^^^^^^^^^^

This example shows how to pack a new string into an array:

`` `C    char *str = "Hello world";    size_t size = 11;    npy_packed_static_string *packed_string = (npy_packed_static_string *)buf;     npy_string_allocator *allocator = NpyString_acquire_allocator(            (PyArray_StringDTypeObject *)descr);     // copy contents of str into packed_string    if (NpyString_pack(allocator, packed_string, str, size) == -1) {        // string packing failed, set error        return -1;    }     // packed_string contains a copy of "Hello world"     NpyString_release_allocator(allocator);  .. _memory:  Cython Support and the Buffer Protocol ``\` ++++++++++++++++++++++++++++++++++++++

It's impossible for `StringDType` to support the Python buffer protocol, so Cython will not support idiomatic typed memoryview syntax for `StringDType` arrays unless special support is added in Cython in the future. We have some preliminary ideas for ways to either update the buffer protocol\[9\] or make use of the Arrow C data interface\[10\] to expose NumPy arrays for DTypes that don't make sense in the buffer protocol, but those efforts will likely not come to fruition in time for NumPy 2.0. This means adapting legacy Cython code that uses arrays of fixed-width strings to work with `StringDType` will be non-trivial. Adapting code that worked with object string arrays should be straightforward since object arrays aren't supported by the buffer protocol either and will likely have no types or have `object` type in Cython.

We will add cython `nogil` wrappers for the public C API functions added as part of this work to ease integration with downstream cython code.

### Memory Layout and Managing Heap Allocations

Below we provide a detailed description of the memory layout we have chosen, but before diving in we want to observe that the C API described above does not publicly expose any of these details. All of the following is subject to future revision, improvement, and change because the precise memory layout of the string data are not publicly exposed.

#### Memory Layout and Small String Optimization

Each array element is represented as a union, with the following definition on little-endian architectures:

`` `c    typedef struct _npy_static_vstring_t {       size_t offset;       size_t size_and_flags;    } _npy_static_string_t;     typedef struct _short_string_buffer {       char buf[sizeof(_npy_static_string_t) - 1];       unsigned char size_and_flags;    } _short_string_buffer;     typedef union _npy_static_string_u {     _npy_static_string_t vstring;     _short_string_buffer direct_buffer;    } _npy_static_string_u;  The ``\_npy\_static\_vstring\_t`representation is most useful for representing`<span class="title-ref"> strings living on the heap directly or in an arena allocation, with the </span><span class="title-ref">offset</span><span class="title-ref"> field either containing a </span><span class="title-ref">size\_t</span><span class="title-ref"> representation of the address directly, or an integer offset into an arena allocation. The </span><span class="title-ref">\_short\_string\_buffer</span><span class="title-ref"> representation is most useful for the small string optimization, with the string data stored in the </span><span class="title-ref">direct\_buffer</span><span class="title-ref"> field and the size in the </span><span class="title-ref">size\_and\_flags</span><span class="title-ref"> field. In both cases the </span><span class="title-ref">size\_and\_flags</span><span class="title-ref"> field stores both the </span><span class="title-ref">size</span><span class="title-ref"> of the string as well as bitflags. Small strings store the size in the final four bits of the buffer, reserving the first four bits of </span><span class="title-ref">size\_and\_flags</span>\` for flags. Heap strings or strings in arena allocations use the most significant byte for flags, reserving the leading bytes for the string size. It's worth pointing out that this choice limits the maximum string sized allowed to be stored in an array, particularly on 32 bit systems where the limit is 16 megabytes per string - small enough to worry about impacting real-world workflows.

On big-endian systems, the layout is reversed, with the `size_and_flags` field appearing first in the structs. This allows the implementation to always use the most significant bits of the `size_and_flags` field for flags. The endian-dependent layouts of these structs is an implementation detail and is not publicly exposed in the API.

Whether or not a string is stored directly on the arena buffer or in the heap is signaled by setting the `NPY_OUTSIDE_ARENA` and `NPY_STRING_LONG` flags on the string data. Because the maximum size of a heap-allocated string is limited to the size of the largest 7-byte unsized integer, these flags can never be set for a valid heap string.

See \[memorylayoutexamples\](\#memorylayoutexamples) for some visual examples of strings in each of these memory layouts.

#### Arena Allocator

Strings longer than 15 bytes on 64 bit systems and 7 bytes on 32 bit systems are stored on the heap outside of the array buffer. The bookkeeping for the allocations is managed by an arena allocator attached to the `StringDType` instance associated with an array. The allocator will be exposed publicly as an opaque `npy_string_allocator` struct. Internally, it has the following layout:

`` `c     struct npy_string_allocator {         npy_string_malloc_func malloc;         npy_string_free_func free;         npy_string_realloc_func realloc;         npy_string_arena arena;         PyThread_type_lock *allocator_lock;     };  This allows us to group memory-allocation functions together and choose ``\` different allocation functions at runtime if we desire. Use of the allocator is guarded by a mutex, see below for more discussion about thread safety.

The memory allocations are handled by the `npy_string_arena` struct member, which has the following layout:

`` `c     struct npy_string_arena {         size_t cursor;         size_t size;         char *buffer;     };  Where ``buffer`is a pointer to the beginning of a heap-allocated arena,`<span class="title-ref"> </span><span class="title-ref">size</span><span class="title-ref"> is the size of that allocation, and </span><span class="title-ref">cursor</span>\` is the location in the arena where the last arena allocation ended. The arena is filled using an exponentially expanding buffer, with an expansion factor of 1.25.

Each string entry in the arena is prepended by a size, stored either in a `char` or a `size_t`, depending on the length of the string. Strings with lengths between 16 or 8 (depending on architecture) and 255 are stored with a `char` size. We refer to these as "medium" strings internally. This choice reduces the overhead for storing smaller strings on the heap by 7 bytes per medium-length string. Strings in the arena with lengths longer than 255 bytes have the `NPY_STRING_LONG` flag set.

If the contents of a packed string are freed and then assigned to a new string with the same size or smaller than the string that was originally stored in the packed string, the existing short string or arena allocation is re-used. There is one exception however, when a string in the arena is overwritten with a short string, the arena metadata is lost and the arena allocation cannot be re-used.

If the string is enlarged, the existing space in the arena buffer cannot be used, so instead we resort to allocating space directly on the heap via `malloc` and the `NPY_STRING_OUTSIDE_ARENA` and `NPY_STRING_LONG` flags are set. Note that `NPY_STRING_LONG` can be set even for strings with lengths less than 255 bytes in this case. Since the heap address overwrites the arena offset, and future string replacements will be stored on the heap or directly in the array buffer as a short string.

No matter where it is stored, once a string is initialized it is marked with the `NPY_STRING_INITIALIZED` flag. This lets us clearly distinguish between an uninitialized empty string and a string that has been mutated into the empty string.

The size of the allocation is stored in the arena to allow reuse of the arena allocation if a string is mutated. In principle we could disallow re-use of the arena buffer and not store the sizes in the arena. This may or may not save memory or be more performant depending on the exact usage pattern. For now we are erring on the side of avoiding unnecessary heap allocations when a string is mutated but in principle we could simplify the implementation by choosing to always store mutated arena strings as heap strings and ignore the arena allocation. See below for more detail on how we deal with the mutability of NumPy arrays in a multithreaded context.

Using a per-array arena allocator ensures that the string buffers for nearby array elements are usually nearby on the heap. We do not guarantee that neighboring array elements are contiguous on the heap to support the small string optimization, missing data, and allow mutation of array entries. See below for more discussion on how these topics affect the memory layout.

#### Mutation and Thread Safety

Mutation introduces the possibility of data races and use-after-free errors when an array is accessed and mutated by multiple threads. Additionally, if we allocate mutated strings in the arena buffer and mandate contiguous storage where the old string is replaced by the new one, mutating a single string may trigger reallocating the arena buffer for the entire array. This is a pathological performance degradation compared with object string arrays or fixed-width strings.

One solution would be to disable mutation, but inevitably there will be downstream uses of object string arrays that mutate array elements that we would like to support.

Instead, we have opted to pair the `npy_string_allocator` instance attached to `PyArray_StringDType` instances with a `PyThread_type_lock` mutex. Any function in the static string C API that allows manipulating heap-allocated data accepts an `allocator` argument. To use the C API correctly, a thread must acquire the allocator mutex before any usage of the `allocator`.

The `PyThread_type_lock` mutex is relatively heavyweight and does not provide more sophisticated locking primitives that allow multiple simultaneous readers. As part of the GIL-removal project, CPython is adding new synchronization primitives to the C API for projects like NumPy to make use of. When this happens, we can update the locking strategy to allow multiple simultaneous reading threads, along with other fixes for threading bugs in NumPy that will be needed once the GIL is removed.

#### Freeing Strings

Existing strings must be freed before discarding or re-using a packed string. The API is constructed to require this for all strings, even for short strings with no heap allocations. In all cases, all data in the packed string are zeroed out, except for the flags, which are preserved.

#### Memory Layout Examples

We have created illustrative diagrams for the three possible string memory layouts. All diagrams assume a 64 bit little endian architecture.

![image](nep-0055-short-string-memory-layout.svg)

Short strings store string data directly in the array buffer. On little-endian architectures, the string data appear first, followed by a single byte that allows space for four flags and stores the size of the string as an unsigned integer in the final 4 bits. In this example, the string contents are "Hello world", with a size of 11. The flags indicate this string is stored outside the arena and is initialized.

![image](nep-0055-arena-string-memory-layout.svg)

Arena strings store string data in a heap-allocated arena buffer that is managed by the `StringDType` instance attached to the array. In this example, the string contents are "Numpy is a very cool library", stored at offset `0x94C` in the arena allocation. Note that the `size` is stored twice, once in the `size_and_flags` field, and once in the arena allocation. This facilitates re-use of the arena allocation if a string is mutated. Also note that because the length of the string is small enough to fit in an `unsigned char`, this is a "medium"-length string and the size requires only one byte in the arena allocation. An arena string larger than 255 bytes would need 8 bytes in the arena to store the size in a `size_t`. The only flag set indicates this string is initialized.

![image](nep-0055-heap-string-memory-layout.svg)

Heap strings store string data in a buffer returned by `PyMem_RawMalloc` and instead of storing an offset into an arena buffer, directly store the address of the heap address returned by `malloc`. In this example, the string contents are "Numpy is a very cool library" and are stored at heap address `0x4d3d3d3`. The string has three flags set, indicating it is a "long" string (e.g. not a short string) stored outside the arena, and is initialized. Note that if this string were stored inside the arena, it would not have the long string flag set because it requires less than 256 bytes to store.

#### Empty Strings and Missing Data

The layout we have chosen has the benefit that newly created array buffer returned by `calloc` will be an array filled with empty strings by construction, since a string with no flags set is an uninitialized zero-length arena string. This is not the only valid representation of an empty string, since other flags may be set to indicate that the empty string is associated with a pre-existing short string or arena string.

Missing strings will have an identical representation, except they will always have a flag, `NPY_STRING_MISSING` set in the flags field. Users will need to check if a string is null before accessing an unpacked string buffer and we have set up the C API in such a way as to force null-checking whenever a string is unpacked. Both missing and empty strings can be detected based on data in the packed string representation and do not require corresponding room in the arena allocation or extra heap allocations.

## Related work

The main comparable prior art in the Python ecosystem is PyArrow arrays, which support variable length strings via Apache Arrow's variable sized binary layout \[11\]. In this approach, the array buffer contains integer offsets that index into a sidecar storage buffer. This allows a string array to be created using only two heap allocations, leaves adjacent strings in the array contiguous in memory, provides good cache locality, and enables straightforward SIMD optimization. Mutation of string array elements isn't allowed and PyArrow only supports 1D arrays, so the design space is somewhat different from NumPy.

Julia stores strings as UTF-8 encoded byte buffers. There is no special optimization for string arrays in Julia, and string arrays are represented as arrays of pointers in memory in the same way as any other array of sequences or containers in Julia.

The tensorflow library supports variable-width UTF-8 encoded strings, implemented with `RaggedTensor`. This makes use of first-class support for ragged arrays in tensorflow.

## Implementation

We have an open pull request\[12\] that is ready to merge into NumPy adding StringDType.

We have created a development branch of Pandas that supports creating Pandas data structures using `StringDType`\[13\]. This illustrates the refactoring necessary to support `StringDType` in downstream libraries that make substantial use of object string arrays.

If accepted, the bulk of the remaining work of this NEP is in updating documentation and polishing the NumPy 2.0 release. We have already done the following:

  - Create an `np.strings` namespace and expose the string ufuncs directly in that namespace.
  - Move the `StringDType` implementation from an external extension module into NumPy, refactoring NumPy where appropriate. This new DType will be added in one large pull request including documentation updates. Where possible, we will extract fixes and refactorings unrelated to `StringDType` into smaller pull requests before issuing the main pull request.

We will continue doing the following:

  - Deal with remaining issues in NumPy related to new DTypes. In particular, we are already aware that remaining usages of `copyswap` in `NumPy` should be migrated to use a cast or an as-yet-to-be-added single-element copy DType API slot. We also need to ensure that DType classes can be used interchangeably with DType instances in the Python API everywhere it makes sense to do so and add useful errors in all other places DType instances can be passed in but DType classes don't make sense to use.

## Alternatives

The main alternative is to maintain the status quo and offer object arrays as the solution for arrays of variable-length strings. While this will work, it means immediate memory usage and performance improvements, as well as future performance improvements, will not be implemented anytime soon and NumPy will lose relevance to other ecosystems with better support for arrays of textual data.

We do not see the proposed DType as mutually exclusive to an improved fixed-width binary string DType that can represent arbitrary binary data or text in any encoding and adding such a DType in the future will be easier once overall support for string data in NumPy has improved after adding `StringDType`.

## Discussion

  - <https://github.com/numpy/numpy/pull/24483>
  - <https://github.com/numpy/numpy/pull/25347>
  - <https://mail.python.org/archives/list/numpy-discussion@python.org/thread/IHSVBZ7DWGMTOD6IEMURN23XM2BYM3RG/>

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  <https://github.com/pandas-dev/pandas/pull/52711>

2.  <https://mail.python.org/pipermail/numpy-discussion/2017-April/thread.html#76668>

3.  <https://mail.python.org/archives/list/numpy-discussion@python.org/message/WXWS4STFDSWFY6D7GP5UK2QB2NFPO3WE/>

4.  <https://mail.python.org/archives/list/numpy-discussion@python.org/message/DDYXJXRAAHVUGJGW47KNHZSESVBD5LKU/>

5.  <https://mail.python.org/archives/list/numpy-discussion@python.org/message/6TNJWGNHZF5DMJ7WUCIWOGYVZD27GQ7L/>

6.  <https://numpy.org/neps/roadmap.html#extensibility>

7.  <https://github.com/h5py/h5py/issues/624#issuecomment-676633529>

8.  <https://github.com/PyTables/PyTables/issues/499>

9.  <https://discuss.python.org/t/buffer-protocol-and-arbitrary-data-types/26256>

10. <https://arrow.apache.org/docs/format/CDataInterface.html>

11. <https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-layout>

12. <https://github.com/numpy/numpy/pull/25347>

13. <https://github.com/ngoldbaum/pandas/tree/stringdtype>

---

nep-0056-array-api-main-namespace.md

---

# NEP 56 â€” Array API standard support in NumPy's main namespace

  - Author  
    Ralf Gommers \<<ralf.gommers@gmail.com>\>

  - Author  
    Mateusz SokÃ³Å‚ \<<msokol@quansight.com>\>

  - Author  
    Nathan Goldbaum \<<ngoldbaum@quansight.com>\>

  - Status  
    Final

  - Replaces  
    \[NEP30\](\#nep30), \[NEP31\](\#nep31), \[NEP37\](\#nep37), \[NEP47\](\#nep47)

  - Type  
    Standards Track

  - Created  
    2023-12-19

  - Resolution  
    <https://mail.python.org/archives/list/numpy-discussion@python.org/message/Z6AA5CL47NHBNEPTFWYOTSUVSRDGHYPN/>

## Abstract

This NEP proposes adding nearly full support for the 2022.12 version of the array API standard in NumPy's main namespace for the 2.0 release.

Adoption in the main namespace has a number of advantages; most importantly for libraries that depend on NumPy and want to start supporting other array libraries. SciPy and scikit-learn are two prominent libraries already moving along this path. The need to support the array API standard in the main namespace draws from lessons learned by those libraries and the experimental `numpy.array_api` implementation with a different array object. There will also be benefits for other array libraries, JIT compilers like Numba, and for end users who may have an easier time switching between different array libraries.

## Motivation and scope

\> **Note** \> The main changes proposed in this NEP were presented in the NumPy 2.0 Developer Meeting in April 2023 (see [here](https://github.com/numpy/archive/blob/main/2.0_developer_meeting/NumPy_2.0_devmeeting_array_API_adoption.pdf) for presentations from that meeting) and given a thumbs up there. The majority of the implementation work for NumPy 2.0 has already been merged. For the rest, PRs are ready - those are mainly the items that are specific to array API support and we'd probably not consider for inclusion in NumPy without that context. This NEP will focus on those APIs and PRs in a bit more detail.

\[NEP47\](\#nep47) contains the motivation for adding array API support to NumPy. This NEP expands on and supersedes NEP 47. The main reason NEP 47 aimed for a separate `numpy.array_api` submodule rather than the main namespace is that casting rules differed too much. With value-based casting being removed (\[NEP50\](\#nep50)), that will be resolved in NumPy 2.0. Having NumPy be a superset of the array API standard will be a significant improvement for code portability to other libraries (CuPy, JAX, PyTorch, etc.) and thereby address one of the top user requests from the 2020 NumPy user survey\[1\] (GPU support). See [the numpy.array\_api API docs (1.26.x)](https://numpy.org/doc/1.26/reference/array_api.html#table-of-differences-between-numpy-array-api-and-numpy) for an overview of differences between it and the main namespace (note that the "strictness" ones are not applicable).

Experiences with `numpy.array_api`, which is still marked as experimental, have shown that the separate strict implementation and separate array object are mostly good for testing purposes, but not for regular usage in downstream libraries. Having support in the main namespace resolves this issue. Hence this NEP supersedes NEP 47. The `numpy.array_api` module will be moved to a standalone package, to facilitate easier updates not tied to a NumPy release cycle.

Some of the key design rules from the array API standard (e.g., output dtypes predictable from input dtypes, no polymorphic APIs with varying number of returns controlled by keywords) will also be applied to NumPy functions that are not part of the array API standard, because those design rules are now understood to be good practice in general. Those two design rules in particular make it easier for Numba and other JIT compilers to support NumPy or NumPy-compatible APIs. We'll note that making existing arguments positional-only and keyword-only is a good idea for functions added to NumPy in the future, but will not be done for existing functions since each such change is a backwards compatibility break and it's not necessary for writing code that is portable across libraries supporting the standard. An additional reason to apply those design rules to all functions in the main namespace now is that it then becomes much easier to deal with potential standardization of new functions already present in NumPy - those could otherwise be blocked or forced to use alternative function names due to the need for backwards compatibility.

It is important that new functions added to the main namespace integrate well with the rest of NumPy. So they should for example follow broadcasting and other rules as expected, and work with all NumPy's dtypes rather than only the ones in the standard. The same goes for backwards-incompatible changes (e.g., linear algebra functions need to all support batching in the same way, and consider the last two axes as matrices). As a result, NumPy should become more rather than less consistent.

Here are what we see as the main expected benefits and costs of the complete set of proposed changes:

Benefits:

  - It will enable array-consuming libraries (the likes of SciPy and scikit-learn, as well as smaller libraries higher up the stack) to implement support for multiple array libraries,
  - It will remove the "having to make a choice between the NumPy API and the array API standard" issue for other array libraries when choosing what API to implement,
  - Easier for CuPy, JAX, PyTorch, Dask, Numba, and other such libraries and compilers to match or support NumPy, through providing a more well-defined and minimal API surface to target, as well as through resolving some differences that were caused by Numpy semantics that were hard to support in JIT compilers,
  - A few new features that have benefits independent of the standard: adding `matrix_transpose` and `ndarray.mT`, adding `vecdot`, introducing `matrix_norm`/`vector_norm` (they can be made gufuncs, vecdot already has a PR making it one),
  - Closer correspondence between the APIs of NumPy and other array libraries will lower the learning curve for end users when they switch from one array library to another one,
  - The array API standard tends to have more consistent behavior than NumPy itself has (in cases where there are differences between the two, see for example the [linear algebra design principles](https://data-apis.org/array-api/2022.12/extensions/linear_algebra_functions.html#design-principles) and [data-dependent output shapes page](https://data-apis.org/array-api/2022.12/design_topics/data_dependent_output_shapes.html) in the standard),

Costs:

  - A number of backwards compatibility breaks (mostly minor, see the Backwards compatibility section further down),
  - Expanding the size of the main namespace with about \~20 aliases (e.g., `acos` & co. with C99 names aliasing `arccos` & co.).

Overall we believe that the benefits significantly outweigh the costs - and are permanent, while the costs are largely temporary. In particular, the benefits to array libraries and compilers that want to achieve compatibility with NumPy are significant. And as a result, the long-term benefits for the PyData (or scientific Python) ecosystem as a whole - because of downstream libraries being able to support multiple array libraries much more easily - are significant too. The number of breaking changes needed is fairly limited, and the impact of those changes seems modest. Not painless, but we believe the impact is smaller than the impact of other breaking changes in NumPy 2.0, and a price worth paying.

In scope for this NEP are:

  - Changes to NumPy's Python API needed to support the 2022.12 version of the array API standard, in the main namespace as well as `numpy.linalg` and `numpy.fft`,
  - Changes in the behavior of existing NumPy functions not (or not yet) present in the array API standard, to align with key design principles of the standard.

Out of scope for this NEP are:

  - Other changes to NumPy's Python API unrelated to the array API standard,
  - Changes to NumPy's C API.

This NEP will supersede the following NEPs:

  - \[NEP30\](\#nep30) (never implemented)
  - \[NEP31\](\#nep31) (never implemented)
  - \[NEP37\](\#nep37) (never implemented; the `__array_module__` idea is basically the same as `__array_namespace__`)
  - \[NEP47\](\#nep47) (implemented with an experimental label in `numpy.array_api`, will be removed)

## Usage and impact

We have several different types of users in mind: end users writing numerical code, downstream packages that depend on NumPy who want to start supporting multiple array libraries, and other array libraries and tools which aim to implement NumPy-like or NumPy-compatible APIs.

The most prominent users who will benefit from array API support are probably downstream libraries that want to start supporting CuPy, PyTorch, JAX, Dask, or other such libraries. SciPy and scikit-learn are already fairly far along the way of doing just that, and successfully support CuPy arrays and PyTorch tensors in a small part of their own APIs (that support is still marked as experimental).

The main principle they use is that they replace the regular `import numpy as np` with a utility function to retrieve the array library namespace from the input array. They call it `xp`, which is effectively an alias to `np` if the input is a NumPy array, `cupy` for a CuPy array, `torch` for a PyTorch tensor. This `xp` then allows writing code that works for all these libraries - because the array API standard is the common denominator. As a concrete example, this code is taken from `scipy.cluster`:

``` python
def vq_py(obs, code_book, check_finite=True):
    """Python version of vq algorithm"""
    xp = array_namespace(obs, code_book)
    obs = as_xparray(obs, xp=xp, check_finite=check_finite)
    code_book = as_xparray(code_book, xp=xp, check_finite=check_finite)

    if obs.ndim != code_book.ndim:
        raise ValueError("Observation and code_book should have the same rank")

    if obs.ndim == 1:
        obs = obs[:, xp.newaxis]
        code_book = code_book[:, xp.newaxis]

    # Once `cdist` has array API support, this `xp.asarray` call can be removed
    dist = xp.asarray(cdist(obs, code_book))
    code = xp.argmin(dist, axis=1)
    min_dist = xp.min(dist, axis=1)
    return code, min_dist
```

It mostly looks like normal NumPy code, but will run with for example PyTorch tensors as input and then return PyTorch tensors. There is a lot more to this story of course then this basic example. These blog posts on scikit-learn\[2\] and SciPy's\[3\] experiences and impact (large performance gains in some cases - `LinearDiscriminantAnalysis.fit` showed \~28x gain with PyTorch on GPU vs. NumPy) paint a more complete picture.

For end users who are using NumPy directly, little changes aside from there being fewer differences between NumPy and other libraries they may want to use as well. This shortens their learning curve and makes it easier to switch between NumPy and PyTorch/JAX/CuPy. In addition, they should benefit from array-consuming libraries starting to support multiple array libraries, making their experience of using a stack of Python packages for scientific computing or data science more seamless.

Finally, for authors of other array libraries as well as tools like Numba, API improvements which align NumPy with the array API standard will also save them time. The design rules (\[4\]), and in some cases new APIs like the `unique_*` ones, are easier to implement on GPU and for JIT compilers as a result of more predictable behavior.

## Backward compatibility

The changes that have a backwards compatibility impact fall into these categories:

1.  Raising errors for consistency/strictness in some places where NumPy now allows more flexible behavior,
2.  Dtypes of returned arrays for some element-wise functions and reductions,
3.  Numerical behavior for a few tolerance keywords,
4.  Functions moved to `numpy.linalg` and supporting stacking/batching,
5.  The semantics of the `copy` keyword in `asarray` and `array`,
6.  Changes to `numpy.fft` functionality.

**Raising errors for consistency/strictness includes**:

1.  Making `.T` error for \>2 dimensions,
2.  Making `cross` error on size-2 vectors (only size-3 vectors are supported),
3.  Making `solve` error on ambiguous input (only accept `x2` as vector if `x2.ndim == 1`),
4.  `outer` raises rather than flattens on \>1-D inputs,

*We expect the impact of this category of changes to be small.*

**Dtypes of returned arrays for some element-wise functions and reductions** includes functions where dtypes need to be preserved: `ceil`, `floor`, and `trunc` will start returning arrays with the same integer dtypes if the input has an integer dtype.

*We expect the impact of this category of changes to be small.*

**Changes in numerical behavior** include:

  - The `rtol` default value for `pinv` changes from `1e-15` to a dtype-dependent default value of `None`, interpreted as `max(M, N) * finfo(result_dtype).eps`,
  - The `tol` keyword to `matrix_rank` changes to `rtol` with a different interpretation. In addition, `matrix_rank` will no longer support 1-D array input,

Raising a `FutureWarning` for these tolerance changes doesn't seem reasonable; they'd be spurious warnings for the vast majority of users, and it would force users to hardcode a tolerance value to avoid the warning. Changes in numerical results are in principle undesirable, so while we expect the impact to be small it would be good to do this in a major release.

*We expect the impact of this category of changes to be medium. It is the only category of changes that does not result in clear exceptions or warnings, and hence if it does matter (e.g., downstream tests start failing or users notice a change in behavior) it may require more work from users to track down the problem. This should happen infrequently - one month after the PR implementing this change was merged (see* [gh-25437](https://github.com/numpy/numpy/pull/25437)), *the impact reported so far is a single test failure in AstroPy.*

**Functions moved to numpy.linalg and supporting stacking/batching** are the `diagonal` and `trace` functions. They part of the `linalg` submodule in the standard, rather than the main namespace. Hence they will be introduced in `numpy.linalg`. They will operate on the last two rather than first two axes. This is done for consistency, since this is now other NumPy functions work, and to support "stacking" (or "batching" in more commonly used terminology in other libraries). Hence the `linalg` and main namespace functions of the same names will differ. This is technically not breaking, but potentially confusing because of the different behavior for functions with the same name. We may deprecate `np.trace` and `np.diagonal` to resolve it, but preferably not immediately to avoid users having to write `if-2.0-else` conditional code.

*We expect the impact of this category of changes to be small.*

**The semantics of the copy keyword in asarray and array** for `copy=False` will change from "copy if needed" to "never copy". there are now three types of behavior rather than two - `copy=None` means "copy if needed".

*We expect the impact of this category of changes to be medium. In case users get an exception because they use* `copy=False` *explicitly in their copy but a copy was previously made anyway, they have to inspect their code and determine whether the intent of the code was the old or the new semantics (both seem roughly equally likely), and adapt the code as appropriate. We expect most cases to be* `np.array(..., copy=False)`, *because until a few years ago that had lower overhead than* `np.asarray(...)`. *This was solved though, and* `np.asarray(...)` *is idiomatic NumPy usage.*

**Changes to numpy.fft**: all functions in the `numpy.fft` submodule need to preserve precision for 32-bit input dtypes rather than upcast to `float64`/`complex128`. This is a desirable change, consistent with the design of NumPy as a whole - but it's possible that the lower precision or the dtype of the returned arrays from calls to functions in this module may affect users. This change was made by via a new gufunc-based implementation and vendoring of the C++ version of PocketFFT in ([gh-25711](https://github.com/numpy/numpy/pull/25711)).

A smaller backwards-incompatible change to `numpy.fft` is to make the behavior of the `s` and `axes` arguments in n-D transforms easier to understand by disallowing `None` values in `s` and requiring that if `s` is used, `axes` must be specified as well (see [gh-25495](https://github.com/numpy/numpy/pull/25495)).

*We expect the impact of this category of changes to be small.*

### Adapting to the changes & tooling support

Some parts of the array API have already been implemented as part of the general Python API cleanup for NumPy 2.0 (see NEP 52), such as:

  - establishing one and way for naming `inf` and `nan` that is array API compatible.
  - removing cryptic dtype names and establishing (array API compatible) canonical names for each dtype.

All instructions for migrating to a NEP 52 compatible codebase are available in the [NumPy 2.0 Migration Guide](https://numpy.org/devdocs/numpy_2_0_migration_guide.html) .

Additionally, a new `ruff` rule was implemented for an automatic migration of Python API changes. It's worth pointing out that the new rule NP201 is only to adhere to the NEP 52 changes, and does not cover using new functions that are part of the array API standard nor APIs with some types of backwards incompatible changes discussed above.

For an automated migration to an array API compatible codebase, a new rule is being implemented (see issue [ruff\#8615](https://github.com/astral-sh/ruff/issues/8615) and PR [ruff\#8910](https://github.com/astral-sh/ruff/pull/8910)).

With both rules in place a downstream user should be able to update their project, to the extent that is possible with automation, to a library agnostic codebase that can benefit from different array libraries and devices.

Backwards incompatible changes that cannot be handled automatically (e.g., a change in `rtol` defaults for a linear algebra function) will be handled the in same way as any other backwards incompatible change in NumPy 2.0 -through documentation, release notes, API migrations and deprecations over several releases.

## Detailed description

In this section we'll focus on specific API additions and functionality that we would not consider introducing into NumPy if the standard did not exist and we didn't have to think/worry about its main goal: writing code that is portable across multiple array libraries and their supported features like GPUs and other hardware accelerators or JIT compilers.

### `device` support

Device support is perhaps the most obvious example. NumPy is and will remain a CPU-only library, so why bother introducing a `ndarray.device` attribute or `device=` keywords in several functions? This one feature is purely meant to make it easier to write code that is portable across libraries. The `.device` attribute will return an object representing CPU, and that object will be accepted as an input to `device=` keywords. For example:

    # Should work when `xp` is `np` and `x1` a numpy array
    x2 = xp.asarray([0, 1, 2, 3], dtype=xp.float64, device=x1.device)

This will work as expected for NumPy, creating a 1-D numpy array from the input list. It will also work for CuPy & co, where it may create a new array on a GPU or other supported device.

### `isdtype`

The array API standard introduced a new function `isdtype` for introspection of dtypes, because there was no suitable alternative in NumPy. The closest one is `np.issubdtype`, however that assumes a complex class hierarchy which other array libraries don't have, isn't the most ergonomic API, and required a larger API surface (`np.floating` and friends). `isdtype` will be the new and canonical way to introspect dtypes. All it requires from a dtype is that `__eq__` is implemented and has the expected behavior when compared with other dtypes from the same library.

Note that as part of the effort on NEP 52, some dtype aliases were removed and canonical Python and C names documented. See also [gh-17325](https://github.com/numpy/numpy/issues/17325) covering issues with NumPy's lack of a good API for this.

### `copy` keyword semantics

The `copy` keyword in `asarray` and `array` will now support `True`/`False`/`None` with new meanings:

  - `True` - Always make a copy.
  - `False` - Never make a copy. If a copy is required, a `ValueError` is raised.
  - `None` - A copy will only be made if it is necessary (previously `False`).

The `copy` keyword in `astype` will stick to its current meaning, because "never copy" when asking for a cast to a different dtype doesn't quite make sense.

There is still one hiccup for the change in semantics: if for user code `np.array(obj, copy=False)`, NumPy may end up calling `obj.__array__` and in that case turning the result into a NumPy array is the responsibility of the implementer of `obj.__array__`. Therefore, we need to add a `copy=None` keyword to `__array__` as well, and pass the copy keyword value along - taking care to not break backwards compatibility when the implementer of `__array__` does not yet have the new keyword (a `DeprecationWarning` will be emitted in that case, to allow for a gradual transition).

### New function name aliases

In the Python API cleanup for NumPy 2.0 (see \[NEP52\](\#nep52)) we spent a lot of effort removing aliases. So introducing new aliases has to have a good rationale. In this case, it is needed in order to match other libraries. The main set of aliases added is for trigonometric functions, where the array API standard chose to follow C99 and other libraries in using `acos`, `asin` etc. rather than `arccos`, `arcsin`, etc. NumPy usually also follows C99; it is not entirely clear why this naming choice was made many years ago.

In total 13 aliases are added to the main namespace and 2 aliases to `numpy.linalg`:

  - trigonometry functions: `acos`, `acosh`, `asin`, `asinh`, `atan`, `atanh`, `atan2`
  - bit-wise functions: `bitwise_left_shift`, `bitwise_invert`, `bitwise_right_shift`
  - other functions: `concat`, `permute_dims`, `pow`
  - in `numpy.linalg`: `tensordot`, `matmul`

In the future NumPy can choose to hide the original names from its `__dir__` to nudge users to the preferred spelling for each function.

### New keywords with overlapping semantics

Similarly to function name aliases, there are a couple of new keywords which have overlap with existing ones:

  - `correction` keyword for `std` and `var` (overlaps with `ddof`)
  - `stable` keyword for `sort` and `argsort` (overlaps with `kind`)

The `correction` name is for clarity ("delta degrees of freedom" is not easy to understand). `stable` is complementary to `kind`, which already has `'stable'` as an option (a separate keyword may be more discoverable though and hence nice to have anyway), allowing a library to reserve the right to change/improve the stable and unstable sorting algorithms.

### New `unique_*` functions

The `unique` function, with `return_index`, `return_inverse`, and `return_counts` arguments that influence the cardinality of the returned tuple, is replaced in the array API by four respective functions: `unique_all`, `unique_counts`, `unique_inverse`, and `unique_values`. These new functions avoid polymorphism, which tends to be a problem for JIT compilers and static typing. Use of these functions therefore helps tools like Numba as well as users of static type checkers like Mypy.

### `np.bool` addition

One of the aliases that used to live in NumPy but was removed is `np.bool`. To comply with the array API it was reintroduced with a different meaning, as now it points to NumPy's bool instead of a Python builtin. This change is a good idea and we were planning to make it anyway, because `bool` is a nicer name than `bool_`. However, we may not have scheduled that reintroduction of the name for 2.0 if it had not been part of the array API standard.

## Parts of the standard that are not adopted

There are a couple of things that the standard prescribes which we propose *not* to follow (at least at this time). These are:

1.  The requirement for `sum` and `prod` to always upcast lower-precision floating-point dtypes to `float64` when `dtype=None`.
    
    *Rationale: this is potentially disruptive (e.g.,* `float32_arr - float32_arr.mean()` *would yield a float64 array, and double memory use). While this upcasting is already done for inputs with lower-precision integer dtypes and seems useful there to prevent overflows, it seems less reasonable to require this for floating-point dtypes.*
    
    [array-api\#731](https://github.com/data-apis/array-api/issues/731) was opened to reconsider this design choice in the standard, and that was accepted for the next standard version.

2.  Making function signatures positional-only and keyword-only in many places.
    
    *Rationale: the 2022.12 version of the standard said "must", but this has already been softened to "should" in the about-to-be-released 2023.12 version, to recognize that it's okay to not do this - it's still possible for users of the array library to write their code using the recommended style after all. For NumPy these changes would be useful, and it seems likely that we may introduce many or all of them over time (and in fact ufuncs are already compliant), however there is no need to rush this change - doing so for 2.0 would be unnecessarily disruptive.*

3.  The requirement "An in-place operation must have the same behavior (including special cases) as its respective binary (i.e., two operand, non-assignment) operation" (excluding the effect on views).
    
    *Rationale: the requirement is very reasonable and probably expected behavior for most NumPy users. However, deprecating unsafe casts for in-place operators is a change for which the impact is hard to predict. Hence this needs to be investigated first, and then if the impact is low enough it may be possible to deprecate the current behavior according to NumPy's normal backwards compatibility guidelines.*
    
    This topic is tracked in [gh-25621](https://github.com/numpy/numpy/issues/25621).

\> **Note** \> We note that one NumPy-specific behavior that remains is returning array scalars rather than 0-D arrays in most cases where the standard, and other array libraries, return 0-D arrays (e.g., indexing and reductions). Array scalars basically duck type 0-D arrays, which is allowed by the standard (it doesn't mandate that there is only one array type, nor contains `isinstance` checks or other semantics that won't work with array scalars). There have been multiple discussions over the past year about the feasibility of removing array scalars from NumPy, or at least no longer returning them by default. However, this would be a large effort with some uncertainty about technical risks and impact of the change, and no one has taken it on. Given that array scalars implement a largely array-compatible interface, this doesn't seem like the highest-prio item regarding array API standard compatibility (or in general).

## Related work

The array API standard ([html docs](https://data-apis.org/array-api/2022.12/), [repository](https://github.com/data-apis/array-api/)) is the first related work; a lot of design discussion in its issue tracker may be relevant in case reasons for particular decisions need to be found.

Downstream adoption from array-consuming libraries is actively happening at the moment, see for example:

  - scikit-learn [docs on array API support](https://scikit-learn.org/dev/modules/array_api.html) and [PRs](https://github.com/scikit-learn/scikit-learn/pulls?q=is%3Aopen+is%3Apr+label%3A%22Array+API%22) and [issues](https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+is%3Aissue+label%3A%22Array+API%22) labeled with *Array API*.
  - SciPy [docs on array API support](http://scipy.github.io/devdocs/dev/api-dev/array_api.html) and [PRs](https://github.com/scipy/scipy/pulls?q=is%3Aopen+is%3Apr+label%3A%22array+types%22) and [issues](https://github.com/scipy/scipy/issues?q=is%3Aopen+is%3Aissue+label%3A%22array+types%22) labeled with *array types*.
  - Einops [docs on supported frameworks](https://einops.rocks/#supported-frameworks) and [PR to implement array API standard support](https://github.com/arogozhnikov/einops/pull/261).

Other array libraries either already have support or are implementing support for the array API standard (in sync with the changes for NumPy 2.0, since they usually try to be as compatible to NumPy as possible). For example:

  - CuPy's [docs on array API support](https://docs.cupy.dev/en/stable/reference/array_api.html) and [PRs labelled with array-api](https://github.com/cupy/cupy/pulls?q=is%3Aopen+is%3Apr+label%3Aarray-api).
  - JAX: enhancement proposal [Scope of JAX NumPy & SciPy Wrappers](https://jax.readthedocs.io/en/latest/jep/18137-numpy-scipy-scope.html#axis-2-array-api-alignment) and [tracking issue](https://github.com/google/jax/issues/18353).

## Implementation

The tracking issue for Array API standard support ([gh-25076](https://github.com/numpy/numpy/issues/25076)) records progress of implementing full support and links to related discussions. It lists all relevant PRs (merged and pending) that verify or provide array API support.

As NEP 52 blends to some degree with this NEP, we can find some relevant implementations and discussion also on its tracking issue ([gh-23999](https://github.com/numpy/numpy/issues/23999)).

The PR that was merged as one of the first contained a new CI job that adds the [array-api-tests](https://github.com/data-apis/array-api-tests) test suite. This way we had a better control over which batch of functions/aliases were being added each time, and could be sure that the implementations conformed to the array API standard (see [gh-25167](https://github.com/numpy/numpy/pull/25167)).

Then, we continued to merge one batch at the time, adding a specific API section. Below we list some of the more substantial ones, including some that we discussed in the previous sections of this NEP:

  - [gh-25167: MAINT: Add array-api-tests CI stage, add ndarray.\_\_array\_namespace\_\_](https://github.com/numpy/numpy/pull/25167).
  - [gh-25088: API: Add Array API setops \[Array API\]](https://github.com/numpy/numpy/pull/25088)
  - [gh-25155: API: Add matrix\_norm, vector\_norm, vecdot and matrix\_transpose \[Array API\]](https://github.com/numpy/numpy/pull/25155)
  - [gh-25080: API: Add and redefine numpy.bool \[Array API\]](https://github.com/numpy/numpy/pull/25080)
  - [gh-25054: API: Introduce np.isdtype function \[Array API\]](https://github.com/numpy/numpy/pull/25054)
  - [gh-25168: API: Introduce copy argument for np.asarray \[Array API\]](https://github.com/numpy/numpy/pull/25168)

## Alternatives

The alternatives to implementing support for the array API standard in NumPy's main namespace include:

  - one or more of the superseded NEPs, or
  - making `ndarray.__array_namespace__()` return a hidden namespace (or even another new public namespace) with compatible functions,
  - not implementing support for the array API standard at all.

The superseded NEPs all have some drawbacks compared to the array API standard, and by now a lot of work has gone into the standard - as well as adoption by other key libraries. So those alternatives are not appealing. Given the amount of interest in this topic, doing nothing also is not appealing. The "hidden namespace" option would be a smaller change to this proposal. We prefer not to do that since it leads to duplicate implementations staying around, a more complex implementation (e.g., potential issues with static typing), and still having two flavors of essentially the same API.

An alternative to removing `numpy.array_api` from NumPy is to keep it in its current place, since it is still useful - it is the best way to test if downstream code is actually portable between array libraries. This is a very reasonable alternative, however there is a slight preference for taking that module and turning it into a standalone package.

## Discussion

## References and footnotes

## Copyright

This document has been placed in the public domain.

1.  <https://numpy.org/user-survey-2020/>, 2020 NumPy User Survey results

2.  <https://labs.quansight.org/blog/array-api-support-scikit-learn>

3.  <https://labs.quansight.org/blog/scipy-array-api>

4.  1.  Meurer et al., "Python Array API Standard: Toward Array Interoperability in the Scientific Python Ecosystem." (2023), <https://conference.scipy.org/proceedings/scipy2023/pdfs/aaron_meurer.pdf>

---

nep-template.md

---

# NEP X â€” Template and instructions

  - Author  
    \<list of authors' real names and optionally, email addresses\>

  - Status  
    \<Draft | Active | Accepted | Deferred | Rejected | Withdrawn | Final | Superseded\>

  - Type  
    \<Standards Track | Process\>

  - Created  
    \<date created on, in yyyy-mm-dd format\>

  - Resolution  
    \<url\> (required for Accepted | Rejected | Withdrawn)

## Abstract

The abstract should be a short description of what the NEP will achieve.

Note that the â€” in the title is an elongated dash, not -.

## Motivation and scope

This section describes the need for the proposed change. It should describe the existing problem, who it affects, what it is trying to solve, and why. This section should explicitly address the scope of and key requirements for the proposed change.

## Usage and impact

This section describes how users of NumPy will use features described in this NEP. It should be comprised mainly of code examples that wouldn't be possible without acceptance and implementation of this NEP, as well as the impact the proposed changes would have on the ecosystem. This section should be written from the perspective of the users of NumPy, and the benefits it will provide them; and as such, it should include implementation details only if necessary to explain the functionality.

## Backward compatibility

This section describes the ways in which the NEP breaks backward compatibility.

The mailing list post will contain the NEP up to and including this section. Its purpose is to provide a high-level summary to users who are not interested in detailed technical discussion, but may have opinions around, e.g., usage and impact.

## Detailed description

This section should provide a detailed description of the proposed change. It should include examples of how the new functionality would be used, intended use-cases and pseudo-code illustrating its use.

## Related work

This section should list relevant and/or similar technologies, possibly in other libraries. It does not need to be comprehensive, just list the major examples of prior and relevant art.

## Implementation

This section lists the major steps required to implement the NEP. Where possible, it should be noted where one step is dependent on another, and which steps may be optionally omitted. Where it makes sense, each step should include a link to related pull requests as the implementation progresses.

Any pull requests or development branches containing work on this NEP should be linked to from here. (A NEP does not need to be implemented in a single pull request if it makes sense to implement it in discrete phases).

## Alternatives

If there were any alternative solutions to solving the same problem, they should be discussed here, along with a justification for the chosen approach.

## Discussion

This section may just be a bullet list including links to any discussions regarding the NEP:

  - This includes links to mailing list threads or relevant GitHub issues.

## References and footnotes

## Copyright

This document has been placed in the public domain.\[1\]

1.  Each NEP must either be explicitly labeled as placed in the public domain (see this NEP as an example) or licensed under the [Open Publication License](https://www.opencontent.org/openpub/).

---

roadmap.md

---

# NumPy roadmap

This is a live snapshot of tasks and features we will be investing resources in. It may be used to encourage and inspire developers and to search for funding.

## Interoperability

We aim to make it easier to interoperate with NumPy. There are many NumPy-like packages that add interesting new capabilities to the Python ecosystem, as well as many libraries that extend NumPy's model in various ways. Work in NumPy to facilitate interoperability with all such packages, and the code that uses them, may include (among other things) interoperability protocols, better duck typing support and ndarray subclass handling.

The key goal is: *make it easy for code written for NumPy to also work with other NumPy-like projects.* This will enable GPU support via, e.g, CuPy, JAX or PyTorch, distributed array support via Dask, and writing special-purpose arrays (either from scratch, or as a `numpy.ndarray` subclass) that work well with SciPy, scikit-learn and other such packages. A large step forward in this area was made in NumPy 2.0, with adoption of and compliance with the array API standard (v2022.12, see \[NEP47\](\#nep47)). Future work in this direction will include support for newer versions of the array API standard, and adding features as needed based on real-world experience and needs.

In addition, the `__array_ufunc__` and `__array_function__` protocols fulfill a role here - they are stable and used by several downstream projects.

## Performance

Improvements to NumPy's performance are important to many users. We have focused this effort on Universal SIMD (see \[NEP38\](\#nep38)) intrinsics which provide nice improvements across various hardware platforms via an abstraction layer. The infrastructure is in place, and we welcome follow-on PRs to add SIMD support across relevant NumPy functionality.

Transitioning from C to C++, both in the SIMD infrastructure and in NumPy internals more widely, is in progress. We have also started to make use of Google Highway (see \[NEP54\](\#nep54)), and that usage is likely to expand. Work towards support for newer SIMD instruction sets, like SVE on arm64, is ongoing.

Other performance improvement ideas include:

  - A better story around parallel execution (related is support for free-threaded CPython, see further down).
  - Enable the ability to allow NumPy to use faster, but less precise, implementations for ufuncs. Until now, the only state modifying ufunc behavior has been `np.errstate`. But, with NumPy 2.0 improvements in the `np.errstate` and the ufunc C implementation make this type of addition easier.
  - Optimizations in individual functions.

Furthermore we would like to improve the benchmarking system, in terms of coverage, easy of use, and publication of the results. Benchmarking PRs/branches compared to the <span class="title-ref">main</span> branch is a primary purpose, and required for PRs that are performance-focused (e.g., adding SIMD acceleration to a function). In addition, we'd like a performance overview like the one we had [here](https://pv.github.io/numpy-bench), set up in a way that is more maintainable long-term.

## Documentation and website

The NumPy [documentation](https://www.numpy.org/devdocs) is of varying quality. The API documentation is in good shape; tutorials and high-level documentation on many topics are missing or outdated. See \[NEP44\](\#nep44) for planned improvements. Adding more tutorials is underway in the [numpy-tutorials repo](https://github.com/numpy/numpy-tutorials).

We also intend to make all the example code in our documentation interactive -work is underway to do so via `jupyterlite-sphinx` and Pyodide.

Our website (<https://numpy.org>) is in good shape. Further work on expanding the number of languages that the website is translated in is desirable. As are improvements to the interactive notebook widget, through JupyterLite.

## Extensibility

We aim to continue making it easier to extend NumPy. The primary topic here is to improve the dtype system - see for example \[NEP41\](\#nep41) and related NEPs linked from it. In NumPy 2.0, a [new C API for user-defined dtypes](https://numpy.org/devdocs/reference/c-api/array.html#custom-data-types) was made public. We aim to encourage its usage and improve this API further, including support for writing a dtype in Python.

Ideas for new dtypes that may be developed outside of the main NumPy repository first, and that could potentially be upstreamed into NumPy later, include:

  - A quad-precision (128-bit) dtype
  - A `bfloat16` dtype
  - A fixed-width string dtype which supports encodings (e.g., `utf8` or `latin1`)
  - A unit dtype

We further plan to extend the ufunc C API as needs arise. One possibility here is creating a new, more powerful, API to allow hooking into existing NumPy ufunc implementations.

## User experience

### Type annotations

Type annotations for most NumPy functionality is complete (although some submodules like `numpy.ma` are missing return types), so users can use tools like [mypy](https://mypy.readthedocs.io) to type check their code and IDEs can improve their support for NumPy. Improving those type annotations, for example to support annotating array shapes (see [gh-16544](https://github.com/numpy/numpy/issues/16544)), is ongoing.

### Platform support

We aim to increase our support for different hardware architectures. This includes adding CI coverage when CI services are available, providing wheels on PyPI for platforms that are in high enough demand (e.g., we added `musllinux` ones for NumPy 2.0), and resolving build issues on platforms that we don't test in CI (e.g., AIX).

We intend to write a NEP covering the support levels we provide and what is required for a platform to move to a higher tier of support, similar to [PEP 11](https://peps.python.org/pep-0011/).

### Further consistency fixes to promotion and scalar logic

NumPy 2.0 fixed many issues around promotion especially with respect to scalars. We plan to continue fixing remaining inconsistencies. For example, NumPy converts 0-D objects to scalars, and some promotions still allowed by NumPy are problematic.

### Support for free-threaded CPython

CPython 3.13 will be the first release to offer a free-threaded build (i.e., a CPython build with the GIL disabled). Work is in progress to support this well in NumPy. After that is stable and complete, there may be opportunities to actually make use of the potential for performance improvements from free-threaded CPython, or make it easier to do so for NumPy's users.

### Binary size reduction

The number of downloads of NumPy from PyPI and other platforms continues to increase - as of May 2024 we're at \>200 million downloads/month from PyPI alone. Reducing the size of an installed NumPy package has many benefits: faster installs, lower disk space usage, smaller load on PyPI, less environmental impact, easier to fit more packages on top of NumPy in resource-constrained environments and platforms like AWS Lambda, lower latency for Pyodide users, and so on. We aim for significant reductions, as well as making it easier for end users and packagers to produce smaller custom builds (e.g., we added support for stripping tests before 2.1.0). See [gh-25737](https://github.com/numpy/numpy/issues/25737) for details.

### Support use of CPython's limited C API

Use of the CPython limited C API, allowing producing `abi3` wheels that use the stable ABI and are hence independent of CPython feature releases, has benefits for both downstream packages that use NumPy's C API and for NumPy itself. In NumPy 2.0, work was done to enable using the limited C API with the Cython support in NumPy (see <span class="title-ref">gh-25531 \<https://github.com/numpy/numpy/pull/25531</span>\_\_). More work and testing is needed to ensure full support for downstream packages.

We also want to explore what is needed for NumPy itself to use the limited C API - this would make testing new CPython dev and pre-release versions across the ecosystem easier, and significantly reduce the maintenance effort for CI jobs in NumPy itself.

### Create a header-only package for NumPy

We have reduced the platform-dependent content in the public NumPy headers to almost nothing. It is now feasible to create a separate package with only NumPy headers and a discovery mechanism for them, in order to enable downstream packages to build against the NumPy C API without having NumPy installed. This will make it easier/cheaper to use NumPy's C API, especially on more niche platforms for which we don't provide wheels.

## NumPy 2.0 stabilization & downstream usage

We made a very large amount of changes (and improvements\!) in NumPy 2.0. The release process has taken a very long time, and part of the ecosystem is still catching up. We may need to slow down for a while, and possibly help the rest of the ecosystem with adapting to the ABI and API changes.

We will need to assess the costs and benefits to NumPy itself, downstream package authors, and end users. Based on that assessment, we need to come to a conclusion on whether it's realistic to do another ABI-breaking release again in the future or not. This will also inform the future evolution of our C API.

## Security

NumPy is quite secure - we get only a limited number of reports about potential vulnerabilities, and most of those are incorrect. We have made strides with a documented security policy, a private disclosure method, and maintaining an OpenSSF scorecard (with a high score). However, we have not changed much in how we approach supply chain security in quite a while. We aim to make improvements here, for example achieving fully reproducible builds for all the build artifacts we publish - and providing full provenance information for them.

## Maintenance

  - `numpy.ma` is still in poor shape and under-maintained. It needs to be improved, ideas include:
      - Rewrite masked arrays to not be a ndarray subclass -- maybe in a separate project?
      - MaskedArray as a duck-array type, and/or
      - dtypes that support missing values
  - Write a strategy on how to deal with overlap between NumPy and SciPy for `linalg`.
  - Deprecate `np.matrix` (very slowly) - this is feasible once the switch-over from sparse matrices to sparse arrays in SciPy is complete.
  - Add new indexing modes for "vectorized indexing" and "outer indexing" (see \[NEP21\](\#nep21)).
  - Make the polynomial API easier to use.

---

scope.md

---

# Scope of NumPy

Here, we describe aspects of N-d array computation that are within scope for NumPy development. This is *not* an aspirational definition of where NumPy should aim, but instead captures the status quoâ€”areas which we have decided to continue supporting, at least for the time being.

  - **In-memory, N-dimensional, homogeneously typed (single pointer + strided) arrays on CPUs**
      - Support for a wide range of data types
      - Not specialized hardware such as GPUs
      - But, do support wide range of CPUs (e.g. ARM, PowerX)
  - **Higher level APIs for N-dimensional arrays**
      - NumPy is a *de facto* standard for array APIs in Python
      - Indexing and fast iteration over elements (ufunc)
      - Interoperability protocols with other data container implementations (like \[\_\_array\_ufunc\_\_ and \_\_array\_function\_\_ \<basics.dispatch\>\](\#\_\_array\_ufunc\_\_-and-\_\_array\_function\_\_-\<basics.dispatch\>).
  - **Python API and a C API** to the ndarray's methods and attributes.
  - Other **specialized types or uses of N-dimensional arrays**:
      - Masked arrays
      - Structured arrays (informally known as record arrays)
      - Memory mapped arrays
  - Historically, NumPy has included the following **basic functionality in support of scientific computation**. We intend to keep supporting (but not to expand) what is currently included:
      - Linear algebra
      - Fast Fourier transforms and windowing
      - Pseudo-random number generators
      - Polynomial fitting
  - NumPy provides some **infrastructure for other packages in the scientific Python ecosystem**:
      - numpy.distutils (build support for C++, Fortran, BLAS/LAPACK, and other relevant libraries for scientific computing)
      - f2py (generating bindings for Fortran code)
      - testing utilities
  - **Speed**: we take performance concerns seriously and aim to execute operations on large arrays with similar performance as native C code. That said, where conflict arises, maintenance and portability take precedence over performance. We aim to prevent regressions where possible (e.g., through asv).

---

benchmarking.md

---

# NumPy benchmarks

Benchmarking NumPy with Airspeed Velocity.

## Usage

Airspeed Velocity manages building and Python virtualenvs by itself, unless told otherwise. To run the benchmarks, you do not need to install a development version of NumPy to your current Python environment.

Before beginning, ensure that *airspeed velocity* is installed. By default, <span class="title-ref">asv</span> ships with support for anaconda and virtualenv:

    pip install asv
    pip install virtualenv

After contributing new benchmarks, you should test them locally before submitting a pull request.

To run all benchmarks, navigate to the root NumPy directory at the command line and execute:

    spin bench

This builds NumPy and runs all available benchmarks defined in `benchmarks/`. (Note: this could take a while. Each benchmark is run multiple times to measure the distribution in execution times.)

For **testing** benchmarks locally, it may be better to run these without replications:

    cd benchmarks/
    export REGEXP="bench.*Ufunc"
    asv run --dry-run --show-stderr --python=same --quick -b $REGEXP

Where the regular expression used to match benchmarks is stored in `$REGEXP`, and <span class="title-ref">--quick</span> is used to avoid repetitions.

To run benchmarks from a particular benchmark module, such as `bench_core.py`, simply append the filename without the extension:

    spin bench -t bench_core

To run a benchmark defined in a class, such as `MeshGrid` from `bench_creation.py`:

    spin bench -t bench_creation.MeshGrid

Compare changes in benchmark results to another version/commit/branch, use the `--compare` option (or the equivalent `-c`):

    spin bench --compare v1.6.2 -t bench_core
    spin bench --compare 20d03bcfd -t bench_core
    spin bench -c main -t bench_core

All of the commands above display the results in plain text in the console, and the results are not saved for comparison with future commits. For greater control, a graphical view, and to have results saved for future comparison you can run ASV commands (record results and generate HTML):

    cd benchmarks
    asv run -n -e --python=same
    asv publish
    asv preview

More on how to use `asv` can be found in [ASV documentation](https://asv.readthedocs.io/) Command-line help is available as usual via `asv --help` and `asv run --help`.

## Benchmarking versions

To benchmark or visualize only releases on different machines locally, the tags with their commits can be generated, before being run with `asv`, that is:

    cd benchmarks
    # Get commits for tags
    # delete tag_commits.txt before re-runs
    for gtag in $(git tag --list --sort taggerdate | grep "^v"); do
    git log $gtag --oneline -n1 --decorate=no | awk '{print $1;}' >> tag_commits.txt
    done
    # Use the last 20
    tail --lines=20 tag_commits.txt > 20_vers.txt
    asv run HASHFILE:20_vers.txt
    # Publish and view
    asv publish
    asv preview

For details on contributing these, see the [benchmark results repository](https://github.com/HaoZeke/asv-numpy).

## Writing benchmarks

See [ASV documentation](https://asv.readthedocs.io/) for basics on how to write benchmarks.

Some things to consider:

  - The benchmark suite should be importable with any NumPy version.
  - The benchmark parameters etc. should not depend on which NumPy version is installed.
  - Try to keep the runtime of the benchmark reasonable.
  - Prefer ASV's `time_` methods for benchmarking times rather than cooking up time measurements via `time.clock`, even if it requires some juggling when writing the benchmark.
  - Preparing arrays etc. should generally be put in the `setup` method rather than the `time_` methods, to avoid counting preparation time together with the time of the benchmarked operation.
  - Be mindful that large arrays created with `np.empty` or `np.zeros` might not be allocated in physical memory until the memory is accessed. If this is desired behaviour, make sure to comment it in your setup function. If you are benchmarking an algorithm, it is unlikely that a user will be executing said algorithm on a newly created empty/zero array. One can force pagefaults to occur in the setup phase either by calling `np.ones` or `arr.fill(value)` after creating the array.

---

blas_lapack.md

---

# BLAS and LAPACK

## Default behavior for BLAS and LAPACK selection

When a NumPy build is invoked, BLAS and LAPACK library detection happens automatically. The build system will attempt to locate a suitable library, and try a number of known libraries in a certain order - most to least performant. A typical order is: MKL, Accelerate, OpenBLAS, FlexiBLAS, BLIS, plain `libblas`/`liblapack`. This may vary per platform or over releases. That order, and which libraries are tried, can be changed through the `blas-order` and `lapack-order` build options, for example:

    $ python -m pip install . -Csetup-args=-Dblas-order=openblas,mkl,blis -Csetup-args=-Dlapack-order=openblas,mkl,lapack

The first suitable library that is found will be used. In case no suitable library is found, the NumPy build will print a warning and then use (slow\!) NumPy-internal fallback routines. In order to disallow use of those slow routines, the `allow-noblas` build option can be used:

    $ python -m pip install . -Csetup-args=-Dallow-noblas=false

By default the LP64 (32-bit integer) interface to BLAS and LAPACK will be used. For building against the ILP64 (64-bit integer) interface, one must use the `use-ilp64` build option:

    $ python -m pip install . -Csetup-args=-Duse-ilp64=true

## Selecting specific BLAS and LAPACK libraries

The `blas` and `lapack` build options are set to "auto" by default, which means trying all known libraries. If you want to use a specific library, you can set these build options to the library name (typically the lower-case name that `pkg-config` expects). For example, to select plain `libblas` and `liblapack` (this is typically Netlib BLAS/LAPACK on Linux distros, and can be dynamically switched between implementations on conda-forge), use:

    $ # for a development build
    $ spin build -C-Dblas=blas -C-Dlapack=lapack
    
    $ # to build and install a wheel
    $ python -m build -Csetup-args=-Dblas=blas -Csetup-args=-Dlapack=lapack
    $ pip install dist/numpy*.whl
    
    $ # Or, with pip>=23.1, this works too:
    $ python -m pip install . -Csetup-args=-Dblas=blas -Csetup-args=-Dlapack=lapack

Other options that should work (as long as they're installed with `pkg-config` support; otherwise they may still be detected but things are inherently more fragile) include `openblas`, `mkl`, `accelerate`, `atlas` and `blis`.

## Using pkg-config to detect libraries in a nonstandard location

The way BLAS and LAPACK detection works under the hood is that Meson tries to discover the specified libraries first with `pkg-config`, and then with CMake. If all you have is a standalone shared library file (e.g., `armpl_lp64.so` in `/a/random/path/lib/` and a corresponding header file in `/a/random/path/include/`), then what you have to do is craft your own pkg-config file. It should have a matching name (so in this example, `armpl_lp64.pc`) and may be located anywhere. The `PKG_CONFIG_PATH` environment variable should be set to point to the location of the `.pc` file. The contents of that file should be:

    libdir=/path/to/library-dir      # e.g., /a/random/path/lib
    includedir=/path/to/include-dir  # e.g., /a/random/path/include
    version=1.2.3                    # set to actual version
    extralib=-lm -lpthread -lgfortran   # if needed, the flags to link in dependencies
    Name: armpl_lp64
    Description: ArmPL - Arm Performance Libraries
    Version: ${version}
    Libs: -L${libdir} -larmpl_lp64      # linker flags
    Libs.private: ${extralib}
    Cflags: -I${includedir}

To check that this works as expected, you should be able to run:

    $ pkg-config --libs armpl_lp64
    -L/path/to/library-dir -larmpl_lp64
    $ pkg-config --cflags armpl_lp64
    -I/path/to/include-dir

## Full list of BLAS and LAPACK related build options

BLAS and LAPACK are complex dependencies. Some libraries have more options that are exposed via build options (see `meson.options` in the root of the repo for all of NumPy's build options).

  - `blas`: name of the BLAS library to use (default: `auto`),
  - `lapack`: name of the LAPACK library to use (default: `auto`),
  - `allow-noblas`: whether or not to allow building without external BLAS/LAPACK libraries (default: `true`),
  - `blas-order`: order of BLAS libraries to try detecting (default may vary per platform),
  - `lapack-order`: order of LAPACK libraries to try detecting,
  - `use-ilp64`: whether to use the ILP64 interface (default: `false`),
  - `blas-symbol-suffix`: the symbol suffix to use for the detected libraries (default: `auto`),
  - `mkl-threading`: which MKL threading layer to use, one of `seq`, `iomp`, `gomp`, `tbb` (default: `auto`).

---

compilers_and_options.md

---

# Compiler selection and customizing a build

## Selecting a specific compiler

Meson supports the standard environment variables `CC`, `CXX` and `FC` to select specific C, C++ and/or Fortran compilers. These environment variables are documented in [the reference tables in the Meson docs](https://mesonbuild.com/Reference-tables.html#compiler-and-linker-flag-environment-variables).

Note that environment variables only get applied from a clean build, because they affect the configure stage (i.e., `meson setup`). An incremental rebuild does not react to changes in environment variables - you have to run `git clean -xdf` and do a full rebuild, or run `meson setup --reconfigure`.

## Adding a custom compiler or linker flag

Meson by design prefers builds being configured through command-line options passed to `meson setup`. It provides many built-in options:

  - For enabling a debug build and the optimization level, see the next section on "build types",
  - Enabling `-Werror` in a portable manner is done via `-Dwerror=true`,
  - Enabling warning levels is done via `-Dwarning_level=<val>`, with `<val>` one of `{0, 1, 2, 3, everything}`,
  - There are many other builtin options, from activating Visual Studio (`-Dvsenv=true`) and building with link time optimization (`-Db_lto`) to changing the default C++ language level (`-Dcpp_std='c++17'`) or linker flags (`-Dcpp_link_args='-Wl,-z,defs'`).

For a comprehensive overview of options, see [Meson's builtin options docs page](https://mesonbuild.com/Builtin-options.html).

Meson also supports the standard environment variables `CFLAGS`, `CXXFLAGS`, `FFLAGS` and `LDFLAGS` to inject extra flags - with the same caveat as in the previous section about those environment variables being picked up only for a clean build and not an incremental build.

## Using different build types with Meson

Meson provides different build types while configuring the project. You can see the available options for build types in [the "core options" section of the Meson documentation](https://mesonbuild.com/Builtin-options.html#core-options).

Assuming that you are building from scratch (do `git clean -xdf` if needed), you can configure the build as following to use the `debug` build type:

    spin build -- -Dbuildtype=debug

Now, you can use the `spin` interface for further building, installing and testing NumPy as normal:

    spin test -s linalg

This will work because after initial configuration, Meson will remember the config options.

## Controlling build parallelism

By default, `ninja` will launch `2*n_cpu + 2`, with `n_cpu` the number of physical CPU cores, parallel build jobs. This is fine in the vast majority of cases, and results in close to optimal build times. In some cases, on machines with a small amount of RAM relative to the number of CPU cores, this leads to a job running out of memory. In case that happens, lower the number of jobs `N` such that you have at least 2 GB RAM per job. For example, to launch 6 jobs:

    python -m pip install . -Ccompile-args="-j6"

or:

    spin build -j6

---

cross_compilation.md

---

# Cross compilation

Cross compilation is a complex topic, we only add some hopefully helpful hints here (for now). As of May 2023, cross-compilation based on `crossenv` is known to work, as used (for example) in conda-forge. Cross-compilation without `crossenv` requires some manual overrides. You instruct these overrides by passing options to `meson setup` via [meson-python](https://meson-python.readthedocs.io/en/latest/how-to-guides/meson-args.html).

All distributions that are known to successfully cross compile NumPy are using `python -m build` (`pypa/build`), but using `pip` for that should be possible as well. Here are links to the NumPy "build recipes" on those distros:

  - [Void Linux](https://github.com/void-linux/void-packages/blob/master/srcpkgs/python3-numpy/template)
  - [Nix](https://github.com/nixos/nixpkgs/blob/master/pkgs/development/python-modules/numpy/default.nix)
  - [Conda-forge](https://github.com/conda-forge/numpy-feedstock/blob/main/recipe/build.sh)

See also [Meson's documentation on cross compilation](https://mesonbuild.com/Cross-compilation.html) to learn what options you may need to pass to Meson to successfully cross compile.

One possible hiccup is that the build requires running a compiled executable in order to determine the `long double` format for the host platform. This may be an obstacle, since it requires `crossenv` or QEMU to run the host (cross) Python. To avoid this problem, specify the paths to the relevant directories in your *cross file*:

``` ini
[properties]
longdouble_format = 'IEEE_DOUBLE_LE'
```

For more details and the current status around cross compilation, see:

  - The state of cross compilation in Python: [pypackaging-native key issue page](https://pypackaging-native.github.io/key-issues/cross_compilation/)
  - Tracking issue for SciPy cross-compilation needs and issues: [scipy\#14812](https://github.com/scipy/scipy/issues/14812)

---

distutils_equivalents.md

---

# Meson and `distutils` ways of doing things

*Old workflows (numpy.distutils based):*

1.  `python runtests.py`
2.  `python setup.py build_ext -i` + `export PYTHONPATH=/home/username/path/to/numpy/reporoot` (and then edit pure Python code in NumPy and run it with `python some_script.py`).
3.  `python setup.py develop` - this is similar to (2), except in-place build is made permanently visible in env.
4.  `python setup.py bdist_wheel` + `pip install dist/numpy*.whl` - build wheel in current env and install it.
5.  `pip install .` - build wheel in an isolated build env against deps in `pyproject.toml` and install it. *Note: be careful, this is usually not the correct command for development installs - typically you want to use (4) or* `pip install . -v --no-build-isolation`.

*New workflows (Meson and meson-python based):*

1.  `spin test`
2.  `pip install -e . --no-build-isolation` (note: only for working on NumPy itself - for more details, see \[IDE support & editable installs \<meson-editable-installs\>\](\#ide-support-&-editable-installs-\<meson-editable-installs\>))
3.  the same as (2)
4.  `python -m build --no-isolation` + `pip install dist/numpy*.whl` - see [pypa/build](https://pypa-build.readthedocs.io/en/latest/).
5.  `pip install .`

---

index.md

---

# Building from source

\> **Note** \> If you are only trying to install NumPy, we recommend using binaries - see [Installation](https://numpy.org/install) for details on that.

Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc.) first, and then invoking a build. The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages. And it may be desired to customize aspects of how the build is done. This guide will cover all these aspects. In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build & packaging documentation that is relevant.

## System-level dependencies

NumPy uses compiled code for speed, which means you need compilers and some other system-level (i.e, non-Python / non-PyPI) dependencies to build it on your system.

\> **Note** \> If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS. All other dependencies will be installed automatically by the `mamba env create -f environment.yml` command.

<div class="tab-set">

<div class="tab-item" data-sync="linux">

Linux

If you want to use the system Python and `pip`, you will need:

  - C and C++ compilers (typically GCC).
  - Python header files (typically a package named `python3-dev` or `python3-devel`)
  - BLAS and LAPACK libraries. [OpenBLAS](https://github.com/OpenMathLib/OpenBLAS/) is the NumPy default; other variants include Apple Accelerate, [MKL](https://software.intel.com/en-us/intel-mkl), [ATLAS](http://math-atlas.sourceforge.net/) and [Netlib](https://www.netlib.org/lapack/index.html) (or "Reference") BLAS and LAPACK.
  - `pkg-config` for dependency detection.
  - A Fortran compiler is needed only for running the `f2py` tests. The instructions below include a Fortran compiler, however you can safely leave it out.

<div class="tab-set">

<div class="tab-item">

Debian/Ubuntu Linux

To install NumPy build requirements, you can do:

    sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev

Alternatively, you can do:

    sudo apt build-dep numpy

This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers.

</div>

<div class="tab-item">

Fedora

To install NumPy build requirements, you can do:

    sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig

Alternatively, you can do:

    sudo dnf builddep numpy

This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers.

</div>

<div class="tab-item">

CentOS/RHEL

To install NumPy build requirements, you can do:

    sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig

Alternatively, you can do:

    sudo yum-builddep numpy

This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers.

</div>

<div class="tab-item">

Arch

To install NumPy build requirements, you can do:

    sudo pacman -S gcc-fortran openblas pkgconf

</div>

</div>

</div>

<div class="tab-item" data-sync="macos">

macOS

Install Apple Developer Tools. An easy way to do this is to [open a terminal window](https://blog.teamtreehouse.com/introduction-to-the-mac-os-x-command-line), enter the command:

    xcode-select --install

and follow the prompts. Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required.

Do *not* use the macOS system Python. Instead, install Python with [the python.org installer](https://www.python.org/downloads/) or with a package manager like Homebrew, MacPorts or Fink.

On macOS \>=13.3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default.

On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS. These are easiest to install with [Homebrew](https://brew.sh/):

    brew install openblas pkg-config gfortran

</div>

<div class="tab-item" data-sync="windows">

Windows

On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC can't be used together. If you don't need to run the `f2py` tests, simply using MSVC is easiest. Otherwise, you will need one of these sets of compilers:

1.  MSVC + Intel Fortran (`ifort`)
2.  Intel compilers (`icc`, `ifort`)
3.  Mingw-w64 compilers (`gcc`, `g++`, `gfortran`)

Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers. It is not possible to just call a one-liner on the command prompt as you would on other platforms.

First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the [Visual Studio download site](https://visualstudio.microsoft.com/downloads/)). This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space). The recommended version of the UCRT is \>= 10.0.22621.0.

<div class="tab-set">

<div class="tab-item">

MSVC

The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a `vswhere.exe` command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a `.bat` file for the correct bitness and architecture (e.g., for 64-bit Intel CPUs, use `vcvars64.bat`).

If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate .bat file executed to set these variables.

For detailed guidance, see [Use the Microsoft C++ toolset from the command line](https://learn.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-170).

</div>

<div class="tab-item">

Intel

Similar to MSVC, the Intel compilers are designed to be used with an activation script (`Intel\oneAPI\setvars.bat`) that you run in the shell you are using. This makes the compilers available on the path. For detailed guidance, see [Get Started with the IntelÂ® oneAPI HPC Toolkit for Windows](https://www.intel.com/content/www/us/en/docs/oneapi-hpc-toolkit/get-started-guide-windows/2023-1/overview.html).

</div>

<div class="tab-item">

MinGW-w64

There are several sources of binaries for MinGW-w64. We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions [here](https://chocolatey.org/install)):

    choco install rtools -y --no-progress --force --version=4.0.0.20220206

</div>

</div>

<div class="note">

<div class="title">

Note

</div>

Compilers should be on the system path (i.e., the `PATH` environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the `PATH`. You can use any shell (e.g., Powershell, `cmd` or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., `gfortran --version` or `ifort --version`).

</div>

\> **Warning**

</div>

</div>

  - \>  
    When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler. If that happens, remove the `compilers` entry from `environment.yml` and try again. The Fortran compiler should be installed as described in this section.

## Building NumPy from source

If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use `pip`. Otherwise, conda is recommended.

\> **Note** \> If you don't have a conda installation yet, we recommend using [Miniforge](https://github.com/conda-forge/miniforge); any conda flavor will work though.

### Building from source to use NumPy

<div class="tab-set">

<div class="tab-item" data-sync="conda">

Conda env

If you are using a conda environment, `pip` is still the tool you use to invoke a from-source build of NumPy. It is important to always use the `--no-build-isolation` flag to the `pip install` command, to avoid building against a `numpy` wheel from PyPI. In order for that to work you must first install the remaining build dependencies into the conda environment:

    # Either install all NumPy dev dependencies into a fresh conda environment
    mamba env create -f environment.yml
    
    # Or, install only the required build dependencies
    mamba install python numpy cython compilers openblas meson-python pkg-config
    
    # To build the latest stable release:
    pip install numpy --no-build-isolation --no-binary numpy
    
    # To build a development version, you need a local clone of the NumPy git repository:
    git clone https://github.com/numpy/numpy.git
    cd numpy
    git submodule update --init
    pip install . --no-build-isolation

\> **Warning**

</div>

</div>

  - \>  
    On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail. These variables are only needed for flang and can be safely unset prior to running pip install.
    
    <div class="tab-item">
    
    Virtual env or system Python
    
    </div>
    
      - sync  
        pip
    
    <!-- end list -->
    
        # To build the latest stable release:
    
    pip install numpy --no-binary numpy
    
    \# To build a development version, you need a local clone of the NumPy git repository: git clone <https://github.com/numpy/numpy.git> cd numpy git submodule update --init pip install .

### Building from source for NumPy development

If you want to build from source in order to work on NumPy itself, first clone the NumPy repository:

    git clone https://github.com/numpy/numpy.git
    cd numpy
    git submodule update --init

Then you want to do the following:

1.  Create a dedicated development environment (virtual environment or conda environment),
2.  Install all needed dependencies (*build*, and also *test*, *doc* and *optional* dependencies),
3.  Build NumPy with the `spin` developer interface.

Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments:

<div class="tab-set">

<div class="tab-item" data-sync="conda">

Conda env

To create a `numpy-dev` development environment with every required and optional dependency installed, run:

    mamba env create -f environment.yml
    mamba activate numpy-dev

</div>

<div class="tab-item" data-sync="pip">

Virtual env or system Python

\> **Note**

</div>

</div>

  - \>  
    There are many tools to manage virtual environments, like `venv`, `virtualenv`/`virtualenvwrapper`, `pyenv`/`pyenv-virtualenv`, Poetry, PDM, Hatch, and more. Here we use the basic `venv` tool that is part of the Python stdlib. You can use any other tool; all we need is an activated Python environment.
    
    Create and activate a virtual environment in a new directory named `venv` ( note that the exact activation command may be different based on your OS and shell - see ["How venvs work"](https://docs.python.org/3/library/venv.html#how-venvs-work) in the `venv` docs).
    
    <div class="tab-set">
    
    </div>
    
    <div class="tab-item" data-sync="linux">
    
    Linux
    
        python -m venv venv
        source venv/bin/activate
    
    </div>
    
    <div class="tab-item" data-sync="macos">
    
    macOS
    
        python -m venv venv
        source venv/bin/activate
    
    </div>
    
    <div class="tab-item" data-sync="windows">
    
    Windows
    
        python -m venv venv
        .\venv\Scripts\activate
    
    </div>
    
    Then install the Python-level dependencies from PyPI with:
    
    python -m pip install -r requirements/all\_requirements.txt

To build NumPy in an activated development environment, run:

    spin build

This will install NumPy inside the repository (by default in a `build-install` directory). You can then run tests (`spin test`), drop into IPython (`spin ipython`), or take other development steps like build the html documentation or running benchmarks. The `spin` interface is self-documenting, so please see `spin --help` and `spin <subcommand> --help` for detailed guidance.

\> **Warning** \> In an activated conda enviroment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail. These variables are only needed for flang and can be safely unset for build.

<div id="meson-editable-installs">

<div class="admonition">

IDE support & editable installs

While the `spin` interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using `spin` will not be recognized automatically within an IDE (e.g., for running a script via a "run" button, or setting breakpoints visually). This will work better with an *in-place build* (or "editable install").

Editable installs are supported. It is important to understand that **you may use either an editable install or \`\`spin\`\` in a given repository clone, but not both**. If you use editable installs, you have to use `pytest` and other development tools directly instead of using `spin`.

To use an editable install, ensure you start from a clean repository (run `git clean -xdf` if you've built with `spin` before) and have all dependencies set up correctly as described higher up on this page. Then do:

    # Note: the --no-build-isolation is important!
    pip install -e . --no-build-isolation
    
    # To run the tests for, e.g., the `numpy.linalg` module:
    pytest numpy/linalg

When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall. NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the [meson-python](https://mesonbuild.com/meson-python/) documentation on editable installs for more details on how that works under the hood.

When you run `git clean -xdf`, which removes the built extension modules, remember to also uninstall NumPy with `pip uninstall numpy`.

<div class="warning">

<div class="title">

Warning

</div>

Note that editable installs are fundamentally incomplete installs. Their only guarantee is that `import numpy` works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy. Headers, entrypoints, and other such things may not be available from an editable install.

</div>

</div>

</div>

## Customizing builds

<div class="toctree" data-maxdepth="1">

compilers\_and\_options blas\_lapack cross\_compilation redistributable\_binaries

</div>

## Background information

<div class="toctree" data-maxdepth="1">

understanding\_meson introspecting\_a\_build distutils\_equivalents

</div>

---

introspecting_a_build.md

---

# Introspecting build steps

When you have an issue with a particular Python extension module or other build target, there are a number of ways to figure out what the build system is doing exactly. Beyond looking at the `meson.build` content for the target of interest, these include:

1.  Reading the generated `build.ninja` file in the build directory,
2.  Using `meson introspect` to learn more about build options, dependencies and flags used for the target,
3.  Reading `<build-dir>/meson-info/*.json` for details on discovered dependencies, where Meson plans to install files to, etc.

These things are all available after the configure stage of the build (i.e., `meson setup`) has run. It is typically more effective to look at this information, rather than running the build and reading the full build log.

For more details on this topic, see the [SciPy doc page on build introspection](http://scipy.github.io/devdocs/building/introspecting_a_build.html).

---

redistributable_binaries.md

---

# Building redistributable binaries

When `python -m build` or `pip wheel` is used to build a NumPy wheel, that wheel will rely on external shared libraries (at least for BLAS/LAPACK and a Fortran compiler runtime library, perhaps other libraries). Such wheels therefore will only run on the system on which they are built. See [the pypackaging-native content under "Building and installing or uploading artifacts"](https://pypackaging-native.github.io/meta-topics/build_steps_conceptual/#building-and-installing-or-uploading-artifacts) for more context on that.

A wheel like that is therefore an intermediate stage to producing a binary that can be distributed. That final binary may be a wheel - in that case, run `auditwheel` (Linux), `delocate` (macOS) or `delvewheel` (Windows) to vendor the required shared libraries into the wheel.

The final binary may also be in another packaging format (e.g., a `.rpm`, `.deb` or `.conda` package). In that case, there are packaging ecosystem-specific tools to first install the wheel into a staging area, then making the extension modules in that install location relocatable (e.g., by rewriting RPATHs), and then repackaging it into the final package format.

---

understanding_meson.md

---

# Understanding Meson

Building NumPy relies on the following tools, which can be considered part of the build system:

  - `meson`: the Meson build system, installable as a pure Python package from PyPI or conda-forge
  - `ninja`: the build tool invoked by Meson to do the actual building (e.g. invoking compilers). Installable also from PyPI (on all common platforms) or conda-forge.
  - `pkg-config`: the tool used for discovering dependencies (in particular BLAS/LAPACK). Available on conda-forge (and Homebrew, Chocolatey, and Linux package managers), but not packaged on PyPI.
  - `meson-python`: the Python build backend (i.e., the thing that gets invoked via a hook in `pyproject.toml` by a build frontend like `pip` or `pypa/build`). This is a thin layer on top of Meson, with as main roles (a) interface with build frontends, and (b) produce sdists and wheels with valid file names and metadata.

\> **Warning** \> As of Dec'23, NumPy vendors a custom version of Meson, which is needed for SIMD and BLAS/LAPACK features that are not yet available in upstream Meson. Hence, using the `meson` executable directly is not possible. Instead, wherever instructions say `meson xxx`, use `python    vendored-meson/meson/meson.py xxx` instead.

Building with Meson happens in stages:

  - A configure stage (`meson setup`) to detect compilers, dependencies and build options, and create the build directory and `build.ninja` file,
  - A compile stage (`meson compile` or `ninja`), where the extension modules that are part of a built NumPy package get compiled,
  - An install stage (`meson install`) to install the installable files from the source and build directories to the target install directory,

Meson has a good build dependency tracking system, so invoking a build for a second time will rebuild only targets for which any sources or dependencies have changed.

## To learn more about Meson

Meson has [very good documentation](https://mesonbuild.com/); it pays off to read it, and is often the best source of answers for "how to do X". Furthermore, an extensive pdf book on Meson can be obtained for free at <https://nibblestew.blogspot.com/2021/12/this-year-receive-gift-of-free-meson.html>

To learn more about the design principles Meson uses, the recent talks linked from [mesonbuild.com/Videos](https://mesonbuild.com/Videos.html) are also a good resource.

## Explanation of build stages

*This is for teaching purposes only; there should be no need to execute these stages separately\!*

Assume we're starting from a clean repo and a fully set up conda environment:

    git clone git@github.com:numpy/numpy.git
    git submodule update --init
    mamba env create -f environment.yml
    mamba activate numpy-dev

To now run the configure stage of the build and instruct Meson to put the build artifacts in `build/` and a local install under `build-install/` relative to the root of the repo, do:

    meson setup build --prefix=$PWD/build-install

To then run the compile stage of the build, do:

    ninja -C build

In the command above, `-C` is followed by the name of the build directory. You can have multiple build directories at the same time. Meson is fully out-of-place, so those builds will not interfere with each other. You can for example have a GCC build, a Clang build and a debug build in different directories.

To then install NumPy into the prefix (`build-install/` here, but note that that's just an arbitrary name we picked here):

    meson install -C build

It will then install to `build-install/lib/python3.11/site-packages/numpy`, which is not on your Python path, so to add it do (*again, this is for learning purposes, using \`\`PYTHONPATH\`\` explicitly is typically not the best idea*):

    export PYTHONPATH=$PWD/build-install/lib/python3.11/site-packages/

Now we should be able to import `numpy` and run the tests. Remembering that we need to move out of the root of the repo to ensure we pick up the package and not the local `numpy/` source directory:

    cd doc
    python -c "import numpy as np; np.test()"

The above runs the "fast" numpy test suite. Other ways of running the tests should also work, for example:

    pytest --pyargs numpy

The full test suite should pass, without any build warnings on Linux (with the GCC version for which `-Werror` is enforced in CI at least) and with at most a moderate amount of warnings on other platforms.

---

alignment.md

---

<div class="currentmodule">

numpy

</div>

# Memory alignment

## NumPy alignment goals

There are three use-cases related to memory alignment in NumPy (as of 1.14):

1.  Creating `structured datatypes <structured data type>` with `fields <field>` aligned like in a C-struct.
2.  Speeding up copy operations by using <span class="title-ref">uint</span> assignment in instead of `memcpy`.
3.  Guaranteeing safe aligned access for ufuncs/setitem/casting code.

NumPy uses two different forms of alignment to achieve these goals: "True alignment" and "Uint alignment".

"True" alignment refers to the architecture-dependent alignment of an equivalent C-type in C. For example, in x64 systems <span class="title-ref">float64</span> is equivalent to `double` in C. On most systems, this has either an alignment of 4 or 8 bytes (and this can be controlled in GCC by the option `malign-double`). A variable is aligned in memory if its memory offset is a multiple of its alignment. On some systems (eg. sparc) memory alignment is required; on others, it gives a speedup.

"Uint" alignment depends on the size of a datatype. It is defined to be the "True alignment" of the uint used by NumPy's copy-code to copy the datatype, or undefined/unaligned if there is no equivalent uint. Currently, NumPy uses `uint8`, `uint16`, `uint32`, `uint64`, and `uint64` to copy data of size 1, 2, 4, 8, 16 bytes respectively, and all other sized datatypes cannot be uint-aligned.

For example, on a (typical Linux x64 GCC) system, the NumPy <span class="title-ref">complex64</span> datatype is implemented as `struct { float real, imag; }`. This has "true" alignment of 4 and "uint" alignment of 8 (equal to the true alignment of `uint64`).

  - Some cases where uint and true alignment are different (default GCC Linux):
    
    <table>
    <thead>
    <tr class="header">
    <th>arch</th>
    <th>type</th>
    <th>true-aln</th>
    <th>uint-aln</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td>x86_64</td>
    <td>complex64</td>
    <td><blockquote>
    <p>4</p>
    </blockquote></td>
    <td><blockquote>
    <p>8</p>
    </blockquote></td>
    </tr>
    <tr class="even">
    <td>x86_64</td>
    <td>float128</td>
    <td><blockquote>
    <p>16</p>
    </blockquote></td>
    <td><blockquote>
    <p>8</p>
    </blockquote></td>
    </tr>
    <tr class="odd">
    <td>x86</td>
    <td>float96</td>
    <td><blockquote>
    <p>4</p>
    </blockquote></td>
    <td><blockquote>
    <p>-</p>
    </blockquote></td>
    </tr>
    </tbody>
    </table>

## Variables in NumPy which control and describe alignment

There are 4 relevant uses of the word `align` used in NumPy:

  - The <span class="title-ref">dtype.alignment</span> attribute (`descr->alignment` in C). This is meant to reflect the "true alignment" of the type. It has arch-dependent default values for all datatypes, except for the structured types created with `align=True` as described below.
  - The `ALIGNED` flag of an ndarray, computed in `IsAligned` and checked by :c\`PyArray\_ISALIGNED\`. This is computed from <span class="title-ref">dtype.alignment</span>. It is set to `True` if every item in the array is at a memory location consistent with <span class="title-ref">dtype.alignment</span>, which is the case if the `data ptr` and all strides of the array are multiples of that alignment.
  - The `align` keyword of the dtype constructor, which only affects \[structured\_arrays\](\#structured\_arrays). If the structure's field offsets are not manually provided, NumPy determines offsets automatically. In that case, `align=True` pads the structure so that each field is "true" aligned in memory and sets <span class="title-ref">dtype.alignment</span> to be the largest of the field "true" alignments. This is like what C-structs usually do. Otherwise if offsets or itemsize were manually provided `align=True` simply checks that all the fields are "true" aligned and that the total itemsize is a multiple of the largest field alignment. In either case <span class="title-ref">dtype.isalignedstruct</span> is also set to True.
  - `IsUintAligned` is used to determine if an ndarray is "uint aligned" in an analogous way to how `IsAligned` checks for true alignment.

## Consequences of alignment

Here is how the variables above are used:

1.  Creating aligned structs: To know how to offset a field when `align=True`, NumPy looks up `field.dtype.alignment`. This includes fields that are nested structured arrays.
2.  Ufuncs: If the `ALIGNED` flag of an array is False, ufuncs will buffer/cast the array before evaluation. This is needed since ufunc inner loops access raw elements directly, which might fail on some archs if the elements are not true-aligned.
3.  Getitem/setitem/copyswap function: Similar to ufuncs, these functions generally have two code paths. If `ALIGNED` is False they will use a code path that buffers the arguments so they are true-aligned.
4.  Strided copy code: Here, "uint alignment" is used instead. If the itemsize of an array is equal to 1, 2, 4, 8 or 16 bytes and the array is uint aligned then instead NumPy will do `*(uintN*)dst) = *(uintN*)src)` for appropriate N. Otherwise, NumPy copies by doing `memcpy(dst, src, N)`.
5.  Nditer code: Since this often calls the strided copy code, it must check for "uint alignment".
6.  Cast code: This checks for "true" alignment, as it does `*dst = CASTFUNC(*src)` if aligned. Otherwise, it does `memmove(srcval, src); dstval = CASTFUNC(srcval); memmove(dst, dstval)` where dstval/srcval are aligned.

Note that the strided-copy and strided-cast code are deeply intertwined and so any arrays being processed by them must be both uint and true aligned, even though the copy-code only needs uint alignment and the cast code only true alignment. If there is ever a big rewrite of this code it would be good to allow them to use different alignments.

---

depending_on_numpy.md

---

# For downstream package authors

This document aims to explain some best practices for authoring a package that depends on NumPy.

## Understanding NumPy's versioning and API/ABI stability

NumPy uses a standard, `440` compliant, versioning scheme: `major.minor.bugfix`. A *major* release is highly unusual and if it happens it will most likely indicate an ABI break. NumPy 1.xx releases happened from 2006 to 2023; NumPy 2.0 in early 2024 is the first release which changed the ABI (minor ABI breaks for corner cases may have happened a few times in minor releases). *Minor* versions are released regularly, typically every 6 months. Minor versions contain new features, deprecations, and removals of previously deprecated code. *Bugfix* releases are made even more frequently; they do not contain any new features or deprecations.

It is important to know that NumPy, like Python itself and most other well known scientific Python projects, does **not** use semantic versioning. Instead, backwards incompatible API changes require deprecation warnings for at least two releases. For more details, see \[NEP23\](\#nep23).

NumPy has both a Python API and a C API. The C API can be used directly or via Cython, f2py, or other such tools. If your package uses the C API, then ABI (application binary interface) stability of NumPy is important. NumPy's ABI is forward but not backward compatible. This means: binaries compiled against a given target version of NumPy's C API will still run correctly with newer NumPy versions, but not with older versions.

## Testing against the NumPy main branch or pre-releases

For large, actively maintained packages that depend on NumPy, we recommend testing against the development version of NumPy in CI. To make this easy, nightly builds are provided as wheels at <https://anaconda.org/scientific-python-nightly-wheels/>. Example install command:

    pip install -U --pre --only-binary :all: -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy

This helps detect regressions in NumPy that need fixing before the next NumPy release. Furthermore, we recommend to raise errors on warnings in CI for this job, either all warnings or otherwise at least `DeprecationWarning` and `FutureWarning`. This gives you an early warning about changes in NumPy to adapt your code.

If you want to test your own wheel builds against the latest NumPy nightly build and you're using `cibuildwheel`, you may need something like this in your CI config file:

    CIBW_ENVIRONMENT: "PIP_PRE=1 PIP_EXTRA_INDEX_URL=https://pypi.anaconda.org/scientific-python-nightly-wheels/simple"

## Adding a dependency on NumPy

### Build-time dependency

\> **Note** \> Before NumPy 1.25, the NumPy C-API was *not* exposed in a backwards compatible way by default. This means that when compiling with a NumPy version earlier than 1.25 you have to compile with the oldest version you wish to support. This can be done by using [oldest-supported-numpy](https://github.com/scipy/oldest-supported-numpy/). Please see the [NumPy 1.24 documentation](https://numpy.org/doc/1.24/dev/depending_on_numpy.html).

If a package either uses the NumPy C API directly or it uses some other tool that depends on it like Cython or Pythran, NumPy is a *build-time* dependency of the package.

By default, NumPy will expose an API that is backwards compatible with the oldest NumPy version that supports the currently oldest compatible Python version. NumPy 1.25.0 supports Python 3.9 and higher and NumPy 1.19 is the first version to support Python 3.9. Thus, we guarantee that, when using defaults, NumPy 1.25 will expose a C-API compatible with NumPy 1.19. (the exact version is set within NumPy-internal header files).

NumPy is also forward compatible for all minor releases, but a major release will require recompilation (see NumPy 2.0-specific advice further down).

The default behavior can be customized for example by adding:

    #define NPY_TARGET_VERSION NPY_1_22_API_VERSION

before including any NumPy headers (or the equivalent `-D` compiler flag) in every extension module that requires the NumPy C-API. This is mainly useful if you need to use newly added API at the cost of not being compatible with older versions.

If for some reason you wish to compile for the currently installed NumPy version by default you can add:

    #ifndef NPY_TARGET_VERSION
        #define NPY_TARGET_VERSION NPY_API_VERSION
    #endif

Which allows a user to override the default via `-DNPY_TARGET_VERSION`. This define must be consistent for each extension module (use of `import_array()`) and also applies to the umath module.

When you compile against NumPy, you should add the proper version restrictions to your `pyproject.toml` (see PEP 517). Since your extension will not be compatible with a new major release of NumPy and may not be compatible with very old versions.

For conda-forge packages, please see [here](https://conda-forge.org/docs/maintainer/knowledge_base.html#building-against-numpy).

as of now, it is usually as easy as including:

    host:
      - numpy
    run:
      - {{ pin_compatible('numpy') }}

### Runtime dependency & version ranges

NumPy itself and many core scientific Python packages have agreed on a schedule for dropping support for old Python and NumPy versions: \[NEP29\](\#nep29). We recommend all packages depending on NumPy to follow the recommendations in NEP 29.

For *run-time dependencies*, specify version bounds using `install_requires` in `setup.py` (assuming you use `numpy.distutils` or `setuptools` to build).

Most libraries that rely on NumPy will not need to set an upper version bound: NumPy is careful to preserve backward-compatibility.

That said, if you are (a) a project that is guaranteed to release frequently, (b) use a large part of NumPy's API surface, and (c) is worried that changes in NumPy may break your code, you can set an upper bound of `<MAJOR.MINOR + N` with N no less than 3, and `MAJOR.MINOR` being the current release of NumPy\[1\]. If you use the NumPy C API (directly or via Cython), you can also pin the current major version to prevent ABI breakage. Note that setting an upper bound on NumPy may [affect the ability of your library to be installed alongside other, newer packages](https://iscinumpy.dev/post/bound-version-constraints/).

\> **Note** \> SciPy has more documentation on how it builds wheels and deals with its build-time and runtime dependencies [here](https://scipy.github.io/devdocs/dev/core-dev/index.html#distributing).

> NumPy and SciPy wheel build CI may also be useful as a reference, it can be found [here for NumPy](https://github.com/MacPython/numpy-wheels) and [here for SciPy](https://github.com/MacPython/scipy-wheels).

### NumPy 2.0-specific advice

NumPy 2.0 is an ABI-breaking release, however it does contain support for building wheels that work on both 2.0 and 1.xx releases. It's important to understand that:

1.  When you build wheels for your package using a NumPy 1.xx version at build time, those **will not work** with NumPy 2.0.
2.  When you build wheels for your package using a NumPy 2.x version at build time, those **will work** with NumPy 1.xx.

The first time the NumPy ABI for 2.0 is guaranteed to be stable will be the release of the first release candidate for 2.0 (i.e., 2.0.0rc1). Our advice for handling your dependency on NumPy is as follows:

1.  In the main (development) branch of your package, do not add any constraints.
2.  If you rely on the NumPy C API (e.g. via direct use in C/C++, or via Cython code that uses NumPy), add a `numpy<2.0` requirement in your package's dependency metadata for releases / in release branches. Do this until numpy `2.0.0rc1` is released and you can target that. *Rationale: the NumPy C ABI will change in 2.0, so any compiled extension modules that rely on NumPy will break; they need to be recompiled.*
3.  If you rely on a large API surface from NumPy's Python API, also consider adding the same `numpy<2.0` requirement to your metadata until you are sure your code is updated for changes in 2.0 (i.e., when you've tested things work against `2.0.0rc1`). *Rationale: we will do a significant API cleanup, with many aliases and deprecated/non-recommended objects being removed (see, e.g.,* \[numpy-2-migration-guide\](\#numpy-2-migration-guide) *and* \[NEP52\](\#nep52)), *so unless you only use modern/recommended functions and objects, your code is likely to require at least some adjustments.*
4.  Plan to do a release of your own packages which depend on `numpy` shortly after the first NumPy 2.0 release candidate is released (probably around 1 Feb 2024). *Rationale: at that point, you can release packages that will work with both 2.0 and 1.X, and hence your own end users will not be seeing much/any disruption (you want* `pip install mypackage` *to continue working on the day NumPy 2.0 is released).*
5.  Once `2.0.0rc1` is available, you can adjust your metadata in `pyproject.toml` in the way outlined below.

There are two cases: you need to keep compatibility with numpy 1.xx while also supporting 2.0, or you are able to drop numpy 1.xx support for new releases of your package and support \>=2.0 only. The latter is simpler, but may be more restrictive for your users. In that case, simply add `numpy>=2.0` (or `numpy>=2.0.0rc1`) to your build and runtime requirements and you're good to go. We'll focus on the "keep compatibility with 1.xx and 2.x" now, which is a little more involved.

*Example for a package using the NumPy C API (via C/Cython/etc.) which wants to support NumPy 1.23.5 and up*:

``` ini
[build-system]
build-backend = ...
requires = [
    # Note for packagers: this constraint is specific to wheels
    # for PyPI; it is also supported to build against 1.xx still.
    # If you do so, please ensure to include a `numpy<2.0`
    # runtime requirement for those binary packages.
    "numpy>=2.0.0rc1",
    ...
]

[project]
dependencies = [
    "numpy>=1.23.5",
]
```

We recommend that you have at least one CI job which builds/installs via a wheel, and then runs tests against the oldest numpy version that the package supports. For example:

``` yaml
- name: Build wheel via wheel, then install it
  run: |
    python -m build  # This will pull in numpy 2.0 in an isolated env
    python -m pip install dist/*.whl

- name: Test against oldest supported numpy version
  run: |
    python -m pip install numpy==1.23.5
    # now run test suite
```

The above only works once NumPy 2.0 is available on PyPI. If you want to test against a NumPy 2.0-dev wheel, you have to use a numpy nightly build (see \[this section \<testing-prereleases\>\](\#this-section-\<testing-prereleases\>) higher up) or build numpy from source.

1.  The reason for setting `N=3` is that NumPy will, on the rare occasion where it makes breaking changes, raise warnings for at least two releases. (NumPy releases about once every six months, so this translates to a window of at least a year; hence the subsequent requirement that your project releases at least on that cadence.)

---

development_advanced_debugging.md

---

# Advanced debugging tools

If you reached here, you want to dive into, or use, more advanced tooling. This is usually not necessary for first time contributors and most day-to-day development. These are used more rarely, for example close to a new NumPy release, or when a large or particular complex change was made.

Since not all of these tools are used on a regular bases and only available on some systems, please expect differences, issues, or quirks; we will be happy to help if you get stuck and appreciate any improvements or suggestions to these workflows.

## Finding C errors with additional tooling

Most development will not require more than a typical debugging toolchain as shown in \[Debugging \<debugging\>\](\#debugging-\<debugging\>). But for example memory leaks can be particularly subtle or difficult to narrow down.

We do not expect any of these tools to be run by most contributors. However, you can ensure that we can track down such issues more easily:

  - Tests should cover all code paths, including error paths.
  - Try to write short and simple tests. If you have a very complicated test consider creating an additional simpler test as well. This can be helpful, because often it is only easy to find which test triggers an issue and not which line of the test.
  - Never use `np.empty` if data is read/used. `valgrind` will notice this and report an error. When you do not care about values, you can generate random values instead.

This will help us catch any oversights before your change is released and means you do not have to worry about making reference counting errors, which can be intimidating.

### Python debug build

Debug builds of Python are easily available for example via the system package manager on Linux systems, but are also available on other platforms, possibly in a less convenient format. If you cannot easily install a debug build of Python from a system package manager, you can build one yourself using [pyenv](https://github.com/pyenv/pyenv). For example, to install and globally activate a debug build of Python 3.10.8, one would do:

    pyenv install -g 3.10.8
    pyenv global 3.10.8

Note that `pyenv install` builds Python from source, so you must ensure that Python's dependencies are installed before building, see the pyenv documentation for platform-specific installation instructions. You can use `pip` to install Python dependencies you may need for your debugging session. If there is no debug wheel available on <span class="title-ref">pypi,</span> you will need to build the dependencies from source and ensure that your dependencies are also compiled as debug builds.

Often debug builds of Python name the Python executable `pythond` instead of `python`. To check if you have a debug build of Python installed, you can run e.g. `pythond -m sysconfig` to get the build configuration for the Python executable. A debug build will be built with debug compiler options in `CFLAGS` (e.g. `-g -Og`).

Running the Numpy tests or an interactive terminal is usually as easy as:

    python3.8d runtests.py
    # or
    python3.8d runtests.py --ipython

and were already mentioned in \[Debugging \<debugging\>\](\#debugging-\<debugging\>).

A Python debug build will help:

  - Find bugs which may otherwise cause random behaviour. One example is when an object is still used after it has been deleted.

  - Python debug builds allows to check correct reference counting. This works using the additional commands:
    
        sys.gettotalrefcount()
        sys.getallocatedblocks()

  - Python debug builds allow easier debugging with gdb and other C debuggers.

#### Use together with `pytest`

Running the test suite only with a debug python build will not find many errors on its own. An additional advantage of a debug build of Python is that it allows detecting memory leaks.

A tool to make this easier is [pytest-leaks](https://github.com/abalkin/pytest-leaks), which can be installed using `pip`. Unfortunately, `pytest` itself may leak memory, but good results can usually (currently) be achieved by removing:

    @pytest.fixture(autouse=True)
    def add_np(doctest_namespace):
        doctest_namespace['np'] = numpy
    
    @pytest.fixture(autouse=True)
    def env_setup(monkeypatch):
        monkeypatch.setenv('PYTHONHASHSEED', '0')

from `numpy/conftest.py` (This may change with new `pytest-leaks` versions or `pytest` updates).

This allows to run the test suite, or part of it, conveniently:

    python3.8d runtests.py -t numpy/_core/tests/test_multiarray.py -- -R2:3 -s

where `-R2:3` is the `pytest-leaks` command (see its documentation), the `-s` causes output to print and may be necessary (in some versions captured output was detected as a leak).

Note that some tests are known (or even designed) to leak references, we try to mark them, but expect some false positives.

### `valgrind`

Valgrind is a powerful tool to find certain memory access problems and should be run on complicated C code. Basic use of `valgrind` usually requires no more than:

    PYTHONMALLOC=malloc valgrind python runtests.py

where `PYTHONMALLOC=malloc` is necessary to avoid false positives from python itself. Depending on the system and valgrind version, you may see more false positives. `valgrind` supports "suppressions" to ignore some of these, and Python does have a suppression file (and even a compile time option) which may help if you find it necessary.

Valgrind helps:

  - Find use of uninitialized variables/memory.

  - Detect memory access violations (reading or writing outside of allocated memory).

  - Find *many* memory leaks. Note that for *most* leaks the python debug build approach (and `pytest-leaks`) is much more sensitive. The reason is that `valgrind` can only detect if memory is definitely lost. If:
    
        dtype = np.dtype(np.int64)
        arr.astype(dtype=dtype)
    
    Has incorrect reference counting for `dtype`, this is a bug, but valgrind cannot see it because `np.dtype(np.int64)` always returns the same object. However, not all dtypes are singletons, so this might leak memory for different input. In rare cases NumPy uses `malloc` and not the Python memory allocators which are invisible to the Python debug build. `malloc` should normally be avoided, but there are some exceptions (e.g. the `PyArray_Dims` structure is public API and cannot use the Python allocators.)

Even though using valgrind for memory leak detection is slow and less sensitive it can be a convenient: you can run most programs with valgrind without modification.

Things to be aware of:

  - Valgrind does not support the numpy `longdouble`, this means that tests will fail or be flagged errors that are completely fine.
  - Expect some errors before and after running your NumPy code.
  - Caches can mean that errors (specifically memory leaks) may not be detected or are only detect at a later, unrelated time.

A big advantage of valgrind is that it has no requirements aside from valgrind itself (although you probably want to use debug builds for better tracebacks).

#### Use together with `pytest`

You can run the test suite with valgrind which may be sufficient when you are only interested in a few tests:

    PYTHOMMALLOC=malloc valgrind python runtests.py \
     -t numpy/_core/tests/test_multiarray.py -- --continue-on-collection-errors

Note the `--continue-on-collection-errors`, which is currently necessary due to missing `longdouble` support causing failures (this will usually not be necessary if you do not run the full test suite).

If you wish to detect memory leaks you will also require `--show-leak-kinds=definite` and possibly more valgrind options. Just as for `pytest-leaks` certain tests are known to leak cause errors in valgrind and may or may not be marked as such.

We have developed [pytest-valgrind](https://github.com/seberg/pytest-valgrind) which:

  - Reports errors for each test individually
  - Narrows down memory leaks to individual tests (by default valgrind only checks for memory leaks after a program stops, which is very cumbersome).

Please refer to its `README` for more information (it includes an example command for NumPy).

---

development_environment.md

---

# Setting up and using your development environment

## Recommended development setup

Since NumPy contains parts written in C and Cython that need to be compiled before use, make sure you have the necessary compilers and Python development headers installed - see \[building-from-source\](\#building-from-source). Building NumPy as of version `2.0` requires C11 and C++17 compliant compilers.

Having compiled code also means that importing NumPy from the development sources needs some additional steps, which are explained below. For the rest of this chapter we assume that you have set up your git repo as described in \[using-git\](\#using-git).

\> **Note** \> If you are having trouble building NumPy from source or setting up your local development environment, you can try to build NumPy with GitHub Codespaces. It allows you to create the correct development environment right in your browser, reducing the need to install local development environments and deal with incompatible dependencies.

> If you have good internet connectivity and want a temporary set-up, it is often faster to work on NumPy in a Codespaces environment. For documentation on how to get started with Codespaces, see [the Codespaces docs](https://docs.github.com/en/codespaces). When creating a codespace for the `numpy/numpy` repository, the default 2-core machine type works; 4-core will build and work a bit faster (but of course at a cost of halving your number of free usage hours). Once your codespace has started, you can run `conda activate numpy-dev` and your development environment is completely set up - you can then follow the relevant parts of the NumPy documentation to build, test, develop, write docs, and contribute to NumPy.

## Using virtual environments

A frequently asked question is "How do I set up a development version of NumPy in parallel to a released version that I use to do my job/research?".

One simple way to achieve this is to install the released version in site-packages, by using pip or conda for example, and set up the development version in a virtual environment.

If you use conda, we recommend creating a separate virtual environment for numpy development using the `environment.yml` file in the root of the repo (this will create the environment and install all development dependencies at once):

    $ conda env create -f environment.yml  # `mamba` works too for this command
    $ conda activate numpy-dev

If you installed Python some other way than conda, first install [virtualenv](https://virtualenv.pypa.io/) (optionally use [virtualenvwrapper](https://doughellmann.com/projects/virtualenvwrapper/)), then create your virtualenv (named `numpy-dev` here), activate it, and install all project dependencies with:

    $ virtualenv numpy-dev
    $ source numpy-dev/bin/activate # activate virtual environment
    $ python -m pip install -r requirements/all_requirements.txt

Now, whenever you want to switch to the virtual environment, you can use the command `source numpy-dev/bin/activate`, and `deactivate` to exit from the virtual environment and back to your previous shell.

## Building from source

See \[building-from-source\](\#building-from-source).

## Testing builds

Before running the tests, first install the test dependencies:

    $ python -m pip install -r requirements/test_requirements.txt
    $ python -m pip install asv # only for running benchmarks

To build the development version of NumPy and run tests, spawn interactive shells with the Python import paths properly set up etc., use the [spin](https://github.com/scientific-python/spin) utility. To run tests, do one of:

    $ spin test -v
    $ spin test numpy/random  # to run the tests in a specific module
    $ spin test -v -t numpy/_core/tests/test_nditer.py::test_iter_c_order

This builds NumPy first, so the first time it may take a few minutes.

You can also use `spin bench` for benchmarking. See `spin --help` for more command line options.

\> **Note** \> If the above commands result in `RuntimeError: Cannot parse version 0+untagged.xxxxx`, run `git pull upstream main --tags`.

Additional arguments may be forwarded to `pytest` by passing the extra arguments after a bare `--`. For example, to run a test method with the `--pdb` flag forwarded to the target, run the following:

    $ spin test -t numpy/tests/test_scripts.py::test_f2py -- --pdb

You can also [match test names using python operators](https://docs.pytest.org/en/latest/usage.html#specifying-tests-selecting-tests) by passing the `-k` argument to pytest:

    $ spin test -v -t numpy/_core/tests/test_multiarray.py -- -k "MatMul and not vector"

To run "doctests" -- to check that the code examples in the documentation is correct --use the <span class="title-ref">check-docs</span> spin command. It relies on the <span class="title-ref">scipy-docs</span> package, which provides several additional features on top of the standard library `doctest` package. Install `scipy-doctest` and run one of:

    $ spin check-docs -v
    $ spin check-docs numpy/linalg
    $ spin check-docs -v -- -k 'det and not slogdet'

\> **Note** \> Remember that all tests of NumPy should pass before committing your changes.

<div class="note">

<div class="title">

Note

</div>

Some of the tests in the test suite require a large amount of memory, and are skipped if your system does not have enough.

</div>

## Other build options

For more options including selecting compilers, setting custom compiler flags and controlling parallelism, see \[scipy:building/compilers\_and\_options\](scipy:building/compilers\_and\_options.md) (from the SciPy documentation.)

## Running tests

Besides using `spin`, there are various ways to run the tests. Inside the interpreter, tests can be run like this:

    >>> np.test()  # doctest: +SKIPBLOCK
    >>> np.test('full')   # Also run tests marked as slow
    >>> np.test('full', verbose=2)   # Additionally print test name/file
    
    An example of a successful test :
    ``4686 passed, 362 skipped, 9 xfailed, 5 warnings in 213.99 seconds``

Or a similar way from the command line:

    $ python -c "import numpy as np; np.test()"

Tests can also be run with `pytest numpy`, however then the NumPy-specific plugin is not found which causes strange side effects.

Running individual test files can be useful; it's much faster than running the whole test suite or that of a whole module (example: `np.random.test()`). This can be done with:

    $ python path_to_testfile/test_file.py

That also takes extra arguments, like `--pdb` which drops you into the Python debugger when a test fails or an exception is raised.

Running tests with [tox](https://tox.readthedocs.io/) is also supported. For example, to build NumPy and run the test suite with Python 3.9, use:

    $ tox -e py39

For more extensive information, see \[testing-guidelines\](\#testing-guidelines).

Note: do not run the tests from the root directory of your numpy git repo without `spin`, that will result in strange test errors.

## Running linting

Lint checks can be performed on newly added lines of Python code.

Install all dependent packages using pip:

    $ python -m pip install -r requirements/linter_requirements.txt

To run lint checks before committing new code, run:

    $ python tools/linter.py

To check all changes in newly added Python code of current branch with target branch, run:

    $ python tools/linter.py --branch main

If there are no errors, the script exits with no message. In case of errors, check the error message for details:

    $ python tools/linter.py --branch main
    ./numpy/_core/tests/test_scalarmath.py:34:5: E303 too many blank lines (3)
    1       E303 too many blank lines (3)

It is advisable to run lint checks before pushing commits to a remote branch since the linter runs as part of the CI pipeline.

For more details on Style Guidelines:

  - [Python Style Guide](https://www.python.org/dev/peps/pep-0008/)
  - \[NEP45\](\#nep45)

## Rebuilding & cleaning the workspace

Rebuilding NumPy after making changes to compiled code can be done with the same build command as you used previously - only the changed files will be re-built. Doing a full build, which sometimes is necessary, requires cleaning the workspace first. The standard way of doing this is (*note: deletes any uncommitted files\!*):

    $ git clean -xdf

When you want to discard all changes and go back to the last commit in the repo, use one of:

    $ git checkout .
    $ git reset --hard

## Debugging

Another frequently asked question is "How do I debug C code inside NumPy?". First, ensure that you have gdb installed on your system with the Python extensions (often the default on Linux). You can see which version of Python is running inside gdb to verify your setup:

    (gdb) python
    >import sys
    >print(sys.version_info)
    >end
    sys.version_info(major=3, minor=7, micro=0, releaselevel='final', serial=0)

Most python builds do not include debug symbols and are built with compiler optimizations enabled. To get the best debugging experience using a debug build of Python is encouraged, see \[advanced\_debugging\](\#advanced\_debugging).

In terms of debugging, NumPy also needs to be built in a debug mode. You need to use `debug` build type and disable optimizations to make sure `-O0` flag is used during object building. Note that NumPy should NOT be installed in your environment before you build with the `spin build` command.

To generate source-level debug information within the build process run:

    $ spin build --clean -- -Dbuildtype=debug -Ddisable-optimization=true

\> **Note** \> In case you are using conda environment be aware that conda sets `CFLAGS` and `CXXFLAGS` automatically, and they will include the `-O2` flag by default. You can safely use `unset CFLAGS && unset CXXFLAGS` to avoid them or provide them at the beginning of the `spin` command: `CFLAGS="-O0 -g" CXXFLAGS="-O0 -g"`. Alternatively, to take control of these variables more permanently, you can create `env_vars.sh` file in the `<path-to-conda-envs>/numpy-dev/etc/conda/activate.d` directory. In this file you can export `CFLAGS` and `CXXFLAGS` variables. For complete instructions please refer to <https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#saving-environment-variables>.

Next you need to write a Python script that invokes the C code whose execution you want to debug. For instance `mytest.py`:

    import numpy as np
    x = np.arange(5)
    np.empty_like(x)

Note that your test file needs to be outside the NumPy clone you have. Now, you can run:

    $ spin gdb /path/to/mytest.py

In case you are using clang toolchain:

    $ spin lldb /path/to/mytest.py

And then in the debugger:

    (gdb) break array_empty_like
    (gdb) run

lldb counterpart:

    (lldb) breakpoint set --name array_empty_like
    (lldb) run

The execution will now stop at the corresponding C function and you can step through it as usual. A number of useful Python-specific commands are available. For example to see where in the Python code you are, use `py-list`, to see the python traceback, use `py-bt`. For more details, see [DebuggingWithGdb](https://wiki.python.org/moin/DebuggingWithGdb). Here are some commonly used commands:

  - `list`: List specified function or line.
  - `next`: Step program, proceeding through subroutine calls.
  - `step`: Continue program being debugged, after signal or breakpoint.
  - `print`: Print value of expression EXP.

Rich support for Python debugging requires that the `python-gdb.py` script distributed with Python is installed in a path where gdb can find it. If you installed your Python build from your system package manager, you likely do not need to manually do anything. However, if you built Python from source, you will likely need to create a `.gdbinit` file in your home directory pointing gdb at the location of your Python installation. For example, a version of python installed via [pyenv](https://github.com/pyenv/pyenv) needs a `.gdbinit` file with the following contents:

`` `text     add-auto-load-safe-path ~/.pyenv  Building NumPy with a Python built with debug support (on Linux distributions ``<span class="title-ref"> typically packaged as </span><span class="title-ref">python-dbg</span>\`) is highly recommended.

## Understanding the code & getting started

The best strategy to better understand the code base is to pick something you want to change and start reading the code to figure out how it works. When in doubt, you can ask questions on the mailing list. It is perfectly okay if your pull requests aren't perfect, the community is always happy to help. As a volunteer project, things do sometimes get dropped and it's totally fine to ping us if something has sat without a response for about two to four weeks.

So go ahead and pick something that annoys or confuses you about NumPy, experiment with the code, hang around for discussions or go through the reference documents to try to fix it. Things will fall in place and soon you'll have a pretty good understanding of the project as a whole. Good Luck\!

---

development_workflow.md

---

# Development workflow

You already have your own forked copy of the NumPy repository, have configured Git, and have linked the upstream repository as explained in \[linking-to-upstream\](\#linking-to-upstream). What is described below is a recommended workflow with Git.

## Basic workflow

In short:

1.  Start a new *feature branch* for each set of edits that you do. See \[below \<making-a-new-feature-branch\>\](\#below-\<making-a-new-feature-branch\>).
2.  Hack away\! See \[below \<editing-workflow\>\](\#below-\<editing-workflow\>)
3.  When finished:
      - *Contributors*: push your feature branch to your own Github repo, and \[create a pull request \<asking-for-merging\>\](\#create-a-pull-request-\<asking-for-merging\>).
      - *Core developers*: If you want to push changes without further review, see the notes \[below \<pushing-to-main\>\](\#below-\<pushing-to-main\>).

This way of working helps to keep work well organized and the history as clear as possible.

### Making a new feature branch

First, fetch new commits from the `upstream` repository:

    git fetch upstream

Then, create a new branch based on the main branch of the upstream repository:

    git checkout -b my-new-feature upstream/main

### The editing workflow

#### Overview

    # hack hack
    git status # Optional
    git diff # Optional
    git add modified_file
    git commit
    # push the branch to your own Github repo
    git push origin my-new-feature

#### In more detail

1.  Make some changes. When you feel that you've made a complete, working set of related changes, move on to the next steps.

2.  Optional: Check which files have changed with `git status`. You'll see a listing like this one:
    
        # On branch my-new-feature
        # Changed but not updated:
        #   (use "git add <file>..." to update what will be committed)
        #   (use "git checkout -- <file>..." to discard changes in working directory)
        #
        #  modified:   README
        #
        # Untracked files:
        #   (use "git add <file>..." to include in what will be committed)
        #
        #  INSTALL
        no changes added to commit (use "git add" and/or "git commit -a")

3.  Optional: Compare the changes with the previous version using with `git diff`. This brings up a simple text browser interface that highlights the difference between your files and the previous version.

4.  Add any relevant modified or new files using `git add modified_file`. This puts the files into a staging area, which is a queue of files that will be added to your next commit. Only add files that have related, complete changes. Leave files with unfinished changes for later commits.

<!-- end list -->

  - \#. To commit the staged files into the local copy of your repo, do `git    commit`. At this point, a text editor will open up to allow you to write a  
    commit message. Read the \[commit message section\<writing-the-commit-message\>\](\#commit-message

  - \---section\<writing-the-commit-message\>) to be sure that you are writing a  
    properly formatted and sufficiently detailed commit message. After saving your message and closing the editor, your commit will be saved. For trivial commits, a short commit message can be passed in through the command line using the `-m` flag. For example, `git commit -am "ENH: Some message"`.
    
    In some cases, you will see this form of the commit command: `git commit -a`. The extra `-a` flag automatically commits all modified files and removes all deleted files. This can save you some typing of numerous `git add` commands; however, it can add unwanted changes to a commit if you're not careful.

<!-- end list -->

1.  Push the changes to your fork on GitHub:
    
        git push origin my-new-feature

\> **Note** \> Assuming you have followed the instructions in these pages, git will create a default link to your GitHb repo called `origin`. You can ensure that the link to origin is permanently set by using the `--set-upstream` option:

    git push --set-upstream origin my-new-feature
    
    From now on, ``git`` will know that ``my-new-feature`` is related to the
    ``my-new-feature`` branch in your own GitHub repo. Subsequent push calls
    are then simplified to the following::
    
    git push
    
    You have to use ``--set-upstream`` for each new branch that you create.

It may be the case that while you were working on your edits, new commits have been added to `upstream` that affect your work. In this case, follow the \[rebasing-on-main\](\#rebasing-on-main) section of this document to apply those changes to your branch.

#### Writing the commit message

Commit messages should be clear and follow a few basic rules. Example:

    ENH: add functionality X to numpy.<submodule>.
    
    The first line of the commit message starts with a capitalized acronym
    (options listed below) indicating what type of commit this is.  Then a blank
    line, then more text if needed.  Lines shouldn't be longer than 72
    characters.  If the commit is related to a ticket, indicate that with
    "See #3456", "See ticket 3456", "Closes #3456" or similar.

Describing the motivation for a change, the nature of a bug for bug fixes or some details on what an enhancement does are also good to include in a commit message. Messages should be understandable without looking at the code changes. A commit message like `MAINT: fixed another one` is an example of what not to do; the reader has to go look for context elsewhere.

Standard acronyms to start the commit message with are:

    API: an (incompatible) API change
    BENCH: changes to the benchmark suite
    BLD: change related to building numpy
    BUG: bug fix
    CI: continuous integration
    DEP: deprecate something, or remove a deprecated object
    DEV: development tool or utility
    DOC: documentation
    ENH: enhancement
    MAINT: maintenance commit (refactoring, typos, etc.)
    MNT: alias for MAINT
    NEP: NumPy enhancement proposals
    REL: related to releasing numpy
    REV: revert an earlier commit
    STY: style fix (whitespace, PEP8)
    TST: addition or modification of tests
    TYP: static typing
    WIP: work in progress, do not merge

##### Commands to skip continuous integration

By default a lot of continuous integration (CI) jobs are run for every PR, from running the test suite on different operating systems and hardware platforms to building the docs. In some cases you already know that CI isn't needed (or not all of it), for example if you work on CI config files, text in the README, or other files that aren't involved in regular build, test or docs sequences. In such cases you may explicitly skip CI by including one or more of these fragments in each commit message of a PR:

  - `[skip ci]`: skip all CI
    
    Only recommended if you are still not ready for the checks to run on your PR (for example, if this is only a draft.)

  - `[skip actions]`: skip GitHub Actions jobs
    
    [GitHub Actions](https://docs.github.com/actions) is where most of the CI checks are run, including the linter, benchmarking, running basic tests for most architectures and OSs, and several compiler and CPU optimization settings. [See the configuration files for these checks.](https://github.com/numpy/numpy/tree/main/.github/workflows)

  - `[skip azp]`: skip Azure jobs
    
    [Azure](https://azure.microsoft.com/en-us/products/devops/pipelines) is where all comprehensive tests are run. This is an expensive run, and one you could typically skip if you do documentation-only changes, for example. [See the main configuration file for these checks.](https://github.com/numpy/numpy/blob/main/azure-pipelines.yml)

  - `[skip circle]`: skip CircleCI jobs
    
    [CircleCI](https://circleci.com/) is where we build the documentation and store the generated artifact for preview in each PR. This check will also run all the docstrings examples and verify their results. If you don't make documentation changes, but you make changes to a function's API, for example, you may need to run these tests to verify that the doctests are still valid. [See the configuration file for these checks.](https://github.com/numpy/numpy/blob/main/.circleci/config.yml)

  - `[skip cirrus]`: skip Cirrus jobs
    
    [CirrusCI](https://cirrus-ci.org/) mostly triggers Linux aarch64 and MacOS Arm64 wheels uploads. [See the configuration file for these checks.](https://github.com/numpy/numpy/blob/main/.cirrus.star)

##### Test building wheels

Numpy currently uses [cibuildwheel](https://https://cibuildwheel.readthedocs.io/en/stable/) in order to build wheels through continuous integration services. To save resources, the cibuildwheel wheel builders are not run by default on every single PR or commit to main.

If you would like to test that your pull request do not break the wheel builders, you can do so by appending `[wheel build]` to the first line of the commit message of the newest commit in your PR. Please only do so for build-related PRs, because running all wheel builds is slow and expensive.

The wheels built via github actions (including 64-bit Linux, x86-64 macOS, and 32/64-bit Windows) will be uploaded as artifacts in zip files. You can access them from the Summary page of the "Wheel builder" action. The aarch64 Linux and arm64 macOS wheels built via Cirrus CI are not available as artifacts. Additionally, the wheels will be uploaded to <https://anaconda.org/scientific-python-nightly-wheels/> on the following conditions:

  - by a weekly cron job or
  - if the GitHub Actions or Cirrus build has been manually triggered, which requires appropriate permissions

The wheels will be uploaded to <https://anaconda.org/multibuild-wheels-staging/> if the build was triggered by a tag to the repo that begins with `v`

### Get the mailing list's opinion

If you plan a new feature or API change, it's wisest to first email the NumPy [mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion) asking for comment. If you haven't heard back in a week, it's OK to ping the list again.

### Asking for your changes to be merged with the main repo

When you feel your work is finished, you can create a pull request (PR). If your changes involve modifications to the API or addition/modification of a function, add a release note to the `doc/release/upcoming_changes/` directory, following the instructions and format in the `doc/release/upcoming_changes/README.rst` file.

### Getting your PR reviewed

We review pull requests as soon as we can, typically within a week. If you get no review comments within two weeks, feel free to ask for feedback by adding a comment on your PR (this will notify maintainers).

If your PR is large or complicated, asking for input on the numpy-discussion mailing list may also be useful.

### Rebasing on main

This updates your feature branch with changes from the upstream NumPy GitHub repo. If you do not absolutely need to do this, try to avoid doing it, except perhaps when you are finished. The first step will be to update the remote repository with new commits from upstream:

    git fetch upstream

Next, you need to update the feature branch:

    # go to the feature branch
    git checkout my-new-feature
    # make a backup in case you mess up
    git branch tmp my-new-feature
    # rebase on upstream main branch
    git rebase upstream/main

If you have made changes to files that have changed also upstream, this may generate merge conflicts that you need to resolve. See \[below\<recovering-from-mess-up\>\](\#below\<recovering-from-mess-up\>) for help in this case.

Finally, remove the backup branch upon a successful rebase:

    git branch -D tmp

\> **Note** \> Rebasing on main is preferred over merging upstream back to your branch. Using `git merge` and `git pull` is discouraged when working on feature branches.

### Recovering from mess-ups

Sometimes, you mess up merges or rebases. Luckily, in Git it is relatively straightforward to recover from such mistakes.

If you mess up during a rebase:

    git rebase --abort

If you notice you messed up after the rebase:

    # reset branch back to the saved point
    git reset --hard tmp

If you forgot to make a backup branch:

    # look at the reflog of the branch
    git reflog show my-feature-branch
    
    8630830 my-feature-branch@{0}: commit: BUG: io: close file handles immediately
    278dd2a my-feature-branch@{1}: rebase finished: refs/heads/my-feature-branch onto 11ee694744f2552d
    26aa21a my-feature-branch@{2}: commit: BUG: lib: make seek_gzip_factory not leak gzip obj
    ...
    
    # reset the branch to where it was before the botched rebase
    git reset --hard my-feature-branch@{2}

If you didn't actually mess up but there are merge conflicts, you need to resolve those.

## Additional things you might want to do

### Rewriting commit history

\> **Note** \> Do this only for your own feature branches.

There's an embarrassing typo in a commit you made? Or perhaps you made several false starts you would like the posterity not to see.

This can be done via *interactive rebasing*.

Suppose that the commit history looks like this:

    git log --oneline
    eadc391 Fix some remaining bugs
    a815645 Modify it so that it works
    2dec1ac Fix a few bugs + disable
    13d7934 First implementation
    6ad92e5 * masked is now an instance of a new object, MaskedConstant
    29001ed Add pre-nep for a couple of structured_array_extensions.
    ...

and `6ad92e5` is the last commit in the `main` branch. Suppose we want to make the following changes:

  - Rewrite the commit message for `13d7934` to something more sensible.
  - Combine the commits `2dec1ac`, `a815645`, `eadc391` into a single one.

We do as follows:

    # make a backup of the current state
    git branch tmp HEAD
    # interactive rebase
    git rebase -i 6ad92e5

This will open an editor with the following text in it:

    pick 13d7934 First implementation
    pick 2dec1ac Fix a few bugs + disable
    pick a815645 Modify it so that it works
    pick eadc391 Fix some remaining bugs
    
    # Rebase 6ad92e5..eadc391 onto 6ad92e5
    #
    # Commands:
    #  p, pick = use commit
    #  r, reword = use commit, but edit the commit message
    #  e, edit = use commit, but stop for amending
    #  s, squash = use commit, but meld into previous commit
    #  f, fixup = like "squash", but discard this commit's log message
    #
    # If you remove a line here THAT COMMIT WILL BE LOST.
    # However, if you remove everything, the rebase will be aborted.
    #

To achieve what we want, we will make the following changes to it:

    r 13d7934 First implementation
    pick 2dec1ac Fix a few bugs + disable
    f a815645 Modify it so that it works
    f eadc391 Fix some remaining bugs

This means that (i) we want to edit the commit message for `13d7934`, and (ii) collapse the last three commits into one. Now we save and quit the editor.

Git will then immediately bring up an editor for editing the commit message. After revising it, we get the output:

    [detached HEAD 721fc64] FOO: First implementation
     2 files changed, 199 insertions(+), 66 deletions(-)
    [detached HEAD 0f22701] Fix a few bugs + disable
     1 files changed, 79 insertions(+), 61 deletions(-)
    Successfully rebased and updated refs/heads/my-feature-branch.

and the history looks now like this:

    0f22701 Fix a few bugs + disable
    721fc64 ENH: Sophisticated feature
    6ad92e5 * masked is now an instance of a new object, MaskedConstant

If it went wrong, recovery is again possible as explained \[above \<recovering-from-mess-up\>\](\#above \<recovering-from-mess-up\>).

### Deleting a branch on GitHub

    git checkout main
    # delete branch locally
    git branch -D my-unwanted-branch
    # delete branch on github
    git push origin --delete my-unwanted-branch

See also: <https://stackoverflow.com/questions/2003505/how-do-i-delete-a-git-branch-locally-and-remotely>

### Several people sharing a single repository

If you want to work on some stuff with other people, where you are all committing into the same repository, or even the same branch, then just share it via GitHub.

First fork NumPy into your account, as from \[forking\](\#forking).

Then, go to your forked repository github page, say `https://github.com/your-user-name/numpy`

Click on the 'Admin' button, and add anyone else to the repo as a collaborator:

![image](pull_button.png)

Now all those people can do:

    git clone git@github.com:your-user-name/numpy.git

Remember that links starting with `git@` use the ssh protocol and are read-write; links starting with `git://` are read-only.

Your collaborators can then commit directly into that repo with the usual:

    git commit -am 'ENH - much better code'
    git push origin my-feature-branch # pushes directly into your repo

### Checkout changes from an existing pull request

If you want to test the changes in a pull request or continue the work in a new pull request, the commits are to be cloned into a local branch in your forked repository

First ensure your upstream points to the main repo, as from \[linking-to-upstream\](\#linking-to-upstream)

Then, fetch the changes and create a local branch. Assuming `$ID` is the pull request number and `$BRANCHNAME` is the name of the *new local* branch you wish to create:

    git fetch upstream pull/$ID/head:$BRANCHNAME

Checkout the newly created branch:

    git checkout $BRANCHNAME

You now have the changes in the pull request.

### Exploring your repository

To see a graphical representation of the repository branches and commits:

    gitk --all

To see a linear list of commits for this branch:

    git log

### Backporting

Backporting is the process of copying new feature/fixes committed in NumPy's `main` branch back to stable release branches. To do this you make a branch off the branch you are backporting to, cherry pick the commits you want from `numpy/main`, and then submit a pull request for the branch containing the backport.

1.  First, you need to make the branch you will work on. This needs to be based on the older version of NumPy (not main):
    
        # Make a new branch based on numpy/maintenance/1.8.x,
        # backport-3324 is our new name for the branch.
        git checkout -b backport-3324 upstream/maintenance/1.8.x

2.  Now you need to apply the changes from main to this branch using `git cherry-pick`:
    
        # Update remote
        git fetch upstream
        # Check the commit log for commits to cherry pick
        git log upstream/main
        # This pull request included commits aa7a047 to c098283 (inclusive)
        # so you use the .. syntax (for a range of commits), the ^ makes the
        # range inclusive.
        git cherry-pick aa7a047^..c098283
        ...
        # Fix any conflicts, then if needed:
        git cherry-pick --continue

3.  You might run into some conflicts cherry picking here. These are resolved the same way as merge/rebase conflicts. Except here you can use `git blame` to see the difference between main and the backported branch to make sure nothing gets screwed up.

4.  Push the new branch to your Github repository:
    
        git push -u origin backport-3324

5.  Finally make a pull request using Github. Make sure it is against the maintenance branch and not main, Github will usually suggest you make the pull request against main.

### Pushing changes to the main repo

*Requires commit rights to the main NumPy repo.*

When you have a set of "ready" changes in a feature branch ready for NumPy's `main` or `maintenance` branches, you can push them to `upstream` as follows:

1.  First, merge or rebase on the target branch.
    
    1)  Only a few, unrelated commits then prefer rebasing:
        
            git fetch upstream
            git rebase upstream/main
        
        See \[rebasing-on-main\](\#rebasing-on-main).
    
    2)  If all of the commits are related, create a merge commit:
        
            git fetch upstream
            git merge --no-ff upstream/main

2.  Check that what you are going to push looks sensible:
    
        git log -p upstream/main..
        git log --oneline --graph

3.  Push to upstream:
    
        git push upstream my-feature-branch:main

\> **Note** \> It's usually a good idea to use the `-n` flag to `git push` to check first that you're about to push the changes you want to the place you want.

---

governance.md

---

# NumPy project governance and decision-making

The purpose of this document is to formalize the governance process used by the NumPy project in both ordinary and extraordinary situations, and to clarify how decisions are made and how the various elements of our community interact, including the relationship between open source collaborative development and work that may be funded by for-profit or non-profit entities.

## Summary

NumPy is a community-owned and community-run project. To the maximum extent possible, decisions about project direction are made by community consensus (but note that "consensus" here has a somewhat technical meaning that might not match everyone's expectations -- see below). Some members of the community additionally contribute by serving on the NumPy steering council, where they are responsible for facilitating the establishment of community consensus, for stewarding project resources, and -- in extreme cases -- for making project decisions if the normal community-based process breaks down.

## The project

The NumPy Project (The Project) is an open source software project affiliated with the 501(c)3 NumFOCUS Foundation. The goal of The Project is to develop open source software for array-based computing in Python, and in particular the `numpy` package, along with related software such as `f2py` and the NumPy Sphinx extensions. The Software developed by The Project is released under the BSD (or similar) open source license, developed openly and hosted on public GitHub repositories under the `numpy` GitHub organization.

The Project is developed by a team of distributed developers, called Contributors. Contributors are individuals who have contributed code, documentation, designs or other work to the Project. Anyone can be a Contributor. Contributors can be affiliated with any legal entity or none. Contributors participate in the project by submitting, reviewing and discussing GitHub Pull Requests and Issues and participating in open and public Project discussions on GitHub, mailing lists, and other channels. The foundation of Project participation is openness and transparency.

The Project Community consists of all Contributors and Users of the Project. Contributors work on behalf of and are responsible to the larger Project Community and we strive to keep the barrier between Contributors and Users as low as possible.

The Project is formally affiliated with the 501(c)3 NumFOCUS Foundation (<https://numfocus.org/>), which serves as its fiscal sponsor, may hold project trademarks and other intellectual property, helps manage project donations and acts as a parent legal entity. NumFOCUS is the only legal entity that has a formal relationship with the project (see Institutional Partners section below).

## Governance

This section describes the governance and leadership model of The Project.

The foundations of Project governance are:

  - Openness & Transparency
  - Active Contribution
  - Institutional Neutrality

### Consensus-based decision making by the community

Normally, all project decisions will be made by consensus of all interested Contributors. The primary goal of this approach is to ensure that the people who are most affected by and involved in any given change can contribute their knowledge in the confidence that their voices will be heard, because thoughtful review from a broad community is the best mechanism we know of for creating high-quality software.

The mechanism we use to accomplish this goal may be unfamiliar for those who are not experienced with the cultural norms around free/open-source software development. We provide a summary here, and highly recommend that all Contributors additionally read [Chapter 4: Social and Political Infrastructure](https://producingoss.com/en/producingoss.html#social-infrastructure) of Karl Fogel's classic *Producing Open Source Software*, and in particular the section on [Consensus-based Democracy](https://producingoss.com/en/producingoss.html#consensus-democracy), for a more detailed discussion.

In this context, consensus does *not* require:

  - that we wait to solicit everybody's opinion on every change,
  - that we ever hold a vote on anything,
  - or that everybody is happy or agrees with every decision.

For us, what consensus means is that we entrust *everyone* with the right to veto any change if they feel it necessary. While this may sound like a recipe for obstruction and pain, this is not what happens. Instead, we find that most people take this responsibility seriously, and only invoke their veto when they judge that a serious problem is being ignored, and that their veto is necessary to protect the project. And in practice, it turns out that such vetoes are almost never formally invoked, because their mere possibility ensures that Contributors are motivated from the start to find some solution that everyone can live with -- thus accomplishing our goal of ensuring that all interested perspectives are taken into account.

How do we know when consensus has been achieved? In principle, this is rather difficult, since consensus is defined by the absence of vetos, which requires us to somehow prove a negative. In practice, we use a combination of our best judgement (e.g., a simple and uncontroversial bug fix posted on GitHub and reviewed by a core developer is probably fine) and best efforts (e.g., all substantive API changes must be posted to the mailing list in order to give the broader community a chance to catch any problems and suggest improvements; we assume that anyone who cares enough about NumPy to invoke their veto right should be on the mailing list). If no-one bothers to comment on the mailing list after a few days, then it's probably fine. And worst case, if a change is more controversial than expected, or a crucial critique is delayed because someone was on vacation, then it's no big deal: we apologize for misjudging the situation, [back up, and sort things out](https://producingoss.com/en/producingoss.html#version-control-relaxation).

If one does need to invoke a formal veto, then it should consist of:

  - an unambiguous statement that a veto is being invoked,
  - an explanation of why it is being invoked, and
  - a description of what conditions (if any) would convince the vetoer to withdraw their veto.

If all proposals for resolving some issue are vetoed, then the status quo wins by default.

In the worst case, if a Contributor is genuinely misusing their veto in an obstructive fashion to the detriment of the project, then they can be ejected from the project by consensus of the Steering Council -- see below.

### Steering council

The Project will have a Steering Council that consists of Project Contributors who have produced contributions that are substantial in quality and quantity, and sustained over at least one year. The overall role of the Council is to ensure, with input from the Community, the long-term well-being of the project, both technically and as a community.

During the everyday project activities, council members participate in all discussions, code review and other project activities as peers with all other Contributors and the Community. In these everyday activities, Council Members do not have any special power or privilege through their membership on the Council. However, it is expected that because of the quality and quantity of their contributions and their expert knowledge of the Project Software and Services that Council Members will provide useful guidance, both technical and in terms of project direction, to potentially less experienced contributors.

The Steering Council and its Members play a special role in certain situations. In particular, the Council may, if necessary:

  - Make decisions about the overall scope, vision and direction of the project.
  - Make decisions about strategic collaborations with other organizations or individuals.
  - Make decisions about specific technical issues, features, bugs and pull requests. They are the primary mechanism of guiding the code review process and merging pull requests.
  - Make decisions about the Services that are run by The Project and manage those Services for the benefit of the Project and Community.
  - Update policy documents such as this one.
  - Make decisions when regular community discussion doesnâ€™t produce consensus on an issue in a reasonable time frame.

However, the Council's primary responsibility is to facilitate the ordinary community-based decision making procedure described above. If we ever have to step in and formally override the community for the health of the Project, then we will do so, but we will consider reaching this point to indicate a failure in our leadership.

#### Council decision making

If it becomes necessary for the Steering Council to produce a formal decision, then they will use a form of the [Apache Foundation voting process](https://www.apache.org/foundation/voting.html). This is a formalized version of consensus, in which +1 votes indicate agreement, -1 votes are vetoes (and must be accompanied with a rationale, as above), and one can also vote fractionally (e.g. -0.5, +0.5) if one wishes to express an opinion without registering a full veto. These numeric votes are also often used informally as a way of getting a general sense of people's feelings on some issue, and should not normally be taken as formal votes. A formal vote only occurs if explicitly declared, and if this does occur then the vote should be held open for long enough to give all interested Council Members a chance to respond -- at least one week.

In practice, we anticipate that for most Steering Council decisions (e.g., voting in new members) a more informal process will suffice.

#### Council membership

A list of current Steering Council Members is maintained at the page [About Us](https://numpy.org/about/).

To become eligible to join the Steering Council, an individual must be a Project Contributor who has produced contributions that are substantial in quality and quantity, and sustained over at least one year. Potential Council Members are nominated by existing Council members, and become members following consensus of the existing Council members, and confirmation that the potential Member is interested and willing to serve in that capacity. The Council will be initially formed from the set of existing Core Developers who, as of late 2015, have been significantly active over the last year.

When considering potential Members, the Council will look at candidates with a comprehensive view of their contributions. This will include but is not limited to code, code review, infrastructure work, mailing list and chat participation, community help/building, education and outreach, design work, etc. We are deliberately not setting arbitrary quantitative metrics (like â€œ100 commits in this repoâ€) to avoid encouraging behavior that plays to the metrics rather than the projectâ€™s overall well-being. We want to encourage a diverse array of backgrounds, viewpoints and talents in our team, which is why we explicitly do not define code as the sole metric on which council membership will be evaluated.

If a Council member becomes inactive in the project for a period of one year, they will be considered for removal from the Council. Before removal, inactive Member will be approached to see if they plan on returning to active participation. If not they will be removed immediately upon a Council vote. If they plan on returning to active participation soon, they will be given a grace period of one year. If they donâ€™t return to active participation within that time period they will be removed by vote of the Council without further grace period. All former Council members can be considered for membership again at any time in the future, like any other Project Contributor. Retired Council members will be listed on the project website, acknowledging the period during which they were active in the Council.

The Council reserves the right to eject current Members, if they are deemed to be actively harmful to the projectâ€™s well-being, and attempts at communication and conflict resolution have failed. This requires the consensus of the remaining Members.

#### Conflict of interest

It is expected that the Council Members will be employed at a wide range of companies, universities and non-profit organizations. Because of this, it is possible that Members will have conflict of interests. Such conflict of interests include, but are not limited to:

  - Financial interests, such as investments, employment or contracting work, outside of The Project that may influence their work on The Project.
  - Access to proprietary information of their employer that could potentially leak into their work with the Project.

All members of the Council shall disclose to the rest of the Council any conflict of interest they may have. Members with a conflict of interest in a particular issue may participate in Council discussions on that issue, but must recuse themselves from voting on the issue.

#### Private communications of the Council

To the maximum extent possible, Council discussions and activities will be public and done in collaboration and discussion with the Project Contributors and Community. The Council will have a private mailing list that will be used sparingly and only when a specific matter requires privacy. When private communications and decisions are needed, the Council will do its best to summarize those to the Community after eliding personal/private/sensitive information that should not be posted to the public internet.

#### Subcommittees

The Council can create subcommittees that provide leadership and guidance for specific aspects of the project. Like the Council as a whole, subcommittees should conduct their business in an open and public manner unless privacy is specifically called for. Private subcommittee communications should happen on the main private mailing list of the Council unless specifically called for.

#### NumFOCUS Subcommittee

The Council will maintain one narrowly focused subcommittee to manage its interactions with NumFOCUS.

  - The NumFOCUS Subcommittee is comprised of 5 persons who manage project funding that comes through NumFOCUS. It is expected that these funds will be spent in a manner that is consistent with the non-profit mission of NumFOCUS and the direction of the Project as determined by the full Council.
  - This Subcommittee shall NOT make decisions about the direction, scope or technical direction of the Project.
  - This Subcommittee will have 5 members, 4 of whom will be current Council Members and 1 of whom will be external to the Steering Council. No more than 2 Subcommittee Members can report to one person through employment or contracting work (including the reportee, i.e. the reportee + 1 is the max). This avoids effective majorities resting on one person.

The current membership of the NumFOCUS Subcommittee is listed at the page [About Us](https://numpy.org/about/).

## Institutional partners and funding

The Steering Council are the primary leadership for the project. No outside institution, individual or legal entity has the ability to own, control, usurp or influence the project other than by participating in the Project as Contributors and Council Members. However, because institutions can be an important funding mechanism for the project, it is important to formally acknowledge institutional participation in the project. These are Institutional Partners.

An Institutional Contributor is any individual Project Contributor who contributes to the project as part of their official duties at an Institutional Partner. Likewise, an Institutional Council Member is any Project Steering Council Member who contributes to the project as part of their official duties at an Institutional Partner.

With these definitions, an Institutional Partner is any recognized legal entity in the United States or elsewhere that employs at least 1 Institutional Contributor of Institutional Council Member. Institutional Partners can be for-profit or non-profit entities.

Institutions become eligible to become an Institutional Partner by employing individuals who actively contribute to The Project as part of their official duties. To state this another way, the only way for a Partner to influence the project is by actively contributing to the open development of the project, in equal terms to any other member of the community of Contributors and Council Members. Merely using Project Software in institutional context does not allow an entity to become an Institutional Partner. Financial gifts do not enable an entity to become an Institutional Partner. Once an institution becomes eligible for Institutional Partnership, the Steering Council must nominate and approve the Partnership.

If at some point an existing Institutional Partner stops having any contributing employees, then a one year grace period commences. If at the end of this one year period they continue not to have any contributing employees, then their Institutional Partnership will lapse, and resuming it will require going through the normal process for new Partnerships.

An Institutional Partner is free to pursue funding for their work on The Project through any legal means. This could involve a non-profit organization raising money from private foundations and donors or a for-profit company building proprietary products and services that leverage Project Software and Services. Funding acquired by Institutional Partners to work on The Project is called Institutional Funding. However, no funding obtained by an Institutional Partner can override the Steering Council. If a Partner has funding to do NumPy work and the Council decides to not pursue that work as a project, the Partner is free to pursue it on their own. However in this situation, that part of the Partnerâ€™s work will not be under the NumPy umbrella and cannot use the Project trademarks in a way that suggests a formal relationship.

Institutional Partner benefits are:

  - Acknowledgement on the NumPy websites, in talks and T-shirts.
  - Ability to acknowledge their own funding sources on the NumPy websites, in talks and T-shirts.
  - Ability to influence the project through the participation of their Council Member.
  - Council Members invited to NumPy Developer Meetings.

A list of current Institutional Partners is maintained at the page [About Us](https://numpy.org/about/).

## Document history

<https://github.com/numpy/numpy/commits/main/doc/source/dev/governance/governance.rst>

## Acknowledgements

Substantial portions of this document were adapted from the [Jupyter/IPython project's governance document](https://github.com/jupyter/governance)

## License

To the extent possible under law, the authors have waived all copyright and related or neighboring rights to the NumPy project governance and decision-making document, as per the [CC-0 public domain dedication / license](https://creativecommons.org/publicdomain/zero/1.0/).

---

index.md

---

# NumPy governance

<div class="toctree" data-maxdepth="3">

governance

</div>

---

howto-docs.md

---

# How to contribute to the NumPy documentation

This guide will help you decide what to contribute and how to submit it to the official NumPy documentation.

## Documentation team meetings

The NumPy community has set a firm goal of improving its documentation. We hold regular documentation meetings on Zoom (dates are announced on the [numpy-discussion mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion)), and everyone is welcome. Reach out if you have questions or need someone to guide you through your first steps -- we're happy to help. Minutes are taken [on hackmd.io](https://hackmd.io/oB_boakvRqKR-_2jRV-Qjg) and stored in the [NumPy Archive repository](https://github.com/numpy/archive).

## What's needed

The \[NumPy Documentation \<numpy\_docs\_mainpage\>\](\#numpy-documentation-\<numpy\_docs\_mainpage\>) has the details covered. API reference documentation is generated directly from [docstrings](https://www.python.org/dev/peps/pep-0257/) in the code when the documentation is \[built\<howto-build-docs\>\](\#built\<howto-build-docs\>). Although we have mostly complete reference documentation for each function and class exposed to users, there is a lack of usage examples for some of them.

What we lack are docs with broader scope -- tutorials, how-tos, and explanations. Reporting defects is another way to contribute. We discuss both.

## Contributing fixes

We're eager to hear about and fix doc defects. But to attack the biggest problems we end up having to defer or overlook some bug reports. Here are the best defects to go after.

Top priority goes to **technical inaccuracies** -- a docstring missing a parameter, a faulty description of a function/parameter/method, and so on. Other "structural" defects like broken links also get priority. All these fixes are easy to confirm and put in place. You can submit a [pull request (PR)](https://numpy.org/devdocs/dev/index.html#devindex) with the fix, if you know how to do that; otherwise please [open an issue](https://github.com/numpy/numpy/issues).

**Typos and misspellings** fall on a lower rung; we welcome hearing about them but may not be able to fix them promptly. These too can be handled as pull requests or issues.

Obvious **wording** mistakes (like leaving out a "not") fall into the typo category, but other rewordings -- even for grammar -- require a judgment call, which raises the bar. Test the waters by first presenting the fix as an issue.

Some functions/objects like numpy.ndarray.transpose, numpy.array etc. defined in C-extension modules have their docstrings defined separately in [\_add\_newdocs.py](https://github.com/numpy/numpy/blob/main/numpy/_core/_add_newdocs.py)

## Contributing new pages

Your frustrations using our documents are our best guide to what needs fixing.

If you write a missing doc you join the front line of open source, but it's a meaningful contribution just to let us know what's missing. If you want to compose a doc, run your thoughts by the [mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion) for further ideas and feedback. If you want to alert us to a gap, [open an issue](https://github.com/numpy/numpy/issues). See [this issue](https://github.com/numpy/numpy/issues/15760) for an example.

If you're looking for subjects, our formal roadmap for documentation is a *NumPy Enhancement Proposal (NEP)*, \[NEP44\](\#nep44). It identifies areas where our docs need help and lists several additions we'd like to see, including \[Jupyter notebooks \<numpy\_tutorials\>\](\#jupyter-notebooks-\<numpy\_tutorials\>).

### Documentation framework

There are formulas for writing useful documents, and four formulas cover nearly everything. There are four formulas because there are four categories of document -- `tutorial`, `how-to guide`, `explanation`, and `reference`. The insight that docs divide up this way belongs to Daniele Procida and his [DiÃ¡taxis Framework](https://diataxis.fr/). When you begin a document or propose one, have in mind which of these types it will be.

### NumPy tutorials

In addition to the documentation that is part of the NumPy source tree, you can submit content in Jupyter Notebook format to the [NumPy Tutorials](https://numpy.org/numpy-tutorials) page. This set of tutorials and educational materials is meant to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with. These resources are developed in a separate GitHub repository, [numpy-tutorials](https://github.com/numpy/numpy-tutorials), where you can check out existing notebooks, open issues to suggest new topics or submit your own tutorials as pull requests.

### More on contributing

Don't worry if English is not your first language, or if you can only come up with a rough draft. Open source is a community effort. Do your best -- we'll help fix issues.

Images and real-life data make text more engaging and powerful, but be sure what you use is appropriately licensed and available. Here again, even a rough idea for artwork can be polished by others.

For now, the only data formats accepted by NumPy are those also used by other Python scientific libraries like pandas, SciPy, or Matplotlib. We're developing a package to accept more formats; contact us for details.

NumPy documentation is kept in the source code tree. To get your document into the docbase you must download the tree, \[build it \<howto-build-docs\>\](\#build-it \<howto-build-docs\>), and submit a pull request. If GitHub and pull requests are new to you, check our \[Contributor Guide \<devindex\>\](\#contributor-guide-\<devindex\>).

Our markup language is reStructuredText (rST), which is more elaborate than Markdown. Sphinx, the tool many Python projects use to build and link project documentation, converts the rST into HTML and other formats. For more on rST, see the [Quick reStructuredText Guide](https://docutils.sourceforge.io/docs/user/rst/quickref.html) or the [reStructuredText Primer](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)

## Contributing indirectly

If you run across outside material that would be a useful addition to the NumPy docs, let us know by [opening an issue](https://github.com/numpy/numpy/issues).

You don't have to contribute here to contribute to NumPy. You've contributed if you write a tutorial on your blog, create a YouTube video, or answer questions on Stack Overflow and other sites.

## Documentation style

### User documentation

  - In general, we follow the [Google developer documentation style guide](https://developers.google.com/style) for the User Guide.

  - NumPy style governs cases where:
    
      - Google has no guidance, or
      - We prefer not to use the Google style
    
    Our current rules:
    
      - We pluralize *index* as *indices* rather than [indexes](https://developers.google.com/style/word-list#letter-i), following the precedent of <span class="title-ref">numpy.indices</span>.
      - For consistency we also pluralize *matrix* as *matrices*.

  - Grammatical issues inadequately addressed by the NumPy or Google rules are decided by the section on "Grammar and Usage" in the most recent edition of the [Chicago Manual of Style](https://en.wikipedia.org/wiki/The_Chicago_Manual_of_Style).

  - We welcome being [alerted](https://github.com/numpy/numpy/issues) to cases we should add to the NumPy style rules.

### Docstrings

When using [Sphinx](https://www.sphinx-doc.org/) in combination with the NumPy conventions, you should use the `numpydoc` extension so that your docstrings will be handled correctly. For example, Sphinx will extract the `Parameters` section from your docstring and convert it into a field list. Using `numpydoc` will also avoid the reStructuredText errors produced by plain Sphinx when it encounters NumPy docstring conventions like section headers (e.g. `-------------`) that sphinx does not expect to find in docstrings.

It is available from:

  - [numpydoc on PyPI](https://pypi.python.org/pypi/numpydoc)
  - [numpydoc on GitHub](https://github.com/numpy/numpydoc/)

Note that for documentation within NumPy, it is not necessary to do `import numpy as np` at the beginning of an example.

Please use the `numpydoc` \[formatting standard \<numpydoc:format\>\](\#formatting-standard-\<numpydoc:format\>) as shown in their \[example \<numpydoc:example\>\](\#example-\<numpydoc:example\>).

### Documenting C/C++ code

NumPy uses [Doxygen](https://www.doxygen.nl/index.html) to parse specially-formatted C/C++ comment blocks. This generates XML files, which are converted by [Breathe](https://breathe.readthedocs.io/en/latest/) into RST, which is used by Sphinx.

**It takes three steps to complete the documentation process**:

#### 1\. Writing the comment blocks

Although there is still no commenting style set to follow, the Javadoc is more preferable than the others due to the similarities with the current existing non-indexed comment blocks.

<div class="note">

<div class="title">

Note

</div>

Please see ["Documenting the code"](https://www.doxygen.nl/manual/docblocks.html).

</div>

**This is what Javadoc style looks like**:

<div class="literalinclude">

examples/doxy\_func.h

</div>

**And here is how it is rendered**:

<div class="doxygenfunction">

doxy\_javadoc\_example

</div>

**For line comment, you can use a triple forward slash. For example**:

<div class="literalinclude">

examples/doxy\_class.hpp

</div>

**And here is how it is rendered**:

<div class="doxygenclass">

DoxyLimbo

</div>

##### Common Doxygen Tags:

<div class="note">

<div class="title">

Note

</div>

For more tags/commands, please take a look at <https://www.doxygen.nl/manual/commands.html>

</div>

`@brief`

Starts a paragraph that serves as a brief description. By default the first sentence of the documentation block is automatically treated as a brief description, since option [JAVADOC\_AUTOBRIEF](https://www.doxygen.nl/manual/config.html#cfg_javadoc_autobrief) is enabled within doxygen configurations.

`@details`

Just like `@brief` starts a brief description, `@details` starts the detailed description. You can also start a new paragraph (blank line) then the `@details` command is not needed.

`@param`

Starts a parameter description for a function parameter with name \<parameter-name\>, followed by a description of the parameter. The existence of the parameter is checked and a warning is given if the documentation of this (or any other) parameter is missing or not present in the function declaration or definition.

`@return`

Starts a return value description for a function. Multiple adjacent `@return` commands will be joined into a single paragraph. The `@return` description ends when a blank line or some other sectioning command is encountered.

`@code/@endcode`

Starts/Ends a block of code. A code block is treated differently from ordinary text. It is interpreted as source code.

`@rst/@endrst`

Starts/Ends a block of reST markup.

##### Example

**Take a look at the following example**:

<div class="literalinclude">

examples/doxy\_rst.h

</div>

**And here is how it is rendered**:

<div class="doxygenfunction">

doxy\_reST\_example

</div>

#### 2\. Feeding Doxygen

Not all headers files are collected automatically. You have to add the desired C/C++ header paths within the sub-config files of Doxygen.

Sub-config files have the unique name `.doxyfile`, which you can usually find near directories that contain documented headers. You need to create a new config file if there's not one located in a path close(2-depth) to the headers you want to add.

Sub-config files can accept any of [Doxygen](https://www.doxygen.nl/index.html) [configuration options](https://www.doxygen.nl/manual/config.html), but do not override or re-initialize any configuration option, rather only use the concatenation operator "+=". For example:

    # to specify certain headers
    INPUT += @CUR_DIR/header1.h \
             @CUR_DIR/header2.h
    # to add all headers in certain path
    INPUT += @CUR_DIR/to/headers
    # to define certain macros
    PREDEFINED += C_MACRO(X)=X
    # to enable certain branches
    PREDEFINED += NPY_HAVE_FEATURE \
                  NPY_HAVE_FEATURE2

<div class="note">

<div class="title">

Note

</div>

@CUR\_DIR is a template constant returns the current dir path of the sub-config file.

</div>

#### 3\. Inclusion directives

[Breathe](https://breathe.readthedocs.io/en/latest/) provides a wide range of custom directives to allow converting the documents generated by [Doxygen](https://www.doxygen.nl/index.html) into reST files.

<div class="note">

<div class="title">

Note

</div>

For more information, please check out "[Directives & Config Variables](https://breathe.readthedocs.io/en/latest/directives.html)"

</div>

##### Common directives:

`doxygenfunction`

This directive generates the appropriate output for a single function. The function name is required to be unique in the project.

    .. doxygenfunction:: <function name>
        :outline:
        :no-link:

Checkout the [example](https://breathe.readthedocs.io/en/latest/function.html#function-example) to see it in action.

`doxygenclass`

This directive generates the appropriate output for a single class. It takes the standard project, path, outline and no-link options and additionally the members, protected-members, private-members, undoc-members, membergroups and members-only options:

    .. doxygenclass:: <class name>
       :members: [...]
       :protected-members:
       :private-members:
       :undoc-members:
       :membergroups: ...
       :members-only:
       :outline:
       :no-link:

Checkout the [doxygenclass documentation](https://breathe.readthedocs.io/en/latest/class.html#class-example) for more details and to see it in action.

`doxygennamespace`

This directive generates the appropriate output for the contents of a namespace. It takes the standard project, path, outline and no-link options and additionally the content-only, members, protected-members, private-members and undoc-members options. To reference a nested namespace, the full namespaced path must be provided, e.g. foo::bar for the bar namespace inside the foo namespace.

    .. doxygennamespace:: <namespace>
       :content-only:
       :outline:
       :members:
       :protected-members:
       :private-members:
       :undoc-members:
       :no-link:

Checkout the [doxygennamespace documentation](https://breathe.readthedocs.io/en/latest/namespace.html#namespace-example) for more details and to see it in action.

`doxygengroup`

This directive generates the appropriate output for the contents of a doxygen group. A doxygen group can be declared with specific doxygen markup in the source comments as covered in the doxygen [grouping documentation](https://www.doxygen.nl/manual/grouping.html).

It takes the standard project, path, outline and no-link options and additionally the content-only, members, protected-members, private-members and undoc-members options.

    .. doxygengroup:: <group name>
       :content-only:
       :outline:
       :members:
       :protected-members:
       :private-members:
       :undoc-members:
       :no-link:
       :inner:

Checkout the [doxygengroup documentation](https://breathe.readthedocs.io/en/latest/group.html#group-example) for more details and to see it in action.

### Legacy directive

If a function, module or API is in *legacy* mode, meaning that it is kept around for backwards compatibility reasons, but is not recommended to use in new code, you can use the `.. legacy::` directive.

By default, if used with no arguments, the legacy directive will generate the following output:

<div class="legacy">

</div>

We strongly recommend that you also add a custom message, such as a new API to replace the old one:

    .. legacy::
    
       For more details, see [distutils-status-migration](#distutils-status-migration).

This message will be appended to the default message and will create the following output:

<div class="legacy">

For more details, see \[distutils-status-migration\](\#distutils-status-migration).

</div>

Finally, if you want to mention a function, method (or any custom object) instead of a *submodule*, you can use an optional argument:

    .. legacy:: function

This will create the following output:

<div class="legacy">

function

</div>

## Documentation reading

  - The leading organization of technical writers, [Write the Docs](https://www.writethedocs.org/), holds conferences, hosts learning resources, and runs a Slack channel.
  - "Every engineer is also a writer," says Google's [collection of technical writing resources](https://developers.google.com/tech-writing), which includes free online courses for developers in planning and writing documents.
  - [Software Carpentry's](https://software-carpentry.org/lessons) mission is teaching software to researchers. In addition to hosting the curriculum, the website explains how to present ideas effectively.

---

howto_build_docs.md

---

# Building the NumPy API and reference docs

If you only want to get the documentation, note that pre-built versions can be found at

<https://numpy.org/doc/>

in several different formats.

## Development environments

Before proceeding further it should be noted that the documentation is built with the `make` tool, which is not natively available on Windows. MacOS or Linux users can jump to \[how-todoc.prerequisites\](\#how-todoc.prerequisites). It is recommended for Windows users to set up their development environment on GitHub Codespaces (see \[recommended-development-setup\](\#recommended-development-setup)) or [Windows Subsystem for Linux (WSL)](https://learn.microsoft.com/en-us/windows/wsl/install). WSL is a good option for a persistent local set-up.

## Prerequisites

Building the NumPy documentation and API reference requires the following:

### NumPy

Since large parts of the main documentation are obtained from NumPy via `import numpy` and examining the docstrings, you will need to first \[build \<development-environment\>\](\#build-\<development-environment\>) and install it so that the correct version is imported. NumPy has to be re-built and re-installed every time you fetch the latest version of the repository, before generating the documentation. This ensures that the NumPy version and the git repository version are in sync.

Note that you can e.g. install NumPy to a temporary location and set the PYTHONPATH environment variable appropriately. Alternatively, if using Python virtual environments (via e.g. `conda`, `virtualenv` or the `venv` module), installing NumPy into a new virtual environment is recommended.

### Dependencies

All of the necessary dependencies for building the NumPy docs except for [Doxygen](https://www.doxygen.nl/index.html) can be installed with:

    pip install -r requirements/doc_requirements.txt

\> **Note** \> It may be necessary to install development versions of the doc dependencies to build the docs locally:

    pip install --pre --force-reinstall --extra-index-url \
    https://pypi.anaconda.org/scientific-python-nightly-wheels/simple \
    -r requirements/doc_requirements.txt

We currently use [Sphinx](https://www.sphinx-doc.org/) along with [Doxygen](https://www.doxygen.nl/index.html) for generating the API and reference documentation for NumPy. In addition, building the documentation requires the Sphinx extension <span class="title-ref">plot\_directive</span>, which is shipped with \[Matplotlib \<matplotlib:index\>\](Matplotlib \<matplotlib:index\>.md). We also use [numpydoc](https://numpydoc.readthedocs.io/en/latest/index.html) to render docstrings in the generated API documentation. \[SciPy \<scipy:index\>\](SciPy \<scipy:index\>.md) is installed since some parts of the documentation require SciPy functions.

For installing [Doxygen](https://www.doxygen.nl/index.html), please check the official [download](https://www.doxygen.nl/download.html#srcbin) and [installation](https://www.doxygen.nl/manual/install.html) pages, or if you are using Linux then you can install it through your distribution package manager.

\> **Note** \> Try to install a newer version of [Doxygen](https://www.doxygen.nl/index.html) \> 1.8.10 otherwise you may get some warnings during the build.

### Submodules

If you obtained NumPy via git, also get the git submodules that contain additional parts required for building the documentation:

    git submodule update --init

## Instructions

Now you are ready to generate the docs, so write:

    spin docs

This will build NumPy from source if you haven't already, and run Sphinx to build the `html` docs. If all goes well, this will generate a `build/html` subdirectory in the `/doc` directory, containing the built documentation.

The documentation for NumPy distributed at <https://numpy.org/doc> in html and pdf format is also built with `make dist`. See [HOWTO RELEASE](https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.md) for details on how to update <https://numpy.org/doc>.

---

index.md

---

# Contributing to NumPy

Not a coder? Not a problem\! NumPy is multi-faceted, and we can use a lot of help. These are all activities we'd like to get help with (they're all important, so we list them in alphabetical order):

  - Code maintenance and development
  - Community coordination
  - DevOps
  - Developing educational content & narrative documentation
  - Fundraising
  - Marketing
  - Project management
  - Translating content
  - Website design and development
  - Writing technical documentation

We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so it's hard to make assumptions about an ideal "first-time-contributor". So, that's why we don't mark issues with the "good-first-issue" label. Instead, you'll find [issues labeled "Sprintable"](https://github.com/numpy/numpy/labels/sprintable). These issues can either be:

  - **Easily fixed** when you have guidance from an experienced contributor (perfect for working in a sprint).
  - **A learning opportunity** for those ready to dive deeper, even if you're not in a sprint.

Additionally, depending on your prior experience, some "Sprintable" issues might be easy, while others could be more challenging for you.

The rest of this document discusses working on the NumPy code base and documentation. We're in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us\! You can do this via the [numpy-discussion mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion), or on [GitHub](https://github.com/numpy/numpy) (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature\!), however if you prefer to discuss in a more private space first, you can do so on Slack (see [numpy.org/contribute](https://numpy.org/contribute/) for details).

## Development process - summary

Here's the short summary, complete TOC links are below:

1.  If you are a first-time contributor:
      - Go to <https://github.com/numpy/numpy> and click the "fork" button to create your own copy of the project.
    
      - Clone the project to your local computer:
        
            git clone --recurse-submodules https://github.com/your-username/numpy.git
    
      - Change the directory:
        
            cd numpy
    
      - Add the upstream repository:
        
            git remote add upstream https://github.com/numpy/numpy.git
    
      - Now, `git remote -v` will show two remote repositories named:
        
          - `upstream`, which refers to the `numpy` repository
          - `origin`, which refers to your personal fork
    
      - Pull the latest changes from upstream, including tags:
        
            git checkout main
            git pull upstream main --tags
    
      - Initialize numpy's submodules:
        
            git submodule update --init
2.  Develop your contribution:
      - Create a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as 'linspace-speedups':
        
            git checkout -b linspace-speedups
    
      - Commit locally as you progress (`git add` and `git commit`) Use a \[properly formatted\<writing-the-commit-message\>\](\#properly-formatted\<writing-the-commit-message\>) commit message, write tests that fail before your change and pass afterward, run all the \[tests locally\<development-environment\>\](\#tests-locally\<development-environment\>). Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring \[standard\<howto-document\>\](\#standard\<howto-document\>).
3.  To submit your contribution:
      - Push your changes back to your fork on GitHub:
        
            git push origin linspace-speedups
    
      - Go to GitHub. The new branch will show up with a green Pull Request button. Make sure the title and message are clear, concise, and self-explanatory. Then click the button to submit it.
    
      - If your commit introduces a new feature or changes functionality, post on the [mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your changes. For bug fixes, documentation updates, etc., this is generally not necessary, though if you do not get any reaction, do feel free to ask for review.

4\. Review process:

>   - Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style. Every single developer working on the project has their code reviewed, and we've come to see it as friendly conversation from which we all learn and the overall code quality benefits. Therefore, please don't let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time you're donating\!). See our \[Reviewer Guidelines \<reviewer-guidelines\>\](\#reviewer-guidelines

\-----\<reviewer-guidelines\>) for more information.

>   - To update your PR, make your changes on your local repository, commit, **run tests, and only if they succeed** push to your fork. As soon as those changes are pushed up (to the same branch as before) the PR will update automatically. If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment.
>   - Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch. The CI tests must pass before your PR can be merged. If CI fails, you can find out why by clicking on the "failed" icon (red cross) and inspecting the build and test log. To avoid overuse and waste of this resource, \[test your work\<recommended-development-setup\>\](\#test-your-work\<recommended-development-setup\>) locally before committing.
>   - A PR must be **approved** by at least one core team member before merging. Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging.

5.  Document changes
    
    Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in `doc/release/upcoming_changes`. The file `doc/release/upcoming_changes/README.rst` details the format and filename conventions.
    
    If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow \[NEP 23 deprecation policy \<NEP23\>\](\#nep-23-deprecation-policy-\<nep23\>) to add the deprecation.

6.  Cross referencing issues
    
    If the PR relates to any issues, you can add the text `xref gh-xxxx` where `xxxx` is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the `xref` with `closes`, `fixes` or any of the other flavors [github accepts \<https://help.github.com/en/articles/ closing-issues-using-keywords\>]().
    
    In the source code, be sure to preface any issue or PR reference with `gh-xxxx`.

For a more detailed discussion, read on and follow the links at the bottom of this page.

### Guidelines

  - All code should have tests (see [test coverage](#test-coverage) below for more details).
  - All code should be [documented \<https://numpydoc.readthedocs.io/ en/latest/format.html\#docstring-standard\>]().
  - No changes are ever committed without review and approval by a core team member. Please ask politely on the PR or on the [mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion) if you get no response to your pull request within a week.

### Stylistic guidelines

  - Set up your editor to follow [PEP 8 \<https://www.python.org/dev/peps/ pep-0008/\>]() (remove trailing white space, no tabs, etc.). Check code with pyflakes / flake8.

  - Use NumPy data types instead of strings (`np.uint8` instead of `"uint8"`).

  - Use the following import conventions:
    
        import numpy as np

  - For C code, see \[NEP 45 \<NEP45\>\](\#nep-45-\<nep45\>).

### Test coverage

Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should \[run the tests \<development-environment\>\](\#run-the-tests \<development-environment\>) before pushing a PR.

Running NumPy's test suite locally requires some additional packages, such as `pytest` and `hypothesis`. The additional testing dependencies are listed in `requirements/test_requirements.txt` in the top-level directory, and can conveniently be installed with:

    $ python -m pip install -r requirements/test_requirements.txt

Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100%.

To measure the test coverage, run:

    $ spin test --coverage

This will create a report in `html` format at `build/coverage`, which can be viewed with your browser, e.g.:

    $ firefox build/coverage/index.html

### Building docs

To build the HTML documentation, use:

    spin docs

You can also run `make` from the `doc` directory. `make help` lists all targets.

To get the appropriate dependencies and other requirements, see \[howto-build-docs\](\#howto-build-docs).

## Development process - details

The rest of the story

<div class="toctree" data-maxdepth="2">

development\_environment howto\_build\_docs development\_workflow development\_advanced\_debugging reviewer\_guidelines ../benchmarking NumPy C style guide \<<https://numpy.org/neps/nep-0045-c_style_guide.html>\> depending\_on\_numpy releasing governance/index howto-docs

</div>

NumPy-specific workflow is in \[numpy-development-workflow \<development-workflow\>\](\#numpy-development-workflow \<development-workflow\>).

---

internals.code-explanations.md

---

<div class="currentmodule">

numpy

</div>

# NumPy C code explanations

> Fanaticism consists of redoubling your efforts when you have forgotten your aim. --- *George Santayana*
> 
> An authority is a person who can tell you more about something than you really care to know. --- *Unknown*

This page attempts to explain the logic behind some of the new pieces of code. The purpose behind these explanations is to enable somebody to be able to understand the ideas behind the implementation somewhat more easily than just staring at the code. Perhaps in this way, the algorithms can be improved on, borrowed from, and/or optimized by more people.

## Memory model

<div class="index">

pair: ndarray; memory model

</div>

One fundamental aspect of the <span class="title-ref">ndarray</span> is that an array is seen as a "chunk" of memory starting at some location. The interpretation of this memory depends on the `stride` information. For each dimension in an \(N\)-dimensional array, an integer (`stride`) dictates how many bytes must be skipped to get to the next element in that dimension. Unless you have a single-segment array, this `stride` information must be consulted when traversing through an array. It is not difficult to write code that accepts strides, you just have to use `char*` pointers because strides are in units of bytes. Keep in mind also that strides do not have to be unit-multiples of the element size. Also, remember that if the number of dimensions of the array is 0 (sometimes called a `rank-0` array), then the `strides <stride>` and `dimensions <dimension>` variables are `NULL`.

Besides the structural information contained in the strides and dimensions members of the :c`PyArrayObject`, the flags contain important information about how the data may be accessed. In particular, the :c\`NPY\_ARRAY\_ALIGNED\` flag is set when the memory is on a suitable boundary according to the datatype array. Even if you have a `contiguous` chunk of memory, you cannot just assume it is safe to dereference a datatype-specific pointer to an element. Only if the :c\`NPY\_ARRAY\_ALIGNED\` flag is set, this is a safe operation. On some platforms it will work but on others, like Solaris, it will cause a bus error. The :c\`NPY\_ARRAY\_WRITEABLE\` should also be ensured if you plan on writing to the memory area of the array. It is also possible to obtain a pointer to an unwritable memory area. Sometimes, writing to the memory area when the :c\`NPY\_ARRAY\_WRITEABLE\` flag is not set will just be rude. Other times it can cause program crashes (*e.g.* a data-area that is a read-only memory-mapped file).

## Data-type encapsulation

<div class="seealso">

\[arrays.dtypes\](\#arrays.dtypes)

</div>

<div class="index">

single: dtype

</div>

The \[datatype \<arrays.dtypes\>\](\#datatype-\<arrays.dtypes\>) is an important abstraction of the <span class="title-ref">ndarray</span>. Operations will look to the datatype to provide the key functionality that is needed to operate on the array. This functionality is provided in the list of function pointers pointed to by the `f` member of the :c`PyArray_Descr` structure. In this way, the number of datatypes can be extended simply by providing a :c`PyArray_Descr` structure with suitable function pointers in the `f` member. For built-in types, there are some optimizations that bypass this mechanism, but the point of the datatype abstraction is to allow new datatypes to be added.

One of the built-in datatypes, the <span class="title-ref">void</span> datatype allows for arbitrary `structured types <structured data type>` containing 1 or more fields as elements of the array. A `field` is simply another datatype object along with an offset into the current structured type. In order to support arbitrarily nested fields, several recursive implementations of datatype access are implemented for the void type. A common idiom is to cycle through the elements of the dictionary and perform a specific operation based on the datatype object stored at the given offset. These offsets can be arbitrary numbers. Therefore, the possibility of encountering misaligned data must be recognized and taken into account if necessary.

## N-D iterators

<div class="seealso">

\[arrays.nditer\](\#arrays.nditer)

</div>

<div class="index">

single: array iterator

</div>

A very common operation in much of NumPy code is the need to iterate over all the elements of a general, strided, N-dimensional array. This operation of a general-purpose N-dimensional loop is abstracted in the notion of an iterator object. To write an N-dimensional loop, you only have to create an iterator object from an ndarray, work with the :c`dataptr <PyArrayIterObject.dataptr>` member of the iterator object structure and call the macro :c\`PyArray\_ITER\_NEXT\` on the iterator object to move to the next element. The `next` element is always in C-contiguous order. The macro works by first special-casing the C-contiguous, 1-D, and 2-D cases which work very simply.

For the general case, the iteration works by keeping track of a list of coordinate counters in the iterator object. At each iteration, the last coordinate counter is increased (starting from 0). If this counter is smaller than one less than the size of the array in that dimension (a pre-computed and stored value), then the counter is increased and the :c`dataptr <PyArrayIterObject.dataptr>` member is increased by the strides in that dimension and the macro ends. If the end of a dimension is reached, the counter for the last dimension is reset to zero and the :c`dataptr <PyArrayIterObject.dataptr>` is moved back to the beginning of that dimension by subtracting the strides value times one less than the number of elements in that dimension (this is also pre-computed and stored in the :c`backstrides <PyArrayIterObject.backstrides>` member of the iterator object). In this case, the macro does not end, but a local dimension counter is decremented so that the next-to-last dimension replaces the role that the last dimension played and the previously-described tests are executed again on the next-to-last dimension. In this way, the :c`dataptr <PyArrayIterObject.dataptr>` is adjusted appropriately for arbitrary striding.

The :c`coordinates <PyArrayIterObject.coordinates>` member of the :c`PyArrayIterObject` structure maintains the current N-d counter unless the underlying array is C-contiguous in which case the coordinate counting is bypassed. The :c`index <PyArrayIterObject.index>` member of the :c`PyArrayIterObject` keeps track of the current flat index of the iterator. It is updated by the :c\`PyArray\_ITER\_NEXT\` macro.

## Broadcasting

<div class="seealso">

\[basics.broadcasting\](\#basics.broadcasting)

</div>

<div class="index">

single: broadcasting

</div>

In Numeric, the ancestor of NumPy, broadcasting was implemented in several lines of code buried deep in `ufuncobject.c`. In NumPy, the notion of broadcasting has been abstracted so that it can be performed in multiple places. Broadcasting is handled by the function :c\`PyArray\_Broadcast\`. This function requires a :c`PyArrayMultiIterObject` (or something that is a binary equivalent) to be passed in. The :c`PyArrayMultiIterObject` keeps track of the broadcast number of dimensions and size in each dimension along with the total size of the broadcast result. It also keeps track of the number of arrays being broadcast and a pointer to an iterator for each of the arrays being broadcast.

The :c\`PyArray\_Broadcast\` function takes the iterators that have already been defined and uses them to determine the broadcast shape in each dimension (to create the iterators at the same time that broadcasting occurs then use the :c\`PyArray\_MultiIterNew\` function). Then, the iterators are adjusted so that each iterator thinks it is iterating over an array with the broadcast size. This is done by adjusting the iterators number of dimensions, and the `shape` in each dimension. This works because the iterator strides are also adjusted. Broadcasting only adjusts (or adds) length-1 dimensions. For these dimensions, the strides variable is simply set to 0 so that the data-pointer for the iterator over that array doesn't move as the broadcasting operation operates over the extended dimension.

Broadcasting was always implemented in Numeric using 0-valued strides for the extended dimensions. It is done in exactly the same way in NumPy. The big difference is that now the array of strides is kept track of in a :c`PyArrayIterObject`, the iterators involved in a broadcast result are kept track of in a :c`PyArrayMultiIterObject`, and the :c\`PyArray\_Broadcast\` call implements the \[general-broadcasting-rules\](\#general-broadcasting-rules).

## Array scalars

<div class="seealso">

\[arrays.scalars\](\#arrays.scalars)

</div>

<div class="index">

single: array scalars

</div>

The array scalars offer a hierarchy of Python types that allow a one-to-one correspondence between the datatype stored in an array and the Python-type that is returned when an element is extracted from the array. An exception to this rule was made with object arrays. Object arrays are heterogeneous collections of arbitrary Python objects. When you select an item from an object array, you get back the original Python object (and not an object array scalar which does exist but is rarely used for practical purposes).

The array scalars also offer the same methods and attributes as arrays with the intent that the same code can be used to support arbitrary dimensions (including 0-dimensions). The array scalars are read-only (immutable) with the exception of the void scalar which can also be written to so that structured array field setting works more naturally (`a[0]['f1'] = value`).

## Indexing

<div class="seealso">

\[basics.indexing\](\#basics.indexing), \[arrays.indexing\](\#arrays.indexing)

</div>

<div class="index">

single: indexing

</div>

All Python indexing operations `arr[index]` are organized by first preparing the index and finding the index type. The supported index types are:

  - integer
  - <span class="title-ref">newaxis</span>
  - `python:slice`
  - :py\`Ellipsis\`
  - integer arrays/array-likes (advanced)
  - boolean (single boolean array); if there is more than one boolean array as the index or the shape does not match exactly, the boolean array will be converted to an integer array instead.
  - 0-d boolean (and also integer); 0-d boolean arrays are a special case that has to be handled in the advanced indexing code. They signal that a 0-d boolean array had to be interpreted as an integer array.

As well as the scalar array special case signaling that an integer array was interpreted as an integer index, which is important because an integer array index forces a copy but is ignored if a scalar is returned (full integer index). The prepared index is guaranteed to be valid with the exception of out of bound values and broadcasting errors for advanced indexing. This includes that an :py\`Ellipsis\` is added for incomplete indices for example when a two-dimensional array is indexed with a single integer.

The next step depends on the type of index which was found. If all dimensions are indexed with an integer a scalar is returned or set. A single boolean indexing array will call specialized boolean functions. Indices containing an :py\`Ellipsis\` or `python:slice` but no advanced indexing will always create a view into the old array by calculating the new strides and memory offset. This view can then either be returned or, for assignments, filled using `PyArray_CopyObject`. Note that `PyArray_CopyObject` may also be called on temporary arrays in other branches to support complicated assignments when the array is of object <span class="title-ref">dtype</span>.

### Advanced indexing

By far the most complex case is advanced indexing, which may or may not be combined with typical view-based indexing. Here integer indices are interpreted as view-based. Before trying to understand this, you may want to make yourself familiar with its subtleties. The advanced indexing code has three different branches and one special case:

  - There is one indexing array and it, as well as the assignment array, can be iterated trivially. For example, they may be contiguous. Also, the indexing array must be of <span class="title-ref">intp</span> type and the value array in assignments should be of the correct type. This is purely a fast path.
  - There are only integer array indices so that no subarray exists.
  - View-based and advanced indexing is mixed. In this case, the view-based indexing defines a collection of subarrays that are combined by the advanced indexing. For example, `arr[[1, 2, 3], :]` is created by vertically stacking the subarrays `arr[1, :]`, `arr[2, :]`, and `arr[3, :]`.
  - There is a subarray but it has exactly one element. This case can be handled as if there is no subarray but needs some care during setup.

Deciding what case applies, checking broadcasting, and determining the kind of transposition needed are all done in `PyArray_MapIterNew`. After setting up, there are two cases. If there is no subarray or it only has one element, no subarray iteration is necessary and an iterator is prepared which iterates all indexing arrays *as well as* the result or value array. If there is a subarray, there are three iterators prepared. One for the indexing arrays, one for the result or value array (minus its subarray), and one for the subarrays of the original and the result/assignment array. The first two iterators give (or allow calculation) of the pointers into the start of the subarray, which then allows restarting the subarray iteration.

When advanced indices are next to each other transposing may be necessary. All necessary transposing is handled by `PyArray_MapIterSwapAxes` and has to be handled by the caller unless `PyArray_MapIterNew` is asked to allocate the result.

After preparation, getting and setting are relatively straightforward, although the different modes of iteration need to be considered. Unless there is only a single indexing array during item getting, the validity of the indices is checked beforehand. Otherwise, it is handled in the inner loop itself for optimization.

## Universal functions

<div class="seealso">

\[ufuncs\](\#ufuncs), \[ufuncs-basics\](\#ufuncs-basics)

</div>

<div class="index">

single: ufunc

</div>

Universal functions are callable objects that take \(N\) inputs and produce \(M\) outputs by wrapping basic 1-D loops that work element-by-element into full easy-to-use functions that seamlessly implement \[broadcasting \<basics.broadcasting\>\](\#broadcasting-\<basics.broadcasting\>), \[type-checking \<ufuncs.casting\>\](\#type-checking-\<ufuncs.casting\>), \[buffered coercion \<use-of-internal-buffers\>\](\#buffered-coercion-\<use-of-internal-buffers\>), and \[output-argument handling \<ufuncs-output-type\>\](\#output-argument-handling-\<ufuncs-output-type\>). New universal functions are normally created in C, although there is a mechanism for creating ufuncs from Python functions (<span class="title-ref">frompyfunc</span>). The user must supply a 1-D loop that implements the basic function taking the input scalar values and placing the resulting scalars into the appropriate output slots as explained in implementation.

### Setup

Every <span class="title-ref">ufunc</span> calculation involves some overhead related to setting up the calculation. The practical significance of this overhead is that even though the actual calculation of the ufunc is very fast, you will be able to write array and type-specific code that will work faster for small arrays than the ufunc. In particular, using ufuncs to perform many calculations on 0-D arrays will be slower than other Python-based solutions (the silently-imported `scalarmath` module exists precisely to give array scalars the look-and-feel of ufunc based calculations with significantly reduced overhead).

When a <span class="title-ref">ufunc</span> is called, many things must be done. The information collected from these setup operations is stored in a loop object. This loop object is a C-structure (that could become a Python object but is not initialized as such because it is only used internally). This loop object has the layout needed to be used with :c\`PyArray\_Broadcast\` so that the broadcasting can be handled in the same way as it is handled in other sections of code.

The first thing done is to look up in the thread-specific global dictionary the current values for the buffer-size, the error mask, and the associated error object. The state of the error mask controls what happens when an error condition is found. It should be noted that checking of the hardware error flags is only performed after each 1-D loop is executed. This means that if the input and output arrays are contiguous and of the correct type so that a single 1-D loop is performed, then the flags may not be checked until all elements of the array have been calculated. Looking up these values in a thread-specific dictionary takes time which is easily ignored for all but very small arrays.

After checking, the thread-specific global variables, the inputs are evaluated to determine how the ufunc should proceed and the input and output arrays are constructed if necessary. Any inputs which are not arrays are converted to arrays (using context if necessary). Which of the inputs are scalars (and therefore converted to 0-D arrays) is noted.

Next, an appropriate 1-D loop is selected from the 1-D loops available to the <span class="title-ref">ufunc</span> based on the input array types. This 1-D loop is selected by trying to match the signature of the datatypes of the inputs against the available signatures. The signatures corresponding to built-in types are stored in the <span class="title-ref">ufunc.types</span> member of the ufunc structure. The signatures corresponding to user-defined types are stored in a linked list of function information with the head element stored as a `CObject` in the `userloops` dictionary keyed by the datatype number (the first user-defined type in the argument list is used as the key). The signatures are searched until a signature is found to which the input arrays can all be cast safely (ignoring any scalar arguments which are not allowed to determine the type of the result). The implication of this search procedure is that "lesser types" should be placed below "larger types" when the signatures are stored. If no 1-D loop is found, then an error is reported. Otherwise, the `argument_list` is updated with the stored signature --- in case casting is necessary and to fix the output types assumed by the 1-D loop.

If the ufunc has 2 inputs and 1 output and the second input is an `Object` array then a special-case check is performed so that `NotImplemented` is returned if the second input is not an ndarray, has the `~numpy.class.__array_priority__` attribute, and has an `__r{op}__` special method. In this way, Python is signaled to give the other object a chance to complete the operation instead of using generic object-array calculations. This allows (for example) sparse matrices to override the multiplication operator 1-D loop.

For input arrays that are smaller than the specified buffer size, copies are made of all non-contiguous, misaligned, or out-of-byteorder arrays to ensure that for small arrays, a single loop is used. Then, array iterators are created for all the input arrays and the resulting collection of iterators is broadcast to a single shape.

The output arguments (if any) are then processed and any missing return arrays are constructed. If any provided output array doesn't have the correct type (or is misaligned) and is smaller than the buffer size, then a new output array is constructed with the special :c\`NPY\_ARRAY\_WRITEBACKIFCOPY\` flag set. At the end of the function, :c\`PyArray\_ResolveWritebackIfCopy\` is called so that its contents will be copied back into the output array. Iterators for the output arguments are then processed.

Finally, the decision is made about how to execute the looping mechanism to ensure that all elements of the input arrays are combined to produce the output arrays of the correct type. The options for loop execution are one-loop (for :term\`contiguous\`, aligned, and correct data type), strided-loop (for non-contiguous but still aligned and correct data type), and a buffered loop (for misaligned or incorrect data type situations). Depending on which execution method is called for, the loop is then set up and computed.

### Function call

This section describes how the basic universal function computation loop is set up and executed for each of the three different kinds of execution. If :c\`NPY\_ALLOW\_THREADS\` is defined during compilation, then as long as no object arrays are involved, the Python Global Interpreter Lock (GIL) is released prior to calling the loops. It is re-acquired if necessary to handle error conditions. The hardware error flags are checked only after the 1-D loop is completed.

#### One loop

This is the simplest case of all. The ufunc is executed by calling the underlying 1-D loop exactly once. This is possible only when we have aligned data of the correct type (including byteorder) for both input and output and all arrays have uniform strides (either `contiguous`, 0-D, or 1-D). In this case, the 1-D computational loop is called once to compute the calculation for the entire array. Note that the hardware error flags are only checked after the entire calculation is complete.

#### Strided loop

When the input and output arrays are aligned and of the correct type, but the striding is not uniform (non-contiguous and 2-D or larger), then a second looping structure is employed for the calculation. This approach converts all of the iterators for the input and output arguments to iterate over all but the largest dimension. The inner loop is then handled by the underlying 1-D computational loop. The outer loop is a standard iterator loop on the converted iterators. The hardware error flags are checked after each 1-D loop is completed.

#### Buffered loop

This is the code that handles the situation whenever the input and/or output arrays are either misaligned or of the wrong datatype (including being byteswapped) from what the underlying 1-D loop expects. The arrays are also assumed to be non-contiguous. The code works very much like the strided-loop except for the inner 1-D loop is modified so that pre-processing is performed on the inputs and post-processing is performed on the outputs in `bufsize` chunks (where `bufsize` is a user-settable parameter). The underlying 1-D computational loop is called on data that is copied over (if it needs to be). The setup code and the loop code is considerably more complicated in this case because it has to handle:

  - memory allocation of the temporary buffers
  - deciding whether or not to use buffers on the input and output data (misaligned and/or wrong datatype)
  - copying and possibly casting data for any inputs or outputs for which buffers are necessary.
  - special-casing `Object` arrays so that reference counts are properly handled when copies and/or casts are necessary.
  - breaking up the inner 1-D loop into `bufsize` chunks (with a possible remainder).

Again, the hardware error flags are checked at the end of each 1-D loop.

### Final output manipulation

Ufuncs allow other array-like classes to be passed seamlessly through the interface in that inputs of a particular class will induce the outputs to be of that same class. The mechanism by which this works is the following. If any of the inputs are not ndarrays and define the `~numpy.class.__array_wrap__` method, then the class with the largest `~numpy.class.__array_priority__` attribute determines the type of all the outputs (with the exception of any output arrays passed in). The `~numpy.class.__array_wrap__` method of the input array will be called with the ndarray being returned from the ufunc as its input. There are two calling styles of the `~numpy.class.__array_wrap__` function supported. The first takes the ndarray as the first argument and a tuple of "context" as the second argument. The context is (ufunc, arguments, output argument number). This is the first call tried. If a `TypeError` occurs, then the function is called with just the ndarray as the first argument.

### Methods

There are three methods of ufuncs that require calculation similar to the general-purpose ufuncs. These are <span class="title-ref">ufunc.reduce</span>, <span class="title-ref">ufunc.accumulate</span>, and <span class="title-ref">ufunc.reduceat</span>. Each of these methods requires a setup command followed by a loop. There are four loop styles possible for the methods corresponding to no-elements, one-element, strided-loop, and buffered-loop. These are the same basic loop styles as implemented for the general-purpose function call except for the no-element and one-element cases which are special-cases occurring when the input array objects have 0 and 1 elements respectively.

#### Setup

The setup function for all three methods is `construct_reduce`. This function creates a reducing loop object and fills it with the parameters needed to complete the loop. All of the methods only work on ufuncs that take 2-inputs and return 1 output. Therefore, the underlying 1-D loop is selected assuming a signature of `[otype, otype, otype]` where `otype` is the requested reduction datatype. The buffer size and error handling are then retrieved from (per-thread) global storage. For small arrays that are misaligned or have incorrect datatype, a copy is made so that the un-buffered section of code is used. Then, the looping strategy is selected. If there is 1 element or 0 elements in the array, then a simple looping method is selected. If the array is not misaligned and has the correct datatype, then strided looping is selected. Otherwise, buffered looping must be performed. Looping parameters are then established, and the return array is constructed. The output array is of a different `shape` depending on whether the method is <span class="title-ref">reduce \<ufunc.reduce\></span>, <span class="title-ref">accumulate \<ufunc.accumulate\></span>, or <span class="title-ref">reduceat \<ufunc.reduceat\></span>. If an output array is already provided, then its shape is checked. If the output array is not C-contiguous, aligned, and of the correct data type, then a temporary copy is made with the :c\`NPY\_ARRAY\_WRITEBACKIFCOPY\` flag set. In this way, the methods will be able to work with a well-behaved output array but the result will be copied back into the true output array when :c\`PyArray\_ResolveWritebackIfCopy\` is called at function completion. Finally, iterators are set up to loop over the correct `axis` (depending on the value of axis provided to the method) and the setup routine returns to the actual computation routine.

#### <span class="title-ref">Reduce \<ufunc.reduce\></span>

<div class="index">

triple: ufunc; methods; reduce

</div>

All of the ufunc methods use the same underlying 1-D computational loops with input and output arguments adjusted so that the appropriate reduction takes place. For example, the key to the functioning of <span class="title-ref">reduce \<ufunc.reduce\></span> is that the 1-D loop is called with the output and the second input pointing to the same position in memory and both having a step-size of 0. The first input is pointing to the input array with a step-size given by the appropriate stride for the selected axis. In this way, the operation performed is

\[\begin{aligned}
\begin{align*}
o & = & i[0] \\
o & = & i[k]\textrm{<op>}o\quad k=1\ldots N
\end{align*}
\end{aligned}\]

where \(N+1\) is the number of elements in the input, \(i\), \(o\) is the output, and \(i[k]\) is the \(k^{\textrm{th}}\) element of \(i\) along the selected axis. This basic operation is repeated for arrays with greater than 1 dimension so that the reduction takes place for every 1-D sub-array along the selected axis. An iterator with the selected dimension removed handles this looping.

For buffered loops, care must be taken to copy and cast data before the loop function is called because the underlying loop expects aligned data of the correct datatype (including byteorder). The buffered loop must handle this copying and casting prior to calling the loop function on chunks no greater than the user-specified `bufsize`.

#### <span class="title-ref">Accumulate \<ufunc.accumulate\></span>

<div class="index">

triple: ufunc; methods; accumulate

</div>

The <span class="title-ref">accumulate \<ufunc.accumulate\></span> method is very similar to the <span class="title-ref">reduce \<ufunc.reduce\></span> method in that the output and the second input both point to the output. The difference is that the second input points to memory one stride behind the current output pointer. Thus, the operation performed is

\[\begin{aligned}
\begin{align*}
o[0] & = & i[0] \\
o[k] & = & i[k]\textrm{<op>}o[k-1]\quad k=1\ldots N.
\end{align*}
\end{aligned}\]

The output has the same shape as the input and each 1-D loop operates over \(N\) elements when the shape in the selected axis is \(N+1\). Again, buffered loops take care to copy and cast the data before calling the underlying 1-D computational loop.

#### <span class="title-ref">Reduceat \<ufunc.reduceat\></span>

<div class="index">

triple: ufunc; methods; reduceat single: ufunc

</div>

The <span class="title-ref">reduceat \<ufunc.reduceat\></span> function is a generalization of both the <span class="title-ref">reduce \<ufunc.reduce\></span> and <span class="title-ref">accumulate \<ufunc.accumulate\></span> functions. It implements a <span class="title-ref">reduce \<ufunc.reduce\></span> over ranges of the input array specified by indices. The extra indices argument is checked to be sure that every input is not too large for the input array along the selected dimension before the loop calculations take place. The loop implementation is handled using code that is very similar to the <span class="title-ref">reduce \<ufunc.reduce\></span> code repeated as many times as there are elements in the indices input. In particular: the first input pointer passed to the underlying 1-D computational loop points to the input array at the correct location indicated by the index array. In addition, the output pointer and the second input pointer passed to the underlying 1-D loop point to the same position in memory. The size of the 1-D computational loop is fixed to be the difference between the current index and the next index (when the current index is the last index, then the next index is assumed to be the length of the array along the selected dimension). In this way, the 1-D loop will implement a <span class="title-ref">reduce \<ufunc.reduce\></span> over the specified indices.

Misaligned or a loop datatype that does not match the input and/or output datatype is handled using buffered code wherein data is copied to a temporary buffer and cast to the correct datatype if necessary prior to calling the underlying 1-D function. The temporary buffers are created in (element) sizes no bigger than the user settable buffer-size value. Thus, the loop must be flexible enough to call the underlying 1-D computational loop enough times to complete the total calculation in chunks no bigger than the buffer-size.

---

internals.md

---

<div class="currentmodule">

numpy

</div>

# Internal organization of NumPy arrays

It helps to understand a bit about how NumPy arrays are handled under the covers to help understand NumPy better. This section will not go into great detail. Those wishing to understand the full details are requested to refer to Travis Oliphant's book [Guide to NumPy](https://web.mit.edu/dvp/Public/numpybook.pdf).

NumPy arrays consist of two major components: the raw array data (from now on, referred to as the data buffer), and the information about the raw array data. The data buffer is typically what people think of as arrays in C or Fortran, a `contiguous` (and fixed) block of memory containing fixed-sized data items. NumPy also contains a significant set of data that describes how to interpret the data in the data buffer. This extra information contains (among other things):

1.  The basic data element's size in bytes.
2.  The start of the data within the data buffer (an offset relative to the beginning of the data buffer).
3.  The number of `dimensions <dimension>` and the size of each dimension.
4.  The separation between elements for each dimension (the `stride`). This does not have to be a multiple of the element size.
5.  The byte order of the data (which may not be the native byte order).
6.  Whether the buffer is read-only.
7.  Information (via the <span class="title-ref">dtype</span> object) about the interpretation of the basic data element. The basic data element may be as simple as an int or a float, or it may be a compound object (e.g., `struct-like <structured data type>`), a fixed character field, or Python object pointers.
8.  Whether the array is to be interpreted as `C-order <C order>` or `Fortran-order <Fortran order>`.

This arrangement allows for the very flexible use of arrays. One thing that it allows is simple changes to the metadata to change the interpretation of the array buffer. Changing the byteorder of the array is a simple change involving no rearrangement of the data. The `shape` of the array can be changed very easily without changing anything in the data buffer or any data copying at all.

Among other things that are made possible is one can create a new array metadata object that uses the same data buffer to create a new `view` of that data buffer that has a different interpretation of the buffer (e.g., different shape, offset, byte order, strides, etc) but shares the same data bytes. Many operations in NumPy do just this such as `slicing <python:slice>`. Other operations, such as transpose, don't move data elements around in the array, but rather change the information about the shape and strides so that the indexing of the array changes, but the data in the array doesn't move.

Typically these new versions of the array metadata but the same data buffer are new views into the data buffer. There is a different <span class="title-ref">ndarray</span> object, but it uses the same data buffer. This is why it is necessary to force copies through the use of the <span class="title-ref">copy</span> method if one really wants to make a new and independent copy of the data buffer.

New views into arrays mean the object reference counts for the data buffer increase. Simply doing away with the original array object will not remove the data buffer if other views of it still exist.

## Multidimensional array indexing order issues

<div class="seealso">

\[basics.indexing\](\#basics.indexing)

</div>

What is the right way to index multi-dimensional arrays? Before you jump to conclusions about the one and true way to index multi-dimensional arrays, it pays to understand why this is a confusing issue. This section will try to explain in detail how NumPy indexing works and why we adopt the convention we do for images, and when it may be appropriate to adopt other conventions.

The first thing to understand is that there are two conflicting conventions for indexing 2-dimensional arrays. Matrix notation uses the first index to indicate which row is being selected and the second index to indicate which column is selected. This is opposite the geometrically oriented-convention for images where people generally think the first index represents x position (i.e., column) and the second represents y position (i.e., row). This alone is the source of much confusion; matrix-oriented users and image-oriented users expect two different things with regard to indexing.

The second issue to understand is how indices correspond to the order in which the array is stored in memory. In Fortran, the first index is the most rapidly varying index when moving through the elements of a two-dimensional array as it is stored in memory. If you adopt the matrix convention for indexing, then this means the matrix is stored one column at a time (since the first index moves to the next row as it changes). Thus Fortran is considered a Column-major language. C has just the opposite convention. In C, the last index changes most rapidly as one moves through the array as stored in memory. Thus C is a Row-major language. The matrix is stored by rows. Note that in both cases it presumes that the matrix convention for indexing is being used, i.e., for both Fortran and C, the first index is the row. Note this convention implies that the indexing convention is invariant and that the data order changes to keep that so.

But that's not the only way to look at it. Suppose one has large two-dimensional arrays (images or matrices) stored in data files. Suppose the data are stored by rows rather than by columns. If we are to preserve our index convention (whether matrix or image) that means that depending on the language we use, we may be forced to reorder the data if it is read into memory to preserve our indexing convention. For example, if we read row-ordered data into memory without reordering, it will match the matrix indexing convention for C, but not for Fortran. Conversely, it will match the image indexing convention for Fortran, but not for C. For C, if one is using data stored in row order, and one wants to preserve the image index convention, the data must be reordered when reading into memory.

In the end, what you do for Fortran or C depends on which is more important, not reordering data or preserving the indexing convention. For large images, reordering data is potentially expensive, and often the indexing convention is inverted to avoid that.

The situation with NumPy makes this issue yet more complicated. The internal machinery of NumPy arrays is flexible enough to accept any ordering of indices. One can simply reorder indices by manipulating the internal `stride` information for arrays without reordering the data at all. NumPy will know how to map the new index order to the data without moving the data.

So if this is true, why not choose the index order that matches what you most expect? In particular, why not define row-ordered images to use the image convention? (This is sometimes referred to as the Fortran convention vs the C convention, thus the 'C' and 'FORTRAN' order options for array ordering in NumPy.) The drawback of doing this is potential performance penalties. It's common to access the data sequentially, either implicitly in array operations or explicitly by looping over rows of an image. When that is done, then the data will be accessed in non-optimal order. As the first index is incremented, what is actually happening is that elements spaced far apart in memory are being sequentially accessed, with usually poor memory access speeds. For example, for a two-dimensional image `im` defined so that `im[0, 10]` represents the value at `x = 0`, `y = 10`. To be consistent with usual Python behavior then `im[0]` would represent a column at `x = 0`. Yet that data would be spread over the whole array since the data are stored in row order. Despite the flexibility of NumPy's indexing, it can't really paper over the fact basic operations are rendered inefficient because of data order or that getting contiguous subarrays is still awkward (e.g., `im[:, 0]` for the first row, vs `im[0]`). Thus one can't use an idiom such as for row in `im`; for col in `im` does work, but doesn't yield contiguous column data.

As it turns out, NumPy is smart enough when dealing with \[ufuncs \<ufuncs-internals\>\](\#ufuncs-\<ufuncs-internals\>) to determine which index is the most rapidly varying one in memory and uses that for the innermost loop. Thus for ufuncs, there is no large intrinsic advantage to either approach in most cases. On the other hand, use of <span class="title-ref">ndarray.flat</span> with a FORTRAN ordered array will lead to non-optimal memory access as adjacent elements in the flattened array (iterator, actually) are not contiguous in memory.

Indeed, the fact is that Python indexing on lists and other sequences naturally leads to an outside-to-inside ordering (the first index gets the largest grouping, the next largest, and the last gets the smallest element). Since image data are normally stored in rows, this corresponds to the position within rows being the last item indexed.

If you do want to use Fortran ordering realize that there are two approaches to consider: 1) accept that the first index is just not the most rapidly changing in memory and have all your I/O routines reorder your data when going from memory to disk or visa versa, or use NumPy's mechanism for mapping the first index to the most rapidly varying data. We recommend the former if possible. The disadvantage of the latter is that many of NumPy's functions will yield arrays without Fortran ordering unless you are careful to use the `order` keyword. Doing this would be highly inconvenient.

Otherwise, we recommend simply learning to reverse the usual order of indices when accessing elements of an array. Granted, it goes against the grain, but it is more in line with Python semantics and the natural order of the data.

---

releasing.md

---

# Releasing a version

The following guides include detailed information on how to prepare a NumPy release.

## How to prepare a release

These instructions give an overview of what is necessary to build binary releases for NumPy.

### Current build and release info

Useful info can be found in the following locations:

  - **Source tree**
      - [INSTALL.rst](https://github.com/numpy/numpy/blob/main/INSTALL.md)
      - [pavement.py](https://github.com/numpy/numpy/blob/main/pavement.py)
  - **NumPy docs**
      - <https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.rst>
      - <https://github.com/numpy/numpy/blob/main/doc/RELEASE_WALKTHROUGH.rst>
      - <https://github.com/numpy/numpy/blob/main/doc/BRANCH_WALKTHROUGH.rst>
  - **Release scripts**
      - <https://github.com/numpy/numpy-vendor>

### Supported platforms and versions

\[NEP 29 \<NEP29\>\](\#nep-29-\<nep29\>) outlines which Python versions are supported; For the first half of 2020, this will be Python \>= 3.6. We test NumPy against all these versions every time we merge code to main. Binary installers may be available for a subset of these versions (see below).

  - **OS X**
    
    OS X versions \>= 10.9 are supported, for Python version support see \[NEP 29 \<NEP29\>\](\#nep-29-\<nep29\>). We build binary wheels for OSX that are compatible with Python.org Python, system Python, homebrew and macports - see this [OSX wheel building summary](https://github.com/MacPython/wiki/wiki/Spinning-wheels) for details.

  - **Windows**
    
    We build 32- and 64-bit wheels on Windows. Windows 7, 8 and 10 are supported. We build NumPy using the [mingw-w64 toolchain](https://mingwpy.github.io), [cibuildwheels](https://cibuildwheel.readthedocs.io/en/stable/) and GitHub actions.

  - **Linux**
    
    We build and ship [manylinux2014](https://www.python.org/dev/peps/pep-0513) wheels for NumPy. Many Linux distributions include their own binary builds of NumPy.

  - **BSD / Solaris**
    
    No binaries are provided, but successful builds on Solaris and BSD have been reported.

### Tool chain

We build all our wheels on cloud infrastructure - so this list of compilers is for information and debugging builds locally. See the `.travis.yml` script in the [numpy wheels](https://github.com/MacPython/numpy-wheels) repo for an outdated source of the build recipes using multibuild.

#### Compilers

The same gcc version is used as the one with which Python itself is built on each platform. At the moment this means:

  - OS X builds on travis currently use <span class="title-ref">clang</span>. It appears that binary wheels for OSX \>= 10.6 can be safely built from the travis-ci OSX 10.9 VMs when building against the Python from the Python.org installers;
  - Windows builds use the [mingw-w64 toolchain](https://mingwpy.github.io);
  - Manylinux2014 wheels use the gcc provided on the Manylinux docker images.

You will need Cython for building the binaries. Cython compiles the `.pyx` files in the NumPy distribution to `.c` files.

#### OpenBLAS

All the wheels link to a version of [OpenBLAS](https://github.com/xianyi/OpenBLAS) supplied via the [openblas-libs](https://github.com/MacPython/openblas-libs) repo. The shared object (or DLL) is shipped with in the wheel, renamed to prevent name collisions with other OpenBLAS shared objects that may exist in the filesystem.

#### Building source archives and wheels

The NumPy wheels and sdist are now built using cibuildwheel with github actions.

#### Building docs

We are no longer building `PDF` files. All that will be needed is

  - virtualenv (pip).

The other requirements will be filled automatically during the documentation build process.

#### Uploading to PyPI

The only application needed for uploading is

  - twine (pip).

You will also need a PyPI token, which is best kept on a keyring. See the twine [keyring](https://twine.readthedocs.io/en/stable/#keyring-support) documentation for how to do that.

#### Generating author/PR lists

You will need a personal access token <https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/> so that scripts can access the github NumPy repository.

  - gitpython (pip)
  - pygithub (pip)

### What is released

  - **Wheels** We currently support Python 3.8-3.10 on Windows, OSX, and Linux.
      - Windows: 32-bit and 64-bit wheels built using Github actions;
      - OSX: x64\_86 and arm64 OSX wheels built using Github actions;
      - Linux: x64\_86 and aarch64 Manylinux2014 wheels built using Github actions.
  - **Other** Release notes and changelog
  - **Source distribution** We build source releases in the .tar.gz format.

### Release process

#### Agree on a release schedule

A typical release schedule is one beta, two release candidates and a final release. It's best to discuss the timing on the mailing list first, in order for people to get their commits in on time, get doc wiki edits merged, etc. After a date is set, create a new maintenance/x.y.z branch, add new empty release notes for the next version in the main branch and update the Trac Milestones.

#### Make sure current branch builds a package correctly

The CI builds wheels when a PR header begins with `REL`. Your last PR before releasing should be so marked and all the tests should pass. You can also do:

    git clean -fxdq
    python setup.py bdist_wheel
    python setup.py sdist

For details of the build process itself, it is best to read the Step-by-Step Directions below.

<div class="note">

<div class="title">

Note

</div>

The following steps are repeated for the beta(s), release candidates(s) and the final release.

</div>

#### Check deprecations

Before \[the release branch is made \<branching\>\](\#the-release-branch-is-made-\<branching\>), it should be checked that all deprecated code that should be removed is actually removed, and all new deprecations say in the docstring or deprecation warning what version the code will be removed.

#### Check the C API version number

The C API version needs to be tracked in three places

  - numpy/\_core/meson.build
  - numpy/\_core/code\_generators/cversions.txt
  - numpy/\_core/include/numpy/numpyconfig.h

There are three steps to the process.

1.  If the API has changed, increment the C\_API\_VERSION in numpy/core/meson.build. The API is unchanged only if any code compiled against the current API will be backward compatible with the last released NumPy version. Any changes to C structures or additions to the public interface will make the new API not backward compatible.

2.  If the C\_API\_VERSION in the first step has changed, or if the hash of the API has changed, the cversions.txt file needs to be updated. To check the hash, run the script numpy/\_core/cversions.py and note the API hash that is printed. If that hash does not match the last hash in numpy/\_core/code\_generators/cversions.txt the hash has changed. Using both the appropriate C\_API\_VERSION and hash, add a new entry to cversions.txt. If the API version was not changed, but the hash differs, you will need to comment out the previous entry for that API version. For instance, in NumPy 1.9 annotations were added, which changed the hash, but the API was the same as in 1.8. The hash serves as a check for API changes, but it is not definitive.
    
    If steps 1 and 2 are done correctly, compiling the release should not give a warning "API mismatch detect at the beginning of the build".

3.  The numpy/\_core/include/numpy/numpyconfig.h will need a new NPY\_X\_Y\_API\_VERSION macro, where X and Y are the major and minor version numbers of the release. The value given to that macro only needs to be increased from the previous version if some of the functions or macros in the include files were deprecated.

The C ABI version number in numpy/\_core/meson.build should only be updated for a major release.

#### Check the release notes

Use [towncrier](https://pypi.org/project/towncrier/) to build the release note and commit the changes. This will remove all the fragments from `doc/release/upcoming_changes` and add `doc/release/<version>-note.rst`.:

    towncrier build --version "<version>"
    git commit -m"Create release note"

Check that the release notes are up-to-date.

Update the release notes with a Highlights section. Mention some of the following:

  - major new features
  - deprecated and removed features
  - supported Python versions
  - for SciPy, supported NumPy version(s)
  - outlook for the near future

## Step-by-step directions

This is a walkthrough of the NumPy 2.1.0 release on Linux, modified for building with GitHub Actions and cibuildwheels and uploading to the [anaconda.org staging repository for NumPy](https://anaconda.org/multibuild-wheels-staging/numpy). The commands can be copied into the command line, but be sure to replace 2.1.0 by the correct version. This should be read together with the \[general release guide \<prepare\_release\>\](\#general-release-guide-\<prepare\_release\>).

### Facility preparation

Before beginning to make a release, use the `requirements/*_requirements.txt` files to ensure that you have the needed software. Most software can be installed with pip, but some will require apt-get, dnf, or whatever your system uses for software. You will also need a GitHub personal access token (PAT) to push the documentation. There are a few ways to streamline things:

  - Git can be set up to use a keyring to store your GitHub personal access token. Search online for the details.
  - You can use the `keyring` app to store the PyPI password for twine. See the online twine documentation for details.

### Prior to release

#### Add/drop Python versions

When adding or dropping Python versions, three files need to be edited:

  - .github/workflows/wheels.yml \# for github cibuildwheel
  - tools/ci/cirrus\_wheels.yml \# for cibuildwheel aarch64/arm64 builds
  - pyproject.toml \# for classifier and minimum version check.

Make these changes in an ordinary PR against main and backport if necessary. Add `[wheel build]` at the end of the title line of the commit summary so that wheel builds will be run to test the changes. We currently release wheels for new Python versions after the first Python rc once manylinux and cibuildwheel support it. For Python 3.11 we were able to release within a week of the rc1 announcement.

#### Backport pull requests

Changes that have been marked for this release must be backported to the maintenance/2.1.x branch.

#### Update 2.1.0 milestones

Look at the issues/prs with 2.1.0 milestones and either push them off to a later version, or maybe remove the milestone. You may need to add a milestone.

### Make a release PR

Four documents usually need to be updated or created for the release PR:

  - The changelog
  - The release notes
  - The `.mailmap` file
  - The `pyproject.toml` file

These changes should be made in an ordinary PR against the maintenance branch. The commit heading should contain a `[wheel build]` directive to test if the wheels build. Other small, miscellaneous fixes may be part of this PR. The commit message might be something like:

    REL: Prepare for the NumPy 2.1.0 release [wheel build]
    
    - Create 2.1.0-changelog.rst.
    - Update 2.1.0-notes.rst.
    - Update .mailmap.
    - Update pyproject.toml

#### Set the release version

Check the `pyproject.toml` file and set the release version if needed:

    $ gvim pyproject.toml

#### Check the `pavement.py` and `doc/source/release.rst` files

Check that the `pavement.py` file points to the correct release notes. It should have been updated after the last release, but if not, fix it now. Also make sure that the notes have an entry in the `release.rst` file:

    $ gvim pavement.py doc/source/release.rst

#### Generate the changelog

The changelog is generated using the changelog tool:

    $ spin changelog $GITHUB v2.0.0..maintenance/2.1.x > doc/changelog/2.1.0-changelog.rst

where `GITHUB` contains your GitHub access token. The text will need to be checked for non-standard contributor names and dependabot entries removed. It is also a good idea to remove any links that may be present in the PR titles as they don't translate well to markdown, replace them with monospaced text. The non-standard contributor names should be fixed by updating the `.mailmap` file, which is a lot of work. It is best to make several trial runs before reaching this point and ping the malefactors using a GitHub issue to get the needed information.

#### Finish the release notes

If there are any release notes snippets in `doc/release/upcoming_changes/`, run `spin notes`, which will incorporate the snippets into the `doc/source/release/notes-towncrier.rst` file and delete the snippets:

    $ spin notes
    $ gvim doc/source/release/notes-towncrier.rst doc/source/release/2.1.0-notes.rst

Once the `notes-towncrier` contents has been incorporated into release note the `<!-- Failed to include notes-towncrier.rst` directive can be removed. The notes --\> will always need some fixups, the introduction will need to be written, and significant changes should be called out. For patch releases the changelog text may also be appended, but not for the initial release as it is too long. Check previous release notes to see how this is done.

### Release walkthrough

Note that in the code snippets below, `upstream` refers to the root repository on GitHub and `origin` to its fork in your personal GitHub repositories. You may need to make adjustments if you have not forked the repository but simply cloned it locally. You can also edit `.git/config` and add `upstream` if it isn't already present.

#### 1\. Prepare the release commit

Checkout the branch for the release, make sure it is up to date, and clean the repository:

    $ git checkout maintenance/2.1.x
    $ git pull upstream maintenance/2.1.x
    $ git submodule update
    $ git clean -xdfq

Sanity check:

    $ python3 -m spin test -m full

Tag the release and push the tag. This requires write permission for the numpy repository:

    $ git tag -a -s v2.1.0 -m"NumPy 2.1.0 release"
    $ git push upstream v2.1.0

If you need to delete the tag due to error:

    $ git tag -d v2.1.0
    $ git push --delete upstream v2.1.0

#### 2\. Build wheels

Tagging the build at the beginning of this process will trigger a wheel build via cibuildwheel and upload wheels and an sdist to the staging repo. The CI run on github actions (for all x86-based and macOS arm64 wheels) takes about 1 1/4 hours. The CI runs on cirrus (for aarch64 and M1) take less time. You can check for uploaded files at the [staging repository](https://anaconda.org/multibuild-wheels-staging/numpy/files), but note that it is not closely synched with what you see of the running jobs.

If you wish to manually trigger a wheel build, you can do so:

  - On github actions -\> [Wheel builder](https://github.com/numpy/numpy/actions/workflows/wheels.yml) there is a "Run workflow" button, click on it and choose the tag to build
  - On Cirrus we don't currently have an easy way to manually trigger builds and uploads.

If a wheel build fails for unrelated reasons, you can rerun it individually:

  - On github actions select [Wheel builder](https://github.com/numpy/numpy/actions/workflows/wheels.yml) click on the commit that contains the build you want to rerun. On the left there is a list of wheel builds, select the one you want to rerun and on the resulting page hit the counterclockwise arrows button.
  - On cirrus, log into cirrusci, look for the v2.1.0 tag and rerun the failed jobs.

#### 3\. Download wheels

When the wheels have all been successfully built and staged, download them from the Anaconda staging directory using the `tools/download-wheels.py` script:

    $ cd ../numpy
    $ mkdir -p release/installers
    $ python3 tools/download-wheels.py 2.1.0

#### 4\. Generate the README files

This needs to be done after all installers are downloaded, but before the pavement file is updated for continued development:

    $ paver write_release

#### 5\. Upload to PyPI

Upload to PyPI using `twine`. A recent version of `twine` of is needed after recent PyPI changes, version `3.4.1` was used here:

    $ cd ../numpy
    $ twine upload release/installers/*.whl
    $ twine upload release/installers/*.gz  # Upload last.

If one of the commands breaks in the middle, you may need to selectively upload the remaining files because PyPI does not allow the same file to be uploaded twice. The source file should be uploaded last to avoid synchronization problems that might occur if pip users access the files while this is in process, causing pip to build from source rather than downloading a binary wheel. PyPI only allows a single source distribution, here we have chosen the zip archive.

#### 6\. Upload files to GitHub

Go to <https://github.com/numpy/numpy/releases>, there should be a `v2.1.0 tag`, click on it and hit the edit button for that tag and update the title to 'v2.1.0 (\<date\>). There are two ways to add files, using an editable text window and as binary uploads. Start by editing the `release/README.md` that is translated from the rst version using pandoc. Things that will need fixing: PR lines from the changelog, if included, are wrapped and need unwrapping, links should be changed to monospaced text. Then copy the contents to the clipboard and paste them into the text window. It may take several tries to get it look right. Then

  - Upload `release/installers/numpy-2.1.0.tar.gz` as a binary file.
  - Upload `release/README.rst` as a binary file.
  - Upload `doc/changelog/2.1.0-changelog.rst` as a binary file.
  - Check the pre-release button if this is a pre-releases.
  - Hit the `{Publish,Update} release` button at the bottom.

#### 7\. Upload documents to numpy.org (skip for prereleases)

<div class="note">

<div class="title">

Note

</div>

You will need a GitHub personal access token to push the update.

</div>

This step is only needed for final releases and can be skipped for pre-releases and most patch releases. `make merge-doc` clones the `numpy/doc` repo into `doc/build/merge` and updates it with the new documentation:

    $ git clean -xdfq
    $ git co v2.1.0
    $ rm -rf doc/build  # want version to be current
    $ python -m spin docs merge-doc --build
    $ pushd doc/build/merge

If the release series is a new one, you will need to add a new section to the `doc/build/merge/index.html` front page just after the "insert here" comment:

    $ gvim index.html +/'insert here'

Further, update the version-switcher json file to add the new release and update the version marked `(stable)` and `preferred`:

    $ gvim _static/versions.json

Then run `update.py` to update the version in `_static`:

    $ python3 update.py

You can "test run" the new documentation in a browser to make sure the links work, although the version dropdown will not change, it pulls its information from `numpy.org`:

    $ firefox index.html  # or google-chrome, etc.

Update the stable link and update:

    $ ln -sfn 2.1 stable
    $ ls -l  # check the link

Once everything seems satisfactory, update, commit and upload the changes:

    $ git commit -a -m"Add documentation for v2.1.0"
    $ git push git@github.com:numpy/doc
    $ popd

#### 8\. Reset the maintenance branch into a development state (skip for prereleases)

Create release notes for next release and edit them to set the version. These notes will be a skeleton and have little content:

    $ git checkout -b begin-2.1.1 maintenance/2.1.x
    $ cp doc/source/release/template.rst doc/source/release/2.1.1-notes.rst
    $ gvim doc/source/release/2.1.1-notes.rst
    $ git add doc/source/release/2.1.1-notes.rst

Add new release notes to the documentation release list and update the `RELEASE_NOTES` variable in `pavement.py`:

    $ gvim doc/source/release.rst pavement.py

Update the `version` in `pyproject.toml`:

    $ gvim pyproject.toml

Commit the result:

    $ git commit -a -m"MAINT: Prepare 2.1.x for further development"
    $ git push origin HEAD

Go to GitHub and make a PR. It should be merged quickly.

#### 9\. Announce the release on numpy.org (skip for prereleases)

This assumes that you have forked <https://github.com/numpy/numpy.org>:

    $ cd ../numpy.org
    $ git checkout main
    $ git pull upstream main
    $ git checkout -b announce-numpy-2.1.0
    $ gvim content/en/news.md

  - For all releases, go to the bottom of the page and add a one line link. Look to the previous links for example.
  - For the `*.0` release in a cycle, add a new section at the top with a short description of the new features and point the news link to it.

commit and push:

    $ git commit -a -m"announce the NumPy 2.1.0 release"
    $ git push origin HEAD

Go to GitHub and make a PR.

#### 10\. Announce to mailing lists

The release should be announced on the numpy-discussion, scipy-devel, and python-announce-list mailing lists. Look at previous announcements for the basic template. The contributor and PR lists are the same as generated for the release notes above. If you crosspost, make sure that python-announce-list is BCC so that replies will not be sent to that list.

#### 11\. Post-release update main (skip for prereleases)

Checkout main and forward port the documentation changes. You may also want to update these notes if procedures have changed or improved:

    $ git checkout -b post-2.1.0-release-update main
    $ git checkout maintenance/2.1.x doc/source/release/2.1.0-notes.rst
    $ git checkout maintenance/2.1.x doc/changelog/2.1.0-changelog.rst
    $ git checkout maintenance/2.1.x .mailmap  # only if updated for release.
    $ gvim doc/source/release.rst  # Add link to new notes
    $ git status  # check status before commit
    $ git commit -a -m"MAINT: Update main after 2.1.0 release."
    $ git push origin HEAD

Go to GitHub and make a PR.

## Branch walkthrough

This guide contains a walkthrough of branching NumPy 1.21.x on Linux. The commands can be copied into the command line, but be sure to replace 1.21 and 1.22 by the correct versions. It is good practice to make `.mailmap` as current as possible before making the branch, that may take several weeks.

This should be read together with the \[general release guide \<prepare\_release\>\](\#general-release-guide-\<prepare\_release\>).

### Branching

#### Make the branch

This is only needed when starting a new maintenance branch. Because NumPy now depends on tags to determine the version, the start of a new development cycle in the main branch needs an annotated tag. That is done as follows:

    $ git checkout main
    $ git pull upstream main
    $ git commit --allow-empty -m'REL: Begin NumPy 1.22.0 development'
    $ git push upstream HEAD

If the push fails because new PRs have been merged, do:

    $ git pull --rebase upstream

and repeat the push. Once the push succeeds, tag it:

    $ git tag -a -s v1.22.0.dev0 -m'Begin NumPy 1.22.0 development'
    $ git push upstream v1.22.0.dev0

then make the new branch and push it:

    $ git branch maintenance/1.21.x HEAD^
    $ git push upstream maintenance/1.21.x

#### Prepare the main branch for further development

Make a PR branch to prepare main for further development:

    $ git checkout -b 'prepare-main-for-1.22.0-development' v1.22.0.dev0

Delete the release note fragments:

    $ git rm doc/release/upcoming_changes/[0-9]*.*.rst

Create the new release notes skeleton and add to index:

    $ cp doc/source/release/template.rst doc/source/release/1.22.0-notes.rst
    $ gvim doc/source/release/1.22.0-notes.rst  # put the correct version
    $ git add doc/source/release/1.22.0-notes.rst
    $ gvim doc/source/release.rst  # add new notes to notes index
    $ git add doc/source/release.rst

Update `pavement.py` and update the `RELEASE_NOTES` variable to point to the new notes:

    $ gvim pavement.py
    $ git add pavement.py

Update `cversions.txt` to add current release. There should be no new hash to worry about at this early point, just add a comment following previous practice:

    $ gvim numpy/_core/code_generators/cversions.txt
    $ git add numpy/_core/code_generators/cversions.txt

Check your work, commit it, and push:

    $ git status  # check work
    $ git commit -m'REL: Prepare main for NumPy 1.22.0 development'
    $ git push origin HEAD

Now make a pull request.

---

reviewer_guidelines.md

---

# Reviewer guidelines

Reviewing open pull requests (PRs) helps move the project forward. We encourage people outside the project to get involved as well; it's a great way to get familiar with the codebase.

## Who can be a reviewer?

Reviews can come from outside the NumPy team -- we welcome contributions from domain experts (for instance, <span class="title-ref">linalg</span> or <span class="title-ref">fft</span>) or maintainers of other projects. You do not need to be a NumPy maintainer (a NumPy team member with permission to merge a PR) to review.

If we do not know you yet, consider introducing yourself in [the mailing list or Slack](https://numpy.org/community/) before you start reviewing pull requests.

## Communication guidelines

  - Every PR, good or bad, is an act of generosity. Opening with a positive comment will help the author feel rewarded, and your subsequent remarks may be heard more clearly. You may feel good also.
  - Begin if possible with the large issues, so the author knows they've been understood. Resist the temptation to immediately go line by line, or to open with small pervasive issues.
  - You are the face of the project, and NumPy some time ago decided [the kind of project it will be](https://numpy.org/code-of-conduct/): open, empathetic, welcoming, friendly and patient. Be [kind](https://youtu.be/tzFWz5fiVKU?t=49m30s) to contributors.
  - Do not let perfect be the enemy of the good, particularly for documentation. If you find yourself making many small suggestions, or being too nitpicky on style or grammar, consider merging the current PR when all important concerns are addressed. Then, either push a commit directly (if you are a maintainer) or open a follow-up PR yourself.
  - If you need help writing replies in reviews, check out some \[standard replies for reviewing\<saved-replies\>\](\#standard-replies-for-reviewing\<saved-replies\>).

## Reviewer checklist

  -   - Is the intended behavior clear under all conditions? Some things to watch:
        
          - What happens with unexpected inputs like empty arrays or nan/inf values?
          - Are axis or shape arguments tested to be <span class="title-ref">int</span> or <span class="title-ref">tuples</span>?
          - Are unusual <span class="title-ref">dtypes</span> tested if a function supports those?

  - Should variable names be improved for clarity or consistency?

  - Should comments be added, or rather removed as unhelpful or extraneous?

  - Does the documentation follow the \[NumPy guidelines\<howto-document\>\](\#numpy-guidelines\<howto-document\>)? Are the docstrings properly formatted?

  - Does the code follow NumPy's \[Stylistic Guidelines\<stylistic-guidelines\>\](\#stylistic-guidelines\<stylistic-guidelines\>)?

  - If you are a maintainer, and it is not obvious from the PR description, add a short explanation of what a branch did to the merge message and, if closing an issue, also add "Closes gh-123" where 123 is the issue number.

  - For code changes, at least one maintainer (i.e. someone with commit rights) should review and approve a pull request. If you are the first to review a PR and approve of the changes use the GitHub [approve review](https://help.github.com/articles/reviewing-changes-in-pull-requests/) tool to mark it as such. If a PR is straightforward, for example it's a clearly correct bug fix, it can be merged straight away. If it's more complex or changes public API, please leave it open for at least a couple of days so other maintainers get a chance to review.

  - If you are a subsequent reviewer on an already approved PR, please use the same review method as for a new PR (focus on the larger issues, resist the temptation to add only a few nitpicks). If you have commit rights and think no more review is needed, merge the PR.

### For maintainers

  - Make sure all automated CI tests pass before merging a PR, and that the \[documentation builds \<building-docs\>\](\#documentation-builds-\<building-docs\>) without any errors.

<!-- end list -->

  - \- In case of merge conflicts, ask the PR submitter to \[rebase on main  
    \<rebasing-on-main\>\](\#rebase-on-main

\--\<rebasing-on-main\>). - For PRs that add new features or are in some way complex, wait at least a day or two before merging it. That way, others get a chance to comment before the code goes in. Consider adding it to the release notes. - When merging contributions, a committer is responsible for ensuring that those meet the requirements outlined in the \[Development process guidelines \<guidelines\>\](\#development-process-guidelines --\<guidelines\>) for NumPy. Also, check that new features and backwards compatibility breaks were discussed on the [numpy-discussion mailing list](https://mail.python.org/mailman/listinfo/numpy-discussion). - Squashing commits or cleaning up commit messages of a PR that you consider too messy is OK. Remember to retain the original author's name when doing this. Make sure commit messages follow the \[rules for NumPy \<writing-the-commit-message\>\](\#rules-for-numpy --\<writing-the-commit-message\>). - When you want to reject a PR: if it's very obvious, you can just close it and explain why. If it's not, then it's a good idea to first explain why you think the PR is not suitable for inclusion in NumPy and then let a second committer comment or close. - If the PR submitter doesn't respond to your comments for 6 months, move the PR in question to the inactive category with the â€œinactiveâ€ tag attached. At this point, the PR can be closed by a maintainer. If there is any interest in finalizing the PR under consideration, this can be indicated at any time, without waiting 6 months, by a comment. - Maintainers are encouraged to finalize PRs when only small changes are necessary before merging (e.g., fixing code style or grammatical errors). If a PR becomes inactive, maintainers may make larger changes. Remember, a PR is a collaboration between a contributor and a reviewer/s, sometimes a direct push is the best way to finish it.

### API changes

As mentioned most public API changes should be discussed ahead of time and often with a wider audience (on the mailing list, or even through a NEP).

For changes in the public C-API be aware that the NumPy C-API is backwards compatible so that any addition must be ABI compatible with previous versions. When it is not the case, you must add a guard.

For example `PyUnicodeScalarObject` struct contains the following:

    #if NPY_FEATURE_VERSION >= NPY_1_20_API_VERSION
        char *buffer_fmt;
    #endif

Because the `buffer_fmt` field was added to its end in NumPy 1.20 (all previous fields remained ABI compatible). Similarly, any function added to the API table in `numpy/_core/code_generators/numpy_api.py` must use the `MinVersion` annotation. For example:

    'PyDataMem_SetHandler':                 (304, MinVersion("1.22")),

Header only functionality (such as a new macro) typically does not need to be guarded.

### GitHub workflow

When reviewing pull requests, please use workflow tracking features on GitHub as appropriate:

  - After you have finished reviewing, if you want to ask for the submitter to make changes, change your review status to "Changes requested." This can be done on GitHub, PR page, Files changed tab, Review changes (button on the top right).
  - If you're happy about the current status, mark the pull request as Approved (same way as Changes requested). Alternatively (for maintainers): merge the pull request, if you think it is ready to be merged.

It may be helpful to have a copy of the pull request code checked out on your own machine so that you can play with it locally. You can use the [GitHub CLI](https://docs.github.com/en/github/getting-started-with-github/github-cli) to do this by clicking the `Open with` button in the upper right-hand corner of the PR page.

Assuming you have your \[development environment\<development-environment\>\](\#development-environment\<development-environment\>) set up, you can now build the code and test it.

## Standard replies for reviewing

It may be helpful to store some of these in GitHub's [saved replies](https://github.com/settings/replies/) for reviewing:

  - **Usage question**
    
      - \`\`\`md  
        You are asking a usage question. The issue tracker is for bugs and new features. I'm going to close this issue, feel free to ask for help via our \[help channels\](<https://numpy.org/gethelp/>).

  - **Youâ€™re welcome to update the docs**
    
    ``` md
    Please feel free to offer a pull request updating the documentation if you feel it could be improved.
    ```

  - **Self-contained example for bug**
    
    ``` md
    Please provide a [self-contained example code](https://stackoverflow.com/help/mcve), including imports and data (if possible), so that other contributors can just run it and reproduce your issue.
    Ideally your example code should be minimal.
    ```

  - **Software versions**
    
    ```` md
    To help diagnose your issue, please paste the output of:
    ```
    python -c 'import numpy; print(numpy.version.version)'
    ```
    Thanks.
    ````

  - **Code blocks**
    
    ``` md
    Readability can be greatly improved if you [format](https://help.github.com/articles/creating-and-highlighting-code-blocks/) your code snippets and complete error messages appropriately.
    You can edit your issue descriptions and comments at any time to improve readability.
    This helps maintainers a lot. Thanks!
    ```

  - **Linking to code**
    
    ``` md
    For clarity's sake, you can link to code like [this](https://help.github.com/articles/creating-a-permanent-link-to-a-code-snippet/).
    ```

  - **Better description and title**
    
    ``` md
    Please make the title of the PR more descriptive.
    The title will become the commit message when this is merged.
    You should state what issue (or PR) it fixes/resolves in the description using the syntax described [here](https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).
    ```

  - **Regression test needed**
    
    ``` md
    Please add a [non-regression test](https://en.wikipedia.org/wiki/Non-regression_testing) that would fail at main but pass in this PR.
    ```

  - **Donâ€™t change unrelated**
    
    ``` md
    Please do not change unrelated lines. It makes your contribution harder to review and may introduce merge conflicts to other pull requests.
    ```

\`\`\`

---

underthehood.md

---

# Under-the-hood documentation for developers

These documents are intended as a low-level look into NumPy; focused towards developers.

<div class="toctree" data-maxdepth="1">

internals internals.code-explanations alignment ../user/byteswapping ../user/basics.dispatch ../user/basics.subclassing

</div>

---

boilerplating.md

---

# Boilerplate reduction and templating

## Using FYPP for binding generic interfaces

`f2py` doesn't currently support binding interface blocks. However, there are workarounds in use. Perhaps the best known is the usage of `tempita` for using `.pyf.src` files as is done in the bindings which [are part of scipy](https://github.com/scipy/scipy/blob/c93da6f46dbed8b3cc0ccd2495b5678f7b740a03/scipy/linalg/clapack.pyf.src). <span class="title-ref">tempita</span> support has been removed and is no longer recommended in any case.

<div class="note">

<div class="title">

Note

</div>

The reason interfaces cannot be supported within `f2py` itself is because they don't correspond to exported symbols in compiled libraries.

``` sh
â¯ nm gen.o
 0000000000000078 T __add_mod_MOD_add_complex
 0000000000000000 T __add_mod_MOD_add_complex_dp
 0000000000000150 T __add_mod_MOD_add_integer
 0000000000000124 T __add_mod_MOD_add_real
 00000000000000ee T __add_mod_MOD_add_real_dp
```

</div>

Here we will discuss a few techniques to leverage `f2py` in conjunction with [fypp](https://fypp.readthedocs.io/en/stable/fypp.html) to emulate generic interfaces and to ease the binding of multiple (similar) functions.

### Basic example: Addition module

Let us build on the example (from the user guide, \[f2py-examples\](\#f2py-examples)) of a subroutine which takes in two arrays and returns its sum.

<div class="literalinclude" data-language="fortran">

./../code/add.f

</div>

We will recast this into modern fortran:

<div class="literalinclude" data-language="fortran">

./../code/advanced/boilerplating/src/adder\_base.f90

</div>

We could go on as in the original example, adding intents by hand among other things, however in production often there are other concerns. For one, we can template via FYPP the construction of similar functions:

<div class="literalinclude">

./../code/advanced/boilerplating/src/gen\_adder.f90.fypp

</div>

This can be pre-processed to generate the full fortran code:

``` sh
â¯ fypp gen_adder.f90.fypp > adder.f90
```

As to be expected, this can be wrapped by `f2py` subsequently.

Now we will consider maintaining the bindings in a separate file. Note the following basic `.pyf` which can be generated for a single subroutine via `f2py -m adder adder_base.f90 -h adder.pyf`:

<div class="literalinclude" data-language="fortran">

./../code/advanced/boilerplating/src/base\_adder.pyf

</div>

With the docstring:

<div class="literalinclude" data-language="reST">

./../code/advanced/boilerplating/res/base\_docstring.dat

</div>

Which is already pretty good. However, `n` should never be passed in the first place so we will make some minor adjustments.

<div class="literalinclude" data-language="fortran">

./../code/advanced/boilerplating/src/improved\_base\_adder.pyf

</div>

Which corresponds to:

<div class="literalinclude" data-language="reST">

./../code/advanced/boilerplating/res/improved\_docstring.dat

</div>

Finally, we can template over this in a similar manner, to attain the original goal of having bindings which make use of `f2py` directives and have minimal spurious repetition.

<div class="literalinclude">

./../code/advanced/boilerplating/src/adder.pyf.fypp

</div>

Usage boils down to:

``` sh
fypp gen_adder.f90.fypp > adder.f90
fypp adder.pyf.fypp > adder.pyf
f2py -m adder -c adder.pyf adder.f90 --backend meson
```

---

use_cases.md

---

# Advanced F2PY use cases

## Adding user-defined functions to F2PY generated modules

User-defined Python C/API functions can be defined inside signature files using `usercode` and `pymethoddef` statements (they must be used inside the `python module` block). For example, the following signature file `spam.pyf`

  - \! -*- f90 -*-python module spam  
    usercode ''' static char doc\_spam\_system\[\] = "Execute a shell command."; static PyObject *spam\_system(PyObject*self, PyObject *args) { char*command; int sts;
    
      - if (\!PyArg\_ParseTuple(args, "s", \&command))  
        return NULL;
    
    sts = system(command); return Py\_BuildValue("i", sts); } ''' pymethoddef ''' {"system", spam\_system, METH\_VARARGS, doc\_spam\_system}, '''

end python module spam

>   - literal

wraps the C library function `system()`:

    f2py -c spam.pyf

In Python this can then be used as:

<div class="literalinclude" data-language="python">

./../code/results/spam\_session.dat

</div>

## Adding user-defined variables

The following example illustrates how to add user-defined variables to a F2PY generated extension module by modifying the dictionary of a F2PY generated module. Consider the following signature file (compiled with `f2py -c var.pyf`):

<div class="literalinclude" data-language="fortran">

./../code/var.pyf

</div>

Notice that the second `usercode` statement must be defined inside an `interface` block and the module dictionary is available through the variable `d` (see `varmodule.c` generated by `f2py var.pyf` for additional details).

Usage in Python:

<div class="literalinclude" data-language="python">

./../code/results/var\_session.dat

</div>

## Dealing with KIND specifiers

Currently, F2PY can handle only `<type spec>(kind=<kindselector>)` declarations where `<kindselector>` is a numeric integer (e.g. 1, 2, 4,...), but not a function call `KIND(..)` or any other expression. F2PY needs to know what would be the corresponding C type and a general solution for that would be too complicated to implement.

However, F2PY provides a hook to overcome this difficulty, namely, users can define their own \<Fortran type\> to \<C type\> maps. For example, if Fortran 90 code contains:

    REAL(kind=KIND(0.0D0)) ...

then create a mapping file containing a Python dictionary:

    {'real': {'KIND(0.0D0)': 'double'}}

for instance.

Use the `--f2cmap` command-line option to pass the file name to F2PY. By default, F2PY assumes file name is `.f2py_f2cmap` in the current working directory.

More generally, the f2cmap file must contain a dictionary with items:

    <Fortran typespec> : {<selector_expr>:<C type>}

that defines mapping between Fortran type:

    <Fortran typespec>([kind=]<selector_expr>)

and the corresponding \<C type\>. The \<C type\> can be one of the following:

    double
    float
    long_double
    char
    signed_char
    unsigned_char
    short
    unsigned_short
    int
    long
    long_long
    unsigned
    complex_float
    complex_double
    complex_long_double
    string

For example, for a Fortran file `func1.f` containing:

<div class="literalinclude" data-language="fortran">

./../code/f2cmap\_demo.f

</div>

In order to convert `int64` and `real64` to valid `C` data types, a `.f2py_f2cmap` file with the following content can be created in the current directory:

`` `python   dict(real=dict(real64='double'), integer=dict(int64='long long'))  and create the module as usual. F2PY checks if a ``.f2py\_f2cmap`file is present`<span class="title-ref"> in the current directory and will use it to map </span><span class="title-ref">KIND</span><span class="title-ref"> specifiers to </span><span class="title-ref">C</span>\` data types.

`` `sh   f2py -c func1.f -m func1  Alternatively, the mapping file can be saved with any other name, for example ``<span class="title-ref"> </span><span class="title-ref">mapfile.txt</span><span class="title-ref">, and this information can be passed to F2PY by using the </span><span class="title-ref">--f2cmap</span>\` option.

`` `sh   f2py -c func1.f -m func1 --f2cmap mapfile.txt  For more information, see F2Py source code ``numpy/f2py/capi\_maps.py`.  .. _Character strings:  Character strings`\` =================

### Assumed length character strings

In Fortran, assumed length character string arguments are declared as `character*(*)` or `character(len=*)`, that is, the length of such arguments are determined by the actual string arguments at runtime. For `intent(in)` arguments, this lack of length information poses no problems for f2py to construct functional wrapper functions. However, for `intent(out)` arguments, the lack of length information is problematic for f2py generated wrappers because there is no size information available for creating memory buffers for such arguments and F2PY assumes the length is 0. Depending on how the length of assumed length character strings are specified, there exist ways to workaround this problem, as exemplified below.

If the length of the `character*(*)` output argument is determined by the state of other input arguments, the required connection can be established in a signature file or within a f2py-comment by adding an extra declaration for the corresponding argument that specifies the length in character selector part. For example, consider a Fortran file `asterisk1.f90`:

  - subroutine foo1(s)  
    character\*(\*), intent(out) :: s \!f2py character(f2py\_len=12) s s = "123456789A12"

end subroutine foo1

>   - literal

Compile it with `f2py -c asterisk1.f90 -m asterisk1` and then in Python:

\>\>\> import asterisk1 \>\>\> asterisk1.foo1() b'123456789A12'

>   - literal

Notice that the extra declaration `character(f2py_len=12) s` is interpreted only by f2py and in the `f2py_len=` specification one can use C-expressions as a length value.

In the following example:

  - subroutine foo2(s, n)  
    character(len=\*), intent(out) :: s integer, intent(in) :: n \!f2py character(f2py\_len=n), depend(n) :: s s = "123456789A123456789B"(1:n)

end subroutine foo2

>   - literal

the length of the output assumed length string depends on an input argument `n`, after wrapping with F2PY, in Python:

\>\>\> import asterisk \>\>\> asterisk.foo2(2) b'12' \>\>\> asterisk.foo2(12) b'123456789A12' \>\>\>

>   - literal

---

cmake.md

---

# Using via `cmake`

In terms of complexity, `cmake` falls between `make` and `meson`. The learning curve is steeper since CMake syntax is not pythonic and is closer to `make` with environment variables.

However, the trade-off is enhanced flexibility and support for most architectures and compilers. An introduction to the syntax is out of scope for this document, but this [extensive CMake collection](https://cliutils.gitlab.io/modern-cmake/) of resources is great.

\> **Note** \> `cmake` is very popular for mixed-language systems, however support for `f2py` is not particularly native or pleasant; and a more natural approach is to consider \[f2py-skbuild\](\#f2py-skbuild)

## Fibonacci walkthrough (F77)

Returning to the `fib` example from \[f2py-getting-started\](\#f2py-getting-started) section.

<div class="literalinclude" data-language="fortran">

./../code/fib1.f

</div>

We do not need to explicitly generate the `python -m numpy.f2py fib1.f` output, which is `fib1module.c`, which is beneficial. With this; we can now initialize a `CMakeLists.txt` file as follows:

<div class="literalinclude" data-language="cmake">

./../code/CMakeLists.txt

</div>

A key element of the `CMakeLists.txt` file defined above is that the `add_custom_command` is used to generate the wrapper `C` files and then added as a dependency of the actual shared library target via a `add_custom_target` directive which prevents the command from running every time. Additionally, the method used for obtaining the `fortranobject.c` file can also be used to grab the `numpy` headers on older `cmake` versions.

This then works in the same manner as the other modules, although the naming conventions are different and the output library is not automatically prefixed with the `cython` information.

``` bash
ls .
# CMakeLists.txt fib1.f
cmake -S . -B build
cmake --build build
cd build
python -c "import numpy as np; import fibby; a = np.zeros(9); fibby.fib(a); print (a)"
# [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]
```

This is particularly useful where an existing toolchain already exists and `scikit-build` or other additional `python` dependencies are discouraged.

---

distutils-to-meson.md

---

# 1 Migrating to `meson`

As per the timeline laid out in \[distutils-status-migration\](\#distutils-status-migration), `distutils` has ceased to be the default build backend for `f2py`. This page collects common workflows in both formats.

\> **Note** \> This is a \***\*living**\*\* document, [pull requests](https://numpy.org/doc/stable/dev/howto-docs.html) are very welcome\!

## 1.1 Baseline

We will start out with a slightly modern variation of the classic Fibonnaci series generator.

``` fortran
! fib.f90
subroutine fib(a, n)
  use iso_c_binding
   integer(c_int), intent(in) :: n
   integer(c_int), intent(out) :: a(n)
   do i = 1, n
      if (i .eq. 1) then
         a(i) = 0.0d0
      elseif (i .eq. 2) then
         a(i) = 1.0d0
      else
         a(i) = a(i - 1) + a(i - 2)
      end if
   end do
end
```

This will not win any awards, but can be a reasonable starting point.

## 1.2 Compilation options

### 1.2.1 Basic Usage

This is unchanged:

``` bash
python -m numpy.f2py -c fib.f90 -m fib
â¯ python -c "import fib; print(fib.fib(30))"
[     0      1      1      2      3      5      8     13     21     34
     55     89    144    233    377    610    987   1597   2584   4181
   6765  10946  17711  28657  46368  75025 121393 196418 317811 514229]
```

### 1.2.2 Specify the backend

<div class="tab-set">

<div class="tab-item" data-sync="distutils">

Distutils

  - \`\`\`bash  
    python -m numpy.f2py -c fib.f90 -m fib --backend distutils

This is the default for Python versions before 3.12.

</div>

<div class="tab-item" data-sync="meson">

Meson

``` bash
python -m numpy.f2py -c fib.f90 -m fib --backend meson
```

This is the only option for Python versions after 3.12.

</div>

</div>

1.2.3 Pass a compiler name `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^  .. tab-set::    .. tab-item:: Distutils     :sync: distutils ``\`bash python -m numpy.f2py -c fib.f90 -m fib --backend distutils --fcompiler=gfortran

> 
> 
> <div class="tab-item" data-sync="meson">
> 
> Meson
> 
> ``` bash
> FC="gfortran" python -m numpy.f2py -c fib.f90 -m fib --backend meson
> ```
> 
> Native files can also be used.
> 
> </div>

Similarly, `CC` can be used in both cases to set the `C` compiler. Since the `` ` environment variables are generally pretty common across both, so a small sample is included below.  .. table::      +------------------------------------+-------------------------------+     | **Name**                           | **What**                      |     +------------------------------------+-------------------------------+     | FC                                 | Fortran compiler              |     +------------------------------------+-------------------------------+     | CC                                 | C compiler                    |     +------------------------------------+-------------------------------+     | CFLAGS                             | C compiler options            |     +------------------------------------+-------------------------------+     | FFLAGS                             | Fortran compiler options      |     +------------------------------------+-------------------------------+     | LDFLAGS                            | Linker options                |     +------------------------------------+-------------------------------+     | LD\ :sub:`LIBRARY`\ \ :sub:`PATH`\ | Library file locations (Unix) |     +------------------------------------+-------------------------------+     | LIBS                               | Libraries to link against     |     +------------------------------------+-------------------------------+     | PATH                               | Search path for executables   |     +------------------------------------+-------------------------------+     | LDFLAGS                            | Linker flags                  |     +------------------------------------+-------------------------------+     | CXX                                | C++ compiler                  |     +------------------------------------+-------------------------------+     | CXXFLAGS                           | C++ compiler options          |     +------------------------------------+-------------------------------+   > **Note** >      For Windows, these may not work very reliably, so `native files <https://mesonbuild.com/Native-environments.html>`_ are likely the     best bet, or by direct `1.3 Customizing builds`_.  1.2.4 Dependencies ^^^^^^^^^^^^^^^^^^  Here, ``meson`can actually be used to set dependencies more robustly.  .. tab-set::    .. tab-item:: Distutils     :sync: distutils`\`bash python -m numpy.f2py -c fib.f90 -m fib --backend distutils -llapack

> Note that this approach in practice is error prone.
> 
> <div class="tab-item">
> 
> Meson
> 
> </div>
> 
>   - sync  
>     meson
> 
> <!-- end list -->
> 
> ``` bash
> python -m numpy.f2py -c fib.f90 -m fib --backend meson --dep lapack
> ```
> 
> This maps to `dependency("lapack")` and so can be used for a wide variety of dependencies. They can be [customized further](https://mesonbuild.com/Dependencies.html) to use CMake or other systems to resolve dependencies.

1.2.5 Libraries `` ` ^^^^^^^^^^^^^^^  Both ``meson`and`distutils`are capable of linking against libraries.  .. tab-set::    .. tab-item:: Distutils     :sync: distutils`\`bash python -m numpy.f2py -c fib.f90 -m fib --backend distutils -lmylib -L/path/to/mylib

> 
> 
> <div class="tab-item" data-sync="meson">
> 
> Meson
> 
> ``` bash
> python -m numpy.f2py -c fib.f90 -m fib --backend meson -lmylib -L/path/to/mylib
> ```
> 
> </div>

1.3 Customizing builds `` ` ~~~~~~~~~~~~~~~~~~~~~~  .. tab-set::    .. tab-item:: Distutils     :sync: distutils ``\`bash python -m numpy.f2py -c fib.f90 -m fib --backend distutils --build-dir blah

> This can be technically integrated with other codes, see \[f2py-distutils\](\#f2py-distutils).

<div class="tab-item" data-sync="meson">

Meson

``` bash
python -m numpy.f2py -c fib.f90 -m fib --backend meson --build-dir blah
```

The resulting build can be customized via the [Meson Build How-To Guide](https://mesonbuild.com/howtox.html). In fact, the resulting set of files can even be committed directly and used as a meson subproject in a separate codebase.

</div>

\`\`\`

---

distutils.md

---

# Using via <span class="title-ref">numpy.distutils</span>

<div class="legacy">

`distutils` has been removed in favor of `meson` see \[distutils-status-migration\](\#distutils-status-migration).

</div>

<div class="currentmodule">

numpy.distutils.core

</div>

`numpy.distutils` is part of NumPy, and extends the standard Python `distutils` module to deal with Fortran sources and F2PY signature files, e.g. compile Fortran sources, call F2PY to construct extension modules, etc.

<div class="topic">

**Example**

Consider the following `setup_file.py` for the `fib` and `scalar` examples from \[f2py-getting-started\](\#f2py-getting-started) section:

<div class="literalinclude" data-language="python">

./../code/setup\_example.py

</div>

Running

`` `bash   python setup_example.py build  will build two extension modules ``scalar`and`fib2\`\` to the build directory.

</div>

Extensions to `distutils` `` ` ===========================  :mod:`numpy.distutils` extends ``distutils``with the following features:  * `Extension` class argument``sources`may contain Fortran source   files. In addition, the list`sources`may contain at most one   F2PY signature file, and in this case, the name of an Extension module must   match with the`\<modulename\>`used in signature file. It is   assumed that an F2PY signature file contains exactly one`python module`block.    If`sources``do not contain a signature file, then F2PY is used to scan   Fortran source files to construct wrappers to the Fortran codes.    Additional options to the F2PY executable can be given using the   `Extension` class argument``f2py\_options`.  * The following new`distutils`commands are defined:`build\_src`to construct Fortran wrapper extension modules, among many other things.`config\_fc`to change Fortran compiler options.    Additionally, the`build\_ext`and`build\_clib`commands are also enhanced   to support Fortran sources.    Run`\`bash python \<setup.py file\> config\_fc build\_src build\_ext --help

> to see available options for these commands.

  - \* When building Python packages containing Fortran sources, one  
    can choose different Fortran compilers by using the `build_ext` command option `--fcompiler=<Vendor>`. Here `<Vendor>` can be one of the following names (on `linux` systems):
    
        absoft compaq fujitsu g95 gnu gnu95 intel intele intelem lahey nag nagfor nv pathf95 pg vast
    
    See `numpy_distutils/fcompiler.py` for an up-to-date list of supported compilers for different platforms, or run
    
    ``` bash
    python -m numpy.f2py -c --backend distutils --help-fcompiler
    ```

\`\`\`

---

index.md

---

# F2PY and build systems

In this section we will cover the various popular build systems and their usage with `f2py`.

<div class="versionchanged">

NumPy 1.26.x

The default build system for `f2py` has traditionally been through the enhanced `numpy.distutils` module. This module is based on `distutils` which was removed in `Python 3.12.0` in **October 2023**. Like the rest of NumPy and SciPy, `f2py` uses `meson` now, see \[distutils-status-migration\](\#distutils-status-migration) for some more details.

> All changes to `f2py` are tested on SciPy, so their [CI configuration](https://docs.scipy.org/doc/scipy/dev/toolchain.html#official-builds) is always supported.

</div>

\> **Note** \> See \[f2py-meson-distutils\](\#f2py-meson-distutils) for migration information.

## Basic concepts

Building an extension module which includes Python and Fortran consists of:

  - Fortran source(s)
  - One or more generated files from `f2py`
      - A `C` wrapper file is always created
      - Code with modules require an additional `.f90` wrapper
      - Code with functions generate an additional `.f` wrapper
  - `fortranobject.{c,h}`
      - Distributed with `numpy`
      - Can be queried via `python -c "import numpy.f2py; print(numpy.f2py.get_include())"`
  - NumPy headers
      - Can be queried via `python -c "import numpy; print(numpy.get_include())"`
  - Python libraries and development headers

Broadly speaking there are three cases which arise when considering the outputs of `f2py`:

  - Fortran 77 programs
    
      - Input file `blah.f`
      - Generates
          - `blahmodule.c`
          - `blah-f2pywrappers.f`
    
    When no `COMMON` blocks are present only a `C` wrapper file is generated. Wrappers are also generated to rewrite assumed shape arrays as automatic arrays.

  - Fortran 90 programs
    
      - Input file `blah.f90`
      - Generates:
          - `blahmodule.c`
          - `blah-f2pywrappers.f`
          - `blah-f2pywrappers2.f90`
    
    The `f90` wrapper is used to handle code which is subdivided into modules. The `f` wrapper makes `subroutines` for `functions`. It rewrites assumed shape arrays as automatic arrays.

  - Signature files
    
      - Input file `blah.pyf`
      - Generates:
          - `blahmodule.c`
          - `blah-f2pywrappers2.f90` (occasionally)
          - `blah-f2pywrappers.f` (occasionally)
    
    Signature files `.pyf` do not signal their language standard via the file extension, they may generate the F90 and F77 specific wrappers depending on their contents; which shifts the burden of checking for generated files onto the build system.

<div class="versionchanged">

NumPy `1.22.4`

`f2py` will deterministically generate wrapper files based on the input file Fortran standard (F77 or greater). `--skip-empty-wrappers` can be passed to `f2py` to restore the previous behaviour of only generating wrappers when needed by the input .

</div>

In theory keeping the above requirements in hand, any build system can be adapted to generate `f2py` extension modules. Here we will cover a subset of the more popular systems.

<div class="note">

<div class="title">

Note

</div>

`make` has no place in a modern multi-language setup, and so is not discussed further.

</div>

## Build systems

<div class="toctree" data-maxdepth="2">

distutils meson cmake skbuild distutils-to-meson

</div>

---

meson.md

---

# Using via `meson`

\> **Note** \> Much of this document is now obsoleted, one can run `f2py` with `--build-dir` to get a skeleton `meson` project with basic dependencies setup.

<div class="versionchanged">

1.26.x

The default build system for `f2py` is now `meson`, see \[distutils-status-migration\](\#distutils-status-migration) for some more details..

</div>

The key advantage gained by leveraging `meson` over the techniques described in \[f2py-distutils\](\#f2py-distutils) is that this feeds into existing systems and larger projects with ease. `meson` has a rather pythonic syntax which makes it more comfortable and amenable to extension for `python` users.

## Fibonacci walkthrough (F77)

We will need the generated `C` wrapper before we can use a general purpose build system like `meson`. We will acquire this by:

`` `bash     python -m numpy.f2py fib1.f -m fib2  Now, consider the following ``meson.build`file for the`fib`and`scalar`  `\` examples from \[f2py-getting-started\](\#f2py-getting-started) section:

<div class="literalinclude">

../code/meson.build

</div>

At this point the build will complete, but the import will fail:

`` `bash    meson setup builddir    meson compile -C builddir    cd builddir    python -c 'import fib2'    Traceback (most recent call last):    File "<string>", line 1, in <module>    ImportError: fib2.cpython-39-x86_64-linux-gnu.so: undefined symbol: FIB_    # Check this isn't a false positive    nm -A fib2.cpython-39-x86_64-linux-gnu.so | grep FIB_    fib2.cpython-39-x86_64-linux-gnu.so: U FIB_  Recall that the original example, as reproduced below, was in SCREAMCASE:  .. literalinclude:: ./../code/fib1.f    :language: fortran  With the standard approach, the subroutine exposed to ``python`is`fib`and`<span class="title-ref"> not </span><span class="title-ref">FIB</span>\`. This means we have a few options. One approach (where possible) is to lowercase the original Fortran file with say:

`` `bash    tr "[:upper:]" "[:lower:]" < fib1.f > fib1.f    python -m numpy.f2py fib1.f -m fib2    meson --wipe builddir    meson compile -C builddir    cd builddir    python -c 'import fib2'  However this requires the ability to modify the source which is not always ``<span class="title-ref"> possible. The easiest way to solve this is to let </span><span class="title-ref">f2py</span>\` deal with it:

`` `bash    python -m numpy.f2py fib1.f -m fib2 --lower    meson --wipe builddir    meson compile -C builddir    cd builddir    python -c 'import fib2'   Automating wrapper generation ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

A major pain point in the workflow defined above, is the manual tracking of inputs. Although it would require more effort to figure out the actual outputs for reasons discussed in \[f2py-bldsys\](\#f2py-bldsys).

\> **Note** \> From NumPy `1.22.4` onwards, `f2py` will deterministically generate wrapper files based on the input file Fortran standard (F77 or greater). `--skip-empty-wrappers` can be passed to `f2py` to restore the previous behaviour of only generating wrappers when needed by the input .

However, we can augment our workflow in a straightforward to take into account files for which the outputs are known when the build system is set up.

<div class="literalinclude">

../code/meson\_upd.build

</div>

This can be compiled and run as before.

`` `bash     rm -rf builddir     meson setup builddir     meson compile -C builddir     cd builddir     python -c "import numpy as np; import fibby; a = np.zeros(9); fibby.fib(a); print (a)"     # [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]  Salient points ``\` ===============

It is worth keeping in mind the following:

  - It is not possible to use SCREAMCASE in this context, so either the contents of the `.f` file or the generated wrapper `.c` needs to be lowered to regular letters; which can be facilitated by the `--lower` option of `F2PY`

---

skbuild.md

---

# Using via `scikit-build`

`scikit-build` provides two separate concepts geared towards the users of Python extension modules.

1.  A `setuptools` replacement (legacy behaviour)
2.  A series of `cmake` modules with definitions which help building Python extensions

\> **Note** \> It is possible to use `scikit-build`'s `cmake` modules to [bypass the cmake setup mechanism](https://scikit-build.readthedocs.io/en/latest/cmake-modules/F2PY.html) completely, and to write targets which call `f2py    -c`. This usage is **not recommended** since the point of these build system documents are to move away from the internal `numpy.distutils` methods.

For situations where no `setuptools` replacements are required or wanted (i.e. if `wheels` are not needed), it is recommended to instead use the vanilla `cmake` setup described in \[f2py-cmake\](\#f2py-cmake).

## Fibonacci walkthrough (F77)

We will consider the `fib` example from \[f2py-getting-started\](\#f2py-getting-started) section.

<div class="literalinclude" data-language="fortran">

./../code/fib1.f

</div>

### `CMake` modules only

Consider using the following `CMakeLists.txt`.

<div class="literalinclude" data-language="cmake">

./../code/CMakeLists\_skbuild.txt

</div>

Much of the logic is the same as in \[f2py-cmake\](\#f2py-cmake), however notably here the appropriate module suffix is generated via `sysconfig.get_config_var("SO")`. The resulting extension can be built and loaded in the standard workflow.

``` bash
ls .
# CMakeLists.txt fib1.f
cmake -S . -B build
cmake --build build
cd build
python -c "import numpy as np; import fibby; a = np.zeros(9); fibby.fib(a); print (a)"
# [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]
```

### `setuptools` replacement

\> **Note** \> **As of November 2021**

> The behavior described here of driving the `cmake` build of a module is considered to be legacy behaviour and should not be depended on.

The utility of `scikit-build` lies in being able to drive the generation of more than extension modules, in particular a common usage pattern is the generation of Python distributables (for example for PyPI).

The workflow with `scikit-build` straightforwardly supports such packaging requirements. Consider augmenting the project with a `setup.py` as defined:

<div class="literalinclude" data-language="python">

./../code/setup\_skbuild.py

</div>

Along with a commensurate `pyproject.toml`

<div class="literalinclude" data-language="toml">

./../code/pyproj\_skbuild.toml

</div>

Together these can build the extension using `cmake` in tandem with other standard `setuptools` outputs. Running `cmake` through `setup.py` is mostly used when it is necessary to integrate with extension modules not built with `cmake`.

``` bash
ls .
# CMakeLists.txt fib1.f pyproject.toml setup.py
python setup.py build_ext --inplace
python -c "import numpy as np; import fibby.fibby; a = np.zeros(9); fibby.fibby.fib(a); print (a)"
# [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]
```

Where we have modified the path to the module as `--inplace` places the extension module in a subfolder.

---

f2py-examples.md

---

# F2PY examples

Below are some examples of F2PY usage. This list is not comprehensive, but can be used as a starting point when wrapping your own code.

\> **Note** \> The best place to look for examples is the [NumPy issue tracker](https://github.com/numpy/numpy/issues?q=is%3Aissue+label%3A%22component%3A+numpy.f2py%22+is%3Aclosed), or the test cases for `f2py`. Some more use cases are in \[f2py-boilerplating\](\#f2py-boilerplating).

## F2PY walkthrough: a basic extension module

### Creating source for a basic extension module

Consider the following subroutine, contained in a file named `add.f`

<div class="literalinclude" data-language="fortran">

./code/add.f

</div>

This routine simply adds the elements in two contiguous arrays and places the result in a third. The memory for all three arrays must be provided by the calling routine. A very basic interface to this routine can be automatically generated by f2py:

    python -m numpy.f2py -m add add.f

This command will produce an extension module named `addmodule.c` in the current directory. This extension module can now be compiled and used from Python just like any other extension module.

### Creating a compiled extension module

You can also get f2py to both compile `add.f` along with the produced extension module leaving only a shared-library extension file that can be imported from Python:

    python -m numpy.f2py -c -m add add.f

This command produces a Python extension module compatible with your platform. This module may then be imported from Python. It will contain a method for each subroutine in `add`. The docstring of each method contains information about how the module method may be called:

`` `python     >>> import add     >>> print(add.zadd.__doc__)     zadd(a,b,c,n)      Wrapper for ``zadd`.      Parameters     ----------     a : input rank-1 array('D') with bounds (*)     b : input rank-1 array('D') with bounds (*)     c : input rank-1 array('D') with bounds (*)     n : input int  Improving the basic interface`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

The default interface is a very literal translation of the Fortran code into Python. The Fortran array arguments are converted to NumPy arrays and the integer argument should be mapped to a `C` integer. The interface will attempt to convert all arguments to their required types (and shapes) and issue an error if unsuccessful. However, because `f2py` knows nothing about the semantics of the arguments (such that `C` is an output and `n` should really match the array sizes), it is possible to abuse this function in ways that can cause Python to crash. For example:

`` `python     >>> add.zadd([1, 2, 3], [1, 2], [3, 4], 1000)  will cause a program crash on most systems. Under the hood, the lists are being ``<span class="title-ref"> converted to arrays but then the underlying </span><span class="title-ref">add</span>\` function is told to cycle way beyond the borders of the allocated memory.

In order to improve the interface, `f2py` supports directives. This is accomplished by constructing a signature file. It is usually best to start from the interfaces that `f2py` produces in that file, which correspond to the default behavior. To get `f2py` to generate the interface file use the `-h` option:

    python -m numpy.f2py -h add.pyf -m add add.f

This command creates the `add.pyf` file in the current directory. The section of this file corresponding to `zadd` is:

<div class="literalinclude" data-language="fortran">

./code/add.pyf

</div>

By placing intent directives and checking code, the interface can be cleaned up quite a bit so the Python module method is both easier to use and more robust to malformed inputs.

<div class="literalinclude" data-language="fortran">

./code/add-edited.pyf

</div>

The intent directive, intent(out) is used to tell f2py that `c` is an output variable and should be created by the interface before being passed to the underlying code. The intent(hide) directive tells f2py to not allow the user to specify the variable, `n`, but instead to get it from the size of `a`. The depend( `a` ) directive is necessary to tell f2py that the value of n depends on the input a (so that it won't try to create the variable n until the variable a is created).

After modifying `add.pyf`, the new Python module file can be generated by compiling both `add.f` and `add.pyf`:

    python -m numpy.f2py -c add.pyf add.f

The new interface's docstring is:

`` `python     >>> import add     >>> print(add.zadd.__doc__)     c = zadd(a,b)      Wrapper for ``zadd`.      Parameters     ----------     a : input rank-1 array('D') with bounds (n)     b : input rank-1 array('D') with bounds (n)      Returns     -------     c : rank-1 array('D') with bounds (n)  Now, the function can be called in a much more robust way:  .. code-block::      >>> add.zadd([1, 2, 3], [4, 5, 6])     array([5.+0.j, 7.+0.j, 9.+0.j])  Notice the automatic conversion to the correct format that occurred.  Inserting directives in Fortran source`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

The robust interface of the previous section can also be generated automatically by placing the variable directives as special comments in the original Fortran code.

\> **Note** \> For projects where the Fortran code is being actively developed, this may be preferred.

Thus, if the source code is modified to contain:

<div class="literalinclude" data-language="fortran">

./code/add-improved.f

</div>

Then, one can compile the extension module using:

    python -m numpy.f2py -c -m add add.f

The resulting signature for the function add.zadd is exactly the same one that was created previously. If the original source code had contained `A(N)` instead of `A(*)` and so forth with `B` and `C`, then nearly the same interface can be obtained by placing the `INTENT(OUT) :: C` comment line in the source code. The only difference is that `N` would be an optional input that would default to the length of `A`.

## A filtering example

This example shows a function that filters a two-dimensional array of double precision floating-point numbers using a fixed averaging filter. The advantage of using Fortran to index into multi-dimensional arrays should be clear from this example.

<div class="literalinclude" data-language="fortran">

./code/filter.f

</div>

This code can be compiled and linked into an extension module named filter using:

    python -m numpy.f2py -c -m filter filter.f

This will produce an extension module in the current directory with a method named `dfilter2d` that returns a filtered version of the input.

## `depends` keyword example

Consider the following code, saved in the file `myroutine.f90`:

<div class="literalinclude" data-language="fortran">

./code/myroutine.f90

</div>

Wrapping this with `python -m numpy.f2py -c myroutine.f90 -m myroutine`, we can do the following in Python:

    >>> import numpy as np
    >>> import myroutine
    >>> x = myroutine.s(2, 3, np.array([5, 6, 7]))
    >>> x
    array([[5., 0., 0.],
           [0., 0., 0.]])

Now, instead of generating the extension module directly, we will create a signature file for this subroutine first. This is a common pattern for multi-step extension module generation. In this case, after running

`` `python     python -m numpy.f2py myroutine.f90 -m myroutine -h myroutine.pyf  the following signature file is generated:  .. literalinclude:: ./code/myroutine.pyf     :language: fortran  Now, if we run ``python -m numpy.f2py -c myroutine.pyf myroutine.f90`we see an`<span class="title-ref"> error; note that the signature file included a </span><span class="title-ref">depend(m,n)</span><span class="title-ref"> statement for </span><span class="title-ref">x</span>\` which is not necessary. Indeed, editing the file above to read

<div class="literalinclude" data-language="fortran">

./code/myroutine-edited.pyf

</div>

and running `f2py -c myroutine.pyf myroutine.f90` yields correct results.

## Read more

  - [Wrapping C codes using f2py](https://scipy.github.io/old-wiki/pages/Cookbook/f2py_and_NumPy.html)
  - [F2py section on the SciPy Cookbook](https://scipy-cookbook.readthedocs.io/items/F2Py.html)
  - [F2py example: Interactive System for Ice sheet Simulation](http://websrv.cs.umt.edu/isis/index.php/F2py_example)
  - ["Interfacing With Other Languages" section on the SciPy Cookbook.](https://scipy-cookbook.readthedocs.io/items/idx_interfacing_with_other_languages.html)

---

f2py-reference.md

---

# F2PY reference manual

<div class="toctree" data-maxdepth="2">

signature-file python-usage buildtools/index advanced/use\_cases.rst advanced/boilerplating.rst f2py-testing

</div>

---

f2py-testing.md

---

# F2PY test suite

F2PY's test suite is present in the directory `numpy/f2py/tests`. Its aim is to ensure that Fortran language features are correctly translated to Python. For example, the user can specify starting and ending indices of arrays in Fortran. This behaviour is translated to the generated CPython library where the arrays strictly start from 0 index.

The directory of the test suite looks like the following:

    ./tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ src
    â”‚   â”œâ”€â”€ abstract_interface
    â”‚   â”œâ”€â”€ array_from_pyobj
    â”‚   â”œâ”€â”€ // ... several test folders
    â”‚   â””â”€â”€ string
    â”œâ”€â”€ test_abstract_interface.py
    â”œâ”€â”€ test_array_from_pyobj.py
    â”œâ”€â”€ // ... several test files
    â”œâ”€â”€ test_symbolic.py
    â””â”€â”€ util.py

Files starting with `test_` contain tests for various aspects of f2py from parsing Fortran files to checking modules' documentation. `src` directory contains the Fortran source files upon which we do the testing. `util.py` contains utility functions for building and importing Fortran modules during test time using a temporary location.

## Adding a test

F2PY's current test suite predates `pytest` and therefore does not use fixtures. Instead, the test files contain test classes that inherit from `F2PyTest` class present in `util.py`.

<div class="literalinclude" data-language="python" data-lines="327-336" data-linenos="">

../../../numpy/f2py/tests/util.py

</div>

This class many helper functions for parsing and compiling test source files. Its child classes can override its `sources` data member to provide their own source files. This superclass will then compile the added source files upon object creation and their functions will be appended to `self.module` data member. Thus, the child classes will be able to access the fortran functions specified in source file by calling `self.module.[fortran_function_name]`.

<div class="versionadded">

v2.0.0b1

</div>

Each of the `f2py` tests should run without failure if no Fortran compilers are present on the host machine. To facilitate this, the `CompilerChecker` is used, essentially providing a `meson` dependent set of utilities namely `has_{c,f77,f90,fortran}_compiler()`.

For the CLI tests in `test_f2py2e`, flags which are expected to call `meson` or otherwise depend on a compiler need to call `compiler_check_f2pycli()` instead of `f2pycli()`.

### Example

Consider the following subroutines, contained in a file named `add-test.f`

<div class="literalinclude" data-language="fortran">

./code/add-test.f

</div>

The first routine <span class="title-ref">addb</span> simply takes an array and increases its elements by 1. The second subroutine <span class="title-ref">addc</span> assigns a new array <span class="title-ref">k</span> with elements greater that the elements of the input array <span class="title-ref">w</span> by 1.

A test can be implemented as follows:

    class TestAdd(util.F2PyTest):
        sources = [util.getpath("add-test.f")]
    
        def test_module(self):
            k = np.array([1, 2, 3], dtype=np.float64)
            w = np.array([1, 2, 3], dtype=np.float64)
            self.module.addb(k)
            assert np.allclose(k, w + 1)
            self.module.addc([w, k])
            assert np.allclose(k, w + 1)

We override the `sources` data member to provide the source file. The source files are compiled and subroutines are attached to module data member when the class object is created. The `test_module` function calls the subroutines and tests their results.

---

f2py-user.md

---

# F2PY user guide

<div class="toctree" data-maxdepth="2">

f2py.getting-started usage f2py-examples

</div>

---

f2py.getting-started.md

---

# Three ways to wrap - getting started

Wrapping Fortran or C functions to Python using F2PY consists of the following steps:

  - Creating the so-called \[signature file \<signature-file\>\](signature file \<signature-file\>.md) that contains descriptions of wrappers to Fortran or C functions, also called the signatures of the functions. For Fortran routines, F2PY can create an initial signature file by scanning Fortran source codes and tracking all relevant information needed to create wrapper functions.
      - Optionally, F2PY-created signature files can be edited to optimize wrapper functions, which can make them "smarter" and more "Pythonic".
  - F2PY reads a signature file and writes a Python C/API module containing Fortran/C/Python bindings.
  - F2PY compiles all sources and builds an extension module containing the wrappers.
      - In building the extension modules, F2PY uses `meson` and used to use `numpy.distutils` For different build systems, see \[f2py-bldsys\](\#f2py-bldsys).

\> **Note** \> See \[f2py-meson-distutils\](\#f2py-meson-distutils) for migration information.

>   - Depending on your operating system, you may need to install the Python development headers (which provide the file `Python.h`) separately. In Linux Debian-based distributions this package should be called `python3-dev`, in Fedora-based distributions it is `python3-devel`. For macOS, depending how Python was installed, your mileage may vary. In Windows, the headers are typically installed already, see \[f2py-windows\](\#f2py-windows).

<div class="note">

<div class="title">

Note

</div>

F2PY supports all the operating systems SciPy is tested on so their [system dependencies panel](http://scipy.github.io/devdocs/building/index.html#system-level-dependencies) is a good reference.

</div>

Depending on the situation, these steps can be carried out in a single composite command or step-by-step; in which case some steps can be omitted or combined with others.

Below, we describe three typical approaches of using F2PY with Fortran 77. These can be read in order of increasing effort, but also cater to different access levels depending on whether the Fortran code can be freely modified.

The following example Fortran 77 code will be used for illustration, save it as `fib1.f`:

<div class="literalinclude" data-language="fortran">

./code/fib1.f

</div>

\> **Note** \> F2PY parses Fortran/C signatures to build wrapper functions to be used with Python. However, it is not a compiler, and does not check for additional errors in source code, nor does it implement the entire language standards. Some errors may pass silently (or as warnings) and need to be verified by the user.

## The quick way

The quickest way to wrap the Fortran subroutine `FIB` for use in Python is to run

    python -m numpy.f2py -c fib1.f -m fib1

or, alternatively, if the `f2py` command-line tool is available,

    f2py -c fib1.f -m fib1

\> **Note** \> Because the `f2py` command might not be available in all system, notably on Windows, we will use the `python -m numpy.f2py` command throughout this guide.

This command compiles and wraps `fib1.f` (`-c`) to create the extension module `fib1.so` (`-m`) in the current directory. A list of command line options can be seen by executing `python -m numpy.f2py`. Now, in Python the Fortran subroutine `FIB` is accessible via `fib1.fib`:

    >>> import numpy as np
    >>> import fib1
    >>> print(fib1.fib.__doc__)
    fib(a,[n])
    
    Wrapper for ``fib``.
    
    Parameters
    ----------
    a : input rank-1 array('d') with bounds (n)
    
    Other parameters
    ----------------
    n : input int, optional
        Default: len(a)
    
    >>> a = np.zeros(8, 'd')
    >>> fib1.fib(a)
    >>> print(a)
    [  0.   1.   1.   2.   3.   5.   8.  13.]

\> **Note** \> \* Note that F2PY recognized that the second argument `n` is the dimension of the first array argument `a`. Since by default all arguments are input-only arguments, F2PY concludes that `n` can be optional with the default value `len(a)`.

>   - One can use different values for optional `n`:
>     
>         >>> a1 = np.zeros(8, 'd')
>         >>> fib1.fib(a1, 6)
>         >>> print(a1)
>         [ 0.  1.  1.  2.  3.  5.  0.  0.]
>     
>     but an exception is raised when it is incompatible with the input array `a`:
>     
>         >>> fib1.fib(a, 10)
>         Traceback (most recent call last):
>           File "<stdin>", line 1, in <module>
>         fib.error: (len(a)>=n) failed for 1st keyword n: fib:n=10
>         >>>
>     
>     F2PY implements basic compatibility checks between related arguments in order to avoid unexpected crashes.
> 
>   - When a NumPy array that is `Fortran <Fortran order>` `contiguous` and has a `dtype` corresponding to a presumed Fortran type is used as an input array argument, then its C pointer is directly passed to Fortran.
>     
>     Otherwise, F2PY makes a contiguous copy (with the proper `dtype`) of the input array and passes a C pointer of the copy to the Fortran subroutine. As a result, any possible changes to the (copy of) input array have no effect on the original argument, as demonstrated below:
>     
>         >>> a = np.ones(8, 'i')
>         >>> fib1.fib(a)
>         >>> print(a)
>         [1 1 1 1 1 1 1 1]
>     
>     Clearly, this is unexpected, as Fortran typically passes by reference. That the above example worked with `dtype=float` is considered accidental.
>     
>     F2PY provides an `intent(inplace)` attribute that modifies the attributes of an input array so that any changes made by the Fortran routine will be reflected in the input argument. For example, if one specifies the `intent(inplace) a` directive (see \[f2py-attributes\](\#f2py-attributes) for details), then the example above would read:
>     
>         >>> a = np.ones(8, 'i')
>         >>> fib1.fib(a)
>         >>> print(a)
>         [  0.   1.   1.   2.   3.   5.   8.  13.]
>     
>     However, the recommended way to have changes made by Fortran subroutine propagate to Python is to use the `intent(out)` attribute. That approach is more efficient and also cleaner.
> 
>   - The usage of `fib1.fib` in Python is very similar to using `FIB` in Fortran. However, using *in situ* output arguments in Python is poor style, as there are no safety mechanisms in Python to protect against wrong argument types. When using Fortran or C, compilers discover any type mismatches during the compilation process, but in Python the types must be checked at runtime. Consequently, using *in situ* output arguments in Python may lead to difficult to find bugs, not to mention the fact that the codes will be less readable when all required type checks are implemented.
> 
> Though the approach to wrapping Fortran routines for Python discussed so far is very straightforward, it has several drawbacks (see the comments above). The drawbacks are due to the fact that there is no way for F2PY to determine the actual intention of the arguments; that is, there is ambiguity in distinguishing between input and output arguments. Consequently, F2PY assumes that all arguments are input arguments by default.
> 
> There are ways (see below) to remove this ambiguity by "teaching" F2PY about the true intentions of function arguments, and F2PY is then able to generate more explicit, easier to use, and less error prone wrappers for Fortran functions.

## The smart way

If we want to have more control over how F2PY will treat the interface to our Fortran code, we can apply the wrapping steps one by one.

  - First, we create a signature file from `fib1.f` by running:
    
        python -m numpy.f2py fib1.f -m fib2 -h fib1.pyf
    
    The signature file is saved to `fib1.pyf` (see the `-h` flag) and its contents are shown below.
    
    <div class="literalinclude" data-language="fortran">
    
    ./code/fib1.pyf
    
    </div>

  - Next, we'll teach F2PY that the argument `n` is an input argument (using the `intent(in)` attribute) and that the result, i.e., the contents of `a` after calling the Fortran function `FIB`, should be returned to Python (using the `intent(out)` attribute). In addition, an array `a` should be created dynamically using the size determined by the input argument `n` (using the `depend(n)` attribute to indicate this dependence relation).
    
    The contents of a suitably modified version of `fib1.pyf` (saved as `fib2.pyf`) are as follows:
    
    <div class="literalinclude" data-language="fortran">
    
    ./code/fib2.pyf
    
    </div>

  - Finally, we build the extension module with `numpy.distutils` by running:
    
        python -m numpy.f2py -c fib2.pyf fib1.f

In Python:

    >>> import fib2
    >>> print(fib2.fib.__doc__)
    a = fib(n)
    
    Wrapper for ``fib``.
    
    Parameters
    ----------
    n : input int
    
    Returns
    -------
    a : rank-1 array('d') with bounds (n)
    
    >>> print(fib2.fib(8))
    [  0.   1.   1.   2.   3.   5.   8.  13.]

\> **Note** \> \* The signature of `fib2.fib` now more closely corresponds to the intention of the Fortran subroutine `FIB`: given the number `n`, `fib2.fib` returns the first `n` Fibonacci numbers as a NumPy array. The new Python signature `fib2.fib` also rules out the unexpected behaviour in `fib1.fib`.

>   - Note that by default, using a single `intent(out)` also implies `intent(hide)`. Arguments that have the `intent(hide)` attribute specified will not be listed in the argument list of a wrapper function.
> 
> For more details, see \[signature-file\](signature-file.md).

## The quick and smart way

The "smart way" of wrapping Fortran functions, as explained above, is suitable for wrapping (e.g. third party) Fortran codes for which modifications to their source codes are not desirable nor even possible.

However, if editing Fortran codes is acceptable, then the generation of an intermediate signature file can be skipped in most cases. F2PY specific attributes can be inserted directly into Fortran source codes using F2PY directives. A F2PY directive consists of special comment lines (starting with `Cf2py` or `!f2py`, for example) which are ignored by Fortran compilers but interpreted by F2PY as normal lines.

Consider a modified version of the previous Fortran code with F2PY directives, saved as `fib3.f`:

<div class="literalinclude" data-language="fortran">

./code/fib3.f

</div>

Building the extension module can be now carried out in one command:

    python -m numpy.f2py -c -m fib3 fib3.f

Notice that the resulting wrapper to `FIB` is as "smart" (unambiguous) as in the previous case:

    >>> import fib3
    >>> print(fib3.fib.__doc__)
    a = fib(n)
    
    Wrapper for ``fib``.
    
    Parameters
    ----------
    n : input int
    
    Returns
    -------
    a : rank-1 array('d') with bounds (n)
    
    >>> print(fib3.fib(8))
    [  0.   1.   1.   2.   3.   5.   8.  13.]

---

index.md

---

# F2PY user guide and reference manual

The purpose of the `F2PY` --*Fortran to Python interface generator*-- utility is to provide a connection between Python and Fortran. F2PY distributed as part of [NumPy](https://www.numpy.org/) (`numpy.f2py`) and once installed is also available as a standalone command line tool. Originally created by Pearu Peterson, and older changelogs are in the [historical reference](https://web.archive.org/web/20140822061353/http://cens.ioc.ee/projects/f2py2e).

F2PY facilitates creating/building native [Python C/API extension modules](https://docs.python.org/3/extending/extending.html#extending-python-with-c-or-c) that make it possible

  - to call Fortran 77/90/95 external subroutines and Fortran 90/95 module subroutines as well as C functions;
  - to access Fortran 77 `COMMON` blocks and Fortran 90/95 module data, including allocatable arrays

from Python.

\> **Note** \> Fortran 77 is essentially feature complete, and an increasing amount of Modern Fortran is supported within F2PY. Most `iso_c_binding` interfaces can be compiled to native extension modules automatically with `f2py`. Bug reports welcome\!

F2PY can be used either as a command line tool `f2py` or as a Python module `numpy.f2py`. While we try to provide the command line tool as part of the numpy setup, some platforms like Windows make it difficult to reliably put the executables on the `PATH`. If the `f2py` command is not available in your system, you may have to run it as a module:

    python -m numpy.f2py

Using the `python -m` invocation is also good practice if you have multiple Python installs with NumPy in your system (outside of virtual environments) and you want to ensure you pick up a particular version of Python/F2PY.

If you run `f2py` with no arguments, and the line `numpy Version` at the end matches the NumPy version printed from `python -m numpy.f2py`, then you can use the shorter version. If not, or if you cannot run `f2py`, you should replace all calls to `f2py` mentioned in this guide with the longer version.

<div class="toctree" data-maxdepth="3">

f2py-user f2py-reference windows/index buildtools/distutils-to-meson

</div>

---

python-usage.md

---

# Using F2PY bindings in Python

In this page, you can find a full description and a few examples of common usage patterns for F2PY with Python and different argument types. For more examples and use cases, see \[f2py-examples\](\#f2py-examples).

## Fortran type objects

All wrappers for Fortran/C routines, common blocks, or Fortran 90 module data generated by F2PY are exposed to Python as `fortran` type objects. Routine wrappers are callable `fortran` type objects while wrappers to Fortran data have attributes referring to data objects.

All `fortran` type objects have an attribute `_cpointer` that contains a :c`PyCapsule` referring to the C pointer of the corresponding Fortran/C function or variable at the C level. Such `PyCapsule` objects can be used as callback arguments for F2PY generated functions to bypass the Python C/API layer for calling Python functions from Fortran or C. This can be useful when the computational aspects of such functions are implemented in C or Fortran and wrapped with F2PY (or any other tool capable of providing the `PyCapsule` containing a function).

Consider a Fortran 77 file `` `ftype.f ``:

<div class="literalinclude" data-language="fortran">

./code/ftype.f

</div>

and a wrapper built using `f2py -c ftype.f -m ftype`.

In Python, you can observe the types of `foo` and `data`, and how to access individual objects of the wrapped Fortran code.

<div class="literalinclude" data-language="python">

./code/results/ftype\_session.dat

</div>

## Scalar arguments

In general, a scalar argument for a F2PY generated wrapper function can be an ordinary Python scalar (integer, float, complex number) as well as an arbitrary sequence object (list, tuple, array, string) of scalars. In the latter case, the first element of the sequence object is passed to the Fortran routine as a scalar argument.

\> **Note** \> \* When type-casting is required and there is possible loss of information via narrowing e.g. when type-casting float to integer or complex to float, F2PY *does not* raise an exception.

>   - For complex to real type-casting only the real part of a complex number is used.
> 
> \* `intent(inout)` scalar arguments are assumed to be array objects in order to have *in situ* changes be effective. It is recommended to use arrays with proper type but also other types work. \[Read more about the intent attribute \<f2py-attributes\>\](\#read-more-about

\-----the-intent-attribute-\<f2py-attributes\>).

Consider the following Fortran 77 code:

<div class="literalinclude" data-language="fortran">

./code/scalar.f

</div>

and wrap it using `f2py -c -m scalar scalar.f`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/scalar\_session.dat

</div>

## String arguments

F2PY generated wrapper functions accept almost any Python object as a string argument, since `str` is applied for non-string objects. Exceptions are NumPy arrays that must have type code `'S1'` or `'b'` (corresponding to the outdated `'c'` or `'1'` typecodes, respectively) when used as string arguments. See \[arrays.scalars\](\#arrays.scalars) for more information on these typecodes.

A string can have an arbitrary length when used as a string argument for an F2PY generated wrapper function. If the length is greater than expected, the string is truncated silently. If the length is smaller than expected, additional memory is allocated and filled with `\0`.

Because Python strings are immutable, an `intent(inout)` argument expects an array version of a string in order to have *in situ* changes be effective.

Consider the following Fortran 77 code:

<div class="literalinclude" data-language="fortran">

./code/string.f

</div>

and wrap it using `f2py -c -m mystring string.f`.

Python session:

<div class="literalinclude" data-language="python">

./code/results/string\_session.dat

</div>

## Array arguments

In general, array arguments for F2PY generated wrapper functions accept arbitrary sequences that can be transformed to NumPy array objects. There are two notable exceptions:

  - `intent(inout)` array arguments must always be `proper-contiguous <contiguous>` and have a compatible `dtype`, otherwise an exception is raised.
  - `intent(inplace)` array arguments will be changed *in situ* if the argument has a different type than expected (see the `intent(inplace)` \[attribute \<f2py-attributes\>\](\#attribute-\<f2py-attributes\>) for more information).

In general, if a NumPy array is `proper-contiguous <contiguous>` and has a proper type then it is directly passed to the wrapped Fortran/C function. Otherwise, an element-wise copy of the input array is made and the copy, being proper-contiguous and with proper type, is used as the array argument.

Usually there is no need to worry about how the arrays are stored in memory and whether the wrapped functions, being either Fortran or C functions, assume one or another storage order. F2PY automatically ensures that wrapped functions get arguments with the proper storage order; the underlying algorithm is designed to make copies of arrays only when absolutely necessary. However, when dealing with very large multidimensional input arrays with sizes close to the size of the physical memory in your computer, then care must be taken to ensure the usage of proper-contiguous and proper type arguments.

To transform input arrays to column major storage order before passing them to Fortran routines, use the function <span class="title-ref">numpy.asfortranarray</span>.

Consider the following Fortran 77 code:

<div class="literalinclude" data-language="fortran">

./code/array.f

</div>

and wrap it using `f2py -c -m arr array.f -DF2PY_REPORT_ON_ARRAY_COPY=1`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/array\_session.dat

</div>

## Call-back arguments

F2PY supports calling Python functions from Fortran or C codes.

Consider the following Fortran 77 code:

<div class="literalinclude" data-language="fortran">

./code/callback.f

</div>

and wrap it using `f2py -c -m callback callback.f`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/callback\_session.dat

</div>

In the above example F2PY was able to guess accurately the signature of the call-back function. However, sometimes F2PY cannot establish the appropriate signature; in these cases the signature of the call-back function must be explicitly defined in the signature file.

To facilitate this, signature files may contain special modules (the names of these modules contain the special `__user__` sub-string) that define the various signatures for call-back functions. Callback arguments in routine signatures have the `external` attribute (see also the `intent(callback)` \[attribute \<f2py-attributes\>\](\#attribute-\<f2py-attributes\>)). To relate a callback argument with its signature in a `__user__` module block, a `use` statement can be utilized as illustrated below. The same signature for a callback argument can be referred to in different routine signatures.

We use the same Fortran 77 code as in the previous example but now we will pretend that F2PY was not able to guess the signatures of call-back arguments correctly. First, we create an initial signature file `callback2.pyf` using F2PY:

    f2py -m callback2 -h callback2.pyf callback.f

Then modify it as follows

  - \! -*- f90 -*-python module \_\_user\_\_routines
    
      - interface
        
          - function fun(i) result (r)  
            integer :: i real\*8 :: r
        
        end function fun
    
    end interface

end python module \_\_user\_\_routines

  - python module callback2
    
      - interface
        
          - subroutine foo(f,r)  
            use \_\_user\_\_routines, f=\>fun external f real\*8 intent(out) :: r
        
        end subroutine foo
    
    end interface

end python module callback2

>   - literal

Finally, we build the extension module using `f2py -c callback2.pyf callback.f`.

An example Python session for this snippet would be identical to the previous example except that the argument names would differ.

Sometimes a Fortran package may require that users provide routines that the package will use. F2PY can construct an interface to such routines so that Python functions can be called from Fortran.

Consider the following Fortran 77 subroutine that takes an array as its input and applies a function `func` to its elements.

<div class="literalinclude" data-language="fortran">

./code/calculate.f

</div>

The Fortran code expects that the function `func` has been defined externally. In order to use a Python function for `func`, it must have an attribute `intent(callback)` and it must be specified before the `external` statement.

Finally, build an extension module using `f2py -c -m foo calculate.f`

In Python:

<div class="literalinclude" data-language="python">

./code/results/calculate\_session.dat

</div>

The function is included as an argument to the python function call to the Fortran subroutine even though it was *not* in the Fortran subroutine argument list. The "external" keyword refers to the C function generated by f2py, not the Python function itself. The python function is essentially being supplied to the C function.

The callback function may also be explicitly set in the module. Then it is not necessary to pass the function in the argument list to the Fortran function. This may be desired if the Fortran function calling the Python callback function is itself called by another Fortran function.

Consider the following Fortran 77 subroutine:

<div class="literalinclude" data-language="fortran">

./code/extcallback.f

</div>

and wrap it using `f2py -c -m pfromf extcallback.f`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/extcallback\_session.dat

</div>

\> **Note** \> When using modified Fortran code via `callstatement` or other directives, the wrapped Python function must be called as a callback, otherwise only the bare Fortran routine will be used. For more details, see <https://github.com/numpy/numpy/issues/26681#issuecomment-2466460943>

### Resolving arguments to call-back functions

F2PY generated interfaces are very flexible with respect to call-back arguments. For each call-back argument an additional optional argument `<name>_extra_args` is introduced by F2PY. This argument can be used to pass extra arguments to user provided call-back functions.

If a F2PY generated wrapper function expects the following call-back argument:

    def fun(a_1,...,a_n):
       ...
       return x_1,...,x_k

but the following Python function

    def gun(b_1,...,b_m):
       ...
       return y_1,...,y_l

is provided by a user, and in addition,

    fun_extra_args = (e_1,...,e_p)

is used, then the following rules are applied when a Fortran or C function evaluates the call-back argument `gun`:

  - If `p == 0` then `gun(a_1, ..., a_q)` is called, here `q = min(m, n)`.
  - If `n + p <= m` then `gun(a_1, ..., a_n, e_1, ..., e_p)` is called.
  - If `p <= m < n + p` then `gun(a_1, ..., a_q, e_1, ..., e_p)` is called, and here `q=m-p`.
  - If `p > m` then `gun(e_1, ..., e_m)` is called.
  - If `n + p` is less than the number of required arguments to `gun` then an exception is raised.

If the function `gun` may return any number of objects as a tuple; then the following rules are applied:

  - If `k < l`, then `y_{k + 1}, ..., y_l` are ignored.
  - If `k > l`, then only `x_1, ..., x_l` are set.

## Common blocks

F2PY generates wrappers to `common` blocks defined in a routine signature block. Common blocks are visible to all Fortran codes linked to the current extension module, but not to other extension modules (this restriction is due to the way Python imports shared libraries). In Python, the F2PY wrappers to `common` blocks are `fortran` type objects that have (dynamic) attributes related to the data members of the common blocks. When accessed, these attributes return as NumPy array objects (multidimensional arrays are Fortran-contiguous) which directly link to data members in common blocks. Data members can be changed by direct assignment or by in-place changes to the corresponding array objects.

Consider the following Fortran 77 code:

<div class="literalinclude" data-language="fortran">

./code/common.f

</div>

and wrap it using `f2py -c -m common common.f`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/common\_session.dat

</div>

## Fortran 90 module data

The F2PY interface to Fortran 90 module data is similar to the handling of Fortran 77 common blocks.

Consider the following Fortran 90 code:

<div class="literalinclude" data-language="fortran">

./code/moddata.f90

</div>

and wrap it using `f2py -c -m moddata moddata.f90`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/moddata\_session.dat

</div>

## Allocatable arrays

F2PY has basic support for Fortran 90 module allocatable arrays.

Consider the following Fortran 90 code:

<div class="literalinclude" data-language="fortran">

./code/allocarr.f90

</div>

and wrap it using `f2py -c -m allocarr allocarr.f90`.

In Python:

<div class="literalinclude" data-language="python">

./code/results/allocarr\_session.dat

</div>

---

signature-file.md

---

# Signature file

The interface definition file (.pyf) is how you can fine-tune the interface between Python and Fortran. The syntax specification for signature files (`.pyf` files) is modeled on the Fortran 90/95 language specification. Almost all Fortran standard constructs are understood, both in free and fixed format (recall that Fortran 77 is a subset of Fortran 90/95). F2PY introduces some extensions to the Fortran 90/95 language specification that help in the design of the Fortran to Python interface, making it more "Pythonic".

Signature files may contain arbitrary Fortran code so that any Fortran 90/95 codes can be treated as signature files. F2PY silently ignores Fortran constructs that are irrelevant for creating the interface. However, this also means that syntax errors are not caught by F2PY and will only be caught when the library is built.

\> **Note** \> Currently, F2PY may fail with some valid Fortran constructs. If this happens, you can check the [NumPy GitHub issue tracker](https://github.com/numpy/numpy/issues) for possible workarounds or work-in-progress ideas.

In general, the contents of the signature files are case-sensitive. When scanning Fortran codes to generate a signature file, F2PY lowers all cases automatically except in multi-line blocks or when the `--no-lower` option is used.

The syntax of signature files is presented below.

## Signature files syntax

### Python module block

A signature file may contain one (recommended) or more `python module` blocks. The `python module` block describes the contents of a Python/C extension module `<modulename>module.c` that F2PY generates.

\> **Warning** \> Exception: if `<modulename>` contains a substring `__user__`, then the corresponding `python module` block describes the signatures of call-back functions (see \[Call-back arguments\](\#call-back-arguments)).

A `python module` block has the following structure:

    python module <modulename>
      [<usercode statement>]...
      [
      interface
        <usercode statement>
        <Fortran block data signatures>
        <Fortran/C routine signatures>
      end [interface]
      ]...
      [
      interface
        module <F90 modulename>
          [<F90 module data type declarations>]
          [<F90 module routine signatures>]
        end [module [<F90 modulename>]]
      end [interface]
      ]...
    end [python module [<modulename>]]

Here brackets `[]` indicate an optional section, dots `...` indicate one or more of a previous section. So, `[]...` is to be read as zero or more of a previous section.

### Fortran/C routine signatures

The signature of a Fortran routine has the following structure:

    [<typespec>] function | subroutine <routine name> \
                  [ ( [<arguments>] ) ] [ result ( <entityname> ) ]
      [<argument/variable type declarations>]
      [<argument/variable attribute statements>]
      [<use statements>]
      [<common block statements>]
      [<other statements>]
    end [ function | subroutine [<routine name>] ]

From a Fortran routine signature F2PY generates a Python/C extension function that has the following signature:

    def <routine name>(<required arguments>[,<optional arguments>]):
         ...
         return <return variables>

The signature of a Fortran block data has the following structure:

    block data [ <block data name> ]
      [<variable type declarations>]
      [<variable attribute statements>]
      [<use statements>]
      [<common block statements>]
      [<include statements>]
    end [ block data [<block data name>] ]

### Type declarations

The definition of the `<argument/variable type declaration>` part is

    <typespec> [ [<attrspec>] :: ] <entitydecl>

where

    <typespec> := byte | character [<charselector>]
               | complex [<kindselector>] | real [<kindselector>]
               | double complex | double precision
               | integer [<kindselector>] | logical [<kindselector>]
    
    <charselector> := * <charlen>
                   | ( [len=] <len> [ , [kind=] <kind>] )
                   | ( kind= <kind> [ , len= <len> ] )
    <kindselector> := * <intlen> | ( [kind=] <kind> )
    
    <entitydecl> := <name> [ [ * <charlen> ] [ ( <arrayspec> ) ]
                          | [ ( <arrayspec> ) ] * <charlen> ]
                         | [ / <init_expr> / | = <init_expr> ] \
                           [ , <entitydecl> ]

and

  - `<attrspec>` is a comma separated list of [attributes](#attributes);
  - `<arrayspec>` is a comma separated list of dimension bounds;
  - `<init_expr>` is a \[C expression \<c-expressions\>\](\#c-expression-\<c-expressions\>);
  - `<intlen>` may be negative integer for `integer` type specifications. In such cases `integer*<negintlen>` represents unsigned C integers;

If an argument has no `<argument type declaration>`, its type is determined by applying `implicit` rules to its name.

### Statements

#### Attribute statements

The `<argument/variable attribute statement>` is similar to the `<argument/variable type declaration>`, but without `<typespec>`.

An attribute statement cannot contain other attributes, and `<entitydecl>` can be only a list of names. See \[f2py-attributes\](\#f2py-attributes) for more details on the attributes that can be used by F2PY.

#### Use statements

  - The definition of the `<use statement>` part is
    
        use <modulename> [ , <rename_list> | , ONLY : <only_list> ]
    
    where
    
        <rename_list> := <local_name> => <use_name> [ , <rename_list> ]

  - Currently F2PY uses `use` statements only for linking call-back modules and `external` arguments (call-back functions). See \[Call-back arguments\](\#call-back-arguments).

#### Common block statements

  - The definition of the `<common block statement>` part is
    
        common / <common name> / <shortentitydecl>
    
    where
    
        <shortentitydecl> := <name> [ ( <arrayspec> ) ] [ , <shortentitydecl> ]

  - If a `python module` block contains two or more `common` blocks with the same name, the variables from the additional declarations are appended. The types of variables in `<shortentitydecl>` are defined using `<argument type declarations>`. Note that the corresponding `<argument type declarations>` may contain array specifications; then these need not be specified in `<shortentitydecl>`.

#### Other statements

  - \* The `<other statement>` part refers to any other Fortran language  
    constructs that are not described above. F2PY ignores most of them except the following:
    
      - `call` statements and function calls of `external` arguments (see \[more details on external arguments \<external\>\](\#more-details-on-external-arguments-\<external\>));
    
      -   - `include` statements
            
                include '<filename>'
                include "<filename>"
            
            If a file `<filename>` does not exist, the `include` statement is ignored. Otherwise, the file `<filename>` is included to a signature file. `include` statements can be used in any part of a signature file, also outside the Fortran/C routine signature blocks.
    
      -   - `implicit` statements
            
                implicit none
        
        implicit \<list of implicit maps\>
        
        > where
        > 
        >     <implicit map> := <typespec> ( <list of letters or range of letters> )
        > 
        > Implicit rules are used to determine the type specification of a variable (from the first-letter of its name) if the variable is not defined using `<variable type declaration>`. Default implicit rules are given by:
        > 
        >     implicit real (a-h,o-z,$_), integer (i-m)
    
      -   - `entry` statements
            
                entry <entry name> [([<arguments>])]
            
            F2PY generates wrappers for all entry names using the signature of the routine block.
            
            \> **Note**

  - \>  
    The `entry` statement can be used to describe the signature of an arbitrary subroutine or function allowing F2PY to generate a number of wrappers from only one routine block signature. There are few restrictions while doing this: `fortranname` cannot be used, `callstatement` and `callprotoargument` can be used only if they are valid for all entry routines, etc.

#### F2PY statements

In addition, F2PY introduces the following statements:

  - `threadsafe`  
    Uses a `Py_BEGIN_ALLOW_THREADS .. Py_END_ALLOW_THREADS` block around the call to Fortran/C function.

  - `callstatement <C-expr|multi-line block>`  
    Replaces the F2PY generated call statement to Fortran/C function with `<C-expr|multi-line block>`. The wrapped Fortran/C function is available as `(*f2py_func)`.
    
    To raise an exception, set `f2py_success = 0` in `<C-expr|multi-line block>`.

  - `callprotoargument <C-typespecs>`  
    When the `callstatement` statement is used, F2PY may not generate proper prototypes for Fortran/C functions (because `<C-expr>` may contain function calls, and F2PY has no way to determine what should be the proper prototype).
    
    With this statement you can explicitly specify the arguments of the corresponding prototype:
    
        extern <return type> FUNC_F(<routine name>,<ROUTINE NAME>)(<callprotoargument>);

  - `fortranname [<actual Fortran/C routine name>]`  
    F2PY allows for the use of an arbitrary `<routine name>` for a given Fortran/C function. Then this statement is used for the `<actual Fortran/C routine name>`.
    
    If `fortranname` statement is used without `<actual Fortran/C routine name>` then a dummy wrapper is generated.

  - `usercode <multi-line block>`  
    When this is used inside a `python module` block, the given C code will be inserted to generated C/API source just before wrapper function definitions.
    
    Here you can define arbitrary C functions to be used for the initialization of optional arguments.
    
    For example, if `usercode` is used twice inside `python module` block then the second multi-line block is inserted after the definition of the external routines.
    
    When used inside `<routine signature>`, then the given C code will be inserted into the corresponding wrapper function just after the declaration of variables but before any C statements. So, the `usercode` follow-up can contain both declarations and C statements.
    
    When used inside the first `interface` block, then the given C code will be inserted at the end of the initialization function of the extension module. This is how the extension modules dictionary can be modified and has many use-cases; for example, to define additional variables.

  - `pymethoddef <multiline block>`  
    This is a multi-line block which will be inserted into the definition of a module methods `PyMethodDef`-array. It must be a comma-separated list of C arrays (see <span class="title-ref">Extending and Embedding</span>\_\_ Python documentation for details). `pymethoddef` statement can be used only inside `python module` block.
    
    \_\_ <https://docs.python.org/extending/index.html>

### Attributes

The following attributes can be used by F2PY.

  - `optional`  
    The corresponding argument is moved to the end of `<optional arguments>` list. A default value for an optional argument can be specified via `<init_expr>` (see the `entitydecl` \[definition \<type-declarations\>\](\#definition-\<type-declarations\>))
    
    \> **Note**

  - \>
    
      - The default value must be given as a valid C expression.
      - Whenever `<init_expr>` is used, the `optional` attribute is set automatically by F2PY.
      - For an optional array argument, all its dimensions must be bounded.

  - `required`  
    The corresponding argument with this attribute is considered mandatory. This is the default. `required` should only be specified if there is a need to disable the automatic `optional` setting when `<init_expr>` is used.
    
    If a Python `None` object is used as a required argument, the argument is treated as optional. That is, in the case of array arguments, the memory is allocated. If `<init_expr>` is given, then the corresponding initialization is carried out.

  - `dimension(<arrayspec>)`  
    The corresponding variable is considered as an array with dimensions given in `<arrayspec>`.

  - `intent(<intentspec>)`  
    This specifies the "intention" of the corresponding argument. `<intentspec>` is a comma separated list of the following keys:
    
      -   - `in`  
            The corresponding argument is considered to be input-only. This means that the value of the argument is passed to a Fortran/C function and that the function is expected to not change the value of this argument.
    
      -   - `inout`  
            The corresponding argument is marked for input/output or as an *in situ* output argument. `intent(inout)` arguments can be only `contiguous` NumPy arrays (in either the Fortran or C sense) with proper type and size. The latter coincides with the default contiguous concept used in NumPy and is effective only if `intent(c)` is used. F2PY assumes Fortran contiguous arguments by default.
            
            <div class="note">
            
            <div class="title">
            
            Note
            
            </div>
            
            Using `intent(inout)` is generally not recommended, as it can cause unexpected results. For example, scalar arguments using `intent(inout)` are assumed to be array objects in order to have *in situ* changes be effective. Use `intent(in,out)` instead.
            
            </div>
            
            See also the `intent(inplace)` attribute.
    
      -   - `inplace`  
            The corresponding argument is considered to be an input/output or *in situ* output argument. `intent(inplace)` arguments must be NumPy arrays of a proper size. If the type of an array is not "proper" or the array is non-contiguous then the array will be modified in-place to fix the type and make it contiguous.
            
            <div class="note">
            
            <div class="title">
            
            Note
            
            </div>
            
            Using `intent(inplace)` is generally not recommended either.
            
            For example, when slices have been taken from an `intent(inplace)` argument then after in-place changes, the data pointers for the slices may point to an unallocated memory area.
            
            </div>
    
      -   - `out`  
            The corresponding argument is considered to be a return variable. It is appended to the `<returned variables>` list. Using `intent(out)` sets `intent(hide)` automatically, unless `intent(in)` or `intent(inout)` are specified as well.
            
            By default, returned multidimensional arrays are Fortran-contiguous. If `intent(c)` attribute is used, then the returned multidimensional arrays are C-contiguous.
    
      -   - `hide`  
            The corresponding argument is removed from the list of required or optional arguments. Typically `intent(hide)` is used with `intent(out)` or when `<init_expr>` completely determines the value of the argument like in the following example:
            
                integer intent(hide),depend(a) :: n = len(a)
                real intent(in),dimension(n) :: a
    
      -   - `c`  
            The corresponding argument is treated as a C scalar or C array argument. For the case of a scalar argument, its value is passed to a C function as a C scalar argument (recall that Fortran scalar arguments are actually C pointer arguments). For array arguments, the wrapper function is assumed to treat multidimensional arrays as C-contiguous arrays.
            
            There is no need to use `intent(c)` for one-dimensional arrays, irrespective of whether the wrapped function is in Fortran or C. This is because the concepts of Fortran- and C contiguity overlap in one-dimensional cases.
            
            If `intent(c)` is used as a statement but without an entity declaration list, then F2PY adds the `intent(c)` attribute to all arguments.
            
            Also, when wrapping C functions, one must use `intent(c)` attribute for `<routine name>` in order to disable Fortran specific `F_FUNC(..,..)` macros.
    
      -   - `cache`  
            The corresponding argument is treated as junk memory. No Fortran nor C contiguity checks are carried out. Using `intent(cache)` makes sense only for array arguments, also in conjunction with `intent(hide)` or `optional` attributes.
    
      -   - `copy`  
            Ensures that the original contents of `intent(in)` argument is preserved. Typically used with the `intent(in,out)` attribute. F2PY creates an optional argument `overwrite_<argument name>` with the default value `0`.
    
      -   - `overwrite`  
            This indicates that the original contents of the `intent(in)` argument may be altered by the Fortran/C function. F2PY creates an optional argument `overwrite_<argument name>` with the default value `1`.
    
      -   - `out=<new name>`  
            Replaces the returned name with `<new name>` in the `__doc__` string of the wrapper function.
    
      -   - `callback`  
            Constructs an external function suitable for calling Python functions from Fortran. `intent(callback)` must be specified before the corresponding `external` statement. If the 'argument' is not in the argument list then it will be added to Python wrapper but only by initializing an external function.
            
            <div class="note">
            
            <div class="title">
            
            Note
            
            </div>
            
            Use `intent(callback)` in situations where the Fortran/C code assumes that the user implemented a function with a given prototype and linked it to an executable. Don't use `intent(callback)` if the function appears in the argument list of a Fortran routine.
            
            </div>
            
            With `intent(hide)` or `optional` attributes specified and using a wrapper function without specifying the callback argument in the argument list; then the call-back function is assumed to be found in the namespace of the F2PY generated extension module where it can be set as a module attribute by a user.
    
      -   - `aux`  
            Defines an auxiliary C variable in the F2PY generated wrapper function. Useful to save parameter values so that they can be accessed in initialization expressions for other variables.
            
            <div class="note">
            
            <div class="title">
            
            Note
            
            </div>
            
            `intent(aux)` silently implies `intent(c)`.
            
            </div>
    
    The following rules apply:
    
      - If none of `intent(in | inout | out | hide)` are specified, `intent(in)` is assumed.
          - `intent(in,inout)` is `intent(in)`;
          - `intent(in,hide)` or `intent(inout,hide)` is `intent(hide)`;
          - `intent(out)` is `intent(out,hide)` unless `intent(in)` or `intent(inout)` is specified.
      - If `intent(copy)` or `intent(overwrite)` is used, then an additional optional argument is introduced with a name `overwrite_<argument name>` and a default value 0 or 1, respectively.
          - `intent(inout,inplace)` is `intent(inplace)`;
          - `intent(in,inplace)` is `intent(inplace)`;
          - `intent(hide)` disables `optional` and `required`.

  - `check([<C-booleanexpr>])`  
    Performs a consistency check on the arguments by evaluating `<C-booleanexpr>`; if `<C-booleanexpr>` returns 0, an exception is raised.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    If `check(..)` is not used then F2PY automatically generates a few standard checks (e.g. in a case of an array argument, it checks for the proper shape and size). Use `check()` to disable checks generated by F2PY.
    
    </div>

  - `depend([<names>])`  
    This declares that the corresponding argument depends on the values of variables in the `<names>` list. For example, `<init_expr>` may use the values of other arguments. Using information given by `depend(..)` attributes, F2PY ensures that arguments are initialized in a proper order. If the `depend(..)` attribute is not used then F2PY determines dependence relations automatically. Use `depend()` to disable the dependence relations generated by F2PY.
    
    When you edit dependence relations that were initially generated by F2PY, be careful not to break the dependence relations of other relevant variables. Another thing to watch out for is cyclic dependencies. F2PY is able to detect cyclic dependencies when constructing wrappers and it complains if any are found.

  - `allocatable`  
    The corresponding variable is a Fortran 90 allocatable array defined as Fortran 90 module data.

<div id="external">

  - `external`  
    The corresponding argument is a function provided by user. The signature of this call-back function can be defined
    
      - in `__user__` module block,
      - or by demonstrative (or real, if the signature file is a real Fortran code) call in the `<other statements>` block.
    
    For example, F2PY generates from:
    
    `` `fortran   external cb_sub, cb_fun   integer n   real a(n),r   call cb_sub(a,n)   r = cb_fun(4)  the following call-back signatures:  .. code-block:: fortran    subroutine cb_sub(a,n)       real dimension(n) :: a       integer optional,check(len(a)>=n),depend(a) :: n=len(a)   end subroutine cb_sub   function cb_fun(e_4_e) result (r)       integer :: e_4_e       real :: r   end function cb_fun  The corresponding user-provided Python function are then:  .. code-block:: python    def cb_sub(a,[n]):       ...       return   def cb_fun(e_4_e):       ...       return r  See also the ``intent(callback)\`\` attribute.

  - `parameter`  
    This indicates that the corresponding variable is a parameter and it must have a fixed value. F2PY replaces all parameter occurrences by their corresponding values.

</div>

Extensions `` ` ----------  F2PY directives ^^^^^^^^^^^^^^^^  The F2PY directives allow using F2PY signature file constructs in Fortran 77/90 source codes. With this feature one  can (almost) completely skip the intermediate signature file generation and apply F2PY directly to Fortran source codes.  F2PY directives have the following form::    <comment char>f2py ...  where allowed comment characters for fixed and free format Fortran codes are ``cC\*\!\#`and`\!`, respectively. Everything that follows`\<comment char\>f2py`is ignored by a compiler but read by F2PY as a normal non-comment  Fortran line:  .. note::   When F2PY finds a line with F2PY directive, the directive is first   replaced by 5 spaces and then the line is reread.  For fixed format Fortran codes,`\<comment char\>`must be at the first column of a file, of course. For free format Fortran codes, the F2PY directives can appear anywhere in a file.  .. _c-expressions:  C expressions ^^^^^^^^^^^^^^  C expressions are used in the following parts of signature files:  *`\<init\_expr\>`for variable initialization; *`\<C-booleanexpr\>`of the`check`attribute; *`\<arrayspec\>`of the`dimension`attribute; *`callstatement`statement, here also a C multi-line block can be used.  A C expression may contain:  * standard C constructs; * functions from`math.h`and`Python.h`; * variables from the argument list, presumably initialized before   according to given dependence relations; * the following CPP macros:`f2py\_rank(\<name\>)`Returns the rank of an array`\<name\>`.`f2py\_shape(\<name\>, \<n\>)`Returns the`\<n\>`-th dimension of an array`\<name\>`.`f2py\_len(\<name\>)`Returns the length of an array`\<name\>`.`f2py\_size(\<name\>)`Returns the size of an array`\<name\>`.`f2py\_itemsize(\<name\>)`Returns the itemsize of an array`\<name\>`.`f2py\_slen(\<name\>)`Returns the length of a string`\<name\>`.   For initializing an array`\<array name\>`, F2PY generates a loop over all indices and dimensions that executes the following pseudo-statement::    <array name>(_i[0],_i[1],...) = <init_expr>;  where`\_i\[\<i\>\]`refers to the`\<i\>`-th index value and that runs from`0`to`shape(\<array name\>,\<i\>)-1`.  For example, a function`myrange(n)`generated from the following signature  .. code-block::         subroutine myrange(a,n)          fortranname        ! myrange is a dummy wrapper          integer intent(in) :: n          real*8 intent(c,out),dimension(n),depend(n) :: a = _i[0]        end subroutine myrange  is equivalent to`numpy.arange(n,dtype=float)`.  > **Warning** >    F2PY may lower cases also in C expressions when scanning Fortran codes   (see`--\[no\]-lower`option).  Multi-line blocks ^^^^^^^^^^^^^^^^^^  A multi-line block starts with`'''`(triple single-quotes) and ends with`'''`in some *strictly* subsequent line.  Multi-line blocks can be used only within .pyf files. The contents of a multi-line block can be arbitrary (except that it cannot contain`'''`) and no transformations (e.g. lowering cases) are applied to it.  Currently, multi-line blocks can be used in the following constructs:  * as a C expression of the`callstatement`statement;  * as a C type specification of the`callprotoargument`statement;  * as a C code block of the`usercode`statement;  * as a list of C arrays of the`pymethoddef\`\` statement;

  - as documentation string.

### Extended char-selector

F2PY extends char-selector specification, usable within a signature file or a F2PY directive, as follows:

    <extended-charselector> := <charselector>
                            | (f2py_len= <len>)

See \[Character Strings\](\#character-strings) for usage.

---

usage.md

---

# Using F2PY

This page contains a reference to all command-line options for the `f2py` command, as well as a reference to internal functions of the `numpy.f2py` module.

## Using `f2py` as a command-line tool

When used as a command-line tool, `f2py` has three major modes, distinguished by the usage of `-c` and `-h` switches.

### 1\. Signature file generation

To scan Fortran sources and generate a signature file, use

`` `sh   f2py -h <filename.pyf> <options> <fortran files>   \     [[ only: <fortran functions>  : ]                \       [ skip: <fortran functions>  : ]]...           \     [<fortran files> ...]  > **Note** >    A Fortran source file can contain many routines, and it is often not   necessary to allow all routines to be usable from Python. In such cases,   either specify which routines should be wrapped (in the ``only: .. :`part)   or which routines F2PY should ignore (in the`skip: .. :`part).    F2PY has no concept of a "per-file"`skip`or`only`list, so if functions   are listed in`only`, no other functions will be taken from any other files.  If`\<filename.pyf\>`is specified as`stdout`, then signatures are written to`\` standard output instead of a file.

Among other options (see below), the following can be used in this mode:

  - `--overwrite-signature`  
    Overwrites an existing signature file.

### 2\. Extension module construction

To construct an extension module, use

`` `sh   f2py -m <modulename> <options> <fortran files>   \     [[ only: <fortran functions>  : ]              \       [ skip: <fortran functions>  : ]]...          \     [<fortran files> ...]  The constructed extension module is saved as ``\<modulename\>module.c`to the`\` current directory.

Here `<fortran files>` may also contain signature files. Among other options (see below), the following options can be used in this mode:

  - `--debug-capi`  
    Adds debugging hooks to the extension module. When using this extension module, various diagnostic information about the wrapper is written to the standard output, for example, the values of variables, the steps taken, etc.

  - `-include'<includefile>'`  
    Add a CPP `#include` statement to the extension module source. `<includefile>` should be given in one of the following forms
    
      - `` `cpp   "filename.ext"   <filename.ext>  The include statement is inserted just before the wrapper functions. This feature enables using arbitrary C functions (defined in ``\<includefile\>`) in F2PY generated wrappers.  .. note:: This option is deprecated. Use`usercode\`\` statement to specify  
        C code snippets directly in signature files.

  - `--[no-]wrap-functions`  
    Create Fortran subroutine wrappers to Fortran functions. `--wrap-functions` is default because it ensures maximum portability and compiler independence.

  - `--[no-]freethreading-compatible`  
    Create a module that declares it does or doesn't require the GIL. The default is `--no-freethreading-compatible` for backwards compatibility. Inspect the fortran code you are wrapping for thread safety issues before passing `--freethreading-compatible`, as `f2py` does not analyze fortran code for thread safety issues.

  - `--include-paths "<path1>:<path2>..."`  
    Search include files from given directories.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    The paths are to be separated by the correct operating system separator :py\`\~os.pathsep\`, that is `:` on Linux / MacOS and `;` on Windows. In `CMake` this corresponds to using `$<SEMICOLON>`.
    
    </div>

  - `--help-link [<list of resources names>]`  
    List system resources found by `numpy_distutils/system_info.py`. For example, try `f2py --help-link lapack_opt`.

3\. Building a module `` ` ~~~~~~~~~~~~~~~~~~~~  To build an extension module, use ``\`sh f2py -c \<options\> \<fortran files\> \[\[ only: \<fortran functions\> : \] \[ skip: \<fortran functions\> : \]\]... \[ \<fortran/c source files\> \] \[ \<.o, .a, .so files\> \]

If `<fortran files>` contains a signature file, then the source for an `` ` extension module is constructed, all Fortran and C sources are compiled, and finally all object and library files are linked to the extension module ``\<modulename\>.so`which is saved into the current directory.  If`\<fortran files\>`does not contain a signature file, then an extension module is constructed by scanning all Fortran source codes for routine signatures, before proceeding to build the extension module.  .. warning::    From Python 3.12 onwards,`distutils`has been removed. Use environment    variables or native files to interact with`meson``instead. See its `FAQ    <https://mesonbuild.com/howtox.html>`__ for more information.  Among other options (see below) and options described for previous modes, the following can be used.  > **Note** >     .. versionchanged:: 1.26.0       There are now two separate build backends which can be used,``distutils`and`meson`. Users are **strongly** recommended to switch to`meson`since it is the default above Python`3.12`.  Common build flags:`--backend \<backend\_type\>`Specify the build backend for the compilation process.  The supported backends   are`meson`and`distutils`.  If not specified, defaults to`distutils`.   On Python 3.12 or higher, the default is`meson`.`--f77flags=\<string\>`Specify F77 compiler flags`--f90flags=\<string\>`Specify F90 compiler flags`--debug`Compile with debugging information`-l\<libname\>`Use the library`\<libname\>`when linking.`-D\<macro\>\[=\<defn=1\>\]`Define macro`\<macro\>`as`\<defn\>`.`-U\<macro\>`Define macro`\<macro\>`  `-I\<dir\>`Append directory`\<dir\>`to the list of directories searched for include   files.`-L\<dir\>`Add directory`\<dir\>`to the list of directories to be searched for`-l`.  The`meson`specific flags are:`--dep \<dependency\>`**meson only**   Specify a meson dependency for the module. This may be passed multiple times   for multiple dependencies. Dependencies are stored in a list for further   processing. Example:`--dep lapack --dep scalapack`This will identify   "lapack" and "scalapack" as dependencies and remove them from argv, leaving a   dependencies list containing ["lapack", "scalapack"].  The older`distutils`flags are:`--help-fcompiler`**no meson**   List the available Fortran compilers.`--fcompiler=\<Vendor\>`**no meson**   Specify a Fortran compiler type by vendor.`--f77exec=\<path\>`**no meson**   Specify the path to a F77 compiler`--f90exec=\<path\>`**no meson**   Specify the path to a F90 compiler`--opt=\<string\>`**no meson**   Specify optimization flags`--arch=\<string\>`**no meson**   Specify architecture specific optimization flags`--noopt`**no meson**   Compile without optimization flags`--noarch`**no meson**   Compile without arch-dependent optimization flags`link-\<resource\>`**no meson**   Link the extension module with <resource> as defined by`numpy\_distutils/system\_info.py`. E.g. to link with optimized LAPACK   libraries (vecLib on MacOSX, ATLAS elsewhere), use`--link-lapack\_opt`.   See also`--help-link`switch.  > **Note** >    The`f2py -c`option must be applied either to an existing`.pyf`file   (plus the source/object/library files) or one must specify the`-m \<modulename\>`option (plus the sources/object/library files). Use one of   the following options:`\`sh f2py -c -m fib1 fib1.f

> or
> 
> ``` sh
> f2py -m fib1 fib1.f -h fib1.pyf
> f2py -c fib1.pyf fib1.f
> ```
> 
> For more information, see the <span class="title-ref">Building C and C++ Extensions</span>\_\_ Python documentation for details.
> 
> \_\_ <https://docs.python.org/3/extending/building.html>

When building an extension module, a combination of the following macros may be `` ` required for non-gcc Fortran compilers: ``\`sh -DPREPEND\_FORTRAN -DNO\_APPEND\_FORTRAN -DUPPERCASE\_FORTRAN

To test the performance of F2PY generated interfaces, use `` ` ``-DF2PY\_REPORT\_ATEXIT`. Then a report of various timings is printed out at the exit of Python. This feature may not work on all platforms, and currently only Linux is supported.  To see whether F2PY generated interface performs copies of array arguments, use`-DF2PY\_REPORT\_ON\_ARRAY\_COPY=\<int\>`. When the size of an array argument is larger than`\<int\>`, a message about the copying is sent to`stderr`.  Other options ~~~~~~~~~~~~~`-m \<modulename\>`Name of an extension module. Default is`untitled`.  .. warning::    Don't use this option if a signature file (`\*.pyf`) is used.     .. versionchanged:: 1.26.3       Will ignore`-m`if a`pyf`file is provided.`--\[no-\]lower`Do [not] lower the cases in`\<fortran files\>`. By default,`--lower`is   assumed with`-h`switch, and`--no-lower`without the`-h`switch.`-include\<header\>`Writes additional headers in the C wrapper, can be passed multiple times,   generates #include <header> each time. Note that this is meant to be passed   in single quotes and without spaces, for example`'-include\<stdbool.h\>'`  `--build-dir \<dirname\>`All F2PY generated files are created in`\<dirname\>`. Default is`tempfile.mkdtemp()`.`--f2cmap \<filename\>`Load Fortran-to-C`KIND`specifications from the given file.`--quiet`Run quietly.`--verbose`Run with extra verbosity.`--skip-empty-wrappers`Do not generate wrapper files unless required by the inputs.   This is a backwards compatibility flag to restore pre 1.22.4 behavior.`-v`Print the F2PY version and exit.  Execute`f2py`without any options to get an up-to-date list of available options.  .. _python-module-numpy.f2py:  Python module`numpy.f2py`============================  > **Warning** >     .. versionchanged:: 2.0.0        There used to be a`f2py.compile`function, which was removed, users       may wrap`python -m numpy.f2py`via`subprocess.run`manually, and       set environment variables to interact with`meson`as required.  When using`numpy.f2py`as a module, the following functions can be invoked.  .. automodule:: numpy.f2py     :members:  Automatic extension module generation =====================================  If you want to distribute your f2py extension module, then you only need to include the .pyf file and the Fortran code. The distutils extensions in NumPy allow you to define an extension module entirely in terms of this interface file. A valid`setup.py`file allowing distribution of the`add.f`module (as part of the package`f2py\_examples`so that it would be loaded as`f2py\_examples.add`) is:`\`python def configuration(parent\_package='', top\_path=None) from numpy.distutils.misc\_util import Configuration config = Configuration('f2py\_examples',parent\_package, top\_path) config.add\_extension('add', sources=\['add.pyf','add.f'\]) return config

>   - if \_\_name\_\_ == '\_\_main\_\_':  
>     from numpy.distutils.core import setup setup(\*\*configuration(top\_path='').todict())

Installation of the new package is easy using:

    pip install .

assuming you have the proper permissions to write to the main site-`` ` packages directory for the version of Python you are using. For the resulting package to work, you need to create a file named ``\_\_init\_\_.py`(in the same directory as`add.pyf`). Notice the extension module is defined entirely in terms of the`add.pyf`and`add.f\`<span class="title-ref"> files. The conversion of the .pyf file to a .c file is handled by \`numpy.distutils</span>.

---

conda.md

---

# F2PY and Conda on Windows

As a convenience measure, we will additionally assume the existence of `scoop`, which can be used to install tools without administrative access.

`` `powershell   Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh')  Now we will setup a ``conda`environment.  .. code-block:: powershell      scoop install miniconda3     # For conda activate / deactivate in powershell     conda install -n root -c pscondaenvs pscondaenvs     Powershell -c Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser     conda init powershell     # Open a new shell for the rest`conda`pulls packages from`msys2``, however, the UX is sufficiently different enough to warrant a separate discussion.  > **Warning** >      As of 30-01-2022, the `MSYS2 binaries`_ shipped with``conda`are **outdated** and this approach is **not preferred**.`\`

---

index.md

---

# F2PY and Windows

\> **Warning** \> F2PY support for Windows is not always at par with Linux support

<div class="note">

<div class="title">

Note

</div>

[ScPy's documentation]() has some information on system-level dependencies which are well tested for Fortran as well.

</div>

Broadly speaking, there are two issues working with F2PY on Windows:

  - the lack of actively developed FOSS Fortran compilers, and,
  - the linking issues related to the C runtime library for building Python-C extensions.

The focus of this section is to establish a guideline for developing and extending Fortran modules for Python natively, via F2PY on Windows.

Currently supported toolchains are:

  - Mingw-w64 C/C++/Fortran compilers
  - Intel compilers
  - Clang-cl + Flang
  - MSVC + Flang

## Overview

From a user perspective, the most UNIX compatible Windows development environment is through emulation, either via the Windows Subsystem on Linux, or facilitated by Docker. In a similar vein, traditional virtualization methods like VirtualBox are also reasonable methods to develop UNIX tools on Windows.

Native Windows support is typically stunted beyond the usage of commercial compilers. However, as of 2022, most commercial compilers have free plans which are sufficient for general use. Additionally, the Fortran language features supported by `f2py` (partial coverage of Fortran 2003), means that newer toolchains are often not required. Briefly, then, for an end user, in order of use:

  - Classic Intel Compilers (commercial)  
    These are maintained actively, though licensing restrictions may apply as further detailed in \[f2py-win-intel\](\#f2py-win-intel).
    
    Suitable for general use for those building native Windows programs by building off of MSVC.

  - MSYS2 (FOSS)  
    In conjunction with the `mingw-w64` project, `gfortran` and `gcc` toolchains can be used to natively build Windows programs.

  - Windows Subsystem for Linux  
    Assuming the usage of `gfortran`, this can be used for cross-compiling Windows applications, but is significantly more complicated.

  - Conda  
    Windows support for compilers in `conda` is facilitated by pulling MSYS2 binaries, however these [are outdated](), and therefore not recommended (as of 30-01-2022).

  - PGI Compilers (commercial)  
    Unmaintained but sufficient if an existing license is present. Works natively, but has been superseded by the Nvidia HPC SDK, with no [native Windows support]().

  - Cygwin (FOSS)  
    Can also be used for `gfortran`. However, the POSIX API compatibility layer provided by Cygwin is meant to compile UNIX software on Windows, instead of building native Windows programs. This means cross compilation is required.

The compilation suites described so far are compatible with the [now deprecated]() `np.distutils` build backend which is exposed by the F2PY CLI. Additional build system usage (`meson`, `cmake`) as described in \[f2py-bldsys\](\#f2py-bldsys) allows for a more flexible set of compiler backends including:

  - Intel oneAPI  
    The newer Intel compilers (`ifx`, `icx`) are based on LLVM and can be used for native compilation. Licensing requirements can be onerous.

  - Classic Flang (FOSS)  
    The backbone of the PGI compilers were cannibalized to form the "classic" or [legacy version of Flang](). This may be compiled from source and used natively. [LLVM Flang]() does not support Windows yet (30-01-2022).

  - LFortran (FOSS)  
    One of two LLVM based compilers. Not all of F2PY supported Fortran can be compiled yet (30-01-2022) but uses MSVC for native linking.

## Baseline

For this document we will assume the following basic tools:

  - The IDE being considered is the community supported [Microsoft Visual Studio Code]()

  - The terminal being used is the [Windows Terminal]()

  - The shell environment is assumed to be [Powershell 7.x]()

  -   - Python 3.10 from [the Microsoft Store](https://www.microsoft.com/en-us/p/python-310/9pjpw5ldxlz5) and this can be tested with  
        `Get-Command python.exe` resolving to `C:\Users\$USERNAME\AppData\Local\Microsoft\WindowsApps\python.exe`

  - The Microsoft Visual C++ (MSVC) toolset

With this baseline configuration, we will further consider a configuration matrix as follows:

<div id="table-f2py-winsup-mat">

| **Fortran Compiler** | **C/C++ Compiler** | **Source**     |
| -------------------- | ------------------ | -------------- |
| Intel Fortran        | MSVC / ICC         | exe            |
| GFortran             | MSVC               | MSYS2/exe      |
| GFortran             | GCC                | WSL            |
| Classic Flang        | MSVC               | Source / Conda |
| Anaconda GFortran    | Anaconda GCC       | exe            |

Support matrix, exe implies a Windows installer

</div>

For an understanding of the key issues motivating the need for such a matrix [Pauli Virtanen's in-depth post on wheels with Fortran for Windows]() is an excellent resource. An entertaining explanation of an application binary interface (ABI) can be found in this post by [JeanHeyd Meneide]().

## PowerShell and MSVC

MSVC is installed either via the Visual Studio Bundle or the lighter (preferred) [Build Tools for Visual Studio]() with the `Desktop development with C++` setting.

\> **Note** \> This can take a significant amount of time as it includes a download of around 2GB and requires a restart.

It is possible to use the resulting environment from a [standard command prompt](). However, it is more pleasant to use a [developer powershell](), with a [profile in Windows Terminal](). This can be achieved by adding the following block to the `profiles->list` section of the JSON file used to configure Windows Terminal (see `Settings->Open JSON file`):

`` `json   {   "name": "Developer PowerShell for VS 2019",   "commandline": "powershell.exe -noe -c \"$vsPath = (Join-Path ${env:ProgramFiles(x86)} -ChildPath 'Microsoft Visual Studio\\2019\\BuildTools'); Import-Module (Join-Path $vsPath 'Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll'); Enter-VsDevShell -VsInstallPath $vsPath -SkipAutomaticLocation\"",   "icon": "ms-appx:///ProfileIcons/{61c54bbd-c2c6-5271-96e7-009a87ff44bf}.png"   }  Now, testing the compiler toolchain could look like:  .. code-block:: powershell     # New Windows Developer Powershell instance / tab    # or    $vsPath = (Join-Path ${env:ProgramFiles(x86)} -ChildPath 'Microsoft Visual Studio\\2019\\BuildTools');     Import-Module (Join-Path $vsPath 'Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll');    Enter-VsDevShell -VsInstallPath $vsPath -SkipAutomaticLocation    **********************************************************************    ** Visual Studio 2019 Developer PowerShell v16.11.9    ** Copyright (c) 2021 Microsoft Corporation    **********************************************************************    cd $HOME    echo "#include<stdio.h>" > blah.cpp; echo 'int main(){printf("Hi");return 1;}' >> blah.cpp    cl blah.cpp   .\blah.exe    # Hi    rm blah.cpp  It is also possible to check that the environment has been updated correctly ``<span class="title-ref"> with </span><span class="title-ref">$ENV:PATH</span>\`.

## Microsoft Store Python paths

The MS Windows version of Python discussed here installs to a non-deterministic path using a hash. This needs to be added to the `PATH` variable.

`` `powershell    $Env:Path += ";$env:LOCALAPPDATA\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\scripts"  .. toctree::    :maxdepth: 2     intel    msys2    conda    pgi ``\` .. \_Microsoft Visual Studio Code: <https://code.visualstudio.com/Download> .. \_more complete POSIX environment: <https://www.cygwin.com/> .. \_This MSYS2 document: <https://www.msys2.org/wiki/How-does-MSYS2-differ-from-Cygwin/> .. \_Build Tools for Visual Studio: <https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019> .. \_Windows Terminal: <https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab> .. \_Powershell 7.x: <https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.1> .. \_standard command prompt: <https://docs.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-160#developer_command_file_locations> .. \_developer powershell: <https://docs.microsoft.com/en-us/visualstudio/ide/reference/command-prompt-powershell?view=vs-2019> .. \_profile in Windows Terminal: <https://techcommunity.microsoft.com/t5/microsoft-365-pnp-blog/add-developer-powershell-and-developer-command-prompt-for-visual/ba-p/2243078> .. \_Pauli Virtanen's in-depth post on wheels with Fortran for Windows: <https://pav.iki.fi/blog/2017-10-08/pywingfortran.html#building-python-wheels-with-fortran-for-windows> .. \_Nvidia HPC SDK: <https://www.pgroup.com/index.html> .. \_JeanHeyd Meneide: <https://thephd.dev/binary-banshees-digital-demons-abi-c-c++-help-me-god-please> .. \_legacy version of Flang: <https://github.com/flang-compiler/flang> .. \_native Windows support: <https://developer.nvidia.com/nvidia-hpc-sdk-downloads#collapseFour> .. \_are outdated: <https://github.com/conda-forge/conda-forge.github.io/issues/1044> .. \_now deprecated: <https://github.com/numpy/numpy/pull/20875> .. \_LLVM Flang: <https://releases.llvm.org/11.0.0/tools/flang/docs/ReleaseNotes.html> .. \_ScPy's documentation: <http://scipy.github.io/devdocs/building/index.html#system-level-dependencies>

---

intel.md

---

# F2PY and Windows Intel Fortran

As of NumPy 1.23, only the classic Intel compilers (`ifort`) are supported.

\> **Note** \> The licensing restrictions for beta software [have been relaxed](https://www.intel.com/content/www/us/en/developer/articles/release-notes/oneapi-fortran-compiler-release-notes.html) during the transition to the LLVM backed `ifx/icc` family of compilers. However this document does not endorse the usage of Intel in downstream projects due to the issues pertaining to [disassembly of components and liability]().

> Neither the Python Intel installation nor the <span class="title-ref">Classic Intel C/C++ Compiler</span> are required.

  - The [Intel Fortran Compilers]() come in a combined installer providing both Classic and Beta versions; these also take around a gigabyte and a half or so.

We will consider the classic example of the generation of Fibonnaci numbers, `fib1.f`, given by:

<div class="literalinclude" data-language="fortran">

../code/fib1.f

</div>

For `cmd.exe` fans, using the Intel oneAPI command prompt is the easiest approach, as it loads the required environment for both `ifort` and `msvc`. Helper batch scripts are also provided.

`` `bat    # cmd.exe    "C:\Program Files (x86)\Intel\oneAPI\setvars.bat"    python -m numpy.f2py -c fib1.f -m fib1    python -c "import fib1; import numpy as np; a=np.zeros(8); fib1.fib(a); print(a)"  Powershell usage is a little less pleasant, and this configuration now works with MSVC as:  .. code-block:: powershell     # Powershell    python -m numpy.f2py -c fib1.f -m fib1 --f77exec='C:\Program Files (x86)\Intel\oneAPI\compiler\latest\windows\bin\intel64\ifort.exe' --f90exec='C:\Program Files (x86)\Intel\oneAPI\compiler\latest\windows\bin\intel64\ifort.exe' -L'C:\Program Files (x86)\Intel\oneAPI\compiler\latest\windows\compiler\lib\ia32'    python -c "import fib1; import numpy as np; a=np.zeros(8); fib1.fib(a); print(a)"    # Alternatively, set environment and reload Powershell in one line    cmd.exe /k '"C:\Program Files (x86)\Intel\oneAPI\setvars.bat" && powershell'    python -m numpy.f2py -c fib1.f -m fib1    python -c "import fib1; import numpy as np; a=np.zeros(8); fib1.fib(a); print(a)"  Note that the actual path to your local installation of `ifort` may vary, and the command above will need to be updated accordingly. ``\` .. \_disassembly of components and liability: <https://software.sintel.com/content/www/us/en/develop/articles/end-user-license-agreement.html> .. \_Intel Fortran Compilers: <https://www.intel.com/content/www/us/en/developer/articles/tool/oneapi-standalone-components.html#inpage-nav-6-1> .. \_Classic Intel C/C++ Compiler: <https://www.intel.com/content/www/us/en/developer/articles/tool/oneapi-standalone-components.html#inpage-nav-6-undefined>

---

msys2.md

---

# F2PY and Windows with MSYS2

Follow the standard [installation instructions](https://www.msys2.org/). Then, to grab the requisite Fortran compiler with `MVSC`:

`` `bash    # Assuming a fresh install    pacman -Syu # Restart the terminal    pacman -Su  # Update packages    # Get the toolchains    pacman -S --needed base-devel gcc-fortran    pacman -S mingw-w64-x86_64-toolchain ``\`

---

pgi.md

---

# F2PY and PGI Fortran on Windows

A variant of these are part of the so called "classic" Flang, however, as classic Flang requires a custom LLVM and compilation from sources.

\> **Warning** \> Since the proprietary compilers are no longer available for usage they are not recommended and will not be ported to the new `f2py` CLI.

\> **Note** \> **As of November 2021**

> As of 29-01-2022, [PGI compiler toolchains](https://www.pgroup.com/index.html) have been superseded by the Nvidia HPC SDK, with no [native Windows support](https://developer.nvidia.com/nvidia-hpc-sdk-downloads#collapseFour).

---

getting_started.md

---

- orphan

# Getting started

---

glossary.md

---

# Glossary

<div class="glossary">

  - (<span class="title-ref">n</span>,)  
    A parenthesized number followed by a comma denotes a tuple with one element. The trailing comma distinguishes a one-element tuple from a parenthesized `n`.

  - \-1
    
      - **In a dimension entry**, instructs NumPy to choose the length that will keep the total number of array elements the same.
        
        > \>\>\> np.arange(12).reshape(4, -1).shape (4, 3)
    
      - **In an index**, any negative value [denotes](https://docs.python.org/dev/faq/programming.html#what-s-a-negative-index) indexing from the right.

  - . . .  
    An :py\`Ellipsis\`.
    
      - **When indexing an array**, shorthand that the missing axes, if they exist, are full slices.
        
        > \>\>\> a = np.arange(24).reshape(2,3,4)
        > 
        > \>\>\> a\[...\].shape (2, 3, 4)
        > 
        > \>\>\> a\[...,0\].shape (2, 3)
        > 
        > \>\>\> a\[0,...\].shape (3, 4)
        > 
        > \>\>\> a\[0,...,0\].shape (3,)
        
        It can be used at most once; `a[...,0,...]` raises an <span class="title-ref">IndexError</span>.
    
      - **In printouts**, NumPy substitutes `...` for the middle elements of large arrays. To see the entire array, use <span class="title-ref">numpy.printoptions</span>

  - :  
    The Python `python:slice` operator. In ndarrays, slicing can be applied to every axis:
    
    > \>\>\> a = np.arange(24).reshape(2,3,4) \>\>\> a array(\[\[\[ 0, 1, 2, 3\], \[ 4, 5, 6, 7\], \[ 8, 9, 10, 11\]\], \<BLANKLINE\> \[\[12, 13, 14, 15\], \[16, 17, 18, 19\], \[20, 21, 22, 23\]\]\]) \<BLANKLINE\> \>\>\> a\[1:,-2:,:-1\] array(\[\[\[16, 17, 18\], \[20, 21, 22\]\]\])
    
    Trailing slices can be omitted: :
    
        >>> a[1] == a[1,:,:]
        array([[ True,  True,  True,  True],
               [ True,  True,  True,  True],
               [ True,  True,  True,  True]])
    
    In contrast to Python, where slicing creates a copy, in NumPy slicing creates a `view`.
    
    For details, see \[combining-advanced-and-basic-indexing\](\#combining-advanced-and-basic-indexing).

  - \<  
    In a dtype declaration, indicates that the data is `little-endian` (the bracket is big on the right). :
    
        >>> dt = np.dtype('<f')  # little-endian single-precision float

  - \>  
    In a dtype declaration, indicates that the data is `big-endian` (the bracket is big on the left). :
    
        >>> dt = np.dtype('>H')  # big-endian unsigned short

  - advanced indexing  
    Rather than using a \[scalar \<reference/arrays.scalars\>\](scalar \<reference/arrays.scalars\>.md) or slice as an index, an axis can be indexed with an array, providing fine-grained selection. This is known as \[advanced indexing\<advanced-indexing\>\](\#advanced-indexing\<advanced-indexing\>) or "fancy indexing".

  - along an axis  
    An operation <span class="title-ref">along axis n</span> of array `a` behaves as if its argument were an array of slices of `a` where each slice has a successive index of axis <span class="title-ref">n</span>.
    
    For example, if `a` is a 3 x <span class="title-ref">N</span> array, an operation along axis 0 behaves as if its argument were an array containing slices of each row:
    
    > \>\>\> np.array((a\[0,:\], a\[1,:\], a\[2,:\])) \#doctest: +SKIP
    
    To make it concrete, we can pick the operation to be the array-reversal function <span class="title-ref">numpy.flip</span>, which accepts an `axis` argument. We construct a 3 x 4 array `a`:
    
    > \>\>\> a = np.arange(12).reshape(3,4) \>\>\> a array(\[\[ 0, 1, 2, 3\], \[ 4, 5, 6, 7\], \[ 8, 9, 10, 11\]\])
    
    Reversing along axis 0 (the row axis) yields
    
    > \>\>\> np.flip(a,axis=0) array(\[\[ 8, 9, 10, 11\], \[ 4, 5, 6, 7\], \[ 0, 1, 2, 3\]\])
    
    Recalling the definition of <span class="title-ref">along an axis</span>, `flip` along axis 0 is treating its argument as if it were
    
    > \>\>\> np.array((a\[0,:\], a\[1,:\], a\[2,:\])) array(\[\[ 0, 1, 2, 3\], \[ 4, 5, 6, 7\], \[ 8, 9, 10, 11\]\])
    
    and the result of `np.flip(a,axis=0)` is to reverse the slices:
    
    > \>\>\> np.array((a\[2,:\],a\[1,:\],a\[0,:\])) array(\[\[ 8, 9, 10, 11\], \[ 4, 5, 6, 7\], \[ 0, 1, 2, 3\]\])

  - array  
    Used synonymously in the NumPy docs with `ndarray`.

  - array\_like  
    Any \[scalar \<reference/arrays.scalars\>\](scalar \<reference/arrays.scalars\>.md) or `python:sequence` that can be interpreted as an ndarray. In addition to ndarrays and scalars this category includes lists (possibly nested and with different element types) and tuples. Any argument accepted by \[numpy.array \<reference/generated/numpy.array\>\](numpy.array \<reference/generated/numpy.array\>.md) is array\_like. :
    
        >>> a = np.array([[1, 2.0], [0, 0], (1+1j, 3.)])
        
        >>> a
        array([[1.+0.j, 2.+0.j],
               [0.+0.j, 0.+0.j],
               [1.+1.j, 3.+0.j]])

  - array scalar  
    An \[array scalar \<reference/arrays.scalars\>\](array scalar \<reference/arrays.scalars\>.md) is an instance of the types/classes float32, float64, etc.. For uniformity in handling operands, NumPy treats a scalar as an array of zero dimension. In contrast, a 0-dimensional array is an \[ndarray \<reference/arrays.ndarray\>\](ndarray \<reference/arrays.ndarray\>.md) instance containing precisely one value.

  - axis  
    Another term for an array dimension. Axes are numbered left to right; axis 0 is the first element in the shape tuple.
    
    In a two-dimensional vector, the elements of axis 0 are rows and the elements of axis 1 are columns.
    
    In higher dimensions, the picture changes. NumPy prints higher-dimensional vectors as replications of row-by-column building blocks, as in this three-dimensional vector:
    
    > \>\>\> a = np.arange(12).reshape(2,2,3) \>\>\> a array(\[\[\[ 0, 1, 2\], \[ 3, 4, 5\]\], \[\[ 6, 7, 8\], \[ 9, 10, 11\]\]\])
    
    `a` is depicted as a two-element array whose elements are 2x3 vectors. From this point of view, rows and columns are the final two axes, respectively, in any shape.
    
    This rule helps you anticipate how a vector will be printed, and conversely how to find the index of any of the printed elements. For instance, in the example, the last two values of 8's index must be 0 and 2. Since 8 appears in the second of the two 2x3's, the first index must be 1:
    
    > \>\>\> a\[1,0,2\] 8
    
    A convenient way to count dimensions in a printed vector is to count `[` symbols after the open-parenthesis. This is useful in distinguishing, say, a (1,2,3) shape from a (2,3) shape:
    
    > \>\>\> a = np.arange(6).reshape(2,3) \>\>\> a.ndim 2 \>\>\> a array(\[\[0, 1, 2\], \[3, 4, 5\]\])
    > 
    > \>\>\> a = np.arange(6).reshape(1,2,3) \>\>\> a.ndim 3 \>\>\> a array(\[\[\[0, 1, 2\], \[3, 4, 5\]\]\])

.base

> If an array does not own its memory, then its \[base \<reference/generated/numpy.ndarray.base\>\](base \<reference/generated/numpy.ndarray.base\>.md) attribute returns the object whose memory the array is referencing. That object may be referencing the memory from still another object, so the owning object may be `a.base.base.base...`. Some writers erroneously claim that testing `base` determines if arrays are `view`s. For the correct way, see <span class="title-ref">numpy.shares\_memory</span>.

  - big-endian  
    See [Endianness](https://en.wikipedia.org/wiki/Endianness).

  - BLAS  
    [Basic Linear Algebra Subprograms](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)

  - broadcast  
    *broadcasting* is NumPy's ability to process ndarrays of different sizes as if all were the same size.
    
    It permits an elegant do-what-I-mean behavior where, for instance, adding a scalar to a vector adds the scalar value to every element.
    
    > \>\>\> a = np.arange(3) \>\>\> a array(\[0, 1, 2\])
    > 
    > \>\>\> a + \[3, 3, 3\] array(\[3, 4, 5\])
    > 
    > \>\>\> a + 3 array(\[3, 4, 5\])
    
    Ordinarily, vector operands must all be the same size, because NumPy works element by element -- for instance, `c = a * b` is :
    
        c[0,0,0] = a[0,0,0] * b[0,0,0]
        c[0,0,1] = a[0,0,1] * b[0,0,1]
        ...
    
    But in certain useful cases, NumPy can duplicate data along "missing" axes or "too-short" dimensions so shapes will match. The duplication costs no memory or time. For details, see \[Broadcasting. \<user/basics.broadcasting\>\](Broadcasting. \<user/basics.broadcasting\>.md)

  - C order  
    Same as `row-major`.

  - casting  
    The process of converting array data from one dtype to another. There exist several casting modes, defined by the following casting rules:
    
      - `no`: The data types should not be cast at all. Any mismatch in data types between the arrays will raise a <span class="title-ref">TypeError</span>.
      - `equiv`: Only byte-order changes are allowed.
      - `safe`: Only casts that can preserve values are allowed. Upcasting (e.g., from int to float) is allowed, but downcasting is not.
      - `same_kind`: The 'same\_kind' casting option allows safe casts and casts within a kind, like float64 to float32.
      - `unsafe`: any data conversions may be done.

  - column-major  
    See [Row- and column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order).

contiguous

> An array is contiguous if:
> 
>   - it occupies an unbroken block of memory, and
>   - array elements with higher indexes occupy higher addresses (that is, no `stride` is negative).
> 
> There are two types of proper-contiguous NumPy arrays:
> 
>   - Fortran-contiguous arrays refer to data that is stored column-wise, i.e. the indexing of data as stored in memory starts from the lowest dimension;
>   - C-contiguous, or simply contiguous arrays, refer to data that is stored row-wise, i.e. the indexing of data as stored in memory starts from the highest dimension.
> 
> For one-dimensional arrays these notions coincide.
> 
> For example, a 2x2 array `A` is Fortran-contiguous if its elements are stored in memory in the following order:
> 
>     A[0,0] A[1,0] A[0,1] A[1,1]
> 
> and C-contiguous if the order is as follows:
> 
>     A[0,0] A[0,1] A[1,0] A[1,1]
> 
> To test whether an array is C-contiguous, use the `.flags.c_contiguous` attribute of NumPy arrays. To test for Fortran contiguity, use the `.flags.f_contiguous` attribute.

  - copy  
    See `view`.

  - dimension  
    See `axis`.

  - dtype  
    The datatype describing the (identically typed) elements in an ndarray. It can be changed to reinterpret the array contents. For details, see \[Data type objects (dtype). \<reference/arrays.dtypes\>\](Data type objects (dtype). \<reference/arrays.dtypes\>.md)

  - fancy indexing  
    Another term for `advanced indexing`.

  - field  
    In a `structured data type`, each subtype is called a <span class="title-ref">field</span>. The <span class="title-ref">field</span> has a name (a string), a type (any valid dtype), and an optional <span class="title-ref">title</span>. See \[arrays.dtypes\](\#arrays.dtypes).

  - Fortran order  
    Same as `column-major`.

  - flattened  
    See `ravel`.

  - homogeneous  
    All elements of a homogeneous array have the same type. ndarrays, in contrast to Python lists, are homogeneous. The type can be complicated, as in a `structured array`, but all elements have that type.
    
    NumPy [object arrays](#term-object-array), which contain references to Python objects, fill the role of heterogeneous arrays.

  - itemsize  
    The size of the dtype element in bytes.

  - little-endian  
    See [Endianness](https://en.wikipedia.org/wiki/Endianness).

  - mask  
    A boolean array used to select only certain elements for an operation:
    
    > \>\>\> x = np.arange(5) \>\>\> x array(\[0, 1, 2, 3, 4\])
    > 
    > \>\>\> mask = (x \> 2) \>\>\> mask array(\[False, False, False, True, True\])
    > 
    > \>\>\> x\[mask\] = -1 \>\>\> x array(\[ 0, 1, 2, -1, -1\])

  - masked array  
    Bad or missing data can be cleanly ignored by putting it in a masked array, which has an internal boolean array indicating invalid entries. Operations with masked arrays ignore these entries. :
    
        >>> a = np.ma.masked_array([np.nan, 2, np.nan], [True, False, True])
        >>> a
        masked_array(data=[--, 2.0, --],
                     mask=[ True, False,  True],
               fill_value=1e+20)
        
        >>> a + [1, 2, 3]
        masked_array(data=[--, 4.0, --],
                     mask=[ True, False,  True],
               fill_value=1e+20)
    
    For details, see \[Masked arrays. \<reference/maskedarray\>\](Masked arrays. \<reference/maskedarray\>.md)

  - matrix  
    NumPy's two-dimensional \[matrix class \<reference/generated/numpy.matrix\>\](matrix class \<reference/generated/numpy.matrix\>.md) should no longer be used; use regular ndarrays.

  - ndarray  
    \[NumPy's basic structure \<reference/arrays\>\](NumPy's basic structure \<reference/arrays\>.md).

  - object array  
    An array whose dtype is `object`; that is, it contains references to Python objects. Indexing the array dereferences the Python objects, so unlike other ndarrays, an object array has the ability to hold heterogeneous objects.

  - ravel  
    \[numpy.ravel \<reference/generated/numpy.ravel\>\](numpy.ravel \<reference/generated/numpy.ravel\>.md) and \[numpy.flatten \<reference/generated/numpy.ndarray.flatten\>\](numpy.flatten \<reference/generated/numpy.ndarray.flatten\>.md) both flatten an ndarray. `ravel` will return a view if possible; `flatten` always returns a copy.
    
    Flattening collapses a multidimensional array to a single dimension; details of how this is done (for instance, whether `a[n+1]` should be the next row or next column) are parameters.

  - record array  
    A `structured array` with allowing access in an attribute style (`a.field`) in addition to `a['field']`. For details, see \[numpy.recarray. \<reference/generated/numpy.recarray\>\](numpy.recarray. \<reference/generated/numpy.recarray\>.md)

  - row-major  
    See [Row- and column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order). NumPy creates arrays in row-major order by default.

  - scalar  
    In NumPy, usually a synonym for `array scalar`.

  - shape  
    A tuple showing the length of each dimension of an ndarray. The length of the tuple itself is the number of dimensions (\[numpy.ndim \<reference/generated/numpy.ndarray.ndim\>\](numpy.ndim \<reference/generated/numpy.ndarray.ndim\>.md)). The product of the tuple elements is the number of elements in the array. For details, see \[numpy.ndarray.shape \<reference/generated/numpy.ndarray.shape\>\](numpy.ndarray.shape \<reference/generated/numpy.ndarray.shape\>.md).

  - stride  
    Physical memory is one-dimensional; strides provide a mechanism to map a given index to an address in memory. For an N-dimensional array, its `strides` attribute is an N-element tuple; advancing from index `i` to index `i+1` on axis `n` means adding `a.strides[n]` bytes to the address.
    
    Strides are computed automatically from an array's dtype and shape, but can be directly specified using \[as\_strided. \<reference/generated/numpy.lib.stride\_tricks.as\_strided\>\](as\_strided. \<reference/generated/numpy.lib.stride\_tricks.as\_strided\>.md)
    
    For details, see \[numpy.ndarray.strides \<reference/generated/numpy.ndarray.strides\>\](numpy.ndarray.strides \<reference/generated/numpy.ndarray.strides\>.md).
    
    To see how striding underlies the power of NumPy views, see [The NumPy array: a structure for efficient numerical computation.](https://arxiv.org/pdf/1102.1523.pdf)

  - structured array  
    Array whose `dtype` is a `structured data type`.

  - structured data type  
    Users can create arbitrarily complex `dtypes <dtype>` that can include other arrays and dtypes. These composite dtypes are called \[structured data types. \<user/basics.rec\>\](structured data types. \<user/basics.rec\>.md)

  - subarray  
    An array nested in a `structured data type`, as `b` is here:
    
    > \>\>\> dt = np.dtype(\[('a', np.int32), ('b', np.float32, (3,))\]) \>\>\> np.zeros(3, dtype=dt) array(\[(0, \[0., 0., 0.\]), (0, \[0., 0., 0.\]), (0, \[0., 0., 0.\])\], dtype=\[('a', '\<i4'), ('b', '\<f4', (3,))\])

  - subarray data type  
    An element of a structured datatype that behaves like an ndarray.

  - title  
    An alias for a field name in a structured datatype.

  - type  
    In NumPy, usually a synonym for `dtype`. For the more general Python meaning, `see here. <python:type>`

  - ufunc  
    NumPy's fast element-by-element computation (`vectorization`) gives a choice which function gets applied. The general term for the function is `ufunc`, short for `universal function`. NumPy routines have built-in ufuncs, but users can also \[write their own. \<reference/ufuncs\>\](write their own. \<reference/ufuncs\>.md)

  - vectorization  
    NumPy hands off array processing to C, where looping and computation are much faster than in Python. To exploit this, programmers using NumPy eliminate Python loops in favor of array-to-array operations. `vectorization` can refer both to the C offloading and to structuring NumPy code to leverage it.

  - view  
    Without touching underlying data, NumPy can make one array appear to change its datatype and shape.
    
    An array created this way is a <span class="title-ref">view</span>, and NumPy often exploits the performance gain of using a view versus making a new array.
    
    A potential drawback is that writing to a view can alter the original as well. If this is a problem, NumPy instead needs to create a physically distinct array -- a <span class="title-ref">copy</span>.
    
    Some NumPy routines always return views, some always return copies, some may return one or the other, and for some the choice can be specified. Responsibility for managing views and copies falls to the programmer. <span class="title-ref">numpy.shares\_memory</span> will check whether `b` is a view of `a`, but an exact answer isn't always feasible, as the documentation page explains.
    
    > \>\>\> x = np.arange(5) \>\>\> x array(\[0, 1, 2, 3, 4\])
    > 
    > \>\>\> y = x\[::2\] \>\>\> y array(\[0, 2, 4\])
    > 
    > \>\>\> x\[0\] = 3 \# changing x changes y as well, since y is a view on x \>\>\> y array(\[3, 2, 4\])

</div>

---

index.md

---

# NumPy documentation

<div class="toctree" data-maxdepth="1" hidden="">

User Guide \<user/index\> API reference \<reference/index\> Building from source \<building/index\> Development \<dev/index\> release

</div>

**Version**:

**Download documentation**: [Historical versions of documentation](https://numpy.org/doc/)

**Useful links**: [Installation](https://numpy.org/install/) | [Source Repository](https://github.com/numpy/numpy) | [Issue Tracker](https://github.com/numpy/numpy/issues) | [Q\&A Support](https://numpy.org/gethelp/) | [Mailing List](https://mail.python.org/mailman/listinfo/numpy-discussion)

NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.

<div class="grid" data-gutter="2 3 4 4">

1 1 2 2

<div class="grid-item-card" data-img-top="../source/_static/index-images/getting_started.svg" data-text-align="center">

Getting started ^^^

New to NumPy? Check out the Absolute Beginner's Guide. It contains an introduction to NumPy's main concepts and links to additional tutorials.

\+++

<div class="button-ref" data-expand="" color="secondary" data-click-parent="">

user/absolute\_beginners

To the absolute beginner's guide

</div>

</div>

<div class="grid-item-card" data-img-top="../source/_static/index-images/user_guide.svg" data-text-align="center">

User guide ^^^

The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation.

\+++

<div class="button-ref" data-expand="" color="secondary" data-click-parent="">

user

To the user guide

</div>

</div>

<div class="grid-item-card" data-img-top="../source/_static/index-images/api.svg" data-text-align="center">

API reference ^^^

The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts.

\+++

<div class="button-ref" data-expand="" color="secondary" data-click-parent="">

reference

To the reference guide

</div>

</div>

<div class="grid-item-card" data-img-top="../source/_static/index-images/contributor.svg" data-text-align="center">

Contributor's guide ^^^

Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy.

\+++

<div class="button-ref" data-expand="" color="secondary" data-click-parent="">

devindex

To the contributor's guide

</div>

</div>

</div>

---

license.md

---

# NumPy license

Copyright (c) 2005-2024, NumPy Developers. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:

>   -   - Redistributions of source code must retain the above copyright  
>         notice, this list of conditions and the following disclaimer.
> 
>   -   - Redistributions in binary form must reproduce the above  
>         copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
> 
>   -   - Neither the name of the NumPy Developers nor the names of any  
>         contributors may be used to endorse or promote products derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

>   - literal

---

numpy_2_0_migration_guide.md

---

# NumPy 2.0 migration guide

This document contains a set of instructions on how to update your code to work with NumPy 2.0. It covers changes in NumPy's Python and C APIs.

\> **Note** \> Note that NumPy 2.0 also breaks binary compatibility - if you are distributing binaries for a Python package that depends on NumPy's C API, please see \[numpy-2-abi-handling\](\#numpy-2-abi-handling).

## Ruff plugin

Many of the changes covered in the 2.0 release notes and in this migration guide can be automatically adapted in downstream code with a dedicated [Ruff](https://docs.astral.sh/ruff/) rule, namely rule [NPY201](https://docs.astral.sh/ruff/rules/numpy2-deprecation/).

You should install `ruff>=0.4.8` and add the `NPY201` rule to your `pyproject.toml`:

    [tool.ruff.lint]
    select = ["NPY201"]

You can also apply the NumPy 2.0 rule directly from the command line:

    $ ruff check path/to/code/ --select NPY201

## Changes to NumPy data type promotion

NumPy 2.0 changes promotion (the result of combining dissimilar data types) as per \[NEP 50 \<NEP50\>\](\#nep-50-\<nep50\>). Please see the NEP for details on this change. It includes a table of example changes and a backwards compatibility section.

The largest backwards compatibility change is that the precision of scalars is now preserved consistently. Two examples are:

  - `np.float32(3) + 3.` now returns a float32 when it previously returned a float64.
  - `np.array([3], dtype=np.float32) + np.float64(3)` will now return a float64 array. (The higher precision of the scalar is not ignored.)

For floating point values, this can lead to lower precision results when working with scalars. For integers, errors or overflows are possible.

To solve this, you may cast explicitly. Very often, it may also be a good solution to ensure you are working with Python scalars via `int()`, `float()`, or `numpy_scalar.item()`.

To track down changes, you can enable emitting warnings for changed behavior (use `warnings.simplefilter` to raise it as an error for a traceback):

    np._set_promotion_state("weak_and_warn")

which is useful during testing. Unfortunately, running this may flag many changes that are irrelevant in practice.

## Windows default integer

The default integer used by NumPy is now 64bit on all 64bit systems (and 32bit on 32bit system). For historic reasons related to Python 2 it was previously equivalent to the C `long` type. The default integer is now equivalent to `np.intp`.

Most end-users should not be affected by this change. Some operations will use more memory, but some operations may actually become faster. If you experience issues due to calling a library written in a compiled language it may help to explicitly cast to a `long`, for example with: `arr = arr.astype("long", copy=False)`.

Libraries interfacing with compiled code that are written in C, Cython, or a similar language may require updating to accommodate user input if they are using the `long` or equivalent type on the C-side. In this case, you may wish to use `intp` and cast user input or support both `long` and `intp` (to better support NumPy 1.x as well). When creating a new integer array in C or Cython, the new `NPY_DEFAULT_INT` macro will evaluate to either `NPY_LONG` or `NPY_INTP` depending on the NumPy version.

Note that the NumPy random API is not affected by this change.

## C-API Changes

Some definitions were removed or replaced due to being outdated or unmaintainable. Some new API definitions will evaluate differently at runtime between NumPy 2.0 and NumPy 1.x. Some are defined in `numpy/_core/include/numpy/npy_2_compat.h` (for example `NPY_DEFAULT_INT`) which can be vendored in full or part to have the definitions available when compiling against NumPy 1.x.

If necessary, `PyArray_RUNTIME_VERSION >= NPY_2_0_API_VERSION` can be used to explicitly implement different behavior on NumPy 1.x and 2.0. (The compat header defines it in a way compatible with such use.)

Please let us know if you require additional workarounds here.

### The `PyArray_Descr` struct has been changed

One of the most impactful C-API changes is that the `PyArray_Descr` struct is now more opaque to allow us to add additional flags and have itemsizes not limited by the size of `int` as well as allow improving structured dtypes in the future and not burden new dtypes with their fields.

Code which only uses the type number and other initial fields is unaffected. Most code will hopefully mainly access the `->elsize` field, when the dtype/descriptor itself is attached to an array (e.g. `arr->descr->elsize`) this is best replaced with `PyArray_ITEMSIZE(arr)`.

Where not possible, new accessor functions are required:

  - `PyDataType_ELSIZE` and `PyDataType_SET_ELSIZE` (note that the result is now `npy_intp` and not `int`).
  - `PyDataType_ALIGNMENT`
  - `PyDataType_FIELDS`, `PyDataType_NAMES`, `PyDataType_SUBARRAY`
  - `PyDataType_C_METADATA`

Cython code should use Cython 3, in which case the change is transparent. (Struct access is available for elsize and alignment when compiling only for NumPy 2.)

For compiling with both 1.x and 2.x if you use these new accessors it is unfortunately necessary to either define them locally via a macro like:

    #if NPY_ABI_VERSION < 0x02000000
      #define PyDataType_ELSIZE(descr) ((descr)->elsize)
    #endif

or adding `npy2_compat.h` into your code base and explicitly include it when compiling with NumPy 1.x (as they are new API). Including the file has no effect on NumPy 2.

Please do not hesitate to open a NumPy issue, if you require assistance or the provided functions are not sufficient.

**Custom User DTypes:** Existing user dtypes must now use :c`PyArray_DescrProto` to define their dtype and slightly modify the code. See note in :c\`PyArray\_RegisterDataType\`.

### Functionality moved to headers requiring `import_array()`

If you previously included only `ndarraytypes.h` you may find that some functionality is not available anymore and requires the inclusion of `ndarrayobject.h` or similar. This include is also needed when vendoring `npy_2_compat.h` into your own codebase to allow use of the new definitions when compiling with NumPy 1.x.

Functionality which previously did not require import includes:

  - Functions to access dtype flags: `PyDataType_FLAGCHK`, `PyDataType_REFCHK`, and the related `NPY_BEGIN_THREADS_DESCR`.
  - `PyArray_GETITEM` and `PyArray_SETITEM`.

<div class="warning">

<div class="title">

Warning

</div>

It is important that the `import_array()` mechanism is used to ensure that the full NumPy API is accessible when using the `npy_2_compat.h` header. In most cases your extension module probably already calls it. However, if not we have added `PyArray_ImportNumPyAPI()` as a preferable way to ensure the NumPy API is imported. This function is light-weight when called multiple times so that you may insert it wherever it may be needed (if you wish to avoid setting it up at module import).

</div>

### Increased maximum number of dimensions

The maximum number of dimensions (and arguments) was increased to 64. This affects the `NPY_MAXDIMS` and `NPY_MAXARGS` macros. It may be good to review their use, and we generally encourage you to not use these macros (especially `NPY_MAXARGS`), so that a future version of NumPy can remove this limitation on the number of dimensions.

`NPY_MAXDIMS` was also used to signal `axis=None` in the C-API, including the `PyArray_AxisConverter`. The latter will return `-2147483648` as an axis (the smallest integer value). Other functions may error with `AxisError: axis 64 is out of bounds for array of dimension` in which case you need to pass `NPY_RAVEL_AXIS` instead of `NPY_MAXDIMS`. `NPY_RAVEL_AXIS` is defined in the `npy_2_compat.h` header and runtime dependent (mapping to 32 on NumPy 1.x and `-2147483648` on NumPy 2.x).

### Complex types - Underlying type changes

The underlying C types for all of the complex types have been changed to use native C99 types. While the memory layout of those types remains identical to the types used in NumPy 1.x, the API is slightly different, since direct field access (like `c.real` or `c.imag`) is no longer possible.

It is recommended to use the functions `npy_creal` and `npy_cimag` (and the corresponding float and long double variants) to retrieve the real or imaginary part of a complex number, as these will work with both NumPy 1.x and with NumPy 2.x. New functions `npy_csetreal` and `npy_csetimag`, along with compatibility macros `NPY_CSETREAL` and `NPY_CSETIMAG` (and the corresponding float and long double variants), have been added for setting the real or imaginary part.

The underlying type remains a struct under C++ (all of the above still remains valid).

This has implications for Cython. It is recommended to always use the native typedefs `cfloat_t`, `cdouble_t`, `clongdouble_t` rather than the NumPy types `npy_cfloat`, etc, unless you have to interface with C code written using the NumPy types. You can still write cython code using the `c.real` and `c.imag` attributes (using the native typedefs), but you can no longer use in-place operators `c.imag += 1` in Cython's c++ mode.

Because NumPy 2 now includes `complex.h` code that uses a variable named `I` may see an error such as

``` C
error: expected â€˜)â€™ before â€˜__extension__â€™
double I,
```

to use the name `I` requires an `#undef I` now.

<div class="note">

<div class="title">

Note

</div>

NumPy 2.0.1 briefly included the `#undef I` to help users not already including `complex.h`.

</div>

## Changes to namespaces

In NumPy 2.0 certain functions, modules, and constants were moved or removed to make the NumPy namespace more user-friendly by removing unnecessary or outdated functionality and clarifying which parts of NumPy are considered private. Please see the tables below for guidance on migration. For most changes this means replacing it with a backwards compatible alternative.

Please refer to \[NEP52\](\#nep52) for more details.

### Main namespace

About 100 members of the main `np` namespace have been deprecated, removed, or moved to a new place. It was done to reduce clutter and establish only one way to access a given attribute. The table below shows members that have been removed:

| removed member                  | migration guideline                                                                                                                                                                     |
| ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| add\_docstring                  | It's still available as `np.lib.add_docstring`.                                                                                                                                         |
| add\_newdoc                     | It's still available as `np.lib.add_newdoc`.                                                                                                                                            |
| add\_newdoc\_ufunc              | It's an internal function and doesn't have a replacement.                                                                                                                               |
| alltrue                         | Use `np.all` instead.                                                                                                                                                                   |
| asfarray                        | Use `np.asarray` with a float dtype instead.                                                                                                                                            |
| byte\_bounds                    | Now it's available under `np.lib.array_utils.byte_bounds`                                                                                                                               |
| cast                            | Use `np.asarray(arr, dtype=dtype)` instead.                                                                                                                                             |
| cfloat                          | Use `np.complex128` instead.                                                                                                                                                            |
| charrarray                      | It's still available as `np.char.chararray`.                                                                                                                                            |
| clongfloat                      | Use `np.clongdouble` instead.                                                                                                                                                           |
| compare\_chararrays             | It's still available as `np.char.compare_chararrays`.                                                                                                                                   |
| compat                          | There's no replacement, as Python 2 is no longer supported.                                                                                                                             |
| complex\_                       | Use `np.complex128` instead.                                                                                                                                                            |
| cumproduct                      | Use `np.cumprod` instead.                                                                                                                                                               |
| DataSource                      | It's still available as `np.lib.npyio.DataSource`.                                                                                                                                      |
| deprecate                       | Emit `DeprecationWarning` with `warnings.warn` directly, or use `typing.deprecated`.                                                                                                    |
| deprecate\_with\_doc            | Emit `DeprecationWarning` with `warnings.warn` directly, or use `typing.deprecated`.                                                                                                    |
| disp                            | Use your own printing function instead.                                                                                                                                                 |
| fastCopyAndTranspose            | Use `arr.T.copy()` instead.                                                                                                                                                             |
| find\_common\_type              | Use `numpy.promote_types` or `numpy.result_type` instead. To achieve semantics for the `scalar_types` argument, use `numpy.result_type` and pass the Python values `0`, `0.0`, or `0j`. |
| format\_parser get\_array\_wrap | It's still available as `np.rec.format_parser`.                                                                                                                                         |
| float\_                         | Use `np.float64` instead.                                                                                                                                                               |
| geterrobj                       | Use the np.errstate context manager instead.                                                                                                                                            |
| Inf                             | Use `np.inf` instead.                                                                                                                                                                   |
| Infinity                        | Use `np.inf` instead.                                                                                                                                                                   |
| infty                           | Use `np.inf` instead.                                                                                                                                                                   |
| issctype                        | Use `issubclass(rep, np.generic)` instead.                                                                                                                                              |
| issubclass\_                    | Use `issubclass` builtin instead.                                                                                                                                                       |
| issubsctype                     | Use `np.issubdtype` instead.                                                                                                                                                            |
| mat                             | Use `np.asmatrix` instead.                                                                                                                                                              |
| maximum\_sctype                 | Use a specific dtype instead. You should avoid relying on any implicit mechanism and select the largest dtype of a kind explicitly in the code.                                         |
| NaN                             | Use `np.nan` instead.                                                                                                                                                                   |
| nbytes                          | Use `np.dtype(<dtype>).itemsize` instead.                                                                                                                                               |
| NINF                            | Use `-np.inf` instead.                                                                                                                                                                  |
| NZERO                           | Use `-0.0` instead.                                                                                                                                                                     |
| longcomplex                     | Use `np.clongdouble` instead.                                                                                                                                                           |
| longfloat                       | Use `np.longdouble` instead.                                                                                                                                                            |
| lookfor                         | Search NumPy's documentation directly.                                                                                                                                                  |
| obj2sctype                      | Use `np.dtype(obj).type` instead.                                                                                                                                                       |
| PINF                            | Use `np.inf` instead.                                                                                                                                                                   |
| product                         | Use `np.prod` instead.                                                                                                                                                                  |
| PZERO                           | Use `0.0` instead.                                                                                                                                                                      |
| recfromcsv                      | Use `np.genfromtxt` with comma delimiter instead.                                                                                                                                       |
| recfromtxt                      | Use `np.genfromtxt` instead.                                                                                                                                                            |
| round\_                         | Use `np.round` instead.                                                                                                                                                                 |
| safe\_eval                      | Use `ast.literal_eval` instead.                                                                                                                                                         |
| sctype2char                     | Use `np.dtype(obj).char` instead.                                                                                                                                                       |
| sctypes                         | Access dtypes explicitly instead.                                                                                                                                                       |
| seterrobj                       | Use the np.errstate context manager instead.                                                                                                                                            |
| set\_numeric\_ops               | For the general case, use `PyUFunc_ReplaceLoopBySignature`. For ndarray subclasses, define the `__array_ufunc__` method and override the relevant ufunc.                                |
| set\_string\_function           | Use `np.set_printoptions` instead with a formatter for custom printing of NumPy objects.                                                                                                |
| singlecomplex                   | Use `np.complex64` instead.                                                                                                                                                             |
| string\_                        | Use `np.bytes_` instead.                                                                                                                                                                |
| sometrue                        | Use `np.any` instead.                                                                                                                                                                   |
| source                          | Use `inspect.getsource` instead.                                                                                                                                                        |
| tracemalloc\_domain             | It's now available from `np.lib`.                                                                                                                                                       |
| unicode\_                       | Use `np.str_` instead.                                                                                                                                                                  |
| who                             | Use an IDE variable explorer or `locals()` instead.                                                                                                                                     |

If the table doesn't contain an item that you were using but was removed in `2.0`, then it means it was a private member. You should either use the existing API or, in case it's infeasible, reach out to us with a request to restore the removed entry.

The next table presents deprecated members, which will be removed in a release after `2.0`:

| deprecated member | migration guideline                                              |
| ----------------- | ---------------------------------------------------------------- |
| in1d              | Use `np.isin` instead.                                           |
| row\_stack        | Use `np.vstack` instead (`row_stack` was an alias for `vstack`). |
| trapz             | Use `np.trapezoid` or a `scipy.integrate` function instead.      |

Finally, a set of internal enums has been removed. As they weren't used in downstream libraries we don't provide any information on how to replace them:

\[`FLOATING_POINT_SUPPORT`, `FPE_DIVIDEBYZERO`, `FPE_INVALID`, `FPE_OVERFLOW`, `FPE_UNDERFLOW`, `UFUNC_BUFSIZE_DEFAULT`, `UFUNC_PYVALS_NAME`, `CLIP`, `WRAP`, `RAISE`, `BUFSIZE`, `ALLOW_THREADS`, `MAXDIMS`, `MAY_SHARE_EXACT`, `MAY_SHARE_BOUNDS`\]

### numpy.lib namespace

Most of the functions available within `np.lib` are also present in the main namespace, which is their primary location. To make it unambiguous how to access each public function, `np.lib` is now empty and contains only a handful of specialized submodules, classes and functions:

  - `array_utils`, `format`, `introspect`, `mixins`, `npyio` and `stride_tricks` submodules,
  - `Arrayterator` and `NumpyVersion` classes,
  - `add_docstring` and `add_newdoc` functions,
  - `tracemalloc_domain` constant.

If you get an `AttributeError` when accessing an attribute from `np.lib` you should try accessing it from the main `np` namespace then. If an item is also missing from the main namespace, then you're using a private member. You should either use the existing API or, in case it's infeasible, reach out to us with a request to restore the removed entry.

### numpy.core namespace

The `np.core` namespace is now officially private and has been renamed to `np._core`. The user should never fetch members from the `_core` directly - instead the main namespace should be used to access the attribute in question. The layout of the `_core` module might change in the future without notice, contrary to public modules which adhere to the deprecation period policy. If an item is also missing from the main namespace, then you should either use the existing API or, in case it's infeasible, reach out to us with a request to restore the removed entry.

### ndarray and scalar methods

A few methods from `np.ndarray` and `np.generic` scalar classes have been removed. The table below provides replacements for the removed members:

| expired member | migration guideline                                    |
| -------------- | ------------------------------------------------------ |
| newbyteorder   | Use `arr.view(arr.dtype.newbyteorder(order))` instead. |
| ptp            | Use `np.ptp(arr, ...)` instead.                        |
| setitem        | Use `arr[index] = value` instead.                      |

### numpy.strings namespace

A new <span class="title-ref">numpy.strings</span> namespace has been created, where most of the string operations are implemented as ufuncs. The old <span class="title-ref">numpy.char</span> namespace still is available, and, wherever possible, uses the new ufuncs for greater performance. We recommend using the <span class="title-ref">\~numpy.strings</span> functions going forward. The <span class="title-ref">\~numpy.char</span> namespace may be deprecated in the future.

## Other changes

### Note about pickled files

NumPy 2.0 is designed to load pickle files created with NumPy 1.26, and vice versa. For versions 1.25 and earlier loading NumPy 2.0 pickle file will throw an exception.

### Adapting to changes in the `copy` keyword

The \[copy keyword behavior changes \<copy-keyword-changes-2.0\>\](\#copy-keyword-behavior-changes-\<copy-keyword-changes-2.0\>) in <span class="title-ref">\~numpy.asarray</span>, <span class="title-ref">\~numpy.array</span> and <span class="title-ref">ndarray.\_\_array\_\_ \<numpy.ndarray.\_\_array\_\_\></span> may require these changes:

  - Code using `np.array(..., copy=False)` can in most cases be changed to `np.asarray(...)`. Older code tended to use `np.array` like this because it had less overhead than the default `np.asarray` copy-if-needed behavior. This is no longer true, and `np.asarray` is the preferred function.
  - For code that explicitly needs to pass `None`/`False` meaning "copy if needed" in a way that's compatible with NumPy 1.x and 2.x, see [scipy\#20172](https://github.com/scipy/scipy/pull/20172) for an example of how to do so.
  - For any `__array__` method on a non-NumPy array-like object, `dtype=None` and `copy=None` keywords must be added to the signature - this will work with older NumPy versions as well (although older numpy versions will never pass in `copy` keyword). If the keywords are added to the `__array__` signature, then for:
      - `copy=True` and any `dtype` value always return a new copy,
      - `copy=None` create a copy if required (for example by `dtype`),
      - `copy=False` a copy must never be made. If a copy is needed to return a numpy array or satisfy `dtype`, then raise an exception (`ValueError`).

### Writing numpy-version-dependent code

It should be fairly rare to have to write code that explicitly branches on the `numpy` version - in most cases, code can be rewritten to be compatible with 1.x and 2.0 at the same time. However, if it is necessary, here is a suggested code pattern to use, using \`numpy.lib.NumpyVersion\`:

    # example with AxisError, which is no longer available in
    # the main namespace in 2.0, and not available in the
    # `exceptions` namespace in <1.25.0 (example uses <2.0.0b1
    # for illustrative purposes):
    if np.lib.NumpyVersion(np.__version__) >= '2.0.0b1':
        from numpy.exceptions import AxisError
    else:
        from numpy import AxisError

This pattern will work correctly including with NumPy release candidates, which is important during the 2.0.0 release period.

---

alignment.md

---

- orphan

# Memory alignment

This document has been moved to \[alignment\](\#alignment).

---

array_api.md

---

# Array API standard compatibility

NumPy's main namespace as well as the <span class="title-ref">numpy.fft</span> and <span class="title-ref">numpy.linalg</span> namespaces are compatible\[1\] with the [2022.12 version](https://data-apis.org/array-api/2022.12/index.html) of the Python array API standard.

NumPy aims to implement support for the [2023.12 version](https://data-apis.org/array-api/2023.12/index.html) and future versions of the standard - assuming that those future versions can be upgraded to given NumPy's \[backwards compatibility policy \<NEP23\>\](\#backwards-compatibility-policy-\<nep23\>).

For usage guidelines for downstream libraries and end users who want to write code that will work with both NumPy and other array libraries, we refer to the documentation of the array API standard itself and to code and developer-focused documentation in SciPy and scikit-learn.

Note that in order to use standard-complaint code with older NumPy versions (\< 2.0), the [array-api-compat](https://github.com/data-apis/array-api-compat) package may be useful. For testing whether NumPy-using code is only using standard-compliant features rather than anything NumPy-specific, the [array-api-strict](https://github.com/data-apis/array-api-strict) package can be used.

<div class="admonition">

History

NumPy 1.22.0 was the first version to include support for the array API standard, via a separate `numpy.array_api` submodule. This module was marked as experimental (it emitted a warning on import) and removed in NumPy 2.0 because full support was included in the main namespace. \[NEP 47 \<NEP47\>\](\#nep-47-\<nep47\>) and \[NEP 56 \<NEP56\>\](\#nep-56-\<nep56\>) describe the motivation and scope for implementing the array API standard in NumPy.

</div>

## Entry point

NumPy installs an [entry point](https://packaging.python.org/en/latest/specifications/entry-points/) that can be used for discovery purposes:

    >>> from importlib.metadata import entry_points
    >>> entry_points(group='array_api', name='numpy')
    [EntryPoint(name='numpy', value='numpy', group='array_api')]

Note that leaving out `name='numpy'` will cause a list of entry points to be returned for all array API standard compatible implementations that installed an entry point.

**Footnotes**

## Inspection

NumPy implements the [array API inspection utilities](https://data-apis.org/array-api/latest/API_specification/inspection.html). These functions can be accessed via the `__array_namespace_info__()` function, which returns a namespace containing the inspection utilities.

<div class="currentmodule">

numpy

</div>

<div class="autosummary" data-toctree="generated">

\_\_array\_namespace\_info\_\_

</div>

1.  With a few very minor exceptions, as documented in \[NEP 56 \<NEP56\>\](\#nep-56-\<nep56\>). The `sum`, `prod` and `trace` behavior adheres to the 2023.12 version instead, as do function signatures; the only known incompatibility that may remain is that the standard forbids unsafe casts for in-place operators while NumPy supports those.

---

arrays.classes.md

---

# Standard array subclasses

<div class="currentmodule">

numpy

</div>

\> **Note** \> Subclassing a `numpy.ndarray` is possible but if your goal is to create an array with *modified* behavior, as do dask arrays for distributed computation and cupy arrays for GPU-based computation, subclassing is discouraged. Instead, using numpy's \[dispatch mechanism \<basics.dispatch\>\](\#dispatch-mechanism-\<basics.dispatch\>) is recommended.

The <span class="title-ref">ndarray</span> can be inherited from (in Python or in C) if desired. Therefore, it can form a foundation for many useful classes. Often whether to sub-class the array object or to simply use the core array component as an internal part of a new class is a difficult decision, and can be simply a matter of choice. NumPy has several tools for simplifying how your new object interacts with other array objects, and so the choice may not be significant in the end. One way to simplify the question is by asking yourself if the object you are interested in can be replaced as a single array or does it really require two or more arrays at its core.

Note that <span class="title-ref">asarray</span> always returns the base-class ndarray. If you are confident that your use of the array object can handle any subclass of an ndarray, then <span class="title-ref">asanyarray</span> can be used to allow subclasses to propagate more cleanly through your subroutine. In principal a subclass could redefine any aspect of the array and therefore, under strict guidelines, <span class="title-ref">asanyarray</span> would rarely be useful. However, most subclasses of the array object will not redefine certain aspects of the array object such as the buffer interface, or the attributes of the array. One important example, however, of why your subroutine may not be able to handle an arbitrary subclass of an array is that matrices redefine the "\*" operator to be matrix-multiplication, rather than element-by-element multiplication.

## Special attributes and methods

<div class="seealso">

\[Subclassing ndarray \<basics.subclassing\>\](\#subclassing-ndarray-\<basics.subclassing\>)

</div>

NumPy provides several hooks that classes can customize:

  - \-------------\<ufuncs-output-type\>), results will *not* be written to the object  
    returned by <span class="title-ref">\_\_array\_\_</span>. This practice will return `TypeError`.

## Matrix objects

<div class="index">

single: matrix

</div>

<div class="note">

<div class="title">

Note

</div>

It is strongly advised *not* to use the matrix subclass. As described below, it makes writing functions that deal consistently with matrices and regular arrays very difficult. Currently, they are mainly used for interacting with `scipy.sparse`. We hope to provide an alternative for this use, however, and eventually remove the `matrix` subclass.

</div>

<span class="title-ref">matrix</span> objects inherit from the ndarray and therefore, they have the same attributes and methods of ndarrays. There are six important differences of matrix objects, however, that may lead to unexpected results when you use matrices but expect them to act like arrays:

1.  Matrix objects can be created using a string notation to allow Matlab-style syntax where spaces separate columns and semicolons (';') separate rows.

2.  Matrix objects are always two-dimensional. This has far-reaching implications, in that m.ravel() is still two-dimensional (with a 1 in the first dimension) and item selection returns two-dimensional objects so that sequence behavior is fundamentally different than arrays.

3.  Matrix objects over-ride multiplication to be matrix-multiplication. **Make sure you understand this for functions that you may want to receive matrices. Especially in light of the fact that asanyarray(m) returns a matrix when m is a matrix.**

4.  Matrix objects over-ride power to be matrix raised to a power. The same warning about using power inside a function that uses asanyarray(...) to get an array object holds for this fact.

5.  The default \_\_array\_priority\_\_ of matrix objects is 10.0, and therefore mixed operations with ndarrays always produce matrices.

6.  Matrices have special attributes which make calculations easier. These are
    
    <div class="autosummary" data-toctree="generated/">
    
    matrix.T matrix.H matrix.I matrix.A
    
    </div>

\> **Warning** \> Matrix objects over-ride multiplication, '*', and power, '*\*', to be matrix-multiplication and matrix power, respectively. If your subroutine can accept sub-classes and you do not convert to base- class arrays, then you must use the ufuncs multiply and power to be sure that you are performing the correct operation for all inputs.

The matrix class is a Python subclass of the ndarray and can be used as a reference for how to construct your own subclass of the ndarray. Matrices can be created from other matrices, strings, and anything else that can be converted to an `ndarray` . The name "mat "is an alias for "matrix "in NumPy.

<div class="autosummary" data-toctree="generated/">

matrix asmatrix bmat

</div>

Example 1: Matrix creation from a string

> \>\>\> import numpy as np \>\>\> a = np.asmatrix('1 2 3; 4 5 3') \>\>\> print((a\*a.T).I) \[\[ 0.29239766 -0.13450292\] \[-0.13450292 0.08187135\]\]

Example 2: Matrix creation from a nested sequence

> \>\>\> import numpy as np \>\>\> np.asmatrix(\[\[1,5,10\],\[1.0,3,4j\]\]) matrix(\[\[ 1.+0.j, 5.+0.j, 10.+0.j\], \[ 1.+0.j, 3.+0.j, 0.+4.j\]\])

Example 3: Matrix creation from an array

> \>\>\> import numpy as np \>\>\> np.asmatrix(np.random.rand(3,3)).T matrix(\[\[4.17022005e-01, 3.02332573e-01, 1.86260211e-01\], \[7.20324493e-01, 1.46755891e-01, 3.45560727e-01\], \[1.14374817e-04, 9.23385948e-02, 3.96767474e-01\]\])

## Memory-mapped file arrays

<div class="index">

single: memory maps

</div>

<div class="currentmodule">

numpy

</div>

Memory-mapped files are useful for reading and/or modifying small segments of a large file with regular layout, without reading the entire file into memory. A simple subclass of the ndarray uses a memory-mapped file for the data buffer of the array. For small files, the over-head of reading the entire file into memory is typically not significant, however for large files using memory mapping can save considerable resources.

Memory-mapped-file arrays have one additional method (besides those they inherit from the ndarray): <span class="title-ref">.flush() \<memmap.flush\></span> which must be called manually by the user to ensure that any changes to the array actually get written to disk.

<div class="autosummary" data-toctree="generated/">

memmap memmap.flush

</div>

Example:

> \>\>\> import numpy as np
> 
> \>\>\> a = np.memmap('newfile.dat', dtype=float, mode='w+', shape=1000) \>\>\> a\[10\] = 10.0 \>\>\> a\[30\] = 30.0 \>\>\> del a
> 
> \>\>\> b = np.fromfile('newfile.dat', dtype=float) \>\>\> print(b\[10\], b\[30\]) 10.0 30.0
> 
> \>\>\> a = np.memmap('newfile.dat', dtype=float) \>\>\> print(a\[10\], a\[30\]) 10.0 30.0

## Character arrays (`numpy.char`)

<div class="seealso">

\[routines.array-creation.char\](\#routines.array-creation.char)

</div>

<div class="index">

single: character arrays

</div>

<div class="note">

<div class="title">

Note

</div>

The <span class="title-ref">\~numpy.char.chararray</span> class exists for backwards compatibility with Numarray, it is not recommended for new development. Starting from numpy 1.4, if one needs arrays of strings, it is recommended to use arrays of <span class="title-ref">dtype</span> <span class="title-ref">object\_</span>, <span class="title-ref">bytes\_</span> or <span class="title-ref">str\_</span>, and use the free functions in the <span class="title-ref">numpy.char</span> module for fast vectorized string operations.

</div>

These are enhanced arrays of either <span class="title-ref">str\_</span> type or <span class="title-ref">bytes\_</span> type. These arrays inherit from the <span class="title-ref">ndarray</span>, but specially-define the operations `+`, `*`, and `%` on a (broadcasting) element-by-element basis. These operations are not available on the standard <span class="title-ref">ndarray</span> of character type. In addition, the <span class="title-ref">\~numpy.char.chararray</span> has all of the standard <span class="title-ref">str</span> (and <span class="title-ref">bytes</span>) methods, executing them on an element-by-element basis. Perhaps the easiest way to create a chararray is to use <span class="title-ref">self.view(chararray) \<ndarray.view\></span> where *self* is an ndarray of str or unicode data-type. However, a chararray can also be created using the <span class="title-ref">\~numpy.char.chararray</span> constructor, or via the <span class="title-ref">numpy.char.array</span> function:

<div class="autosummary" data-toctree="generated/">

char.chararray char.array

</div>

Another difference with the standard ndarray of str data-type is that the chararray inherits the feature introduced by Numarray that white-space at the end of any element in the array will be ignored on item retrieval and comparison operations.

## Record arrays

<div class="seealso">

\[routines.array-creation.rec\](\#routines.array-creation.rec), \[routines.dtype\](\#routines.dtype), \[arrays.dtypes\](\#arrays.dtypes).

</div>

NumPy provides the <span class="title-ref">recarray</span> class which allows accessing the fields of a structured array as attributes, and a corresponding scalar data type object <span class="title-ref">record</span>.

<div class="currentmodule">

numpy

</div>

<div class="autosummary" data-toctree="generated/">

recarray record

</div>

\> **Note** \> The pandas DataFrame is more powerful than record array. If possible, please use pandas DataFrame instead.

## Masked arrays (`numpy.ma`)

<div class="seealso">

\[maskedarray\](\#maskedarray)

</div>

## Standard container class

<div class="currentmodule">

numpy

</div>

For backward compatibility and as a standard "container "class, the UserArray from Numeric has been brought over to NumPy and named <span class="title-ref">numpy.lib.user\_array.container</span> The container class is a Python class whose self.array attribute is an ndarray. Multiple inheritance is probably easier with numpy.lib.user\_array.container than with the ndarray itself and so it is included by default. It is not documented here beyond mentioning its existence because you are encouraged to use the ndarray class directly if you can.

<div class="autosummary" data-toctree="generated/">

numpy.lib.user\_array.container

</div>

<div class="index">

single: user\_array single: container class

</div>

## Array iterators

<div class="currentmodule">

numpy

</div>

<div class="index">

single: array iterator

</div>

Iterators are a powerful concept for array processing. Essentially, iterators implement a generalized for-loop. If *myiter* is an iterator object, then the Python code:

    for val in myiter:
        ...
        some code involving val
        ...

calls `val = next(myiter)` repeatedly until <span class="title-ref">StopIteration</span> is raised by the iterator. There are several ways to iterate over an array that may be useful: default iteration, flat iteration, and \(N\)-dimensional enumeration.

### Default iteration

The default iterator of an ndarray object is the default Python iterator of a sequence type. Thus, when the array object itself is used as an iterator. The default behavior is equivalent to:

    for i in range(arr.shape[0]):
        val = arr[i]

This default iterator selects a sub-array of dimension \(N-1\) from the array. This can be a useful construct for defining recursive algorithms. To loop over the entire array requires \(N\) for-loops.

> \>\>\> import numpy as np \>\>\> a = np.arange(24).reshape(3,2,4) + 10 \>\>\> for val in a: ... print('item:', val) item: \[\[10 11 12 13\] \[14 15 16 17\]\] item: \[\[18 19 20 21\] \[22 23 24 25\]\] item: \[\[26 27 28 29\] \[30 31 32 33\]\]

### Flat iteration

<div class="autosummary" data-toctree="generated/">

ndarray.flat

</div>

As mentioned previously, the flat attribute of ndarray objects returns an iterator that will cycle over the entire array in C-style contiguous order.

> \>\>\> import numpy as np \>\>\> a = np.arange(24).reshape(3,2,4) + 10 \>\>\> for i, val in enumerate(a.flat): ... if i%5 == 0: print(i, val) 0 10 5 15 10 20 15 25 20 30

Here, I've used the built-in enumerate iterator to return the iterator index as well as the value.

### N-dimensional enumeration

<div class="autosummary" data-toctree="generated/">

ndenumerate

</div>

Sometimes it may be useful to get the N-dimensional index while iterating. The ndenumerate iterator can achieve this.

> \>\>\> import numpy as np \>\>\> for i, val in np.ndenumerate(a): ... if sum(i)%5 == 0: print(i, val) (0, 0, 0) 10 (1, 1, 3) 25 (2, 0, 3) 29 (2, 1, 2) 32

### Iterator for broadcasting

<div class="autosummary" data-toctree="generated/">

broadcast

</div>

The general concept of broadcasting is also available from Python using the <span class="title-ref">broadcast</span> iterator. This object takes \(N\) objects as inputs and returns an iterator that returns tuples providing each of the input sequence elements in the broadcasted result.

> \>\>\> import numpy as np \>\>\> for val in np.broadcast(\[\[1, 0\], \[2, 3\]\], \[0, 1\]): ... print(val) (np.int64(1), np.int64(0)) (np.int64(0), np.int64(1)) (np.int64(2), np.int64(0)) (np.int64(3), np.int64(1))

---

arrays.datetime.md

---

<div class="currentmodule">

numpy

</div>

# Datetimes and timedeltas

Starting in NumPy 1.7, there are core array data types which natively support datetime functionality. The data type is called <span class="title-ref">datetime64</span>, so named because <span class="title-ref">\~datetime.datetime</span> is already taken by the Python standard library.

## Datetime64 conventions and assumptions

Similar to the Python <span class="title-ref">\~datetime.date</span> class, dates are expressed in the current Gregorian Calendar, indefinitely extended both in the future and in the past. \[1\] Contrary to Python <span class="title-ref">\~datetime.date</span>, which supports only years in the 1 AD â€” 9999 AD range, <span class="title-ref">datetime64</span> allows also for dates BC; years BC follow the [Astronomical year numbering](https://en.wikipedia.org/wiki/Astronomical_year_numbering) convention, i.e. year 2 BC is numbered âˆ’1, year 1 BC is numbered 0, year 1 AD is numbered 1.

Time instants, say 16:23:32.234, are represented counting hours, minutes, seconds and fractions from midnight: i.e. 00:00:00.000 is midnight, 12:00:00.000 is noon, etc. Each calendar day has exactly 86400 seconds. This is a "naive" time, with no explicit notion of timezones or specific time scales (UT1, UTC, TAI, etc.).\[2\]

## Basic datetimes

The most basic way to create datetimes is from strings in ISO 8601 date or datetime format. It is also possible to create datetimes from an integer by offset relative to the Unix epoch (00:00:00 UTC on 1 January 1970). The unit for internal storage is automatically selected from the form of the string, and can be either a \[date unit \<arrays.dtypes.dateunits\>\](\#date-unit-\<arrays.dtypes.dateunits\>) or a \[time unit \<arrays.dtypes.timeunits\>\](\#time-unit-\<arrays.dtypes.timeunits\>). The date units are years ('Y'), months ('M'), weeks ('W'), and days ('D'), while the time units are hours ('h'), minutes ('m'), seconds ('s'), milliseconds ('ms'), and some additional SI-prefix seconds-based units. The <span class="title-ref">datetime64</span> data type also accepts the string "NAT", in any combination of lowercase/uppercase letters, for a "Not A Time" value.

<div class="admonition">

Example

A simple ISO date:

\>\>\> import numpy as np

\>\>\> np.datetime64('2005-02-25') np.datetime64('2005-02-25')

From an integer and a date unit, 1 year since the UNIX epoch:

\>\>\> np.datetime64(1, 'Y') np.datetime64('1971')

Using months for the unit:

\>\>\> np.datetime64('2005-02') np.datetime64('2005-02')

Specifying just the month, but forcing a 'days' unit:

\>\>\> np.datetime64('2005-02', 'D') np.datetime64('2005-02-01')

From a date and time:

\>\>\> np.datetime64('2005-02-25T03:30') np.datetime64('2005-02-25T03:30')

NAT (not a time):

\>\>\> np.datetime64('nat') np.datetime64('NaT')

</div>

When creating an array of datetimes from a string, it is still possible to automatically select the unit from the inputs, by using the datetime type with generic units.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.array(\['2007-07-13', '2006-01-13', '2010-08-13'\], dtype='datetime64') array(\['2007-07-13', '2006-01-13', '2010-08-13'\], dtype='datetime64\[D\]')

\>\>\> np.array(\['2001-01-01T12:00', '2002-02-03T13:56:03.172'\], dtype='datetime64') array(\['2001-01-01T12:00:00.000', '2002-02-03T13:56:03.172'\], dtype='datetime64\[ms\]')

</div>

An array of datetimes can be constructed from integers representing POSIX timestamps with the given unit.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.array(\[0, 1577836800\], dtype='datetime64\[s\]') array(\['1970-01-01T00:00:00', '2020-01-01T00:00:00'\], dtype='datetime64\[s\]')

\>\>\> np.array(\[0, 1577836800000\]).astype('datetime64\[ms\]') array(\['1970-01-01T00:00:00.000', '2020-01-01T00:00:00.000'\], dtype='datetime64\[ms\]')

</div>

The datetime type works with many common NumPy functions, for example <span class="title-ref">arange</span> can be used to generate ranges of dates.

<div class="admonition">

Example

All the dates for one month:

\>\>\> import numpy as np

\>\>\> np.arange('2005-02', '2005-03', dtype='datetime64\[D\]') array(\['2005-02-01', '2005-02-02', '2005-02-03', '2005-02-04', '2005-02-05', '2005-02-06', '2005-02-07', '2005-02-08', '2005-02-09', '2005-02-10', '2005-02-11', '2005-02-12', '2005-02-13', '2005-02-14', '2005-02-15', '2005-02-16', '2005-02-17', '2005-02-18', '2005-02-19', '2005-02-20', '2005-02-21', '2005-02-22', '2005-02-23', '2005-02-24', '2005-02-25', '2005-02-26', '2005-02-27', '2005-02-28'\], dtype='datetime64\[D\]')

</div>

The datetime object represents a single moment in time. If two datetimes have different units, they may still be representing the same moment of time, and converting from a bigger unit like months to a smaller unit like days is considered a 'safe' cast because the moment of time is still being represented exactly.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.datetime64('2005') == np.datetime64('2005-01-01') True

\>\>\> np.datetime64('2010-03-14T15') == np.datetime64('2010-03-14T15:00:00.00') True

</div>

<div class="deprecated">

1.11.0

NumPy does not store timezone information. For backwards compatibility, datetime64 still parses timezone offsets, which it handles by converting to UTCÂ±00:00 (Zulu time). This behaviour is deprecated and will raise an error in the future.

</div>

## Datetime and timedelta arithmetic

NumPy allows the subtraction of two datetime values, an operation which produces a number with a time unit. Because NumPy doesn't have a physical quantities system in its core, the <span class="title-ref">timedelta64</span> data type was created to complement <span class="title-ref">datetime64</span>. The arguments for <span class="title-ref">timedelta64</span> are a number, to represent the number of units, and a date/time unit, such as (D)ay, (M)onth, (Y)ear, (h)ours, (m)inutes, or (s)econds. The <span class="title-ref">timedelta64</span> data type also accepts the string "NAT" in place of the number for a "Not A Time" value.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.timedelta64(1, 'D') np.timedelta64(1,'D')

\>\>\> np.timedelta64(4, 'h') np.timedelta64(4,'h')

\>\>\> np.timedelta64('nAt') np.timedelta64('NaT')

</div>

Datetimes and Timedeltas work together to provide ways for simple datetime calculations.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.datetime64('2009-01-01') - np.datetime64('2008-01-01') np.timedelta64(366,'D')

\>\>\> np.datetime64('2009') + np.timedelta64(20, 'D') np.datetime64('2009-01-21')

\>\>\> np.datetime64('2011-06-15T00:00') + np.timedelta64(12, 'h') np.datetime64('2011-06-15T12:00')

\>\>\> np.timedelta64(1,'W') / np.timedelta64(1,'D') 7.0

\>\>\> np.timedelta64(1,'W') % np.timedelta64(10,'D') np.timedelta64(7,'D')

\>\>\> np.datetime64('nat') - np.datetime64('2009-01-01') np.timedelta64('NaT','D')

\>\>\> np.datetime64('2009-01-01') + np.timedelta64('nat') np.datetime64('NaT')

</div>

There are two Timedelta units ('Y', years and 'M', months) which are treated specially, because how much time they represent changes depending on when they are used. While a timedelta day unit is equivalent to 24 hours, month and year units cannot be converted directly into days without using 'unsafe' casting.

The <span class="title-ref">numpy.ndarray.astype</span> method can be used for unsafe conversion of months/years to days. The conversion follows calculating the averaged values from the 400 year leap-year cycle.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.timedelta64(1, 'Y')

\>\>\> np.timedelta64(a, 'M') numpy.timedelta64(12,'M')

\>\>\> np.timedelta64(a, 'D') Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> TypeError: Cannot cast NumPy timedelta64 scalar from metadata \[Y\] to \[D\] according to the rule 'same\_kind'

</div>

## Datetime units

The Datetime and Timedelta data types support a large number of time units, as well as generic units which can be coerced into any of the other units based on input data.

Datetimes are always stored with an epoch of 1970-01-01T00:00. This means the supported dates are always a symmetric interval around the epoch, called "time span" in the table below.

The length of the span is the range of a 64-bit integer times the length of the date or unit. For example, the time span for 'W' (week) is exactly 7 times longer than the time span for 'D' (day), and the time span for 'D' (day) is exactly 24 times longer than the time span for 'h' (hour).

Here are the date units:

<div id="arrays.dtypes.dateunits">

<table>
<thead>
<tr class="header">
<th>Code</th>
<th>Meaning</th>
<th>Time span (relative)</th>
<th>Time span (absolute)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Y</p>
</blockquote></td>
<td><blockquote>
<p>year</p>
</blockquote></td>
<td><blockquote>
<p>+/- 9.2e18 years</p>
</blockquote></td>
<td><blockquote>
<p>[9.2e18 BC, 9.2e18 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>M</p>
</blockquote></td>
<td><blockquote>
<p>month</p>
</blockquote></td>
<td><blockquote>
<p>+/- 7.6e17 years</p>
</blockquote></td>
<td><blockquote>
<p>[7.6e17 BC, 7.6e17 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>W</p>
</blockquote></td>
<td><blockquote>
<p>week</p>
</blockquote></td>
<td><blockquote>
<p>+/- 1.7e17 years</p>
</blockquote></td>
<td><blockquote>
<p>[1.7e17 BC, 1.7e17 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>D</p>
</blockquote></td>
<td><blockquote>
<p>day</p>
</blockquote></td>
<td><blockquote>
<p>+/- 2.5e16 years</p>
</blockquote></td>
<td><blockquote>
<p>[2.5e16 BC, 2.5e16 AD]</p>
</blockquote></td>
</tr>
</tbody>
</table>

</div>

And here are the time units:

<div id="arrays.dtypes.timeunits">

<table>
<thead>
<tr class="header">
<th>Code</th>
<th>Meaning</th>
<th>Time span (relative)</th>
<th>Time span (absolute)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>h</p>
</blockquote></td>
<td><blockquote>
<p>hour</p>
</blockquote></td>
<td><blockquote>
<p>+/- 1.0e15 years</p>
</blockquote></td>
<td><blockquote>
<p>[1.0e15 BC, 1.0e15 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>m</p>
</blockquote></td>
<td><blockquote>
<p>minute</p>
</blockquote></td>
<td><blockquote>
<p>+/- 1.7e13 years</p>
</blockquote></td>
<td><blockquote>
<p>[1.7e13 BC, 1.7e13 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>s</p>
</blockquote></td>
<td><blockquote>
<p>second</p>
</blockquote></td>
<td><blockquote>
<p>+/- 2.9e11 years</p>
</blockquote></td>
<td><blockquote>
<p>[2.9e11 BC, 2.9e11 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>ms</p>
</blockquote></td>
<td><blockquote>
<p>millisecond</p>
</blockquote></td>
<td><blockquote>
<p>+/- 2.9e8 years</p>
</blockquote></td>
<td><blockquote>
<p>[ 2.9e8 BC, 2.9e8 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>us / Î¼s</td>
<td><blockquote>
<p>microsecond</p>
</blockquote></td>
<td><blockquote>
<p>+/- 2.9e5 years</p>
</blockquote></td>
<td><blockquote>
<p>[290301 BC, 294241 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>ns</p>
</blockquote></td>
<td><blockquote>
<p>nanosecond</p>
</blockquote></td>
<td><blockquote>
<p>+/- 292 years</p>
</blockquote></td>
<td><blockquote>
<p>[ 1678 AD, 2262 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>ps</p>
</blockquote></td>
<td><blockquote>
<p>picosecond</p>
</blockquote></td>
<td><blockquote>
<p>+/- 106 days</p>
</blockquote></td>
<td><blockquote>
<p>[ 1969 AD, 1970 AD]</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>fs</p>
</blockquote></td>
<td><blockquote>
<p>femtosecond</p>
</blockquote></td>
<td><blockquote>
<p>+/- 2.6 hours</p>
</blockquote></td>
<td><blockquote>
<p>[ 1969 AD, 1970 AD]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>as</p>
</blockquote></td>
<td><blockquote>
<p>attosecond</p>
</blockquote></td>
<td><blockquote>
<p>+/- 9.2 seconds</p>
</blockquote></td>
<td><blockquote>
<p>[ 1969 AD, 1970 AD]</p>
</blockquote></td>
</tr>
</tbody>
</table>

</div>

## Business day functionality

To allow the datetime to be used in contexts where only certain days of the week are valid, NumPy includes a set of "busday" (business day) functions.

The default for busday functions is that the only valid days are Monday through Friday (the usual business days). The implementation is based on a "weekmask" containing 7 Boolean flags to indicate valid days; custom weekmasks are possible that specify other sets of valid days.

The "busday" functions can additionally check a list of "holiday" dates, specific dates that are not valid days.

The function <span class="title-ref">busday\_offset</span> allows you to apply offsets specified in business days to datetimes with a unit of 'D' (day).

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.busday\_offset('2011-06-23', 1) np.datetime64('2011-06-24')

\>\>\> np.busday\_offset('2011-06-23', 2) np.datetime64('2011-06-27')

</div>

When an input date falls on the weekend or a holiday, <span class="title-ref">busday\_offset</span> first applies a rule to roll the date to a valid business day, then applies the offset. The default rule is 'raise', which simply raises an exception. The rules most typically used are 'forward' and 'backward'.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.busday\_offset('2011-06-25', 2) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> ValueError: Non-business day date in busday\_offset

\>\>\> np.busday\_offset('2011-06-25', 0, roll='forward') np.datetime64('2011-06-27')

\>\>\> np.busday\_offset('2011-06-25', 2, roll='forward') np.datetime64('2011-06-29')

\>\>\> np.busday\_offset('2011-06-25', 0, roll='backward') np.datetime64('2011-06-24')

\>\>\> np.busday\_offset('2011-06-25', 2, roll='backward') np.datetime64('2011-06-28')

</div>

In some cases, an appropriate use of the roll and the offset is necessary to get a desired answer.

<div class="admonition">

Example

The first business day on or after a date:

\>\>\> import numpy as np

\>\>\> np.busday\_offset('2011-03-20', 0, roll='forward') np.datetime64('2011-03-21') \>\>\> np.busday\_offset('2011-03-22', 0, roll='forward') np.datetime64('2011-03-22')

The first business day strictly after a date:

\>\>\> np.busday\_offset('2011-03-20', 1, roll='backward') np.datetime64('2011-03-21') \>\>\> np.busday\_offset('2011-03-22', 1, roll='backward') np.datetime64('2011-03-23')

</div>

The function is also useful for computing some kinds of days like holidays. In Canada and the U.S., Mother's day is on the second Sunday in May, which can be computed with a custom weekmask.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.busday\_offset('2012-05', 1, roll='forward', weekmask='Sun') np.datetime64('2012-05-13')

</div>

When performance is important for manipulating many business dates with one particular choice of weekmask and holidays, there is an object <span class="title-ref">busdaycalendar</span> which stores the data necessary in an optimized form.

### np.is\_busday():

To test a <span class="title-ref">datetime64</span> value to see if it is a valid day, use <span class="title-ref">is\_busday</span>.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.is\_busday(np.datetime64('2011-07-15')) \# a Friday True \>\>\> np.is\_busday(np.datetime64('2011-07-16')) \# a Saturday False \>\>\> np.is\_busday(np.datetime64('2011-07-16'), weekmask="Sat Sun") True \>\>\> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18')) \>\>\> np.is\_busday(a) array(\[ True, True, True, True, True, False, False\])

</div>

### np.busday\_count():

To find how many valid days there are in a specified range of datetime64 dates, use \`busday\_count\`:

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> np.busday\_count(np.datetime64('2011-07-11'), np.datetime64('2011-07-18')) 5 \>\>\> np.busday\_count(np.datetime64('2011-07-18'), np.datetime64('2011-07-11')) -5

</div>

If you have an array of datetime64 day values, and you want a count of how many of them are valid dates, you can do this:

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18')) \>\>\> np.count\_nonzero(np.is\_busday(a)) 5

</div>

### Custom weekmasks

Here are several examples of custom weekmask values. These examples specify the "busday" default of Monday through Friday being valid days.

Some examples:

    # Positional sequences; positions are Monday through Sunday.
    # Length of the sequence must be exactly 7.
    weekmask = [1, 1, 1, 1, 1, 0, 0]
    # list or other sequence; 0 == invalid day, 1 == valid day
    weekmask = "1111100"
    # string '0' == invalid day, '1' == valid day
    
    # string abbreviations from this list: Mon Tue Wed Thu Fri Sat Sun
    weekmask = "Mon Tue Wed Thu Fri"
    # any amount of whitespace is allowed; abbreviations are case-sensitive.
    weekmask = "MonTue Wed  Thu\tFri"

## Datetime64 shortcomings

The assumption that all days are exactly 86400 seconds long makes <span class="title-ref">datetime64</span> largely compatible with Python <span class="title-ref">datetime</span> and "POSIX time" semantics; therefore they all share the same well known shortcomings with respect to the UTC timescale and historical time determination. A brief non exhaustive summary is given below.

  - It is impossible to parse valid UTC timestamps occurring during a positive leap second.
    
    <div class="admonition">
    
    Example
    
    "2016-12-31 23:59:60 UTC" was a leap second, therefore "2016-12-31 23:59:60.450 UTC" is a valid timestamp which is not parseable by \`datetime64\`:
    
    > \>\>\> import numpy as np
    > 
    > \>\>\> np.datetime64("2016-12-31 23:59:60.450") Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> ValueError: Seconds out of range in datetime string "2016-12-31 23:59:60.450"
    
    </div>

  - Timedelta64 computations between two UTC dates can be wrong by an integer number of SI seconds.
    
    <div class="admonition">
    
    Example
    
    Compute the number of SI seconds between "2021-01-01 12:56:23.423 UTC" and "2001-01-01 00:00:00.000 UTC":
    
    > \>\>\> import numpy as np
    > 
    > \>\>\> ( ... np.datetime64("2021-01-01 12:56:23.423") ... - np.datetime64("2001-01-01") ... ) / np.timedelta64(1, "s") 631198583.423
    > 
    > However, the correct answer is <span class="title-ref">631198588.423</span> SI seconds, because there were 5 leap seconds between 2001 and 2021.
    
    </div>

  - Timedelta64 computations for dates in the past do not return SI seconds, as one would expect.
    
    <div class="admonition">
    
    Example
    
    Compute the number of seconds between "000-01-01 UT" and "1600-01-01 UT", where UT is [universal time](https://en.wikipedia.org/wiki/Universal_Time):
    
    > \>\>\> import numpy as np
    > 
    > \>\>\> a = np.datetime64("0000-01-01", "us") \>\>\> b = np.datetime64("1600-01-01", "us") \>\>\> b - a numpy.timedelta64(50491123200000000,'us')
    > 
    > The computed results, <span class="title-ref">50491123200</span> seconds, are obtained as the elapsed number of days (<span class="title-ref">584388</span>) times <span class="title-ref">86400</span> seconds; this is the number of seconds of a clock in sync with the Earth's rotation. The exact value in SI seconds can only be estimated, e.g., using data published in [Measurement of the Earth's rotation: 720 BC to AD 2015, 2016, Royal Society's Proceedings A 472, by Stephenson et.al.](https://doi.org/10.1098/rspa.2016.0404). A sensible estimate is <span class="title-ref">50491112870 Â± 90</span> seconds, with a difference of 10330 seconds.
    
    </div>

<!-- end list -->

1.  The calendar obtained by extending the Gregorian calendar before its official adoption on Oct. 15, 1582 is called [Proleptic Gregorian Calendar](https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar)

2.  The assumption of 86400 seconds per calendar day is not valid for UTC, the present day civil time scale. In fact due to the presence of [leap seconds](https://en.wikipedia.org/wiki/Leap_second) on rare occasions a day may be 86401 or 86399 seconds long. On the contrary the 86400s day assumption holds for the TAI timescale. An explicit support for TAI and TAI to UTC conversion, accounting for leap seconds, is proposed but not yet implemented. See also the [shortcomings](#shortcomings) section below.

---

arrays.dtypes.md

---

<div class="currentmodule">

numpy

</div>

# Data type objects (<span class="title-ref">dtype</span>)

A data type object (an instance of <span class="title-ref">numpy.dtype</span> class) describes how the bytes in the fixed-size block of memory corresponding to an array item should be interpreted. It describes the following aspects of the data:

1.  Type of the data (integer, float, Python object, etc.)
2.  Size of the data (how many bytes is in *e.g.* the integer)
3.  Byte order of the data (`little-endian` or `big-endian`)
4.  If the data type is `structured data type`, an aggregate of other data types, (*e.g.*, describing an array item consisting of an integer and a float),
    1.  what are the names of the "`fields <field>`" of the structure, by which they can be \[accessed \<arrays.indexing.fields\>\](\#accessed-\<arrays.indexing.fields\>),
    2.  what is the data-type of each `field`, and
    3.  which part of the memory block each field takes.
5.  If the data type is a sub-array, what is its shape and data type.

<div class="index">

pair: dtype; scalar

</div>

To describe the type of scalar data, there are several \[built-in scalar types \<arrays.scalars.built-in\>\](\#built-in scalar-types-\<arrays.scalars.built-in\>) in NumPy for various precision of integers, floating-point numbers, *etc*. An item extracted from an array, *e.g.*, by indexing, will be a Python object whose type is the scalar type associated with the data type of the array.

Note that the scalar types are not <span class="title-ref">dtype</span> objects, even though they can be used in place of one whenever a data type specification is needed in NumPy.

<div class="index">

pair: dtype; field

</div>

Structured data types are formed by creating a data type whose `field` contain other data types. Each field has a name by which it can be \[accessed \<arrays.indexing.fields\>\](\#accessed-\<arrays.indexing.fields\>). The parent data type should be of sufficient size to contain all its fields; the parent is nearly always based on the <span class="title-ref">void</span> type which allows an arbitrary item size. Structured data types may also contain nested structured sub-array data types in their fields.

<div class="index">

pair: dtype; sub-array

</div>

Finally, a data type can describe items that are themselves arrays of items of another data type. These sub-arrays must, however, be of a fixed size.

If an array is created using a data-type describing a sub-array, the dimensions of the sub-array are appended to the shape of the array when the array is created. Sub-arrays in a field of a structured type behave differently, see \[arrays.indexing.fields\](\#arrays.indexing.fields).

Sub-arrays always have a C-contiguous memory layout.

<div class="admonition">

Example

A simple data type containing a 32-bit big-endian integer: (see \[arrays.dtypes.constructing\](\#arrays.dtypes.constructing) for details on construction)

> \>\>\> import numpy as np
> 
> \>\>\> dt = np.dtype('\>i4') \>\>\> dt.byteorder '\>' \>\>\> dt.itemsize 4 \>\>\> dt.name 'int32' \>\>\> dt.type is np.int32 True

The corresponding array scalar type is <span class="title-ref">int32</span>.

</div>

<div class="admonition">

Example

A structured data type containing a 16-character string (in field 'name') and a sub-array of two 64-bit floating-point number (in field 'grades'):

> \>\>\> import numpy as np
> 
> \>\>\> dt = np.dtype(\[('name', [np.str](), 16), ('grades', np.float64, (2,))\]) \>\>\> dt\['name'\] dtype('\<U16') \>\>\> dt\['grades'\] dtype(('\<f8', (2,)))

Items of an array of this data type are wrapped in an \[array scalar \<arrays.scalars\>\](\#array

</div>

\---scalar-\<arrays.scalars\>) type that also has two fields:

> \>\>\> import numpy as np
> 
> \>\>\> x = np.array(\[('Sarah', (8.0, 7.0)), ('John', (6.0, 7.0))\], dtype=dt) \>\>\> x\[1\] ('John', \[6., 7.\]) \>\>\> x\[1\]\['grades'\] array(\[6., 7.\]) \>\>\> type(x\[1\]) \<class 'numpy.void'\> \>\>\> type(x\[1\]\['grades'\]) \<class 'numpy.ndarray'\>

## Specifying and constructing data types

Whenever a data-type is required in a NumPy function or method, either a <span class="title-ref">dtype</span> object or something that can be converted to one can be supplied. Such conversions are done by the <span class="title-ref">dtype</span> constructor:

<div class="autosummary" data-toctree="generated/">

dtype

</div>

What can be converted to a data-type object is described below:

  - <span class="title-ref">dtype</span> object
    
    <div class="index">
    
    triple: dtype; construction; from dtype
    
    </div>
    
    Used as-is.

  - None
    
    <div class="index">
    
    triple: dtype; construction; from None
    
    </div>
    
    The default data type: <span class="title-ref">float64</span>.

<div class="index">

triple: dtype; construction; from type

</div>

  - Array-scalar types  
    The 24 built-in \[array scalar type objects \<arrays.scalars.built-in\>\](\#array-scalar-type-objects

  - \----\<arrays.scalars.built-in\>) all convert to an associated data-type object.  
    This is true for their sub-classes as well.
    
    Note that not all data-type information can be supplied with a type-object: for example, <span class="title-ref">flexible</span> data-types have a default *itemsize* of 0, and require an explicitly given size to be useful.
    
    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype(np.int32) \# 32-bit integer \>\>\> dt = np.dtype(np.complex128) \# 128-bit complex floating-point number
    
    </div>

  - Generic types  
    The generic hierarchical type objects convert to corresponding type objects according to the associations:
    
    |                                                                                                                                                                                                                                                                                                                                                                                          |        |
    | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ |
    | <span class="title-ref">number</span>, <span class="title-ref">inexact</span>, <span class="title-ref">floating</span> <span class="title-ref">float64</span>                                                                                                                                                                                                                            |        |
    | <span class="title-ref">complexfloating</span> <span class="title-ref">comple \`integer</span>, <span class="title-ref">signedinteger</span> <span class="title-ref">int\_</span> <span class="title-ref">unsignedinteger</span> <span class="title-ref">uint</span> <span class="title-ref">generic</span>, <span class="title-ref">flexible</span> <span class="title-ref">void</span> | x128\` |

    <div class="deprecated">
    
    1.19
    
    This conversion of generic scalar types is deprecated. This is because it can be unexpected in a context such as `arr.astype(dtype=np.floating)`, which casts an array of `float32` to an array of `float64`, even though `float32` is a subdtype of `np.floating`.
    
    </div>

  - Built-in Python types  
    Several python types are equivalent to a corresponding array scalar when used to generate a <span class="title-ref">dtype</span> object:
    
    |                                                                                                                                                                                                   |                                         |
    | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
    | <span class="title-ref">int</span> <span class="title-ref">int\_</span>                                                                                                                           |                                         |
    | <span class="title-ref">bool</span> \`bool\_                                                                                                                                                      | \`                                      |
    | <span class="title-ref">float</span> \`float6                                                                                                                                                     | 4\`                                     |
    | <span class="title-ref">complex</span> \`comple                                                                                                                                                   | x128\`                                  |
    | <span class="title-ref">bytes</span> \`bytes<span class="title-ref">str</span> <span class="title-ref">str\_</span> <span class="title-ref">memoryview</span> <span class="title-ref">void</span> | \_\`                                    |
    | (all others)                                                                                                                                                                                      | <span class="title-ref">object\_</span> |

    Note that `str_` corresponds to UCS4 encoded unicode strings.
    
    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype(float) \# Python-compatible floating-point number \>\>\> dt = np.dtype(int) \# Python-compatible integer \>\>\> dt = np.dtype(object) \# Python object
    
    </div>
    
    \> **Note**

  - \>  
    All other types map to `object_` for convenience. Code should expect that such types may map to a specific (new) dtype in the future.

  - Types with `.dtype`  
    Any type object with a `dtype` attribute: The attribute will be accessed and used directly. The attribute must return something that is convertible into a dtype object.

<div class="index">

triple: dtype; construction; from string

</div>

Several kinds of strings can be converted. Recognized strings can be prepended with `'>'` (`big-endian`), `'<'` (`little-endian`), or `'='` (hardware-native, the default), to specify the byte order.

  - One-character strings  
    Each built-in data-type has a character code (the updated Numeric typecodes), that uniquely identifies it.
    
    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype('b') \# byte, native byte order \>\>\> dt = np.dtype('\>H') \# big-endian unsigned short \>\>\> dt = np.dtype('\<f') \# little-endian single-precision float \>\>\> dt = np.dtype('d') \# double-precision floating-point number
    
    </div>

  - Array-protocol type strings (see \[arrays.interface\](\#arrays.interface))  
    The first character specifies the kind of data and the remaining characters specify the number of bytes per item, except for Unicode, where it is interpreted as the number of characters. The item size must correspond to an existing type, or an error will be raised. The supported kinds are
    
    |              |                                                |
    | ------------ | ---------------------------------------------- |
    | `'?'`        | boolean                                        |
    | `'b'`        | (signed) byte                                  |
    | `'B'`        | unsigned byte                                  |
    | `'i'`        | (signed) integer                               |
    | `'u'`        | unsigned integer                               |
    | `'f'`        | floating-point                                 |
    | `'c'`        | complex-floating point                         |
    | `'m'`        | timedelta                                      |
    | `'M'`        | datetime                                       |
    | `'O'`        | (Python) objects                               |
    | `'S'`, `'a'` | zero-terminated bytes (not recommended)        |
    | `'U'`        | Unicode string                                 |
    | `'V'`        | raw data (<span class="title-ref">void</span>) |

    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype('i4') \# 32-bit signed integer \>\>\> dt = np.dtype('f8') \# 64-bit floating-point number \>\>\> dt = np.dtype('c16') \# 128-bit complex floating-point number \>\>\> dt = np.dtype('S25') \# 25-length zero-terminated bytes \>\>\> dt = np.dtype('U25') \# 25-character string
    
    </div>
    
    <div id="string-dtype-note">
    
    <div class="admonition">
    
    Note on string types
    
    For backward compatibility with existing code originally written to support Python 2, `S` and `a` typestrings are zero-terminated bytes. For unicode strings, use `U`, <span class="title-ref">numpy.str\_</span>. For signed bytes that do not need zero-termination `b` or `i1` can be used.
    
    </div>
    
    </div>

  - String with comma-separated fields  
    A short-hand notation for specifying the format of a structured data type is a comma-separated string of basic formats.
    
    A basic format in this context is an optional shape specifier followed by an array-protocol type string. Parenthesis are required on the shape if it has more than one dimension. NumPy allows a modification on the format in that any string that can uniquely identify the type can be used to specify the data-type in a field. The generated data-type fields are named `'f0'`, `'f1'`, ..., `'f<N-1>'` where N (\>1) is the number of comma-separated basic formats in the string. If the optional shape specifier is provided, then the data-type for the corresponding field describes a sub-array.
    
    <div class="admonition">
    
    Example
    
      - field named `f0` containing a 32-bit integer
    
      - field named `f1` containing a 2 x 3 sub-array of 64-bit floating-point numbers
    
      - field named `f2` containing a 32-bit floating-point number
        
        > \>\>\> import numpy as np \>\>\> dt = np.dtype("i4, (2,3)f8, f4")
    
      - field named `f0` containing a 3-character string
    
      - field named `f1` containing a sub-array of shape (3,) containing 64-bit unsigned integers
    
      - field named `f2` containing a 3 x 4 sub-array containing 10-character strings
        
        > \>\>\> import numpy as np \>\>\> dt = np.dtype("S3, 3u8, (3,4)S10")
    
    </div>

  - Type strings  
    Any string name of a NumPy dtype, e.g.:
    
    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype('uint32') \# 32-bit unsigned integer \>\>\> dt = np.dtype('float64') \# 64-bit floating-point number
    
    </div>

<div class="index">

triple: dtype; construction; from tuple

</div>

  - `(flexible_dtype, itemsize)`  
    The first argument must be an object that is converted to a zero-sized flexible data-type object, the second argument is an integer providing the desired itemsize.
    
    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype((np.void, 10)) \# 10-byte wide data block \>\>\> dt = np.dtype(('U', 10)) \# 10-character unicode string
    
    </div>

  - `(fixed_dtype, shape)`
    
    <div class="index">
    
    pair: dtype; sub-array
    
    </div>
    
    The first argument is any object that can be converted into a fixed-size data-type object. The second argument is the desired shape of this type. If the shape parameter is 1, then the data-type object used to be equivalent to fixed dtype. This behaviour is deprecated since NumPy 1.17 and will raise an error in the future. If *shape* is a tuple, then the new dtype defines a sub-array of the given shape.
    
    <div class="admonition">
    
    Example
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype((np.int32, (2,2))) \# 2 x 2 integer sub-array \>\>\> dt = np.dtype(('i4, (2,3)f8, f4', (2,3))) \# 2 x 3 structured sub-array
    
    </div>

<div class="index">

triple: dtype; construction; from list

</div>

  - `[(field_name, field_dtype, field_shape), ...]`  
    *obj* should be a list of fields where each field is described by a tuple of length 2 or 3. (Equivalent to the `descr` item in the `~object.__array_interface__` attribute.)
    
    The first element, *field\_name*, is the field name (if this is `''` then a standard field name, `'f#'`, is assigned). The field name may also be a 2-tuple of strings where the first string is either a "title" (which may be any string or unicode string) or meta-data for the field which can be any object, and the second string is the "name" which must be a valid Python identifier.
    
    The second element, *field\_dtype*, can be anything that can be interpreted as a data-type.
    
    The optional third element *field\_shape* contains the shape if this field represents an array of the data-type in the second element. Note that a 3-tuple with a third argument equal to 1 is equivalent to a 2-tuple.
    
    This style does not accept *align* in the <span class="title-ref">dtype</span> constructor as it is assumed that all of the memory is accounted for by the array interface description.
    
    <div class="admonition">
    
    Example
    
    Data-type with fields `big` (big-endian 32-bit integer) and `little` (little-endian 32-bit integer):
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype(\[('big', '\>i4'), ('little', '\<i4')\])
    
    Data-type with fields `R`, `G`, `B`, `A`, each being an unsigned 8-bit integer:
    
    \>\>\> dt = np.dtype(\[('R','u1'), ('G','u1'), ('B','u1'), ('A','u1')\])
    
    </div>

<div class="index">

triple: dtype; construction; from dict

</div>

  - `{'names': ..., 'formats': ..., 'offsets': ..., 'titles': ..., 'itemsize': ...}`  
    This style has two required and three optional keys. The *names* and *formats* keys are required. Their respective values are equal-length lists with the field names and the field formats. The field names must be strings and the field formats can be any object accepted by <span class="title-ref">dtype</span> constructor.
    
    When the optional keys *offsets* and *titles* are provided, their values must each be lists of the same length as the *names* and *formats* lists. The *offsets* value is a list of byte offsets (limited to <span class="title-ref">ctypes.c\_int</span>) for each field, while the *titles* value is a list of titles for each field (`None` can be used if no title is desired for that field). The *titles* can be any object, but when a <span class="title-ref">str</span> object will add another entry to the fields dictionary keyed by the title and referencing the same field tuple which will contain the title as an additional tuple member.
    
    The *itemsize* key allows the total size of the dtype to be set, and must be an integer large enough so all the fields are within the dtype. If the dtype being constructed is aligned, the *itemsize* must also be divisible by the struct alignment. Total dtype *itemsize* is limited to <span class="title-ref">ctypes.c\_int</span>.
    
    <div class="admonition">
    
    Example
    
    Data type with fields `r`, `g`, `b`, `a`, each being an 8-bit unsigned integer:
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype({'names': \['r','g','b','a'\], ... 'formats': \[np.uint8, np.uint8, np.uint8, np.uint8\]})
    
    Data type with fields `r` and `b` (with the given titles), both being 8-bit unsigned integers, the first at byte position 0 from the start of the field and the second at position 2:
    
    \>\>\> dt = np.dtype({'names': \['r','b'\], 'formats': \['u1', 'u1'\], ... 'offsets': \[0, 2\], ... 'titles': \['Red pixel', 'Blue pixel'\]})
    
    </div>

  - `{'field1': ..., 'field2': ..., ...}`  
    This usage is discouraged, because it is ambiguous with the other dict-based construction method. If you have a field called 'names' and a field called 'formats' there will be a conflict.
    
    This style allows passing in the <span class="title-ref">fields \<dtype.fields\></span> attribute of a data-type object.
    
    *obj* should contain string or unicode keys that refer to `(data-type, offset)` or `(data-type, offset, title)` tuples.
    
    <div class="admonition">
    
    Example
    
    Data type containing field `col1` (10-character string at byte position 0), `col2` (32-bit float at byte position 10), and `col3` (integers at byte position 14):
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype({'col1': ('U10', 0), 'col2': (np.float32, 10), ... 'col3': (int, 14)})
    
    </div>

  - `(base_dtype, new_dtype)`  
    In NumPy 1.7 and later, this form allows <span class="title-ref">base\_dtype</span> to be interpreted as a structured dtype. Arrays created with this dtype will have underlying dtype <span class="title-ref">base\_dtype</span> but will have fields and flags taken from <span class="title-ref">new\_dtype</span>. This is useful for creating custom structured dtypes, as done in \[record arrays \<arrays.classes.rec\>\](\#record-arrays-\<arrays.classes.rec\>).
    
    This form also makes it possible to specify struct dtypes with overlapping fields, functioning like the 'union' type in C. This usage is discouraged, however, and the union mechanism is preferred.
    
    Both arguments must be convertible to data-type objects with the same total size.
    
    <div class="admonition">
    
    Example
    
    32-bit integer, whose first two bytes are interpreted as an integer via field `real`, and the following two bytes via field `imag`.
    
    \>\>\> import numpy as np
    
    \>\>\> dt = np.dtype((np.int32,{'real':(np.int16, 0),'imag':(np.int16, 2)}))
    
    32-bit integer, which is interpreted as consisting of a sub-array of shape `(4,)` containing 8-bit integers:
    
    \>\>\> dt = np.dtype((np.int32, (np.int8, 4)))
    
    32-bit integer, containing fields `r`, `g`, `b`, `a` that interpret the 4 bytes in the integer as four unsigned integers:
    
    \>\>\> dt = np.dtype(('i4', \[('r','u1'),('g','u1'),('b','u1'),('a','u1')\]))
    
    </div>

## Checking the data type

When checking for a specific data type, use `==` comparison.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.array(\[1, 2\], dtype=np.float32) \>\>\> a.dtype == np.float32 True

</div>

As opposed to Python types, a comparison using `is` should not be used.

First, NumPy treats data type specifications (everything that can be passed to the <span class="title-ref">dtype</span> constructor) as equivalent to the data type object itself. This equivalence can only be handled through `==`, not through `is`.

<div class="admonition">

Example

A <span class="title-ref">dtype</span> object is equal to all data type specifications that are equivalent to it.

\>\>\> import numpy as np

\>\>\> a = np.array(\[1, 2\], dtype=float) \>\>\> a.dtype == np.dtype(np.float64) True \>\>\> a.dtype == np.float64 True \>\>\> a.dtype == float True \>\>\> a.dtype == "float64" True \>\>\> a.dtype == "d" True

</div>

Second, there is no guarantee that data type objects are singletons.

<div class="admonition">

Example

Do not use `is` because data type objects may or may not be singletons.

\>\>\> import numpy as np

\>\>\> np.dtype(float) is np.dtype(float) True \>\>\> np.dtype(\[('a', float)\]) is np.dtype(\[('a', float)\]) False

</div>

## <span class="title-ref">dtype</span>

NumPy data type descriptions are instances of the <span class="title-ref">dtype</span> class.

### Attributes

The type of the data is described by the following <span class="title-ref">dtype</span> attributes:

<div class="autosummary" data-toctree="generated/">

dtype.type dtype.kind dtype.char dtype.num dtype.str

</div>

Size of the data is in turn described by:

<div class="autosummary" data-toctree="generated/">

dtype.name dtype.itemsize

</div>

Endianness of this data:

<div class="autosummary" data-toctree="generated/">

dtype.byteorder

</div>

Information about sub-data-types in a `structured data type`:

<div class="autosummary" data-toctree="generated/">

dtype.fields dtype.names

</div>

For data types that describe sub-arrays:

<div class="autosummary" data-toctree="generated/">

dtype.subdtype dtype.shape

</div>

Attributes providing additional information:

<div class="autosummary" data-toctree="generated/">

dtype.hasobject dtype.flags dtype.isbuiltin dtype.isnative dtype.descr dtype.alignment dtype.base

</div>

Metadata attached by the user:

<div class="autosummary" data-toctree="generated/">

dtype.metadata

</div>

### Methods

Data types have the following method for changing the byte order:

<div class="autosummary" data-toctree="generated/">

dtype.newbyteorder

</div>

The following methods implement the pickle protocol:

<div class="autosummary" data-toctree="generated/">

dtype.\_\_reduce\_\_ dtype.\_\_setstate\_\_

</div>

Utility method for typing:

<div class="autosummary" data-toctree="generated/">

dtype.\_\_class\_getitem\_\_

</div>

Comparison operations:

<div class="autosummary" data-toctree="generated/">

dtype.\_\_ge\_\_ dtype.\_\_gt\_\_ dtype.\_\_le\_\_ dtype.\_\_lt\_\_

</div>

---

arrays.interface.md

---

<div class="index">

pair: array; interface pair: array; protocol

</div>

# The array interface protocol

\> **Note** \> This page describes the NumPy-specific API for accessing the contents of a NumPy array from other C extensions. `3118` -- :c\`The Revised Buffer Protocol \<PyObject\_GetBuffer\><span class="title-ref"> introduces similar, standardized API to Python 2.6 and 3.0 for any extension module to use. Cython\_\_'s buffer array support uses the :pep:\`3118</span> API; see the [Cython NumPy tutorial](https://cython.org/). Cython provides a way to write code that supports the buffer protocol with Python versions older than 2.6 because it has a backward-compatible implementation utilizing the array interface described here.

  - version  
    3

The array interface (sometimes called array protocol) was created in 2005 as a means for array-like Python objects to reuse each other's data buffers intelligently whenever possible. The homogeneous N-dimensional array interface is a default mechanism for objects to share N-dimensional array memory and information. The interface consists of a Python-side and a C-side using two attributes. Objects wishing to be considered an N-dimensional array in application code should support at least one of these attributes. Objects wishing to support an N-dimensional array in application code should look for at least one of these attributes and use the information provided appropriately.

This interface describes homogeneous arrays in the sense that each item of the array has the same "type". This type can be very simple or it can be a quite arbitrary and complicated C-like structure.

There are two ways to use the interface: A Python side and a C-side. Both are separate attributes.

## Python side

This approach to the interface consists of the object having an <span class="title-ref">\~object.\_\_array\_interface\_\_</span> attribute.

<div class="data">

object.\_\_[array\_interface](https://github.com/cython/cython/wiki/tutorials-numpy)

A dictionary of items (3 required and 5 optional). The optional keys in the dictionary have implied defaults if they are not provided.

The keys are:

  - **shape** (required)  
    Tuple whose elements are the array size in each dimension. Each entry is an integer (a Python :py\`int\`). Note that these integers could be larger than the platform `int` or `long` could hold (a Python :py\`int\` is a C `long`). It is up to the code using this attribute to handle this appropriately; either by raising an error when overflow is possible, or by using `long long` as the C type for the shapes.

  - **typestr** (required)  
    A string providing the basic type of the homogeneous array The basic string format consists of 3 parts: a character describing the byteorder of the data (`<`: little-endian, `>`: big-endian, `|`: not-relevant), a character code giving the basic type of the array, and an integer providing the number of bytes the type uses.
    
    The basic type character codes are:
    
    |     |                                                                          |
    | --- | ------------------------------------------------------------------------ |
    | `t` | Bit field (following integer gives the number of bits in the bit field). |
    | `b` | Boolean (integer type where all values are only `True` or `False`)       |
    | `i` | Integer                                                                  |
    | `u` | Unsigned integer                                                         |
    | `f` | Floating point                                                           |
    | `c` | Complex floating point                                                   |
    | `m` | Timedelta                                                                |
    | `M` | Datetime                                                                 |
    | `O` | Object (i.e. the memory contains a pointer to :c`PyObject`)              |
    | `S` | String (fixed-length sequence of char)                                   |
    | `U` | Unicode (fixed-length sequence of :c`Py_UCS4`)                           |
    | `V` | Other (void \* -- each item is a fixed-size chunk of memory)             |

  - **descr** (optional)  
    A list of tuples providing a more detailed description of the memory layout for each item in the homogeneous array. Each tuple in the list has two or three elements. Normally, this attribute would be used when *typestr* is `V[0-9]+`, but this is not a requirement. The only requirement is that the number of bytes represented in the *typestr* key is the same as the total number of bytes represented here. The idea is to support descriptions of C-like structs that make up array elements. The elements of each tuple in the list are
    
      - 1\. A string providing a name associated with this portion of  
        the datatype. This could also be a tuple of \`\`('full name',
    
    'basic\_name')`where basic name would be a valid Python     variable name representing the full name of the field.  2. Either a basic-type description string as in *typestr* or    another list (for nested structured types)  3. An optional shape tuple providing how many times this part    of the structure should be repeated.  No repeats are assumed    if this is not given.  Very complicated structures can be    described using this generic interface.  Notice, however,    that each element of the array is still of the same    data-type.  Some examples of using this interface are given    below.  **Default**:`\[('', typestr)\]\`\`

  - **data** (optional)  
    A 2-tuple whose first argument is a \[Python integer \<python:c-api/long\>\](Python integer \<python:c-api/long\>.md) that points to the data-area storing the array contents.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    When converting from C/C++ via `PyLong_From*` or high-level bindings such as Cython or pybind11, make sure to use an integer of sufficiently large bitness.
    
    </div>
    
    This pointer must point to the first element of data (in other words any offset is always ignored in this case). The second entry in the tuple is a read-only flag (true means the data area is read-only).
    
    This attribute can also be an object exposing the \[buffer interface \<bufferobjects\>\](\#buffer-interface-\<bufferobjects\>) which will be used to share the data. If this key is not present (or returns None), then memory sharing will be done through the buffer interface of the object itself. In this case, the offset key can be used to indicate the start of the buffer. A reference to the object exposing the array interface must be stored by the new object if the memory area is to be secured.
    
    **Default**: `None`

  - **strides** (optional)  
    Either `None` to indicate a C-style contiguous array or a tuple of strides which provides the number of bytes needed to jump to the next array element in the corresponding dimension. Each entry must be an integer (a Python :py\`int\`). As with shape, the values may be larger than can be represented by a C `int` or `long`; the calling code should handle this appropriately, either by raising an error, or by using `long long` in C. The default is `None` which implies a C-style contiguous memory buffer. In this model, the last dimension of the array varies the fastest. For example, the default strides tuple for an object whose array entries are 8 bytes long and whose shape is `(10, 20, 30)` would be `(4800, 240, 8)`.
    
    **Default**: `None` (C-style contiguous)

  - **mask** (optional)  
    `None` or an object exposing the array interface. All elements of the mask array should be interpreted only as true or not true indicating which elements of this array are valid. The shape of this object should be <span class="title-ref">"broadcastable" \<arrays.broadcasting.broadcastable\></span> to the shape of the original array.
    
    **Default**: `None` (All array values are valid)

  - **offset** (optional)  
    An integer offset into the array data region. This can only be used when data is `None` or returns a <span class="title-ref">memoryview</span> object.
    
    **Default**: `0`.

  - **version** (required)  
    An integer showing the version of the interface (i.e. 3 for this version). Be careful not to use this to invalidate objects exposing future versions of the interface.

</div>

## C-struct access

This approach to the array interface allows for faster access to an array using only one attribute lookup and a well-defined C-structure.

<div class="data">

object.\_\_array\_struct\_\_

A :c`PyCapsule` whose `pointer` member contains a pointer to a filled :c`PyArrayInterface` structure. Memory for the structure is dynamically created and the :c`PyCapsule` is also created with an appropriate destructor so the retriever of this attribute simply has to apply :c\`Py\_DECREF()<span class="title-ref"> to the object returned by this attribute when it is finished. Also, either the data needs to be copied out, or a reference to the object exposing this attribute must be held to ensure the data is not freed. Objects exposing the :obj:</span>\_\_array\_struct\_\_\` interface must also not reallocate their memory if other objects are referencing them.

</div>

The :c`PyArrayInterface` structure is defined in `numpy/ndarrayobject.h` as:

    typedef struct {
      int two;              /* contains the integer 2 -- simple sanity check */
      int nd;               /* number of dimensions */
      char typekind;        /* kind in array --- character code of typestr */
      int itemsize;         /* size of each element */
      int flags;            /* flags indicating how the data should be interpreted */
                            /*   must set ARR_HAS_DESCR bit to validate descr */
      Py_ssize_t *shape;    /* A length-nd array of shape information */
      Py_ssize_t *strides;  /* A length-nd array of stride information */
      void *data;           /* A pointer to the first element of the array */
      PyObject *descr;      /* NULL or data-description (same as descr key
                                    of __array_interface__) -- must set ARR_HAS_DESCR
                                    flag or this will be ignored. */
    } PyArrayInterface;

The flags member may consist of 5 bits showing how the data should be interpreted and one bit showing how the Interface should be interpreted. The data-bits are :c`NPY_ARRAY_C_CONTIGUOUS` (0x1), :c`NPY_ARRAY_F_CONTIGUOUS` (0x2), :c`NPY_ARRAY_ALIGNED` (0x100), :c`NPY_ARRAY_NOTSWAPPED` (0x200), and :c`NPY_ARRAY_WRITEABLE` (0x400). A final flag :c`NPY_ARR_HAS_DESCR` (0x800) indicates whether or not this structure has the arrdescr field. The field should not be accessed unless this flag is present.

<div class="admonition">

New since June 16, 2006:

In the past most implementations used the `desc` member of the `PyCObject` (now :c`PyCapsule`) itself (do not confuse this with the "descr" member of the :c`PyArrayInterface` structure above --- they are two separate things) to hold the pointer to the object exposing the interface. This is now an explicit part of the interface. Be sure to take a reference to the object and call :c\`PyCapsule\_SetContext\` before returning the :c`PyCapsule`, and configure a destructor to decref this reference.

</div>

\> **Note** \> `~object.__array_struct__` is considered legacy and should not be used for new code. Use the \[buffer protocol \<python:c-api/buffer\>\](buffer protocol \<python:c-api/buffer\>.md) or the DLPack protocol <span class="title-ref">numpy.from\_dlpack</span> instead.

## Type description examples

For clarity it is useful to provide some examples of the type description and corresponding <span class="title-ref">\~object.\_\_array\_interface\_\_</span> 'descr' entries. Thanks to Scott Gilbert for these examples:

In every case, the 'descr' key is optional, but of course provides more information which may be important for various applications:

    * Float data
        typestr == '>f4'
        descr == [('','>f4')]
    
    * Complex double
        typestr == '>c8'
        descr == [('real','>f4'), ('imag','>f4')]
    
    * RGB Pixel data
        typestr == '|V3'
        descr == [('r','|u1'), ('g','|u1'), ('b','|u1')]
    
    * Mixed endian (weird but could happen).
        typestr == '|V8' (or '>u8')
        descr == [('big','>i4'), ('little','<i4')]
    
    * Nested structure
        struct {
            int ival;
            struct {
                unsigned short sval;
                unsigned char bval;
                unsigned char cval;
            } sub;
        }
        typestr == '|V8' (or '<u8' if you want)
        descr == [('ival','<i4'), ('sub', [('sval','<u2'), ('bval','|u1'), ('cval','|u1') ]) ]
    
    * Nested array
        struct {
            int ival;
            double data[16*4];
        }
        typestr == '|V516'
        descr == [('ival','>i4'), ('data','>f8',(16,4))]
    
    * Padded structure
        struct {
            int ival;
            double dval;
        }
        typestr == '|V16'
        descr == [('ival','>i4'),('','|V4'),('dval','>f8')]

It should be clear that any structured type could be described using this interface.

## Differences with array interface (version 2)

The version 2 interface was very similar. The differences were largely aesthetic. In particular:

1.  The PyArrayInterface structure had no descr member at the end (and therefore no flag ARR\_HAS\_DESCR)

<!-- end list -->

  - 2\. The `context` member of the :c`PyCapsule` (formally the `desc`  
    member of the `PyCObject`) returned from `__array_struct__` was not specified. Usually, it was the object exposing the array (so that a reference to it could be kept and destroyed when the C-object was destroyed). It is now an explicit requirement that this field be used in some way to hold a reference to the owning object.
    
    \> **Note**

  - \>  
    Until August 2020, this said:
    
    > Now it must be a tuple whose first element is a string with "PyArrayInterface Version \#" and whose second element is the object exposing the array.
    
    This design was retracted almost immediately after it was proposed, in \<<https://mail.python.org/pipermail/numpy-discussion/2006-June/020995.html>\>. Despite 14 years of documentation to the contrary, at no point was it valid to assume that `__array_interface__` capsules held this tuple content.

  - 3\. The tuple returned from `__array_interface__['data']` used to be a  
    hex-string (now it is an integer or a long integer).

  - 4\. There was no `__array_interface__` attribute instead all of the keys  
    (except for version) in the `__array_interface__` dictionary were their own attribute: Thus to obtain the Python-side information you had to access separately the attributes:
    
      - `__array_data__`
      - `__array_shape__`
      - `__array_strides__`
      - `__array_typestr__`
      - `__array_descr__`
      - `__array_offset__`
      - `__array_mask__`

---

arrays.md

---

# Array objects

<div class="currentmodule">

numpy

</div>

NumPy provides an N-dimensional array type, the \[ndarray \<arrays.ndarray\>\](\#ndarray \<arrays.ndarray\>), which describes a collection of "items" of the same type. The items can be \[indexed \<arrays.indexing\>\](\#indexed-\<arrays.indexing\>) using for example N integers.

All ndarrays are `homogeneous`: every item takes up the same size block of memory, and all blocks are interpreted in exactly the same way. How each item in the array is to be interpreted is specified by a separate \[data-type object \<arrays.dtypes\>\](\#data-type-object-\<arrays.dtypes\>), one of which is associated with every array. In addition to basic types (integers, floats, *etc.*), the data type objects can also represent data structures.

An item extracted from an array, *e.g.*, by indexing, is represented by a Python object whose type is one of the \[array scalar types \<arrays.scalars\>\](\#array-scalar-types \<arrays.scalars\>) built in NumPy. The array scalars allow easy manipulation of also more complicated arrangements of data.

![**Figure** Conceptual diagram showing the relationship between the three fundamental objects used to describe the data in an array: 1) the ndarray itself, 2) the data-type object that describes the layout of a single fixed-size element of the array, 3) the array-scalar Python object that is returned when a single element of the array is accessed.](figures/threefundamental.png)

<div class="toctree" data-maxdepth="2">

arrays.ndarray arrays.scalars arrays.dtypes arrays.promotion arrays.nditer arrays.classes maskedarray arrays.interface arrays.datetime

</div>

---

arrays.ndarray.md

---

<div class="currentmodule">

numpy

</div>

# The N-dimensional array (<span class="title-ref">ndarray</span>)

An <span class="title-ref">ndarray</span> is a (usually fixed-size) multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its <span class="title-ref">shape \<ndarray.shape\></span>, which is a <span class="title-ref">tuple</span> of *N* non-negative integers that specify the sizes of each dimension. The type of items in the array is specified by a separate \[data-type object (dtype) \<arrays.dtypes\>\](\#data-type-object-(dtype)-\<arrays.dtypes\>), one of which is associated with each ndarray.

As with other container objects in Python, the contents of an <span class="title-ref">ndarray</span> can be accessed and modified by \[indexing or slicing \<arrays.indexing\>\](\#indexing-or slicing-\<arrays.indexing\>) the array (using, for example, *N* integers), and via the methods and attributes of the <span class="title-ref">ndarray</span>.

<div class="index">

view, base

</div>

Different <span class="title-ref">ndarrays \<ndarray\></span> can share the same data, so that changes made in one <span class="title-ref">ndarray</span> may be visible in another. That is, an ndarray can be a *"view"* to another ndarray, and the data it is referring to is taken care of by the *"base"* ndarray. ndarrays can also be views to memory owned by Python <span class="title-ref">strings \<str\></span> or objects implementing the <span class="title-ref">memoryview</span> or \[array \<arrays.interface\>\](\#array \<arrays.interface\>) interfaces.

<div class="admonition">

Example

A 2-dimensional array of size 2 x 3, composed of 4-byte integer elements:

\>\>\> import numpy as np

\>\>\> x = np.array(\[\[1, 2, 3\], \[4, 5, 6\]\], np.int32) \>\>\> type(x) \<class 'numpy.ndarray'\> \>\>\> x.shape (2, 3) \>\>\> x.dtype dtype('int32')

The array can be indexed using Python container-like syntax:

\>\>\> \# The element of x in the *second* row, *third* column, namely, 6. \>\>\> x\[1, 2\] 6

For example \[slicing \<arrays.indexing\>\](\#slicing-\<arrays.indexing\>) can produce views of the array:

\>\>\> y = x\[:,1\] \>\>\> y array(\[2, 5\], dtype=int32) \>\>\> y\[0\] = 9 \# this also changes the corresponding element in x \>\>\> y array(\[9, 5\], dtype=int32) \>\>\> x array(\[\[1, 9, 3\], \[4, 5, 6\]\], dtype=int32)

</div>

## Constructing arrays

New arrays can be constructed using the routines detailed in \[routines.array-creation\](\#routines.array-creation), and also by using the low-level <span class="title-ref">ndarray</span> constructor:

<div class="autosummary" data-toctree="generated/">

ndarray

</div>

## Indexing arrays

Arrays can be indexed using an extended Python slicing syntax, `array[selection]`. Similar syntax is also used for accessing fields in a `structured data type`.

<div class="seealso">

\[Array Indexing \<arrays.indexing\>\](\#array-indexing-\<arrays.indexing\>).

</div>

## Internal memory layout of an ndarray

An instance of class <span class="title-ref">ndarray</span> consists of a contiguous one-dimensional segment of computer memory (owned by the array, or by some other object), combined with an indexing scheme that maps *N* integers into the location of an item in the block. The ranges in which the indices can vary is specified by the `shape
<ndarray.shape>` of the array. How many bytes each item takes and how the bytes are interpreted is defined by the \[data-type object \<arrays.dtypes\>\](\#data-type-object \<arrays.dtypes\>) associated with the array.

<div class="index">

C-order, Fortran-order, row-major, column-major, stride, offset

</div>

A segment of memory is inherently 1-dimensional, and there are many different schemes for arranging the items of an *N*-dimensional array in a 1-dimensional block. NumPy is flexible, and <span class="title-ref">ndarray</span> objects can accommodate any *strided indexing scheme*. In a strided scheme, the N-dimensional index \((n_0, n_1, ..., n_{N-1})\) corresponds to the offset (in bytes):

\[n_{\mathrm{offset}} = \sum_{k=0}^{N-1} s_k n_k\]

from the beginning of the memory block associated with the array. Here, \(s_k\) are integers which specify the `strides
<ndarray.strides>` of the array. The `column-major` order (used, for example, in the Fortran language and in *Matlab*) and `row-major` order (used in C) schemes are just specific kinds of strided scheme, and correspond to memory that can be *addressed* by the strides:

\[s_k^{\mathrm{column}} = \mathrm{itemsize} \prod_{j=0}^{k-1} d_j ,
\quad  s_k^{\mathrm{row}} = \mathrm{itemsize} \prod_{j=k+1}^{N-1} d_j .\]

<div class="index">

single-segment, contiguous, non-contiguous

</div>

where \(d_j\) <span class="title-ref">= self.shape\[j\]</span>.

Both the C and Fortran orders are `contiguous`, *i.e.,* single-segment, memory layouts, in which every part of the memory block can be accessed by some combination of the indices.

\> **Note** \> *Contiguous arrays* and *single-segment arrays* are synonymous and are used interchangeably throughout the documentation.

While a C-style and Fortran-style contiguous array, which has the corresponding flags set, can be addressed with the above strides, the actual strides may be different. This can happen in two cases:

1.  If `self.shape[k] == 1` then for any legal index `index[k] == 0`. This means that in the formula for the offset \(n_k = 0\) and thus \(s_k n_k = 0\) and the value of \(s_k\) <span class="title-ref">= self.strides\[k\]</span> is arbitrary.
2.  If an array has no elements (`self.size == 0`) there is no legal index and the strides are never used. Any array with no elements may be considered C-style and Fortran-style contiguous.

Point 1. means that `self` and `self.squeeze()` always have the same contiguity and `aligned` flags value. This also means that even a high dimensional array could be C-style and Fortran-style contiguous at the same time.

<div class="index">

aligned

</div>

An array is considered aligned if the memory offsets for all elements and the base offset itself is a multiple of <span class="title-ref">self.itemsize \<ndarray.itemsize\></span>. Understanding *memory-alignment* leads to better performance on most hardware.

\> **Warning** \> It does *not* generally hold that `self.strides[-1] == self.itemsize` for C-style contiguous arrays or `self.strides[0] == self.itemsize` for Fortran-style contiguous arrays is true.

Data in new <span class="title-ref">ndarrays \<ndarray\></span> is in the `row-major` (C) order, unless otherwise specified, but, for example, \[basic array slicing \<arrays.indexing\>\](\#basic array-slicing-\<arrays.indexing\>) often produces `views <view>` in a different scheme.

\> **Note** \> Several algorithms in NumPy work on arbitrarily strided arrays. However, some algorithms require single-segment arrays. When an irregularly strided array is passed in to such algorithms, a copy is automatically made.

## Array attributes

Array attributes reflect information that is intrinsic to the array itself. Generally, accessing an array through its attributes allows you to get and sometimes set intrinsic properties of the array without creating a new array. The exposed attributes are the core parts of an array and only some of them can be reset meaningfully without creating a new array. Information on each attribute is given below.

### Memory layout

The following attributes contain information about the memory layout of the array:

<div class="autosummary" data-toctree="generated/">

ndarray.flags ndarray.shape ndarray.strides ndarray.ndim ndarray.data ndarray.size ndarray.itemsize ndarray.nbytes ndarray.base

</div>

### Data type

<div class="seealso">

\[Data type objects \<arrays.dtypes\>\](\#data-type-objects-\<arrays.dtypes\>)

</div>

The data type object associated with the array can be found in the <span class="title-ref">dtype \<ndarray.dtype\></span> attribute:

<div class="autosummary" data-toctree="generated/">

ndarray.dtype

</div>

### Other attributes

<div class="autosummary" data-toctree="generated/">

ndarray.T ndarray.real ndarray.imag ndarray.flat

</div>

### Array interface

<div class="seealso">

\[arrays.interface\](\#arrays.interface).

</div>

|                               |                                    |
| ----------------------------- | ---------------------------------- |
| `~object.__array_interface__` | Python-side of the array interface |
| `~object.__array_struct__`    | C-side of the array interface      |

### `ctypes` foreign function interface

<div class="autosummary" data-toctree="generated/">

ndarray.ctypes

</div>

## Array methods

An <span class="title-ref">ndarray</span> object has many methods which operate on or with the array in some fashion, typically returning an array result. These methods are briefly explained below. (Each method's docstring has a more complete description.)

For the following methods there are also corresponding functions in `numpy`: <span class="title-ref">all</span>, <span class="title-ref">any</span>, <span class="title-ref">argmax</span>, <span class="title-ref">argmin</span>, <span class="title-ref">argpartition</span>, <span class="title-ref">argsort</span>, <span class="title-ref">choose</span>, <span class="title-ref">clip</span>, <span class="title-ref">compress</span>, <span class="title-ref">copy</span>, <span class="title-ref">cumprod</span>, <span class="title-ref">cumsum</span>, <span class="title-ref">diagonal</span>, <span class="title-ref">imag</span>, <span class="title-ref">max \<amax\></span>, <span class="title-ref">mean</span>, <span class="title-ref">min \<amin\></span>, <span class="title-ref">nonzero</span>, <span class="title-ref">partition</span>, <span class="title-ref">prod</span>, <span class="title-ref">put</span>, <span class="title-ref">ravel</span>, <span class="title-ref">real</span>, <span class="title-ref">repeat</span>, <span class="title-ref">reshape</span>, <span class="title-ref">round \<around\></span>, <span class="title-ref">searchsorted</span>, <span class="title-ref">sort</span>, <span class="title-ref">squeeze</span>, <span class="title-ref">std</span>, <span class="title-ref">sum</span>, <span class="title-ref">swapaxes</span>, <span class="title-ref">take</span>, <span class="title-ref">trace</span>, <span class="title-ref">transpose</span>, <span class="title-ref">var</span>.

### Array conversion

<div class="autosummary" data-toctree="generated/">

ndarray.item ndarray.tolist ndarray.tostring ndarray.tobytes ndarray.tofile ndarray.dump ndarray.dumps ndarray.astype ndarray.byteswap ndarray.copy ndarray.view ndarray.getfield ndarray.setflags ndarray.fill

</div>

### Shape manipulation

For reshape, resize, and transpose, the single tuple argument may be replaced with `n` integers which will be interpreted as an n-tuple.

<div class="autosummary" data-toctree="generated/">

ndarray.reshape ndarray.resize ndarray.transpose ndarray.swapaxes ndarray.flatten ndarray.ravel ndarray.squeeze

</div>

### Item selection and manipulation

For array methods that take an *axis* keyword, it defaults to *None*. If axis is *None*, then the array is treated as a 1-D array. Any other value for *axis* represents the dimension along which the operation should proceed.

<div class="autosummary" data-toctree="generated/">

ndarray.take ndarray.put ndarray.repeat ndarray.choose ndarray.sort ndarray.argsort ndarray.partition ndarray.argpartition ndarray.searchsorted ndarray.nonzero ndarray.compress ndarray.diagonal

</div>

### Calculation

<div class="index">

axis

</div>

Many of these methods take an argument named *axis*. In such cases,

  - If *axis* is *None* (the default), the array is treated as a 1-D array and the operation is performed over the entire array. This behavior is also the default if self is a 0-dimensional array or array scalar. (An array scalar is an instance of the types/classes float32, float64, etc., whereas a 0-dimensional array is an ndarray instance containing precisely one array scalar.)
  - If *axis* is an integer, then the operation is done over the given axis (for each 1-D subarray that can be created along the given axis).

<div class="admonition">

Example of the *axis* argument

A 3-dimensional array of size 3 x 3 x 3, summed over each of its three axes:

\>\>\> import numpy as np

\>\>\> x = np.arange(27).reshape((3,3,3)) \>\>\> x array(\[\[\[ 0, 1, 2\], \[ 3, 4, 5\], \[ 6, 7, 8\]\], \[\[ 9, 10, 11\], \[12, 13, 14\], \[15, 16, 17\]\], \[\[18, 19, 20\], \[21, 22, 23\], \[24, 25, 26\]\]\]) \>\>\> x.sum(axis=0) array(\[\[27, 30, 33\], \[36, 39, 42\], \[45, 48, 51\]\]) \>\>\> \# for sum, axis is the first keyword, so we may omit it, \>\>\> \# specifying only its value \>\>\> x.sum(0), x.sum(1), x.sum(2) (array(\[\[27, 30, 33\], \[36, 39, 42\], \[45, 48, 51\]\]), array(\[\[ 9, 12, 15\], \[36, 39, 42\], \[63, 66, 69\]\]), array(\[\[ 3, 12, 21\], \[30, 39, 48\], \[57, 66, 75\]\]))

</div>

The parameter *dtype* specifies the data type over which a reduction operation (like summing) should take place. The default reduce data type is the same as the data type of *self*. To avoid overflow, it can be useful to perform the reduction using a larger data type.

For several methods, an optional *out* argument can also be provided and the result will be placed into the output array given. The *out* argument must be an <span class="title-ref">ndarray</span> and have the same number of elements. It can have a different data type in which case casting will be performed.

<div class="autosummary" data-toctree="generated/">

ndarray.max ndarray.argmax ndarray.min ndarray.argmin ndarray.clip ndarray.conj ndarray.round ndarray.trace ndarray.sum ndarray.cumsum ndarray.mean ndarray.var ndarray.std ndarray.prod ndarray.cumprod ndarray.all ndarray.any

</div>

## Arithmetic, matrix multiplication, and comparison operations

<div class="index">

comparison, arithmetic, matrix, operation, operator

</div>

Arithmetic and comparison operations on <span class="title-ref">ndarrays \<ndarray\></span> are defined as element-wise operations, and generally yield <span class="title-ref">ndarray</span> objects as results.

Each of the arithmetic operations (`+`, `-`, `*`, `/`, `//`, `%`, `divmod()`, `**` or `pow()`, `<<`, `>>`, `&`, `^`, `|`, `~`) and the comparisons (`==`, `<`, `>`, `<=`, `>=`, `!=`) is equivalent to the corresponding universal function (or `ufunc` for short) in NumPy. For more information, see the section on \[Universal Functions \<ufuncs\>\](\#universal-functions \<ufuncs\>).

Comparison operators:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_lt\_\_ ndarray.\_\_le\_\_ ndarray.\_\_gt\_\_ ndarray.\_\_ge\_\_ ndarray.\_\_eq\_\_ ndarray.\_\_ne\_\_

</div>

Truth value of an array (<span class="title-ref">bool() \<bool\></span>):

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_bool\_\_

</div>

\> **Note** \> Truth-value testing of an array invokes <span class="title-ref">ndarray.\_\_bool\_\_</span>, which raises an error if the number of elements in the array is not 1, because the truth value of such arrays is ambiguous. Use <span class="title-ref">.any() \<ndarray.any\></span> and <span class="title-ref">.all() \<ndarray.all\></span> instead to be clear about what is meant in such cases. (If you wish to check for whether an array is empty, use for example `.size > 0`.)

Unary operations:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_neg\_\_ ndarray.\_\_pos\_\_ ndarray.\_\_abs\_\_ ndarray.\_\_invert\_\_

</div>

Arithmetic:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_add\_\_ ndarray.\_\_sub\_\_ ndarray.\_\_mul\_\_ ndarray.\_\_truediv\_\_ ndarray.\_\_floordiv\_\_ ndarray.\_\_mod\_\_ ndarray.\_\_divmod\_\_ ndarray.\_\_pow\_\_ ndarray.\_\_lshift\_\_ ndarray.\_\_rshift\_\_ ndarray.\_\_and\_\_ ndarray.\_\_or\_\_ ndarray.\_\_xor\_\_

</div>

<div class="note">

<div class="title">

Note

</div>

  - Any third argument to <span class="title-ref">pow()</span> is silently ignored, as the underlying <span class="title-ref">ufunc \<power\></span> takes only two arguments.
  - Because <span class="title-ref">ndarray</span> is a built-in type (written in C), the `__r{op}__` special methods are not directly defined.
  - The functions called to implement many arithmetic special methods for arrays can be modified using <span class="title-ref">\_\_array\_ufunc\_\_ \<numpy.class.\_\_array\_ufunc\_\_\></span>.

</div>

Arithmetic, in-place:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_iadd\_\_ ndarray.\_\_isub\_\_ ndarray.\_\_imul\_\_ ndarray.\_\_itruediv\_\_ ndarray.\_\_ifloordiv\_\_ ndarray.\_\_imod\_\_ ndarray.\_\_ipow\_\_ ndarray.\_\_ilshift\_\_ ndarray.\_\_irshift\_\_ ndarray.\_\_iand\_\_ ndarray.\_\_ior\_\_ ndarray.\_\_ixor\_\_

</div>

\> **Warning** \> In place operations will perform the calculation using the precision decided by the data type of the two operands, but will silently downcast the result (if necessary) so it can fit back into the array. Therefore, for mixed precision calculations, `A {op}=    B` can be different than `A = A {op} B`. For example, suppose `a = ones((3,3))`. Then, `a += 3j` is different than `a = a +    3j`: while they both perform the same computation, `a += 3` casts the result to fit back in `a`, whereas `a = a + 3j` re-binds the name `a` to the result.

Matrix Multiplication:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_matmul\_\_

</div>

<div class="note">

<div class="title">

Note

</div>

Matrix operators `@` and `@=` were introduced in Python 3.5 following `465`, and the `@` operator has been introduced in NumPy 1.10.0. Further information can be found in the <span class="title-ref">matmul</span> documentation.

</div>

## Special methods

For standard library functions:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_copy\_\_ ndarray.\_\_deepcopy\_\_ ndarray.\_\_reduce\_\_ ndarray.\_\_setstate\_\_

</div>

Basic customization:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_new\_\_ ndarray.\_\_array\_\_ ndarray.\_\_array\_wrap\_\_

</div>

Container customization: (see \[Indexing \<arrays.indexing\>\](\#indexing-\<arrays.indexing\>))

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_len\_\_ ndarray.\_\_getitem\_\_ ndarray.\_\_setitem\_\_ ndarray.\_\_contains\_\_

</div>

Conversion; the operations <span class="title-ref">int() \<int\></span>, <span class="title-ref">float() \<float\></span> and <span class="title-ref">complex() \<complex\></span>. They work only on arrays that have one element in them and return the appropriate scalar.

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_int\_\_ ndarray.\_\_float\_\_ ndarray.\_\_complex\_\_

</div>

String representations:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_str\_\_ ndarray.\_\_repr\_\_

</div>

Utility method for typing:

<div class="autosummary" data-toctree="generated/">

ndarray.\_\_class\_getitem\_\_

</div>

---

arrays.nditer.cython.md

---

# Putting the inner loop in Cython

Those who want really good performance out of their low level operations should strongly consider directly using the iteration API provided in C, but for those who are not comfortable with C or C++, Cython is a good middle ground with reasonable performance tradeoffs. For the <span class="title-ref">\~numpy.nditer</span> object, this means letting the iterator take care of broadcasting, dtype conversion, and buffering, while giving the inner loop to Cython.

For our example, we'll create a sum of squares function. To start, let's implement this function in straightforward Python. We want to support an 'axis' parameter similar to the numpy <span class="title-ref">sum</span> function, so we will need to construct a list for the <span class="title-ref">op\_axes</span> parameter. Here's how this looks.

<div class="admonition">

Example

\>\>\> def axis\_to\_axeslist(axis, ndim): ... if axis is None: ... return \[-1\] \* ndim ... else: ... if type(axis) is not tuple: ... axis = (axis,) ... axeslist = \[1\] \* ndim ... for i in axis: ... axeslist\[i\] = -1 ... ax = 0 ... for i in range(ndim): ... if axeslist\[i\] \!= -1: ... axeslist\[i\] = ax ... ax += 1 ... return axeslist ... \>\>\> def sum\_squares\_py(arr, axis=None, out=None): ... axeslist = axis\_to\_axeslist(axis, arr.ndim) ... it = np.nditer(\[arr, out\], flags=\['reduce\_ok', ... 'buffered', 'delay\_bufalloc'\], ... op\_flags=\[\['readonly'\], \['readwrite', 'allocate'\]\], ... op\_axes=\[None, axeslist\], ... op\_dtypes=\['float64', 'float64'\]) ... with it: ... it.operands\[1\]\[...\] = 0 ... it.reset() ... for x, y in it: ... y\[...\] += x\*x ... return it.operands\[1\] ... \>\>\> a = np.arange(6).reshape(2,3) \>\>\> sum\_squares\_py(a) array(55.) \>\>\> sum\_squares\_py(a, axis=-1) array(\[ 5., 50.\])

</div>

To Cython-ize this function, we replace the inner loop (y\[...\] += x\*x) with Cython code that's specialized for the float64 dtype. With the 'external\_loop' flag enabled, the arrays provided to the inner loop will always be one-dimensional, so very little checking needs to be done.

Here's the listing of sum\_squares.pyx:

    import numpy as np
    cimport numpy as np
    cimport cython
    
    def axis_to_axeslist(axis, ndim):
        if axis is None:
            return [-1] * ndim
        else:
            if type(axis) is not tuple:
                axis = (axis,)
            axeslist = [1] * ndim
            for i in axis:
                axeslist[i] = -1
            ax = 0
            for i in range(ndim):
                if axeslist[i] != -1:
                    axeslist[i] = ax
                    ax += 1
            return axeslist
    
    @cython.boundscheck(False)
    def sum_squares_cy(arr, axis=None, out=None):
        cdef np.ndarray[double] x
        cdef np.ndarray[double] y
        cdef int size
        cdef double value
    
        axeslist = axis_to_axeslist(axis, arr.ndim)
        it = np.nditer([arr, out], flags=['reduce_ok', 'external_loop',
                                          'buffered', 'delay_bufalloc'],
                    op_flags=[['readonly'], ['readwrite', 'allocate']],
                    op_axes=[None, axeslist],
                    op_dtypes=['float64', 'float64'])
        with it:
            it.operands[1][...] = 0
            it.reset()
            for xarr, yarr in it:
                x = xarr
                y = yarr
                size = x.shape[0]
                for i in range(size):
                   value = x[i]
                   y[i] = y[i] + value * value
            return it.operands[1]

On this machine, building the .pyx file into a module looked like the following, but you may have to find some Cython tutorials to tell you the specifics for your system configuration.:

    $ cython sum_squares.pyx
    $ gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -I/usr/include/python2.7 -fno-strict-aliasing -o sum_squares.so sum_squares.c

Running this from the Python interpreter produces the same answers as our native Python/NumPy code did.

<div class="admonition">

Example

\>\>\> from sum\_squares import sum\_squares\_cy \#doctest: +SKIP \>\>\> a = np.arange(6).reshape(2,3) \>\>\> sum\_squares\_cy(a) \#doctest: +SKIP array(55.0) \>\>\> sum\_squares\_cy(a, axis=-1) \#doctest: +SKIP array(\[ 5., 50.\])

</div>

Doing a little timing in IPython shows that the reduced overhead and memory allocation of the Cython inner loop is providing a very nice speedup over both the straightforward Python code and an expression using NumPy's built-in sum function.:

    >>> a = np.random.rand(1000,1000)
    
    >>> timeit sum_squares_py(a, axis=-1)
    10 loops, best of 3: 37.1 ms per loop
    
    >>> timeit np.sum(a*a, axis=-1)
    10 loops, best of 3: 20.9 ms per loop
    
    >>> timeit sum_squares_cy(a, axis=-1)
    100 loops, best of 3: 11.8 ms per loop
    
    >>> np.all(sum_squares_cy(a, axis=-1) == np.sum(a*a, axis=-1))
    True
    
    >>> np.all(sum_squares_py(a, axis=-1) == np.sum(a*a, axis=-1))
    True

---

arrays.nditer.md

---

<div class="currentmodule">

numpy

</div>

# Iterating over arrays

\> **Note** \> Arrays support the iterator protocol and can be iterated over like Python lists. See the \[quickstart.indexing-slicing-and-iterating\](\#quickstart.indexing-slicing-and-iterating) section in the Quickstart guide for basic usage and examples. The remainder of this document presents the <span class="title-ref">nditer</span> object and covers more advanced usage.

The iterator object <span class="title-ref">nditer</span>, introduced in NumPy 1.6, provides many flexible ways to visit all the elements of one or more arrays in a systematic fashion. This page introduces some basic ways to use the object for computations on arrays in Python, then concludes with how one can accelerate the inner loop in Cython. Since the Python exposure of <span class="title-ref">nditer</span> is a relatively straightforward mapping of the C array iterator API, these ideas will also provide help working with array iteration from C or C++.

## Single array iteration

The most basic task that can be done with the <span class="title-ref">nditer</span> is to visit every element of an array. Each element is provided one by one using the standard Python iterator interface.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> for x in np.nditer(a): ... print(x, end=' ') ... 0 1 2 3 4 5

</div>

An important thing to be aware of for this iteration is that the order is chosen to match the memory layout of the array instead of using a standard C or Fortran ordering. This is done for access efficiency, reflecting the idea that by default one simply wants to visit each element without concern for a particular ordering. We can see this by iterating over the transpose of our previous array, compared to taking a copy of that transpose in C order.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> for x in np.nditer(a.T): ... print(x, end=' ') ... 0 1 2 3 4 5

\>\>\> for x in np.nditer(a.T.copy(order='C')): ... print(x, end=' ') ... 0 3 1 4 2 5

</div>

The elements of both <span class="title-ref">a</span> and <span class="title-ref">a.T</span> get traversed in the same order, namely the order they are stored in memory, whereas the elements of <span class="title-ref">a.T.copy(order='C')</span> get visited in a different order because they have been put into a different memory layout.

### Controlling iteration order

There are times when it is important to visit the elements of an array in a specific order, irrespective of the layout of the elements in memory. The <span class="title-ref">nditer</span> object provides an <span class="title-ref">order</span> parameter to control this aspect of iteration. The default, having the behavior described above, is order='K' to keep the existing order. This can be overridden with order='C' for C order and order='F' for Fortran order.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> for x in np.nditer(a, order='F'): ... print(x, end=' ') ... 0 3 1 4 2 5 \>\>\> for x in np.nditer(a.T, order='C'): ... print(x, end=' ') ... 0 3 1 4 2 5

</div>

### Modifying array values

By default, the <span class="title-ref">nditer</span> treats the input operand as a read-only object. To be able to modify the array elements, you must specify either read-write or write-only mode using the <span class="title-ref">'readwrite'</span> or <span class="title-ref">'writeonly'</span> per-operand flags.

The nditer will then yield writeable buffer arrays which you may modify. However, because the nditer must copy this buffer data back to the original array once iteration is finished, you must signal when the iteration is ended, by one of two methods. You may either:

  - used the nditer as a context manager using the `with` statement, and the temporary data will be written back when the context is exited.
  - call the iterator's <span class="title-ref">\~nditer.close</span> method once finished iterating, which will trigger the write-back.

The nditer can no longer be iterated once either <span class="title-ref">\~nditer.close</span> is called or its context is exited.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> a array(\[\[0, 1, 2\], \[3, 4, 5\]\]) \>\>\> with np.nditer(a, op\_flags=\['readwrite'\]) as it: ... for x in it: ... x\[...\] = 2 \* x ... \>\>\> a array(\[\[ 0, 2, 4\], \[ 6, 8, 10\]\])

</div>

If you are writing code that needs to support older versions of numpy, note that prior to 1.15, <span class="title-ref">nditer</span> was not a context manager and did not have a <span class="title-ref">\~nditer.close</span> method. Instead it relied on the destructor to initiate the writeback of the buffer.

### Using an external loop

In all the examples so far, the elements of <span class="title-ref">a</span> are provided by the iterator one at a time, because all the looping logic is internal to the iterator. While this is simple and convenient, it is not very efficient. A better approach is to move the one-dimensional innermost loop into your code, external to the iterator. This way, NumPy's vectorized operations can be used on larger chunks of the elements being visited.

The <span class="title-ref">nditer</span> will try to provide chunks that are as large as possible to the inner loop. By forcing 'C' and 'F' order, we get different external loop sizes. This mode is enabled by specifying an iterator flag.

Observe that with the default of keeping native memory order, the iterator is able to provide a single one-dimensional chunk, whereas when forcing Fortran order, it has to provide three chunks of two elements each.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> for x in np.nditer(a, flags=\['external\_loop'\]): ... print(x, end=' ') ... \[0 1 2 3 4 5\]

\>\>\> for x in np.nditer(a, flags=\['external\_loop'\], order='F'): ... print(x, end=' ') ... \[0 3\] \[1 4\] \[2 5\]

</div>

### Tracking an index or multi-index

During iteration, you may want to use the index of the current element in a computation. For example, you may want to visit the elements of an array in memory order, but use a C-order, Fortran-order, or multidimensional index to look up values in a different array.

The index is tracked by the iterator object itself, and accessible through the <span class="title-ref">index</span> or <span class="title-ref">multi\_index</span> properties, depending on what was requested. The examples below show printouts demonstrating the progression of the index:

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> it = np.nditer(a, flags=\['f\_index'\]) \>\>\> for x in it: ... print("%d \<%d\>" % (x, it.index), end=' ') ... 0 \<0\> 1 \<2\> 2 \<4\> 3 \<1\> 4 \<3\> 5 \<5\>

\>\>\> it = np.nditer(a, flags=\['multi\_index'\]) \>\>\> for x in it: ... print("%d \<%s\>" % (x, it.multi\_index), end=' ') ... 0 \<(0, 0)\> 1 \<(0, 1)\> 2 \<(0, 2)\> 3 \<(1, 0)\> 4 \<(1, 1)\> 5 \<(1, 2)\>

\>\>\> with np.nditer(a, flags=\['multi\_index'\], op\_flags=\['writeonly'\]) as it: ... for x in it: ... x\[...\] = it.multi\_index\[1\] - it.multi\_index\[0\] ... \>\>\> a array(\[\[ 0, 1, 2\], \[-1, 0, 1\]\])

</div>

Tracking an index or multi-index is incompatible with using an external loop, because it requires a different index value per element. If you try to combine these flags, the <span class="title-ref">nditer</span> object will raise an exception.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.zeros((2,3)) \>\>\> it = np.nditer(a, flags=\['c\_index', 'external\_loop'\]) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> ValueError: Iterator flag EXTERNAL\_LOOP cannot be used if an index or multi-index is being tracked

</div>

### Alternative looping and element access

To make its properties more readily accessible during iteration, <span class="title-ref">nditer</span> has an alternative syntax for iterating, which works explicitly with the iterator object itself. With this looping construct, the current value is accessible by indexing into the iterator. Other properties, such as tracked indices remain as before. The examples below produce identical results to the ones in the previous section.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> it = np.nditer(a, flags=\['f\_index'\]) \>\>\> while not it.finished: ... print("%d \<%d\>" % (it\[0\], it.index), end=' ') ... is\_not\_finished = it.iternext() ... 0 \<0\> 1 \<2\> 2 \<4\> 3 \<1\> 4 \<3\> 5 \<5\>

\>\>\> it = np.nditer(a, flags=\['multi\_index'\]) \>\>\> while not it.finished: ... print("%d \<%s\>" % (it\[0\], it.multi\_index), end=' ') ... is\_not\_finished = it.iternext() ... 0 \<(0, 0)\> 1 \<(0, 1)\> 2 \<(0, 2)\> 3 \<(1, 0)\> 4 \<(1, 1)\> 5 \<(1, 2)\>

\>\>\> with np.nditer(a, flags=\['multi\_index'\], op\_flags=\['writeonly'\]) as it: ... while not it.finished: ... it\[0\] = it.multi\_index\[1\] - it.multi\_index\[0\] ... is\_not\_finished = it.iternext() ... \>\>\> a array(\[\[ 0, 1, 2\], \[-1, 0, 1\]\])

</div>

### Buffering the array elements

When forcing an iteration order, we observed that the external loop option may provide the elements in smaller chunks because the elements can't be visited in the appropriate order with a constant stride. When writing C code, this is generally fine, however in pure Python code this can cause a significant reduction in performance.

By enabling buffering mode, the chunks provided by the iterator to the inner loop can be made larger, significantly reducing the overhead of the Python interpreter. In the example forcing Fortran iteration order, the inner loop gets to see all the elements in one go when buffering is enabled.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) \>\>\> for x in np.nditer(a, flags=\['external\_loop'\], order='F'): ... print(x, end=' ') ... \[0 3\] \[1 4\] \[2 5\]

\>\>\> for x in np.nditer(a, flags=\['external\_loop','buffered'\], order='F'): ... print(x, end=' ') ... \[0 3 1 4 2 5\]

</div>

### Iterating as a specific data type

There are times when it is necessary to treat an array as a different data type than it is stored as. For instance, one may want to do all computations on 64-bit floats, even if the arrays being manipulated are 32-bit floats. Except when writing low-level C code, it's generally better to let the iterator handle the copying or buffering instead of casting the data type yourself in the inner loop.

There are two mechanisms which allow this to be done, temporary copies and buffering mode. With temporary copies, a copy of the entire array is made with the new data type, then iteration is done in the copy. Write access is permitted through a mode which updates the original array after all the iteration is complete. The major drawback of temporary copies is that the temporary copy may consume a large amount of memory, particularly if the iteration data type has a larger itemsize than the original one.

Buffering mode mitigates the memory usage issue and is more cache-friendly than making temporary copies. Except for special cases, where the whole array is needed at once outside the iterator, buffering is recommended over temporary copying. Within NumPy, buffering is used by the ufuncs and other functions to support flexible inputs with minimal memory overhead.

In our examples, we will treat the input array with a complex data type, so that we can take square roots of negative numbers. Without enabling copies or buffering mode, the iterator will raise an exception if the data type doesn't match precisely.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) - 3 \>\>\> for x in np.nditer(a, op\_dtypes=\['complex128'\]): ... print(np.sqrt(x), end=' ') ... Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> TypeError: Iterator operand required copying or buffering, but neither copying nor buffering was enabled

</div>

In copying mode, 'copy' is specified as a per-operand flag. This is done to provide control in a per-operand fashion. Buffering mode is specified as an iterator flag.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6).reshape(2,3) - 3 \>\>\> for x in np.nditer(a, op\_flags=\['readonly','copy'\], ... op\_dtypes=\['complex128'\]): ... print(np.sqrt(x), end=' ') ... 1.7320508075688772j 1.4142135623730951j 1j 0j (1+0j) (1.4142135623730951+0j)

\>\>\> for x in np.nditer(a, flags=\['buffered'\], op\_dtypes=\['complex128'\]): ... print(np.sqrt(x), end=' ') ... 1.7320508075688772j 1.4142135623730951j 1j 0j (1+0j) (1.4142135623730951+0j)

</div>

The iterator uses NumPy's casting rules to determine whether a specific conversion is permitted. By default, it enforces 'safe' casting. This means, for example, that it will raise an exception if you try to treat a 64-bit float array as a 32-bit float array. In many cases, the rule 'same\_kind' is the most reasonable rule to use, since it will allow conversion from 64 to 32-bit float, but not from float to int or from complex to float.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6.) \>\>\> for x in np.nditer(a, flags=\['buffered'\], op\_dtypes=\['float32'\]): ... print(x, end=' ') ... Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> TypeError: Iterator operand 0 dtype could not be cast from dtype('float64') to dtype('float32') according to the rule 'safe'

\>\>\> for x in np.nditer(a, flags=\['buffered'\], op\_dtypes=\['float32'\], ... casting='same\_kind'): ... print(x, end=' ') ... 0.0 1.0 2.0 3.0 4.0 5.0

\>\>\> for x in np.nditer(a, flags=\['buffered'\], op\_dtypes=\['int32'\], casting='same\_kind'): ... print(x, end=' ') ... Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> TypeError: Iterator operand 0 dtype could not be cast from dtype('float64') to dtype('int32') according to the rule 'same\_kind'

</div>

One thing to watch out for is conversions back to the original data type when using a read-write or write-only operand. A common case is to implement the inner loop in terms of 64-bit floats, and use 'same\_kind' casting to allow the other floating-point types to be processed as well. While in read-only mode, an integer array could be provided, read-write mode will raise an exception because conversion back to the array would violate the casting rule.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(6) \>\>\> for x in np.nditer(a, flags=\['buffered'\], op\_flags=\['readwrite'\], ... op\_dtypes=\['float64'\], casting='same\_kind'): ... x\[...\] = x / 2.0 ... Traceback (most recent call last): File "\<stdin\>", line 2, in \<module\> TypeError: Iterator requested dtype could not be cast from dtype('float64') to dtype('int64'), the operand 0 dtype, according to the rule 'same\_kind'

</div>

## Broadcasting array iteration

NumPy has a set of rules for dealing with arrays that have differing shapes which are applied whenever functions take multiple operands which combine element-wise. This is called \[broadcasting \<ufuncs.broadcasting\>\](\#broadcasting-\<ufuncs.broadcasting\>). The <span class="title-ref">nditer</span> object can apply these rules for you when you need to write such a function.

As an example, we print out the result of broadcasting a one and a two dimensional array together.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(3) \>\>\> b = np.arange(6).reshape(2,3) \>\>\> for x, y in np.nditer(\[a,b\]): ... print("%d:%d" % (x,y), end=' ') ... 0:0 1:1 2:2 0:3 1:4 2:5

</div>

When a broadcasting error occurs, the iterator raises an exception which includes the input shapes to help diagnose the problem.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(2) \>\>\> b = np.arange(6).reshape(2,3) \>\>\> for x, y in np.nditer(\[a,b\]): ... print("%d:%d" % (x,y), end=' ') ... Traceback (most recent call last): ... ValueError: operands could not be broadcast together with shapes (2,) (2,3)

</div>

### Iterator-allocated output arrays

A common case in NumPy functions is to have outputs allocated based on the broadcasting of the input, and additionally have an optional parameter called 'out' where the result will be placed when it is provided. The <span class="title-ref">nditer</span> object provides a convenient idiom that makes it very easy to support this mechanism.

We'll show how this works by creating a function <span class="title-ref">square</span> which squares its input. Let's start with a minimal function definition excluding 'out' parameter support.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> def square(a): ... with np.nditer(\[a, None\]) as it: ... for x, y in it: ... y\[...\] = x\*x ... return it.operands\[1\] ... \>\>\> square(\[1,2,3\]) array(\[1, 4, 9\])

</div>

By default, the <span class="title-ref">nditer</span> uses the flags 'allocate' and 'writeonly' for operands that are passed in as None. This means we were able to provide just the two operands to the iterator, and it handled the rest.

When adding the 'out' parameter, we have to explicitly provide those flags, because if someone passes in an array as 'out', the iterator will default to 'readonly', and our inner loop would fail. The reason 'readonly' is the default for input arrays is to prevent confusion about unintentionally triggering a reduction operation. If the default were 'readwrite', any broadcasting operation would also trigger a reduction, a topic which is covered later in this document.

While we're at it, let's also introduce the 'no\_broadcast' flag, which will prevent the output from being broadcast. This is important, because we only want one input value for each output. Aggregating more than one input value is a reduction operation which requires special handling. It would already raise an error because reductions must be explicitly enabled in an iterator flag, but the error message that results from disabling broadcasting is much more understandable for end-users. To see how to generalize the square function to a reduction, look at the sum of squares function in the section about Cython.

For completeness, we'll also add the 'external\_loop' and 'buffered' flags, as these are what you will typically want for performance reasons.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> def square(a, out=None): ... it = np.nditer(\[a, out\], ... flags = \['external\_loop', 'buffered'\], ... op\_flags = \[\['readonly'\], ... \['writeonly', 'allocate', 'no\_broadcast'\]\]) ... with it: ... for x, y in it: ... y\[...\] = x\*x ... return it.operands\[1\] ...

\>\>\> square(\[1,2,3\]) array(\[1, 4, 9\])

\>\>\> b = np.zeros((3,)) \>\>\> square(\[1,2,3\], out=b) array(\[1., 4., 9.\]) \>\>\> b array(\[1., 4., 9.\])

\>\>\> square(np.arange(6).reshape(2,3), out=b) Traceback (most recent call last): ... ValueError: non-broadcastable output operand with shape (3,) doesn't match the broadcast shape (2,3)

</div>

### Outer product iteration

Any binary operation can be extended to an array operation in an outer product fashion like in <span class="title-ref">outer</span>, and the <span class="title-ref">nditer</span> object provides a way to accomplish this by explicitly mapping the axes of the operands. It is also possible to do this with <span class="title-ref">newaxis</span> indexing, but we will show you how to directly use the nditer <span class="title-ref">op\_axes</span> parameter to accomplish this with no intermediate views.

We'll do a simple outer product, placing the dimensions of the first operand before the dimensions of the second operand. The <span class="title-ref">op\_axes</span> parameter needs one list of axes for each operand, and provides a mapping from the iterator's axes to the axes of the operand.

Suppose the first operand is one dimensional and the second operand is two dimensional. The iterator will have three dimensions, so <span class="title-ref">op\_axes</span> will have two 3-element lists. The first list picks out the one axis of the first operand, and is -1 for the rest of the iterator axes, with a final result of \[0, -1, -1\]. The second list picks out the two axes of the second operand, but shouldn't overlap with the axes picked out in the first operand. Its list is \[-1, 0, 1\]. The output operand maps onto the iterator axes in the standard manner, so we can provide None instead of constructing another list.

The operation in the inner loop is a straightforward multiplication. Everything to do with the outer product is handled by the iterator setup.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(3) \>\>\> b = np.arange(8).reshape(2,4) \>\>\> it = np.nditer(\[a, b, None\], flags=\['external\_loop'\], ... op\_axes=\[\[0, -1, -1\], \[-1, 0, 1\], None\]) \>\>\> with it: ... for x, y, z in it: ... z\[...\] = x\*y ... result = it.operands\[2\] \# same as z ... \>\>\> result array(\[\[\[ 0, 0, 0, 0\], \[ 0, 0, 0, 0\]\], \[\[ 0, 1, 2, 3\], \[ 4, 5, 6, 7\]\], \[\[ 0, 2, 4, 6\], \[ 8, 10, 12, 14\]\]\])

</div>

Note that once the iterator is closed we can not access <span class="title-ref">operands \<nditer.operands\></span> and must use a reference created inside the context manager.

### Reduction iteration

Whenever a writeable operand has fewer elements than the full iteration space, that operand is undergoing a reduction. The <span class="title-ref">nditer</span> object requires that any reduction operand be flagged as read-write, and only allows reductions when 'reduce\_ok' is provided as an iterator flag.

For a simple example, consider taking the sum of all elements in an array.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(24).reshape(2,3,4) \>\>\> b = np.array(0) \>\>\> with np.nditer(\[a, b\], flags=\['reduce\_ok'\], ... op\_flags=\[\['readonly'\], \['readwrite'\]\]) as it: ... for x,y in it: ... y\[...\] += x ... \>\>\> b array(276) \>\>\> np.sum(a) 276

</div>

Things are a little bit more tricky when combining reduction and allocated operands. Before iteration is started, any reduction operand must be initialized to its starting values. Here's how we can do this, taking sums along the last axis of <span class="title-ref">a</span>.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(24).reshape(2,3,4) \>\>\> it = np.nditer(\[a, None\], flags=\['reduce\_ok'\], ... op\_flags=\[\['readonly'\], \['readwrite', 'allocate'\]\], ... op\_axes=\[None, \[0,1,-1\]\]) \>\>\> with it: ... it.operands\[1\]\[...\] = 0 ... for x, y in it: ... y\[...\] += x ... result = it.operands\[1\] ... \>\>\> result array(\[\[ 6, 22, 38\], \[54, 70, 86\]\]) \>\>\> np.sum(a, axis=2) array(\[\[ 6, 22, 38\], \[54, 70, 86\]\])

</div>

To do buffered reduction requires yet another adjustment during the setup. Normally the iterator construction involves copying the first buffer of data from the readable arrays into the buffer. Any reduction operand is readable, so it may be read into a buffer. Unfortunately, initialization of the operand after this buffering operation is complete will not be reflected in the buffer that the iteration starts with, and garbage results will be produced.

The iterator flag "delay\_bufalloc" is there to allow iterator-allocated reduction operands to exist together with buffering. When this flag is set, the iterator will leave its buffers uninitialized until it receives a reset, after which it will be ready for regular iteration. Here's how the previous example looks if we also enable buffering.

<div class="admonition">

Example

\>\>\> import numpy as np

\>\>\> a = np.arange(24).reshape(2,3,4) \>\>\> it = np.nditer(\[a, None\], flags=\['reduce\_ok', ... 'buffered', 'delay\_bufalloc'\], ... op\_flags=\[\['readonly'\], \['readwrite', 'allocate'\]\], ... op\_axes=\[None, \[0,1,-1\]\]) \>\>\> with it: ... it.operands\[1\]\[...\] = 0 ... it.reset() ... for x, y in it: ... y\[...\] += x ... result = it.operands\[1\] ... \>\>\> result array(\[\[ 6, 22, 38\], \[54, 70, 86\]\])

</div>

## Putting the inner loop in Cython

Those who want really good performance out of their low level operations should strongly consider directly using the iteration API provided in C, but for those who are not comfortable with C or C++, Cython is a good middle ground with reasonable performance tradeoffs. For the <span class="title-ref">\~numpy.nditer</span> object, this means letting the iterator take care of broadcasting, dtype conversion, and buffering, while giving the inner loop to Cython.

For our example, we'll create a sum of squares function. To start, let's implement this function in straightforward Python. We want to support an 'axis' parameter similar to the numpy <span class="title-ref">sum</span> function, so we will need to construct a list for the <span class="title-ref">op\_axes</span> parameter. Here's how this looks.

<div class="admonition">

Example

\>\>\> def axis\_to\_axeslist(axis, ndim): ... if axis is None: ... return \[-1\] \* ndim ... else: ... if type(axis) is not tuple: ... axis = (axis,) ... axeslist = \[1\] \* ndim ... for i in axis: ... axeslist\[i\] = -1 ... ax = 0 ... for i in range(ndim): ... if axeslist\[i\] \!= -1: ... axeslist\[i\] = ax ... ax += 1 ... return axeslist ... \>\>\> def sum\_squares\_py(arr, axis=None, out=None): ... axeslist = axis\_to\_axeslist(axis, arr.ndim) ... it = np.nditer(\[arr, out\], flags=\['reduce\_ok', ... 'buffered', 'delay\_bufalloc'\], ... op\_flags=\[\['readonly'\], \['readwrite', 'allocate'\]\], ... op\_axes=\[None, axeslist\], ... op\_dtypes=\['float64', 'float64'\]) ... with it: ... it.operands\[1\]\[...\] = 0 ... it.reset() ... for x, y in it: ... y\[...\] += x\*x ... return it.operands\[1\] ... \>\>\> a = np.arange(6).reshape(2,3) \>\>\> sum\_squares\_py(a) array(55.) \>\>\> sum\_squares\_py(a, axis=-1) array(\[ 5., 50.\])

</div>

To Cython-ize this function, we replace the inner loop (y\[...\] += x\*x) with Cython code that's specialized for the float64 dtype. With the 'external\_loop' flag enabled, the arrays provided to the inner loop will always be one-dimensional, so very little checking needs to be done.

Here's the listing of sum\_squares.pyx:

    import numpy as np
    cimport numpy as np
    cimport cython
    
    def axis_to_axeslist(axis, ndim):
        if axis is None:
            return [-1] * ndim
        else:
            if type(axis) is not tuple:
                axis = (axis,)
            axeslist = [1] * ndim
            for i in axis:
                axeslist[i] = -1
            ax = 0
            for i in range(ndim):
                if axeslist[i] != -1:
                    axeslist[i] = ax
                    ax += 1
            return axeslist
    
    @cython.boundscheck(False)
    def sum_squares_cy(arr, axis=None, out=None):
        cdef np.ndarray[double] x
        cdef np.ndarray[double] y
        cdef int size
        cdef double value
    
        axeslist = axis_to_axeslist(axis, arr.ndim)
        it = np.nditer([arr, out], flags=['reduce_ok', 'external_loop',
                                          'buffered', 'delay_bufalloc'],
                    op_flags=[['readonly'], ['readwrite', 'allocate']],
                    op_axes=[None, axeslist],
                    op_dtypes=['float64', 'float64'])
        with it:
            it.operands[1][...] = 0
            it.reset()
            for xarr, yarr in it:
                x = xarr
                y = yarr
                size = x.shape[0]
                for i in range(size):
                   value = x[i]
                   y[i] = y[i] + value * value
            return it.operands[1]

On this machine, building the .pyx file into a module looked like the following, but you may have to find some Cython tutorials to tell you the specifics for your system configuration.:

    $ cython sum_squares.pyx
    $ gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -I/usr/include/python2.7 -fno-strict-aliasing -o sum_squares.so sum_squares.c

Running this from the Python interpreter produces the same answers as our native Python/NumPy code did.

<div class="admonition">

Example

\>\>\> from sum\_squares import sum\_squares\_cy \#doctest: +SKIP \>\>\> a = np.arange(6).reshape(2,3) \>\>\> sum\_squares\_cy(a) \#doctest: +SKIP array(55.0) \>\>\> sum\_squares\_cy(a, axis=-1) \#doctest: +SKIP array(\[ 5., 50.\])

</div>

Doing a little timing in IPython shows that the reduced overhead and memory allocation of the Cython inner loop is providing a very nice speedup over both the straightforward Python code and an expression using NumPy's built-in sum function.:

    >>> a = np.random.rand(1000,1000)
    
    >>> timeit sum_squares_py(a, axis=-1)
    10 loops, best of 3: 37.1 ms per loop
    
    >>> timeit np.sum(a*a, axis=-1)
    10 loops, best of 3: 20.9 ms per loop
    
    >>> timeit sum_squares_cy(a, axis=-1)
    100 loops, best of 3: 11.8 ms per loop
    
    >>> np.all(sum_squares_cy(a, axis=-1) == np.sum(a*a, axis=-1))
    True
    
    >>> np.all(sum_squares_py(a, axis=-1) == np.sum(a*a, axis=-1))
    True

---

arrays.promotion.md

---

<div class="currentmodule">

numpy

</div>

# Data type promotion in NumPy

When mixing two different data types, NumPy has to determine the appropriate dtype for the result of the operation. This step is referred to as *promotion* or *finding the common dtype*.

In typical cases, the user does not need to worry about the details of promotion, since the promotion step usually ensures that the result will either match or exceed the precision of the input.

For example, when the inputs are of the same dtype, the dtype of the result matches the dtype of the inputs:

> \>\>\> np.int8(1) + np.int8(1) np.int8(2)

Mixing two different dtypes normally produces a result with the dtype of the higher precision input:

> \>\>\> np.int8(4) + np.int64(8) \# 64 \> 8 np.int64(12) \>\>\> np.float32(3) + np.float16(3) \# 32 \> 16 np.float32(6.0)

In typical cases, this does not lead to surprises. However, if you work with non-default dtypes like unsigned integers and low-precision floats, or if you mix NumPy integers, NumPy floats, and Python scalars, some details of NumPy promotion rules may be relevant. Note that these detailed rules do not always match those of other languages\[1\].

Numerical dtypes come in four "kinds" with a natural hierarchy.

1.  unsigned integers (`uint`)
2.  signed integers (`int`)
3.  float (`float`)
4.  complex (`complex`)

In addition to kind, NumPy numerical dtypes also have an associated precision, specified in bits. Together, the kind and precision specify the dtype. For example, a `uint8` is an unsigned integer stored using 8 bits.

The result of an operation will always be of an equal or higher kind of any of the inputs. Furthermore, the result will always have a precision greater than or equal to those of the inputs. Already, this can lead to some examples which may be unexpected:

1.  When mixing floating point numbers and integers, the precision of the integer may force the result to a higher precision floating point. For example, the result of an operation involving `int64` and `float16` is `float64`.
2.  When mixing unsigned and signed integers with the same precision, the result will have *higher* precision than either inputs. Additionally, if one of them has 64bit precision already, no higher precision integer is available and for example an operation involving `int64` and `uint64` gives `float64`.

Please see the <span class="title-ref">Numerical promotion</span> section and image below for details on both.

## Detailed behavior of Python scalars

Since NumPy 2.0\[2\], an important point in our promotion rules is that although operations involving two NumPy dtypes never lose precision, operations involving a NumPy dtype and a Python scalar (`int`, `float`, or `complex`) *can* lose precision. For instance, it is probably intuitive that the result of an operation between a Python integer and a NumPy integer should be a NumPy integer. However, Python integers have arbitrary precision whereas all NumPy dtypes have fixed precision, so the arbitrary precision of Python integers cannot be preserved.

More generally, NumPy considers the "kind" of Python scalars, but ignores their precision when determining the result dtype. This is often convenient. For instance, when working with arrays of a low precision dtype, it is usually desirable for simple operations with Python scalars to preserve the dtype.

> \>\>\> arr\_float32 = np.array(\[1, 2.5, 2.1\], dtype="float32") \>\>\> arr\_float32 + 10.0 \# undesirable to promote to float64 array(\[11. , 12.5, 12.1\], dtype=float32) \>\>\> arr\_int16 = np.array(\[3, 5, 7\], dtype="int16") \>\>\> arr\_int16 + 10 \# undesirable to promote to int64 array(\[13, 15, 17\], dtype=int16)

In both cases, the result precision is dictated by the NumPy dtype. Because of this, `arr_float32 + 3.0` behaves the same as `arr_float32 + np.float32(3.0)`, and `arr_int16 + 10` behaves as `arr_int16 + np.int16(10.)`.

As another example, when mixing NumPy integers with a Python `float` or `complex`, the result always has type `float64` or `complex128`:

> \>\> np.int16(1) + 1.0 np.float64(2.0)

However, these rules can also lead to surprising behavior when working with low precision dtypes.

First, since the Python value is converted to a NumPy one before the operation can by performed, operations can fail with an error when the result seems obvious. For instance, `np.int8(1) + 1000` cannot continue because `1000` exceeds the maximum value of an `int8`. When the Python scalar cannot be coerced to the NumPy dtype, an error is raised:

> \>\>\> np.int8(1) + 1000 Traceback (most recent call last): ... OverflowError: Python integer 1000 out of bounds for int8 \>\>\> np.int64(1) \* 10\*\*100 Traceback (most recent call last): ... OverflowError: Python int too large to convert to C long \>\>\> np.float32(1) + 1e300 np.float32(inf) ... RuntimeWarning: overflow encountered in cast

Second, since the Python float or integer precision is always ignored, a low precision NumPy scalar will keep using its lower precision unless explicitly converted to a higher precision NumPy dtype or Python scalar (e.g. via `int()`, `float()`, or `scalar.item()`). This lower precision may be detrimental to some calculations or lead to incorrect results, especially in the case of integer overflows:

> \>\>\> np.int8(100) + 100 \# the result exceeds the capacity of int8 np.int8(-56) ... RuntimeWarning: overflow encountered in scalar add

Note that NumPy warns when overflows occur for scalars, but not for arrays; e.g., `np.array(100, dtype="uint8") + 100` will *not* warn.

## Numerical promotion

The following image shows the numerical promotion rules with the kinds on the vertical axis and the precision on the horizontal axis.

![](figures/nep-0050-promotion-no-fonts.svg)

The input dtype with the higher kind determines the kind of the result dtype. The result dtype has a precision as low as possible without appearing to the left of either input dtype in the diagram.

Note the following specific rules and observations:

1.  When a Python `float` or `complex` interacts with a NumPy integer the result will be `float64` or `complex128` (yellow border). NumPy booleans will also be cast to the default integer\[3\]. This is not relevant when additionally NumPy floating point values are involved.
2.  The precision is drawn such that `float16 < int16 < uint16` because large `uint16` do not fit `int16` and large `int16` will lose precision when stored in a `float16`. This pattern however is broken since NumPy always considers `float64` and `complex128` to be acceptable promotion results for any integer value.
3.  A special case is that NumPy promotes many combinations of signed and unsigned integers to `float64`. A higher kind is used here because no signed integer dtype is sufficiently precise to hold a `uint64`.

## Exceptions to the general promotion rules

In NumPy promotion refers to what specific functions do with the result and in some cases, this means that NumPy may deviate from what the <span class="title-ref">np.result\_type</span> would give.

### Behavior of `sum` and `prod`

`np.sum` and `np.prod` will always return the default integer type when summing over integer values (or booleans). This is usually an `int64`. The reason for this is that integer summations are otherwise very likely to overflow and give confusing results. This rule also applies to the underlying `np.add.reduce` and `np.multiply.reduce`.

### Notable behavior with NumPy or Python integer scalars

NumPy promotion refers to the result dtype and operation precision, but the operation will sometimes dictate that result. Division always returns floating point values and comparison always booleans.

This leads to what may appear as "exceptions" to the rules:

  - NumPy comparisons with Python integers or mixed precision integers always return the correct result. The inputs will never be cast in a way which loses precision.
  - Equality comparisons between types which cannot be promoted will be considered all `False` (equality) or all `True` (not-equal).
  - Unary math functions like `np.sin` that always return floating point values, accept any Python integer input by converting it to `float64`.
  - Division always returns floating point values and thus also allows divisions between any NumPy integer with any Python integer value by casting both to `float64`.

In principle, some of these exceptions may make sense for other functions. Please raise an issue if you feel this is the case.

## Promotion of non-numerical datatypes

NumPy extends the promotion to non-numerical types, although in many cases promotion is not well defined and simply rejected.

The following rules apply:

  - NumPy byte strings (`np.bytes_`) can be promoted to unicode strings (`np.str_`). However, casting the bytes to unicode will fail for non-ascii characters.
  - For some purposes NumPy will promote almost any other datatype to strings. This applies to array creation or concatenation.
  - The array constructors like `np.array()` will use `object` dtype when there is no viable promotion.
  - Structured dtypes can promote when their field names and order matches. In that case all fields are promoted individually.
  - NumPy `timedelta` can in some cases promote with integers.

<div class="note">

<div class="title">

Note

</div>

Some of these rules are somewhat surprising, and are being considered for change in the future. However, any backward-incompatible changes have to be weighed against the risks of breaking existing code. Please raise an issue if you have particular ideas about how promotion should work.

</div>

## Details of promoted `dtype` instances

The above discussion has mainly dealt with the behavior when mixing different DType classes. A `dtype` instance attached to an array can carry additional information such as byte-order, metadata, string length, or exact structured dtype layout.

While the string length or field names of a structured dtype are important, NumPy considers byte-order, metadata, and the exact layout of a structured dtype as storage details. During promotion NumPy does *not* take these storage details into account: \* Byte-order is converted to native byte-order. \* Metadata attached to the dtype may or may not be preserved. \* Resulting structured dtypes will be packed (but aligned if inputs were).

This behaviors is the best behavior for most programs where storage details are not relevant to the final results and where the use of incorrect byte-order could drastically slow down evaluation.

1.  To a large degree, this may just be for choices made early on in NumPy's predecessors. For more details, see \[NEP 50 \<NEP50\>\](\#nep-50-\<nep50\>).

2.  See also \[NEP 50 \<NEP50\>\](\#nep-50-\<nep50\>) which changed the rules for NumPy 2.0. Previous versions of NumPy would sometimes return higher precision results based on the input value of Python scalars. Further, previous versions of NumPy would typically ignore the higher precision of NumPy scalars or 0-D arrays for promotion purposes.

3.  The default integer is marked as `int64` in the schema but is `int32` on 32bit platforms. However, normal PCs are 64bit.

---

arrays.scalars.md

---

# Scalars

<div class="currentmodule">

numpy

</div>

Python defines only one type of a particular data class (there is only one integer type, one floating-point type, etc.). This can be convenient in applications that don't need to be concerned with all the ways data can be represented in a computer. For scientific computing, however, more control is often needed.

In NumPy, there are 24 new fundamental Python types to describe different types of scalars. These type descriptors are mostly based on the types available in the C language that CPython is written in, with several additional types compatible with Python's types.

Array scalars have the same attributes and methods as <span class="title-ref">ndarrays \<ndarray\></span>.\[1\] This allows one to treat items of an array partly on the same footing as arrays, smoothing out rough edges that result when mixing scalar and array operations.

Array scalars live in a hierarchy (see the Figure below) of data types. They can be detected using the hierarchy: For example, `isinstance(val, np.generic)` will return :py\`True\` if *val* is an array scalar object. Alternatively, what kind of array scalar is present can be determined using other members of the data type hierarchy. Thus, for example `isinstance(val, np.complexfloating)` will return :py\`True\` if *val* is a complex valued type, while `isinstance(val, np.flexible)` will return true if *val* is one of the flexible itemsize array types (<span class="title-ref">str\_</span>, <span class="title-ref">bytes\_</span>, <span class="title-ref">void</span>).

![**Figure:** Hierarchy of type objects representing the array data types. Not shown are the two integer types <span class="title-ref">intp</span> and <span class="title-ref">uintp</span> which are used for indexing (the same as the default integer since NumPy 2).](figures/dtype-hierarchy.png)

## Built-in scalar types<span id="arrays.scalars.character-codes"></span>

The built-in scalar types are shown below. The C-like names are associated with character codes, which are shown in their descriptions. Use of the character codes, however, is discouraged.

Some of the scalar types are essentially equivalent to fundamental Python types and therefore inherit from them as well as from the generic array scalar type:

<table>
<thead>
<tr class="header">
<th>Array scalar type</th>
<th>Related Python type</th>
<th>Inherits?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><span class="title-ref">int_</span> <span class="title-ref">int</span> <span class="title-ref">double</span> <span class="title-ref">float</span> <span class="title-ref">cdouble</span> <span class="title-ref">comple `bytes_</span> <span class="title-ref">bytes</span> <span class="title-ref">str_</span> <span class="title-ref">str</span> <span class="title-ref">bool_</span> <span class="title-ref">bool</span> <span class="title-ref">datetime64</span> <span class="title-ref">dateti `timedelta64</span> `dateti</p></td>
<td><blockquote>
<p>Python 2 only yes</p>
</blockquote>
<dl>
<dt>x` yes</dt>
<dd><p>yes yes no</p>
</dd>
</dl>
<p>me.datetime` no me.timedelta` no</p></td>
<td></td>
</tr>
</tbody>
</table>

The <span class="title-ref">bool\_</span> data type is very similar to the Python <span class="title-ref">bool</span> but does not inherit from it because Python's <span class="title-ref">bool</span> does not allow itself to be inherited from, and on the C-level the size of the actual bool data is not the same as a Python Boolean scalar.

\> **Warning** \> The <span class="title-ref">int\_</span> type does **not** inherit from the <span class="title-ref">int</span> built-in under Python 3, because type <span class="title-ref">int</span> is no longer a fixed-width integer type.

<div class="tip">

<div class="title">

Tip

</div>

The default data type in NumPy is <span class="title-ref">double</span>.

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.generic

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.number

</div>

### Integer types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.integer

</div>

\> **Note** \> The numpy integer types mirror the behavior of C integers, and can therefore be subject to \[overflow-errors\](\#overflow-errors).

#### Signed integer types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.signedinteger

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.byte

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.short

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.intc

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

[numpy.int]()

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.long

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.longlong

</div>

#### Unsigned integer types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.unsignedinteger

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.ubyte

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.ushort

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.uintc

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.uint

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.ulong

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.ulonglong

</div>

### Inexact types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.inexact

</div>

\> **Note** \> Inexact scalars are printed using the fewest decimal digits needed to distinguish their value from other values of the same datatype, by judicious rounding. See the `unique` parameter of <span class="title-ref">format\_float\_positional</span> and <span class="title-ref">format\_float\_scientific</span>.

> This means that variables with equal binary values but whose datatypes are of different precisions may display differently:
> 
> > \>\>\> import numpy as np
> > 
> > \>\>\> f16 = np.float16("0.1") \>\>\> f32 = np.float32(f16) \>\>\> f64 = np.float64(f32) \>\>\> f16 == f32 == f64 True \>\>\> f16, f32, f64 (0.1, 0.099975586, 0.0999755859375)
> > 
> > Note that none of these floats hold the exact value \(\frac{1}{10}\); `f16` prints as `0.1` because it is as close to that value as possible, whereas the other types do not as they have more precision and therefore have closer values.
> > 
> > Conversely, floating-point scalars of different precisions which approximate the same decimal value may compare unequal despite printing identically:
> > 
> > \>\>\> f16 = np.float16("0.1") \>\>\> f32 = np.float32("0.1") \>\>\> f64 = np.float64("0.1") \>\>\> f16 == f32 == f64 False \>\>\> f16, f32, f64 (0.1, 0.1, 0.1)

#### Floating-point types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.floating

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.half

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.single

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.double

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.longdouble

</div>

#### Complex floating-point types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.complexfloating

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.csingle

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.cdouble

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.clongdouble

</div>

### Other types

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

[numpy.bool]()

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.bool

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.datetime64

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.timedelta64

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

[numpy.object]()

</div>

\> **Note** \> The data actually stored in object arrays (*i.e.*, arrays having dtype <span class="title-ref">object\_</span>) are references to Python objects, not the objects themselves. Hence, object arrays behave more like usual Python <span class="title-ref">lists \<list\></span>, in the sense that their contents need not be of the same Python type.

> The object type is also special because an array containing <span class="title-ref">object\_</span> items does not return an <span class="title-ref">object\_</span> object on item access, but instead returns the actual object that the array item refers to.

<div class="index">

flexible

</div>

The following data types are **flexible**: they have no predefined size and the data they describe can be of different length in different arrays. (In the character codes `#` is an integer denoting how many elements the data type consists of.)

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.flexible

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.character

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

[numpy.bytes]()

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

[numpy.str]()

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

numpy.void

</div>

\> **Warning** \> See \[Note on string types\<string-dtype-note\>\](\#note-on-string-types\<string-dtype-note\>).

> Numeric Compatibility: If you used old typecode characters in your Numeric code (which was never recommended), you will need to change some of them to the new characters. In particular, the needed changes are `c -> S1`, `b -> B`, `1 -> b`, `s -> h`, `w -> H`, and `u -> I`. These changes make the type character convention more consistent with other Python modules such as the `struct` module.

### Sized aliases

Along with their (mostly) C-derived names, the integer, float, and complex data-types are also available using a bit-width convention so that an array of the right size can always be ensured. Two aliases (<span class="title-ref">numpy.intp</span> and <span class="title-ref">numpy.uintp</span>) pointing to the integer type that is sufficiently large to hold a C pointer are also provided.

<div class="attribute">

int8 int16 int32 int64

Aliases for the signed integer types (one of <span class="title-ref">numpy.byte</span>, <span class="title-ref">numpy.short</span>, <span class="title-ref">numpy.intc</span>, <span class="title-ref">numpy.int\_</span>, <span class="title-ref">numpy.long</span> and <span class="title-ref">numpy.longlong</span>) with the specified number of bits.

Compatible with the C99 `int8_t`, `int16_t`, `int32_t`, and `int64_t`, respectively.

</div>

<div class="attribute">

uint8 uint16 uint32 uint64

Alias for the unsigned integer types (one of <span class="title-ref">numpy.ubyte</span>, <span class="title-ref">numpy.ushort</span>, <span class="title-ref">numpy.uintc</span>, <span class="title-ref">numpy.uint</span>, <span class="title-ref">numpy.ulong</span> and <span class="title-ref">numpy.ulonglong</span>) with the specified number of bits.

Compatible with the C99 `uint8_t`, `uint16_t`, `uint32_t`, and `uint64_t`, respectively.

</div>

<div class="attribute">

intp

Alias for the signed integer type (one of <span class="title-ref">numpy.byte</span>, <span class="title-ref">numpy.short</span>, <span class="title-ref">numpy.intc</span>, <span class="title-ref">numpy.int\_</span>, <span class="title-ref">numpy.long</span> and <span class="title-ref">numpy.longlong</span>) that is used as a default integer and for indexing.

Compatible with the C `Py_ssize_t`.

  - Character code  
    `'n'`

<div class="versionchanged">

2.0 Before NumPy 2, this had the same size as a pointer. In practice this is almost always identical, but the character code `'p'` maps to the C `intptr_t`. The character code `'n'` was added in NumPy 2.0.

</div>

</div>

<div class="attribute">

uintp

Alias for the unsigned integer type that is the same size as `intp`.

Compatible with the C `size_t`.

  - Character code  
    `'N'`

<div class="versionchanged">

2.0 Before NumPy 2, this had the same size as a pointer. In practice this is almost always identical, but the character code `'P'` maps to the C `uintptr_t`. The character code `'N'` was added in NumPy 2.0.

</div>

</div>

<div class="autoclass">

numpy.float16

</div>

<div class="autoclass">

numpy.float32

</div>

<div class="autoclass">

numpy.float64

</div>

<div class="attribute">

float96 float128

Alias for <span class="title-ref">numpy.longdouble</span>, named after its size in bits. The existence of these aliases depends on the platform.

</div>

<div class="autoclass">

numpy.complex64

</div>

<div class="autoclass">

numpy.complex128

</div>

<div class="attribute">

complex192 complex256

Alias for <span class="title-ref">numpy.clongdouble</span>, named after its size in bits. The existence of these aliases depends on the platform.

</div>

## Attributes

The array scalar objects have an `array priority
<class.__array_priority__>` of :c\`NPY\_SCALAR\_PRIORITY\` (-1,000,000.0). They also do not (yet) have a <span class="title-ref">ctypes \<ndarray.ctypes\></span> attribute. Otherwise, they share the same attributes as arrays:

<div class="autosummary" data-toctree="generated/">

generic.flags generic.shape generic.strides generic.ndim generic.data generic.size generic.itemsize generic.base generic.dtype generic.real generic.imag generic.flat generic.T generic.\_\_array\_interface\_\_ generic.\_\_array\_struct\_\_ generic.\_\_array\_priority\_\_ generic.\_\_array\_wrap\_\_

</div>

## Indexing

<div class="seealso">

\[arrays.indexing\](\#arrays.indexing), \[arrays.dtypes\](\#arrays.dtypes)

</div>

Array scalars can be indexed like 0-dimensional arrays: if *x* is an array scalar,

  - `x[()]` returns a copy of array scalar
  - `x[...]` returns a 0-dimensional <span class="title-ref">ndarray</span>
  - `x['field-name']` returns the array scalar in the field *field-name*. (*x* can have fields, for example, when it corresponds to a structured data type.)

## Methods

Array scalars have exactly the same methods as arrays. The default behavior of these methods is to internally convert the scalar to an equivalent 0-dimensional array and to call the corresponding array method. In addition, math operations on array scalars are defined so that the same hardware flags are set and used to interpret the results as for \[ufunc \<ufuncs\>\](\#ufunc-\<ufuncs\>), so that the error state used for ufuncs also carries over to the math on array scalars.

The exceptions to the above rules are given below:

<div class="autosummary" data-toctree="generated/">

generic.\_\_array\_\_ generic.\_\_array\_wrap\_\_ generic.squeeze generic.byteswap generic.\_\_reduce\_\_ generic.\_\_setstate\_\_ generic.setflags

</div>

Utility method for typing:

<div class="autosummary" data-toctree="generated/">

number.\_\_class\_getitem\_\_

</div>

## Defining new types

There are two ways to effectively define a new array scalar type (apart from composing structured types \[dtypes \<arrays.dtypes\>\](\#dtypes-\<arrays.dtypes\>) from the built-in scalar types): One way is to simply subclass the <span class="title-ref">ndarray</span> and overwrite the methods of interest. This will work to a degree, but internally certain behaviors are fixed by the data type of the array. To fully customize the data type of an array you need to define a new data-type, and register it with NumPy. Such new types can only be defined in C, using the \[NumPy C-API \<c-api\>\](\#numpy-c-api-\<c-api\>).

1.  However, array scalars are immutable, so none of the array scalar attributes are settable.

---

array.md

---

# Array API

<div class="sectionauthor">

Travis E. Oliphant

</div>

Â Â Â The test of a first-rate intelligence is the ability to hold two  
Â Â Â opposed ideas in the mind at the same time, and still retain the  
Â Â Â ability to function.  
Â Â Â --- *F. Scott Fitzgerald*

Â Â Â For a successful technology, reality must take precedence over public  
Â Â Â relations, for Nature cannot be fooled.  
Â Â Â --- *Richard P. Feynman*

<div class="index">

pair: ndarray; C-API pair: C-API; array

</div>

## Array structure and data access

These macros access the :c`PyArrayObject` structure members and are defined in `ndarraytypes.h`. The input argument, *arr*, can be any :c`PyObject *` that is directly interpretable as a :c`PyArrayObject *` (any instance of the :c\`PyArray\_Type\` and its sub-types).

### Data access

These functions and macros provide easy access to elements of the ndarray from C. These work for all arrays. You may need to take care when accessing the data in the array, however, if it is not in machine byte-order, misaligned, or not writeable. In other words, be sure to respect the state of the flags unless you know what you are doing, or have previously guaranteed an array that is writeable, aligned, and in machine byte-order using :c\`PyArray\_FromAny\`. If you wish to handle all types of arrays, the copyswap function for each type is useful for handling misbehaved arrays. Some platforms (e.g. Solaris) do not like misaligned data and will crash if you de-reference a misaligned pointer. Other platforms (e.g. x86 Linux) will just work more slowly with misaligned data.

## Creating arrays

### From scratch

\> **Warning** \> If data is passed to :c\`PyArray\_NewFromDescr\` or :c\`PyArray\_New\`, this memory must not be deallocated until the new array is deleted. If this data came from another Python object, this can be accomplished using :c\`Py\_INCREF\` on that object and setting the base member of the new array to point to that object. If strides are passed in they must be consistent with the dimensions, the itemsize, and the data of the array.

### From other objects

## Dealing with types

### General check of Python Type

### Data-type accessors

Some of the descriptor attributes may not always be defined and should or cannot not be accessed directly.

<div class="versionchanged">

2.0 Prior to NumPy 2.0 the ABI was different but unnecessary large for user DTypes. These accessors were all added in 2.0 and can be backported (see \[migration\_c\_descr\](\#migration\_c\_descr)).

</div>

Data-type checking `` ` ~~~~~~~~~~~~~~~~~~  For the typenum macros, the argument is an integer representing an enumerated array data type. For the array type checking macros the argument must be a :c:expr:`PyObject *` that can be directly interpreted as a :c:expr:`PyArrayObject *`.  .. c:function:: int PyTypeNum_ISUNSIGNED(int num)  .. c:function:: int PyDataType_ISUNSIGNED(PyArray_Descr *descr)  .. c:function:: int PyArray_ISUNSIGNED(PyArrayObject *obj)      Type represents an unsigned integer.  .. c:function:: int PyTypeNum_ISSIGNED(int num)  .. c:function:: int PyDataType_ISSIGNED(PyArray_Descr *descr)  .. c:function:: int PyArray_ISSIGNED(PyArrayObject *obj)      Type represents a signed integer.  .. c:function:: int PyTypeNum_ISINTEGER(int num)  .. c:function:: int PyDataType_ISINTEGER(PyArray_Descr* descr)  .. c:function:: int PyArray_ISINTEGER(PyArrayObject *obj)      Type represents any integer.  .. c:function:: int PyTypeNum_ISFLOAT(int num)  .. c:function:: int PyDataType_ISFLOAT(PyArray_Descr* descr)  .. c:function:: int PyArray_ISFLOAT(PyArrayObject *obj)      Type represents any floating point number.  .. c:function:: int PyTypeNum_ISCOMPLEX(int num)  .. c:function:: int PyDataType_ISCOMPLEX(PyArray_Descr* descr)  .. c:function:: int PyArray_ISCOMPLEX(PyArrayObject *obj)      Type represents any complex floating point number.  .. c:function:: int PyTypeNum_ISNUMBER(int num)  .. c:function:: int PyDataType_ISNUMBER(PyArray_Descr* descr)  .. c:function:: int PyArray_ISNUMBER(PyArrayObject *obj)      Type represents any integer, floating point, or complex floating point     number.  .. c:function:: int PyTypeNum_ISSTRING(int num)  .. c:function:: int PyDataType_ISSTRING(PyArray_Descr* descr)  .. c:function:: int PyArray_ISSTRING(PyArrayObject *obj)      Type represents a string data type.  .. c:function:: int PyTypeNum_ISFLEXIBLE(int num)  .. c:function:: int PyDataType_ISFLEXIBLE(PyArray_Descr* descr)  .. c:function:: int PyArray_ISFLEXIBLE(PyArrayObject *obj)      Type represents one of the flexible array types ( :c`NPY_STRING`,     :c`NPY_UNICODE`, or :c`NPY_VOID` ).  .. c:function:: int PyDataType_ISUNSIZED(PyArray_Descr* descr)      Type has no size information attached, and can be resized. Should only be     called on flexible dtypes. Types that are attached to an array will always     be sized, hence the array form of this macro not existing.      For structured datatypes with no fields this function now returns False.  .. c:function:: int PyTypeNum_ISUSERDEF(int num)  .. c:function:: int PyDataType_ISUSERDEF(PyArray_Descr* descr)  .. c:function:: int PyArray_ISUSERDEF(PyArrayObject *obj)      Type represents a user-defined type.  .. c:function:: int PyTypeNum_ISEXTENDED(int num)  .. c:function:: int PyDataType_ISEXTENDED(PyArray_Descr* descr)  .. c:function:: int PyArray_ISEXTENDED(PyArrayObject *obj)      Type is either flexible or user-defined.  .. c:function:: int PyTypeNum_ISOBJECT(int num)  .. c:function:: int PyDataType_ISOBJECT(PyArray_Descr* descr)  .. c:function:: int PyArray_ISOBJECT(PyArrayObject *obj)      Type represents object data type.  .. c:function:: int PyTypeNum_ISBOOL(int num)  .. c:function:: int PyDataType_ISBOOL(PyArray_Descr* descr)  .. c:function:: int PyArray_ISBOOL(PyArrayObject *obj)      Type represents Boolean data type.  .. c:function:: int PyDataType_HASFIELDS(PyArray_Descr* descr)  .. c:function:: int PyArray_HASFIELDS(PyArrayObject *obj)      Type has fields associated with it.  .. c:function:: int PyArray_ISNOTSWAPPED(PyArrayObject *m)      Evaluates true if the data area of the ndarray *m* is in machine     byte-order according to the array's data-type descriptor.  .. c:function:: int PyArray_ISBYTESWAPPED(PyArrayObject *m)      Evaluates true if the data area of the ndarray *m* is **not** in     machine byte-order according to the array's data-type descriptor.  .. c:function:: npy_bool PyArray_EquivTypes( \         PyArray_Descr* type1, PyArray_Descr* type2)      Return :c`NPY_TRUE` if *type1* and *type2* actually represent     equivalent types for this platform (the fortran member of each     type is ignored). For example, on 32-bit platforms,     :c`NPY_LONG` and :c`NPY_INT` are equivalent. Otherwise     return :c`NPY_FALSE`.  .. c:function:: npy_bool PyArray_EquivArrTypes( \         PyArrayObject* a1, PyArrayObject * a2)      Return :c`NPY_TRUE` if *a1* and *a2* are arrays with equivalent     types for this platform.  .. c:function:: npy_bool PyArray_EquivTypenums(int typenum1, int typenum2)      Special case of :c`PyArray_EquivTypes` (...) that does not accept     flexible data types but may be easier to call.  .. c:function:: int PyArray_EquivByteorders(int b1, int b2)      True if byteorder characters *b1* and *b2* ( :c`NPY_LITTLE`,     :c`NPY_BIG`, :c`NPY_NATIVE`, :c`NPY_IGNORE` ) are     either equal or equivalent as to their specification of a native     byte order. Thus, on a little-endian machine :c`NPY_LITTLE`     and :c`NPY_NATIVE` are equivalent where they are not     equivalent on a big-endian machine.   Converting data types ~~~~~~~~~~~~~~~~~~~~~  .. c:function:: PyObject* PyArray_Cast(PyArrayObject* arr, int typenum)      Mainly for backwards compatibility to the Numeric C-API and for     simple casts to non-flexible types. Return a new array object with     the elements of *arr* cast to the data-type *typenum* which must     be one of the enumerated types and not a flexible type.  .. c:function:: PyObject* PyArray_CastToType( \         PyArrayObject* arr, PyArray_Descr* type, int fortran)      Return a new array of the *type* specified, casting the elements     of *arr* as appropriate. The fortran argument specifies the     ordering of the output array.  .. c:function:: int PyArray_CastTo(PyArrayObject* out, PyArrayObject* in)      As of 1.6, this function simply calls :c`PyArray_CopyInto`,     which handles the casting.      Cast the elements of the array *in* into the array *out*. The     output array should be writeable, have an integer-multiple of the     number of elements in the input array (more than one copy can be     placed in out), and have a data type that is one of the builtin     types.  Returns 0 on success and -1 if an error occurs.  .. c:function:: int PyArray_CanCastSafely(int fromtype, int totype)      Returns non-zero if an array of data type *fromtype* can be cast     to an array of data type *totype* without losing information. An     exception is that 64-bit integers are allowed to be cast to 64-bit     floating point values even though this can lose precision on large     integers so as not to proliferate the use of long doubles without     explicit requests. Flexible array types are not checked according     to their lengths with this function.  .. c:function:: int PyArray_CanCastTo( \         PyArray_Descr* fromtype, PyArray_Descr* totype)      :c`PyArray_CanCastTypeTo` supersedes this function in     NumPy 1.6 and later.      Equivalent to PyArray_CanCastTypeTo(fromtype, totype, NPY_SAFE_CASTING).  .. c:function:: int PyArray_CanCastTypeTo( \         PyArray_Descr* fromtype, PyArray_Descr* totype, NPY_CASTING casting)      Returns non-zero if an array of data type *fromtype* (which can     include flexible types) can be cast safely to an array of data     type *totype* (which can include flexible types) according to     the casting rule *casting*. For simple types with :c`NPY_SAFE_CASTING`,     this is basically a wrapper around :c`PyArray_CanCastSafely`, but     for flexible types such as strings or unicode, it produces results     taking into account their sizes. Integer and float types can only be cast     to a string or unicode type using :c`NPY_SAFE_CASTING` if the string     or unicode type is big enough to hold the max value of the integer/float     type being cast from.  .. c:function:: int PyArray_CanCastArrayTo( \         PyArrayObject* arr, PyArray_Descr* totype, NPY_CASTING casting)      Returns non-zero if *arr* can be cast to *totype* according     to the casting rule given in *casting*.  If *arr* is an array     scalar, its value is taken into account, and non-zero is also     returned when the value will not overflow or be truncated to     an integer when converting to a smaller type.  .. c:function:: PyArray_Descr* PyArray_MinScalarType(PyArrayObject* arr)      .. note::         With the adoption of NEP 50 in NumPy 2, this function is not used         internally.  It is currently provided for backwards compatibility,         but expected to be eventually deprecated.      If *arr* is an array, returns its data type descriptor, but if     *arr* is an array scalar (has 0 dimensions), it finds the data type     of smallest size to which the value may be converted     without overflow or truncation to an integer.      This function will not demote complex to float or anything to     boolean, but will demote a signed integer to an unsigned integer     when the scalar value is positive.  .. c:function:: PyArray_Descr* PyArray_PromoteTypes( \         PyArray_Descr* type1, PyArray_Descr* type2)      Finds the data type of smallest size and kind to which *type1* and     *type2* may be safely converted. This function is symmetric and     associative. A string or unicode result will be the proper size for     storing the max value of the input types converted to a string or unicode.  .. c:function:: PyArray_Descr* PyArray_ResultType( \         npy_intp narrs, PyArrayObject **arrs, npy_intp ndtypes, \         PyArray_Descr **dtypes)      This applies type promotion to all the input arrays and dtype     objects, using the NumPy rules for combining scalars and arrays, to     determine the output type for an operation with the given set of     operands. This is the same result type that ufuncs produce.      See the documentation of `numpy.result_type` for more     detail about the type promotion algorithm.  .. c:function:: int PyArray_ObjectType(PyObject* op, int mintype)      This function is superseded by :c`PyArray_ResultType`.      This function is useful for determining a common type that two or     more arrays can be converted to. It only works for non-flexible     array types as no itemsize information is passed. The *mintype*     argument represents the minimum type acceptable, and *op*     represents the object that will be converted to an array. The     return value is the enumerated typenumber that represents the     data-type that *op* should have.  .. c:function:: PyArrayObject** PyArray_ConvertToCommonType( \         PyObject* op, int* n)      The functionality this provides is largely superseded by iterator     :c:type:`NpyIter` introduced in 1.6, with flag     :c`NPY_ITER_COMMON_DTYPE` or with the same dtype parameter for     all operands.      Convert a sequence of Python objects contained in *op* to an array     of ndarrays each having the same data type. The type is selected     in the same way as :c`PyArray_ResultType`. The length of the sequence is     returned in *n*, and an *n* -length array of :c:type:`PyArrayObject`     pointers is the return value (or ``NULL``if an error occurs).     The returned array must be freed by the caller of this routine     (using :c`PyDataMem_FREE` ) and all the array objects in it``DECREF`'d or a memory-leak will occur. The example template-code     below shows a typical usage:`\`c mps = PyArray\_ConvertToCommonType(obj, \&n); if (mps==NULL) return NULL; {code} \<before return\> for (i=0; i\<n; i++) Py\_DECREF(mps\[i\]); PyDataMem\_FREE(mps); {return}

User-defined data types `` ` ~~~~~~~~~~~~~~~~~~~~~~~  .. c:function:: void PyArray_InitArrFuncs(PyArray_ArrFuncs* f)      Initialize all function pointers and members to ``NULL`.  .. c:function:: int PyArray_RegisterDataType(PyArray_DescrProto* dtype)      .. note::         As of NumPy 2.0 this API is considered legacy, the new DType API         is more powerful and provides additional flexibility.         The API may eventually be deprecated but support is continued for         the time being.          **Compiling for NumPy 1.x and 2.x**          NumPy 2.x requires passing in a`PyArray\_DescrProto`typed struct         rather than a`PyArray\_Descr`.  This is necessary to allow changes.         To allow code to run and compile on both 1.x and 2.x you need to         change the type of your struct to`PyArray\_DescrProto`and add::              /* Allow compiling on NumPy 1.x */             #if NPY_ABI_VERSION < 0x02000000             #define PyArray_DescrProto PyArray_Descr             #endif          for 1.x compatibility.  Further, the struct will *not* be the actual         descriptor anymore, only it's type number will be updated.         After successful registration, you must thus fetch the actual         dtype with::              int type_num = PyArray_RegisterDataType(&my_descr_proto);             if (type_num < 0) {                 /* error */             }             PyArray_Descr *my_descr = PyArray_DescrFromType(type_num);          With these two changes, the code should compile and work on both 1.x         and 2.x or later.          In the unlikely case that you are heap allocating the dtype struct you         should free it again on NumPy 2, since a copy is made.         The struct is not a valid Python object, so do not use`Py\_DECREF`on it.      Register a data-type as a new user-defined data type for     arrays. The type must have most of its entries filled in. This is     not always checked and errors can produce segfaults. In     particular, the typeobj member of the`dtype`structure must be     filled with a Python type that has a fixed-size element-size that     corresponds to the elsize member of *dtype*. Also the`f`member must have the required functions: nonzero, copyswap,     copyswapn, getitem, setitem, and cast (some of the cast functions     may be`NULL``if no support is desired). To avoid confusion, you     should choose a unique character typecode but this is not enforced     and not relied on internally.      A user-defined type number is returned that uniquely identifies     the type. A pointer to the new structure can then be obtained from     :c`PyArray_DescrFromType` using the returned type number. A -1 is     returned if an error occurs.  If this *dtype* has already been     registered (checked only by the address of the pointer), then     return the previously-assigned type-number.      The number of user DTypes known to numpy is stored in``NPY\_NUMUSERTYPES`, a static global variable that is public in the     C API.  Accessing this symbol is inherently *not* thread-safe. If     for some reason you need to use this API in a multithreaded context,     you will need to add your own locking, NumPy does not ensure new     data types can be added in a thread-safe manner.  .. c:function:: int PyArray_RegisterCastFunc( \         PyArray_Descr* descr, int totype, PyArray_VectorUnaryFunc* castfunc)      Register a low-level casting function, *castfunc*, to convert     from the data-type, *descr*, to the given data-type number,     *totype*. Any old casting function is over-written. A`0`is     returned on success or a`-1``on failure.      .. c:type:: PyArray_VectorUnaryFunc          The function pointer type for low-level casting functions.  .. c:function:: int PyArray_RegisterCanCast( \         PyArray_Descr* descr, int totype, NPY_SCALARKIND scalar)      Register the data-type number, *totype*, as castable from     data-type object, *descr*, of the given *scalar* kind. Use     *scalar* = :c`NPY_NOSCALAR` to register that an array of data-type     *descr* can be cast safely to a data-type whose type_number is     *totype*. The return value is 0 on success or -1 on failure.   Special functions for NPY_OBJECT ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  > **Warning** >      When working with arrays or buffers filled with objects NumPy tries to     ensure such buffers are filled with``None`before any data may be read.     However, code paths may existed where an array is only initialized to`NULL`.     NumPy itself accepts`NULL`as an alias for`None`, but may`assert`non-`NULL`when compiled in debug mode.      Because NumPy is not yet consistent about initialization with None,     users **must** expect a value of`NULL`when working with buffers created     by NumPy.  Users **should** also ensure to pass fully initialized buffers     to NumPy, since NumPy may make this a strong requirement in the future.      There is currently an intention to ensure that NumPy always initializes     object arrays before they may be read.  Any failure to do so will be     regarded as a bug.     In the future, users may be able to rely on non-NULL values when reading     from any array, although exceptions for writing to freshly created arrays     may remain (e.g. for output arrays in ufunc code).  As of NumPy 1.23     known code paths exists where proper filling is not done.   .. c:function:: int PyArray_INCREF(PyArrayObject* op)      Used for an array, *op*, that contains any Python objects. It     increments the reference count of every object in the array     according to the data-type of *op*. A -1 is returned if an error     occurs, otherwise 0 is returned.  .. c:function:: void PyArray_Item_INCREF(char* ptr, PyArray_Descr* dtype)      A function to INCREF all the objects at the location *ptr*     according to the data-type *dtype*. If *ptr* is the start of a     structured type with an object at any offset, then this will (recursively)     increment the reference count of all object-like items in the     structured type.  .. c:function:: int PyArray_XDECREF(PyArrayObject* op)      Used for an array, *op*, that contains any Python objects. It     decrements the reference count of every object in the array     according to the data-type of *op*. Normal return value is 0. A     -1 is returned if an error occurs.  .. c:function:: void PyArray_Item_XDECREF(char* ptr, PyArray_Descr* dtype)      A function to XDECREF all the object-like items at the location     *ptr* as recorded in the data-type, *dtype*. This works     recursively so that if`dtype`itself has fields with data-types     that contain object-like items, all the object-like fields will be     XDECREF`'d`.  .. c:function:: int PyArray_SetWritebackIfCopyBase(PyArrayObject* arr, PyArrayObject* base)      Precondition:`arr`is a copy of`base``(though possibly with different     strides, ordering, etc.) Sets the :c`NPY_ARRAY_WRITEBACKIFCOPY` flag     and``arr-\>base`, and set`base``to READONLY. Call     :c`PyArray_ResolveWritebackIfCopy` before calling     :c`Py_DECREF` in order to copy any changes back to``base`and     reset the READONLY flag.      Returns 0 for success, -1 for failure.  .. _array-flags:  Array flags -----------  The`flags`attribute of the`PyArrayObject`structure contains important information about the memory used by the array (pointed to by the data member) This flag information must be kept accurate or strange results and even segfaults may result.  There are 6 (binary) flags that describe the memory area used by the data buffer.  These constants are defined in`arrayobject.h`and determine the bit-position of the flag.  Python exposes a nice attribute- based interface as well as a dictionary-like interface for getting (and, if appropriate, setting) these flags.  Memory areas of all kinds can be pointed to by an ndarray, necessitating these flags.  If you get an arbitrary`PyArrayObject``in C-code, you need to be aware of the flags that are set.  If you need to guarantee a certain kind of array (like :c`NPY_ARRAY_C_CONTIGUOUS` and :c`NPY_ARRAY_BEHAVED`), then pass these requirements into the PyArray_FromAny function.  In versions 1.6 and earlier of NumPy, the following flags did not have the _ARRAY_ macro namespace in them. That form of the constant names is deprecated in 1.7.   Basic Array Flags ~~~~~~~~~~~~~~~~~  An ndarray can have a data segment that is not a simple contiguous chunk of well-behaved memory you can manipulate. It may not be aligned with word boundaries (very important on some platforms). It might have its data in a different byte-order than the machine recognizes. It might not be writeable. It might be in Fortran-contiguous order. The array flags are used to indicate what can be said about data associated with an array.  .. c:macro:: NPY_ARRAY_C_CONTIGUOUS      The data area is in C-style contiguous order (last index varies the     fastest).  .. c:macro:: NPY_ARRAY_F_CONTIGUOUS      The data area is in Fortran-style contiguous order (first index varies     the fastest).  > **Note** >      Arrays can be both C-style and Fortran-style contiguous simultaneously.     This is clear for 1-dimensional arrays, but can also be true for higher     dimensional arrays.      Even for contiguous arrays a stride for a given dimension``arr.strides\[dim\]`may be *arbitrary* if`arr.shape\[dim\] == 1`or the array has no elements.     It does *not* generally hold that`self.strides\[-1\] == self.itemsize`for C-style contiguous arrays or`self.strides\[0\] == self.itemsize`for     Fortran-style contiguous arrays is true. The correct way to access the`itemsize`of an array from the C API is`PyArray\_ITEMSIZE(arr)`.      .. seealso:: [Internal memory layout of an ndarray <arrays.ndarray>](#internal-memory-layout-of-an-ndarray-<arrays.ndarray>)  .. c:macro:: NPY_ARRAY_OWNDATA      The data area is owned by this array. Should never be set manually, instead     create a`PyObject`wrapping the data and set the array's base to that     object. For an example, see the test in`test\_mem\_policy``.  .. c:macro:: NPY_ARRAY_ALIGNED      The data area and all array elements are aligned appropriately.  .. c:macro:: NPY_ARRAY_WRITEABLE      The data area can be written to.      Notice that the above 3 flags are defined so that a new, well-     behaved array has these flags defined as true.  .. c:macro:: NPY_ARRAY_WRITEBACKIFCOPY      The data area represents a (well-behaved) copy whose information     should be transferred back to the original when     :c`PyArray_ResolveWritebackIfCopy` is called.      This is a special flag that is set if this array represents a copy     made because a user required certain flags in     :c`PyArray_FromAny` and a copy had to be made of some other     array (and the user asked for this flag to be set in such a     situation). The base attribute then points to the "misbehaved"     array (which is set read_only). :c`PyArray_ResolveWritebackIfCopy`     will copy its contents back to the "misbehaved"     array (casting if necessary) and will reset the "misbehaved" array     to :c`NPY_ARRAY_WRITEABLE`. If the "misbehaved" array was not     :c`NPY_ARRAY_WRITEABLE` to begin with then :c`PyArray_FromAny`     would have returned an error because :c`NPY_ARRAY_WRITEBACKIFCOPY`     would not have been possible.  :c`PyArray_UpdateFlags` (obj, flags) will update the``obj-\>flags`for`flags``which can be any of :c`NPY_ARRAY_C_CONTIGUOUS`, :c`NPY_ARRAY_F_CONTIGUOUS`, :c`NPY_ARRAY_ALIGNED`, or :c`NPY_ARRAY_WRITEABLE`.   Combinations of array flags ~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. c:macro:: NPY_ARRAY_BEHAVED      :c`NPY_ARRAY_ALIGNED` \| :c`NPY_ARRAY_WRITEABLE`  .. c:macro:: NPY_ARRAY_CARRAY      :c`NPY_ARRAY_C_CONTIGUOUS` \| :c`NPY_ARRAY_BEHAVED`  .. c:macro:: NPY_ARRAY_CARRAY_RO      :c`NPY_ARRAY_C_CONTIGUOUS` \| :c`NPY_ARRAY_ALIGNED`  .. c:macro:: NPY_ARRAY_FARRAY      :c`NPY_ARRAY_F_CONTIGUOUS` \| :c`NPY_ARRAY_BEHAVED`  .. c:macro:: NPY_ARRAY_FARRAY_RO      :c`NPY_ARRAY_F_CONTIGUOUS` \| :c`NPY_ARRAY_ALIGNED`  .. c:macro:: NPY_ARRAY_DEFAULT      :c`NPY_ARRAY_CARRAY`  .. c:macro:: NPY_ARRAY_IN_ARRAY      :c`NPY_ARRAY_C_CONTIGUOUS` \| :c`NPY_ARRAY_ALIGNED`  .. c:macro:: NPY_ARRAY_IN_FARRAY      :c`NPY_ARRAY_F_CONTIGUOUS` \| :c`NPY_ARRAY_ALIGNED`  .. c:macro:: NPY_ARRAY_OUT_ARRAY      :c`NPY_ARRAY_C_CONTIGUOUS` \| :c`NPY_ARRAY_WRITEABLE` \|     :c`NPY_ARRAY_ALIGNED`  .. c:macro:: NPY_ARRAY_OUT_FARRAY      :c`NPY_ARRAY_F_CONTIGUOUS` \| :c`NPY_ARRAY_WRITEABLE` \|     :c`NPY_ARRAY_ALIGNED`  .. c:macro:: NPY_ARRAY_INOUT_ARRAY      :c`NPY_ARRAY_C_CONTIGUOUS` \| :c`NPY_ARRAY_WRITEABLE` \|     :c`NPY_ARRAY_ALIGNED` \| :c`NPY_ARRAY_WRITEBACKIFCOPY`  .. c:macro:: NPY_ARRAY_INOUT_FARRAY      :c`NPY_ARRAY_F_CONTIGUOUS` \| :c`NPY_ARRAY_WRITEABLE` \|     :c`NPY_ARRAY_ALIGNED` \| :c`NPY_ARRAY_WRITEBACKIFCOPY`  .. c:macro:: NPY_ARRAY_UPDATE_ALL      :c`NPY_ARRAY_C_CONTIGUOUS` \| :c`NPY_ARRAY_F_CONTIGUOUS` \| :c`NPY_ARRAY_ALIGNED`   Flag-like constants ~~~~~~~~~~~~~~~~~~~  These constants are used in :c`PyArray_FromAny` (and its macro forms) to specify desired properties of the new array.  .. c:macro:: NPY_ARRAY_FORCECAST      Cast to the desired type, even if it can't be done without losing     information.  .. c:macro:: NPY_ARRAY_ENSURECOPY      Make sure the resulting array is a copy of the original.  .. c:macro:: NPY_ARRAY_ENSUREARRAY      Make sure the resulting object is an actual ndarray, and not a sub-class.  These constants are used in :c`PyArray_CheckFromAny` (and its macro forms) to specify desired properties of the new array.  .. c:macro:: NPY_ARRAY_NOTSWAPPED      Make sure the returned array has a data-type descriptor that is in     machine byte-order, over-riding any specification in the *dtype*     argument. Normally, the byte-order requirement is determined by     the *dtype* argument. If this flag is set and the dtype argument     does not indicate a machine byte-order descriptor (or is NULL and     the object is already an array with a data-type descriptor that is     not in machine byte- order), then a new data-type descriptor is     created and used with its byte-order field set to native.  .. c:macro:: NPY_ARRAY_BEHAVED_NS      :c`NPY_ARRAY_ALIGNED` \| :c`NPY_ARRAY_WRITEABLE` \|     :c`NPY_ARRAY_NOTSWAPPED`  .. c:macro:: NPY_ARRAY_ELEMENTSTRIDES      Make sure the returned array has strides that are multiples of the     element size.   Flag checking ~~~~~~~~~~~~~  For all of these macros *arr* must be an instance of a (subclass of) :c`PyArray_Type`.  .. c:function:: int PyArray_CHKFLAGS(PyObject *arr, int flags)      The first parameter, arr, must be an ndarray or subclass. The     parameter, *flags*, should be an integer consisting of bitwise     combinations of the possible flags an array can have:     :c`NPY_ARRAY_C_CONTIGUOUS`, :c`NPY_ARRAY_F_CONTIGUOUS`,     :c`NPY_ARRAY_OWNDATA`, :c`NPY_ARRAY_ALIGNED`,     :c`NPY_ARRAY_WRITEABLE`, :c`NPY_ARRAY_WRITEBACKIFCOPY`.  .. c:function:: int PyArray_IS_C_CONTIGUOUS(PyObject *arr)      Evaluates true if *arr* is C-style contiguous.  .. c:function:: int PyArray_IS_F_CONTIGUOUS(PyObject *arr)      Evaluates true if *arr* is Fortran-style contiguous.  .. c:function:: int PyArray_ISFORTRAN(PyObject *arr)      Evaluates true if *arr* is Fortran-style contiguous and *not*     C-style contiguous. :c`PyArray_IS_F_CONTIGUOUS`     is the correct way to test for Fortran-style contiguity.  .. c:function:: int PyArray_ISWRITEABLE(PyObject *arr)      Evaluates true if the data area of *arr* can be written to  .. c:function:: int PyArray_ISALIGNED(PyObject *arr)      Evaluates true if the data area of *arr* is properly aligned on     the machine.  .. c:function:: int PyArray_ISBEHAVED(PyObject *arr)      Evaluates true if the data area of *arr* is aligned and writeable     and in machine byte-order according to its descriptor.  .. c:function:: int PyArray_ISBEHAVED_RO(PyObject *arr)      Evaluates true if the data area of *arr* is aligned and in machine     byte-order.  .. c:function:: int PyArray_ISCARRAY(PyObject *arr)      Evaluates true if the data area of *arr* is C-style contiguous,     and :c`PyArray_ISBEHAVED` (*arr*) is true.  .. c:function:: int PyArray_ISFARRAY(PyObject *arr)      Evaluates true if the data area of *arr* is Fortran-style     contiguous and :c`PyArray_ISBEHAVED` (*arr*) is true.  .. c:function:: int PyArray_ISCARRAY_RO(PyObject *arr)      Evaluates true if the data area of *arr* is C-style contiguous,     aligned, and in machine byte-order.  .. c:function:: int PyArray_ISFARRAY_RO(PyObject *arr)      Evaluates true if the data area of *arr* is Fortran-style     contiguous, aligned, and in machine byte-order **.**  .. c:function:: int PyArray_ISONESEGMENT(PyObject *arr)      Evaluates true if the data area of *arr* consists of a single     (C-style or Fortran-style) contiguous segment.  .. c:function:: void PyArray_UpdateFlags(PyArrayObject* arr, int flagmask)      The :c`NPY_ARRAY_C_CONTIGUOUS`, :c`NPY_ARRAY_ALIGNED`, and     :c`NPY_ARRAY_F_CONTIGUOUS` array flags can be "calculated" from the     array object itself. This routine updates one or more of these     flags of *arr* as specified in *flagmask* by performing the     required calculation.   > **Warning** >      It is important to keep the flags updated (using     :c`PyArray_UpdateFlags` can help) whenever a manipulation with an     array is performed that might cause them to change. Later     calculations in NumPy that rely on the state of these flags do not     repeat the calculation to update them.  .. c:function:: int PyArray_FailUnlessWriteable(PyArrayObject *obj, const char *name)      This function does nothing and returns 0 if *obj* is writeable.     It raises an exception and returns -1 if *obj* is not writeable.     It may also do other house-keeping, such as issuing warnings on     arrays which are transitioning to become views. Always call this     function at some point before writing to an array.      *name* is a name for the array, used to give better error messages.     It can be something like "assignment destination", "output array",     or even just "array".  ArrayMethod API ---------------  ArrayMethod loops are intended as a generic mechanism for writing loops over arrays, including ufunc loops and casts. The public API is defined in the``numpy/dtype\_api.h`header. See [arraymethod-structs](#arraymethod-structs) for documentation on the C structs exposed in the ArrayMethod API.  .. _arraymethod-typedefs:  Slots and Typedefs ~~~~~~~~~~~~~~~~~~  These are used to identify which kind of function an ArrayMethod slot implements. See [arraymethod-typedefs](#arraymethod-typedefs) below for documentation on the functions that must be implemented for each slot.  .. c:macro:: NPY_METH_resolve_descriptors  .. c:type:: NPY_CASTING (PyArrayMethod_ResolveDescriptors)( \                 struct PyArrayMethodObject_tag *method, \                 PyArray_DTypeMeta *const *dtypes, \                 PyArray_Descr *const *given_descrs, \                 PyArray_Descr **loop_descrs, \                 npy_intp *view_offset)     The function used to set the descriptors for an operation based on    the descriptors of the operands. For example, a ufunc operation with    two input operands and one output operand that is called without`out`being set in the python API,`resolve\_descriptors`will be    passed the descriptors for the two operands and determine the correct    descriptor to use for the output based on the output DType set for    the ArrayMethod. If`out`is set, then the output descriptor would    be passed in as well and should not be overridden.     The *method* is a pointer to the underlying cast or ufunc loop. In    the future we may expose this struct publicly but for now this is an    opaque pointer and the method cannot be inspected. The *dtypes* is an`nargs`length array of`PyArray\_DTypeMeta`pointers,    *given_descrs* is an`nargs`length array of input descriptor    instances (output descriptors may be NULL if no output was provided    by the user), and *loop_descrs* is an`nargs`length array of    descriptors that must be filled in by the resolve descriptors    implementation.  *view_offset* is currently only interesting for    casts and can normally be ignored.  When a cast does not require any    operation, this can be signalled by setting`view\_offset`to 0.  On    error, you must return`(NPY\_CASTING)-1`with an error set.  .. c:macro:: NPY_METH_strided_loop .. c:macro:: NPY_METH_contiguous_loop .. c:macro:: NPY_METH_unaligned_strided_loop .. c:macro:: NPY_METH_unaligned_contiguous_loop     One dimensional strided loops implementing the behavior (either a    ufunc or cast).  In most cases,`NPY\_METH\_strided\_loop`is the    generic and only version that needs to be implemented.`NPY\_METH\_contiguous\_loop`can be implemented additionally as a    more light-weight/faster version and it is used when all inputs and    outputs are contiguous.     To deal with possibly unaligned data, NumPy needs to be able to copy    unaligned to aligned data.  When implementing a new DType, the "cast"    or copy for it needs to implement`NPY\_METH\_unaligned\_strided\_loop`.  Unlike the normal versions,    this loop must not assume that the data can be accessed in an aligned    fashion.  These loops must copy each value before accessing or    storing::         type_in in_value;        type_out out_value        memcpy(&value, in_data, sizeof(type_in));        out_value = in_value;        memcpy(out_data, &out_value, sizeof(type_out)     while a normal loop can just use::         *(type_out *)out_data = *(type_in)in_data;     The unaligned loops are currently only used in casts and will never    be picked in ufuncs (ufuncs create a temporary copy to ensure aligned    inputs).  These slot IDs are ignored when`NPY\_METH\_get\_loop`is    defined, where instead whichever loop returned by the`get\_loop`function is used.  .. c:macro:: NPY_METH_contiguous_indexed_loop     A specialized inner-loop option to speed up common`ufunc.at`computations.  .. c:type:: int (PyArrayMethod_StridedLoop)(PyArrayMethod_Context *context, \         char *const *data, const npy_intp *dimensions, const npy_intp *strides, \         NpyAuxData *auxdata)     An implementation of an ArrayMethod loop. All of the loop slot IDs    listed above must provide a`PyArrayMethod\_StridedLoop`implementation. The *context* is a struct containing context for the    loop operation - in particular the input descriptors. The *data* are    an array of pointers to the beginning of the input and output array    buffers. The *dimensions* are the loop dimensions for the    operation. The *strides* are an`nargs`length array of strides for    each input. The *auxdata* is an optional set of auxiliary data that    can be passed in to the loop - helpful to turn on and off optional    behavior or reduce boilerplate by allowing similar ufuncs to share    loop implementations or to allocate space that is persistent over    multiple strided loop calls.  .. c:macro:: NPY_METH_get_loop     Allows more fine-grained control over loop selection. Accepts an    implementation of PyArrayMethod_GetLoop, which in turn returns a    strided loop implementation. If`NPY\_METH\_get\_loop`is defined,    the other loop slot IDs are ignored, if specified.  .. c:type:: int (PyArrayMethod_GetLoop)( \         PyArrayMethod_Context *context, int aligned, int move_references, \         const npy_intp *strides, PyArrayMethod_StridedLoop **out_loop, \         NpyAuxData **out_transferdata, NPY_ARRAYMETHOD_FLAGS *flags);     Sets the loop to use for an operation at runtime. The *context* is the    runtime context for the operation. *aligned* indicates whether the data    access for the loop is aligned (1) or unaligned (0). *move_references*    indicates whether embedded references in the data should be copied. *strides*    are the strides for the input array, *out_loop* is a pointer that must be    filled in with a pointer to the loop implementation. *out_transferdata* can    be optionally filled in to allow passing in extra user-defined context to an    operation. *flags* must be filled in with ArrayMethod flags relevant for the    operation.  This is for example necessary to indicate if the inner loop    requires the Python GIL to be held.  .. c:macro:: NPY_METH_get_reduction_initial  .. c:type:: int (PyArrayMethod_GetReductionInitial)( \         PyArrayMethod_Context *context, npy_bool reduction_is_empty, \         char *initial)     Query an ArrayMethod for the initial value for use in reduction. The    *context* is the ArrayMethod context, mainly to access the input    descriptors. *reduction_is_empty* indicates whether the reduction is    empty. When it is, the value returned may differ.  In this case it is a    "default" value that may differ from the "identity" value normally used.    For example:     -`0.0`is the default for`sum(\[\])`.  But`-0.0`is the correct      identity otherwise as it preserves the sign for`sum(\[-0.0\])`.    - We use no identity for object, but return the default of`0`and`1`for the empty`sum(\[\], dtype=object)`and`prod(\[\], dtype=object)`.      This allows`np.sum(np.array(\["a", "b"\], dtype=object))`to work.    -`-inf`or`INT\_MIN`for`max`is an identity, but at least`INT\_MIN`not a good *default* when there are no items.     *initial* is a pointer to the data for the initial value, which should be    filled in. Returns -1, 0, or 1 indicating error, no initial value, and the    initial value being successfully filled. Errors must not be given when no    initial value is correct, since NumPy may call this even when it is not    strictly necessary to do so.  Flags ~~~~~  .. c:enum:: NPY_ARRAYMETHOD_FLAGS     These flags allow switching on and off custom runtime behavior for    ArrayMethod loops.  For example, if a ufunc cannot possibly trigger floating    point errors, then the`NPY\_METH\_NO\_FLOATINGPOINT\_ERRORS`flag should be    set on the ufunc when it is registered.     .. c:enumerator:: NPY_METH_REQUIRES_PYAPI        Indicates the method must hold the GIL. If this flag is not set, the GIL       is released before the loop is called.     .. c:enumerator:: NPY_METH_NO_FLOATINGPOINT_ERRORS        Indicates the method cannot generate floating errors, so checking for       floating errors after the loop completes can be skipped.     .. c:enumerator:: NPY_METH_SUPPORTS_UNALIGNED        Indicates the method supports unaligned access.     .. c:enumerator:: NPY_METH_IS_REORDERABLE        Indicates that the result of applying the loop repeatedly (for example, in       a reduction operation) does not depend on the order of application.     .. c:enumerator:: NPY_METH_RUNTIME_FLAGS        The flags that can be changed at runtime.  Typedefs ~~~~~~~~  Typedefs for functions that users of the ArrayMethod API can implement are described below.  .. c:type:: int (PyArrayMethod_TraverseLoop)( \         void *traverse_context, const PyArray_Descr *descr, char *data, \         npy_intp size, npy_intp stride, NpyAuxData *auxdata)     A traverse loop working on a single array. This is similar to the general    strided-loop function. This is designed for loops that need to visit every    element of a single array.     Currently this is used for array clearing, via the`NPY\_DT\_get\_clear\_loop`DType API hook, and zero-filling, via the`NPY\_DT\_get\_fill\_zero\_loop`DType API hook.  These are most useful for handling arrays storing embedded    references to python objects or heap-allocated data.     The *descr* is the descriptor for the array, *data* is a pointer to the array    buffer, *size* is the 1D size of the array buffer, *stride* is the stride,    and *auxdata* is optional extra data for the loop.     The *traverse_context* is passed in because we may need to pass in    Interpreter state or similar in the future, but we don't want to pass in a    full context (with pointers to dtypes, method, caller which all make no sense    for a traverse function). We assume for now that this context can be just    passed through in the future (for structured dtypes).  .. c:type:: int (PyArrayMethod_GetTraverseLoop)( \                 void *traverse_context, const PyArray_Descr *descr, \                 int aligned, npy_intp fixed_stride, \                 PyArrayMethod_TraverseLoop **out_loop, NpyAuxData **out_auxdata, \                 NPY_ARRAYMETHOD_FLAGS *flags)     Simplified get_loop function specific to dtype traversal     It should set the flags needed for the traversal loop and set *out_loop* to the    loop function, which must be a valid`PyArrayMethod\_TraverseLoop``pointer. Currently this is used for zero-filling and clearing arrays storing    embedded references.  API Functions and Typedefs ~~~~~~~~~~~~~~~~~~~~~~~~~~  These functions are part of the main numpy array API and were added along with the rest of the ArrayMethod API.  .. c:function::  int PyUFunc_AddLoopFromSpec( \                          PyObject *ufunc, PyArrayMethod_Spec *spec)     Add loop directly to a ufunc from a given ArrayMethod spec.    the main ufunc registration function.  This adds a new implementation/loop    to a ufunc.  It replaces `PyUFunc_RegisterLoopForType`.  .. c:function:: int PyUFunc_AddPromoter( \                         PyObject *ufunc, PyObject *DType_tuple, PyObject *promoter)     Note that currently the output dtypes are always``NULL`unless they are    also part of the signature. This is an implementation detail and could    change in the future. However, in general promoters should not have a    need for output dtypes.    Register a new promoter for a ufunc. The first argument is the ufunc to    register the promoter with. The second argument is a Python tuple containing    DTypes or None matching the number of inputs and outputs for the ufuncs. The    last argument is a promoter is a function stored in a PyCapsule.  It is    passed the operation and requested DType signatures and can mutate it to    attempt a new search for a matching loop/promoter.  .. c:type:: int (PyArrayMethod_PromoterFunction)(PyObject *ufunc, \                 PyArray_DTypeMeta *const op_dtypes[], \                 PyArray_DTypeMeta *const signature[], \                 PyArray_DTypeMeta *new_op_dtypes[])     Type of the promoter function, which must be wrapped into a`PyCapsule`with name`"numpy.\_ufunc\_promoter"``. It is passed the    operation and requested DType signatures and can mutate the signatures to    attempt a search for a new loop or promoter that can accomplish the operation    by casting the inputs to the "promoted" DTypes.  .. c:function:: int PyUFunc_GiveFloatingpointErrors( \                         const char *name, int fpe_errors)      Checks for a floating point error after performing a floating point     operation in a manner that takes into account the error signaling configured     via `numpy.errstate`. Takes the name of the operation to use in the error     message and an integer flag that is one of``NPY\_FPE\_DIVIDEBYZERO`,`NPY\_FPE\_OVERFLOW`,`NPY\_FPE\_UNDERFLOW`,`NPY\_FPE\_INVALID`to indicate     which error to check for.      Returns -1 on failure (an error was raised) and 0 on success.  .. c:function:: int PyUFunc_AddWrappingLoop(PyObject *ufunc_obj, \             PyArray_DTypeMeta *new_dtypes[], \             PyArray_DTypeMeta *wrapped_dtypes[], \             PyArrayMethod_TranslateGivenDescriptors *translate_given_descrs, \             PyArrayMethod_TranslateLoopDescriptors *translate_loop_descrs)      Allows creating of a fairly lightweight wrapper around an existing     ufunc loop.  The idea is mainly for units, as this is currently     slightly limited in that it enforces that you cannot use a loop from     another ufunc.  .. c:type:: int (PyArrayMethod_TranslateGivenDescriptors)( \                     int nin, int nout, \                     PyArray_DTypeMeta *wrapped_dtypes[], \                     PyArray_Descr *given_descrs[], \                     PyArray_Descr *new_descrs[]);      The function to convert the given descriptors (passed in to`resolve\_descriptors``) and translates them for the wrapped loop.     The new descriptors MUST be viewable with the old ones, `NULL` must be     supported (for output arguments) and should normally be forwarded.      The output of of this function will be used to construct     views of the arguments as if they were the translated dtypes and     does not use a cast. This means this mechanism is mostly useful for     DTypes that "wrap" another DType implementation. For example, a unit     DType could use this to wrap an existing floating point DType     without needing to re-implement low-level ufunc logic. In the unit     example,``resolve\_descriptors``would handle computing the output     unit from the input unit.  .. c:type:: int (PyArrayMethod_TranslateLoopDescriptors)( \                     int nin, int nout, PyArray_DTypeMeta *new_dtypes[], \                     PyArray_Descr *given_descrs[], \                     PyArray_Descr *original_descrs[], \                     PyArray_Descr *loop_descrs[]);     The function to convert the actual loop descriptors (as returned by    the original `resolve_descriptors` function) to the ones the output    array should use. This function must return "viewable" types, it must    not mutate them in any form that would break the inner-loop logic.    Does not need to support NULL.  Wrapping Loop Example ^^^^^^^^^^^^^^^^^^^^^  Suppose you want to wrap the``float64`multiply implementation for a`WrappedDoubleDType`. You would add a wrapping loop like so:`\`c PyArray\_DTypeMeta *orig\_dtypes\[3\] = { \&WrappedDoubleDType, \&WrappedDoubleDType, \&WrappedDoubleDType}; PyArray\_DTypeMeta*wrapped\_dtypes\[3\] = { \&PyArray\_Float64DType, \&PyArray\_Float64DType, \&PyArray\_Float64DType}

> PyObject *mod = PyImport\_ImportModule("numpy"); if (mod == NULL) { return -1; } PyObject*multiply = PyObject\_GetAttrString(mod, "multiply"); Py\_DECREF(mod);
> 
>   - if (multiply == NULL) {  
>     return -1;
> 
> }
> 
>   - int res = PyUFunc\_AddWrappingLoop(  
>     multiply, orig\_dtypes, wrapped\_dtypes, \&translate\_given\_descrs \&translate\_loop\_descrs);
> 
> Py\_DECREF(multiply);

Note that this also requires two functions to be defined above this `` ` code: ``\`c static int translate\_given\_descrs(int nin, int nout, PyArray\_DTypeMeta *NPY\_UNUSED(wrapped\_dtypes\[\]), PyArray\_Descr*given\_descrs\[\], PyArray\_Descr \*new\_descrs\[\]) { for (int i = 0; i \< nin + nout; i++) { if (given\_descrs\[i\] == NULL) { new\_descrs\[i\] = NULL; } else { new\_descrs\[i\] = PyArray\_DescrFromType(NPY\_DOUBLE); } } return 0; }

> static int translate\_loop\_descrs(int nin, int NPY\_UNUSED(nout), PyArray\_DTypeMeta *NPY\_UNUSED(new\_dtypes\[\]), PyArray\_Descr*given\_descrs\[\], PyArray\_Descr *original\_descrs\[\], PyArray\_Descr*loop\_descrs\[\]) { // more complicated parametric DTypes may need to // to do additional checking, but we know the wrapped // DTypes *have* to be float64 for this example. loop\_descrs\[0\] = PyArray\_DescrFromType(NPY\_FLOAT64); Py\_INCREF(loop\_descrs\[0\]); loop\_descrs\[1\] = PyArray\_DescrFromType(NPY\_FLOAT64); Py\_INCREF(loop\_descrs\[1\]); loop\_descrs\[2\] = PyArray\_DescrFromType(NPY\_FLOAT64); Py\_INCREF(loop\_descrs\[2\]); }

API for calling array methods `` ` -----------------------------  Conversion ~~~~~~~~~~  .. c:function:: PyObject* PyArray_GetField( \         PyArrayObject* self, PyArray_Descr* dtype, int offset)      Equivalent to `ndarray.getfield<numpy.ndarray.getfield>`     (*self*, *dtype*, *offset*). This function `steals a reference     <https://docs.python.org/3/c-api/intro.html?reference-count-details>`_     to :c`PyArray_Descr` and returns a new array of the given `dtype` using     the data in the current array at a specified `offset` in bytes. The     `offset` plus the itemsize of the new array type must be less than ``self-\>descr-\>elsize``or an error is raised. The same shape and strides     as the original array are used. Therefore, this function has the     effect of returning a field from a structured array. But, it can also     be used to select specific bytes or groups of bytes from any array     type.  .. c:function:: int PyArray_SetField( \         PyArrayObject* self, PyArray_Descr* dtype, int offset, PyObject* val)      Equivalent to `ndarray.setfield<numpy.ndarray.setfield>` (*self*, *val*, *dtype*, *offset*     ). Set the field starting at *offset* in bytes and of the given     *dtype* to *val*. The *offset* plus *dtype* ->elsize must be less     than *self* ->descr->elsize or an error is raised. Otherwise, the     *val* argument is converted to an array and copied into the field     pointed to. If necessary, the elements of *val* are repeated to     fill the destination array, But, the number of elements in the     destination must be an integer multiple of the number of elements     in *val*.  .. c:function:: PyObject* PyArray_Byteswap(PyArrayObject* self, npy_bool inplace)      Equivalent to `ndarray.byteswap<numpy.ndarray.byteswap>` (*self*, *inplace*). Return an array     whose data area is byteswapped. If *inplace* is non-zero, then do     the byteswap inplace and return a reference to self. Otherwise,     create a byteswapped copy and leave self unchanged.  .. c:function:: PyObject* PyArray_NewCopy(PyArrayObject* old, NPY_ORDER order)      Equivalent to `ndarray.copy<numpy.ndarray.copy>` (*self*, *fortran*). Make a copy of the     *old* array. The returned array is always aligned and writeable     with data interpreted the same as the old array. If *order* is     :c`NPY_CORDER`, then a C-style contiguous array is returned. If     *order* is :c`NPY_FORTRANORDER`, then a Fortran-style contiguous     array is returned. If *order is* :c`NPY_ANYORDER`, then the array     returned is Fortran-style contiguous only if the old one is;     otherwise, it is C-style contiguous.  .. c:function:: PyObject* PyArray_ToList(PyArrayObject* self)      Equivalent to `ndarray.tolist<numpy.ndarray.tolist>` (*self*). Return a nested Python list     from *self*.  .. c:function:: PyObject* PyArray_ToString(PyArrayObject* self, NPY_ORDER order)      Equivalent to `ndarray.tobytes<numpy.ndarray.tobytes>` (*self*, *order*). Return the bytes     of this array in a Python string.  .. c:function:: PyObject* PyArray_ToFile( \         PyArrayObject* self, FILE* fp, char* sep, char* format)      Write the contents of *self* to the file pointer *fp* in C-style     contiguous fashion. Write the data as binary bytes if *sep* is the     string ""or``NULL`. Otherwise, write the contents of *self* as     text using the *sep* string as the item separator. Each item will     be printed to the file.  If the *format* string is not`NULL``or     "", then it is a Python print statement format string showing how     the items are to be written.  .. c:function:: int PyArray_Dump(PyObject* self, PyObject* file, int protocol)      Pickle the object in *self* to the given *file* (either a string     or a Python file object). If *file* is a Python string it is     considered to be the name of a file which is then opened in binary     mode. The given *protocol* is used (if *protocol* is negative, or     the highest available is used). This is a simple wrapper around     cPickle.dump(*self*, *file*, *protocol*).  .. c:function:: PyObject* PyArray_Dumps(PyObject* self, int protocol)      Pickle the object in *self* to a Python string and return it. Use     the Pickle *protocol* provided (or the highest available if     *protocol* is negative).  .. c:function:: int PyArray_FillWithScalar(PyArrayObject* arr, PyObject* obj)      Fill the array, *arr*, with the given scalar object, *obj*. The     object is first converted to the data type of *arr*, and then     copied into every location. A -1 is returned if an error occurs,     otherwise 0 is returned.  .. c:function:: PyObject* PyArray_View( \         PyArrayObject* self, PyArray_Descr* dtype, PyTypeObject *ptype)      Equivalent to `ndarray.view<numpy.ndarray.view>` (*self*, *dtype*). Return a new     view of the array *self* as possibly a different data-type, *dtype*,     and different array subclass *ptype*.      If *dtype* is``NULL``, then the returned array will have the same     data type as *self*. The new data-type must be consistent with the     size of *self*. Either the itemsizes must be identical, or *self* must     be single-segment and the total number of bytes must be the same.     In the latter case the dimensions of the returned array will be     altered in the last (or first for Fortran-style contiguous arrays)     dimension. The data area of the returned array and self is exactly     the same.   Shape Manipulation ~~~~~~~~~~~~~~~~~~  .. c:function:: PyObject* PyArray_Newshape( \         PyArrayObject* self, PyArray_Dims* newshape, NPY_ORDER order)      Result will be a new array (pointing to the same memory location     as *self* if possible), but having a shape given by *newshape*.     If the new shape is not compatible with the strides of *self*,     then a copy of the array with the new specified shape will be     returned.  .. c:function:: PyObject* PyArray_Reshape(PyArrayObject* self, PyObject* shape)      Equivalent to `ndarray.reshape<numpy.ndarray.reshape>` (*self*, *shape*) where *shape* is a     sequence. Converts *shape* to a :c:type:`PyArray_Dims` structure and     calls :c`PyArray_Newshape` internally.     For back-ward compatibility -- Not recommended  .. c:function:: PyObject* PyArray_Squeeze(PyArrayObject* self)      Equivalent to `ndarray.squeeze<numpy.ndarray.squeeze>` (*self*). Return a new view of *self*     with all of the dimensions of length 1 removed from the shape.  > **Warning** >      matrix objects are always 2-dimensional. Therefore,     :c`PyArray_Squeeze` has no effect on arrays of matrix sub-class.  .. c:function:: PyObject* PyArray_SwapAxes(PyArrayObject* self, int a1, int a2)      Equivalent to `ndarray.swapaxes<numpy.ndarray.swapaxes>` (*self*, *a1*, *a2*). The returned     array is a new view of the data in *self* with the given axes,     *a1* and *a2*, swapped.  .. c:function:: PyObject* PyArray_Resize( \         PyArrayObject* self, PyArray_Dims* newshape, int refcheck, \         NPY_ORDER fortran)      Equivalent to `ndarray.resize<numpy.ndarray.resize>` (*self*, *newshape*, refcheck``=`*refcheck*, order= fortran ). This function only works on     single-segment arrays. It changes the shape of *self* inplace and     will reallocate the memory for *self* if *newshape* has a     different total number of elements then the old shape. If     reallocation is necessary, then *self* must own its data, have     *self* -`\>base==NULL`, have *self* -`\>weakrefs==NULL``, and     (unless refcheck is 0) not be referenced by any other array.     The fortran argument can be :c`NPY_ANYORDER`, :c`NPY_CORDER`,     or :c`NPY_FORTRANORDER`. It currently has no effect. Eventually     it could be used to determine how the resize operation should view     the data when constructing a differently-dimensioned array.     Returns None on success and NULL on error.  .. c:function:: PyObject* PyArray_Transpose( \         PyArrayObject* self, PyArray_Dims* permute)      Equivalent to `ndarray.transpose<numpy.ndarray.transpose>` (*self*, *permute*). Permute the     axes of the ndarray object *self* according to the data structure     *permute* and return the result. If *permute* is``NULL``, then     the resulting array has its axes reversed. For example if *self*     has shape :math:`10\times20\times30`, and *permute*``.ptr``is     (0,2,1) the shape of the result is :math:`10\times30\times20.` If     *permute* is``NULL``, the shape of the result is     :math:`30\times20\times10.`  .. c:function:: PyObject* PyArray_Flatten(PyArrayObject* self, NPY_ORDER order)      Equivalent to `ndarray.flatten<numpy.ndarray.flatten>` (*self*, *order*). Return a 1-d copy     of the array. If *order* is :c`NPY_FORTRANORDER` the elements are     scanned out in Fortran order (first-dimension varies the     fastest). If *order* is :c`NPY_CORDER`, the elements of``self``are scanned in C-order (last dimension varies the fastest). If     *order* :c`NPY_ANYORDER`, then the result of     :c`PyArray_ISFORTRAN` (*self*) is used to determine which order     to flatten.  .. c:function:: PyObject* PyArray_Ravel(PyArrayObject* self, NPY_ORDER order)      Equivalent to *self*.ravel(*order*). Same basic functionality     as :c`PyArray_Flatten` (*self*, *order*) except if *order* is 0     and *self* is C-style contiguous, the shape is altered but no copy     is performed.   Item selection and manipulation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. c:function:: PyObject* PyArray_TakeFrom( \         PyArrayObject* self, PyObject* indices, int axis, PyArrayObject* ret, \         NPY_CLIPMODE clipmode)      Equivalent to `ndarray.take<numpy.ndarray.take>` (*self*, *indices*, *axis*, *ret*,     *clipmode*) except *axis* =None in Python is obtained by setting     *axis* = :c`NPY_MAXDIMS` in C. Extract the items from self     indicated by the integer-valued *indices* along the given *axis.*     The clipmode argument can be :c`NPY_RAISE`, :c`NPY_WRAP`, or     :c`NPY_CLIP` to indicate what to do with out-of-bound indices. The     *ret* argument can specify an output array rather than having one     created internally.  .. c:function:: PyObject* PyArray_PutTo( \         PyArrayObject* self, PyObject* values, PyObject* indices, \         NPY_CLIPMODE clipmode)      Equivalent to *self*.put(*values*, *indices*, *clipmode*     ). Put *values* into *self* at the corresponding (flattened)     *indices*. If *values* is too small it will be repeated as     necessary.  .. c:function:: PyObject* PyArray_PutMask( \         PyArrayObject* self, PyObject* values, PyObject* mask)      Place the *values* in *self* wherever corresponding positions     (using a flattened context) in *mask* are true. The *mask* and     *self* arrays must have the same total number of elements. If     *values* is too small, it will be repeated as necessary.  .. c:function:: PyObject* PyArray_Repeat( \         PyArrayObject* self, PyObject* op, int axis)      Equivalent to `ndarray.repeat<numpy.ndarray.repeat>` (*self*, *op*, *axis*). Copy the     elements of *self*, *op* times along the given *axis*. Either     *op* is a scalar integer or a sequence of length *self*     ->dimensions[ *axis* ] indicating how many times to repeat each     item along the axis.  .. c:function:: PyObject* PyArray_Choose( \         PyArrayObject* self, PyObject* op, PyArrayObject* ret, \         NPY_CLIPMODE clipmode)      Equivalent to `ndarray.choose<numpy.ndarray.choose>` (*self*, *op*, *ret*, *clipmode*).     Create a new array by selecting elements from the sequence of     arrays in *op* based on the integer values in *self*. The arrays     must all be broadcastable to the same shape and the entries in     *self* should be between 0 and len(*op*). The output is placed     in *ret* unless it is``NULL``in which case a new output is     created. The *clipmode* argument determines behavior for when     entries in *self* are not between 0 and len(*op*).      .. c:macro:: NPY_RAISE          raise a ValueError;      .. c:macro:: NPY_WRAP          wrap values < 0 by adding len(*op*) and values >=len(*op*)         by subtracting len(*op*) until they are in range;      .. c:macro:: NPY_CLIP          all values are clipped to the region [0, len(*op*) ).   .. c:function:: PyObject* PyArray_Sort(PyArrayObject* self, int axis, NPY_SORTKIND kind)      Equivalent to `ndarray.sort<numpy.ndarray.sort>` (*self*, *axis*, *kind*).     Return an array with the items of *self* sorted along *axis*. The array     is sorted using the algorithm denoted by *kind*, which is an integer/enum pointing     to the type of sorting algorithms used.  .. c:function:: PyObject* PyArray_ArgSort(PyArrayObject* self, int axis)      Equivalent to `ndarray.argsort<numpy.ndarray.argsort>` (*self*, *axis*).     Return an array of indices such that selection of these indices     along the given``axis``would return a sorted version of *self*. If *self* ->descr     is a data-type with fields defined, then self->descr->names is used     to determine the sort order. A comparison where the first field is equal     will use the second field and so on. To alter the sort order of a     structured array, create a new data-type with a different order of names     and construct a view of the array with that new data-type.  .. c:function:: PyObject* PyArray_LexSort(PyObject* sort_keys, int axis)      Given a sequence of arrays (*sort_keys*) of the same shape,     return an array of indices (similar to :c`PyArray_ArgSort` (...))     that would sort the arrays lexicographically. A lexicographic sort     specifies that when two keys are found to be equal, the order is     based on comparison of subsequent keys. A merge sort (which leaves     equal entries unmoved) is required to be defined for the     types. The sort is accomplished by sorting the indices first using     the first *sort_key* and then using the second *sort_key* and so     forth. This is equivalent to the lexsort(*sort_keys*, *axis*)     Python command. Because of the way the merge-sort works, be sure     to understand the order the *sort_keys* must be in (reversed from     the order you would use when comparing two elements).      If these arrays are all collected in a structured array, then     :c`PyArray_Sort` (...) can also be used to sort the array     directly.  .. c:function:: PyObject* PyArray_SearchSorted( \         PyArrayObject* self, PyObject* values, NPY_SEARCHSIDE side, \         PyObject* perm)      Equivalent to `ndarray.searchsorted<numpy.ndarray.searchsorted>` (*self*, *values*, *side*,     *perm*). Assuming *self* is a 1-d array in ascending order, then the     output is an array of indices the same shape as *values* such that, if     the elements in *values* were inserted before the indices, the order of     *self* would be preserved. No checking is done on whether or not self is     in ascending order.      The *side* argument indicates whether the index returned should be that of     the first suitable location (if :c`NPY_SEARCHLEFT`) or of the last     (if :c`NPY_SEARCHRIGHT`).      The *sorter* argument, if not``NULL``, must be a 1D array of integer     indices the same length as *self*, that sorts it into ascending order.     This is typically the result of a call to :c`PyArray_ArgSort` (...)     Binary search is used to find the required insertion points.  .. c:function:: int PyArray_Partition( \         PyArrayObject *self, PyArrayObject * ktharray, int axis, \         NPY_SELECTKIND which)      Equivalent to `ndarray.partition<numpy.ndarray.partition>` (*self*, *ktharray*, *axis*,     *kind*). Partitions the array so that the values of the element indexed by     *ktharray* are in the positions they would be if the array is fully sorted     and places all elements smaller than the kth before and all elements equal     or greater after the kth element. The ordering of all elements within the     partitions is undefined.     If *self*->descr is a data-type with fields defined, then     self->descr->names is used to determine the sort order. A comparison where     the first field is equal will use the second field and so on. To alter the     sort order of a structured array, create a new data-type with a different     order of names and construct a view of the array with that new data-type.     Returns zero on success and -1 on failure.  .. c:function:: PyObject* PyArray_ArgPartition( \         PyArrayObject *op, PyArrayObject * ktharray, int axis, \         NPY_SELECTKIND which)      Equivalent to `ndarray.argpartition<numpy.ndarray.argpartition>` (*self*, *ktharray*, *axis*,     *kind*). Return an array of indices such that selection of these indices     along the given``axis``would return a partitioned version of *self*.  .. c:function:: PyObject* PyArray_Diagonal( \         PyArrayObject* self, int offset, int axis1, int axis2)      Equivalent to `ndarray.diagonal<numpy.ndarray.diagonal>` (*self*, *offset*, *axis1*, *axis2*     ). Return the *offset* diagonals of the 2-d arrays defined by     *axis1* and *axis2*.  .. c:function:: npy_intp PyArray_CountNonzero(PyArrayObject* self)      Counts the number of non-zero elements in the array object *self*.  .. c:function:: PyObject* PyArray_Nonzero(PyArrayObject* self)      Equivalent to `ndarray.nonzero<numpy.ndarray.nonzero>` (*self*). Returns a tuple of index     arrays that select elements of *self* that are nonzero. If (nd=     :c`PyArray_NDIM` (``self``))==1, then a single index array is     returned. The index arrays have data type :c`NPY_INTP`. If a     tuple is returned (nd :math:`\neq` 1), then its length is nd.  .. c:function:: PyObject* PyArray_Compress( \         PyArrayObject* self, PyObject* condition, int axis, PyArrayObject* out)      Equivalent to `ndarray.compress<numpy.ndarray.compress>` (*self*, *condition*, *axis*     ). Return the elements along *axis* corresponding to elements of     *condition* that are true.   Calculation ~~~~~~~~~~~  > **Tip** >      Pass in :c`NPY_RAVEL_AXIS` for axis in order to achieve the same     effect that is obtained by passing in``axis=None`in Python     (treating the array as a 1-d array).   > **Note** >      The out argument specifies where to place the result. If out is     NULL, then the output array is created, otherwise the output is     placed in out which must be the correct size and type. A new     reference to the output array is always returned even when out     is not NULL. The caller of the routine has the responsibility     to`Py\_DECREF``out if not NULL or a memory-leak will occur.   .. c:function:: PyObject* PyArray_ArgMax( \         PyArrayObject* self, int axis, PyArrayObject* out)      Equivalent to `ndarray.argmax<numpy.ndarray.argmax>` (*self*, *axis*). Return the index of     the largest element of *self* along *axis*.  .. c:function:: PyObject* PyArray_ArgMin( \         PyArrayObject* self, int axis, PyArrayObject* out)      Equivalent to `ndarray.argmin<numpy.ndarray.argmin>` (*self*, *axis*). Return the index of     the smallest element of *self* along *axis*.  .. c:function:: PyObject* PyArray_Max( \         PyArrayObject* self, int axis, PyArrayObject* out)      Equivalent to `ndarray.max<numpy.ndarray.max>` (*self*, *axis*). Returns the largest     element of *self* along the given *axis*. When the result is a single     element, returns a numpy scalar instead of an ndarray.  .. c:function:: PyObject* PyArray_Min( \         PyArrayObject* self, int axis, PyArrayObject* out)      Equivalent to `ndarray.min<numpy.ndarray.min>` (*self*, *axis*). Return the smallest     element of *self* along the given *axis*. When the result is a single     element, returns a numpy scalar instead of an ndarray.   .. c:function:: PyObject* PyArray_Ptp( \         PyArrayObject* self, int axis, PyArrayObject* out)      Return the difference between the largest element of *self* along *axis* and the     smallest element of *self* along *axis*. When the result is a single     element, returns a numpy scalar instead of an ndarray.     .. note::      The rtype argument specifies the data-type the reduction should     take place over. This is important if the data-type of the array     is not "large" enough to handle the output. By default, all     integer data-types are made at least as large as :c`NPY_LONG`     for the "add" and "multiply" ufuncs (which form the basis for     mean, sum, cumsum, prod, and cumprod functions).  .. c:function:: PyObject* PyArray_Mean( \         PyArrayObject* self, int axis, int rtype, PyArrayObject* out)      Equivalent to `ndarray.mean<numpy.ndarray.mean>` (*self*, *axis*, *rtype*). Returns the     mean of the elements along the given *axis*, using the enumerated     type *rtype* as the data type to sum in. Default sum behavior is     obtained using :c`NPY_NOTYPE` for *rtype*.  .. c:function:: PyObject* PyArray_Trace( \         PyArrayObject* self, int offset, int axis1, int axis2, int rtype, \         PyArrayObject* out)      Equivalent to `ndarray.trace<numpy.ndarray.trace>` (*self*, *offset*, *axis1*, *axis2*,     *rtype*). Return the sum (using *rtype* as the data type of     summation) over the *offset* diagonal elements of the 2-d arrays     defined by *axis1* and *axis2* variables. A positive offset     chooses diagonals above the main diagonal. A negative offset     selects diagonals below the main diagonal.  .. c:function:: PyObject* PyArray_Clip( \         PyArrayObject* self, PyObject* min, PyObject* max)      Equivalent to `ndarray.clip<numpy.ndarray.clip>` (*self*, *min*, *max*). Clip an array,     *self*, so that values larger than *max* are fixed to *max* and     values less than *min* are fixed to *min*.  .. c:function:: PyObject* PyArray_Conjugate(PyArrayObject* self, PyArrayObject* out)      Equivalent to `ndarray.conjugate<numpy.ndarray.conjugate>` (*self*).     Return the complex conjugate of *self*. If *self* is not of     complex data type, then return *self* with a reference.      :param self: Input array.     :param out:  Output array. If provided, the result is placed into this array.      :return: The complex conjugate of *self*.    .. c:function:: PyObject* PyArray_Round( \         PyArrayObject* self, int decimals, PyArrayObject* out)      Equivalent to `ndarray.round<numpy.ndarray.round>` (*self*, *decimals*, *out*). Returns     the array with elements rounded to the nearest decimal place. The     decimal place is defined as the :math:`10^{-\textrm{decimals}}`     digit so that negative *decimals* cause rounding to the nearest 10's, 100's, etc. If out is``NULL``, then the output array is created, otherwise the output is placed in *out* which must be the correct size and type.  .. c:function:: PyObject* PyArray_Std( \         PyArrayObject* self, int axis, int rtype, PyArrayObject* out)      Equivalent to `ndarray.std<numpy.ndarray.std>` (*self*, *axis*, *rtype*). Return the     standard deviation using data along *axis* converted to data type     *rtype*.  .. c:function:: PyObject* PyArray_Sum( \         PyArrayObject* self, int axis, int rtype, PyArrayObject* out)      Equivalent to `ndarray.sum<numpy.ndarray.sum>` (*self*, *axis*, *rtype*). Return 1-d     vector sums of elements in *self* along *axis*. Perform the sum     after converting data to data type *rtype*.  .. c:function:: PyObject* PyArray_CumSum( \         PyArrayObject* self, int axis, int rtype, PyArrayObject* out)      Equivalent to `ndarray.cumsum<numpy.ndarray.cumsum>` (*self*, *axis*, *rtype*). Return     cumulative 1-d sums of elements in *self* along *axis*. Perform     the sum after converting data to data type *rtype*.  .. c:function:: PyObject* PyArray_Prod( \         PyArrayObject* self, int axis, int rtype, PyArrayObject* out)      Equivalent to `ndarray.prod<numpy.ndarray.prod>` (*self*, *axis*, *rtype*). Return 1-d     products of elements in *self* along *axis*. Perform the product     after converting data to data type *rtype*.  .. c:function:: PyObject* PyArray_CumProd( \         PyArrayObject* self, int axis, int rtype, PyArrayObject* out)      Equivalent to `ndarray.cumprod<numpy.ndarray.cumprod>` (*self*, *axis*, *rtype*). Return     1-d cumulative products of elements in``self`along`axis`.     Perform the product after converting data to data type`rtype``.  .. c:function:: PyObject* PyArray_All( \         PyArrayObject* self, int axis, PyArrayObject* out)      Equivalent to `ndarray.all<numpy.ndarray.all>` (*self*, *axis*). Return an array with     True elements for every 1-d sub-array of``self`defined by`axis``in which all the elements are True.  .. c:function:: PyObject* PyArray_Any( \         PyArrayObject* self, int axis, PyArrayObject* out)      Equivalent to `ndarray.any<numpy.ndarray.any>` (*self*, *axis*). Return an array with     True elements for every 1-d sub-array of *self* defined by *axis*     in which any of the elements are True.  Functions ---------   Array Functions ~~~~~~~~~~~~~~~  .. c:function:: int PyArray_AsCArray( \         PyObject** op, void* ptr, npy_intp* dims, int nd, \         PyArray_Descr* typedescr)      Sometimes it is useful to access a multidimensional array as a     C-style multi-dimensional array so that algorithms can be     implemented using C's a[i][j][k] syntax. This routine returns a     pointer, *ptr*, that simulates this kind of C-style array, for     1-, 2-, and 3-d ndarrays.      :param op:          The address to any Python object. This Python object will be replaced         with an equivalent well-behaved, C-style contiguous, ndarray of the         given data type specified by the last two arguments. Be sure that         stealing a reference in this way to the input object is justified.      :param ptr:          The address to a (ctype* for 1-d, ctype** for 2-d or ctype*** for 3-d)         variable where ctype is the equivalent C-type for the data type. On         return, *ptr* will be addressable as a 1-d, 2-d, or 3-d array.      :param dims:          An output array that contains the shape of the array object. This         array gives boundaries on any looping that will take place.      :param nd:          The dimensionality of the array (1, 2, or 3).      :param typedescr:          A :c:type:`PyArray_Descr` structure indicating the desired data-type         (including required byteorder). The call will steal a reference to         the parameter.  > **Note** >      The simulation of a C-style array is not complete for 2-d and 3-d     arrays. For example, the simulated arrays of pointers cannot be passed     to subroutines expecting specific, statically-defined 2-d and 3-d     arrays. To pass to functions requiring those kind of inputs, you must     statically define the required array and copy data.  .. c:function:: int PyArray_Free(PyObject* op, void* ptr)      Must be called with the same objects and memory locations returned     from :c`PyArray_AsCArray` (...). This function cleans up memory     that otherwise would get leaked.  .. c:function:: PyObject* PyArray_Concatenate(PyObject* obj, int axis)      Join the sequence of objects in *obj* together along *axis* into a     single array. If the dimensions or types are not compatible an     error is raised.  .. c:function:: PyObject* PyArray_InnerProduct(PyObject* obj1, PyObject* obj2)      Compute a product-sum over the last dimensions of *obj1* and     *obj2*. Neither array is conjugated.  .. c:function:: PyObject* PyArray_MatrixProduct(PyObject* obj1, PyObject* obj)      Compute a product-sum over the last dimension of *obj1* and the     second-to-last dimension of *obj2*. For 2-d arrays this is a     matrix-product. Neither array is conjugated.  .. c:function:: PyObject* PyArray_MatrixProduct2( \         PyObject* obj1, PyObject* obj, PyArrayObject* out)      Same as PyArray_MatrixProduct, but store the result in *out*.  The     output array must have the correct shape, type, and be     C-contiguous, or an exception is raised.  .. c:function:: PyArrayObject* PyArray_EinsteinSum( \         char* subscripts, npy_intp nop, PyArrayObject** op_in, \         PyArray_Descr* dtype, NPY_ORDER order, NPY_CASTING casting, \         PyArrayObject* out)      Applies the Einstein summation convention to the array operands     provided, returning a new array or placing the result in *out*.     The string in *subscripts* is a comma separated list of index     letters. The number of operands is in *nop*, and *op_in* is an     array containing those operands. The data type of the output can     be forced with *dtype*, the output order can be forced with *order*     (:c`NPY_KEEPORDER` is recommended), and when *dtype* is specified,     *casting* indicates how permissive the data conversion should be.      See the `~numpy.einsum` function for more details.  .. c:function:: PyObject* PyArray_Correlate( \         PyObject* op1, PyObject* op2, int mode)      Compute the 1-d correlation of the 1-d arrays *op1* and *op2*     . The correlation is computed at each output point by multiplying     *op1* by a shifted version of *op2* and summing the result. As a     result of the shift, needed values outside of the defined range of     *op1* and *op2* are interpreted as zero. The mode determines how     many shifts to return: 0 - return only shifts that did not need to     assume zero- values; 1 - return an object that is the same size as     *op1*, 2 - return all possible shifts (any overlap at all is     accepted).      .. rubric:: Notes      This does not compute the usual correlation: if op2 is larger than op1, the     arguments are swapped, and the conjugate is never taken for complex arrays.     See PyArray_Correlate2 for the usual signal processing correlation.  .. c:function:: PyObject* PyArray_Correlate2( \         PyObject* op1, PyObject* op2, int mode)      Updated version of PyArray_Correlate, which uses the usual definition of     correlation for 1d arrays. The correlation is computed at each output point     by multiplying *op1* by a shifted version of *op2* and summing the result.     As a result of the shift, needed values outside of the defined range of     *op1* and *op2* are interpreted as zero. The mode determines how many     shifts to return: 0 - return only shifts that did not need to assume zero-     values; 1 - return an object that is the same size as *op1*, 2 - return all     possible shifts (any overlap at all is accepted).      .. rubric:: Notes      Compute z as follows::        z[k] = sum_n op1[n] * conj(op2[n+k])  .. c:function:: PyObject* PyArray_Where( \         PyObject* condition, PyObject* x, PyObject* y)      If both``x`and`y`are`NULL``, then return     :c`PyArray_Nonzero` (*condition*). Otherwise, both *x* and *y*     must be given and the object returned is shaped like *condition*     and has elements of *x* and *y* where *condition* is respectively     True or False.   Other functions ~~~~~~~~~~~~~~~  .. c:function:: npy_bool PyArray_CheckStrides( \         int elsize, int nd, npy_intp numbytes, npy_intp const* dims, \         npy_intp const* newstrides)      Determine if *newstrides* is a strides array consistent with the     memory of an *nd* -dimensional array with shape``dims``and     element-size, *elsize*. The *newstrides* array is checked to see     if jumping by the provided number of bytes in each direction will     ever mean jumping more than *numbytes* which is the assumed size     of the available memory segment. If *numbytes* is 0, then an     equivalent *numbytes* is computed assuming *nd*, *dims*, and     *elsize* refer to a single-segment array. Return :c`NPY_TRUE` if     *newstrides* is acceptable, otherwise return :c`NPY_FALSE`.  .. c:function:: npy_intp PyArray_MultiplyList(npy_intp const* seq, int n)  .. c:function:: int PyArray_MultiplyIntList(int const* seq, int n)      Both of these routines multiply an *n* -length array, *seq*, of     integers and return the result. No overflow checking is performed.  .. c:function:: int PyArray_CompareLists(npy_intp const* l1, npy_intp const* l2, int n)      Given two *n* -length arrays of integers, *l1*, and *l2*, return     1 if the lists are identical; otherwise, return 0.   Auxiliary data with object semantics ------------------------------------  .. c:type:: NpyAuxData  When working with more complex dtypes which are composed of other dtypes, such as the struct dtype, creating inner loops that manipulate the dtypes requires carrying along additional data. NumPy supports this idea through a struct :c:type:`NpyAuxData`, mandating a few conventions so that it is possible to do this.  Defining an :c:type:`NpyAuxData` is similar to defining a class in C++, but the object semantics have to be tracked manually since the API is in C. Here's an example for a function which doubles up an element using an element copier function as a primitive.``\`c typedef struct { NpyAuxData base; ElementCopier\_Func *func; NpyAuxData*funcdata; } eldoubler\_aux\_data;

>   - void free\_element\_doubler\_aux\_data(NpyAuxData *data) { eldoubler\_aux\_data*d = (eldoubler\_aux\_data *)data; /* Free the memory owned by this auxdata \*/  
>     NPY\_AUXDATA\_FREE(d-\>funcdata); PyArray\_free(d);
> 
> }
> 
> NpyAuxData *clone\_element\_doubler\_aux\_data(NpyAuxData*data) { eldoubler\_aux\_data \*ret = PyArray\_malloc(sizeof(eldoubler\_aux\_data)); if (ret == NULL) { return NULL; }
> 
> > /\* Raw copy of all data \*/ memcpy(ret, data, sizeof(eldoubler\_aux\_data));
> > 
> > /\* Fix up the owned auxdata so we have our own copy \*/ ret-\>funcdata = NPY\_AUXDATA\_CLONE(ret-\>funcdata); if (ret-\>funcdata == NULL) { PyArray\_free(ret); return NULL; }
> > 
> > return (NpyAuxData \*)ret;
> 
> }
> 
>   - NpyAuxData *create\_element\_doubler\_aux\_data( ElementCopier\_Func*func,  
>     NpyAuxData \*funcdata)
> 
>   - {  
>     eldoubler\_aux\_data \*ret = PyArray\_malloc(sizeof(eldoubler\_aux\_data)); if (ret == NULL) { PyErr\_NoMemory(); return NULL; } memset(\&ret, 0, sizeof(eldoubler\_aux\_data)); ret-\>base-\>free = \&free\_element\_doubler\_aux\_data; ret-\>base-\>clone = \&clone\_element\_doubler\_aux\_data; ret-\>func = func; ret-\>funcdata = funcdata;
>     
>     return (NpyAuxData \*)ret;
> 
> }

Array iterators `` ` ---------------  As of NumPy 1.6.0, these array iterators are superseded by the new array iterator, :c:type:`NpyIter`.  An array iterator is a simple way to access the elements of an N-dimensional array quickly and efficiently, as seen in [the example <iteration-example>](#the example-<iteration-example>) which provides more description of this useful approach to looping over an array from C.  .. c:function:: PyObject* PyArray_IterNew(PyObject* arr)      Return an array iterator object from the array, *arr*. This is     equivalent to *arr*. **flat**. The array iterator object makes     it easy to loop over an N-dimensional non-contiguous array in     C-style contiguous fashion.  .. c:function:: PyObject* PyArray_IterAllButAxis(PyObject* arr, int* axis)      Return an array iterator that will iterate over all axes but the     one provided in *\*axis*. The returned iterator cannot be used     with :c`PyArray_ITER_GOTO1D`. This iterator could be used to     write something similar to what ufuncs do wherein the loop over     the largest axis is done by a separate sub-routine. If *\*axis* is     negative then *\*axis* will be set to the axis having the smallest     stride and that axis will be used.  .. c:function:: PyObject *PyArray_BroadcastToShape( \         PyObject* arr, npy_intp const *dimensions, int nd)      Return an array iterator that is broadcast to iterate as an array     of the shape provided by *dimensions* and *nd*.  .. c:function:: int PyArrayIter_Check(PyObject* op)      Evaluates true if *op* is an array iterator (or instance of a     subclass of the array iterator type).  .. c:function:: void PyArray_ITER_RESET(PyObject* iterator)      Reset an *iterator* to the beginning of the array.  .. c:function:: void PyArray_ITER_NEXT(PyObject* iterator)      Increment the index and the dataptr members of the *iterator* to     point to the next element of the array. If the array is not     (C-style) contiguous, also increment the N-dimensional coordinates     array.  .. c:function:: void *PyArray_ITER_DATA(PyObject* iterator)      A pointer to the current element of the array.  .. c:function:: void PyArray_ITER_GOTO( \         PyObject* iterator, npy_intp* destination)      Set the *iterator* index, dataptr, and coordinates members to the     location in the array indicated by the N-dimensional c-array,     *destination*, which must have size at least *iterator*     ->nd_m1+1.  .. c:function:: void PyArray_ITER_GOTO1D(PyObject* iterator, npy_intp index)      Set the *iterator* index and dataptr to the location in the array     indicated by the integer *index* which points to an element in the     C-styled flattened array.  .. c:function:: int PyArray_ITER_NOTDONE(PyObject* iterator)      Evaluates TRUE as long as the iterator has not looped through all of     the elements, otherwise it evaluates FALSE.   Broadcasting (multi-iterators) ------------------------------  .. c:function:: PyObject* PyArray_MultiIterNew(int num, ...)      A simplified interface to broadcasting. This function takes the     number of arrays to broadcast and then *num* extra ( :c:type:`PyObject *<PyObject>`     ) arguments. These arguments are converted to arrays and iterators     are created. :c`PyArray_Broadcast` is then called on the resulting     multi-iterator object. The resulting, broadcasted mult-iterator     object is then returned. A broadcasted operation can then be     performed using a single loop and using :c`PyArray_MultiIter_NEXT`     (..)  .. c:function:: void PyArray_MultiIter_RESET(PyObject* multi)      Reset all the iterators to the beginning in a multi-iterator     object, *multi*.  .. c:function:: void PyArray_MultiIter_NEXT(PyObject* multi)      Advance each iterator in a multi-iterator object, *multi*, to its     next (broadcasted) element.  .. c:function:: void *PyArray_MultiIter_DATA(PyObject* multi, int i)      Return the data-pointer of the *i* :math:`^{\textrm{th}}` iterator     in a multi-iterator object.  .. c:function:: void PyArray_MultiIter_NEXTi(PyObject* multi, int i)      Advance the pointer of only the *i* :math:`^{\textrm{th}}` iterator.  .. c:function:: void PyArray_MultiIter_GOTO( \         PyObject* multi, npy_intp* destination)      Advance each iterator in a multi-iterator object, *multi*, to the     given :math:`N` -dimensional *destination* where :math:`N` is the     number of dimensions in the broadcasted array.  .. c:function:: void PyArray_MultiIter_GOTO1D(PyObject* multi, npy_intp index)      Advance each iterator in a multi-iterator object, *multi*, to the     corresponding location of the *index* into the flattened     broadcasted array.  .. c:function:: int PyArray_MultiIter_NOTDONE(PyObject* multi)      Evaluates TRUE as long as the multi-iterator has not looped     through all of the elements (of the broadcasted result), otherwise     it evaluates FALSE.  .. c:function:: npy_intp PyArray_MultiIter_SIZE(PyArrayMultiIterObject* multi)      .. versionadded:: 1.26.0      Returns the total broadcasted size of a multi-iterator object.  .. c:function:: int PyArray_MultiIter_NDIM(PyArrayMultiIterObject* multi)      .. versionadded:: 1.26.0      Returns the number of dimensions in the broadcasted result of     a multi-iterator object.  .. c:function:: npy_intp PyArray_MultiIter_INDEX(PyArrayMultiIterObject* multi)      .. versionadded:: 1.26.0      Returns the current (1-d) index into the broadcasted result     of a multi-iterator object.  .. c:function:: int PyArray_MultiIter_NUMITER(PyArrayMultiIterObject* multi)      .. versionadded:: 1.26.0      Returns the number of iterators that are represented by a     multi-iterator object.  .. c:function:: void** PyArray_MultiIter_ITERS(PyArrayMultiIterObject* multi)      .. versionadded:: 1.26.0      Returns an array of iterator objects that holds the iterators for the     arrays to be broadcast together. On return, the iterators are adjusted     for broadcasting.  .. c:function:: npy_intp* PyArray_MultiIter_DIMS(PyArrayMultiIterObject* multi)      .. versionadded:: 1.26.0      Returns a pointer to the dimensions/shape of the broadcasted result of a     multi-iterator object.  .. c:function:: int PyArray_Broadcast(PyArrayMultiIterObject* mit)      This function encapsulates the broadcasting rules. The *mit*     container should already contain iterators for all the arrays that     need to be broadcast. On return, these iterators will be adjusted     so that iteration over each simultaneously will accomplish the     broadcasting. A negative number is returned if an error occurs.  .. c:function:: int PyArray_RemoveSmallest(PyArrayMultiIterObject* mit)      This function takes a multi-iterator object that has been     previously "broadcasted," finds the dimension with the smallest     "sum of strides" in the broadcasted result and adapts all the     iterators so as not to iterate over that dimension (by effectively     making them of length-1 in that dimension). The corresponding     dimension is returned unless *mit* ->nd is 0, then -1 is     returned. This function is useful for constructing ufunc-like     routines that broadcast their inputs correctly and then call a     strided 1-d version of the routine as the inner-loop.  This 1-d     version is usually optimized for speed and for this reason the     loop should be performed over the axis that won't require large     stride jumps.  Neighborhood iterator ---------------------  Neighborhood iterators are subclasses of the iterator object, and can be used to iter over a neighborhood of a point. For example, you may want to iterate over every voxel of a 3d image, and for every such voxel, iterate over an hypercube. Neighborhood iterator automatically handle boundaries, thus making this kind of code much easier to write than manual boundaries handling, at the cost of a slight overhead.  .. c:function:: PyObject* PyArray_NeighborhoodIterNew( \         PyArrayIterObject* iter, npy_intp bounds, int mode, \         PyArrayObject* fill_value)      This function creates a new neighborhood iterator from an existing     iterator.  The neighborhood will be computed relatively to the position     currently pointed by *iter*, the bounds define the shape of the     neighborhood iterator, and the mode argument the boundaries handling mode.      The *bounds* argument is expected to be a (2 * iter->ao->nd) arrays, such     as the range bound[2*i]->bounds[2*i+1] defines the range where to walk for     dimension i (both bounds are included in the walked coordinates). The     bounds should be ordered for each dimension (bounds[2*i] <= bounds[2*i+1]).      The mode should be one of:      .. c:macro:: NPY_NEIGHBORHOOD_ITER_ZERO_PADDING              Zero padding. Outside bounds values will be 0.      .. c:macro:: NPY_NEIGHBORHOOD_ITER_ONE_PADDING              One padding, Outside bounds values will be 1.      .. c:macro:: NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING              Constant padding. Outside bounds values will be the             same as the first item in fill_value.      .. c:macro:: NPY_NEIGHBORHOOD_ITER_MIRROR_PADDING              Mirror padding. Outside bounds values will be as if the             array items were mirrored. For example, for the array [1, 2, 3, 4],             x[-2] will be 2, x[-2] will be 1, x[4] will be 4, x[5] will be 1,             etc...      .. c:macro:: NPY_NEIGHBORHOOD_ITER_CIRCULAR_PADDING              Circular padding. Outside bounds values will be as if the array             was repeated. For example, for the array [1, 2, 3, 4], x[-2] will             be 3, x[-2] will be 4, x[4] will be 1, x[5] will be 2, etc...      If the mode is constant filling (:c:macro:`NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING`),     fill_value should point to an array object which holds the filling value     (the first item will be the filling value if the array contains more than     one item). For other cases, fill_value may be NULL.      - The iterator holds a reference to iter     - Return NULL on failure (in which case the reference count of iter is not       changed)     - iter itself can be a Neighborhood iterator: this can be useful for .e.g       automatic boundaries handling     - the object returned by this function should be safe to use as a normal       iterator     - If the position of iter is changed, any subsequent call to       PyArrayNeighborhoodIter_Next is undefined behavior, and       PyArrayNeighborhoodIter_Reset must be called.     - If the position of iter is not the beginning of the data and the       underlying data for iter is contiguous, the iterator will point to the       start of the data instead of position pointed by iter.       To avoid this situation, iter should be moved to the required position       only after the creation of iterator, and PyArrayNeighborhoodIter_Reset       must be called. ``\`c PyArrayIterObject *iter; PyArrayNeighborhoodIterObject*neigh\_iter; iter = PyArray\_IterNew(x);

> /*For a 3x3 kernel*/ bounds = {-1, 1, -1, 1}; neigh\_iter = (PyArrayNeighborhoodIterObject\*)PyArray\_NeighborhoodIterNew( iter, bounds, NPY\_NEIGHBORHOOD\_ITER\_ZERO\_PADDING, NULL);
> 
>   - for(i = 0; i \< iter-\>size; ++i) {
>     
>       - for (j = 0; j \< neigh\_iter-\>size; ++j) {  
>         /\* Walk around the item currently pointed by iter-\>dataptr \*/ PyArrayNeighborhoodIter\_Next(neigh\_iter);
>     
>     }
>     
>     /\* Move to the next point of iter \*/ PyArrayIter\_Next(iter); PyArrayNeighborhoodIter\_Reset(neigh\_iter);
> 
> }

Array scalars `` ` -------------  .. c:function:: PyObject* PyArray_Return(PyArrayObject* arr)      This function steals a reference to *arr*.      This function checks to see if *arr* is a 0-dimensional array and,     if so, returns the appropriate array scalar. It should be used     whenever 0-dimensional arrays could be returned to Python.  .. c:function:: PyObject* PyArray_Scalar( \         void* data, PyArray_Descr* dtype, PyObject* base)      Return an array scalar object of the given *dtype* by **copying**     from memory pointed to by *data*.  *base* is expected to be the     array object that is the owner of the data.  *base* is required     if `dtype` is a ``void`scalar, or if the`NPY\_USE\_GETITEM`flag is set and it is known that the`getitem`method uses     the`arr`argument without checking if it is`NULL``.  Otherwise     `base` may be``NULL`.      If the data is not in native byte order (as indicated by`dtype-\>byteorder``) then this function will byteswap the data,     because array scalars are always in correct machine-byte order.  .. c:function:: PyObject* PyArray_ToScalar(void* data, PyArrayObject* arr)      Return an array scalar object of the type and itemsize indicated     by the array object *arr* copied from the memory pointed to by     *data* and swapping if the data in *arr* is not in machine     byte-order.  .. c:function:: PyObject* PyArray_FromScalar( \         PyObject* scalar, PyArray_Descr* outcode)      Return a 0-dimensional array of type determined by *outcode* from     *scalar* which should be an array-scalar object. If *outcode* is     NULL, then the type is determined from *scalar*.  .. c:function:: void PyArray_ScalarAsCtype(PyObject* scalar, void* ctypeptr)      Return in *ctypeptr* a pointer to the actual value in an array     scalar. There is no error checking so *scalar* must be an     array-scalar object, and ctypeptr must have enough space to hold     the correct type. For flexible-sized types, a pointer to the data     is copied into the memory of *ctypeptr*, for all other types, the     actual data is copied into the address pointed to by *ctypeptr*.  .. c:function:: int PyArray_CastScalarToCtype( \         PyObject* scalar, void* ctypeptr, PyArray_Descr* outcode)      Return the data (cast to the data type indicated by *outcode*)     from the array-scalar, *scalar*, into the memory pointed to by     *ctypeptr* (which must be large enough to handle the incoming     memory).      Returns -1 on failure, and 0 on success.  .. c:function:: PyObject* PyArray_TypeObjectFromType(int type)      Returns a scalar type-object from a type-number, *type*     . Equivalent to :c`PyArray_DescrFromType` (*type*)->typeobj     except for reference counting and error-checking. Returns a new     reference to the typeobject on success or``NULL`on failure.  .. c:function:: NPY_SCALARKIND PyArray_ScalarKind( \         int typenum, PyArrayObject** arr)      Legacy way to query special promotion for scalar values.  This is not     used in NumPy itself anymore and is expected to be deprecated eventually.      New DTypes can define promotion rules specific to Python scalars.  .. c:function:: int PyArray_CanCoerceScalar( \         char thistype, char neededtype, NPY_SCALARKIND scalar)      Legacy way to query special promotion for scalar values.  This is not     used in NumPy itself anymore and is expected to be deprecated eventually.      Use`PyArray\_ResultType``for similar purposes.   Data-type descriptors ---------------------    > **Warning** >      Data-type objects must be reference counted so be aware of the     action on the data-type reference of different C-API calls. The     standard rule is that when a data-type object is returned it is a     new reference.  Functions that take :c:expr:`PyArray_Descr *` objects and     return arrays steal references to the data-type their inputs     unless otherwise noted. Therefore, you must own a reference to any     data-type object used as input to such a function.  .. c:function:: int PyArray_DescrCheck(PyObject* obj)      Evaluates as true if *obj* is a data-type object ( :c:expr:`PyArray_Descr *` ).  .. c:function:: PyArray_Descr* PyArray_DescrNew(PyArray_Descr* obj)      Return a new data-type object copied from *obj* (the fields     reference is just updated so that the new object points to the     same fields dictionary if any).  .. c:function:: PyArray_Descr* PyArray_DescrNewFromType(int typenum)      Create a new data-type object from the built-in (or     user-registered) data-type indicated by *typenum*. All builtin     types should not have any of their fields changed. This creates a     new copy of the :c:type:`PyArray_Descr` structure so that you can fill     it in as appropriate. This function is especially needed for     flexible data-types which need to have a new elsize member in     order to be meaningful in array construction.  .. c:function:: PyArray_Descr* PyArray_DescrNewByteorder( \         PyArray_Descr* obj, char newendian)      Create a new data-type object with the byteorder set according to     *newendian*. All referenced data-type objects (in subdescr and     fields members of the data-type object) are also changed     (recursively).      The value of *newendian* is one of these macros: ..     dedent the enumeration of flags to avoid missing references sphinx warnings  .. c:macro:: NPY_IGNORE              NPY_SWAP              NPY_NATIVE              NPY_LITTLE              NPY_BIG      If a byteorder of :c`NPY_IGNORE` is encountered it     is left alone. If newendian is :c`NPY_SWAP`, then all byte-orders     are swapped. Other valid newendian values are :c`NPY_NATIVE`,     :c`NPY_LITTLE`, and :c`NPY_BIG` which all cause     the returned data-typed descriptor (and all it's     referenced data-type descriptors) to have the corresponding byte-     order.  .. c:function:: PyArray_Descr* PyArray_DescrFromObject( \         PyObject* op, PyArray_Descr* mintype)      Determine an appropriate data-type object from the object *op*     (which should be a "nested" sequence object) and the minimum     data-type descriptor mintype (which can be``NULL``). Similar in     behavior to array(*op*).dtype. Don't confuse this function with     :c`PyArray_DescrConverter`. This function essentially looks at     all the objects in the (nested) sequence and determines the     data-type from the elements it finds.  .. c:function:: PyArray_Descr* PyArray_DescrFromScalar(PyObject* scalar)      Return a data-type object from an array-scalar object. No checking     is done to be sure that *scalar* is an array scalar. If no     suitable data-type can be determined, then a data-type of     :c`NPY_OBJECT` is returned by default.  .. c:function:: PyArray_Descr* PyArray_DescrFromType(int typenum)      Returns a data-type object corresponding to *typenum*. The     *typenum* can be one of the enumerated types, a character code for     one of the enumerated types, or a user-defined type. If you want to use a     flexible size array, then you need to``flexible typenum`and set the     results`elsize``parameter to the desired size. The typenum is one of the     :c`NPY_TYPES`.  .. c:function:: int PyArray_DescrConverter(PyObject* obj, PyArray_Descr** dtype)      Convert any compatible Python object, *obj*, to a data-type object     in *dtype*. A large number of Python objects can be converted to     data-type objects. See [arrays.dtypes](#arrays.dtypes) for a complete     description. This version of the converter converts None objects     to a :c`NPY_DEFAULT_TYPE` data-type object. This function can     be used with the "O&" character code in :c`PyArg_ParseTuple`     processing.  .. c:function:: int PyArray_DescrConverter2( \         PyObject* obj, PyArray_Descr** dtype)      Convert any compatible Python object, *obj*, to a data-type     object in *dtype*. This version of the converter converts None     objects so that the returned data-type is``NULL``. This function     can also be used with the "O&" character in PyArg_ParseTuple     processing.  .. c:function:: int PyArray_DescrAlignConverter( \         PyObject* obj, PyArray_Descr** dtype)      Like :c`PyArray_DescrConverter` except it aligns C-struct-like     objects on word-boundaries as the compiler would.  .. c:function:: int PyArray_DescrAlignConverter2( \         PyObject* obj, PyArray_Descr** dtype)      Like :c`PyArray_DescrConverter2` except it aligns C-struct-like     objects on word-boundaries as the compiler would.  Data Type Promotion and Inspection ----------------------------------  .. c:function:: PyArray_DTypeMeta *PyArray_CommonDType( \             const PyArray_DTypeMeta *dtype1, const PyArray_DTypeMeta *dtype2)     This function defines the common DType operator. Note that the common DType    will not be``object`(unless one of the DTypes is`object``). Similar to    `numpy.result_type`, but works on the classes and not instances.  .. c:function:: PyArray_DTypeMeta *PyArray_PromoteDTypeSequence( \                     npy_intp length, PyArray_DTypeMeta **dtypes_in)     Promotes a list of DTypes with each other in a way that should guarantee    stable results even when changing the order.  This function is smarter and    can often return successful and unambiguous results when``common\_dtype(common\_dtype(dt1, dt2), dt3)`would depend on the operation    order or fail.  Nevertheless, DTypes should aim to ensure that their    common-dtype implementation is associative and commutative!  (Mainly,    unsigned and signed integers are not.)     For guaranteed consistent results DTypes must implement common-Dtype    "transitively".  If A promotes B and B promotes C, than A must generally    also promote C; where "promotes" means implements the promotion.  (There    are some exceptions for abstract DTypes)     In general this approach always works as long as the most generic dtype    is either strictly larger, or compatible with all other dtypes.    For example promoting`float16`with any other float, integer, or unsigned    integer again gives a floating point number.  .. c:function:: PyArray_Descr *PyArray_GetDefaultDescr(const PyArray_DTypeMeta *DType)     Given a DType class, returns the default instance (descriptor).  This checks    for a`singleton`first and only calls the`default\_descr``function if    necessary.  .. _dtype-api:  Custom Data Types -----------------  .. versionadded:: 2.0  These functions allow defining custom flexible data types outside of NumPy.  See [NEP 42 <NEP42>](#nep-42-<nep42>) for more details about the rationale and design of the new DType system. See the `numpy-user-dtypes repository <https://github.com/numpy/numpy-user-dtypes>`_ for a number of example DTypes. Also see [dtypemeta](#dtypemeta) for documentation on``PyArray\_DTypeMeta`and`PyArrayDTypeMeta\_Spec``.  .. c:function:: int PyArrayInitDTypeMeta_FromSpec( \                 PyArray_DTypeMeta *Dtype, PyArrayDTypeMeta_Spec *spec)   Initialize a new DType.  It must currently be a static Python C type that is  declared as :c:type:`PyArray_DTypeMeta` and not :c:type:`PyTypeObject`.  Further, it must subclass `np.dtype` and set its type to  :c:type:`PyArrayDTypeMeta_Type` (before calling :c`PyType_Ready()`),  which has additional fields compared to a normal :c:type:`PyTypeObject`. See  the examples in the``numpy-user-dtypes`repository for usage with both  parametric and non-parametric data types.  .. _dtype-flags:  Flags ~~~~~  Flags that can be set on the`PyArrayDTypeMeta\_Spec`to initialize the DType.  .. c:macro:: NPY_DT_ABSTRACT     Indicates the DType is an abstract "base" DType in a DType hierarchy and    should not be directly instantiated.  .. c:macro:: NPY_DT_PARAMETRIC     Indicates the DType is parametric and does not have a unique singleton    instance.  .. c:macro:: NPY_DT_NUMERIC     Indicates the DType represents a numerical value.   .. _dtype-slots:  Slot IDs and API Function Typedefs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  These IDs correspond to slots in the DType API and are used to identify implementations of each slot from the items of the`slots`array member of`PyArrayDTypeMeta\_Spec`struct.  .. c:macro:: NPY_DT_discover_descr_from_pyobject  .. c:type:: PyArray_Descr *(PyArrayDTypeMeta_DiscoverDescrFromPyobject)( \                 PyArray_DTypeMeta *cls, PyObject *obj)     Used during DType inference to find the correct DType for a given    PyObject. Must return a descriptor instance appropriate to store the    data in the python object that is passed in. *obj* is the python object    to inspect and *cls* is the DType class to create a descriptor for.  .. c:macro:: NPY_DT_default_descr  .. c:type:: PyArray_Descr *(PyArrayDTypeMeta_DefaultDescriptor)( \                 PyArray_DTypeMeta *cls)     Returns the default descriptor instance for the DType. Must be    defined for parametric data types. Non-parametric data types return    the singleton by default.  .. c:macro:: NPY_DT_common_dtype  .. c:type:: PyArray_DTypeMeta *(PyArrayDTypeMeta_CommonDType)( \                 PyArray_DTypeMeta *dtype1, PyArray_DTypeMeta *dtype2)     Given two input DTypes, determines the appropriate "common" DType    that can store values for both types. Returns`Py\_NotImplemented`if no such type exists.  .. c:macro:: NPY_DT_common_instance  .. c:type:: PyArray_Descr *(PyArrayDTypeMeta_CommonInstance)( \                PyArray_Descr *dtype1, PyArray_Descr *dtype2)     Given two input descriptors, determines the appropriate "common"    descriptor that can store values for both instances. Returns`NULL`on error.  .. c:macro:: NPY_DT_ensure_canonical  .. c:type:: PyArray_Descr *(PyArrayDTypeMeta_EnsureCanonical)( \                 PyArray_Descr *dtype)     Returns the "canonical" representation for a descriptor instance. The    notion of a canonical descriptor generalizes the concept of byte    order, in that a canonical descriptor always has native byte    order. If the descriptor is already canonical, this function returns    a new reference to the input descriptor.  .. c:macro:: NPY_DT_setitem  .. c:type:: int(PyArrayDTypeMeta_SetItem)(PyArray_Descr *, PyObject *, char *)     Implements scalar setitem for an array element given a PyObject.  .. c:macro:: NPY_DT_getitem  .. c:type:: PyObject *(PyArrayDTypeMeta_GetItem)(PyArray_Descr *, char *)     Implements scalar getitem for an array element. Must return a python    scalar.  .. c:macro:: NPY_DT_get_clear_loop     If defined, sets a traversal loop that clears data in the array. This    is most useful for arrays of references that must clean up array    entries before the array is garbage collected. Implements`PyArrayMethod\_GetTraverseLoop``.  .. c:macro:: NPY_DT_get_fill_zero_loop     If defined, sets a traversal loop that fills an array with "zero"    values, which may have a DType-specific meaning. This is called    inside `numpy.zeros` for arrays that need to write a custom sentinel    value that represents zero if for some reason a zero-filled array is    not sufficient. Implements``PyArrayMethod\_GetTraverseLoop`.  .. c:macro:: NPY_DT_finalize_descr  .. c:type:: PyArray_Descr *(PyArrayDTypeMeta_FinalizeDescriptor)( \                 PyArray_Descr *dtype)     If defined, a function that is called to "finalize" a descriptor    instance after an array is created. One use of this function is to    force newly created arrays to have a newly created descriptor    instance, no matter what input descriptor is provided by a user.  PyArray_ArrFuncs slots ^^^^^^^^^^^^^^^^^^^^^^  In addition the above slots, the following slots are exposed to allow filling the [arrfuncs-type](#arrfuncs-type) struct attached to descriptor instances. Note that in the future these will be replaced by proper DType API slots but for now we have exposed the legacy`PyArray\_ArrFuncs`slots.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_getitem     Allows setting a per-dtype getitem. Note that this is not necessary    to define unless the default version calling the function defined    with the`NPY\_DT\_getitem`ID is unsuitable. This version will be slightly    faster than using`NPY\_DT\_getitem`at the cost of sometimes needing to deal    with a NULL input array.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_setitem     Allows setting a per-dtype setitem. Note that this is not necessary    to define unless the default version calling the function defined    with the`NPY\_DT\_setitem``ID is unsuitable for some reason.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_compare     Computes a comparison for `numpy.sort`, implements``PyArray\_CompareFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_argmax     Computes the argmax for `numpy.argmax`, implements``PyArray\_ArgFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_argmin     Computes the argmin for `numpy.argmin`, implements``PyArray\_ArgFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_dotfunc     Computes the dot product for `numpy.dot`, implements``PyArray\_DotFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_scanfunc     A formatted input function for `numpy.fromfile`, implements``PyArray\_ScanFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_fromstr     A string parsing function for `numpy.fromstring`, implements``PyArray\_FromStrFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_nonzero     Computes the nonzero function for `numpy.nonzero`, implements``PyArray\_NonzeroFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_fill     An array filling function for `numpy.ndarray.fill`, implements``PyArray\_FillFunc``.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_fillwithscalar     A function to fill an array with a scalar value for `numpy.ndarray.fill`,    implements``PyArray\_FillWithScalarFunc`.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_sort     An array of PyArray_SortFunc of length`NPY\_NSORTS`. If set, allows    defining custom sorting implementations for each of the sorting    algorithms numpy implements.  .. c:macro:: NPY_DT_PyArray_ArrFuncs_argsort     An array of PyArray_ArgSortFunc of length`NPY\_NSORTS`. If set,    allows defining custom argsorting implementations for each of the    sorting algorithms numpy implements.  Macros and Static Inline Functions ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  These macros and static inline functions are provided to allow more understandable and idiomatic code when working with`PyArray\_DTypeMeta`instances.  .. c:macro:: NPY_DTYPE(descr)     Returns a`PyArray\_DTypeMeta \*`pointer to the DType of a given    descriptor instance.  .. c:function:: static inline PyArray_DTypeMeta \                 *NPY_DT_NewRef(PyArray_DTypeMeta *o)     Returns a`PyArray\_DTypeMeta \*``pointer to a new reference to a    DType.  Conversion utilities --------------------  For use with :c`PyArg_ParseTuple` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  All of these functions can be used in :c`PyArg_ParseTuple` (...) with the "O&" format specifier to automatically convert any Python object to the required C-object. All of these functions return :c`NPY_SUCCEED` if successful and :c`NPY_FAIL` if not. The first argument to all of these function is a Python object. The second argument is the **address** of the C-type to convert the Python object to.   > **Warning** >      Be sure to understand what steps you should take to manage the     memory when using these conversion functions. These functions can     require freeing memory, and/or altering the reference counts of     specific objects based on your use.  .. c:function:: int PyArray_Converter(PyObject* obj, PyObject** address)      Convert any Python object to a :c:type:`PyArrayObject`. If     :c`PyArray_Check` (*obj*) is TRUE then its reference count is     incremented and a reference placed in *address*. If *obj* is not     an array, then convert it to an array using :c`PyArray_FromAny`     . No matter what is returned, you must DECREF the object returned     by this routine in *address* when you are done with it.  .. c:function:: int PyArray_OutputConverter( \         PyObject* obj, PyArrayObject** address)      This is a default converter for output arrays given to     functions. If *obj* is :c`Py_None` or``NULL`, then *\*address*     will be`NULL``but the call will succeed. If :c`PyArray_Check` (     *obj*) is TRUE then it is returned in *\*address* without     incrementing its reference count.  .. c:function:: int PyArray_IntpConverter(PyObject* obj, PyArray_Dims* seq)      Convert any Python sequence, *obj*, smaller than :c`NPY_MAXDIMS`     to a C-array of :c:type:`npy_intp`. The Python object could also be a     single number. The *seq* variable is a pointer to a structure with     members ptr and len. On successful return, *seq* ->ptr contains a     pointer to memory that must be freed, by calling :c`PyDimMem_FREE`,     to avoid a memory leak. The restriction on memory size allows this     converter to be conveniently used for sequences intended to be     interpreted as array shapes.  .. c:function:: int PyArray_BufferConverter(PyObject* obj, PyArray_Chunk* buf)      Convert any Python object, *obj*, with a (single-segment) buffer     interface to a variable with members that detail the object's use     of its chunk of memory. The *buf* variable is a pointer to a     structure with base, ptr, len, and flags members. The     :c:type:`PyArray_Chunk` structure is binary compatible with the     Python's buffer object (through its len member on 32-bit platforms     and its ptr member on 64-bit platforms). On return, the base member     is set to *obj* (or its base if *obj* is already a buffer object     pointing to another object). If you need to hold on to the memory     be sure to INCREF the base member. The chunk of memory is pointed     to by *buf* ->ptr member and has length *buf* ->len. The flags     member of *buf* is :c`NPY_ARRAY_ALIGNED` with the     :c`NPY_ARRAY_WRITEABLE` flag set if *obj* has a writeable     buffer interface.  .. c:function:: int PyArray_AxisConverter(PyObject* obj, int* axis)      Convert a Python object, *obj*, representing an axis argument to     the proper value for passing to the functions that take an integer     axis. Specifically, if *obj* is None, *axis* is set to     :c`NPY_RAVEL_AXIS` which is interpreted correctly by the C-API     functions that take axis arguments.  .. c:function:: int PyArray_BoolConverter(PyObject* obj, npy_bool* value)      Convert any Python object, *obj*, to :c`NPY_TRUE` or     :c`NPY_FALSE`, and place the result in *value*.  .. c:function:: int PyArray_ByteorderConverter(PyObject* obj, char* endian)      Convert Python strings into the corresponding byte-order     character:     '>', '<', 's', '=', or '\|'.  .. c:function:: int PyArray_SortkindConverter(PyObject* obj, NPY_SORTKIND* sort)      Convert Python strings into one of :c`NPY_QUICKSORT` (starts     with 'q' or 'Q'), :c`NPY_HEAPSORT` (starts with 'h' or 'H'),     :c`NPY_MERGESORT` (starts with 'm' or 'M') or :c`NPY_STABLESORT`     (starts with 't' or 'T'). :c`NPY_MERGESORT` and :c`NPY_STABLESORT`     are aliased to each other for backwards compatibility and may refer to one     of several stable sorting algorithms depending on the data type.  .. c:function:: int PyArray_SearchsideConverter( \         PyObject* obj, NPY_SEARCHSIDE* side)      Convert Python strings into one of :c`NPY_SEARCHLEFT` (starts with 'l'     or 'L'), or :c`NPY_SEARCHRIGHT` (starts with 'r' or 'R').  .. c:function:: int PyArray_OrderConverter(PyObject* obj, NPY_ORDER* order)     Convert the Python strings 'C', 'F', 'A', and 'K' into the :c:type:`NPY_ORDER`    enumeration :c`NPY_CORDER`, :c`NPY_FORTRANORDER`,    :c`NPY_ANYORDER`, and :c`NPY_KEEPORDER`.  .. c:function:: int PyArray_CastingConverter( \         PyObject* obj, NPY_CASTING* casting)     Convert the Python strings 'no', 'equiv', 'safe', 'same_kind', and    'unsafe' into the :c:type:`NPY_CASTING` enumeration :c`NPY_NO_CASTING`,    :c`NPY_EQUIV_CASTING`, :c`NPY_SAFE_CASTING`,    :c`NPY_SAME_KIND_CASTING`, and :c`NPY_UNSAFE_CASTING`.  .. c:function:: int PyArray_ClipmodeConverter( \         PyObject* object, NPY_CLIPMODE* val)      Convert the Python strings 'clip', 'wrap', and 'raise' into the     :c:type:`NPY_CLIPMODE` enumeration :c`NPY_CLIP`, :c`NPY_WRAP`,     and :c`NPY_RAISE`.  .. c:function:: int PyArray_ConvertClipmodeSequence( \         PyObject* object, NPY_CLIPMODE* modes, int n)     Converts either a sequence of clipmodes or a single clipmode into    a C array of :c:type:`NPY_CLIPMODE` values. The number of clipmodes *n*    must be known before calling this function. This function is provided    to help functions allow a different clipmode for each dimension.  Other conversions ~~~~~~~~~~~~~~~~~  .. c:function:: int PyArray_PyIntAsInt(PyObject* op)      Convert all kinds of Python objects (including arrays and array     scalars) to a standard integer. On error, -1 is returned and an     exception set. You may find useful the macro:``\`c \#define error\_converting(x) (((x) == -1) && PyErr\_Occurred())

<div id="including-the-c-api">

Including and importing the C API `` ` ---------------------------------  To use the NumPy C-API you typically need to include the ``numpy/ndarrayobject.h`header and`numpy/ufuncobject.h`for some ufunc related functionality (`arrayobject.h`is an alias for`ndarrayobject.h`).  These two headers export most relevant functionality.  In general any project which uses the NumPy API must import NumPy using one of the functions`PyArray\_ImportNumPyAPI()`or`import\_array()`. In some places, functionality which requires`import\_array()`is not needed, because you only need type definitions.  In this case, it is sufficient to include`numpy/ndarratypes.h`.  For the typical Python project, multiple C or C++ files will be compiled into a single shared object (the Python C-module) and`PyArray\_ImportNumPyAPI()`should be called inside it's module initialization.  When you have a single C-file, this will consist of:`\`c \#include "numpy/ndarrayobject.h"

</div>

> PyMODINIT\_FUNC PyInit\_my\_module(void) { if (PyArray\_ImportNumPyAPI() \< 0) { return NULL; } /\* Other initialization code. \*/ }

However, most projects will have additional C files which are all `` ` linked together into a single Python module. In this case, the helper C files typically do not have a canonical place where ``PyArray\_ImportNumPyAPI`should be called (although it is OK and fast to call it often).  To solve this, NumPy provides the following pattern that the the main file is modified to define`PY\_ARRAY\_UNIQUE\_SYMBOL`before the include:`\`c /\* Main module file \*/ \#define PY\_ARRAY\_UNIQUE\_SYMBOL MyModule \#include "numpy/ndarrayobject.h"

> PyMODINIT\_FUNC PyInit\_my\_module(void) { if (PyArray\_ImportNumPyAPI() \< 0) { return NULL; } /\* Other initialization code. \*/ }

while the other files use:

``` C
/* Second file without any import */
#define NO_IMPORT_ARRAY
#define PY_ARRAY_UNIQUE_SYMBOL MyModule
#include "numpy/ndarrayobject.h"
```

You can of course add the defines to a local header used throughout. `` ` You just have to make sure that the main file does _not_ define ``NO\_IMPORT\_ARRAY`.  For`numpy/ufuncobject.h`the same logic applies, but the unique symbol mechanism is`\#define PY\_UFUNC\_UNIQUE\_SYMBOL`(both can match).  Additionally, you will probably wish to add a`\#define NPY\_NO\_DEPRECATED\_API NPY\_1\_7\_API\_VERSION`to avoid warnings about possible use of old API.  .. note::     If you are experiencing access violations make sure that the NumPy API     was properly imported and the symbol`PyArray\_API`is not`NULL`.     When in a debugger, this symbols actual name will be`PY\_ARRAY\_UNIQUE\_SYMBOL`+`PyArray\_API`, so for example`MyModulePyArray\_API`in the above.     (E.g. even a`printf("%pn", PyArray\_API);`just before the crash.)   Mechanism details and dynamic linking ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The main part of the mechanism is that without NumPy needs to define a`void \*\*PyArray\_API``table for you to look up all functions. Depending on your macro setup, this takes different routes depending on whether :c:macro:`NO_IMPORT_ARRAY` and  :c:macro:`PY_ARRAY_UNIQUE_SYMBOL` are defined:  * If neither is defined, the C-API is declared to``static void **PyArray\_API\`\`, so it is only visible within the compilation unit/file using \`\`\#include \<numpy/arrayobject.h\>\`\`. \* If only \`\`PY\_ARRAY\_UNIQUE\_SYMBOL\`\` is defined (it could be empty) then the it is declared to a non-static \`\`void**`allowing it to be used   by other files which are linked. * If`NO\_IMPORT\_ARRAY`is defined, the table is declared as`extern void \*\*`, meaning that it must be linked to a file which does not   use`NO\_IMPORT\_ARRAY`.  The`PY\_ARRAY\_UNIQUE\_SYMBOL``mechanism additionally mangles the names to avoid conflicts.  .. versionchanged::     NumPy 2.1 changed the headers to avoid sharing the table outside of a     single shared object/dll (this was always the case on Windows).     Please see :c:macro:`NPY_API_SYMBOL_ATTRIBUTE` for details.  In order to make use of the C-API from another extension module, the :c`import_array` function must be called. If the extension module is self-contained in a single .c file, then that is all that needs to be done. If, however, the extension module involves multiple files where the C-API is needed then some additional steps must be taken.  .. c:function:: int PyArray_ImportNumPyAPI(void)      Ensures that the NumPy C-API is imported and usable.  It returns``0`on success and`-1`with an error set if NumPy couldn't be imported.     While preferable to call it once at module initialization, this function     is very light-weight if called multiple times.      .. versionadded:: 2.0         This function is backported in the`npy\_2\_compat.h`header.  .. c:macro:: import_array(void)      This function must be called in the initialization section of a     module that will make use of the C-API. It imports the module     where the function-pointer table is stored and points the correct     variable to it.     This macro includes a`return NULL;`on error, so that`PyArray\_ImportNumPyAPI()`is preferable for custom error checking.     You may also see use of`\_import\_array()`(a function, not     a macro, but you may want to raise a better error if it fails) and     the variations`import\_array1(ret)`which customizes the return value.  .. c:macro:: PY_ARRAY_UNIQUE_SYMBOL  .. c:macro:: NPY_API_SYMBOL_ATTRIBUTE      .. versionadded:: 2.1      An additional symbol which can be used to share e.g. visibility beyond     shared object boundaries.     By default, NumPy adds the C visibility hidden attribute (if available):`void \_\_attribute\_\_((visibility("hidden"))) **PyArray\_API;\`\`. You can change this by defining \`\`NPY\_API\_SYMBOL\_ATTRIBUTE\`\`, which will make this: \`\`void NPY\_API\_SYMBOL\_ATTRIBUTE**PyArray\_API;`(with additional     name mangling via the unique symbol).      Adding an empty`\#define NPY\_API\_SYMBOL\_ATTRIBUTE`will have the same     behavior as NumPy 1.x.      .. note::         Windows never had shared visibility although you can use this macro         to achieve it.  We generally discourage sharing beyond shared boundary         lines since importing the array API includes NumPy version checks.  .. c:macro:: NO_IMPORT_ARRAY      Defining`NO\_IMPORT\_ARRAY`before the`ndarrayobject.h`include     indicates that the NumPy C API import is handled in a different file     and the include mechanism will not be added here.     You must have one file without`NO\_IMPORT\_ARRAY`defined.`\`c \#define PY\_ARRAY\_UNIQUE\_SYMBOL cool\_ARRAY\_API \#include \<numpy/arrayobject.h\>

> On the other hand, coolhelper.c would contain at the top:
> 
> ``` c
> #define NO_IMPORT_ARRAY
> #define PY_ARRAY_UNIQUE_SYMBOL cool_ARRAY_API
> #include <numpy/arrayobject.h>
> ```
> 
> You can also put the common two last lines into an extension-local header file as long as you make sure that NO\_IMPORT\_ARRAY is \#defined before \#including that file.
> 
> Internally, these \#defines work as follows:
> 
>   - If neither is defined, the C-API is declared to be `static void**`, so it is only visible within the compilation unit that \#includes numpy/arrayobject.h.
>   - If :c`PY_ARRAY_UNIQUE_SYMBOL` is \#defined, but :c`NO_IMPORT_ARRAY` is not, the C-API is declared to be `void**`, so that it will also be visible to other compilation units.
>   - If :c`NO_IMPORT_ARRAY` is \#defined, regardless of whether :c`PY_ARRAY_UNIQUE_SYMBOL` is, the C-API is declared to be `extern void**`, so it is expected to be defined in another compilation unit.
>   - Whenever :c`PY_ARRAY_UNIQUE_SYMBOL` is \#defined, it also changes the name of the variable holding the C-API, which defaults to `PyArray_API`, to whatever the macro is \#defined to.

Checking the API Version `` ` ~~~~~~~~~~~~~~~~~~~~~~~~  Because python extensions are not used in the same way as usual libraries on most platforms, some errors cannot be automatically detected at build time or even runtime. For example, if you build an extension using a function available only for numpy >= 1.3.0, and you import the extension later with numpy 1.2, you will not get an import error (but almost certainly a segmentation fault when calling the function). That's why several functions are provided to check for numpy versions. The macros :c`NPY_VERSION`  and :c`NPY_FEATURE_VERSION` corresponds to the numpy version used to build the extension, whereas the versions returned by the functions :c`PyArray_GetNDArrayCVersion` and :c`PyArray_GetNDArrayCFeatureVersion` corresponds to the runtime numpy's version.  The rules for ABI and API compatibilities can be summarized as follows:  * Whenever :c`NPY_VERSION` != ``PyArray\_GetNDArrayCVersion()``, the   extension has to be recompiled (ABI incompatibility). * :c`NPY_VERSION` ==``PyArray\_GetNDArrayCVersion()``and   :c`NPY_FEATURE_VERSION` <=``PyArray\_GetNDArrayCFeatureVersion()``means   backward compatible changes.  ABI incompatibility is automatically detected in every numpy's version. API incompatibility detection was added in numpy 1.4.0. If you want to supported many different numpy versions with one extension binary, you have to build your extension with the lowest :c`NPY_FEATURE_VERSION` as possible.  .. c:macro:: NPY_VERSION      The current version of the ndarray object (check to see if this     variable is defined to guarantee the``numpy/arrayobject.h``header is     being used).  .. c:macro:: NPY_FEATURE_VERSION      The current version of the C-API.  .. c:function:: unsigned int PyArray_GetNDArrayCVersion(void)      This just returns the value :c`NPY_VERSION`. :c`NPY_VERSION`     changes whenever a backward incompatible change at the ABI level. Because     it is in the C-API, however, comparing the output of this function from the     value defined in the current header gives a way to test if the C-API has     changed thus requiring a re-compilation of extension modules that use the     C-API. This is automatically checked in the function :c`import_array`.  .. c:function:: unsigned int PyArray_GetNDArrayCFeatureVersion(void)      This just returns the value :c`NPY_FEATURE_VERSION`.     :c`NPY_FEATURE_VERSION` changes whenever the API changes (e.g. a     function is added). A changed value does not always require a recompile.   Memory management ~~~~~~~~~~~~~~~~~  .. c:function:: char* PyDataMem_NEW(size_t nbytes)  .. c:function:: void PyDataMem_FREE(char* ptr)  .. c:function:: char* PyDataMem_RENEW(void * ptr, size_t newbytes)      Functions to allocate, free, and reallocate memory. These are used     internally to manage array data memory unless overridden.  .. c:function:: npy_intp*  PyDimMem_NEW(int nd)  .. c:function:: void PyDimMem_FREE(char* ptr)  .. c:function:: npy_intp* PyDimMem_RENEW(void* ptr, size_t newnd)      Macros to allocate, free, and reallocate dimension and strides memory.  .. c:function:: void* PyArray_malloc(size_t nbytes)  .. c:function:: void PyArray_free(void* ptr)  .. c:function:: void* PyArray_realloc(npy_intp* ptr, size_t nbytes)      These macros use different memory allocators, depending on the     constant :c`NPY_USE_PYMEM`. The system malloc is used when     :c`NPY_USE_PYMEM` is 0, if :c`NPY_USE_PYMEM` is 1, then     the Python memory allocator is used.      .. c:macro:: NPY_USE_PYMEM  .. c:function:: int PyArray_ResolveWritebackIfCopy(PyArrayObject* obj)      If``obj-\>flags``has :c`NPY_ARRAY_WRITEBACKIFCOPY`, this function     clears the flags, `DECREF` s     `obj->base` and makes it writeable, and sets``obj-\>base`to NULL. It then     copies`obj-\>data``to `obj->base->data`, and returns the error state of     the copy operation. This is the opposite of     :c`PyArray_SetWritebackIfCopyBase`. Usually this is called once     you are finished with``obj`, just before`Py\_DECREF(obj)`. It may be called     multiple times, or with`NULL``input. See also     :c`PyArray_DiscardWritebackIfCopy`.      Returns 0 if nothing was done, -1 on error, and 1 if action was taken.  Threading support ~~~~~~~~~~~~~~~~~  These macros are only meaningful if :c`NPY_ALLOW_THREADS` evaluates True during compilation of the extension module. Otherwise, these macros are equivalent to whitespace. Python uses a single Global Interpreter Lock (GIL) for each Python process so that only a single thread may execute at a time (even on multi-cpu machines). When calling out to a compiled function that may take time to compute (and does not have side-effects for other threads like updated global variables), the GIL should be released so that other Python threads can run while the time-consuming calculations are performed. This can be accomplished using two groups of macros. Typically, if one macro in a group is used in a code block, all of them must be used in the same code block. :c`NPY_ALLOW_THREADS` is true (defined as``1`) unless the build option`-Ddisable-threading`is set to`true``- in which case :c`NPY_ALLOW_THREADS` is false (``0``).  .. c:macro:: NPY_ALLOW_THREADS  Group 1 ^^^^^^^  This group is used to call code that may take some time but does not use any Python C-API calls. Thus, the GIL should be released during its calculation.  .. c:macro:: NPY_BEGIN_ALLOW_THREADS      Equivalent to :c:macro:`Py_BEGIN_ALLOW_THREADS` except it uses     :c`NPY_ALLOW_THREADS` to determine if the macro if     replaced with white-space or not.  .. c:macro:: NPY_END_ALLOW_THREADS      Equivalent to :c:macro:`Py_END_ALLOW_THREADS` except it uses     :c`NPY_ALLOW_THREADS` to determine if the macro if     replaced with white-space or not.  .. c:macro:: NPY_BEGIN_THREADS_DEF      Place in the variable declaration area. This macro sets up the     variable needed for storing the Python state.  .. c:macro:: NPY_BEGIN_THREADS      Place right before code that does not need the Python     interpreter (no Python C-API calls). This macro saves the     Python state and releases the GIL.  .. c:macro:: NPY_END_THREADS      Place right after code that does not need the Python     interpreter. This macro acquires the GIL and restores the     Python state from the saved variable.  .. c:function:: void NPY_BEGIN_THREADS_DESCR(PyArray_Descr *dtype)      Useful to release the GIL only if *dtype* does not contain     arbitrary Python objects which may need the Python interpreter     during execution of the loop.  .. c:function:: void NPY_END_THREADS_DESCR(PyArray_Descr *dtype)      Useful to regain the GIL in situations where it was released     using the BEGIN form of this macro.  .. c:function:: void NPY_BEGIN_THREADS_THRESHOLDED(int loop_size)      Useful to release the GIL only if *loop_size* exceeds a     minimum threshold, currently set to 500. Should be matched     with a :c:macro:`NPY_END_THREADS` to regain the GIL.  Group 2 ^^^^^^^  This group is used to re-acquire the Python GIL after it has been released. For example, suppose the GIL has been released (using the previous calls), and then some path in the code (perhaps in a different subroutine) requires use of the Python C-API, then these macros are useful to acquire the GIL. These macros accomplish essentially a reverse of the previous three (acquire the LOCK saving what state it had) and then re-release it with the saved state.  .. c:macro:: NPY_ALLOW_C_API_DEF      Place in the variable declaration area to set up the necessary     variable.  .. c:macro:: NPY_ALLOW_C_API      Place before code that needs to call the Python C-API (when it is     known that the GIL has already been released).  .. c:macro:: NPY_DISABLE_C_API      Place after code that needs to call the Python C-API (to re-release     the GIL).  > **Tip** >      Never use semicolons after the threading support macros.   Priority ~~~~~~~~  .. c:macro:: NPY_PRIORITY      Default priority for arrays.  .. c:macro:: NPY_SUBTYPE_PRIORITY      Default subtype priority.  .. c:macro:: NPY_SCALAR_PRIORITY      Default scalar priority (very small)  .. c:function:: double PyArray_GetPriority(PyObject* obj, double def)      Return the :obj:`~numpy.class.__array_priority__` attribute (converted to a     double) of *obj* or *def* if no attribute of that name     exists. Fast returns that avoid the attribute lookup are provided     for objects of type :c`PyArray_Type`.   Default buffers ~~~~~~~~~~~~~~~  .. c:macro:: NPY_BUFSIZE      Default size of the user-settable internal buffers.  .. c:macro:: NPY_MIN_BUFSIZE      Smallest size of user-settable internal buffers.  .. c:macro:: NPY_MAX_BUFSIZE      Largest size allowed for the user-settable buffers.   Other constants ~~~~~~~~~~~~~~~  .. c:macro:: NPY_NUM_FLOATTYPE      The number of floating-point types  .. c:macro:: NPY_MAXDIMS      The maximum number of dimensions that may be used by NumPy.     This is set to 64 and was 32 before NumPy 2.      .. note::         We encourage you to avoid``NPY\_MAXDIMS`.  A future version of NumPy         may wish to remove any dimension limitation (and thus the constant).         The limitation was created so that NumPy can use stack allocations         internally for scratch space.          If your algorithm has a reasonable maximum number of dimension you         could check and use that locally.  .. c:macro:: NPY_MAXARGS      The maximum number of array arguments that can be used in some     functions.  This used to be 32 before NumPy 2 and is now 64.     To continue to allow using it as a check whether a number of arguments     is compatible ufuncs, this macro is now runtime dependent.      .. note::         We discourage any use of`NPY\_MAXARGS``that isn't explicitly tied         to checking for known NumPy limitations.  .. c:macro:: NPY_FALSE      Defined as 0 for use with Bool.  .. c:macro:: NPY_TRUE      Defined as 1 for use with Bool.  .. c:macro:: NPY_FAIL      The return value of failed converter functions which are called using     the "O&" syntax in :c`PyArg_ParseTuple`-like functions.  .. c:macro:: NPY_SUCCEED      The return value of successful converter functions which are called     using the "O&" syntax in :c`PyArg_ParseTuple`-like functions.  .. c:macro:: NPY_RAVEL_AXIS      Some NumPy functions (mainly the C-entrypoints for Python functions)     have an``axis`argument.  This macro may be passed for`axis=None`.      .. note::         This macro is NumPy version dependent at runtime. The value is now         the minimum integer. However, on NumPy 1.x`NPY\_MAXDIMS`was used         (at the time set to 32).   Miscellaneous Macros ~~~~~~~~~~~~~~~~~~~~  .. c:function:: int PyArray_SAMESHAPE(PyArrayObject *a1, PyArrayObject *a2)      Evaluates as True if arrays *a1* and *a2* have the same shape.  .. c:macro:: PyArray_MAX(a,b)      Returns the maximum of *a* and *b*. If (*a*) or (*b*) are     expressions they are evaluated twice.  .. c:macro:: PyArray_MIN(a,b)      Returns the minimum of *a* and *b*. If (*a*) or (*b*) are     expressions they are evaluated twice.  .. c:function:: void PyArray_DiscardWritebackIfCopy(PyArrayObject* obj)      If`obj-\>flags``has :c`NPY_ARRAY_WRITEBACKIFCOPY`, this function     clears the flags, `DECREF` s     `obj->base` and makes it writeable, and sets``obj-\>base``to NULL. In     contrast to :c`PyArray_ResolveWritebackIfCopy` it makes no attempt     to copy the data from `obj->base`. This undoes     :c`PyArray_SetWritebackIfCopyBase`. Usually this is called after an     error when you are finished with``obj`, just before`Py\_DECREF(obj)`.     It may be called multiple times, or with`NULL\`\` input.

### Enumerated Types

<div class="index">

pair: ndarray; C-API

</div>

---

config.md

---

# System configuration

<div class="sectionauthor">

Travis E. Oliphant

</div>

When NumPy is built, information about system configuration is recorded, and is made available for extension modules using NumPy's C API. These are mostly defined in `numpyconfig.h` (included in `ndarrayobject.h`). The public symbols are prefixed by `NPY_*`. NumPy also offers some functions for querying information about the platform in use.

For private use, NumPy also constructs a `config.h` in the NumPy include directory, which is not exported by NumPy (that is a python extension which use the numpy C API will not see those symbols), to avoid namespace pollution.

## Data type sizes

The `NPY_SIZEOF_{CTYPE}` constants are defined so that sizeof information is available to the pre-processor.

## Platform information

## Compiler directives

---

coremath.md

---

# NumPy core math library

The numpy core math library (`npymath`) is a first step in this direction. This library contains most math-related C99 functionality, which can be used on platforms where C99 is not well supported. The core math functions have the same API as the C99 ones, except for the `npy_*` prefix.

The available functions are defined in `<numpy/npy_math.h>` - please refer to this header when in doubt.

\> **Note** \> An effort is underway to make `npymath` smaller (since C99 compatibility of compilers has improved over time) and more easily vendorable or usable as a header-only dependency. That will avoid problems with shipping a static library built with a compiler which may not match the compiler used by a downstream package or end user. See [gh-20880](https://github.com/numpy/numpy/issues/20880) for details.

## Floating point classification

## Useful math constants

The following math constants are available in `npy_math.h`. Single and extended precision are also available by adding the `f` and `l` suffixes respectively.

## Low-level floating point manipulation

Those can be useful for precise floating point comparison.

## Support for complex numbers

C99-like complex functions have been added. Those can be used if you wish to implement portable C extensions. Since NumPy 2.0 we use C99 complex types as the underlying type:

`` `c     typedef double _Complex npy_cdouble;     typedef float _Complex npy_cfloat;     typedef long double _Complex npy_clongdouble;  MSVC does not support the ``\_Complex`type itself, but has added support for`<span class="title-ref"> the C99 </span><span class="title-ref">complex.h</span>\` header by providing its own implementation. Thus, under MSVC, the equivalent MSVC types will be used:

`` `c     typedef _Dcomplex npy_cdouble;     typedef _Fcomplex npy_cfloat;     typedef _Lcomplex npy_clongdouble;  Because MSVC still does not support C99 syntax for initializing a complex ``\` number, you need to restrict to C90-compatible syntax, e.g.:

`` `c         /* a = 1 + 2i \*/         npy_complex a = npy_cpack(1, 2);         npy_complex b;          b = npy_log(a);  A few utilities have also been added in ``<span class="title-ref"> </span><span class="title-ref">numpy/npy\_math.h</span>\`, in order to retrieve or set the real or the imaginary part of a complex number:

`` `c     npy_cdouble c;     npy_csetreal(&c, 1.0);     npy_csetimag(&c, 0.0);     printf("%d + %di\n", npy_creal(c), npy_cimag(c));  .. versionchanged:: 2.0.0      The underlying C types for all of numpy's complex types have been changed to     use C99 complex types. Up until now the following was being used to represent     complex types:      .. code-block:: c          typedef struct { double real, imag; } npy_cdouble;         typedef struct { float real, imag; } npy_cfloat;         typedef struct {npy_longdouble real, imag;} npy_clongdouble;      Using the ``struct`representation ensured that complex numbers could be used     on all platforms, even the ones without support for built-in complex types. It     also meant that a static library had to be shipped together with NumPy to     provide a C99 compatibility layer for downstream packages to use. In recent     years however, support for native complex types has been improved immensely,     with MSVC adding built-in support for the`complex.h`header in 2019.      To ease cross-version compatibility, macros that use the new set APIs have     been added.      .. code-block:: c          #define NPY_CSETREAL(z, r) npy_csetreal(z, r)         #define NPY_CSETIMAG(z, i) npy_csetimag(z, i)      A compatibility layer is also provided in`numpy/npy\_2\_complexcompat.h`. It     checks whether the macros exist, and falls back to the 1.x syntax in case they     don't.      .. code-block:: c          #include <numpy/npy_math.h>          #ifndef NPY_CSETREALF         #define NPY_CSETREALF(c, r) (c)->real = (r)         #endif         #ifndef NPY_CSETIMAGF         #define NPY_CSETIMAGF(c, i) (c)->imag = (i)         #endif      We suggest all downstream packages that need this functionality to copy-paste     the compatibility layer code into their own sources and use that, so that     they can continue to support both NumPy 1.x and 2.x without issues. Note also     that the`complex.h`header is included in`numpy/npy\_common.h`, which     makes`complex`a reserved keyword.  .. _linking-npymath:  Linking against the core math library in an extension`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

To use the core math library that NumPy ships as a static library in your own Python extension, you need to add the `npymath` compile and link options to your extension. The exact steps to take will depend on the build system you are using. The generic steps to take are:

1.  Add the numpy include directory (= the value of `np.get_include()`) to your include directories,
2.  The `npymath` static library resides in the `lib` directory right next to numpy's include directory (i.e., `pathlib.Path(np.get_include()) / '..' / 'lib'`). Add that to your library search directories,
3.  Link with `libnpymath` and `libm`.

\> **Note** \> Keep in mind that when you are cross compiling, you must use the `numpy` for the platform you are building for, not the native one for the build machine. Otherwise you pick up a static library built for the wrong architecture.

When you build with `numpy.distutils` (deprecated), then use this in your `setup.py`:

> \>\>\> from numpy.distutils.misc\_util import get\_info \>\>\> info = get\_info('npymath') \>\>\> \_ = config.add\_extension('foo', sources=\['foo.c'\], extra\_info=info)

In other words, the usage of `info` is exactly the same as when using `blas_info` and co.

When you are building with [Meson](https://mesonbuild.com), use:

    # Note that this will get easier in the future, when Meson has
    # support for numpy built in; most of this can then be replaced
    # by `dependency('numpy')`.
    incdir_numpy = run_command(py3,
      [
        '-c',
        'import os; os.chdir(".."); import numpy; print(numpy.get_include())'
      ],
      check: true
    ).stdout().strip()
    
    inc_np = include_directories(incdir_numpy)
    
    cc = meson.get_compiler('c')
    npymath_path = incdir_numpy / '..' / 'lib'
    npymath_lib = cc.find_library('npymath', dirs: npymath_path)
    
    py3.extension_module('module_name',
      ...
      include_directories: inc_np,
      dependencies: [npymath_lib],

## Half-precision functions

The header file `<numpy/halffloat.h>` provides functions to work with IEEE 754-2008 16-bit floating point values. While this format is not typically used for numerical computations, it is useful for storing values which require floating point but do not need much precision. It can also be used as an educational tool to understand the nature of floating point round-off error.

Like for other types, NumPy includes a typedef npy\_half for the 16 bit float. Unlike for most of the other types, you cannot use this as a normal type in C, since it is a typedef for npy\_uint16. For example, 1.0 looks like 0x3c00 to C, and if you do an equality comparison between the different signed zeros, you will get -0.0 \!= 0.0 (0x8000 \!= 0x0000), which is incorrect.

For these reasons, NumPy provides an API to work with npy\_half values accessible by including `<numpy/halffloat.h>` and linking to `npymath`. For functions that are not provided directly, such as the arithmetic operations, the preferred method is to convert to float or double and back again, as in the following example.

`` `c         npy_half sum(int n, npy_half *array) {             float ret = 0;             while(n--) {                 ret += npy_half_to_float(*array++);             }             return npy_float_to_half(ret);         }  External Links:  * `754-2008 IEEE Standard for Floating-Point Arithmetic`__ ``<span class="title-ref"> \* \`Half-precision Float Wikipedia Article</span>\_\_. \* [OpenGL Half Float Pixel Support](https://ieeexplore.ieee.org/document/4610935/) \* [The OpenEXR image format](https://en.wikipedia.org/wiki/Half-precision_floating-point_format).

---

data_memory.md

---

# Memory management in NumPy

The <span class="title-ref">numpy.ndarray</span> is a python class. It requires additional memory allocations to hold <span class="title-ref">numpy.ndarray.strides</span>, <span class="title-ref">numpy.ndarray.shape</span> and <span class="title-ref">numpy.ndarray.data</span> attributes. These attributes are specially allocated after creating the python object in <span class="title-ref">\~object.\_\_new\_\_</span>. The `strides` and `shape` are stored in a piece of memory allocated internally.

The `data` allocation used to store the actual array values (which could be pointers in the case of `object` arrays) can be very large, so NumPy has provided interfaces to manage its allocation and release. This document details how those interfaces work.

## Historical overview

Since version 1.7.0, NumPy has exposed a set of `PyDataMem_*` functions (:c\`PyDataMem\_NEW\`, :c\`PyDataMem\_FREE\`, :c\`PyDataMem\_RENEW\`) which are backed by <span class="title-ref">alloc</span>, <span class="title-ref">free</span>, <span class="title-ref">realloc</span> respectively.

Since those early days, Python also improved its memory management capabilities, and began providing various \[management policies \<memoryoverview\>\](\#management-policies-\<memoryoverview\>) beginning in version 3.4. These routines are divided into a set of domains, each domain has a :c`PyMemAllocatorEx` structure of routines for memory management. Python also added a <span class="title-ref">tracemalloc</span> module to trace calls to the various routines. These tracking hooks were added to the NumPy `PyDataMem_*` routines.

NumPy added a small cache of allocated memory in its internal `npy_alloc_cache`, `npy_alloc_cache_zero`, and `npy_free_cache` functions. These wrap `alloc`, `alloc-and-memset(0)` and `free` respectively, but when `npy_free_cache` is called, it adds the pointer to a short list of available blocks marked by size. These blocks can be re-used by subsequent calls to `npy_alloc*`, avoiding memory thrashing.

## Configurable memory routines in NumPy (NEP 49)

Users may wish to override the internal data memory routines with ones of their own. Since NumPy does not use the Python domain strategy to manage data memory, it provides an alternative set of C-APIs to change memory routines. There are no Python domain-wide strategies for large chunks of object data, so those are less suited to NumPy's needs. User who wish to change the NumPy data memory management routines can use :c\`PyDataMem\_SetHandler\`, which uses a :c`PyDataMem_Handler` structure to hold pointers to functions used to manage the data memory. The calls are still wrapped by internal routines to call :c\`PyTraceMalloc\_Track\`, :c\`PyTraceMalloc\_Untrack\`. Since the functions may change during the lifetime of the process, each `ndarray` carries with it the functions used at the time of its instantiation, and these will be used to reallocate or free the data memory of the instance.

For an example of setting up and using the PyDataMem\_Handler, see the test in `` ` :file:`numpy/_core/tests/test_mem_policy.py`   What happens when deallocating if there is no policy set --------------------------------------------------------  A rare but useful technique is to allocate a buffer outside NumPy, use :c`PyArray_NewFromDescr` to wrap the buffer in a ``ndarray`, then switch the`OWNDATA`flag to true. When the`ndarray`is released, the appropriate function from the`ndarray`'s`PyDataMem\_Handler`should be called to free the buffer. But the`PyDataMem\_Handler`field was never set, it will be`NULL`. For backward compatibility, NumPy will call`free()`to release the buffer. If`NUMPY\_WARN\_IF\_NO\_MEM\_POLICY`is set to`1`, a warning will be emitted. The current default is not to emit a warning, this may change in a future version of NumPy.  A better technique would be to use a`PyCapsule`as a base object:`\`c /\* define a PyCapsule\_Destructor, using the correct deallocator for buff */ void free\_wrap(void*capsule){ void \* obj = PyCapsule\_GetPointer(capsule, PyCapsule\_GetName(capsule)); free(obj); };

> /\* then inside the function that creates arr from buff \*/ ... arr = PyArray\_NewFromDescr(... buf, ...); if (arr == NULL) { return NULL; } capsule = PyCapsule\_New(buf, "my\_wrapped\_buffer", (PyCapsule\_Destructor)\&free\_wrap); if (PyArray\_SetBaseObject(arr, capsule) == -1) { Py\_DECREF(arr); return NULL; } ...

Example of memory tracing with `np.lib.tracemalloc_domain` `` ` ------------------------------------------------------------  Note that since Python 3.6 (or newer), the builtin ``tracemalloc`module can be used to track allocations inside NumPy. NumPy places its CPU memory allocations into the`np.lib.tracemalloc\_domain`domain. For additional information, check: https://docs.python.org/3/library/tracemalloc.html.  Here is an example on how to use`np.lib.tracemalloc\_domain`:`\`python """ The goal of this example is to show how to trace memory from an application that has NumPy and non-NumPy sections. We only select the sections using NumPy related calls. """

import tracemalloc import numpy as np

\# Flag to determine if we select NumPy domain use\_np\_domain = True

nx = 300 ny = 500

\# Start to trace memory tracemalloc.start()

\# Section 1 \# ---------

\# NumPy related call a = np.zeros((nx,ny))

\# non-NumPy related call b = \[i\**2 for i in range(nx*ny)\]

snapshot1 = tracemalloc.take\_snapshot() \# We filter the snapshot to only select NumPy related calls np\_domain = np.lib.tracemalloc\_domain dom\_filter = tracemalloc.DomainFilter(inclusive=use\_np\_domain, domain=np\_domain) snapshot1 = snapshot1.filter\_traces(\[dom\_filter\]) top\_stats1 = snapshot1.statistics('traceback')

print("================ SNAPSHOT 1 =================") for stat in top\_stats1: print(f"{stat.count} memory blocks: {stat.size / 1024:.1f} KiB") print(stat.traceback.format()\[-1\])

\# Clear traces of memory blocks allocated by Python \# before moving to the next section. tracemalloc.clear\_traces()

\# Section 2 \#----------

\# We are only using NumPy c = np.sum(a\*a)

snapshot2 = tracemalloc.take\_snapshot() top\_stats2 = snapshot2.statistics('traceback')

print() print("================ SNAPSHOT 2 =================") for stat in top\_stats2: print(f"{stat.count} memory blocks: {stat.size / 1024:.1f} KiB") print(stat.traceback.format()\[-1\])

tracemalloc.stop()

print() print("============================================") print("nTracing Status : ", tracemalloc.is\_tracing())

  - try:  
    print("nTrying to Take Snapshot After Tracing is Stopped.") snap = tracemalloc.take\_snapshot()

  - except Exception as e:  
    print("Exception : ", e)

\`\`\`

---

datetimes.md

---

# Datetime API

NumPy represents dates internally using an int64 counter and a unit metadata struct. Time differences are represented similarly using an int64 and a unit metadata struct. The functions described below are available to to facilitate converting between ISO 8601 date strings, NumPy datetimes, and Python datetime objects in C.

## Data types

In addition to the :c`npy_datetime` and :c`npy_timedelta` typedefs for :c`npy_int64`, NumPy defines two additional structs that represent time unit metadata and an "exploded" view of a datetime.

Conversion functions `` ` --------------------  .. c:function:: int NpyDatetime_ConvertDatetimeStructToDatetime64( \         PyArray_DatetimeMetaData *meta, const npy_datetimestruct *dts, \         npy_datetime *out)      Converts a datetime from a datetimestruct to a datetime in the units     specified by the unit metadata. The date is assumed to be valid.      If the ``num`member of the metadata struct is large, there may     be integer overflow in this function.      Returns 0 on success and -1 on failure.  .. c:function:: int NpyDatetime_ConvertDatetime64ToDatetimeStruct( \         PyArray_DatetimeMetaData *meta, npy_datetime dt, \         npy_datetimestruct *out)      Converts a datetime with units specified by the unit metadata to an     exploded datetime struct.      Returns 0 on success and -1 on failure.  .. c:function:: int NpyDatetime_ConvertPyDateTimeToDatetimeStruct( \         PyObject *obj, npy_datetimestruct *out, \         NPY_DATETIMEUNIT *out_bestunit, int apply_tzinfo)      Tests for and converts a Python`datetime.datetime`or`datetime.date`object into a NumPy`npy\_datetimestruct`.`out\_bestunit`gives a suggested unit based on whether the object     was a`datetime.date`or`datetime.datetime`object.      If`apply\_tzinfo`is 1, this function uses the tzinfo to convert     to UTC time, otherwise it returns the struct with the local time.      Returns -1 on error, 0 on success, and 1 (with no error set)     if obj doesn't have the needed date or datetime attributes.  .. c:function:: int NpyDatetime_ParseISO8601Datetime( \         char const *str, Py_ssize_t len, NPY_DATETIMEUNIT unit, \         NPY_CASTING casting, npy_datetimestruct *out, \         NPY_DATETIMEUNIT *out_bestunit, npy_bool *out_special)      Parses (almost) standard ISO 8601 date strings. The differences are:      * The date "20100312" is parsed as the year 20100312, not as       equivalent to "2010-03-12". The '-' in the dates are not optional.     * Only seconds may have a decimal point, with up to 18 digits after it       (maximum attoseconds precision).     * Either a 'T' as in ISO 8601 or a ' ' may be used to separate       the date and the time. Both are treated equivalently.     * Doesn't (yet) handle the "YYYY-DDD" or "YYYY-Www" formats.     * Doesn't handle leap seconds (seconds value has 60 in these cases).     * Doesn't handle 24:00:00 as synonym for midnight (00:00:00) tomorrow     * Accepts special values "NaT" (not a time), "Today", (current       day according to local time) and "Now" (current time in UTC).`str`must be a NULL-terminated string, and`len`must be its length.`unit`should contain -1 if the unit is unknown, or the unit     which will be used if it is.`casting`controls how the detected unit from the string is allowed     to be cast to the 'unit' parameter.`out`gets filled with the parsed date-time.`out\_bestunit`gives a suggested unit based on the amount of     resolution provided in the string, or -1 for NaT.`out\_special`gets set to 1 if the parsed time was 'today',     'now', empty string, or 'NaT'. For 'today', the unit recommended is     'D', for 'now', the unit recommended is 's', and for 'NaT'     the unit recommended is 'Y'.      Returns 0 on success, -1 on failure.  .. c:function:: int NpyDatetime_GetDatetimeISO8601StrLen(\         int local, NPY_DATETIMEUNIT base)      Returns the string length to use for converting datetime     objects with the given local time and unit settings to strings.     Use this when constructing strings to supply to`NpyDatetime\_MakeISO8601Datetime`.  .. c:function:: int NpyDatetime_MakeISO8601Datetime(\         npy_datetimestruct *dts, char *outstr, npy_intp outlen, \         int local, int utc, NPY_DATETIMEUNIT base, int tzoffset, \         NPY_CASTING casting)      Converts an`npy\_datetimestruct`to an (almost) ISO 8601     NULL-terminated string. If the string fits in the space exactly,     it leaves out the NULL terminator and returns success.      The differences from ISO 8601 are the 'NaT' string, and     the number of year digits is >= 4 instead of strictly 4.      If`local`is non-zero, it produces a string in local time with     a +-#### timezone offset. If`local`is zero and`utc`is non-zero,     produce a string ending with 'Z' to denote UTC. By default, no time     zone information is attached.`base`restricts the output to that unit. Set`base`to     -1 to auto-detect a base after which all the values are zero.`tzoffset`is used if`local`is enabled, and`tzoffset`is     set to a value other than -1. This is a manual override for     the local time zone to use, as an offset in minutes.`casting`controls whether data loss is allowed by truncating     the data to a coarser unit. This interacts with`local\`\`, slightly, in order to form a date unit string as a local time, the casting must be unsafe.

> Returns 0 on success, -1 on failure (for example if the output string was too short).

---

deprecations.md

---

# C API deprecations

## Background

The API exposed by NumPy for third-party extensions has grown over years of releases, and has allowed programmers to directly access NumPy functionality from C. This API can be best described as "organic". It has emerged from multiple competing desires and from multiple points of view over the years, strongly influenced by the desire to make it easy for users to move to NumPy from Numeric and Numarray. The core API originated with Numeric in 1995 and there are patterns such as the heavy use of macros written to mimic Python's C-API as well as account for compiler technology of the late 90's. There is also only a small group of volunteers who have had very little time to spend on improving this API.

There is an ongoing effort to improve the API. It is important in this effort to ensure that code that compiles for NumPy 1.X continues to compile for NumPy 1.X. At the same time, certain API's will be marked as deprecated so that future-looking code can avoid these API's and follow better practices.

Another important role played by deprecation markings in the C API is to move towards hiding internal details of the NumPy implementation. For those needing direct, easy, access to the data of ndarrays, this will not remove this ability. Rather, there are many potential performance optimizations which require changing the implementation details, and NumPy developers have been unable to try them because of the high value of preserving ABI compatibility. By deprecating this direct access, we will in the future be able to improve NumPy's performance in ways we cannot presently.

## Deprecation mechanism NPY\_NO\_DEPRECATED\_API

In C, there is no equivalent to the deprecation warnings that Python supports. One way to do deprecations is to flag them in the documentation and release notes, then remove or change the deprecated features in a future major version (NumPy 2.0 and beyond). Minor versions of NumPy should not have major C-API changes, however, that prevent code that worked on a previous minor release. For example, we will do our best to ensure that code that compiled and worked on NumPy 1.4 should continue to work on NumPy 1.7 (but perhaps with compiler warnings).

To use the NPY\_NO\_DEPRECATED\_API mechanism, you need to \#define it to the target API version of NumPy before \#including any NumPy headers. If you want to confirm that your code is clean against 1.7, use:

`` `c     #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION  On compilers which support a #warning mechanism, NumPy issues a ``\` compiler warning if you do not define the symbol NPY\_NO\_DEPRECATED\_API. This way, the fact that there are deprecations will be flagged for third-party developers who may not have read the release notes closely.

Note that defining NPY\_NO\_DEPRECATED\_API is not sufficient to make your extension ABI compatible with a given NumPy version. See \[for-downstream-package-authors\](\#for-downstream-package-authors).

---

dtype.md

---

# Data type API

<div class="sectionauthor">

Travis E. Oliphant

</div>

The standard array can have 25 different data types (and has some support for adding your own types). These data types all have an enumerated type, an enumerated type-character, and a corresponding array scalar Python type object (placed in a hierarchy). There are also standard C typedefs to make it easier to manipulate elements of the given data type. For the numeric types, there are also bit-width equivalent C typedefs and named typenumbers that make it easier to select the precision desired.

\> **Warning** \> The names for the types in c code follows c naming conventions more closely. The Python names for these types follow Python conventions. Thus, :c\`NPY\_FLOAT\` picks up a 32-bit float in C, but <span class="title-ref">numpy.float64</span> in Python corresponds to a 64-bit double. The bit-width names can be used in both Python and C for clarity.

## Enumerated types

Other useful related constants are

The various character codes indicating certain types are also part of an enumerated list. References to type characters (should they be needed at all) should always use these enumerations. The form of them is `NPY_{NAME}LTR` where `{NAME}` can be

> **BOOL**, **BYTE**, **UBYTE**, **SHORT**, **USHORT**, **INT**, **UINT**, **LONG**, **ULONG**, **LONGLONG**, **ULONGLONG**, **HALF**, **FLOAT**, **DOUBLE**, **LONGDOUBLE**, **CFLOAT**, **CDOUBLE**, **CLONGDOUBLE**, **DATETIME**, **TIMEDELTA**, **OBJECT**, **STRING**, **UNICODE**, **VSTRING**, **VOID**
> 
> **INTP**, **UINTP**
> 
> **GENBOOL**, **SIGNED**, **UNSIGNED**, **FLOATING**, **COMPLEX**

The latter group of `{NAME}s` corresponds to letters used in the array interface typestring specification.

## Defines

### Max and min values for integers

  - `NPY_MAX_INT{bits}`, `NPY_MAX_UINT{bits}`, `NPY_MIN_INT{bits}`  
    These are defined for `{bits}` = 8, 16, 32, 64, 128, and 256 and provide the maximum (minimum) value of the corresponding (unsigned) integer type. Note: the actual integer type may not be available on all platforms (i.e. 128-bit and 256-bit integers are rare).

  - `NPY_MIN_{type}`  
    This is defined for `{type}` = **BYTE**, **SHORT**, **INT**, **LONG**, **LONGLONG**, **INTP**

  - `NPY_MAX_{type}`  
    This is defined for all defined for `{type}` = **BYTE**, **UBYTE**, **SHORT**, **USHORT**, **INT**, **UINT**, **LONG**, **ULONG**, **LONGLONG**, **ULONGLONG**, **INTP**, **UINTP**

### Number of bits in data types

All `NPY_SIZEOF_{CTYPE}` constants have corresponding `NPY_BITSOF_{CTYPE}` constants defined. The `NPY_BITSOF_{CTYPE}` constants provide the number of bits in the data type. Specifically, the available `{CTYPE}s` are

> **BOOL**, **CHAR**, **SHORT**, **INT**, **LONG**, **LONGLONG**, **FLOAT**, **DOUBLE**, **LONGDOUBLE**

### Bit-width references to enumerated typenums

All of the numeric data types (integer, floating point, and complex) have constants that are defined to be a specific enumerated type number. Exactly which enumerated type a bit-width type refers to is platform dependent. In particular, the constants available are `PyArray_{NAME}{BITS}` where `{NAME}` is **INT**, **UINT**, **FLOAT**, **COMPLEX** and `{BITS}` can be 8, 16, 32, 64, 80, 96, 128, 160, 192, 256, and 512. Obviously not all bit-widths are available on all platforms for all the kinds of numeric types. Commonly 8-, 16-, 32-, 64-bit integers; 32-, 64-bit floats; and 64-, 128-bit complex types are available.

### Further integer aliases

The constants **NPY\_INTP** and **NPY\_UINTP** refer to an `Py_ssize_t` and `size_t`. Although in practice normally true, these types are strictly speaking not pointer sized and the character codes `'p'` and `'P'` can be used for pointer sized integers. (Before NumPy 2, `intp` was pointer size, but this almost never matched the actual use, which is the reason for the name.)

Since NumPy 2, **NPY\_DEFAULT\_INT** is additionally defined. The value of the macro is runtime dependent: Since NumPy 2, it maps to `NPY_INTP` while on earlier versions it maps to `NPY_LONG`.

## C-type names

There are standard variable types for each of the numeric data types and the bool data type. Some of these are already available in the C-specification. You can create variables in extension code with these types.

### Boolean

### (Un)Signed Integer

Unsigned versions of the integers can be defined by prepending a 'u' to the front of the integer name.

### (Complex) Floating point

complex types are structures with **.real** and **.imag** members (in that order).

### Bit-width names

There are also typedefs for signed integers, unsigned integers, floating point, and complex floating point types of specific bit-widths. The available type names are

> `npy_int{bits}`, `npy_uint{bits}`, `npy_float{bits}`, and `npy_complex{bits}`

where `{bits}` is the number of bits in the type and can be **8**, **16**, **32**, **64**, 128, and 256 for integer types; 16, **32** , **64**, 80, 96, 128, and 256 for floating-point types; and 32, **64**, **128**, 160, 192, and 512 for complex-valued types. Which bit-widths are available is platform dependent. The bolded bit-widths are usually available on all platforms.

### Time and timedelta

## Printf formatting

For help in printing, the following strings are defined as the correct format specifier in printf and related commands.

---

generalized-ufuncs.md

---

# Generalized universal function API

There is a general need for looping over not only functions on scalars but also over functions on vectors (or arrays). This concept is realized in NumPy by generalizing the universal functions (ufuncs). In regular ufuncs, the elementary function is limited to element-by-element operations, whereas the generalized version (gufuncs) supports "sub-array" by "sub-array" operations. The Perl vector library PDL provides a similar functionality and its terms are re-used in the following.

Each generalized ufunc has information associated with it that states what the "core" dimensionality of the inputs is, as well as the corresponding dimensionality of the outputs (the element-wise ufuncs have zero core dimensions). The list of the core dimensions for all arguments is called the "signature" of a ufunc. For example, the ufunc `numpy.add` has signature `(),()->()` defining two scalar inputs and one scalar output.

Another example is the function `inner1d(a, b)` with a signature of `(i),(i)->()`. This applies the inner product along the last axis of each input, but keeps the remaining indices intact. For example, where `a` is of shape `(3, 5, N)` and `b` is of shape `(5, N)`, this will return an output of shape `(3,5)`. The underlying elementary function is called `3 * 5` times. In the signature, we specify one core dimension `(i)` for each input and zero core dimensions `()` for the output, since it takes two 1-d arrays and returns a scalar. By using the same name `i`, we specify that the two corresponding dimensions should be of the same size.

The dimensions beyond the core dimensions are called "loop" dimensions. In the above example, this corresponds to `(3, 5)`.

The signature determines how the dimensions of each input/output array are split into core and loop dimensions:

1.  Each dimension in the signature is matched to a dimension of the corresponding passed-in array, starting from the end of the shape tuple. These are the core dimensions, and they must be present in the arrays, or an error will be raised.
2.  Core dimensions assigned to the same label in the signature (e.g. the `i` in `inner1d`'s `(i),(i)->()`) must have exactly matching sizes, no broadcasting is performed.
3.  The core dimensions are removed from all inputs and the remaining dimensions are broadcast together, defining the loop dimensions.
4.  The shape of each output is determined from the loop dimensions plus the output's core dimensions

Typically, the size of all core dimensions in an output will be determined by the size of a core dimension with the same label in an input array. This is not a requirement, and it is possible to define a signature where a label comes up for the first time in an output, although some precautions must be taken when calling such a function. An example would be the function `euclidean_pdist(a)`, with signature `(n,d)->(p)`, that given an array of `n` `d`-dimensional vectors, computes all unique pairwise Euclidean distances among them. The output dimension `p` must therefore be equal to `n * (n - 1) / 2`, but by default, it is the caller's responsibility to pass in an output array of the right size. If the size of a core dimension of an output cannot be determined from a passed in input or output array, an error will be raised. This can be changed by defining a `PyUFunc_ProcessCoreDimsFunc` function and assigning it to the `proces_core_dims_func` field of the `PyUFuncObject` structure. See below for more details.

Note: Prior to NumPy 1.10.0, less strict checks were in place: missing core dimensions were created by prepending 1's to the shape as necessary, core dimensions with the same label were broadcast together, and undetermined dimensions were created with size 1.

## Definitions

  - Elementary Function  
    Each ufunc consists of an elementary function that performs the most basic operation on the smallest portion of array arguments (e.g. adding two numbers is the most basic operation in adding two arrays). The ufunc applies the elementary function multiple times on different parts of the arrays. The input/output of elementary functions can be vectors; e.g., the elementary function of `inner1d` takes two vectors as input.

  - Signature  
    A signature is a string describing the input/output dimensions of the elementary function of a ufunc. See section below for more details.

  - Core Dimension  
    The dimensionality of each input/output of an elementary function is defined by its core dimensions (zero core dimensions correspond to a scalar input/output). The core dimensions are mapped to the last dimensions of the input/output arrays.

  - Dimension Name  
    A dimension name represents a core dimension in the signature. Different dimensions may share a name, indicating that they are of the same size.

  - Dimension Index  
    A dimension index is an integer representing a dimension name. It enumerates the dimension names according to the order of the first occurrence of each name in the signature.

## Details of signature

The signature defines "core" dimensionality of input and output variables, and thereby also defines the contraction of the dimensions. The signature is represented by a string of the following format:

  - Core dimensions of each input or output array are represented by a list of dimension names in parentheses, `(i_1,...,i_N)`; a scalar input/output is denoted by `()`. Instead of `i_1`, `i_2`, etc, one can use any valid Python variable name.
  - Dimension lists for different arguments are separated by `","`. Input/output arguments are separated by `"->"`.
  - If one uses the same dimension name in multiple locations, this enforces the same size of the corresponding dimensions.

The formal syntax of signatures is as follows:

    <Signature>            ::= <Input arguments> "->" <Output arguments>
    <Input arguments>      ::= <Argument list>
    <Output arguments>     ::= <Argument list>
    <Argument list>        ::= nil | <Argument> | <Argument> "," <Argument list>
    <Argument>             ::= "(" <Core dimension list> ")"
    <Core dimension list>  ::= nil | <Core dimension> |
                               <Core dimension> "," <Core dimension list>
    <Core dimension>       ::= <Dimension name> <Dimension modifier>
    <Dimension name>       ::= valid Python variable name | valid integer
    <Dimension modifier>   ::= nil | "?"

Notes:

1.  All quotes are for clarity.
2.  Unmodified core dimensions that share the same name must have the same size. Each dimension name typically corresponds to one level of looping in the elementary function's implementation.
3.  White spaces are ignored.
4.  An integer as a dimension name freezes that dimension to the value.
5.  If the name is suffixed with the "?" modifier, the dimension is a core dimension only if it exists on all inputs and outputs that share it; otherwise it is ignored (and replaced by a dimension of size 1 for the elementary function).

Here are some examples of signatures:

<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 36%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>signature</th>
<th>common usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>add</td>
<td><code>(),()-&gt;()</code></td>
<td>binary ufunc</td>
</tr>
<tr class="even">
<td>sum1d</td>
<td><code>(i)-&gt;()</code></td>
<td>reduction</td>
</tr>
<tr class="odd">
<td>inner1d</td>
<td><code>(i),(i)-&gt;()</code></td>
<td>vector-vector multiplication</td>
</tr>
<tr class="even">
<td>matmat</td>
<td><code>(m,n),(n,p)-&gt;(m,p)</code></td>
<td>matrix multiplication</td>
</tr>
<tr class="odd">
<td>vecmat</td>
<td><code>(n),(n,p)-&gt;(p)</code></td>
<td>vector-matrix multiplication</td>
</tr>
<tr class="even">
<td>matvec</td>
<td><code>(m,n),(n)-&gt;(m)</code></td>
<td>matrix-vector multiplication</td>
</tr>
<tr class="odd">
<td>matmul</td>
<td><code>(m?,n),(n,p?)-&gt;(m?,p?)</code></td>
<td>combination of the four above</td>
</tr>
<tr class="even">
<td>outer_inner</td>
<td><code>(i,t),(j,t)-&gt;(i,j)</code></td>
<td>inner over the last dimension, outer over the second to last, and loop/broadcast over the rest.</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>cross1d</p>
</blockquote></td>
<td><code>(3),(3)-&gt;(3)</code></td>
<td>cross product where the last dimension is frozen and must be 3</td>
</tr>
</tbody>
</table>

<div id="frozen">

The last is an instance of freezing a core dimension and can be used to improve ufunc performance

</div>

## C-API for implementing elementary functions

The current interface remains unchanged, and `PyUFunc_FromFuncAndData` can still be used to implement (specialized) ufuncs, consisting of scalar elementary functions.

One can use `PyUFunc_FromFuncAndDataAndSignature` to declare a more general ufunc. The argument list is the same as `PyUFunc_FromFuncAndData`, with an additional argument specifying the signature as C string.

Furthermore, the callback function is of the same type as before, `void (*foo)(char **args, intp *dimensions, intp *steps, void *func)`. When invoked, `args` is a list of length `nargs` containing the data of all input/output arguments. For a scalar elementary function, `steps` is also of length `nargs`, denoting the strides used for the arguments. `dimensions` is a pointer to a single integer defining the size of the axis to be looped over.

For a non-trivial signature, `dimensions` will also contain the sizes of the core dimensions as well, starting at the second entry. Only one size is provided for each unique dimension name and the sizes are given according to the first occurrence of a dimension name in the signature.

The first `nargs` elements of `steps` remain the same as for scalar ufuncs. The following elements contain the strides of all core dimensions for all arguments in order.

For example, consider a ufunc with signature `(i,j),(i)->()`. In this case, `args` will contain three pointers to the data of the input/output arrays `a`, `b`, `c`. Furthermore, `dimensions` will be `[N, I, J]` to define the size of `N` of the loop and the sizes `I` and `J` for the core dimensions `i` and `j`. Finally, `steps` will be `[a_N, b_N, c_N, a_i, a_j, b_i]`, containing all necessary strides.

## Customizing core dimension size processing

The optional function of type `PyUFunc_ProcessCoreDimsFunc`, stored on the `process_core_dims_func` attribute of the ufunc, provides the author of the ufunc a "hook" into the processing of the core dimensions of the arrays that were passed to the ufunc. The two primary uses of this "hook" are:

  - Check that constraints on the core dimensions required by the ufunc are satisfied (and set an exception if they are not).
  - Compute output shapes for any output core dimensions that were not determined by the input arrays.

As an example of the first use, consider the generalized ufunc `minmax` with signature `(n)->(2)` that simultaneously computes the minimum and maximum of a sequence. It should require that `n > 0`, because the minimum and maximum of a sequence with length 0 is not meaningful. In this case, the ufunc author might define the function like this:

>   - \`\`\`c
>     
>       - int minmax\_process\_core\_dims(PyUFuncObject ufunc,  
>         npy\_intp \*core\_dim\_sizes)
>     
>       - {  
>         npy\_intp n = core\_dim\_sizes\[0\]; if (n == 0) { PyExc\_SetString("minmax requires the core dimension " "to be at least 1."); return -1; } return 0;
>     
>     }

In this case, the length of the array `core_dim_sizes` will be 2. `` ` The second value in the array will always be 2, so there is no need for the function to inspect it.  The core dimension ``n`is stored in the first element.  The function sets an exception and returns -1 if it finds that`n`is 0.  The second use for the "hook" is to compute the size of output arrays when the output arrays are not provided by the caller and one or more core dimension of the output is not also an input core dimension. If the ufunc does not have a function defined on the`process\_core\_dims\_func`attribute, an unspecified output core dimension size will result in an exception being raised.  With the "hook" provided by`process\_core\_dims\_func`, the author of the ufunc can set the output size to whatever is appropriate for the ufunc.  In the array passed to the "hook" function, core dimensions that were not determined by the input are indicating by having the value -1 in the`core\_dim\_sizes`array.  The function can replace the -1 with whatever value is appropriate for the ufunc, based on the core dimensions that occurred in the input arrays.  .. warning::     The function must never change a value in`core\_dim\_sizes`that     is not -1 on input.  Changing a value that was not -1 will generally     result in incorrect output from the ufunc, and could result in the     Python interpreter crashing.  For example, consider the generalized ufunc`conv1d`for which the elementary function computes the "full" convolution of two one-dimensional arrays`x`and`y`with lengths`m`and`n`, respectively.  The output of this convolution has length`m + n - 1`. To implement this as a generalized ufunc, the signature is set to`(m),(n)-\>(p)`, and in the "hook" function, if the core dimension`p`is found to be -1, it is replaced with`m + n - 1`.  If`p`is *not* -1, it must be verified that the given value equals`m + n - 1`. If it does not, the function must set an exception and return -1. For a meaningful result, the operation also requires that`m + n`is at least 1, i.e. both inputs can't have length 0.  Here's how that might look in code:`\`c int conv1d\_process\_core\_dims(PyUFuncObject *ufunc, npy\_intp*core\_dim\_sizes) { // core\_dim\_sizes will hold the core dimensions \[m, n, p\]. // p will be -1 if the caller did not provide the out argument. npy\_intp m = core\_dim\_sizes\[0\]; npy\_intp n = core\_dim\_sizes\[1\]; npy\_intp p = core\_dim\_sizes\[2\]; npy\_intp required\_p = m + n - 1;

>   - if (m == 0 && n == 0) {  
>     // Disallow both inputs having length 0. PyErr\_SetString(PyExc\_ValueError, "conv1d: both inputs have core dimension 0; the function " "requires that at least one input has size greater than 0."); return -1;
> 
> } if (p == -1) { // Output array was not given in the call of the ufunc. // Set the correct output size here. core\_dim\_sizes\[2\] = required\_p; return 0; } // An output array *was* given. Validate its core dimension. if (p \!= required\_p) { PyErr\_Format(PyExc\_ValueError, "conv1d: the core dimension p of the out parameter " "does not equal m + n - 1, where m and n are the " "core dimensions of the inputs x and y; got m=%zd " "and n=%zd so p must be %zd, but got p=%zd.", m, n, required\_p, p); return -1; } return 0;

### }

---

index.md

---

# NumPy C-API

<div class="sectionauthor">

Travis E. Oliphant

</div>

Â Â Â Beware of the man who won't be bothered with details.  
Â Â Â --- *William Feather, Sr.*

Â Â Â The truth is out there.  
Â Â Â --- *Chris Carter, The X Files*

NumPy provides a C-API to enable users to extend the system and get access to the array object for use in other routines. The best way to truly understand the C-API is to read the source code. If you are unfamiliar with (C) source code, however, this can be a daunting experience at first. Be assured that the task becomes easier with practice, and you may be surprised at how simple the C-code can be to understand. Even if you don't think you can write C-code from scratch, it is much easier to understand and modify already-written source code than create it *de novo*.

Python extensions are especially straightforward to understand because they all have a very similar structure. Admittedly, NumPy is not a trivial extension to Python, and may take a little more snooping to grasp. This is especially true because of the code-generation techniques, which simplify maintenance of very similar code, but can make the code a little less readable to beginners. Still, with a little persistence, the code can be opened to your understanding. It is my hope, that this guide to the C-API can assist in the process of becoming familiar with the compiled-level work that can be done with NumPy in order to squeeze that last bit of necessary speed out of your code.

<div class="currentmodule">

numpy-c-api

</div>

<div class="toctree" data-maxdepth="2">

types-and-structures config dtype array iterator ufunc generalized-ufuncs strings coremath datetimes deprecations data\_memory

</div>

---

iterator.md

---

# Array iterator API

<div class="sectionauthor">

Mark Wiebe

</div>

<div class="index">

pair: iterator; C-API pair: C-API; iterator

</div>

## Array iterator

The array iterator encapsulates many of the key features in ufuncs, allowing user code to support features like output parameters, preservation of memory layouts, and buffering of data with the wrong alignment or type, without requiring difficult coding.

This page documents the API for the iterator. The iterator is named `NpyIter` and functions are named `NpyIter_*`.

There is an \[introductory guide to array iteration \<arrays.nditer\>\](\#introductory-guide-to-array-iteration-\<arrays.nditer\>) which may be of interest for those using this C API. In many instances, testing out ideas by creating the iterator in Python is a good idea before writing the C iteration code.

## Iteration example

The best way to become familiar with the iterator is to look at its usage within the NumPy codebase itself. For example, here is a slightly tweaked version of the code for :c\`PyArray\_CountNonzero\`, which counts the number of non-zero elements in an array.

`` `c     npy_intp PyArray_CountNonzero(PyArrayObject* self)     {         /* Nonzero boolean function */         PyArray_NonzeroFunc* nonzero = PyArray_DESCR(self)->f->nonzero;          NpyIter* iter;         NpyIter_IterNextFunc *iternext;         char** dataptr;         npy_intp nonzero_count;         npy_intp* strideptr,* innersizeptr;          /* Handle zero-sized arrays specially */         if (PyArray_SIZE(self) == 0) {             return 0;         }          /*          * Create and use an iterator to count the nonzeros.          *   flag NPY_ITER_READONLY          *     - The array is never written to.          *   flag NPY_ITER_EXTERNAL_LOOP          *     - Inner loop is done outside the iterator for efficiency.          *   flag NPY_ITER_NPY_ITER_REFS_OK          *     - Reference types are acceptable.          *   order NPY_KEEPORDER          *     - Visit elements in memory order, regardless of strides.          *       This is good for performance when the specific order          *       elements are visited is unimportant.          *   casting NPY_NO_CASTING          *     - No casting is required for this operation.          */         iter = NpyIter_New(self, NPY_ITER_READONLY|                                  NPY_ITER_EXTERNAL_LOOP|                                  NPY_ITER_REFS_OK,                             NPY_KEEPORDER, NPY_NO_CASTING,                             NULL);         if (iter == NULL) {             return -1;         }          /*          * The iternext function gets stored in a local variable          * so it can be called repeatedly in an efficient manner.          */         iternext = NpyIter_GetIterNext(iter, NULL);         if (iternext == NULL) {             NpyIter_Deallocate(iter);             return -1;         }         /* The location of the data pointer which the iterator may update */         dataptr = NpyIter_GetDataPtrArray(iter);         /* The location of the stride which the iterator may update */         strideptr = NpyIter_GetInnerStrideArray(iter);         /* The location of the inner loop size which the iterator may update */         innersizeptr = NpyIter_GetInnerLoopSizePtr(iter);          nonzero_count = 0;         do {             /* Get the inner loop data/stride/count values */             char* data = *dataptr;             npy_intp stride = *strideptr;             npy_intp count = *innersizeptr;              /* This is a typical inner loop for NPY_ITER_EXTERNAL_LOOP */             while (count--) {                 if (nonzero(data, self)) {                     ++nonzero_count;                 }                 data += stride;             }              /* Increment the iterator to the next inner loop */         } while(iternext(iter));          NpyIter_Deallocate(iter);          return nonzero_count;     }  Multi-iteration example ``\` ------------------------------

Here is a copy function using the iterator. The `order` parameter is used to control the memory layout of the allocated result, typically :c\`NPY\_KEEPORDER\` is desired.

`` `c     PyObject *CopyArray(PyObject *arr, NPY_ORDER order)     {         NpyIter *iter;         NpyIter_IterNextFunc *iternext;         PyObject *op[2], *ret;         npy_uint32 flags;         npy_uint32 op_flags[2];         npy_intp itemsize, *innersizeptr, innerstride;         char **dataptrarray;          /*          * No inner iteration - inner loop is handled by CopyArray code          */         flags = NPY_ITER_EXTERNAL_LOOP;         /*          * Tell the constructor to automatically allocate the output.          * The data type of the output will match that of the input.          */         op[0] = arr;         op[1] = NULL;         op_flags[0] = NPY_ITER_READONLY;         op_flags[1] = NPY_ITER_WRITEONLY | NPY_ITER_ALLOCATE;          /* Construct the iterator */         iter = NpyIter_MultiNew(2, op, flags, order, NPY_NO_CASTING,                                 op_flags, NULL);         if (iter == NULL) {             return NULL;         }          /*          * Make a copy of the iternext function pointer and          * a few other variables the inner loop needs.          */         iternext = NpyIter_GetIterNext(iter, NULL);         innerstride = NpyIter_GetInnerStrideArray(iter)[0];         itemsize = NpyIter_GetDescrArray(iter)[0]->elsize;         /*          * The inner loop size and data pointers may change during the          * loop, so just cache the addresses.          */         innersizeptr = NpyIter_GetInnerLoopSizePtr(iter);         dataptrarray = NpyIter_GetDataPtrArray(iter);          /*          * Note that because the iterator allocated the output,          * it matches the iteration order and is packed tightly,          * so we don't need to check it like the input.          */         if (innerstride == itemsize) {             do {                 memcpy(dataptrarray[1], dataptrarray[0],                                         itemsize * (*innersizeptr));             } while (iternext(iter));         } else {             /* For efficiency, should specialize this based on item size... */             npy_intp i;             do {                 npy_intp size = *innersizeptr;                 char *src = dataptrarray[0], *dst = dataptrarray[1];                 for(i = 0; i < size; i++, src += innerstride, dst += itemsize) {                     memcpy(dst, src, itemsize);                 }             } while (iternext(iter));         }          /* Get the result from the iterator object array */         ret = NpyIter_GetOperandArray(iter)[1];         Py_INCREF(ret);          if (NpyIter_Deallocate(iter) != NPY_SUCCEED) {             Py_DECREF(ret);             return NULL;         }          return ret;     }   Multi index tracking example ``\` ----------------------------

This example shows you how to work with the :c\`NPY\_ITER\_MULTI\_INDEX\` flag. For simplicity, we assume the argument is a two-dimensional array.

`` `c    int PrintMultiIndex(PyArrayObject *arr) {        NpyIter *iter;        NpyIter_IterNextFunc *iternext;        npy_intp multi_index[2];         iter = NpyIter_New(            arr, NPY_ITER_READONLY | NPY_ITER_MULTI_INDEX | NPY_ITER_REFS_OK,            NPY_KEEPORDER, NPY_NO_CASTING, NULL);        if (iter == NULL) {            return -1;        }        if (NpyIter_GetNDim(iter) != 2) {            NpyIter_Deallocate(iter);            PyErr_SetString(PyExc_ValueError, "Array must be 2-D");            return -1;        }        if (NpyIter_GetIterSize(iter) != 0) {            iternext = NpyIter_GetIterNext(iter, NULL);            if (iternext == NULL) {                NpyIter_Deallocate(iter);                return -1;            }            NpyIter_GetMultiIndexFunc *get_multi_index =                NpyIter_GetGetMultiIndex(iter, NULL);            if (get_multi_index == NULL) {                NpyIter_Deallocate(iter);                return -1;            }             do {                get_multi_index(iter, multi_index);                printf("multi_index is [%" NPY_INTP_FMT ", %" NPY_INTP_FMT "]\n",                       multi_index[0], multi_index[1]);            } while (iternext(iter));        }        if (!NpyIter_Deallocate(iter)) {            return -1;        }        return 0;    }  When called with a 2x3 array, the above example prints:  .. code-block:: sh     multi_index is [0, 0]    multi_index is [0, 1]    multi_index is [0, 2]    multi_index is [1, 0]    multi_index is [1, 1]    multi_index is [1, 2]   Iterator data types ``\` ---------------------

The iterator layout is an internal detail, and user code only sees an incomplete struct.

## Construction and destruction

  - `` ` ..     dedent the enumeration of flags to avoid missing references sphinx warnings   .. c:macro:: NPY_ITER_C_INDEX      Causes the iterator to track a raveled flat index matching C     order. This option cannot be used with :c`NPY_ITER_F_INDEX`.  .. c:macro:: NPY_ITER_F_INDEX      Causes the iterator to track a raveled flat index matching Fortran     order. This option cannot be used with :c`NPY_ITER_C_INDEX`.  .. c:macro:: NPY_ITER_MULTI_INDEX      Causes the iterator to track a multi-index.     This prevents the iterator from coalescing axes to     produce bigger inner loops. If the loop is also not buffered     and no index is being tracked (:c`NpyIter_RemoveAxis` can be called),     then the iterator size can be ``-1``to indicate that the iterator     is too large. This can happen due to complex broadcasting and     will result in errors being created when the setting the iterator     range, removing the multi index, or getting the next function.     However, it is possible to remove axes again and use the iterator     normally if the size is small enough after removal.  .. c:macro:: NPY_ITER_EXTERNAL_LOOP      Causes the iterator to skip iteration of the innermost     loop, requiring the user of the iterator to handle it.      This flag is incompatible with :c`NPY_ITER_C_INDEX`,     :c`NPY_ITER_F_INDEX`, and :c`NPY_ITER_MULTI_INDEX`.  .. c:macro:: NPY_ITER_DONT_NEGATE_STRIDES      This only affects the iterator when :c:type:`NPY_KEEPORDER` is     specified for the order parameter.  By default with     :c:type:`NPY_KEEPORDER`, the iterator reverses axes which have     negative strides, so that memory is traversed in a forward     direction.  This disables this step.  Use this flag if you     want to use the underlying memory-ordering of the axes,     but don't want an axis reversed. This is the behavior of``numpy.ravel(a, order='K')`, for instance.  .. c:macro:: NPY_ITER_COMMON_DTYPE      Causes the iterator to convert all the operands to a common     data type, calculated based on the ufunc type promotion rules.     Copying or buffering must be enabled.      If the common data type is known ahead of time, don't use this     flag.  Instead, set the requested dtype for all the operands.  .. c:macro:: NPY_ITER_REFS_OK      Indicates that arrays with reference types (object     arrays or structured arrays containing an object type)     may be accepted and used in the iterator.  If this flag     is enabled, the caller must be sure to check whether`NpyIter\_IterationNeedsAPI(iter)``is true, in which case     it may not release the GIL during iteration.  .. c:macro:: NPY_ITER_ZEROSIZE_OK      Indicates that arrays with a size of zero should be permitted.     Since the typical iteration loop does not naturally work with     zero-sized arrays, you must check that the IterSize is larger     than zero before entering the iteration loop.     Currently only the operands are checked, not a forced shape.  .. c:macro:: NPY_ITER_REDUCE_OK      Permits writeable operands with a dimension with zero     stride and size greater than one.  Note that such operands     must be read/write.      When buffering is enabled, this also switches to a special     buffering mode which reduces the loop length as necessary to     not trample on values being reduced.      Note that if you want to do a reduction on an automatically     allocated output, you must use :c`NpyIter_GetOperandArray`     to get its reference, then set every value to the reduction     unit before doing the iteration loop.  In the case of a     buffered reduction, this means you must also specify the     flag :c`NPY_ITER_DELAY_BUFALLOC`, then reset the iterator     after initializing the allocated operand to prepare the     buffers.  .. c:macro:: NPY_ITER_RANGED      Enables support for iteration of sub-ranges of the full``iterindex`range`\[0, NpyIter\_IterSize(iter))``.  Use     the function :c`NpyIter_ResetToIterIndexRange` to specify     a range for iteration.      This flag can only be used with :c`NPY_ITER_EXTERNAL_LOOP`     when :c`NPY_ITER_BUFFERED` is enabled.  This is because     without buffering, the inner loop is always the size of the     innermost iteration dimension, and allowing it to get cut up     would require special handling, effectively making it more     like the buffered version.  .. c:macro:: NPY_ITER_BUFFERED      Causes the iterator to store buffering data, and use buffering     to satisfy data type, alignment, and byte-order requirements.     To buffer an operand, do not specify the :c`NPY_ITER_COPY`     or :c`NPY_ITER_UPDATEIFCOPY` flags, because they will     override buffering.  Buffering is especially useful for Python     code using the iterator, allowing for larger chunks     of data at once to amortize the Python interpreter overhead.      If used with :c`NPY_ITER_EXTERNAL_LOOP`, the inner loop     for the caller may get larger chunks than would be possible     without buffering, because of how the strides are laid out.      Note that if an operand is given the flag :c`NPY_ITER_COPY`     or :c`NPY_ITER_UPDATEIFCOPY`, a copy will be made in preference     to buffering.  Buffering will still occur when the array was     broadcast so elements need to be duplicated to get a constant     stride.      In normal buffering, the size of each inner loop is equal     to the buffer size, or possibly larger if     :c`NPY_ITER_GROWINNER` is specified.  If     :c`NPY_ITER_REDUCE_OK` is enabled and a reduction occurs,     the inner loops may become smaller depending     on the structure of the reduction.  .. c:macro:: NPY_ITER_GROWINNER      When buffering is enabled, this allows the size of the inner     loop to grow when buffering isn't necessary.  This option     is best used if you're doing a straight pass through all the     data, rather than anything with small cache-friendly arrays     of temporary values for each inner loop.  .. c:macro:: NPY_ITER_DELAY_BUFALLOC      When buffering is enabled, this delays allocation of the     buffers until :c`NpyIter_Reset` or another reset function is     called.  This flag exists to avoid wasteful copying of     buffer data when making multiple copies of a buffered     iterator for multi-threaded iteration.      Another use of this flag is for setting up reduction operations.     After the iterator is created, and a reduction output     is allocated automatically by the iterator (be sure to use     READWRITE access), its value may be initialized to the reduction     unit.  Use :c`NpyIter_GetOperandArray` to get the object.     Then, call :c`NpyIter_Reset` to allocate and fill the buffers     with their initial values.  .. c:macro:: NPY_ITER_COPY_IF_OVERLAP      If any write operand has overlap with any read operand, eliminate all     overlap by making temporary copies (enabling UPDATEIFCOPY for write     operands, if necessary). A pair of operands has overlap if there is     a memory address that contains data common to both arrays.      Because exact overlap detection has exponential runtime     in the number of dimensions, the decision is made based     on heuristics, which has false positives (needless copies in unusual     cases) but has no false negatives.      If any read/write overlap exists, this flag ensures the result of the     operation is the same as if all operands were copied.     In cases where copies would need to be made, **the result of the     computation may be undefined without this flag!**      Flags that may be passed in``op\_flags\[i\]`, where`0 \<= i \< nop`: ..     dedent the enumeration of flags to avoid missing references sphinx warnings   .. c:macro:: NPY_ITER_READWRITE .. c:macro:: NPY_ITER_READONLY .. c:macro:: NPY_ITER_WRITEONLY      Indicate how the user of the iterator will read or write     to`op\[i\]`.  Exactly one of these flags must be specified     per operand. Using`NPY\_ITER\_READWRITE`or`NPY\_ITER\_WRITEONLY`for a user-provided operand may trigger`WRITEBACKIFCOPY`semantics. The data will be written back to the original array     when`NpyIter\_Deallocate`is called.  .. c:macro:: NPY_ITER_COPY      Allow a copy of`op\[i\]``to be made if it does not     meet the data type or alignment requirements as specified     by the constructor flags and parameters.  .. c:macro:: NPY_ITER_UPDATEIFCOPY      Triggers :c`NPY_ITER_COPY`, and when an array operand     is flagged for writing and is copied, causes the data     in a copy to be copied back to``op\[i\]`when`NpyIter\_Deallocate`is called.      If the operand is flagged as write-only and a copy is needed,     an uninitialized temporary array will be created and then copied     to back to`op\[i\]`on calling`NpyIter\_Deallocate`, instead of     doing the unnecessary copy operation.  .. c:macro:: NPY_ITER_NBO .. c:macro:: NPY_ITER_ALIGNED .. c:macro:: NPY_ITER_CONTIG      Causes the iterator to provide data for`op\[i\]``that is in native byte order, aligned according to     the dtype requirements, contiguous, or any combination.      By default, the iterator produces pointers into the     arrays provided, which may be aligned or unaligned, and     with any byte order.  If copying or buffering is not     enabled and the operand data doesn't satisfy the constraints,     an error will be raised.      The contiguous constraint applies only to the inner loop,     successive inner loops may have arbitrary pointer changes.      If the requested data type is in non-native byte order,     the NBO flag overrides it and the requested data type is     converted to be in native byte order.  .. c:macro:: NPY_ITER_ALLOCATE      This is for output arrays, and requires that the flag     :c`NPY_ITER_WRITEONLY` or :c`NPY_ITER_READWRITE`     be set.  If``op\[i\]`is NULL, creates a new array with     the final broadcast dimensions, and a layout matching     the iteration order of the iterator.      When`op\[i\]`is NULL, the requested data type`op\_dtypes\[i\]``may be NULL as well, in which case it is     automatically generated from the dtypes of the arrays which     are flagged as readable.  The rules for generating the dtype     are the same is for UFuncs.  Of special note is handling     of byte order in the selected dtype.  If there is exactly     one input, the input's dtype is used as is.  Otherwise,     if more than one input dtypes are combined together, the     output will be in native byte order.      After being allocated with this flag, the caller may retrieve     the new array by calling :c`NpyIter_GetOperandArray` and     getting the i-th object in the returned C array.  The caller     must call Py_INCREF on it to claim a reference to the array.  .. c:macro:: NPY_ITER_NO_SUBTYPE      For use with :c`NPY_ITER_ALLOCATE`, this flag disables     allocating an array subtype for the output, forcing     it to be a straight ndarray.      TODO: Maybe it would be better to introduce a function``NpyIter\_GetWrappedOutput``and remove this flag?  .. c:macro:: NPY_ITER_NO_BROADCAST      Ensures that the input or output matches the iteration     dimensions exactly.  .. c:macro:: NPY_ITER_ARRAYMASK      Indicates that this operand is the mask to use for     selecting elements when writing to operands which have     the :c`NPY_ITER_WRITEMASKED` flag applied to them.     Only one operand may have :c`NPY_ITER_ARRAYMASK` flag     applied to it.      The data type of an operand with this flag should be either     :c`NPY_BOOL`, :c`NPY_MASK`, or a struct dtype     whose fields are all valid mask dtypes. In the latter case,     it must match up with a struct operand being WRITEMASKED,     as it is specifying a mask for each field of that array.      This flag only affects writing from the buffer back to     the array. This means that if the operand is also     :c`NPY_ITER_READWRITE` or :c`NPY_ITER_WRITEONLY`,     code doing iteration can write to this operand to     control which elements will be untouched and which ones will be     modified. This is useful when the mask should be a combination     of input masks.  .. c:macro:: NPY_ITER_WRITEMASKED      This array is the mask for all `writemasked <numpy.nditer>`     operands. Code uses the``writemasked`flag which indicates      that only elements where the chosen ARRAYMASK operand is True     will be written to. In general, the iterator does not enforce     this, it is up to the code doing the iteration to follow that     promise.      When`writemasked`flag is used, and this operand is buffered,     this changes how data is copied from the buffer into the array.     A masked copying routine is used, which only copies the     elements in the buffer for which`writemasked`returns true from the corresponding element in the ARRAYMASK     operand.  .. c:macro:: NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE      In memory overlap checks, assume that operands with`NPY\_ITER\_OVERLAP\_ASSUME\_ELEMENTWISE`enabled are accessed only     in the iterator order.      This enables the iterator to reason about data dependency,     possibly avoiding unnecessary copies.      This flag has effect only if`NPY\_ITER\_COPY\_IF\_OVERLAP``is enabled     on the iterator.  .. c:function:: NpyIter* NpyIter_AdvancedNew( \         npy_intp nop, PyArrayObject** op, npy_uint32 flags, NPY_ORDER order, \         NPY_CASTING casting, npy_uint32* op_flags, PyArray_Descr** op_dtypes, \         int oa_ndim, int** op_axes, npy_intp const* itershape, npy_intp buffersize)      Extends :c`NpyIter_MultiNew` with several advanced options providing     more control over broadcasting and buffering.      If -1/NULL values are passed to``oa\_ndim`,`op\_axes`,`itershape`,     and`buffersize``, it is equivalent to :c`NpyIter_MultiNew`.      The parameter``oa\_ndim`, when not zero or -1, specifies the number of     dimensions that will be iterated with customized broadcasting.     If it is provided,`op\_axes`must and`itershape`can also be provided.     The`op\_axes`parameter let you control in detail how the     axes of the operand arrays get matched together and iterated.     In`op\_axes`, you must provide an array of`nop`pointers     to`oa\_ndim`-sized arrays of type`npy\_intp`.  If an entry     in`op\_axes`is NULL, normal broadcasting rules will apply.     In`op\_axes\[j\]\[i\]`is stored either a valid axis of`op\[j\]`, or     -1 which means`newaxis`.  Within each`op\_axes\[j\]`array, axes     may not be repeated.  The following example is how normal broadcasting     applies to a 3-D array, a 2-D array, a 1-D array and a scalar.      **Note**: Before NumPy 1.8`oa\_ndim == 0`was used for signalling     that`op\_axes`and`itershape``are unused. This is deprecated and     should be replaced with -1. Better backward compatibility may be     achieved by using :c`NpyIter_MultiNew` for this case.``\`c  
    int oa\_ndim = 3; /\* \# iteration axes */ int op0\_axes\[\] = {0, 1, 2}; /* 3-D operand */ int op1\_axes\[\] = {-1, 0, 1}; /* 2-D operand */ int op2\_axes\[\] = {-1, -1, 0}; /* 1-D operand */ int op3\_axes\[\] = {-1, -1, -1} /* 0-D (scalar) operand */ int* op\_axes\[\] = {op0\_axes, op1\_axes, op2\_axes, op3\_axes};
    
    The `itershape` parameter allows you to force the iterator to have a specific iteration shape. It is an array of length `oa_ndim`. When an entry is negative, its value is determined from the operands. This parameter allows automatically allocated outputs to get additional dimensions which don't match up with any dimension of an input.
    
    If `buffersize` is zero, a default buffer size is used, otherwise it specifies how big of a buffer to use. Buffers which are powers of 2 such as 4096 or 8192 are recommended.
    
    Returns NULL if there is an error, otherwise returns the allocated iterator.

Functions for iteration `` ` -----------------------  .. c:function:: NpyIter_IterNextFunc* NpyIter_GetIterNext( \         NpyIter* iter, char** errmsg)      Returns a function pointer for iteration.  A specialized version     of the function pointer may be calculated by this function     instead of being stored in the iterator structure. Thus, to     get good performance, it is required that the function pointer     be saved in a variable rather than retrieved for each loop iteration.      Returns NULL if there is an error.  If errmsg is non-NULL,     no Python exception is set when ``NPY\_FAIL`is returned.     Instead, \*errmsg is set to an error message.  When errmsg is     non-NULL, the function may be safely called without holding     the Python GIL.      The typical looping construct is as follows.`\`c NpyIter\_IterNextFunc *iternext = NpyIter\_GetIterNext(iter, NULL); char*\* dataptr = NpyIter\_GetDataPtrArray(iter);

>   - do {  
>     /\* use the addresses dataptr\[0\], ... dataptr\[nop-1\] \*/
> 
> } while(iternext(iter));
> 
> When :c\`NPY\_ITER\_EXTERNAL\_LOOP\` is specified, the typical inner loop construct is as follows.
> 
> ``` c
> ```
> 
> NpyIter\_IterNextFunc *iternext = NpyIter\_GetIterNext(iter, NULL); char*\* dataptr = NpyIter\_GetDataPtrArray(iter); npy\_intp\* stride = NpyIter\_GetInnerStrideArray(iter); npy\_intp\* size\_ptr = NpyIter\_GetInnerLoopSizePtr(iter), size; npy\_intp iop, nop = NpyIter\_GetNOp(iter);
> 
>   - do {
>     
>       - size = *size\_ptr; while (size--) { /* use the addresses dataptr\[0\], ... dataptr\[nop-1\] \*/
>         
>           - for (iop = 0; iop \< nop; ++iop) {  
>             dataptr\[iop\] += stride\[iop\];
>         
>         }
>     
>     }
> 
> } while (iternext());
> 
> Observe that we are using the dataptr array inside the iterator, not copying the values to a local temporary. This is possible because when `iternext()` is called, these pointers will be overwritten with fresh values, not incrementally updated.
> 
> If a compile-time fixed buffer is being used (both flags :c\`NPY\_ITER\_BUFFERED\` and :c\`NPY\_ITER\_EXTERNAL\_LOOP\`), the inner size may be used as a signal as well. The size is guaranteed to become zero when `iternext()` returns false, enabling the following loop construct. Note that if you use this construct, you should not pass :c\`NPY\_ITER\_GROWINNER\` as a flag, because it will cause larger sizes under some circumstances.
> 
> ``` c
> ```
> 
> /\* The constructor should have buffersize passed as this value \*/ \#define FIXED\_BUFFER\_SIZE 1024
> 
> NpyIter\_IterNextFunc *iternext = NpyIter\_GetIterNext(iter, NULL); chardataptr = NpyIter\_GetDataPtrArray(iter); npy\_intp*stride = NpyIter\_GetInnerStrideArray(iter); npy\_intp \*size\_ptr = NpyIter\_GetInnerLoopSizePtr(iter), size; npy\_intp i, iop, nop = NpyIter\_GetNOp(iter);
> 
> /\* One loop with a fixed inner size */ size =*size\_ptr; while (size == FIXED\_BUFFER\_SIZE) { /\* \* This loop could be manually unrolled by a factor \* which divides into FIXED\_BUFFER\_SIZE */ for (i = 0; i \< FIXED\_BUFFER\_SIZE; ++i) { /* use the addresses dataptr\[0\], ... dataptr\[nop-1\] */ for (iop = 0; iop \< nop; ++iop) { dataptr\[iop\] += stride\[iop\]; } } iternext(); size =*size\_ptr; }
> 
>   - /\* Finish-up loop with variable inner size */ if (size \> 0) do { size =*size\_ptr;
>     
>       - while (size--) {  
>         /\* use the addresses dataptr\[0\], ... dataptr\[nop-1\] \*/ for (iop = 0; iop \< nop; ++iop) { dataptr\[iop\] += stride\[iop\]; }
>     
>     }
> 
> } while (iternext());

When the flag :c\`NPY\_ITER\_EXTERNAL\_LOOP\` is used, the code `` ` needs to know the parameters for doing the inner loop.  These functions provide that information.  .. c:function:: npy_intp* NpyIter_GetInnerStrideArray(NpyIter* iter)      Returns a pointer to an array of the ``nop`strides,     one for each iterated object, to be used by the inner loop.      This pointer may be cached before the iteration loop, calling`iternext`will not change it. This function may be safely     called without holding the Python GIL.      **WARNING**: While the pointer may be cached, its values may     change if the iterator is buffered.  .. c:function:: npy_intp* NpyIter_GetInnerLoopSizePtr(NpyIter* iter)      Returns a pointer to the number of iterations the     inner loop should execute.      This address may be cached before the iteration loop, calling`iternext``will not change it.  The value itself may change during     iteration, in particular if buffering is enabled.  This function     may be safely called without holding the Python GIL.  .. c:function:: void NpyIter_GetInnerFixedStrideArray( \         NpyIter* iter, npy_intp* out_strides)      Gets an array of strides which are fixed, or will not change during     the entire iteration.  For strides that may change, the value     NPY_MAX_INTP is placed in the stride.      Once the iterator is prepared for iteration (after a reset if     :c`NPY_ITER_DELAY_BUFALLOC` was used), call this to get the strides     which may be used to select a fast inner loop function.  For example,     if the stride is 0, that means the inner loop can always load its     value into a variable once, then use the variable throughout the loop,     or if the stride equals the itemsize, a contiguous version for that     operand may be used.      This function may be safely called without holding the Python GIL.  .. index::     pair: iterator; C-API  Converting from previous NumPy iterators ----------------------------------------  The old iterator API includes functions like PyArrayIter_Check, PyArray_Iter* and PyArray_ITER_*.  The multi-iterator array includes PyArray_MultiIter*, PyArray_Broadcast, and PyArray_RemoveSmallest.  The new iterator design replaces all of this functionality with a single object and associated API.  One goal of the new API is that all uses of the existing iterator should be replaceable with the new iterator without significant effort. In 1.6, the major exception to this is the neighborhood iterator, which does not have corresponding features in this iterator.  Here is a conversion table for which functions to use with the new iterator:  =====================================  =================================================== *Iterator Functions* :c`PyArray_IterNew`              :c`NpyIter_New` :c`PyArray_IterAllButAxis`       :c`NpyIter_New` +``axes``parameter **or**                                        Iterator flag :c`NPY_ITER_EXTERNAL_LOOP` :c`PyArray_BroadcastToShape`     **NOT SUPPORTED** (Use the support for                                        multiple operands instead.) :c`PyArrayIter_Check`            Will need to add this in Python exposure :c`PyArray_ITER_RESET`           :c`NpyIter_Reset` :c`PyArray_ITER_NEXT`            Function pointer from :c`NpyIter_GetIterNext` :c`PyArray_ITER_DATA`            :c`NpyIter_GetDataPtrArray` :c`PyArray_ITER_GOTO`            :c`NpyIter_GotoMultiIndex` :c`PyArray_ITER_GOTO1D`          :c`NpyIter_GotoIndex` or                                        :c`NpyIter_GotoIterIndex` :c`PyArray_ITER_NOTDONE`         Return value of``iternext``function pointer *Multi-iterator Functions* :c`PyArray_MultiIterNew`         :c`NpyIter_MultiNew` :c`PyArray_MultiIter_RESET`      :c`NpyIter_Reset` :c`PyArray_MultiIter_NEXT`       Function pointer from :c`NpyIter_GetIterNext` :c`PyArray_MultiIter_DATA`       :c`NpyIter_GetDataPtrArray` :c`PyArray_MultiIter_NEXTi`      **NOT SUPPORTED** (always lock-step iteration) :c`PyArray_MultiIter_GOTO`       :c`NpyIter_GotoMultiIndex` :c`PyArray_MultiIter_GOTO1D`     :c`NpyIter_GotoIndex` or                                        :c`NpyIter_GotoIterIndex` :c`PyArray_MultiIter_NOTDONE`    Return value of``iternext\`<span class="title-ref"> function pointer :c\`PyArray\_Broadcast</span> Handled by :c\`NpyIter\_MultiNew\` :c\`PyArray\_RemoveSmallest\` Iterator flag :c\`NPY\_ITER\_EXTERNAL\_LOOP\` *Other Functions* :c\`PyArray\_ConvertToCommonType\` Iterator flag :c\`NPY\_ITER\_COMMON\_DTYPE\` ===================================== ===================================================

---

strings.md

---

# NpyString API

<div class="sectionauthor">

Nathan Goldbaum

</div>

<div class="versionadded">

2.0

</div>

This API allows access to the UTF-8 string data stored in NumPy StringDType arrays. See \[NEP-55 \<NEP55\>\](\#nep-55-\<nep55\>) for more in-depth details into the design of StringDType.

## Examples

### Loading a String

Say we are writing a ufunc implementation for `StringDType`. If we are given `const char *buf` pointer to the beginning of a `StringDType` array entry, and a `PyArray_Descr *` pointer to the array descriptor, one can access the underlying string data like so:

`` `C    npy_string_allocator *allocator = NpyString_acquire_allocator(            (PyArray_StringDTypeObject *)descr);     npy_static_string sdata = {0, NULL};    npy_packed_static_string *packed_string = (npy_packed_static_string *)buf;    int is_null = 0;     is_null = NpyString_load(allocator, packed_string, &sdata);     if (is_null == -1) {        // failed to load string, set error        return -1;    }    else if (is_null) {        // handle missing string        // sdata->buf is NULL        // sdata->size is 0    }    else {        // sdata->buf is a pointer to the beginning of a string        // sdata->size is the size of the string    }    NpyString_release_allocator(allocator);  Packing a String ``\` ^^^^^^^^^^^^^^^^

This example shows how to pack a new string entry into an array:

`` `C    char *str = "Hello world";    size_t size = 11;    npy_packed_static_string *packed_string = (npy_packed_static_string *)buf;     npy_string_allocator *allocator = NpyString_acquire_allocator(            (PyArray_StringDTypeObject *)descr);     // copy contents of str into packed_string    if (NpyString_pack(allocator, packed_string, str, size) == -1) {        // string packing failed, set error        return -1;    }     // packed_string contains a copy of "Hello world"     NpyString_release_allocator(allocator);  Types ``\` -----

Functions `` ` ---------  .. c:function:: npy_string_allocator *NpyString_acquire_allocator( \         const PyArray_StringDTypeObject *descr)       Acquire the mutex locking the allocator attached to ``descr`.`NpyString\_release\_allocator`must be called on the allocator      returned by this function exactly once. Note that functions requiring the      GIL should not be called while the allocator mutex is held, as doing so may      cause deadlocks.  .. c:function:: void NpyString_acquire_allocators( \         size_t n_descriptors, PyArray_Descr *const descrs[], \         npy_string_allocator *allocators[])       Simultaneously acquire the mutexes locking the allocators attached to      multiple descriptors. Writes a pointer to the associated allocator in the      allocators array for each StringDType descriptor in the array. If any of      the descriptors are not StringDType instances, write NULL to the allocators      array for that entry.`n\_descriptors`is the number of descriptors in the descrs array that      should be examined. Any descriptor after`n\_descriptors`elements is      ignored. A buffer overflow will happen if the`descrs`array does not      contain n_descriptors elements.       If pointers to the same descriptor are passed multiple times, only acquires      the allocator mutex once but sets identical allocator pointers appropriately.      The allocator mutexes must be released after this function returns, see`NpyString\_release\_allocators`.       Note that functions requiring the GIL should not be called while the      allocator mutex is held, as doing so may cause deadlocks.  .. c:function:: void NpyString_release_allocator( \         npy_string_allocator *allocator)       Release the mutex locking an allocator. This must be called exactly once      after acquiring the allocator mutex and all operations requiring the      allocator are done.       If you need to release multiple allocators, see      NpyString_release_allocators, which can correctly handle releasing the      allocator once when given several references to the same allocator.  .. c:function:: void NpyString_release_allocators( \         size_t length, npy_string_allocator *allocators[])       Release the mutexes locking N allocators.`length`is the length of the      allocators array. NULL entries are ignored.       If pointers to the same allocator are passed multiple times, only releases      the allocator mutex once.  .. c:function:: int NpyString_load(npy_string_allocator *allocator, \                const npy_packed_static_string *packed_string, \                npy_static_string *unpacked_string)       Extract the packed contents of`packed\_string`into`unpacked\_string`.       The`unpacked\_string`is a read-only view onto the`packed\_string`data      and should not be used to modify the string data. If`packed\_string`is      the null string, sets`unpacked\_string.buf`to the NULL      pointer. Returns -1 if unpacking the string fails, returns 1 if`packed\_string`is the null string, and returns 0 otherwise.       A useful pattern is to define a stack-allocated npy_static_string instance      initialized to`{0, NULL}`and pass a pointer to the stack-allocated      unpacked string to this function.  This function can be used to      simultaneously unpack a string and determine if it is a null string.  .. c:function:: int NpyString_pack_null( \         npy_string_allocator *allocator, \         npy_packed_static_string *packed_string)     Pack the null string into`packed\_string`. Returns 0 on success and -1 on    failure.  .. c:function:: int NpyString_pack( \         npy_string_allocator *allocator, \         npy_packed_static_string *packed_string, \         const char *buf, \         size_t size)     Copy and pack the first`size`entries of the buffer pointed to by`buf`into the`packed\_string\`\`. Returns 0 on success and -1 on failure.

---

types-and-structures.md

---

# Python types and C-structures

<div class="sectionauthor">

Travis E. Oliphant

</div>

Several new types are defined in the C-code. Most of these are accessible from Python, but a few are not exposed due to their limited use. Every new Python type has an associated :c`PyObject *` with an internal structure that includes a pointer to a "method table" that defines how the new object behaves in Python. When you receive a Python object into C code, you always get a pointer to a :c`PyObject` structure. Because a :c`PyObject` structure is very generic and defines only :c`PyObject_HEAD`, by itself it is not very interesting. However, different objects contain more details after the :c`PyObject_HEAD` (but you have to cast to the correct type to access them --- or use accessor functions or macros).

## New Python types defined

Python types are the functional equivalent in C of classes in Python. By constructing a new Python type you make available a new object for Python. The ndarray object is an example of a new type defined in C. New types are defined in C by two basic steps:

1.  creating a C-structure (usually named `Py{Name}Object`) that is binary- compatible with the :c`PyObject` structure itself but holds the additional information needed for that particular object;
2.  populating the :c`PyTypeObject` table (pointed to by the ob\_type member of the :c`PyObject` structure) with pointers to functions that implement the desired behavior for the type.

Instead of special method names which define behavior for Python classes, there are "function tables" which point to functions that implement the desired results. Since Python 2.2, the PyTypeObject itself has become dynamic which allows C types that can be "sub-typed "from other C-types in C, and sub-classed in Python. The children types inherit the attributes and methods from their parent(s).

There are two major new types: the ndarray ( :c\`PyArray\_Type\` ) and the ufunc ( :c\`PyUFunc\_Type\` ). Additional types play a supportive role: the :c\`PyArrayIter\_Type\`, the :c\`PyArrayMultiIter\_Type\`, and the :c\`PyArrayDescr\_Type\` . The :c\`PyArrayIter\_Type\` is the type for a flat iterator for an ndarray (the object that is returned when getting the flat attribute). The :c\`PyArrayMultiIter\_Type\` is the type of the object returned when calling `broadcast`. It handles iteration and broadcasting over a collection of nested sequences. Also, the :c\`PyArrayDescr\_Type\` is the data-type-descriptor type whose instances describe the data and :c\`PyArray\_DTypeMeta\` is the metaclass for data-type descriptors. There are also new scalar-array types which are new Python scalars corresponding to each of the fundamental data types available for arrays. Additional types are placeholders that allow the array scalars to fit into a hierarchy of actual Python types. Finally, the :c\`PyArray\_DTypeMeta\` instances corresponding to the NumPy built-in data types are also publicly visible.

### PyArray\_Type and PyArrayObject

  - \>  
    Further members are considered private and version dependent. If the size of the struct is important for your code, special care must be taken. A possible use-case when this is relevant is subclassing in C. If your code relies on `sizeof(PyArrayObject)` to be constant, you must add the following check at import time:
    
    ``` c
    if (sizeof(PyArrayObject) < PyArray_Type.tp_basicsize) {
        PyErr_SetString(PyExc_ImportError,
           "Binary incompatibility with NumPy, must recompile/update X.");
        return NULL;
    }
    ```
    
    To ensure that your code does not have to be compiled for a specific NumPy version, you may add a constant, leaving room for changes in NumPy. A solution guaranteed to be compatible with any future NumPy version requires the use of a runtime calculate offset and allocation size.

The :c\`PyArray\_Type\` typeobject implements many of the features of `` ` :c:type:`Python objects <PyTypeObject>` including the :c:member:`tp_as_number <PyTypeObject.tp_as_number>`, :c:member:`tp_as_sequence <PyTypeObject.tp_as_sequence>`, :c:member:`tp_as_mapping <PyTypeObject.tp_as_mapping>`, and :c:member:`tp_as_buffer <PyTypeObject.tp_as_buffer>` interfaces. The :c:type:`rich comparison <richcmpfunc>`) is also used along with new-style attribute lookup for member (:c:member:`tp_members <PyTypeObject.tp_members>`) and properties (:c:member:`tp_getset <PyTypeObject.tp_getset>`). The :c`PyArray_Type` can also be sub-typed.  > **Tip** >      The :c:member:`tp_as_number <PyTypeObject.tp_as_number>` methods use     a generic approach to call whatever function has been registered for     handling the operation. When the ``\_multiarray\_umath``module is imported,     it sets the numeric operations for all arrays to the corresponding ufuncs.     This choice can be changed with :c`PyUFunc_ReplaceLoopBySignature`.  PyGenericArrType_Type ---------------------  .. c:var:: PyTypeObject PyGenericArrType_Type     The :c`PyGenericArrType_Type` is the PyTypeObject definition which    create the `numpy.generic` python type.   PyArrayDescr_Type and PyArray_Descr -----------------------------------  .. c:var:: PyTypeObject PyArrayDescr_Type     The :c`PyArrayDescr_Type` is the built-in type of the    data-type-descriptor objects used to describe how the bytes comprising    the array are to be interpreted.  There are 21 statically-defined    :c:type:`PyArray_Descr` objects for the built-in data-types. While these    participate in reference counting, their reference count should never    reach zero.  There is also a dynamic table of user-defined    :c:type:`PyArray_Descr` objects that is also maintained. Once a    data-type-descriptor object is "registered" it should never be    deallocated either. The function :c`PyArray_DescrFromType` (...) can    be used to retrieve a :c:type:`PyArray_Descr` object from an enumerated    type-number (either built-in or user- defined).  .. c:type:: PyArray_DescrProto     Identical structure to :c:type:`PyArray_Descr`. This struct is used    for static definition of a prototype for registering a new legacy    DType by :c`PyArray_RegisterDataType`.     See the note in :c`PyArray_RegisterDataType` for details.  .. c:type:: PyArray_Descr     The :c:type:`PyArray_Descr` structure lies at the heart of the    :c`PyArrayDescr_Type`. While it is described here for    completeness, it should be considered internal to NumPy and manipulated via``[PyArrayDescr]()*\`\` or \`\`PyDataType*`functions and macros. The size of this    structure is subject to change across versions of NumPy. To ensure    compatibility:     - Never declare a non-pointer instance of the struct    - Never perform pointer arithmetic    - Never use`sizeof(PyArray\_Descr)`It has the following structure:`\`c typedef struct { PyObject\_HEAD PyTypeObject *typeobj; char kind; char type; char byteorder; char \_former\_flags; // unused field int type\_num; /* \* Definitions after this one must be accessed through accessor \* functions (see below) when compiling with NumPy 1.x support. */ npy\_uint64 flags; npy\_intp elsize; npy\_intp alignment; NpyAuxData*c\_metadata; npy\_hash\_t hash; void \*reserved\_null\[2\]; // unused field, must be NULLed. } PyArray\_Descr;

> Some dtypes have additional members which are accessible through <span class="title-ref">PyDataType\_NAMES</span>, <span class="title-ref">PyDataType\_FIELDS</span>, <span class="title-ref">PyDataType\_SUBARRAY</span>, and in some cases (times) <span class="title-ref">PyDataType\_C\_METADATA</span>.

<div id="arrfuncs-type">

PyArray\_ArrFuncs `` ` ----------------  .. c:function:: PyArray_ArrFuncs *PyDataType_GetArrFuncs(PyArray_Descr *dtype)      Fetch the legacy `PyArray_ArrFuncs` of the datatype (cannot fail).      .. versionadded:: NumPy 2.0         This function was added in a backwards compatible and backportable         way in NumPy 2.0 (see ``npy\_2\_compat.h`).         Any code that previously accessed the`-\>f`slot of the`PyArray\_Descr`, must now use this function and backport it to         compile with 1.x.         (The`npy\_2\_compat.h`header can be vendored for this purpose.)  .. c:type:: PyArray_ArrFuncs      Functions implementing internal features. Not all of these     function pointers must be defined for a given type. The required     members are`nonzero`,`copyswap`,`copyswapn`,`setitem`,`getitem`, and`cast`. These are assumed to be non-`NULL`and`NULL`entries will cause a program crash. The other     functions may be`NULL`which will just mean reduced     functionality for that data-type. (Also, the nonzero function will     be filled in with a default function if it is`NULL`when you     register a user-defined data-type).`\`c typedef struct { PyArray\_VectorUnaryFunc *cast\[NPY\_NTYPES\_LEGACY\]; PyArray\_GetItemFunc*getitem; PyArray\_SetItemFunc *setitem; PyArray\_CopySwapNFunc*copyswapn; PyArray\_CopySwapFunc *copyswap; PyArray\_CompareFunc*compare; PyArray\_ArgFunc *argmax; PyArray\_DotFunc*dotfunc; PyArray\_ScanFunc *scanfunc; PyArray\_FromStrFunc*fromstr; PyArray\_NonzeroFunc *nonzero; PyArray\_FillFunc*fill; PyArray\_FillWithScalarFunc *fillwithscalar; PyArray\_SortFunc*sort\[NPY\_NSORTS\]; PyArray\_ArgSortFunc *argsort\[NPY\_NSORTS\]; PyObject*castdict; PyArray\_ScalarKindFunc *scalarkind; intcancastscalarkindto; int*cancastto; void *\_unused1; void*\_unused2; void *\_unused3; PyArray\_ArgFunc*argmin; } PyArray\_ArrFuncs;

</div>

> The concept of a behaved segment is used in the description of the function pointers. A behaved segment is one that is aligned and in native machine byte-order for the data-type. The `nonzero`, `copyswap`, `copyswapn`, `getitem`, and `setitem` functions can (and must) deal with mis-behaved arrays. The other functions require behaved memory segments.
> 
> <div class="note">
> 
> <div class="title">
> 
> Note
> 
> </div>
> 
> The functions are largely legacy API, however, some are still used. As of NumPy 2.x they are only available via <span class="title-ref">PyDataType\_GetArrFuncs</span> (see the function for more details). Before using any function defined in the struct you should check whether it is `NULL`. In general, the functions `getitem`, `setitem`, `copyswap`, and `copyswapn` can be expected to be defined, but all functions are expected to be replaced with newer API. For example, `PyArray_Pack` is a more powerful version of `setitem` that for example correctly deals with casts.
> 
> </div>

<div id="arraymethod-structs">

PyArrayMethod\_Context and PyArrayMethod\_Spec `` ` --------------------------------------------  .. c:type:: PyArrayMethodObject_tag     An opaque struct used to represent the method "self" in ArrayMethod loops.  .. c:type:: PyArrayMethod_Context     A struct that is passed in to ArrayMethod loops to provide context for the    runtime usage of the loop. ``\`c typedef struct { PyObject *caller; struct PyArrayMethodObject\_tag*method; PyArray\_Descr *const*descriptors; } PyArrayMethod\_Context

</div>

> 

<div id="dtypemeta">

PyArray\_DTypeMeta and PyArrayDTypeMeta\_Spec `` ` -------------------------------------------  .. c:var:: PyTypeObject PyArrayDTypeMeta_Type     The python type object corresponding to :c:type:`PyArray_DTypeMeta`.  .. c:type:: PyArray_DTypeMeta     A largely opaque struct representing DType classes. Each instance defines a    metaclass for a single NumPy data type. Data types can either be    non-parametric or parametric. For non-parametric types, the DType class has    a one-to-one correspondence with the descriptor instance created from the    DType class. Parametric types can correspond to many different dtype    instances depending on the chosen parameters. This type is available in the    public ``numpy/dtype\_api.h`header. Currently use of this struct is not    supported in the limited CPython API, so if`Py\_LIMITED\_API`is set, this    type is a typedef for`PyTypeObject`.`\`c typedef struct { PyHeapTypeObject super; PyArray\_Descr *singleton; int type\_num; PyTypeObject*scalar\_type; npy\_uint64 flags; void *dt\_slots; void*reserved\[3\]; } PyArray\_DTypeMeta

</div>

> 

Exposed DTypes classes (`PyArray_DTypeMeta` objects) `` ` ------------------------------------------------------  For use with promoters, NumPy exposes a number of Dtypes following the pattern ``[PyArray]()\<Name\>DType``corresponding to those found in `np.dtypes`.  Additionally, the three DTypes,``PyArray\_PyLongDType`,`PyArray\_PyFloatDType`,`PyArray\_PyComplexDType`correspond to the Python scalar values.  These cannot be used in all places, but do allow for example the common dtype operation and implementing promotion with them may be necessary.  Further, the following abstract DTypes are defined which cover both the builtin NumPy ones and the python ones, and users can in principle subclass from them (this does not inherit any DType specific functionality): *`PyArray\_IntAbstractDType`*`PyArray\_FloatAbstractDType`*`PyArray\_ComplexAbstractDType``.. warning::     As of NumPy 2.0, the *only* valid use for these DTypes is registering a     promoter conveniently to e.g. match "any integers" (and subclass checks).     Because of this, they are not exposed to Python.   PyUFunc_Type and PyUFuncObject ------------------------------  .. c:var:: PyTypeObject PyUFunc_Type     The ufunc object is implemented by creation of the    :c`PyUFunc_Type`. It is a very simple type that implements only    basic getattribute behavior, printing behavior, and has call    behavior which allows these objects to act like functions. The    basic idea behind the ufunc is to hold a reference to fast    1-dimensional (vector) loops for each data type that supports the    operation. These one-dimensional loops all have the same signature    and are the key to creating a new ufunc. They are called by the    generic looping code as appropriate to implement the N-dimensional    function. There are also some generic 1-d loops defined for    floating and complexfloating arrays that allow you to define a    ufunc using a single scalar function (*e.g.* atanh).   .. c:type:: PyUFuncObject     The core of the ufunc is the :c:type:`PyUFuncObject` which contains all    the information needed to call the underlying C-code loops that    perform the actual work. While it is described here for completeness, it    should be considered internal to NumPy and manipulated via``[PyUFunc]()\*`functions. The size of this structure is subject to change across versions    of NumPy. To ensure compatibility:     - Never declare a non-pointer instance of the struct    - Never perform pointer arithmetic    - Never use`sizeof(PyUFuncObject)`It has the following structure:`\`c typedef struct { PyObject\_HEAD int nin; int nout; int nargs; int identity; PyUFuncGenericFunction *functions; voiddata; int ntypes; int reserved1; const char*name; char *types; const char*doc; void *ptr; PyObject*obj; PyObject *userloops; int core\_enabled; int core\_num\_dim\_ix; int*core\_num\_dims; int *core\_dim\_ixs; int*core\_offsets; char *core\_signature; PyUFunc\_TypeResolutionFunc*type\_resolver; void *reserved2; void*reserved3; npy\_uint32 *op\_flags; npy\_uint32*iter\_flags; /\* new in API version 0x0000000D */ npy\_intp*core\_dim\_sizes; npy\_uint32 *core\_dim\_flags; PyObject*identity\_value; /\* Further private slots (size depends on the NumPy version) \*/ } PyUFuncObject;

> Added in API version 0x0000000D

PyArrayIter\_Type and PyArrayIterObject `` ` --------------------------------------  .. c:var:: PyTypeObject PyArrayIter_Type     This is an iterator object that makes it easy to loop over an    N-dimensional array. It is the object returned from the flat    attribute of an ndarray. It is also used extensively throughout the    implementation internals to loop over an N-dimensional array. The    tp_as_mapping interface is implemented so that the iterator object    can be indexed (using 1-d indexing), and a few methods are    implemented through the tp_methods table. This object implements the    next method and can be used anywhere an iterator can be used in    Python.  .. c:type:: PyArrayIterObject     The C-structure corresponding to an object of :c`PyArrayIter_Type` is    the :c:type:`PyArrayIterObject`. The :c:type:`PyArrayIterObject` is used to    keep track of a pointer into an N-dimensional array. It contains associated    information used to quickly march through the array. The pointer can    be adjusted in three basic ways: 1) advance to the "next" position in    the array in a C-style contiguous fashion, 2) advance to an arbitrary    N-dimensional coordinate in the array, and 3) advance to an arbitrary    one-dimensional index into the array. The members of the    :c:type:`PyArrayIterObject` structure are used in these    calculations. Iterator objects keep their own dimension and strides    information about an array. This can be adjusted as needed for    "broadcasting," or to loop over only specific dimensions. ``\`c typedef struct { PyObject\_HEAD int nd\_m1; npy\_intp index; npy\_intp size; npy\_intp coordinates\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp dims\_m1\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp strides\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp backstrides\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp factors\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; PyArrayObject *ao; char*dataptr; npy\_bool contiguous; } PyArrayIterObject;

> 

How to use an array iterator on a C-level is explained more fully in `` ` later sections. Typically, you do not need to concern yourself with the internal structure of the iterator object, and merely interact with it through the use of the macros :c`PyArray_ITER_NEXT` (it), :c`PyArray_ITER_GOTO` (it, dest), or :c`PyArray_ITER_GOTO1D` (it, index). All of these macros require the argument *it* to be a :c:expr:`PyArrayIterObject *`.   PyArrayMultiIter_Type and PyArrayMultiIterObject ------------------------------------------------  .. c:var:: PyTypeObject PyArrayMultiIter_Type     This type provides an iterator that encapsulates the concept of    broadcasting. It allows :math:`N` arrays to be broadcast together    so that the loop progresses in C-style contiguous fashion over the    broadcasted array. The corresponding C-structure is the    :c:type:`PyArrayMultiIterObject` whose memory layout must begin any    object, *obj*, passed in to the :c`PyArray_Broadcast` (obj)    function. Broadcasting is performed by adjusting array iterators so    that each iterator represents the broadcasted shape and size, but    has its strides adjusted so that the correct element from the array    is used at each iteration.   .. c:type:: PyArrayMultiIterObject ``\`c typedef struct { PyObject\_HEAD int numiter; npy\_intp size; npy\_intp index; int nd; npy\_intp dimensions\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; PyArrayIterObject \*iters\[\]; } PyArrayMultiIterObject;

> 

PyArrayNeighborhoodIter\_Type and PyArrayNeighborhoodIterObject `` ` --------------------------------------------------------------  .. c:var:: PyTypeObject PyArrayNeighborhoodIter_Type     This is an iterator object that makes it easy to loop over an    N-dimensional neighborhood.  .. c:type:: PyArrayNeighborhoodIterObject     The C-structure corresponding to an object of    :c`PyArrayNeighborhoodIter_Type` is the    :c:type:`PyArrayNeighborhoodIterObject`. ``\`c typedef struct { PyObject\_HEAD int nd\_m1; npy\_intp index, size; npy\_intp coordinates\[NPY\_MAXDIMS\_LEGACY\_ITERS\] npy\_intp dims\_m1\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp strides\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp backstrides\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_intp factors\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; PyArrayObject *ao; char*dataptr; npy\_bool contiguous; npy\_intp bounds\[NPY\_MAXDIMS\_LEGACY\_ITERS\]\[2\]; npy\_intp limits\[NPY\_MAXDIMS\_LEGACY\_ITERS\]\[2\]; npy\_intp limits\_sizes\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; npy\_iter\_get\_dataptr\_t translate; npy\_intp nd; npy\_intp dimensions\[NPY\_MAXDIMS\_LEGACY\_ITERS\]; PyArrayIterObject\* \_internal\_iter; char\* constant; int mode; } PyArrayNeighborhoodIterObject;

ScalarArrayTypes `` ` ----------------  There is a Python type for each of the different built-in data types that can be present in the array. Most of these are simple wrappers around the corresponding data type in C. The C-names for these types are ``Py{TYPE}ArrType\_Type`where`{TYPE}`can be      **Bool**, **Byte**, **Short**, **Int**, **Long**, **LongLong**,     **UByte**, **UShort**, **UInt**, **ULong**, **ULongLong**,     **Half**, **Float**, **Double**, **LongDouble**, **CFloat**,     **CDouble**, **CLongDouble**, **String**, **Unicode**, **Void**,     **Datetime**, **Timedelta**, and **Object**.  These type names are part of the C-API and can therefore be created in extension C-code. There is also a`PyIntpArrType\_Type`and a`PyUIntpArrType\_Type``that are simple substitutes for one of the integer types that can hold a pointer on the platform. The structure of these scalar objects is not exposed to C-code. The function :c`PyArray_ScalarAsCtype` (..) can be used to extract the C-type value from the array scalar and the function :c`PyArray_Scalar` (...) can be used to construct an array scalar from a C-value.   Other C-structures ==================  A few new C-structures were found to be useful in the development of NumPy. These C-structures are used in at least one C-API call and are therefore documented here. The main reason these structures were defined is to make it easy to use the Python ParseTuple C-API to convert from Python objects to a useful C-Object.   PyArray_Dims ------------  .. c:type:: PyArray_Dims     This structure is very useful when shape and/or strides information    is supposed to be interpreted. The structure is:``\`c typedef struct { npy\_intp \*ptr; int len; } PyArray\_Dims;

> The members of this structure are

PyArray\_Chunk `` ` -------------  .. c:type:: PyArray_Chunk     This is equivalent to the buffer object structure in Python up to    the ptr member. On 32-bit platforms (*i.e.* if :c`NPY_SIZEOF_INT`    == :c`NPY_SIZEOF_INTP`), the len member also matches an equivalent    member of the buffer object. It is useful to represent a generic    single-segment chunk of memory. ``\`c typedef struct { PyObject\_HEAD PyObject *base; void*ptr; npy\_intp len; int flags; } PyArray\_Chunk;

> The members are

PyArrayInterface `` ` ----------------  .. seealso:: [arrays.interface](#arrays.interface)  .. c:type:: PyArrayInterface     The :c:type:`PyArrayInterface` structure is defined so that NumPy and    other extension modules can use the rapid array interface    protocol. The :obj:`~object.__array_struct__` method of an object that    supports the rapid array interface protocol should return a    :c:type:`PyCapsule` that contains a pointer to a :c:type:`PyArrayInterface`    structure with the relevant details of the array. After the new    array is created, the attribute should be ``DECREF``'d which will    free the :c:type:`PyArrayInterface` structure. Remember to``INCREF``the    object (whose :obj:`~object.__array_struct__` attribute was retrieved) and    point the base member of the new :c:type:`PyArrayObject` to this same    object. In this way the memory for the array will be managed    correctly.``\`c typedef struct { int two; int nd; char typekind; int itemsize; int flags; npy\_intp *shape; npy\_intp*strides; void *data; PyObject*descr; } PyArrayInterface;

> 

Internally used structures `` ` --------------------------  Internally, the code uses some additional Python objects primarily for memory management. These types are not accessible directly from Python, and are not exposed to the C-API. They are included here only for completeness and assistance in understanding the code.  .. c:type:: PyUFunc_Loop1d     A simple linked-list of C-structures containing the information needed    to define a 1-d loop for a ufunc for every defined signature of a    user-defined data-type.  .. c:var:: PyTypeObject PyArrayMapIter_Type     Advanced indexing is handled with this Python type. It is simply a    loose wrapper around the C-structure containing the variables    needed for advanced array indexing.  .. c:type:: PyArrayMapIterObject     The C-structure associated with :c:var:`PyArrayMapIter_Type`.    This structure is useful if you are trying to    understand the advanced-index mapping code. It is defined in the ``arrayobject.h`header. This type is not exposed to Python and    could be replaced with a C-structure. As a Python type it takes    advantage of reference- counted memory management.   NumPy C-API and C complex ========================= When you use the NumPy C-API, you will have access to complex real declarations`npy\_cdouble`and`npy\_cfloat`, which are declared in terms of the C standard types from`complex.h`. Unfortunately,`complex.h``contains `#define I ...`` (where the actual definition depends on the compiler), which means that any downstream user that does `#include <numpy/arrayobject.h>` could get `I` defined, and using something like declaring `double I;` in their code will result in an obscure compiler error like

``` C
error: expected â€˜)â€™ before â€˜__extension__â€™
double I,
```

This error can be avoided by adding:

    #undef I

to your code.

<div class="versionchanged">

2.0 The inclusion of `complex.h` was new in NumPy 2, so that code defining a different `I` may not have required the `#undef I` on older versions. NumPy 2.0.1 briefly included the `#under I`

</div>

---

ufunc.md

---

# ufunc API

<div class="sectionauthor">

Travis E. Oliphant

</div>

<div class="index">

pair: ufunc; C-API

</div>

## Constants

  - `UFUNC_{THING}_{ERR}`

  - `PyUFunc_{VALUE}`

## Macros

## Types

Functions `` ` ---------  .. c:function:: PyObject *PyUFunc_FromFuncAndData( \         PyUFuncGenericFunction *func, void *const *data, const char *types, \         int ntypes, int nin, int nout, int identity, const char *name, \         const char *doc, int unused)      Create a new broadcasting universal function from required variables.     Each ufunc builds around the notion of an element-by-element     operation. Each ufunc object contains pointers to 1-d loops     implementing the basic functionality for each supported type.      > **Note** >         The *func*, *data*, *types*, *name*, and *doc* arguments are not        copied by :c`PyUFunc_FromFuncAndData`. The caller must ensure        that the memory used by these arrays is not freed as long as the        ufunc object is alive.      :param func:         Must point to an array containing *ntypes*         :c:type:`PyUFuncGenericFunction` elements.      :param data:         Should be ``NULL`or a pointer to an array of size *ntypes*.         This array may contain arbitrary extra-data to be passed to         the corresponding loop function in the func array, including`NULL`.      :param types:        Length`(nin + nout) \* ntypes`array of`char``encoding the        `numpy.dtype.num` (built-in only) that the corresponding        function in the``func`array accepts. For instance, for a comparison        ufunc with three`ntypes`, two`nin`and one`nout``, where the        first function accepts `numpy.int32` and the second        `numpy.int64`, with both returning `numpy.bool_`,``types`would        be`(char\[\]) {5, 5, 0, 7, 7, 0}`since`NPY\_INT32`is 5,`NPY\_INT64`is 7, and`NPY\_BOOL``is 0.         The bit-width names can also be used (e.g. :c`NPY_INT32`,        :c`NPY_COMPLEX128` ) if desired.         [ufuncs.casting](#ufuncs.casting) will be used at runtime to find the first``func``callable by the input/output provided.      :param ntypes:         How many different data-type-specific functions the ufunc has implemented.      :param nin:         The number of inputs to this operation.      :param nout:         The number of outputs      :param identity:          Either :c`PyUFunc_One`, :c`PyUFunc_Zero`,         :c`PyUFunc_MinusOne`, or :c`PyUFunc_None`.         This specifies what should be returned when         an empty array is passed to the reduce method of the ufunc.         The special value :c`PyUFunc_IdentityValue` may only be used with         the :c`PyUFunc_FromFuncAndDataAndSignatureAndIdentity` method, to         allow an arbitrary python object to be used as the identity.      :param name:         The name for the ufunc as a``NULL``terminated string.  Specifying         a name of 'add' or 'multiply' enables a special behavior for         integer-typed reductions when no dtype is given. If the input type is an         integer (or boolean) data type smaller than the size of the `numpy.int_`         data type, it will be internally upcast to the `numpy.int_` (or         `numpy.uint`) data type.      :param doc:         Allows passing in a documentation string to be stored with the         ufunc.  The documentation string should not contain the name         of the function or the calling signature as that will be         dynamically determined from the object and available when         accessing the **__doc__** attribute of the ufunc.      :param unused:         Unused and present for backwards compatibility of the C-API.  .. c:function:: PyObject *PyUFunc_FromFuncAndDataAndSignature( \         PyUFuncGenericFunction *func, void *const *data, const char *types, \         int ntypes, int nin, int nout, int identity, const char *name, \         const char *doc, int unused, const char *signature)     This function is very similar to PyUFunc_FromFuncAndData above, but has    an extra *signature* argument, to define a    [generalized universal functions <c-api.generalized-ufuncs>](#generalized-universal-functions-<c-api.generalized-ufuncs>).    Similarly to how ufuncs are built around an element-by-element operation,    gufuncs are around subarray-by-subarray operations, the    [signature <details-of-signature>](#signature-<details-of-signature>) defining the subarrays to operate on.     :param signature:         The signature for the new gufunc. Setting it to NULL is equivalent         to calling PyUFunc_FromFuncAndData. A copy of the string is made,         so the passed in buffer can be freed.  .. c:function:: PyObject* PyUFunc_FromFuncAndDataAndSignatureAndIdentity( \         PyUFuncGenericFunction *func, void **data, char *types, int ntypes, \         int nin, int nout, int identity, char *name, char *doc, int unused, \         char *signature, PyObject *identity_value)     This function is very similar to :c`PyUFunc_FromFuncAndDataAndSignature` above,    but has an extra *identity_value* argument, to define an arbitrary identity    for the ufunc when``identity`is passed as`PyUFunc\_IdentityValue`.     :param identity_value:         The identity for the new gufunc. Must be passed as`NULL`unless the`identity`argument is`PyUFunc\_IdentityValue``. Setting it to NULL         is equivalent to calling PyUFunc_FromFuncAndDataAndSignature.   .. c:function:: int PyUFunc_RegisterLoopForType( \         PyUFuncObject* ufunc, int usertype, PyUFuncGenericFunction function, \         int* arg_types, void* data)      This function allows the user to register a 1-d loop with an     already- created ufunc to be used whenever the ufunc is called     with any of its input arguments as the user-defined     data-type. This is needed in order to make ufuncs work with     built-in data-types. The data-type must have been previously     registered with the numpy system. The loop is passed in as     *function*. This loop can take arbitrary data which should be     passed in as *data*. The data-types the loop requires are passed     in as *arg_types* which must be a pointer to memory at least as     large as ufunc->nargs.  .. c:function:: int PyUFunc_RegisterLoopForDescr( \         PyUFuncObject* ufunc, PyArray_Descr* userdtype, \         PyUFuncGenericFunction function, PyArray_Descr** arg_dtypes, void* data)     This function behaves like PyUFunc_RegisterLoopForType above, except    that it allows the user to register a 1-d loop using PyArray_Descr    objects instead of dtype type num values. This allows a 1-d loop to be    registered for structured array data-dtypes and custom data-types    instead of scalar data-types.  .. c:function:: int PyUFunc_ReplaceLoopBySignature( \         PyUFuncObject* ufunc, PyUFuncGenericFunction newfunc, int* signature, \         PyUFuncGenericFunction* oldfunc)      Replace a 1-d loop matching the given *signature* in the     already-created *ufunc* with the new 1-d loop newfunc. Return the     old 1-d loop function in *oldfunc*. Return 0 on success and -1 on     failure. This function works only with built-in types (use     :c`PyUFunc_RegisterLoopForType` for user-defined types). A     signature is an array of data-type numbers indicating the inputs     followed by the outputs assumed by the 1-d loop.  .. c:function:: void PyUFunc_clearfperr()      Clear the IEEE error flags.   Generic functions -----------------  At the core of every ufunc is a collection of type-specific functions that defines the basic functionality for each of the supported types. These functions must evaluate the underlying function :math:`N\geq1` times. Extra-data may be passed in that may be used during the calculation. This feature allows some general functions to be used as these basic looping functions. The general function has all the code needed to point variables to the right place and set up a function call. The general function assumes that the actual function to call is passed in as the extra data and calls it with the correct values. All of these functions are suitable for placing directly in the array of functions stored in the functions member of the PyUFuncObject structure.  .. c:function:: void PyUFunc_f_f_As_d_d( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_d_d( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_f_f( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_g_g( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_F_F_As_D_D( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_F_F( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_D_D( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_G_G( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_e_e( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_e_e_As_f_f( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_e_e_As_d_d( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)      Type specific, core 1-d functions for ufuncs where each     calculation is obtained by calling a function taking one input     argument and returning one output. This function is passed in``func`. The letters correspond to dtypechar's of the supported     data types (`e`- half,`f`- float,`d`- double,`g`- long double,`F`- cfloat,`D`- cdouble,`G``- clongdouble). The argument *func* must support the same     signature. The _As_X_X variants assume ndarray's of one data type     but cast the values to use an underlying function that takes a     different data type. Thus, :c`PyUFunc_f_f_As_d_d` uses     ndarrays of data type :c`NPY_FLOAT` but calls out to a     C-function that takes double and returns double.  .. c:function:: void PyUFunc_ff_f_As_dd_d( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_ff_f( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_dd_d( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_gg_g( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_FF_F_As_DD_D( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_DD_D( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_FF_F( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_GG_G( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_ee_e( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_ee_e_As_ff_f( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_ee_e_As_dd_d( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)      Type specific, core 1-d functions for ufuncs where each     calculation is obtained by calling a function taking two input     arguments and returning one output. The underlying function to     call is passed in as *func*. The letters correspond to     dtypechar's of the specific data type supported by the     general-purpose function. The argument``func`must support the     corresponding signature. The`\_As\_XX\_X``variants assume ndarrays     of one data type but cast the values at each iteration of the loop     to use the underlying function that takes a different data type.  .. c:function:: void PyUFunc_O_O( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)  .. c:function:: void PyUFunc_OO_O( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)      One-input, one-output, and two-input, one-output core 1-d functions     for the :c`NPY_OBJECT` data type. These functions handle reference     count issues and return early on error. The actual function to call is     *func* and it must accept calls with the signature``(PyObject\*) (PyObject\*)``for :c`PyUFunc_O_O` or``(PyObject\*)(PyObject *, PyObject*)``for :c`PyUFunc_OO_O`.  .. c:function:: void PyUFunc_O_O_method( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)      This general purpose 1-d core function assumes that *func* is a string     representing a method of the input object. For each     iteration of the loop, the Python object is extracted from the array     and its *func* method is called returning the result to the output array.  .. c:function:: void PyUFunc_OO_O_method( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)      This general purpose 1-d core function assumes that *func* is a     string representing a method of the input object that takes one     argument. The first argument in *args* is the method whose function is     called, the second argument in *args* is the argument passed to the     function. The output of the function is stored in the third entry     of *args*.  .. c:function:: void PyUFunc_On_Om( \         char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)      This is the 1-d core function used by the dynamic ufuncs created     by umath.frompyfunc(function, nin, nout). In this case *func* is a     pointer to a :c:type:`PyUFunc_PyFuncData` structure which has definition      .. c:type:: PyUFunc_PyFuncData``\`c typedef struct { int nin; int nout; PyObject \*callable; } PyUFunc\_PyFuncData;

> At each iteration of the loop, the *nin* input objects are extracted from their object arrays and placed into an argument tuple, the Python *callable* is called with the input arguments, and the nout outputs are placed into their object arrays.

Importing the API `` ` -----------------  .. c:macro:: PY_UFUNC_UNIQUE_SYMBOL  .. c:macro:: NO_IMPORT_UFUNC  .. c:function:: int PyUFunc_ImportUFuncAPI(void)      Ensures that the UFunc C-API is imported and usable.  It returns ``0`on success and`-1`with an error set if NumPy couldn't be imported.     While preferable to call it once at module initialization, this function     is very light-weight if called multiple times.      .. versionadded:: 2.0         This function mainly checks for`PyUFunc\_API == NULL`so it can be         manually backported if desired.  .. c:macro:: import_ufunc(void)      These are the constants and functions for accessing the ufunc     C-API from extension modules in precisely the same way as the     array C-API can be accessed. The`import\_ufunc\`<span class="title-ref"> () function must always be called (in the initialization subroutine of the extension module). If your extension module is in one file then that is all that is required. The other two constants are useful if your extension module makes use of multiple files. In that case, define :c\`PY\_UFUNC\_UNIQUE\_SYMBOL</span> to something unique to your code and then in source files that do not contain the module initialization function but still need access to the UFUNC API, define :c\`PY\_UFUNC\_UNIQUE\_SYMBOL\` to the same name used previously and also define :c\`NO\_IMPORT\_UFUNC\`.

> The C-API is actually an array of function pointers. This array is created (and pointed to by a global variable) by import\_ufunc. The global variable is either statically defined or allowed to be seen by other files depending on the state of :c\`PY\_UFUNC\_UNIQUE\_SYMBOL\` and :c\`NO\_IMPORT\_UFUNC\`.

<div class="index">

pair: ufunc; C-API

</div>

---

constants.md

---

<div class="currentmodule">

numpy

</div>

# Constants

NumPy includes several constants:

<div class="data">

e

Euler's constant, base of natural logarithms, Napier's constant.

`e = 2.71828182845904523536028747135266249775724709369995...`

**See Also**

exp : Exponential function log : Natural logarithm

**References**

<https://en.wikipedia.org/wiki/E_%28mathematical_constant%29>

</div>

<div class="data">

euler\_gamma

`Î³ = 0.5772156649015328606065120900824024310421...`

**References**

<https://en.wikipedia.org/wiki/Euler%27s_constant>

</div>

<div class="data">

inf

IEEE 754 floating point representation of (positive) infinity.

**Returns**

  - y : float  
    A floating point representation of positive infinity.

**See Also**

isinf : Shows which elements are positive or negative infinity

isposinf : Shows which elements are positive infinity

isneginf : Shows which elements are negative infinity

isnan : Shows which elements are Not a Number

isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)

**Notes**

NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Also that positive infinity is not equivalent to negative infinity. But infinity is equivalent to positive infinity.

**Examples**

\>\>\> import numpy as np \>\>\> np.inf inf \>\>\> np.array(\[1\]) / 0. array(\[inf\])

</div>

<div class="data">

nan

IEEE 754 floating point representation of Not a Number (NaN).

**Returns**

y : A floating point representation of Not a Number.

**See Also**

isnan : Shows which elements are Not a Number.

isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)

**Notes**

NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity.

**Examples**

\>\>\> import numpy as np \>\>\> np.nan nan \>\>\> np.log(-1) np.float64(nan) \>\>\> np.log(\[-1, 1, 2\]) array(\[ nan, 0. , 0.69314718\])

</div>

<div class="data">

newaxis

A convenient alias for None, useful for indexing arrays.

**Examples**

\>\>\> import numpy as np \>\>\> np.newaxis is None True \>\>\> x = np.arange(3) \>\>\> x array(\[0, 1, 2\]) \>\>\> x\[:, np.newaxis\] array(\[\[0\], \[1\], \[2\]\]) \>\>\> x\[:, np.newaxis, np.newaxis\] array(\[\[\[0\]\], \[\[1\]\], \[\[2\]\]\]) \>\>\> x\[:, np.newaxis\] \* x array(\[\[0, 0, 0\], \[0, 1, 2\], \[0, 2, 4\]\])

Outer product, same as `outer(x, y)`:

\>\>\> y = np.arange(3, 6) \>\>\> x\[:, np.newaxis\] \* y array(\[\[ 0, 0, 0\], \[ 3, 4, 5\], \[ 6, 8, 10\]\])

`x[np.newaxis, :]` is equivalent to `x[np.newaxis]` and `x[None]`:

\>\>\> x\[np.newaxis, :\].shape (1, 3) \>\>\> x\[np.newaxis\].shape (1, 3) \>\>\> x\[None\].shape (1, 3) \>\>\> x\[:, np.newaxis\].shape (3, 1)

</div>

<div class="data">

pi

`pi = 3.1415926535897932384626433...`

**References**

<https://en.wikipedia.org/wiki/Pi>

</div>

---

misc_util.md

---

# distutils.misc\_util

<div class="automodule" data-members="" data-undoc-members="" data-exclude-members="Configuration">

numpy.distutils.misc\_util

</div>

---

distutils.md

---

# Packaging (`numpy.distutils`)

<div class="module">

numpy.distutils

</div>

\> **Warning** \> `numpy.distutils` is deprecated, and will be removed for Python \>= 3.12. For more details, see \[distutils-status-migration\](\#distutils-status-migration)

<div class="warning">

<div class="title">

Warning

</div>

Note that `setuptools` does major releases often and those may contain changes that break `numpy.distutils`, which will *not* be updated anymore for new `setuptools` versions. It is therefore recommended to set an upper version bound in your build configuration for the last known version of `setuptools` that works with your build.

</div>

NumPy provides enhanced distutils functionality to make it easier to build and install sub-packages, auto-generate code, and extension modules that use Fortran-compiled libraries. A useful <span class="title-ref">Configuration \<misc\_util.Configuration\></span> class is also provided in `numpy.distutils.misc_util` that can make it easier to construct keyword arguments to pass to the setup function (by passing the dictionary obtained from the todict() method of the class). More information is available in the \[distutils-user-guide\](\#distutils-user-guide).

The choice and location of linked libraries such as BLAS and LAPACK as well as include paths and other such build options can be specified in a `site.cfg` file located in the NumPy root repository or a `.numpy-site.cfg` file in your home directory. See the `site.cfg.example` example file included in the NumPy repository or sdist for documentation.

<div class="index">

single: distutils

</div>

## Modules in `numpy.distutils`

<div class="toctree" data-maxdepth="2">

distutils/misc\_util

</div>

<div class="currentmodule">

numpy.distutils

</div>

<div class="autosummary" data-toctree="generated/">

ccompiler ccompiler\_opt cpuinfo.cpu core.Extension exec\_command log.set\_verbosity system\_info.get\_info system\_info.get\_standard\_file

</div>

## Configuration class

<div class="currentmodule">

numpy.distutils.misc\_util

</div>

<div class="Configuration(package_name=None, parent_name=None, top_path=None, package_path=None, **attrs)">

Construct a configuration instance for the given package name. If *parent\_name* is not None, then construct the package as a sub-package of the *parent\_name* package. If *top\_path* and *package\_path* are None then they are assumed equal to the path of the file this instance was created in. The setup.py files in the numpy distribution are good examples of how to use the <span class="title-ref">Configuration</span> instance.

<div class="automethod">

todict

</div>

<div class="automethod">

get\_distribution

</div>

<div class="automethod">

get\_subpackage

</div>

<div class="automethod">

add\_subpackage

</div>

<div class="automethod">

add\_data\_files

</div>

<div class="automethod">

add\_data\_dir

</div>

<div class="automethod">

add\_include\_dirs

</div>

<div class="automethod">

add\_headers

</div>

<div class="automethod">

add\_extension

</div>

<div class="automethod">

add\_library

</div>

<div class="automethod">

add\_scripts

</div>

<div class="automethod">

add\_installed\_library

</div>

<div class="automethod">

add\_npy\_pkg\_config

</div>

<div class="automethod">

paths

</div>

<div class="automethod">

get\_config\_cmd

</div>

<div class="automethod">

get\_build\_temp\_dir

</div>

<div class="automethod">

have\_f77c

</div>

<div class="automethod">

have\_f90c

</div>

<div class="automethod">

get\_version

</div>

<div class="automethod">

make\_svn\_version\_py

</div>

<div class="automethod">

make\_config\_py

</div>

<div class="automethod">

get\_info

</div>

</div>

## Building installable C libraries

Conventional C libraries (installed through <span class="title-ref">add\_library</span>) are not installed, and are just used during the build (they are statically linked). An installable C library is a pure C library, which does not depend on the python C runtime, and is installed such that it may be used by third-party packages. To build and install the C library, you just use the method <span class="title-ref">add\_installed\_library</span> instead of <span class="title-ref">add\_library</span>, which takes the same arguments except for an additional `install_dir` argument:

    .. hidden in a comment so as to be included in refguide but not rendered documentation
      >>> import numpy.distutils.misc_util
      >>> config = np.distutils.misc_util.Configuration(None, '', '.')
      >>> with open('foo.c', 'w') as f: pass
    
    >>> config.add_installed_library('foo', sources=['foo.c'], install_dir='lib')

### npy-pkg-config files

To make the necessary build options available to third parties, you could use the <span class="title-ref">npy-pkg-config</span> mechanism implemented in <span class="title-ref">numpy.distutils</span>. This mechanism is based on a .ini file which contains all the options. A .ini file is very similar to .pc files as used by the pkg-config unix utility:

    [meta]
    Name: foo
    Version: 1.0
    Description: foo library
    
    [variables]
    prefix = /home/user/local
    libdir = ${prefix}/lib
    includedir = ${prefix}/include
    
    [default]
    cflags = -I${includedir}
    libs = -L${libdir} -lfoo

Generally, the file needs to be generated during the build, since it needs some information known at build time only (e.g. prefix). This is mostly automatic if one uses the <span class="title-ref">Configuration</span> method <span class="title-ref">add\_npy\_pkg\_config</span>. Assuming we have a template file foo.ini.in as follows:

    [meta]
    Name: foo
    Version: @version@
    Description: foo library
    
    [variables]
    prefix = @prefix@
    libdir = ${prefix}/lib
    includedir = ${prefix}/include
    
    [default]
    cflags = -I${includedir}
    libs = -L${libdir} -lfoo

and the following code in setup.py:

    >>> config.add_installed_library('foo', sources=['foo.c'], install_dir='lib')
    >>> subst = {'version': '1.0'}
    >>> config.add_npy_pkg_config('foo.ini.in', 'lib', subst_dict=subst)

This will install the file foo.ini into the directory package\_dir/lib, and the foo.ini file will be generated from foo.ini.in, where each `@version@` will be replaced by `subst_dict['version']`. The dictionary has an additional prefix substitution rule automatically added, which contains the install prefix (since this is not easy to get from setup.py).

### Reusing a C library from another package

Info are easily retrieved from the <span class="title-ref">get\_info</span> function in \`numpy.distutils.misc\_util\`:

    >>> info = np.distutils.misc_util.get_info('npymath')
    >>> config.add_extension('foo', sources=['foo.c'], extra_info=info)
    <numpy.distutils.extension.Extension('foo') at 0x...>

An additional list of paths to look for .ini files can be given to <span class="title-ref">get\_info</span>.

## Conversion of `.src` files

NumPy distutils supports automatic conversion of source files named \<somefile\>.src. This facility can be used to maintain very similar code blocks requiring only simple changes between blocks. During the build phase of setup, if a template file named \<somefile\>.src is encountered, a new file named \<somefile\> is constructed from the template and placed in the build directory to be used instead. Two forms of template conversion are supported. The first form occurs for files named \<file\>.ext.src where ext is a recognized Fortran extension (f, f90, f95, f77, for, ftn, pyf). The second form is used for all other cases. See \[templating\](\#templating).

---

distutils_guide.md

---

# `numpy.distutils` user guide

\> **Warning** \> `numpy.distutils` is deprecated, and will be removed for Python \>= 3.12. For more details, see \[distutils-status-migration\](\#distutils-status-migration)

# NumPy distutils - users guide

<div class="contents">

</div>

## SciPy structure

Currently SciPy project consists of two packages:

  - NumPy --- it provides packages like:
      - numpy.distutils - extension to Python distutils
      - numpy.f2py - a tool to bind Fortran/C codes to Python
      - numpy.\_core - future replacement of Numeric and numarray packages
      - numpy.lib - extra utility functions
      - numpy.testing - numpy-style tools for unit testing
      - etc
  - SciPy --- a collection of scientific tools for Python.

The aim of this document is to describe how to add new tools to SciPy.

## Requirements for SciPy packages

SciPy consists of Python packages, called SciPy packages, that are available to Python users via the `scipy` namespace. Each SciPy package may contain other SciPy packages. And so on. Therefore, the SciPy directory tree is a tree of packages with arbitrary depth and width. Any SciPy package may depend on NumPy packages but the dependence on other SciPy packages should be kept minimal or zero.

A SciPy package contains, in addition to its sources, the following files and directories:

  - `setup.py` --- building script
  - `__init__.py` --- package initializer
  - `tests/` --- directory of unittests

Their contents are described below.

## The `setup.py` file

In order to add a Python package to SciPy, its build script (`setup.py`) must meet certain requirements. The most important requirement is that the package define a `configuration(parent_package='',top_path=None)` function which returns a dictionary suitable for passing to `numpy.distutils.core.setup(..)`. To simplify the construction of this dictionary, `numpy.distutils.misc_util` provides the `Configuration` class, described below.

### SciPy pure Python package example

Below is an example of a minimal `setup.py` file for a pure SciPy package:

    #!/usr/bin/env python3
    def configuration(parent_package='',top_path=None):
        from numpy.distutils.misc_util import Configuration
        config = Configuration('mypackage',parent_package,top_path)
        return config
    
    if __name__ == "__main__":
        from numpy.distutils.core import setup
        #setup(**configuration(top_path='').todict())
        setup(configuration=configuration)

The arguments of the `configuration` function specify the name of parent SciPy package (`parent_package`) and the directory location of the main `setup.py` script (`top_path`). These arguments, along with the name of the current package, should be passed to the `Configuration` constructor.

The `Configuration` constructor has a fourth optional argument, `package_path`, that can be used when package files are located in a different location than the directory of the `setup.py` file.

Remaining `Configuration` arguments are all keyword arguments that will be used to initialize attributes of `Configuration` instance. Usually, these keywords are the same as the ones that `setup(..)` function would expect, for example, `packages`, `ext_modules`, `data_files`, `include_dirs`, `libraries`, `headers`, `scripts`, `package_dir`, etc. However, the direct specification of these keywords is not recommended as the content of these keyword arguments will not be processed or checked for the consistency of SciPy building system.

Finally, `Configuration` has `.todict()` method that returns all the configuration data as a dictionary suitable for passing on to the `setup(..)` function.

### `Configuration` instance attributes

In addition to attributes that can be specified via keyword arguments to `Configuration` constructor, `Configuration` instance (let us denote as `config`) has the following attributes that can be useful in writing setup scripts:

  - `config.name` - full name of the current package. The names of parent packages can be extracted as `config.name.split('.')`.
  - `config.local_path` - path to the location of current `setup.py` file.
  - `config.top_path` - path to the location of main `setup.py` file.

### `Configuration` instance methods

  - `config.todict()` --- returns configuration dictionary suitable for passing to `numpy.distutils.core.setup(..)` function.

  - `config.paths(*paths) --- applies`glob.glob(..)`to items of`paths`if necessary. Fixes`paths`item that is relative to`config.local\_path\`\`.

  - `config.get_subpackage(subpackage_name,subpackage_path=None)` ---returns a list of subpackage configurations. Subpackage is looked in the current directory under the name `subpackage_name` but the path can be specified also via optional `subpackage_path` argument. If `subpackage_name` is specified as `None` then the subpackage name will be taken the basename of `subpackage_path`. Any `*` used for subpackage names are expanded as wildcards.

  - `config.add_subpackage(subpackage_name,subpackage_path=None)` ---add SciPy subpackage configuration to the current one. The meaning and usage of arguments is explained above, see `config.get_subpackage()` method.

  - `config.add_data_files(*files)` --- prepend `files` to `data_files` list. If `files` item is a tuple then its first element defines the suffix of where data files are copied relative to package installation directory and the second element specifies the path to data files. By default data files are copied under package installation directory. For example,
    
        config.add_data_files('foo.dat',
                          ('fun',['gun.dat','nun/pun.dat','/tmp/sun.dat']),
                              'bar/car.dat'.
                              '/full/path/to/can.dat',
                              )
    
    will install data files to the following locations
    
        <installation path of config.name package>/
          foo.dat
          fun/
            gun.dat
        pun.dat
            sun.dat
          bar/
            car.dat
          can.dat
    
    Path to data files can be a function taking no arguments and returning path(s) to data files -- this is a useful when data files are generated while building the package. (XXX: explain the step when this function are called exactly)

  - `config.add_data_dir(data_path)` --- add directory `data_path` recursively to `data_files`. The whole directory tree starting at `data_path` will be copied under package installation directory. If `data_path` is a tuple then its first element defines the suffix of where data files are copied relative to package installation directory and the second element specifies the path to data directory. By default, data directory are copied under package installation directory under the basename of `data_path`. For example,
    
        config.add_data_dir('fun')  # fun/ contains foo.dat bar/car.dat
        config.add_data_dir(('sun','fun'))
        config.add_data_dir(('gun','/full/path/to/fun'))
    
    will install data files to the following locations
    
        <installation path of config.name package>/
          fun/
             foo.dat
             bar/
                car.dat
          sun/
             foo.dat
             bar/
                car.dat
          gun/
             foo.dat
             bar/
                car.dat

  - `config.add_include_dirs(*paths)` --- prepend `paths` to `include_dirs` list. This list will be visible to all extension modules of the current package.

  - `config.add_headers(*files)` --- prepend `files` to `headers` list. By default, headers will be installed under `<prefix>/include/pythonX.X/<config.name.replace('.','/')>/` directory. If `files` item is a tuple then it's first argument specifies the installation suffix relative to `<prefix>/include/pythonX.X/` path. This is a Python distutils method; its use is discouraged for NumPy and SciPy in favour of `config.add_data_files(*files)`.

  - `config.add_scripts(*files)` --- prepend `files` to `scripts` list. Scripts will be installed under `<prefix>/bin/` directory.

  - `config.add_extension(name,sources,**kw)` --- create and add an `Extension` instance to `ext_modules` list. The first argument `name` defines the name of the extension module that will be installed under `config.name` package. The second argument is a list of sources. `add_extension` method takes also keyword arguments that are passed on to the `Extension` constructor. The list of allowed keywords is the following: `include_dirs`, `define_macros`, `undef_macros`, `library_dirs`, `libraries`, `runtime_library_dirs`, `extra_objects`, `extra_compile_args`, `extra_link_args`, `export_symbols`, `swig_opts`, `depends`, `language`, `f2py_options`, `module_dirs`, `extra_info`, `extra_f77_compile_args`, `extra_f90_compile_args`.
    
    Note that `config.paths` method is applied to all lists that may contain paths. `extra_info` is a dictionary or a list of dictionaries that content will be appended to keyword arguments. The list `depends` contains paths to files or directories that the sources of the extension module depend on. If any path in the `depends` list is newer than the extension module, then the module will be rebuilt.
    
    The list of sources may contain functions ('source generators') with a pattern `def <funcname>(ext, build_dir): return <source(s) or None>`. If `funcname` returns `None`, no sources are generated. And if the `Extension` instance has no sources after processing all source generators, no extension module will be built. This is the recommended way to conditionally define extension modules. Source generator functions are called by the `build_src` sub-command of `numpy.distutils`.
    
    For example, here is a typical source generator function:
    
        def generate_source(ext,build_dir):
            import os
            from distutils.dep_util import newer
            target = os.path.join(build_dir,'somesource.c')
            if newer(target,__file__):
                # create target file
            return target
    
    The first argument contains the Extension instance that can be useful to access its attributes like `depends`, `sources`, etc. lists and modify them during the building process. The second argument gives a path to a build directory that must be used when creating files to a disk.

  - `config.add_library(name, sources, **build_info)` --- add a library to `libraries` list. Allowed keywords arguments are `depends`, `macros`, `include_dirs`, `extra_compiler_args`, `f2py_options`, `extra_f77_compile_args`, `extra_f90_compile_args`. See `.add_extension()` method for more information on arguments.

  - `config.have_f77c()` --- return True if Fortran 77 compiler is available (read: a simple Fortran 77 code compiled successfully).

  - `config.have_f90c()` --- return True if Fortran 90 compiler is available (read: a simple Fortran 90 code compiled successfully).

  - `config.get_version()` --- return version string of the current package, `None` if version information could not be detected. This methods scans files `__version__.py`, `<packagename>_version.py`, `version.py`, `__svn_version__.py` for string variables `version`, `__version__`, `<packagename>_version`.

  - `config.make_svn_version_py()` --- appends a data function to `data_files` list that will generate `__svn_version__.py` file to the current package directory. The file will be removed from the source directory when Python exits.

  - `config.get_build_temp_dir()` --- return a path to a temporary directory. This is the place where one should build temporary files.

  - `config.get_distribution()` --- return distutils `Distribution` instance.

  - `config.get_config_cmd()` --- returns `numpy.distutils` config command instance.

  - `config.get_info(*names)` ---

### Conversion of `.src` files using templates

NumPy distutils supports automatic conversion of source files named \<somefile\>.src. This facility can be used to maintain very similar code blocks requiring only simple changes between blocks. During the build phase of setup, if a template file named \<somefile\>.src is encountered, a new file named \<somefile\> is constructed from the template and placed in the build directory to be used instead. Two forms of template conversion are supported. The first form occurs for files named \<file\>.ext.src where ext is a recognized Fortran extension (f, f90, f95, f77, for, ftn, pyf). The second form is used for all other cases.

<div class="index">

single: code generation

</div>

### Fortran files

This template converter will replicate all **function** and **subroutine** blocks in the file with names that contain '\<...\>' according to the rules in '\<...\>'. The number of comma-separated words in '\<...\>' determines the number of times the block is repeated. What these words are indicates what that repeat rule, '\<...\>', should be replaced with in each block. All of the repeat rules in a block must contain the same number of comma-separated words indicating the number of times that block should be repeated. If the word in the repeat rule needs a comma, leftarrow, or rightarrow, then prepend it with a backslash ' '. If a word in the repeat rule matches ' \\\<index\>' then it will be replaced with the \<index\>-th word in the same repeat specification. There are two forms for the repeat rule: named and short.

#### Named repeat rule

A named repeat rule is useful when the same set of repeats must be used several times in a block. It is specified using \<rule1=item1, item2, item3,..., itemN\>, where N is the number of times the block should be repeated. On each repeat of the block, the entire expression, '\<...\>' will be replaced first with item1, and then with item2, and so forth until N repeats are accomplished. Once a named repeat specification has been introduced, the same repeat rule may be used **in the current block** by referring only to the name (i.e. \<rule1\>).

#### Short repeat rule

A short repeat rule looks like \<item1, item2, item3, ..., itemN\>. The rule specifies that the entire expression, '\<...\>' should be replaced first with item1, and then with item2, and so forth until N repeats are accomplished.

#### Pre-defined names

The following predefined named repeat rules are available:

  - \<prefix=s,d,c,z\>
  - \<\_c=s,d,c,z\>
  - \<\_t=real, double precision, complex, double complex\>
  - \<ftype=real, double precision, complex, double complex\>
  - \<ctype=float, double, complex\_float, complex\_double\>
  - \<ftypereal=float, double precision, \\0, \\1\>
  - \<ctypereal=float, double, \\0, \\1\>

### Other files

Non-Fortran files use a separate syntax for defining template blocks that should be repeated using a variable expansion similar to the named repeat rules of the Fortran-specific repeats.

NumPy Distutils preprocesses C source files (extension: `.c.src`) written in a custom templating language to generate C code. The `@` symbol is used to wrap macro-style variables to empower a string substitution mechanism that might describe (for instance) a set of data types.

The template language blocks are delimited by `/**begin repeat` and `/**end repeat**/` lines, which may also be nested using consecutively numbered delimiting lines such as `/**begin repeat1` and `/**end repeat1**/`:

1.  `/**begin repeat` on a line by itself marks the beginning of a segment that should be repeated.
2.  Named variable expansions are defined using `#name=item1, item2, item3, ..., itemN#` and placed on successive lines. These variables are replaced in each repeat block with corresponding word. All named variables in the same repeat block must define the same number of words.
3.  In specifying the repeat rule for a named variable, `item*N` is short-hand for `item, item, ..., item` repeated N times. In addition, parenthesis in combination with `*N` can be used for grouping several items that should be repeated. Thus, `#name=(item1, item2)*4#` is equivalent to `#name=item1, item2, item1, item2, item1, item2, item1, item2#`.
4.  `*/` on a line by itself marks the end of the variable expansion naming. The next line is the first line that will be repeated using the named rules.
5.  Inside the block to be repeated, the variables that should be expanded are specified as `@name@`.
6.  `/**end repeat**/` on a line by itself marks the previous line as the last line of the block to be repeated.
7.  A loop in the NumPy C source code may have a `@TYPE@` variable, targeted for string substitution, which is preprocessed to a number of otherwise identical loops with several strings such as `INT`, `LONG`, `UINT`, `ULONG`. The `@TYPE@` style syntax thus reduces code duplication and maintenance burden by mimicking languages that have generic type support.

The above rules may be clearer in the following template source example:

``` NumPyC
/* TIMEDELTA to non-float types */

/**begin repeat
 *
 * #TOTYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONG, ULONG,
 *           LONGLONG, ULONGLONG, DATETIME,
 *           TIMEDELTA#
 * #totype = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,
 *           npy_long, npy_ulong, npy_longlong, npy_ulonglong,
 *           npy_datetime, npy_timedelta#
 */

/**begin repeat1
 *
 * #FROMTYPE = TIMEDELTA#
 * #fromtype = npy_timedelta#
 */
static void
@FROMTYPE@_to_@TOTYPE@(void *input, void *output, npy_intp n,
        void *NPY_UNUSED(aip), void *NPY_UNUSED(aop))
{
    const @fromtype@ *ip = input;
    @totype@ *op = output;

    while (n--) {
        *op++ = (@totype@)*ip++;
    }
}
/**end repeat1**/

/**end repeat**/
```

The preprocessing of generically-typed C source files (whether in NumPy proper or in any third party package using NumPy Distutils) is performed by [conv\_template.py](https://github.com/numpy/numpy/blob/main/numpy/distutils/conv_template.py). The type-specific C files generated (extension: `.c`) by these modules during the build process are ready to be compiled. This form of generic typing is also supported for C header files (preprocessed to produce `.h` files).

### Useful functions in `numpy.distutils.misc_util`

  - `get_numpy_include_dirs()` --- return a list of NumPy base include directories. NumPy base include directories contain header files such as `numpy/arrayobject.h`, `numpy/funcobject.h` etc. For installed NumPy the returned list has length 1 but when building NumPy the list may contain more directories, for example, a path to `config.h` file that `numpy/base/setup.py` file generates and is used by `numpy` header files.
  - `append_path(prefix,path)` --- smart append `path` to `prefix`.
  - `gpaths(paths, local_path='')` --- apply glob to paths and prepend `local_path` if needed.
  - `njoin(*path)` --- join pathname components + convert `/`-separated path to `os.sep`-separated path and resolve `..`, `.` from paths. Ex. `njoin('a',['b','./c'],'..','g') -> os.path.join('a','b','g')`.
  - `minrelpath(path)` --- resolves dots in `path`.
  - `rel_path(path, parent_path)` --- return `path` relative to `parent_path`.
  - `def get_cmd(cmdname,_cache={})` --- returns `numpy.distutils` command instance.
  - `all_strings(lst)`
  - `has_f_sources(sources)`
  - `has_cxx_sources(sources)`
  - `filter_sources(sources)` --- return `c_sources, cxx_sources, f_sources, fmodule_sources`
  - `get_dependencies(sources)`
  - `is_local_src_dir(directory)`
  - `get_ext_source_files(ext)`
  - `get_script_files(scripts)`
  - `get_lib_source_files(lib)`
  - `get_data_files(data)`
  - `dot_join(*args)` --- join non-zero arguments with a dot.
  - `get_frame(level=0)` --- return frame object from call stack with given level.
  - `cyg2win32(path)`
  - `mingw32()` --- return `True` when using mingw32 environment.
  - `terminal_has_colors()`, `red_text(s)`, `green_text(s)`, `yellow_text(s)`, `blue_text(s)`, `cyan_text(s)`
  - `get_path(mod_name,parent_path=None)` --- return path of a module relative to parent\_path when given. Handles also `__main__` and `__builtin__` modules.
  - `allpath(name)` --- replaces `/` with `os.sep` in `name`.
  - `cxx_ext_match`, `fortran_ext_match`, `f90_ext_match`, `f90_module_name_match`

### `numpy.distutils.system_info` module

  - `get_info(name,notfound_action=0)`
  - `combine_paths(*args,**kws)`
  - `show_all()`

### `numpy.distutils.cpuinfo` module

  - `cpuinfo`

### `numpy.distutils.log` module

  - `set_verbosity(v)`

### `numpy.distutils.exec_command` module

  - `get_pythonexe()`
  - `find_executable(exe, path=None)`
  - `exec_command( command, execute_in='', use_shell=None, use_tee=None, **env )`

## The `__init__.py` file

The header of a typical SciPy `__init__.py` is:

    """
    Package docstring, typically with a brief description and function listing.
    """
    
    # import functions into module namespace
    from .subpackage import *
    ...
    
    __all__ = [s for s in dir() if not s.startswith('_')]
    
    from numpy.testing import Tester
    test = Tester().test
    bench = Tester().bench

## Extra features in NumPy Distutils

### Specifying config\_fc options for libraries in setup.py script

It is possible to specify config\_fc options in setup.py scripts. For example, using:

    config.add_library('library',
                       sources=[...],
                       config_fc={'noopt':(__file__,1)})

will compile the `library` sources without optimization flags.

It's recommended to specify only those config\_fc options in such a way that are compiler independent.

### Getting extra Fortran 77 compiler options from source

Some old Fortran codes need special compiler options in order to work correctly. In order to specify compiler options per source file, `numpy.distutils` Fortran compiler looks for the following pattern:

    CF77FLAGS(<fcompiler type>) = <fcompiler f77flags>

in the first 20 lines of the source and use the `f77flags` for specified type of the fcompiler (the first character `C` is optional).

TODO: This feature can be easily extended for Fortran 90 codes as well. Let us know if you would need such a feature.

>   - start-line  
>     6

---

distutils_status_migration.md

---

# Status of `numpy.distutils` and migration advice

<span class="title-ref">numpy.distutils</span> has been deprecated in NumPy `1.23.0`. It will be removed for Python 3.12; for Python \<= 3.11 it will not be removed until 2 years after the Python 3.12 release (Oct 2025).

\> **Warning** \> `numpy.distutils` is only tested with `setuptools < 60.0`, newer versions may break. See \[numpy-setuptools-interaction\](\#numpy-setuptools-interaction) for details.

## Migration advice

There are several build systems which are good options to migrate to. Assuming you have compiled code in your package (if not, you have several good options, e.g. the build backends offered by Poetry, Hatch or PDM) and you want to be using a well-designed, modern and reliable build system, we recommend:

1.  [Meson](https://mesonbuild.com/), and the [meson-python](https://meson-python.readthedocs.io) build backend
2.  [CMake](https://cmake.org/), and the [scikit-build-core](https://scikit-build-core.readthedocs.io/en/latest/) build backend

If you have modest needs (only simple Cython/C extensions; no need for Fortran, BLAS/LAPACK, nested `setup.py` files, or other features of `numpy.distutils`) and have been happy with `numpy.distutils` so far, you can also consider switching to `setuptools`. Note that most functionality of `numpy.distutils` is unlikely to be ported to `setuptools`.

### Moving to Meson

SciPy has moved to Meson and meson-python for its 1.9.0 release. During this process, remaining issues with Meson's Python support and feature parity with `numpy.distutils` were resolved. *Note: parity means a large superset (because Meson is a good general-purpose build system); only a few BLAS/LAPACK library selection niceties are missing*. SciPy uses almost all functionality that `numpy.distutils` offers, so if SciPy has successfully made a release with Meson as the build system, there should be no blockers left to migrate, and SciPy will be a good reference for other packages who are migrating. For more details about the SciPy migration, see:

  - [RFC: switch to Meson as a build system](https://github.com/scipy/scipy/issues/13615)
  - [Tracking issue for Meson support](https://github.com/rgommers/scipy/issues/22)

NumPy will migrate to Meson for the 1.26 release.

### Moving to CMake / scikit-build

The next generation of scikit-build is called [scikit-build-core](https://scikit-build-core.readthedocs.io/en/latest/). Where the older `scikit-build` used `setuptools` underneath, the rewrite does not. Like Meson, CMake is a good general-purpose build system.

### Moving to `setuptools`

For projects that only use `numpy.distutils` for historical reasons, and do not actually use features beyond those that `setuptools` also supports, moving to `setuptools` is likely the solution which costs the least effort. To assess that, there are the `numpy.distutils` features that are *not* present in `setuptools`:

  - Nested `setup.py` files
  - Fortran build support
  - BLAS/LAPACK library support (OpenBLAS, MKL, ATLAS, Netlib LAPACK/BLAS, BLIS, 64-bit ILP interface, etc.)
  - Support for a few other scientific libraries, like FFTW and UMFPACK
  - Better MinGW support
  - Per-compiler build flag customization (e.g. <span class="title-ref">-O3</span> and <span class="title-ref">SSE2</span> flags are default)
  - a simple user build config system, see [site.cfg.example](https://github.com/numpy/numpy/blob/master/site.cfg.example)
  - SIMD intrinsics support
  - Support for the NumPy-specific `.src` templating format for `.c`/`.h` files

The most widely used feature is nested `setup.py` files. This feature may perhaps still be ported to `setuptools` in the future (it needs a volunteer though, see [gh-18588](https://github.com/numpy/numpy/issues/18588) for status). Projects only using that feature could move to `setuptools` after that is done. In case a project uses only a couple of `setup.py` files, it also could make sense to simply aggregate all the content of those files into a single `setup.py` file and then move to `setuptools`. This involves dropping all `Configuration` instances, and using `Extension` instead. E.g.,:

    from distutils.core import setup
    from distutils.extension import Extension
    setup(name='foobar',
          version='1.0',
          ext_modules=[
              Extension('foopkg.foo', ['foo.c']),
              Extension('barpkg.bar', ['bar.c']),
              ],
          )

For more details, see the [setuptools documentation](https://setuptools.pypa.io/en/latest/setuptools.html)

## Interaction of `numpy.distutils` with `setuptools`

It is recommended to use `setuptools < 60.0`. Newer versions may work, but are not guaranteed to. The reason for this is that `setuptools` 60.0 enabled a vendored copy of `distutils`, including backwards incompatible changes that affect some functionality in `numpy.distutils`.

If you are using only simple Cython or C extensions with minimal use of `numpy.distutils` functionality beyond nested `setup.py` files (its most popular feature, see <span class="title-ref">Configuration \<numpy.distutils.misc\_util.Configuration\></span>), then latest `setuptools` is likely to continue working. In case of problems, you can also try `SETUPTOOLS_USE_DISTUTILS=stdlib` to avoid the backwards incompatible changes in `setuptools`.

Whatever you do, it is recommended to put an upper bound on your `setuptools` build requirement in `pyproject.toml` to avoid future breakage - see \[for-downstream-package-authors\](\#for-downstream-package-authors).

---

global_state.md

---

# Global Configuration Options

NumPy has a few import-time, compile-time, or runtime configuration options which change the global behaviour. Most of these are related to performance or for debugging purposes and will not be interesting to the vast majority of users.

## Performance-related options

### Number of threads used for linear algebra

NumPy itself is normally intentionally limited to a single thread during function calls, however it does support multiple Python threads running at the same time. Note that for performant linear algebra NumPy uses a BLAS backend such as OpenBLAS or MKL, which may use multiple threads that may be controlled by environment variables such as `OMP_NUM_THREADS` depending on what is used. One way to control the number of threads is the package [threadpoolctl](https://pypi.org/project/threadpoolctl/)

### madvise hugepage on Linux

When working with very large arrays on modern Linux kernels, you can experience a significant speedup when [transparent hugepage](https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html) is used. The current system policy for transparent hugepages can be seen by:

    cat /sys/kernel/mm/transparent_hugepage/enabled

When set to `madvise` NumPy will typically use hugepages for a performance boost. This behaviour can be modified by setting the environment variable:

    NUMPY_MADVISE_HUGEPAGE=0

or setting it to `1` to always enable it. When not set, the default is to use madvise on Kernels 4.6 and newer. These kernels presumably experience a large speedup with hugepage support. This flag is checked at import time.

### SIMD feature selection

Setting `NPY_DISABLE_CPU_FEATURES` will exclude simd features at runtime. See \[runtime-simd-dispatch\](\#runtime-simd-dispatch).

## Debugging-related options

### Warn if no memory allocation policy when deallocating data

Some users might pass ownership of the data pointer to the `ndarray` by setting the `OWNDATA` flag. If they do this without setting (manually) a memory allocation policy, the default will be to call `free`. If `NUMPY_WARN_IF_NO_MEM_POLICY` is set to `"1"`, a `RuntimeWarning` will be emitted. A better alternative is to use a `PyCapsule` with a deallocator and set the `ndarray.base`.

<div class="versionchanged">

1.25.2 This variable is only checked on the first import.

</div>

---

index.md

---

<div class="module">

numpy

</div>

# NumPy reference

  - Release  

  - Date  

This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the \[complete documentation \<numpy\_docs\_mainpage\>\](\#complete-documentation-\<numpy\_docs\_mainpage\>).

## Python API

<div class="toctree" data-maxdepth="1">

module\_structure

</div>

<div class="toctree" data-maxdepth="2">

arrays

</div>

<div class="toctree" data-maxdepth="1">

ufuncs

</div>

<div class="toctree" data-maxdepth="2">

routines

</div>

<div class="toctree" data-maxdepth="1">

typing distutils

</div>

## C API

<div class="toctree" data-maxdepth="2">

c-api/index

</div>

## Other topics

<div class="toctree" data-maxdepth="1">

array\_api simd/index thread\_safety global\_state security distutils\_status\_migration distutils\_guide swig

</div>

## Acknowledgements

Large parts of this manual originate from Travis E. Oliphant's book [Guide to NumPy](https://archive.org/details/NumPyBook) (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy.

---

internals.code-explanations.md

---

- orphan

# NumPy C code explanations

This document has been moved to \[c-code-explanations\](\#c-code-explanations).

---

internals.md

---

- orphan

# NumPy internals

This document has been moved to \[numpy-internals\](\#numpy-internals).

---

maskedarray.baseclass.md

---

<div class="currentmodule">

numpy.ma

</div>

# Constants of the `numpy.ma` module

In addition to the <span class="title-ref">MaskedArray</span> class, the `numpy.ma` module defines several constants.

<div class="data">

masked

The <span class="title-ref">masked</span> constant is a special case of <span class="title-ref">MaskedArray</span>, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:

    >>> import numpy as np
    
    >>> x = ma.array([1, 2, 3], mask=[0, 1, 0])
    >>> x[1] is ma.masked
    True
    >>> x[-1] = ma.masked
    >>> x
    masked_array(data=[1, --, --],
                 mask=[False,  True,  True],
           fill_value=999999)

</div>

<div class="data">

nomask

Value indicating that a masked array has no invalid entry. <span class="title-ref">nomask</span> is used internally to speed up computations when the mask is not needed. It is represented internally as `np.False_`.

</div>

<div class="data">

masked\_print\_option

String used in lieu of missing data when a masked array is printed. By default, this string is `'--'`.

Use `set_display()` to change the default string. Example usage: `numpy.ma.masked_print_option.set_display('X')` replaces missing data with `'X'`.

</div>

# The <span class="title-ref">MaskedArray</span> class

<div class="MaskedArray">

A subclass of <span class="title-ref">\~numpy.ndarray</span> designed to manipulate numerical arrays with missing data.

</div>

An instance of <span class="title-ref">MaskedArray</span> can be thought as the combination of several elements:

  - The <span class="title-ref">\~MaskedArray.data</span>, as a regular <span class="title-ref">numpy.ndarray</span> of any shape or datatype (the data).
  - A boolean <span class="title-ref">\~numpy.ma.MaskedArray.mask</span> with the same shape as the data, where a `True` value indicates that the corresponding element of the data is invalid. The special value <span class="title-ref">nomask</span> is also acceptable for arrays without named fields, and indicates that no data is invalid.
  - A <span class="title-ref">\~numpy.ma.MaskedArray.fill\_value</span>, a value that may be used to replace the invalid entries in order to return a standard <span class="title-ref">numpy.ndarray</span>.

## Attributes and properties of masked arrays

<div class="seealso">

\[Array Attributes \<arrays.ndarray.attributes\>\](\#array-attributes-\<arrays.ndarray.attributes\>)

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.data

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.mask

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.recordmask

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.fill\_value

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.baseclass

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.sharedmask

</div>

<div class="autoattribute">

numpy::ma.MaskedArray.hardmask

</div>

As <span class="title-ref">MaskedArray</span> is a subclass of <span class="title-ref">\~numpy.ndarray</span>, a masked array also inherits all the attributes and properties of a <span class="title-ref">\~numpy.ndarray</span> instance.

<div class="autosummary" data-toctree="generated/">

MaskedArray.base MaskedArray.ctypes MaskedArray.dtype MaskedArray.flags

MaskedArray.itemsize MaskedArray.nbytes MaskedArray.ndim MaskedArray.shape MaskedArray.size MaskedArray.strides

MaskedArray.imag MaskedArray.real

MaskedArray.flat MaskedArray.\_\_array\_priority\_\_

</div>

# <span class="title-ref">MaskedArray</span> methods

<div class="seealso">

\[Array methods \<array.ndarray.methods\>\](\#array-methods-\<array.ndarray.methods\>)

</div>

## Conversion

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_float\_\_ MaskedArray.\_\_int\_\_

MaskedArray.view MaskedArray.astype MaskedArray.byteswap

MaskedArray.compressed MaskedArray.filled MaskedArray.tofile MaskedArray.toflex MaskedArray.tolist MaskedArray.torecords MaskedArray.tostring MaskedArray.tobytes

</div>

## Shape manipulation

For reshape, resize, and transpose, the single tuple argument may be replaced with `n` integers which will be interpreted as an n-tuple.

<div class="autosummary" data-toctree="generated/">

MaskedArray.flatten MaskedArray.ravel MaskedArray.reshape MaskedArray.resize MaskedArray.squeeze MaskedArray.swapaxes MaskedArray.transpose MaskedArray.T

</div>

## Item selection and manipulation

For array methods that take an `axis` keyword, it defaults to None. If axis is None, then the array is treated as a 1-D array. Any other value for `axis` represents the dimension along which the operation should proceed.

<div class="autosummary" data-toctree="generated/">

MaskedArray.argmax MaskedArray.argmin MaskedArray.argsort MaskedArray.choose MaskedArray.compress MaskedArray.diagonal MaskedArray.fill MaskedArray.item MaskedArray.nonzero MaskedArray.put MaskedArray.repeat MaskedArray.searchsorted MaskedArray.sort MaskedArray.take

</div>

## Pickling and copy

<div class="autosummary" data-toctree="generated/">

MaskedArray.copy MaskedArray.dump MaskedArray.dumps

</div>

## Calculations

<div class="autosummary" data-toctree="generated/">

MaskedArray.all MaskedArray.anom MaskedArray.any MaskedArray.clip MaskedArray.conj MaskedArray.conjugate MaskedArray.cumprod MaskedArray.cumsum MaskedArray.max MaskedArray.mean MaskedArray.min MaskedArray.prod MaskedArray.product MaskedArray.ptp MaskedArray.round MaskedArray.std MaskedArray.sum MaskedArray.trace MaskedArray.var

</div>

## Arithmetic and comparison operations

<div class="index">

comparison, arithmetic, operation, operator

</div>

### Comparison operators:

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_lt\_\_ MaskedArray.\_\_le\_\_ MaskedArray.\_\_gt\_\_ MaskedArray.\_\_ge\_\_ MaskedArray.\_\_eq\_\_ MaskedArray.\_\_ne\_\_

</div>

### Truth value of an array (<span class="title-ref">bool() \<bool\></span>):

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_bool\_\_

</div>

### Arithmetic:

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_abs\_\_ MaskedArray.\_\_add\_\_ MaskedArray.\_\_radd\_\_ MaskedArray.\_\_sub\_\_ MaskedArray.\_\_rsub\_\_ MaskedArray.\_\_mul\_\_ MaskedArray.\_\_rmul\_\_ MaskedArray.\_\_div\_\_ MaskedArray.\_\_truediv\_\_ MaskedArray.\_\_rtruediv\_\_ MaskedArray.\_\_floordiv\_\_ MaskedArray.\_\_rfloordiv\_\_ MaskedArray.\_\_mod\_\_ MaskedArray.\_\_rmod\_\_ MaskedArray.\_\_divmod\_\_ MaskedArray.\_\_rdivmod\_\_ MaskedArray.\_\_pow\_\_ MaskedArray.\_\_rpow\_\_ MaskedArray.\_\_lshift\_\_ MaskedArray.\_\_rlshift\_\_ MaskedArray.\_\_rshift\_\_ MaskedArray.\_\_rrshift\_\_ MaskedArray.\_\_and\_\_ MaskedArray.\_\_rand\_\_ MaskedArray.\_\_or\_\_ MaskedArray.\_\_ror\_\_ MaskedArray.\_\_xor\_\_ MaskedArray.\_\_rxor\_\_

</div>

### Arithmetic, in-place:

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_iadd\_\_ MaskedArray.\_\_isub\_\_ MaskedArray.\_\_imul\_\_ MaskedArray.\_\_idiv\_\_ MaskedArray.\_\_itruediv\_\_ MaskedArray.\_\_ifloordiv\_\_ MaskedArray.\_\_imod\_\_ MaskedArray.\_\_ipow\_\_ MaskedArray.\_\_ilshift\_\_ MaskedArray.\_\_irshift\_\_ MaskedArray.\_\_iand\_\_ MaskedArray.\_\_ior\_\_ MaskedArray.\_\_ixor\_\_

</div>

## Representation

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_repr\_\_ MaskedArray.\_\_str\_\_

MaskedArray.ids MaskedArray.iscontiguous

</div>

## Special methods

For standard library functions:

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_copy\_\_ MaskedArray.\_\_deepcopy\_\_ MaskedArray.\_\_getstate\_\_ MaskedArray.\_\_reduce\_\_ MaskedArray.\_\_setstate\_\_

</div>

Basic customization:

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_new\_\_ MaskedArray.\_\_array\_\_ MaskedArray.\_\_array\_wrap\_\_

</div>

Container customization: (see \[Indexing \<arrays.indexing\>\](\#indexing-\<arrays.indexing\>))

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_len\_\_ MaskedArray.\_\_getitem\_\_ MaskedArray.\_\_setitem\_\_ MaskedArray.\_\_delitem\_\_ MaskedArray.\_\_contains\_\_

</div>

## Specific methods

### Handling the mask

The following methods can be used to access information about the mask or to manipulate the mask.

<div class="autosummary" data-toctree="generated/">

MaskedArray.\_\_setmask\_\_

MaskedArray.harden\_mask MaskedArray.soften\_mask MaskedArray.unshare\_mask MaskedArray.shrink\_mask

</div>

### Handling the <span class="title-ref">fill\_value</span>

<div class="autosummary" data-toctree="generated/">

MaskedArray.get\_fill\_value MaskedArray.set\_fill\_value

</div>

### Counting the missing elements

<div class="autosummary" data-toctree="generated/">

MaskedArray.count

</div>

---

maskedarray.generic.md

---

<div class="currentmodule">

numpy.ma

</div>

<div id="maskedarray.generic">

<div class="module">

numpy.ma

</div>

</div>

# The `numpy.ma` module

## Rationale

Masked arrays are arrays that may have missing or invalid entries. The `numpy.ma` module provides a nearly work-alike replacement for numpy that supports data arrays with masks.

## What is a masked array?

In many circumstances, datasets can be incomplete or tainted by the presence of invalid data. For example, a sensor may have failed to record a data, or recorded an invalid value. The `numpy.ma` module provides a convenient way to address this issue, by introducing masked arrays.

A masked array is the combination of a standard <span class="title-ref">numpy.ndarray</span> and a mask. A mask is either <span class="title-ref">nomask</span>, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is `False`, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is `True`, the corresponding element of the associated array is said to be masked (invalid).

The package ensures that masked entries are not used in computations.

> As an illustration, let's consider the following dataset:
> 
> \>\>\> import numpy as np \>\>\> import numpy.ma as ma \>\>\> x = np.array(\[1, 2, 3, -1, 5\])
> 
> We wish to mark the fourth entry as invalid. The easiest is to create a masked array:
> 
>     >>> mx = ma.masked_array(x, mask=[0, 0, 0, 1, 0])
> 
> We can now compute the mean of the dataset, without taking the invalid data into account:
> 
> \>\>\> mx.mean() 2.75

## The `numpy.ma` module

The main feature of the `numpy.ma` module is the <span class="title-ref">MaskedArray</span> class, which is a subclass of <span class="title-ref">numpy.ndarray</span>. The class, its attributes and methods are described in more details in the \[MaskedArray class \<maskedarray.baseclass\>\](\#maskedarray-class-\<maskedarray.baseclass\>) section.

The `numpy.ma` module can be used as an addition to `numpy`:

> \>\>\> import numpy as np \>\>\> import numpy.ma as ma
> 
> To create an array with the second element invalid, we would do:
> 
>     >>> y = ma.array([1, 2, 3], mask = [0, 1, 0])
> 
> To create a masked array where all values close to 1.e20 are invalid, we would do:
> 
> \>\>\> z = ma.masked\_values(\[1.0, 1.e20, 3.0, 4.0\], 1.e20)

For a complete discussion of creation methods for masked arrays please see section \[Constructing masked arrays \<maskedarray.generic.constructing\>\](\#constructing-masked-arrays-\<maskedarray.generic.constructing\>).

# Using numpy.ma

## Constructing masked arrays

There are several ways to construct a masked array.

  - A first possibility is to directly invoke the <span class="title-ref">MaskedArray</span> class.

  - A second possibility is to use the two masked array constructors, <span class="title-ref">array</span> and <span class="title-ref">masked\_array</span>.
    
    <div class="autosummary" data-toctree="generated/">
    
    array masked\_array
    
    </div>

  - A third option is to take the view of an existing array. In that case, the mask of the view is set to <span class="title-ref">nomask</span> if the array has no named fields, or an array of boolean with the same structure as the array otherwise.
    
    > \>\>\> import numpy as np \>\>\> x = np.array(\[1, 2, 3\]) \>\>\> x.view(ma.MaskedArray) masked\_array(data=\[1, 2, 3\], mask=False, fill\_value=999999) \>\>\> x = np.array(\[(1, 1.), (2, 2.)\], dtype=\[('a',int), ('b', float)\]) \>\>\> x.view(ma.MaskedArray) masked\_array(data=\[(1, 1.0), (2, 2.0)\], mask=\[(False, False), (False, False)\], fill\_value=(999999, 1e+20), dtype=\[('a', '\<i8'), ('b', '\<f8')\])

  - Yet another possibility is to use any of the following functions:
    
    <div class="autosummary" data-toctree="generated/">
    
    asarray asanyarray fix\_invalid masked\_equal masked\_greater masked\_greater\_equal masked\_inside masked\_invalid masked\_less masked\_less\_equal masked\_not\_equal masked\_object masked\_outside masked\_values masked\_where
    
    </div>

## Accessing the data

The underlying data of a masked array can be accessed in several ways:

  - through the <span class="title-ref">\~MaskedArray.data</span> attribute. The output is a view of the array as a <span class="title-ref">numpy.ndarray</span> or one of its subclasses, depending on the type of the underlying data at the masked array creation.
  - through the <span class="title-ref">\~MaskedArray.\_\_array\_\_</span> method. The output is then a <span class="title-ref">numpy.ndarray</span>.
  - by directly taking a view of the masked array as a <span class="title-ref">numpy.ndarray</span> or one of its subclass (which is actually what using the <span class="title-ref">\~MaskedArray.data</span> attribute does).
  - by using the <span class="title-ref">getdata</span> function.

None of these methods is completely satisfactory if some entries have been marked as invalid. As a general rule, where a representation of the array is required without any masked entries, it is recommended to fill the array with the <span class="title-ref">filled</span> method.

## Accessing the mask

The mask of a masked array is accessible through its <span class="title-ref">\~MaskedArray.mask</span> attribute. We must keep in mind that a `True` entry in the mask indicates an *invalid* data.

Another possibility is to use the <span class="title-ref">getmask</span> and <span class="title-ref">getmaskarray</span> functions. `getmask(x)` outputs the mask of `x` if `x` is a masked array, and the special value <span class="title-ref">nomask</span> otherwise. `getmaskarray(x)` outputs the mask of `x` if `x` is a masked array. If `x` has no invalid entry or is not a masked array, the function outputs a boolean array of `False` with as many elements as `x`.

## Accessing only the valid entries

To retrieve only the valid entries, we can use the inverse of the mask as an index. The inverse of the mask can be calculated with the <span class="title-ref">numpy.logical\_not</span> function or simply with the `~` operator:

> \>\>\> import numpy as np \>\>\> x = ma.array(\[\[1, 2\], \[3, 4\]\], mask=\[\[0, 1\], \[1, 0\]\]) \>\>\> x\[\~x.mask\] masked\_array(data=\[1, 4\], mask=\[False, False\], fill\_value=999999)
> 
> Another way to retrieve the valid data is to use the <span class="title-ref">compressed</span> method, which returns a one-dimensional <span class="title-ref">\~numpy.ndarray</span> (or one of its subclasses, depending on the value of the <span class="title-ref">\~MaskedArray.baseclass</span> attribute):
> 
> \>\>\> x.compressed() array(\[1, 4\])
> 
> Note that the output of <span class="title-ref">compressed</span> is always 1D.

## Modifying the mask

### Masking an entry

The recommended way to mark one or several specific entries of a masked array as invalid is to assign the special value <span class="title-ref">masked</span> to them:

> \>\>\> x = ma.array(\[1, 2, 3\]) \>\>\> x\[0\] = ma.masked \>\>\> x masked\_array(data=\[--, 2, 3\], mask=\[ True, False, False\], fill\_value=999999) \>\>\> y = ma.array(\[\[1, 2, 3\], \[4, 5, 6\], \[7, 8, 9\]\]) \>\>\> y\[(0, 1, 2), (1, 2, 0)\] = ma.masked \>\>\> y masked\_array( data=\[\[1, --, 3\], \[4, 5, --\], \[--, 8, 9\]\], mask=\[\[False, True, False\], \[False, False, True\], \[ True, False, False\]\], fill\_value=999999) \>\>\> z = ma.array(\[1, 2, 3, 4\]) \>\>\> z\[:-2\] = ma.masked \>\>\> z masked\_array(data=\[--, --, 3, 4\], mask=\[ True, True, False, False\], fill\_value=999999)

A second possibility is to modify the <span class="title-ref">\~MaskedArray.mask</span> directly, but this usage is discouraged.

<div class="note">

<div class="title">

Note

</div>

When creating a new masked array with a simple, non-structured datatype, the mask is initially set to the special value <span class="title-ref">nomask</span>, that corresponds roughly to the boolean `False`. Trying to set an element of <span class="title-ref">nomask</span> will fail with a <span class="title-ref">TypeError</span> exception, as a boolean does not support item assignment.

</div>

All the entries of an array can be masked at once by assigning `True` to the mask:

> \>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[1, 2, 3\], mask=\[0, 0, 1\]) \>\>\> x.mask = True \>\>\> x masked\_array(data=\[--, --, --\], mask=\[ True, True, True\], fill\_value=999999, dtype=int64)
> 
> Finally, specific entries can be masked and/or unmasked by assigning to the mask a sequence of booleans:
> 
> \>\>\> x = ma.array(\[1, 2, 3\]) \>\>\> x.mask = \[0, 1, 0\] \>\>\> x masked\_array(data=\[1, --, 3\], mask=\[False, True, False\], fill\_value=999999)

### Unmasking an entry

To unmask one or several specific entries, we can just assign one or several new valid values to them:

> \>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[1, 2, 3\], mask=\[0, 0, 1\]) \>\>\> x masked\_array(data=\[1, 2, --\], mask=\[False, False, True\], fill\_value=999999) \>\>\> x\[-1\] = 5 \>\>\> x masked\_array(data=\[1, 2, 5\], mask=\[False, False, False\], fill\_value=999999)

<div class="note">

<div class="title">

Note

</div>

Unmasking an entry by direct assignment will silently fail if the masked array has a *hard* mask, as shown by the <span class="title-ref">\~MaskedArray.hardmask</span> attribute. This feature was introduced to prevent overwriting the mask. To force the unmasking of an entry where the array has a hard mask, the mask must first to be softened using the <span class="title-ref">soften\_mask</span> method before the allocation. It can be re-hardened with <span class="title-ref">harden\_mask</span> as follows:

\>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[1, 2, 3\], mask=\[0, 0, 1\], hard\_mask=True) \>\>\> x masked\_array(data=\[1, 2, --\], mask=\[False, False, True\], fill\_value=999999) \>\>\> x\[-1\] = 5 \>\>\> x masked\_array(data=\[1, 2, --\], mask=\[False, False, True\], fill\_value=999999) \>\>\> x.soften\_mask() masked\_array(data=\[1, 2, --\], mask=\[False, False, True\], fill\_value=999999) \>\>\> x\[-1\] = 5 \>\>\> x masked\_array(data=\[1, 2, 5\], mask=\[False, False, False\], fill\_value=999999) \>\>\> x.harden\_mask() masked\_array(data=\[1, 2, 5\], mask=\[False, False, False\], fill\_value=999999)

</div>

To unmask all masked entries of a masked array (provided the mask isn't a hard mask), the simplest solution is to assign the constant <span class="title-ref">nomask</span> to the mask:

> \>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[1, 2, 3\], mask=\[0, 0, 1\]) \>\>\> x masked\_array(data=\[1, 2, --\], mask=\[False, False, True\], fill\_value=999999) \>\>\> x.mask = ma.nomask \>\>\> x masked\_array(data=\[1, 2, 3\], mask=\[False, False, False\], fill\_value=999999)

## Indexing and slicing

As a <span class="title-ref">MaskedArray</span> is a subclass of <span class="title-ref">numpy.ndarray</span>, it inherits its mechanisms for indexing and slicing.

When accessing a single entry of a masked array with no named fields, the output is either a scalar (if the corresponding entry of the mask is `False`) or the special value <span class="title-ref">masked</span> (if the corresponding entry of the mask is `True`):

> \>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[1, 2, 3\], mask=\[0, 0, 1\]) \>\>\> x\[0\] 1 \>\>\> x\[-1\] masked \>\>\> x\[-1\] is ma.masked True

If the masked array has named fields, accessing a single entry returns a <span class="title-ref">numpy.void</span> object if none of the fields are masked, or a 0d masked array with the same dtype as the initial array if at least one of the fields is masked.

> \>\>\> import numpy.ma as ma \>\>\> y = ma.masked\_array(\[(1,2), (3, 4)\], ... mask=\[(0, 0), (0, 1)\], ... dtype=\[('a', int), ('b', int)\]) \>\>\> y\[0\] (1, 2) \>\>\> y\[-1\] (3, --)

When accessing a slice, the output is a masked array whose <span class="title-ref">\~MaskedArray.data</span> attribute is a view of the original data, and whose mask is either <span class="title-ref">nomask</span> (if there was no invalid entries in the original array) or a view of the corresponding slice of the original mask. The view is required to ensure propagation of any modification of the mask to the original.

> \>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[1, 2, 3, 4, 5\], mask=\[0, 1, 0, 0, 1\]) \>\>\> mx = x\[:3\] \>\>\> mx masked\_array(data=\[1, --, 3\], mask=\[False, True, False\], fill\_value=999999) \>\>\> mx\[1\] = -1 \>\>\> mx masked\_array(data=\[1, -1, 3\], mask=\[False, False, False\], fill\_value=999999) \>\>\> x.mask array(\[False, False, False, False, True\]) \>\>\> x.data array(\[ 1, -1, 3, 4, 5\])

Accessing a field of a masked array with structured datatype returns a <span class="title-ref">MaskedArray</span>.

## Operations on masked arrays

Arithmetic and comparison operations are supported by masked arrays. As much as possible, invalid entries of a masked array are not processed, meaning that the corresponding <span class="title-ref">\~MaskedArray.data</span> entries *should* be the same before and after the operation.

<div class="warning">

<div class="title">

Warning

</div>

We need to stress that this behavior may not be systematic, that masked data may be affected by the operation in some cases and therefore users should not rely on this data remaining unchanged.

</div>

The `numpy.ma` module comes with a specific implementation of most ufuncs. Unary and binary functions that have a validity domain (such as <span class="title-ref">\~numpy.log</span> or <span class="title-ref">\~numpy.divide</span>) return the <span class="title-ref">masked</span> constant whenever the input is masked or falls outside the validity domain:

> \>\>\> import numpy.ma as ma \>\>\> ma.log(\[-1, 0, 1, 2\]) masked\_array(data=\[--, --, 0.0, 0.6931471805599453\], mask=\[ True, True, False, False\], fill\_value=1e+20)

Masked arrays also support standard numpy ufuncs. The output is then a masked array. The result of a unary ufunc is masked wherever the input is masked. The result of a binary ufunc is masked wherever any of the input is masked. If the ufunc also returns the optional context output (a 3-element tuple containing the name of the ufunc, its arguments and its domain), the context is processed and entries of the output masked array are masked wherever the corresponding input fall outside the validity domain:

> \>\>\> import numpy.ma as ma \>\>\> x = ma.array(\[-1, 1, 0, 2, 3\], mask=\[0, 0, 0, 0, 1\]) \>\>\> np.log(x) masked\_array(data=\[--, 0.0, --, 0.6931471805599453, --\], mask=\[ True, False, True, False, True\], fill\_value=1e+20)

# Examples

## Data with a given value representing missing data

Let's consider a list of elements, `x`, where values of -9999. represent missing data. We wish to compute the average value of the data and the vector of anomalies (deviations from the average):

> \>\>\> import numpy.ma as ma \>\>\> x = \[0.,1.,-9999.,3.,4.\] \>\>\> mx = ma.masked\_values (x, -9999.) \>\>\> print(mx.mean()) 2.0 \>\>\> print(mx - mx.mean()) \[-2.0 -1.0 -- 1.0 2.0\] \>\>\> print(mx.anom()) \[-2.0 -1.0 -- 1.0 2.0\]

## Filling in the missing data

Suppose now that we wish to print that same data, but with the missing values replaced by the average value.

> \>\>\> import numpy.ma as ma \>\>\> mx = ma.masked\_values (x, -9999.) \>\>\> print(mx.filled(mx.mean())) \[0. 1. 2. 3. 4.\]

## Numerical operations

Numerical operations can be easily performed without worrying about missing values, dividing by zero, square roots of negative numbers, etc.:

    >>> import numpy.ma as ma
    >>> x = ma.array([1., -1., 3., 4., 5., 6.], mask=[0,0,0,0,1,0])
    >>> y = ma.array([1., 2., 0., 4., 5., 6.], mask=[0,0,0,0,0,1])
    >>> print(ma.sqrt(x/y))
    [1.0 -- -- 1.0 -- --]

Four values of the output are invalid: the first one comes from taking the square root of a negative number, the second from the division by zero, and the last two where the inputs were masked.

## Ignoring extreme values

Let's consider an array `d` of floats between 0 and 1. We wish to compute the average of the values of `d` while ignoring any data outside the range `[0.2, 0.9]`:

> \>\>\> import numpy as np \>\>\> import numpy.ma as ma \>\>\> d = np.linspace(0, 1, 20) \>\>\> print(d.mean() - ma.masked\_outside(d, 0.2, 0.9).mean()) -0.05263157894736836

---

maskedarray.md

---

# Masked arrays

Masked arrays are arrays that may have missing or invalid entries. The `numpy.ma` module provides a nearly work-alike replacement for numpy that supports data arrays with masks.

<div class="index">

single: masked arrays

</div>

<div class="toctree" data-maxdepth="2">

maskedarray.generic maskedarray.baseclass routines.ma

</div>

---

module_structure.md

---

# NumPy's module structure

NumPy has a large number of submodules. Most regular usage of NumPy requires only the main namespace and a smaller set of submodules. The rest either either special-purpose or niche namespaces.

## Main namespaces

Regular/recommended user-facing namespaces for general use:

  - \[numpy \<routines\>\](\#numpy-\<routines\>)
  - \[numpy.exceptions \<routines.exceptions\>\](\#numpy.exceptions-\<routines.exceptions\>)
  - \[numpy.fft \<routines.fft\>\](\#numpy.fft-\<routines.fft\>)
  - \[numpy.linalg \<routines.linalg\>\](\#numpy.linalg-\<routines.linalg\>)
  - \[numpy.polynomial \<numpy-polynomial\>\](\#numpy.polynomial-\<numpy-polynomial\>)
  - \[numpy.random \<numpyrandom\>\](\#numpy.random-\<numpyrandom\>)
  - \[numpy.strings \<routines.strings\>\](\#numpy.strings-\<routines.strings\>)
  - \[numpy.testing \<routines.testing\>\](\#numpy.testing-\<routines.testing\>)
  - \[numpy.typing \<typing\>\](\#numpy.typing-\<typing\>)

## Special-purpose namespaces

  - \[numpy.ctypeslib \<routines.ctypeslib\>\](\#numpy.ctypeslib-\<routines.ctypeslib\>) - interacting with NumPy objects with <span class="title-ref">ctypes</span>
  - \[numpy.dtypes \<routines.dtypes\>\](\#numpy.dtypes-\<routines.dtypes\>) - dtype classes (typically not used directly by end users)
  - \[numpy.emath \<routines.emath\>\](\#numpy.emath-\<routines.emath\>) - mathematical functions with automatic domain
  - \[numpy.lib \<routines.lib\>\](\#numpy.lib-\<routines.lib\>) - utilities & functionality which do not fit the main namespace
  - \[numpy.rec \<routines.rec\>\](\#numpy.rec-\<routines.rec\>) - record arrays (largely superseded by dataframe libraries)
  - \[numpy.version \<routines.version\>\](\#numpy.version-\<routines.version\>) - small module with more detailed version info

## Legacy namespaces

Prefer not to use these namespaces for new code. There are better alternatives and/or this code is deprecated or isn't reliable.

  - \[numpy.char \<routines.char\>\](\#numpy.char-\<routines.char\>) - legacy string functionality, only for fixed-width strings
  - \[numpy.distutils \<numpy-distutils-refguide\>\](\#numpy.distutils-\<numpy-distutils-refguide\>) (deprecated) - build system support
  - \[numpy.f2py \<python-module-numpy.f2py\>\](\#numpy.f2py-\<python-module-numpy.f2py\>) - Fortran binding generation (usually used from the command line only)
  - \[numpy.ma \<routines.ma\>\](\#numpy.ma-\<routines.ma\>) - masked arrays (not very reliable, needs an overhaul)
  - \[numpy.matlib \<routines.matlib\>\](\#numpy.matlib-\<routines.matlib\>) (pending deprecation) - functions supporting `matrix` instances

<div class="toctree" hidden="">

numpy.exceptions \<routines.exceptions\> numpy.fft \<routines.fft\> numpy.linalg \<routines.linalg\> numpy.polynomial \<routines.polynomials-package\> numpy.random \<random/index\> numpy.strings \<routines.strings\> numpy.testing \<routines.testing\> numpy.typing \<typing\> numpy.ctypeslib \<routines.ctypeslib\> numpy.dtypes \<routines.dtypes\> numpy.emath \<routines.emath\> numpy.lib \<routines.lib\> numpy.rec \<routines.rec\> numpy.version \<routines.version\> numpy.char \<routines.char\> numpy.distutils \<distutils\> numpy.f2py \<../f2py/index\> numpy.ma \<routines.ma\> numpy.matlib \<routines.matlib\>

</div>

---

index.md

---

<div class="currentmodule">

numpy.random

</div>

# Bit generators

The random values produced by <span class="title-ref">\~Generator</span> originate in a BitGenerator. The BitGenerators do not directly provide random numbers and only contains methods used for seeding, getting or setting the state, jumping or advancing the state, and for accessing low-level wrappers for consumption by code that can efficiently access the functions provided, e.g., [numba](https://numba.pydata.org).

## Supported BitGenerators

The included BitGenerators are:

  - PCG-64 - The default. A fast generator that can be advanced by an arbitrary amount. See the documentation for <span class="title-ref">\~.PCG64.advance</span>. PCG-64 has a period of \(2^{128}\). See the [PCG author's page](https://www.pcg-random.org/) for more details about this class of PRNG.
  - PCG-64 DXSM - An upgraded version of PCG-64 with better statistical properties in parallel contexts. See \[upgrading-pcg64\](\#upgrading-pcg64) for more information on these improvements.
  - MT19937 - The standard Python BitGenerator. Adds a <span class="title-ref">MT19937.jumped</span> function that returns a new generator with state as-if \(2^{128}\) draws have been made.
  - Philox - A counter-based generator capable of being advanced an arbitrary number of steps or generating independent streams. See the [Random123](https://www.deshawresearch.com/resources_random123.html) page for more details about this class of bit generators.
  - SFC64 - A fast generator based on random invertible mappings. Usually the fastest generator of the four. See the [SFC author's page](https://pracrand.sourceforge.net/RNG_engines.txt) for (a little) more detail.

<div class="autosummary" data-toctree="generated/">

BitGenerator

</div>

<div class="toctree" data-maxdepth="1">

MT19937 \<mt19937\> PCG64 \<pcg64\> PCG64DXSM \<pcg64dxsm\> Philox \<philox\> SFC64 \<sfc64\>

</div>

# Seeding and entropy

A BitGenerator provides a stream of random values. In order to generate reproducible streams, BitGenerators support setting their initial state via a seed. All of the provided BitGenerators will take an arbitrary-sized non-negative integer, or a list of such integers, as a seed. BitGenerators need to take those inputs and process them into a high-quality internal state for the BitGenerator. All of the BitGenerators in numpy delegate that task to <span class="title-ref">SeedSequence</span>, which uses hashing techniques to ensure that even low-quality seeds generate high-quality initial states.

`` `python     from numpy.random import PCG64      bg = PCG64(12345678903141592653589793)  .. end_block  `~SeedSequence` is designed to be convenient for implementing best practices. ``<span class="title-ref"> We recommend that a stochastic program defaults to using entropy from the OS so that each run is different. The program should print out or log that entropy. In order to reproduce a past value, the program should allow the user to provide that value through some mechanism, a command-line argument is common, so that the user can then re-enter that entropy to reproduce the result. </span>\~SeedSequence\` can take care of everything except for communicating with the user, which is up to you.

`` `python     from numpy.random import PCG64, SeedSequence      # Get the user's seed somehow, maybe through `argparse`.     # If the user did not provide a seed, it should return `None`.     seed = get_user_seed()     ss = SeedSequence(seed)     print('seed = {}'.format(ss.entropy))     bg = PCG64(ss)  .. end_block  We default to using a 128-bit integer using entropy gathered from the OS. This ``\` is a good amount of entropy to initialize all of the generators that we have in numpy. We do not recommend using small seeds below 32 bits for general use. Using just a small set of seeds to instantiate larger state spaces means that there are some initial states that are impossible to reach. This creates some biases if everyone uses such values.

There will not be anything *wrong* with the results, per se; even a seed of 0 is perfectly fine thanks to the processing that <span class="title-ref">\~SeedSequence</span> does. If you just need *some* fixed value for unit tests or debugging, feel free to use whatever seed you like. But if you want to make inferences from the results or publish them, drawing from a larger set of seeds is good practice.

If you need to generate a good seed "offline", then `SeedSequence().entropy` or using `secrets.randbits(128)` from the standard library are both convenient ways.

If you need to run several stochastic simulations in parallel, best practice is to construct a random generator instance for each simulation. To make sure that the random streams have distinct initial states, you can use the <span class="title-ref">spawn</span> method of <span class="title-ref">\~SeedSequence</span>. For instance, here we construct a list of 12 instances:

`` `python     from numpy.random import PCG64, SeedSequence      # High quality initial entropy     entropy = 0x87351080e25cb0fad77a44a3be03b491     base_seq = SeedSequence(entropy)     child_seqs = base_seq.spawn(12)    # a list of 12 SeedSequences     generators = [PCG64(seq) for seq in child_seqs]  .. end_block  If you already have an initial random generator instance, you can shorten ``<span class="title-ref"> the above by using the </span>\~BitGenerator.spawn\` method:

`` `python     from numpy.random import PCG64, SeedSequence     # High quality initial entropy     entropy = 0x87351080e25cb0fad77a44a3be03b491     base_bitgen = PCG64(entropy)     generators = base_bitgen.spawn(12)  An alternative way is to use the fact that a `~SeedSequence` can be initialized ``<span class="title-ref"> by a tuple of elements. Here we use a base entropy value and an integer </span><span class="title-ref">worker\_id</span>\`

`` `python     from numpy.random import PCG64, SeedSequence      # High quality initial entropy     entropy = 0x87351080e25cb0fad77a44a3be03b491         sequences = [SeedSequence((entropy, worker_id)) for worker_id in range(12)]     generators = [PCG64(seq) for seq in sequences]  .. end_block  Note that the sequences produced by the latter method will be distinct from ``<span class="title-ref"> those constructed via </span>\~SeedSequence.spawn\`.

<div class="autosummary" data-toctree="generated/">

SeedSequence

</div>

---

mt19937.md

---

# Mersenne Twister (MT19937)

<div class="currentmodule">

numpy.random

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

MT19937

</div>

## State

<div class="autosummary" data-toctree="generated/">

\~MT19937.state

</div>

## Parallel generation

<div class="autosummary" data-toctree="generated/">

\~MT19937.jumped

</div>

## Extending

<div class="autosummary" data-toctree="generated/">

\~MT19937.cffi \~MT19937.ctypes

</div>

---

pcg64.md

---

# Permuted congruential generator (64-bit, PCG64)

<div class="currentmodule">

numpy.random

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

PCG64

</div>

## State

<div class="autosummary" data-toctree="generated/">

\~PCG64.state

</div>

## Parallel generation

<div class="autosummary" data-toctree="generated/">

\~PCG64.advance \~PCG64.jumped

</div>

## Extending

<div class="autosummary" data-toctree="generated/">

\~PCG64.cffi \~PCG64.ctypes

</div>

---

pcg64dxsm.md

---

# Permuted congruential generator (64-bit, PCG64 DXSM)

<div class="currentmodule">

numpy.random

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

PCG64DXSM

</div>

## State

<div class="autosummary" data-toctree="generated/">

\~PCG64DXSM.state

</div>

## Parallel generation

<div class="autosummary" data-toctree="generated/">

\~PCG64DXSM.advance \~PCG64DXSM.jumped

</div>

## Extending

<div class="autosummary" data-toctree="generated/">

\~PCG64DXSM.cffi \~PCG64DXSM.ctypes

</div>

---

philox.md

---

# Philox counter-based RNG

<div class="currentmodule">

numpy.random

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

Philox

</div>

## State

<div class="autosummary" data-toctree="generated/">

\~Philox.state

</div>

## Parallel generation

<div class="autosummary" data-toctree="generated/">

\~Philox.advance \~Philox.jumped

</div>

## Extending

<div class="autosummary" data-toctree="generated/">

\~Philox.cffi \~Philox.ctypes

</div>

---

sfc64.md

---

# SFC64 Small Fast Chaotic PRNG

<div class="currentmodule">

numpy.random

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

SFC64

</div>

## State

<div class="autosummary" data-toctree="generated/">

\~SFC64.state

</div>

## Extending

<div class="autosummary" data-toctree="generated/">

\~SFC64.cffi \~SFC64.ctypes

</div>

---

c-api.md

---

# C API for random

<div class="currentmodule">

numpy.random

</div>

Access to various distributions below is available via Cython or C-wrapper libraries like CFFI. All the functions accept a :c`bitgen_t` as their first argument. To access these from Cython or C, you must link with the `npyrandom` static library which is part of the NumPy distribution, located in `numpy/random/lib`. Note that you must *also* link with `npymath`, see \[linking-npymath\](\#linking-npymath).

See \[extending\](extending.md) for examples of using these functions.

The functions are named with the following conventions:

  - "standard" refers to the reference values for any parameters. For instance "standard\_uniform" means a uniform distribution on the interval `0.0` to `1.0`
  - "fill" functions will fill the provided `out` with `cnt` values.
  - The functions without "standard" in their name require additional parameters to describe the distributions.
  - Functions with `inv` in their name are based on the slower inverse method instead of a ziggurat lookup algorithm, which is significantly faster. The non-ziggurat variants are used in corner cases and for legacy compatibility.

<!-- end list -->

  - `` ` .. c:function:: double random_noncentral_f(bitgen_t *bitgen_state, double dfnum, double dfden, double nonc) .. c:function:: double random_wald(bitgen_t *bitgen_state, double mean, double scale)  .. c:function:: double random_vonmises(bitgen_t *bitgen_state, double mu, double kappa)  .. c:function:: double random_triangular(bitgen_t *bitgen_state, double left, double mode, double right)  .. c:function:: npy_int64 random_poisson(bitgen_t *bitgen_state, double lam)  .. c:function:: npy_int64 random_negative_binomial(bitgen_t *bitgen_state, double n, double p)  .. c:type:: binomial_t ``\`c
    
      - typedef struct s\_binomial\_t {  
        int has\_binomial; /\* \!=0: following parameters initialized for binomial \*/ double psave; RAND\_INT\_TYPE nsave; double r; double q; double fm; RAND\_INT\_TYPE m; double p1; double xm; double xl; double xr; double c; double laml; double lamr; double p2; double p3; double p4;
    
    } binomial\_t;

Generate a single integer

Generate random uint64 numbers in closed interval \[off, off + rng\].

\`\`\`

---

compatibility.md

---

<div id="random-compatibility">

<div class="currentmodule">

numpy.random

</div>

</div>

# Compatibility policy

<span class="title-ref">numpy.random</span> has a somewhat stricter compatibility policy than the rest of NumPy. Users of pseudorandomness often have use cases for being able to reproduce runs in fine detail given the same seed (so-called "stream compatibility"), and so we try to balance those needs with the flexibility to enhance our algorithms. \[NEP 19 \<NEP19\>\](\#nep-19-\<nep19\>) describes the evolution of this policy.

The main kind of compatibility that we enforce is stream-compatibility from run to run under certain conditions. If you create a <span class="title-ref">Generator</span> with the same <span class="title-ref">BitGenerator</span>, with the same seed, perform the same sequence of method calls with the same arguments, on the same build of `numpy`, in the same environment, on the same machine, you should get the same stream of numbers. Note that these conditions are very strict. There are a number of factors outside of NumPy's control that limit our ability to guarantee much more than this. For example, different CPUs implement floating point arithmetic differently, and this can cause differences in certain edge cases that cascade to the rest of the stream. <span class="title-ref">Generator.multivariate\_normal</span>, for another example, uses a matrix decomposition from <span class="title-ref">numpy.linalg</span>. Even on the same platform, a different build of `numpy` may use a different version of this matrix decomposition algorithm from the LAPACK that it links to, causing <span class="title-ref">Generator.multivariate\_normal</span> to return completely different (but equally valid\!) results. We strive to prefer algorithms that are more resistant to these effects, but this is always imperfect.

\> **Note** \> Most of the <span class="title-ref">Generator</span> methods allow you to draw multiple values from a distribution as arrays. The requested size of this array is a parameter, for the purposes of the above policy. Calling `rng.random()` 5 times is not *guaranteed* to give the same numbers as `rng.random(5)`. We reserve the ability to decide to use different algorithms for different-sized blocks. In practice, this happens rarely.

Like the rest of NumPy, we generally maintain API source compatibility from version to version. If we *must* make an API-breaking change, then we will only do so with an appropriate deprecation period and warnings, according to \[general NumPy policy \<NEP23\>\](\#general-numpy-policy-\<nep23\>).

Breaking stream-compatibility in order to introduce new features or improve performance in <span class="title-ref">Generator</span> or <span class="title-ref">default\_rng</span> will be *allowed* with *caution*. Such changes will be considered features, and as such will be no faster than the standard release cadence of features (i.e. on `X.Y` releases, never `X.Y.Z`). Slowness will not be considered a bug for this purpose. Correctness bug fixes that break stream-compatibility can happen on bugfix releases, per usual, but developers should consider if they can wait until the next feature release. We encourage developers to strongly weight userâ€™s pain from the break in stream-compatibility against the improvements. One example of a worthwhile improvement would be to change algorithms for a significant increase in performance, for example, moving from the [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) method of Gaussian variate generation to the faster [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm). An example of a discouraged improvement would be tweaking the Ziggurat tables just a little bit for a small performance improvement.

\> **Note** \> In particular, <span class="title-ref">default\_rng</span> is allowed to change the default <span class="title-ref">BitGenerator</span> that it uses (again, with *caution* and plenty of advance warning).

In general, <span class="title-ref">BitGenerator</span> classes have stronger guarantees of version-to-version stream compatibility. This allows them to be a firmer building block for downstream users that need it. Their limited API surface makes it easier for them to maintain this compatibility from version to version. See the docstrings of each <span class="title-ref">BitGenerator</span> class for their individual compatibility guarantees.

The legacy <span class="title-ref">RandomState</span> and the \[associated convenience functions \<functions-in-numpy-random\>\](\#associated-convenience-functions \<functions-in-numpy-random\>) have a stricter version-to-version compatibility guarantee. For reasons outlined in \[NEP 19 \<NEP19\>\](\#nep-19-\<nep19\>), we had made stronger promises about their version-to-version stability early in NumPy's development. There are still some limited use cases for this kind of compatibility (like generating data for tests), so we maintain as much compatibility as we can. There will be no more modifications to <span class="title-ref">RandomState</span>, not even to fix correctness bugs. There are a few gray areas where we can make minor fixes to keep <span class="title-ref">RandomState</span> working without segfaulting as NumPy's internals change, and some docstring fixes. However, the previously-mentioned caveats about the variability from machine to machine and build to build still apply to <span class="title-ref">RandomState</span> just as much as it does to <span class="title-ref">Generator</span>.

---

cffi.md

---

# Extending via CFFI

<div class="literalinclude" data-language="python">

../../../../../numpy/random/\_examples/cffi/extending.py

</div>

---

extending.pyx.md

---

# extending.pyx

<div class="literalinclude" data-language="cython">

../../../../../../numpy/random/\_examples/cython/extending.pyx

</div>

---

extending_distributions.pyx.md

---

# extending\_distributions.pyx

<div class="literalinclude" data-language="cython">

../../../../../../numpy/random/\_examples/cython/extending\_distributions.pyx

</div>

---

index.md

---

# Extending <span class="title-ref">numpy.random</span> via Cython

<div id="note">

Starting with NumPy 1.26.0, Meson is the default build system for NumPy. See \[distutils-status-migration\](\#distutils-status-migration).

</div>

<div class="toctree">

meson.build.rst extending.pyx extending\_distributions.pyx

</div>

---

meson.build.md

---

# meson.build

<div class="literalinclude" data-language="python">

../../../../../../numpy/random/\_examples/cython/meson.build

</div>

---

numba.md

---

# Extending via Numba

<div class="literalinclude" data-language="python">

../../../../../numpy/random/\_examples/numba/extending.py

</div>

---

numba_cffi.md

---

# Extending via Numba and CFFI

<div class="literalinclude" data-language="python">

../../../../../numpy/random/\_examples/numba/extending\_distributions.py

</div>

---

extending.md

---

<div class="currentmodule">

numpy.random

</div>

# Extending

The <span class="title-ref">BitGenerator</span>s have been designed to be extendable using standard tools for high-performance Python -- numba and Cython. The <span class="title-ref">Generator</span> object can also be used with user-provided <span class="title-ref">BitGenerator</span>s as long as these export a small set of required functions.

## Numba

Numba can be used with either CTypes or CFFI. The current iteration of the <span class="title-ref">BitGenerator</span>s all export a small set of functions through both interfaces.

This example shows how numba can be used to produce gaussian samples using a pure Python implementation which is then compiled. The random numbers are provided by `ctypes.next_double`.

<div class="literalinclude" data-language="python" data-end-before="example 2">

../../../../numpy/random/\_examples/numba/extending.py

</div>

Both CTypes and CFFI allow the more complicated distributions to be used directly in Numba after compiling the file distributions.c into a `DLL` or `so`. An example showing the use of a more complicated distribution is in the [Examples]() section below.

## Cython

Cython can be used to unpack the `PyCapsule` provided by a <span class="title-ref">BitGenerator</span>. This example uses <span class="title-ref">PCG64</span> and the example from above. The usual caveats for writing high-performance code using Cython -- removing bounds checks and wrap around, providing array alignment information -- still apply.

<div class="literalinclude" data-language="cython" data-end-before="example 2">

../../../../numpy/random/\_examples/cython/extending\_distributions.pyx

</div>

The <span class="title-ref">BitGenerator</span> can also be directly accessed using the members of the `bitgen_t` struct.

<div class="literalinclude" data-language="cython" data-start-after="example 2" data-end-before="example 3">

../../../../numpy/random/\_examples/cython/extending\_distributions.pyx

</div>

Cython can be used to directly access the functions in `numpy/random/c_distributions.pxd`. This requires linking with the `npyrandom` library located in `numpy/random/lib`.

<div class="literalinclude" data-language="cython" data-start-after="example 3">

../../../../numpy/random/\_examples/cython/extending\_distributions.pyx

</div>

See \[extending\_cython\_example\](\#extending\_cython\_example) for the complete listings of these examples and a minimal `setup.py` to build the c-extension modules.

## CFFI

CFFI can be used to directly access the functions in `include/numpy/random/distributions.h`. Some "massaging" of the header file is required:

<div class="literalinclude" data-language="python" data-end-before="dlopen">

../../../../numpy/random/\_examples/cffi/extending.py

</div>

Once the header is parsed by `ffi.cdef`, the functions can be accessed directly from the `_generator` shared object, using the <span class="title-ref">BitGenerator.cffi</span> interface.

<div class="literalinclude" data-language="python" data-start-after="dlopen">

../../../../numpy/random/\_examples/cffi/extending.py

</div>

## New BitGenerators

<span class="title-ref">Generator</span> can be used with user-provided <span class="title-ref">BitGenerator</span>s. The simplest way to write a new <span class="title-ref">BitGenerator</span> is to examine the pyx file of one of the existing <span class="title-ref">BitGenerator</span>s. The key structure that must be provided is the `capsule` which contains a `PyCapsule` to a struct pointer of type `bitgen_t`,

`` `c   typedef struct bitgen {     void *state;     uint64_t (*next_uint64)(void *st);     uint32_t (*next_uint32)(void *st);     double (*next_double)(void *st);     uint64_t (*next_raw)(void *st);   } bitgen_t;  which provides 5 pointers. The first is an opaque pointer to the data structure ``<span class="title-ref"> used by the \`BitGenerator</span>s. The next three are function pointers which return the next 64- and 32-bit unsigned integers, the next random double and the next raw value. This final function is used for testing and so can be set to the next 64-bit unsigned integer function if not needed. Functions inside <span class="title-ref">Generator</span> use this structure as in

`` `c   bitgen_state->next_uint64(bitgen_state->state)  Examples ``\` --------

<div class="toctree">

Numba \<examples/numba\> CFFI + Numba \<examples/numba\_cffi\> Cython \<examples/cython/index\> CFFI \<examples/cffi\>

</div>

---

generator.md

---

<div class="currentmodule">

numpy.random

</div>

# Random `Generator`

The <span class="title-ref">Generator</span> provides access to a wide range of distributions, and served as a replacement for <span class="title-ref">\~numpy.random.RandomState</span>. The main difference between the two is that <span class="title-ref">Generator</span> relies on an additional BitGenerator to manage state and generate the random bits, which are then transformed into random values from useful distributions. The default BitGenerator used by <span class="title-ref">Generator</span> is <span class="title-ref">PCG64</span>. The BitGenerator can be changed by passing an instantized BitGenerator to <span class="title-ref">Generator</span>.

<div class="autofunction">

default\_rng

</div>

<div class="autoclass" data-members="__init__" data-exclude-members="__init__">

Generator

</div>

## Accessing the BitGenerator and spawning

<div class="autosummary" data-toctree="generated/">

\~numpy.random.Generator.bit\_generator \~numpy.random.Generator.spawn

</div>

## Simple random data

<div class="autosummary" data-toctree="generated/">

\~numpy.random.Generator.integers \~numpy.random.Generator.random \~numpy.random.Generator.choice \~numpy.random.Generator.bytes

</div>

## Permutations

The methods for randomly permuting a sequence are

<div class="autosummary" data-toctree="generated/">

\~numpy.random.Generator.shuffle \~numpy.random.Generator.permutation \~numpy.random.Generator.permuted

</div>

The following table summarizes the behaviors of the methods.

| method      | copy/in-place                   | axis handling    |
| ----------- | ------------------------------- | ---------------- |
| shuffle     | in-place                        | as if 1d         |
| permutation | copy                            | as if 1d         |
| permuted    | either (use 'out' for in-place) | axis independent |

The following subsections provide more details about the differences.

### In-place vs. copy

The main difference between <span class="title-ref">Generator.shuffle</span> and <span class="title-ref">Generator.permutation</span> is that <span class="title-ref">Generator.shuffle</span> operates in-place, while <span class="title-ref">Generator.permutation</span> returns a copy.

By default, <span class="title-ref">Generator.permuted</span> returns a copy. To operate in-place with <span class="title-ref">Generator.permuted</span>, pass the same array as the first argument *and* as the value of the `out` parameter. For example,

> \>\>\> import numpy as np \>\>\> rng = np.random.default\_rng() \>\>\> x = np.arange(0, 15).reshape(3, 5) \>\>\> x \#doctest: +SKIP array(\[\[ 0, 1, 2, 3, 4\], \[ 5, 6, 7, 8, 9\], \[10, 11, 12, 13, 14\]\]) \>\>\> y = rng.permuted(x, axis=1, out=x) \>\>\> x \#doctest: +SKIP array(\[\[ 1, 0, 2, 4, 3\], \# random \[ 6, 7, 8, 9, 5\], \[10, 14, 11, 13, 12\]\])
> 
> Note that when `out` is given, the return value is `out`:
> 
> \>\>\> y is x True

### Handling the `axis` parameter

An important distinction for these methods is how they handle the `axis` parameter. Both <span class="title-ref">Generator.shuffle</span> and <span class="title-ref">Generator.permutation</span> treat the input as a one-dimensional sequence, and the `axis` parameter determines which dimension of the input array to use as the sequence. In the case of a two-dimensional array, `axis=0` will, in effect, rearrange the rows of the array, and `axis=1` will rearrange the columns. For example

> \>\>\> import numpy as np \>\>\> rng = np.random.default\_rng() \>\>\> x = np.arange(0, 15).reshape(3, 5) \>\>\> x array(\[\[ 0, 1, 2, 3, 4\], \[ 5, 6, 7, 8, 9\], \[10, 11, 12, 13, 14\]\]) \>\>\> rng.permutation(x, axis=1) \#doctest: +SKIP array(\[\[ 1, 3, 2, 0, 4\], \# random \[ 6, 8, 7, 5, 9\], \[11, 13, 12, 10, 14\]\])

Note that the columns have been rearranged "in bulk": the values within each column have not changed.

The method <span class="title-ref">Generator.permuted</span> treats the `axis` parameter similar to how <span class="title-ref">numpy.sort</span> treats it. Each slice along the given axis is shuffled independently of the others. Compare the following example of the use of <span class="title-ref">Generator.permuted</span> to the above example of \`Generator.permutation\`:

> \>\>\> import numpy as np \>\>\> rng = np.random.default\_rng() \>\>\> rng.permuted(x, axis=1) \#doctest: +SKIP array(\[\[ 1, 0, 2, 4, 3\], \# random \[ 5, 7, 6, 9, 8\], \[10, 14, 12, 13, 11\]\])

In this example, the values within each row (i.e. the values along `axis=1`) have been shuffled independently. This is not a "bulk" shuffle of the columns.

### Shuffling non-NumPy sequences

<span class="title-ref">Generator.shuffle</span> works on non-NumPy sequences. That is, if it is given a sequence that is not a NumPy array, it shuffles that sequence in-place.

> \>\>\> import numpy as np \>\>\> rng = np.random.default\_rng() \>\>\> a = \['A', 'B', 'C', 'D', 'E'\] \>\>\> rng.shuffle(a) \# shuffle the list in-place \>\>\> a \#doctest: +SKIP \['B', 'D', 'A', 'E', 'C'\] \# random

## Distributions

<div class="autosummary" data-toctree="generated/">

\~numpy.random.Generator.beta \~numpy.random.Generator.binomial \~numpy.random.Generator.chisquare \~numpy.random.Generator.dirichlet \~numpy.random.Generator.exponential \~numpy.random.Generator.f \~numpy.random.Generator.gamma \~numpy.random.Generator.geometric \~numpy.random.Generator.gumbel \~numpy.random.Generator.hypergeometric \~numpy.random.Generator.laplace \~numpy.random.Generator.logistic \~numpy.random.Generator.lognormal \~numpy.random.Generator.logseries \~numpy.random.Generator.multinomial \~numpy.random.Generator.multivariate\_hypergeometric \~numpy.random.Generator.multivariate\_normal \~numpy.random.Generator.negative\_binomial \~numpy.random.Generator.noncentral\_chisquare \~numpy.random.Generator.noncentral\_f \~numpy.random.Generator.normal \~numpy.random.Generator.pareto \~numpy.random.Generator.poisson \~numpy.random.Generator.power \~numpy.random.Generator.rayleigh \~numpy.random.Generator.standard\_cauchy \~numpy.random.Generator.standard\_exponential \~numpy.random.Generator.standard\_gamma \~numpy.random.Generator.standard\_normal \~numpy.random.Generator.standard\_t \~numpy.random.Generator.triangular \~numpy.random.Generator.uniform \~numpy.random.Generator.vonmises \~numpy.random.Generator.wald \~numpy.random.Generator.weibull \~numpy.random.Generator.zipf

</div>

---

index.md

---

<div id="numpyrandom">

</div>

<div class="currentmodule">

numpy.random

</div>

# Random sampling (`numpy.random`)

## Quick start

The `numpy.random` module implements pseudo-random number generators (PRNGs or RNGs, for short) with the ability to draw samples from a variety of probability distributions. In general, users will create a <span class="title-ref">Generator</span> instance with <span class="title-ref">default\_rng</span> and call the various methods on it to obtain samples from different distributions.

> \>\>\> import numpy as np \>\>\> rng = np.random.default\_rng() \# Generate one random float uniformly distributed over the range \[0, 1) \>\>\> rng.random() \#doctest: +SKIP 0.06369197489564249 \# may vary \# Generate an array of 10 numbers according to a unit Gaussian distribution \>\>\> rng.standard\_normal(10) \#doctest: +SKIP array(\[-0.31018314, -1.8922078 , -0.3628523 , -0.63526532, 0.43181166, \# may vary 0.51640373, 1.25693945, 0.07779185, 0.84090247, -2.13406828\]) \# Generate an array of 5 integers uniformly over the range \[0, 10) \>\>\> rng.integers(low=0, high=10, size=5) \#doctest: +SKIP array(\[8, 7, 6, 2, 0\]) \# may vary

Our RNGs are deterministic sequences and can be reproduced by specifying a seed integer to derive its initial state. By default, with no seed provided, <span class="title-ref">default\_rng</span> will seed the RNG from nondeterministic data from the operating system and therefore generate different numbers each time. The pseudo-random sequences will be independent for all practical purposes, at least those purposes for which our pseudo-randomness was good for in the first place.

> \>\>\> import numpy as np \>\>\> rng1 = np.random.default\_rng() \>\>\> rng1.random() \#doctest: +SKIP 0.6596288841243357 \# may vary \>\>\> rng2 = np.random.default\_rng() \>\>\> rng2.random() \#doctest: +SKIP 0.11885628817151628 \# may vary

\> **Warning** \> The pseudo-random number generators implemented in this module are designed for statistical modeling and simulation. They are not suitable for security or cryptographic purposes. See the :py`secrets` module from the standard library for such use cases.

<div id="recommend-secrets-randbits">

Seeds should be large positive integers. <span class="title-ref">default\_rng</span> can take positive integers of any size. We recommend using very large, unique numbers to ensure that your seed is different from anyone else's. This is good practice to ensure that your results are statistically independent from theirs unless you are intentionally *trying* to reproduce their result. A convenient way to get such a seed number is to use :py\`secrets.randbits\` to get an arbitrary 128-bit integer.

</div>

> \>\>\> import numpy as np \>\>\> import secrets \>\>\> import numpy as np \>\>\> secrets.randbits(128) \#doctest: +SKIP 122807528840384100672342137672332424406 \# may vary \>\>\> rng1 = np.random.default\_rng(122807528840384100672342137672332424406) \>\>\> rng1.random() 0.5363922081269535 \>\>\> rng2 = np.random.default\_rng(122807528840384100672342137672332424406) \>\>\> rng2.random() 0.5363922081269535

See the documentation on <span class="title-ref">default\_rng</span> and <span class="title-ref">SeedSequence</span> for more advanced options for controlling the seed in specialized scenarios.

<span class="title-ref">Generator</span> and its associated infrastructure was introduced in NumPy version 1.17.0. There is still a lot of code that uses the older <span class="title-ref">RandomState</span> and the functions in <span class="title-ref">numpy.random</span>. While there are no plans to remove them at this time, we do recommend transitioning to <span class="title-ref">Generator</span> as you can. The algorithms are faster, more flexible, and will receive more improvements in the future. For the most part, <span class="title-ref">Generator</span> can be used as a replacement for <span class="title-ref">RandomState</span>. See \[legacy\](\#legacy) for information on the legacy infrastructure, \[new-or-different\](\#new-or-different) for information on transitioning, and \[NEP 19 \<NEP19\>\](\#nep-19 \<nep19\>) for some of the reasoning for the transition.

## Design

Users primarily interact with <span class="title-ref">Generator</span> instances. Each <span class="title-ref">Generator</span> instance owns a <span class="title-ref">BitGenerator</span> instance that implements the core RNG algorithm. The <span class="title-ref">BitGenerator</span> has a limited set of responsibilities. It manages state and provides functions to produce random doubles and random unsigned 32- and 64-bit values.

The <span class="title-ref">Generator</span> takes the bit generator-provided stream and transforms them into more useful distributions, e.g., simulated normal random values. This structure allows alternative bit generators to be used with little code duplication.

NumPy implements several different <span class="title-ref">BitGenerator</span> classes implementing different RNG algorithms. <span class="title-ref">default\_rng</span> currently uses <span class="title-ref">\~PCG64</span> as the default <span class="title-ref">BitGenerator</span>. It has better statistical properties and performance than the <span class="title-ref">\~MT19937</span> algorithm used in the legacy <span class="title-ref">RandomState</span>. See \[random-bit-generators\](\#random-bit-generators) for more details on the supported BitGenerators.

<span class="title-ref">default\_rng</span> and BitGenerators delegate the conversion of seeds into RNG states to <span class="title-ref">SeedSequence</span> internally. <span class="title-ref">SeedSequence</span> implements a sophisticated algorithm that intermediates between the user's input and the internal implementation details of each <span class="title-ref">BitGenerator</span> algorithm, each of which can require different amounts of bits for its state. Importantly, it lets you use arbitrary-sized integers and arbitrary sequences of such integers to mix together into the RNG state. This is a useful primitive for constructing a \[flexible pattern for parallel RNG streams \<seedsequence-spawn\>\](\#flexible-pattern-for-parallel-rng-streams-\<seedsequence-spawn\>).

For backward compatibility, we still maintain the legacy <span class="title-ref">RandomState</span> class. It continues to use the <span class="title-ref">\~MT19937</span> algorithm by default, and old seeds continue to reproduce the same results. The convenience \[functions-in-numpy-random\](\#functions-in-numpy-random) are still aliases to the methods on a single global <span class="title-ref">RandomState</span> instance. See \[legacy\](\#legacy) for the complete details. See \[new-or-different\](\#new-or-different) for a detailed comparison between <span class="title-ref">Generator</span> and <span class="title-ref">RandomState</span>.

### Parallel Generation

The included generators can be used in parallel, distributed applications in a number of ways:

  - \[seedsequence-spawn\](\#seedsequence-spawn)
  - \[sequence-of-seeds\](\#sequence-of-seeds)
  - \[independent-streams\](\#independent-streams)
  - \[parallel-jumped\](\#parallel-jumped)

Users with a very large amount of parallelism will want to consult \[upgrading-pcg64\](\#upgrading-pcg64).

## Concepts

<div class="toctree" data-maxdepth="1">

generator Legacy Generator (RandomState) \<legacy\> BitGenerators, SeedSequences \<bit\_generators/index\> Upgrading PCG64 with PCG64DXSM \<upgrading-pcg64\> compatibility

</div>

## Features

<div class="toctree" data-maxdepth="2">

Parallel Applications \<parallel\> Multithreaded Generation \<multithreading\> new-or-different Comparing Performance \<performance\> c-api Examples of using Numba, Cython, CFFI \<extending\>

</div>

### Original Source of the Generator and BitGenerators

This package was developed independently of NumPy and was integrated in version 1.17.0. The original repo is at <https://github.com/bashtage/randomgen>.

---

legacy.md

---

<div class="currentmodule">

numpy.random

</div>

# Legacy random generation

The <span class="title-ref">RandomState</span> provides access to legacy generators. This generator is considered frozen and will have no further improvements. It is guaranteed to produce the same values as the final point release of NumPy v1.16. These all depend on Box-Muller normals or inverse CDF exponentials or gammas. This class should only be used if it is essential to have randoms that are identical to what would have been produced by previous versions of NumPy.

<span class="title-ref">RandomState</span> adds additional information to the state which is required when using Box-Muller normals since these are produced in pairs. It is important to use <span class="title-ref">RandomState.get\_state</span>, and not the underlying bit generators <span class="title-ref">state</span>, when accessing the state so that these extra values are saved.

Although we provide the <span class="title-ref">MT19937</span> BitGenerator for use independent of <span class="title-ref">RandomState</span>, note that its default seeding uses <span class="title-ref">SeedSequence</span> rather than the legacy seeding algorithm. <span class="title-ref">RandomState</span> will use the legacy seeding algorithm. The methods to use the legacy seeding algorithm are currently private as the main reason to use them is just to implement <span class="title-ref">RandomState</span>. However, one can reset the state of <span class="title-ref">MT19937</span> using the state of the \`RandomState\`:

`` `python    from numpy.random import MT19937    from numpy.random import RandomState     rs = RandomState(12345)    mt19937 = MT19937()    mt19937.state = rs.get_state()    rs2 = RandomState(mt19937)     # Same output    rs.standard_normal()    rs2.standard_normal()     rs.random()    rs2.random()     rs.standard_exponential()    rs2.standard_exponential()   .. autoclass:: RandomState     :members: __init__     :exclude-members: __init__  Seeding and state ``\` =================

<div class="autosummary" data-toctree="generated/">

\~RandomState.get\_state \~RandomState.set\_state \~RandomState.seed

</div>

## Simple random data

<div class="autosummary" data-toctree="generated/">

\~RandomState.rand \~RandomState.randn \~RandomState.randint \~RandomState.random\_integers \~RandomState.random\_sample \~RandomState.choice \~RandomState.bytes

</div>

## Permutations

<div class="autosummary" data-toctree="generated/">

\~RandomState.shuffle \~RandomState.permutation

</div>

## Distributions

<div class="autosummary" data-toctree="generated/">

\~RandomState.beta \~RandomState.binomial \~RandomState.chisquare \~RandomState.dirichlet \~RandomState.exponential \~RandomState.f \~RandomState.gamma \~RandomState.geometric \~RandomState.gumbel \~RandomState.hypergeometric \~RandomState.laplace \~RandomState.logistic \~RandomState.lognormal \~RandomState.logseries \~RandomState.multinomial \~RandomState.multivariate\_normal \~RandomState.negative\_binomial \~RandomState.noncentral\_chisquare \~RandomState.noncentral\_f \~RandomState.normal \~RandomState.pareto \~RandomState.poisson \~RandomState.power \~RandomState.rayleigh \~RandomState.standard\_cauchy \~RandomState.standard\_exponential \~RandomState.standard\_gamma \~RandomState.standard\_normal \~RandomState.standard\_t \~RandomState.triangular \~RandomState.uniform \~RandomState.vonmises \~RandomState.wald \~RandomState.weibull \~RandomState.zipf

</div>

## Functions in <span class="title-ref">numpy.random</span>

Many of the RandomState methods above are exported as functions in <span class="title-ref">numpy.random</span> This usage is discouraged, as it is implemented via a global <span class="title-ref">RandomState</span> instance which is not advised on two counts:

  - It uses global state, which means results will change as the code changes
  - It uses a <span class="title-ref">RandomState</span> rather than the more modern <span class="title-ref">Generator</span>.

For backward compatible legacy reasons, we will not change this.

<div class="autosummary" data-toctree="generated/">

beta binomial bytes chisquare choice dirichlet exponential f gamma geometric get\_state gumbel hypergeometric laplace logistic lognormal logseries multinomial multivariate\_normal negative\_binomial noncentral\_chisquare noncentral\_f normal pareto permutation poisson power rand randint randn random random\_integers random\_sample ranf rayleigh sample seed set\_state shuffle standard\_cauchy standard\_exponential standard\_gamma standard\_normal standard\_t triangular uniform vonmises wald weibull zipf

</div>

---

multithreading.md

---

# Multithreaded generation

The four core distributions (<span class="title-ref">\~.Generator.random</span>, <span class="title-ref">\~.Generator.standard\_normal</span>, <span class="title-ref">\~.Generator.standard\_exponential</span>, and <span class="title-ref">\~.Generator.standard\_gamma</span>) all allow existing arrays to be filled using the `out` keyword argument. Existing arrays need to be contiguous and well-behaved (writable and aligned). Under normal circumstances, arrays created using the common constructors such as <span class="title-ref">numpy.empty</span> will satisfy these requirements.

This example makes use of Python 3 `concurrent.futures` to fill an array using multiple threads. Threads are long-lived so that repeated calls do not require any additional overheads from thread creation.

The random numbers generated are reproducible in the sense that the same seed will produce the same outputs, given that the number of threads does not change.

`` `ipython     from numpy.random import default_rng, SeedSequence     import multiprocessing     import concurrent.futures     import numpy as np      class MultithreadedRNG:         def __init__(self, n, seed=None, threads=None):             if threads is None:                 threads = multiprocessing.cpu_count()             self.threads = threads              seq = SeedSequence(seed)             self._random_generators = [default_rng(s)                                        for s in seq.spawn(threads)]              self.n = n             self.executor = concurrent.futures.ThreadPoolExecutor(threads)             self.values = np.empty(n)             self.step = np.ceil(n / threads).astype(np.int_)          def fill(self):             def _fill(random_state, out, first, last):                 random_state.standard_normal(out=out[first:last])              futures = {}             for i in range(self.threads):                 args = (_fill,                         self._random_generators[i],                         self.values,                         i * self.step,                         (i + 1) * self.step)                 futures[self.executor.submit(*args)] = i             concurrent.futures.wait(futures)          def __del__(self):             self.executor.shutdown(False)    The multithreaded random number generator can be used to fill an array. ``<span class="title-ref"> The </span><span class="title-ref">values</span>\` attributes shows the zero-value before the fill and the random value after.

`` `ipython     In [2]: mrng = MultithreadedRNG(10000000, seed=12345)        ...: print(mrng.values[-1])     Out[2]: 0.0      In [3]: mrng.fill()        ...: print(mrng.values[-1])     Out[3]: 2.4545724517479104  The time required to produce using multiple threads can be compared to ``\` the time required to generate using a single thread.

`` `ipython     In [4]: print(mrng.threads)        ...: %timeit mrng.fill()      Out[4]: 4        ...: 32.8 ms Â± 2.71 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)  The single threaded call directly uses the BitGenerator.  .. code-block:: ipython      In [5]: values = np.empty(10000000)        ...: rg = default_rng()        ...: %timeit rg.standard_normal(out=values)      Out[5]: 99.6 ms Â± 222 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)  The gains are substantial and the scaling is reasonable even for arrays that ``\` are only moderately large. The gains are even larger when compared to a call that does not use an existing array due to array creation overhead.

`` `ipython     In [6]: rg = default_rng()        ...: %timeit rg.standard_normal(10000000)      Out[6]: 125 ms Â± 309 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)  Note that if ``threads`is not set by the user, it will be determined by`<span class="title-ref"> </span><span class="title-ref">multiprocessing.cpu\_count()</span>\`.

`` `ipython In [7]: # simulate the behavior for `threads=None`, if the machine had only one thread    ...: mrng = MultithreadedRNG(10000000, seed=12345, threads=1)    ...: print(mrng.values[-1]) Out[7]: 1.1800150052158556 ``\`

---

new-or-different.md

---

<div id="new-or-different">

<div class="currentmodule">

numpy.random

</div>

</div>

# What's new or different

NumPy 1.17.0 introduced <span class="title-ref">Generator</span> as an improved replacement for the \[legacy \<legacy\>\](\#legacy-\<legacy\>) <span class="title-ref">RandomState</span>. Here is a quick comparison of the two implementations.

<table>
<thead>
<tr class="header">
<th>Feature</th>
<th>Older Equivalent</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><span class="title-ref">Generator</span></p></td>
<td><p><span class="title-ref">RandomState</span></p></td>
<td><p><span class="title-ref">Generator</span> requires a stream source, called a <span class="title-ref">BitGenerator</span> A number of these are provided. <span class="title-ref">RandomState</span> uses the Mersenne Twister <span class="title-ref">MT19937</span> by default, but can also be instantiated with any BitGenerator.</p></td>
</tr>
<tr class="even">
<td>-----------------------</td>
<td>------------------</td>
<td>-------------</td>
</tr>
<tr class="odd">
<td><p><span class="title-ref">~.Generator.random</span></p></td>
<td><p><span class="title-ref">random_sample</span>, <span class="title-ref">rand</span></p></td>
<td><p>Access the values in a BitGenerator, convert them to <code>float64</code> in the interval <code>[0.0., 1.0)</code>. In addition to the <code>size</code> kwarg, now supports <code>dtype='d'</code> or <code>dtype='f'</code>, and an <code>out</code> kwarg to fill a user-supplied array.</p>
<p>Many other distributions are also supported.</p></td>
</tr>
<tr class="even">
<td>-----------------------</td>
<td>------------------</td>
<td>-------------</td>
</tr>
<tr class="odd">
<td><p><span class="title-ref">~.Generator.integers</span></p></td>
<td><p><span class="title-ref">randint</span>, <span class="title-ref">random_integers</span></p></td>
<td><p>Use the <code>endpoint</code> kwarg to adjust the inclusion or exclusion of the <code>high</code> interval endpoint.</p></td>
</tr>
</tbody>
</table>

  - The normal, exponential and gamma generators use 256-step Ziggurat methods which are 2-10 times faster than NumPy's default implementation in <span class="title-ref">\~.Generator.standard\_normal</span>, <span class="title-ref">\~.Generator.standard\_exponential</span> or <span class="title-ref">\~.Generator.standard\_gamma</span>. Because of the change in algorithms, it is not possible to reproduce the exact random values using <span class="title-ref">Generator</span> for these distributions or any distribution method that relies on them.

<div class="ipython">

python

import numpy.random rng = np.random.default\_rng() %timeit -n 1 rng.standard\_normal(100000) %timeit -n 1 numpy.random.standard\_normal(100000)

</div>

<div class="ipython">

python

%timeit -n 1 rng.standard\_exponential(100000) %timeit -n 1 numpy.random.standard\_exponential(100000)

</div>

<div class="ipython">

python

%timeit -n 1 rng.standard\_gamma(3.0, 100000) %timeit -n 1 numpy.random.standard\_gamma(3.0, 100000)

</div>

  - <span class="title-ref">\~.Generator.integers</span> is now the canonical way to generate integer random numbers from a discrete uniform distribution. This replaces both <span class="title-ref">randint</span> and the deprecated <span class="title-ref">random\_integers</span>.
  - The <span class="title-ref">rand</span> and <span class="title-ref">randn</span> methods are only available through the legacy <span class="title-ref">\~.RandomState</span>.
  - <span class="title-ref">Generator.random</span> is now the canonical way to generate floating-point random numbers, which replaces <span class="title-ref">RandomState.random\_sample</span>, <span class="title-ref">sample</span>, and <span class="title-ref">ranf</span>, all of which were aliases. This is consistent with Python's <span class="title-ref">random.random</span>.
  - All bit generators can produce doubles, uint64s and uint32s via CTypes (<span class="title-ref">\~PCG64.ctypes</span>) and CFFI (<span class="title-ref">\~PCG64.cffi</span>). This allows these bit generators to be used in numba.
  - The bit generators can be used in downstream projects via Cython.

<!-- end list -->

  - \* All bit generators use <span class="title-ref">SeedSequence</span> to \[convert seed integers to  
    initialized states \<seeding\_and\_entropy\>\](\#convert-seed-integers-to

\--initialized-states-\<seeding\_and\_entropy\>). \* Optional `dtype` argument that accepts `np.float32` or `np.float64` to produce either single or double precision uniform random variables for select distributions. <span class="title-ref">\~.Generator.integers</span> accepts a `dtype` argument with any signed or unsigned integer dtype.

>   - Uniforms (<span class="title-ref">\~.Generator.random</span> and <span class="title-ref">\~.Generator.integers</span>)
>   - Normals (<span class="title-ref">\~.Generator.standard\_normal</span>)
>   - Standard Gammas (<span class="title-ref">\~.Generator.standard\_gamma</span>)
>   - Standard Exponentials (<span class="title-ref">\~.Generator.standard\_exponential</span>)

<div class="ipython">

python

rng = np.random.default\_rng() rng.random(3, dtype=np.float64) rng.random(3, dtype=np.float32) rng.integers(0, 256, size=3, dtype=np.uint8)

</div>

  - Optional `out` argument that allows existing arrays to be filled for select distributions
    
      - Uniforms (<span class="title-ref">\~.Generator.random</span>)
      - Normals (<span class="title-ref">\~.Generator.standard\_normal</span>)
      - Standard Gammas (<span class="title-ref">\~.Generator.standard\_gamma</span>)
      - Standard Exponentials (<span class="title-ref">\~.Generator.standard\_exponential</span>)
    
    This allows multithreading to fill large arrays in chunks using suitable BitGenerators in parallel.

<div class="ipython">

python

rng = np.random.default\_rng() existing = np.zeros(4) rng.random(out=existing\[:2\]) print(existing)

</div>

  - Optional `axis` argument for methods like <span class="title-ref">\~.Generator.choice</span>, <span class="title-ref">\~.Generator.permutation</span> and <span class="title-ref">\~.Generator.shuffle</span> that controls which axis an operation is performed over for multi-dimensional arrays.

<div class="ipython">

python

rng = np.random.default\_rng() a = np.arange(12).reshape((3, 4)) a rng.choice(a, axis=1, size=5) rng.shuffle(a, axis=1) \# Shuffle in-place a

</div>

  - Added a method to sample from the complex normal distribution (<span class="title-ref">\~.Generator.complex\_normal</span>)

---

parallel.md

---

# Parallel random number generation

There are four main strategies implemented that can be used to produce repeatable pseudo-random numbers across multiple processes (local or distributed).

<div class="currentmodule">

numpy.random

</div>

## <span class="title-ref">\~SeedSequence</span> spawning

NumPy allows you to spawn new (with very high probability) independent <span class="title-ref">BitGenerator</span> and <span class="title-ref">Generator</span> instances via their `spawn()` method. This spawning is implemented by the <span class="title-ref">SeedSequence</span> used for initializing the bit generators random stream.

<span class="title-ref">SeedSequence</span> [implements an algorithm](https://www.pcg-random.org/posts/developing-a-seed_seq-alternative.html) to process a user-provided seed, typically as an integer of some size, and to convert it into an initial state for a <span class="title-ref">BitGenerator</span>. It uses hashing techniques to ensure that low-quality seeds are turned into high quality initial states (at least, with very high probability).

For example, <span class="title-ref">MT19937</span> has a state consisting of 624 `uint32` integers. A naive way to take a 32-bit integer seed would be to just set the last element of the state to the 32-bit seed and leave the rest 0s. This is a valid state for <span class="title-ref">MT19937</span>, but not a good one. The Mersenne Twister algorithm [suffers if there are too many 0s](http://www.math.sci.hiroshima-u.ac.jp/m-mat/MT/MT2002/emt19937ar.html). Similarly, two adjacent 32-bit integer seeds (i.e. `12345` and `12346`) would produce very similar streams.

<span class="title-ref">SeedSequence</span> avoids these problems by using successions of integer hashes with good [avalanche properties](https://en.wikipedia.org/wiki/Avalanche_effect) to ensure that flipping any bit in the input has about a 50% chance of flipping any bit in the output. Two input seeds that are very close to each other will produce initial states that are very far from each other (with very high probability). It is also constructed in such a way that you can provide arbitrary-sized integers or lists of integers. <span class="title-ref">SeedSequence</span> will take all of the bits that you provide and mix them together to produce however many bits the consuming <span class="title-ref">BitGenerator</span> needs to initialize itself.

These properties together mean that we can safely mix together the usual user-provided seed with simple incrementing counters to get <span class="title-ref">BitGenerator</span> states that are (to very high probability) independent of each other. We can wrap this together into an API that is easy to use and difficult to misuse. Note that while <span class="title-ref">SeedSequence</span> attempts to solve many of the issues related to user-provided small seeds, we still \[recommend\<recommend-secrets-randbits\>\](\#recommend\<recommend-secrets-randbits\>) using :py\`secrets.randbits\` to generate seeds with 128 bits of entropy to avoid the remaining biases introduced by human-chosen seeds.

`` `python   from numpy.random import SeedSequence, default_rng    ss = SeedSequence(12345)    # Spawn off 10 child SeedSequences to pass to child processes.   child_seeds = ss.spawn(10)   streams = [default_rng(s) for s in child_seeds]  .. end_block  For convenience the direct use of `SeedSequence` is not necessary. ``<span class="title-ref"> The above </span><span class="title-ref">streams</span><span class="title-ref"> can be spawned directly from a parent generator via </span>\~Generator.spawn\`:

`` `python   parent_rng = default_rng(12345)   streams = parent_rng.spawn(10)  .. end_block  Child objects can also spawn to make grandchildren, and so on. ``<span class="title-ref"> Each child has a \`SeedSequence</span> with its position in the tree of spawned child objects mixed in with the user-provided seed to generate independent (with very high probability) streams.

`` `python   grandchildren = streams[0].spawn(4)  .. end_block  This feature lets you make local decisions about when and how to split up ``<span class="title-ref"> streams without coordination between processes. You do not have to preallocate space to avoid overlapping or request streams from a common global service. This general "tree-hashing" scheme is \`not unique to numpy</span>\_ but not yet widespread. Python has increasingly-flexible mechanisms for parallelization available, and this scheme fits in very well with that kind of use.

Using this scheme, an upper bound on the probability of a collision can be estimated if one knows the number of streams that you derive. <span class="title-ref">SeedSequence</span> hashes its inputs, both the seed and the spawn-tree-path, down to a 128-bit pool by default. The probability that there is a collision in that pool, pessimistically-estimated (\[1\]), will be about \(n^2*2^{-128}\) where <span class="title-ref">n</span> is the number of streams spawned. If a program uses an aggressive million streams, about \(2^{20}\), then the probability that at least one pair of them are identical is about \(2^{-88}\), which is in solidly-ignorable territory (\[2\]).

## Sequence of integer seeds

As discussed in the previous section, <span class="title-ref">SeedSequence</span> can not only take an integer seed, it can also take an arbitrary-length sequence of (non-negative) integers. If one exercises a little care, one can use this feature to design *ad hoc* schemes for getting safe parallel PRNG streams with similar safety guarantees as spawning.

For example, one common use case is that a worker process is passed one root seed integer for the whole calculation and also an integer worker ID (or something more granular like a job ID, batch ID, or something similar). If these IDs are created deterministically and uniquely, then one can derive reproducible parallel PRNG streams by combining the ID and the root seed integer in a list.

`` `python   # default_rng() and each of the BitGenerators use SeedSequence underneath, so   # they all accept sequences of integers as seeds the same way.   from numpy.random import default_rng    def worker(root_seed, worker_id):       rng = default_rng([worker_id, root_seed])       # Do work ...    root_seed = 0x8c3c010cb4754c905776bdac5ee7501   results = [worker(root_seed, worker_id) for worker_id in range(10)]  .. end_block  This can be used to replace a number of unsafe strategies that have been used ``<span class="title-ref"> in the past which try to combine the root seed and the ID back into a single integer seed value. For example, it is common to see users add the worker ID to the root seed, especially with the legacy \`RandomState</span> code.

`` `python   # UNSAFE! Do not do this!   worker_seed = root_seed + worker_id   rng = np.random.RandomState(worker_seed)  .. end_block  It is true that for any one run of a parallel program constructed this way, ``\` each worker will have distinct streams. However, it is quite likely that multiple invocations of the program with different seeds will get overlapping sets of worker seeds. It is not uncommon (in the author's self-experience) to change the root seed merely by an increment or two when doing these repeat runs. If the worker seeds are also derived by small increments of the worker ID, then subsets of the workers will return identical results, causing a bias in the overall ensemble of results.

Combining the worker ID and the root seed as a list of integers eliminates this risk. Lazy seeding practices will still be fairly safe.

This scheme does require that the extra IDs be unique and deterministically created. This may require coordination between the worker processes. It is recommended to place the varying IDs *before* the unvarying root seed. <span class="title-ref">\~SeedSequence.spawn</span> *appends* integers after the user-provided seed, so if you might be mixing both this *ad hoc* mechanism and spawning, or passing your objects down to library code that might be spawning, then it is a little bit safer to prepend your worker IDs rather than append them to avoid a collision.

`` `python   # Good.   worker_seed = [worker_id, root_seed]    # Less good. It will *work*, but it's less flexible.   worker_seed = [root_seed, worker_id]  .. end_block  With those caveats in mind, the safety guarantees against collision are about ``\` the same as with spawning, discussed in the previous section. The algorithmic mechanisms are the same.

## Independent streams

<span class="title-ref">Philox</span> is a counter-based RNG based which generates values by encrypting an incrementing counter using weak cryptographic primitives. The seed determines the key that is used for the encryption. Unique keys create unique, independent streams. <span class="title-ref">Philox</span> lets you bypass the seeding algorithm to directly set the 128-bit key. Similar, but different, keys will still create independent streams.

`` `python   import secrets   from numpy.random import Philox    # 128-bit number as a seed   root_seed = secrets.getrandbits(128)   streams = [Philox(key=root_seed + stream_id) for stream_id in range(10)]  .. end_block  This scheme does require that you avoid reusing stream IDs. This may require ``\` coordination between the parallel processes.

## Jumping the BitGenerator state

`jumped` advances the state of the BitGenerator *as-if* a large number of random numbers have been drawn, and returns a new instance with this state. The specific number of draws varies by BitGenerator, and ranges from \(2^{64}\) to \(2^{128}\). Additionally, the *as-if* draws also depend on the size of the default random number produced by the specific BitGenerator. The BitGenerators that support `jumped`, along with the period of the BitGenerator, the size of the jump and the bits in the default unsigned random are listed below.

| BitGenerator                             | Period          | Jump Size            | Bits per Draw |
| ---------------------------------------- | --------------- | -------------------- | ------------- |
| <span class="title-ref">MT19937</span>   | \(2^{19937}-1\) | \(2^{128}\)          | 32            |
| <span class="title-ref">PCG64</span>     | \(2^{128}\)     | \(~2^{127}\) (\[3\]) | 64            |
| <span class="title-ref">PCG64DXSM</span> | \(2^{128}\)     | \(~2^{127}\) (\[4\]) | 64            |
| <span class="title-ref">Philox</span>    | \(2^{256}\)     | \(2^{128}\)          | 64            |

`jumped` can be used to produce long blocks which should be long enough to not overlap.

`` `python   import secrets   from numpy.random import PCG64    seed = secrets.getrandbits(128)   blocked_rng = []   rng = PCG64(seed)   for i in range(10):       blocked_rng.append(rng.jumped(i))  .. end_block  When using ``jumped`, one does have to take care not to jump to a stream that`<span class="title-ref"> was already used. In the above example, one could not later use </span><span class="title-ref">blocked\_rng\[0\].jumped()</span><span class="title-ref"> as it would overlap with </span><span class="title-ref">blocked\_rng\[1\]</span><span class="title-ref">. Like with the independent streams, if the main process here wants to split off 10 more streams by jumping, then it needs to start with </span><span class="title-ref">range(10, 20)</span>\`, otherwise it would recreate the same streams. On the other hand, if you carefully construct the streams, then you are guaranteed to have streams that do not overlap.

1.  The algorithm is carefully designed to eliminate a number of possible ways to collide. For example, if one only does one level of spawning, it is guaranteed that all states will be unique. But it's easier to estimate the naive upper bound on a napkin and take comfort knowing that the probability is actually lower.

2.  In this calculation, we can mostly ignore the amount of numbers drawn from each stream. See \[upgrading-pcg64\](\#upgrading-pcg64) for the technical details about <span class="title-ref">PCG64</span>. The other PRNGs we provide have some extra protection built in that avoids overlaps if the <span class="title-ref">SeedSequence</span> pools differ in the slightest bit. <span class="title-ref">PCG64DXSM</span> has \(2^{127}\) separate cycles determined by the seed in addition to the position in the \(2^{128}\) long period for each cycle, so one has to both get on or near the same cycle *and* seed a nearby position in the cycle. <span class="title-ref">Philox</span> has completely independent cycles determined by the seed. <span class="title-ref">SFC64</span> incorporates a 64-bit counter so every unique seed is at least \(2^{64}\) iterations away from any other seed. And finally, <span class="title-ref">MT19937</span> has just an unimaginably huge period. Getting a collision internal to <span class="title-ref">SeedSequence</span> is the way a failure would be observed.

3.  The jump size is \((\phi-1)*2^{128}\) where \(\phi\) is the golden ratio. As the jumps wrap around the period, the actual distances between neighboring streams will slowly grow smaller than the jump size, but using the golden ratio this way is a classic method of constructing a low-discrepancy sequence that spreads out the states around the period optimally. You will not be able to jump enough to make those distances small enough to overlap in your lifetime.

4.  The jump size is \((\phi-1)*2^{128}\) where \(\phi\) is the golden ratio. As the jumps wrap around the period, the actual distances between neighboring streams will slowly grow smaller than the jump size, but using the golden ratio this way is a classic method of constructing a low-discrepancy sequence that spreads out the states around the period optimally. You will not be able to jump enough to make those distances small enough to overlap in your lifetime.

---

performance.md

---

# Performance

<div class="currentmodule">

numpy.random

</div>

## Recommendation

The recommended generator for general use is <span class="title-ref">PCG64</span> or its upgraded variant <span class="title-ref">PCG64DXSM</span> for heavily-parallel use cases. They are statistically high quality, full-featured, and fast on most platforms, but somewhat slow when compiled for 32-bit processes. See \[upgrading-pcg64\](\#upgrading-pcg64) for details on when heavy parallelism would indicate using <span class="title-ref">PCG64DXSM</span>.

<span class="title-ref">Philox</span> is fairly slow, but its statistical properties have very high quality, and it is easy to get an assuredly-independent stream by using unique keys. If that is the style you wish to use for parallel streams, or you are porting from another system that uses that style, then <span class="title-ref">Philox</span> is your choice.

<span class="title-ref">SFC64</span> is statistically high quality and very fast. However, it lacks jumpability. If you are not using that capability and want lots of speed, even on 32-bit processes, this is your choice.

<span class="title-ref">MT19937</span> [fails some statistical tests](https://www.iro.umontreal.ca/~lecuyer/myftp/papers/testu01.pdf) and is not especially fast compared to modern PRNGs. For these reasons, we mostly do not recommend using it on its own, only through the legacy <span class="title-ref">RandomState</span> for reproducing old results. That said, it has a very long history as a default in many systems.

## Timings

The timings below are the time in ns to produce 1 random value from a specific distribution. The original <span class="title-ref">MT19937</span> generator is much slower since it requires 2 32-bit values to equal the output of the faster generators.

Integer performance has a similar ordering.

The pattern is similar for other, more complex generators. The normal performance of the legacy <span class="title-ref">RandomState</span> generator is much lower than the other since it uses the Box-Muller transform rather than the Ziggurat method. The performance gap for Exponentials is also large due to the cost of computing the log function to invert the CDF. The column labeled MT19973 uses the same 32-bit generator as <span class="title-ref">RandomState</span> but produces random variates using <span class="title-ref">Generator</span>.

|                      | MT19937 | PCG64 | PCG64DXSM | Philox | SFC64 | RandomState |
| -------------------- | ------- | ----- | --------- | ------ | ----- | ----------- |
| 32-bit Unsigned Ints | 3.3     | 1.9   | 2.0       | 3.3    | 1.8   | 3.1         |
| 64-bit Unsigned Ints | 5.6     | 3.2   | 2.9       | 4.9    | 2.5   | 5.5         |
| Uniforms             | 5.9     | 3.1   | 2.9       | 5.0    | 2.6   | 6.0         |
| Normals              | 13.9    | 10.8  | 10.5      | 12.0   | 8.3   | 56.8        |
| Exponentials         | 9.1     | 6.0   | 5.8       | 8.1    | 5.4   | 63.9        |
| Gammas               | 37.2    | 30.8  | 28.9      | 34.0   | 27.5  | 77.0        |
| Binomials            | 21.3    | 17.4  | 17.6      | 19.3   | 15.6  | 21.4        |
| Laplaces             | 73.2    | 72.3  | 76.1      | 73.0   | 72.3  | 82.5        |
| Poissons             | 111.7   | 103.4 | 100.5     | 109.4  | 90.7  | 115.2       |

The next table presents the performance in percentage relative to values generated by the legacy generator, `RandomState(MT19937())`. The overall performance was computed using a geometric mean.

|                      | MT19937 | PCG64 | PCG64DXSM | Philox | SFC64 |
| -------------------- | ------- | ----- | --------- | ------ | ----- |
| 32-bit Unsigned Ints | 96      | 162   | 160       | 96     | 175   |
| 64-bit Unsigned Ints | 97      | 171   | 188       | 113    | 218   |
| Uniforms             | 102     | 192   | 206       | 121    | 233   |
| Normals              | 409     | 526   | 541       | 471    | 684   |
| Exponentials         | 701     | 1071  | 1101      | 784    | 1179  |
| Gammas               | 207     | 250   | 266       | 227    | 281   |
| Binomials            | 100     | 123   | 122       | 111    | 138   |
| Laplaces             | 113     | 114   | 108       | 113    | 114   |
| Poissons             | 103     | 111   | 115       | 105    | 127   |
| Overall              | 159     | 219   | 225       | 174    | 251   |

\> **Note** \> All timings were taken using Linux on an AMD Ryzen 9 3900X processor.

## Performance on different operating systems

Performance differs across platforms due to compiler and hardware availability (e.g., register width) differences. The default bit generator has been chosen to perform well on 64-bit platforms. Performance on 32-bit operating systems is very different.

The values reported are normalized relative to the speed of MT19937 in each table. A value of 100 indicates that the performance matches the MT19937. Higher values indicate improved performance. These values cannot be compared across tables.

### 64-bit Linux

<table>
<thead>
<tr class="header">
<th>Distribution</th>
<th>MT19937</th>
<th>PCG64</th>
<th>PCG64DXSM</th>
<th>Philox</th>
<th>SFC64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>32-bit Unsigned Ints</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>168</p>
</blockquote></td>
<td><blockquote>
<p>166</p>
</blockquote></td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>182</p>
</blockquote></td>
</tr>
<tr class="even">
<td>64-bit Unsigned Ints</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>176</p>
</blockquote></td>
<td><blockquote>
<p>193</p>
</blockquote></td>
<td><blockquote>
<p>116</p>
</blockquote></td>
<td><blockquote>
<p>224</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Uniforms</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>188</p>
</blockquote></td>
<td><blockquote>
<p>202</p>
</blockquote></td>
<td><blockquote>
<p>118</p>
</blockquote></td>
<td><blockquote>
<p>228</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Normals</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>128</p>
</blockquote></td>
<td><blockquote>
<p>132</p>
</blockquote></td>
<td><blockquote>
<p>115</p>
</blockquote></td>
<td><blockquote>
<p>167</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Exponentials</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>152</p>
</blockquote></td>
<td><blockquote>
<p>157</p>
</blockquote></td>
<td><blockquote>
<p>111</p>
</blockquote></td>
<td><blockquote>
<p>168</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Overall</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>161</p>
</blockquote></td>
<td><blockquote>
<p>168</p>
</blockquote></td>
<td><blockquote>
<p>112</p>
</blockquote></td>
<td><blockquote>
<p>192</p>
</blockquote></td>
</tr>
</tbody>
</table>

### 64-bit Windows

The relative performance on 64-bit Linux and 64-bit Windows is broadly similar with the notable exception of the Philox generator.

<table>
<thead>
<tr class="header">
<th>Distribution</th>
<th>MT19937</th>
<th>PCG64</th>
<th>PCG64DXSM</th>
<th>Philox</th>
<th>SFC64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>32-bit Unsigned Ints</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>155</p>
</blockquote></td>
<td><blockquote>
<p>131</p>
</blockquote></td>
<td><blockquote>
<p>29</p>
</blockquote></td>
<td><blockquote>
<p>150</p>
</blockquote></td>
</tr>
<tr class="even">
<td>64-bit Unsigned Ints</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>157</p>
</blockquote></td>
<td><blockquote>
<p>143</p>
</blockquote></td>
<td><blockquote>
<p>25</p>
</blockquote></td>
<td><blockquote>
<p>154</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Uniforms</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>151</p>
</blockquote></td>
<td><blockquote>
<p>144</p>
</blockquote></td>
<td><blockquote>
<p>24</p>
</blockquote></td>
<td><blockquote>
<p>155</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Normals</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>129</p>
</blockquote></td>
<td><blockquote>
<p>128</p>
</blockquote></td>
<td><blockquote>
<p>37</p>
</blockquote></td>
<td><blockquote>
<p>150</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Exponentials</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>150</p>
</blockquote></td>
<td><blockquote>
<p>145</p>
</blockquote></td>
<td><blockquote>
<p>28</p>
</blockquote></td>
<td><blockquote>
<p>159</p>
</blockquote></td>
</tr>
<tr class="even">
<td><strong>Overall</strong></td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>148</p>
</blockquote></td>
<td><blockquote>
<p>138</p>
</blockquote></td>
<td><blockquote>
<p>28</p>
</blockquote></td>
<td><blockquote>
<p>154</p>
</blockquote></td>
</tr>
</tbody>
</table>

### 32-bit Windows

The performance of 64-bit generators on 32-bit Windows is much lower than on 64-bit operating systems due to register width. MT19937, the generator that has been in NumPy since 2005, operates on 32-bit integers.

<table>
<thead>
<tr class="header">
<th>Distribution</th>
<th>MT19937</th>
<th>PCG64</th>
<th>PCG64DXSM</th>
<th>Philox</th>
<th>SFC64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>32-bit Unsigned Ints</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>24</p>
</blockquote></td>
<td><blockquote>
<p>34</p>
</blockquote></td>
<td><blockquote>
<p>14</p>
</blockquote></td>
<td><blockquote>
<p>57</p>
</blockquote></td>
</tr>
<tr class="even">
<td>64-bit Unsigned Ints</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>21</p>
</blockquote></td>
<td><blockquote>
<p>32</p>
</blockquote></td>
<td><blockquote>
<p>14</p>
</blockquote></td>
<td><blockquote>
<p>74</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Uniforms</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>21</p>
</blockquote></td>
<td><blockquote>
<p>34</p>
</blockquote></td>
<td><blockquote>
<p>16</p>
</blockquote></td>
<td><blockquote>
<p>73</p>
</blockquote></td>
</tr>
<tr class="even">
<td>Normals</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>36</p>
</blockquote></td>
<td><blockquote>
<p>57</p>
</blockquote></td>
<td><blockquote>
<p>28</p>
</blockquote></td>
<td><blockquote>
<p>101</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>Exponentials</td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>28</p>
</blockquote></td>
<td><blockquote>
<p>44</p>
</blockquote></td>
<td><blockquote>
<p>20</p>
</blockquote></td>
<td><blockquote>
<p>88</p>
</blockquote></td>
</tr>
<tr class="even">
<td><strong>Overall</strong></td>
<td><blockquote>
<p>100</p>
</blockquote></td>
<td><blockquote>
<p>25</p>
</blockquote></td>
<td><blockquote>
<p>39</p>
</blockquote></td>
<td><blockquote>
<p>18</p>
</blockquote></td>
<td><blockquote>
<p>77</p>
</blockquote></td>
</tr>
</tbody>
</table>

\> **Note** \> Linux timings used Ubuntu 20.04 and GCC 9.3.0. Windows timings were made on Windows 10 using Microsoft C/C++ Optimizing Compiler Version 19 (Visual Studio 2019). All timings were produced on an AMD Ryzen 9 3900X processor.

---

upgrading-pcg64.md

---

<div id="upgrading-pcg64">

<div class="currentmodule">

numpy.random

</div>

</div>

# Upgrading <span class="title-ref">PCG64</span> with <span class="title-ref">PCG64DXSM</span>

Uses of the <span class="title-ref">PCG64</span> <span class="title-ref">BitGenerator</span> in a massively-parallel context have been shown to have statistical weaknesses that were not apparent at the first release in numpy 1.17. Most users will never observe this weakness and are safe to continue to use <span class="title-ref">PCG64</span>. We have introduced a new <span class="title-ref">PCG64DXSM</span> <span class="title-ref">BitGenerator</span> that will eventually become the new default <span class="title-ref">BitGenerator</span> implementation used by <span class="title-ref">default\_rng</span> in future releases. <span class="title-ref">PCG64DXSM</span> solves the statistical weakness while preserving the performance and the features of <span class="title-ref">PCG64</span>.

## Does this affect me?

If you

1.  only use a single <span class="title-ref">Generator</span> instance,
2.  only use <span class="title-ref">RandomState</span> or the functions in <span class="title-ref">numpy.random</span>,
3.  only use the <span class="title-ref">PCG64.jumped</span> method to generate parallel streams,
4.  explicitly use a <span class="title-ref">BitGenerator</span> other than <span class="title-ref">PCG64</span>,

then this weakness does not affect you at all. Carry on.

If you use moderate numbers of parallel streams created with <span class="title-ref">default\_rng</span> or <span class="title-ref">SeedSequence.spawn</span>, in the 1000s, then the chance of observing this weakness is negligibly small. You can continue to use <span class="title-ref">PCG64</span> comfortably.

If you use very large numbers of parallel streams, in the millions, and draw large amounts of numbers from each, then the chance of observing this weakness can become non-negligible, if still small. An example of such a use case would be a very large distributed reinforcement learning problem with millions of long Monte Carlo playouts each generating billions of random number draws. Such use cases should consider using <span class="title-ref">PCG64DXSM</span> explicitly or another modern <span class="title-ref">BitGenerator</span> like <span class="title-ref">SFC64</span> or <span class="title-ref">Philox</span>, but it is unlikely that any old results you may have calculated are invalid. In any case, the weakness is a kind of [Birthday Paradox](https://en.wikipedia.org/wiki/Birthday_problem) collision. That is, a single pair of parallel streams out of the millions, considered together, might fail a stringent set of statistical tests of randomness. The remaining millions of streams would all be perfectly fine, and the effect of the bad pair in the whole calculation is very likely to be swamped by the remaining streams in most applications.

## Technical details

Like many PRNG algorithms, <span class="title-ref">PCG64</span> is constructed from a transition function, which advances a 128-bit state, and an output function, that mixes the 128-bit state into a 64-bit integer to be output. One of the guiding design principles of the PCG family of PRNGs is to balance the computational cost (and pseudorandomness strength) between the transition function and the output function. The transition function is a 128-bit linear congruential generator (LCG), which consists of multiplying the 128-bit state with a fixed multiplication constant and then adding a user-chosen increment, in 128-bit modular arithmetic. LCGs are well-analyzed PRNGs with known weaknesses, though 128-bit LCGs are large enough to pass stringent statistical tests on their own, with only the trivial output function. The output function of <span class="title-ref">PCG64</span> is intended to patch up some of those known weaknesses by doing "just enough" scrambling of the bits to assist in the statistical properties without adding too much computational cost.

One of these known weaknesses is that advancing the state of the LCG by steps numbering a power of two (`bg.advance(2**N)`) will leave the lower `N` bits identical to the state that was just left. For a single stream drawn from sequentially, this is of little consequence. The remaining \(128-N\) bits provide plenty of pseudorandomness that will be mixed in for any practical `N` that can be observed in a single stream, which is why one does not need to worry about this if you only use a single stream in your application. Similarly, the <span class="title-ref">PCG64.jumped</span> method uses a carefully chosen number of steps to avoid creating these collisions. However, once you start creating "randomly-initialized" parallel streams, either using OS entropy by calling <span class="title-ref">default\_rng</span> repeatedly or using <span class="title-ref">SeedSequence.spawn</span>, then we need to consider how many lower bits need to "collide" in order to create a bad pair of streams, and then evaluate the probability of creating such a collision. [Empirically](https://github.com/numpy/numpy/issues/16313), it has been determined that if one shares the lower 58 bits of state and shares an increment, then the pair of streams, when interleaved, will fail [PractRand](https://pracrand.sourceforge.net/) in a reasonable amount of time, after drawing a few gigabytes of data. Following the standard Birthday Paradox calculations for a collision of 58 bits, we can see that we can create \(2^{29}\), or about half a billion, streams which is when the probability of such a collision becomes high. Half a billion streams is quite high, and the amount of data each stream needs to draw before the statistical correlations become apparent to even the strict `PractRand` tests is in the gigabytes. But this is on the horizon for very large applications like distributed reinforcement learning. There are reasons to expect that even in these applications a collision probably will not have a practical effect in the total result, since the statistical problem is constrained to just the colliding pair.

Now, let us consider the case when the increment is not constrained to be the same. Our implementation of <span class="title-ref">PCG64</span> seeds both the state and the increment; that is, two calls to <span class="title-ref">default\_rng</span> (almost certainly) have different states and increments. Upon our first release, we believed that having the seeded increment would provide a certain amount of extra protection, that one would have to be "close" in both the state space and increment space in order to observe correlations (`PractRand` failures) in a pair of streams. If that were true, then the "bottleneck" for collisions would be the 128-bit entropy pool size inside of <span class="title-ref">SeedSequence</span> (and 128-bit collisions are in the "preposterously unlikely" category). Unfortunately, this is not true.

One of the known properties of an LCG is that different increments create *distinct* streams, but with a known relationship. Each LCG has an orbit that traverses all \(2^{128}\) different 128-bit states. Two LCGs with different increments are related in that one can "rotate" the orbit of the first LCG (advance it by a number of steps that we can compute from the two increments) such that then both LCGs will always then have the same state, up to an additive constant and maybe an inversion of the bits. If you then iterate both streams in lockstep, then the states will *always* remain related by that same additive constant (and the inversion, if present). Recall that <span class="title-ref">PCG64</span> is constructed from both a transition function (the LCG) and an output function. It was expected that the scrambling effect of the output function would have been strong enough to make the distinct streams practically independent (i.e. "passing the `PractRand` tests") unless the two increments were pathologically related to each other (e.g. 1 and 3). The output function XSL-RR of the then-standard PCG algorithm that we implemented in <span class="title-ref">PCG64</span> turns out to be too weak to cover up for the 58-bit collision of the underlying LCG that we described above. For any given pair of increments, the size of the "colliding" space of states is the same, so for this weakness, the extra distinctness provided by the increments does not translate into extra protection from statistical correlations that `PractRand` can detect.

Fortunately, strengthening the output function is able to correct this weakness and *does* turn the extra distinctness provided by differing increments into additional protection from these low-bit collisions. To the [PCG author's credit](https://github.com/numpy/numpy/issues/13635#issuecomment-506088698), she had developed a stronger output function in response to related discussions during the long birth of the new <span class="title-ref">BitGenerator</span> system. We NumPy developers chose to be "conservative" and use the XSL-RR variant that had undergone a longer period of testing at that time. The DXSM output function adopts a "xorshift-multiply" construction used in strong integer hashes that has much better avalanche properties than the XSL-RR output function. While there are "pathological" pairs of increments that induce "bad" additive constants that relate the two streams, the vast majority of pairs induce "good" additive constants that make the merely-distinct streams of LCG states into practically-independent output streams. Indeed, now the claim we once made about <span class="title-ref">PCG64</span> is actually true of \`PCG64DXSM\`: collisions are possible, but both streams have to simultaneously be both "close" in the 128 bit state space *and* "close" in the 127-bit increment space, so that would be less likely than the negligible chance of colliding in the 128-bit internal <span class="title-ref">SeedSequence</span> pool. The DXSM output function is more computationally intensive than XSL-RR, but some optimizations in the LCG more than make up for the performance hit on most machines, so <span class="title-ref">PCG64DXSM</span> is a good, safe upgrade. There are, of course, an infinite number of stronger output functions that one could consider, but most will have a greater computational cost, and the DXSM output function has now received many CPU cycles of testing via `PractRand` at this time.

---

routines.array-creation.md

---

# Array creation routines

<div class="seealso">

\[Array creation \<arrays.creation\>\](\#array-creation-\<arrays.creation\>)

</div>

<div class="currentmodule">

numpy

</div>

## From shape or value

<div class="autosummary" data-toctree="generated/">

empty empty\_like eye identity ones ones\_like zeros zeros\_like full full\_like

</div>

## From existing data

<div class="autosummary" data-toctree="generated/">

array asarray asanyarray ascontiguousarray asmatrix astype copy frombuffer from\_dlpack fromfile fromfunction fromiter fromstring loadtxt

</div>

## Creating record arrays

<div class="note">

<div class="title">

Note

</div>

Please refer to \[arrays.classes.rec\](\#arrays.classes.rec) for record arrays.

</div>

<div class="autosummary" data-toctree="generated/">

rec.array rec.fromarrays rec.fromrecords rec.fromstring rec.fromfile

</div>

## Creating character arrays (`numpy.char`)

<div class="note">

<div class="title">

Note

</div>

`numpy.char` is used to create character arrays.

</div>

<div class="autosummary" data-toctree="generated/">

char.array char.asarray

</div>

## Numerical ranges

<div class="autosummary" data-toctree="generated/">

arange linspace logspace geomspace meshgrid mgrid ogrid

</div>

## Building matrices

<div class="autosummary" data-toctree="generated/">

diag diagflat tri tril triu vander

</div>

## The matrix class

<div class="autosummary" data-toctree="generated/">

bmat

</div>

---

routines.array-manipulation.md

---

# Array manipulation routines

<div class="currentmodule">

numpy

</div>

## Basic operations

<div class="autosummary" data-toctree="generated/">

copyto ndim shape size

</div>

## Changing array shape

<div class="autosummary" data-toctree="generated/">

reshape ravel ndarray.flat ndarray.flatten

</div>

## Transpose-like operations

<div class="autosummary" data-toctree="generated/">

moveaxis rollaxis swapaxes ndarray.T transpose permute\_dims matrix\_transpose (Array API compatible)

</div>

## Changing number of dimensions

<div class="autosummary" data-toctree="generated/">

atleast\_1d atleast\_2d atleast\_3d broadcast broadcast\_to broadcast\_arrays expand\_dims squeeze

</div>

## Changing kind of array

<div class="autosummary" data-toctree="generated/">

asarray asanyarray asmatrix asfortranarray ascontiguousarray asarray\_chkfinite require

</div>

## Joining arrays

<div class="autosummary" data-toctree="generated/">

concatenate concat stack block vstack hstack dstack column\_stack

</div>

## Splitting arrays

<div class="autosummary" data-toctree="generated/">

split array\_split dsplit hsplit vsplit unstack

</div>

## Tiling arrays

<div class="autosummary" data-toctree="generated/">

tile repeat

</div>

## Adding and removing elements

<div class="autosummary" data-toctree="generated/">

delete insert append resize trim\_zeros unique pad

</div>

## Rearranging elements

<div class="autosummary" data-toctree="generated/">

flip fliplr flipud roll rot90

</div>

---

routines.bitwise.md

---

# Bit-wise operations

<div class="currentmodule">

numpy

</div>

## Elementwise bit operations

<div class="autosummary" data-toctree="generated/">

bitwise\_and bitwise\_or bitwise\_xor invert bitwise\_invert left\_shift bitwise\_left\_shift right\_shift bitwise\_right\_shift

</div>

## Bit packing

<div class="autosummary" data-toctree="generated/">

packbits unpackbits

</div>

## Output formatting

<div class="autosummary" data-toctree="generated/">

binary\_repr

</div>

---

routines.char.md

---

# Legacy fixed-width string functionality

<div class="currentmodule">

numpy.char

</div>

<div class="module">

numpy.char

</div>

<div class="legacy">

The string operations in this module, as well as the <span class="title-ref">numpy.char.chararray</span> class, are planned to be deprecated in the future. Use <span class="title-ref">numpy.strings</span> instead.

</div>

The <span class="title-ref">numpy.char</span> module provides a set of vectorized string operations for arrays of type <span class="title-ref">numpy.str\_</span> or <span class="title-ref">numpy.bytes\_</span>. For example

> \>\>\> import numpy as np \>\>\> np.char.capitalize(\["python", "numpy"\]) array(\['Python', 'Numpy'\], dtype='\<U6') \>\>\> np.char.add(\["num", "doc"\], \["py", "umentation"\]) array(\['numpy', 'documentation'\], dtype='\<U13')

The methods in this module are based on the methods in :py`string`

## String operations

<div class="autosummary" data-toctree="generated/">

add multiply mod capitalize center decode encode expandtabs join ljust lower lstrip partition replace rjust rpartition rsplit rstrip split splitlines strip swapcase title translate upper zfill

</div>

## Comparison

Unlike the standard numpy comparison operators, the ones in the <span class="title-ref">char</span> module strip trailing whitespace characters before performing the comparison.

<div class="autosummary" data-toctree="generated/">

equal not\_equal greater\_equal less\_equal greater less compare\_chararrays

</div>

## String information

<div class="autosummary" data-toctree="generated/">

count endswith find index isalpha isalnum isdecimal isdigit islower isnumeric isspace istitle isupper rfind rindex startswith str\_len

</div>

## Convenience class

<div class="autosummary" data-toctree="generated/">

array asarray chararray

</div>

---

routines.ctypeslib.md

---

# ctypes foreign function interface (`numpy.ctypeslib`)

<div class="currentmodule">

numpy.ctypeslib

</div>

<div class="autofunction">

as\_array

</div>

<div class="autofunction">

as\_ctypes

</div>

<div class="autofunction">

as\_ctypes\_type

</div>

<div class="autofunction">

load\_library

</div>

<div class="autofunction">

ndpointer

</div>

<div class="c_intp">

A <span class="title-ref">ctypes</span> signed integer type of the same size as <span class="title-ref">numpy.intp</span>.

Depending on the platform, it can be an alias for either <span class="title-ref">\~ctypes.c\_int</span>, <span class="title-ref">\~ctypes.c\_long</span> or <span class="title-ref">\~ctypes.c\_longlong</span>.

</div>

---

routines.datetime.md

---

# Datetime support functions

<div class="currentmodule">

numpy

</div>

<div class="autosummary" data-toctree="generated/">

datetime\_as\_string datetime\_data

</div>

## Business day functions

<div class="currentmodule">

numpy

</div>

<div class="autosummary" data-toctree="generated/">

busdaycalendar is\_busday busday\_offset busday\_count

</div>

---

routines.dtype.md

---

# Data type routines

<div class="currentmodule">

numpy

</div>

<div class="autosummary" data-toctree="generated/">

can\_cast promote\_types min\_scalar\_type result\_type common\_type

</div>

## Creating data types

<div class="autosummary" data-toctree="generated/">

dtype rec.format\_parser

</div>

## Data type information

<div class="autosummary" data-toctree="generated/">

finfo iinfo

</div>

## Data type testing

<div class="autosummary" data-toctree="generated/">

isdtype issubdtype

</div>

## Miscellaneous

<div class="autosummary" data-toctree="generated/">

typename mintypecode

</div>

---

routines.dtypes.md

---

# Data type classes (`numpy.dtypes`)

<div class="automodule" data-no-members="">

numpy.dtypes

</div>

## Boolean

<div class="attribute">

BoolDType

</div>

## Bit-sized integers

<div class="attribute">

Int8DType UInt8DType Int16DType UInt16DType Int32DType UInt32DType Int64DType UInt64DType

</div>

## C-named integers (may be aliases)

<div class="attribute">

ByteDType UByteDType ShortDType UShortDType IntDType UIntDType LongDType ULongDType LongLongDType ULongLongDType

</div>

## Floating point

<div class="attribute">

Float16DType Float32DType Float64DType LongDoubleDType

</div>

## Complex

<div class="attribute">

Complex64DType Complex128DType CLongDoubleDType

</div>

## Strings and Bytestrings

<div class="attribute">

StrDType BytesDType StringDType

</div>

## Times

<div class="attribute">

DateTime64DType TimeDelta64DType

</div>

## Others

<div class="attribute">

ObjectDType VoidDType

</div>

---

routines.emath.md

---

# Mathematical functions with automatic domain

<div class="currentmodule">

numpy

</div>

<div class="note">

<div class="title">

Note

</div>

`numpy.emath` is a preferred alias for `numpy.lib.scimath`, available after `numpy` is imported.

</div>

<div class="automodule">

numpy.emath

</div>

---

routines.err.md

---

# Floating point error handling

<div class="currentmodule">

numpy

</div>

## Setting and getting error handling

<div class="autosummary" data-toctree="generated/">

seterr geterr seterrcall geterrcall errstate

</div>

---

routines.functional.md

---

# Functional programming

<div class="currentmodule">

numpy

</div>

<div class="autosummary" data-toctree="generated/">

apply\_along\_axis apply\_over\_axes vectorize frompyfunc piecewise

</div>

---

routines.indexing.md

---

# Indexing routines<span id="routines.indexing"></span>

<div class="seealso">

\[basics.indexing\](\#basics.indexing)

</div>

<div class="currentmodule">

numpy

</div>

## Generating index arrays

<div class="autosummary" data-toctree="generated/">

[c]() [r]() [s]() nonzero where indices [ix]() ogrid ravel\_multi\_index unravel\_index diag\_indices diag\_indices\_from mask\_indices tril\_indices tril\_indices\_from triu\_indices triu\_indices\_from

</div>

## Indexing-like operations

<div class="autosummary" data-toctree="generated/">

take take\_along\_axis choose compress diag diagonal select

</div>

## Inserting data into arrays

<div class="autosummary" data-toctree="generated/">

place put put\_along\_axis putmask fill\_diagonal

</div>

## Iterating over arrays

<div class="autosummary" data-toctree="generated/">

nditer ndenumerate ndindex nested\_iters flatiter iterable

</div>

---

routines.io.md

---

# Input and output

<div class="currentmodule">

numpy

</div>

## NumPy binary files (npy, npz)

<div class="autosummary" data-toctree="generated/">

load save savez savez\_compressed lib.npyio.NpzFile

</div>

The format of these binary file types is documented in :py`numpy.lib.format`

## Text files

<div class="autosummary" data-toctree="generated/">

loadtxt savetxt genfromtxt fromregex fromstring ndarray.tofile ndarray.tolist

</div>

## Raw binary files

<div class="autosummary">

fromfile ndarray.tofile

</div>

## String formatting

<div class="autosummary" data-toctree="generated/">

array2string array\_repr array\_str format\_float\_positional format\_float\_scientific

</div>

## Memory mapping files

<div class="autosummary" data-toctree="generated/">

memmap lib.format.open\_memmap

</div>

## Text formatting options

<div class="autosummary" data-toctree="generated/">

set\_printoptions get\_printoptions printoptions

</div>

## Base-n representations

<div class="autosummary" data-toctree="generated/">

binary\_repr base\_repr

</div>

## Data sources

<div class="autosummary" data-toctree="generated/">

lib.npyio.DataSource

</div>

## Binary format description

<div class="autosummary" data-toctree="generated/">

lib.format

</div>

---

routines.lib.md

---

# Lib module (`numpy.lib`)

<div class="currentmodule">

numpy.lib

</div>

## Functions & other objects

<div class="autosummary" data-toctree="generated/">

add\_docstring add\_newdoc Arrayterator NumpyVersion

</div>

## Submodules

<div class="autosummary" data-toctree="generated/">

array\_utils format introspect mixins npyio scimath stride\_tricks

</div>

---

routines.linalg.md

---

<div id="routines.linalg">

<div class="module">

numpy.linalg

</div>

</div>

# Linear algebra (`numpy.linalg`)

The NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient low level implementations of standard linear algebra algorithms. Those libraries may be provided by NumPy itself using C versions of a subset of their reference implementations but, when possible, highly optimized libraries that take advantage of specialized processor functionality are preferred. Examples of such libraries are [OpenBLAS](https://www.openblas.net/), MKL (TM), and ATLAS. Because those libraries are multithreaded and processor dependent, environmental variables and external packages such as [threadpoolctl](https://github.com/joblib/threadpoolctl) may be needed to control the number of threads or specify the processor architecture.

The SciPy library also contains a <span class="title-ref">\~scipy.linalg</span> submodule, and there is overlap in the functionality provided by the SciPy and NumPy submodules. SciPy contains functions not found in <span class="title-ref">numpy.linalg</span>, such as functions related to LU decomposition and the Schur decomposition, multiple ways of calculating the pseudoinverse, and matrix transcendentals such as the matrix logarithm. Some functions that exist in both have augmented functionality in <span class="title-ref">scipy.linalg</span>. For example, <span class="title-ref">scipy.linalg.eig</span> can take a second matrix argument for solving generalized eigenvalue problems. Some functions in NumPy, however, have more flexible broadcasting options. For example, <span class="title-ref">numpy.linalg.solve</span> can handle "stacked" arrays, while <span class="title-ref">scipy.linalg.solve</span> accepts only a single square array as its first argument.

\> **Note** \> The term *matrix* as it is used on this page indicates a 2d <span class="title-ref">numpy.array</span> object, and *not* a <span class="title-ref">numpy.matrix</span> object. The latter is no longer recommended, even for linear algebra. See \[the matrix object documentation\<matrix-objects\>\](\#the-matrix-object-documentation\<matrix-objects\>) for more information.

## The `@` operator

Introduced in NumPy 1.10.0, the `@` operator is preferable to other methods when computing the matrix product between 2d arrays. The <span class="title-ref">numpy.matmul</span> function implements the `@` operator.

<div class="currentmodule">

numpy

</div>

## Matrix and vector products

<div class="autosummary" data-toctree="generated/">

dot linalg.multi\_dot vdot vecdot linalg.vecdot inner outer matmul linalg.matmul (Array API compatible location) tensordot linalg.tensordot (Array API compatible location) einsum einsum\_path linalg.matrix\_power kron linalg.cross

</div>

## Decompositions

<div class="autosummary" data-toctree="generated/">

linalg.cholesky linalg.outer linalg.qr linalg.svd linalg.svdvals

</div>

## Matrix eigenvalues

<div class="autosummary" data-toctree="generated/">

linalg.eig linalg.eigh linalg.eigvals linalg.eigvalsh

</div>

## Norms and other numbers

<div class="autosummary" data-toctree="generated/">

linalg.norm linalg.matrix\_norm (Array API compatible) linalg.vector\_norm (Array API compatible) linalg.cond linalg.det linalg.matrix\_rank linalg.slogdet trace linalg.trace (Array API compatible)

</div>

## Solving equations and inverting matrices

<div class="autosummary" data-toctree="generated/">

linalg.solve linalg.tensorsolve linalg.lstsq linalg.inv linalg.pinv linalg.tensorinv

</div>

## Other matrix operations

<div class="autosummary" data-toctree="generated/">

diagonal linalg.diagonal (Array API compatible) linalg.matrix\_transpose (Array API compatible)

</div>

## Exceptions

<div class="autosummary" data-toctree="generated/">

linalg.LinAlgError

</div>

## Linear algebra on several matrices at once

Several of the linear algebra routines listed above are able to compute results for several matrices at once, if they are stacked into the same array.

This is indicated in the documentation via input parameter specifications such as `a : (..., M, M) array_like`. This means that if for instance given an input array `a.shape == (N, M, M)`, it is interpreted as a "stack" of N matrices, each of size M-by-M. Similar specification applies to return values, for instance the determinant has `det : (...)` and will in this case return an array of shape `det(a).shape == (N,)`. This generalizes to linear algebra operations on higher-dimensional arrays: the last 1 or 2 dimensions of a multidimensional array are interpreted as vectors or matrices, as appropriate for each operation.

---

routines.logic.md

---

# Logic functions

<div class="currentmodule">

numpy

</div>

## Truth value testing

<div class="autosummary" data-toctree="generated/">

all any

</div>

## Array contents

<div class="autosummary" data-toctree="generated/">

isfinite isinf isnan isnat isneginf isposinf

</div>

## Array type testing

<div class="autosummary" data-toctree="generated/">

iscomplex iscomplexobj isfortran isreal isrealobj isscalar

</div>

## Logical operations

<div class="autosummary" data-toctree="generated/">

logical\_and logical\_or logical\_not logical\_xor

</div>

## Comparison

<div class="autosummary" data-toctree="generated/">

allclose isclose array\_equal array\_equiv

</div>

<div class="autosummary" data-toctree="generated/">

greater greater\_equal less less\_equal equal not\_equal

</div>

---

routines.ma.md

---

# Masked array operations

<div class="currentmodule">

numpy

</div>

## Constants

<div class="autosummary" data-toctree="generated/">

ma.MaskType

</div>

## Creation

### From existing data

<div class="autosummary" data-toctree="generated/">

ma.masked\_array ma.array ma.copy ma.frombuffer ma.fromfunction

ma.MaskedArray.copy ma.diagflat

</div>

### Ones and zeros

<div class="autosummary" data-toctree="generated/">

ma.empty ma.empty\_like ma.masked\_all ma.masked\_all\_like ma.ones ma.ones\_like ma.zeros ma.zeros\_like

</div>

## Inspecting the array

<div class="autosummary" data-toctree="generated/">

ma.all ma.any ma.count ma.count\_masked ma.getmask ma.getmaskarray ma.getdata ma.nonzero ma.shape ma.size ma.is\_masked ma.is\_mask ma.isMaskedArray ma.isMA ma.isarray ma.isin ma.in1d ma.unique

ma.MaskedArray.all ma.MaskedArray.any ma.MaskedArray.count ma.MaskedArray.nonzero ma.shape ma.size

</div>

<div class="autosummary">

ma.MaskedArray.data ma.MaskedArray.mask ma.MaskedArray.recordmask

</div>

## Manipulating a MaskedArray

### Changing the shape

<div class="autosummary" data-toctree="generated/">

ma.ravel ma.reshape ma.resize

ma.MaskedArray.flatten ma.MaskedArray.ravel ma.MaskedArray.reshape ma.MaskedArray.resize

</div>

### Modifying axes

<div class="autosummary" data-toctree="generated/">

ma.swapaxes ma.transpose

ma.MaskedArray.swapaxes ma.MaskedArray.transpose

</div>

### Changing the number of dimensions

<div class="autosummary" data-toctree="generated/">

ma.atleast\_1d ma.atleast\_2d ma.atleast\_3d ma.expand\_dims ma.squeeze

ma.MaskedArray.squeeze

ma.stack ma.column\_stack ma.concatenate ma.dstack ma.hstack ma.hsplit [ma.mr]() ma.vstack

</div>

### Joining arrays

<div class="autosummary" data-toctree="generated/">

ma.concatenate ma.stack ma.vstack ma.hstack ma.dstack ma.column\_stack ma.append

</div>

## Operations on masks

### Creating a mask

<div class="autosummary" data-toctree="generated/">

ma.make\_mask ma.make\_mask\_none ma.mask\_or ma.make\_mask\_descr

</div>

### Accessing a mask

<div class="autosummary" data-toctree="generated/">

ma.getmask ma.getmaskarray ma.masked\_array.mask

</div>

### Finding masked data

<div class="autosummary" data-toctree="generated/">

ma.ndenumerate ma.flatnotmasked\_contiguous ma.flatnotmasked\_edges ma.notmasked\_contiguous ma.notmasked\_edges ma.clump\_masked ma.clump\_unmasked

</div>

### Modifying a mask

<div class="autosummary" data-toctree="generated/">

ma.mask\_cols ma.mask\_or ma.mask\_rowcols ma.mask\_rows ma.harden\_mask ma.soften\_mask

ma.MaskedArray.harden\_mask ma.MaskedArray.soften\_mask ma.MaskedArray.shrink\_mask ma.MaskedArray.unshare\_mask

</div>

## Conversion operations

### \> to a masked array

<div class="autosummary" data-toctree="generated/">

ma.asarray ma.asanyarray ma.fix\_invalid ma.masked\_equal ma.masked\_greater ma.masked\_greater\_equal ma.masked\_inside ma.masked\_invalid ma.masked\_less ma.masked\_less\_equal ma.masked\_not\_equal ma.masked\_object ma.masked\_outside ma.masked\_values ma.masked\_where

</div>

### \> to a ndarray

<div class="autosummary" data-toctree="generated/">

ma.compress\_cols ma.compress\_rowcols ma.compress\_rows ma.compressed ma.filled

ma.MaskedArray.compressed ma.MaskedArray.filled

</div>

### \> to another object

<div class="autosummary" data-toctree="generated/">

ma.MaskedArray.tofile ma.MaskedArray.tolist ma.MaskedArray.torecords ma.MaskedArray.tobytes

</div>

### Filling a masked array

<div class="autosummary" data-toctree="generated/">

ma.common\_fill\_value ma.default\_fill\_value ma.maximum\_fill\_value ma.minimum\_fill\_value ma.set\_fill\_value

ma.MaskedArray.get\_fill\_value ma.MaskedArray.set\_fill\_value

</div>

<div class="autosummary">

ma.MaskedArray.fill\_value

</div>

## Masked arrays arithmetic

### Arithmetic

<div class="autosummary" data-toctree="generated/">

ma.anom ma.anomalies ma.average ma.conjugate ma.corrcoef ma.cov ma.cumsum ma.cumprod ma.mean ma.median ma.power ma.prod ma.std ma.sum ma.var

ma.MaskedArray.anom ma.MaskedArray.cumprod ma.MaskedArray.cumsum ma.MaskedArray.mean ma.MaskedArray.prod ma.MaskedArray.std ma.MaskedArray.sum ma.MaskedArray.var

</div>

### Minimum/maximum

<div class="autosummary" data-toctree="generated/">

ma.argmax ma.argmin ma.max ma.min ma.ptp ma.diff

ma.MaskedArray.argmax ma.MaskedArray.argmin ma.MaskedArray.max ma.MaskedArray.min ma.MaskedArray.ptp

</div>

### Sorting

<div class="autosummary" data-toctree="generated/">

ma.argsort ma.sort ma.MaskedArray.argsort ma.MaskedArray.sort

</div>

### Algebra

<div class="autosummary" data-toctree="generated/">

ma.diag ma.dot ma.identity ma.inner ma.innerproduct ma.outer ma.outerproduct ma.trace ma.transpose

ma.MaskedArray.trace ma.MaskedArray.transpose

</div>

### Polynomial fit

<div class="autosummary" data-toctree="generated/">

ma.vander ma.polyfit

</div>

### Clipping and rounding

<div class="autosummary" data-toctree="generated/">

ma.around ma.clip ma.round

ma.MaskedArray.clip ma.MaskedArray.round

</div>

### Set operations

<div class="autosummary" data-toctree="generated/">

ma.intersect1d ma.setdiff1d ma.setxor1d ma.union1d

</div>

### Miscellanea

<div class="autosummary" data-toctree="generated/">

ma.allequal ma.allclose ma.amax ma.amin ma.apply\_along\_axis ma.apply\_over\_axes ma.arange ma.choose ma.compress\_nd ma.convolve ma.correlate ma.ediff1d ma.flatten\_mask ma.flatten\_structured\_array ma.fromflex ma.indices ma.left\_shift ma.ndim ma.put ma.putmask ma.right\_shift [ma.round]() ma.take ma.where

</div>

---

routines.math.md

---

# Mathematical functions

<div class="currentmodule">

numpy

</div>

## Trigonometric functions

<div class="autosummary" data-toctree="generated/">

sin cos tan arcsin asin arccos acos arctan atan hypot arctan2 atan2 degrees radians unwrap deg2rad rad2deg

</div>

## Hyperbolic functions

<div class="autosummary" data-toctree="generated/">

sinh cosh tanh arcsinh asinh arccosh acosh arctanh atanh

</div>

## Rounding

<div class="autosummary" data-toctree="generated/">

round around rint fix floor ceil trunc

</div>

## Sums, products, differences

<div class="autosummary" data-toctree="generated/">

prod sum nanprod nansum cumulative\_sum cumulative\_prod cumprod cumsum nancumprod nancumsum diff ediff1d gradient cross trapezoid

</div>

## Exponents and logarithms

<div class="autosummary" data-toctree="generated/">

exp expm1 exp2 log log10 log2 log1p logaddexp logaddexp2

</div>

## Other special functions

<div class="autosummary" data-toctree="generated/">

i0 sinc

</div>

## Floating point routines

<div class="autosummary" data-toctree="generated/">

signbit copysign frexp ldexp nextafter spacing

</div>

## Rational routines

<div class="autosummary" data-toctree="generated/">

lcm gcd

</div>

## Arithmetic operations

<div class="autosummary" data-toctree="generated/">

add reciprocal positive negative multiply divide power pow subtract true\_divide floor\_divide float\_power

fmod mod modf remainder divmod

</div>

## Handling complex numbers

<div class="autosummary" data-toctree="generated/">

angle real imag conj conjugate

</div>

## Extrema finding

<div class="autosummary" data-toctree="generated/">

maximum max amax fmax nanmax

minimum min amin fmin nanmin

</div>

## Miscellaneous

<div class="autosummary" data-toctree="generated/">

convolve clip

sqrt cbrt square

absolute fabs sign heaviside

nan\_to\_num real\_if\_close

interp

bitwise\_count

</div>

---

routines.matlib.md

---

# Matrix library (`numpy.matlib`)

<div class="currentmodule">

numpy

</div>

This module contains all functions in the `numpy` namespace, with the following replacement functions that return <span class="title-ref">matrices \<matrix\></span> instead of <span class="title-ref">ndarrays \<ndarray\></span>.

<div class="currentmodule">

numpy

</div>

Functions that are also in the numpy namespace and return matrices

<div class="autosummary">

matrix asmatrix bmat

</div>

Replacement functions in <span class="title-ref">matlib</span>

<div class="currentmodule">

numpy.matlib

</div>

<div class="autosummary" data-toctree="generated/">

empty zeros ones eye identity repmat rand randn

</div>

---

routines.md

---

# Routines and objects by topic

In this chapter routine docstrings are presented, grouped by functionality. Many docstrings contain example code, which demonstrates basic usage of the routine. The examples assume that NumPy is imported with:

    >>> import numpy as np

A convenient way to execute examples is the `%doctest_mode` mode of IPython, which allows for pasting of multi-line examples and preserves indentation.

<div class="toctree" data-maxdepth="1">

constants routines.array-creation routines.array-manipulation routines.bitwise routines.strings routines.datetime routines.dtype routines.emath routines.err routines.exceptions routines.fft routines.functional routines.io routines.indexing routines.linalg routines.logic routines.ma routines.math routines.other routines.polynomials random/index routines.set routines.sort routines.statistics routines.testing routines.window

</div>

---

routines.other.md

---

# Miscellaneous routines

<div class="toctree">

</div>

<div class="currentmodule">

numpy

</div>

## Performance tuning

<div class="autosummary" data-toctree="generated/">

setbufsize getbufsize

</div>

## Memory ranges

<div class="autosummary" data-toctree="generated/">

shares\_memory may\_share\_memory

</div>

## Utility

<div class="autosummary" data-toctree="generated/">

get\_include show\_config show\_runtime broadcast\_shapes

</div>

## NumPy-specific help function

<div class="autosummary" data-toctree="generated/">

info

</div>

---

routines.polynomials-package.md

---

- orphan

# `numpy.polynomial`

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial

</div>

## Configuration

<div class="autosummary" data-toctree="generated/">

numpy.polynomial.set\_default\_printstyle

</div>

---

routines.polynomials.chebyshev.md

---

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial.chebyshev

</div>

---

routines.polynomials.classes.md

---

# Using the convenience classes

The convenience classes provided by the polynomial package are:

<table>
<thead>
<tr class="header">
<th>Name</th>
<th>Provides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="title-ref">~numpy.polynomial.polynomial.Polynomial</span> Power s</td>
<td>eries</td>
</tr>
<tr class="even">
<td><span class="title-ref">~numpy.polynomial.chebyshev.Chebyshev</span> Chebysh</td>
<td>ev series</td>
</tr>
<tr class="odd">
<td><span class="title-ref">~numpy.polynomial.legendre.Legendre</span> Legendr</td>
<td>e series</td>
</tr>
<tr class="even">
<td><span class="title-ref">~numpy.polynomial.laguerre.Laguerre</span> Laguerr</td>
<td>e series</td>
</tr>
<tr class="odd">
<td><span class="title-ref">~numpy.polynomial.hermite.Hermite</span> Hermite</td>
<td><blockquote>
<p>series</p>
</blockquote></td>
</tr>
<tr class="even">
<td><span class="title-ref">~numpy.polynomial.hermite_e.HermiteE</span> Hermite</td>
<td>E series</td>
</tr>
</tbody>
</table>

The series in this context are finite sums of the corresponding polynomial basis functions multiplied by coefficients. For instance, a power series looks like

\[p(x) = 1 + 2x + 3x^2\]

and has coefficients \([1, 2, 3]\). The Chebyshev series with the same coefficients looks like

\[p(x) = 1 T_0(x) + 2 T_1(x) + 3 T_2(x)\]

and more generally

\[p(x) = \sum_{i=0}^n c_i T_i(x)\]

where in this case the \(T_n\) are the Chebyshev functions of degree \(n\), but could just as easily be the basis functions of any of the other classes. The convention for all the classes is that the coefficient \(c[i]\) goes with the basis function of degree i.

All of the classes are immutable and have the same methods, and especially they implement the Python numeric operators +, -, \*, //, %, divmod, \*\*, ==, and \!=. The last two can be a bit problematic due to floating point roundoff errors. We now give a quick demonstration of the various operations using NumPy version 1.7.0.

## Basics

First we need a polynomial class and a polynomial instance to play with. The classes can be imported directly from the polynomial package or from the module of the relevant type. Here we import from the package and use the conventional Polynomial class because of its familiarity:

    >>> from numpy.polynomial import Polynomial as P
    >>> p = P([1,2,3])
    >>> p
    Polynomial([1., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Note that there are three parts to the long version of the printout. The first is the coefficients, the second is the domain, and the third is the window:

    >>> p.coef
    array([1., 2., 3.])
    >>> p.domain
    array([-1.,  1.])
    >>> p.window
    array([-1.,  1.])

Printing a polynomial yields the polynomial expression in a more familiar format:

    >>> print(p)
    1.0 + 2.0Â·x + 3.0Â·xÂ²

Note that the string representation of polynomials uses Unicode characters by default (except on Windows) to express powers and subscripts. An ASCII-based representation is also available (default on Windows). The polynomial string format can be toggled at the package-level with the <span class="title-ref">\~numpy.polynomial.set\_default\_printstyle</span> function:

    >>> np.polynomial.set_default_printstyle('ascii')
    >>> print(p)
    1.0 + 2.0 x + 3.0 x**2

or controlled for individual polynomial instances with string formatting:

    >>> print(f"{p:unicode}")
    1.0 + 2.0Â·x + 3.0Â·xÂ²

We will deal with the domain and window when we get to fitting, for the moment we ignore them and run through the basic algebraic and arithmetic operations.

Addition and Subtraction:

    >>> p + p
    Polynomial([2., 4., 6.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> p - p
    Polynomial([0.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Multiplication:

    >>> p * p
    Polynomial([ 1.,   4.,  10.,  12.,   9.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Powers:

    >>> p**2
    Polynomial([ 1.,   4., 10., 12.,  9.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Division:

Floor division, '//', is the division operator for the polynomial classes, polynomials are treated like integers in this regard. For Python versions \< 3.x the '/' operator maps to '//', as it does for Python, for later versions the '/' will only work for division by scalars. At some point it will be deprecated:

    >>> p // P([-1, 1])
    Polynomial([5.,  3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Remainder:

    >>> p % P([-1, 1])
    Polynomial([6.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Divmod:

    >>> quo, rem = divmod(p, P([-1, 1]))
    >>> quo
    Polynomial([5.,  3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> rem
    Polynomial([6.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Evaluation:

    >>> x = np.arange(5)
    >>> p(x)
    array([  1.,   6.,  17.,  34.,  57.])
    >>> x = np.arange(6).reshape(3,2)
    >>> p(x)
    array([[ 1.,   6.],
           [17.,  34.],
           [57.,  86.]])

Substitution:

Substitute a polynomial for x and expand the result. Here we substitute p in itself leading to a new polynomial of degree 4 after expansion. If the polynomials are regarded as functions this is composition of functions:

    >>> p(p)
    Polynomial([ 6., 16., 36., 36., 27.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Roots:

    >>> p.roots()
    array([-0.33333333-0.47140452j, -0.33333333+0.47140452j])

It isn't always convenient to explicitly use Polynomial instances, so tuples, lists, arrays, and scalars are automatically cast in the arithmetic operations:

    >>> p + [1, 2, 3]
    Polynomial([2., 4., 6.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> [1, 2, 3] * p
    Polynomial([ 1.,  4., 10., 12.,  9.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> p / 2
    Polynomial([0.5, 1. , 1.5], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Polynomials that differ in domain, window, or class can't be mixed in arithmetic:

    >>> from numpy.polynomial import Chebyshev as T
    >>> p + P([1], domain=[0,1])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "<string>", line 213, in __add__
    TypeError: Domains differ
    >>> p + P([1], window=[0,1])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "<string>", line 215, in __add__
    TypeError: Windows differ
    >>> p + T([1])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "<string>", line 211, in __add__
    TypeError: Polynomial types differ

But different types can be used for substitution. In fact, this is how conversion of Polynomial classes among themselves is done for type, domain, and window casting:

    >>> p(T([0, 1]))
    Chebyshev([2.5, 2. , 1.5], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Which gives the polynomial <span class="title-ref">p</span> in Chebyshev form. This works because \(T_1(x) = x\) and substituting \(x\) for \(x\) doesn't change the original polynomial. However, all the multiplications and divisions will be done using Chebyshev series, hence the type of the result.

It is intended that all polynomial instances are immutable, therefore augmented operations (`+=`, `-=`, etc.) and any other functionality that would violate the immutablity of a polynomial instance are intentionally unimplemented.

## Calculus

Polynomial instances can be integrated and differentiated.:

    >>> from numpy.polynomial import Polynomial as P
    >>> p = P([2, 6])
    >>> p.integ()
    Polynomial([0., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> p.integ(2)
    Polynomial([0., 0., 1., 1.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

The first example integrates <span class="title-ref">p</span> once, the second example integrates it twice. By default, the lower bound of the integration and the integration constant are 0, but both can be specified.:

    >>> p.integ(lbnd=-1)
    Polynomial([-1.,  2.,  3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> p.integ(lbnd=-1, k=1)
    Polynomial([0., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

In the first case the lower bound of the integration is set to -1 and the integration constant is 0. In the second the constant of integration is set to 1 as well. Differentiation is simpler since the only option is the number of times the polynomial is differentiated:

    >>> p = P([1, 2, 3])
    >>> p.deriv(1)
    Polynomial([2., 6.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> p.deriv(2)
    Polynomial([6.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

## Other polynomial constructors

Constructing polynomials by specifying coefficients is just one way of obtaining a polynomial instance, they may also be created by specifying their roots, by conversion from other polynomial types, and by least squares fits. Fitting is discussed in its own section, the other methods are demonstrated below:

    >>> from numpy.polynomial import Polynomial as P
    >>> from numpy.polynomial import Chebyshev as T
    >>> p = P.fromroots([1, 2, 3])
    >>> p
    Polynomial([-6., 11., -6.,  1.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> p.convert(kind=T)
    Chebyshev([-9.  , 11.75, -3.  ,  0.25], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

The convert method can also convert domain and window:

    >>> p.convert(kind=T, domain=[0, 1])
    Chebyshev([-2.4375 ,  2.96875, -0.5625 ,  0.03125], domain=[0.,  1.], window=[-1.,  1.], symbol='x')
    >>> p.convert(kind=P, domain=[0, 1])
    Polynomial([-1.875,  2.875, -1.125,  0.125], domain=[0.,  1.], window=[-1.,  1.], symbol='x')

In numpy versions \>= 1.7.0 the <span class="title-ref">basis</span> and <span class="title-ref">cast</span> class methods are also available. The cast method works like the convert method while the basis method returns the basis polynomial of given degree:

    >>> P.basis(3)
    Polynomial([0., 0., 0., 1.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
    >>> T.cast(p)
    Chebyshev([-9.  , 11.75, -3. ,  0.25], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')

Conversions between types can be useful, but it is *not* recommended for routine use. The loss of numerical precision in passing from a Chebyshev series of degree 50 to a Polynomial series of the same degree can make the results of numerical evaluation essentially random.

## Fitting

Fitting is the reason that the <span class="title-ref">domain</span> and <span class="title-ref">window</span> attributes are part of the convenience classes. To illustrate the problem, the values of the Chebyshev polynomials up to degree 5 are plotted below.

<div class="plot">

\>\>\> import matplotlib.pyplot as plt \>\>\> from numpy.polynomial import Chebyshev as T \>\>\> x = np.linspace(-1, 1, 100) \>\>\> for i in range(6): ... ax = plt.plot(x, T.basis(i)(x), lw=2, label=f"$[T](){i}$") ... \>\>\> plt.legend(loc="upper left") \>\>\> plt.show()

</div>

In the range -1 \<= <span class="title-ref">x</span> \<= 1 they are nice, equiripple functions lying between +/- 1. The same plots over the range -2 \<= <span class="title-ref">x</span> \<= 2 look very different:

<div class="plot">

\>\>\> import matplotlib.pyplot as plt \>\>\> from numpy.polynomial import Chebyshev as T \>\>\> x = np.linspace(-2, 2, 100) \>\>\> for i in range(6): ... ax = plt.plot(x, T.basis(i)(x), lw=2, label=f"$[T](){i}$") ... \>\>\> plt.legend(loc="lower right") \>\>\> plt.show()

</div>

As can be seen, the "good" parts have shrunk to insignificance. In using Chebyshev polynomials for fitting we want to use the region where <span class="title-ref">x</span> is between -1 and 1 and that is what the <span class="title-ref">window</span> specifies. However, it is unlikely that the data to be fit has all its data points in that interval, so we use <span class="title-ref">domain</span> to specify the interval where the data points lie. When the fit is done, the domain is first mapped to the window by a linear transformation and the usual least squares fit is done using the mapped data points. The window and domain of the fit are part of the returned series and are automatically used when computing values, derivatives, and such. If they aren't specified in the call the fitting routine will use the default window and the smallest domain that holds all the data points. This is illustrated below for a fit to a noisy sine curve.

<div class="plot">

\>\>\> import numpy as np \>\>\> import matplotlib.pyplot as plt \>\>\> from numpy.polynomial import Chebyshev as T \>\>\> np.random.seed(11) \>\>\> x = np.linspace(0, 2\*np.pi, 20) \>\>\> y = np.sin(x) + np.random.normal(scale=.1, size=x.shape) \>\>\> p = T.fit(x, y, 5) \>\>\> plt.plot(x, y, 'o') \>\>\> xx, yy = p.linspace() \>\>\> plt.plot(xx, yy, lw=2) \>\>\> p.domain array(\[0. , 6.28318531\]) \>\>\> p.window array(\[-1., 1.\]) \>\>\> plt.show()

</div>

---

routines.polynomials.hermite.md

---

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial.hermite

</div>

---

routines.polynomials.hermite_e.md

---

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial.hermite\_e

</div>

---

routines.polynomials.laguerre.md

---

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial.laguerre

</div>

---

routines.polynomials.legendre.md

---

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial.legendre

</div>

---

routines.polynomials.md

---

# Polynomials

Polynomials in NumPy can be *created*, *manipulated*, and even *fitted* using the \[convenience classes \<routines.polynomials.classes\>\](convenience classes \<routines.polynomials.classes\>.md) of the <span class="title-ref">numpy.polynomial</span> package, introduced in NumPy 1.4.

Prior to NumPy 1.4, <span class="title-ref">numpy.poly1d</span> was the class of choice and it is still available in order to maintain backward compatibility. However, the newer <span class="title-ref">polynomial package \<numpy.polynomial\></span> is more complete and its <span class="title-ref">convenience classes \<routines.polynomials.classes\></span> provide a more consistent, better-behaved interface for working with polynomial expressions. Therefore `numpy.polynomial` is recommended for new coding.

<div class="note">

<div class="title">

Note

</div>

**Terminology**

The term *polynomial module* refers to the old API defined in `numpy.lib.polynomial`, which includes the <span class="title-ref">numpy.poly1d</span> class and the polynomial functions prefixed with *poly* accessible from the <span class="title-ref">numpy</span> namespace (e.g. <span class="title-ref">numpy.polyadd</span>, <span class="title-ref">numpy.polyval</span>, <span class="title-ref">numpy.polyfit</span>, etc.).

The term *polynomial package* refers to the new API defined in <span class="title-ref">numpy.polynomial</span>, which includes the convenience classes for the different kinds of polynomials (<span class="title-ref">\~numpy.polynomial.polynomial.Polynomial</span>, <span class="title-ref">\~numpy.polynomial.chebyshev.Chebyshev</span>, etc.).

</div>

## Transitioning from <span class="title-ref">numpy.poly1d</span> to <span class="title-ref">numpy.polynomial</span>

As noted above, the <span class="title-ref">poly1d class \<numpy.poly1d\></span> and associated functions defined in `numpy.lib.polynomial`, such as <span class="title-ref">numpy.polyfit</span> and <span class="title-ref">numpy.poly</span>, are considered legacy and should **not** be used in new code. Since NumPy version 1.4, the <span class="title-ref">numpy.polynomial</span> package is preferred for working with polynomials.

### Quick Reference

The following table highlights some of the main differences between the legacy polynomial module and the polynomial package for common tasks. The <span class="title-ref">\~numpy.polynomial.polynomial.Polynomial</span> class is imported for brevity:

    from numpy.polynomial import Polynomial

<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 32%" />
<col style="width: 41%" />
</colgroup>
<tbody>
<tr class="odd">
<td><blockquote>
<p><strong>How to...</strong></p>
</blockquote></td>
<td>Legacy (<span class="title-ref">numpy.poly1d</span>)</td>
<td><span class="title-ref">numpy.polynomial</span></td>
</tr>
<tr class="even">
<td>Create a polynomial object from coefficients[1]</td>
<td><code>p = np.poly1d([1, 2, 3])</code></td>
<td><code>p = Polynomial([3, 2, 1])</code></td>
</tr>
<tr class="odd">
<td>Create a polynomial object from roots</td>
<td><code>r = np.poly([-1, 1])</code> <code>p = np.poly1d(r)</code></td>
<td><code>p = Polynomial.fromroots([-1, 1])</code></td>
</tr>
<tr class="even">
<td>Fit a polynomial of degree <code>deg</code> to data</td>
<td><code>np.polyfit(x, y, deg)</code></td>
<td><code>Polynomial.fit(x, y, deg)</code></td>
</tr>
</tbody>
</table>

### Transition Guide

There are significant differences between `numpy.lib.polynomial` and <span class="title-ref">numpy.polynomial</span>. The most significant difference is the ordering of the coefficients for the polynomial expressions. The various routines in <span class="title-ref">numpy.polynomial</span> all deal with series whose coefficients go from degree zero upward, which is the *reverse order* of the poly1d convention. The easy way to remember this is that indices correspond to degree, i.e., `coef[i]` is the coefficient of the term of degree *i*.

Though the difference in convention may be confusing, it is straightforward to convert from the legacy polynomial API to the new. For example, the following demonstrates how you would convert a <span class="title-ref">numpy.poly1d</span> instance representing the expression \(x^{2} + 2x + 3\) to a <span class="title-ref">\~numpy.polynomial.polynomial.Polynomial</span> instance representing the same expression:

> \>\>\> import numpy as np
> 
> \>\>\> p1d = np.poly1d(\[1, 2, 3\]) \>\>\> p = np.polynomial.Polynomial(p1d.coef\[::-1\])
> 
> In addition to the `coef` attribute, polynomials from the polynomial package also have `domain` and `window` attributes. These attributes are most relevant when fitting polynomials to data, though it should be noted that polynomials with different `domain` and `window` attributes are not considered equal, and can't be mixed in arithmetic:
> 
> \>\>\> p1 = np.polynomial.Polynomial(\[1, 2, 3\]) \>\>\> p1 Polynomial(\[1., 2., 3.\], domain=\[-1., 1.\], window=\[-1., 1.\], symbol='x') \>\>\> p2 = np.polynomial.Polynomial(\[1, 2, 3\], domain=\[-2, 2\]) \>\>\> p1 == p2 False \>\>\> p1 + p2 Traceback (most recent call last): ... TypeError: Domains differ

See the documentation for the [convenience classes](routines.polynomials.classes) for further details on the `domain` and `window` attributes.

Another major difference between the legacy polynomial module and the polynomial package is polynomial fitting. In the old module, fitting was done via the <span class="title-ref">\~numpy.polyfit</span> function. In the polynomial package, the <span class="title-ref">\~numpy.polynomial.polynomial.Polynomial.fit</span> class method is preferred. For example, consider a simple linear fit to the following data:

<div class="ipython">

python

rng = np.random.default\_rng() x = np.arange(10) y = np.arange(10) + rng.standard\_normal(10)

</div>

With the legacy polynomial module, a linear fit (i.e. polynomial of degree 1) could be applied to these data with \`\~numpy.polyfit\`:

<div class="ipython">

python

np.polyfit(x, y, deg=1)

</div>

With the new polynomial API, the <span class="title-ref">\~numpy.polynomial.polynomial.Polynomial.fit</span> class method is preferred:

<div class="ipython">

python

p\_fitted = np.polynomial.Polynomial.fit(x, y, deg=1) p\_fitted

</div>

Note that the coefficients are given *in the scaled domain* defined by the linear mapping between the `window` and `domain`. <span class="title-ref">\~numpy.polynomial.polynomial.Polynomial.convert</span> can be used to get the coefficients in the unscaled data domain.

<div class="ipython">

python

p\_fitted.convert()

</div>

## Documentation for the <span class="title-ref">\~numpy.polynomial</span> package

In addition to standard power series polynomials, the polynomial package provides several additional kinds of polynomials including Chebyshev, Hermite (two subtypes), Laguerre, and Legendre polynomials. Each of these has an associated <span class="title-ref">convenience class \<routines.polynomials.classes\></span> available from the <span class="title-ref">numpy.polynomial</span> namespace that provides a consistent interface for working with polynomials regardless of their type.

<div class="toctree" data-maxdepth="1">

routines.polynomials.classes

</div>

Documentation pertaining to specific functions defined for each kind of polynomial individually can be found in the corresponding module documentation:

<div class="toctree" data-maxdepth="1">

routines.polynomials.polynomial routines.polynomials.chebyshev routines.polynomials.hermite routines.polynomials.hermite\_e routines.polynomials.laguerre routines.polynomials.legendre routines.polynomials.polyutils

</div>

## Documentation for legacy polynomials

<div class="toctree" data-maxdepth="2">

routines.polynomials.poly1d

</div>

1.  Note the reversed ordering of the coefficients

---

routines.polynomials.poly1d.md

---

# Poly1d

<div class="currentmodule">

numpy

</div>

## Basics

<div class="autosummary" data-toctree="generated/">

poly1d polyval poly roots

</div>

## Fitting

<div class="autosummary" data-toctree="generated/">

polyfit

</div>

## Calculus

<div class="autosummary" data-toctree="generated/">

polyder polyint

</div>

## Arithmetic

<div class="autosummary" data-toctree="generated/">

polyadd polydiv polymul polysub

</div>

---

routines.polynomials.polynomial.md

---

<div class="automodule" data-no-members="" data-no-inherited-members="" data-no-special-members="">

numpy.polynomial.polynomial

</div>

---

routines.polynomials.polyutils.md

---

# Polyutils

<div class="automodule">

numpy.polynomial.polyutils

</div>

---

routines.rec.md

---

# Record Arrays (`numpy.rec`)

<div class="currentmodule">

numpy.rec

</div>

<div class="module">

numpy.rec

</div>

Record arrays expose the fields of structured arrays as properties.

Most commonly, ndarrays contain elements of a single type, e.g. floats, integers, bools etc. However, it is possible for elements to be combinations of these using structured types, such as:

> \>\>\> import numpy as np \>\>\> a = np.array(\[(1, 2.0), (1, 2.0)\], ... dtype=\[('x', np.int64), ('y', np.float64)\]) \>\>\> a array(\[(1, 2.), (1, 2.)\], dtype=\[('x', '\<i8'), ('y', '\<f8')\])
> 
> Here, each element consists of two fields: x (and int), and y (a float). This is known as a structured array. The different fields are analogous to columns in a spread-sheet. The different fields can be accessed as one would a dictionary:
> 
> \>\>\> a\['x'\] array(\[1, 1\])
> 
> \>\>\> a\['y'\] array(\[2., 2.\])
> 
> Record arrays allow us to access fields as properties:
> 
> \>\>\> ar = np.rec.array(a) \>\>\> ar.x array(\[1, 1\]) \>\>\> ar.y array(\[2., 2.\])

## Functions

<div class="autosummary" data-toctree="generated/">

array find\_duplicate format\_parser fromarrays fromfile fromrecords fromstring

</div>

Also, the <span class="title-ref">numpy.recarray</span> class and the <span class="title-ref">numpy.record</span> scalar dtype are present in this namespace.

---

routines.set.md

---

# Set routines

<div class="currentmodule">

numpy

</div>

## Making proper sets

<div class="autosummary" data-toctree="generated/">

unique unique\_all unique\_counts unique\_inverse unique\_values

</div>

## Boolean operations

<div class="autosummary" data-toctree="generated/">

in1d intersect1d isin setdiff1d setxor1d union1d

</div>

---

routines.sort.md

---

# Sorting, searching, and counting

<div class="currentmodule">

numpy

</div>

## Sorting

<div class="autosummary" data-toctree="generated/">

sort lexsort argsort ndarray.sort sort\_complex partition argpartition

</div>

## Searching

<div class="autosummary" data-toctree="generated/">

argmax nanargmax argmin nanargmin argwhere nonzero flatnonzero where searchsorted extract

</div>

## Counting

<div class="autosummary" data-toctree="generated/">

count\_nonzero

</div>

---

routines.statistics.md

---

# Statistics

<div class="currentmodule">

numpy

</div>

## Order statistics

<div class="autosummary" data-toctree="generated/">

ptp percentile nanpercentile quantile nanquantile

</div>

## Averages and variances

<div class="autosummary" data-toctree="generated/">

median average mean std var nanmedian nanmean nanstd nanvar

</div>

## Correlating

<div class="autosummary" data-toctree="generated/">

corrcoef correlate cov

</div>

## Histograms

<div class="autosummary" data-toctree="generated/">

histogram histogram2d histogramdd bincount histogram\_bin\_edges digitize

</div>

---

routines.strings.md

---

# String functionality

<div class="currentmodule">

numpy.strings

</div>

<div class="module">

numpy.strings

</div>

The <span class="title-ref">numpy.strings</span> module provides a set of universal functions operating on arrays of type <span class="title-ref">numpy.str\_</span> or <span class="title-ref">numpy.bytes\_</span>. For example

> \>\>\> np.strings.add(\["num", "doc"\], \["py", "umentation"\]) array(\['numpy', 'documentation'\], dtype='\<U13')

These universal functions are also used in <span class="title-ref">numpy.char</span>, which provides the <span class="title-ref">numpy.char.chararray</span> array subclass, in order for those routines to get the performance benefits as well.

\> **Note** \> Prior to NumPy 2.0, all string functionality was in <span class="title-ref">numpy.char</span>, which only operated on fixed-width strings. That module will not be getting updates and will be deprecated at some point in the future.

## String operations

<div class="autosummary" data-toctree="generated/">

add center capitalize decode encode expandtabs ljust lower lstrip mod multiply partition replace rjust rpartition rstrip strip swapcase title translate upper zfill

</div>

## Comparison

The <span class="title-ref">numpy.strings</span> module also exports the comparison universal functions that can now operate on string arrays as well.

<div class="autosummary" data-toctree="generated/">

equal not\_equal greater\_equal less\_equal greater less

</div>

## String information

<div class="autosummary" data-toctree="generated/">

count endswith find index isalnum isalpha isdecimal isdigit islower isnumeric isspace istitle isupper rfind rindex startswith str\_len

</div>

---

routines.testing.md

---

# Test support (`numpy.testing`)

<div class="currentmodule">

numpy.testing

</div>

Common test support for all numpy test scripts.

This single module should provide all the common functionality for numpy tests in a single location, so that \[test scripts \<development-environment\>\](\#test-scripts \<development-environment\>) can just import it and work right away. For background, see the \[testing-guidelines\](\#testing-guidelines)

## Asserts

<div class="autosummary" data-toctree="generated/">

assert\_allclose assert\_array\_almost\_equal\_nulp assert\_array\_max\_ulp assert\_array\_equal assert\_array\_less assert\_equal assert\_raises assert\_raises\_regex assert\_warns assert\_no\_warnings assert\_no\_gc\_cycles assert\_string\_equal

</div>

## Asserts (not recommended)

It is recommended to use one of <span class="title-ref">assert\_allclose</span>, <span class="title-ref">assert\_array\_almost\_equal\_nulp</span> or <span class="title-ref">assert\_array\_max\_ulp</span> instead of these functions for more consistent floating point comparisons.

<div class="autosummary" data-toctree="generated/">

[assert]() assert\_almost\_equal assert\_approx\_equal assert\_array\_almost\_equal print\_assert\_equal

</div>

## Decorators

<div class="autosummary" data-toctree="generated/">

decorate\_methods

</div>

## Test running

<div class="autosummary" data-toctree="generated/">

clear\_and\_catch\_warnings measure rundocs suppress\_warnings

</div>

<div class="module">

numpy.testing.overrides

</div>

## Testing custom array containers (`numpy.testing.overrides`)

These functions can be useful when testing custom array container implementations which make use of `__array_ufunc__`/`__array_function__`.

<div class="currentmodule">

numpy.testing.overrides

</div>

<div class="autosummary" data-toctree="generated/">

allows\_array\_function\_override allows\_array\_ufunc\_override get\_overridable\_numpy\_ufuncs get\_overridable\_numpy\_array\_functions

</div>

## Guidelines

<div class="toctree">

testing

</div>

---

routines.version.md

---

<div class="currentmodule">

numpy.version

</div>

# Version information

The `numpy.version` submodule includes several constants that expose more detailed information about the exact version of the installed `numpy` package:

<div class="data">

version

Version string for the installed package - matches `numpy.__version__`.

</div>

<div class="data">

full\_version

Version string - the same as `numpy.version.version`.

</div>

<div class="data">

short\_version

Version string without any local build identifiers.

**Examples**

\>\>\> np.\_\_version\_\_ '2.1.0.dev0+git20240319.2ea7ce0' \# may vary \>\>\> np.version.short\_version '2.1.0.dev0' \# may vary

</div>

<div class="data">

git\_revision

String containing the git hash of the commit from which `numpy` was built.

</div>

<div class="data">

release

`True` if this version is a `numpy` release, `False` if a dev version.

</div>

---

routines.window.md

---

# Window functions

<div class="currentmodule">

numpy

</div>

## Various windows

<div class="autosummary" data-toctree="generated/">

bartlett blackman hamming hanning kaiser

</div>

---

security.md

---

# NumPy security

Security issues can be reported privately as described in the project README and when opening a [new issue on the issue tracker](https://github.com/numpy/numpy/issues/new/choose). The [Python security reporting guidelines](https://www.python.org/dev/security/) are a good resource and its notes apply also to NumPy.

NumPy's maintainers are not security experts. However, we are conscientious about security and experts of both the NumPy codebase and how it's used. Please do notify us before creating security advisories against NumPy as we are happy to prioritize issues or help with assessing the severity of a bug. A security advisory we are not aware of beforehand can lead to a lot of work for all involved parties.

## Advice for using NumPy on untrusted data

A user who can freely execute NumPy (or Python) functions must be considered to have the same privilege as the process/Python interpreter.

That said, NumPy should be generally safe to use on *data* provided by unprivileged users and read through safe API functions (e.g. loaded from a text file or `.npy` file without pickle support). Malicious *values* or *data sizes* should never lead to privilege escalation. Note that the above refers to array data. We do not currently consider for example `f2py` to be safe: it is typically used to compile a program that is then run. Any `f2py` invocation must thus use the same privilege as the later execution.

The following points may be useful or should be noted when working with untrusted data:

  - Exhausting memory can result in an out-of-memory kill, which is a possible denial of service attack. Possible causes could be:
      - Functions reading text files, which may require much more memory than the original input file size.
      - If users can create arbitrarily shaped arrays, NumPy's broadcasting means that intermediate or result arrays can be much larger than the inputs.
  - NumPy structured dtypes allow for a large amount of complexity. Fortunately, most code fails gracefully when a structured dtype is provided unexpectedly. However, code should either disallow untrusted users to provide these (e.g. via `.npy` files) or carefully check the fields included for nested structured/subarray dtypes.
  - Passing on user input should generally be considered unsafe (except for the data being read). An example would be `np.dtype(user_string)` or `dtype=user_string`.
  - The speed of operations can depend on values and memory order can lead to larger temporary memory use and slower execution. This means that operations may be significantly slower or use more memory compared to simple test cases.
  - When reading data, consider enforcing a specific shape (e.g. one dimensional) or dtype such as `float64`, `float32`, or `int64` to reduce complexity.

When working with non-trivial untrusted data, it is advisable to sandbox the analysis to guard against potential privilege escalation. This is especially advisable if further libraries based on NumPy are used since these add additional complexity and potential security issues.

---

build-options.md

---

# CPU build options

## Description

The following options are mainly used to change the default behavior of optimizations that target certain CPU features:

  - \- `cpu-baseline`: minimal set of required CPU features.  
    Default value is `min` which provides the minimum CPU features that can safely run on a wide range of platforms within the processor family.
    
    \> **Note**

  - \>  
    During the runtime, NumPy modules will fail to load if any of specified features are not supported by the target CPU (raises Python runtime error).

  - \- `cpu-dispatch`: dispatched set of additional CPU features.  
    Default value is `max -xop -fma4` which enables all CPU features, except for AMD legacy features (in case of X86).
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    During the runtime, NumPy modules will skip any specified features that are not available in the target CPU.
    
    </div>

These options are accessible at build time by passing setup arguments to meson-python via the build frontend (e.g., `pip` or `build`). They accept a set of \[CPU features \<opt-supported-features\>\](\#cpu-features-\<opt-supported-features\>) or groups of features that gather several features or \[special options \<opt-special-options\>\](\#special-options-\<opt-special-options\>) that perform a series of procedures.

To customize CPU/build options:

    pip install . -Csetup-args=-Dcpu-baseline="avx2 fma3" -Csetup-args=-Dcpu-dispatch="max"

## Quick start

In general, the default settings tend to not impose certain CPU features that may not be available on some older processors. Raising the ceiling of the baseline features will often improve performance and may also reduce binary size.

The following are the most common scenarios that may require changing the default settings:

### I am building NumPy for my local use

And I do not intend to export the build to other users or target a different CPU than what the host has.

Set `native` for baseline, or manually specify the CPU features in case of option `native` isn't supported by your platform:

    python -m build --wheel -Csetup-args=-Dcpu-baseline="native"

Building NumPy with extra CPU features isn't necessary for this case, since all supported features are already defined within the baseline features:

    python -m build --wheel -Csetup-args=-Dcpu-baseline="native" \
    -Csetup-args=-Dcpu-dispatch="none"

\> **Note** \> A fatal error will be raised if `native` isn't supported by the host platform.

### I do not want to support the old processors of the x86 architecture

Since most of the CPUs nowadays support at least `AVX`, `F16C` features, you can use:

    python -m build --wheel -Csetup-args=-Dcpu-baseline="avx f16c"

\> **Note** \> `cpu-baseline` force combine all implied features, so there's no need to add SSE features.

### I'm facing the same case above but with ppc64 architecture

Then raise the ceiling of the baseline features to Power8:

    python -m build --wheel -Csetup-args=-Dcpu-baseline="vsx2"

### Having issues with AVX512 features?

You may have some reservations about including of `AVX512` or any other CPU feature and you want to exclude from the dispatched features:

    python -m build --wheel -Csetup-args=-Dcpu-dispatch="max -avx512f -avx512cd \
    -avx512_knl -avx512_knm -avx512_skx -avx512_clx -avx512_cnl -avx512_icl"

## Supported features

The names of the features can express one feature or a group of features, as shown in the following tables supported depend on the lowest interest:

\> **Note** \> The following features may not be supported by all compilers, also some compilers may produce different set of implied features when it comes to features like `AVX512`, `AVX2`, and `FMA3`. See \[opt-platform-differences\](\#opt-platform-differences) for more details.

### On x86

| Name                                                                                                              | Implies                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Gathers                                         |
| ----------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `XOP` `FMA4` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD` | `SSE2` `SSE` `SSE` `SSE2` `SSE` `SSE2` `SSE3` `SSE` `SSE2` `SSE3` `SSSE3` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` |                                                 |
| `AVX512_KNL`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `AVX512ER` `AVX512PF`                           |
| `AVX512_KNM`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD` `AVX512_KNL`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `AVX5124FMAPS` `AVX5124VNNIW` `AVX512VPOPCNTDQ` |
| `AVX512_SKX`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `AVX512VL` `AVX512BW` `AVX512DQ`                |
| `AVX512_CLX`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD` `AVX512_SKX`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `AVX512VNNI`                                    |
| `AVX512_CNL`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD` `AVX512_SKX`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `AVX512IFMA` `AVX512VBMI`                       |
| `AVX512_ICL`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD` `AVX512_SKX` `AVX512_CLX` `AVX512_CNL`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `AVX512VBMI2` `AVX512BITALG` `AVX512VPOPCNTDQ`  |
| `AVX512_SPR`                                                                                                      | `SSE` `SSE2` `SSE3` `SSSE3` `SSE41` `POPCNT` `SSE42` `AVX` `F16C` `FMA3` `AVX2` `AVX512F` `AVX512CD` `AVX512_SKX` `AVX512_CLX` `AVX512_CNL` `AVX512_ICL`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `AVX512FP16`                                    |

### On IBM/POWER big-endian

| Name   | Implies             |
| ------ | ------------------- |
| `VSX`  |                     |
| `VSX2` | `VSX`               |
| `VSX3` | `VSX` `VSX2`        |
| `VSX4` | `VSX` `VSX2` `VSX3` |

### On IBM/POWER little-endian

| Name   | Implies             |
| ------ | ------------------- |
| `VSX`  | `VSX2`              |
| `VSX2` | `VSX`               |
| `VSX3` | `VSX` `VSX2`        |
| `VSX4` | `VSX` `VSX2` `VSX3` |

### On ARMv7/A32

| Name         | Implies                                           |
| ------------ | ------------------------------------------------- |
| `NEON`       |                                                   |
| `NEON_FP16`  | `NEON`                                            |
| `NEON_VFPV4` | `NEON` `NEON_FP16`                                |
| `ASIMD`      | `NEON` `NEON_FP16` `NEON_VFPV4`                   |
| `ASIMDHP`    | `NEON` `NEON_FP16` `NEON_VFPV4` `ASIMD`           |
| `ASIMDDP`    | `NEON` `NEON_FP16` `NEON_VFPV4` `ASIMD`           |
| `ASIMDFHM`   | `NEON` `NEON_FP16` `NEON_VFPV4` `ASIMD` `ASIMDHP` |

### On ARMv8/A64

| Name         | Implies                                           |
| ------------ | ------------------------------------------------- |
| `NEON`       | `NEON_FP16` `NEON_VFPV4` `ASIMD`                  |
| `NEON_FP16`  | `NEON` `NEON_VFPV4` `ASIMD`                       |
| `NEON_VFPV4` | `NEON` `NEON_FP16` `ASIMD`                        |
| `ASIMD`      | `NEON` `NEON_FP16` `NEON_VFPV4`                   |
| `ASIMDHP`    | `NEON` `NEON_FP16` `NEON_VFPV4` `ASIMD`           |
| `ASIMDDP`    | `NEON` `NEON_FP16` `NEON_VFPV4` `ASIMD`           |
| `ASIMDFHM`   | `NEON` `NEON_FP16` `NEON_VFPV4` `ASIMD` `ASIMDHP` |

### On IBM/ZSYSTEM(S390X)

| Name   | Implies    |
| ------ | ---------- |
| `VX`   |            |
| `VXE`  | `VX`       |
| `VXE2` | `VX` `VXE` |

## Special options

  - `NONE`: enable no features.

  - `NATIVE`: Enables all CPU features that supported by the host CPU, this operation is based on the compiler flags (`-march=native`, `-xHost`, `/QxHost`)

  - `MIN`: Enables the minimum CPU features that can safely run on a wide range of platforms:
    
    <table>
    <thead>
    <tr class="header">
    <th>For Arch</th>
    <th>Implies</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td><blockquote>
    <p>x86 (32-bit mode)</p>
    </blockquote></td>
    <td><code>SSE</code> <code>SSE2</code></td>
    </tr>
    <tr class="even">
    <td><blockquote>
    <p>x86_64</p>
    </blockquote></td>
    <td><code>SSE</code> <code>SSE2</code> <code>SSE3</code></td>
    </tr>
    <tr class="odd">
    <td><blockquote>
    <p>IBM/POWER (big-endian mode)</p>
    </blockquote></td>
    <td><code>NONE</code></td>
    </tr>
    <tr class="even">
    <td><blockquote>
    <p>IBM/POWER (little-endian mode)</p>
    </blockquote></td>
    <td><code>VSX</code> <code>VSX2</code></td>
    </tr>
    <tr class="odd">
    <td><blockquote>
    <p>ARMHF</p>
    </blockquote></td>
    <td><code>NONE</code></td>
    </tr>
    <tr class="even">
    <td><blockquote>
    <p>ARM64 A.K. AARCH64</p>
    </blockquote></td>
    <td><p><code>NEON</code> <code>NEON_FP16</code> <code>NEON_VFPV4</code> <code>ASIMD</code></p></td>
    </tr>
    <tr class="odd">
    <td><blockquote>
    <p>IBM/ZSYSTEM(S390X)</p>
    </blockquote></td>
    <td><code>NONE</code></td>
    </tr>
    </tbody>
    </table>

  - `MAX`: Enables all supported CPU features by the compiler and platform.

  - `Operators-/+`: remove or add features, useful with options `MAX`, `MIN` and `NATIVE`.

## Behaviors

  - CPU features and other options are case-insensitive, for example:
    
        python -m build --wheel -Csetup-args=-Dcpu-dispatch="SSE41 avx2 FMA3"

  - The order of the requested optimizations doesn't matter:
    
        python -m build --wheel -Csetup-args=-Dcpu-dispatch="SSE41 AVX2 FMA3"
        # equivalent to
        python -m build --wheel -Csetup-args=-Dcpu-dispatch="FMA3 AVX2 SSE41"

  - Either commas or spaces or '+' can be used as a separator, for example:
    
        python -m build --wheel -Csetup-args=-Dcpu-dispatch="avx2 avx512f"
        # or
        python -m build --wheel -Csetup-args=-Dcpu-dispatch=avx2,avx512f
        # or
        python -m build --wheel -Csetup-args=-Dcpu-dispatch="avx2+avx512f"
    
    all works but arguments should be enclosed in quotes or escaped by backslash if any spaces are used.

  - `cpu-baseline` combines all implied CPU features, for example:
    
        python -m build --wheel -Csetup-args=-Dcpu-baseline=sse42
        # equivalent to
        python -m build --wheel -Csetup-args=-Dcpu-baseline="sse sse2 sse3 ssse3 sse41 popcnt sse42"

  - `cpu-baseline` will be treated as "native" if compiler native flag `-march=native` or `-xHost` or `/QxHost` is enabled through environment variable `CFLAGS`:
    
        export CFLAGS="-march=native"
        pip install .
        # is equivalent to
        pip install . -Csetup-args=-Dcpu-baseline=native

<!-- end list -->

  - \- `cpu-baseline` escapes any specified features that aren't supported  
    by the target platform or compiler rather than raising fatal errors.
    
    \> **Note**

  - \>  
    Since `cpu-baseline` combines all implied features, the maximum supported of implied features will be enabled rather than escape all of them. For example:
    
        # Requesting `AVX2,FMA3` but the compiler only support **SSE** features
        python -m build --wheel -Csetup-args=-Dcpu-baseline="avx2 fma3"
        # is equivalent to
        python -m build --wheel -Csetup-args=-Dcpu-baseline="sse sse2 sse3 ssse3 sse41 popcnt sse42"

  - \- `cpu-dispatch` does not combine any of implied CPU features,  
    so you must add them unless you want to disable one or all of them:
    
        # Only dispatches AVX2 and FMA3
        python -m build --wheel -Csetup-args=-Dcpu-dispatch=avx2,fma3
        # Dispatches AVX and SSE features
        python -m build --wheel -Csetup-args=-Dcpu-dispatch=ssse3,sse41,sse42,avx,avx2,fma3

  - \- `cpu-dispatch` escapes any specified baseline features and also escapes  
    any features not supported by the target platform or compiler without raising fatal errors.

Eventually, you should always check the final report through the build log to verify the enabled features. See \[opt-build-report\](\#opt-build-report) for more details.

## Platform differences

Some exceptional conditions force us to link some features together when it come to certain compilers or architectures, resulting in the impossibility of building them separately.

These conditions can be divided into two parts, as follows:

**Architectural compatibility**

The need to align certain CPU features that are assured to be supported by successive generations of the same architecture, some cases:

  - On ppc64le `VSX(ISA 2.06)` and `VSX2(ISA 2.07)` both imply one another since the first generation that supports little-endian mode is Power-8\`(ISA 2.07)\`
  - On AArch64 `NEON NEON_FP16 NEON_VFPV4 ASIMD` implies each other since they are part of the hardware baseline.

For example:

    # On ARMv8/A64, specify NEON is going to enable Advanced SIMD
    # and all predecessor extensions
    python -m build --wheel -Csetup-args=-Dcpu-baseline=neon
    # which is equivalent to
    python -m build --wheel -Csetup-args=-Dcpu-baseline="neon neon_fp16 neon_vfpv4 asimd"

\> **Note** \> Please take a deep look at \[opt-supported-features\](\#opt-supported-features), in order to determine the features that imply one another.

**Compilation compatibility**

Some compilers don't provide independent support for all CPU features. For instance **Intel**'s compiler doesn't provide separated flags for `AVX2` and `FMA3`, it makes sense since all Intel CPUs that comes with `AVX2` also support `FMA3`, but this approach is incompatible with other **x86** CPUs from **AMD** or **VIA**.

For example:

    # Specify AVX2 will force enables FMA3 on Intel compilers
    python -m build --wheel -Csetup-args=-Dcpu-baseline=avx2
    # which is equivalent to
    python -m build --wheel -Csetup-args=-Dcpu-baseline="avx2 fma3"

The following tables only show the differences imposed by some compilers from the general context that been shown in the \[opt-supported-features\](\#opt-supported-features) tables:

\> **Note** \> Features names with strikeout represent the unsupported CPU features.

<style>
    .enabled-feature {color:green; font-weight:bold;}
    .disabled-feature {color:red; text-decoration: line-through;}
</style>

### On x86::Intel Compiler

| Name                                                                                   | Implies                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Gathers                                  |
| -------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| FMA3 AVX2 AVX512F <span class="disabled">XOP</span> <span class="disabled">FMA4</span> | SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C <span class="enabled">AVX2</span> SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C <span class="enabled">FMA3</span> SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 <span class="enabled">AVX512CD</span> <span class="disabled">SSE</span> <span class="disabled">SSE2</span> <span class="disabled">SSE3</span> <span class="disabled">SSSE3</span> <span class="disabled">SSE41</span> <span class="disabled">POPCNT</span> <span class="disabled">SSE42</span> <span class="disabled">AVX</span> <span class="disabled">SSE</span> <span class="disabled">SSE2</span> <span class="disabled">SSE3</span> <span class="disabled">SSSE3</span> <span class="disabled">SSE41</span> <span class="disabled">POPCNT</span> <span class="disabled">SSE42</span> <span class="disabled">AVX</span> |                                          |
| <span class="disabled">AVX512\_SPR</span>                                              | <span class="disabled">SSE</span> <span class="disabled">SSE2</span> <span class="disabled">SSE3</span> <span class="disabled">SSSE3</span> <span class="disabled">SSE41</span> <span class="disabled">POPCNT</span> <span class="disabled">SSE42</span> <span class="disabled">AVX</span> <span class="disabled">F16C</span> <span class="disabled">FMA3</span> <span class="disabled">AVX2</span> <span class="disabled">AVX512F</span> <span class="disabled">AVX512CD</span> <span class="disabled">AVX512\_SKX</span> <span class="disabled">AVX512\_CLX</span> <span class="disabled">AVX512\_CNL</span> <span class="disabled">AVX512\_ICL</span>                                                                                                                                                                                                  | <span class="disabled">AVX512FP16</span> |

### On x86::Microsoft Visual C/C++

| Name                                      | Implies                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Gathers                                                                                                                             |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| FMA3 AVX2 AVX512F AVX512CD                | SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C <span class="enabled">AVX2</span> SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C <span class="enabled">FMA3</span> SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 <span class="enabled">AVX512CD</span> <span class="enabled">AVX512\_SKX</span> SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F <span class="enabled">AVX512\_SKX</span>                                                                                                                                                                                                                                  |                                                                                                                                     |
| <span class="disabled">AVX512\_KNL</span> | <span class="disabled">SSE</span> <span class="disabled">SSE2</span> <span class="disabled">SSE3</span> <span class="disabled">SSSE3</span> <span class="disabled">SSE41</span> <span class="disabled">POPCNT</span> <span class="disabled">SSE42</span> <span class="disabled">AVX</span> <span class="disabled">F16C</span> <span class="disabled">FMA3</span> <span class="disabled">AVX2</span> <span class="disabled">AVX512F</span> <span class="disabled">AVX512CD</span>                                                                                                                                                                         | <span class="disabled">AVX512ER</span> <span class="disabled">AVX512PF</span>                                                       |
| <span class="disabled">AVX512\_KNM</span> | <span class="disabled">SSE</span> <span class="disabled">SSE2</span> <span class="disabled">SSE3</span> <span class="disabled">SSSE3</span> <span class="disabled">SSE41</span> <span class="disabled">POPCNT</span> <span class="disabled">SSE42</span> <span class="disabled">AVX</span> <span class="disabled">F16C</span> <span class="disabled">FMA3</span> <span class="disabled">AVX2</span> <span class="disabled">AVX512F</span> <span class="disabled">AVX512CD</span> <span class="disabled">AVX512\_KNL</span>                                                                                                                               | <span class="disabled">AVX5124FMAPS</span> <span class="disabled">AVX5124VNNIW</span> <span class="disabled">AVX512VPOPCNTDQ</span> |
| <span class="disabled">AVX512\_SPR</span> | <span class="disabled">SSE</span> <span class="disabled">SSE2</span> <span class="disabled">SSE3</span> <span class="disabled">SSSE3</span> <span class="disabled">SSE41</span> <span class="disabled">POPCNT</span> <span class="disabled">SSE42</span> <span class="disabled">AVX</span> <span class="disabled">F16C</span> <span class="disabled">FMA3</span> <span class="disabled">AVX2</span> <span class="disabled">AVX512F</span> <span class="disabled">AVX512CD</span> <span class="disabled">AVX512\_SKX</span> <span class="disabled">AVX512\_CLX</span> <span class="disabled">AVX512\_CNL</span> <span class="disabled">AVX512\_ICL</span> | <span class="disabled">AVX512FP16</span>                                                                                            |

## Build report

In most cases, the CPU build options do not produce any fatal errors that lead to hanging the build. Most of the errors that may appear in the build log serve as heavy warnings due to the lack of some expected CPU features by the compiler.

So we strongly recommend checking the final report log, to be aware of what kind of CPU features are enabled and what are not.

You can find the final report of CPU optimizations at the end of the build log, and here is how it looks on x86\_64/gcc:

<style>#build-report .highlight-bash pre{max-height:450px; overflow-y: scroll;}</style>

<div class="literalinclude" data-language="bash">

log\_example.txt

</div>

There is a separate report for each of `build_ext` and `build_clib` that includes several sections, and each section has several values, representing the following:

**Platform**:

  - <span class="enabled">Architecture</span>: The architecture name of target CPU. It should be one of `x86`, `x64`, `ppc64`, `ppc64le`, `armhf`, `aarch64`, `s390x` or `unknown`.
  - <span class="enabled">Compiler</span>: The compiler name. It should be one of gcc, clang, msvc, icc, iccw or unix-like.

**CPU baseline**:

  - <span class="enabled">Requested</span>: The specific features and options to `cpu-baseline` as-is.
  - <span class="enabled">Enabled</span>: The final set of enabled CPU features.
  - <span class="enabled">Flags</span>: The compiler flags that were used to all NumPy C/C++ sources during the compilation except for temporary sources that have been used for generating the binary objects of dispatched features.
  - <span class="enabled">Extra checks</span>: list of internal checks that activate certain functionality or intrinsics related to the enabled features, useful for debugging when it comes to developing SIMD kernels.

**CPU dispatch**:

  - <span class="enabled">Requested</span>: The specific features and options to `cpu-dispatch` as-is.
  - <span class="enabled">Enabled</span>: The final set of enabled CPU features.
  - <span class="enabled">Generated</span>: At the beginning of the next row of this property, the features for which optimizations have been generated are shown in the form of several sections with similar properties explained as follows:
      - <span class="enabled">One or multiple dispatched feature</span>: The implied CPU features.
      - <span class="enabled">Flags</span>: The compiler flags that been used for these features.
      - <span class="enabled">Extra checks</span>: Similar to the baseline but for these dispatched features.
      - <span class="enabled">Detect</span>: Set of CPU features that need be detected in runtime in order to execute the generated optimizations.
      - The lines that come after the above property and end with a ':' on a separate line, represent the paths of c/c++ sources that define the generated optimizations.

## Runtime dispatch

Importing NumPy triggers a scan of the available CPU features from the set of dispatchable features. This can be further restricted by setting the environment variable `NPY_DISABLE_CPU_FEATURES` to a comma-, tab-, or space-separated list of features to disable. This will raise an error if parsing fails or if the feature was not enabled. For instance, on `x86_64` this will disable `AVX2` and `FMA3`:

    NPY_DISABLE_CPU_FEATURES="AVX2,FMA3"

If the feature is not available, a warning will be emitted.

## Tracking dispatched functions

Discovering which CPU targets are enabled for different optimized functions is achievable through the Python function `numpy.lib.introspect.opt_func_info`. This function offers the flexibility of applying filters using two optional arguments: one for refining function names and the other for specifying data types in the signatures.

For example:

    >> func_info = numpy.lib.introspect.opt_func_info(func_name='add|abs', signature='float64|complex64')
    >> print(json.dumps(func_info, indent=2))
    {
      "absolute": {
        "dd": {
          "current": "SSE41",
          "available": "SSE41 baseline(SSE SSE2 SSE3)"
        },
        "Ff": {
          "current": "FMA3__AVX2",
          "available": "AVX512F FMA3__AVX2 baseline(SSE SSE2 SSE3)"
        },
        "Dd": {
          "current": "FMA3__AVX2",
          "available": "AVX512F FMA3__AVX2 baseline(SSE SSE2 SSE3)"
        }
      },
      "add": {
        "ddd": {
          "current": "FMA3__AVX2",
          "available": "FMA3__AVX2 baseline(SSE SSE2 SSE3)"
        },
        "FFF": {
          "current": "FMA3__AVX2",
          "available": "FMA3__AVX2 baseline(SSE SSE2 SSE3)"
        }
     }
    }

---

how-it-works.md

---

# How does the CPU dispatcher work?

NumPy dispatcher is based on multi-source compiling, which means taking a certain source and compiling it multiple times with different compiler flags and also with different **C** definitions that affect the code paths. This enables certain instruction-sets for each compiled object depending on the required optimizations and ends with linking the returned objects together.

![](../figures/opt-infra.png)

This mechanism should support all compilers and it doesn't require any compiler-specific extension, but at the same time it adds a few steps to normal compilation that are explained as follows.

## 1- Configuration

Configuring the required optimization by the user before starting to build the source files via the two command arguments as explained above:

  - `--cpu-baseline`: minimal set of required optimizations.
  - `--cpu-dispatch`: dispatched set of additional optimizations.

## 2- Discovering the environment

In this part, we check the compiler and platform architecture and cache some of the intermediary results to speed up rebuilding.

## 3- Validating the requested optimizations

By testing them against the compiler, and seeing what the compiler can support according to the requested optimizations.

## 4- Generating the main configuration header

The generated header `_cpu_dispatch.h` contains all the definitions and headers of instruction-sets for the required optimizations that have been validated during the previous step.

It also contains extra C definitions that are used for defining NumPy's Python-level module attributes `__cpu_baseline__` and `__cpu_dispatch__`.

**What is in this header?**

The example header was dynamically generated by gcc on an X86 machine. The compiler supports `--cpu-baseline="sse sse2 sse3"` and `--cpu-dispatch="ssse3 sse41"`, and the result is below.

``` c
// The header should be located at numpy/numpy/_core/src/common/_cpu_dispatch.h
/**NOTE
 ** C definitions prefixed with "NPY_HAVE_" represent
 ** the required optimizations.
 **
 ** C definitions prefixed with 'NPY__CPU_TARGET_' are protected and
 ** shouldn't be used by any NumPy C sources.
 */
/******* baseline features *******/
/** SSE **/
#define NPY_HAVE_SSE 1
#include <xmmintrin.h>
/** SSE2 **/
#define NPY_HAVE_SSE2 1
#include <emmintrin.h>
/** SSE3 **/
#define NPY_HAVE_SSE3 1
#include <pmmintrin.h>

/******* dispatch-able features *******/
#ifdef NPY__CPU_TARGET_SSSE3
  /** SSSE3 **/
  #define NPY_HAVE_SSSE3 1
  #include <tmmintrin.h>
#endif
#ifdef NPY__CPU_TARGET_SSE41
  /** SSE41 **/
  #define NPY_HAVE_SSE41 1
  #include <smmintrin.h>
#endif
```

**Baseline features** are the minimal set of required optimizations configured via `--cpu-baseline`. They have no preprocessor guards and they're always on, which means they can be used in any source.

Does this mean NumPy's infrastructure passes the compiler's flags of baseline features to all sources?

Definitely, yes. But the \[dispatch-able sources \<dispatchable-sources\>\](\#dispatch-able-sources-\<dispatchable-sources\>) are treated differently.

What if the user specifies certain **baseline features** during the build but at runtime the machine doesn't support even these features? Will the compiled code be called via one of these definitions, or maybe the compiler itself auto-generated/vectorized certain piece of code based on the provided command line compiler flags?

During the loading of the NumPy module, there's a validation step which detects this behavior. It will raise a Python runtime error to inform the user. This is to prevent the CPU reaching an illegal instruction error causing a segfault.

**Dispatch-able features** are our dispatched set of additional optimizations that were configured via `--cpu-dispatch`. They are not activated by default and are always guarded by other C definitions prefixed with `NPY__CPU_TARGET_`. C definitions `NPY__CPU_TARGET_` are only enabled within **dispatch-able sources**.

## 5- Dispatch-able sources and configuration statements

Dispatch-able sources are special **C** files that can be compiled multiple times with different compiler flags and also with different **C** definitions. These affect code paths to enable certain instruction-sets for each compiled object according to "**the configuration statements**" that must be declared between a **C** comment`(/**/)` and start with a special mark **@targets** at the top of each dispatch-able source. At the same time, dispatch-able sources will be treated as normal **C** sources if the optimization was disabled by the command argument `--disable-optimization` .

**What are configuration statements?**

Configuration statements are sort of keywords combined together to determine the required optimization for the dispatch-able source.

Example:

``` c
/*@targets avx2 avx512f vsx2 vsx3 asimd asimdhp */
// C code
```

The keywords mainly represent the additional optimizations configured through `--cpu-dispatch`, but it can also represent other options such as:

  - Target groups: pre-configured configuration statements used for managing the required optimizations from outside the dispatch-able source.
  - Policies: collections of options used for changing the default behaviors or forcing the compilers to perform certain things.
  - "baseline": a unique keyword represents the minimal optimizations that configured through `--cpu-baseline`

**Numpy's infrastructure handles dispatch-able sources in four steps**:

  - **(A) Recognition**: Just like source templates and F2PY, the dispatch-able sources requires a special extension `*.dispatch.c` to mark C dispatch-able source files, and for C++ `*.dispatch.cpp` or `*.dispatch.cxx` **NOTE**: C++ not supported yet.

  - **(B) Parsing and validating**: In this step, the dispatch-able sources that had been filtered by the previous step are parsed and validated by the configuration statements for each one of them one by one in order to determine the required optimizations.

  - **(C) Wrapping**: This is the approach taken by NumPy's infrastructure, which has proved to be sufficiently flexible in order to compile a single source multiple times with different **C** definitions and flags that affect the code paths. The process is achieved by creating a temporary **C** source for each required optimization that related to the additional optimization, which contains the declarations of the **C** definitions and includes the involved source via the **C** directive **\#include**. For more clarification take a look at the following code for AVX512F :
    
    ``` c
    /*
     * this definition is used by NumPy utilities as suffixes for the
     * exported symbols
     */
    #define NPY__CPU_TARGET_CURRENT AVX512F
    /*
     * The following definitions enable
     * definitions of the dispatch-able features that are defined within the main
     * configuration header. These are definitions for the implied features.
     */
    #define NPY__CPU_TARGET_SSE
    #define NPY__CPU_TARGET_SSE2
    #define NPY__CPU_TARGET_SSE3
    #define NPY__CPU_TARGET_SSSE3
    #define NPY__CPU_TARGET_SSE41
    #define NPY__CPU_TARGET_POPCNT
    #define NPY__CPU_TARGET_SSE42
    #define NPY__CPU_TARGET_AVX
    #define NPY__CPU_TARGET_F16C
    #define NPY__CPU_TARGET_FMA3
    #define NPY__CPU_TARGET_AVX2
    #define NPY__CPU_TARGET_AVX512F
    // our dispatch-able source
    #include "/the/absolute/path/of/hello.dispatch.c"
    ```

  - **(D) Dispatch-able configuration header**: The infrastructure generates a config header for each dispatch-able source, this header mainly contains two abstract **C** macros used for identifying the generated objects, so they can be used for runtime dispatching certain symbols from the generated objects by any **C** source. It is also used for forward declarations.
    
    The generated header takes the name of the dispatch-able source after excluding the extension and replace it with `.h`, for example assume we have a dispatch-able source called `hello.dispatch.c` and contains the following:
    
    ``` c
    // hello.dispatch.c
    /*@targets baseline sse42 avx512f */
    #include <stdio.h>
    #include "numpy/utils.h" // NPY_CAT, NPY_TOSTR
    
    #ifndef NPY__CPU_TARGET_CURRENT
      // wrapping the dispatch-able source only happens to the additional optimizations
      // but if the keyword 'baseline' provided within the configuration statements,
      // the infrastructure will add extra compiling for the dispatch-able source by
      // passing it as-is to the compiler without any changes.
      #define CURRENT_TARGET(X) X
      #define NPY__CPU_TARGET_CURRENT baseline // for printing only
    #else
      // since we reach to this point, that's mean we're dealing with
        // the additional optimizations, so it could be SSE42 or AVX512F
      #define CURRENT_TARGET(X) NPY_CAT(NPY_CAT(X, _), NPY__CPU_TARGET_CURRENT)
    #endif
    // Macro 'CURRENT_TARGET' adding the current target as suffix to the exported symbols,
    // to avoid linking duplications, NumPy already has a macro called
    // 'NPY_CPU_DISPATCH_CURFX' similar to it, located at
    // numpy/numpy/_core/src/common/npy_cpu_dispatch.h
    // NOTE: we tend to not adding suffixes to the baseline exported symbols
    void CURRENT_TARGET(simd_whoami)(const char *extra_info)
    {
        printf("I'm " NPY_TOSTR(NPY__CPU_TARGET_CURRENT) ", %s\n", extra_info);
    }
    ```
    
    Now assume you attached **hello.dispatch.c** to the source tree, then the infrastructure should generate a temporary config header called **hello.dispatch.h** that can be reached by any source in the source tree, and it should contain the following code :
    
    ``` c
    #ifndef NPY__CPU_DISPATCH_EXPAND_
      // To expand the macro calls in this header
        #define NPY__CPU_DISPATCH_EXPAND_(X) X
    #endif
    // Undefining the following macros, due to the possibility of including config headers
    // multiple times within the same source and since each config header represents
    // different required optimizations according to the specified configuration
    // statements in the dispatch-able source that derived from it.
    #undef NPY__CPU_DISPATCH_BASELINE_CALL
    #undef NPY__CPU_DISPATCH_CALL
    // nothing strange here, just a normal preprocessor callback
    // enabled only if 'baseline' specified within the configuration statements
    #define NPY__CPU_DISPATCH_BASELINE_CALL(CB, ...) \
      NPY__CPU_DISPATCH_EXPAND_(CB(__VA_ARGS__))
    // 'NPY__CPU_DISPATCH_CALL' is an abstract macro is used for dispatching
    // the required optimizations that specified within the configuration statements.
    //
    // @param CHK, Expected a macro that can be used to detect CPU features
    // in runtime, which takes a CPU feature name without string quotes and
    // returns the testing result in a shape of boolean value.
    // NumPy already has macro called "NPY_CPU_HAVE", which fits this requirement.
    //
    // @param CB, a callback macro that expected to be called multiple times depending
    // on the required optimizations, the callback should receive the following arguments:
    //  1- The pending calls of @param CHK filled up with the required CPU features,
    //     that need to be tested first in runtime before executing call belong to
    //     the compiled object.
    //  2- The required optimization name, same as in 'NPY__CPU_TARGET_CURRENT'
    //  3- Extra arguments in the macro itself
    //
    // By default the callback calls are sorted depending on the highest interest
    // unless the policy "$keep_sort" was in place within the configuration statements
    // see "Dive into the CPU dispatcher" for more clarification.
    #define NPY__CPU_DISPATCH_CALL(CHK, CB, ...) \
      NPY__CPU_DISPATCH_EXPAND_(CB((CHK(AVX512F)), AVX512F, __VA_ARGS__)) \
      NPY__CPU_DISPATCH_EXPAND_(CB((CHK(SSE)&&CHK(SSE2)&&CHK(SSE3)&&CHK(SSSE3)&&CHK(SSE41)), SSE41, __VA_ARGS__))
    ```
    
    An example of using the config header in light of the above:
    
    ``` c
    // NOTE: The following macros are only defined for demonstration purposes only.
    // NumPy already has a collections of macros located at
    // numpy/numpy/_core/src/common/npy_cpu_dispatch.h, that covers all dispatching
    // and declarations scenarios.
    
    #include "numpy/npy_cpu_features.h" // NPY_CPU_HAVE
    #include "numpy/utils.h" // NPY_CAT, NPY_EXPAND
    
    // An example for setting a macro that calls all the exported symbols at once
    // after checking if they're supported by the running machine.
    #define DISPATCH_CALL_ALL(FN, ARGS) \
        NPY__CPU_DISPATCH_CALL(NPY_CPU_HAVE, DISPATCH_CALL_ALL_CB, FN, ARGS) \
        NPY__CPU_DISPATCH_BASELINE_CALL(DISPATCH_CALL_BASELINE_ALL_CB, FN, ARGS)
    // The preprocessor callbacks.
    // The same suffixes as we define it in the dispatch-able source.
    #define DISPATCH_CALL_ALL_CB(CHECK, TARGET_NAME, FN, ARGS) \
      if (CHECK) { NPY_CAT(NPY_CAT(FN, _), TARGET_NAME) ARGS; }
    #define DISPATCH_CALL_BASELINE_ALL_CB(FN, ARGS) \
      FN NPY_EXPAND(ARGS);
    
    // An example for setting a macro that calls the exported symbols of highest
    // interest optimization, after checking if they're supported by the running machine.
    #define DISPATCH_CALL_HIGH(FN, ARGS) \
      if (0) {} \
        NPY__CPU_DISPATCH_CALL(NPY_CPU_HAVE, DISPATCH_CALL_HIGH_CB, FN, ARGS) \
        NPY__CPU_DISPATCH_BASELINE_CALL(DISPATCH_CALL_BASELINE_HIGH_CB, FN, ARGS)
    // The preprocessor callbacks
    // The same suffixes as we define it in the dispatch-able source.
    #define DISPATCH_CALL_HIGH_CB(CHECK, TARGET_NAME, FN, ARGS) \
      else if (CHECK) { NPY_CAT(NPY_CAT(FN, _), TARGET_NAME) ARGS; }
    #define DISPATCH_CALL_BASELINE_HIGH_CB(FN, ARGS) \
      else { FN NPY_EXPAND(ARGS); }
    
    // NumPy has a macro called 'NPY_CPU_DISPATCH_DECLARE' can be used
    // for forward declarations any kind of prototypes based on
    // 'NPY__CPU_DISPATCH_CALL' and 'NPY__CPU_DISPATCH_BASELINE_CALL'.
    // However in this example, we just handle it manually.
    void simd_whoami(const char *extra_info);
    void simd_whoami_AVX512F(const char *extra_info);
    void simd_whoami_SSE41(const char *extra_info);
    
    void trigger_me(void)
    {
        // bring the auto-generated config header
        // which contains config macros 'NPY__CPU_DISPATCH_CALL' and
        // 'NPY__CPU_DISPATCH_BASELINE_CALL'.
        // it is highly recommended to include the config header before executing
      // the dispatching macros in case if there's another header in the scope.
        #include "hello.dispatch.h"
        DISPATCH_CALL_ALL(simd_whoami, ("all"))
        DISPATCH_CALL_HIGH(simd_whoami, ("the highest interest"))
        // An example of including multiple config headers in the same source
        // #include "hello2.dispatch.h"
        // DISPATCH_CALL_HIGH(another_function, ("the highest interest"))
    }
    ```

---

index.md

---

# CPU/SIMD optimizations

NumPy comes with a flexible working mechanism that allows it to harness the SIMD features that CPUs own, in order to provide faster and more stable performance on all popular platforms. Currently, NumPy supports the X86, IBM/Power, ARM7 and ARM8 architectures.

The optimization process in NumPy is carried out in three layers:

  - Code is *written* using the universal intrinsics which is a set of types, macros and functions that are mapped to each supported instruction-sets by using guards that will enable use of the them only when the compiler recognizes them. This allow us to generate multiple kernels for the same functionality, in which each generated kernel represents a set of instructions that related one or multiple certain CPU features. The first kernel represents the minimum (baseline) CPU features, and the other kernels represent the additional (dispatched) CPU features.
  - At *compile* time, CPU build options are used to define the minimum and additional features to support, based on user choice and compiler support. The appropriate intrinsics are overlaid with the platform / architecture intrinsics, and multiple kernels are compiled.
  - At *runtime import*, the CPU is probed for the set of supported CPU features. A mechanism is used to grab the pointer to the most appropriate kernel, and this will be the one called for the function.

\> **Note** \> NumPy community had a deep discussion before implementing this work, please check [NEP-38](https://numpy.org/neps/nep-0038-SIMD-optimizations.html) for more clarification.

<div class="toctree">

build-options how-it-works

</div>

---

simd-optimizations.md

---

- orphan

<html>
    <head>
        <meta http-equiv="refresh" content="0; url=index.html"/>
    </head>
</html>

The location of this document has been changed , if you are not redirected in few seconds, [click here](index.html).

---

swig.interface-file.md

---

# numpy.i: a SWIG interface file for NumPy

## Introduction

The Simple Wrapper and Interface Generator (or [SWIG](https://www.swig.org)) is a powerful tool for generating wrapper code for interfacing to a wide variety of scripting languages. [SWIG]() can parse header files, and using only the code prototypes, create an interface to the target language. But [SWIG]() is not omnipotent. For example, it cannot know from the prototype:

    double rms(double* seq, int n);

what exactly `seq` is. Is it a single value to be altered in-place? Is it an array, and if so what is its length? Is it input-only? Output-only? Input-output? [SWIG]() cannot determine these details, and does not attempt to do so.

If we designed `rms`, we probably made it a routine that takes an input-only array of length `n` of `double` values called `seq` and returns the root mean square. The default behavior of [SWIG](), however, will be to create a wrapper function that compiles, but is nearly impossible to use from the scripting language in the way the C routine was intended.

For Python, the preferred way of handling contiguous (or technically, *strided*) blocks of homogeneous data is with NumPy, which provides full object-oriented access to multidimensial arrays of data. Therefore, the most logical Python interface for the `rms` function would be (including doc string):

    def rms(seq):
        """
        rms: return the root mean square of a sequence
        rms(numpy.ndarray) -> double
        rms(list) -> double
        rms(tuple) -> double
        """

where `seq` would be a NumPy array of `double` values, and its length `n` would be extracted from `seq` internally before being passed to the C routine. Even better, since NumPy supports construction of arrays from arbitrary Python sequences, `seq` itself could be a nearly arbitrary sequence (so long as each element can be converted to a `double`) and the wrapper code would internally convert it to a NumPy array before extracting its data and length.

[SWIG]() allows these types of conversions to be defined via a mechanism called *typemaps*. This document provides information on how to use `numpy.i`, a [SWIG]() interface file that defines a series of typemaps intended to make the type of array-related conversions described above relatively simple to implement. For example, suppose that the `rms` function prototype defined above was in a header file named `rms.h`. To obtain the Python interface discussed above, your [SWIG]() interface file would need the following:

    %{
    #define SWIG_FILE_WITH_INIT
    #include "rms.h"
    %}
    
    %include "numpy.i"
    
    %init %{
    import_array();
    %}
    
    %apply (double* IN_ARRAY1, int DIM1) {(double* seq, int n)};
    %include "rms.h"

Typemaps are keyed off a list of one or more function arguments, either by type or by type and name. We will refer to such lists as *signatures*. One of the many typemaps defined by `numpy.i` is used above and has the signature `(double* IN_ARRAY1, int DIM1)`. The argument names are intended to suggest that the `double*` argument is an input array of one dimension and that the `int` represents the size of that dimension. This is precisely the pattern in the `rms` prototype.

Most likely, no actual prototypes to be wrapped will have the argument names `IN_ARRAY1` and `DIM1`. We use the [SWIG]() `%apply` directive to apply the typemap for one-dimensional input arrays of type `double` to the actual prototype used by `rms`. Using `numpy.i` effectively, therefore, requires knowing what typemaps are available and what they do.

A [SWIG]() interface file that includes the [SWIG]() directives given above will produce wrapper code that looks something like:

    1 PyObject *_wrap_rms(PyObject *args) {
    2   PyObject *resultobj = 0;
    3   double *arg1 = (double *) 0 ;
    4   int arg2 ;
    5   double result;
    6   PyArrayObject *array1 = NULL ;
    7   int is_new_object1 = 0 ;
    8   PyObject * obj0 = 0 ;
    9
    10   if (!PyArg_ParseTuple(args,(char *)"O:rms",&obj0)) SWIG_fail;
    11   {
    12     array1 = obj_to_array_contiguous_allow_conversion(
    13                  obj0, NPY_DOUBLE, &is_new_object1);
    14     npy_intp size[1] = {
    15       -1
    16     };
    17     if (!array1 || !require_dimensions(array1, 1) ||
    18         !require_size(array1, size, 1)) SWIG_fail;
    19     arg1 = (double*) array1->data;
    20     arg2 = (int) array1->dimensions[0];
    21   }
    22   result = (double)rms(arg1,arg2);
    23   resultobj = SWIG_From_double((double)(result));
    24   {
    25     if (is_new_object1 && array1) Py_DECREF(array1);
    26   }
    27   return resultobj;
    28 fail:
    29   {
    30     if (is_new_object1 && array1) Py_DECREF(array1);
    31   }
    32   return NULL;
    33 }

The typemaps from `numpy.i` are responsible for the following lines of code: 12--20, 25 and 30. Line 10 parses the input to the `rms` function. From the format string `"O:rms"`, we can see that the argument list is expected to be a single Python object (specified by the `O` before the colon) and whose pointer is stored in `obj0`. A number of functions, supplied by `numpy.i`, are called to make and check the (possible) conversion from a generic Python object to a NumPy array. These functions are explained in the section [Helper Functions](#helper-functions), but hopefully their names are self-explanatory. At line 12 we use `obj0` to construct a NumPy array. At line 17, we check the validity of the result: that it is non-null and that it has a single dimension of arbitrary length. Once these states are verified, we extract the data buffer and length in lines 19 and 20 so that we can call the underlying C function at line 22. Line 25 performs memory management for the case where we have created a new array that is no longer needed.

This code has a significant amount of error handling. Note the `SWIG_fail` is a macro for `goto fail`, referring to the label at line 28. If the user provides the wrong number of arguments, this will be caught at line 10. If construction of the NumPy array fails or produces an array with the wrong number of dimensions, these errors are caught at line 17. And finally, if an error is detected, memory is still managed correctly at line 30.

Note that if the C function signature was in a different order:

    double rms(int n, double* seq);

that [SWIG]() would not match the typemap signature given above with the argument list for `rms`. Fortunately, `numpy.i` has a set of typemaps with the data pointer given last:

    %apply (int DIM1, double* IN_ARRAY1) {(int n, double* seq)};

This simply has the effect of switching the definitions of `arg1` and `arg2` in lines 3 and 4 of the generated code above, and their assignments in lines 19 and 20.

## Using numpy.i

The `numpy.i` file is currently located in the `tools/swig` sub-directory under the `numpy` installation directory. Typically, you will want to copy it to the directory where you are developing your wrappers.

A simple module that only uses a single [SWIG]() interface file should include the following:

    %{
    #define SWIG_FILE_WITH_INIT
    %}
    %include "numpy.i"
    %init %{
    import_array();
    %}

Within a compiled Python module, `import_array()` should only get called once. This could be in a C/C++ file that you have written and is linked to the module. If this is the case, then none of your interface files should `#define SWIG_FILE_WITH_INIT` or call `import_array()`. Or, this initialization call could be in a wrapper file generated by [SWIG]() from an interface file that has the `%init` block as above. If this is the case, and you have more than one [SWIG]() interface file, then only one interface file should `#define SWIG_FILE_WITH_INIT` and call `import_array()`.

## Available typemaps

The typemap directives provided by `numpy.i` for arrays of different data types, say `double` and `int`, and dimensions of different types, say `int` or `long`, are identical to one another except for the C and NumPy type specifications. The typemaps are therefore implemented (typically behind the scenes) via a macro:

    %numpy_typemaps(DATA_TYPE, DATA_TYPECODE, DIM_TYPE)

that can be invoked for appropriate `(DATA_TYPE, DATA_TYPECODE, DIM_TYPE)` triplets. For example:

    %numpy_typemaps(double, NPY_DOUBLE, int)
    %numpy_typemaps(int,    NPY_INT   , int)

The `numpy.i` interface file uses the `%numpy_typemaps` macro to implement typemaps for the following C data types and `int` dimension types:

  - `signed char`
  - `unsigned char`
  - `short`
  - `unsigned short`
  - `int`
  - `unsigned int`
  - `long`
  - `unsigned long`
  - `long long`
  - `unsigned long long`
  - `float`
  - `double`

In the following descriptions, we reference a generic `DATA_TYPE`, which could be any of the C data types listed above, and `DIM_TYPE` which should be one of the many types of integers.

The typemap signatures are largely differentiated on the name given to the buffer pointer. Names with `FARRAY` are for Fortran-ordered arrays, and names with `ARRAY` are for C-ordered (or 1D arrays).

### Input Arrays

Input arrays are defined as arrays of data that are passed into a routine but are not altered in-place or returned to the user. The Python input array is therefore allowed to be almost any Python sequence (such as a list) that can be converted to the requested type of array. The input array signatures are

1D:

  - `(   DATA_TYPE IN_ARRAY1[ANY] )`
  - `(   DATA_TYPE* IN_ARRAY1, int DIM1 )`
  - `(   int DIM1, DATA_TYPE* IN_ARRAY1 )`

2D:

  - `(   DATA_TYPE IN_ARRAY2[ANY][ANY] )`
  - `(   DATA_TYPE* IN_ARRAY2, int DIM1, int DIM2 )`
  - `(   int DIM1, int DIM2, DATA_TYPE* IN_ARRAY2 )`
  - `(   DATA_TYPE* IN_FARRAY2, int DIM1, int DIM2 )`
  - `(   int DIM1, int DIM2, DATA_TYPE* IN_FARRAY2 )`

3D:

  - `(   DATA_TYPE IN_ARRAY3[ANY][ANY][ANY] )`
  - `(   DATA_TYPE* IN_ARRAY3, int DIM1, int DIM2, int DIM3 )`
  - `(   int DIM1, int DIM2, int DIM3, DATA_TYPE* IN_ARRAY3 )`
  - `(   DATA_TYPE* IN_FARRAY3, int DIM1, int DIM2, int DIM3 )`
  - `(   int DIM1, int DIM2, int DIM3, DATA_TYPE* IN_FARRAY3 )`

4D:

  - `(DATA_TYPE IN_ARRAY4[ANY][ANY][ANY][ANY])`
  - `(DATA_TYPE* IN_ARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)`
  - `(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, , DIM_TYPE DIM4, DATA_TYPE* IN_ARRAY4)`
  - `(DATA_TYPE* IN_FARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)`
  - `(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4, DATA_TYPE* IN_FARRAY4)`

The first signature listed, `( DATA_TYPE IN_ARRAY[ANY] )` is for one-dimensional arrays with hard-coded dimensions. Likewise, `( DATA_TYPE IN_ARRAY2[ANY][ANY] )` is for two-dimensional arrays with hard-coded dimensions, and similarly for three-dimensional.

### In-Place Arrays

In-place arrays are defined as arrays that are modified in-place. The input values may or may not be used, but the values at the time the function returns are significant. The provided Python argument must therefore be a NumPy array of the required type. The in-place signatures are

1D:

  - `(   DATA_TYPE INPLACE_ARRAY1[ANY] )`
  - `(   DATA_TYPE* INPLACE_ARRAY1, int DIM1 )`
  - `(   int DIM1, DATA_TYPE* INPLACE_ARRAY1 )`

2D:

  - `(   DATA_TYPE INPLACE_ARRAY2[ANY][ANY] )`
  - `(   DATA_TYPE* INPLACE_ARRAY2, int DIM1, int DIM2 )`
  - `(   int DIM1, int DIM2, DATA_TYPE* INPLACE_ARRAY2 )`
  - `(   DATA_TYPE* INPLACE_FARRAY2, int DIM1, int DIM2 )`
  - `(   int DIM1, int DIM2, DATA_TYPE* INPLACE_FARRAY2 )`

3D:

  - `(   DATA_TYPE INPLACE_ARRAY3[ANY][ANY][ANY] )`
  - `(   DATA_TYPE* INPLACE_ARRAY3, int DIM1, int DIM2, int DIM3 )`
  - `(   int DIM1, int DIM2, int DIM3, DATA_TYPE* INPLACE_ARRAY3 )`
  - `(   DATA_TYPE* INPLACE_FARRAY3, int DIM1, int DIM2, int DIM3 )`
  - `(   int DIM1, int DIM2, int DIM3, DATA_TYPE* INPLACE_FARRAY3 )`

4D:

  - `(DATA_TYPE INPLACE_ARRAY4[ANY][ANY][ANY][ANY])`
  - `(DATA_TYPE* INPLACE_ARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)`
  - `(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, , DIM_TYPE DIM4, DATA_TYPE* INPLACE_ARRAY4)`
  - `(DATA_TYPE* INPLACE_FARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)`
  - `(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4, DATA_TYPE* INPLACE_FARRAY4)`

These typemaps now check to make sure that the `INPLACE_ARRAY` arguments use native byte ordering. If not, an exception is raised.

There is also a "flat" in-place array for situations in which you would like to modify or process each element, regardless of the number of dimensions. One example is a "quantization" function that quantizes each element of an array in-place, be it 1D, 2D or whatever. This form checks for continuity but allows either C or Fortran ordering.

ND:

  - `(DATA_TYPE* INPLACE_ARRAY_FLAT, DIM_TYPE DIM_FLAT)`

### Argout Arrays

Argout arrays are arrays that appear in the input arguments in C, but are in fact output arrays. This pattern occurs often when there is more than one output variable and the single return argument is therefore not sufficient. In Python, the conventional way to return multiple arguments is to pack them into a sequence (tuple, list, etc.) and return the sequence. This is what the argout typemaps do. If a wrapped function that uses these argout typemaps has more than one return argument, they are packed into a tuple or list, depending on the version of Python. The Python user does not pass these arrays in, they simply get returned. For the case where a dimension is specified, the python user must provide that dimension as an argument. The argout signatures are

1D:

  - `(   DATA_TYPE ARGOUT_ARRAY1[ANY] )`
  - `(   DATA_TYPE* ARGOUT_ARRAY1, int DIM1 )`
  - `(   int DIM1, DATA_TYPE* ARGOUT_ARRAY1 )`

2D:

  - `(   DATA_TYPE ARGOUT_ARRAY2[ANY][ANY] )`

3D:

  - `(   DATA_TYPE ARGOUT_ARRAY3[ANY][ANY][ANY] )`

4D:

  - `(   DATA_TYPE ARGOUT_ARRAY4[ANY][ANY][ANY][ANY] )`

These are typically used in situations where in C/C++, you would allocate a(n) array(s) on the heap, and call the function to fill the array(s) values. In Python, the arrays are allocated for you and returned as new array objects.

Note that we support `DATA_TYPE*` argout typemaps in 1D, but not 2D or 3D. This is because of a quirk with the [SWIG]() typemap syntax and cannot be avoided. Note that for these types of 1D typemaps, the Python function will take a single argument representing `DIM1`.

### Argout View Arrays

Argoutview arrays are for when your C code provides you with a view of its internal data and does not require any memory to be allocated by the user. This can be dangerous. There is almost no way to guarantee that the internal data from the C code will remain in existence for the entire lifetime of the NumPy array that encapsulates it. If the user destroys the object that provides the view of the data before destroying the NumPy array, then using that array may result in bad memory references or segmentation faults. Nevertheless, there are situations, working with large data sets, where you simply have no other choice.

The C code to be wrapped for argoutview arrays are characterized by pointers: pointers to the dimensions and double pointers to the data, so that these values can be passed back to the user. The argoutview typemap signatures are therefore

1D:

  - `( DATA_TYPE** ARGOUTVIEW_ARRAY1, DIM_TYPE* DIM1 )`
  - `( DIM_TYPE* DIM1, DATA_TYPE** ARGOUTVIEW_ARRAY1 )`

2D:

  - `( DATA_TYPE** ARGOUTVIEW_ARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2 )`
  - `( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEW_ARRAY2 )`
  - `( DATA_TYPE** ARGOUTVIEW_FARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2 )`
  - `( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEW_FARRAY2 )`

3D:

  - `( DATA_TYPE** ARGOUTVIEW_ARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)`
  - `( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEW_ARRAY3)`
  - `( DATA_TYPE** ARGOUTVIEW_FARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)`
  - `( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEW_FARRAY3)`

4D:

  - `(DATA_TYPE** ARGOUTVIEW_ARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEW_ARRAY4)`
  - `(DATA_TYPE** ARGOUTVIEW_FARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEW_FARRAY4)`

Note that arrays with hard-coded dimensions are not supported. These cannot follow the double pointer signatures of these typemaps.

### Memory Managed Argout View Arrays

A recent addition to `numpy.i` are typemaps that permit argout arrays with views into memory that is managed.

1D:

  - `(DATA_TYPE** ARGOUTVIEWM_ARRAY1, DIM_TYPE* DIM1)`
  - `(DIM_TYPE* DIM1, DATA_TYPE** ARGOUTVIEWM_ARRAY1)`

2D:

  - `(DATA_TYPE** ARGOUTVIEWM_ARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEWM_ARRAY2)`
  - `(DATA_TYPE** ARGOUTVIEWM_FARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEWM_FARRAY2)`

3D:

  - `(DATA_TYPE** ARGOUTVIEWM_ARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEWM_ARRAY3)`
  - `(DATA_TYPE** ARGOUTVIEWM_FARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEWM_FARRAY3)`

4D:

  - `(DATA_TYPE** ARGOUTVIEWM_ARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEWM_ARRAY4)`
  - `(DATA_TYPE** ARGOUTVIEWM_FARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)`
  - `(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEWM_FARRAY4)`

### Output Arrays

The `numpy.i` interface file does not support typemaps for output arrays, for several reasons. First, C/C++ return arguments are limited to a single value. This prevents obtaining dimension information in a general way. Second, arrays with hard-coded lengths are not permitted as return arguments. In other words:

    double[3] newVector(double x, double y, double z);

is not legal C/C++ syntax. Therefore, we cannot provide typemaps of the form:

    %typemap(out) (TYPE[ANY]);

If you run into a situation where a function or method is returning a pointer to an array, your best bet is to write your own version of the function to be wrapped, either with `%extend` for the case of class methods or `%ignore` and `%rename` for the case of functions.

### Other Common Types: bool

Note that C++ type `bool` is not supported in the list in the [Available Typemaps](#available-typemaps) section. NumPy bools are a single byte, while the C++ `bool` is four bytes (at least on my system). Therefore:

    %numpy_typemaps(bool, NPY_BOOL, int)

will result in typemaps that will produce code that reference improper data lengths. You can implement the following macro expansion:

    %numpy_typemaps(bool, NPY_UINT, int)

to fix the data length problem, and [Input Arrays](#input-arrays) will work fine, but [In-Place Arrays](#in-place-arrays) might fail type-checking.

### Other Common Types: complex

Typemap conversions for complex floating-point types is also not supported automatically. This is because Python and NumPy are written in C, which does not have native complex types. Both Python and NumPy implement their own (essentially equivalent) `struct` definitions for complex variables:

    /* Python */
    typedef struct {double real; double imag;} Py_complex;
    
    /* NumPy */
    typedef struct {float  real, imag;} npy_cfloat;
    typedef struct {double real, imag;} npy_cdouble;

We could have implemented:

    %numpy_typemaps(Py_complex , NPY_CDOUBLE, int)
    %numpy_typemaps(npy_cfloat , NPY_CFLOAT , int)
    %numpy_typemaps(npy_cdouble, NPY_CDOUBLE, int)

which would have provided automatic type conversions for arrays of type `Py_complex`, `npy_cfloat` and `npy_cdouble`. However, it seemed unlikely that there would be any independent (non-Python, non-NumPy) application code that people would be using [SWIG]() to generate a Python interface to, that also used these definitions for complex types. More likely, these application codes will define their own complex types, or in the case of C++, use `std::complex`. Assuming these data structures are compatible with Python and NumPy complex types, `%numpy_typemap` expansions as above (with the user's complex type substituted for the first argument) should work.

## NumPy array scalars and SWIG

[SWIG]() has sophisticated type checking for numerical types. For example, if your C/C++ routine expects an integer as input, the code generated by [SWIG]() will check for both Python integers and Python long integers, and raise an overflow error if the provided Python integer is too big to cast down to a C integer. With the introduction of NumPy scalar arrays into your Python code, you might conceivably extract an integer from a NumPy array and attempt to pass this to a [SWIG]()-wrapped C/C++ function that expects an `int`, but the [SWIG]() type checking will not recognize the NumPy array scalar as an integer. (Often, this does in fact work -- it depends on whether NumPy recognizes the integer type you are using as inheriting from the Python integer type on the platform you are using. Sometimes, this means that code that works on a 32-bit machine will fail on a 64-bit machine.)

If you get a Python error that looks like the following:

    TypeError: in method 'MyClass_MyMethod', argument 2 of type 'int'

and the argument you are passing is an integer extracted from a NumPy array, then you have stumbled upon this problem. The solution is to modify the [SWIG]() type conversion system to accept NumPy array scalars in addition to the standard integer types. Fortunately, this capability has been provided for you. Simply copy the file:

    pyfragments.swg

to the working build directory for you project, and this problem will be fixed. It is suggested that you do this anyway, as it only increases the capabilities of your Python interface.

### Why is There a Second File?

The [SWIG]() type checking and conversion system is a complicated combination of C macros, [SWIG]() macros, [SWIG]() typemaps and [SWIG]() fragments. Fragments are a way to conditionally insert code into your wrapper file if it is needed, and not insert it if not needed. If multiple typemaps require the same fragment, the fragment only gets inserted into your wrapper code once.

There is a fragment for converting a Python integer to a C `long`. There is a different fragment that converts a Python integer to a C `int`, that calls the routine defined in the `long` fragment. We can make the changes we want here by changing the definition for the `long` fragment. [SWIG]() determines the active definition for a fragment using a "first come, first served" system. That is, we need to define the fragment for `long` conversions prior to [SWIG]() doing it internally. [SWIG]() allows us to do this by putting our fragment definitions in the file `pyfragments.swg`. If we were to put the new fragment definitions in `numpy.i`, they would be ignored.

## Helper functions

The `numpy.i` file contains several macros and routines that it uses internally to build its typemaps. However, these functions may be useful elsewhere in your interface file. These macros and routines are implemented as fragments, which are described briefly in the previous section. If you try to use one or more of the following macros or functions, but your compiler complains that it does not recognize the symbol, then you need to force these fragments to appear in your code using:

    %fragment("NumPy_Fragments");

in your [SWIG]() interface file.

### Macros

  - **is\_array(a)**  
    Evaluates as true if `a` is non-`NULL` and can be cast to a `PyArrayObject*`.

  - **array\_type(a)**  
    Evaluates to the integer data type code of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_numdims(a)**  
    Evaluates to the integer number of dimensions of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_dimensions(a)**  
    Evaluates to an array of type `npy_intp` and length `array_numdims(a)`, giving the lengths of all of the dimensions of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_size(a,i)**  
    Evaluates to the `i`-th dimension size of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_strides(a)**  
    Evaluates to an array of type `npy_intp` and length `array_numdims(a)`, giving the stridess of all of the dimensions of `a`, assuming `a` can be cast to a `PyArrayObject*`. A stride is the distance in bytes between an element and its immediate neighbor along the same axis.

  - **array\_stride(a,i)**  
    Evaluates to the `i`-th stride of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_data(a)**  
    Evaluates to a pointer of type `void*` that points to the data buffer of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_descr(a)**  
    Returns a borrowed reference to the dtype property (`PyArray_Descr*`) of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_flags(a)**  
    Returns an integer representing the flags of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_enableflags(a,f)**  
    Sets the flag represented by `f` of `a`, assuming `a` can be cast to a `PyArrayObject*`.

  - **array\_is\_contiguous(a)**  
    Evaluates as true if `a` is a contiguous array. Equivalent to `(PyArray_ISCONTIGUOUS(a))`.

  - **array\_is\_native(a)**  
    Evaluates as true if the data buffer of `a` uses native byte order. Equivalent to `(PyArray_ISNOTSWAPPED(a))`.

  - **array\_is\_fortran(a)**  
    Evaluates as true if `a` is FORTRAN ordered.

### Routines

  - **pytype\_string()**  
    Return type: `const char*`
    
    Arguments:
    
      - `PyObject* py_obj`, a general Python object.
    
    Return a string describing the type of `py_obj`.

  - **typecode\_string()**  
    Return type: `const char*`
    
    Arguments:
    
      - `int typecode`, a NumPy integer typecode.
    
    Return a string describing the type corresponding to the NumPy `typecode`.

  - **type\_match()**  
    Return type: `int`
    
    Arguments:
    
      - `int actual_type`, the NumPy typecode of a NumPy array.
      - `int desired_type`, the desired NumPy typecode.
    
    Make sure that `actual_type` is compatible with `desired_type`. For example, this allows character and byte types, or int and long types, to match. This is now equivalent to `PyArray_EquivTypenums()`.

  - **obj\_to\_array\_no\_conversion()**  
    Return type: `PyArrayObject*`
    
    Arguments:
    
      - `PyObject* input`, a general Python object.
      - `int typecode`, the desired NumPy typecode.
    
    Cast `input` to a `PyArrayObject*` if legal, and ensure that it is of type `typecode`. If `input` cannot be cast, or the `typecode` is wrong, set a Python error and return `NULL`.

  - **obj\_to\_array\_allow\_conversion()**  
    Return type: `PyArrayObject*`
    
    Arguments:
    
      - `PyObject* input`, a general Python object.
      - `int typecode`, the desired NumPy typecode of the resulting array.
      - `int* is_new_object`, returns a value of 0 if no conversion performed, else 1.
    
    Convert `input` to a NumPy array with the given `typecode`. On success, return a valid `PyArrayObject*` with the correct type. On failure, the Python error string will be set and the routine returns `NULL`.

  - **make\_contiguous()**  
    Return type: `PyArrayObject*`
    
    Arguments:
    
      - `PyArrayObject* ary`, a NumPy array.
      - `int* is_new_object`, returns a value of 0 if no conversion performed, else 1.
      - `int min_dims`, minimum allowable dimensions.
      - `int max_dims`, maximum allowable dimensions.
    
    Check to see if `ary` is contiguous. If so, return the input pointer and flag it as not a new object. If it is not contiguous, create a new `PyArrayObject*` using the original data, flag it as a new object and return the pointer.

  - **make\_fortran()**  
    Return type: `PyArrayObject*`
    
    Arguments
    
      - `PyArrayObject* ary`, a NumPy array.
      - `int* is_new_object`, returns a value of 0 if no conversion performed, else 1.
    
    Check to see if `ary` is Fortran contiguous. If so, return the input pointer and flag it as not a new object. If it is not Fortran contiguous, create a new `PyArrayObject*` using the original data, flag it as a new object and return the pointer.

  - **obj\_to\_array\_contiguous\_allow\_conversion()**  
    Return type: `PyArrayObject*`
    
    Arguments:
    
      - `PyObject* input`, a general Python object.
      - `int typecode`, the desired NumPy typecode of the resulting array.
      - `int* is_new_object`, returns a value of 0 if no conversion performed, else 1.
    
    Convert `input` to a contiguous `PyArrayObject*` of the specified type. If the input object is not a contiguous `PyArrayObject*`, a new one will be created and the new object flag will be set.

  - **obj\_to\_array\_fortran\_allow\_conversion()**  
    Return type: `PyArrayObject*`
    
    Arguments:
    
      - `PyObject* input`, a general Python object.
      - `int typecode`, the desired NumPy typecode of the resulting array.
      - `int* is_new_object`, returns a value of 0 if no conversion performed, else 1.
    
    Convert `input` to a Fortran contiguous `PyArrayObject*` of the specified type. If the input object is not a Fortran contiguous `PyArrayObject*`, a new one will be created and the new object flag will be set.

  - **require\_contiguous()**  
    Return type: `int`
    
    Arguments:
    
      - `PyArrayObject* ary`, a NumPy array.
    
    Test whether `ary` is contiguous. If so, return 1. Otherwise, set a Python error and return 0.

  - **require\_native()**  
    Return type: `int`
    
    Arguments:
    
      - `PyArray_Object* ary`, a NumPy array.
    
    Require that `ary` is not byte-swapped. If the array is not byte-swapped, return 1. Otherwise, set a Python error and return 0.

  - **require\_dimensions()**  
    Return type: `int`
    
    Arguments:
    
      - `PyArrayObject* ary`, a NumPy array.
      - `int exact_dimensions`, the desired number of dimensions.
    
    Require `ary` to have a specified number of dimensions. If the array has the specified number of dimensions, return 1. Otherwise, set a Python error and return 0.

  - **require\_dimensions\_n()**  
    Return type: `int`
    
    Arguments:
    
      - `PyArrayObject* ary`, a NumPy array.
      - `int* exact_dimensions`, an array of integers representing acceptable numbers of dimensions.
      - `int n`, the length of `exact_dimensions`.
    
    Require `ary` to have one of a list of specified number of dimensions. If the array has one of the specified number of dimensions, return 1. Otherwise, set the Python error string and return 0.

  - **require\_size()**  
    Return type: `int`
    
    Arguments:
    
      - `PyArrayObject* ary`, a NumPy array.
      - `npy_int* size`, an array representing the desired lengths of each dimension.
      - `int n`, the length of `size`.
    
    Require `ary` to have a specified shape. If the array has the specified shape, return 1. Otherwise, set the Python error string and return 0.

  - **require\_fortran()**  
    Return type: `int`
    
    Arguments:
    
      - `PyArrayObject* ary`, a NumPy array.
    
    Require the given `PyArrayObject` to be Fortran ordered. If the `PyArrayObject` is already Fortran ordered, do nothing. Else, set the Fortran ordering flag and recompute the strides.

## Beyond the provided typemaps

There are many C or C++ array/NumPy array situations not covered by a simple `%include "numpy.i"` and subsequent `%apply` directives.

### A Common Example

Consider a reasonable prototype for a dot product function:

    double dot(int len, double* vec1, double* vec2);

The Python interface that we want is:

    def dot(vec1, vec2):
        """
        dot(PyObject,PyObject) -> double
        """

The problem here is that there is one dimension argument and two array arguments, and our typemaps are set up for dimensions that apply to a single array (in fact, [SWIG]() does not provide a mechanism for associating `len` with `vec2` that takes two Python input arguments). The recommended solution is the following:

    %apply (int DIM1, double* IN_ARRAY1) {(int len1, double* vec1),
                                          (int len2, double* vec2)}
    %rename (dot) my_dot;
    %exception my_dot {
        $action
    if (PyErr_Occurred()) SWIG_fail;
    }
    %inline %{
    double my_dot(int len1, double* vec1, int len2, double* vec2) {
        if (len1 != len2) {
        PyErr_Format(PyExc_ValueError,
                         "Arrays of lengths (%d,%d) given",
                         len1, len2);
        return 0.0;
        }
        return dot(len1, vec1, vec2);
    }
    %}

If the header file that contains the prototype for `double dot()` also contains other prototypes that you want to wrap, so that you need to `%include` this header file, then you will also need a `%ignore dot;` directive, placed after the `%rename` and before the `%include` directives. Or, if the function in question is a class method, you will want to use `%extend` rather than `%inline` in addition to `%ignore`.

**A note on error handling:** Note that `my_dot` returns a `double` but that it can also raise a Python error. The resulting wrapper function will return a Python float representation of 0.0 when the vector lengths do not match. Since this is not `NULL`, the Python interpreter will not know to check for an error. For this reason, we add the `%exception` directive above for `my_dot` to get the behavior we want (note that `$action` is a macro that gets expanded to a valid call to `my_dot`). In general, you will probably want to write a [SWIG]() macro to perform this task.

### Other Situations

There are other wrapping situations in which `numpy.i` may be helpful when you encounter them.

  - In some situations, it is possible that you could use the `%numpy_typemaps` macro to implement typemaps for your own types. See the [Other Common Types: bool](#other-common-types-bool) or [Other Common Types: complex](#other-common-types-complex) sections for examples. Another situation is if your dimensions are of a type other than `int` (say `long` for example):
    
        %numpy_typemaps(double, NPY_DOUBLE, long)

  - You can use the code in `numpy.i` to write your own typemaps. For example, if you had a five-dimensional array as a function argument, you could cut-and-paste the appropriate four-dimensional typemaps into your interface file. The modifications for the fourth dimension would be trivial.

  - Sometimes, the best approach is to use the `%extend` directive to define new methods for your classes (or overload existing ones) that take a `PyObject*` (that either is or can be converted to a `PyArrayObject*`) instead of a pointer to a buffer. In this case, the helper routines in `numpy.i` can be very useful.

  - Writing typemaps can be a bit nonintuitive. If you have specific questions about writing [SWIG]() typemaps for NumPy, the developers of `numpy.i` do monitor the [Numpy-discussion](mailto:Numpy-discussion@python.org) and [Swig-user](mailto:Swig-user@lists.sourceforge.net) mail lists.

### A Final Note

When you use the `%apply` directive, as is usually necessary to use `numpy.i`, it will remain in effect until you tell [SWIG]() that it shouldn't be. If the arguments to the functions or methods that you are wrapping have common names, such as `length` or `vector`, these typemaps may get applied in situations you do not expect or want. Therefore, it is always a good idea to add a `%clear` directive after you are done with a specific typemap:

    %apply (double* IN_ARRAY1, int DIM1) {(double* vector, int length)}
    %include "my_header.h"
    %clear (double* vector, int length);

In general, you should target these typemap signatures specifically where you want them, and then clear them after you are done.

## Summary

Out of the box, `numpy.i` provides typemaps that support conversion between NumPy arrays and C arrays:

  - That can be one of 12 different scalar types: `signed char`, `unsigned char`, `short`, `unsigned short`, `int`, `unsigned int`, `long`, `unsigned long`, `long long`, `unsigned long long`, `float` and `double`.
  - That support 74 different argument signatures for each data type, including:
      - One-dimensional, two-dimensional, three-dimensional and four-dimensional arrays.
      - Input-only, in-place, argout, argoutview, and memory managed argoutview behavior.
      - Hard-coded dimensions, data-buffer-then-dimensions specification, and dimensions-then-data-buffer specification.
      - Both C-ordering ("last dimension fastest") or Fortran-ordering ("first dimension fastest") support for 2D, 3D and 4D arrays.

The `numpy.i` interface file also provides additional tools for wrapper developers, including:

  - A [SWIG]() macro (`%numpy_typemaps`) with three arguments for implementing the 74 argument signatures for the user's choice of
    1)  C data type, (2) NumPy data type (assuming they match), and
    2)  dimension type.
  - Fourteen C macros and fifteen C functions that can be used to write specialized typemaps, extensions, or inlined functions that handle cases not covered by the provided typemaps. Note that the macros and functions are coded specifically to work with the NumPy C/API regardless of NumPy version number, both before and after the deprecation of some aspects of the API after version 1.6.

---

swig.md

---

# NumPy and SWIG

<div class="sectionauthor">

Bill Spotz

</div>

<div class="toctree" data-maxdepth="2">

swig.interface-file swig.testing

</div>

---

swig.testing.md

---

# Testing the numpy.i typemaps

## Introduction

Writing tests for the `numpy.i` [SWIG](https://www.swig.org/) interface file is a combinatorial headache. At present, 12 different data types are supported, each with 74 different argument signatures, for a total of 888 typemaps supported "out of the box". Each of these typemaps, in turn, might require several unit tests in order to verify expected behavior for both proper and improper inputs. Currently, this results in more than 1,000 individual unit tests executed when `make test` is run in the `numpy/tools/swig` subdirectory.

To facilitate this many similar unit tests, some high-level programming techniques are employed, including C and [SWIG]() macros, as well as Python inheritance. The purpose of this document is to describe the testing infrastructure employed to verify that the `numpy.i` typemaps are working as expected.

## Testing organization

There are three independent testing frameworks supported, for one-, two-, and three-dimensional arrays respectively. For one-dimensional arrays, there are two C++ files, a header and a source, named:

    Vector.h
    Vector.cxx

that contain prototypes and code for a variety of functions that have one-dimensional arrays as function arguments. The file:

    Vector.i

is a [SWIG]() interface file that defines a python module `Vector` that wraps the functions in `Vector.h` while utilizing the typemaps in `numpy.i` to correctly handle the C arrays.

The `Makefile` calls `swig` to generate `Vector.py` and `Vector_wrap.cxx`, and also executes the `setup.py` script that compiles `Vector_wrap.cxx` and links together the extension module `_Vector.so` or `_Vector.dylib`, depending on the platform. This extension module and the proxy file `Vector.py` are both placed in a subdirectory under the `build` directory.

The actual testing takes place with a Python script named:

    testVector.py

that uses the standard Python library module `unittest`, which performs several tests of each function defined in `Vector.h` for each data type supported.

Two-dimensional arrays are tested in exactly the same manner. The above description applies, but with `Matrix` substituted for `Vector`. For three-dimensional tests, substitute `Tensor` for `Vector`. For four-dimensional tests, substitute `SuperTensor` for `Vector`. For flat in-place array tests, substitute `Flat` for `Vector`. For the descriptions that follow, we will reference the `Vector` tests, but the same information applies to `Matrix`, `Tensor` and `SuperTensor` tests.

The command `make test` will ensure that all of the test software is built and then run all three test scripts.

## Testing header files

`Vector.h` is a C++ header file that defines a C macro called `TEST_FUNC_PROTOS` that takes two arguments: `TYPE`, which is a data type name such as `unsigned int`; and `SNAME`, which is a short name for the same data type with no spaces, e.g. `uint`. This macro defines several function prototypes that have the prefix `SNAME` and have at least one argument that is an array of type `TYPE`. Those functions that have return arguments return a `TYPE` value.

`TEST_FUNC_PROTOS` is then implemented for all of the data types supported by `numpy.i`:

  - `signed char`
  - `unsigned char`
  - `short`
  - `unsigned short`
  - `int`
  - `unsigned int`
  - `long`
  - `unsigned long`
  - `long long`
  - `unsigned long long`
  - `float`
  - `double`

## Testing source files

`Vector.cxx` is a C++ source file that implements compilable code for each of the function prototypes specified in `Vector.h`. It defines a C macro `TEST_FUNCS` that has the same arguments and works in the same way as `TEST_FUNC_PROTOS` does in `Vector.h`. `TEST_FUNCS` is implemented for each of the 12 data types as above.

## Testing SWIG interface files

`Vector.i` is a [SWIG]() interface file that defines python module `Vector`. It follows the conventions for using `numpy.i` as described in this chapter. It defines a [SWIG]() macro `%apply_numpy_typemaps` that has a single argument `TYPE`. It uses the [SWIG]() directive `%apply` to apply the provided typemaps to the argument signatures found in `Vector.h`. This macro is then implemented for all of the data types supported by `numpy.i`. It then does a `%include "Vector.h"` to wrap all of the function prototypes in `Vector.h` using the typemaps in `numpy.i`.

## Testing Python scripts

After `make` is used to build the testing extension modules, `testVector.py` can be run to execute the tests. As with other scripts that use `unittest` to facilitate unit testing, `testVector.py` defines a class that inherits from `unittest.TestCase`:

    class VectorTestCase(unittest.TestCase):

However, this class is not run directly. Rather, it serves as a base class to several other python classes, each one specific to a particular data type. The `VectorTestCase` class stores two strings for typing information:

  - **self.typeStr**  
    A string that matches one of the `SNAME` prefixes used in `Vector.h` and `Vector.cxx`. For example, `"double"`.

  - **self.typeCode**  
    A short (typically single-character) string that represents a data type in numpy and corresponds to `self.typeStr`. For example, if `self.typeStr` is `"double"`, then `self.typeCode` should be `"d"`.

Each test defined by the `VectorTestCase` class extracts the python function it is trying to test by accessing the `Vector` module's dictionary:

    length = Vector.__dict__[self.typeStr + "Length"]

In the case of double precision tests, this will return the python function `Vector.doubleLength`.

We then define a new test case class for each supported data type with a short definition such as:

    class doubleTestCase(VectorTestCase):
        def __init__(self, methodName="runTest"):
            VectorTestCase.__init__(self, methodName)
            self.typeStr  = "double"
            self.typeCode = "d"

Each of these 12 classes is collected into a `unittest.TestSuite`, which is then executed. Errors and failures are summed together and returned as the exit argument. Any non-zero result indicates that at least one test did not pass.

---

testing.md

---

# Testing guidelines

# NumPy/SciPy testing guidelines

<div class="contents">

</div>

## Introduction

Until the 1.15 release, NumPy used the [nose](https://nose.readthedocs.io/en/latest/) testing framework, it now uses the [pytest](https://pytest.readthedocs.io) framework. The older framework is still maintained in order to support downstream projects that use the old numpy framework, but all tests for NumPy should use pytest.

Our goal is that every module and package in NumPy should have a thorough set of unit tests. These tests should exercise the full functionality of a given routine as well as its robustness to erroneous or unexpected input arguments. Well-designed tests with good coverage make an enormous difference to the ease of refactoring. Whenever a new bug is found in a routine, you should write a new test for that specific case and add it to the test suite to prevent that bug from creeping back in unnoticed.

\> **Note** \> SciPy uses the testing framework from `numpy.testing`, so all of the NumPy examples shown below are also applicable to SciPy

## Testing NumPy

NumPy can be tested in a number of ways, choose any way you feel comfortable.

### Running tests from inside Python

You can test an installed NumPy by <span class="title-ref">numpy.test</span>, for example, To run NumPy's full test suite, use the following:

    >>> import numpy
    >>> numpy.test(label='slow')

The test method may take two or more arguments; the first `label` is a string specifying what should be tested and the second `verbose` is an integer giving the level of output verbosity. See the docstring <span class="title-ref">numpy.test</span> for details. The default value for `label` is 'fast' - which will run the standard tests. The string 'full' will run the full battery of tests, including those identified as being slow to run. If `verbose` is 1 or less, the tests will just show information messages about the tests that are run; but if it is greater than 1, then the tests will also provide warnings on missing tests. So if you want to run every test and get messages about which modules don't have tests:

    >>> numpy.test(label='full', verbose=2)  # or numpy.test('full', 2)

Finally, if you are only interested in testing a subset of NumPy, for example, the `_core` module, use the following:

    >>> numpy._core.test()

### Running tests from the command line

If you want to build NumPy in order to work on NumPy itself, use the `spin` utility. To run NumPy's full test suite:

    $ spin test -m full

Testing a subset of NumPy:

    $ spin test -t numpy/_core/tests

For detailed info on testing, see \[testing-builds\](\#testing-builds)

### Running doctests

NumPy documentation contains code examples, "doctests". To check that the examples are correct, install the `scipy-doctest` package:

    $ pip install scipy-doctest

and run one of:

    $ spin check-docs -v
    $ spin check-docs numpy/linalg
    $ spin check-docs -- -k 'det and not slogdet'

Note that the doctests are not run when you use `spin test`.

### Other methods of running tests

Run tests using your favourite IDE such as [vscode](https://code.visualstudio.com/docs/python/testing#_enable-a-test-framework) or [pycharm](https://www.jetbrains.com/help/pycharm/testing-your-first-python-application.html)

## Writing your own tests

If you are writing code that you'd like to become part of NumPy, please write the tests as you develop your code. Every Python module, extension module, or subpackage in the NumPy package directory should have a corresponding `test_<name>.py` file. Pytest examines these files for test methods (named `test*`) and test classes (named `Test*`).

Suppose you have a NumPy module `numpy/xxx/yyy.py` containing a function `zzz()`. To test this function you would create a test module called `test_yyy.py`. If you only need to test one aspect of `zzz`, you can simply add a test function:

    def test_zzz():
        assert zzz() == 'Hello from zzz'

More often, we need to group a number of tests together, so we create a test class:

    import pytest
    
    # import xxx symbols
    from numpy.xxx.yyy import zzz
    import pytest
    
    class TestZzz:
        def test_simple(self):
            assert zzz() == 'Hello from zzz'
    
        def test_invalid_parameter(self):
            with pytest.raises(ValueError, match='.*some matching regex.*'):
                ...

Within these test methods, the `assert` statement or a specialized assertion function is used to test whether a certain assumption is valid. If the assertion fails, the test fails. Common assertion functions include:

  - <span class="title-ref">numpy.testing.assert\_equal</span> for testing exact elementwise equality between a result array and a reference,
  - <span class="title-ref">numpy.testing.assert\_allclose</span> for testing near elementwise equality between a result array and a reference (i.e. with specified relative and absolute tolerances), and
  - <span class="title-ref">numpy.testing.assert\_array\_less</span> for testing (strict) elementwise ordering between a result array and a reference.

By default, these assertion functions only compare the numerical values in the arrays. Consider using the `strict=True` option to check the array dtype and shape, too.

When you need custom assertions, use the Python `assert` statement. Note that `pytest` internally rewrites `assert` statements to give informative output when it fails, so it should be preferred over the legacy variant `numpy.testing.assert_`. Whereas plain `assert` statements are ignored when running Python in optimized mode with `-O`, this is not an issue when running tests with pytest.

Similarly, the pytest functions <span class="title-ref">pytest.raises</span> and <span class="title-ref">pytest.warns</span> should be preferred over their legacy counterparts <span class="title-ref">numpy.testing.assert\_raises</span> and <span class="title-ref">numpy.testing.assert\_warns</span>, which are more broadly used. These versions also accept a `match` parameter, which should always be used to precisely target the intended warning or error.

Note that `test_` functions or methods should not have a docstring, because that makes it hard to identify the test from the output of running the test suite with `verbose=2` (or similar verbosity setting). Use plain comments (`#`) to describe the intent of the test and help the unfamiliar reader to interpret the code.

Also, since much of NumPy is legacy code that was originally written without unit tests, there are still several modules that don't have tests yet. Please feel free to choose one of these modules and develop tests for it.

### Using C code in tests

NumPy exposes a rich \[C-API\<c-api\>\](\#c-api\<c-api\>) . These are tested using c-extension modules written "as-if" they know nothing about the internals of NumPy, rather using the official C-API interfaces only. Examples of such modules are tests for a user-defined `rational` dtype in `_rational_tests` or the ufunc machinery tests in `_umath_tests` which are part of the binary distribution. Starting from version 1.21, you can also write snippets of C code in tests that will be compiled locally into c-extension modules and loaded into python.

<div class="currentmodule">

numpy.testing.extbuild

</div>

<div class="autofunction">

build\_and\_import\_extension

</div>

### Labeling tests

Unlabeled tests like the ones above are run in the default `numpy.test()` run. If you want to label your test as slow - and therefore reserved for a full `numpy.test(label='full')` run, you can label it with `pytest.mark.slow`:

    import pytest
    
    @pytest.mark.slow
    def test_big(self):
        print('Big, slow test')

Similarly for methods:

    class test_zzz:
        @pytest.mark.slow
        def test_simple(self):
            assert_(zzz() == 'Hello from zzz')

### Easier setup and teardown functions / methods

Testing looks for module-level or class method-level setup and teardown functions by name; thus:

    def setup_module():
        """Module-level setup"""
        print('doing setup')
    
    def teardown_module():
        """Module-level teardown"""
        print('doing teardown')

    class TestMe:
        def setup_method(self):
            """Class-level setup"""
            print('doing setup')
    
        def teardown_method():
            """Class-level teardown"""
            print('doing teardown')

Setup and teardown functions to functions and methods are known as "fixtures", and they should be used sparingly. `pytest` supports more general fixture at various scopes which may be used automatically via special arguments. For example, the special argument name `tmpdir` is used in test to create a temporary directory.

### Parametric tests

One very nice feature of `pytest` is the ease of testing across a range of parameter values using the `pytest.mark.parametrize` decorator. For example, suppose you wish to test `linalg.solve` for all combinations of three array sizes and two data types:

    @pytest.mark.parametrize('dimensionality', [3, 10, 25])
    @pytest.mark.parametrize('dtype', [np.float32, np.float64])
    def test_solve(dimensionality, dtype):
        np.random.seed(842523)
        A = np.random.random(size=(dimensionality, dimensionality)).astype(dtype)
        b = np.random.random(size=dimensionality).astype(dtype)
        x = np.linalg.solve(A, b)
        eps = np.finfo(dtype).eps
        assert_allclose(A @ x, b, rtol=eps*1e2, atol=0)
        assert x.dtype == np.dtype(dtype)

### Doctests

Doctests are a convenient way of documenting the behavior of a function and allowing that behavior to be tested at the same time. The output of an interactive Python session can be included in the docstring of a function, and the test framework can run the example and compare the actual output to the expected output.

The doctests can be run by adding the `doctests` argument to the `test()` call; for example, to run all tests (including doctests) for numpy.lib:

    >>> import numpy as np
    >>> np.lib.test(doctests=True)

The doctests are run as if they are in a fresh Python instance which has executed `import numpy as np`. Tests that are part of a NumPy subpackage will have that subpackage already imported. E.g. for a test in `numpy/linalg/tests/`, the namespace will be created such that `from numpy import linalg` has already executed.

### `tests/`

Rather than keeping the code and the tests in the same directory, we put all the tests for a given subpackage in a `tests/` subdirectory. For our example, if it doesn't already exist you will need to create a `tests/` directory in `numpy/xxx/`. So the path for `test_yyy.py` is `numpy/xxx/tests/test_yyy.py`.

Once the `numpy/xxx/tests/test_yyy.py` is written, its possible to run the tests by going to the `tests/` directory and typing:

    python test_yyy.py

Or if you add `numpy/xxx/tests/` to the Python path, you could run the tests interactively in the interpreter like this:

    >>> import test_yyy
    >>> test_yyy.test()

### `__init__.py` and `setup.py`

Usually, however, adding the `tests/` directory to the python path isn't desirable. Instead it would better to invoke the test straight from the module `xxx`. To this end, simply place the following lines at the end of your package's `__init__.py` file:

    ...
    def test(level=1, verbosity=1):
        from numpy.testing import Tester
        return Tester().test(level, verbosity)

You will also need to add the tests directory in the configuration section of your setup.py:

    ...
    def configuration(parent_package='', top_path=None):
        ...
        config.add_subpackage('tests')
        return config
    ...

Now you can do the following to test your module:

    >>> import numpy
    >>> numpy.xxx.test()

Also, when invoking the entire NumPy test suite, your tests will be found and run:

    >>> import numpy
    >>> numpy.test()
    # your tests are included and run automatically!

## Tips & Tricks

### Known failures & skipping tests

Sometimes you might want to skip a test or mark it as a known failure, such as when the test suite is being written before the code it's meant to test, or if a test only fails on a particular architecture.

To skip a test, simply use `skipif`:

    import pytest
    
    @pytest.mark.skipif(SkipMyTest, reason="Skipping this test because...")
    def test_something(foo):
        ...

The test is marked as skipped if `SkipMyTest` evaluates to nonzero, and the message in verbose test output is the second argument given to `skipif`. Similarly, a test can be marked as a known failure by using `xfail`:

    import pytest
    
    @pytest.mark.xfail(MyTestFails, reason="This test is known to fail because...")
    def test_something_else(foo):
        ...

Of course, a test can be unconditionally skipped or marked as a known failure by using `skip` or `xfail` without argument, respectively.

A total of the number of skipped and known failing tests is displayed at the end of the test run. Skipped tests are marked as `'S'` in the test results (or `'SKIPPED'` for `verbose > 1`), and known failing tests are marked as `'x'` (or `'XFAIL'` if `verbose > 1`).

### Tests on random data

Tests on random data are good, but since test failures are meant to expose new bugs or regressions, a test that passes most of the time but fails occasionally with no code changes is not helpful. Make the random data deterministic by setting the random number seed before generating it. Use either Python's `random.seed(some_number)` or NumPy's `numpy.random.seed(some_number)`, depending on the source of random numbers.

Alternatively, you can use [Hypothesis](https://hypothesis.readthedocs.io/en/latest/) to generate arbitrary data. Hypothesis manages both Python's and Numpy's random seeds for you, and provides a very concise and powerful way to describe data (including `hypothesis.extra.numpy`, e.g. for a set of mutually-broadcastable shapes).

The advantages over random generation include tools to replay and share failures without requiring a fixed seed, reporting *minimal* examples for each failure, and better-than-naive-random techniques for triggering bugs.

### Documentation for `numpy.test`

<div class="autofunction">

numpy.test

  - start-line  
    6

</div>

---

thread_safety.md

---

# Thread Safety

NumPy supports use in a multithreaded context via the <span class="title-ref">threading</span> module in the standard library. Many NumPy operations release the GIL, so unlike many situations in Python, it is possible to improve parallel performance by exploiting multithreaded parallelism in Python.

The easiest performance gains happen when each worker thread owns its own array or set of array objects, with no data directly shared between threads. Because NumPy releases the GIL for many low-level operations, threads that spend most of the time in low-level code will run in parallel.

It is possible to share NumPy arrays between threads, but extreme care must be taken to avoid creating thread safety issues when mutating arrays that are shared between multiple threads. If two threads simultaneously read from and write to the same array, they will at best produce inconsistent, racey results that are not reproducible, let alone correct. It is also possible to crash the Python interpreter by, for example, resizing an array while another thread is reading from it to compute a ufunc operation.

In the future, we may add locking to ndarray to make writing multithreaded algorithms using NumPy arrays safer, but for now we suggest focusing on read-only access of arrays that are shared between threads, or adding your own locking if you need to mutation and multithreading.

Note that operations that *do not* release the GIL will see no performance gains from use of the <span class="title-ref">threading</span> module, and instead might be better served with <span class="title-ref">multiprocessing</span>. In particular, operations on arrays with `dtype=object` do not release the GIL.

## Free-threaded Python

<div class="versionadded">

2.1

</div>

Starting with NumPy 2.1 and CPython 3.13, NumPy also has experimental support for python runtimes with the GIL disabled. See <https://py-free-threading.github.io> for more information about installing and using free-threaded Python, as well as information about supporting it in libraries that depend on NumPy.

Because free-threaded Python does not have a global interpreter lock to serialize access to Python objects, there are more opportunities for threads to mutate shared state and create thread safety issues. In addition to the limitations about locking of the ndarray object noted above, this also means that arrays with `dtype=object` are not protected by the GIL, creating data races for python objects that are not possible outside free-threaded python.

---

ufuncs.md

---

<div class="sectionauthor">

adapted from "Guide to NumPy" by Travis E. Oliphant

</div>

<div class="currentmodule">

numpy

</div>

# Universal functions (<span class="title-ref">ufunc</span>)

<div class="seealso">

\[ufuncs-basics\](\#ufuncs-basics)

</div>

A universal function (or `ufunc` for short) is a function that operates on <span class="title-ref">ndarrays \<numpy.ndarray\></span> in an element-by-element fashion, supporting \[array broadcasting \<ufuncs.broadcasting\>\](\#array-broadcasting-\<ufuncs.broadcasting\>), \[type casting \<ufuncs.casting\>\](\#type casting-\<ufuncs.casting\>), and several other standard features. That is, a ufunc is a "`vectorized <vectorization>`" wrapper for a function that takes a fixed number of specific inputs and produces a fixed number of specific outputs. For detailed information on universal functions, see \[ufuncs-basics\](\#ufuncs-basics).

## <span class="title-ref">ufunc</span>

<div class="autosummary" data-toctree="generated/">

numpy.ufunc

</div>

### Optional keyword arguments

All ufuncs take optional keyword arguments. Most of these represent advanced usage and will not typically be used.

<div class="index">

pair: ufunc; keyword arguments

</div>

***out***

The first output can be provided as either a positional or a keyword parameter. Keyword 'out' arguments are incompatible with positional ones.

The 'out' keyword argument is expected to be a tuple with one entry per output (which can be None for arrays to be allocated by the ufunc). For ufuncs with a single output, passing a single array (instead of a tuple holding a single array) is also valid.

Passing a single array in the 'out' keyword argument to a ufunc with multiple outputs is deprecated, and will raise a warning in numpy 1.10, and an error in a future release.

If 'out' is None (the default), a uninitialized return array is created. The output array is then filled with the results of the ufunc in the places that the broadcast 'where' is True. If 'where' is the scalar True (the default), then this corresponds to the entire output being filled. Note that outputs not explicitly filled are left with their uninitialized values.

Operations where ufunc input and output operands have memory overlap are defined to be the same as for equivalent operations where there is no memory overlap. Operations affected make temporary copies as needed to eliminate data dependency. As detecting these cases is computationally expensive, a heuristic is used, which may in rare cases result in needless temporary copies. For operations where the data dependency is simple enough for the heuristic to analyze, temporary copies will not be made even if the arrays overlap, if it can be deduced copies are not necessary. As an example, `np.add(a, b, out=a)` will not involve copies.

***where***

Accepts a boolean array which is broadcast together with the operands. Values of True indicate to calculate the ufunc at that position, values of False indicate to leave the value in the output alone. This argument cannot be used for generalized ufuncs as those take non-scalar input.

Note that if an uninitialized return array is created, values of False will leave those values **uninitialized**.

***axes***

A list of tuples with indices of axes a generalized ufunc should operate on. For instance, for a signature of `(i,j),(j,k)->(i,k)` appropriate for matrix multiplication, the base elements are two-dimensional matrices and these are taken to be stored in the two last axes of each argument. The corresponding axes keyword would be `[(-2, -1), (-2, -1), (-2, -1)]`. For simplicity, for generalized ufuncs that operate on 1-dimensional arrays (vectors), a single integer is accepted instead of a single-element tuple, and for generalized ufuncs for which all outputs are scalars, the output tuples can be omitted.

***axis***

A single axis over which a generalized ufunc should operate. This is a short-cut for ufuncs that operate over a single, shared core dimension, equivalent to passing in `axes` with entries of `(axis,)` for each single-core-dimension argument and `()` for all others. For instance, for a signature `(i),(i)->()`, it is equivalent to passing in `axes=[(axis,), (axis,), ()]`.

***keepdims***

If this is set to <span class="title-ref">True</span>, axes which are reduced over will be left in the result as a dimension with size one, so that the result will broadcast correctly against the inputs. This option can only be used for generalized ufuncs that operate on inputs that all have the same number of core dimensions and with outputs that have no core dimensions, i.e., with signatures like `(i),(i)->()` or `(m,m)->()`. If used, the location of the dimensions in the output can be controlled with `axes` and `axis`.

***casting***

May be 'no', 'equiv', 'safe', 'same\_kind', or 'unsafe'. See <span class="title-ref">can\_cast</span> for explanations of the parameter values.

Provides a policy for what kind of casting is permitted. For compatibility with previous versions of NumPy, this defaults to 'unsafe' for numpy \< 1.7. In numpy 1.7 a transition to 'same\_kind' was begun where ufuncs produce a DeprecationWarning for calls which are allowed under the 'unsafe' rules, but not under the 'same\_kind' rules. From numpy 1.10 and onwards, the default is 'same\_kind'.

***order***

Specifies the calculation iteration order/memory layout of the output array. Defaults to 'K'. 'C' means the output should be C-contiguous, 'F' means F-contiguous, 'A' means F-contiguous if the inputs are F-contiguous and not also not C-contiguous, C-contiguous otherwise, and 'K' means to match the element ordering of the inputs as closely as possible.

***dtype***

Overrides the DType of the output arrays the same way as the *signature*. This should ensure a matching precision of the calculation. The exact calculation DTypes chosen may depend on the ufunc and the inputs may be cast to this DType to perform the calculation.

***subok***

Defaults to true. If set to false, the output will always be a strict array, not a subtype.

***signature***

Either a Dtype, a tuple of DTypes, or a special signature string indicating the input and output types of a ufunc.

This argument allows the user to specify exact DTypes to be used for the calculation. Casting will be used as necessary. The actual DType of the input arrays is not considered unless `signature` is `None` for that array.

When all DTypes are fixed, a specific loop is chosen or an error raised if no matching loop exists. If some DTypes are not specified and left `None`, the behaviour may depend on the ufunc. At this time, a list of available signatures is provided by the **types** attribute of the ufunc. (This list may be missing DTypes not defined by NumPy.)

The `signature` only specifies the DType class/type. For example, it can specify that the operation should be `datetime64` or `float64` operation. It does not specify the `datetime64` time-unit or the `float64` byte-order.

For backwards compatibility this argument can also be provided as *sig*, although the long form is preferred. Note that this should not be confused with the generalized ufunc \[signature \<details-of-signature\>\](\#signature-\<details-of-signature\>) that is stored in the **signature** attribute of the of the ufunc object.

### Attributes

There are some informational attributes that universal functions possess. None of the attributes can be set.

<div class="index">

pair: ufunc; attributes

</div>

|                  |                                                                                                                                                                                                                                               |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **\_\_doc\_\_**  | A docstring for each ufunc. The first part of the docstring is dynamically generated from the number of outputs, the name, and the number of inputs. The second part of the docstring is provided at creation time and stored with the ufunc. |
| **\_\_name\_\_** | The name of the ufunc.                                                                                                                                                                                                                        |

<div class="autosummary" data-toctree="generated/">

ufunc.nin ufunc.nout ufunc.nargs ufunc.ntypes ufunc.types ufunc.identity ufunc.signature

</div>

### Methods

<div class="index">

pair: ufunc; methods

</div>

<div class="autosummary" data-toctree="generated/">

ufunc.reduce ufunc.accumulate ufunc.reduceat ufunc.outer ufunc.at

</div>

\> **Warning** \> A reduce-like operation on an array with a data-type that has a range "too small" to handle the result will silently wrap. One should use <span class="title-ref">dtype</span> to increase the size of the data-type over which reduction takes place.

## Available ufuncs

There are currently more than 60 universal functions defined in `numpy` on one or more types, covering a wide variety of operations. Some of these ufuncs are called automatically on arrays when the relevant infix notation is used (*e.g.*, <span class="title-ref">add(a, b) \<add\></span> is called internally when `a + b` is written and *a* or *b* is an <span class="title-ref">ndarray</span>). Nevertheless, you may still want to use the ufunc call in order to use the optional output argument(s) to place the output(s) in an object (or objects) of your choice.

Recall that each ufunc operates element-by-element. Therefore, each scalar ufunc will be described as if acting on a set of scalar inputs to return a set of scalar outputs.

\> **Note** \> The ufunc still returns its output(s) even if you use the optional output argument(s).

### Math operations

<div class="autosummary">

add subtract multiply matmul divide logaddexp logaddexp2 true\_divide floor\_divide negative positive power float\_power remainder mod fmod divmod absolute fabs rint sign heaviside conj conjugate exp exp2 log log2 log10 expm1 log1p sqrt square cbrt reciprocal gcd lcm

</div>

\> **Tip** \> The optional output arguments can be used to help you save memory for large calculations. If your arrays are large, complicated expressions can take longer than absolutely necessary due to the creation and (later) destruction of temporary calculation spaces. For example, the expression `G = A * B + C` is equivalent to `T1 = A * B; G = T1 + C; del T1`. It will be more quickly executed as `G = A * B; add(G, C, G)` which is the same as `G = A * B; G += C`.

### Trigonometric functions

All trigonometric functions use radians when an angle is called for. The ratio of degrees to radians is \(180^{\circ}/\pi.\)

<div class="autosummary">

sin cos tan arcsin arccos arctan arctan2 hypot sinh cosh tanh arcsinh arccosh arctanh degrees radians deg2rad rad2deg

</div>

### Bit-twiddling functions

These function all require integer arguments and they manipulate the bit-pattern of those arguments.

<div class="autosummary">

bitwise\_and bitwise\_or bitwise\_xor invert left\_shift right\_shift

</div>

### Comparison functions

<div class="autosummary">

greater greater\_equal less less\_equal not\_equal equal

</div>

\> **Warning** \> Do not use the Python keywords `and` and `or` to combine logical array expressions. These keywords will test the truth value of the entire array (not element-by-element as you might expect). Use the bitwise operators & and | instead.

<div class="autosummary">

logical\_and logical\_or logical\_xor logical\_not

</div>

<div class="warning">

<div class="title">

Warning

</div>

The bit-wise operators & and | are the proper way to perform element-by-element array comparisons. Be sure you understand the operator precedence: `(a > 2) & (a < 5)` is the proper syntax because `a > 2 & a < 5` will result in an error due to the fact that `2 & a` is evaluated first.

</div>

<div class="autosummary">

maximum

</div>

\> **Tip** \> The Python function `max()` will find the maximum over a one-dimensional array, but it will do so using a slower sequence interface. The reduce method of the maximum ufunc is much faster. Also, the `max()` method will not give answers you might expect for arrays with greater than one dimension. The reduce method of minimum also allows you to compute a total minimum over an array.

<div class="autosummary">

minimum

</div>

<div class="warning">

<div class="title">

Warning

</div>

the behavior of `maximum(a, b)` is different than that of `max(a, b)`. As a ufunc, `maximum(a, b)` performs an element-by-element comparison of <span class="title-ref">a</span> and <span class="title-ref">b</span> and chooses each element of the result according to which element in the two arrays is larger. In contrast, `max(a, b)` treats the objects <span class="title-ref">a</span> and <span class="title-ref">b</span> as a whole, looks at the (total) truth value of `a > b` and uses it to return either <span class="title-ref">a</span> or <span class="title-ref">b</span> (as a whole). A similar difference exists between `minimum(a, b)` and `min(a, b)`.

</div>

<div class="autosummary">

fmax fmin

</div>

### Floating functions

Recall that all of these functions work element-by-element over an array, returning an array output. The description details only a single operation.

<div class="autosummary">

isfinite isinf isnan isnat fabs signbit copysign nextafter spacing modf ldexp frexp fmod floor ceil trunc

</div>

---

1.10.0-notes.md

---

# NumPy 1.10.0 Release Notes

This release supports Python 2.6 - 2.7 and 3.2 - 3.5.

## Highlights

  - numpy.distutils now supports parallel compilation via the --parallel/-j argument passed to setup.py build
  - numpy.distutils now supports additional customization via site.cfg to control compilation parameters, i.e. runtime libraries, extra linking/compilation flags.
  - Addition of *np.linalg.multi\_dot*: compute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order.
  - The new function <span class="title-ref">np.stack</span> provides a general interface for joining a sequence of arrays along a new axis, complementing <span class="title-ref">np.concatenate</span> for joining along an existing axis.
  - Addition of <span class="title-ref">nanprod</span> to the set of nanfunctions.
  - Support for the '@' operator in Python 3.5.

## Dropped Support

  - The \_dotblas module has been removed. CBLAS Support is now in Multiarray.
  - The testcalcs.py file has been removed.
  - The polytemplate.py file has been removed.
  - npy\_PyFile\_Dup and npy\_PyFile\_DupClose have been removed from npy\_3kcompat.h.
  - splitcmdline has been removed from numpy/distutils/exec\_command.py.
  - try\_run and get\_output have been removed from numpy/distutils/command/config.py
  - The a.\_format attribute is no longer supported for array printing.
  - Keywords `skiprows` and `missing` removed from np.genfromtxt.
  - Keyword `old_behavior` removed from np.correlate.

## Future Changes

  - In array comparisons like `arr1 == arr2`, many corner cases involving strings or structured dtypes that used to return scalars now issue `FutureWarning` or `DeprecationWarning`, and in the future will be change to either perform elementwise comparisons or raise an error.
  - In `np.lib.split` an empty array in the result always had dimension `(0,)` no matter the dimensions of the array being split. In Numpy 1.11 that behavior will be changed so that the dimensions will be preserved. A `FutureWarning` for this change has been in place since Numpy 1.9 but, due to a bug, sometimes no warning was raised and the dimensions were already preserved.
  - The SafeEval class will be removed in Numpy 1.11.
  - The alterdot and restoredot functions will be removed in Numpy 1.11.

See below for more details on these changes.

## Compatibility notes

### Default casting rule change

Default casting for inplace operations has changed to `'same_kind'`. For instance, if n is an array of integers, and f is an array of floats, then `n += f` will result in a `TypeError`, whereas in previous Numpy versions the floats would be silently cast to ints. In the unlikely case that the example code is not an actual bug, it can be updated in a backward compatible way by rewriting it as `np.add(n, f, out=n, casting='unsafe')`. The old `'unsafe'` default has been deprecated since Numpy 1.7.

### numpy version string

The numpy version string for development builds has been changed from `x.y.z.dev-githash` to `x.y.z.dev0+githash` (note the +) in order to comply with PEP 440.

### relaxed stride checking

NPY\_RELAXED\_STRIDE\_CHECKING is now true by default.

UPDATE: In 1.10.2 the default value of NPY\_RELAXED\_STRIDE\_CHECKING was changed to false for back compatibility reasons. More time is needed before it can be made the default. As part of the roadmap a deprecation of dimension changing views of f\_contiguous not c\_contiguous arrays was also added.

### Concatenation of 1d arrays along any but `axis=0` raises `IndexError`

Using axis \!= 0 has raised a DeprecationWarning since NumPy 1.7, it now raises an error.

### *np.ravel*, *np.diagonal* and *np.diag* now preserve subtypes

There was inconsistent behavior between *x.ravel()* and *np.ravel(x)*, as well as between *x.diagonal()* and *np.diagonal(x)*, with the methods preserving subtypes while the functions did not. This has been fixed and the functions now behave like the methods, preserving subtypes except in the case of matrices. Matrices are special cased for backward compatibility and still return 1-D arrays as before. If you need to preserve the matrix subtype, use the methods instead of the functions.

### *rollaxis* and *swapaxes* always return a view

Previously, a view was returned except when no change was made in the order of the axes, in which case the input array was returned. A view is now returned in all cases.

### *nonzero* now returns base ndarrays

Previously, an inconsistency existed between 1-D inputs (returning a base ndarray) and higher dimensional ones (which preserved subclasses). Behavior has been unified, and the return will now be a base ndarray. Subclasses can still override this behavior by providing their own *nonzero* method.

### C API

The changes to *swapaxes* also apply to the *PyArray\_SwapAxes* C function, which now returns a view in all cases.

The changes to *nonzero* also apply to the *PyArray\_Nonzero* C function, which now returns a base ndarray in all cases.

The dtype structure (PyArray\_Descr) has a new member at the end to cache its hash value. This shouldn't affect any well-written applications.

The change to the concatenation function DeprecationWarning also affects PyArray\_ConcatenateArrays,

### recarray field return types

Previously the returned types for recarray fields accessed by attribute and by index were inconsistent, and fields of string type were returned as chararrays. Now, fields accessed by either attribute or indexing will return an ndarray for fields of non-structured type, and a recarray for fields of structured type. Notably, this affect recarrays containing strings with whitespace, as trailing whitespace is trimmed from chararrays but kept in ndarrays of string type. Also, the dtype.type of nested structured fields is now inherited.

### recarray views

Viewing an ndarray as a recarray now automatically converts the dtype to np.record. See new record array documentation. Additionally, viewing a recarray with a non-structured dtype no longer converts the result's type to ndarray -the result will remain a recarray.

### 'out' keyword argument of ufuncs now accepts tuples of arrays

When using the 'out' keyword argument of a ufunc, a tuple of arrays, one per ufunc output, can be provided. For ufuncs with a single output a single array is also a valid 'out' keyword argument. Previously a single array could be provided in the 'out' keyword argument, and it would be used as the first output for ufuncs with multiple outputs, is deprecated, and will result in a <span class="title-ref">DeprecationWarning</span> now and an error in the future.

### byte-array indices now raises an IndexError

Indexing an ndarray using a byte-string in Python 3 now raises an IndexError instead of a ValueError.

### Masked arrays containing objects with arrays

For such (rare) masked arrays, getting a single masked item no longer returns a corrupted masked array, but a fully masked version of the item.

### Median warns and returns nan when invalid values are encountered

Similar to mean, median and percentile now emits a Runtime warning and returns <span class="title-ref">NaN</span> in slices where a <span class="title-ref">NaN</span> is present. To compute the median or percentile while ignoring invalid values use the new <span class="title-ref">nanmedian</span> or <span class="title-ref">nanpercentile</span> functions.

### Functions available from numpy.ma.testutils have changed

All functions from numpy.testing were once available from numpy.ma.testutils but not all of them were redefined to work with masked arrays. Most of those functions have now been removed from numpy.ma.testutils with a small subset retained in order to preserve backward compatibility. In the long run this should help avoid mistaken use of the wrong functions, but it may cause import problems for some.

## New Features

### Reading extra flags from site.cfg

Previously customization of compilation of dependency libraries and numpy itself was only accomplishable via code changes in the distutils package. Now numpy.distutils reads in the following extra flags from each group of the *site.cfg*:

  -   - `runtime_library_dirs/rpath`, sets runtime library directories to override  
        `LD_LIBRARY_PATH`

  - `extra_compile_args`, add extra flags to the compilation of sources

  - `extra_link_args`, add extra flags when linking libraries

This should, at least partially, complete user customization.

### *np.cbrt* to compute cube root for real floats

*np.cbrt* wraps the C99 cube root function *cbrt*. Compared to *np.power(x, 1./3.)* it is well defined for negative real floats and a bit faster.

### numpy.distutils now allows parallel compilation

By passing *--parallel=n* or *-j n* to *setup.py build* the compilation of extensions is now performed in *n* parallel processes. The parallelization is limited to files within one extension so projects using Cython will not profit because it builds extensions from single files.

### *genfromtxt* has a new `max_rows` argument

A `max_rows` argument has been added to *genfromtxt* to limit the number of rows read in a single call. Using this functionality, it is possible to read in multiple arrays stored in a single file by making repeated calls to the function.

### New function *np.broadcast\_to* for invoking array broadcasting

*np.broadcast\_to* manually broadcasts an array to a given shape according to numpy's broadcasting rules. The functionality is similar to broadcast\_arrays, which in fact has been rewritten to use broadcast\_to internally, but only a single array is necessary.

### New context manager *clear\_and\_catch\_warnings* for testing warnings

When Python emits a warning, it records that this warning has been emitted in the module that caused the warning, in a module attribute `__warningregistry__`. Once this has happened, it is not possible to emit the warning again, unless you clear the relevant entry in `__warningregistry__`. This makes is hard and fragile to test warnings, because if your test comes after another that has already caused the warning, you will not be able to emit the warning or test it. The context manager `clear_and_catch_warnings` clears warnings from the module registry on entry and resets them on exit, meaning that warnings can be re-raised.

### *cov* has new `fweights` and `aweights` arguments

The `fweights` and `aweights` arguments add new functionality to covariance calculations by applying two types of weighting to observation vectors. An array of `fweights` indicates the number of repeats of each observation vector, and an array of `aweights` provides their relative importance or probability.

### Support for the '@' operator in Python 3.5+

Python 3.5 adds support for a matrix multiplication operator '@' proposed in PEP465. Preliminary support for that has been implemented, and an equivalent function `matmul` has also been added for testing purposes and use in earlier Python versions. The function is preliminary and the order and number of its optional arguments can be expected to change.

### New argument `norm` to fft functions

The default normalization has the direct transforms unscaled and the inverse transforms are scaled by \(1/n\). It is possible to obtain unitary transforms by setting the keyword argument `norm` to `"ortho"` (default is <span class="title-ref">None</span>) so that both direct and inverse transforms will be scaled by \(1/\\sqrt{n}\).

## Improvements

### *np.digitize* using binary search

*np.digitize* is now implemented in terms of *np.searchsorted*. This means that a binary search is used to bin the values, which scales much better for larger number of bins than the previous linear search. It also removes the requirement for the input array to be 1-dimensional.

### *np.poly* now casts integer inputs to float

*np.poly* will now cast 1-dimensional input arrays of integer type to double precision floating point, to prevent integer overflow when computing the monic polynomial. It is still possible to obtain higher precision results by passing in an array of object type, filled e.g. with Python ints.

### *np.interp* can now be used with periodic functions

*np.interp* now has a new parameter *period* that supplies the period of the input data *xp*. In such case, the input data is properly normalized to the given period and one end point is added to each extremity of *xp* in order to close the previous and the next period cycles, resulting in the correct interpolation behavior.

### *np.pad* supports more input types for `pad_width` and `constant_values`

`constant_values` parameters now accepts NumPy arrays and float values. NumPy arrays are supported as input for `pad_width`, and an exception is raised if its values are not of integral type.

### *np.argmax* and *np.argmin* now support an `out` argument

The `out` parameter was added to *np.argmax* and *np.argmin* for consistency with *ndarray.argmax* and *ndarray.argmin*. The new parameter behaves exactly as it does in those methods.

### More system C99 complex functions detected and used

All of the functions `in complex.h` are now detected. There are new fallback implementations of the following functions.

  - npy\_ctan,
  - npy\_cacos, npy\_casin, npy\_catan
  - npy\_ccosh, npy\_csinh, npy\_ctanh,
  - npy\_cacosh, npy\_casinh, npy\_catanh

As a result of these improvements, there will be some small changes in returned values, especially for corner cases.

### *np.loadtxt* support for the strings produced by the `float.hex` method

The strings produced by `float.hex` look like `0x1.921fb54442d18p+1`, so this is not the hex used to represent unsigned integer types.

### *np.isclose* properly handles minimal values of integer dtypes

In order to properly handle minimal values of integer types, *np.isclose* will now cast to the float dtype during comparisons. This aligns its behavior with what was provided by *np.allclose*.

### *np.allclose* uses *np.isclose* internally.

*np.allclose* now uses *np.isclose* internally and inherits the ability to compare NaNs as equal by setting `equal_nan=True`. Subclasses, such as *np.ma.MaskedArray*, are also preserved now.

### *np.genfromtxt* now handles large integers correctly

*np.genfromtxt* now correctly handles integers larger than `2**31-1` on 32-bit systems and larger than `2**63-1` on 64-bit systems (it previously crashed with an `OverflowError` in these cases). Integers larger than `2**63-1` are converted to floating-point values.

### *np.load*, *np.save* have pickle backward compatibility flags

The functions *np.load* and *np.save* have additional keyword arguments for controlling backward compatibility of pickled Python objects. This enables Numpy on Python 3 to load npy files containing object arrays that were generated on Python 2.

### MaskedArray support for more complicated base classes

Built-in assumptions that the baseclass behaved like a plain array are being removed. In particular, setting and getting elements and ranges will respect baseclass overrides of `__setitem__` and `__getitem__`, and arithmetic will respect overrides of `__add__`, `__sub__`, etc.

## Changes

### dotblas functionality moved to multiarray

The cblas versions of dot, inner, and vdot have been integrated into the multiarray module. In particular, vdot is now a multiarray function, which it was not before.

### stricter check of gufunc signature compliance

Inputs to generalized universal functions are now more strictly checked against the function's signature: all core dimensions are now required to be present in input arrays; core dimensions with the same label must have the exact same size; and output core dimension's must be specified, either by a same label input core dimension or by a passed-in output array.

### views returned from *np.einsum* are writeable

Views returned by *np.einsum* will now be writeable whenever the input array is writeable.

### *np.argmin* skips NaT values

*np.argmin* now skips NaT values in datetime64 and timedelta64 arrays, making it consistent with *np.min*, *np.argmax* and *np.max*.

## Deprecations

### Array comparisons involving strings or structured dtypes

Normally, comparison operations on arrays perform elementwise comparisons and return arrays of booleans. But in some corner cases, especially involving strings are structured dtypes, NumPy has historically returned a scalar instead. For example:

    ### Current behaviour
    
    np.arange(2) == "foo"
    # -> False
    
    np.arange(2) < "foo"
    # -> True on Python 2, error on Python 3
    
    np.ones(2, dtype="i4,i4") == np.ones(2, dtype="i4,i4,i4")
    # -> False

Continuing work started in 1.9, in 1.10 these comparisons will now raise `FutureWarning` or `DeprecationWarning`, and in the future they will be modified to behave more consistently with other comparison operations, e.g.:

    ### Future behaviour
    
    np.arange(2) == "foo"
    # -> array([False, False])
    
    np.arange(2) < "foo"
    # -> error, strings and numbers are not orderable
    
    np.ones(2, dtype="i4,i4") == np.ones(2, dtype="i4,i4,i4")
    # -> [False, False]

### SafeEval

The SafeEval class in numpy/lib/utils.py is deprecated and will be removed in the next release.

### alterdot, restoredot

The alterdot and restoredot functions no longer do anything, and are deprecated.

### pkgload, PackageLoader

These ways of loading packages are now deprecated.

### bias, ddof arguments to corrcoef

The values for the `bias` and `ddof` arguments to the `corrcoef` function canceled in the division implied by the correlation coefficient and so had no effect on the returned values.

We now deprecate these arguments to `corrcoef` and the masked array version `ma.corrcoef`.

Because we are deprecating the `bias` argument to `ma.corrcoef`, we also deprecate the use of the `allow_masked` argument as a positional argument, as its position will change with the removal of `bias`. `allow_masked` will in due course become a keyword-only argument.

### dtype string representation changes

Since 1.6, creating a dtype object from its string representation, e.g. `'f4'`, would issue a deprecation warning if the size did not correspond to an existing type, and default to creating a dtype of the default size for the type. Starting with this release, this will now raise a `TypeError`.

The only exception is object dtypes, where both `'O4'` and `'O8'` will still issue a deprecation warning. This platform-dependent representation will raise an error in the next release.

In preparation for this upcoming change, the string representation of an object dtype, i.e. `np.dtype(object).str`, no longer includes the item size, i.e. will return `'|O'` instead of `'|O4'` or `'|O8'` as before.

---

1.10.1-notes.md

---

# NumPy 1.10.1 Release Notes

This release deals with a few build problems that showed up in 1.10.0. Most users would not have seen these problems. The differences are:

  - Compiling with msvc9 or msvc10 for 32 bit Windows now requires SSE2. This was the easiest fix for what looked to be some miscompiled code when SSE2 was not used. If you need to compile for 32 bit Windows systems without SSE2 support, mingw32 should still work.
  - Make compiling with VS2008 python2.7 SDK easier
  - Change Intel compiler options so that code will also be generated to support systems without SSE4.2.
  - Some \_config test functions needed an explicit integer return in order to avoid the openSUSE rpmlinter erring out.
  - We ran into a problem with pipy not allowing reuse of filenames and a resulting proliferation of *.*.\*.postN releases. Not only were the names getting out of hand, some packages were unable to work with the postN suffix.

Numpy 1.10.1 supports Python 2.6 - 2.7 and 3.2 - 3.5.

Commits:

45a3d84 DEP: Remove warning for <span class="title-ref">full</span> when dtype is set. 0c1a5df BLD: import setuptools to allow compile with VS2008 python2.7 sdk 04211c6 BUG: mask nan to 1 in ordered compare 826716f DOC: Document the reason msvc requires SSE2 on 32 bit platforms. 49fa187 BLD: enable SSE2 for 32-bit msvc 9 and 10 compilers dcbc4cc MAINT: remove Wreturn-type warnings from config checks d6564cb BLD: do not build exclusively for SSE4.2 processors 15cb66f BLD: do not build exclusively for SSE4.2 processors c38bc08 DOC: fix var. reference in percentile docstring 78497f4 DOC: Sync 1.10.0-notes.rst in 1.10.x branch with master.

---

1.10.2-notes.md

---

# NumPy 1.10.2 Release Notes

This release deals with a number of bugs that turned up in 1.10.1 and adds various build and release improvements.

Numpy 1.10.1 supports Python 2.6 - 2.7 and 3.2 - 3.5.

## Compatibility notes

### Relaxed stride checking is no longer the default

There were back compatibility problems involving views changing the dtype of multidimensional Fortran arrays that need to be dealt with over a longer timeframe.

### Fix swig bug in `numpy.i`

Relaxed stride checking revealed a bug in `array_is_fortran(a)`, that was using PyArray\_ISFORTRAN to check for Fortran contiguity instead of PyArray\_IS\_F\_CONTIGUOUS. You may want to regenerate swigged files using the updated numpy.i

### Deprecate views changing dimensions in fortran order

This deprecates assignment of a new descriptor to the dtype attribute of a non-C-contiguous array if it result in changing the shape. This effectively bars viewing a multidimensional Fortran array using a dtype that changes the element size along the first axis.

The reason for the deprecation is that, when relaxed strides checking is enabled, arrays that are both C and Fortran contiguous are always treated as C contiguous which breaks some code that depended the two being mutually exclusive for non-scalar arrays of ndim \> 1. This deprecation prepares the way to always enable relaxed stride checking.

## Issues Fixed

  - gh-6019 Masked array repr fails for structured array with multi-dimensional column.
  - gh-6462 Median of empty array produces IndexError.
  - gh-6467 Performance regression for record array access.
  - gh-6468 numpy.interp uses 'left' value even when x\[0\]==xp\[0\].
  - gh-6475 np.allclose returns a memmap when one of its arguments is a memmap.
  - gh-6491 Error in broadcasting stride\_tricks array.
  - gh-6495 Unrecognized command line option '-ffpe-summary' in gfortran.
  - gh-6497 Failure of reduce operation on recarrays.
  - gh-6498 Mention change in default casting rule in 1.10 release notes.
  - gh-6530 The partition function errors out on empty input.
  - gh-6532 numpy.inner return wrong inaccurate value sometimes.
  - gh-6563 Intent(out) broken in recent versions of f2py.
  - gh-6569 Cannot run tests after 'python setup.py build\_ext -i'
  - gh-6572 Error in broadcasting stride\_tricks array component.
  - gh-6575 BUG: Split produces empty arrays with wrong number of dimensions
  - gh-6590 Fortran Array problem in numpy 1.10.
  - gh-6602 Random \_\_all\_\_ missing choice and dirichlet.
  - gh-6611 ma.dot no longer always returns a masked array in 1.10.
  - gh-6618 NPY\_FORTRANORDER in make\_fortran() in numpy.i
  - gh-6636 Memory leak in nested dtypes in numpy.recarray
  - gh-6641 Subsetting recarray by fields yields a structured array.
  - gh-6667 ma.make\_mask handles ma.nomask input incorrectly.
  - gh-6675 Optimized blas detection broken in master and 1.10.
  - gh-6678 Getting unexpected error from: X.dtype = complex (or Y = X.view(complex))
  - gh-6718 f2py test fail in pip installed numpy-1.10.1 in virtualenv.
  - gh-6719 Error compiling Cython file: Pythonic division not allowed without gil.
  - gh-6771 Numpy.rec.fromarrays losing dtype metadata between versions 1.9.2 and 1.10.1
  - gh-6781 The travis-ci script in maintenance/1.10.x needs fixing.
  - gh-6807 Windows testing errors for 1.10.2

## Merged PRs

The following PRs have been merged into 1.10.2. When the PR is a backport, the PR number for the original PR against master is listed.

  - gh-5773 MAINT: Hide testing helper tracebacks when using them with pytest.
  - gh-6094 BUG: Fixed a bug with string representation of masked structured arrays.
  - gh-6208 MAINT: Speedup field access by removing unneeded safety checks.
  - gh-6460 BUG: Replacing the os.environ.clear by less invasive procedure.
  - gh-6470 BUG: Fix AttributeError in numpy distutils.
  - gh-6472 MAINT: Use Python 3.5 instead of 3.5-dev for travis 3.5 testing.
  - gh-6474 REL: Update Paver script for sdist and auto-switch test warnings.
  - gh-6478 BUG: Fix Intel compiler flags for OS X build.
  - gh-6481 MAINT: LIBPATH with spaces is now supported Python 2.7+ and Win32.
  - gh-6487 BUG: Allow nested use of parameters in definition of arrays in f2py.
  - gh-6488 BUG: Extend common blocks rather than overwriting in f2py.
  - gh-6499 DOC: Mention that default casting for inplace operations has changed.
  - gh-6500 BUG: Recarrays viewed as subarrays don't convert to np.record type.
  - gh-6501 REL: Add "make upload" command for built docs, update "make dist".
  - gh-6526 BUG: Fix use of \_\_doc\_\_ in setup.py for -OO mode.
  - gh-6527 BUG: Fix the IndexError when taking the median of an empty array.
  - gh-6537 BUG: Make [ma.atleast]()\* with scalar argument return arrays.
  - gh-6538 BUG: Fix ma.masked\_values does not shrink mask if requested.
  - gh-6546 BUG: Fix inner product regression for non-contiguous arrays.
  - gh-6553 BUG: Fix partition and argpartition error for empty input.
  - gh-6556 BUG: Error in broadcast\_arrays with as\_strided array.
  - gh-6558 MAINT: Minor update to "make upload" doc build command.
  - gh-6562 BUG: Disable view safety checks in recarray.
  - gh-6567 BUG: Revert some import \* fixes in f2py.
  - gh-6574 DOC: Release notes for Numpy 1.10.2.
  - gh-6577 BUG: Fix for \#6569, allowing build\_ext --inplace
  - gh-6579 MAINT: Fix mistake in doc upload rule.
  - gh-6596 BUG: Fix swig for relaxed stride checking.
  - gh-6606 DOC: Update 1.10.2 release notes.
  - gh-6614 BUG: Add choice and dirichlet to numpy.random.\_\_all\_\_.
  - gh-6621 BUG: Fix swig make\_fortran function.
  - gh-6628 BUG: Make allclose return python bool.
  - gh-6642 BUG: Fix memleak in \_convert\_from\_dict.
  - gh-6643 ENH: make recarray.getitem return a recarray.
  - gh-6653 BUG: Fix ma dot to always return masked array.
  - gh-6668 BUG: ma.make\_mask should always return nomask for nomask argument.
  - gh-6686 BUG: Fix a bug in assert\_string\_equal.
  - gh-6695 BUG: Fix removing tempdirs created during build.
  - gh-6697 MAINT: Fix spurious semicolon in macro definition of PyArray\_FROM\_OT.
  - gh-6698 TST: test np.rint bug for large integers.
  - gh-6717 BUG: Readd fallback CBLAS detection on linux.
  - gh-6721 BUG: Fix for \#6719.
  - gh-6726 BUG: Fix bugs exposed by relaxed stride rollback.
  - gh-6757 BUG: link cblas library if cblas is detected.
  - gh-6756 TST: only test f2py, not f2py2.7 etc, fixes \#6718.
  - gh-6747 DEP: Deprecate changing shape of non-C-contiguous array via descr.
  - gh-6775 MAINT: Include from \_\_future\_\_ boilerplate in some files missing it.
  - gh-6780 BUG: metadata is not copied to base\_dtype.
  - gh-6783 BUG: Fix travis ci testing for new google infrastructure.
  - gh-6785 BUG: Quick and dirty fix for interp.
  - gh-6813 TST,BUG: Make test\_mvoid\_multidim\_print work for 32 bit systems.
  - gh-6817 BUG: Disable 32-bit msvc9 compiler optimizations for npy\_rint.
  - gh-6819 TST: Fix test\_mvoid\_multidim\_print failures on Python 2.x for Windows.

Initial support for mingwpy was reverted as it was causing problems for non-windows builds.

  - gh-6536 BUG: Revert gh-5614 to fix non-windows build problems

A fix for np.lib.split was reverted because it resulted in "fixing" behavior that will be present in the Numpy 1.11 and that was already present in Numpy 1.9. See the discussion of the issue at gh-6575 for clarification.

  - gh-6576 BUG: Revert gh-6376 to fix split behavior for empty arrays.

Relaxed stride checking was reverted. There were back compatibility problems involving views changing the dtype of multidimensional Fortran arrays that need to be dealt with over a longer timeframe.

  - gh-6735 MAINT: Make no relaxed stride checking the default for 1.10.

## Notes

A bug in the Numpy 1.10.1 release resulted in exceptions being raised for `RuntimeWarning` and `DeprecationWarning` in projects depending on Numpy. That has been fixed.

---

1.10.3-notes.md

---

# NumPy 1.10.3 Release Notes

N/A this release did not happen due to various screwups involving PyPI.

---

1.10.4-notes.md

---

# NumPy 1.10.4 Release Notes

This release is a bugfix source release motivated by a segfault regression. No windows binaries are provided for this release, as there appear to be bugs in the toolchain we use to generate those files. Hopefully that problem will be fixed for the next release. In the meantime, we suggest using one of the providers of windows binaries.

## Compatibility notes

  - The trace function now calls the trace method on subclasses of ndarray, except for matrix, for which the current behavior is preserved. This is to help with the units package of AstroPy and hopefully will not cause problems.

## Issues Fixed

  - gh-6922 BUG: numpy.recarray.sort segfaults on Windows.
  - gh-6937 BUG: busday\_offset does the wrong thing with modifiedpreceding roll.
  - gh-6949 BUG: Type is lost when slicing a subclass of recarray.

## Merged PRs

The following PRs have been merged into 1.10.4. When the PR is a backport, the PR number for the original PR against master is listed.

  - gh-6840 TST: Update travis testing script in 1.10.x
  - gh-6843 BUG: Fix use of python 3 only FileNotFoundError in test\_f2py.
  - gh-6884 REL: Update pavement.py and setup.py to reflect current version.
  - gh-6916 BUG: Fix test\_f2py so it runs correctly in runtests.py.
  - gh-6924 BUG: Fix segfault gh-6922.
  - gh-6942 Fix datetime roll='modifiedpreceding' bug.
  - gh-6943 DOC,BUG: Fix some latex generation problems.
  - gh-6950 BUG trace is not subclass aware, np.trace(ma) \!= ma.trace().
  - gh-6952 BUG recarray slices should preserve subclass.

---

1.11.0-notes.md

---

# NumPy 1.11.0 Release Notes

This release supports Python 2.6 - 2.7 and 3.2 - 3.5 and contains a number of enhancements and improvements. Note also the build system changes listed below as they may have subtle effects.

No Windows (TM) binaries are provided for this release due to a broken toolchain. One of the providers of Python packages for Windows (TM) is your best bet.

## Highlights

Details of these improvements can be found below.

  - The datetime64 type is now timezone naive.
  - A dtype parameter has been added to `randint`.
  - Improved detection of two arrays possibly sharing memory.
  - Automatic bin size estimation for `np.histogram`.
  - Speed optimization of A @ A.T and dot(A, A.T).
  - New function `np.moveaxis` for reordering array axes.

## Build System Changes

  - Numpy now uses `setuptools` for its builds instead of plain distutils. This fixes usage of `install_requires='numpy'` in the `setup.py` files of projects that depend on Numpy (see gh-6551). It potentially affects the way that build/install methods for Numpy itself behave though. Please report any unexpected behavior on the Numpy issue tracker.
  - Bento build support and related files have been removed.
  - Single file build support and related files have been removed.

## Future Changes

The following changes are scheduled for Numpy 1.12.0.

  - Support for Python 2.6, 3.2, and 3.3 will be dropped.
  - Relaxed stride checking will become the default. See the 1.8.0 release notes for a more extended discussion of what this change implies.
  - The behavior of the datetime64 "not a time" (NaT) value will be changed to match that of floating point "not a number" (NaN) values: all comparisons involving NaT will return False, except for NaT \!= NaT which will return True.
  - Indexing with floats will raise IndexError, e.g., a\[0, 0.0\].
  - Indexing with non-integer array\_like will raise `IndexError`, e.g., `a['1', '2']`
  - Indexing with multiple ellipsis will raise `IndexError`, e.g., `a[..., ...]`.
  - Non-integers used as index values will raise `TypeError`, e.g., in `reshape`, `take`, and specifying reduce axis.

In a future release the following changes will be made.

  - The `rand` function exposed in `numpy.testing` will be removed. That function is left over from early Numpy and was implemented using the Python random module. The random number generators from `numpy.random` should be used instead.
  - The `ndarray.view` method will only allow c\_contiguous arrays to be viewed using a dtype of different size causing the last dimension to change. That differs from the current behavior where arrays that are f\_contiguous but not c\_contiguous can be viewed as a dtype type of different size causing the first dimension to change.
  - Slicing a `MaskedArray` will return views of both data **and** mask. Currently the mask is copy-on-write and changes to the mask in the slice do not propagate to the original mask. See the FutureWarnings section below for details.

## Compatibility notes

### datetime64 changes

In prior versions of NumPy the experimental datetime64 type always stored times in UTC. By default, creating a datetime64 object from a string or printing it would convert from or to local time:

    # old behavior
    >>> np.datetime64('2000-01-01T00:00:00')
    numpy.datetime64('2000-01-01T00:00:00-0800')  # note the timezone offset -08:00

A consensus of datetime64 users agreed that this behavior is undesirable and at odds with how datetime64 is usually used (e.g., by [pandas](http://pandas.pydata.org)). For most use cases, a timezone naive datetime type is preferred, similar to the `datetime.datetime` type in the Python standard library. Accordingly, datetime64 no longer assumes that input is in local time, nor does it print local times:

    >>> np.datetime64('2000-01-01T00:00:00')
    numpy.datetime64('2000-01-01T00:00:00')

For backwards compatibility, datetime64 still parses timezone offsets, which it handles by converting to UTC. However, the resulting datetime is timezone naive:

    >>> np.datetime64('2000-01-01T00:00:00-08')
    DeprecationWarning: parsing timezone aware datetimes is deprecated;
    this will raise an error in the future
    numpy.datetime64('2000-01-01T08:00:00')

As a corollary to this change, we no longer prohibit casting between datetimes with date units and datetimes with time units. With timezone naive datetimes, the rule for casting from dates to times is no longer ambiguous.

### `linalg.norm` return type changes

The return type of the `linalg.norm` function is now floating point without exception. Some of the norm types previously returned integers.

### polynomial fit changes

The various fit functions in the numpy polynomial package no longer accept non-integers for degree specification.

### *np.dot* now raises `TypeError` instead of `ValueError`

This behaviour mimics that of other functions such as `np.inner`. If the two arguments cannot be cast to a common type, it could have raised a `TypeError` or `ValueError` depending on their order. Now, `np.dot` will now always raise a `TypeError`.

### FutureWarning to changed behavior

  - In `np.lib.split` an empty array in the result always had dimension `(0,)` no matter the dimensions of the array being split. This has been changed so that the dimensions will be preserved. A `FutureWarning` for this change has been in place since Numpy 1.9 but, due to a bug, sometimes no warning was raised and the dimensions were already preserved.

### `%` and `//` operators

These operators are implemented with the `remainder` and `floor_divide` functions respectively. Those functions are now based around `fmod` and are computed together so as to be compatible with each other and with the Python versions for float types. The results should be marginally more accurate or outright bug fixes compared to the previous results, but they may differ significantly in cases where roundoff makes a difference in the integer returned by `floor_divide`. Some corner cases also change, for instance, NaN is always returned for both functions when the divisor is zero, `divmod(1.0, inf)` returns `(0.0, 1.0)` except on MSVC 2008, and `divmod(-1.0, inf)` returns `(-1.0, inf)`.

### C API

Removed the `check_return` and `inner_loop_selector` members of the `PyUFuncObject` struct (replacing them with `reserved` slots to preserve struct layout). These were never used for anything, so it's unlikely that any third-party code is using them either, but we mention it here for completeness.

### object dtype detection for old-style classes

In python 2, objects which are instances of old-style user-defined classes no longer automatically count as 'object' type in the dtype-detection handler. Instead, as in python 3, they may potentially count as sequences, but only if they define both a <span class="title-ref">\_\_len\_\_</span> and a <span class="title-ref">\_\_getitem\_\_</span> method. This fixes a segfault and inconsistency between python 2 and 3.

## New Features

  - `np.histogram` now provides plugin estimators for automatically estimating the optimal number of bins. Passing one of \['auto', 'fd', 'scott', 'rice', 'sturges'\] as the argument to 'bins' results in the corresponding estimator being used.

  - A benchmark suite using [Airspeed Velocity](https://asv.readthedocs.io/) has been added, converting the previous vbench-based one. You can run the suite locally via `python runtests.py --bench`. For more details, see `benchmarks/README.rst`.

  - A new function `np.shares_memory` that can check exactly whether two arrays have memory overlap is added. `np.may_share_memory` also now has an option to spend more effort to reduce false positives.

  - `SkipTest` and `KnownFailureException` exception classes are exposed in the `numpy.testing` namespace. Raise them in a test function to mark the test to be skipped or mark it as a known failure, respectively.

  - `f2py.compile` has a new `extension` keyword parameter that allows the fortran extension to be specified for generated temp files. For instance, the files can be specifies to be `*.f90`. The `verbose` argument is also activated, it was previously ignored.

  - A `dtype` parameter has been added to `np.random.randint` Random ndarrays of the following types can now be generated:
    
      - `np.bool_`,
      - `np.int8`, `np.uint8`,
      - `np.int16`, `np.uint16`,
      - `np.int32`, `np.uint32`,
      - `np.int64`, `np.uint64`,
      - `np.int_`, `np.intp`
    
    The specification is by precision rather than by C type. Hence, on some platforms `np.int64` may be a `long` instead of `long long` even if the specified dtype is `long long` because the two may have the same precision. The resulting type depends on which C type numpy uses for the given precision. The byteorder specification is also ignored, the generated arrays are always in native byte order.

  - A new `np.moveaxis` function allows for moving one or more array axes to a new position by explicitly providing source and destination axes. This function should be easier to use than the current `rollaxis` function as well as providing more functionality.

  - The `deg` parameter of the various `numpy.polynomial` fits has been extended to accept a list of the degrees of the terms to be included in the fit, the coefficients of all other terms being constrained to zero. The change is backward compatible, passing a scalar `deg` will behave as before.

  - A divmod function for float types modeled after the Python version has been added to the npy\_math library.

## Improvements

### `np.gradient` now supports an `axis` argument

The `axis` parameter was added to `np.gradient` for consistency. It allows to specify over which axes the gradient is calculated.

### `np.lexsort` now supports arrays with object data-type

The function now internally calls the generic `npy_amergesort` when the type does not implement a merge-sort kind of `argsort` method.

### `np.ma.core.MaskedArray` now supports an `order` argument

When constructing a new `MaskedArray` instance, it can be configured with an `order` argument analogous to the one when calling `np.ndarray`. The addition of this argument allows for the proper processing of an `order` argument in several MaskedArray-related utility functions such as `np.ma.core.array` and `np.ma.core.asarray`.

### Memory and speed improvements for masked arrays

Creating a masked array with `mask=True` (resp. `mask=False`) now uses `np.ones` (resp. `np.zeros`) to create the mask, which is faster and avoid a big memory peak. Another optimization was done to avoid a memory peak and useless computations when printing a masked array.

### `ndarray.tofile` now uses fallocate on linux

The function now uses the fallocate system call to reserve sufficient disk space on file systems that support it.

### Optimizations for operations of the form `A.T @ A` and `A @ A.T`

Previously, `gemm` BLAS operations were used for all matrix products. Now, if the matrix product is between a matrix and its transpose, it will use `syrk` BLAS operations for a performance boost. This optimization has been extended to `@`, `numpy.dot`, `numpy.inner`, and `numpy.matmul`.

**Note:** Requires the transposed and non-transposed matrices to share data.

### `np.testing.assert_warns` can now be used as a context manager

This matches the behavior of `assert_raises`.

### Speed improvement for np.random.shuffle

`np.random.shuffle` is now much faster for 1d ndarrays.

## Changes

### Pyrex support was removed from `numpy.distutils`

The method `build_src.generate_a_pyrex_source` will remain available; it has been monkeypatched by users to support Cython instead of Pyrex. It's recommended to switch to a better supported method of build Cython extensions though.

### `np.broadcast` can now be called with a single argument

The resulting object in that case will simply mimic iteration over a single array. This change obsoletes distinctions like

>   - if len(x) == 1:  
>     shape = x\[0\].shape
> 
>   - else:  
>     shape = np.broadcast(\*x).shape

Instead, `np.broadcast` can be used in all cases.

### `np.trace` now respects array subclasses

This behaviour mimics that of other functions such as `np.diagonal` and ensures, e.g., that for masked arrays `np.trace(ma)` and `ma.trace()` give the same result.

### `np.dot` now raises `TypeError` instead of `ValueError`

This behaviour mimics that of other functions such as `np.inner`. If the two arguments cannot be cast to a common type, it could have raised a `TypeError` or `ValueError` depending on their order. Now, `np.dot` will now always raise a `TypeError`.

### `linalg.norm` return type changes

The `linalg.norm` function now does all its computations in floating point and returns floating results. This change fixes bugs due to integer overflow and the failure of abs with signed integers of minimum value, e.g., int8(-128). For consistency, floats are used even where an integer might work.

## Deprecations

### Views of arrays in Fortran order

The F\_CONTIGUOUS flag was used to signal that views using a dtype that changed the element size would change the first index. This was always problematical for arrays that were both F\_CONTIGUOUS and C\_CONTIGUOUS because C\_CONTIGUOUS took precedence. Relaxed stride checking results in more such dual contiguous arrays and breaks some existing code as a result. Note that this also affects changing the dtype by assigning to the dtype attribute of an array. The aim of this deprecation is to restrict views to C\_CONTIGUOUS arrays at some future time. A work around that is backward compatible is to use `a.T.view(...).T` instead. A parameter may also be added to the view method to explicitly ask for Fortran order views, but that will not be backward compatible.

### Invalid arguments for array ordering

It is currently possible to pass in arguments for the `order` parameter in methods like `array.flatten` or `array.ravel` that were not one of the following: 'C', 'F', 'A', 'K' (note that all of these possible values are both unicode and case insensitive). Such behavior will not be allowed in future releases.

### Random number generator in the `testing` namespace

The Python standard library random number generator was previously exposed in the `testing` namespace as `testing.rand`. Using this generator is not recommended and it will be removed in a future release. Use generators from `numpy.random` namespace instead.

### Random integer generation on a closed interval

In accordance with the Python C API, which gives preference to the half-open interval over the closed one, `np.random.random_integers` is being deprecated in favor of calling `np.random.randint`, which has been enhanced with the `dtype` parameter as described under "New Features". However, `np.random.random_integers` will not be removed anytime soon.

## FutureWarnings

### Assigning to slices/views of `MaskedArray`

Currently a slice of a masked array contains a view of the original data and a copy-on-write view of the mask. Consequently, any changes to the slice's mask will result in a copy of the original mask being made and that new mask being changed rather than the original. For example, if we make a slice of the original like so, `view = original[:]`, then modifications to the data in one array will affect the data of the other but, because the mask will be copied during assignment operations, changes to the mask will remain local. A similar situation occurs when explicitly constructing a masked array using `MaskedArray(data, mask)`, the returned array will contain a view of `data` but the mask will be a copy-on-write view of `mask`.

In the future, these cases will be normalized so that the data and mask arrays are treated the same way and modifications to either will propagate between views. In 1.11, numpy will issue a `MaskedArrayFutureWarning` warning whenever user code modifies the mask of a view that in the future may cause values to propagate back to the original. To silence these warnings and make your code robust against the upcoming changes, you have two options: if you want to keep the current behavior, call `masked_view.unshare_mask()` before modifying the mask. If you want to get the future behavior early, use `masked_view._sharedmask = False`. However, note that setting the `_sharedmask` attribute will break following explicit calls to `masked_view.unshare_mask()`.

---

1.11.1-notes.md

---

# NumPy 1.11.1 Release Notes

Numpy 1.11.1 supports Python 2.6 - 2.7 and 3.2 - 3.5. It fixes bugs and regressions found in Numpy 1.11.0 and includes several build related improvements. Wheels for Linux, Windows, and OSX can be found on PyPI.

## Fixes Merged

  - \#7506 BUG: Make sure numpy imports on python 2.6 when nose is unavailable.
  - \#7530 BUG: Floating exception with invalid axis in np.lexsort.
  - \#7535 BUG: Extend glibc complex trig functions blacklist to glibc \< 2.18.
  - \#7551 BUG: Allow graceful recovery for no compiler.
  - \#7558 BUG: Constant padding expected wrong type in constant\_values.
  - \#7578 BUG: Fix OverflowError in Python 3.x. in swig interface.
  - \#7590 BLD: Fix configparser.InterpolationSyntaxError.
  - \#7597 BUG: Make np.ma.take work on scalars.
  - \#7608 BUG: linalg.norm(): Don't convert object arrays to float.
  - \#7638 BLD: Correct C compiler customization in system\_info.py.
  - \#7654 BUG: ma.median of 1d array should return a scalar.
  - \#7656 BLD: Remove hardcoded Intel compiler flag -xSSE4.2.
  - \#7660 BUG: Temporary fix for str(mvoid) for object field types.
  - \#7665 BUG: Fix incorrect printing of 1D masked arrays.
  - \#7670 BUG: Correct initial index estimate in histogram.
  - \#7671 BUG: Boolean assignment no GIL release when transfer needs API.
  - \#7676 BUG: Fix handling of right edge of final histogram bin.
  - \#7680 BUG: Fix np.clip bug NaN handling for Visual Studio 2015.
  - \#7724 BUG: Fix segfaults in np.random.shuffle.
  - \#7731 MAINT: Change mkl\_info.dir\_env\_var from MKL to MKLROOT.
  - \#7737 BUG: Fix issue on OS X with Python 3.x, npymath.ini not installed.

---

1.11.2-notes.md

---

# NumPy 1.11.2 Release Notes

Numpy 1.11.2 supports Python 2.6 - 2.7 and 3.2 - 3.5. It fixes bugs and regressions found in Numpy 1.11.1 and includes several build related improvements. Wheels for Linux, Windows, and OS X can be found on PyPI.

## Pull Requests Merged

Fixes overridden by later merges and release notes updates are omitted.

  - \#7736 BUG: Many functions silently drop 'keepdims' kwarg.
  - \#7738 ENH: Add extra kwargs and update doc of many MA methods.
  - \#7778 DOC: Update Numpy 1.11.1 release notes.
  - \#7793 BUG: MaskedArray.count treats negative axes incorrectly.
  - \#7816 BUG: Fix array too big error for wide dtypes.
  - \#7821 BUG: Make sure [npy\_mul\_with\_overflow]()\<type\> detects overflow.
  - \#7824 MAINT: Allocate fewer bytes for empty arrays.
  - \#7847 MAINT,DOC: Fix some imp module uses and update f2py.compile docstring.
  - \#7849 MAINT: Fix remaining uses of deprecated Python imp module.
  - \#7851 BLD: Fix ATLAS version detection.
  - \#7896 BUG: Construct ma.array from np.array which contains padding.
  - \#7904 BUG: Fix float16 type not being called due to wrong ordering.
  - \#7917 BUG: Production install of numpy should not require nose.
  - \#7919 BLD: Fixed MKL detection for recent versions of this library.
  - \#7920 BUG: Fix for issue \#7835 (ma.median of 1d).
  - \#7932 BUG: Monkey-patch \_msvccompile.gen\_lib\_option like other compilers.
  - \#7939 BUG: Check for HAVE\_LDOUBLE\_DOUBLE\_DOUBLE\_LE in npy\_math\_complex.
  - \#7953 BUG: Guard against buggy comparisons in generic quicksort.
  - \#7954 BUG: Use keyword arguments to initialize Extension base class.
  - \#7955 BUG: Make sure numpy globals keep identity after reload.
  - \#7972 BUG: MSVCCompiler grows 'lib' & 'include' env strings exponentially.
  - \#8005 BLD: Remove \_\_NUMPY\_SETUP\_\_ from builtins at end of setup.py.
  - \#8010 MAINT: Remove leftover imp module imports.
  - \#8020 BUG: Fix return of np.ma.count if keepdims is True and axis is None.
  - \#8024 BUG: Fix numpy.ma.median.
  - \#8031 BUG: Fix np.ma.median with only one non-masked value.
  - \#8044 BUG: Fix bug in NpyIter buffering with discontinuous arrays.

---

1.11.3-notes.md

---

# NumPy 1.11.3 Release Notes

Numpy 1.11.3 fixes a bug that leads to file corruption when very large files opened in append mode are used in `ndarray.tofile`. It supports Python versions 2.6 - 2.7 and 3.2 - 3.5. Wheels for Linux, Windows, and OS X can be found on PyPI.

## Contributors to maintenance/1.11.3

A total of 2 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Pavel Potocek +

## Pull Requests Merged

  - [\#8341](https://github.com/numpy/numpy/pull/8341): BUG: Fix ndarray.tofile large file corruption in append mode.
  - [\#8346](https://github.com/numpy/numpy/pull/8346): TST: Fix tests in PR \#8341 for NumPy 1.11.x

---

1.12.0-notes.md

---

# NumPy 1.12.0 Release Notes

This release supports Python 2.7 and 3.4 - 3.6.

## Highlights

The NumPy 1.12.0 release contains a large number of fixes and improvements, but few that stand out above all others. That makes picking out the highlights somewhat arbitrary but the following may be of particular interest or indicate areas likely to have future consequences.

  - Order of operations in `np.einsum` can now be optimized for large speed improvements.
  - New `signature` argument to `np.vectorize` for vectorizing with core dimensions.
  - The `keepdims` argument was added to many functions.
  - New context manager for testing warnings
  - Support for BLIS in numpy.distutils
  - Much improved support for PyPy (not yet finished)

## Dropped Support

  - Support for Python 2.6, 3.2, and 3.3 has been dropped.

## Added Support

  - Support for PyPy 2.7 v5.6.0 has been added. While not complete (nditer `updateifcopy` is not supported yet), this is a milestone for PyPy's C-API compatibility layer.

## Build System Changes

  - Library order is preserved, instead of being reordered to match that of the directories.

## Deprecations

### Assignment of ndarray object's `data` attribute

Assigning the 'data' attribute is an inherently unsafe operation as pointed out in gh-7083. Such a capability will be removed in the future.

### Unsafe int casting of the num attribute in `linspace`

`np.linspace` now raises DeprecationWarning when num cannot be safely interpreted as an integer.

### Insufficient bit width parameter to `binary_repr`

If a 'width' parameter is passed into `binary_repr` that is insufficient to represent the number in base 2 (positive) or 2's complement (negative) form, the function used to silently ignore the parameter and return a representation using the minimal number of bits needed for the form in question. Such behavior is now considered unsafe from a user perspective and will raise an error in the future.

## Future Changes

  - In 1.13 NAT will always compare False except for `NAT != NAT`, which will be True. In short, NAT will behave like NaN
  - In 1.13 `np.average` will preserve subclasses, to match the behavior of most other numpy functions such as np.mean. In particular, this means calls which returned a scalar may return a 0-d subclass object instead.

### Multiple-field manipulation of structured arrays

In 1.13 the behavior of structured arrays involving multiple fields will change in two ways:

First, indexing a structured array with multiple fields (eg, `arr[['f1', 'f3']]`) will return a view into the original array in 1.13, instead of a copy. Note the returned view will have extra padding bytes corresponding to intervening fields in the original array, unlike the copy in 1.12, which will affect code such as `arr[['f1', 'f3']].view(newdtype)`.

Second, for numpy versions 1.6 to 1.12 assignment between structured arrays occurs "by field name": Fields in the destination array are set to the identically-named field in the source array or to 0 if the source does not have a field:

    >>> a = np.array([(1,2),(3,4)], dtype=[('x', 'i4'), ('y', 'i4')])
    >>> b = np.ones(2, dtype=[('z', 'i4'), ('y', 'i4'), ('x', 'i4')])
    >>> b[:] = a
    >>> b
    array([(0, 2, 1), (0, 4, 3)],
          dtype=[('z', '<i4'), ('y', '<i4'), ('x', '<i4')])

In 1.13 assignment will instead occur "by position": The Nth field of the destination will be set to the Nth field of the source regardless of field name. The old behavior can be obtained by using indexing to reorder the fields before assignment, e.g., `b[['x', 'y']] = a[['y', 'x']]`.

## Compatibility notes

### DeprecationWarning to error

  - Indexing with floats raises `IndexError`, e.g., a\[0, 0.0\].
  - Indexing with non-integer array\_like raises `IndexError`, e.g., `a['1', '2']`
  - Indexing with multiple ellipsis raises `IndexError`, e.g., `a[..., ...]`.
  - Non-integers used as index values raise `TypeError`, e.g., in `reshape`, `take`, and specifying reduce axis.

### FutureWarning to changed behavior

  - `np.full` now returns an array of the fill-value's dtype if no dtype is given, instead of defaulting to float.
  - `np.average` will emit a warning if the argument is a subclass of ndarray, as the subclass will be preserved starting in 1.13. (see Future Changes)

### `power` and `**` raise errors for integer to negative integer powers

The previous behavior depended on whether numpy scalar integers or numpy integer arrays were involved.

For arrays

  - Zero to negative integer powers returned least integral value.
  - Both 1, -1 to negative integer powers returned correct values.
  - The remaining integers returned zero when raised to negative integer powers.

For scalars

  - Zero to negative integer powers returned least integral value.
  - Both 1, -1 to negative integer powers returned correct values.
  - The remaining integers sometimes returned zero, sometimes the correct float depending on the integer type combination.

All of these cases now raise a `ValueError` except for those integer combinations whose common type is float, for instance uint64 and int8. It was felt that a simple rule was the best way to go rather than have special exceptions for the integer units. If you need negative powers, use an inexact type.

### Relaxed stride checking is the default

This will have some impact on code that assumed that `F_CONTIGUOUS` and `C_CONTIGUOUS` were mutually exclusive and could be set to determine the default order for arrays that are now both.

### The `np.percentile` 'midpoint' interpolation method fixed for exact indices

The 'midpoint' interpolator now gives the same result as 'lower' and 'higher' when the two coincide. Previous behavior of 'lower' + 0.5 is fixed.

### `keepdims` kwarg is passed through to user-class methods

numpy functions that take a `keepdims` kwarg now pass the value through to the corresponding methods on ndarray sub-classes. Previously the `keepdims` keyword would be silently dropped. These functions now have the following behavior:

1.  If user does not provide `keepdims`, no keyword is passed to the underlying method.
2.  Any user-provided value of `keepdims` is passed through as a keyword argument to the method.

This will raise in the case where the method does not support a `keepdims` kwarg and the user explicitly passes in `keepdims`.

The following functions are changed: `sum`, `product`, `sometrue`, `alltrue`, `any`, `all`, `amax`, `amin`, `prod`, `mean`, `std`, `var`, `nanmin`, `nanmax`, `nansum`, `nanprod`, `nanmean`, `nanmedian`, `nanvar`, `nanstd`

### `bitwise_and` identity changed

The previous identity was 1, it is now -1. See entry in Improvements for more explanation.

### ma.median warns and returns nan when unmasked invalid values are encountered

Similar to unmasked median the masked median <span class="title-ref">ma.median</span> now emits a Runtime warning and returns <span class="title-ref">NaN</span> in slices where an unmasked <span class="title-ref">NaN</span> is present.

### Greater consistency in `assert_almost_equal`

The precision check for scalars has been changed to match that for arrays. It is now:

    abs(actual - desired) < 1.5 * 10**(-decimal)

Note that this is looser than previously documented, but agrees with the previous implementation used in `assert_array_almost_equal`. Due to the change in implementation some very delicate tests may fail that did not fail before.

### `NoseTester` behaviour of warnings during testing

When `raise_warnings="develop"` is given, all uncaught warnings will now be considered a test failure. Previously only selected ones were raised. Warnings which are not caught or raised (mostly when in release mode) will be shown once during the test cycle similar to the default python settings.

### `assert_warns` and `deprecated` decorator more specific

The `assert_warns` function and context manager are now more specific to the given warning category. This increased specificity leads to them being handled according to the outer warning settings. This means that no warning may be raised in cases where a wrong category warning is given and ignored outside the context. Alternatively the increased specificity may mean that warnings that were incorrectly ignored will now be shown or raised. See also the new `suppress_warnings` context manager. The same is true for the `deprecated` decorator.

### C API

No changes.

## New Features

### Writeable keyword argument for `as_strided`

`np.lib.stride_tricks.as_strided` now has a `writeable` keyword argument. It can be set to False when no write operation to the returned array is expected to avoid accidental unpredictable writes.

### `axes` keyword argument for `rot90`

The `axes` keyword argument in `rot90` determines the plane in which the array is rotated. It defaults to `axes=(0,1)` as in the original function.

### Generalized `flip`

`flipud` and `fliplr` reverse the elements of an array along axis=0 and axis=1 respectively. The newly added `flip` function reverses the elements of an array along any given axis.

  - `np.count_nonzero` now has an `axis` parameter, allowing non-zero counts to be generated on more than just a flattened array object.

### BLIS support in `numpy.distutils`

Building against the BLAS implementation provided by the BLIS library is now supported. See the `[blis]` section in `site.cfg.example` (in the root of the numpy repo or source distribution).

### Hook in `numpy/__init__.py` to run distribution-specific checks

Binary distributions of numpy may need to run specific hardware checks or load specific libraries during numpy initialization. For example, if we are distributing numpy with a BLAS library that requires SSE2 instructions, we would like to check the machine on which numpy is running does have SSE2 in order to give an informative error.

Add a hook in `numpy/__init__.py` to import a `numpy/_distributor_init.py` file that will remain empty (bar a docstring) in the standard numpy source, but that can be overwritten by people making binary distributions of numpy.

### New nanfunctions `nancumsum` and `nancumprod` added

Nan-functions `nancumsum` and `nancumprod` have been added to compute `cumsum` and `cumprod` by ignoring nans.

### `np.interp` can now interpolate complex values

`np.lib.interp(x, xp, fp)` now allows the interpolated array `fp` to be complex and will interpolate at `complex128` precision.

### New polynomial evaluation function `polyvalfromroots` added

The new function `polyvalfromroots` evaluates a polynomial at given points from the roots of the polynomial. This is useful for higher order polynomials, where expansion into polynomial coefficients is inaccurate at machine precision.

### New array creation function `geomspace` added

The new function `geomspace` generates a geometric sequence. It is similar to `logspace`, but with start and stop specified directly: `geomspace(start, stop)` behaves the same as `logspace(log10(start), log10(stop))`.

### New context manager for testing warnings

A new context manager `suppress_warnings` has been added to the testing utils. This context manager is designed to help reliably test warnings. Specifically to reliably filter/ignore warnings. Ignoring warnings by using an "ignore" filter in Python versions before 3.4.x can quickly result in these (or similar) warnings not being tested reliably.

The context manager allows to filter (as well as record) warnings similar to the `catch_warnings` context, but allows for easier specificity. Also printing warnings that have not been filtered or nesting the context manager will work as expected. Additionally, it is possible to use the context manager as a decorator which can be useful when multiple tests give need to hide the same warning.

### New masked array functions `ma.convolve` and `ma.correlate` added

These functions wrapped the non-masked versions, but propagate through masked values. There are two different propagation modes. The default causes masked values to contaminate the result with masks, but the other mode only outputs masks if there is no alternative.

### New `float_power` ufunc

The new `float_power` ufunc is like the `power` function except all computation is done in a minimum precision of float64. There was a long discussion on the numpy mailing list of how to treat integers to negative integer powers and a popular proposal was that the `__pow__` operator should always return results of at least float64 precision. The `float_power` function implements that option. Note that it does not support object arrays.

### `np.loadtxt` now supports a single integer as `usecol` argument

Instead of using `usecol=(n,)` to read the nth column of a file it is now allowed to use `usecol=n`. Also the error message is more user friendly when a non-integer is passed as a column index.

### Improved automated bin estimators for `histogram`

Added 'doane' and 'sqrt' estimators to `histogram` via the `bins` argument. Added support for range-restricted histograms with automated bin estimation.

### `np.roll` can now roll multiple axes at the same time

The `shift` and `axis` arguments to `roll` are now broadcast against each other, and each specified axis is shifted accordingly.

### The `__complex__` method has been implemented for the ndarrays

Calling `complex()` on a size 1 array will now cast to a python complex.

### `pathlib.Path` objects now supported

The standard `np.load`, `np.save`, `np.loadtxt`, `np.savez`, and similar functions can now take `pathlib.Path` objects as an argument instead of a filename or open file object.

### New `bits` attribute for `np.finfo`

This makes `np.finfo` consistent with `np.iinfo` which already has that attribute.

### New `signature` argument to `np.vectorize`

This argument allows for vectorizing user defined functions with core dimensions, in the style of NumPy's \[generalized universal functions\<c-api.generalized-ufuncs\>\](\#generalized-universal-functions\<c-api.generalized-ufuncs\>). This allows for vectorizing a much broader class of functions. For example, an arbitrary distance metric that combines two vectors to produce a scalar could be vectorized with `signature='(n),(n)->()'`. See `np.vectorize` for full details.

### Emit py3kwarnings for division of integer arrays

To help people migrate their code bases from Python 2 to Python 3, the python interpreter has a handy option -3, which issues warnings at runtime. One of its warnings is for integer division:

    $ python -3 -c "2/3"
    
    -c:1: DeprecationWarning: classic int division

In Python 3, the new integer division semantics also apply to numpy arrays. With this version, numpy will emit a simil\> **Warning** \> $ python -3 -c "import numpy as np; np.array(2)/np.array(3)"

> \-c:1: DeprecationWarning: numpy: classic int division

### numpy.sctypes now includes bytes on Python3 too

Previously, it included str (bytes) and unicode on Python2, but only str (unicode) on Python3.

## Improvements

### `bitwise_and` identity changed

The previous identity was 1 with the result that all bits except the LSB were masked out when the reduce method was used. The new identity is -1, which should work properly on twos complement machines as all bits will be set to one.

### Generalized Ufuncs will now unlock the GIL

Generalized Ufuncs, including most of the linalg module, will now unlock the Python global interpreter lock.

### Caches in <span class="title-ref">np.fft</span> are now bounded in total size and item count

The caches in <span class="title-ref">np.fft</span> that speed up successive FFTs of the same length can no longer grow without bounds. They have been replaced with LRU (least recently used) caches that automatically evict no longer needed items if either the memory size or item count limit has been reached.

### Improved handling of zero-width string/unicode dtypes

Fixed several interfaces that explicitly disallowed arrays with zero-width string dtypes (i.e. `dtype('S0')` or `dtype('U0')`, and fixed several bugs where such dtypes were not handled properly. In particular, changed `ndarray.__new__` to not implicitly convert `dtype('S0')` to `dtype('S1')` (and likewise for unicode) when creating new arrays.

### Integer ufuncs vectorized with AVX2

If the cpu supports it at runtime the basic integer ufuncs now use AVX2 instructions. This feature is currently only available when compiled with GCC.

### Order of operations optimization in `np.einsum`

`np.einsum` now supports the `optimize` argument which will optimize the order of contraction. For example, `np.einsum` would complete the chain dot example `np.einsum(â€˜ij,jk,kl->ilâ€™, a, b, c)` in a single pass which would scale like `N^4`; however, when `optimize=True` `np.einsum` will create an intermediate array to reduce this scaling to `N^3` or effectively `np.dot(a, b).dot(c)`. Usage of intermediate tensors to reduce scaling has been applied to the general einsum summation notation. See `np.einsum_path` for more details.

### quicksort has been changed to an introsort

The quicksort kind of `np.sort` and `np.argsort` is now an introsort which is regular quicksort but changing to a heapsort when not enough progress is made. This retains the good quicksort performance while changing the worst case runtime from `O(N^2)` to `O(N*log(N))`.

### `ediff1d` improved performance and subclass handling

The ediff1d function uses an array instead on a flat iterator for the subtraction. When to\_begin or to\_end is not None, the subtraction is performed in place to eliminate a copy operation. A side effect is that certain subclasses are handled better, namely astropy.Quantity, since the complete array is created, wrapped, and then begin and end values are set, instead of using concatenate.

### Improved precision of `ndarray.mean` for float16 arrays

The computation of the mean of float16 arrays is now carried out in float32 for improved precision. This should be useful in packages such as Theano where the precision of float16 is adequate and its smaller footprint is desirable.

## Changes

### All array-like methods are now called with keyword arguments in fromnumeric.py

Internally, many array-like methods in fromnumeric.py were being called with positional arguments instead of keyword arguments as their external signatures were doing. This caused a complication in the downstream 'pandas' library that encountered an issue with 'numpy' compatibility. Now, all array-like methods in this module are called with keyword arguments instead.

### Operations on np.memmap objects return numpy arrays in most cases

Previously operations on a memmap object would misleadingly return a memmap instance even if the result was actually not memmapped. For example, `arr + 1` or `arr + arr` would return memmap instances, although no memory from the output array is memmapped. Version 1.12 returns ordinary numpy arrays from these operations.

Also, reduction of a memmap (e.g. `.sum(axis=None`) now returns a numpy scalar instead of a 0d memmap.

### stacklevel of warnings increased

The stacklevel for python based warnings was increased so that most warnings will report the offending line of the user code instead of the line the warning itself is given. Passing of stacklevel is now tested to ensure that new warnings will receive the `stacklevel` argument.

This causes warnings with the "default" or "module" filter to be shown once for every offending user code line or user module instead of only once. On python versions before 3.4, this can cause warnings to appear that were falsely ignored before, which may be surprising especially in test suits.

---

1.12.1-notes.md

---

# NumPy 1.12.1 Release Notes

NumPy 1.12.1 supports Python 2.7 and 3.4 - 3.6 and fixes bugs and regressions found in NumPy 1.12.0. In particular, the regression in f2py constant parsing is fixed. Wheels for Linux, Windows, and OSX can be found on PyPI,

## Bugs Fixed

  - BUG: Fix wrong future nat warning and equiv type logic error...
  - BUG: Fix wrong masked median for some special cases
  - DOC: Place np.average in inline code
  - TST: Work around isfinite inconsistency on i386
  - BUG: Guard against replacing constants without '\_' spec in f2py.
  - BUG: Fix mean for float 16 non-array inputs for 1.12
  - BUG: Fix calling python api with error set and minor leaks for...
  - BUG: Make iscomplexobj compatible with custom dtypes again
  - BUG: Fix undefined behaviour induced by bad \_\_array\_wrap\_\_
  - BUG: Fix MaskedArray.\_\_setitem\_\_
  - BUG: PPC64el machines are POWER for Fortran in f2py
  - BUG: Look up methods on MaskedArray in <span class="title-ref">\_frommethod</span>
  - BUG: Remove extra digit in binary\_repr at limit
  - BUG: Fix deepcopy regression for empty arrays.
  - BUG: Fix ma.median for empty ndarrays

---

1.13.0-notes.md

---

# NumPy 1.13.0 Release Notes

This release supports Python 2.7 and 3.4 - 3.6.

## Highlights

  - Operations like `a + b + c` will reuse temporaries on some platforms, resulting in less memory use and faster execution.
  - Inplace operations check if inputs overlap outputs and create temporaries to avoid problems.
  - New `__array_ufunc__` attribute provides improved ability for classes to override default ufunc behavior.
  - New `np.block` function for creating blocked arrays.

## New functions

  - New `np.positive` ufunc.
  - New `np.divmod` ufunc provides more efficient divmod.
  - New `np.isnat` ufunc tests for NaT special values.
  - New `np.heaviside` ufunc computes the Heaviside function.
  - New `np.isin` function, improves on `in1d`.
  - New `np.block` function for creating blocked arrays.
  - New `PyArray_MapIterArrayCopyIfOverlap` added to NumPy C-API.

See below for details.

## Deprecations

  - Calling `np.fix`, `np.isposinf`, and `np.isneginf` with `f(x, y=out)` is deprecated - the argument should be passed as `f(x, out=out)`, which matches other ufunc-like interfaces.
  - Use of the C-API `NPY_CHAR` type number deprecated since version 1.7 will now raise deprecation warnings at runtime. Extensions built with older f2py versions need to be recompiled to remove the warning.
  - `np.ma.argsort`, `np.ma.minimum.reduce`, and `np.ma.maximum.reduce` should be called with an explicit <span class="title-ref">axis</span> argument when applied to arrays with more than 2 dimensions, as the default value of this argument (`None`) is inconsistent with the rest of numpy (`-1`, `0`, and `0`, respectively).
  - `np.ma.MaskedArray.mini` is deprecated, as it almost duplicates the functionality of `np.MaskedArray.min`. Exactly equivalent behaviour can be obtained with `np.ma.minimum.reduce`.
  - The single-argument form of `np.ma.minimum` and `np.ma.maximum` is deprecated. `np.maximum`. `np.ma.minimum(x)` should now be spelt `np.ma.minimum.reduce(x)`, which is consistent with how this would be done with `np.minimum`.
  - Calling `ndarray.conjugate` on non-numeric dtypes is deprecated (it should match the behavior of `np.conjugate`, which throws an error).
  - Calling `expand_dims` when the `axis` keyword does not satisfy `-a.ndim - 1 <= axis <= a.ndim`, where `a` is the array being reshaped, is deprecated.

## Future Changes

  - Assignment between structured arrays with different field names will change in NumPy 1.14. Previously, fields in the dst would be set to the value of the identically-named field in the src. In numpy 1.14 fields will instead be assigned 'by position': The n-th field of the dst will be set to the n-th field of the src array. Note that the `FutureWarning` raised in NumPy 1.12 incorrectly reported this change as scheduled for NumPy 1.13 rather than NumPy 1.14.

## Build System Changes

  - `numpy.distutils` now automatically determines C-file dependencies with GCC compatible compilers.

## Compatibility notes

### Error type changes

  - `numpy.hstack()` now throws `ValueError` instead of `IndexError` when input is empty.
  - Functions taking an axis argument, when that argument is out of range, now throw `np.AxisError` instead of a mixture of `IndexError` and `ValueError`. For backwards compatibility, `AxisError` subclasses both of these.

### Tuple object dtypes

Support has been removed for certain obscure dtypes that were unintentionally allowed, of the form `(old_dtype, new_dtype)`, where either of the dtypes is or contains the `object` dtype. As an exception, dtypes of the form `(object, [('name', object)])` are still supported due to evidence of existing use.

### DeprecationWarning to error

See Changes section for more detail.

  - `partition`, TypeError when non-integer partition index is used.
  - `NpyIter_AdvancedNew`, ValueError when `oa_ndim == 0` and `op_axes` is NULL
  - `negative(bool_)`, TypeError when negative applied to booleans.
  - `subtract(bool_, bool_)`, TypeError when subtracting boolean from boolean.
  - `np.equal, np.not_equal`, object identity doesn't override failed comparison.
  - `np.equal, np.not_equal`, object identity doesn't override non-boolean comparison.
  - Deprecated boolean indexing behavior dropped. See Changes below for details.
  - Deprecated `np.alterdot()` and `np.restoredot()` removed.

### FutureWarning to changed behavior

See Changes section for more detail.

  - `numpy.average` preserves subclasses
  - `array == None` and `array != None` do element-wise comparison.
  - `np.equal, np.not_equal`, object identity doesn't override comparison result.

### dtypes are now always true

Previously `bool(dtype)` would fall back to the default python implementation, which checked if `len(dtype) > 0`. Since `dtype` objects implement `__len__` as the number of record fields, `bool` of scalar dtypes would evaluate to `False`, which was unintuitive. Now `bool(dtype) == True` for all dtypes.

### `__getslice__` and `__setslice__` are no longer needed in `ndarray` subclasses

When subclassing np.ndarray in Python 2.7, it is no longer \_[necessary]() to implement `__*slice__` on the derived class, as `__*item__` will intercept these calls correctly.

Any code that did implement these will work exactly as before. Code that invokes`ndarray.__getslice__` (e.g. through `super(...).__getslice__`) will now issue a DeprecationWarning - `.__getitem__(slice(start, end))` should be used instead.

### Indexing MaskedArrays/Constants with `...` (ellipsis) now returns MaskedArray

This behavior mirrors that of np.ndarray, and accounts for nested arrays in MaskedArrays of object dtype, and ellipsis combined with other forms of indexing.

## C API changes

### GUfuncs on empty arrays and NpyIter axis removal

It is now allowed to remove a zero-sized axis from NpyIter. Which may mean that code removing axes from NpyIter has to add an additional check when accessing the removed dimensions later on.

The largest followup change is that gufuncs are now allowed to have zero-sized inner dimensions. This means that a gufunc now has to anticipate an empty inner dimension, while this was never possible and an error raised instead.

For most gufuncs no change should be necessary. However, it is now possible for gufuncs with a signature such as `(..., N, M) -> (..., M)` to return a valid result if `N=0` without further wrapping code.

### `PyArray_MapIterArrayCopyIfOverlap` added to NumPy C-API

Similar to `PyArray_MapIterArray` but with an additional `copy_if_overlap` argument. If `copy_if_overlap != 0`, checks if input has memory overlap with any of the other arrays and make copies as appropriate to avoid problems if the input is modified during the iteration. See the documentation for more complete documentation.

## New Features

### `__array_ufunc__` added

This is the renamed and redesigned `__numpy_ufunc__`. Any class, ndarray subclass or not, can define this method or set it to `None` in order to override the behavior of NumPy's ufuncs. This works quite similarly to Python's `__mul__` and other binary operation routines. See the documentation for a more detailed description of the implementation and behavior of this new option. The API is provisional, we do not yet guarantee backward compatibility as modifications may be made pending feedback. See [NEP 13](http://www.numpy.org/neps/nep-0013-ufunc-overrides.html) and [documentation](https://github.com/numpy/numpy/blob/master/doc/source/reference/arrays.classes.md) for more details.

### New `positive` ufunc

This ufunc corresponds to unary <span class="title-ref">+</span>, but unlike <span class="title-ref">+</span> on an ndarray it will raise an error if array values do not support numeric operations.

### New `divmod` ufunc

This ufunc corresponds to the Python builtin <span class="title-ref">divmod</span>, and is used to implement <span class="title-ref">divmod</span> when called on numpy arrays. `np.divmod(x, y)` calculates a result equivalent to `(np.floor_divide(x, y), np.remainder(x, y))` but is approximately twice as fast as calling the functions separately.

### `np.isnat` ufunc tests for NaT special datetime and timedelta values

The new ufunc `np.isnat` finds the positions of special NaT values within datetime and timedelta arrays. This is analogous to `np.isnan`.

### `np.heaviside` ufunc computes the Heaviside function

The new function `np.heaviside(x, h0)` (a ufunc) computes the Heaviside function:

    { 0   if x < 0,
    heaviside(x, h0) = { h0  if x == 0,
    { 1   if x > 0.

### `np.block` function for creating blocked arrays

Add a new `block` function to the current stacking functions `vstack`, `hstack`, and `stack`. This allows concatenation across multiple axes simultaneously, with a similar syntax to array creation, but where elements can themselves be arrays. For instance:

    >>> A = np.eye(2) * 2
    >>> B = np.eye(3) * 3
    >>> np.block([
    ...     [A,               np.zeros((2, 3))],
    ...     [np.ones((3, 2)), B               ]
    ... ])
    array([[ 2.,  0.,  0.,  0.,  0.],
           [ 0.,  2.,  0.,  0.,  0.],
           [ 1.,  1.,  3.,  0.,  0.],
           [ 1.,  1.,  0.,  3.,  0.],
           [ 1.,  1.,  0.,  0.,  3.]])

While primarily useful for block matrices, this works for arbitrary dimensions of arrays.

It is similar to Matlab's square bracket notation for creating block matrices.

### `isin` function, improving on `in1d`

The new function `isin` tests whether each element of an N-dimensional array is present anywhere within a second array. It is an enhancement of `in1d` that preserves the shape of the first array.

### Temporary elision

On platforms providing the `backtrace` function NumPy will try to avoid creating temporaries in expression involving basic numeric types. For example `d = a + b + c` is transformed to `d = a + b; d += c` which can improve performance for large arrays as less memory bandwidth is required to perform the operation.

### `axes` argument for `unique`

In an N-dimensional array, the user can now choose the axis along which to look for duplicate N-1-dimensional elements using `numpy.unique`. The original behaviour is recovered if `axis=None` (default).

### `np.gradient` now supports unevenly spaced data

Users can now specify a not-constant spacing for data. In particular `np.gradient` can now take:

1.  A single scalar to specify a sample distance for all dimensions.
2.  N scalars to specify a constant sample distance for each dimension. i.e. `dx`, `dy`, `dz`, ...
3.  N arrays to specify the coordinates of the values along each dimension of F. The length of the array must match the size of the corresponding dimension
4.  Any combination of N scalars/arrays with the meaning of 2. and 3.

This means that, e.g., it is now possible to do the following:

    >>> f = np.array([[1, 2, 6], [3, 4, 5]], dtype=np.float_)
    >>> dx = 2.
    >>> y = [1., 1.5, 3.5]
    >>> np.gradient(f, dx, y)
    [array([[ 1. ,  1. , -0.5], [ 1. ,  1. , -0.5]]),
     array([[ 2. ,  2. ,  2. ], [ 2. ,  1.7,  0.5]])]

### Support for returning arrays of arbitrary dimensions in `apply_along_axis`

Previously, only scalars or 1D arrays could be returned by the function passed to `apply_along_axis`. Now, it can return an array of any dimensionality (including 0D), and the shape of this array replaces the axis of the array being iterated over.

### `.ndim` property added to `dtype` to complement `.shape`

For consistency with `ndarray` and `broadcast`, `d.ndim` is a shorthand for `len(d.shape)`.

### Support for tracemalloc in Python 3.6

NumPy now supports memory tracing with [tracemalloc](https://docs.python.org/3/library/tracemalloc.html) module of Python 3.6 or newer. Memory allocations from NumPy are placed into the domain defined by `numpy.lib.tracemalloc_domain`. Note that NumPy allocation will not show up in [tracemalloc](https://docs.python.org/3/library/tracemalloc.html) of earlier Python versions.

### NumPy may be built with relaxed stride checking debugging

Setting NPY\_RELAXED\_STRIDES\_DEBUG=1 in the environment when relaxed stride checking is enabled will cause NumPy to be compiled with the affected strides set to the maximum value of npy\_intp in order to help detect invalid usage of the strides in downstream projects. When enabled, invalid usage often results in an error being raised, but the exact type of error depends on the details of the code. TypeError and OverflowError have been observed in the wild.

It was previously the case that this option was disabled for releases and enabled in master and changing between the two required editing the code. It is now disabled by default but can be enabled for test builds.

## Improvements

### Ufunc behavior for overlapping inputs

Operations where ufunc input and output operands have memory overlap produced undefined results in previous NumPy versions, due to data dependency issues. In NumPy 1.13.0, results from such operations are now defined to be the same as for equivalent operations where there is no memory overlap.

Operations affected now make temporary copies, as needed to eliminate data dependency. As detecting these cases is computationally expensive, a heuristic is used, which may in rare cases result to needless temporary copies. For operations where the data dependency is simple enough for the heuristic to analyze, temporary copies will not be made even if the arrays overlap, if it can be deduced copies are not necessary. As an example,`np.add(a, b, out=a)` will not involve copies.

To illustrate a previously undefined operation:

    >>> x = np.arange(16).astype(float)
    >>> np.add(x[1:], x[:-1], out=x[1:])

In NumPy 1.13.0 the last line is guaranteed to be equivalent to:

    >>> np.add(x[1:].copy(), x[:-1].copy(), out=x[1:])

A similar operation with simple non-problematic data dependence is:

    >>> x = np.arange(16).astype(float)
    >>> np.add(x[1:], x[:-1], out=x[:-1])

It will continue to produce the same results as in previous NumPy versions, and will not involve unnecessary temporary copies.

The change applies also to in-place binary operations, for example:

    >>> x = np.random.rand(500, 500)
    >>> x += x.T

This statement is now guaranteed to be equivalent to `x[...] = x + x.T`, whereas in previous NumPy versions the results were undefined.

### Partial support for 64-bit f2py extensions with MinGW

Extensions that incorporate Fortran libraries can now be built using the free [MinGW](https://sf.net/projects/mingw-w64/files/Toolchains%20targetting%20Win64/Personal%20Builds/mingw-builds/6.2.0/threads-win32/seh/) toolset, also under Python 3.5. This works best for extensions that only do calculations and uses the runtime modestly (reading and writing from files, for instance). Note that this does not remove the need for Mingwpy; if you make extensive use of the runtime, you will most likely run into [issues](https://mingwpy.github.io/issues.html). Instead, it should be regarded as a band-aid until Mingwpy is fully functional.

Extensions can also be compiled using the MinGW toolset using the runtime library from the (moveable) WinPython 3.4 distribution, which can be useful for programs with a PySide1/Qt4 front-end.

### Performance improvements for `packbits` and `unpackbits`

The functions `numpy.packbits` with boolean input and `numpy.unpackbits` have been optimized to be a significantly faster for contiguous data.

### Fix for PPC long double floating point information

In previous versions of NumPy, the `finfo` function returned invalid information about the [double double](https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format#Double-double_arithmetic) format of the `longdouble` float type on Power PC (PPC). The invalid values resulted from the failure of the NumPy algorithm to deal with the variable number of digits in the significand that are a feature of <span class="title-ref">PPC long doubles</span>. This release by-passes the failing algorithm by using heuristics to detect the presence of the PPC double double format. A side-effect of using these heuristics is that the `finfo` function is faster than previous releases.

### Better default repr for `ndarray` subclasses

Subclasses of ndarray with no `repr` specialization now correctly indent their data and type lines.

### More reliable comparisons of masked arrays

Comparisons of masked arrays were buggy for masked scalars and failed for structured arrays with dimension higher than one. Both problems are now solved. In the process, it was ensured that in getting the result for a structured array, masked fields are properly ignored, i.e., the result is equal if all fields that are non-masked in both are equal, thus making the behaviour identical to what one gets by comparing an unstructured masked array and then doing `.all()` over some axis.

### np.matrix with booleans elements can now be created using the string syntax

`np.matrix` failed whenever one attempts to use it with booleans, e.g., `np.matrix('True')`. Now, this works as expected.

### More `linalg` operations now accept empty vectors and matrices

All of the following functions in `np.linalg` now work when given input arrays with a 0 in the last two dimensions: `det`, `slogdet`, `pinv`, `eigvals`, `eigvalsh`, `eig`, `eigh`.

### Bundled version of LAPACK is now 3.2.2

NumPy comes bundled with a minimal implementation of lapack for systems without a lapack library installed, under the name of `lapack_lite`. This has been upgraded from LAPACK 3.0.0 (June 30, 1999) to LAPACK 3.2.2 (June 30, 2010). See the [LAPACK changelogs](http://www.netlib.org/lapack/release_notes.html#_4_history_of_lapack_releases) for details on the all the changes this entails.

While no new features are exposed through `numpy`, this fixes some bugs regarding "workspace" sizes, and in some places may use faster algorithms.

### `reduce` of `np.hypot.reduce` and `np.logical_xor` allowed in more cases

This now works on empty arrays, returning 0, and can reduce over multiple axes. Previously, a `ValueError` was thrown in these cases.

### Better `repr` of object arrays

Object arrays that contain themselves no longer cause a recursion error.

Object arrays that contain `list` objects are now printed in a way that makes clear the difference between a 2d object array, and a 1d object array of lists.

## Changes

### `argsort` on masked arrays takes the same default arguments as `sort`

By default, `argsort` now places the masked values at the end of the sorted array, in the same way that `sort` already did. Additionally, the `end_with` argument is added to `argsort`, for consistency with `sort`. Note that this argument is not added at the end, so breaks any code that passed `fill_value` as a positional argument.

### `average` now preserves subclasses

For ndarray subclasses, `numpy.average` will now return an instance of the subclass, matching the behavior of most other NumPy functions such as `mean`. As a consequence, also calls that returned a scalar may now return a subclass array scalar.

### `array == None` and `array != None` do element-wise comparison

Previously these operations returned scalars `False` and `True` respectively.

### `np.equal, np.not_equal` for object arrays ignores object identity

Previously, these functions always treated identical objects as equal. This had the effect of overriding comparison failures, comparison of objects that did not return booleans, such as np.arrays, and comparison of objects where the results differed from object identity, such as NaNs.

### Boolean indexing changes

  - Boolean array-likes (such as lists of python bools) are always treated as boolean indexes.
  - Boolean scalars (including python `True`) are legal boolean indexes and never treated as integers.
  - Boolean indexes must match the dimension of the axis that they index.
  - Boolean indexes used on the lhs of an assignment must match the dimensions of the rhs.
  - Boolean indexing into scalar arrays return a new 1-d array. This means that `array(1)[array(True)]` gives `array([1])` and not the original array.

### `np.random.multivariate_normal` behavior with bad covariance matrix

It is now possible to adjust the behavior the function will have when dealing with the covariance matrix by using two new keyword arguments:

  - `tol` can be used to specify a tolerance to use when checking that the covariance matrix is positive semidefinite.
  - `check_valid` can be used to configure what the function will do in the presence of a matrix that is not positive semidefinite. Valid options are `ignore`, `warn` and `raise`. The default value, `warn` keeps the the behavior used on previous releases.

### `assert_array_less` compares `np.inf` and `-np.inf` now

Previously, `np.testing.assert_array_less` ignored all infinite values. This is not the expected behavior both according to documentation and intuitively. Now, -inf \< x \< inf is considered `True` for any real number x and all other cases fail.

### `assert_array_` and masked arrays `assert_equal` hide less warnings

Some warnings that were previously hidden by the `assert_array_` functions are not hidden anymore. In most cases the warnings should be correct and, should they occur, will require changes to the tests using these functions. For the masked array `assert_equal` version, warnings may occur when comparing NaT. The function presently does not handle NaT or NaN specifically and it may be best to avoid it at this time should a warning show up due to this change.

### `offset` attribute value in `memmap` objects

The `offset` attribute in a `memmap` object is now set to the offset into the file. This is a behaviour change only for offsets greater than `mmap.ALLOCATIONGRANULARITY`.

### `np.real` and `np.imag` return scalars for scalar inputs

Previously, `np.real` and `np.imag` used to return array objects when provided a scalar input, which was inconsistent with other functions like `np.angle` and `np.conj`.

### The polynomial convenience classes cannot be passed to ufuncs

The ABCPolyBase class, from which the convenience classes are derived, sets `__array_ufun__ = None` in order of opt out of ufuncs. If a polynomial convenience class instance is passed as an argument to a ufunc, a `TypeError` will now be raised.

### Output arguments to ufuncs can be tuples also for ufunc methods

For calls to ufuncs, it was already possible, and recommended, to use an `out` argument with a tuple for ufuncs with multiple outputs. This has now been extended to output arguments in the `reduce`, `accumulate`, and `reduceat` methods. This is mostly for compatibility with `__array_ufunc`; there are no ufuncs yet that have more than one output.

---

1.13.1-notes.md

---

# NumPy 1.13.1 Release Notes

This is a bugfix release for problems found in 1.13.0. The major changes are fixes for the new memory overlap detection and temporary elision as well as reversion of the removal of the boolean binary `-` operator. Users of 1.13.0 should upgrade.

Thr Python versions supported are 2.7 and 3.4 - 3.6. Note that the Python 3.6 wheels available from PIP are built against 3.6.1, hence will not work when used with 3.6.0 due to Python bug [29943](https://bugs.python.org/issue29943). NumPy 1.13.2 will be released shortly after Python 3.6.2 is out to fix that problem. If you are using 3.6.0 the workaround is to upgrade to 3.6.1 or use an earlier Python version.

## Pull requests merged

A total of 19 pull requests were merged for this release.

  - \#9240 DOC: BLD: fix lots of Sphinx warnings/errors.
  - \#9255 Revert "DEP: Raise TypeError for subtract(bool, bool)."
  - \#9261 BUG: don't elide into readonly and updateifcopy temporaries for...
  - \#9262 BUG: fix missing keyword rename for common block in numpy.f2py
  - \#9263 BUG: handle resize of 0d array
  - \#9267 DOC: update f2py front page and some doc build metadata.
  - \#9299 BUG: Fix Intel compilation on Unix.
  - \#9317 BUG: fix wrong ndim used in empty where check
  - \#9319 BUG: Make extensions compilable with MinGW on Py2.7
  - \#9339 BUG: Prevent crash if ufunc doc string is null
  - \#9340 BUG: umath: un-break ufunc where= when no out= is given
  - \#9371 DOC: Add isnat/positive ufunc to documentation
  - \#9372 BUG: Fix error in fromstring function from numpy.core.records...
  - \#9373 BUG: ')' is printed at the end pointer of the buffer in numpy.f2py.
  - \#9374 DOC: Create NumPy 1.13.1 release notes.
  - \#9376 BUG: Prevent hang traversing ufunc userloop linked list
  - \#9377 DOC: Use x1 and x2 in the heaviside docstring.
  - \#9378 DOC: Add $PARAMS to the isnat docstring
  - \#9379 DOC: Update the 1.13.1 release notes

## Contributors

A total of 12 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Andras Deak +
  - Bob Eldering +
  - Charles Harris
  - Daniel Hrisca +
  - Eric Wieser
  - Joshua Leahy +
  - Julian Taylor
  - Michael Seifert
  - Pauli Virtanen
  - Ralf Gommers
  - Roland Kaufmann
  - Warren Weckesser

---

1.13.2-notes.md

---

# NumPy 1.13.2 Release Notes

This is a bugfix release for some problems found since 1.13.1. The most important fixes are for CVE-2017-12852 and temporary elision. Users of earlier versions of 1.13 should upgrade.

The Python versions supported are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. The Windows wheels are now built with OpenBlas instead ATLAS, which should improve the performance of the linear algebra functions.

## Contributors

A total of 12 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Brandon Carter
  - Charles Harris
  - Eric Wieser
  - Iryna Shcherbina +
  - James Bourbeau +
  - Jonathan Helmus
  - Julian Taylor
  - Matti Picus
  - Michael Lamparski +
  - Michael Seifert
  - Ralf Gommers

## Pull requests merged

A total of 20 pull requests were merged for this release.

  - \#9390 BUG: Return the poly1d coefficients array directly
  - \#9555 BUG: Fix regression in 1.13.x in distutils.mingw32ccompiler.
  - \#9556 BUG: Fix true\_divide when dtype=np.float64 specified.
  - \#9557 DOC: Fix some rst markup in numpy/doc/basics.py.
  - \#9558 BLD: Remove -xhost flag from IntelFCompiler.
  - \#9559 DOC: Removes broken docstring example (source code, png, pdf)...
  - \#9580 BUG: Add hypot and cabs functions to WIN32 blacklist.
  - \#9732 BUG: Make scalar function elision check if temp is writeable.
  - \#9736 BUG: Various fixes to np.gradient
  - \#9742 BUG: Fix np.pad for CVE-2017-12852
  - \#9744 BUG: Check for exception in sort functions, add tests
  - \#9745 DOC: Add whitespace after "versionadded::" directive so it actually...
  - \#9746 BUG: Memory leak in np.dot of size 0
  - \#9747 BUG: Adjust gfortran version search regex
  - \#9757 BUG: Cython 0.27 breaks NumPy on Python 3.
  - \#9764 BUG: Ensure <span class="title-ref">\_npy\_scaled\_cexp{,f,l}</span> is defined when needed.
  - \#9765 BUG: PyArray\_CountNonzero does not check for exceptions
  - \#9766 BUG: Fixes histogram monotonicity check for unsigned bin values
  - \#9767 BUG: Ensure consistent result dtype of count\_nonzero
  - \#9771 BUG, MAINT: Fix mtrand for Cython 0.27.

---

1.13.3-notes.md

---

# NumPy 1.13.3 Release Notes

This is a bugfix release for some problems found since 1.13.1. The most important fixes are for CVE-2017-12852 and temporary elision. Users of earlier versions of 1.13 should upgrade.

The Python versions supported are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. It was cythonized with Cython 0.26.1, which should be free of the bugs found in 0.27 while also being compatible with Python 3.7-dev. The Windows wheels were built with OpenBlas instead ATLAS, which should improve the performance of the linear algebra functions.

The NumPy 1.13.3 release is a re-release of 1.13.2, which suffered from a bug in Cython 0.27.0.

## Contributors

A total of 12 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Brandon Carter
  - Charles Harris
  - Eric Wieser
  - Iryna Shcherbina +
  - James Bourbeau +
  - Jonathan Helmus
  - Julian Taylor
  - Matti Picus
  - Michael Lamparski +
  - Michael Seifert
  - Ralf Gommers

## Pull requests merged

A total of 22 pull requests were merged for this release.

  - \#9390 BUG: Return the poly1d coefficients array directly
  - \#9555 BUG: Fix regression in 1.13.x in distutils.mingw32ccompiler.
  - \#9556 BUG: Fix true\_divide when dtype=np.float64 specified.
  - \#9557 DOC: Fix some rst markup in numpy/doc/basics.py.
  - \#9558 BLD: Remove -xhost flag from IntelFCompiler.
  - \#9559 DOC: Removes broken docstring example (source code, png, pdf)...
  - \#9580 BUG: Add hypot and cabs functions to WIN32 blacklist.
  - \#9732 BUG: Make scalar function elision check if temp is writeable.
  - \#9736 BUG: Various fixes to np.gradient
  - \#9742 BUG: Fix np.pad for CVE-2017-12852
  - \#9744 BUG: Check for exception in sort functions, add tests
  - \#9745 DOC: Add whitespace after "versionadded::" directive so it actually...
  - \#9746 BUG: Memory leak in np.dot of size 0
  - \#9747 BUG: Adjust gfortran version search regex
  - \#9757 BUG: Cython 0.27 breaks NumPy on Python 3.
  - \#9764 BUG: Ensure <span class="title-ref">\_npy\_scaled\_cexp{,f,l}</span> is defined when needed.
  - \#9765 BUG: PyArray\_CountNonzero does not check for exceptions
  - \#9766 BUG: Fixes histogram monotonicity check for unsigned bin values
  - \#9767 BUG: Ensure consistent result dtype of count\_nonzero
  - \#9771 BUG: MAINT: Fix mtrand for Cython 0.27.
  - \#9772 DOC: Create the 1.13.2 release notes.
  - \#9794 DOC: Create 1.13.3 release notes.

---

1.14.0-notes.md

---

# NumPy 1.14.0 Release Notes

Numpy 1.14.0 is the result of seven months of work and contains a large number of bug fixes and new features, along with several changes with potential compatibility issues. The major change that users will notice are the stylistic changes in the way numpy arrays and scalars are printed, a change that will affect doctests. See below for details on how to preserve the old style printing when needed.

A major decision affecting future development concerns the schedule for dropping Python 2.7 support in the runup to 2020. The decision has been made to support 2.7 for all releases made in 2018, with the last release being designated a long term release with support for bug fixes extending through 2019. In 2019 support for 2.7 will be dropped in all new releases. More details can be found in [NEP 12](http://www.numpy.org/neps/nep-0014-dropping-python2.7-proposal.html).

This release supports Python 2.7 and 3.4 - 3.6.

## Highlights

  - The <span class="title-ref">np.einsum</span> function uses BLAS when possible
  - `genfromtxt`, `loadtxt`, `fromregex` and `savetxt` can now handle files with arbitrary Python supported encoding.
  - Major improvements to printing of NumPy arrays and scalars.

## New functions

  - `parametrize`: decorator added to numpy.testing
  - `chebinterpolate`: Interpolate function at Chebyshev points.
  - `format_float_positional` and `format_float_scientific` : format floating-point scalars unambiguously with control of rounding and padding.
  - `PyArray_ResolveWritebackIfCopy` and `PyArray_SetWritebackIfCopyBase`, new C-API functions useful in achieving PyPy compatibility.

## Deprecations

  - Using `np.bool_` objects in place of integers is deprecated. Previously `operator.index(np.bool_)` was legal and allowed constructs such as `[1, 2, 3][np.True_]`. That was misleading, as it behaved differently from `np.array([1, 2, 3])[np.True_]`.
  - Truth testing of an empty array is deprecated. To check if an array is not empty, use `array.size > 0`.
  - Calling `np.bincount` with `minlength=None` is deprecated. `minlength=0` should be used instead.
  - Calling `np.fromstring` with the default value of the `sep` argument is deprecated. When that argument is not provided, a broken version of `np.frombuffer` is used that silently accepts unicode strings and -- after encoding them as either utf-8 (python 3) or the default encoding (python 2) -- treats them as binary data. If reading binary data is desired, `np.frombuffer` should be used directly.
  - The `style` option of array2string is deprecated in non-legacy printing mode.
  - `PyArray_SetUpdateIfCopyBase` has been deprecated. For NumPy versions \>= 1.14 use `PyArray_SetWritebackIfCopyBase` instead, see <span class="title-ref">C API changes</span> below for more details.
  - The use of `UPDATEIFCOPY` arrays is deprecated, see <span class="title-ref">C API changes</span> below for details. We will not be dropping support for those arrays, but they are not compatible with PyPy.

## Future Changes

  - `np.issubdtype` will stop downcasting dtype-like arguments. It might be expected that `issubdtype(np.float32, 'float64')` and `issubdtype(np.float32, np.float64)` mean the same thing - however, there was an undocumented special case that translated the former into `issubdtype(np.float32, np.floating)`, giving the surprising result of True.
    
    This translation now gives a warning that explains what translation is occurring. In the future, the translation will be disabled, and the first example will be made equivalent to the second.

  - `np.linalg.lstsq` default for `rcond` will be changed. The `rcond` parameter to `np.linalg.lstsq` will change its default to machine precision times the largest of the input array dimensions. A FutureWarning is issued when `rcond` is not passed explicitly.

  - `a.flat.__array__()` will return a writeable copy of `a` when `a` is non-contiguous. Previously it returned an UPDATEIFCOPY array when `a` was writeable. Currently it returns a non-writeable copy. See gh-7054 for a discussion of the issue.

  - Unstructured void array's `.item` method will return a bytes object. In the future, calling `.item()` on arrays or scalars of `np.void` datatype will return a `bytes` object instead of a buffer or int array, the same as returned by `bytes(void_scalar)`. This may affect code which assumed the return value was mutable, which will no longer be the case. A `FutureWarning` is now issued when this would occur.

## Compatibility notes

### The mask of a masked array view is also a view rather than a copy

There was a FutureWarning about this change in NumPy 1.11.x. In short, it is now the case that, when changing a view of a masked array, changes to the mask are propagated to the original. That was not previously the case. This change affects slices in particular. Note that this does not yet work properly if the mask of the original array is `nomask` and the mask of the view is changed. See gh-5580 for an extended discussion. The original behavior of having a copy of the mask can be obtained by calling the `unshare_mask` method of the view.

### `np.ma.masked` is no longer writeable

Attempts to mutate the `masked` constant now error, as the underlying arrays are marked readonly. In the past, it was possible to get away with:

    # emulating a function that sometimes returns np.ma.masked
    val = random.choice([np.ma.masked, 10])
    var_arr = np.asarray(val)
    val_arr += 1  # now errors, previously changed np.ma.masked.data

### `np.ma` functions producing `fill_value` s have changed

Previously, `np.ma.default_fill_value` would return a 0d array, but `np.ma.minimum_fill_value` and `np.ma.maximum_fill_value` would return a tuple of the fields. Instead, all three methods return a structured `np.void` object, which is what you would already find in the `.fill_value` attribute.

Additionally, the dtype guessing now matches that of `np.array` - so when passing a python scalar `x`, `maximum_fill_value(x)` is always the same as `maximum_fill_value(np.array(x))`. Previously `x = long(1)` on Python 2 violated this assumption.

### `a.flat.__array__()` returns non-writeable arrays when `a` is non-contiguous

The intent is that the UPDATEIFCOPY array previously returned when `a` was non-contiguous will be replaced by a writeable copy in the future. This temporary measure is aimed to notify folks who expect the underlying array be modified in this situation that that will no longer be the case. The most likely places for this to be noticed is when expressions of the form `np.asarray(a.flat)` are used, or when `a.flat` is passed as the out parameter to a ufunc.

### `np.tensordot` now returns zero array when contracting over 0-length dimension

Previously `np.tensordot` raised a ValueError when contracting over 0-length dimension. Now it returns a zero array, which is consistent with the behaviour of `np.dot` and `np.einsum`.

### `numpy.testing` reorganized

This is not expected to cause problems, but possibly something has been left out. If you experience an unexpected import problem using `numpy.testing` let us know.

### `np.asfarray` no longer accepts non-dtypes through the `dtype` argument

This previously would accept `dtype=some_array`, with the implied semantics of `dtype=some_array.dtype`. This was undocumented, unique across the numpy functions, and if used would likely correspond to a typo.

### 1D `np.linalg.norm` preserves float input types, even for arbitrary orders

Previously, this would promote to `float64` when arbitrary orders were passed, despite not doing so under the simple cases:

    >>> f32 = np.float32([[1, 2]])
    >>> np.linalg.norm(f32, 2.0, axis=-1).dtype
    dtype('float32')
    >>> np.linalg.norm(f32, 2.0001, axis=-1).dtype
    dtype('float64')  # numpy 1.13
    dtype('float32')  # numpy 1.14

This change affects only `float32` and `float16` arrays.

### `count_nonzero(arr, axis=())` now counts over no axes, not all axes

Elsewhere, `axis==()` is always understood as "no axes", but <span class="title-ref">count\_nonzero</span> had a special case to treat this as "all axes". This was inconsistent and surprising. The correct way to count over all axes has always been to pass `axis == None`.

### `__init__.py` files added to test directories

This is for pytest compatibility in the case of duplicate test file names in the different directories. As a result, `run_module_suite` no longer works, i.e., `python <path-to-test-file>` results in an error.

### `.astype(bool)` on unstructured void arrays now calls `bool` on each element

On Python 2, `void_array.astype(bool)` would always return an array of `True`, unless the dtype is `V0`. On Python 3, this operation would usually crash. Going forwards, <span class="title-ref">astype</span> matches the behavior of `bool(np.void)`, considering a buffer of all zeros as false, and anything else as true. Checks for `V0` can still be done with `arr.dtype.itemsize == 0`.

### `MaskedArray.squeeze` never returns `np.ma.masked`

`np.squeeze` is documented as returning a view, but the masked variant would sometimes return `masked`, which is not a view. This has been fixed, so that the result is always a view on the original masked array. This breaks any code that used `masked_arr.squeeze() is np.ma.masked`, but fixes code that writes to the result of <span class="title-ref">.squeeze()</span>.

### Renamed first parameter of `can_cast` from `from` to `from_`

The previous parameter name `from` is a reserved keyword in Python, which made it difficult to pass the argument by name. This has been fixed by renaming the parameter to `from_`.

### `isnat` raises `TypeError` when passed wrong type

The ufunc `isnat` used to raise a `ValueError` when it was not passed variables of type `datetime` or `timedelta`. This has been changed to raising a `TypeError`.

### `dtype.__getitem__` raises `TypeError` when passed wrong type

When indexed with a float, the dtype object used to raise `ValueError`.

### User-defined types now need to implement `__str__` and `__repr__`

Previously, user-defined types could fall back to a default implementation of `__str__` and `__repr__` implemented in numpy, but this has now been removed. Now user-defined types will fall back to the python default `object.__str__` and `object.__repr__`.

### Many changes to array printing, disableable with the new "legacy" printing mode

The `str` and `repr` of ndarrays and numpy scalars have been changed in a variety of ways. These changes are likely to break downstream user's doctests.

These new behaviors can be disabled to mostly reproduce numpy 1.13 behavior by enabling the new 1.13 "legacy" printing mode. This is enabled by calling `np.set_printoptions(legacy="1.13")`, or using the new `legacy` argument to `np.array2string`, as `np.array2string(arr, legacy='1.13')`.

In summary, the major changes are:

  - For floating-point types:
      - The `repr` of float arrays often omits a space previously printed in the sign position. See the new `sign` option to `np.set_printoptions`.
      - Floating-point arrays and scalars use a new algorithm for decimal representations, giving the shortest unique representation. This will usually shorten `float16` fractional output, and sometimes `float32` and `float128` output. `float64` should be unaffected. See the new `floatmode` option to `np.set_printoptions`.
      - Float arrays printed in scientific notation no longer use fixed-precision, and now instead show the shortest unique representation.
      - The `str` of floating-point scalars is no longer truncated in python2.
  - For other data types:
      - Non-finite complex scalars print like `nanj` instead of `nan*j`.
      - `NaT` values in datetime arrays are now properly aligned.
      - Arrays and scalars of `np.void` datatype are now printed using hex notation.
  - For line-wrapping:
      - The "dtype" part of ndarray reprs will now be printed on the next line if there isn't space on the last line of array output.
      - The `linewidth` format option is now always respected. The <span class="title-ref">repr</span> or <span class="title-ref">str</span> of an array will never exceed this, unless a single element is too wide.
      - The last line of an array string will never have more elements than earlier lines.
      - An extra space is no longer inserted on the first line if the elements are too wide.
  - For summarization (the use of `...` to shorten long arrays):
      - A trailing comma is no longer inserted for `str`. Previously, `str(np.arange(1001))` gave `'[   0    1    2 ...,  998  999 1000]'`, which has an extra comma.
      - For arrays of 2-D and beyond, when `...` is printed on its own line in order to summarize any but the last axis, newlines are now appended to that line to match its leading newlines and a trailing space character is removed.
  - `MaskedArray` arrays now separate printed elements with commas, always print the dtype, and correctly wrap the elements of long arrays to multiple lines. If there is more than 1 dimension, the array attributes are now printed in a new "left-justified" printing style.
  - `recarray` arrays no longer print a trailing space before their dtype, and wrap to the right number of columns.
  - 0d arrays no longer have their own idiosyncratic implementations of `str` and `repr`. The `style` argument to `np.array2string` is deprecated.
  - Arrays of `bool` datatype will omit the datatype in the `repr`.
  - User-defined `dtypes` (subclasses of `np.generic`) now need to implement `__str__` and `__repr__`.

Some of these changes are described in more detail below. If you need to retain the previous behavior for doctests or other reasons, you may want to do something like:

    # FIXME: We need the str/repr formatting used in Numpy < 1.14.
    try:
        np.set_printoptions(legacy='1.13')
    except TypeError:
        pass

## C API changes

### PyPy compatible alternative to `UPDATEIFCOPY` arrays

`UPDATEIFCOPY` arrays are contiguous copies of existing arrays, possibly with different dimensions, whose contents are copied back to the original array when their refcount goes to zero and they are deallocated. Because PyPy does not use refcounts, they do not function correctly with PyPy. NumPy is in the process of eliminating their use internally and two new C-API functions,

  - `PyArray_SetWritebackIfCopyBase`
  - `PyArray_ResolveWritebackIfCopy`,

have been added together with a complementary flag, `NPY_ARRAY_WRITEBACKIFCOPY`. Using the new functionality also requires that some flags be changed when new arrays are created, to wit: `NPY_ARRAY_INOUT_ARRAY` should be replaced by `NPY_ARRAY_INOUT_ARRAY2` and `NPY_ARRAY_INOUT_FARRAY` should be replaced by `NPY_ARRAY_INOUT_FARRAY2`. Arrays created with these new flags will then have the `WRITEBACKIFCOPY` semantics.

If PyPy compatibility is not a concern, these new functions can be ignored, although there will be a `DeprecationWarning`. If you do wish to pursue PyPy compatibility, more information on these functions and their use may be found in the [c-api](https://github.com/numpy/numpy/blob/master/doc/source/reference/c-api.array.md) documentation and the example in [how-to-extend](https://github.com/numpy/numpy/blob/master/doc/source/user/c-info.how-to-extend.md).

## New Features

### Encoding argument for text IO functions

`genfromtxt`, `loadtxt`, `fromregex` and `savetxt` can now handle files with arbitrary encoding supported by Python via the encoding argument. For backward compatibility the argument defaults to the special `bytes` value which continues to treat text as raw byte values and continues to pass latin1 encoded bytes to custom converters. Using any other value (including `None` for system default) will switch the functions to real text IO so one receives unicode strings instead of bytes in the resulting arrays.

### External `nose` plugins are usable by `numpy.testing.Tester`

`numpy.testing.Tester` is now aware of `nose` plugins that are outside the `nose` built-in ones. This allows using, for example, `nose-timer` like so: `np.test(extra_argv=['--with-timer', '--timer-top-n', '20'])` to obtain the runtime of the 20 slowest tests. An extra keyword `timer` was also added to `Tester.test`, so `np.test(timer=20)` will also report the 20 slowest tests.

### `parametrize` decorator added to `numpy.testing`

A basic `parametrize` decorator is now available in `numpy.testing`. It is intended to allow rewriting yield based tests that have been deprecated in pytest so as to facilitate the transition to pytest in the future. The nose testing framework has not been supported for several years and looks like abandonware.

The new `parametrize` decorator does not have the full functionality of the one in pytest. It doesn't work for classes, doesn't support nesting, and does not substitute variable names. Even so, it should be adequate to rewrite the NumPy tests.

### `chebinterpolate` function added to `numpy.polynomial.chebyshev`

The new `chebinterpolate` function interpolates a given function at the Chebyshev points of the first kind. A new `Chebyshev.interpolate` class method adds support for interpolation over arbitrary intervals using the scaled and shifted Chebyshev points of the first kind.

### Support for reading lzma compressed text files in Python 3

With Python versions containing the `lzma` module the text IO functions can now transparently read from files with `xz` or `lzma` extension.

### `sign` option added to `np.setprintoptions` and `np.array2string`

This option controls printing of the sign of floating-point types, and may be one of the characters '-', '+' or ' '. With '+' numpy always prints the sign of positive values, with ' ' it always prints a space (whitespace character) in the sign position of positive values, and with '-' it will omit the sign character for positive values. The new default is '-'.

This new default changes the float output relative to numpy 1.13. The old behavior can be obtained in 1.13 "legacy" printing mode, see compatibility notes above.

### `hermitian` option added to`np.linalg.matrix_rank`

The new `hermitian` option allows choosing between standard SVD based matrix rank calculation and the more efficient eigenvalue based method for symmetric/hermitian matrices.

### `threshold` and `edgeitems` options added to `np.array2string`

These options could previously be controlled using `np.set_printoptions`, but now can be changed on a per-call basis as arguments to `np.array2string`.

### `concatenate` and `stack` gained an `out` argument

A preallocated buffer of the desired dtype can now be used for the output of these functions.

### Support for PGI flang compiler on Windows

The PGI flang compiler is a Fortran front end for LLVM released by NVIDIA under the Apache 2 license. It can be invoked by :

    python setup.py config --compiler=clang --fcompiler=flang install

There is little experience with this new compiler, so any feedback from people using it will be appreciated.

## Improvements

### Numerator degrees of freedom in `random.noncentral_f` need only be positive.

Prior to NumPy 1.14.0, the numerator degrees of freedom needed to be \> 1, but the distribution is valid for values \> 0, which is the new requirement.

### The GIL is released for all `np.einsum` variations

Some specific loop structures which have an accelerated loop version did not release the GIL prior to NumPy 1.14.0. This oversight has been fixed.

### The <span class="title-ref">np.einsum</span> function will use BLAS when possible and optimize by default

The `np.einsum` function will now call `np.tensordot` when appropriate. Because `np.tensordot` uses BLAS when possible, that will speed up execution. By default, `np.einsum` will also attempt optimization as the overhead is small relative to the potential improvement in speed.

### `f2py` now handles arrays of dimension 0

`f2py` now allows for the allocation of arrays of dimension 0. This allows for more consistent handling of corner cases downstream.

### `numpy.distutils` supports using MSVC and mingw64-gfortran together

Numpy distutils now supports using Mingw64 gfortran and MSVC compilers together. This enables the production of Python extension modules on Windows containing Fortran code while retaining compatibility with the binaries distributed by Python.org. Not all use cases are supported, but most common ways to wrap Fortran for Python are functional.

Compilation in this mode is usually enabled automatically, and can be selected via the `--fcompiler` and `--compiler` options to `setup.py`. Moreover, linking Fortran codes to static OpenBLAS is supported; by default a gfortran compatible static archive `openblas.a` is looked for.

### `np.linalg.pinv` now works on stacked matrices

Previously it was limited to a single 2d array.

### `numpy.save` aligns data to 64 bytes instead of 16

Saving NumPy arrays in the `npy` format with `numpy.save` inserts padding before the array data to align it at 64 bytes. Previously this was only 16 bytes (and sometimes less due to a bug in the code for version 2). Now the alignment is 64 bytes, which matches the widest SIMD instruction set commonly available, and is also the most common cache line size. This makes `npy` files easier to use in programs which open them with `mmap`, especially on Linux where an `mmap` offset must be a multiple of the page size.

### NPZ files now can be written without using temporary files

In Python 3.6+ `numpy.savez` and `numpy.savez_compressed` now write directly to a ZIP file, without creating intermediate temporary files.

### Better support for empty structured and string types

Structured types can contain zero fields, and string dtypes can contain zero characters. Zero-length strings still cannot be created directly, and must be constructed through structured dtypes:

    str0 = np.empty(10, np.dtype([('v', str, N)]))['v']
    void0 = np.empty(10, np.void)

It was always possible to work with these, but the following operations are now supported for these arrays:

>   - <span class="title-ref">arr.sort()</span>
>   - <span class="title-ref">arr.view(bytes)</span>
>   - <span class="title-ref">arr.resize(...)</span>
>   - <span class="title-ref">pickle.dumps(arr)</span>

### Support for `decimal.Decimal` in `np.lib.financial`

Unless otherwise stated all functions within the `financial` package now support using the `decimal.Decimal` built-in type.

### Float printing now uses "dragon4" algorithm for shortest decimal representation

The `str` and `repr` of floating-point values (16, 32, 64 and 128 bit) are now printed to give the shortest decimal representation which uniquely identifies the value from others of the same type. Previously this was only true for `float64` values. The remaining float types will now often be shorter than in numpy 1.13. Arrays printed in scientific notation now also use the shortest scientific representation, instead of fixed precision as before.

> Additionally, the <span class="title-ref">str</span> of float scalars scalars will no longer be truncated in python2, unlike python2 <span class="title-ref">float</span>s. <span class="title-ref">np.double</span> scalars now have a `str` and `repr` identical to that of a python3 float.

New functions `np.format_float_scientific` and `np.format_float_positional` are provided to generate these decimal representations.

A new option `floatmode` has been added to `np.set_printoptions` and `np.array2string`, which gives control over uniqueness and rounding of printed elements in an array. The new default is `floatmode='maxprec'` with `precision=8`, which will print at most 8 fractional digits, or fewer if an element can be uniquely represented with fewer. A useful new mode is `floatmode="unique"`, which will output enough digits to specify the array elements uniquely.

Numpy complex-floating-scalars with values like `inf*j` or `nan*j` now print as `infj` and `nanj`, like the pure-python `complex` type.

The `FloatFormat` and `LongFloatFormat` classes are deprecated and should both be replaced by `FloatingFormat`. Similarly `ComplexFormat` and `LongComplexFormat` should be replaced by `ComplexFloatingFormat`.

### `void` datatype elements are now printed in hex notation

A hex representation compatible with the python `bytes` type is now printed for unstructured `np.void` elements, e.g., `V4` datatype. Previously, in python2 the raw void data of the element was printed to stdout, or in python3 the integer byte values were shown.

### printing style for `void` datatypes is now independently customizable

The printing style of `np.void` arrays is now independently customizable using the `formatter` argument to `np.set_printoptions`, using the `'void'` key, instead of the catch-all `numpystr` key as before.

### Reduced memory usage of `np.loadtxt`

`np.loadtxt` now reads files in chunks instead of all at once which decreases its memory usage significantly for large files.

## Changes

### Multiple-field indexing/assignment of structured arrays

The indexing and assignment of structured arrays with multiple fields has changed in a number of ways, as warned about in previous releases.

First, indexing a structured array with multiple fields, e.g., `arr[['f1', 'f3']]`, returns a view into the original array instead of a copy. The returned view will have extra padding bytes corresponding to intervening fields in the original array, unlike the copy in 1.13, which will affect code such as `arr[['f1', 'f3']].view(newdtype)`.

Second, assignment between structured arrays will now occur "by position" instead of "by field name". The Nth field of the destination will be set to the Nth field of the source regardless of field name, unlike in numpy versions 1.6 to 1.13 in which fields in the destination array were set to the identically-named field in the source array or to 0 if the source did not have a field.

Correspondingly, the order of fields in a structured dtypes now matters when computing dtype equality. For example, with the dtypes :

    x = dtype({'names': ['A', 'B'], 'formats': ['i4', 'f4'], 'offsets': [0, 4]})
    y = dtype({'names': ['B', 'A'], 'formats': ['f4', 'i4'], 'offsets': [4, 0]})

the expression `x == y` will now return `False`, unlike before. This makes dictionary based dtype specifications like `dtype({'a': ('i4', 0), 'b': ('f4', 4)})` dangerous in python \< 3.6 since dict key order is not preserved in those versions.

Assignment from a structured array to a boolean array now raises a ValueError, unlike in 1.13, where it always set the destination elements to `True`.

Assignment from structured array with more than one field to a non-structured array now raises a ValueError. In 1.13 this copied just the first field of the source to the destination.

Using field "titles" in multiple-field indexing is now disallowed, as is repeating a field name in a multiple-field index.

The documentation for structured arrays in the user guide has been significantly updated to reflect these changes.

### Integer and Void scalars are now unaffected by `np.set_string_function`

Previously, unlike most other numpy scalars, the `str` and `repr` of integer and void scalars could be controlled by `np.set_string_function`. This is no longer possible.

### 0d array printing changed, `style` arg of array2string deprecated

Previously the `str` and `repr` of 0d arrays had idiosyncratic implementations which returned `str(a.item())` and `'array(' + repr(a.item()) + ')'` respectively for 0d array `a`, unlike both numpy scalars and higher dimension ndarrays.

Now, the `str` of a 0d array acts like a numpy scalar using `str(a[()])` and the `repr` acts like higher dimension arrays using `formatter(a[()])`, where `formatter` can be specified using `np.set_printoptions`. The `style` argument of `np.array2string` is deprecated.

This new behavior is disabled in 1.13 legacy printing mode, see compatibility notes above.

### Seeding `RandomState` using an array requires a 1-d array

`RandomState` previously would accept empty arrays or arrays with 2 or more dimensions, which resulted in either a failure to seed (empty arrays) or for some of the passed values to be ignored when setting the seed.

### `MaskedArray` objects show a more useful `repr`

The `repr` of a `MaskedArray` is now closer to the python code that would produce it, with arrays now being shown with commas and dtypes. Like the other formatting changes, this can be disabled with the 1.13 legacy printing mode in order to help transition doctests.

### The `repr` of `np.polynomial` classes is more explicit

It now shows the domain and window parameters as keyword arguments to make them more clear:

    >>> np.polynomial.Polynomial(range(4))
    Polynomial([0.,  1.,  2.,  3.], domain=[-1,  1], window=[-1,  1])

---

1.14.1-notes.md

---

# NumPy 1.14.1 Release Notes

This is a bugfix release for some problems reported following the 1.14.0 release. The major problems fixed are the following.

  - Problems with the new array printing, particularly the printing of complex values, Please report any additional problems that may turn up.
  - Problems with `np.einsum` due to the new `optimized=True` default. Some fixes for optimization have been applied and `optimize=False` is now the default.
  - The sort order in `np.unique` when `axis=<some-number>` will now always be lexicographic in the subarray elements. In previous NumPy versions there was an optimization that could result in sorting the subarrays as unsigned byte strings.
  - The change in 1.14.0 that multi-field indexing of structured arrays returns a view instead of a copy has been reverted but remains on track for NumPy 1.15. Affected users should read the 1.14.1 Numpy User Guide section "basics/structured arrays/accessing multiple fields" for advice on how to manage this transition.

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. The source releases were cythonized with Cython 0.26.1, which is known to **not** support the upcoming Python 3.7 release. People who wish to run Python 3.7 should check out the NumPy repo and try building with the, as yet, unreleased master branch of Cython.

## Contributors

A total of 14 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Charles Harris
  - Daniel Smith
  - Dennis Weyland +
  - Eric Larson
  - Eric Wieser
  - Jarrod Millman
  - Kenichi Maehashi +
  - Marten van Kerkwijk
  - Mathieu Lamarre
  - Sebastian Berg
  - Simon Conseil
  - Simon Gibbons
  - xoviat

## Pull requests merged

A total of 36 pull requests were merged for this release.

  - [\#10339](https://github.com/numpy/numpy/pull/10339): BUG: restrict the \_\_config\_\_ modifications to win32
  - [\#10368](https://github.com/numpy/numpy/pull/10368): MAINT: Adjust type promotion in linalg.norm
  - [\#10375](https://github.com/numpy/numpy/pull/10375): BUG: add missing paren and remove quotes from repr of fieldless...
  - [\#10395](https://github.com/numpy/numpy/pull/10395): MAINT: Update download URL in setup.py.
  - [\#10396](https://github.com/numpy/numpy/pull/10396): BUG: fix einsum issue with unicode input and py2
  - [\#10397](https://github.com/numpy/numpy/pull/10397): BUG: fix error message not formatted in einsum
  - [\#10398](https://github.com/numpy/numpy/pull/10398): DOC: add documentation about how to handle new array printing
  - [\#10403](https://github.com/numpy/numpy/pull/10403): BUG: Set einsum optimize parameter default to <span class="title-ref">False</span>.
  - [\#10424](https://github.com/numpy/numpy/pull/10424): ENH: Fix repr of np.record objects to match np.void types \#10412
  - [\#10425](https://github.com/numpy/numpy/pull/10425): MAINT: Update zesty to artful for i386 testing
  - [\#10431](https://github.com/numpy/numpy/pull/10431): REL: Add 1.14.1 release notes template
  - [\#10435](https://github.com/numpy/numpy/pull/10435): MAINT: Use ValueError for duplicate field names in lookup (backport)
  - [\#10534](https://github.com/numpy/numpy/pull/10534): BUG: Provide a better error message for out-of-order fields
  - [\#10536](https://github.com/numpy/numpy/pull/10536): BUG: Resize bytes columns in genfromtxt (backport of \#10401)
  - [\#10537](https://github.com/numpy/numpy/pull/10537): BUG: multifield-indexing adds padding bytes: revert for 1.14.1
  - [\#10539](https://github.com/numpy/numpy/pull/10539): BUG: fix np.save issue with python 2.7.5
  - [\#10540](https://github.com/numpy/numpy/pull/10540): BUG: Add missing DECREF in Py2 int() cast
  - [\#10541](https://github.com/numpy/numpy/pull/10541): TST: Add circleci document testing to maintenance/1.14.x
  - [\#10542](https://github.com/numpy/numpy/pull/10542): BUG: complex repr has extra spaces, missing + (1.14 backport)
  - [\#10550](https://github.com/numpy/numpy/pull/10550): BUG: Set missing exception after malloc
  - [\#10557](https://github.com/numpy/numpy/pull/10557): BUG: In numpy.i, clear CARRAY flag if wrapped buffer is not C\_CONTIGUOUS.
  - [\#10558](https://github.com/numpy/numpy/pull/10558): DEP: Issue FutureWarning when malformed records detected.
  - [\#10559](https://github.com/numpy/numpy/pull/10559): BUG: Fix einsum optimize logic for singleton dimensions
  - [\#10560](https://github.com/numpy/numpy/pull/10560): BUG: Fix calling ufuncs with a positional output argument.
  - [\#10561](https://github.com/numpy/numpy/pull/10561): BUG: Fix various Big-Endian test failures (ppc64)
  - [\#10562](https://github.com/numpy/numpy/pull/10562): BUG: Make dtype.descr error for out-of-order fields.
  - [\#10563](https://github.com/numpy/numpy/pull/10563): BUG: arrays not being flattened in <span class="title-ref">union1d</span>
  - [\#10607](https://github.com/numpy/numpy/pull/10607): MAINT: Update sphinxext submodule hash.
  - [\#10608](https://github.com/numpy/numpy/pull/10608): BUG: Revert sort optimization in np.unique.
  - [\#10609](https://github.com/numpy/numpy/pull/10609): BUG: infinite recursion in str of 0d subclasses
  - [\#10610](https://github.com/numpy/numpy/pull/10610): BUG: Align type definition with generated lapack
  - [\#10612](https://github.com/numpy/numpy/pull/10612): BUG/ENH: Improve output for structured non-void types
  - [\#10622](https://github.com/numpy/numpy/pull/10622): BUG: deallocate recursive closure in arrayprint.py (1.14 backport)
  - [\#10624](https://github.com/numpy/numpy/pull/10624): BUG: Correctly identify comma separated dtype strings
  - [\#10629](https://github.com/numpy/numpy/pull/10629): BUG: deallocate recursive closure in arrayprint.py (backport...
  - [\#10630](https://github.com/numpy/numpy/pull/10630): REL: Prepare for 1.14.1 release.

---

1.14.2-notes.md

---

# NumPy 1.14.2 Release Notes

This is a bugfix release for some bugs reported following the 1.14.1 release. The major problems dealt with are as follows.

  - Residual bugs in the new array printing functionality.
  - Regression resulting in a relocation problem with shared library.
  - Improved PyPy compatibility.

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. The source releases were cythonized with Cython 0.26.1, which is known to **not** support the upcoming Python 3.7 release. People who wish to run Python 3.7 should check out the NumPy repo and try building with the, as yet, unreleased master branch of Cython.

## Contributors

A total of 4 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Charles Harris
  - Eric Wieser
  - Pauli Virtanen

## Pull requests merged

A total of 5 pull requests were merged for this release.

  - [\#10674](https://github.com/numpy/numpy/pull/10674): BUG: Further back-compat fix for subclassed array repr
  - [\#10725](https://github.com/numpy/numpy/pull/10725): BUG: dragon4 fractional output mode adds too many trailing zeros
  - [\#10726](https://github.com/numpy/numpy/pull/10726): BUG: Fix f2py generated code to work on PyPy
  - [\#10727](https://github.com/numpy/numpy/pull/10727): BUG: Fix missing NPY\_VISIBILITY\_HIDDEN on npy\_longdouble\_to\_PyLong
  - [\#10729](https://github.com/numpy/numpy/pull/10729): DOC: Create 1.14.2 notes and changelog.

---

1.14.3-notes.md

---

# NumPy 1.14.3 Release Notes

This is a bugfix release for a few bugs reported following the 1.14.2 release:

  - np.lib.recfunctions.fromrecords accepts a list-of-lists, until 1.15
  - In python2, float types use the new print style when printing to a file
  - style arg in "legacy" print mode now works for 0d arrays

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. The source releases were cythonized with Cython 0.28.2.

## Contributors

A total of 6 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Charles Harris
  - Jonathan March +
  - Malcolm Smith +
  - Matti Picus
  - Pauli Virtanen

## Pull requests merged

A total of 8 pull requests were merged for this release.

  - [\#10862](https://github.com/numpy/numpy/pull/10862): BUG: floating types should override tp\_print (1.14 backport)
  - [\#10905](https://github.com/numpy/numpy/pull/10905): BUG: for 1.14 back-compat, accept list-of-lists in fromrecords
  - [\#10947](https://github.com/numpy/numpy/pull/10947): BUG: 'style' arg to array2string broken in legacy mode (1.14...
  - [\#10959](https://github.com/numpy/numpy/pull/10959): BUG: test, fix for missing flags\['WRITEBACKIFCOPY'\] key
  - [\#10960](https://github.com/numpy/numpy/pull/10960): BUG: Add missing underscore to prototype in check\_embedded\_lapack
  - [\#10961](https://github.com/numpy/numpy/pull/10961): BUG: Fix encoding regression in ma/bench.py (Issue \#10868)
  - [\#10962](https://github.com/numpy/numpy/pull/10962): BUG: core: fix NPY\_TITLE\_KEY macro on pypy
  - [\#10974](https://github.com/numpy/numpy/pull/10974): BUG: test, fix PyArray\_DiscardWritebackIfCopy...

---

1.14.4-notes.md

---

# NumPy 1.14.4 Release Notes

This is a bugfix release for bugs reported following the 1.14.3 release. The most significant fixes are:

  - fixes for compiler instruction reordering that resulted in NaN's not being properly propagated in <span class="title-ref">np.max</span> and <span class="title-ref">np.min</span>,
  - fixes for bus faults on SPARC and older ARM due to incorrect alignment checks.

There are also improvements to printing of long doubles on PPC platforms. All is not yet perfect on that platform, the whitespace padding is still incorrect and is to be fixed in numpy 1.15, consequently NumPy still fails some printing-related (and other) unit tests on ppc systems. However, the printed values are now correct.

Note that NumPy will error on import if it detects incorrect float32 <span class="title-ref">dot</span> results. This problem has been seen on the Mac when working in the Anaconda environment and is due to a subtle interaction between MKL and PyQt5. It is not strictly a NumPy problem, but it is best that users be aware of it. See the gh-8577 NumPy issue for more information.

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. The source releases were cythonized with Cython 0.28.2 and should work for the upcoming Python 3.7.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Charles Harris
  - Marten van Kerkwijk
  - Matti Picus
  - Pauli Virtanen
  - Ryan Soklaski +
  - Sebastian Berg

## Pull requests merged

A total of 11 pull requests were merged for this release.

  - [\#11104](https://github.com/numpy/numpy/pull/11104): BUG: str of DOUBLE\_DOUBLE format wrong on ppc64
  - [\#11170](https://github.com/numpy/numpy/pull/11170): TST: linalg: add regression test for gh-8577
  - [\#11174](https://github.com/numpy/numpy/pull/11174): MAINT: add sanity-checks to be run at import time
  - [\#11181](https://github.com/numpy/numpy/pull/11181): BUG: void dtype setup checked offset not actual pointer for alignment
  - [\#11194](https://github.com/numpy/numpy/pull/11194): BUG: Python2 doubles don't print correctly in interactive shell.
  - [\#11198](https://github.com/numpy/numpy/pull/11198): BUG: optimizing compilers can reorder call to npy\_get\_floatstatus
  - [\#11199](https://github.com/numpy/numpy/pull/11199): BUG: reduce using SSE only warns if inside SSE loop
  - [\#11203](https://github.com/numpy/numpy/pull/11203): BUG: Bytes delimiter/comments in genfromtxt should be decoded
  - [\#11211](https://github.com/numpy/numpy/pull/11211): BUG: Fix reference count/memory leak exposed by better testing
  - [\#11219](https://github.com/numpy/numpy/pull/11219): BUG: Fixes einsum broadcasting bug when optimize=True
  - [\#11251](https://github.com/numpy/numpy/pull/11251): DOC: Document 1.14.4 release.

---

1.14.5-notes.md

---

# NumPy 1.14.5 Release Notes

This is a bugfix release for bugs reported following the 1.14.4 release. The most significant fixes are:

  - fixes for compilation errors on alpine and NetBSD

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python 3.6 wheels available from PIP are built with Python 3.6.2 and should be compatible with all previous versions of Python 3.6. The source releases were cythonized with Cython 0.28.2 and should work for the upcoming Python 3.7.

## Contributors

A total of 1 person contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris

## Pull requests merged

A total of 2 pull requests were merged for this release.

  - [\#11274](https://github.com/numpy/numpy/pull/11274): BUG: Correct use of NPY\_UNUSED.
  - [\#11294](https://github.com/numpy/numpy/pull/11294): BUG: Remove extra trailing parentheses.

---

1.14.6-notes.md

---

# NumPy 1.14.6 Release Notes

This is a bugfix release for bugs reported following the 1.14.5 release. The most significant fixes are:

  - Fix for behavior change in `ma.masked_values(shrink=True)`
  - Fix the new cached allocations machinery to be thread safe.

The Python versions supported in this release are 2.7 and 3.4 - 3.7. The Python 3.6 wheels on PyPI should be compatible with all Python 3.6 versions.

## Contributors

A total of 4 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Eric Wieser
  - Julian Taylor
  - Matti Picus

## Pull requests merged

A total of 4 pull requests were merged for this release.

  - [\#11985](https://github.com/numpy/numpy/pull/11985): BUG: fix cached allocations without the GIL
  - [\#11986](https://github.com/numpy/numpy/pull/11986): BUG: Undo behavior change in ma.masked\_values(shrink=True)
  - [\#11987](https://github.com/numpy/numpy/pull/11987): BUG: fix refcount leak in PyArray\_AdaptFlexibleDType
  - [\#11995](https://github.com/numpy/numpy/pull/11995): TST: Add Python 3.7 testing to NumPy 1.14.

---

1.15.0-notes.md

---

# NumPy 1.15.0 Release Notes

NumPy 1.15.0 is a release with an unusual number of cleanups, many deprecations of old functions, and improvements to many existing functions. Please read the detailed descriptions below to see if you are affected.

For testing, we have switched to pytest as a replacement for the no longer maintained nose framework. The old nose based interface remains for downstream projects who may still be using it.

The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg problems reported for NumPy 1.14.

## Highlights

  - NumPy has switched to pytest for testing.
  - A new <span class="title-ref">numpy.printoptions</span> context manager.
  - Many improvements to the histogram functions.
  - Support for unicode field names in python 2.7.
  - Improved support for PyPy.
  - Fixes and improvements to <span class="title-ref">numpy.einsum</span>.

## New functions

  - <span class="title-ref">numpy.gcd</span> and <span class="title-ref">numpy.lcm</span>, to compute the greatest common divisor and least common multiple.

  - <span class="title-ref">numpy.ma.stack</span>, the <span class="title-ref">numpy.stack</span> array-joining function generalized to masked arrays.

  - <span class="title-ref">numpy.quantile</span> function, an interface to `percentile` without factors of 100

  - <span class="title-ref">numpy.nanquantile</span> function, an interface to `nanpercentile` without factors of 100

  - <span class="title-ref">numpy.printoptions</span>, a context manager that sets print options temporarily for the scope of the `with` block:
    
        >>> with np.printoptions(precision=2):
        ...     print(np.array([2.0]) / 3)
        [0.67]

  - <span class="title-ref">numpy.histogram\_bin\_edges</span>, a function to get the edges of the bins used by a histogram without needing to calculate the histogram.

  - C functions <span class="title-ref">npy\_get\_floatstatus\_barrier</span> and <span class="title-ref">npy\_clear\_floatstatus\_barrier</span> have been added to deal with compiler optimization changing the order of operations. See below for details.

## Deprecations

  - Aliases of builtin <span class="title-ref">pickle</span> functions are deprecated, in favor of their unaliased `pickle.<func>` names:
      - <span class="title-ref">numpy.loads</span>
      - <span class="title-ref">numpy.core.numeric.load</span>
      - <span class="title-ref">numpy.core.numeric.loads</span>
      - <span class="title-ref">numpy.ma.loads</span>, <span class="title-ref">numpy.ma.dumps</span>
      - <span class="title-ref">numpy.ma.load</span>, <span class="title-ref">numpy.ma.dump</span> - these functions already failed on python 3 when called with a string.
  - Multidimensional indexing with anything but a tuple is deprecated. This means that the index list in `ind = [slice(None), 0]; arr[ind]` should be changed to a tuple, e.g., `ind = [slice(None), 0]; arr[tuple(ind)]` or `arr[(slice(None), 0)]`. That change is necessary to avoid ambiguity in expressions such as `arr[[[0, 1], [0, 1]]]`, currently interpreted as `arr[array([0, 1]), array([0, 1])]`, that will be interpreted as `arr[array([[0, 1], [0, 1]])]` in the future.
  - Imports from the following sub-modules are deprecated, they will be removed at some future date.
      - <span class="title-ref">numpy.testing.utils</span>
      - <span class="title-ref">numpy.testing.decorators</span>
      - <span class="title-ref">numpy.testing.nosetester</span>
      - <span class="title-ref">numpy.testing.noseclasses</span>
      - <span class="title-ref">numpy.core.umath\_tests</span>
  - Giving a generator to <span class="title-ref">numpy.sum</span> is now deprecated. This was undocumented behavior, but worked. Previously, it would calculate the sum of the generator expression. In the future, it might return a different result. Use `np.sum(np.from_iter(generator))` or the built-in Python `sum` instead.
  - Users of the C-API should call `PyArrayResolveWriteBackIfCopy` or `PyArray_DiscardWritebackIfCopy` on any array with the `WRITEBACKIFCOPY` flag set, before deallocating the array. A deprecation warning will be emitted if those calls are not used when needed.
  - Users of `nditer` should use the nditer object as a context manager anytime one of the iterator operands is writeable, so that numpy can manage writeback semantics, or should call `it.close()`. A <span class="title-ref">RuntimeWarning</span> may be emitted otherwise in these cases.
  - The `normed` argument of `np.histogram`, deprecated long ago in 1.6.0, now emits a `DeprecationWarning`.

## Future Changes

  - NumPy 1.16 will drop support for Python 3.4.
  - NumPy 1.17 will drop support for Python 2.7.

## Compatibility notes

### Compiled testing modules renamed and made private

The following compiled modules have been renamed and made private:

  - `umath_tests` -\> `_umath_tests`
  - `test_rational` -\> `_rational_tests`
  - `multiarray_tests` -\> `_multiarray_tests`
  - `struct_ufunc_test` -\> `_struct_ufunc_tests`
  - `operand_flag_tests` -\> `_operand_flag_tests`

The `umath_tests` module is still available for backwards compatibility, but will be removed in the future.

### The `NpzFile` returned by `np.savez` is now a `collections.abc.Mapping`

This means it behaves like a readonly dictionary, and has a new `.values()` method and `len()` implementation.

For python 3, this means that `.iteritems()`, `.iterkeys()` have been deprecated, and `.keys()` and `.items()` now return views and not lists. This is consistent with how the builtin `dict` type changed between python 2 and python 3.

### Under certain conditions, `nditer` must be used in a context manager

When using an <span class="title-ref">numpy.nditer</span> with the `"writeonly"` or `"readwrite"` flags, there are some circumstances where nditer doesn't actually give you a view of the writable array. Instead, it gives you a copy, and if you make changes to the copy, nditer later writes those changes back into your actual array. Currently, this writeback occurs when the array objects are garbage collected, which makes this API error-prone on CPython and entirely broken on PyPy. Therefore, `nditer` should now be used as a context manager whenever it is used with writeable arrays, e.g., `with np.nditer(...) as it: ...`. You may also explicitly call `it.close()` for cases where a context manager is unusable, for instance in generator expressions.

### Numpy has switched to using pytest instead of nose for testing

The last nose release was 1.3.7 in June, 2015, and development of that tool has ended, consequently NumPy has now switched to using pytest. The old decorators and nose tools that were previously used by some downstream projects remain available, but will not be maintained. The standard testing utilities, `assert_almost_equal` and such, are not be affected by this change except for the nose specific functions `import_nose` and `raises`. Those functions are not used in numpy, but are kept for downstream compatibility.

### Numpy no longer monkey-patches `ctypes` with `__array_interface__`

Previously numpy added `__array_interface__` attributes to all the integer types from `ctypes`.

### `np.ma.notmasked_contiguous` and `np.ma.flatnotmasked_contiguous` always return lists

This is the documented behavior, but previously the result could be any of slice, None, or list.

All downstream users seem to check for the `None` result from `flatnotmasked_contiguous` and replace it with `[]`. Those callers will continue to work as before.

### `np.squeeze` restores old behavior of objects that cannot handle an `axis` argument

Prior to version `1.7.0`, <span class="title-ref">numpy.squeeze</span> did not have an `axis` argument and all empty axes were removed by default. The incorporation of an `axis` argument made it possible to selectively squeeze single or multiple empty axes, but the old API expectation was not respected because axes could still be selectively removed (silent success) from an object expecting all empty axes to be removed. That silent, selective removal of empty axes for objects expecting the old behavior has been fixed and the old behavior restored.

### unstructured void array's `.item` method now returns a bytes object

`.item` now returns a `bytes` object instead of a buffer or byte array. This may affect code which assumed the return value was mutable, which is no longer the case.

### `copy.copy` and `copy.deepcopy` no longer turn `masked` into an array

Since `np.ma.masked` is a readonly scalar, copying should be a no-op. These functions now behave consistently with `np.copy()`.

### Multifield Indexing of Structured Arrays will still return a copy

The change that multi-field indexing of structured arrays returns a view instead of a copy is pushed back to 1.16. A new method `numpy.lib.recfunctions.repack_fields` has been introduced to help mitigate the effects of this change, which can be used to write code compatible with both numpy 1.15 and 1.16. For more information on how to update code to account for this future change see the "accessing multiple fields" section of the [user guide](https://docs.scipy.org/doc/numpy/user/basics.rec.html).

## C API changes

### New functions `npy_get_floatstatus_barrier` and `npy_clear_floatstatus_barrier`

Functions `npy_get_floatstatus_barrier` and `npy_clear_floatstatus_barrier` have been added and should be used in place of the `npy_get_floatstatus`and `npy_clear_status` functions. Optimizing compilers like GCC 8.1 and Clang were rearranging the order of operations when the previous functions were used in the ufunc SIMD functions, resulting in the floatstatus flags being checked before the operation whose status we wanted to check was run. See [\#10339](https://github.com/numpy/numpy/issues/10370).

### Changes to `PyArray_GetDTypeTransferFunction`

`PyArray_GetDTypeTransferFunction` now defaults to using user-defined `copyswapn` / `copyswap` for user-defined dtypes. If this causes a significant performance hit, consider implementing `copyswapn` to reflect the implementation of `PyArray_GetStridedCopyFn`. See [\#10898](https://github.com/numpy/numpy/pull/10898).

## New Features

### `np.gcd` and `np.lcm` ufuncs added for integer and objects types

These compute the greatest common divisor, and lowest common multiple, respectively. These work on all the numpy integer types, as well as the builtin arbitrary-precision `Decimal` and `long` types.

### Support for cross-platform builds for iOS

The build system has been modified to add support for the `_PYTHON_HOST_PLATFORM` environment variable, used by `distutils` when compiling on one platform for another platform. This makes it possible to compile NumPy for iOS targets.

This only enables you to compile NumPy for one specific platform at a time. Creating a full iOS-compatible NumPy package requires building for the 5 architectures supported by iOS (i386, x86\_64, armv7, armv7s and arm64), and combining these 5 compiled builds products into a single "fat" binary.

### `return_indices` keyword added for `np.intersect1d`

New keyword `return_indices` returns the indices of the two input arrays that correspond to the common elements.

### `np.quantile` and `np.nanquantile`

Like `np.percentile` and `np.nanpercentile`, but takes quantiles in \[0, 1\] rather than percentiles in \[0, 100\]. `np.percentile` is now a thin wrapper around `np.quantile` with the extra step of dividing by 100.

### Build system

Added experimental support for the 64-bit RISC-V architecture.

## Improvements

### `np.einsum` updates

Syncs einsum path optimization tech between <span class="title-ref">numpy</span> and <span class="title-ref">opt\_einsum</span>. In particular, the <span class="title-ref">greedy</span> path has received many enhancements by @jcmgray. A full list of issues fixed are:

  - Arbitrary memory can be passed into the <span class="title-ref">greedy</span> path. Fixes gh-11210.
  - The greedy path has been updated to contain more dynamic programming ideas preventing a large number of duplicate (and expensive) calls that figure out the actual pair contraction that takes place. Now takes a few seconds on several hundred input tensors. Useful for matrix product state theories.
  - Reworks the broadcasting dot error catching found in gh-11218 gh-10352 to be a bit earlier in the process.
  - Enhances the <span class="title-ref">can\_dot</span> functionality that previous missed an edge case (part of gh-11308).

### `np.ufunc.reduce` and related functions now accept an initial value

`np.ufunc.reduce`, `np.sum`, `np.prod`, `np.min` and `np.max` all now accept an `initial` keyword argument that specifies the value to start the reduction with.

### `np.flip` can operate over multiple axes

`np.flip` now accepts None, or tuples of int, in its `axis` argument. If axis is None, it will flip over all the axes.

### `histogram` and `histogramdd` functions have moved to `np.lib.histograms`

These were originally found in `np.lib.function_base`. They are still available under their un-scoped `np.histogram(dd)` names, and to maintain compatibility, aliased at `np.lib.function_base.histogram(dd)`.

Code that does `from np.lib.function_base import *` will need to be updated with the new location, and should consider not using `import *` in future.

### `histogram` will accept NaN values when explicit bins are given

Previously it would fail when trying to compute a finite range for the data. Since the range is ignored anyway when the bins are given explicitly, this error was needless.

Note that calling `histogram` on NaN values continues to raise the `RuntimeWarning` s typical of working with nan values, which can be silenced as usual with `errstate`.

### `histogram` works on datetime types, when explicit bin edges are given

Dates, times, and timedeltas can now be histogrammed. The bin edges must be passed explicitly, and are not yet computed automatically.

### `histogram` "auto" estimator handles limited variance better

No longer does an IQR of 0 result in `n_bins=1`, rather the number of bins chosen is related to the data size in this situation.

The edges returned by <span class="title-ref">histogram</span><span class="title-ref"> and </span><span class="title-ref">histogramdd</span><span class="title-ref"> now match the data float type ------------------------------------------------------------------------------------ When passed </span><span class="title-ref">np.float16</span><span class="title-ref">, </span><span class="title-ref">np.float32</span><span class="title-ref">, or </span><span class="title-ref">np.longdouble</span><span class="title-ref"> data, the returned edges are now of the same dtype. Previously, </span><span class="title-ref">histogram</span><span class="title-ref"> would only return the same type if explicit bins were given, and </span><span class="title-ref">histogram</span><span class="title-ref"> would produce </span><span class="title-ref">float64</span>\` bins no matter what the inputs.

### `histogramdd` allows explicit ranges to be given in a subset of axes

The `range` argument of <span class="title-ref">numpy.histogramdd</span> can now contain `None` values to indicate that the range for the corresponding axis should be computed from the data. Previously, this could not be specified on a per-axis basis.

### The normed arguments of `histogramdd` and `histogram2d` have been renamed

These arguments are now called `density`, which is consistent with `histogram`. The old argument continues to work, but the new name should be preferred.

### `np.r_` works with 0d arrays, and `np.ma.mr_` works with `np.ma.masked`

0d arrays passed to the <span class="title-ref">r\_</span> and <span class="title-ref">mr\_</span> concatenation helpers are now treated as though they are arrays of length 1. Previously, passing these was an error. As a result, <span class="title-ref">numpy.ma.mr\_</span> now works correctly on the `masked` constant.

### `np.ptp` accepts a `keepdims` argument, and extended axis tuples

`np.ptp` (peak-to-peak) can now work over multiple axes, just like `np.max` and `np.min`.

### `MaskedArray.astype` now is identical to `ndarray.astype`

This means it takes all the same arguments, making more code written for ndarray work for masked array too.

### Enable AVX2/AVX512 at compile time

Change to simd.inc.src to allow use of AVX2 or AVX512 at compile time. Previously compilation for avx2 (or 512) with -march=native would still use the SSE code for the simd functions even when the rest of the code got AVX2.

### `nan_to_num` always returns scalars when receiving scalar or 0d inputs

Previously an array was returned for integer scalar inputs, which is inconsistent with the behavior for float inputs, and that of ufuncs in general. For all types of scalar or 0d input, the result is now a scalar.

### `np.flatnonzero` works on numpy-convertible types

`np.flatnonzero` now uses `np.ravel(a)` instead of `a.ravel()`, so it works for lists, tuples, etc.

### `np.interp` returns numpy scalars rather than builtin scalars

Previously `np.interp(0.5, [0, 1], [10, 20])` would return a `float`, but now it returns a `np.float64` object, which more closely matches the behavior of other functions.

Additionally, the special case of `np.interp(object_array_0d, ...)` is no longer supported, as `np.interp(object_array_nd)` was never supported anyway.

As a result of this change, the `period` argument can now be used on 0d arrays.

### Allow dtype field names to be unicode in Python 2

Previously `np.dtype([(u'name', float)])` would raise a `TypeError` in Python 2, as only bytestrings were allowed in field names. Now any unicode string field names will be encoded with the `ascii` codec, raising a `UnicodeEncodeError` upon failure.

This change makes it easier to write Python 2/3 compatible code using `from __future__ import unicode_literals`, which previously would cause string literal field names to raise a TypeError in Python 2.

### Comparison ufuncs accept `dtype=object`, overriding the default `bool`

This allows object arrays of symbolic types, which override `==` and other operators to return expressions, to be compared elementwise with `np.equal(a, b, dtype=object)`.

### `sort` functions accept `kind='stable'`

Up until now, to perform a stable sort on the data, the user must do:

> \>\>\> np.sort(\[5, 2, 6, 2, 1\], kind='mergesort') \[1, 2, 2, 5, 6\]

because merge sort is the only stable sorting algorithm available in NumPy. However, having kind='mergesort' does not make it explicit that the user wants to perform a stable sort thus harming the readability.

This change allows the user to specify kind='stable' thus clarifying the intent.

### Do not make temporary copies for in-place accumulation

When ufuncs perform accumulation they no longer make temporary copies because of the overlap between input an output, that is, the next element accumulated is added before the accumulated result is stored in its place, hence the overlap is safe. Avoiding the copy results in faster execution.

### `linalg.matrix_power` can now handle stacks of matrices

Like other functions in `linalg`, `matrix_power` can now deal with arrays of dimension larger than 2, which are treated as stacks of matrices. As part of the change, to further improve consistency, the name of the first argument has been changed to `a` (from `M`), and the exceptions for non-square matrices have been changed to `LinAlgError` (from `ValueError`).

### Increased performance in `random.permutation` for multidimensional arrays

`permutation` uses the fast path in `random.shuffle` for all input array dimensions. Previously the fast path was only used for 1-d arrays.

### Generalized ufuncs now accept `axes`, `axis` and `keepdims` arguments

One can control over which axes a generalized ufunc operates by passing in an `axes` argument, a list of tuples with indices of particular axes. For instance, for a signature of `(i,j),(j,k)->(i,k)` appropriate for matrix multiplication, the base elements are two-dimensional matrices and these are taken to be stored in the two last axes of each argument. The corresponding axes keyword would be `[(-2, -1), (-2, -1), (-2, -1)]`. If one wanted to use leading dimensions instead, one would pass in `[(0, 1), (0, 1), (0, 1)]`.

For simplicity, for generalized ufuncs that operate on 1-dimensional arrays (vectors), a single integer is accepted instead of a single-element tuple, and for generalized ufuncs for which all outputs are scalars, the (empty) output tuples can be omitted. Hence, for a signature of `(i),(i)->()` appropriate for an inner product, one could pass in `axes=[0, 0]` to indicate that the vectors are stored in the first dimensions of the two inputs arguments.

As a short-cut for generalized ufuncs that are similar to reductions, i.e., that act on a single, shared core dimension such as the inner product example above, one can pass an `axis` argument. This is equivalent to passing in `axes` with identical entries for all arguments with that core dimension (e.g., for the example above, `axes=[(axis,), (axis,)]`).

Furthermore, like for reductions, for generalized ufuncs that have inputs that all have the same number of core dimensions and outputs with no core dimension, one can pass in `keepdims` to leave a dimension with size 1 in the outputs, thus allowing proper broadcasting against the original inputs. The location of the extra dimension can be controlled with `axes`. For instance, for the inner-product example, `keepdims=True, axes=[-2, -2, -2]` would act on the inner-product example, `keepdims=True, axis=-2` would act on the one-but-last dimension of the input arguments, and leave a size 1 dimension in that place in the output.

### float128 values now print correctly on ppc systems

Previously printing float128 values was buggy on ppc, since the special double-double floating-point-format on these systems was not accounted for. float128s now print with correct rounding and uniqueness.

Warning to ppc users: You should upgrade glibc if it is version \<=2.23, especially if using float128. On ppc, glibc's malloc in these version often misaligns allocated memory which can crash numpy when using float128 values.

### New `np.take_along_axis` and `np.put_along_axis` functions

When used on multidimensional arrays, `argsort`, `argmin`, `argmax`, and `argpartition` return arrays that are difficult to use as indices. `take_along_axis` provides an easy way to use these indices to lookup values within an array, so that:

    np.take_along_axis(a, np.argsort(a, axis=axis), axis=axis)

is the same as:

    np.sort(a, axis=axis)

`np.put_along_axis` acts as the dual operation for writing to these indices within an array.

---

1.15.1-notes.md

---

# NumPy 1.15.1 Release Notes

This is a bugfix release for bugs and regressions reported following the 1.15.0 release.

  - The annoying but harmless RuntimeWarning that "numpy.dtype size changed" has been suppressed. The long standing suppression was lost in the transition to pytest.
  - The update to Cython 0.28.3 exposed a problematic use of a gcc attribute used to prefer code size over speed in module initialization, possibly resulting in incorrect compiled code. This has been fixed in latest Cython but has been disabled here for safety.
  - Support for big-endian and ARMv8 architectures has been improved.

The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg problems reported for NumPy 1.14.

## Compatibility Note

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit binaries. That will also be the case in future releases. See [\#11625](https://github.com/numpy/numpy/issues/11625) for the related discussion. Those needing 32-bit support should look elsewhere or build from source.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Chris Billington
  - Elliott Sales de Andrade +
  - Eric Wieser
  - Jeremy Manning +
  - Matti Picus
  - Ralf Gommers

## Pull requests merged

A total of 24 pull requests were merged for this release.

  - [\#11647](https://github.com/numpy/numpy/pull/11647): MAINT: Filter Cython warnings in `__init__.py`
  - [\#11648](https://github.com/numpy/numpy/pull/11648): BUG: Fix doc source links to unwrap decorators
  - [\#11657](https://github.com/numpy/numpy/pull/11657): BUG: Ensure singleton dimensions are not dropped when converting...
  - [\#11661](https://github.com/numpy/numpy/pull/11661): BUG: Warn on Nan in minimum,maximum for scalars
  - [\#11665](https://github.com/numpy/numpy/pull/11665): BUG: cython sometimes emits invalid gcc attribute
  - [\#11682](https://github.com/numpy/numpy/pull/11682): BUG: Fix regression in void\_getitem
  - [\#11698](https://github.com/numpy/numpy/pull/11698): BUG: Make matrix\_power again work for object arrays.
  - [\#11700](https://github.com/numpy/numpy/pull/11700): BUG: Add missing PyErr\_NoMemory after failing malloc
  - [\#11719](https://github.com/numpy/numpy/pull/11719): BUG: Fix undefined functions on big-endian systems.
  - [\#11720](https://github.com/numpy/numpy/pull/11720): MAINT: Make einsum optimize default to False.
  - [\#11746](https://github.com/numpy/numpy/pull/11746): BUG: Fix regression in loadtxt for bz2 text files in Python 2.
  - [\#11757](https://github.com/numpy/numpy/pull/11757): BUG: Revert use of <span class="title-ref">console\_scripts</span>.
  - [\#11758](https://github.com/numpy/numpy/pull/11758): BUG: Fix Fortran kind detection for aarch64 & s390x.
  - [\#11759](https://github.com/numpy/numpy/pull/11759): BUG: Fix printing of longdouble on ppc64le.
  - [\#11760](https://github.com/numpy/numpy/pull/11760): BUG: Fixes for unicode field names in Python 2
  - [\#11761](https://github.com/numpy/numpy/pull/11761): BUG: Increase required cython version on python 3.7
  - [\#11763](https://github.com/numpy/numpy/pull/11763): BUG: check return value of \_buffer\_format\_string
  - [\#11775](https://github.com/numpy/numpy/pull/11775): MAINT: Make assert\_array\_compare more generic.
  - [\#11776](https://github.com/numpy/numpy/pull/11776): TST: Fix urlopen stubbing.
  - [\#11777](https://github.com/numpy/numpy/pull/11777): BUG: Fix regression in intersect1d.
  - [\#11779](https://github.com/numpy/numpy/pull/11779): BUG: Fix test sensitive to platform byte order.
  - [\#11781](https://github.com/numpy/numpy/pull/11781): BUG: Avoid signed overflow in histogram
  - [\#11785](https://github.com/numpy/numpy/pull/11785): BUG: Fix pickle and memoryview for datetime64, timedelta64 scalars
  - [\#11786](https://github.com/numpy/numpy/pull/11786): BUG: Deprecation triggers segfault

---

1.15.2-notes.md

---

# NumPy 1.15.2 Release Notes

This is a bugfix release for bugs and regressions reported following the 1.15.1 release.

  - The matrix PendingDeprecationWarning is now suppressed in pytest 3.8.
  - The new cached allocations machinery has been fixed to be thread safe.
  - The boolean indexing of subclasses now works correctly.
  - A small memory leak in PyArray\_AdaptFlexibleDType has been fixed.

The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg problems reported for NumPy 1.14.

## Compatibility Note

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit binaries. That will also be the case in future releases. See [\#11625](https://github.com/numpy/numpy/issues/11625) for the related discussion. Those needing 32-bit support should look elsewhere or build from source.

## Contributors

A total of 4 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Julian Taylor
  - Marten van Kerkwijk
  - Matti Picus

## Pull requests merged

A total of 4 pull requests were merged for this release.

  - [\#11902](https://github.com/numpy/numpy/pull/11902): BUG: Fix matrix PendingDeprecationWarning suppression for pytest...
  - [\#11981](https://github.com/numpy/numpy/pull/11981): BUG: fix cached allocations without the GIL for 1.15.x
  - [\#11982](https://github.com/numpy/numpy/pull/11982): BUG: fix refcount leak in PyArray\_AdaptFlexibleDType
  - [\#11992](https://github.com/numpy/numpy/pull/11992): BUG: Ensure boolean indexing of subclasses sets base correctly.

---

1.15.3-notes.md

---

# NumPy 1.15.3 Release Notes

This is a bugfix release for bugs and regressions reported following the 1.15.2 release. The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg problems reported for NumPy 1.14.

## Compatibility Note

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit binaries. That will also be the case in future releases. See [\#11625](https://github.com/numpy/numpy/issues/11625) for the related discussion. Those needing 32-bit support should look elsewhere or build from source.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Charles Harris
  - Jeroen Demeyer
  - Kevin Sheppard
  - Matthew Bowden +
  - Matti Picus
  - Tyler Reddy

## Pull requests merged

A total of 12 pull requests were merged for this release.

  - [\#12080](https://github.com/numpy/numpy/pull/12080): MAINT: Blacklist some MSVC complex functions.
  - [\#12083](https://github.com/numpy/numpy/pull/12083): TST: Add azure CI testing to 1.15.x branch.
  - [\#12084](https://github.com/numpy/numpy/pull/12084): BUG: test\_path() now uses Path.resolve()
  - [\#12085](https://github.com/numpy/numpy/pull/12085): TST, MAINT: Fix some failing tests on azure-pipelines mac and...
  - [\#12187](https://github.com/numpy/numpy/pull/12187): BUG: Fix memory leak in mapping.c
  - [\#12188](https://github.com/numpy/numpy/pull/12188): BUG: Allow boolean subtract in histogram
  - [\#12189](https://github.com/numpy/numpy/pull/12189): BUG: Fix in-place permutation
  - [\#12190](https://github.com/numpy/numpy/pull/12190): BUG: limit default for get\_num\_build\_jobs() to 8
  - [\#12191](https://github.com/numpy/numpy/pull/12191): BUG: [OBJECT\_to]()\* should check for errors
  - [\#12192](https://github.com/numpy/numpy/pull/12192): DOC: Prepare for NumPy 1.15.3 release.
  - [\#12237](https://github.com/numpy/numpy/pull/12237): BUG: Fix MaskedArray fill\_value type conversion.
  - [\#12238](https://github.com/numpy/numpy/pull/12238): TST: Backport azure-pipeline testing fixes for Mac

---

1.15.4-notes.md

---

# NumPy 1.15.4 Release Notes

This is a bugfix release for bugs and regressions reported following the 1.15.3 release. The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg problems reported for NumPy 1.14.

## Compatibility Note

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit binaries. That will also be the case in future releases. See [\#11625](https://github.com/numpy/numpy/issues/11625) for the related discussion. Those needing 32-bit support should look elsewhere or build from source.

## Contributors

A total of 4 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Matti Picus
  - Sebastian Berg
  - bbbbbbbbba +

## Pull requests merged

A total of 4 pull requests were merged for this release.

  - [\#12296](https://github.com/numpy/numpy/pull/12296): BUG: Dealloc cached buffer info
  - [\#12297](https://github.com/numpy/numpy/pull/12297): BUG: Fix fill value in masked array '==' and '\!=' ops.
  - [\#12307](https://github.com/numpy/numpy/pull/12307): DOC: Correct the default value of <span class="title-ref">optimize</span> in <span class="title-ref">numpy.einsum</span>
  - [\#12320](https://github.com/numpy/numpy/pull/12320): REL: Prepare for the NumPy 1.15.4 release

---

1.16.0-notes.md

---

# NumPy 1.16.0 Release Notes

This NumPy release is the last one to support Python 2.7 and will be maintained as a long term release with bug fixes until 2020. Support for Python 3.4 been dropped, the supported Python versions are 2.7 and 3.5-3.7. The wheels on PyPI are linked with OpenBLAS v0.3.4+, which should fix the known threading issues found in previous OpenBLAS versions.

Downstream developers building this release should use Cython \>= 0.29 and, if using OpenBLAS, OpenBLAS \> v0.3.4.

This release has seen a lot of refactoring and features many bug fixes, improved code organization, and better cross platform compatibility. Not all of these improvements will be visible to users, but they should help make maintenance easier going forward.

## Highlights

  - Experimental (opt-in only) support for overriding numpy functions, see `__array_function__` below.
  - The `matmul` function is now a ufunc. This provides better performance and allows overriding with `__array_ufunc__`.
  - Improved support for the ARM and POWER architectures.
  - Improved support for AIX and PyPy.
  - Improved interop with ctypes.
  - Improved support for PEP 3118.

## New functions

  - New functions added to the <span class="title-ref">numpy.lib.recfuntions</span> module to ease the structured assignment changes:
    
    >   - `assign_fields_by_name`
    >   - `structured_to_unstructured`
    >   - `unstructured_to_structured`
    >   - `apply_along_fields`
    >   - `require_fields`
    
    See the user guide at \<<https://docs.scipy.org/doc/numpy/user/basics.rec.html>\> for more info.

## New deprecations

  - The type dictionaries <span class="title-ref">numpy.core.typeNA</span> and <span class="title-ref">numpy.core.sctypeNA</span> are deprecated. They were buggy and not documented and will be removed in the 1.18 release. Use `numpy.sctypeDict` instead.
  - The <span class="title-ref">numpy.asscalar</span> function is deprecated. It is an alias to the more powerful <span class="title-ref">numpy.ndarray.item</span>, not tested, and fails for scalars.
  - The <span class="title-ref">numpy.set\_array\_ops</span> and <span class="title-ref">numpy.get\_array\_ops</span> functions are deprecated. As part of <span class="title-ref">NEP 15</span>, they have been deprecated along with the C-API functions `PyArray_SetNumericOps` and `PyArray_GetNumericOps`. Users who wish to override the inner loop functions in built-in ufuncs should use :c\`PyUFunc\_ReplaceLoopBySignature\`.
  - The <span class="title-ref">numpy.unravel\_index</span> keyword argument `dims` is deprecated, use `shape` instead.
  - The <span class="title-ref">numpy.histogram</span> `normed` argument is deprecated. It was deprecated previously, but no warning was issued.
  - The `positive` operator (`+`) applied to non-numerical arrays is deprecated. See below for details.
  - Passing an iterator to the stack functions is deprecated

## Expired deprecations

  - NaT comparisons now return `False` without a warning, finishing a deprecation cycle begun in NumPy 1.11.
  - `np.lib.function_base.unique` was removed, finishing a deprecation cycle begun in NumPy 1.4. Use <span class="title-ref">numpy.unique</span> instead.
  - multi-field indexing now returns views instead of copies, finishing a deprecation cycle begun in NumPy 1.7. The change was previously attempted in NumPy 1.14 but reverted until now.
  - `np.PackageLoader` and `np.pkgload` have been removed. These were deprecated in 1.10, had no tests, and seem to no longer work in 1.15.

## Future changes

  - NumPy 1.17 will drop support for Python 2.7.

## Compatibility notes

### f2py script on Windows

On Windows, the installed script for running f2py is now an `.exe` file rather than a `*.py` file and should be run from the command line as `f2py` whenever the `Scripts` directory is in the path. Running `f2py` as a module `python -m numpy.f2py [...]` will work without path modification in any version of NumPy.

### NaT comparisons

Consistent with the behavior of NaN, all comparisons other than inequality checks with datetime64 or timedelta64 NaT ("not-a-time") values now always return `False`, and inequality checks with NaT now always return `True`. This includes comparisons between NaT values. For compatibility with the old behavior, use `np.isnat` to explicitly check for NaT or convert datetime64/timedelta64 arrays with `.astype(np.int64)` before making comparisons.

### complex64/128 alignment has changed

The memory alignment of complex types is now the same as a C-struct composed of two floating point values, while before it was equal to the size of the type. For many users (for instance on x64/unix/gcc) this means that complex64 is now 4-byte aligned instead of 8-byte aligned. An important consequence is that aligned structured dtypes may now have a different size. For instance, `np.dtype('c8,u1', align=True)` used to have an itemsize of 16 (on x64/gcc) but now it is 12.

More in detail, the complex64 type now has the same alignment as a C-struct `struct {float r, i;}`, according to the compiler used to compile numpy, and similarly for the complex128 and complex256 types.

### nd\_grid \_\_[len](#module__-attribute-now-points-to-public-modules) removal

`len(np.mgrid)` and `len(np.ogrid)` are now considered nonsensical and raise a `TypeError`.

### `np.unravel_index` now accepts `shape` keyword argument

Previously, only the `dims` keyword argument was accepted for specification of the shape of the array to be used for unraveling. `dims` remains supported, but is now deprecated.

### multi-field views return a view instead of a copy

Indexing a structured array with multiple fields, e.g., `arr[['f1', 'f3']]`, returns a view into the original array instead of a copy. The returned view will often have extra padding bytes corresponding to intervening fields in the original array, unlike before, which will affect code such as `arr[['f1', 'f3']].view('float64')`. This change has been planned since numpy 1.7. Operations hitting this path have emitted `FutureWarnings` since then. Additional `FutureWarnings` about this change were added in 1.12.

To help users update their code to account for these changes, a number of functions have been added to the `numpy.lib.recfunctions` module which safely allow such operations. For instance, the code above can be replaced with `structured_to_unstructured(arr[['f1', 'f3']], dtype='float64')`. See the "accessing multiple fields" section of the [user guide](https://docs.scipy.org/doc/numpy/user/basics.rec.html#accessing-multiple-fields).

## C API changes

The :c\`NPY\_FEATURE\_VERSION\` was incremented to 0x0000D, due to the addition of:

  - :c`PyUFuncObject.core_dim_flags`
  - :c`PyUFuncObject.core_dim_sizes`
  - :c`PyUFuncObject.identity_value`
  - :c\`PyUFunc\_FromFuncAndDataAndSignatureAndIdentity\`

## New Features

### Integrated squared error (ISE) estimator added to `histogram`

This method (`bins='stone'`) for optimizing the bin number is a generalization of the Scott's rule. The Scott's rule assumes the distribution is approximately Normal, while the [ISE](https://en.wikipedia.org/wiki/Histogram#Minimizing_cross-validation_estimated_squared_error) is a non-parametric method based on cross-validation.

### `max_rows` keyword added for `np.loadtxt`

New keyword `max_rows` in <span class="title-ref">numpy.loadtxt</span> sets the maximum rows of the content to be read after `skiprows`, as in <span class="title-ref">numpy.genfromtxt</span>.

### modulus operator support added for `np.timedelta64` operands

The modulus (remainder) operator is now supported for two operands of type `np.timedelta64`. The operands may have different units and the return value will match the type of the operands.

## Improvements

### no-copy pickling of numpy arrays

Up to protocol 4, numpy array pickling created 2 spurious copies of the data being serialized. With pickle protocol 5, and the `PickleBuffer` API, a large variety of numpy arrays can now be serialized without any copy using out-of-band buffers, and with one less copy using in-band buffers. This results, for large arrays, in an up to 66% drop in peak memory usage.

### build shell independence

NumPy builds should no longer interact with the host machine shell directly. `exec_command` has been replaced with `subprocess.check_output` where appropriate.

### <span class="title-ref">np.polynomial.Polynomial</span> classes render in LaTeX in Jupyter notebooks

When used in a front-end that supports it, <span class="title-ref">Polynomial</span> instances are now rendered through LaTeX. The current format is experimental, and is subject to change.

### `randint` and `choice` now work on empty distributions

Even when no elements needed to be drawn, `np.random.randint` and `np.random.choice` raised an error when the arguments described an empty distribution. This has been fixed so that e.g. `np.random.choice([], 0) == np.array([], dtype=float64)`.

### `linalg.lstsq`, `linalg.qr`, and `linalg.svd` now work with empty arrays

Previously, a `LinAlgError` would be raised when an empty matrix/empty matrices (with zero rows and/or columns) is/are passed in. Now outputs of appropriate shapes are returned.

### Chain exceptions to give better error messages for invalid PEP3118 format strings

This should help track down problems.

### Einsum optimization path updates and efficiency improvements

Einsum was synchronized with the current upstream work.

### <span class="title-ref">numpy.angle</span> and <span class="title-ref">numpy.expand\_dims</span> now work on `ndarray` subclasses

In particular, they now work for masked arrays.

### `NPY_NO_DEPRECATED_API` compiler warning suppression

Setting `NPY_NO_DEPRECATED_API` to a value of 0 will suppress the current compiler warnings when the deprecated numpy API is used.

### `np.diff` Added kwargs prepend and append

New kwargs `prepend` and `append`, allow for values to be inserted on either end of the differences. Similar to options for <span class="title-ref">ediff1d</span>. Now the inverse of <span class="title-ref">cumsum</span> can be obtained easily via `prepend=0`.

### ARM support updated

Support for ARM CPUs has been updated to accommodate 32 and 64 bit targets, and also big and little endian byte ordering. AARCH32 memory alignment issues have been addressed. CI testing has been expanded to include AARCH64 targets via the services of shippable.com.

### Appending to build flags

<span class="title-ref">numpy.distutils</span> has always overridden rather than appended to <span class="title-ref">LDFLAGS</span> and other similar such environment variables for compiling Fortran extensions. Now, if the <span class="title-ref">NPY\_DISTUTILS\_APPEND\_FLAGS</span> environment variable is set to 1, the behavior will be appending. This applied to: <span class="title-ref">LDFLAGS</span>, <span class="title-ref">F77FLAGS</span>, <span class="title-ref">F90FLAGS</span>, <span class="title-ref">FREEFLAGS</span>, <span class="title-ref">FOPT</span>, <span class="title-ref">FDEBUG</span>, and <span class="title-ref">FFLAGS</span>. See gh-11525 for more details.

### Generalized ufunc signatures now allow fixed-size dimensions

By using a numerical value in the signature of a generalized ufunc, one can indicate that the given function requires input or output to have dimensions with the given size. E.g., the signature of a function that converts a polar angle to a two-dimensional cartesian unit vector would be `()->(2)`; that for one that converts two spherical angles to a three-dimensional unit vector would be `(),()->(3)`; and that for the cross product of two three-dimensional vectors would be `(3),(3)->(3)`.

Note that to the elementary function these dimensions are not treated any differently from variable ones indicated with a name starting with a letter; the loop still is passed the corresponding size, but it can now count on that size being equal to the fixed one given in the signature.

### Generalized ufunc signatures now allow flexible dimensions

Some functions, in particular numpy's implementation of `@` as `matmul`, are very similar to generalized ufuncs in that they operate over core dimensions, but one could not present them as such because they were able to deal with inputs in which a dimension is missing. To support this, it is now allowed to postfix a dimension name with a question mark to indicate that the dimension does not necessarily have to be present.

With this addition, the signature for `matmul` can be expressed as `(m?,n),(n,p?)->(m?,p?)`. This indicates that if, e.g., the second operand has only one dimension, for the purposes of the elementary function it will be treated as if that input has core shape `(n, 1)`, and the output has the corresponding core shape of `(m, 1)`. The actual output array, however, has the flexible dimension removed, i.e., it will have shape `(..., m)`. Similarly, if both arguments have only a single dimension, the inputs will be presented as having shapes `(1, n)` and `(n, 1)` to the elementary function, and the output as `(1, 1)`, while the actual output array returned will have shape `()`. In this way, the signature allows one to use a single elementary function for four related but different signatures, `(m,n),(n,p)->(m,p)`, `(n),(n,p)->(p)`, `(m,n),(n)->(m)` and `(n),(n)->()`.

### `np.clip` and the `clip` method check for memory overlap

The `out` argument to these functions is now always tested for memory overlap to avoid corrupted results when memory overlap occurs.

### New value `unscaled` for option `cov` in `np.polyfit`

A further possible value has been added to the `cov` parameter of the `np.polyfit` function. With `cov='unscaled'` the scaling of the covariance matrix is disabled completely (similar to setting `absolute_sigma=True` in `scipy.optimize.curve_fit`). This would be useful in occasions, where the weights are given by 1/sigma with sigma being the (known) standard errors of (Gaussian distributed) data points, in which case the unscaled matrix is already a correct estimate for the covariance matrix.

### Detailed docstrings for scalar numeric types

The `help` function, when applied to numeric types such as <span class="title-ref">numpy.intc</span>, <span class="title-ref">numpy.int\_</span>, and <span class="title-ref">numpy.longlong</span>, now lists all of the aliased names for that type, distinguishing between platform -dependent and -independent aliases.

### `__module__` attribute now points to public modules

The `__module__` attribute on most NumPy functions has been updated to refer to the preferred public module from which to access a function, rather than the module in which the function happens to be defined. This produces more informative displays for functions in tools such as IPython, e.g., instead of `<function 'numpy.core.fromnumeric.sum'>` you now see `<function 'numpy.sum'>`.

### Large allocations marked as suitable for transparent hugepages

On systems that support transparent hugepages over the madvise system call numpy now marks that large memory allocations can be backed by hugepages which reduces page fault overhead and can in some fault heavy cases improve performance significantly. On Linux the setting for huge pages to be used, <span class="title-ref">/sys/kernel/mm/transparent\_hugepage/enabled</span>, must be at least <span class="title-ref">madvise</span>. Systems which already have it set to <span class="title-ref">always</span> will not see much difference as the kernel will automatically use huge pages where appropriate.

Users of very old Linux kernels (\~3.x and older) should make sure that <span class="title-ref">/sys/kernel/mm/transparent\_hugepage/defrag</span> is not set to <span class="title-ref">always</span> to avoid performance problems due concurrency issues in the memory defragmentation.

### Alpine Linux (and other musl c library distros) support

We now default to use <span class="title-ref">fenv.h</span> for floating point status error reporting. Previously we had a broken default that sometimes would not report underflow, overflow, and invalid floating point operations. Now we can support non-glibc distributions like Alpine Linux as long as they ship <span class="title-ref">fenv.h</span>.

### Speedup `np.block` for large arrays

Large arrays (greater than `512 * 512`) now use a blocking algorithm based on copying the data directly into the appropriate slice of the resulting array. This results in significant speedups for these large arrays, particularly for arrays being blocked along more than 2 dimensions.

#### `arr.ctypes.data_as(...)` holds a reference to arr

Previously the caller was responsible for keeping the array alive for the lifetime of the pointer.

### Speedup `np.take` for read-only arrays

The implementation of `np.take` no longer makes an unnecessary copy of the source array when its `writeable` flag is set to `False`.

### Support path-like objects for more functions

The `np.core.records.fromfile` function now supports `pathlib.Path` and other path-like objects in addition to a file object. Furthermore, the `np.load` function now also supports path-like objects when using memory mapping (`mmap_mode` keyword argument).

### Better behaviour of ufunc identities during reductions

Universal functions have an `.identity` which is used when `.reduce` is called on an empty axis.

As of this release, the logical binary ufuncs, <span class="title-ref">logical\_and</span>, <span class="title-ref">logical\_or</span>, and <span class="title-ref">logical\_xor</span>, now have `identity` s of type <span class="title-ref">bool</span>, where previously they were of type <span class="title-ref">int</span>. This restores the 1.14 behavior of getting `bool` s when reducing empty object arrays with these ufuncs, while also keeping the 1.15 behavior of getting `int` s when reducing empty object arrays with arithmetic ufuncs like `add` and `multiply`.

Additionally, <span class="title-ref">logaddexp</span> now has an identity of `-inf`, allowing it to be called on empty sequences, where previously it could not be.

This is possible thanks to the new :c\`PyUFunc\_FromFuncAndDataAndSignatureAndIdentity\`, which allows arbitrary values to be used as identities now.

### Improved conversion from ctypes objects

Numpy has always supported taking a value or type from `ctypes` and converting it into an array or dtype, but only behaved correctly for simpler types. As of this release, this caveat is lifted - now:

  - The `_pack_` attribute of `ctypes.Structure`, used to emulate C's `__attribute__((packed))`, is respected.
  - Endianness of all ctypes objects is preserved
  - `ctypes.Union` is supported
  - Non-representable constructs raise exceptions, rather than producing dangerously incorrect results:
      - Bitfields are no longer interpreted as sub-arrays
      - Pointers are no longer replaced with the type that they point to

### A new `ndpointer.contents` member

This matches the `.contents` member of normal ctypes arrays, and can be used to construct an `np.array` around the pointers contents. This replaces `np.array(some_nd_pointer)`, which stopped working in 1.15. As a side effect of this change, `ndpointer` now supports dtypes with overlapping fields and padding.

### `matmul` is now a `ufunc`

<span class="title-ref">numpy.matmul</span> is now a ufunc which means that both the function and the `__matmul__` operator can now be overridden by `__array_ufunc__`. Its implementation has also changed. It uses the same BLAS routines as <span class="title-ref">numpy.dot</span>, ensuring its performance is similar for large matrices.

### Start and stop arrays for `linspace`, `logspace` and `geomspace`

These functions used to be limited to scalar stop and start values, but can now take arrays, which will be properly broadcast and result in an output which has one axis prepended. This can be used, e.g., to obtain linearly interpolated points between sets of points.

### CI extended with additional services

We now use additional free CI services, thanks to the companies that provide:

  - Codecoverage testing via codecov.io
  - Arm testing via shippable.com
  - Additional test runs on azure pipelines

These are in addition to our continued use of travis, appveyor (for wheels) and LGTM

## Changes

### Comparison ufuncs will now error rather than return NotImplemented

Previously, comparison ufuncs such as `np.equal` would return <span class="title-ref">NotImplemented</span> if their arguments had structured dtypes, to help comparison operators such as `__eq__` deal with those. This is no longer needed, as the relevant logic has moved to the comparison operators proper (which thus do continue to return <span class="title-ref">NotImplemented</span> as needed). Hence, like all other ufuncs, the comparison ufuncs will now error on structured dtypes.

### Positive will now raise a deprecation warning for non-numerical arrays

Previously, `+array` unconditionally returned a copy. Now, it will raise a `DeprecationWarning` if the array is not numerical (i.e., if `np.positive(array)` raises a `TypeError`. For `ndarray` subclasses that override the default `__array_ufunc__` implementation, the `TypeError` is passed on.

### `NDArrayOperatorsMixin` now implements matrix multiplication

Previously, `np.lib.mixins.NDArrayOperatorsMixin` did not implement the special methods for Python's matrix multiplication operator (`@`). This has changed now that `matmul` is a ufunc and can be overridden using `__array_ufunc__`.

### The scaling of the covariance matrix in `np.polyfit` is different

So far, `np.polyfit` used a non-standard factor in the scaling of the the covariance matrix. Namely, rather than using the standard `chisq/(M-N)`, it scaled it with `chisq/(M-N-2)` where M is the number of data points and N is the number of parameters. This scaling is inconsistent with other fitting programs such as e.g. `scipy.optimize.curve_fit` and was changed to `chisq/(M-N)`.

### `maximum` and `minimum` no longer emit warnings

As part of code introduced in 1.10, `float32` and `float64` set invalid float status when a Nan is encountered in <span class="title-ref">numpy.maximum</span> and <span class="title-ref">numpy.minimum</span>, when using SSE2 semantics. This caused a <span class="title-ref">RuntimeWarning</span> to sometimes be emitted. In 1.15 we fixed the inconsistencies which caused the warnings to become more conspicuous. Now no warnings will be emitted.

### Umath and multiarray c-extension modules merged into a single module

The two modules were merged, according to [NEP 15](). Previously <span class="title-ref">np.core.umath</span> and <span class="title-ref">np.core.multiarray</span> were separate c-extension modules. They are now python wrappers to the single <span class="title-ref">np.core/\_multiarray\_math</span> c-extension module.

### `getfield` validity checks extended

<span class="title-ref">numpy.ndarray.getfield</span> now checks the dtype and offset arguments to prevent accessing invalid memory locations.

### NumPy functions now support overrides with `__array_function__`

NumPy has a new experimental mechanism for overriding the implementation of almost all NumPy functions on non-NumPy arrays by defining an `__array_function__` method, as described in [NEP 18]().

This feature is not yet been enabled by default, but has been released to facilitate experimentation by potential users. See the NEP for details on setting the appropriate environment variable. We expect the NumPy 1.17 release will enable overrides by default, which will also be more performant due to a new implementation written in C.

### Arrays based off readonly buffers cannot be set `writeable`

We now disallow setting the `writeable` flag True on arrays created from `fromstring(readonly-buffer)`.

---

1.16.1-notes.md

---

# NumPy 1.16.1 Release Notes

The NumPy 1.16.1 release fixes bugs reported against the 1.16.0 release, and also backports several enhancements from master that seem appropriate for a release series that is the last to support Python 2.7. The wheels on PyPI are linked with OpenBLAS v0.3.4+, which should fix the known threading issues found in previous OpenBLAS versions.

Downstream developers building this release should use Cython \>= 0.29.2 and, if using OpenBLAS, OpenBLAS \> v0.3.4.

If you are installing using pip, you may encounter a problem with older installed versions of NumPy that pip did not delete becoming mixed with the current version, resulting in an `ImportError`. That problem is particularly common on Debian derived distributions due to a modified pip. The fix is to make sure all previous NumPy versions installed by pip have been removed. See [\#12736](https://github.com/numpy/numpy/issues/12736) for discussion of the issue. Note that previously this problem resulted in an `AttributeError`.

## Contributors

A total of 16 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Antoine Pitrou
  - Arcesio Castaneda Medina +
  - Charles Harris
  - Chris Markiewicz +
  - Christoph Gohlke
  - Christopher J. Markiewicz +
  - Daniel Hrisca +
  - EelcoPeacs +
  - Eric Wieser
  - Kevin Sheppard
  - Matti Picus
  - OBATA Akio +
  - Ralf Gommers
  - Sebastian Berg
  - Stephan Hoyer
  - Tyler Reddy

## Enhancements

  - [\#12767](https://github.com/numpy/numpy/pull/12767): ENH: add mm-\>q floordiv
  - [\#12768](https://github.com/numpy/numpy/pull/12768): ENH: port np.core.overrides to C for speed
  - [\#12769](https://github.com/numpy/numpy/pull/12769): ENH: Add np.ctypeslib.as\_ctypes\_type(dtype), improve <span class="title-ref">np.ctypeslib.as\_ctypes</span>
  - [\#12773](https://github.com/numpy/numpy/pull/12773): ENH: add "max difference" messages to np.testing.assert\_array\_equal...
  - [\#12820](https://github.com/numpy/numpy/pull/12820): ENH: Add mm-\>qm divmod
  - [\#12890](https://github.com/numpy/numpy/pull/12890): ENH: add \_dtype\_ctype to namespace for freeze analysis

## Compatibility notes

  - The changed error message emitted by array comparison testing functions may affect doctests. See below for detail.
  - Casting from double and single denormals to float16 has been corrected. In some rare cases, this may result in results being rounded up instead of down, changing the last bit (ULP) of the result.

## New Features

### divmod operation is now supported for two `timedelta64` operands

The divmod operator now handles two `np.timedelta64` operands, with type signature `mm->qm`.

## Improvements

### Further improvements to `ctypes` support in `np.ctypeslib`

A new <span class="title-ref">numpy.ctypeslib.as\_ctypes\_type</span> function has been added, which can be used to converts a <span class="title-ref">dtype</span> into a best-guess <span class="title-ref">ctypes</span> type. Thanks to this new function, <span class="title-ref">numpy.ctypeslib.as\_ctypes</span> now supports a much wider range of array types, including structures, booleans, and integers of non-native endianness.

### Array comparison assertions include maximum differences

Error messages from array comparison tests such as <span class="title-ref">np.testing.assert\_allclose</span> now include "max absolute difference" and "max relative difference," in addition to the previous "mismatch" percentage. This information makes it easier to update absolute and relative error tolerances.

## Changes

### `timedelta64 % 0` behavior adjusted to return `NaT`

The modulus operation with two `np.timedelta64` operands now returns `NaT` in the case of division by zero, rather than returning zero

---

1.16.2-notes.md

---

# NumPy 1.16.2 Release Notes

NumPy 1.16.2 is a quick release fixing several problems encountered on Windows. The Python versions supported are 2.7 and 3.5-3.7. The Windows problems addressed are:

  - DLL load problems for NumPy wheels on Windows,
  - distutils command line parsing on Windows.

There is also a regression fix correcting signed zeros produced by divmod, see below for details.

Downstream developers building this release should use Cython \>= 0.29.2 and, if using OpenBLAS, OpenBLAS \> v0.3.4.

If you are installing using pip, you may encounter a problem with older installed versions of NumPy that pip did not delete becoming mixed with the current version, resulting in an `ImportError`. That problem is particularly common on Debian derived distributions due to a modified pip. The fix is to make sure all previous NumPy versions installed by pip have been removed. See [\#12736](https://github.com/numpy/numpy/issues/12736) for discussion of the issue.

## Compatibility notes

### Signed zero when using divmod

Starting in version 1.12.0, numpy incorrectly returned a negatively signed zero when using the `divmod` and `floor_divide` functions when the result was zero. For example:

    >>> np.zeros(10)//1
    array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])

With this release, the result is correctly returned as a positively signed zero:

    >>> np.zeros(10)//1
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

## Contributors

A total of 5 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Eric Wieser
  - Matti Picus
  - Tyler Reddy
  - Tony LaTorre +

## Pull requests merged

A total of 7 pull requests were merged for this release.

  - [\#12909](https://github.com/numpy/numpy/pull/12909): TST: fix vmImage dispatch in Azure
  - [\#12923](https://github.com/numpy/numpy/pull/12923): MAINT: remove complicated test of multiarray import failure mode
  - [\#13020](https://github.com/numpy/numpy/pull/13020): BUG: fix signed zero behavior in npy\_divmod
  - [\#13026](https://github.com/numpy/numpy/pull/13026): MAINT: Add functions to parse shell-strings in the platform-native...
  - [\#13028](https://github.com/numpy/numpy/pull/13028): BUG: Fix regression in parsing of F90 and F77 environment variables
  - [\#13038](https://github.com/numpy/numpy/pull/13038): BUG: parse shell escaping in extra\_compile\_args and extra\_link\_args
  - [\#13041](https://github.com/numpy/numpy/pull/13041): BLD: Windows absolute path DLL loading

---

1.16.3-notes.md

---

# NumPy 1.16.3 Release Notes

The NumPy 1.16.3 release fixes bugs reported against the 1.16.2 release, and also backports several enhancements from master that seem appropriate for a release series that is the last to support Python 2.7. The wheels on PyPI are linked with OpenBLAS v0.3.4+, which should fix the known threading issues found in previous OpenBLAS versions.

Downstream developers building this release should use Cython \>= 0.29.2 and, if using OpenBLAS, OpenBLAS \> v0.3.4.

The most noticeable change in this release is that unpickling object arrays when loading `*.npy` or `*.npz` files now requires an explicit opt-in. This backwards incompatible change was made in response to [CVE-2019-6446](https://nvd.nist.gov/vuln/detail/CVE-2019-6446).

## Compatibility notes

### Unpickling while loading requires explicit opt-in

The functions `np.load`, and `np.lib.format.read_array` take an <span class="title-ref">allow\_pickle</span> keyword which now defaults to `False` in response to [CVE-2019-6446](https://nvd.nist.gov/vuln/detail/CVE-2019-6446).

## Improvements

### Covariance in <span class="title-ref">random.mvnormal</span> cast to double

This should make the tolerance used when checking the singular values of the covariance matrix more meaningful.

## Changes

### `__array_interface__` offset now works as documented

The interface may use an `offset` value that was previously mistakenly ignored.

---

1.16.4-notes.md

---

# NumPy 1.16.4 Release Notes

The NumPy 1.16.4 release fixes bugs reported against the 1.16.3 release, and also backports several enhancements from master that seem appropriate for a release series that is the last to support Python 2.7. The wheels on PyPI are linked with OpenBLAS v0.3.7-dev, which should fix issues on Skylake series cpus.

Downstream developers building this release should use Cython \>= 0.29.2 and, if using OpenBLAS, OpenBLAS \> v0.3.7. The supported Python versions are 2.7 and 3.5-3.7.

## New deprecations

### Writeable flag of C-API wrapped arrays

When an array is created from the C-API to wrap a pointer to data, the only indication we have of the read-write nature of the data is the `writeable` flag set during creation. It is dangerous to force the flag to writeable. In the future it will not be possible to switch the writeable flag to `True` from python. This deprecation should not affect many users since arrays created in such a manner are very rare in practice and only available through the NumPy C-API.

## Compatibility notes

### Potential changes to the random stream

Due to bugs in the application of log to random floating point numbers, the stream may change when sampling from `np.random.beta`, `np.random.binomial`, `np.random.laplace`, `np.random.logistic`, `np.random.logseries` or `np.random.multinomial` if a 0 is generated in the underlying MT19937 random stream. There is a 1 in \(10^{53}\) chance of this occurring, and so the probability that the stream changes for any given seed is extremely small. If a 0 is encountered in the underlying generator, then the incorrect value produced (either `np.inf` or `np.nan`) is now dropped.

## Changes

### <span class="title-ref">numpy.lib.recfunctions.structured\_to\_unstructured</span> does not squeeze single-field views

Previously `structured_to_unstructured(arr[['a']])` would produce a squeezed result inconsistent with `structured_to_unstructured(arr[['a', b']])`. This was accidental. The old behavior can be retained with `structured_to_unstructured(arr[['a']]).squeeze(axis=-1)` or far more simply, `arr['a']`.

## Contributors

A total of 10 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Eric Wieser
  - Dennis Zollo +
  - Hunter Damron +
  - Jingbei Li +
  - Kevin Sheppard
  - Matti Picus
  - Nicola Soranzo +
  - Sebastian Berg
  - Tyler Reddy

## Pull requests merged

A total of 16 pull requests were merged for this release.

  - [\#13392](https://github.com/numpy/numpy/pull/13392): BUG: Some PyPy versions lack PyStructSequence\_InitType2.
  - [\#13394](https://github.com/numpy/numpy/pull/13394): MAINT, DEP: Fix deprecated `assertEquals()`
  - [\#13396](https://github.com/numpy/numpy/pull/13396): BUG: Fix structured\_to\_unstructured on single-field types (backport)
  - [\#13549](https://github.com/numpy/numpy/pull/13549): BLD: Make CI pass again with pytest 4.5
  - [\#13552](https://github.com/numpy/numpy/pull/13552): TST: Register markers in conftest.py.
  - [\#13559](https://github.com/numpy/numpy/pull/13559): BUG: Removes ValueError for empty kwargs in arraymultiter\_new
  - [\#13560](https://github.com/numpy/numpy/pull/13560): BUG: Add TypeError to accepted exceptions in crackfortran.
  - [\#13561](https://github.com/numpy/numpy/pull/13561): BUG: Handle subarrays in descr\_to\_dtype
  - [\#13562](https://github.com/numpy/numpy/pull/13562): BUG: Protect generators from log(0.0)
  - [\#13563](https://github.com/numpy/numpy/pull/13563): BUG: Always return views from structured\_to\_unstructured when...
  - [\#13564](https://github.com/numpy/numpy/pull/13564): BUG: Catch stderr when checking compiler version
  - [\#13565](https://github.com/numpy/numpy/pull/13565): BUG: longdouble(int) does not work
  - [\#13587](https://github.com/numpy/numpy/pull/13587): BUG: distutils/system\_info.py fix missing subprocess import (\#13523)
  - [\#13620](https://github.com/numpy/numpy/pull/13620): BUG,DEP: Fix writeable flag setting for arrays without base
  - [\#13641](https://github.com/numpy/numpy/pull/13641): MAINT: Prepare for the 1.16.4 release.
  - [\#13644](https://github.com/numpy/numpy/pull/13644): BUG: special case object arrays when printing rel-, abs-error

---

1.16.5-notes.md

---

# NumPy 1.16.5 Release Notes

The NumPy 1.16.5 release fixes bugs reported against the 1.16.4 release, and also backports several enhancements from master that seem appropriate for a release series that is the last to support Python 2.7. The wheels on PyPI are linked with OpenBLAS v0.3.7-dev, which should fix errors on Skylake series cpus.

Downstream developers building this release should use Cython \>= 0.29.2 and, if using OpenBLAS, OpenBLAS \>= v0.3.7. The supported Python versions are 2.7 and 3.5-3.7.

## Contributors

A total of 18 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Alexander Shadchin
  - Allan Haldane
  - Bruce Merry +
  - Charles Harris
  - Colin Snyder +
  - Dan Allan +
  - Emile +
  - Eric Wieser
  - Grey Baker +
  - Maksim Shabunin +
  - Marten van Kerkwijk
  - Matti Picus
  - Peter Andreas Entschev +
  - Ralf Gommers
  - Richard Harris +
  - Sebastian Berg
  - Sergei Lebedev +
  - Stephan Hoyer

## Pull requests merged

A total of 23 pull requests were merged for this release.

  - [\#13742](https://github.com/numpy/numpy/pull/13742): ENH: Add project URLs to setup.py
  - [\#13823](https://github.com/numpy/numpy/pull/13823): TEST, ENH: fix tests and ctypes code for PyPy
  - [\#13845](https://github.com/numpy/numpy/pull/13845): BUG: use npy\_intp instead of int for indexing array
  - [\#13867](https://github.com/numpy/numpy/pull/13867): TST: Ignore DeprecationWarning during nose imports
  - [\#13905](https://github.com/numpy/numpy/pull/13905): BUG: Fix use-after-free in boolean indexing
  - [\#13933](https://github.com/numpy/numpy/pull/13933): MAINT/BUG/DOC: Fix errors in \_add\_newdocs
  - [\#13984](https://github.com/numpy/numpy/pull/13984): BUG: fix byte order reversal for datetime64\[ns\]
  - [\#13994](https://github.com/numpy/numpy/pull/13994): MAINT,BUG: Use nbytes to also catch empty descr during allocation
  - [\#14042](https://github.com/numpy/numpy/pull/14042): BUG: np.array cleared errors occurred in PyMemoryView\_FromObject
  - [\#14043](https://github.com/numpy/numpy/pull/14043): BUG: Fixes for Undefined Behavior Sanitizer (UBSan) errors.
  - [\#14044](https://github.com/numpy/numpy/pull/14044): BUG: ensure that casting to/from structured is properly checked.
  - [\#14045](https://github.com/numpy/numpy/pull/14045): MAINT: fix histogram\*d dispatchers
  - [\#14046](https://github.com/numpy/numpy/pull/14046): BUG: further fixup to histogram2d dispatcher.
  - [\#14052](https://github.com/numpy/numpy/pull/14052): BUG: Replace contextlib.suppress for Python 2.7
  - [\#14056](https://github.com/numpy/numpy/pull/14056): BUG: fix compilation of 3rd party modules with Py\_LIMITED\_API...
  - [\#14057](https://github.com/numpy/numpy/pull/14057): BUG: Fix memory leak in dtype from dict constructor
  - [\#14058](https://github.com/numpy/numpy/pull/14058): DOC: Document array\_function at a higher level.
  - [\#14084](https://github.com/numpy/numpy/pull/14084): BUG, DOC: add new recfunctions to <span class="title-ref">\_\_all\_\_</span>
  - [\#14162](https://github.com/numpy/numpy/pull/14162): BUG: Remove stray print that causes a SystemError on python 3.7
  - [\#14297](https://github.com/numpy/numpy/pull/14297): TST: Pin pytest version to 5.0.1.
  - [\#14322](https://github.com/numpy/numpy/pull/14322): ENH: Enable huge pages in all Linux builds
  - [\#14346](https://github.com/numpy/numpy/pull/14346): BUG: fix behavior of structured\_to\_unstructured on non-trivial...
  - [\#14382](https://github.com/numpy/numpy/pull/14382): REL: Prepare for the NumPy 1.16.5 release.

---

1.16.6-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.16.6 Release Notes

The NumPy 1.16.6 release fixes bugs reported against the 1.16.5 release, and also backports several enhancements from master that seem appropriate for a release series that is the last to support Python 2.7. The wheels on PyPI are linked with OpenBLAS v0.3.7, which should fix errors on Skylake series cpus.

Downstream developers building this release should use Cython \>= 0.29.2 and, if using OpenBLAS, OpenBLAS \>= v0.3.7. The supported Python versions are 2.7 and 3.5-3.7.

## Highlights

  - The `np.testing.utils` functions have been updated from 1.19.0-dev0. This improves the function documentation and error messages as well extending the `assert_array_compare` function to additional types.

## New functions

### Allow matmul (<span class="title-ref">@</span> operator) to work with object arrays.

This is an enhancement that was added in NumPy 1.17 and seems reasonable to include in the LTS 1.16 release series.

## Compatibility notes

### Fix regression in matmul (<span class="title-ref">@</span> operator) for boolean types

Booleans were being treated as integers rather than booleans, which was a regression from previous behavior.

## Improvements

### Array comparison assertions include maximum differences

Error messages from array comparison tests such as `testing.assert_allclose` now include "max absolute difference" and "max relative difference," in addition to the previous "mismatch" percentage. This information makes it easier to update absolute and relative error tolerances.

## Contributors

A total of 10 people contributed to this release.

  - CakeWithSteak
  - Charles Harris
  - Chris Burr
  - Eric Wieser
  - Fernando Saravia
  - Lars Grueter
  - Matti Picus
  - Maxwell Aladago
  - Qiming Sun
  - Warren Weckesser

## Pull requests merged

A total of 14 pull requests were merged for this release.

  - [\#14211](https://github.com/numpy/numpy/pull/14211): BUG: Fix uint-overflow if padding with linear\_ramp and negative...
  - [\#14275](https://github.com/numpy/numpy/pull/14275): BUG: fixing to allow unpickling of PY3 pickles from PY2
  - [\#14340](https://github.com/numpy/numpy/pull/14340): BUG: Fix misuse of .names and .fields in various places (backport...
  - [\#14423](https://github.com/numpy/numpy/pull/14423): BUG: test, fix regression in converting to ctypes.
  - [\#14434](https://github.com/numpy/numpy/pull/14434): BUG: Fixed maximum relative error reporting in assert\_allclose
  - [\#14509](https://github.com/numpy/numpy/pull/14509): BUG: Fix regression in boolean matmul.
  - [\#14686](https://github.com/numpy/numpy/pull/14686): BUG: properly define PyArray\_DescrCheck
  - [\#14853](https://github.com/numpy/numpy/pull/14853): BLD: add 'apt update' to shippable
  - [\#14854](https://github.com/numpy/numpy/pull/14854): BUG: Fix \_ctypes class circular reference. (\#13808)
  - [\#14856](https://github.com/numpy/numpy/pull/14856): BUG: Fix <span class="title-ref">np.einsum</span> errors on Power9 Linux and z/Linux
  - [\#14863](https://github.com/numpy/numpy/pull/14863): BLD: Prevent -flto from optimising long double representation...
  - [\#14864](https://github.com/numpy/numpy/pull/14864): BUG: lib: Fix histogram problem with signed integer arrays.
  - [\#15172](https://github.com/numpy/numpy/pull/15172): ENH: Backport improvements to testing functions.
  - [\#15191](https://github.com/numpy/numpy/pull/15191): REL: Prepare for 1.16.6 release.

---

1.17.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.17.0 Release Notes

This NumPy release contains a number of new features that should substantially improve its performance and usefulness, see Highlights below for a summary. The Python versions supported are 3.5-3.7, note that Python 2.7 has been dropped. Python 3.8b2 should work with the released source packages, but there are no future guarantees.

Downstream developers should use Cython \>= 0.29.11 for Python 3.8 support and OpenBLAS \>= 3.7 (not currently out) to avoid problems on the Skylake architecture. The NumPy wheels on PyPI are built from the OpenBLAS development branch in order to avoid those problems.

## Highlights

  - A new extensible <span class="title-ref">random</span> module along with four selectable <span class="title-ref">random number generators \<random.BitGenerators\></span> and improved seeding designed for use in parallel processes has been added. The currently available bit generators are <span class="title-ref">MT19937 \<random.mt19937.MT19937\></span>, <span class="title-ref">PCG64 \<random.pcg64.PCG64\></span>, <span class="title-ref">Philox \<random.philox.Philox\></span>, and <span class="title-ref">SFC64 \<random.sfc64.SFC64\></span>. See below under New Features.
  - NumPy's <span class="title-ref">FFT \<fft\></span> implementation was changed from fftpack to pocketfft, resulting in faster, more accurate transforms and better handling of datasets of prime length. See below under Improvements.
  - New radix sort and timsort sorting methods. It is currently not possible to choose which will be used. They are hardwired to the datatype and used when either `stable` or `mergesort` is passed as the method. See below under Improvements.
  - Overriding numpy functions is now possible by default, see `__array_function__` below.

## New functions

  - <span class="title-ref">numpy.errstate</span> is now also a function decorator

## Deprecations

### <span class="title-ref">numpy.polynomial</span> functions warn when passed `float` in place of `int`

Previously functions in this module would accept `float` values provided they were integral (`1.0`, `2.0`, etc). For consistency with the rest of numpy, doing so is now deprecated, and in future will raise a `TypeError`.

Similarly, passing a float like `0.5` in place of an integer will now raise a `TypeError` instead of the previous `ValueError`.

### Deprecate <span class="title-ref">numpy.distutils.exec\_command</span> and `temp_file_name`

The internal use of these functions has been refactored and there are better alternatives. Replace `exec_command` with <span class="title-ref">subprocess.Popen</span> and <span class="title-ref">temp\_file\_name \<numpy.distutils.exec\_command\></span> with <span class="title-ref">tempfile.mkstemp</span>.

### Writeable flag of C-API wrapped arrays

When an array is created from the C-API to wrap a pointer to data, the only indication we have of the read-write nature of the data is the `writeable` flag set during creation. It is dangerous to force the flag to writeable. In the future it will not be possible to switch the writeable flag to `True` from python. This deprecation should not affect many users since arrays created in such a manner are very rare in practice and only available through the NumPy C-API.

### <span class="title-ref">numpy.nonzero</span> should no longer be called on 0d arrays

The behavior of <span class="title-ref">numpy.nonzero</span> on 0d arrays was surprising, making uses of it almost always incorrect. If the old behavior was intended, it can be preserved without a warning by using `nonzero(atleast_1d(arr))` instead of `nonzero(arr)`. In a future release, it is most likely this will raise a `ValueError`.

### Writing to the result of <span class="title-ref">numpy.broadcast\_arrays</span> will warn

Commonly <span class="title-ref">numpy.broadcast\_arrays</span> returns a writeable array with internal overlap, making it unsafe to write to. A future version will set the `writeable` flag to `False`, and require users to manually set it to `True` if they are sure that is what they want to do. Now writing to it will emit a deprecation warning with instructions to set the `writeable` flag `True`. Note that if one were to inspect the flag before setting it, one would find it would already be `True`. Explicitly setting it, though, as one will need to do in future versions, clears an internal flag that is used to produce the deprecation warning. To help alleviate confusion, an additional <span class="title-ref">FutureWarning</span> will be emitted when accessing the `writeable` flag state to clarify the contradiction.

Note that for the C-side buffer protocol such an array will return a readonly buffer immediately unless a writable buffer is requested. If a writeable buffer is requested a warning will be given. When using cython, the `const` qualifier should be used with such arrays to avoid the warning (e.g. `cdef const double[::1] view`).

## Future Changes

### Shape-1 fields in dtypes won't be collapsed to scalars in a future version

Currently, a field specified as `[(name, dtype, 1)]` or `"1type"` is interpreted as a scalar field (i.e., the same as `[(name, dtype)]` or `[(name, dtype, ()]`). This now raises a FutureWarning; in a future version, it will be interpreted as a shape-(1,) field, i.e. the same as `[(name, dtype, (1,))]` or `"(1,)type"` (consistently with `[(name, dtype, n)]` / `"ntype"` with `n>1`, which is already equivalent to `[(name, dtype, (n,)]` / `"(n,)type"`).

## Compatibility notes

### `float16` subnormal rounding

Casting from a different floating point precision to `float16` used incorrect rounding in some edge cases. This means in rare cases, subnormal results will now be rounded up instead of down, changing the last bit (ULP) of the result.

### Signed zero when using divmod

Starting in version <span class="title-ref">1.12.0</span>, numpy incorrectly returned a negatively signed zero when using the `divmod` and `floor_divide` functions when the result was zero. For example:

    >>> np.zeros(10)//1
    array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])

With this release, the result is correctly returned as a positively signed zero:

    >>> np.zeros(10)//1
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

### `MaskedArray.mask` now returns a view of the mask, not the mask itself

Returning the mask itself was unsafe, as it could be reshaped in place which would violate expectations of the masked array code. The behavior of <span class="title-ref">mask \<ma.MaskedArray.mask\></span> is now consistent with <span class="title-ref">data \<ma.MaskedArray.data\></span>, which also returns a view.

The underlying mask can still be accessed with `._mask` if it is needed. Tests that contain `assert x.mask is not y.mask` or similar will need to be updated.

### Do not lookup `__buffer__` attribute in <span class="title-ref">numpy.frombuffer</span>

Looking up `__buffer__` attribute in <span class="title-ref">numpy.frombuffer</span> was undocumented and non-functional. This code was removed. If needed, use `frombuffer(memoryview(obj), ...)` instead.

### `out` is buffered for memory overlaps in <span class="title-ref">take</span>, <span class="title-ref">choose</span>, <span class="title-ref">put</span>

If the out argument to these functions is provided and has memory overlap with the other arguments, it is now buffered to avoid order-dependent behavior.

### Unpickling while loading requires explicit opt-in

The functions <span class="title-ref">load</span>, and `lib.format.read_array` take an `allow_pickle` keyword which now defaults to `False` in response to [CVE-2019-6446](https://nvd.nist.gov/vuln/detail/CVE-2019-6446).

<div class="currentmodule">

numpy.random

</div>

### Potential changes to the random stream in old random module

Due to bugs in the application of `log` to random floating point numbers, the stream may change when sampling from <span class="title-ref">\~RandomState.beta</span>, <span class="title-ref">\~RandomState.binomial</span>, <span class="title-ref">\~RandomState.laplace</span>, <span class="title-ref">\~RandomState.logistic</span>, <span class="title-ref">\~RandomState.logseries</span> or <span class="title-ref">\~RandomState.multinomial</span> if a `0` is generated in the underlying <span class="title-ref">MT19937</span> random stream. There is a `1` in \(10^{53}\) chance of this occurring, so the probability that the stream changes for any given seed is extremely small. If a `0` is encountered in the underlying generator, then the incorrect value produced (either <span class="title-ref">numpy.inf</span> or <span class="title-ref">numpy.nan</span>) is now dropped.

<div class="currentmodule">

numpy

</div>

### <span class="title-ref">i0</span> now always returns a result with the same shape as the input

Previously, the output was squeezed, such that, e.g., input with just a single element would lead to an array scalar being returned, and inputs with shapes such as `(10, 1)` would yield results that would not broadcast against the input.

Note that we generally recommend the SciPy implementation over the numpy one: it is a proper ufunc written in C, and more than an order of magnitude faster.

### <span class="title-ref">can\_cast</span> no longer assumes all unsafe casting is allowed

Previously, <span class="title-ref">can\_cast</span> returned <span class="title-ref">True</span> for almost all inputs for `casting='unsafe'`, even for cases where casting was not possible, such as from a structured dtype to a regular one. This has been fixed, making it more consistent with actual casting using, e.g., the <span class="title-ref">.astype \<ndarray.astype\></span> method.

### `ndarray.flags.writeable` can be switched to true slightly more often

In rare cases, it was not possible to switch an array from not writeable to writeable, although a base array is writeable. This can happen if an intermediate <span class="title-ref">ndarray.base</span> object is writeable. Previously, only the deepest base object was considered for this decision. However, in rare cases this object does not have the necessary information. In that case switching to writeable was never allowed. This has now been fixed.

## C API changes

### dimension or stride input arguments are now passed by `npy_intp const*`

Previously these function arguments were declared as the more strict `npy_intp*`, which prevented the caller passing constant data. This change is backwards compatible, but now allows code like:

    npy_intp const fixed_dims[] = {1, 2, 3};
    // no longer complains that the const-qualifier is discarded
    npy_intp size = PyArray_MultiplyList(fixed_dims, 3);

## New Features

<div class="currentmodule">

numpy.random

</div>

### New extensible <span class="title-ref">numpy.random</span> module with selectable random number generators

A new extensible <span class="title-ref">numpy.random</span> module along with four selectable random number generators and improved seeding designed for use in parallel processes has been added. The currently available <span class="title-ref">Bit Generators</span> are <span class="title-ref">\~mt19937.MT19937</span>, <span class="title-ref">\~pcg64.PCG64</span>, <span class="title-ref">\~philox.Philox</span>, and <span class="title-ref">\~sfc64.SFC64</span>. `PCG64` is the new default while `MT19937` is retained for backwards compatibility. Note that the legacy random module is unchanged and is now frozen, your current results will not change. More information is available in the \[API change description \<new-or-different\>\](\#api-change-description-\<new-or-different\>) and in the <span class="title-ref">top-level view \<numpy.random\></span> documentation.

<div class="currentmodule">

numpy

</div>

### libFLAME

Support for building NumPy with the libFLAME linear algebra package as the LAPACK, implementation, see [libFLAME](https://www.cs.utexas.edu/~flame/web/libFLAME.html) for details.

### User-defined BLAS detection order

<span class="title-ref">distutils</span> now uses an environment variable, comma-separated and case insensitive, to determine the detection order for BLAS libraries. By default `NPY_BLAS_ORDER=mkl,blis,openblas,atlas,accelerate,blas`. However, to force the use of OpenBLAS simply do:

    NPY_BLAS_ORDER=openblas python setup.py build

which forces the use of OpenBLAS. This may be helpful for users which have a MKL installation but wishes to try out different implementations.

### User-defined LAPACK detection order

`numpy.distutils` now uses an environment variable, comma-separated and case insensitive, to determine the detection order for LAPACK libraries. By default `NPY_LAPACK_ORDER=mkl,openblas,flame,atlas,accelerate,lapack`. However, to force the use of OpenBLAS simply do:

    NPY_LAPACK_ORDER=openblas python setup.py build

which forces the use of OpenBLAS. This may be helpful for users which have a MKL installation but wishes to try out different implementations.

### <span class="title-ref">ufunc.reduce</span> and related functions now accept a `where` mask

<span class="title-ref">ufunc.reduce</span>, <span class="title-ref">sum</span>, <span class="title-ref">prod</span>, <span class="title-ref">min</span>, <span class="title-ref">max</span> all now accept a `where` keyword argument, which can be used to tell which elements to include in the reduction. For reductions that do not have an identity, it is necessary to also pass in an initial value (e.g., `initial=np.inf` for <span class="title-ref">min</span>). For instance, the equivalent of <span class="title-ref">nansum</span> would be `np.sum(a, where=~np.isnan(a))`.

### Timsort and radix sort have replaced mergesort for stable sorting

Both radix sort and timsort have been implemented and are now used in place of mergesort. Due to the need to maintain backward compatibility, the sorting `kind` options `"stable"` and `"mergesort"` have been made aliases of each other with the actual sort implementation depending on the array type. Radix sort is used for small integer types of 16 bits or less and timsort for the remaining types. Timsort features improved performance on data containing already or nearly sorted data and performs like mergesort on random data and requires \(O(n/2)\) working space. Details of the timsort algorithm can be found at [CPython listsort.txt](https://github.com/python/cpython/blob/3.7/Objects/listsort.txt).

### <span class="title-ref">packbits</span> and <span class="title-ref">unpackbits</span> accept an `order` keyword

The `order` keyword defaults to `big`, and will order the **bits** accordingly. For `'order=big'` 3 will become `[0, 0, 0, 0, 0, 0, 1, 1]`, and `[1, 1, 0, 0, 0, 0, 0, 0]` for `order=little`

### <span class="title-ref">unpackbits</span> now accepts a `count` parameter

`count` allows subsetting the number of bits that will be unpacked up-front, rather than reshaping and subsetting later, making the <span class="title-ref">packbits</span> operation invertible, and the unpacking less wasteful. Counts larger than the number of available bits add zero padding. Negative counts trim bits off the end instead of counting from the beginning. None counts implement the existing behavior of unpacking everything.

### <span class="title-ref">linalg.svd</span> and <span class="title-ref">linalg.pinv</span> can be faster on hermitian inputs

These functions now accept a `hermitian` argument, matching the one added to <span class="title-ref">linalg.matrix\_rank</span> in 1.14.0.

### divmod operation is now supported for two `timedelta64` operands

The divmod operator now handles two `timedelta64` operands, with type signature `mm->qm`.

### <span class="title-ref">fromfile</span> now takes an `offset` argument

This function now takes an `offset` keyword argument for binary files, which specifics the offset (in bytes) from the file's current position. Defaults to `0`.

### New mode "empty" for <span class="title-ref">pad</span>

This mode pads an array to a desired shape without initializing the new entries.

### <span class="title-ref">empty\_like</span> and related functions now accept a `shape` argument

<span class="title-ref">empty\_like</span>, <span class="title-ref">full\_like</span>, <span class="title-ref">ones\_like</span> and <span class="title-ref">zeros\_like</span> now accept a `shape` keyword argument, which can be used to create a new array as the prototype, overriding its shape as well. This is particularly useful when combined with the `__array_function__` protocol, allowing the creation of new arbitrary-shape arrays from NumPy-like libraries when such an array is used as the prototype.

### Floating point scalars implement `as_integer_ratio` to match the builtin float

This returns a (numerator, denominator) pair, which can be used to construct a <span class="title-ref">fractions.Fraction</span>.

### Structured `dtype` objects can be indexed with multiple fields names

`arr.dtype[['a', 'b']]` now returns a dtype that is equivalent to `arr[['a', 'b']].dtype`, for consistency with `arr.dtype['a'] == arr['a'].dtype`.

Like the dtype of structured arrays indexed with a list of fields, this dtype has the same `itemsize` as the original, but only keeps a subset of the fields.

This means that `arr[['a', 'b']]` and `arr.view(arr.dtype[['a', 'b']])` are equivalent.

### `.npy` files support unicode field names

A new format version of 3.0 has been introduced, which enables structured types with non-latin1 field names. This is used automatically when needed.

## Improvements

### Array comparison assertions include maximum differences

Error messages from array comparison tests such as <span class="title-ref">testing.assert\_allclose</span> now include "max absolute difference" and "max relative difference," in addition to the previous "mismatch" percentage. This information makes it easier to update absolute and relative error tolerances.

### Replacement of the fftpack based <span class="title-ref">fft</span> module by the pocketfft library

Both implementations have the same ancestor (Fortran77 FFTPACK by Paul N. Swarztrauber), but pocketfft contains additional modifications which improve both accuracy and performance in some circumstances. For FFT lengths containing large prime factors, pocketfft uses Bluestein's algorithm, which maintains \(O(N log N)\) run time complexity instead of deteriorating towards \(O(N*N)\) for prime lengths. Also, accuracy for real valued FFTs with near prime lengths has improved and is on par with complex valued FFTs.

### Further improvements to `ctypes` support in <span class="title-ref">numpy.ctypeslib</span>

A new <span class="title-ref">numpy.ctypeslib.as\_ctypes\_type</span> function has been added, which can be used to converts a <span class="title-ref">dtype</span> into a best-guess <span class="title-ref">ctypes</span> type. Thanks to this new function, <span class="title-ref">numpy.ctypeslib.as\_ctypes</span> now supports a much wider range of array types, including structures, booleans, and integers of non-native endianness.

### <span class="title-ref">numpy.errstate</span> is now also a function decorator

Currently, if you have a function like:

    def foo():
        pass

and you want to wrap the whole thing in <span class="title-ref">errstate</span>, you have to rewrite it like so:

    def foo():
        with np.errstate(...):
            pass

but with this change, you can do:

    @np.errstate(...)
    def foo():
        pass

thereby saving a level of indentation

### <span class="title-ref">numpy.exp</span> and <span class="title-ref">numpy.log</span> speed up for float32 implementation

float32 implementation of <span class="title-ref">exp</span> and <span class="title-ref">log</span> now benefit from AVX2/AVX512 instruction set which are detected during runtime. <span class="title-ref">exp</span> has a max ulp error of 2.52 and <span class="title-ref">log</span> has a max ulp error or 3.83.

### Improve performance of <span class="title-ref">numpy.pad</span>

The performance of the function has been improved for most cases by filling in a preallocated array with the desired padded shape instead of using concatenation.

### <span class="title-ref">numpy.interp</span> handles infinities more robustly

In some cases where <span class="title-ref">interp</span> would previously return <span class="title-ref">nan</span>, it now returns an appropriate infinity.

### Pathlib support for <span class="title-ref">fromfile</span>, <span class="title-ref">tofile</span> and <span class="title-ref">ndarray.dump</span>

<span class="title-ref">fromfile</span>, <span class="title-ref">ndarray.ndarray.tofile</span> and <span class="title-ref">ndarray.dump</span> now support the <span class="title-ref">pathlib.Path</span> type for the `file`/`fid` parameter.

### Specialized <span class="title-ref">isnan</span>, <span class="title-ref">isinf</span>, and <span class="title-ref">isfinite</span> ufuncs for bool and int types

The boolean and integer types are incapable of storing <span class="title-ref">nan</span> and <span class="title-ref">inf</span> values, which allows us to provide specialized ufuncs that are up to 250x faster than the previous approach.

### <span class="title-ref">isfinite</span> supports `datetime64` and `timedelta64` types

Previously, <span class="title-ref">isfinite</span> used to raise a <span class="title-ref">TypeError</span> on being used on these two types.

### New keywords added to <span class="title-ref">nan\_to\_num</span>

<span class="title-ref">nan\_to\_num</span> now accepts keywords `nan`, `posinf` and `neginf` allowing the user to define the value to replace the `nan`, positive and negative `np.inf` values respectively.

### MemoryErrors caused by allocated overly large arrays are more descriptive

Often the cause of a MemoryError is incorrect broadcasting, which results in a very large and incorrect shape. The message of the error now includes this shape to help diagnose the cause of failure.

### <span class="title-ref">floor</span>, <span class="title-ref">ceil</span>, and <span class="title-ref">trunc</span> now respect builtin magic methods

These ufuncs now call the `__floor__`, `__ceil__`, and `__trunc__` methods when called on object arrays, making them compatible with <span class="title-ref">decimal.Decimal</span> and <span class="title-ref">fractions.Fraction</span> objects.

### <span class="title-ref">quantile</span> now works on <span class="title-ref">fraction.Fraction</span> and <span class="title-ref">decimal.Decimal</span> objects

In general, this handles object arrays more gracefully, and avoids floating-point operations if exact arithmetic types are used.

### Support of object arrays in <span class="title-ref">matmul</span>

It is now possible to use <span class="title-ref">matmul</span> (or the `@` operator) with object arrays. For instance, it is now possible to do:

    from fractions import Fraction
    a = np.array([[Fraction(1, 2), Fraction(1, 3)], [Fraction(1, 3), Fraction(1, 2)]])
    b = a @ a

## Changes

### <span class="title-ref">median</span> and <span class="title-ref">percentile</span> family of functions no longer warn about `nan`

<span class="title-ref">numpy.median</span>, <span class="title-ref">numpy.percentile</span>, and <span class="title-ref">numpy.quantile</span> used to emit a `RuntimeWarning` when encountering an <span class="title-ref">nan</span>. Since they return the `nan` value, the warning is redundant and has been removed.

### `timedelta64 % 0` behavior adjusted to return `NaT`

The modulus operation with two `np.timedelta64` operands now returns `NaT` in the case of division by zero, rather than returning zero

### NumPy functions now always support overrides with `__array_function__`

NumPy now always checks the `__array_function__` method to implement overrides of NumPy functions on non-NumPy arrays, as described in [NEP 18](). The feature was available for testing with NumPy 1.16 if appropriate environment variables are set, but is now always enabled.

### `lib.recfunctions.structured_to_unstructured` does not squeeze single-field views

Previously `structured_to_unstructured(arr[['a']])` would produce a squeezed result inconsistent with `structured_to_unstructured(arr[['a', b']])`. This was accidental. The old behavior can be retained with `structured_to_unstructured(arr[['a']]).squeeze(axis=-1)` or far more simply, `arr['a']`.

### <span class="title-ref">clip</span> now uses a ufunc under the hood

This means that registering clip functions for custom dtypes in C via `descr->f->fastclip` is deprecated - they should use the ufunc registration mechanism instead, attaching to the `np.core.umath.clip` ufunc.

It also means that `clip` accepts `where` and `casting` arguments, and can be override with `__array_ufunc__`.

A consequence of this change is that some behaviors of the old `clip` have been deprecated:

  - Passing `nan` to mean "do not clip" as one or both bounds. This didn't work in all cases anyway, and can be better handled by passing infinities of the appropriate sign.
  - Using "unsafe" casting by default when an `out` argument is passed. Using `casting="unsafe"` explicitly will silence this warning.

Additionally, there are some corner cases with behavior changes:

  - Padding `max < min` has changed to be more consistent across dtypes, but should not be relied upon.
  - Scalar `min` and `max` take part in promotion rules like they do in all other ufuncs.

### `__array_interface__` offset now works as documented

The interface may use an `offset` value that was mistakenly ignored.

### Pickle protocol in <span class="title-ref">savez</span> set to 3 for `force zip64` flag

<span class="title-ref">savez</span> was not using the `force_zip64` flag, which limited the size of the archive to 2GB. But using the flag requires us to use pickle protocol 3 to write `object` arrays. The protocol used was bumped to 3, meaning the archive will be unreadable by Python2.

### Structured arrays indexed with non-existent fields raise `KeyError` not `ValueError`

`arr['bad_field']` on a structured type raises `KeyError`, for consistency with `dict['bad_field']`.

---

1.17.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.17.1 Release Notes

This release contains a number of fixes for bugs reported against NumPy 1.17.0 along with a few documentation and build improvements. The Python versions supported are 3.5-3.7, note that Python 2.7 has been dropped. Python 3.8b3 should work with the released source packages, but there are no future guarantees.

Downstream developers should use Cython \>= 0.29.13 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid problems on the Skylake architecture. The NumPy wheels on PyPI are built from the OpenBLAS development branch in order to avoid those problems.

## Contributors

A total of 17 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Alexander Jung +
  - Allan Haldane
  - Charles Harris
  - Eric Wieser
  - Giuseppe Cuccu +
  - Hiroyuki V. Yamazaki
  - JÃ©rÃ©mie du Boisberranger
  - Kmol Yuan +
  - Matti Picus
  - Max Bolingbroke +
  - Maxwell Aladago +
  - Oleksandr Pavlyk
  - Peter Andreas Entschev
  - Sergei Lebedev
  - Seth Troisi +
  - Vladimir Pershin +
  - Warren Weckesser

## Pull requests merged

A total of 24 pull requests were merged for this release.

  - [\#14156](https://github.com/numpy/numpy/pull/14156): TST: Allow fuss in testing strided/non-strided exp/log loops
  - [\#14157](https://github.com/numpy/numpy/pull/14157): BUG: avx2\_scalef\_ps must be static
  - [\#14158](https://github.com/numpy/numpy/pull/14158): BUG: Remove stray print that causes a SystemError on python 3.7.
  - [\#14159](https://github.com/numpy/numpy/pull/14159): BUG: Fix DeprecationWarning in python 3.8.
  - [\#14160](https://github.com/numpy/numpy/pull/14160): BLD: Add missing gcd/lcm definitions to npy\_math.h
  - [\#14161](https://github.com/numpy/numpy/pull/14161): DOC, BUILD: cleanups and fix (again) 'build dist'
  - [\#14166](https://github.com/numpy/numpy/pull/14166): TST: Add 3.8-dev to travisCI testing.
  - [\#14194](https://github.com/numpy/numpy/pull/14194): BUG: Remove the broken clip wrapper (Backport)
  - [\#14198](https://github.com/numpy/numpy/pull/14198): DOC: Fix hermitian argument docs in svd.
  - [\#14199](https://github.com/numpy/numpy/pull/14199): MAINT: Workaround for Intel compiler bug leading to failing test
  - [\#14200](https://github.com/numpy/numpy/pull/14200): TST: Clean up of test\_pocketfft.py
  - [\#14201](https://github.com/numpy/numpy/pull/14201): BUG: Make advanced indexing result on read-only subclass writeable...
  - [\#14236](https://github.com/numpy/numpy/pull/14236): BUG: Fixed default BitGenerator name
  - [\#14237](https://github.com/numpy/numpy/pull/14237): ENH: add c-imported modules for freeze analysis in np.random
  - [\#14296](https://github.com/numpy/numpy/pull/14296): TST: Pin pytest version to 5.0.1
  - [\#14301](https://github.com/numpy/numpy/pull/14301): BUG: Fix leak in the f2py-generated module init and <span class="title-ref">PyMem\_Del</span>...
  - [\#14302](https://github.com/numpy/numpy/pull/14302): BUG: Fix formatting error in exception message
  - [\#14307](https://github.com/numpy/numpy/pull/14307): MAINT: random: Match type of SeedSequence.pool\_size to DEFAULT\_POOL\_SIZE.
  - [\#14308](https://github.com/numpy/numpy/pull/14308): BUG: Fix numpy.random bug in platform detection
  - [\#14309](https://github.com/numpy/numpy/pull/14309): ENH: Enable huge pages in all Linux builds
  - [\#14330](https://github.com/numpy/numpy/pull/14330): BUG: Fix segfault in <span class="title-ref">random.permutation(x)</span> when x is a string.
  - [\#14338](https://github.com/numpy/numpy/pull/14338): BUG: don't fail when lexsorting some empty arrays (\#14228)
  - [\#14339](https://github.com/numpy/numpy/pull/14339): BUG: Fix misuse of .names and .fields in various places (backport...
  - [\#14345](https://github.com/numpy/numpy/pull/14345): BUG: fix behavior of structured\_to\_unstructured on non-trivial...
  - [\#14350](https://github.com/numpy/numpy/pull/14350): REL: Prepare 1.17.1 release

---

1.17.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.17.2 Release Notes

This release contains fixes for bugs reported against NumPy 1.17.1 along with a some documentation improvements. The most important fix is for lexsort when the keys are of type (u)int8 or (u)int16. If you are currently using 1.17 you should upgrade.

The Python versions supported in this release are 3.5-3.7, Python 2.7 has been dropped. Python 3.8b4 should work with the released source packages, but there are no future guarantees.

Downstream developers should use Cython \>= 0.29.13 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture. The NumPy wheels on PyPI are built from the OpenBLAS development branch in order to avoid those errors.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - CakeWithSteak +
  - Charles Harris
  - Dan Allan
  - Hameer Abbasi
  - Lars Grueter
  - Matti Picus
  - Sebastian Berg

## Pull requests merged

A total of 8 pull requests were merged for this release.

  - [\#14418](https://github.com/numpy/numpy/pull/14418): BUG: Fix aradixsort indirect indexing.
  - [\#14420](https://github.com/numpy/numpy/pull/14420): DOC: Fix a minor typo in dispatch documentation.
  - [\#14421](https://github.com/numpy/numpy/pull/14421): BUG: test, fix regression in converting to ctypes
  - [\#14430](https://github.com/numpy/numpy/pull/14430): BUG: Do not show Override module in private error classes.
  - [\#14432](https://github.com/numpy/numpy/pull/14432): BUG: Fixed maximum relative error reporting in assert\_allclose.
  - [\#14433](https://github.com/numpy/numpy/pull/14433): BUG: Fix uint-overflow if padding with linear\_ramp and negative...
  - [\#14436](https://github.com/numpy/numpy/pull/14436): BUG: Update 1.17.x with 1.18.0-dev pocketfft.py.
  - [\#14446](https://github.com/numpy/numpy/pull/14446): REL: Prepare for NumPy 1.17.2 release.

---

1.17.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.17.3 Release Notes

This release contains fixes for bugs reported against NumPy 1.17.2 along with a some documentation improvements. The Python versions supported in this release are 3.5-3.8.

Downstream developers should use Cython \>= 0.29.13 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Highlights

  - Wheels for Python 3.8
  - Boolean `matmul` fixed to use booleans instead of integers.

## Compatibility notes

  - The seldom used `PyArray_DescrCheck` macro has been changed/fixed.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Charles Harris
  - Kevin Sheppard
  - Matti Picus
  - Ralf Gommers
  - Sebastian Berg
  - Warren Weckesser

## Pull requests merged

A total of 12 pull requests were merged for this release.

  - [\#14456](https://github.com/numpy/numpy/pull/14456): MAINT: clean up pocketfft modules inside numpy.fft namespace.
  - [\#14463](https://github.com/numpy/numpy/pull/14463): BUG: random.hypergeometic assumes npy\_long is npy\_int64, hung...
  - [\#14502](https://github.com/numpy/numpy/pull/14502): BUG: random: Revert gh-14458 and refix gh-14557.
  - [\#14504](https://github.com/numpy/numpy/pull/14504): BUG: add a specialized loop for boolean matmul.
  - [\#14506](https://github.com/numpy/numpy/pull/14506): MAINT: Update pytest version for Python 3.8
  - [\#14512](https://github.com/numpy/numpy/pull/14512): DOC: random: fix doc linking, was referencing private submodules.
  - [\#14513](https://github.com/numpy/numpy/pull/14513): BUG,MAINT: Some fixes and minor cleanup based on clang analysis
  - [\#14515](https://github.com/numpy/numpy/pull/14515): BUG: Fix randint when range is 2\*\*32
  - [\#14519](https://github.com/numpy/numpy/pull/14519): MAINT: remove the entropy c-extension module
  - [\#14563](https://github.com/numpy/numpy/pull/14563): DOC: remove note about Pocketfft license file (non-existing here).
  - [\#14578](https://github.com/numpy/numpy/pull/14578): BUG: random: Create a legacy implementation of random.binomial.
  - [\#14687](https://github.com/numpy/numpy/pull/14687): BUG: properly define PyArray\_DescrCheck

---

1.17.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.17.4 Release Notes

This release contains fixes for bugs reported against NumPy 1.17.3 along with some build improvements. The Python versions supported in this release are 3.5-3.8.

Downstream developers should use Cython \>= 0.29.13 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Highlights

  - Fixed <span class="title-ref">random.random\_integers</span> biased generation of 8 and 16 bit integers.
  - Fixed <span class="title-ref">np.einsum</span> regression on Power9 and z/Linux.
  - Fixed histogram problem with signed integer arrays.

## Contributors

A total of 5 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Chris Burr +
  - Matti Picus
  - Qiming Sun +
  - Warren Weckesser

## Pull requests merged

A total of 8 pull requests were merged for this release.

  - [\#14758](https://github.com/numpy/numpy/pull/14758): BLD: declare support for python 3.8
  - [\#14781](https://github.com/numpy/numpy/pull/14781): BUG: random: biased samples from integers() with 8 or 16 bit...
  - [\#14851](https://github.com/numpy/numpy/pull/14851): BUG: Fix \_ctypes class circular reference. (\#13808)
  - [\#14852](https://github.com/numpy/numpy/pull/14852): BLD: add 'apt update' to shippable
  - [\#14855](https://github.com/numpy/numpy/pull/14855): BUG: Fix <span class="title-ref">np.einsum</span> errors on Power9 Linux and z/Linux
  - [\#14857](https://github.com/numpy/numpy/pull/14857): BUG: lib: Fix histogram problem with signed integer arrays.
  - [\#14858](https://github.com/numpy/numpy/pull/14858): BLD: Prevent -flto from optimising long double representation...
  - [\#14866](https://github.com/numpy/numpy/pull/14866): MAINT: move buffer.h -\> npy\_buffer.h to avoid conflicts

---

1.17.5-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.17.5 Release Notes

This release contains fixes for bugs reported against NumPy 1.17.4 along with some build improvements. The Python versions supported in this release are 3.5-3.8.

Downstream developers should use Cython \>= 0.29.14 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

It is recommended that developers interested in the new random bit generators upgrade to the NumPy 1.18.x series, as it has updated documentation and many small improvements.

## Contributors

A total of 6 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Eric Wieser
  - Ilhan Polat
  - Matti Picus
  - Michael Hudson-Doyle
  - Ralf Gommers

## Pull requests merged

A total of 8 pull requests were merged for this release.

  - [\#14593](https://github.com/numpy/numpy/pull/14593): MAINT: backport Cython API cleanup to 1.17.x, remove docs
  - [\#14937](https://github.com/numpy/numpy/pull/14937): BUG: fix integer size confusion in handling array's ndmin argument
  - [\#14939](https://github.com/numpy/numpy/pull/14939): BUILD: remove SSE2 flag from numpy.random builds
  - [\#14993](https://github.com/numpy/numpy/pull/14993): MAINT: Added Python3.8 branch to dll lib discovery
  - [\#15038](https://github.com/numpy/numpy/pull/15038): BUG: Fix refcounting in ufunc object loops
  - [\#15067](https://github.com/numpy/numpy/pull/15067): BUG: Exceptions tracebacks are dropped
  - [\#15175](https://github.com/numpy/numpy/pull/15175): ENH: Backport improvements to testing functions.
  - [\#15213](https://github.com/numpy/numpy/pull/15213): REL: Prepare for the NumPy 1.17.5 release.

---

1.18.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.18.0 Release Notes

In addition to the usual bug fixes, this NumPy release cleans up and documents the new random C-API, expires a large number of old deprecations, and improves the appearance of the documentation. The Python versions supported are 3.5-3.8. This is the last NumPy release series that will support Python 3.5.

Downstream developers should use Cython \>= 0.29.14 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid problems on the Skylake architecture.

## Highlights

  - The C-API for `numpy.random` has been defined and documented.
  - Basic infrastructure for linking with 64 bit BLAS and LAPACK libraries.
  - Many documentation improvements.

## New functions

### Multivariate hypergeometric distribution added to `numpy.random`

The method `multivariate_hypergeometric` has been added to the class <span class="title-ref">numpy.random.Generator</span>. This method generates random variates from the multivariate hypergeometric probability distribution. ([gh-13794](https://github.com/numpy/numpy/pull/13794))

## Deprecations

### `np.fromfile` and `np.fromstring` will error on bad data

In future numpy releases, the functions `np.fromfile` and `np.fromstring` will throw an error when parsing bad data. This will now give a `DeprecationWarning` where previously partial or even invalid data was silently returned. This deprecation also affects the C defined functions `PyArray_FromString` and `PyArray_FromFile` ([gh-13605](https://github.com/numpy/numpy/pull/13605))

### Deprecate non-scalar arrays as fill values in `ma.fill_value`

Setting a `MaskedArray.fill_value` to a non-scalar array is deprecated since the logic to broadcast the fill value to the array is fragile, especially when slicing. ([gh-13698](https://github.com/numpy/numpy/pull/13698))

### Deprecate `PyArray_As1D`, `PyArray_As2D`

`PyArray_As1D`, `PyArray_As2D` are deprecated, use `PyArray_AsCArray` instead ([gh-14036](https://github.com/numpy/numpy/pull/14036))

### Deprecate `np.alen`

`np.alen` was deprecated. Use `len` instead. ([gh-14181](https://github.com/numpy/numpy/pull/14181))

### Deprecate the financial functions

In accordance with [NEP-32](https://numpy.org/neps/nep-0032-remove-financial-functions.html), the financial functions `fv` `ipmt`, `irr`, `mirr`, `nper`, `npv`, `pmt`, `ppmt`, `pv` and `rate` are deprecated, and will be removed from NumPy 1.20.The replacement for these functions is the Python package [numpy-financial](https://pypi.org/project/numpy-financial). ([gh-14720](https://github.com/numpy/numpy/pull/14720))

### The `axis` argument to `numpy.ma.mask_cols` and `numpy.ma.mask_row` is deprecated

This argument was always ignored. ([gh-14996](https://github.com/numpy/numpy/pull/14996))

## Expired deprecations

  - `PyArray_As1D` and `PyArray_As2D` have been removed in favor of `PyArray_AsCArray` ([gh-14036](https://github.com/numpy/numpy/pull/14036))
  - `np.rank` has been removed. This was deprecated in NumPy 1.10 and has been replaced by `np.ndim`. ([gh-14039](https://github.com/numpy/numpy/pull/14039))
  - The deprecation of `expand_dims` out-of-range axes in 1.13.0 has expired. ([gh-14051](https://github.com/numpy/numpy/pull/14051))
  - `PyArray_FromDimsAndDataAndDescr` and `PyArray_FromDims` have been removed (they will always raise an error). Use `PyArray_NewFromDescr` and `PyArray_SimpleNew` instead. ([gh-14100](https://github.com/numpy/numpy/pull/14100))
  - `numeric.loads`, `numeric.load`, `np.ma.dump`, `np.ma.dumps`, `np.ma.load`, `np.ma.loads` are removed, use `pickle` methods instead ([gh-14256](https://github.com/numpy/numpy/pull/14256))
  - `arrayprint.FloatFormat`, `arrayprint.LongFloatFormat` has been removed, use `FloatingFormat` instead
  - `arrayprint.ComplexFormat`, `arrayprint.LongComplexFormat` has been removed, use `ComplexFloatingFormat` instead
  - `arrayprint.StructureFormat` has been removed, use `StructureVoidFormat` instead ([gh-14259](https://github.com/numpy/numpy/pull/14259))
  - `np.testing.rand` has been removed. This was deprecated in NumPy 1.11 and has been replaced by `np.random.rand`. ([gh-14325](https://github.com/numpy/numpy/pull/14325))
  - Class `SafeEval` in `numpy/lib/utils.py` has been removed. This was deprecated in NumPy 1.10. Use `np.safe_eval` instead. ([gh-14335](https://github.com/numpy/numpy/pull/14335))
  - Remove deprecated support for boolean and empty condition lists in `np.select` ([gh-14583](https://github.com/numpy/numpy/pull/14583))
  - Array order only accepts 'C', 'F', 'A', and 'K'. More permissive options were deprecated in NumPy 1.11. ([gh-14596](https://github.com/numpy/numpy/pull/14596))
  - np.linspace parameter `num` must be an integer. Deprecated in NumPy 1.12. ([gh-14620](https://github.com/numpy/numpy/pull/14620))
  - UFuncs with multiple outputs must use a tuple for the `out` kwarg. This finishes a deprecation started in NumPy 1.10. ([gh-14682](https://github.com/numpy/numpy/pull/14682))

The files `numpy/testing/decorators.py`, `numpy/testing/noseclasses.py` and `numpy/testing/nosetester.py` have been removed. They were never meant to be public (all relevant objects are present in the `numpy.testing` namespace), and importing them has given a deprecation warning since NumPy 1.15.0 ([gh-14567](https://github.com/numpy/numpy/pull/14567))

## Compatibility notes

### <span class="title-ref">numpy.lib.recfunctions.drop\_fields</span> can no longer return None

If `drop_fields` is used to drop all fields, previously the array would be completely discarded and None returned. Now it returns an array of the same shape as the input, but with no fields. The old behavior can be retained with:

    dropped_arr = drop_fields(arr, ['a', 'b'])
    if dropped_arr.dtype.names == ():
        dropped_arr = None

converting the empty recarray to None ([gh-14510](https://github.com/numpy/numpy/pull/14510))

### `numpy.argmin/argmax/min/max` returns `NaT` if it exists in array

`numpy.argmin`, `numpy.argmax`, `numpy.min`, and `numpy.max` will return `NaT` if it exists in the array. ([gh-14717](https://github.com/numpy/numpy/pull/14717))

### `np.can_cast(np.uint64, np.timedelta64, casting='safe')` is now `False`

Previously this was `True` - however, this was inconsistent with `uint64` not being safely castable to `int64`, and resulting in strange type resolution.

If this impacts your code, cast `uint64` to `int64` first. ([gh-14718](https://github.com/numpy/numpy/pull/14718))

### Changed random variate stream from `numpy.random.Generator.integers`

There was a bug in `numpy.random.Generator.integers` that caused biased sampling of 8 and 16 bit integer types. Fixing that bug has changed the output stream from what it was in previous releases. ([gh-14777](https://github.com/numpy/numpy/pull/14777))

### Add more ufunc loops for `datetime64`, `timedelta64`

`np.datetime('NaT')` should behave more like `float('Nan')`. Add needed infrastructure so `np.isinf(a)` and `np.isnan(a)` will run on `datetime64` and `timedelta64` dtypes. Also added specific loops for `numpy.fmin` and `numpy.fmax` that mask `NaT`. This may require adjustment to user- facing code. Specifically, code that either disallowed the calls to `numpy.isinf` or `numpy.isnan` or checked that they raised an exception will require adaptation, and code that mistakenly called `numpy.fmax` and `numpy.fmin` instead of `numpy.maximum` or `numpy.minimum` respectively will require adjustment. This also affects `numpy.nanmax` and `numpy.nanmin`. ([gh-14841](https://github.com/numpy/numpy/pull/14841))

### Moved modules in `numpy.random`

As part of the API cleanup, the submodules in `numpy.random` `bit_generator`, `philox`, `pcg64`, `sfc64,`common`,`generator`, and`bounded\_integers`were moved to`\_bit\_generator`,`\_philox`,`\_pcg64`,`\_sfc64, `_common`, `_generator`, and `_bounded_integers` respectively to indicate that they are not part of the public interface. ([gh-14608](https://github.com/numpy/numpy/pull/14608))

## C API changes

### `PyDataType_ISUNSIZED(descr)` now returns False for structured datatypes

Previously this returned True for any datatype of itemsize 0, but now this returns false for the non-flexible datatype with itemsize 0, `np.dtype([])`. ([gh-14393](https://github.com/numpy/numpy/pull/14393))

## New Features

### Add our own `*.pxd` cython import file

Added a `numpy/__init__.pxd` file. It will be used for `cimport numpy` ([gh-12284](https://github.com/numpy/numpy/pull/12284))

### A tuple of axes can now be input to `expand_dims`

The `numpy.expand_dims` `axis` keyword can now accept a tuple of axes. Previously, `axis` was required to be an integer. ([gh-14051](https://github.com/numpy/numpy/pull/14051))

### Support for 64-bit OpenBLAS

Added support for 64-bit (ILP64) OpenBLAS. See `site.cfg.example` for details. ([gh-15012](https://github.com/numpy/numpy/pull/15012))

### Add `--f2cmap` option to F2PY

Allow specifying a file to load Fortran-to-C type map customizations from. ([gh-15113](https://github.com/numpy/numpy/pull/15113))

## Improvements

### Different C numeric types of the same size have unique names

On any given platform, two of `np.intc`, `np.int_`, and `np.longlong` would previously appear indistinguishable through their `repr`, despite their corresponding `dtype` having different properties. A similar problem existed for the unsigned counterparts to these types, and on some platforms for `np.double` and `np.longdouble`

These types now always print with a unique `__name__`. ([gh-10151](https://github.com/numpy/numpy/pull/10151))

### `argwhere` now produces a consistent result on 0d arrays

On N-d arrays, `numpy.argwhere` now always produces an array of shape `(n_non_zero, arr.ndim)`, even when `arr.ndim == 0`. Previously, the last axis would have a dimension of 1 in this case. ([gh-13610](https://github.com/numpy/numpy/pull/13610))

### Add `axis` argument for `random.permutation` and `random.shuffle`

Previously the `random.permutation` and `random.shuffle` functions can only shuffle an array along the first axis; they now have a new argument `axis` which allows shuffle along a specified axis. ([gh-13829](https://github.com/numpy/numpy/pull/13829))

### `method` keyword argument for `np.random.multivariate_normal`

A `method` keyword argument is now available for `np.random.multivariate_normal` with possible values `{'svd', 'eigh', 'cholesky'}`. To use it, write `np.random.multivariate_normal(..., method=<method>)`. ([gh-14197](https://github.com/numpy/numpy/pull/14197))

### Add complex number support for `numpy.fromstring`

Now `numpy.fromstring` can read complex numbers. ([gh-14227](https://github.com/numpy/numpy/pull/14227))

### `numpy.unique` has consistent axes order when `axis` is not None

Using `moveaxis` instead of `swapaxes` in `numpy.unique`, so that the ordering of axes except the axis in arguments will not be broken. ([gh-14255](https://github.com/numpy/numpy/pull/14255))

### `numpy.matmul` with boolean output now converts to boolean values

Calling `numpy.matmul` where the output is a boolean array would fill the array with uint8 equivalents of the result, rather than 0/1. Now it forces the output to 0 or 1 (`NPY_TRUE` or `NPY_FALSE`). ([gh-14464](https://github.com/numpy/numpy/pull/14464))

### `numpy.random.randint` produced incorrect value when the range was `2**32`

The implementation introduced in 1.17.0 had an incorrect check when determining whether to use the 32-bit path or the full 64-bit path that incorrectly redirected random integer generation with a high - low range of `2**32` to the 64-bit generator. ([gh-14501](https://github.com/numpy/numpy/pull/14501))

### Add complex number support for `numpy.fromfile`

Now `numpy.fromfile` can read complex numbers. ([gh-14730](https://github.com/numpy/numpy/pull/14730))

### `std=c99` added if compiler is named `gcc`

GCC before version 5 requires the `-std=c99` command line argument. Newer compilers automatically turn on C99 mode. The compiler setup code will automatically add the code if the compiler name has `gcc` in it. ([gh-14771](https://github.com/numpy/numpy/pull/14771))

## Changes

### `NaT` now sorts to the end of arrays

`NaT` is now effectively treated as the largest integer for sorting purposes, so that it sorts to the end of arrays. This change is for consistency with `NaN` sorting behavior. ([gh-12658](https://github.com/numpy/numpy/pull/12658)) ([gh-15068](https://github.com/numpy/numpy/pull/15068))

### Incorrect `threshold` in `np.set_printoptions` raises `TypeError` or `ValueError`

Previously an incorrect `threshold` raised `ValueError`; it now raises `TypeError` for non-numeric types and `ValueError` for `nan` values. ([gh-13899](https://github.com/numpy/numpy/pull/13899))

### Warn when saving a dtype with metadata

A `UserWarning` will be emitted when saving an array via `numpy.save` with `metadata`. Saving such an array may not preserve metadata, and if metadata is preserved, loading it will cause a `ValueError`. This shortcoming in save and load will be addressed in a future release. ([gh-14142](https://github.com/numpy/numpy/pull/14142))

### `numpy.distutils` append behavior changed for LDFLAGS and similar

<span class="title-ref">numpy.distutils</span> has always overridden rather than appended to `LDFLAGS` and other similar such environment variables for compiling Fortran extensions. Now the default behavior has changed to appending - which is the expected behavior in most situations. To preserve the old (overwriting) behavior, set the `NPY_DISTUTILS_APPEND_FLAGS` environment variable to 0. This applies to: `LDFLAGS`, `F77FLAGS`, `F90FLAGS`, `FREEFLAGS`, `FOPT`, `FDEBUG`, and `FFLAGS`. NumPy 1.16 and 1.17 gave build warnings in situations where this change in behavior would have affected the compile flags used. ([gh-14248](https://github.com/numpy/numpy/pull/14248))

### Remove `numpy.random.entropy` without a deprecation

`numpy.random.entropy` was added to the `numpy.random` namespace in 1.17.0. It was meant to be a private c-extension module, but was exposed as public. It has been replaced by `numpy.random.SeedSequence` so the module was completely removed. ([gh-14498](https://github.com/numpy/numpy/pull/14498))

### Add options to quiet build configuration and build with `-Werror`

Added two new configuration options. During the `build_src` subcommand, as part of configuring NumPy, the files `_numpyconfig.h` and `config.h` are created by probing support for various runtime functions and routines. Previously, the very verbose compiler output during this stage clouded more important information. By default the output is silenced. Running `runtests.py --debug-info` will add `--verbose-cfg` to the `build_src` subcommand,which will restore the previous behaviour.

Adding `CFLAGS=-Werror` to turn warnings into errors would trigger errors during the configuration. Now `runtests.py --warn-error` will add `--warn-error` to the `build` subcommand, which will percolate to the `build_ext` and `build_lib` subcommands. This will add the compiler flag to those stages and turn compiler warnings into errors while actually building NumPy itself, avoiding the `build_src` subcommand compiler calls.

([gh-14527](https://github.com/numpy/numpy/pull/14527)) ([gh-14518](https://github.com/numpy/numpy/pull/14518))

---

1.18.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.18.1 Release Notes

This release contains fixes for bugs reported against NumPy 1.18.0. Two bugs in particular that caused widespread problems downstream were:

  - The cython random extension test was not using a temporary directory for building, resulting in a permission violation. Fixed.
  - Numpy distutils was appending <span class="title-ref">-std=c99</span> to all C compiler runs, leading to changed behavior and compile problems downstream. That flag is now only applied when building numpy C code.

The Python versions supported in this release are 3.5-3.8. Downstream developers should use Cython \>= 0.29.14 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Matti Picus
  - Maxwell Aladago
  - Pauli Virtanen
  - Ralf Gommers
  - Tyler Reddy
  - Warren Weckesser

## Pull requests merged

A total of 13 pull requests were merged for this release.

  - [\#15158](https://github.com/numpy/numpy/pull/15158): MAINT: Update pavement.py for towncrier.
  - [\#15159](https://github.com/numpy/numpy/pull/15159): DOC: add moved modules to 1.18 release note
  - [\#15161](https://github.com/numpy/numpy/pull/15161): MAINT, DOC: Minor backports and updates for 1.18.x
  - [\#15176](https://github.com/numpy/numpy/pull/15176): TST: Add assert\_array\_equal test for big integer arrays
  - [\#15184](https://github.com/numpy/numpy/pull/15184): BUG: use tmp dir and check version for cython test (\#15170)
  - [\#15220](https://github.com/numpy/numpy/pull/15220): BUG: distutils: fix msvc+gfortran openblas handling corner case
  - [\#15221](https://github.com/numpy/numpy/pull/15221): BUG: remove -std=c99 for c++ compilation (\#15194)
  - [\#15222](https://github.com/numpy/numpy/pull/15222): MAINT: unskip test on win32
  - [\#15223](https://github.com/numpy/numpy/pull/15223): TST: add BLAS ILP64 run in Travis & Azure
  - [\#15245](https://github.com/numpy/numpy/pull/15245): MAINT: only add --std=c99 where needed
  - [\#15246](https://github.com/numpy/numpy/pull/15246): BUG: lib: Fix handling of integer arrays by gradient.
  - [\#15247](https://github.com/numpy/numpy/pull/15247): MAINT: Do not use private Python function in testing
  - [\#15250](https://github.com/numpy/numpy/pull/15250): REL: Prepare for the NumPy 1.18.1 release.

---

1.18.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.18.2 Release Notes

This small release contains a fix for a performance regression in numpy/random and several bug/maintenance updates.

The Python versions supported in this release are 3.5-3.8. Downstream developers should use Cython \>= 0.29.15 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Contributors

A total of 5 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Ganesh Kathiresan +
  - Matti Picus
  - Sebastian Berg
  - przemb +

## Pull requests merged

A total of 7 pull requests were merged for this release.

  - [\#15675](https://github.com/numpy/numpy/pull/15675): TST: move \_no\_tracing to testing.\_private
  - [\#15676](https://github.com/numpy/numpy/pull/15676): MAINT: Large overhead in some random functions
  - [\#15677](https://github.com/numpy/numpy/pull/15677): TST: Do not create gfortran link in azure Mac testing.
  - [\#15679](https://github.com/numpy/numpy/pull/15679): BUG: Added missing error check in <span class="title-ref">ndarray.\_\_contains\_\_</span>
  - [\#15722](https://github.com/numpy/numpy/pull/15722): MAINT: use list-based APIs to call subprocesses
  - [\#15729](https://github.com/numpy/numpy/pull/15729): REL: Prepare for 1.18.2 release.
  - [\#15734](https://github.com/numpy/numpy/pull/15734): BUG: fix logic error when nm fails on 32-bit

---

1.18.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.18.3 Release Notes

This release contains various bug/regression fixes.

The Python versions supported in this release are 3.5-3.8. Downstream developers should use Cython \>= 0.29.15 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Highlights

  - Fix for the <span class="title-ref">method='eigh'</span> and <span class="title-ref">method='cholesky'</span> methods in <span class="title-ref">numpy.random.multivariate\_normal</span>. Those were producing samples from the wrong distribution.

## Contributors

A total of 6 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Max Balandat +
  - @Mibu287 +
  - Pan Jan +
  - Sebastian Berg
  - @panpiort8 +

## Pull requests merged

A total of 5 pull requests were merged for this release.

  - [\#15916](https://github.com/numpy/numpy/pull/15916): BUG: Fix eigh and cholesky methods of numpy.random.multivariate\_normal
  - [\#15929](https://github.com/numpy/numpy/pull/15929): BUG,MAINT: Remove incorrect special case in string to number...
  - [\#15930](https://github.com/numpy/numpy/pull/15930): BUG: Guarantee array is in valid state after memory error occurs...
  - [\#15954](https://github.com/numpy/numpy/pull/15954): BUG: Check that <span class="title-ref">pvals</span> is 1D in <span class="title-ref">\_generator.multinomial</span>.
  - [\#16017](https://github.com/numpy/numpy/pull/16017): BUG: Alpha parameter must be 1D in <span class="title-ref">generator.dirichlet</span>

---

1.18.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.18.4 Release Notes

This is the last planned release in the 1.18.x series. It reverts the `bool("0")` behavior introduced in 1.18.3 and fixes a bug in `Generator.integers`. There is also a link to a new troubleshooting section in the documentation included in the error message emitted when numpy import fails.

The Python versions supported in this release are 3.5-3.8. Downstream developers should use Cython \>= 0.29.15 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Contributors

A total of 4 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Matti Picus
  - Sebastian Berg
  - Warren Weckesser

## Pull requests merged

A total of 6 pull requests were merged for this release.

  - [\#16055](https://github.com/numpy/numpy/pull/16055): BLD: add i686 for 1.18 builds
  - [\#16090](https://github.com/numpy/numpy/pull/16090): BUG: random: `Generator.integers(2**32)` always returned 0.
  - [\#16091](https://github.com/numpy/numpy/pull/16091): BLD: fix path to libgfortran on macOS
  - [\#16109](https://github.com/numpy/numpy/pull/16109): REV: Reverts side-effect changes to casting
  - [\#16114](https://github.com/numpy/numpy/pull/16114): BLD: put openblas library in local directory on windows
  - [\#16132](https://github.com/numpy/numpy/pull/16132): DOC: Change import error "howto" to link to new troubleshooting...

---

1.18.5-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.18.5 Release Notes

This is a short release to allow pickle `protocol=5` to be used in Python3.5. It is motivated by the recent backport of pickle5 to Python3.5.

The Python versions supported in this release are 3.5-3.8. Downstream developers should use Cython \>= 0.29.15 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid errors on the Skylake architecture.

## Contributors

A total of 3 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Matti Picus
  - Siyuan Zhuang +

## Pull requests merged

A total of 2 pull requests were merged for this release.

  - [\#16439](https://github.com/numpy/numpy/pull/16439): ENH: enable pickle protocol 5 support for python3.5
  - [\#16441](https://github.com/numpy/numpy/pull/16441): BUG: relpath fails for different drives on windows

---

1.19.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.19.0 Release Notes

This NumPy release is marked by the removal of much technical debt: support for Python 2 has been removed, many deprecations have been expired, and documentation has been improved. The polishing of the random module continues apace with bug fixes and better usability from Cython.

The Python versions supported for this release are 3.6-3.8. Downstream developers should use Cython \>= 0.29.16 for Python 3.8 support and OpenBLAS \>= 3.7 to avoid problems on the Skylake architecture.

## Highlights

  - Code compatibility with Python versions \< 3.6 (including Python 2) was dropped from both the python and C code. The shims in `numpy.compat` will remain to support third-party packages, but they may be deprecated in a future release. Note that 1.19.x will *not* compile with earlier versions of Python due to the use of f-strings.
    
    ([gh-15233](https://github.com/numpy/numpy/pull/15233))

## Expired deprecations

### `numpy.insert` and `numpy.delete` can no longer be passed an axis on 0d arrays

This concludes a deprecation from 1.9, where when an `axis` argument was passed to a call to `~numpy.insert` and `~numpy.delete` on a 0d array, the `axis` and `obj` argument and indices would be completely ignored. In these cases, `insert(arr, "nonsense", 42, axis=0)` would actually overwrite the entire array, while `delete(arr, "nonsense", axis=0)` would be `arr.copy()`

Now passing `axis` on a 0d array raises `~numpy.AxisError`.

([gh-15802](https://github.com/numpy/numpy/pull/15802))

### `numpy.delete` no longer ignores out-of-bounds indices

This concludes deprecations from 1.8 and 1.9, where `np.delete` would ignore both negative and out-of-bounds items in a sequence of indices. This was at odds with its behavior when passed a single index.

Now out-of-bounds items throw `IndexError`, and negative items index from the end.

([gh-15804](https://github.com/numpy/numpy/pull/15804))

### `numpy.insert` and `numpy.delete` no longer accept non-integral indices

This concludes a deprecation from 1.9, where sequences of non-integers indices were allowed and cast to integers. Now passing sequences of non-integral indices raises `IndexError`, just like it does when passing a single non-integral scalar.

([gh-15805](https://github.com/numpy/numpy/pull/15805))

### `numpy.delete` no longer casts boolean indices to integers

This concludes a deprecation from 1.8, where `np.delete` would cast boolean arrays and scalars passed as an index argument into integer indices. The behavior now is to treat boolean arrays as a mask, and to raise an error on boolean scalars.

([gh-15815](https://github.com/numpy/numpy/pull/15815))

## Compatibility notes

### Changed random variate stream from `numpy.random.Generator.dirichlet`

A bug in the generation of random variates for the Dirichlet distribution with small 'alpha' values was fixed by using a different algorithm when `max(alpha) < 0.1`. Because of the change, the stream of variates generated by `dirichlet` in this case will be different from previous releases.

([gh-14924](https://github.com/numpy/numpy/pull/14924))

### Scalar promotion in `PyArray_ConvertToCommonType`

The promotion of mixed scalars and arrays in `PyArray_ConvertToCommonType` has been changed to adhere to those used by `np.result_type`. This means that input such as `(1000, np.array([1], dtype=np.uint8)))` will now return `uint16` dtypes. In most cases the behaviour is unchanged. Note that the use of this C-API function is generally discouraged. This also fixes `np.choose` to behave the same way as the rest of NumPy in this respect.

([gh-14933](https://github.com/numpy/numpy/pull/14933))

### Fasttake and fastputmask slots are deprecated and NULL'ed

The fasttake and fastputmask slots are now never used and must always be set to NULL. This will result in no change in behaviour. However, if a user dtype should set one of these a DeprecationWarning will be given.

([gh-14942](https://github.com/numpy/numpy/pull/14942))

### `np.ediff1d` casting behaviour with `to_end` and `to_begin`

`np.ediff1d` now uses the `"same_kind"` casting rule for its additional `to_end` and `to_begin` arguments. This ensures type safety except when the input array has a smaller integer type than `to_begin` or `to_end`. In rare cases, the behaviour will be more strict than it was previously in 1.16 and 1.17. This is necessary to solve issues with floating point NaN.

([gh-14981](https://github.com/numpy/numpy/pull/14981))

### Converting of empty array-like objects to NumPy arrays

Objects with `len(obj) == 0` which implement an "array-like" interface, meaning an object implementing `obj.__array__()`, `obj.__array_interface__`, `obj.__array_struct__`, or the python buffer interface and which are also sequences (i.e. Pandas objects) will now always retain there shape correctly when converted to an array. If such an object has a shape of `(0, 1)` previously, it could be converted into an array of shape `(0,)` (losing all dimensions after the first 0).

([gh-14995](https://github.com/numpy/numpy/pull/14995))

### Removed `multiarray.int_asbuffer`

As part of the continued removal of Python 2 compatibility, `multiarray.int_asbuffer` was removed. On Python 3, it threw a `NotImplementedError` and was unused internally. It is expected that there are no downstream use cases for this method with Python 3.

([gh-15229](https://github.com/numpy/numpy/pull/15229))

### `numpy.distutils.compat` has been removed

This module contained only the function `get_exception()`, which was used as:

    try:
        ...
    except Exception:
        e = get_exception()

Its purpose was to handle the change in syntax introduced in Python 2.6, from `except Exception, e:` to `except Exception as e:`, meaning it was only necessary for codebases supporting Python 2.5 and older.

([gh-15255](https://github.com/numpy/numpy/pull/15255))

### `issubdtype` no longer interprets `float` as `np.floating`

`numpy.issubdtype` had a FutureWarning since NumPy 1.14 which has expired now. This means that certain input where the second argument was neither a datatype nor a NumPy scalar type (such as a string or a python type like `int` or `float`) will now be consistent with passing in `np.dtype(arg2).type`. This makes the result consistent with expectations and leads to a false result in some cases which previously returned true.

([gh-15773](https://github.com/numpy/numpy/pull/15773))

### Change output of `round` on scalars to be consistent with Python

Output of the `__round__` dunder method and consequently the Python built-in `round` has been changed to be a Python `int` to be consistent with calling it on Python `float` objects when called with no arguments. Previously, it would return a scalar of the `np.dtype` that was passed in.

([gh-15840](https://github.com/numpy/numpy/pull/15840))

### The `numpy.ndarray` constructor no longer interprets `strides=()` as `strides=None`

The former has changed to have the expected meaning of setting `numpy.ndarray.strides` to `()`, while the latter continues to result in strides being chosen automatically.

([gh-15882](https://github.com/numpy/numpy/pull/15882))

### C-Level string to datetime casts changed

The C-level casts from strings were simplified. This changed also fixes string to datetime and timedelta casts to behave correctly (i.e. like Python casts using `string_arr.astype("M8")` while previously the cast would behave like `string_arr.astype(np.int_).astype("M8")`. This only affects code using low-level C-API to do manual casts (not full array casts) of single scalar values or using e.g. `PyArray_GetCastFunc`, and should thus not affect the vast majority of users.

([gh-16068](https://github.com/numpy/numpy/pull/16068))

### `SeedSequence` with small seeds no longer conflicts with spawning

Small seeds (less than `2**96`) were previously implicitly 0-padded out to 128 bits, the size of the internal entropy pool. When spawned, the spawn key was concatenated before the 0-padding. Since the first spawn key is `(0,)`, small seeds before the spawn created the same states as the first spawned `SeedSequence`. Now, the seed is explicitly 0-padded out to the internal pool size before concatenating the spawn key. Spawned `SeedSequences` will produce different results than in the previous release. Unspawned `SeedSequences` will still produce the same results.

([gh-16551](https://github.com/numpy/numpy/pull/16551))

## Deprecations

### Deprecate automatic `dtype=object` for ragged input

Calling `np.array([[1, [1, 2, 3]])` will issue a `DeprecationWarning` as per [NEP 34](https://numpy.org/neps/nep-0034.html). Users should explicitly use `dtype=object` to avoid the warning.

([gh-15119](https://github.com/numpy/numpy/pull/15119))

### Passing `shape=0` to factory functions in `numpy.rec` is deprecated

`0` is treated as a special case and is aliased to `None` in the functions:

  - `numpy.core.records.fromarrays`
  - `numpy.core.records.fromrecords`
  - `numpy.core.records.fromstring`
  - `numpy.core.records.fromfile`

In future, `0` will not be special cased, and will be treated as an array length like any other integer.

([gh-15217](https://github.com/numpy/numpy/pull/15217))

### Deprecation of probably unused C-API functions

The following C-API functions are probably unused and have been deprecated:

  - `PyArray_GetArrayParamsFromObject`
  - `PyUFunc_GenericFunction`
  - `PyUFunc_SetUsesArraysAsData`

In most cases `PyArray_GetArrayParamsFromObject` should be replaced by converting to an array, while `PyUFunc_GenericFunction` can be replaced with `PyObject_Call` (see documentation for details).

([gh-15427](https://github.com/numpy/numpy/pull/15427))

### Converting certain types to dtypes is Deprecated

The super classes of scalar types, such as `np.integer`, `np.generic`, or `np.inexact` will now give a deprecation warning when converted to a dtype (or used in a dtype keyword argument). The reason for this is that `np.integer` is converted to `np.int_`, while it would be expected to represent *any* integer (e.g. also `int8`, `int16`, etc. For example, `dtype=np.floating` is currently identical to `dtype=np.float64`, even though also `np.float32` is a subclass of `np.floating`.

([gh-15534](https://github.com/numpy/numpy/pull/15534))

### Deprecation of `round` for `np.complexfloating` scalars

Output of the `__round__` dunder method and consequently the Python built-in `round` has been deprecated on complex scalars. This does not affect `np.round`.

([gh-15840](https://github.com/numpy/numpy/pull/15840))

### `numpy.ndarray.tostring()` is deprecated in favor of `tobytes()`

`~numpy.ndarray.tobytes` has existed since the 1.9 release, but until this release `~numpy.ndarray.tostring` emitted no warning. The change to emit a warning brings NumPy in line with the builtin `array.array` methods of the same name.

([gh-15867](https://github.com/numpy/numpy/pull/15867))

## C API changes

### Better support for `const` dimensions in API functions

The following functions now accept a constant array of `npy_intp`:

  - `PyArray_BroadcastToShape`
  - `PyArray_IntTupleFromIntp`
  - `PyArray_OverflowMultiplyList`

Previously the caller would have to cast away the const-ness to call these functions.

([gh-15251](https://github.com/numpy/numpy/pull/15251))

### Const qualify UFunc inner loops

`UFuncGenericFunction` now expects pointers to const `dimension` and `strides` as arguments. This means inner loops may no longer modify either `dimension` or `strides`. This change leads to an `incompatible-pointer-types` warning forcing users to either ignore the compiler warnings or to const qualify their own loop signatures.

([gh-15355](https://github.com/numpy/numpy/pull/15355))

## New Features

### `numpy.frompyfunc` now accepts an identity argument

This allows the <span class="title-ref">numpy.ufunc.identity</span> attribute to be set on the resulting ufunc, meaning it can be used for empty and multi-dimensional calls to <span class="title-ref">numpy.ufunc.reduce</span>.

([gh-8255](https://github.com/numpy/numpy/pull/8255))

### `np.str_` scalars now support the buffer protocol

`np.str_` arrays are always stored as UCS4, so the corresponding scalars now expose this through the buffer interface, meaning `memoryview(np.str_('test'))` now works.

([gh-15385](https://github.com/numpy/numpy/pull/15385))

### `subok` option for `numpy.copy`

A new kwarg, `subok`, was added to `numpy.copy` to allow users to toggle the behavior of `numpy.copy` with respect to array subclasses. The default value is `False` which is consistent with the behavior of `numpy.copy` for previous numpy versions. To create a copy that preserves an array subclass with `numpy.copy`, call `np.copy(arr, subok=True)`. This addition better documents that the default behavior of `numpy.copy` differs from the `numpy.ndarray.copy` method which respects array subclasses by default.

([gh-15685](https://github.com/numpy/numpy/pull/15685))

### `numpy.linalg.multi_dot` now accepts an `out` argument

`out` can be used to avoid creating unnecessary copies of the final product computed by `numpy.linalg.multidot`.

([gh-15715](https://github.com/numpy/numpy/pull/15715))

### `keepdims` parameter for `numpy.count_nonzero`

The parameter `keepdims` was added to `numpy.count_nonzero`. The parameter has the same meaning as it does in reduction functions such as `numpy.sum` or `numpy.mean`.

([gh-15870](https://github.com/numpy/numpy/pull/15870))

### `equal_nan` parameter for `numpy.array_equal`

The keyword argument `equal_nan` was added to `numpy.array_equal`. `equal_nan` is a boolean value that toggles whether or not `nan` values are considered equal in comparison (default is `False`). This matches API used in related functions such as `numpy.isclose` and `numpy.allclose`.

([gh-16128](https://github.com/numpy/numpy/pull/16128))

## Improvements

## Improve detection of CPU features

Replace `npy_cpu_supports` which was a gcc specific mechanism to test support of AVX with more general functions `npy_cpu_init` and `npy_cpu_have`, and expose the results via a `NPY_CPU_HAVE` c-macro as well as a python-level `__cpu_features__` dictionary.

([gh-13421](https://github.com/numpy/numpy/pull/13421))

### Use 64-bit integer size on 64-bit platforms in fallback lapack\_lite

Use 64-bit integer size on 64-bit platforms in the fallback LAPACK library, which is used when the system has no LAPACK installed, allowing it to deal with linear algebra for large arrays.

([gh-15218](https://github.com/numpy/numpy/pull/15218))

### Use AVX512 intrinsic to implement `np.exp` when input is `np.float64`

Use AVX512 intrinsic to implement `np.exp` when input is `np.float64`, which can improve the performance of `np.exp` with `np.float64` input 5-7x faster than before. The `_multiarray_umath.so` module has grown about 63 KB on linux64.

([gh-15648](https://github.com/numpy/numpy/pull/15648))

### Ability to disable madvise hugepages

On Linux NumPy has previously added support for madavise hugepages which can improve performance for very large arrays. Unfortunately, on older Kernel versions this led to performance regressions, thus by default the support has been disabled on kernels before version 4.6. To override the default, you can use the environment variable:

    NUMPY_MADVISE_HUGEPAGE=0

or set it to 1 to force enabling support. Note that this only makes a difference if the operating system is set up to use madvise transparent hugepage.

([gh-15769](https://github.com/numpy/numpy/pull/15769))

### `numpy.einsum` accepts NumPy `int64` type in subscript list

There is no longer a type error thrown when `numpy.einsum` is passed a NumPy `int64` array as its subscript list.

([gh-16080](https://github.com/numpy/numpy/pull/16080))

### `np.logaddexp2.identity` changed to `-inf`

The ufunc `~numpy.logaddexp2` now has an identity of `-inf`, allowing it to be called on empty sequences. This matches the identity of `~numpy.logaddexp`.

([gh-16102](https://github.com/numpy/numpy/pull/16102))

## Changes

### Remove handling of extra argument to `__array__`

A code path and test have been in the code since NumPy 0.4 for a two-argument variant of `__array__(dtype=None, context=None)`. It was activated when calling `ufunc(op)` or `ufunc.reduce(op)` if `op.__array__` existed. However that variant is not documented, and it is not clear what the intention was for its use. It has been removed.

([gh-15118](https://github.com/numpy/numpy/pull/15118))

### `numpy.random._bit_generator` moved to `numpy.random.bit_generator`

In order to expose `numpy.random.BitGenerator` and `numpy.random.SeedSequence` to Cython, the `_bitgenerator` module is now public as `numpy.random.bit_generator`

### Cython access to the random distributions is provided via a `pxd` file

`c_distributions.pxd` provides access to the c functions behind many of the random distributions from Cython, making it convenient to use and extend them.

([gh-15463](https://github.com/numpy/numpy/pull/15463))

### Fixed `eigh` and `cholesky` methods in `numpy.random.multivariate_normal`

Previously, when passing `method='eigh'` or `method='cholesky'`, `numpy.random.multivariate_normal` produced samples from the wrong distribution. This is now fixed.

([gh-15872](https://github.com/numpy/numpy/pull/15872))

### Fixed the jumping implementation in `MT19937.jumped`

This fix changes the stream produced from jumped MT19937 generators. It does not affect the stream produced using `RandomState` or `MT19937` that are directly seeded.

The translation of the jumping code for the MT19937 contained a reversed loop ordering. `MT19937.jumped` matches the Makoto Matsumoto's original implementation of the Horner and Sliding Window jump methods.

([gh-16153](https://github.com/numpy/numpy/pull/16153))

---

1.19.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.19.1 Release Notes

NumPy 1.19.1 fixes several bugs found in the 1.19.0 release, replaces several functions deprecated in the upcoming Python-3.9 release, has improved support for AIX, and has a number of development related updates to keep CI working with recent upstream changes.

This release supports Python 3.6-3.8. Cython \>= 0.29.21 needs to be used when building with Python 3.9 for testing purposes.

## Contributors

A total of 15 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Abhinav Reddy +
  - Anirudh Subramanian
  - Antonio Larrosa +
  - Charles Harris
  - Chunlin Fang
  - Eric Wieser
  - Etienne Guesnet +
  - Kevin Sheppard
  - Matti Picus
  - Raghuveer Devulapalli
  - Roman Yurchak
  - Ross Barnowski
  - Sayed Adel
  - Sebastian Berg
  - Tyler Reddy

## Pull requests merged

A total of 25 pull requests were merged for this release.

  - [\#16649](https://github.com/numpy/numpy/pull/16649): MAINT, CI: disable Shippable cache
  - [\#16652](https://github.com/numpy/numpy/pull/16652): MAINT: Replace <span class="title-ref">PyUString\_GET\_SIZE</span> with <span class="title-ref">PyUnicode\_GetLength</span>.
  - [\#16654](https://github.com/numpy/numpy/pull/16654): REL: Fix outdated docs link
  - [\#16656](https://github.com/numpy/numpy/pull/16656): BUG: raise IEEE exception on AIX
  - [\#16672](https://github.com/numpy/numpy/pull/16672): BUG: Fix bug in AVX complex absolute while processing array of...
  - [\#16693](https://github.com/numpy/numpy/pull/16693): TST: Add extra debugging information to CPU features detection
  - [\#16703](https://github.com/numpy/numpy/pull/16703): BLD: Add CPU entry for Emscripten / WebAssembly
  - [\#16705](https://github.com/numpy/numpy/pull/16705): TST: Disable Python 3.9-dev testing.
  - [\#16714](https://github.com/numpy/numpy/pull/16714): MAINT: Disable use\_hugepages in case of ValueError
  - [\#16724](https://github.com/numpy/numpy/pull/16724): BUG: Fix PyArray\_SearchSorted signature.
  - [\#16768](https://github.com/numpy/numpy/pull/16768): MAINT: Fixes for deprecated functions in scalartypes.c.src
  - [\#16772](https://github.com/numpy/numpy/pull/16772): MAINT: Remove unneeded call to PyUnicode\_READY
  - [\#16776](https://github.com/numpy/numpy/pull/16776): MAINT: Fix deprecated functions in scalarapi.c
  - [\#16779](https://github.com/numpy/numpy/pull/16779): BLD, ENH: Add RPATH support for AIX
  - [\#16780](https://github.com/numpy/numpy/pull/16780): BUG: Fix default fallback in genfromtxt
  - [\#16784](https://github.com/numpy/numpy/pull/16784): BUG: Added missing return after raising error in methods.c
  - [\#16795](https://github.com/numpy/numpy/pull/16795): BLD: update cython to 0.29.21
  - [\#16832](https://github.com/numpy/numpy/pull/16832): MAINT: setuptools 49.2.0 emits a warning, avoid it
  - [\#16872](https://github.com/numpy/numpy/pull/16872): BUG: Validate output size in bin- and multinomial
  - [\#16875](https://github.com/numpy/numpy/pull/16875): BLD, MAINT: Pin setuptools
  - [\#16904](https://github.com/numpy/numpy/pull/16904): DOC: Reconstruct Testing Guideline.
  - [\#16905](https://github.com/numpy/numpy/pull/16905): TST, BUG: Re-raise MemoryError exception in test\_large\_zip's...
  - [\#16906](https://github.com/numpy/numpy/pull/16906): BUG,DOC: Fix bad MPL kwarg.
  - [\#16916](https://github.com/numpy/numpy/pull/16916): BUG: Fix string/bytes to complex assignment
  - [\#16922](https://github.com/numpy/numpy/pull/16922): REL: Prepare for NumPy 1.19.1 release

---

1.19.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.19.2 Release Notes

NumPy 1.19.2 fixes several bugs, prepares for the upcoming Cython 3.x release. and pins setuptools to keep distutils working while upstream modifications are ongoing. The aarch64 wheels are built with the latest manylinux2014 release that fixes the problem of differing page sizes used by different linux distros.

This release supports Python 3.6-3.8. Cython \>= 0.29.21 needs to be used when building with Python 3.9 for testing purposes.

There is a known problem with Windows 10 version=2004 and OpenBLAS svd that we are trying to debug. If you are running that Windows version you should use a NumPy version that links to the MKL library, earlier Windows versions are fine.

## Improvements

### Add NumPy declarations for Cython 3.0 and later

The pxd declarations for Cython 3.0 were improved to avoid using deprecated NumPy C-API features. Extension modules built with Cython 3.0+ that use NumPy can now set the C macro `NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION` to avoid C compiler warnings about deprecated API usage.

## Contributors

A total of 8 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Matti Picus
  - Pauli Virtanen
  - Philippe Ombredanne +
  - Sebastian Berg
  - Stefan Behnel +
  - Stephan Loyd +
  - Zac Hatfield-Dodds

## Pull requests merged

A total of 9 pull requests were merged for this release.

  - [\#16959](https://github.com/numpy/numpy/pull/16959): TST: Change aarch64 to arm64 in travis.yml.
  - [\#16998](https://github.com/numpy/numpy/pull/16998): MAINT: Configure hypothesis in `np.test()` for determinism,...
  - [\#17000](https://github.com/numpy/numpy/pull/17000): BLD: pin setuptools \< 49.2.0
  - [\#17015](https://github.com/numpy/numpy/pull/17015): ENH: Add NumPy declarations to be used by Cython 3.0+
  - [\#17125](https://github.com/numpy/numpy/pull/17125): BUG: Remove non-threadsafe sigint handling from fft calculation
  - [\#17243](https://github.com/numpy/numpy/pull/17243): BUG: core: fix ilp64 blas dot/vdot/... for strides \> int32 max
  - [\#17244](https://github.com/numpy/numpy/pull/17244): DOC: Use SPDX license expressions with correct license
  - [\#17245](https://github.com/numpy/numpy/pull/17245): DOC: Fix the link to the quick-start in the old API functions
  - [\#17272](https://github.com/numpy/numpy/pull/17272): BUG: fix pickling of arrays larger than 2GiB

---

1.19.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.19.3 Release Notes

NumPy 1.19.3 is a small maintenance release with two major improvements:

  - Python 3.9 binary wheels on all supported platforms.
  - OpenBLAS fixes for Windows 10 version 2004 fmod bug.

This release supports Python 3.6-3.9 and is linked with OpenBLAS 0.3.12 to avoid some of the fmod problems on Windows 10 version 2004. Microsoft is aware of the problem and users should upgrade when the fix becomes available, the fix here is limited in scope.

## Contributors

A total of 8 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris

  - Chris Brown +

  - Daniel Vanzo +

  - 5.  Madison Bray +

  - Hugo van Kemenade +

  - Ralf Gommers

  - Sebastian Berg

  - @danbeibei +

## Pull requests merged

A total of 10 pull requests were merged for this release.

  - [\#17298](https://github.com/numpy/numpy/pull/17298): BLD: set upper versions for build dependencies
  - [\#17336](https://github.com/numpy/numpy/pull/17336): BUG: Set deprecated fields to null in PyArray\_InitArrFuncs
  - [\#17446](https://github.com/numpy/numpy/pull/17446): ENH: Warn on unsupported Python 3.10+
  - [\#17450](https://github.com/numpy/numpy/pull/17450): MAINT: Update test\_requirements.txt.
  - [\#17522](https://github.com/numpy/numpy/pull/17522): ENH: Support for the NVIDIA HPC SDK nvfortran compiler
  - [\#17568](https://github.com/numpy/numpy/pull/17568): BUG: Cygwin Workaround for \#14787 on affected platforms
  - [\#17647](https://github.com/numpy/numpy/pull/17647): BUG: Fix memory leak of buffer-info cache due to relaxed strides
  - [\#17652](https://github.com/numpy/numpy/pull/17652): MAINT: Backport openblas\_support from master.
  - [\#17653](https://github.com/numpy/numpy/pull/17653): TST: Add Python 3.9 to the CI testing on Windows, Mac.
  - [\#17660](https://github.com/numpy/numpy/pull/17660): TST: Simplify source path names in test\_extending.

---

1.19.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.19.4 Release Notes

NumPy 1.19.4 is a quick release to revert the OpenBLAS library version. It was hoped that the 0.3.12 OpenBLAS version used in 1.19.3 would work around the Microsoft fmod bug, but problems in some docker environments turned up. Instead, 1.19.4 will use the older library and run a sanity check on import, raising an error if the problem is detected. Microsoft is aware of the problem and has promised a fix, users should upgrade when it becomes available.

This release supports Python 3.6-3.9

## Contributors

A total of 1 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris

## Pull requests merged

A total of 2 pull requests were merged for this release.

  - [\#17679](https://github.com/numpy/numpy/pull/17679): MAINT: Add check for Windows 10 version 2004 bug.
  - [\#17680](https://github.com/numpy/numpy/pull/17680): REV: Revert OpenBLAS to 1.19.2 version for 1.19.4

---

1.19.5-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.19.5 Release Notes

NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the main improvement is the update to OpenBLAS 0.3.13 that works around the windows 2004 bug while not breaking execution on other platforms. This release supports Python 3.6-3.9 and is planned to be the last release in the 1.19.x cycle.

## Contributors

A total of 8 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Christoph Gohlke
  - Matti Picus
  - Raghuveer Devulapalli
  - Sebastian Berg
  - Simon Graham +
  - Veniamin Petrenko +
  - Bernie Gray +

## Pull requests merged

A total of 11 pull requests were merged for this release.

  - [\#17756](https://github.com/numpy/numpy/pull/17756): BUG: Fix segfault due to out of bound pointer in floatstatus...
  - [\#17774](https://github.com/numpy/numpy/pull/17774): BUG: fix np.timedelta64('nat').\_\_format\_\_ throwing an exception
  - [\#17775](https://github.com/numpy/numpy/pull/17775): BUG: Fixed file handle leak in array\_tofile.
  - [\#17786](https://github.com/numpy/numpy/pull/17786): BUG: Raise recursion error during dimension discovery
  - [\#17917](https://github.com/numpy/numpy/pull/17917): BUG: Fix subarray dtype used with too large count in fromfile
  - [\#17918](https://github.com/numpy/numpy/pull/17918): BUG: 'bool' object has no attribute 'ndim'
  - [\#17919](https://github.com/numpy/numpy/pull/17919): BUG: ensure \_UFuncNoLoopError can be pickled
  - [\#17924](https://github.com/numpy/numpy/pull/17924): BLD: use BUFFERSIZE=20 in OpenBLAS
  - [\#18026](https://github.com/numpy/numpy/pull/18026): BLD: update to OpenBLAS 0.3.13
  - [\#18036](https://github.com/numpy/numpy/pull/18036): BUG: make a variable volatile to work around clang compiler bug
  - [\#18114](https://github.com/numpy/numpy/pull/18114): REL: Prepare for the NumPy 1.19.5 release.

---

1.20.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.20.0 Release Notes

This NumPy release is the largest so made to date, some 684 PRs contributed by 184 people have been merged. See the list of highlights below for more details. The Python versions supported for this release are 3.7-3.9, support for Python 3.6 has been dropped. Highlights are

  - Annotations for NumPy functions. This work is ongoing and improvements can be expected pending feedback from users.
  - Wider use of SIMD to increase execution speed of ufuncs. Much work has been done in introducing universal functions that will ease use of modern features across different hardware platforms. This work is ongoing.
  - Preliminary work in changing the dtype and casting implementations in order to provide an easier path to extending dtypes. This work is ongoing but enough has been done to allow experimentation and feedback.
  - Extensive documentation improvements comprising some 185 PR merges. This work is ongoing and part of the larger project to improve NumPy's online presence and usefulness to new users.
  - Further cleanups related to removing Python 2.7. This improves code readability and removes technical debt.
  - Preliminary support for the upcoming Cython 3.0.

## New functions

### The random.Generator class has a new `permuted` function.

The new function differs from `shuffle` and `permutation` in that the subarrays indexed by an axis are permuted rather than the axis being treated as a separate 1-D array for every combination of the other indexes. For example, it is now possible to permute the rows or columns of a 2-D array.

([gh-15121](https://github.com/numpy/numpy/pull/15121))

### `sliding_window_view` provides a sliding window view for numpy arrays

<span class="title-ref">numpy.lib.stride\_tricks.sliding\_window\_view</span> constructs views on numpy arrays that offer a sliding or moving window access to the array. This allows for the simple implementation of certain algorithms, such as running means.

([gh-17394](https://github.com/numpy/numpy/pull/17394))

### <span class="title-ref">numpy.broadcast\_shapes</span> is a new user-facing function

<span class="title-ref">\~numpy.broadcast\_shapes</span> gets the resulting shape from broadcasting the given shape tuples against each other.

``` python
>>> np.broadcast_shapes((1, 2), (3, 1))
(3, 2)

>>> np.broadcast_shapes(2, (3, 1))
(3, 2)

>>> np.broadcast_shapes((6, 7), (5, 6, 1), (7,), (5, 1, 7))
(5, 6, 7)
```

([gh-17535](https://github.com/numpy/numpy/pull/17535))

## Deprecations

### Using the aliases of builtin types like `np.int` is deprecated

For a long time, `np.int` has been an alias of the builtin `int`. This is repeatedly a cause of confusion for newcomers, and existed mainly for historic reasons.

These aliases have been deprecated. The table below shows the full list of deprecated aliases, along with their exact meaning. Replacing uses of items in the first column with the contents of the second column will work identically and silence the deprecation warning.

The third column lists alternative NumPy names which may occasionally be preferential. See also \[basics.types\](\#basics.types) for additional details.

| Deprecated name | Identical to | NumPy scalar type names                                                                                                                                    |
| --------------- | ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `numpy.bool`    | `bool`       | <span class="title-ref">numpy.bool\_</span>                                                                                                                |
| `numpy.int`     | `int`        | <span class="title-ref">numpy.int\_</span> (default), `numpy.int64`, or `numpy.int32`                                                                      |
| `numpy.float`   | `float`      | <span class="title-ref">numpy.float64</span>, <span class="title-ref">numpy.float\_</span>, <span class="title-ref">numpy.double</span> (equivalent)       |
| `numpy.complex` | `complex`    | <span class="title-ref">numpy.complex128</span>, <span class="title-ref">numpy.complex\_</span>, <span class="title-ref">numpy.cdouble</span> (equivalent) |
| `numpy.object`  | `object`     | <span class="title-ref">numpy.object\_</span>                                                                                                              |
| `numpy.str`     | `str`        | <span class="title-ref">numpy.str\_</span>                                                                                                                 |
| `numpy.long`    | `int`        | <span class="title-ref">numpy.int\_</span> (C `long`), <span class="title-ref">numpy.longlong</span> (largest integer type)                                |
| `numpy.unicode` | `str`        | <span class="title-ref">numpy.unicode\_</span>                                                                                                             |

To give a clear guideline for the vast majority of cases, for the types `bool`, `object`, `str` (and `unicode`) using the plain version is shorter and clear, and generally a good replacement. For `float` and `complex` you can use `float64` and `complex128` if you wish to be more explicit about the precision.

For `np.int` a direct replacement with `np.int_` or `int` is also good and will not change behavior, but the precision will continue to depend on the computer and operating system. If you want to be more explicit and review the current use, you have the following alternatives:

  - `np.int64` or `np.int32` to specify the precision exactly. This ensures that results cannot depend on the computer or operating system.
  - `np.int_` or `int` (the default), but be aware that it depends on the computer and operating system.
  - The C types: `np.cint` (int), `np.int_` (long), `np.longlong`.
  - `np.intp` which is 32bit on 32bit machines 64bit on 64bit machines. This can be the best type to use for indexing.

When used with `np.dtype(...)` or `dtype=...` changing it to the NumPy name as mentioned above will have no effect on the output. If used as a scalar with:

    np.float(123)

changing it can subtly change the result. In this case, the Python version `float(123)` or `int(12.)` is normally preferable, although the NumPy version may be useful for consistency with NumPy arrays (for example, NumPy behaves differently for things like division by zero).

([gh-14882](https://github.com/numpy/numpy/pull/14882))

### Passing `shape=None` to functions with a non-optional shape argument is deprecated

Previously, this was an alias for passing `shape=()`. This deprecation is emitted by <span class="title-ref">PyArray\_IntpConverter</span> in the C API. If your API is intended to support passing `None`, then you should check for `None` prior to invoking the converter, so as to be able to distinguish `None` and `()`.

([gh-15886](https://github.com/numpy/numpy/pull/15886))

### Indexing errors will be reported even when index result is empty

In the future, NumPy will raise an IndexError when an integer array index contains out of bound values even if a non-indexed dimension is of length 0. This will now emit a DeprecationWarning. This can happen when the array is previously empty, or an empty slice is involved:

    arr1 = np.zeros((5, 0))
    arr1[[20]]
    arr2 = np.zeros((5, 5))
    arr2[[20], :0]

Previously the non-empty index `[20]` was not checked for correctness. It will now be checked causing a deprecation warning which will be turned into an error. This also applies to assignments.

([gh-15900](https://github.com/numpy/numpy/pull/15900))

### Inexact matches for `mode` and `searchside` are deprecated

Inexact and case insensitive matches for `mode` and `searchside` were valid inputs earlier and will give a DeprecationWarning now. For example, below are some example usages which are now deprecated and will give a DeprecationWarning:

    import numpy as np
    arr = np.array([[3, 6, 6], [4, 5, 1]])
    # mode: inexact match
    np.ravel_multi_index(arr, (7, 6), mode="clap")  # should be "clip"
    # searchside: inexact match
    np.searchsorted(arr[0], 4, side='random')  # should be "right"

([gh-16056](https://github.com/numpy/numpy/pull/16056))

### Deprecation of <span class="title-ref">numpy.dual</span>

The module <span class="title-ref">numpy.dual</span> is deprecated. Instead of importing functions from <span class="title-ref">numpy.dual</span>, the functions should be imported directly from NumPy or SciPy.

([gh-16156](https://github.com/numpy/numpy/pull/16156))

### `outer` and `ufunc.outer` deprecated for matrix

`np.matrix` use with <span class="title-ref">\~numpy.outer</span> or generic ufunc outer calls such as `numpy.add.outer`. Previously, matrix was converted to an array here. This will not be done in the future requiring a manual conversion to arrays.

([gh-16232](https://github.com/numpy/numpy/pull/16232))

### Further Numeric Style types Deprecated

The remaining numeric-style type codes `Bytes0`, `Str0`, `Uint32`, `Uint64`, and `Datetime64` have been deprecated. The lower-case variants should be used instead. For bytes and string `"S"` and `"U"` are further alternatives.

([gh-16554](https://github.com/numpy/numpy/pull/16554))

### The `ndincr` method of `ndindex` is deprecated

The documentation has warned against using this function since NumPy 1.8. Use `next(it)` instead of `it.ndincr()`.

([gh-17233](https://github.com/numpy/numpy/pull/17233))

### ArrayLike objects which do not define `__len__` and `__getitem__`

Objects which define one of the protocols `__array__`, `__array_interface__`, or `__array_struct__` but are not sequences (usually defined by having a `__len__` and `__getitem__`) will behave differently during array-coercion in the future.

When nested inside sequences, such as `np.array([array_like])`, these were handled as a single Python object rather than an array. In the future they will behave identically to:

    np.array([np.array(array_like)])

This change should only have an effect if `np.array(array_like)` is not 0-D. The solution to this warning may depend on the object:

  - Some array-likes may expect the new behaviour, and users can ignore the warning. The object can choose to expose the sequence protocol to opt-in to the new behaviour.
  - For example, `shapely` will allow conversion to an array-like using `line.coords` rather than `np.asarray(line)`. Users may work around the warning, or use the new convention when it becomes available.

Unfortunately, using the new behaviour can only be achieved by calling `np.array(array_like)`.

If you wish to ensure that the old behaviour remains unchanged, please create an object array and then fill it explicitly, for example:

    arr = np.empty(3, dtype=object)
    arr[:] = [array_like1, array_like2, array_like3]

This will ensure NumPy knows to not enter the array-like and use it as a object instead.

([gh-17973](https://github.com/numpy/numpy/pull/17973))

## Future Changes

### Arrays cannot be using subarray dtypes

Array creation and casting using `np.array(arr, dtype)` and `arr.astype(dtype)` will use different logic when `dtype` is a subarray dtype such as `np.dtype("(2)i,")`.

For such a `dtype` the following behaviour is true:

    res = np.array(arr, dtype)
    
    res.dtype is not dtype
    res.dtype is dtype.base
    res.shape == arr.shape + dtype.shape

But `res` is filled using the logic:

    res = np.empty(arr.shape + dtype.shape, dtype=dtype.base)
    res[...] = arr

which uses incorrect broadcasting (and often leads to an error). In the future, this will instead cast each element individually, leading to the same result as:

    res = np.array(arr, dtype=np.dtype(["f", dtype]))["f"]

Which can normally be used to opt-in to the new behaviour.

This change does not affect `np.array(list, dtype="(2)i,")` unless the `list` itself includes at least one array. In particular, the behaviour is unchanged for a list of tuples.

([gh-17596](https://github.com/numpy/numpy/pull/17596))

## Expired deprecations

  - The deprecation of numeric style type-codes `np.dtype("Complex64")` (with upper case spelling), is expired. `"Complex64"` corresponded to `"complex128"` and `"Complex32"` corresponded to `"complex64"`.

  - The deprecation of `np.sctypeNA` and `np.typeNA` is expired. Both have been removed from the public API. Use `np.typeDict` instead.
    
    ([gh-16554](https://github.com/numpy/numpy/pull/16554))

  - The 14-year deprecation of `np.ctypeslib.ctypes_load_library` is expired. Use <span class="title-ref">\~numpy.ctypeslib.load\_library</span> instead, which is identical.
    
    ([gh-17116](https://github.com/numpy/numpy/pull/17116))

### Financial functions removed

In accordance with NEP 32, the financial functions are removed from NumPy 1.20. The functions that have been removed are `fv`, `ipmt`, `irr`, `mirr`, `nper`, `npv`, `pmt`, `ppmt`, `pv`, and `rate`. These functions are available in the [numpy\_financial](https://pypi.org/project/numpy-financial) library.

([gh-17067](https://github.com/numpy/numpy/pull/17067))

## Compatibility notes

### `isinstance(dtype, np.dtype)` and not `type(dtype) is not np.dtype`

NumPy dtypes are not direct instances of `np.dtype` anymore. Code that may have used `type(dtype) is np.dtype` will always return `False` and must be updated to use the correct version `isinstance(dtype, np.dtype)`.

This change also affects the C-side macro `PyArray_DescrCheck` if compiled against a NumPy older than 1.16.6. If code uses this macro and wishes to compile against an older version of NumPy, it must replace the macro (see also [C API changes](#c-api-changes) section).

### Same kind casting in concatenate with `axis=None`

When <span class="title-ref">\~numpy.concatenate</span> is called with `axis=None`, the flattened arrays were cast with `unsafe`. Any other axis choice uses "same kind". That different default has been deprecated and "same kind" casting will be used instead. The new `casting` keyword argument can be used to retain the old behaviour.

([gh-16134](https://github.com/numpy/numpy/pull/16134))

### NumPy Scalars are cast when assigned to arrays

When creating or assigning to arrays, in all relevant cases NumPy scalars will now be cast identically to NumPy arrays. In particular this changes the behaviour in some cases which previously raised an error:

    np.array([np.float64(np.nan)], dtype=np.int64)

will succeed and return an undefined result (usually the smallest possible integer). This also affects assignments:

    arr[0] = np.float64(np.nan)

At this time, NumPy retains the behaviour for:

    np.array(np.float64(np.nan), dtype=np.int64)

The above changes do not affect Python scalars:

    np.array([float("NaN")], dtype=np.int64)

remains unaffected (`np.nan` is a Python `float`, not a NumPy one). Unlike signed integers, unsigned integers do not retain this special case, since they always behaved more like casting. The following code stops raising an error:

    np.array([np.float64(np.nan)], dtype=np.uint64)

To avoid backward compatibility issues, at this time assignment from `datetime64` scalar to strings of too short length remains supported. This means that `np.asarray(np.datetime64("2020-10-10"), dtype="S5")` succeeds now, when it failed before. In the long term this may be deprecated or the unsafe cast may be allowed generally to make assignment of arrays and scalars behave consistently.

### Array coercion changes when Strings and other types are mixed

When strings and other types are mixed, such as:

    np.array(["string", np.float64(3.)], dtype="S")

The results will change, which may lead to string dtypes with longer strings in some cases. In particularly, if `dtype="S"` is not provided any numerical value will lead to a string results long enough to hold all possible numerical values. (e.g. "S32" for floats). Note that you should always provide `dtype="S"` when converting non-strings to strings.

If `dtype="S"` is provided the results will be largely identical to before, but NumPy scalars (not a Python float like `1.0`), will still enforce a uniform string length:

    np.array([np.float64(3.)], dtype="S")  # gives "S32"
    np.array([3.0], dtype="S")  # gives "S3"

Previously the first version gave the same result as the second.

### Array coercion restructure

Array coercion has been restructured. In general, this should not affect users. In extremely rare corner cases where array-likes are nested:

    np.array([array_like1])

Things will now be more consistent with:

    np.array([np.array(array_like1)])

This can subtly change output for some badly defined array-likes. One example for this are array-like objects which are not also sequences of matching shape. In NumPy 1.20, a warning will be given when an array-like is not also a sequence (but behaviour remains identical, see deprecations). If an array like is also a sequence (defines `__getitem__` and `__len__`) NumPy will now only use the result given by `__array__`, `__array_interface__`, or `__array_struct__`. This will result in differences when the (nested) sequence describes a different shape.

([gh-16200](https://github.com/numpy/numpy/pull/16200))

### Writing to the result of <span class="title-ref">numpy.broadcast\_arrays</span> will export readonly buffers

In NumPy 1.17 <span class="title-ref">numpy.broadcast\_arrays</span> started warning when the resulting array was written to. This warning was skipped when the array was used through the buffer interface (e.g. `memoryview(arr)`). The same thing will now occur for the two protocols `__array_interface__`, and `__array_struct__` returning read-only buffers instead of giving a warning.

([gh-16350](https://github.com/numpy/numpy/pull/16350))

### Numeric-style type names have been removed from type dictionaries

To stay in sync with the deprecation for `np.dtype("Complex64")` and other numeric-style (capital case) types. These were removed from `np.sctypeDict` and `np.typeDict`. You should use the lower case versions instead. Note that `"Complex64"` corresponds to `"complex128"` and `"Complex32"` corresponds to `"complex64"`. The numpy style (new) versions, denote the full size and not the size of the real/imaginary part.

([gh-16554](https://github.com/numpy/numpy/pull/16554))

### The `operator.concat` function now raises TypeError for array arguments

The previous behavior was to fall back to addition and add the two arrays, which was thought to be unexpected behavior for a concatenation function.

([gh-16570](https://github.com/numpy/numpy/pull/16570))

### `nickname` attribute removed from ABCPolyBase

An abstract property `nickname` has been removed from `ABCPolyBase` as it was no longer used in the derived convenience classes. This may affect users who have derived classes from `ABCPolyBase` and overridden the methods for representation and display, e.g. `__str__`, `__repr__`, `_repr_latex`, etc.

([gh-16589](https://github.com/numpy/numpy/pull/16589))

### `float->timedelta` and `uint64->timedelta` promotion will raise a TypeError

Float and timedelta promotion consistently raises a TypeError. `np.promote_types("float32", "m8")` aligns with `np.promote_types("m8", "float32")` now and both raise a TypeError. Previously, `np.promote_types("float32", "m8")` returned `"m8"` which was considered a bug.

Uint64 and timedelta promotion consistently raises a TypeError. `np.promote_types("uint64", "m8")` aligns with `np.promote_types("m8", "uint64")` now and both raise a TypeError. Previously, `np.promote_types("uint64", "m8")` returned `"m8"` which was considered a bug.

([gh-16592](https://github.com/numpy/numpy/pull/16592))

### `numpy.genfromtxt` now correctly unpacks structured arrays

Previously, <span class="title-ref">numpy.genfromtxt</span> failed to unpack if it was called with `unpack=True` and a structured datatype was passed to the `dtype` argument (or `dtype=None` was passed and a structured datatype was inferred). For example:

    >>> data = StringIO("21 58.0\n35 72.0")
    >>> np.genfromtxt(data, dtype=None, unpack=True)
    array([(21, 58.), (35, 72.)], dtype=[('f0', '<i8'), ('f1', '<f8')])

Structured arrays will now correctly unpack into a list of arrays, one for each column:

    >>> np.genfromtxt(data, dtype=None, unpack=True)
    [array([21, 35]), array([58., 72.])]

([gh-16650](https://github.com/numpy/numpy/pull/16650))

### `mgrid`, `r_`, etc. consistently return correct outputs for non-default precision input

Previously, `np.mgrid[np.float32(0.1):np.float32(0.35):np.float32(0.1),]` and `np.r_[0:10:np.complex64(3j)]` failed to return meaningful output. This bug potentially affects <span class="title-ref">\~numpy.mgrid</span>, <span class="title-ref">\~numpy.ogrid</span>, <span class="title-ref">\~numpy.r\_</span>, and <span class="title-ref">\~numpy.c\_</span> when an input with dtype other than the default `float64` and `complex128` and equivalent Python types were used. The methods have been fixed to handle varying precision correctly.

([gh-16815](https://github.com/numpy/numpy/pull/16815))

### Boolean array indices with mismatching shapes now properly give `IndexError`

Previously, if a boolean array index matched the size of the indexed array but not the shape, it was incorrectly allowed in some cases. In other cases, it gave an error, but the error was incorrectly a `ValueError` with a message about broadcasting instead of the correct `IndexError`.

For example, the following used to incorrectly give `ValueError: operands could not be broadcast together with shapes (2,2) (1,4)`:

``` python
np.empty((2, 2))[np.array([[True, False, False, False]])]
```

And the following used to incorrectly return `array([], dtype=float64)`:

``` python
np.empty((2, 2))[np.array([[False, False, False, False]])]
```

Both now correctly give `IndexError: boolean index did not match indexed array along dimension 0; dimension is 2 but corresponding boolean dimension is 1`.

([gh-17010](https://github.com/numpy/numpy/pull/17010))

### Casting errors interrupt Iteration

When iterating while casting values, an error may stop the iteration earlier than before. In any case, a failed casting operation always returned undefined, partial results. Those may now be even more undefined and partial. For users of the `NpyIter` C-API such cast errors will now cause the <span class="title-ref">iternext()</span> function to return 0 and thus abort iteration. Currently, there is no API to detect such an error directly. It is necessary to check `PyErr_Occurred()`, which may be problematic in combination with `NpyIter_Reset`. These issues always existed, but new API could be added if required by users.

([gh-17029](https://github.com/numpy/numpy/pull/17029))

### f2py generated code may return unicode instead of byte strings

Some byte strings previously returned by f2py generated code may now be unicode strings. This results from the ongoing Python2 -\> Python3 cleanup.

([gh-17068](https://github.com/numpy/numpy/pull/17068))

### The first element of the `__array_interface__["data"]` tuple must be an integer

This has been the documented interface for many years, but there was still code that would accept a byte string representation of the pointer address. That code has been removed, passing the address as a byte string will now raise an error.

([gh-17241](https://github.com/numpy/numpy/pull/17241))

### poly1d respects the dtype of all-zero argument

Previously, constructing an instance of `poly1d` with all-zero coefficients would cast the coefficients to `np.float64`. This affected the output dtype of methods which construct `poly1d` instances internally, such as `np.polymul`.

([gh-17577](https://github.com/numpy/numpy/pull/17577))

### The numpy.i file for swig is Python 3 only.

Uses of Python 2.7 C-API functions have been updated to Python 3 only. Users who need the old version should take it from an older version of NumPy.

([gh-17580](https://github.com/numpy/numpy/pull/17580))

### Void dtype discovery in `np.array`

In calls using `np.array(..., dtype="V")`, `arr.astype("V")`, and similar a TypeError will now be correctly raised unless all elements have the identical void length. An example for this is:

    np.array([b"1", b"12"], dtype="V")

Which previously returned an array with dtype `"V2"` which cannot represent `b"1"` faithfully.

([gh-17706](https://github.com/numpy/numpy/pull/17706))

## C API changes

### The `PyArray_DescrCheck` macro is modified

The `PyArray_DescrCheck` macro has been updated since NumPy 1.16.6 to be:

    #define PyArray_DescrCheck(op) PyObject_TypeCheck(op, &PyArrayDescr_Type)

Starting with NumPy 1.20 code that is compiled against an earlier version will be API incompatible with NumPy 1.20. The fix is to either compile against 1.16.6 (if the NumPy 1.16 release is the oldest release you wish to support), or manually inline the macro by replacing it with the new definition:

    PyObject_TypeCheck(op, &PyArrayDescr_Type)

which is compatible with all NumPy versions.

### Size of `np.ndarray` and `np.void_` changed

The size of the `PyArrayObject` and `PyVoidScalarObject` structures have changed. The following header definition has been removed:

    #define NPY_SIZEOF_PYARRAYOBJECT (sizeof(PyArrayObject_fields))

since the size must not be considered a compile time constant: it will change for different runtime versions of NumPy.

The most likely relevant use are potential subclasses written in C which will have to be recompiled and should be updated. Please see the documentation for :c`PyArrayObject` for more details and contact the NumPy developers if you are affected by this change.

NumPy will attempt to give a graceful error but a program expecting a fixed structure size may have undefined behaviour and likely crash.

([gh-16938](https://github.com/numpy/numpy/pull/16938))

## New Features

### `where` keyword argument for `numpy.all` and `numpy.any` functions

The keyword argument `where` is added and allows to only consider specified elements or subaxes from an array in the Boolean evaluation of `all` and `any`. This new keyword is available to the functions `all` and `any` both via `numpy` directly or in the methods of `numpy.ndarray`.

Any broadcastable Boolean array or a scalar can be set as `where`. It defaults to `True` to evaluate the functions for all elements in an array if `where` is not set by the user. Examples are given in the documentation of the functions.

### `where` keyword argument for `numpy` functions `mean`, `std`, `var`

The keyword argument `where` is added and allows to limit the scope in the calculation of `mean`, `std` and `var` to only a subset of elements. It is available both via `numpy` directly or in the methods of `numpy.ndarray`.

Any broadcastable Boolean array or a scalar can be set as `where`. It defaults to `True` to evaluate the functions for all elements in an array if `where` is not set by the user. Examples are given in the documentation of the functions.

([gh-15852](https://github.com/numpy/numpy/pull/15852))

### `norm=backward`, `forward` keyword options for `numpy.fft` functions

The keyword argument option `norm=backward` is added as an alias for `None` and acts as the default option; using it has the direct transforms unscaled and the inverse transforms scaled by `1/n`.

Using the new keyword argument option `norm=forward` has the direct transforms scaled by `1/n` and the inverse transforms unscaled (i.e. exactly opposite to the default option `norm=backward`).

([gh-16476](https://github.com/numpy/numpy/pull/16476))

### NumPy is now typed

Type annotations have been added for large parts of NumPy. There is also a new <span class="title-ref">numpy.typing</span> module that contains useful types for end-users. The currently available types are

  - `ArrayLike`: for objects that can be coerced to an array
  - `DtypeLike`: for objects that can be coerced to a dtype

([gh-16515](https://github.com/numpy/numpy/pull/16515))

### `numpy.typing` is accessible at runtime

The types in `numpy.typing` can now be imported at runtime. Code like the following will now work:

``` python
from numpy.typing import ArrayLike
x: ArrayLike = [1, 2, 3, 4]
```

([gh-16558](https://github.com/numpy/numpy/pull/16558))

### New `__f2py_numpy_version__` attribute for f2py generated modules.

Because f2py is released together with NumPy, `__f2py_numpy_version__` provides a way to track the version f2py used to generate the module.

([gh-16594](https://github.com/numpy/numpy/pull/16594))

### `mypy` tests can be run via runtests.py

Currently running mypy with the NumPy stubs configured requires either:

  - Installing NumPy
  - Adding the source directory to MYPYPATH and linking to the `mypy.ini`

Both options are somewhat inconvenient, so add a `--mypy` option to runtests that handles setting things up for you. This will also be useful in the future for any typing codegen since it will ensure the project is built before type checking.

([gh-17123](https://github.com/numpy/numpy/pull/17123))

### Negation of user defined BLAS/LAPACK detection order

<span class="title-ref">\~numpy.distutils</span> allows negation of libraries when determining BLAS/LAPACK libraries. This may be used to remove an item from the library resolution phase, i.e. to disallow NetLIB libraries one could do:

``` bash
NPY_BLAS_ORDER='^blas' NPY_LAPACK_ORDER='^lapack' python setup.py build
```

That will use any of the accelerated libraries instead.

([gh-17219](https://github.com/numpy/numpy/pull/17219))

### Allow passing optimizations arguments to asv build

It is now possible to pass `-j`, `--cpu-baseline`, `--cpu-dispatch` and `--disable-optimization` flags to ASV build when the `--bench-compare` argument is used.

([gh-17284](https://github.com/numpy/numpy/pull/17284))

### The NVIDIA HPC SDK nvfortran compiler is now supported

Support for the nvfortran compiler, a version of pgfortran, has been added.

([gh-17344](https://github.com/numpy/numpy/pull/17344))

### `dtype` option for `cov` and `corrcoef`

The `dtype` option is now available for <span class="title-ref">numpy.cov</span> and <span class="title-ref">numpy.corrcoef</span>. It specifies which data-type the returned result should have. By default the functions still return a <span class="title-ref">numpy.float64</span> result.

([gh-17456](https://github.com/numpy/numpy/pull/17456))

## Improvements

### Improved string representation for polynomials (`__str__`)

The string representation (`__str__`) of all six polynomial types in <span class="title-ref">numpy.polynomial</span> has been updated to give the polynomial as a mathematical expression instead of an array of coefficients. Two package-wide formats for the polynomial expressions are available - one using Unicode characters for superscripts and subscripts, and another using only ASCII characters.

([gh-15666](https://github.com/numpy/numpy/pull/15666))

### Remove the Accelerate library as a candidate LAPACK library

Apple no longer supports Accelerate. Remove it.

([gh-15759](https://github.com/numpy/numpy/pull/15759))

### Object arrays containing multi-line objects have a more readable `repr`

If elements of an object array have a `repr` containing new lines, then the wrapped lines will be aligned by column. Notably, this improves the `repr` of nested arrays:

    >>> np.array([np.eye(2), np.eye(3)], dtype=object)
    array([array([[1., 0.],
                  [0., 1.]]),
           array([[1., 0., 0.],
                  [0., 1., 0.],
                  [0., 0., 1.]])], dtype=object)

([gh-15997](https://github.com/numpy/numpy/pull/15997))

### Concatenate supports providing an output dtype

Support was added to <span class="title-ref">\~numpy.concatenate</span> to provide an output `dtype` and `casting` using keyword arguments. The `dtype` argument cannot be provided in conjunction with the `out` one.

([gh-16134](https://github.com/numpy/numpy/pull/16134))

### Thread safe f2py callback functions

Callback functions in f2py are now thread safe.

([gh-16519](https://github.com/numpy/numpy/pull/16519))

### `numpy.core.records.fromfile` now supports file-like objects

`numpy.core.records.fromfile` can now use file-like objects, for instance :py\`io.BytesIO\`

([gh-16675](https://github.com/numpy/numpy/pull/16675))

### RPATH support on AIX added to distutils

This allows SciPy to be built on AIX.

([gh-16710](https://github.com/numpy/numpy/pull/16710))

### Use f90 compiler specified by the command line args

The compiler command selection for Fortran Portland Group Compiler is changed in `numpy.distutils.fcompiler`. This only affects the linking command. This forces the use of the executable provided by the command line option (if provided) instead of the pgfortran executable. If no executable is provided to the command line option it defaults to the pgf90 executable, which is an alias for pgfortran according to the PGI documentation.

([gh-16730](https://github.com/numpy/numpy/pull/16730))

### Add NumPy declarations for Cython 3.0 and later

The pxd declarations for Cython 3.0 were improved to avoid using deprecated NumPy C-API features. Extension modules built with Cython 3.0+ that use NumPy can now set the C macro `NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION` to avoid C compiler warnings about deprecated API usage.

([gh-16986](https://github.com/numpy/numpy/pull/16986))

### Make the window functions exactly symmetric

Make sure the window functions provided by NumPy are symmetric. There were previously small deviations from symmetry due to numerical precision that are now avoided by better arrangement of the computation.

([gh-17195](https://github.com/numpy/numpy/pull/17195))

## Performance improvements and changes

### Enable multi-platform SIMD compiler optimizations

A series of improvements for NumPy infrastructure to pave the way to **NEP-38**, that can be summarized as follow:

  - **New Build Arguments**
    
      - `--cpu-baseline` to specify the minimal set of required optimizations, default value is `min` which provides the minimum CPU features that can safely run on a wide range of users platforms.
      - `--cpu-dispatch` to specify the dispatched set of additional optimizations, default value is `max -xop -fma4` which enables all CPU features, except for AMD legacy features.
      - `--disable-optimization` to explicitly disable the whole new improvements, It also adds a new **C** compiler \#definition called `NPY_DISABLE_OPTIMIZATION` which it can be used as guard for any SIMD code.

  - **Advanced CPU dispatcher**
    
    A flexible cross-architecture CPU dispatcher built on the top of Python/Numpy distutils, support all common compilers with a wide range of CPU features.
    
    The new dispatcher requires a special file extension `*.dispatch.c` to mark the dispatch-able **C** sources. These sources have the ability to be compiled multiple times so that each compilation process represents certain CPU features and provides different \#definitions and flags that affect the code paths.

  - **New auto-generated C header \`\`core/src/common/\_cpu\_dispatch.h\`\`**
    
    This header is generated by the distutils module `ccompiler_opt`, and contains all the \#definitions and headers of instruction sets, that had been configured through command arguments '--cpu-baseline' and '--cpu-dispatch'.

  - **New C header \`\`core/src/common/npy\_cpu\_dispatch.h\`\`**
    
    This header contains all utilities that required for the whole CPU dispatching process, it also can be considered as a bridge linking the new infrastructure work with NumPy CPU runtime detection.

  - **Add new attributes to NumPy umath module(Python level)**
    
      - `__cpu_baseline__` a list contains the minimal set of required optimizations that supported by the compiler and platform according to the specified values to command argument '--cpu-baseline'.
      - `__cpu_dispatch__` a list contains the dispatched set of additional optimizations that supported by the compiler and platform according to the specified values to command argument '--cpu-dispatch'.

  - **Print the supported CPU features during the run of PytestTester**

([gh-13516](https://github.com/numpy/numpy/pull/13516))

## Changes

### Changed behavior of `divmod(1., 0.)` and related functions

The changes also assure that different compiler versions have the same behavior for nan or inf usages in these operations. This was previously compiler dependent, we now force the invalid and divide by zero flags, making the results the same across compilers. For example, gcc-5, gcc-8, or gcc-9 now result in the same behavior. The changes are tabulated below:

| Operator                   | Old Warning | New Warning              | Old Result | New Result | Works on MacOS |
| -------------------------- | ----------- | ------------------------ | ---------- | ---------- | -------------- |
| np.divmod(1.0, 0.0)        | Invalid     | Invalid and Dividebyzero | nan, nan   | inf, nan   | Yes            |
| np.fmod(1.0, 0.0)          | Invalid     | Invalid                  | nan        | nan        | No? Yes        |
| np.floor\_divide(1.0, 0.0) | Invalid     | Dividebyzero             | nan        | inf        | Yes            |
| np.remainder(1.0, 0.0)     | Invalid     | Invalid                  | nan        | nan        | Yes            |

Summary of New Behavior

([gh-16161](https://github.com/numpy/numpy/pull/16161))

### `np.linspace` on integers now uses floor

When using a `int` dtype in <span class="title-ref">numpy.linspace</span>, previously float values would be rounded towards zero. Now <span class="title-ref">numpy.floor</span> is used instead, which rounds toward `-inf`. This changes the results for negative values. For example, the following would previously give:

    >>> np.linspace(-3, 1, 8, dtype=int)
    array([-3, -2, -1, -1,  0,  0,  0,  1])

and now results in:

    >>> np.linspace(-3, 1, 8, dtype=int)
    array([-3, -3, -2, -2, -1, -1,  0,  1])

The former result can still be obtained with:

    >>> np.linspace(-3, 1, 8).astype(int)
    array([-3, -2, -1, -1,  0,  0,  0,  1])

([gh-16841](https://github.com/numpy/numpy/pull/16841))

---

1.20.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.20.1 Release Notes

NumPy 1,20.1 is a rapid bugfix release fixing several bugs and regressions reported after the 1.20.0 release.

## Highlights

  - The distutils bug that caused problems with downstream projects is fixed.
  - The `random.shuffle` regression is fixed.

## Contributors

A total of 8 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Nicholas McKibben +
  - Pearu Peterson
  - Ralf Gommers
  - Sebastian Berg
  - Tyler Reddy
  - @Aerysv +

## Pull requests merged

A total of 15 pull requests were merged for this release.

  - [\#18306](https://github.com/numpy/numpy/pull/18306): MAINT: Add missing placeholder annotations
  - [\#18310](https://github.com/numpy/numpy/pull/18310): BUG: Fix typo in `numpy.__init__.py`
  - [\#18326](https://github.com/numpy/numpy/pull/18326): BUG: don't mutate list of fake libraries while iterating over...
  - [\#18327](https://github.com/numpy/numpy/pull/18327): MAINT: gracefully shuffle memoryviews
  - [\#18328](https://github.com/numpy/numpy/pull/18328): BUG: Use C linkage for random distributions
  - [\#18336](https://github.com/numpy/numpy/pull/18336): CI: fix when GitHub Actions builds trigger, and allow ci skips
  - [\#18337](https://github.com/numpy/numpy/pull/18337): BUG: Allow unmodified use of isclose, allclose, etc. with timedelta
  - [\#18345](https://github.com/numpy/numpy/pull/18345): BUG: Allow pickling all relevant DType types/classes
  - [\#18351](https://github.com/numpy/numpy/pull/18351): BUG: Fix missing signed\_char dependency. Closes \#18335.
  - [\#18352](https://github.com/numpy/numpy/pull/18352): DOC: Change license date 2020 -\> 2021
  - [\#18353](https://github.com/numpy/numpy/pull/18353): CI: CircleCI seems to occasionally time out, increase the limit
  - [\#18354](https://github.com/numpy/numpy/pull/18354): BUG: Fix f2py bugs when wrapping F90 subroutines.
  - [\#18356](https://github.com/numpy/numpy/pull/18356): MAINT: crackfortran regex simplify
  - [\#18357](https://github.com/numpy/numpy/pull/18357): BUG: threads.h existence test requires GLIBC \> 2.12.
  - [\#18359](https://github.com/numpy/numpy/pull/18359): REL: Prepare for the NumPy 1.20.1 release.

---

1.20.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.20.2 Release Notes

NumPy 1.20.2 is a bugfix release containing several fixes merged to the main branch after the NumPy 1.20.1 release.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Allan Haldane
  - Bas van Beek
  - Charles Harris
  - Christoph Gohlke
  - Mateusz SokÃ³Å‚ +
  - Michael Lamparski
  - Sebastian Berg

## Pull requests merged

A total of 20 pull requests were merged for this release.

  - [\#18382](https://github.com/numpy/numpy/pull/18382): MAINT: Update f2py from master.
  - [\#18459](https://github.com/numpy/numpy/pull/18459): BUG: `diagflat` could overflow on windows or 32-bit platforms
  - [\#18460](https://github.com/numpy/numpy/pull/18460): BUG: Fix refcount leak in f2py `complex_double_from_pyobj`.
  - [\#18461](https://github.com/numpy/numpy/pull/18461): BUG: Fix tiny memory leaks when `like=` overrides are used
  - [\#18462](https://github.com/numpy/numpy/pull/18462): BUG: Remove temporary change of descr/flags in VOID functions
  - [\#18469](https://github.com/numpy/numpy/pull/18469): BUG: Segfault in nditer buffer dealloc for Object arrays
  - [\#18485](https://github.com/numpy/numpy/pull/18485): BUG: Remove suspicious type casting
  - [\#18486](https://github.com/numpy/numpy/pull/18486): BUG: remove nonsensical comparison of pointer \< 0
  - [\#18487](https://github.com/numpy/numpy/pull/18487): BUG: verify pointer against NULL before using it
  - [\#18488](https://github.com/numpy/numpy/pull/18488): BUG: check if PyArray\_malloc succeeded
  - [\#18546](https://github.com/numpy/numpy/pull/18546): BUG: incorrect error fallthrough in nditer
  - [\#18559](https://github.com/numpy/numpy/pull/18559): CI: Backport CI fixes from main.
  - [\#18599](https://github.com/numpy/numpy/pull/18599): MAINT: Add annotations for `dtype.__getitem__`, <span class="title-ref">\_\_mul\_\_</span> and...
  - [\#18611](https://github.com/numpy/numpy/pull/18611): BUG: NameError in numpy.distutils.fcompiler.compaq
  - [\#18612](https://github.com/numpy/numpy/pull/18612): BUG: Fixed `where` keyword for `np.mean` & `np.var` methods
  - [\#18617](https://github.com/numpy/numpy/pull/18617): CI: Update apt package list before Python install
  - [\#18636](https://github.com/numpy/numpy/pull/18636): MAINT: Ensure that re-exported sub-modules are properly annotated
  - [\#18638](https://github.com/numpy/numpy/pull/18638): BUG: Fix ma coercion list-of-ma-arrays if they do not cast to...
  - [\#18661](https://github.com/numpy/numpy/pull/18661): BUG: Fix small valgrind-found issues
  - [\#18671](https://github.com/numpy/numpy/pull/18671): BUG: Fix small issues found with pytest-leaks

---

1.20.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.20.3 Release Notes

NumPy 1.20.3 is a bugfix release containing several fixes merged to the main branch after the NumPy 1.20.2 release.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Anne Archibald
  - Bas van Beek
  - Charles Harris
  - Dong Keun Oh +
  - Kamil Choudhury +
  - Sayed Adel
  - Sebastian Berg

## Pull requests merged

A total of 15 pull requests were merged for this release.

  - [\#18763](https://github.com/numpy/numpy/pull/18763): BUG: Correct `datetime64` missing type overload for `datetime.date`...
  - [\#18764](https://github.com/numpy/numpy/pull/18764): MAINT: Remove `__all__` in favor of explicit re-exports
  - [\#18768](https://github.com/numpy/numpy/pull/18768): BLD: Strip extra newline when dumping gfortran version on MacOS
  - [\#18769](https://github.com/numpy/numpy/pull/18769): BUG: fix segfault in object/longdouble operations
  - [\#18794](https://github.com/numpy/numpy/pull/18794): MAINT: Use towncrier build explicitly
  - [\#18887](https://github.com/numpy/numpy/pull/18887): MAINT: Relax certain integer-type constraints
  - [\#18915](https://github.com/numpy/numpy/pull/18915): MAINT: Remove unsafe unions and ABCs from return-annotations
  - [\#18921](https://github.com/numpy/numpy/pull/18921): MAINT: Allow more recursion depth for scalar tests.
  - [\#18922](https://github.com/numpy/numpy/pull/18922): BUG: Initialize the full nditer buffer in case of error
  - [\#18923](https://github.com/numpy/numpy/pull/18923): BLD: remove unnecessary flag `-faltivec` on macOS
  - [\#18924](https://github.com/numpy/numpy/pull/18924): MAINT, CI: treats \_SIMD module build warnings as errors through...
  - [\#18925](https://github.com/numpy/numpy/pull/18925): BUG: for MINGW, threads.h existence test requires GLIBC \> 2.12
  - [\#18941](https://github.com/numpy/numpy/pull/18941): BUG: Make changelog recognize gh- as a PR number prefix.
  - [\#18948](https://github.com/numpy/numpy/pull/18948): REL, DOC: Prepare for the NumPy 1.20.3 release.
  - [\#18953](https://github.com/numpy/numpy/pull/18953): BUG: Fix failing mypy test in 1.20.x.

---

1.21.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.0 Release Notes

The NumPy 1.21.0 release highlights are

  - continued SIMD work covering more functions and platforms,
  - initial work on the new dtype infrastructure and casting,
  - universal2 wheels for Python 3.8 and Python 3.9 on Mac,
  - improved documentation,
  - improved annotations,
  - new `PCG64DXSM` bitgenerator for random numbers.

In addition there are the usual large number of bug fixes and other improvements.

The Python versions supported for this release are 3.7-3.9. Official support for Python 3.10 will be added when it is released.

<div class="warning">

<div class="title">

Warning

</div>

There are unresolved problems compiling NumPy 1.20.0 with gcc-11.1.

  - Optimization level <span class="title-ref">-O3</span> results in many incorrect warnings when running the tests.
  - On some hardware NumPY will hang in an infinite loop.

</div>

## New functions

<div class="currentmodule">

numpy.random

</div>

### Add <span class="title-ref">PCG64DXSM</span> <span class="title-ref">BitGenerator</span>

Uses of the `PCG64` `BitGenerator` in a massively-parallel context have been shown to have statistical weaknesses that were not apparent at the first release in numpy 1.17. Most users will never observe this weakness and are safe to continue to use `PCG64`. We have introduced a new `PCG64DXSM` `BitGenerator` that will eventually become the new default `BitGenerator` implementation used by `default_rng` in future releases. `PCG64DXSM` solves the statistical weakness while preserving the performance and the features of `PCG64`.

See \[upgrading-pcg64\](\#upgrading-pcg64) for more details.

<div class="currentmodule">

numpy

</div>

([gh-18906](https://github.com/numpy/numpy/pull/18906))

## Expired deprecations

  - The `shape` argument <span class="title-ref">\~numpy.unravel\_index</span> cannot be passed as `dims` keyword argument anymore. (Was deprecated in NumPy 1.16.)
    
    ([gh-17900](https://github.com/numpy/numpy/pull/17900))

  - The function `PyUFunc_GenericFunction` has been disabled. It was deprecated in NumPy 1.19. Users should call the ufunc directly using the Python API.
    
    ([gh-18697](https://github.com/numpy/numpy/pull/18697))

  - The function `PyUFunc_SetUsesArraysAsData` has been disabled. It was deprecated in NumPy 1.19.
    
    ([gh-18697](https://github.com/numpy/numpy/pull/18697))

  - The class `PolyBase` has been removed (deprecated in numpy 1.9.0). Please use the abstract `ABCPolyBase` class instead.
    
    ([gh-18963](https://github.com/numpy/numpy/pull/18963))

  - The unused `PolyError` and `PolyDomainError` exceptions are removed.
    
    ([gh-18963](https://github.com/numpy/numpy/pull/18963))

## Deprecations

### The `.dtype` attribute must return a `dtype`

A `DeprecationWarning` is now given if the `.dtype` attribute of an object passed into `np.dtype` or as a `dtype=obj` argument is not a dtype. NumPy will stop attempting to recursively coerce the result of `.dtype`.

([gh-13578](https://github.com/numpy/numpy/pull/13578))

### Inexact matches for `numpy.convolve` and `numpy.correlate` are deprecated

<span class="title-ref">\~numpy.convolve</span> and <span class="title-ref">\~numpy.correlate</span> now emit a warning when there are case insensitive and/or inexact matches found for `mode` argument in the functions. Pass full `"same"`, `"valid"`, `"full"` strings instead of `"s"`, `"v"`, `"f"` for the `mode` argument.

([gh-17492](https://github.com/numpy/numpy/pull/17492))

### `np.typeDict` has been formally deprecated

`np.typeDict` is a deprecated alias for `np.sctypeDict` and has been so for over 14 years ([6689502](https://github.com/numpy/numpy/commit/668950285c407593a368336ff2e737c5da84af7d)). A deprecation warning will now be issued whenever getting `np.typeDict`.

([gh-17586](https://github.com/numpy/numpy/pull/17586))

### Exceptions will be raised during array-like creation

When an object raised an exception during access of the special attributes `__array__` or `__array_interface__`, this exception was usually ignored. A warning is now given when the exception is anything but AttributeError. To silence the warning, the type raising the exception has to be adapted to raise an `AttributeError`.

([gh-19001](https://github.com/numpy/numpy/pull/19001))

### Four `ndarray.ctypes` methods have been deprecated

Four methods of the <span class="title-ref">ndarray.ctypes</span> object have been deprecated, as they are (undocumentated) implementation artifacts of their respective properties.

The methods in question are:

  - `_ctypes.get_data` (use `_ctypes.data` instead)
  - `_ctypes.get_shape` (use `_ctypes.shape` instead)
  - `_ctypes.get_strides` (use `_ctypes.strides` instead)
  - `_ctypes.get_as_parameter` (use `_ctypes._as_parameter_` instead)

([gh-19031](https://github.com/numpy/numpy/pull/19031))

## Expired deprecations

  - The `shape` argument <span class="title-ref">numpy.unravel\_index</span> cannot be passed as `dims` keyword argument anymore. (Was deprecated in NumPy 1.16.)
    
    ([gh-17900](https://github.com/numpy/numpy/pull/17900))

  - The function `PyUFunc_GenericFunction` has been disabled. It was deprecated in NumPy 1.19. Users should call the ufunc directly using the Python API.
    
    ([gh-18697](https://github.com/numpy/numpy/pull/18697))

  - The function `PyUFunc_SetUsesArraysAsData` has been disabled. It was deprecated in NumPy 1.19.
    
    ([gh-18697](https://github.com/numpy/numpy/pull/18697))

### Remove deprecated `PolyBase` and unused `PolyError` and `PolyDomainError`

The class `PolyBase` has been removed (deprecated in numpy 1.9.0). Please use the abstract `ABCPolyBase` class instead.

Furthermore, the unused `PolyError` and `PolyDomainError` exceptions are removed from the <span class="title-ref">numpy.polynomial</span>.

([gh-18963](https://github.com/numpy/numpy/pull/18963))

## Compatibility notes

### Error type changes in universal functions

The universal functions may now raise different errors on invalid input in some cases. The main changes should be that a `RuntimeError` was replaced with a more fitting `TypeError`. When multiple errors were present in the same call, NumPy may now raise a different one.

([gh-15271](https://github.com/numpy/numpy/pull/15271))

### `__array_ufunc__` argument validation

NumPy will now partially validate arguments before calling `__array_ufunc__`. Previously, it was possible to pass on invalid arguments (such as a non-existing keyword argument) when dispatch was known to occur.

([gh-15271](https://github.com/numpy/numpy/pull/15271))

### `__array_ufunc__` and additional positional arguments

Previously, all positionally passed arguments were checked for `__array_ufunc__` support. In the case of `reduce`, `accumulate`, and `reduceat` all arguments may be passed by position. This means that when they were passed by position, they could previously have been asked to handle the ufunc call via `__array_ufunc__`. Since this depended on the way the arguments were passed (by position or by keyword), NumPy will now only dispatch on the input and output array. For example, NumPy will never dispatch on the `where` array in a reduction such as `np.add.reduce`.

([gh-15271](https://github.com/numpy/numpy/pull/15271))

### Validate input values in `Generator.uniform`

Checked that `high - low >= 0` in `np.random.Generator.uniform`. Raises `ValueError` if `low > high`. Previously out-of-order inputs were accepted and silently swapped, so that if `low > high`, the value generated was `high + (low - high) * random()`.

([gh-17921](https://github.com/numpy/numpy/pull/17921))

### `/usr/include` removed from default include paths

The default include paths when building a package with `numpy.distutils` no longer include `/usr/include`. This path is normally added by the compiler, and hardcoding it can be problematic. In case this causes a problem, please open an issue. A workaround is documented in PR 18658.

([gh-18658](https://github.com/numpy/numpy/pull/18658))

### Changes to comparisons with `dtype=...`

When the `dtype=` (or `signature`) arguments to comparison ufuncs (`equal`, `less`, etc.) is used, this will denote the desired output dtype in the future. This means that:

> np.equal(2, 3, dtype=object)

will give a `FutureWarning` that it will return an `object` array in the future, which currently happens for:

> np.equal(None, None, dtype=object)

due to the fact that `np.array(None)` is already an object array. (This also happens for some other dtypes.)

Since comparisons normally only return boolean arrays, providing any other dtype will always raise an error in the future and give a `DeprecationWarning` now.

([gh-18718](https://github.com/numpy/numpy/pull/18718))

### Changes to `dtype` and `signature` arguments in ufuncs

The universal function arguments `dtype` and `signature` which are also valid for reduction such as `np.add.reduce` (which is the implementation for `np.sum`) will now issue a warning when the `dtype` provided is not a "basic" dtype.

NumPy almost always ignored metadata, byteorder or time units on these inputs. NumPy will now always ignore it and raise an error if byteorder or time unit changed. The following are the most important examples of changes which will give the error. In some cases previously the information stored was not ignored, in all of these an error is now raised:

    # Previously ignored the byte-order (affect if non-native)
    np.add(3, 5, dtype=">i32")
    
    # The biggest impact is for timedelta or datetimes:
    arr = np.arange(10, dtype="m8[s]")
    # The examples always ignored the time unit "ns":
    np.add(arr, arr, dtype="m8[ns]")
    np.maximum.reduce(arr, dtype="m8[ns]")
    
    # The following previously did use "ns" (as opposed to `arr.dtype`)
    np.add(3, 5, dtype="m8[ns]")  # Now return generic time units
    np.maximum(arr, arr, dtype="m8[ns]")  # Now returns "s" (from `arr`)

The same applies for functions like `np.sum` which use these internally. This change is necessary to achieve consistent handling within NumPy.

If you run into these, in most cases pass for example `dtype=np.timedelta64` which clearly denotes a general `timedelta64` without any unit or byte-order defined. If you need to specify the output dtype precisely, you may do so by either casting the inputs or providing an output array using <span class="title-ref">out=</span>.

NumPy may choose to allow providing an exact output `dtype` here in the future, which would be preceded by a `FutureWarning`.

([gh-18718](https://github.com/numpy/numpy/pull/18718))

### Ufunc `signature=...` and `dtype=` generalization and `casting`

The behaviour for `np.ufunc(1.0, 1.0, signature=...)` or `np.ufunc(1.0, 1.0, dtype=...)` can now yield different loops in 1.21 compared to 1.20 because of changes in promotion. When `signature` was previously used, the casting check on inputs was relaxed, which could lead to downcasting inputs unsafely especially if combined with `casting="unsafe"`.

Casting is now guaranteed to be safe. If a signature is only partially provided, for example using `signature=("float64", None, None)`, this could lead to no loop being found (an error). In that case, it is necessary to provide the complete signature to enforce casting the inputs. If `dtype="float64"` is used or only outputs are set (e.g. `signature=(None, None, "float64")` the is unchanged. We expect that very few users are affected by this change.

Further, the meaning of `dtype="float64"` has been slightly modified and now strictly enforces only the correct output (and not input) DTypes. This means it is now always equivalent to:

    signature=(None, None, "float64")

(If the ufunc has two inputs and one output). Since this could lead to no loop being found in some cases, NumPy will normally also search for the loop:

    signature=("float64", "float64", "float64")

if the first search failed. In the future, this behaviour may be customized to achieve the expected results for more complex ufuncs. (For some universal functions such as `np.ldexp` inputs can have different DTypes.)

([gh-18880](https://github.com/numpy/numpy/pull/18880))

### Distutils forces strict floating point model on clang

NumPy distutils will now always add the `-ffp-exception-behavior=strict` compiler flag when compiling with clang. Clang defaults to a non-strict version, which allows the compiler to generate code that does not set floating point warnings/errors correctly.

([gh-19049](https://github.com/numpy/numpy/pull/19049))

## C API changes

### Use of `ufunc->type_resolver` and "type tuple"

NumPy now normalizes the "type tuple" argument to the type resolver functions before calling it. Note that in the use of this type resolver is legacy behaviour and NumPy will not do so when possible. Calling `ufunc->type_resolver` or `PyUFunc_DefaultTypeResolver` is strongly discouraged and will now enforce a normalized type tuple if done. Note that this does not affect providing a type resolver, which is expected to keep working in most circumstances. If you have an unexpected use-case for calling the type resolver, please inform the NumPy developers so that a solution can be found.

([gh-18718](https://github.com/numpy/numpy/pull/18718))

## New Features

### Added a mypy plugin for handling platform-specific `numpy.number` precisions

A [mypy](http://mypy-lang.org/) plugin is now available for automatically assigning the (platform-dependent) precisions of certain <span class="title-ref">\~numpy.number</span> subclasses, including the likes of <span class="title-ref">\~numpy.int\_</span>, <span class="title-ref">\~numpy.intp</span> and <span class="title-ref">\~numpy.longlong</span>. See the documentation on \[scalar types \<arrays.scalars.built-in\>\](\#scalar-types-\<arrays.scalars.built-in\>) for a comprehensive overview of the affected classes.

Note that while usage of the plugin is completely optional, without it the precision of above-mentioned classes will be inferred as <span class="title-ref">\~typing.Any</span>.

To enable the plugin, one must add it to their mypy [configuration file]():

`` `ini     [mypy]     plugins = numpy.typing.mypy_plugin ``\` .. \_configuration file: <https://mypy.readthedocs.io/en/stable/config_file.html>

([gh-17843](https://github.com/numpy/numpy/pull/17843))

### Let the mypy plugin manage extended-precision `numpy.number` subclasses

The [mypy](http://mypy-lang.org/) plugin, introduced in [numpy/numpy\#17843](), has been expanded: the plugin now removes annotations for platform-specific extended-precision types that are not available to the platform in question. For example, it will remove <span class="title-ref">\~numpy.float128</span> when not available.

Without the plugin *all* extended-precision types will, as far as mypy is concerned, be available on all platforms.

To enable the plugin, one must add it to their mypy [configuration file]():

`` `ini     [mypy]     plugins = numpy.typing.mypy_plugin ``\` .. \_configuration file: <https://mypy.readthedocs.io/en/stable/config_file.html> .. \_\`numpy/numpy\#17843\`: <https://github.com/numpy/numpy/pull/17843>

([gh-18322](https://github.com/numpy/numpy/pull/18322))

### New `min_digits` argument for printing float values

A new `min_digits` argument has been added to the dragon4 float printing functions <span class="title-ref">\~numpy.format\_float\_positional</span> and <span class="title-ref">\~numpy.format\_float\_scientific</span> . This kwd guarantees that at least the given number of digits will be printed when printing in unique=True mode, even if the extra digits are unnecessary to uniquely specify the value. It is the counterpart to the precision argument which sets the maximum number of digits to be printed. When unique=False in fixed precision mode, it has no effect and the precision argument fixes the number of digits.

([gh-18629](https://github.com/numpy/numpy/pull/18629))

### f2py now recognizes Fortran abstract interface blocks

<span class="title-ref">\~numpy.f2py</span> can now parse abstract interface blocks.

([gh-18695](https://github.com/numpy/numpy/pull/18695))

### BLAS and LAPACK configuration via environment variables

Autodetection of installed BLAS and LAPACK libraries can be bypassed by using the `NPY_BLAS_LIBS` and `NPY_LAPACK_LIBS` environment variables. Instead, the link flags in these environment variables will be used directly, and the language is assumed to be F77. This is especially useful in automated builds where the BLAS and LAPACK that are installed are known exactly. A use case is replacing the actual implementation at runtime via stub library links.

If `NPY_CBLAS_LIBS` is set (optional in addition to `NPY_BLAS_LIBS`), this will be used as well, by defining `HAVE_CBLAS` and appending the environment variable content to the link flags.

([gh-18737](https://github.com/numpy/numpy/pull/18737))

### A runtime-subcriptable alias has been added for `ndarray`

`numpy.typing.NDArray` has been added, a runtime-subscriptable alias for `np.ndarray[Any, np.dtype[~Scalar]]`. The new type alias can be used for annotating arrays with a given dtype and unspecified shape. <sup>1</sup>

<sup>1</sup> NumPy does not support the annotating of array shapes as of 1.21, this is expected to change in the future though (see `646`).

#### Examples

`` `python     >>> import numpy as np     >>> import numpy.typing as npt      >>> print(npt.NDArray)     numpy.ndarray[typing.Any, numpy.dtype[~ScalarType]]      >>> print(npt.NDArray[np.float64])     numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]      >>> NDArrayInt = npt.NDArray[np.int_]     >>> a: NDArrayInt = np.arange(10)      >>> def func(a: npt.ArrayLike) -> npt.NDArray[Any]:     ...     return np.array(a)  (`gh-18935 <https://github.com/numpy/numpy/pull/18935>`__)   Improvements ``\` ============

### Arbitrary `period` option for `numpy.unwrap`

The size of the interval over which phases are unwrapped is no longer restricted to `2 * pi`. This is especially useful for unwrapping degrees, but can also be used for other intervals.

``` python
>>> phase_deg = np.mod(np.linspace(0,720,19), 360) - 180
>>> phase_deg
array([-180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
       -180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
       -180.])

>>> unwrap(phase_deg, period=360)
array([-180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
        180.,  220.,  260.,  300.,  340.,  380.,  420.,  460.,  500.,
        540.])
```

([gh-16987](https://github.com/numpy/numpy/pull/16987))

### `np.unique` now returns single `NaN`

When `np.unique` operated on an array with multiple `NaN` entries, its return included a `NaN` for each entry that was `NaN` in the original array. This is now improved such that the returned array contains just one `NaN` as the last element.

Also for complex arrays all `NaN` values are considered equivalent (no matter whether the `NaN` is in the real or imaginary part). As the representant for the returned array the smallest one in the lexicographical order is chosen - see `np.sort` for how the lexicographical order is defined for complex arrays.

([gh-18070](https://github.com/numpy/numpy/pull/18070))

### `Generator.rayleigh` and `Generator.geometric` performance improved

The performance of Rayleigh and geometric random variate generation in `Generator` has improved. These are both transformation of exponential random variables and the slow log-based inverse cdf transformation has been replaced with the Ziggurat-based exponential variate generator.

This change breaks the stream of variates generated when variates from either of these distributions are produced.

([gh-18666](https://github.com/numpy/numpy/pull/18666))

### Placeholder annotations have been improved

All placeholder annotations, that were previously annotated as `typing.Any`, have been improved. Where appropriate they have been replaced with explicit function definitions, classes or other miscellaneous objects.

([gh-18934](https://github.com/numpy/numpy/pull/18934))

## Performance improvements

### Improved performance in integer division of NumPy arrays

Integer division of NumPy arrays now uses [libdivide](https://libdivide.com/) when the divisor is a constant. With the usage of libdivide and other minor optimizations, there is a large speedup. The `//` operator and `np.floor_divide` makes use of the new changes.

([gh-17727](https://github.com/numpy/numpy/pull/17727))

### Improve performance of `np.save` and `np.load` for small arrays

`np.save` is now a lot faster for small arrays.

`np.load` is also faster for small arrays, but only when serializing with a version \>= `(3, 0)`.

Both are done by removing checks that are only relevant for Python 2, while still maintaining compatibility with arrays which might have been created by Python 2.

([gh-18657](https://github.com/numpy/numpy/pull/18657))

## Changes

### <span class="title-ref">numpy.piecewise</span> output class now matches the input class

When <span class="title-ref">\~numpy.ndarray</span> subclasses are used on input to <span class="title-ref">\~numpy.piecewise</span>, they are passed on to the functions. The output will now be of the same subclass as well.

([gh-18110](https://github.com/numpy/numpy/pull/18110))

### Enable Accelerate Framework

With the release of macOS 11.3, several different issues that numpy was encountering when using Accelerate Framework's implementation of BLAS and LAPACK should be resolved. This change enables the Accelerate Framework as an option on macOS. If additional issues are found, please file a bug report against Accelerate using the developer feedback assistant tool (<https://developer.apple.com/bug-reporting/>). We intend to address issues promptly and plan to continue supporting and updating our BLAS and LAPACK libraries.

([gh-18874](https://github.com/numpy/numpy/pull/18874))

---

1.21.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.1 Release Notes

The NumPy 1.21.1 is maintenance release that fixes bugs discovered after the 1.21.0 release and updates OpenBLAS to v0.3.17 to deal with problems on arm64.

The Python versions supported for this release are 3.7-3.9. The 1.21.x series is compatible with development Python 3.10. Python 3.10 will be officially supported after it is released.

<div class="warning">

<div class="title">

Warning

</div>

There are unresolved problems compiling NumPy 1.20.0 with gcc-11.1.

  - Optimization level <span class="title-ref">-O3</span> results in many incorrect warnings when running the tests.
  - On some hardware NumPY will hang in an infinite loop.

</div>

## Contributors

A total of 11 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Ganesh Kathiresan
  - Gregory R. Lee
  - Hugo Defois +
  - Kevin Sheppard
  - Matti Picus
  - Ralf Gommers
  - Sayed Adel
  - Sebastian Berg
  - Thomas J. Fan

## Pull requests merged

A total of 26 pull requests were merged for this release.

  - [\#19311](https://github.com/numpy/numpy/pull/19311): REV,BUG: Replace `NotImplemented` with `typing.Any`
  - [\#19324](https://github.com/numpy/numpy/pull/19324): MAINT: Fixed the return-dtype of `ndarray.real` and `imag`
  - [\#19330](https://github.com/numpy/numpy/pull/19330): MAINT: Replace `"dtype[Any]"` with `dtype` in the definition of...
  - [\#19342](https://github.com/numpy/numpy/pull/19342): DOC: Fix some docstrings that crash pdf generation.
  - [\#19343](https://github.com/numpy/numpy/pull/19343): MAINT: bump scipy-mathjax
  - [\#19347](https://github.com/numpy/numpy/pull/19347): BUG: Fix arr.flat.index for large arrays and big-endian machines
  - [\#19348](https://github.com/numpy/numpy/pull/19348): ENH: add `numpy.f2py.get_include` function
  - [\#19349](https://github.com/numpy/numpy/pull/19349): BUG: Fix reference count leak in ufunc dtype handling
  - [\#19350](https://github.com/numpy/numpy/pull/19350): MAINT: Annotate missing attributes of `np.number` subclasses
  - [\#19351](https://github.com/numpy/numpy/pull/19351): BUG: Fix cast safety and comparisons for zero sized voids
  - [\#19352](https://github.com/numpy/numpy/pull/19352): BUG: Correct Cython declaration in random
  - [\#19353](https://github.com/numpy/numpy/pull/19353): BUG: protect against accessing base attribute of a NULL subarray
  - [\#19365](https://github.com/numpy/numpy/pull/19365): BUG, SIMD: Fix detecting AVX512 features on Darwin
  - [\#19366](https://github.com/numpy/numpy/pull/19366): MAINT: remove `print()`'s in distutils template handling
  - [\#19390](https://github.com/numpy/numpy/pull/19390): ENH: SIMD architectures to show\_config
  - [\#19391](https://github.com/numpy/numpy/pull/19391): BUG: Do not raise deprecation warning for all nans in unique...
  - [\#19392](https://github.com/numpy/numpy/pull/19392): BUG: Fix NULL special case in object-to-any cast code
  - [\#19430](https://github.com/numpy/numpy/pull/19430): MAINT: Use arm64-graviton2 for testing on travis
  - [\#19495](https://github.com/numpy/numpy/pull/19495): BUILD: update OpenBLAS to v0.3.17
  - [\#19496](https://github.com/numpy/numpy/pull/19496): MAINT: Avoid unicode characters in division SIMD code comments
  - [\#19499](https://github.com/numpy/numpy/pull/19499): BUG, SIMD: Fix infinite loop during count non-zero on GCC-11
  - [\#19500](https://github.com/numpy/numpy/pull/19500): BUG: fix a numpy.npiter leak in npyiter\_multi\_index\_set
  - [\#19501](https://github.com/numpy/numpy/pull/19501): TST: Fix a `GenericAlias` test failure for python 3.9.0
  - [\#19502](https://github.com/numpy/numpy/pull/19502): MAINT: Start testing with Python 3.10.0b3.
  - [\#19503](https://github.com/numpy/numpy/pull/19503): MAINT: Add missing dtype overloads for object- and ctypes-based...
  - [\#19510](https://github.com/numpy/numpy/pull/19510): REL: Prepare for NumPy 1.21.1 release.

---

1.21.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.2 Release Notes

The NumPy 1.21.2 is a maintenance release that fixes bugs discovered after 1.21.1. It also provides 64 bit manylinux Python 3.10.0rc1 wheels for downstream testing. Note that Python 3.10 is not yet final. It also has preliminary support for Windows on ARM64, but there is no OpenBLAS for that platform and no wheels are available.

The Python versions supported for this release are 3.7-3.9. The 1.21.x series is compatible with Python 3.10.0rc1 and Python 3.10 will be officially supported after it is released. The previous problems with gcc-11.1 have been fixed by gcc-11.2, check your version if you are using gcc-11.

## Contributors

A total of 10 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Carl Johnsen +
  - Charles Harris
  - Gwyn Ciesla +
  - Matthieu Dartiailh
  - Matti Picus
  - Niyas Sait +
  - Ralf Gommers
  - Sayed Adel
  - Sebastian Berg

## Pull requests merged

A total of 18 pull requests were merged for this release.

  - [\#19497](https://github.com/numpy/numpy/pull/19497): MAINT: set Python version for 1.21.x to `<3.11`
  - [\#19533](https://github.com/numpy/numpy/pull/19533): BUG: Fix an issue wherein importing `numpy.typing` could raise
  - [\#19646](https://github.com/numpy/numpy/pull/19646): MAINT: Update Cython version for Python 3.10.
  - [\#19648](https://github.com/numpy/numpy/pull/19648): TST: Bump the python 3.10 test version from beta4 to rc1
  - [\#19651](https://github.com/numpy/numpy/pull/19651): TST: avoid distutils.sysconfig in runtests.py
  - [\#19652](https://github.com/numpy/numpy/pull/19652): MAINT: add missing dunder method to nditer type hints
  - [\#19656](https://github.com/numpy/numpy/pull/19656): BLD, SIMD: Fix testing extra checks when `-Werror` isn't applicable...
  - [\#19657](https://github.com/numpy/numpy/pull/19657): BUG: Remove logical object ufuncs with bool output
  - [\#19658](https://github.com/numpy/numpy/pull/19658): MAINT: Include .coveragerc in source distributions to support...
  - [\#19659](https://github.com/numpy/numpy/pull/19659): BUG: Fix bad write in masked iterator output copy paths
  - [\#19660](https://github.com/numpy/numpy/pull/19660): ENH: Add support for windows on arm targets
  - [\#19661](https://github.com/numpy/numpy/pull/19661): BUG: add base to templated arguments for platlib
  - [\#19662](https://github.com/numpy/numpy/pull/19662): BUG,DEP: Non-default UFunc signature/dtype usage should be deprecated
  - [\#19666](https://github.com/numpy/numpy/pull/19666): MAINT: Add Python 3.10 to supported versions.
  - [\#19668](https://github.com/numpy/numpy/pull/19668): TST,BUG: Sanitize path-separators when running `runtest.py`
  - [\#19671](https://github.com/numpy/numpy/pull/19671): BLD: load extra flags when checking for libflame
  - [\#19676](https://github.com/numpy/numpy/pull/19676): BLD: update circleCI docker image
  - [\#19677](https://github.com/numpy/numpy/pull/19677): REL: Prepare for 1.21.2 release.

---

1.21.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.3 Release Notes

NumPy 1.21.3 is a maintenance release that fixes a few bugs discovered after 1.21.2. It also provides 64 bit Python 3.10.0 wheels. Note a few oddities about Python 3.10:

  - There are no 32 bit wheels for Windows, Mac, or Linux.
  - The Mac Intel builds are only available in universal2 wheels.

The Python versions supported in this release are 3.7-3.10. If you want to compile your own version using gcc-11, you will need to use gcc-11.2+ to avoid problems.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Aaron Meurer
  - Bas van Beek
  - Charles Harris
  - Developer-Ecosystem-Engineering +
  - Kevin Sheppard
  - Sebastian Berg
  - Warren Weckesser

## Pull requests merged

A total of 8 pull requests were merged for this release.

  - [\#19745](https://github.com/numpy/numpy/pull/19745): ENH: Add dtype-support to 3 `` `generic ``/`ndarray` methods
  - [\#19955](https://github.com/numpy/numpy/pull/19955): BUG: Resolve Divide by Zero on Apple silicon + test failures...
  - [\#19958](https://github.com/numpy/numpy/pull/19958): MAINT: Mark type-check-only ufunc subclasses as ufunc aliases...
  - [\#19994](https://github.com/numpy/numpy/pull/19994): BUG: np.tan(np.inf) test failure
  - [\#20080](https://github.com/numpy/numpy/pull/20080): BUG: Correct incorrect advance in PCG with emulated int128
  - [\#20081](https://github.com/numpy/numpy/pull/20081): BUG: Fix NaT handling in the PyArray\_CompareFunc for datetime...
  - [\#20082](https://github.com/numpy/numpy/pull/20082): DOC: Ensure that we add documentation also as to the dict for...
  - [\#20106](https://github.com/numpy/numpy/pull/20106): BUG: core: result\_type(0, np.timedelta64(4)) would seg. fault.

---

1.21.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.4 Release Notes

The NumPy 1.21.4 is a maintenance release that fixes a few bugs discovered after 1.21.3. The most important fix here is a fix for the NumPy header files to make them work for both x86\_64 and M1 hardware when included in the Mac universal2 wheels. Previously, the header files only worked for M1 and this caused problems for folks building x86\_64 extensions. This problem was not seen before Python 3.10 because there were thin wheels for x86\_64 that had precedence. This release also provides thin x86\_64 Mac wheels for Python 3.10.

The Python versions supported in this release are 3.7-3.10. If you want to compile your own version using gcc-11, you will need to use gcc-11.2+ to avoid problems.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Isuru Fernando
  - Matthew Brett
  - Sayed Adel
  - Sebastian Berg
  - å‚…ç«‹ä¸šï¼ˆChris Fuï¼‰ +

## Pull requests merged

A total of 9 pull requests were merged for this release.

  - [\#20278](https://github.com/numpy/numpy/pull/20278): BUG: Fix shadowed reference of `dtype` in type stub
  - [\#20293](https://github.com/numpy/numpy/pull/20293): BUG: Fix headers for universal2 builds
  - [\#20294](https://github.com/numpy/numpy/pull/20294): BUG: `VOID_nonzero` could sometimes mutate alignment flag
  - [\#20295](https://github.com/numpy/numpy/pull/20295): BUG: Do not use nonzero fastpath on unaligned arrays
  - [\#20296](https://github.com/numpy/numpy/pull/20296): BUG: Distutils patch to allow for 2 as a minor version (\!)
  - [\#20297](https://github.com/numpy/numpy/pull/20297): BUG, SIMD: Fix 64-bit/8-bit integer division by a scalar
  - [\#20298](https://github.com/numpy/numpy/pull/20298): BUG, SIMD: Workaround broadcasting SIMD 64-bit integers on MSVC...
  - [\#20300](https://github.com/numpy/numpy/pull/20300): REL: Prepare for the NumPy 1.21.4 release.
  - [\#20302](https://github.com/numpy/numpy/pull/20302): TST: Fix a `Arrayterator` typing test failure

---

1.21.5-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.5 Release Notes

NumPy 1.21.5 is a maintenance release that fixes a few bugs discovered after the 1.21.4 release and does some maintenance to extend the 1.21.x lifetime. The Python versions supported in this release are 3.7-3.10. If you want to compile your own version using gcc-11, you will need to use gcc-11.2+ to avoid problems.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Matti Picus
  - Rohit Goswami
  - Ross Barnowski
  - Sayed Adel
  - Sebastian Berg

## Pull requests merged

A total of 11 pull requests were merged for this release.

  - [\#20357](https://github.com/numpy/numpy/pull/20357): MAINT: Do not forward `__(deep)copy__` calls of `_GenericAlias`...
  - [\#20462](https://github.com/numpy/numpy/pull/20462): BUG: Fix float16 einsum fastpaths using wrong tempvar
  - [\#20463](https://github.com/numpy/numpy/pull/20463): BUG, DIST: Print os error message when the executable not exist
  - [\#20464](https://github.com/numpy/numpy/pull/20464): BLD: Verify the ability to compile C++ sources before initiating...
  - [\#20465](https://github.com/numpy/numpy/pull/20465): BUG: Force ``npymath` ` to respect``npy\_longdouble\`\`
  - [\#20466](https://github.com/numpy/numpy/pull/20466): BUG: Fix failure to create aligned, empty structured dtype
  - [\#20467](https://github.com/numpy/numpy/pull/20467): ENH: provide a convenience function to replace npy\_load\_module
  - [\#20495](https://github.com/numpy/numpy/pull/20495): MAINT: update wheel to version that supports python3.10
  - [\#20497](https://github.com/numpy/numpy/pull/20497): BUG: Clear errors correctly in F2PY conversions
  - [\#20613](https://github.com/numpy/numpy/pull/20613): DEV: add a warningfilter to fix pytest workflow.
  - [\#20618](https://github.com/numpy/numpy/pull/20618): MAINT: Help boost::python libraries at least not crash

---

1.21.6-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.21.6 Release Notes

NumPy 1.21.6 is a very small release that achieves two things:

  - Backs out the mistaken backport of C++ code into 1.21.5.
  - Provides a 32 bit Windows wheel for Python 3.10.

The provision of the 32 bit wheel is intended to make life easier for oldest-supported-numpy.

---

1.22.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.22.0 Release Notes

NumPy 1.22.0 is a big release featuring the work of 153 contributors spread over 609 pull requests. There have been many improvements, highlights are:

  - Annotations of the main namespace are essentially complete. Upstream is a moving target, so there will likely be further improvements, but the major work is done. This is probably the most user visible enhancement in this release.
  - A preliminary version of the proposed Array-API is provided. This is a step in creating a standard collection of functions that can be used across applications such as CuPy and JAX.
  - NumPy now has a DLPack backend. DLPack provides a common interchange format for array (tensor) data.
  - New methods for `quantile`, `percentile`, and related functions. The new methods provide a complete set of the methods commonly found in the literature.
  - The universal functions have been refactored to implement most of \[NEP 43 \<NEP43\>\](\#nep-43-\<nep43\>). This also unlocks the ability to experiment with the future DType API.
  - A new configurable allocator for use by downstream projects.

These are in addition to the ongoing work to provide SIMD support for commonly used functions, improvements to F2PY, and better documentation.

The Python versions supported in this release are 3.8-3.10, Python 3.7 has been dropped. Note that the Mac wheels are now based on OS X 10.14 rather than 10.9 that was used in previous NumPy release cycles. 10.14 is the oldest release supported by Apple. Also note that 32 bit wheels are only provided for Python 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of Ubuntu, Fedora, and other Linux distributions dropping 32 bit support. All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix the occasional problems encountered by folks using truly huge arrays.

## Expired deprecations

### Deprecated numeric style dtype strings have been removed

Using the strings `"Bytes0"`, `"Datetime64"`, `"Str0"`, `"Uint32"`, and `"Uint64"` as a dtype will now raise a `TypeError`.

([gh-19539](https://github.com/numpy/numpy/pull/19539))

### Expired deprecations for `loads`, `ndfromtxt`, and `mafromtxt` in npyio

`numpy.loads` was deprecated in v1.15, with the recommendation that users use `pickle.loads` instead. `ndfromtxt` and `mafromtxt` were both deprecated in v1.17 - users should use `numpy.genfromtxt` instead with the appropriate value for the `usemask` parameter.

([gh-19615](https://github.com/numpy/numpy/pull/19615))

## Deprecations

### Use delimiter rather than delimitor as kwarg in mrecords

The misspelled keyword argument `delimitor` of `numpy.ma.mrecords.fromtextfile()` has been changed to `delimiter`, using it will emit a deprecation warning.

([gh-19921](https://github.com/numpy/numpy/pull/19921))

### Passing boolean `kth` values to (arg-)partition has been deprecated

`numpy.partition` and `numpy.argpartition` would previously accept boolean values for the `kth` parameter, which would subsequently be converted into integers. This behavior has now been deprecated.

([gh-20000](https://github.com/numpy/numpy/pull/20000))

### The `np.MachAr` class has been deprecated

The `numpy.MachAr` class and `finfo.machar <numpy.finfo>` attribute have been deprecated. Users are encouraged to access the property if interest directly from the corresponding `numpy.finfo` attribute.

([gh-20201](https://github.com/numpy/numpy/pull/20201))

## Compatibility notes

### Distutils forces strict floating point model on clang

NumPy now sets the `-ftrapping-math` option on clang to enforce correct floating point error handling for universal functions. Clang defaults to non-IEEE and C99 conform behaviour otherwise. This change (using the equivalent but newer `-ffp-exception-behavior=strict`) was attempted in NumPy 1.21, but was effectively never used.

([gh-19479](https://github.com/numpy/numpy/pull/19479))

### Removed floor division support for complex types

Floor division of complex types will now result in a `TypeError`

`` `python     >>> a = np.arange(10) + 1j* np.arange(10)     >>> a // 1     TypeError: ufunc 'floor_divide' not supported for the input types...  (`gh-19135 <https://github.com/numpy/numpy/pull/19135>`__) ``numpy.vectorize`functions now produce the same output class as the base function`<span class="title-ref"> ------------------------------------------------------------------------------------ When a function that respects </span><span class="title-ref">numpy.ndarray</span><span class="title-ref"> subclasses is vectorized using </span><span class="title-ref">numpy.vectorize</span><span class="title-ref">, the vectorized function will now be subclass-safe also for cases that a signature is given (i.e., when creating a </span><span class="title-ref">gufunc</span>\`): the output class will be the same as that returned by the first call to the underlying function.

([gh-19356](https://github.com/numpy/numpy/pull/19356))

### Python 3.7 is no longer supported

Python support has been dropped. This is rather strict, there are changes that require Python \>= 3.8.

([gh-19665](https://github.com/numpy/numpy/pull/19665))

### str/repr of complex dtypes now include space after punctuation

The repr of `np.dtype({"names": ["a"], "formats": [int], "offsets": [2]})` is now `dtype({'names': ['a'], 'formats': ['<i8'], 'offsets': [2], 'itemsize': 10})`, whereas spaces where previously omitted after colons and between fields.

The old behavior can be restored via `np.set_printoptions(legacy="1.21")`.

([gh-19687](https://github.com/numpy/numpy/pull/19687))

### Corrected `advance` in `PCG64DSXM` and `PCG64`

Fixed a bug in the `advance` method of `PCG64DSXM` and `PCG64`. The bug only affects results when the step was larger than \(2^{64}\) on platforms that do not support 128-bit integers(e.g., Windows and 32-bit Linux).

([gh-20049](https://github.com/numpy/numpy/pull/20049))

### Change in generation of random 32 bit floating point variates

There was bug in the generation of 32 bit floating point values from the uniform distribution that would result in the least significant bit of the random variate always being 0. This has been fixed.

This change affects the variates produced by the `random.Generator` methods `random`, `standard_normal`, `standard_exponential`, and `standard_gamma`, but only when the dtype is specified as `numpy.float32`.

([gh-20314](https://github.com/numpy/numpy/pull/20314))

## C API changes

### Masked inner-loops cannot be customized anymore

The masked inner-loop selector is now never used. A warning will be given in the unlikely event that it was customized.

We do not expect that any code uses this. If you do use it, you must unset the selector on newer NumPy version. Please also contact the NumPy developers, we do anticipate providing a new, more specific, mechanism.

The customization was part of a never-implemented feature to allow for faster masked operations.

([gh-19259](https://github.com/numpy/numpy/pull/19259))

### Experimental exposure of future DType and UFunc API

The new header `experimental_public_dtype_api.h` allows to experiment with future API for improved universal function and especially user DType support. At this time it is advisable to experiment using the development version of NumPy since some changes are expected and new features will be unlocked.

([gh-19919](https://github.com/numpy/numpy/pull/19919))

## New Features

### NEP 49 configurable allocators

As detailed in [NEP 49](https://numpy.org/neps/nep-0049.html), the function used for allocation of the data segment of a ndarray can be changed. The policy can be set globally or in a context. For more information see the NEP and the \[data\_memory\](\#data\_memory) reference docs. Also add a `NUMPY_WARN_IF_NO_MEM_POLICY` override to warn on dangerous use of transferring ownership by setting `NPY_ARRAY_OWNDATA`.

([gh-17582](https://github.com/numpy/numpy/pull/17582))

### Implementation of the NEP 47 (adopting the array API standard)

An initial implementation of [NEP 47](https://numpy.org/neps/nep-0047-array-api-standard.html) (adoption the array API standard) has been added as `numpy.array_api`. The implementation is experimental and will issue a UserWarning on import, as the [array API standard](https://data-apis.org/array-api/latest/index.html) is still in draft state. `numpy.array_api` is a conforming implementation of the array API standard, which is also minimal, meaning that only those functions and behaviors that are required by the standard are implemented (see the NEP for more info). Libraries wishing to make use of the array API standard are encouraged to use `numpy.array_api` to check that they are only using functionality that is guaranteed to be present in standard conforming implementations.

([gh-18585](https://github.com/numpy/numpy/pull/18585))

### Generate C/C++ API reference documentation from comments blocks is now possible

This feature depends on [Doxygen](https://www.doxygen.nl/index.html) in the generation process and on [Breathe](https://breathe.readthedocs.io/en/latest/) to integrate it with Sphinx.

([gh-18884](https://github.com/numpy/numpy/pull/18884))

### Assign the platform-specific `c_intp` precision via a mypy plugin

The [mypy](http://mypy-lang.org/) plugin, introduced in [numpy/numpy\#17843](), has again been expanded: the plugin now is now responsible for setting the platform-specific precision of `numpy.ctypeslib.c_intp`, the latter being used as data type for various `numpy.ndarray.ctypes` attributes.

Without the plugin, aforementioned type will default to `ctypes.c_int64`.

To enable the plugin, one must add it to their mypy [configuration file]():

`` `ini     [mypy]     plugins = numpy.typing.mypy_plugin ``\` .. \_configuration file: <https://mypy.readthedocs.io/en/stable/config_file.html> .. \_\`numpy/numpy\#17843\`: <https://github.com/numpy/numpy/pull/17843>

([gh-19062](https://github.com/numpy/numpy/pull/19062))

### Add NEP 47-compatible dlpack support

Add a `ndarray.__dlpack__()` method which returns a `dlpack` C structure wrapped in a `PyCapsule`. Also add a `np._from_dlpack(obj)` function, where `obj` supports `__dlpack__()`, and returns an `ndarray`.

([gh-19083](https://github.com/numpy/numpy/pull/19083))

### `keepdims` optional argument added to `numpy.argmin`, `numpy.argmax`

`keepdims` argument is added to `numpy.argmin`, `numpy.argmax`. If set to `True`, the axes which are reduced are left in the result as dimensions with size one. The resulting array has the same number of dimensions and will broadcast with the input array.

([gh-19211](https://github.com/numpy/numpy/pull/19211))

### `bit_count` to compute the number of 1-bits in an integer

Computes the number of 1-bits in the absolute value of the input. This works on all the numpy integer types. Analogous to the builtin `int.bit_count` or `popcount` in C++.

`` `python     >>> np.uint32(1023).bit_count()     10     >>> np.int32(-127).bit_count()     7  (`gh-19355 <https://github.com/numpy/numpy/pull/19355>`__)  The ``ndim`and`axis`attributes have been added to`numpy.AxisError`  `<span class="title-ref"> --------------------------------------------------------------------------- The </span><span class="title-ref">ndim</span><span class="title-ref"> and </span><span class="title-ref">axis</span><span class="title-ref"> parameters are now also stored as attributes within each </span><span class="title-ref">numpy.AxisError</span>\` instance.

([gh-19459](https://github.com/numpy/numpy/pull/19459))

### Preliminary support for `windows/arm64` target

`numpy` added support for windows/arm64 target. Please note `OpenBLAS` support is not yet available for windows/arm64 target.

([gh-19513](https://github.com/numpy/numpy/pull/19513))

### Added support for LoongArch

LoongArch is a new instruction set, numpy compilation failure on LoongArch architecture, so add the commit.

([gh-19527](https://github.com/numpy/numpy/pull/19527))

### A `.clang-format` file has been added

Clang-format is a C/C++ code formatter, together with the added `.clang-format` file, it produces code close enough to the NumPy C\_STYLE\_GUIDE for general use. Clang-format version 12+ is required due to the use of several new features, it is available in Fedora 34 and Ubuntu Focal among other distributions.

([gh-19754](https://github.com/numpy/numpy/pull/19754))

### `is_integer` is now available to `numpy.floating` and `numpy.integer`

Based on its counterpart in Python `float` and `int`, the numpy floating point and integer types now support `float.is_integer`. Returns `True` if the number is finite with integral value, and `False` otherwise.

`` `python     >>> np.float32(-2.0).is_integer()     True     >>> np.float64(3.2).is_integer()     False     >>> np.int32(-2).is_integer()     True  (`gh-19803 <https://github.com/numpy/numpy/pull/19803>`__)  Symbolic parser for Fortran dimension specifications ``\` ----------------------------------------------------A new symbolic parser has been added to f2py in order to correctly parse dimension specifications. The parser is the basis for future improvements and provides compatibility with Draft Fortran 202x.

([gh-19805](https://github.com/numpy/numpy/pull/19805))

### `ndarray`, `dtype` and `number` are now runtime-subscriptable

Mimicking `585`, the `numpy.ndarray`, `numpy.dtype` and `numpy.number` classes are now subscriptable for python 3.9 and later. Consequently, expressions that were previously only allowed in .pyi stub files or with the help of `from __future__ import annotations` are now also legal during runtime.

`` `python     >>> import numpy as np     >>> from typing import Any      >>> np.ndarray[Any, np.dtype[np.float64]]     numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]  (`gh-19879 <https://github.com/numpy/numpy/pull/19879>`__)   Improvements ``\` ============

### `ctypeslib.load_library` can now take any path-like object

All parameters in the can now take any `python:path-like object`. This includes the likes of strings, bytes and objects implementing the <span class="title-ref">\_\_fspath\_\_\<os.PathLike.\_\_fspath\_\_\></span> protocol.

([gh-17530](https://github.com/numpy/numpy/pull/17530))

### Add `smallest_normal` and `smallest_subnormal` attributes to `finfo`

The attributes `smallest_normal` and `smallest_subnormal` are available as an extension of `finfo` class for any floating-point data type. To use these new attributes, write `np.finfo(np.float64).smallest_normal` or `np.finfo(np.float64).smallest_subnormal`.

([gh-18536](https://github.com/numpy/numpy/pull/18536))

### `numpy.linalg.qr` accepts stacked matrices as inputs

`numpy.linalg.qr` is able to produce results for stacked matrices as inputs. Moreover, the implementation of QR decomposition has been shifted to C from Python.

([gh-19151](https://github.com/numpy/numpy/pull/19151))

### `numpy.fromregex` now accepts `os.PathLike` implementations

`numpy.fromregex` now accepts objects implementing the `__fspath__<os.PathLike>` protocol, *e.g.* `pathlib.Path`.

([gh-19680](https://github.com/numpy/numpy/pull/19680))

### Add new methods for `quantile` and `percentile`

`quantile` and `percentile` now have have a `method=` keyword argument supporting 13 different methods. This replaces the `interpolation=` keyword argument.

The methods are now aligned with nine methods which can be found in scientific literature and the R language. The remaining methods are the previous discontinuous variations of the default "linear" one.

Please see the documentation of `numpy.percentile` for more information.

([gh-19857](https://github.com/numpy/numpy/pull/19857))

### Missing parameters have been added to the `nan<x>` functions

A number of the `nan<x>` functions previously lacked parameters that were present in their `<x>`-based counterpart, *e.g.* the `where` parameter was present in `numpy.mean` but absent from `numpy.nanmean`.

The following parameters have now been added to the `nan<x>` functions:

  - nanmin: `initial` & `where`
  - nanmax: `initial` & `where`
  - nanargmin: `keepdims` & `out`
  - nanargmax: `keepdims` & `out`
  - nansum: `initial` & `where`
  - nanprod: `initial` & `where`
  - nanmean: `where`
  - nanvar: `where`
  - nanstd: `where`

([gh-20027](https://github.com/numpy/numpy/pull/20027))

### Annotating the main Numpy namespace

Starting from the 1.20 release, PEP 484 type annotations have been included for parts of the NumPy library; annotating the remaining functions being a work in progress. With the release of 1.22 this process has been completed for the main NumPy namespace, which is now fully annotated.

Besides the main namespace, a limited number of sub-packages contain annotations as well. This includes, among others, `numpy.testing`, `numpy.linalg` and `numpy.random` (available since 1.21).

([gh-20217](https://github.com/numpy/numpy/pull/20217))

### Vectorize umath module using AVX-512

By leveraging Intel Short Vector Math Library (SVML), 18 umath functions (`exp2`, `log2`, `log10`, `expm1`, `log1p`, `cbrt`, `sin`, `cos`, `tan`, `arcsin`, `arccos`, `arctan`, `sinh`, `cosh`, `tanh`, `arcsinh`, `arccosh`, `arctanh`) are vectorized using AVX-512 instruction set for both single and double precision implementations. This change is currently enabled only for Linux users and on processors with AVX-512 instruction set. It provides an average speed up of 32x and 14x for single and double precision functions respectively.

([gh-19478](https://github.com/numpy/numpy/pull/19478))

### OpenBLAS v0.3.18

Update the OpenBLAS used in testing and in wheels to v0.3.18

([gh-20058](https://github.com/numpy/numpy/pull/20058))

---

1.22.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.22.1 Release Notes

The NumPy 1.22.1 is a maintenance release that fixes bugs discovered after the 1.22.0 release. Notable fixes are:

  - Fix f2PY docstring problems (SciPy)
  - Fix reduction type problems (AstroPy)
  - Fix various typing bugs.

The Python versions supported for this release are 3.8-3.10.

## Contributors

A total of 14 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Arryan Singh
  - Bas van Beek
  - Charles Harris
  - Denis Laxalde
  - Isuru Fernando
  - Kevin Sheppard
  - Matthew Barber
  - Matti Picus
  - Melissa Weber MendonÃ§a
  - Mukulika Pahari
  - Omid Rajaei +
  - Pearu Peterson
  - Ralf Gommers
  - Sebastian Berg

## Pull requests merged

A total of 20 pull requests were merged for this release.

  - [\#20702](https://github.com/numpy/numpy/pull/20702): MAINT, DOC: Post 1.22.0 release fixes.
  - [\#20703](https://github.com/numpy/numpy/pull/20703): DOC, BUG: Use pngs instead of svgs.
  - [\#20704](https://github.com/numpy/numpy/pull/20704): DOC: Fixed the link on user-guide landing page
  - [\#20714](https://github.com/numpy/numpy/pull/20714): BUG: Restore vc141 support
  - [\#20724](https://github.com/numpy/numpy/pull/20724): BUG: Fix array dimensions solver for multidimensional arguments...
  - [\#20725](https://github.com/numpy/numpy/pull/20725): TYP: change type annotation for `__array_namespace__` to ModuleType
  - [\#20726](https://github.com/numpy/numpy/pull/20726): TYP, MAINT: Allow `ndindex` to accept integer tuples
  - [\#20757](https://github.com/numpy/numpy/pull/20757): BUG: Relax dtype identity check in reductions
  - [\#20763](https://github.com/numpy/numpy/pull/20763): TYP: Allow time manipulation functions to accept `date` and `timedelta`...
  - [\#20768](https://github.com/numpy/numpy/pull/20768): TYP: Relax the type of `ndarray.__array_finalize__`
  - [\#20795](https://github.com/numpy/numpy/pull/20795): MAINT: Raise RuntimeError if setuptools version is too recent.
  - [\#20796](https://github.com/numpy/numpy/pull/20796): BUG, DOC: Fixes SciPy docs build warnings
  - [\#20797](https://github.com/numpy/numpy/pull/20797): DOC: fix OpenBLAS version in release note
  - [\#20798](https://github.com/numpy/numpy/pull/20798): PERF: Optimize array check for bounded 0,1 values
  - [\#20805](https://github.com/numpy/numpy/pull/20805): BUG: Fix that reduce-likes honor out always (and live in the...
  - [\#20806](https://github.com/numpy/numpy/pull/20806): BUG: `array_api.argsort(descending=True)` respects relative...
  - [\#20807](https://github.com/numpy/numpy/pull/20807): BUG: Allow integer inputs for pow-related functions in `array_api`
  - [\#20814](https://github.com/numpy/numpy/pull/20814): DOC: Refer to NumPy, not pandas, in main page
  - [\#20815](https://github.com/numpy/numpy/pull/20815): DOC: Update Copyright to 2022 \[License\]
  - [\#20819](https://github.com/numpy/numpy/pull/20819): BUG: Return correctly shaped inverse indices in array\_api set...

---

1.22.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.22.2 Release Notes

The NumPy 1.22.2 is maintenance release that fixes bugs discovered after the 1.22.1 release. Notable fixes are:

  - Several build related fixes for downstream projects and other platforms.
  - Various Annotation fixes/additions.
  - Numpy wheels for Windows will use the 1.41 tool chain, fixing downstream link problems for projects using NumPy provided libraries on Windows.
  - Deal with CVE-2021-41495 complaint.

The Python versions supported for this release are 3.8-3.10.

## Contributors

A total of 14 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Andrew J. Hesford +
  - Bas van Beek
  - BrÃ©nainn Woodsend +
  - Charles Harris
  - Hood Chatham
  - Janus Heide +
  - Leo Singer
  - Matti Picus
  - Mukulika Pahari
  - Niyas Sait
  - Pearu Peterson
  - Ralf Gommers
  - Sebastian Berg
  - Serge Guelton

## Pull requests merged

A total of 21 pull requests were merged for this release.

  - [\#20842](https://github.com/numpy/numpy/pull/20842): BLD: Add NPY\_DISABLE\_SVML env var to opt out of SVML
  - [\#20843](https://github.com/numpy/numpy/pull/20843): BUG: Fix build of third party extensions with Py\_LIMITED\_API
  - [\#20844](https://github.com/numpy/numpy/pull/20844): TYP: Fix pyright being unable to infer the `real` and `imag`...
  - [\#20845](https://github.com/numpy/numpy/pull/20845): BUG: Fix comparator function signatures
  - [\#20906](https://github.com/numpy/numpy/pull/20906): BUG: Avoid importing `numpy.distutils` on import numpy.testing
  - [\#20907](https://github.com/numpy/numpy/pull/20907): MAINT: remove outdated mingw32 fseek support
  - [\#20908](https://github.com/numpy/numpy/pull/20908): TYP: Relax the return type of `np.vectorize`
  - [\#20909](https://github.com/numpy/numpy/pull/20909): BUG: fix f2py's define for threading when building with Mingw
  - [\#20910](https://github.com/numpy/numpy/pull/20910): BUG: distutils: fix building mixed C/Fortran extensions
  - [\#20912](https://github.com/numpy/numpy/pull/20912): DOC,TST: Fix Pandas code example as per new release
  - [\#20935](https://github.com/numpy/numpy/pull/20935): TYP, MAINT: Add annotations for `flatiter.__setitem__`
  - [\#20936](https://github.com/numpy/numpy/pull/20936): MAINT, TYP: Added missing where typehints in `fromnumeric.pyi`
  - [\#20937](https://github.com/numpy/numpy/pull/20937): BUG: Fix build\_ext interaction with non numpy extensions
  - [\#20938](https://github.com/numpy/numpy/pull/20938): BUG: Fix missing intrinsics for windows/arm64 target
  - [\#20945](https://github.com/numpy/numpy/pull/20945): REL: Prepare for the NumPy 1.22.2 release.
  - [\#20982](https://github.com/numpy/numpy/pull/20982): MAINT: f2py: don't generate code that triggers `-Wsometimes-uninitialized`.
  - [\#20983](https://github.com/numpy/numpy/pull/20983): BUG: Fix incorrect return type in reduce without initial value
  - [\#20984](https://github.com/numpy/numpy/pull/20984): ENH: review return values for PyArray\_DescrNew
  - [\#20985](https://github.com/numpy/numpy/pull/20985): MAINT: be more tolerant of setuptools \>= 60
  - [\#20986](https://github.com/numpy/numpy/pull/20986): BUG: Fix misplaced return.
  - [\#20992](https://github.com/numpy/numpy/pull/20992): MAINT: Further small return value validation fixes

---

1.22.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.22.3 Release Notes

NumPy 1.22.3 is a maintenance release that fixes bugs discovered after the 1.22.2 release. The most noticeable fixes may be those for DLPack. One that may cause some problems is disallowing strings as inputs to logical ufuncs. It is still undecided how strings should be treated in those functions and it was thought best to simply disallow them until a decision was reached. That should not cause problems with older code.

The Python versions supported for this release are 3.8-3.10. Note that the Mac wheels are now based on OS X 10.14 rather than 10.9 that was used in previous NumPy release cycles. 10.14 is the oldest release supported by Apple.

## Contributors

A total of 9 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - @GalaxySnail +
  - Alexandre de Siqueira
  - Bas van Beek
  - Charles Harris
  - Melissa Weber MendonÃ§a
  - Ross Barnowski
  - Sebastian Berg
  - Tirth Patel
  - Matthieu Darbois

## Pull requests merged

A total of 10 pull requests were merged for this release.

  - [\#21048](https://github.com/numpy/numpy/pull/21048): MAINT: Use "3.10" instead of "3.10-dev" on travis.
  - [\#21106](https://github.com/numpy/numpy/pull/21106): TYP,MAINT: Explicitly allow sequences of array-likes in `np.concatenate`
  - [\#21137](https://github.com/numpy/numpy/pull/21137): BLD,DOC: skip broken ipython 8.1.0
  - [\#21138](https://github.com/numpy/numpy/pull/21138): BUG, ENH: np.\_from\_dlpack: export correct device information
  - [\#21139](https://github.com/numpy/numpy/pull/21139): BUG: Fix numba DUFuncs added loops getting picked up
  - [\#21140](https://github.com/numpy/numpy/pull/21140): BUG: Fix unpickling an empty ndarray with a non-zero dimension...
  - [\#21141](https://github.com/numpy/numpy/pull/21141): BUG: use ThreadPoolExecutor instead of ThreadPool
  - [\#21142](https://github.com/numpy/numpy/pull/21142): API: Disallow strings in logical ufuncs
  - [\#21143](https://github.com/numpy/numpy/pull/21143): MAINT, DOC: Fix SciPy intersphinx link
  - [\#21148](https://github.com/numpy/numpy/pull/21148): BUG,ENH: np.\_from\_dlpack: export arrays with any strided size-1...

---

1.22.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.22.4 Release Notes

NumPy 1.22.4 is a maintenance release that fixes bugs discovered after the 1.22.3 release. In addition, the wheels for this release are built using the recently released Cython 0.29.30, which should fix the reported problems with [debugging](https://github.com/numpy/numpy/issues/21008).

The Python versions supported for this release are 3.8-3.10. Note that the Mac wheels are based on OS X 10.15 rather than 10.9 that was used in previous NumPy release cycles.

## Contributors

A total of 12 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Alexander Shadchin
  - Bas van Beek
  - Charles Harris
  - Hood Chatham
  - Jarrod Millman
  - John-Mark Gurney +
  - Junyan Ou +
  - Mariusz Felisiak +
  - Ross Barnowski
  - Sebastian Berg
  - Serge Guelton
  - Stefan van der Walt

## Pull requests merged

A total of 22 pull requests were merged for this release.

  - [\#21191](https://github.com/numpy/numpy/pull/21191): TYP, BUG: Fix `np.lib.stride_tricks` re-exported under the...
  - [\#21192](https://github.com/numpy/numpy/pull/21192): TST: Bump mypy from 0.931 to 0.940
  - [\#21243](https://github.com/numpy/numpy/pull/21243): MAINT: Explicitly re-export the types in `numpy._typing`
  - [\#21245](https://github.com/numpy/numpy/pull/21245): MAINT: Specify sphinx, numpydoc versions for CI doc builds
  - [\#21275](https://github.com/numpy/numpy/pull/21275): BUG: Fix typos
  - [\#21277](https://github.com/numpy/numpy/pull/21277): ENH, BLD: Fix math feature detection for wasm
  - [\#21350](https://github.com/numpy/numpy/pull/21350): MAINT: Fix failing simd and cygwin tests.
  - [\#21438](https://github.com/numpy/numpy/pull/21438): MAINT: Fix failing Python 3.8 32-bit Windows test.
  - [\#21444](https://github.com/numpy/numpy/pull/21444): BUG: add linux guard per \#21386
  - [\#21445](https://github.com/numpy/numpy/pull/21445): BUG: Allow legacy dtypes to cast to datetime again
  - [\#21446](https://github.com/numpy/numpy/pull/21446): BUG: Make mmap handling safer in frombuffer
  - [\#21447](https://github.com/numpy/numpy/pull/21447): BUG: Stop using PyBytesObject.ob\_shash deprecated in Python 3.11.
  - [\#21448](https://github.com/numpy/numpy/pull/21448): ENH: Introduce numpy.core.setup\_common.NPY\_CXX\_FLAGS
  - [\#21472](https://github.com/numpy/numpy/pull/21472): BUG: Ensure compile errors are raised correctly
  - [\#21473](https://github.com/numpy/numpy/pull/21473): BUG: Fix segmentation fault
  - [\#21474](https://github.com/numpy/numpy/pull/21474): MAINT: Update doc requirements
  - [\#21475](https://github.com/numpy/numpy/pull/21475): MAINT: Mark `npy_memchr` with `no_sanitize("alignment")` on clang
  - [\#21512](https://github.com/numpy/numpy/pull/21512): DOC: Proposal - make the doc landing page cards more similar...
  - [\#21525](https://github.com/numpy/numpy/pull/21525): MAINT: Update Cython version to 0.29.30.
  - [\#21536](https://github.com/numpy/numpy/pull/21536): BUG: Fix GCC error during build configuration
  - [\#21541](https://github.com/numpy/numpy/pull/21541): REL: Prepare for the NumPy 1.22.4 release.
  - [\#21547](https://github.com/numpy/numpy/pull/21547): MAINT: Skip tests that fail on PyPy.

---

1.23.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.23.0 Release Notes

The NumPy 1.23.0 release continues the ongoing work to improve the handling and promotion of dtypes, increase the execution speed, clarify the documentation, and expire old deprecations. The highlights are:

  - Implementation of `loadtxt` in C, greatly improving its performance.
  - Exposing DLPack at the Python level for easy data exchange.
  - Changes to the promotion and comparisons of structured dtypes.
  - Improvements to f2py.

See below for the details,

## New functions

  - A masked array specialization of `ndenumerate` is now available as `numpy.ma.ndenumerate`. It provides an alternative to `numpy.ndenumerate` and skips masked values by default.
    
    ([gh-20020](https://github.com/numpy/numpy/pull/20020))

  - `numpy.from_dlpack` has been added to allow easy exchange of data using the DLPack protocol. It accepts Python objects that implement the `__dlpack__` and `__dlpack_device__` methods and returns a ndarray object which is generally the view of the data of the input object.
    
    ([gh-21145](https://github.com/numpy/numpy/pull/21145))

## Deprecations

  - Setting `__array_finalize__` to `None` is deprecated. It must now be a method and may wish to call `super().__array_finalize__(obj)` after checking for `None` or if the NumPy version is sufficiently new.
    
    ([gh-20766](https://github.com/numpy/numpy/pull/20766))

  - Using `axis=32` (`axis=np.MAXDIMS`) in many cases had the same meaning as `axis=None`. This is deprecated and `axis=None` must be used instead.
    
    ([gh-20920](https://github.com/numpy/numpy/pull/20920))

  - The hook function `PyDataMem_SetEventHook` has been deprecated and the demonstration of its use in tool/allocation\_tracking has been removed. The ability to track allocations is now built-in to python via `tracemalloc`.
    
    ([gh-20394](https://github.com/numpy/numpy/pull/20394))

  - `numpy.distutils` has been deprecated, as a result of `distutils` itself being deprecated. It will not be present in NumPy for Python \>= 3.12, and will be removed completely 2 years after the release of Python 3.12 For more details, see \[distutils-status-migration\](\#distutils-status-migration).
    
    ([gh-20875](https://github.com/numpy/numpy/pull/20875))

  - `numpy.loadtxt` will now give a `DeprecationWarning` when an integer `dtype` is requested but the value is formatted as a floating point number.
    
    ([gh-21663](https://github.com/numpy/numpy/pull/21663))

## Expired deprecations

  - The `NpzFile.iteritems()` and `NpzFile.iterkeys()` methods have been removed as part of the continued removal of Python 2 compatibility. This concludes the deprecation from 1.15.
    
    ([gh-16830](https://github.com/numpy/numpy/pull/16830))

  - The `alen` and `asscalar` functions have been removed.
    
    ([gh-20414](https://github.com/numpy/numpy/pull/20414))

  - The `UPDATEIFCOPY` array flag has been removed together with the enum `NPY_ARRAY_UPDATEIFCOPY`. The associated (and deprecated) `PyArray_XDECREF_ERR` was also removed. These were all deprecated in 1.14. They are replaced by `NPY_ARRAY_WRITEBACKIFCOPY`, that requires calling `PyArray_ResolveWritebackIfCopy` before the array is deallocated.
    
    ([gh-20589](https://github.com/numpy/numpy/pull/20589))

  - Exceptions will be raised during array-like creation. When an object raised an exception during access of the special attributes `__array__` or `__array_interface__`, this exception was usually ignored. This behaviour was deprecated in 1.21, and the exception will now be raised.
    
    ([gh-20835](https://github.com/numpy/numpy/pull/20835))

  - Multidimensional indexing with non-tuple values is not allowed. Previously, code such as `arr[ind]` where `ind = [[0, 1], [0, 1]]` produced a `FutureWarning` and was interpreted as a multidimensional index (i.e., `arr[tuple(ind)]`). Now this example is treated like an array index over a single dimension (`arr[array(ind)]`). Multidimensional indexing with anything but a tuple was deprecated in NumPy 1.15.
    
    ([gh-21029](https://github.com/numpy/numpy/pull/21029))

  - Changing to a dtype of different size in F-contiguous arrays is no longer permitted. Deprecated since Numpy 1.11.0. See below for an extended explanation of the effects of this change.
    
    ([gh-20722](https://github.com/numpy/numpy/pull/20722))

## New Features

### crackfortran has support for operator and assignment overloading

`crackfortran` parser now understands operator and assignment definitions in a module. They are added in the `body` list of the module which contains a new key `implementedby` listing the names of the subroutines or functions implementing the operator or assignment.

([gh-15006](https://github.com/numpy/numpy/pull/15006))

### f2py supports reading access type attributes from derived type statements

As a result, one does not need to use `public` or `private` statements to specify derived type access properties.

([gh-15844](https://github.com/numpy/numpy/pull/15844))

### New parameter `ndmin` added to `genfromtxt`

This parameter behaves the same as `ndmin` from `numpy.loadtxt`.

([gh-20500](https://github.com/numpy/numpy/pull/20500))

### `np.loadtxt` now supports quote character and single converter function

`numpy.loadtxt` now supports an additional `quotechar` keyword argument which is not set by default. Using `quotechar='"'` will read quoted fields as used by the Excel CSV dialect.

Further, it is now possible to pass a single callable rather than a dictionary for the `converters` argument.

([gh-20580](https://github.com/numpy/numpy/pull/20580))

### Changing to dtype of a different size now requires contiguity of only the last axis

Previously, viewing an array with a dtype of a different item size required that the entire array be C-contiguous. This limitation would unnecessarily force the user to make contiguous copies of non-contiguous arrays before being able to change the dtype.

This change affects not only `ndarray.view`, but other construction mechanisms, including the discouraged direct assignment to `ndarray.dtype`.

This change expires the deprecation regarding the viewing of F-contiguous arrays, described elsewhere in the release notes.

([gh-20722](https://github.com/numpy/numpy/pull/20722))

### Deterministic output files for F2PY

For F77 inputs, `f2py` will generate `modname-f2pywrappers.f` unconditionally, though these may be empty. For free-form inputs, `modname-f2pywrappers.f`, `modname-f2pywrappers2.f90` will both be generated unconditionally, and may be empty. This allows writing generic output rules in `cmake` or `meson` and other build systems. Older behavior can be restored by passing `--skip-empty-wrappers` to `f2py`. \[f2py-meson\](\#f2py-meson) details usage.

([gh-21187](https://github.com/numpy/numpy/pull/21187))

### `keepdims` parameter for `average`

The parameter `keepdims` was added to the functions `numpy.average` and `numpy.ma.average`. The parameter has the same meaning as it does in reduction functions such as `numpy.sum` or `numpy.mean`.

([gh-21485](https://github.com/numpy/numpy/pull/21485))

### New parameter `equal_nan` added to `np.unique`

`np.unique` was changed in 1.21 to treat all `NaN` values as equal and return a single `NaN`. Setting `equal_nan=False` will restore pre-1.21 behavior to treat `NaNs` as unique. Defaults to `True`.

([gh-21623](https://github.com/numpy/numpy/pull/21623))

## Compatibility notes

### 1D `np.linalg.norm` preserves float input types, even for scalar results

Previously, this would promote to `float64` when the `ord` argument was not one of the explicitly listed values, e.g. `ord=3`:

    >>> f32 = np.float32([1, 2])
    >>> np.linalg.norm(f32, 2).dtype
    dtype('float32')
    >>> np.linalg.norm(f32, 3)
    dtype('float64')  # numpy 1.22
    dtype('float32')  # numpy 1.23

This change affects only `float32` and `float16` vectors with `ord` other than `-Inf`, `0`, `1`, `2`, and `Inf`.

([gh-17709](https://github.com/numpy/numpy/pull/17709))

### Changes to structured (void) dtype promotion and comparisons

In general, NumPy now defines correct, but slightly limited, promotion for structured dtypes by promoting the subtypes of each field instead of raising an exception:

    >>> np.result_type(np.dtype("i,i"), np.dtype("i,d"))
    dtype([('f0', '<i4'), ('f1', '<f8')])

For promotion matching field names, order, and titles are enforced, however padding is ignored. Promotion involving structured dtypes now always ensures native byte-order for all fields (which may change the result of `np.concatenate`) and ensures that the result will be "packed", i.e. all fields are ordered contiguously and padding is removed. See \[structured\_dtype\_comparison\_and\_promotion\](\#structured\_dtype\_comparison\_and\_promotion) for further details.

The `repr` of aligned structures will now never print the long form including `offsets` and `itemsize` unless the structure includes padding not guaranteed by `align=True`.

In alignment with the above changes to the promotion logic, the casting safety has been updated:

  - `"equiv"` enforces matching names and titles. The itemsize is allowed to differ due to padding.
  - `"safe"` allows mismatching field names and titles
  - The cast safety is limited by the cast safety of each included field.
  - The order of fields is used to decide cast safety of each individual field. Previously, the field names were used and only unsafe casts were possible when names mismatched.

The main important change here is that name mismatches are now considered "safe" casts.

([gh-19226](https://github.com/numpy/numpy/pull/19226))

### `NPY_RELAXED_STRIDES_CHECKING` has been removed

NumPy cannot be compiled with `NPY_RELAXED_STRIDES_CHECKING=0` anymore. Relaxed strides have been the default for many years and the option was initially introduced to allow a smoother transition.

([gh-20220](https://github.com/numpy/numpy/pull/20220))

### `np.loadtxt` has received several changes

The row counting of `numpy.loadtxt` was fixed. `loadtxt` ignores fully empty lines in the file, but counted them towards `max_rows`. When `max_rows` is used and the file contains empty lines, these will now not be counted. Previously, it was possible that the result contained fewer than `max_rows` rows even though more data was available to be read. If the old behaviour is required, `itertools.islice` may be used:

    import itertools
    lines = itertools.islice(open("file"), 0, max_rows)
    result = np.loadtxt(lines, ...)

While generally much faster and improved, `numpy.loadtxt` may now fail to converter certain strings to numbers that were previously successfully read. The most important cases for this are:

  - Parsing floating point values such as `1.0` into integers is now deprecated.
  - Parsing hexadecimal floats such as `0x3p3` will fail
  - An `_` was previously accepted as a thousands delimiter `100_000`. This will now result in an error.

If you experience these limitations, they can all be worked around by passing appropriate `converters=`. NumPy now supports passing a single converter to be used for all columns to make this more convenient. For example, `converters=float.fromhex` can read hexadecimal float numbers and `converters=int` will be able to read `100_000`.

Further, the error messages have been generally improved. However, this means that error types may differ. In particularly, a `ValueError` is now always raised when parsing of a single entry fails.

([gh-20580](https://github.com/numpy/numpy/pull/20580))

## Improvements

### `ndarray.__array_finalize__` is now callable

This means subclasses can now use `super().__array_finalize__(obj)` without worrying whether `ndarray` is their superclass or not. The actual call remains a no-op.

([gh-20766](https://github.com/numpy/numpy/pull/20766))

### Add support for VSX4/Power10

With VSX4/Power10 enablement, the new instructions available in Power ISA 3.1 can be used to accelerate some NumPy operations, e.g., floor\_divide, modulo, etc.

([gh-20821](https://github.com/numpy/numpy/pull/20821))

### `np.fromiter` now accepts objects and subarrays

The `numpy.fromiter` function now supports object and subarray dtypes. Please see he function documentation for examples.

([gh-20993](https://github.com/numpy/numpy/pull/20993))

### Math C library feature detection now uses correct signatures

Compiling is preceded by a detection phase to determine whether the underlying libc supports certain math operations. Previously this code did not respect the proper signatures. Fixing this enables compilation for the `wasm-ld` backend (compilation for web assembly) and reduces the number of warnings.

([gh-21154](https://github.com/numpy/numpy/pull/21154))

### `np.kron` now maintains subclass information

`np.kron` maintains subclass information now such as masked arrays while computing the Kronecker product of the inputs

`` `python     >>> x = ma.array([[1, 2], [3, 4]], mask=[[0, 1], [1, 0]])     >>> np.kron(x,x)     masked_array(       data=[[1, --, --, --],             [--, 4, --, --],             [--, --, 4, --],             [--, --, --, 16]],       mask=[[False,  True,  True,  True],             [ True, False,  True,  True],             [ True,  True, False,  True],             [ True,  True,  True, False]],       fill_value=999999)  .. warning:: ``np.kron`output now follows`ufunc`ordering (`multiply``)     to determine the output class type      .. code-block:: python          >>> class myarr(np.ndarray):         >>>    __array_priority__ = -1         >>> a = np.ones([2, 2])         >>> ma = myarray(a.shape, a.dtype, a.data)         >>> type(np.kron(a, ma)) == np.ndarray         False # Before it was True         >>> type(np.kron(a, ma)) == myarr         True  (`gh-21262 <https://github.com/numpy/numpy/pull/21262>`__)   Performance improvements and changes``\` ====================================

### Faster `np.loadtxt`

`numpy.loadtxt` is now generally much faster than previously as most of it is now implemented in C.

([gh-20580](https://github.com/numpy/numpy/pull/20580))

### Faster reduction operators

Reduction operations like `numpy.sum`, `numpy.prod`, `numpy.add.reduce`, `numpy.logical_and.reduce` on contiguous integer-based arrays are now much faster.

([gh-21001](https://github.com/numpy/numpy/pull/21001))

### Faster `np.where`

`numpy.where` is now much faster than previously on unpredictable/random input data.

([gh-21130](https://github.com/numpy/numpy/pull/21130))

### Faster operations on NumPy scalars

Many operations on NumPy scalars are now significantly faster, although rare operations (e.g. with 0-D arrays rather than scalars) may be slower in some cases. However, even with these improvements users who want the best performance for their scalars, may want to convert a known NumPy scalar into a Python one using `scalar.item()`.

([gh-21188](https://github.com/numpy/numpy/pull/21188))

### Faster `np.kron`

`numpy.kron` is about 80% faster as the product is now computed using broadcasting.

([gh-21354](https://github.com/numpy/numpy/pull/21354))

---

1.23.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.23.1 Release Notes

NumPy 1.23.1 is a maintenance release that fixes bugs discovered after the 1.23.0 release. Notable fixes are:

  - Fix searchsorted for float16 NaNs
  - Fix compilation on Apple M1
  - Fix KeyError in crackfortran operator support (Slycot)

The Python version supported for this release are 3.8-3.10.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Matthias Koeppe +
  - Pranab Das +
  - Rohit Goswami
  - Sebastian Berg
  - Serge Guelton
  - Srimukh Sripada +

## Pull requests merged

A total of 8 pull requests were merged for this release.

  - [\#21866](https://github.com/numpy/numpy/pull/21866): BUG: Fix discovered MachAr (still used within valgrind)
  - [\#21867](https://github.com/numpy/numpy/pull/21867): BUG: Handle NaNs correctly for float16 during sorting
  - [\#21868](https://github.com/numpy/numpy/pull/21868): BUG: Use `keepdims` during normalization in `np.average` and...
  - [\#21869](https://github.com/numpy/numpy/pull/21869): DOC: mention changes to `max_rows` behaviour in `np.loadtxt`
  - [\#21870](https://github.com/numpy/numpy/pull/21870): BUG: Reject non integer array-likes with size 1 in delete
  - [\#21949](https://github.com/numpy/numpy/pull/21949): BLD: Make can\_link\_svml return False for 32bit builds on x86\_64
  - [\#21951](https://github.com/numpy/numpy/pull/21951): BUG: Reorder extern "C" to only apply to function declarations...
  - [\#21952](https://github.com/numpy/numpy/pull/21952): BUG: Fix KeyError in crackfortran operator support

---

1.23.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.23.2 Release Notes

NumPy 1.23.2 is a maintenance release that fixes bugs discovered after the 1.23.1 release. Notable features are:

  - Typing changes needed for Python 3.11
  - Wheels for Python 3.11.0rc1

The Python versions supported for this release are 3.8-3.11.

## Contributors

A total of 9 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Alexander Grund +
  - Bas van Beek
  - Charles Harris
  - Jon Cusick +
  - Matti Picus
  - Michael Osthege +
  - Pal Barta +
  - Ross Barnowski
  - Sebastian Berg

## Pull requests merged

A total of 15 pull requests were merged for this release.

  - [\#22030](https://github.com/numpy/numpy/pull/22030): ENH: Add `__array_ufunc__` typing support to the `nin=1` ufuncs
  - [\#22031](https://github.com/numpy/numpy/pull/22031): MAINT, TYP: Fix `np.angle` dtype-overloads
  - [\#22032](https://github.com/numpy/numpy/pull/22032): MAINT: Do not let `_GenericAlias` wrap the underlying classes'...
  - [\#22033](https://github.com/numpy/numpy/pull/22033): TYP,MAINT: Allow `einsum` subscripts to be passed via integer...
  - [\#22034](https://github.com/numpy/numpy/pull/22034): MAINT,TYP: Add object-overloads for the `np.generic` rich comparisons
  - [\#22035](https://github.com/numpy/numpy/pull/22035): MAINT,TYP: Allow the `squeeze` and `transpose` method to...
  - [\#22036](https://github.com/numpy/numpy/pull/22036): BUG: Fix subarray to object cast ownership details
  - [\#22037](https://github.com/numpy/numpy/pull/22037): BUG: Use `Popen` to silently invoke f77 -v
  - [\#22038](https://github.com/numpy/numpy/pull/22038): BUG: Avoid errors on NULL during deepcopy
  - [\#22039](https://github.com/numpy/numpy/pull/22039): DOC: Add versionchanged for converter callable behavior.
  - [\#22057](https://github.com/numpy/numpy/pull/22057): MAINT: Quiet the anaconda uploads.
  - [\#22078](https://github.com/numpy/numpy/pull/22078): ENH: reorder includes for testing on top of system installations...
  - [\#22106](https://github.com/numpy/numpy/pull/22106): TST: fix test\_linear\_interpolation\_formula\_symmetric
  - [\#22107](https://github.com/numpy/numpy/pull/22107): BUG: Fix skip condition for test\_loss\_of\_precision\[complex256\]
  - [\#22115](https://github.com/numpy/numpy/pull/22115): BLD: Build python3.11.0rc1 wheels.

---

1.23.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.23.3 Release Notes

NumPy 1.23.3 is a maintenance release that fixes bugs discovered after the 1.23.2 release. There is no major theme for this release, the main improvements are for some downstream builds and some annotation corner cases. The Python versions supported for this release are 3.8-3.11.

Note that we will move to MacOS 11 for the NumPy 1.23.4 release, the 10.15 version currently used will no longer be supported by our build infrastructure at that point.

## Contributors

A total of 16 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Aaron Meurer
  - Bas van Beek
  - Charles Harris
  - Ganesh Kathiresan
  - Gavin Zhang +
  - Iantra Solari+
  - Jyn Spring ç´æ˜¥ +
  - Matti Picus
  - Rafael Cardoso Fernandes Sousa
  - Rafael Sousa +
  - Ralf Gommers
  - Rin Cat (éˆ´çŒ«) +
  - Saransh Chopra +
  - Sayed Adel
  - Sebastian Berg
  - Serge Guelton

## Pull requests merged

A total of 14 pull requests were merged for this release.

  - [\#22136](https://github.com/numpy/numpy/pull/22136): BLD: Add Python 3.11 wheels to aarch64 build
  - [\#22148](https://github.com/numpy/numpy/pull/22148): MAINT: Update setup.py for Python 3.11.
  - [\#22155](https://github.com/numpy/numpy/pull/22155): CI: Test NumPy build against old versions of GCC(6, 7, 8)
  - [\#22156](https://github.com/numpy/numpy/pull/22156): MAINT: support IBM i system
  - [\#22195](https://github.com/numpy/numpy/pull/22195): BUG: Fix circleci build
  - [\#22214](https://github.com/numpy/numpy/pull/22214): BUG: Expose heapsort algorithms in a shared header
  - [\#22215](https://github.com/numpy/numpy/pull/22215): BUG: Support using libunwind for backtrack
  - [\#22216](https://github.com/numpy/numpy/pull/22216): MAINT: fix an incorrect pointer type usage in f2py
  - [\#22220](https://github.com/numpy/numpy/pull/22220): BUG: change overloads to play nice with pyright.
  - [\#22221](https://github.com/numpy/numpy/pull/22221): TST,BUG: Use fork context to fix MacOS savez test
  - [\#22222](https://github.com/numpy/numpy/pull/22222): TYP,BUG: Reduce argument validation in C-based `__class_getitem__`
  - [\#22223](https://github.com/numpy/numpy/pull/22223): TST: ensure `np.equal.reduce` raises a `TypeError`
  - [\#22224](https://github.com/numpy/numpy/pull/22224): BUG: Fix the implementation of numpy.array\_api.vecdot
  - [\#22230](https://github.com/numpy/numpy/pull/22230): BUG: Better report integer division overflow (backport)

---

1.23.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.23.4 Release Notes

NumPy 1.23.4 is a maintenance release that fixes bugs discovered after the 1.23.3 release and keeps the build infrastructure current. The main improvements are fixes for some annotation corner cases, a fix for a long time `nested_iters` memory leak, and a fix of complex vector dot for very large arrays. The Python versions supported for this release are 3.8-3.11.

Note that the mypy version needs to be 0.981+ if you test using Python 3.10.7, otherwise the typing tests will fail.

## Contributors

A total of 8 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Matthew Barber
  - Matti Picus
  - Ralf Gommers
  - Ross Barnowski
  - Sebastian Berg
  - Sicheng Zeng +

## Pull requests merged

A total of 13 pull requests were merged for this release.

  - [\#22368](https://github.com/numpy/numpy/pull/22368): BUG: Add `__array_api_version__` to `numpy.array_api` namespace
  - [\#22370](https://github.com/numpy/numpy/pull/22370): MAINT: update sde toolkit to 9.0, fix download link
  - [\#22382](https://github.com/numpy/numpy/pull/22382): BLD: use macos-11 image on azure, macos-1015 is deprecated
  - [\#22383](https://github.com/numpy/numpy/pull/22383): MAINT: random: remove `get_info` from "extending with Cython"...
  - [\#22384](https://github.com/numpy/numpy/pull/22384): BUG: Fix complex vector dot with more than NPY\_CBLAS\_CHUNK elements
  - [\#22387](https://github.com/numpy/numpy/pull/22387): REV: Loosen `lookfor`'s import try/except again
  - [\#22388](https://github.com/numpy/numpy/pull/22388): TYP,ENH: Mark `numpy.typing` protocols as runtime checkable
  - [\#22389](https://github.com/numpy/numpy/pull/22389): TYP,MAINT: Change more overloads to play nice with pyright
  - [\#22390](https://github.com/numpy/numpy/pull/22390): TST,TYP: Bump mypy to 0.981
  - [\#22391](https://github.com/numpy/numpy/pull/22391): DOC: Update delimiter param description.
  - [\#22392](https://github.com/numpy/numpy/pull/22392): BUG: Memory leaks in numpy.nested\_iters
  - [\#22413](https://github.com/numpy/numpy/pull/22413): REL: Prepare for the NumPy 1.23.4 release.
  - [\#22424](https://github.com/numpy/numpy/pull/22424): TST: Fix failing aarch64 wheel builds.

---

1.23.5-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.23.5 Release Notes

NumPy 1.23.5 is a maintenance release that fixes bugs discovered after the 1.23.4 release and keeps the build infrastructure current. The Python versions supported for this release are 3.8-3.11.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - @DWesl
  - Aayush Agrawal +
  - Adam Knapp +
  - Charles Harris
  - Navpreet Singh +
  - Sebastian Berg
  - Tania Allard

## Pull requests merged

A total of 10 pull requests were merged for this release.

  - [\#22489](https://github.com/numpy/numpy/pull/22489): TST, MAINT: Replace most setup with setup\_method (also teardown)
  - [\#22490](https://github.com/numpy/numpy/pull/22490): MAINT, CI: Switch to <cygwin/cygwin-install-action@v2>
  - [\#22494](https://github.com/numpy/numpy/pull/22494): TST: Make test\_partial\_iteration\_cleanup robust but require leak...
  - [\#22592](https://github.com/numpy/numpy/pull/22592): MAINT: Ensure graceful handling of large header sizes
  - [\#22593](https://github.com/numpy/numpy/pull/22593): TYP: Spelling alignment for array flag literal
  - [\#22594](https://github.com/numpy/numpy/pull/22594): BUG: Fix bounds checking for `random.logseries`
  - [\#22595](https://github.com/numpy/numpy/pull/22595): DEV: Update GH actions and Dockerfile for Gitpod
  - [\#22596](https://github.com/numpy/numpy/pull/22596): CI: Only fetch in actions/checkout
  - [\#22597](https://github.com/numpy/numpy/pull/22597): BUG: Decrement ref count in gentype\_reduce if allocated memory...
  - [\#22625](https://github.com/numpy/numpy/pull/22625): BUG: Histogramdd breaks on big arrays in Windows

---

1.24.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.24 Release Notes

The NumPy 1.24.0 release continues the ongoing work to improve the handling and promotion of dtypes, increase the execution speed, and clarify the documentation. There are also a large number of new and expired deprecations due to changes in promotion and cleanups. This might be called a deprecation release. Highlights are

  - Many new deprecations, check them out.
  - Many expired deprecations,
  - New F2PY features and fixes.
  - New "dtype" and "casting" keywords for stacking functions.

See below for the details,

This release supports Python versions 3.8-3.11.

## Deprecations

### Deprecate fastCopyAndTranspose and PyArray\_CopyAndTranspose

The `numpy.fastCopyAndTranspose` function has been deprecated. Use the corresponding copy and transpose methods directly:

    arr.T.copy()

The underlying C function `PyArray_CopyAndTranspose` has also been deprecated from the NumPy C-API.

([gh-22313](https://github.com/numpy/numpy/pull/22313))

### Conversion of out-of-bound Python integers

Attempting a conversion from a Python integer to a NumPy value will now always check whether the result can be represented by NumPy. This means the following examples will fail in the future and give a `DeprecationWarning` now:

    np.uint8(-1)
    np.array([3000], dtype=np.int8)

Many of these did succeed before. Such code was mainly useful for unsigned integers with negative values such as `np.uint8(-1)` giving `np.iinfo(np.uint8).max`.

Note that conversion between NumPy integers is unaffected, so that `np.array(-1).astype(np.uint8)` continues to work and use C integer overflow logic. For negative values, it will also work to view the array: `np.array(-1, dtype=np.int8).view(np.uint8)`. In some cases, using `np.iinfo(np.uint8).max` or `val % 2**8` may also work well.

In rare cases input data may mix both negative values and very large unsigned values (i.e. `-1` and `2**63`). There it is unfortunately necessary to use `%` on the Python value or use signed or unsigned conversion depending on whether negative values are expected.

([gh-22385](https://github.com/numpy/numpy/pull/22385))

### Deprecate `msort`

The `numpy.msort` function is deprecated. Use `np.sort(a, axis=0)` instead.

([gh-22456](https://github.com/numpy/numpy/pull/22456))

### `np.str0` and similar are now deprecated

The scalar type aliases ending in a 0 bit size: `np.object0`, `np.str0`, `np.bytes0`, `np.void0`, `np.int0`, `np.uint0` as well as `np.bool8` are now deprecated and will eventually be removed.

([gh-22607](https://github.com/numpy/numpy/pull/22607))

## Expired deprecations

  - The `normed` keyword argument has been removed from <span class="title-ref">np.histogram</span>, <span class="title-ref">np.histogram2d</span>, and <span class="title-ref">np.histogramdd</span>. Use `density` instead. If `normed` was passed by position, `density` is now used.
    
    ([gh-21645](https://github.com/numpy/numpy/pull/21645))

  - Ragged array creation will now always raise a `ValueError` unless `dtype=object` is passed. This includes very deeply nested sequences.
    
    ([gh-22004](https://github.com/numpy/numpy/pull/22004))

  - Support for Visual Studio 2015 and earlier has been removed.

  - Support for the Windows Interix POSIX interop layer has been removed.
    
    ([gh-22139](https://github.com/numpy/numpy/pull/22139))

  - Support for Cygwin \< 3.3 has been removed.
    
    ([gh-22159](https://github.com/numpy/numpy/pull/22159))

  - The mini() method of `np.ma.MaskedArray` has been removed. Use either `np.ma.MaskedArray.min()` or `np.ma.minimum.reduce()`.

  - The single-argument form of `np.ma.minimum` and `np.ma.maximum` has been removed. Use `np.ma.minimum.reduce()` or `np.ma.maximum.reduce()` instead.
    
    ([gh-22228](https://github.com/numpy/numpy/pull/22228))

  - Passing dtype instances other than the canonical (mainly native byte-order) ones to `dtype=` or `signature=` in ufuncs will now raise a `TypeError`. We recommend passing the strings `"int8"` or scalar types `np.int8` since the byte-order, datetime/timedelta unit, etc. are never enforced. (Initially deprecated in NumPy 1.21.)
    
    ([gh-22540](https://github.com/numpy/numpy/pull/22540))

  - The `dtype=` argument to comparison ufuncs is now applied correctly. That means that only `bool` and `object` are valid values and `dtype=object` is enforced.
    
    ([gh-22541](https://github.com/numpy/numpy/pull/22541))

  - The deprecation for the aliases `np.object`, `np.bool`, `np.float`, `np.complex`, `np.str`, and `np.int` is expired (introduces NumPy 1.20). Some of these will now give a FutureWarning in addition to raising an error since they will be mapped to the NumPy scalars in the future.
    
    ([gh-22607](https://github.com/numpy/numpy/pull/22607))

## Compatibility notes

### `array.fill(scalar)` may behave slightly different

`numpy.ndarray.fill` may in some cases behave slightly different now due to the fact that the logic is aligned with item assignment:

    arr = np.array([1])  # with any dtype/value
    arr.fill(scalar)
    # is now identical to:
    arr[0] = scalar

Previously casting may have produced slightly different answers when using values that could not be represented in the target `dtype` or when the target had `object` dtype.

([gh-20924](https://github.com/numpy/numpy/pull/20924))

### Subarray to object cast now copies

Casting a dtype that includes a subarray to an object will now ensure a copy of the subarray. Previously an unsafe view was returned:

    arr = np.ones(3, dtype=[("f", "i", 3)])
    subarray_fields = arr.astype(object)[0]
    subarray = subarray_fields[0]  # "f" field
    
    np.may_share_memory(subarray, arr)

Is now always false. While previously it was true for the specific cast.

([gh-21925](https://github.com/numpy/numpy/pull/21925))

### Returned arrays respect uniqueness of dtype kwarg objects

When the `dtype` keyword argument is used with :py\`array()<span class="title-ref"> or :py\`asarray()</span>, the dtype of the returned array now always exactly matches the dtype provided by the caller.

In some cases this change means that a *view* rather than the input array is returned. The following is an example for this on 64bit Linux where `long` and `longlong` are the same precision but different `dtypes`:

    >>> arr = np.array([1, 2, 3], dtype="long")
    >>> new_dtype = np.dtype("longlong")
    >>> new = np.asarray(arr, dtype=new_dtype)
    >>> new.dtype is new_dtype
    True
    >>> new is arr
    False

Before the change, the `dtype` did not match because `new is arr` was `True`.

([gh-21995](https://github.com/numpy/numpy/pull/21995))

### DLPack export raises `BufferError`

When an array buffer cannot be exported via DLPack a `BufferError` is now always raised where previously `TypeError` or `RuntimeError` was raised. This allows falling back to the buffer protocol or `__array_interface__` when DLPack was tried first.

([gh-22542](https://github.com/numpy/numpy/pull/22542))

### NumPy builds are no longer tested on GCC-6

Ubuntu 18.04 is deprecated for GitHub actions and GCC-6 is not available on Ubuntu 20.04, so builds using that compiler are no longer tested. We still test builds using GCC-7 and GCC-8.

([gh-22598](https://github.com/numpy/numpy/pull/22598))

## New Features

### New attribute `symbol` added to polynomial classes

The polynomial classes in the `numpy.polynomial` package have a new `symbol` attribute which is used to represent the indeterminate of the polynomial. This can be used to change the value of the variable when printing:

    >>> P_y = np.polynomial.Polynomial([1, 0, -1], symbol="y")
    >>> print(P_y)
    1.0 + 0.0Â·yÂ¹ - 1.0Â·yÂ²

Note that the polynomial classes only support 1D polynomials, so operations that involve polynomials with different symbols are disallowed when the result would be multivariate:

    >>> P = np.polynomial.Polynomial([1, -1])  # default symbol is "x"
    >>> P_z = np.polynomial.Polynomial([1, 1], symbol="z")
    >>> P * P_z
    Traceback (most recent call last)
       ...
    ValueError: Polynomial symbols differ

The symbol can be any valid Python identifier. The default is `symbol=x`, consistent with existing behavior.

([gh-16154](https://github.com/numpy/numpy/pull/16154))

### F2PY support for Fortran `character` strings

F2PY now supports wrapping Fortran functions with:

  - character (e.g. `character x`)
  - character array (e.g. `character, dimension(n) :: x`)
  - character string (e.g. `character(len=10) x`)
  - and character string array (e.g. `character(len=10), dimension(n, m) :: x`)

arguments, including passing Python unicode strings as Fortran character string arguments.

([gh-19388](https://github.com/numpy/numpy/pull/19388))

### New function `np.show_runtime`

A new function `numpy.show_runtime` has been added to display the runtime information of the machine in addition to `numpy.show_config` which displays the build-related information.

([gh-21468](https://github.com/numpy/numpy/pull/21468))

### `strict` option for `testing.assert_array_equal`

The `strict` option is now available for `testing.assert_array_equal`. Setting `strict=True` will disable the broadcasting behaviour for scalars and ensure that input arrays have the same data type.

([gh-21595](https://github.com/numpy/numpy/pull/21595))

### New parameter `equal_nan` added to `np.unique`

`np.unique` was changed in 1.21 to treat all `NaN` values as equal and return a single `NaN`. Setting `equal_nan=False` will restore pre-1.21 behavior to treat `NaNs` as unique. Defaults to `True`.

([gh-21623](https://github.com/numpy/numpy/pull/21623))

### `casting` and `dtype` keyword arguments for `numpy.stack`

The `casting` and `dtype` keyword arguments are now available for `numpy.stack`. To use them, write `np.stack(..., dtype=None, casting='same_kind')`.

### `casting` and `dtype` keyword arguments for `numpy.vstack`

The `casting` and `dtype` keyword arguments are now available for `numpy.vstack`. To use them, write `np.vstack(..., dtype=None, casting='same_kind')`.

### `casting` and `dtype` keyword arguments for `numpy.hstack`

The `casting` and `dtype` keyword arguments are now available for `numpy.hstack`. To use them, write `np.hstack(..., dtype=None, casting='same_kind')`.

([gh-21627](https://github.com/numpy/numpy/pull/21627))

### The bit generator underlying the singleton RandomState can be changed

The singleton `RandomState` instance exposed in the `numpy.random` module is initialized at startup with the `MT19937` bit generator. The new function `set_bit_generator` allows the default bit generator to be replaced with a user-provided bit generator. This function has been introduced to provide a method allowing seamless integration of a high-quality, modern bit generator in new code with existing code that makes use of the singleton-provided random variate generating functions. The companion function `get_bit_generator` returns the current bit generator being used by the singleton `RandomState`. This is provided to simplify restoring the original source of randomness if required.

The preferred method to generate reproducible random numbers is to use a modern bit generator in an instance of `Generator`. The function `default_rng` simplifies instantiation:

    >>> rg = np.random.default_rng(3728973198)
    >>> rg.random()

The same bit generator can then be shared with the singleton instance so that calling functions in the `random` module will use the same bit generator:

    >>> orig_bit_gen = np.random.get_bit_generator()
    >>> np.random.set_bit_generator(rg.bit_generator)
    >>> np.random.normal()

The swap is permanent (until reversed) and so any call to functions in the `random` module will use the new bit generator. The original can be restored if required for code to run correctly:

    >>> np.random.set_bit_generator(orig_bit_gen)

([gh-21976](https://github.com/numpy/numpy/pull/21976))

### `np.void` now has a `dtype` argument

NumPy now allows constructing structured void scalars directly by passing the `dtype` argument to `np.void`.

([gh-22316](https://github.com/numpy/numpy/pull/22316))

## Improvements

### F2PY Improvements

  - The generated extension modules don't use the deprecated NumPy-C API anymore
  - Improved `f2py` generated exception messages
  - Numerous bug and `flake8` warning fixes
  - various CPP macros that one can use within C-expressions of signature files are prefixed with `f2py_`. For example, one should use `f2py_len(x)` instead of `len(x)`
  - A new construct `character(f2py_len=...)` is introduced to support returning assumed length character strings (e.g. `character(len=*)`) from wrapper functions

A hook to support rewriting `f2py` internal data structures after reading all its input files is introduced. This is required, for instance, for BC of SciPy support where character arguments are treated as character strings arguments in `C` expressions.

([gh-19388](https://github.com/numpy/numpy/pull/19388))

### IBM zSystems Vector Extension Facility (SIMD)

Added support for SIMD extensions of zSystem (z13, z14, z15), through the universal intrinsics interface. This support leads to performance improvements for all SIMD kernels implemented using the universal intrinsics, including the following operations: rint, floor, trunc, ceil, sqrt, absolute, square, reciprocal, tanh, sin, cos, equal, not\_equal, greater, greater\_equal, less, less\_equal, maximum, minimum, fmax, fmin, argmax, argmin, add, subtract, multiply, divide.

([gh-20913](https://github.com/numpy/numpy/pull/20913))

### NumPy now gives floating point errors in casts

In most cases, NumPy previously did not give floating point warnings or errors when these happened during casts. For examples, casts like:

    np.array([2e300]).astype(np.float32)  # overflow for float32
    np.array([np.inf]).astype(np.int64)

Should now generally give floating point warnings. These warnings should warn that floating point overflow occurred. For errors when converting floating point values to integers users should expect invalid value warnings.

Users can modify the behavior of these warnings using `np.errstate`.

Note that for float to int casts, the exact warnings that are given may be platform dependent. For example:

    arr = np.full(100, fill_value=1000, dtype=np.float64)
    arr.astype(np.int8)

May give a result equivalent to (the intermediate cast means no warning is given):

    arr.astype(np.int64).astype(np.int8)

May return an undefined result, with a warning set:

    RuntimeWarning: invalid value encountered in cast

The precise behavior is subject to the C99 standard and its implementation in both software and hardware.

([gh-21437](https://github.com/numpy/numpy/pull/21437))

### F2PY supports the value attribute

The Fortran standard requires that variables declared with the `value` attribute must be passed by value instead of reference. F2PY now supports this use pattern correctly. So `integer, intent(in), value :: x` in Fortran codes will have correct wrappers generated.

([gh-21807](https://github.com/numpy/numpy/pull/21807))

### Added pickle support for third-party BitGenerators

The pickle format for bit generators was extended to allow each bit generator to supply its own constructor when during pickling. Previous versions of NumPy only supported unpickling `Generator` instances created with one of the core set of bit generators supplied with NumPy. Attempting to unpickle a `Generator` that used a third-party bit generators would fail since the constructor used during the unpickling was only aware of the bit generators included in NumPy.

([gh-22014](https://github.com/numpy/numpy/pull/22014))

### arange() now explicitly fails with dtype=str

Previously, the `np.arange(n, dtype=str)` function worked for `n=1` and `n=2`, but would raise a non-specific exception message for other values of `n`. Now, it raises a <span class="title-ref">TypeError</span> informing that `arange` does not support string dtypes:

    >>> np.arange(2, dtype=str)
    Traceback (most recent call last)
       ...
    TypeError: arange() not supported for inputs with DType <class 'numpy.dtype[str_]'>.

([gh-22055](https://github.com/numpy/numpy/pull/22055))

### `numpy.typing` protocols are now runtime checkable

The protocols used in `numpy.typing.ArrayLike` and `numpy.typing.DTypeLike` are now properly marked as runtime checkable, making them easier to use for runtime type checkers.

([gh-22357](https://github.com/numpy/numpy/pull/22357))

## Performance improvements and changes

### Faster version of `np.isin` and `np.in1d` for integer arrays

`np.in1d` (used by `np.isin`) can now switch to a faster algorithm (up to \>10x faster) when it is passed two integer arrays. This is often automatically used, but you can use `kind="sort"` or `kind="table"` to force the old or new method, respectively.

([gh-12065](https://github.com/numpy/numpy/pull/12065))

### Faster comparison operators

The comparison functions (`numpy.equal`, `numpy.not_equal`, `numpy.less`, `numpy.less_equal`, `numpy.greater` and `numpy.greater_equal`) are now much faster as they are now vectorized with universal intrinsics. For a CPU with SIMD extension AVX512BW, the performance gain is up to 2.57x, 1.65x and 19.15x for integer, float and boolean data types, respectively (with N=50000).

([gh-21483](https://github.com/numpy/numpy/pull/21483))

## Changes

### Better reporting of integer division overflow

Integer division overflow of scalars and arrays used to provide a `RuntimeWarning` and the return value was undefined leading to crashes at rare occasions:

    >>> np.array([np.iinfo(np.int32).min]*10, dtype=np.int32) // np.int32(-1)
    <stdin>:1: RuntimeWarning: divide by zero encountered in floor_divide
    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)

Integer division overflow now returns the input dtype's minimum value and raise the following `RuntimeWarning`:

    >>> np.array([np.iinfo(np.int32).min]*10, dtype=np.int32) // np.int32(-1)
    <stdin>:1: RuntimeWarning: overflow encountered in floor_divide
    array([-2147483648, -2147483648, -2147483648, -2147483648, -2147483648,
           -2147483648, -2147483648, -2147483648, -2147483648, -2147483648],
          dtype=int32)

([gh-21506](https://github.com/numpy/numpy/pull/21506))

### `masked_invalid` now modifies the mask in-place

When used with `copy=False`, `numpy.ma.masked_invalid` now modifies the input masked array in-place. This makes it behave identically to `masked_where` and better matches the documentation.

([gh-22046](https://github.com/numpy/numpy/pull/22046))

### `nditer`/`NpyIter` allows all allocating all operands

The NumPy iterator available through `np.nditer` in Python and as `NpyIter` in C now supports allocating all arrays. The iterator shape defaults to `()` in this case. The operands dtype must be provided, since a "common dtype" cannot be inferred from the other inputs.

([gh-22457](https://github.com/numpy/numpy/pull/22457))

---

1.24.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.24.1 Release Notes

NumPy 1.24.1 is a maintenance release that fixes bugs and regressions discovered after the 1.24.0 release. The Python versions supported by this release are 3.8-3.11.

## Contributors

A total of 12 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Andrew Nelson
  - Ben Greiner +
  - Charles Harris
  - ClÃ©ment Robert
  - Matteo Raso
  - Matti Picus
  - Melissa Weber MendonÃ§a
  - Miles Cranmer
  - Ralf Gommers
  - Rohit Goswami
  - Sayed Adel
  - Sebastian Berg

## Pull requests merged

A total of 18 pull requests were merged for this release.

  - [\#22820](https://github.com/numpy/numpy/pull/22820): BLD: add workaround in setup.py for newer setuptools
  - [\#22830](https://github.com/numpy/numpy/pull/22830): BLD: CIRRUS\_TAG redux
  - [\#22831](https://github.com/numpy/numpy/pull/22831): DOC: fix a couple typos in 1.23 notes
  - [\#22832](https://github.com/numpy/numpy/pull/22832): BUG: Fix refcounting errors found using pytest-leaks
  - [\#22834](https://github.com/numpy/numpy/pull/22834): BUG, SIMD: Fix invalid value encountered in several ufuncs
  - [\#22837](https://github.com/numpy/numpy/pull/22837): TST: ignore more np.distutils.log imports
  - [\#22839](https://github.com/numpy/numpy/pull/22839): BUG: Do not use getdata() in np.ma.masked\_invalid
  - [\#22847](https://github.com/numpy/numpy/pull/22847): BUG: Ensure correct behavior for rows ending in delimiter in...
  - [\#22848](https://github.com/numpy/numpy/pull/22848): BUG, SIMD: Fix the bitmask of the boolean comparison
  - [\#22857](https://github.com/numpy/numpy/pull/22857): BLD: Help raspian arm + clang 13 about \_\_builtin\_mul\_overflow
  - [\#22858](https://github.com/numpy/numpy/pull/22858): API: Ensure a full mask is returned for masked\_invalid
  - [\#22866](https://github.com/numpy/numpy/pull/22866): BUG: Polynomials now copy properly (\#22669)
  - [\#22867](https://github.com/numpy/numpy/pull/22867): BUG, SIMD: Fix memory overlap in ufunc comparison loops
  - [\#22868](https://github.com/numpy/numpy/pull/22868): BUG: Fortify string casts against floating point warnings
  - [\#22875](https://github.com/numpy/numpy/pull/22875): TST: Ignore nan-warnings in randomized out tests
  - [\#22883](https://github.com/numpy/numpy/pull/22883): MAINT: restore npymath implementations needed for freebsd
  - [\#22884](https://github.com/numpy/numpy/pull/22884): BUG: Fix integer overflow in in1d for mixed integer dtypes \#22877
  - [\#22887](https://github.com/numpy/numpy/pull/22887): BUG: Use whole file for encoding checks with `charset_normalizer`.

---

1.24.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.24.2 Release Notes

NumPy 1.24.2 is a maintenance release that fixes bugs and regressions discovered after the 1.24.1 release. The Python versions supported by this release are 3.8-3.11.

## Contributors

A total of 14 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Khem Raj +
  - Mark Harfouche
  - Matti Picus
  - Panagiotis Zestanakis +
  - Peter Hawkins
  - Pradipta Ghosh
  - Ross Barnowski
  - Sayed Adel
  - Sebastian Berg
  - Syam Gadde +
  - dmbelov +
  - pkubaj +

## Pull requests merged

A total of 17 pull requests were merged for this release.

  - [\#22965](https://github.com/numpy/numpy/pull/22965): MAINT: Update python 3.11-dev to 3.11.
  - [\#22966](https://github.com/numpy/numpy/pull/22966): DOC: Remove dangling deprecation warning
  - [\#22967](https://github.com/numpy/numpy/pull/22967): ENH: Detect CPU features on FreeBSD/powerpc64\*
  - [\#22968](https://github.com/numpy/numpy/pull/22968): BUG: np.loadtxt cannot load text file with quoted fields separated...
  - [\#22969](https://github.com/numpy/numpy/pull/22969): TST: Add fixture to avoid issue with randomizing test order.
  - [\#22970](https://github.com/numpy/numpy/pull/22970): BUG: Fix fill violating read-only flag. (\#22959)
  - [\#22971](https://github.com/numpy/numpy/pull/22971): MAINT: Add additional information to missing scalar AttributeError
  - [\#22972](https://github.com/numpy/numpy/pull/22972): MAINT: Move export for scipy arm64 helper into main module
  - [\#22976](https://github.com/numpy/numpy/pull/22976): BUG, SIMD: Fix spurious invalid exception for sin/cos on arm64/clang
  - [\#22989](https://github.com/numpy/numpy/pull/22989): BUG: Ensure correct loop order in sin, cos, and arctan2
  - [\#23030](https://github.com/numpy/numpy/pull/23030): DOC: Add version added information for the strict parameter in...
  - [\#23031](https://github.com/numpy/numpy/pull/23031): BUG: use `_Alignof` rather than `offsetof()` on most compilers
  - [\#23147](https://github.com/numpy/numpy/pull/23147): BUG: Fix for npyv\_\_trunc\_s32\_f32 (VXE)
  - [\#23148](https://github.com/numpy/numpy/pull/23148): BUG: Fix integer / float scalar promotion
  - [\#23149](https://github.com/numpy/numpy/pull/23149): BUG: Add missing \<type\_traits\> header.
  - [\#23150](https://github.com/numpy/numpy/pull/23150): TYP, MAINT: Add a missing explicit `Any` parameter to the `npt.ArrayLike`...
  - [\#23161](https://github.com/numpy/numpy/pull/23161): BLD: remove redundant definition of npy\_nextafter \[wheel build\]

---

1.24.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.24.3 Release Notes

NumPy 1.24.3 is a maintenance release that fixes bugs and regressions discovered after the 1.24.2 release. The Python versions supported by this release are 3.8-3.11.

## Contributors

A total of 12 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Aleksei Nikiforov +
  - Alexander Heger
  - Bas van Beek
  - Bob Eldering
  - Brock Mendel
  - Charles Harris
  - Kyle Sunden
  - Peter Hawkins
  - Rohit Goswami
  - Sebastian Berg
  - Warren Weckesser
  - dependabot\[bot\]

## Pull requests merged

A total of 17 pull requests were merged for this release.

  - [\#23206](https://github.com/numpy/numpy/pull/23206): BUG: fix for f2py string scalars (\#23194)
  - [\#23207](https://github.com/numpy/numpy/pull/23207): BUG: datetime64/timedelta64 comparisons return NotImplemented
  - [\#23208](https://github.com/numpy/numpy/pull/23208): MAINT: Pin matplotlib to version 3.6.3 for refguide checks
  - [\#23221](https://github.com/numpy/numpy/pull/23221): DOC: Fix matplotlib error in documentation
  - [\#23226](https://github.com/numpy/numpy/pull/23226): CI: Ensure submodules are initialized in gitpod.
  - [\#23341](https://github.com/numpy/numpy/pull/23341): TYP: Replace duplicate reduce in ufunc type signature with reduceat.
  - [\#23342](https://github.com/numpy/numpy/pull/23342): TYP: Remove duplicate CLIP/WRAP/RAISE in `__init__.pyi`.
  - [\#23343](https://github.com/numpy/numpy/pull/23343): TYP: Mark `d` argument to fftfreq and rfftfreq as optional...
  - [\#23344](https://github.com/numpy/numpy/pull/23344): TYP: Add type annotations for comparison operators to MaskedArray.
  - [\#23345](https://github.com/numpy/numpy/pull/23345): TYP: Remove some stray type-check-only imports of `msort`
  - [\#23370](https://github.com/numpy/numpy/pull/23370): BUG: Ensure like is only stripped for `like=` dispatched functions
  - [\#23543](https://github.com/numpy/numpy/pull/23543): BUG: fix loading and storing big arrays on s390x
  - [\#23544](https://github.com/numpy/numpy/pull/23544): MAINT: Bump larsoner/circleci-artifacts-redirector-action
  - [\#23634](https://github.com/numpy/numpy/pull/23634): BUG: Ignore invalid and overflow warnings in masked setitem
  - [\#23635](https://github.com/numpy/numpy/pull/23635): BUG: Fix masked array raveling when `order="A"` or `order="K"`
  - [\#23636](https://github.com/numpy/numpy/pull/23636): MAINT: Update conftest for newer hypothesis versions
  - [\#23637](https://github.com/numpy/numpy/pull/23637): BUG: Fix bug in parsing F77 style string arrays.

---

1.24.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.24.4 Release Notes

NumPy 1.24.4 is a maintenance release that fixes bugs and regressions discovered after the 1.24.3 release. The Python versions supported by this release are 3.8-3.11.

## Contributors

A total of 4 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bas van Beek
  - Charles Harris
  - Sebastian Berg
  - Hongyang Peng +

## Pull requests merged

A total of 6 pull requests were merged for this release.

  - [\#23720](https://github.com/numpy/numpy/pull/23720): MAINT, BLD: Pin rtools to version 4.0 for Windows builds.
  - [\#23739](https://github.com/numpy/numpy/pull/23739): BUG: fix the method for checking local files for 1.24.x
  - [\#23760](https://github.com/numpy/numpy/pull/23760): MAINT: Copy rtools installation from install-rtools.
  - [\#23761](https://github.com/numpy/numpy/pull/23761): BUG: Fix masked array ravel order for A (and somewhat K)
  - [\#23890](https://github.com/numpy/numpy/pull/23890): TYP,DOC: Annotate and document the `metadata` parameter of...
  - [\#23994](https://github.com/numpy/numpy/pull/23994): MAINT: Update rtools installation

---

1.25.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.25.0 Release Notes

The NumPy 1.25.0 release continues the ongoing work to improve the handling and promotion of dtypes, increase the execution speed, and clarify the documentation. There has also been work to prepare for the future NumPy 2.0.0 release, resulting in a large number of new and expired deprecation. Highlights are:

  - Support for MUSL, there are now MUSL wheels.
  - Support the Fujitsu C/C++ compiler.
  - Object arrays are now supported in einsum
  - Support for inplace matrix multiplication (`@=`).

We will be releasing a NumPy 1.26 when Python 3.12 comes out. That is needed because distutils has been dropped by Python 3.12 and we will be switching to using meson for future builds. The next mainline release will be NumPy 2.0.0. We plan that the 2.0 series will still support downstream projects built against earlier versions of NumPy.

The Python versions supported in this release are 3.9-3.11.

## Deprecations

  - `np.core.MachAr` is deprecated. It is private API. In names defined in `np.core` should generally be considered private.
    
    ([gh-22638](https://github.com/numpy/numpy/pull/22638))

  - `np.finfo(None)` is deprecated.
    
    ([gh-23011](https://github.com/numpy/numpy/pull/23011))

  - `np.round_` is deprecated. Use <span class="title-ref">np.round</span> instead.
    
    ([gh-23302](https://github.com/numpy/numpy/pull/23302))

  - `np.product` is deprecated. Use <span class="title-ref">np.prod</span> instead.
    
    ([gh-23314](https://github.com/numpy/numpy/pull/23314))

  - `np.cumproduct` is deprecated. Use <span class="title-ref">np.cumprod</span> instead.
    
    ([gh-23314](https://github.com/numpy/numpy/pull/23314))

  - `np.sometrue` is deprecated. Use <span class="title-ref">np.any</span> instead.
    
    ([gh-23314](https://github.com/numpy/numpy/pull/23314))

  - `np.alltrue` is deprecated. Use <span class="title-ref">np.all</span> instead.
    
    ([gh-23314](https://github.com/numpy/numpy/pull/23314))

  - Only ndim-0 arrays are treated as scalars. NumPy used to treat all arrays of size 1 (e.g., `np.array([3.14])`) as scalars. In the future, this will be limited to arrays of ndim 0 (e.g., `np.array(3.14)`). The following expressions will report a deprecation warning:
    
      - \`\`\`python  
        a = np.array(\[3.14\]) float(a) \# better: a\[0\] to get the numpy.float or a.item()
        
        b = np.array(\[\[3.14\]\]) c = numpy.random.rand(10) c\[0\] = b \# better: c\[0\] = b\[0, 0\]
    
    ([gh-10615](https://github.com/numpy/numpy/pull/10615))

  - `np.find_common_type` is deprecated. <span class="title-ref">numpy.find\_common\_type</span> is now deprecated and its use should be replaced with either <span class="title-ref">numpy.result\_type</span> or <span class="title-ref">numpy.promote\_types</span>. Most users leave the second `scalar_types` argument to `find_common_type` as `[]` in which case `np.result_type` and `np.promote_types` are both faster and more robust. When not using `scalar_types` the main difference is that the replacement intentionally converts non-native byte-order to native byte order. Further, `find_common_type` returns `object` dtype rather than failing promotion. This leads to differences when the inputs are not all numeric. Importantly, this also happens for e.g. timedelta/datetime for which NumPy promotion rules are currently sometimes surprising.
    
    When the `scalar_types` argument is not `[]` things are more complicated. In most cases, using `np.result_type` and passing the Python values `0`, `0.0`, or `0j` has the same result as using `int`, `float`, or `complex` in <span class="title-ref">scalar\_types</span>.
    
    When `scalar_types` is constructed, `np.result_type` is the correct replacement and it may be passed scalar values like `np.float32(0.0)`. Passing values other than 0, may lead to value-inspecting behavior (which `np.find_common_type` never used and NEP 50 may change in the future). The main possible change in behavior in this case, is when the array types are signed integers and scalar types are unsigned.
    
    If you are unsure about how to replace a use of `scalar_types` or when non-numeric dtypes are likely, please do not hesitate to open a NumPy issue to ask for help.
    
    ([gh-22539](https://github.com/numpy/numpy/pull/22539))

Expired deprecations `` ` ====================  * ``np.core.machar`and`np.finfo.machar``have been removed.    (`gh-22638 <https://github.com/numpy/numpy/pull/22638>`__)  *``+arr``will now raise an error when the dtype is not   numeric (and positive is undefined).    (`gh-22998 <https://github.com/numpy/numpy/pull/22998>`__)  * A sequence must now be passed into the stacking family of functions   (``stack`,`vstack`,`hstack`,`dstack`and`column\_stack``).    (`gh-23019 <https://github.com/numpy/numpy/pull/23019>`__)  *``np.clip``now defaults to same-kind casting. Falling back to   unsafe casting was deprecated in NumPy 1.17.    (`gh-23403 <https://github.com/numpy/numpy/pull/23403>`__)  *``np.clip`will now propagate`np.nan`values passed as`min`or`max``.   Previously, a scalar NaN was usually ignored.  This was deprecated in NumPy 1.17.    (`gh-23403 <https://github.com/numpy/numpy/pull/23403>`__)  * The``np.dual``submodule has been removed.    (`gh-23480 <https://github.com/numpy/numpy/pull/23480>`__)  * NumPy now always ignores sequence behavior for an array-like (defining   one of the array protocols).  (Deprecation started NumPy 1.20)    (`gh-23660 <https://github.com/numpy/numpy/pull/23660>`__)  * The niche``FutureWarning`when casting to a subarray dtype in`astype`or the array creation functions such as`asarray``is now finalized.   The behavior is now always the same as if the subarray dtype was   wrapped into a single field (which was the workaround, previously).   (FutureWarning since NumPy 1.20)    (`gh-23666 <https://github.com/numpy/numpy/pull/23666>`__)  *``==`and`\!=`warnings have been finalized.  The`==`and`\!=`operators on arrays now always:    * raise errors that occur during comparisons such as when the arrays     have incompatible shapes (`np.array(\[1, 2\]) == np.array(\[1, 2, 3\])`).   * return an array of all`True`or all`False`when values are     fundamentally not comparable (e.g. have different dtypes).  An example     is`np.array(\["a"\]) == np.array(\[1\])`.      This mimics the Python behavior of returning`False`and`True`when comparing incompatible types like`"a" == 1`and`"a" \!= 1`.     For a long time these gave`DeprecationWarning`or`FutureWarning``.    (`gh-22707 <https://github.com/numpy/numpy/pull/22707>`__)  * Nose support has been removed. NumPy switched to using pytest in 2018 and nose   has been unmaintained for many years. We have kept NumPy's nose support to   avoid breaking downstream projects who might have been using it and not yet   switched to pytest or some other testing framework. With the arrival of   Python 3.12, unpatched nose will raise an error. It is time to move on.    *Decorators removed*:    - raises   - slow   - setastest   - skipif   - knownfailif   - deprecated   - parametrize   - _needs_refcount    These are not to be confused with pytest versions with similar names, e.g.,   pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.    *Functions removed*:    - Tester   - import_nose   - run_module_suite    (`gh-23041 <https://github.com/numpy/numpy/pull/23041>`__)  * The``numpy.testing.utils`shim has been removed.  Importing from the`numpy.testing.utils`shim has been deprecated since 2019, the shim has now   been removed. All imports should be made directly from`numpy.testing``.    (`gh-23060 <https://github.com/numpy/numpy/pull/23060>`__)  * The environment variable to disable dispatching has been removed.   Support for the``NUMPY\_EXPERIMENTAL\_ARRAY\_FUNCTION`environment variable has   been removed. This variable disabled dispatching with`\_\_[array\_function](#array_function__-machinery-is-now-much-faster)``.    (`gh-23376 <https://github.com/numpy/numpy/pull/23376>`__)  * Support for``y=`as an alias of`out=`has been removed.   The`fix`,`isposinf`and`isneginf`functions allowed using`y=`as a   (deprecated) alias for`out=``. This is no longer supported.    (`gh-23376 <https://github.com/numpy/numpy/pull/23376>`__)   Compatibility notes ===================  * The``busday\_count`method now correctly handles cases where the`begindates`is later in time   than the`enddates`. Previously, the`enddates``was included, even though the documentation states   it is always excluded.    (`gh-23229 <https://github.com/numpy/numpy/pull/23229>`__)  * When comparing datetimes and timedelta using``np.equal`or`np.not\_equal`numpy previously allowed the comparison with`casting="unsafe"`.   This operation now fails. Forcing the output dtype using the`dtype``kwarg can make the operation succeed, but we do not recommend it.    (`gh-22707 <https://github.com/numpy/numpy/pull/22707>`__)  * When loading data from a file handle using``np.load`,   if the handle is at the end of file, as can happen when reading   multiple arrays by calling`np.load`repeatedly, numpy previously   raised`ValueError`if`allow\_pickle=False`, and`OSError`if`allow\_pickle=True`. Now it raises`EOFError``instead, in both cases.    (`gh-23105 <https://github.com/numpy/numpy/pull/23105>`__)``np.pad`with`mode=wrap`pads with strict multiples of original data ------------------------------------------------------------------------- Code based on earlier version of`pad`that uses`mode="wrap"`will return different results when the padding size is larger than initial array.`np.pad`with`mode=wrap``now always fills the space with strict multiples of original data even if the padding size is larger than the initial array.  (`gh-22575 <https://github.com/numpy/numpy/pull/22575>`__)  Cython``long\_t`and`ulong\_t`removed -----------------------------------------`long\_t`and`ulong\_t`were aliases for`longlong\_t`and`ulonglong\_t`and confusing (a remainder from of Python 2).  This change may lead to the errors::       'long_t' is not a type identifier      'ulong_t' is not a type identifier  We recommend use of bit-sized types such as`cnp.int64\_t`or the use of`cnp.intp\_t`which is 32 bits on 32 bit systems and 64 bits on 64 bit systems (this is most compatible with indexing). If C`long`is desired, use plain`long`or`npy\_long`.`cnp.int\_t`is also`long`(NumPy's default integer).  However,`long``is 32 bit on 64 bit windows and we may wish to adjust this even in NumPy. (Please do not hesitate to contact NumPy developers if you are curious about this.)  (`gh-22637 <https://github.com/numpy/numpy/pull/22637>`__)  Changed error message and type for bad``axes`argument to`ufunc`--------------------------------------------------------------------- The error message and type when a wrong`axes`value is passed to`ufunc(..., axes=\[...\])`` ` has changed. The message is now more indicative of the problem, and if the value is mismatched an ``AxisError`will be raised. A`TypeError``will still be raised for invalid input types.  (`gh-22675 <https://github.com/numpy/numpy/pull/22675>`__)  Array-likes that define``\_\_array\_ufunc\_\_`can now override ufuncs if used as`where`---------------------------------------------------------------------------------------- If the`where``keyword argument of a `numpy.ufunc` is a subclass of `numpy.ndarray` or is a duck type that defines `numpy.class.__array_ufunc__` it can override the behavior of the ufunc using the same mechanism as the input and output arguments. Note that for this to work properly, the``where.\_\_array\_ufunc\_\_`implementation will have to unwrap the`where`argument to pass it into the default implementation of the`ufunc``or, for `numpy.ndarray` subclasses before using``super().\_\_array\_ufunc\_\_``.  (`gh-23240 <https://github.com/numpy/numpy/pull/23240>`__)  Compiling against the NumPy C API is now backwards compatible by default ------------------------------------------------------------------------ NumPy now defaults to exposing a backwards compatible subset of the C-API. This makes the use of``oldest-supported-numpy`unnecessary. Libraries can override the default minimal version to be compatible with using::      #define NPY_TARGET_VERSION NPY_1_22_API_VERSION  before including NumPy or by passing the equivalent`-D`option to the compiler. The NumPy 1.25 default is`NPY\_1\_19\_API\_VERSION``.  Because the NumPy 1.19 C API was identical to the NumPy 1.16 one resulting programs will be compatible with NumPy 1.16 (from a C-API perspective). This default will be increased in future non-bugfix releases. You can still compile against an older NumPy version and run on a newer one.  For more details please see [for-downstream-package-authors](#for-downstream-package-authors).  (`gh-23528 <https://github.com/numpy/numpy/pull/23528>`__)   New Features ============``np.einsum`now accepts arrays with`object`dtype ------------------------------------------------------ The code path will call python operators on object dtype arrays, much like`np.dot`and`np.matmul``.  (`gh-18053 <https://github.com/numpy/numpy/pull/18053>`__)  Add support for inplace matrix multiplication --------------------------------------------- It is now possible to perform inplace matrix multiplication via the``@=`operator.`\`python \>\>\> import numpy as np

> \>\>\> a = np.arange(6).reshape(3, 2) \>\>\> print(a) \[\[0 1\] \[2 3\] \[4 5\]\]
> 
> \>\>\> b = np.ones((2, 2), dtype=int) \>\>\> a @= b \>\>\> print(a) \[\[1 1\] \[5 5\] \[9 9\]\]

([gh-21120](https://github.com/numpy/numpy/pull/21120))

Added `NPY_ENABLE_CPU_FEATURES` environment variable `` ` ------------------------------------------------------ Users may now choose to enable only a subset of the built CPU features at runtime by specifying the `NPY_ENABLE_CPU_FEATURES` environment variable. Note that these specified features must be outside the baseline, since those are always assumed. Errors will be raised if attempting to enable a feature that is either not supported by your CPU, or that NumPy was not built with.  (`gh-22137 <https://github.com/numpy/numpy/pull/22137>`__)  NumPy now has an ``np.exceptions``namespace -------------------------------------------- NumPy now has a dedicated namespace making most exceptions and warnings available.  All of these remain available in the main namespace, although some may be moved slowly in the future. The main reason for this is to increase discoverability and add future exceptions.  (`gh-22644 <https://github.com/numpy/numpy/pull/22644>`__)``np.linalg`functions return NamedTuples ------------------------------------------`np.linalg`functions that return tuples now return namedtuples. These functions are`eig()`,`eigh()`,`qr()`,`slogdet()`, and`svd()`. The return type is unchanged in instances where these functions return non-tuples with certain keyword arguments (like`svd(compute\_uv=False)``).  (`gh-22786 <https://github.com/numpy/numpy/pull/22786>`__)  String functions in``np.char`are compatible with NEP 42 custom dtypes ------------------------------------------------------------------------ Custom dtypes that represent unicode strings or byte strings can now be passed to the string functions in`np.char``.  (`gh-22863 <https://github.com/numpy/numpy/pull/22863>`__)  String dtype instances can be created from the string abstract dtype classes ---------------------------------------------------------------------------- It is now possible to create a string dtype instance with a size without using the string name of the dtype. For example,``type(np.dtype('U'))(8)`will create a dtype that is equivalent to`np.dtype('U8')``. This feature is most useful when writing generic code dealing with string dtype classes.  (`gh-22963 <https://github.com/numpy/numpy/pull/22963>`__)  Fujitsu C/C++ compiler is now supported --------------------------------------- Support for Fujitsu compiler has been added. To build with Fujitsu compiler, run:      python setup.py build -c fujitsu   SSL2 is now supported --------------------- Support for SSL2 has been added. SSL2 is a library that provides OpenBLAS compatible GEMM functions.  To enable SSL2, it need to edit site.cfg and build with Fujitsu compiler.  See site.cfg.example.  (`gh-22982 <https://github.com/numpy/numpy/pull/22982>`__)   Improvements ============``NDArrayOperatorsMixin`specifies that it has no`\_\_slots\_\_`---------------------------------------------------------------- The`NDArrayOperatorsMixin`class now specifies that it contains no`\_\_slots\_\_``, ensuring that subclasses can now make use of this feature in Python.  (`gh-23113 <https://github.com/numpy/numpy/pull/23113>`__)  Fix power of complex zero -------------------------``np.power`now returns a different result for`0^{non-zero}`for complex numbers.  Note that the value is only defined when the real part of the exponent is larger than zero. Previously, NaN was returned unless the imaginary part was strictly zero.  The return value is either`0+0j`or`0-0j``.  (`gh-18535 <https://github.com/numpy/numpy/pull/18535>`__)  New``DTypePromotionError`--------------------------- NumPy now has a new`DTypePromotionError``which is used when two dtypes cannot be promoted to a common one, for example::      np.result_type("M8[s]", np.complex128)  raises this new exception.  (`gh-22707 <https://github.com/numpy/numpy/pull/22707>`__)  `np.show_config` uses information from Meson -------------------------------------------- Build and system information now contains information from Meson. `np.show_config` now has a new optional parameter``mode``to help customize the output.  (`gh-22769 <https://github.com/numpy/numpy/pull/22769>`__)  Fix``np.ma.diff`not preserving the mask when called with arguments prepend/append. ------------------------------------------------------------------------------------- Calling`np.ma.diff`with arguments prepend and/or append now returns a`MaskedArray`with the input mask preserved.  Previously, a`MaskedArray``without the mask was returned.  (`gh-22776 <https://github.com/numpy/numpy/pull/22776>`__)  Corrected error handling for NumPy C-API in Cython -------------------------------------------------- Many NumPy C functions defined for use in Cython were lacking the correct error indicator like``except -1`or`except \*``. These have now been added.  (`gh-22997 <https://github.com/numpy/numpy/pull/22997>`__)  Ability to directly spawn random number generators -------------------------------------------------- `numpy.random.Generator.spawn` now allows to directly spawn new independent child generators via the `numpy.random.SeedSequence.spawn` mechanism. `numpy.random.BitGenerator.spawn` does the same for the underlying bit generator.  Additionally, `numpy.random.BitGenerator.seed_seq` now gives direct access to the seed sequence used for initializing the bit generator. This allows for example::      seed = 0x2e09b90939db40c400f8f22dae617151     rng = np.random.default_rng(seed)     child_rng1, child_rng2 = rng.spawn(2)      # safely use rng, child_rng1, and child_rng2  Previously, this was hard to do without passing the``SeedSequence``explicitly.  Please see `numpy.random.SeedSequence` for more information.  (`gh-23195 <https://github.com/numpy/numpy/pull/23195>`__)``numpy.logspace`now supports a non-scalar`base`argument -------------------------------------------------------------- The`base`argument of`numpy.logspace`can now be array-like if it is broadcastable against the`start`and`stop``arguments.  (`gh-23275 <https://github.com/numpy/numpy/pull/23275>`__)``np.ma.dot()`now supports for non-2d arrays ---------------------------------------------- Previously`np.ma.dot()`only worked if`a`and`b`were both 2d. Now it works for non-2d arrays as well as`np.dot()``.  (`gh-23322 <https://github.com/numpy/numpy/pull/23322>`__)  Explicitly show keys of .npz file in repr -----------------------------------------``NpzFile`shows keys of loaded .npz file when printed.`\`python \>\>\> npzfile = np.load('arr.npz') \>\>\> npzfile NpzFile 'arr.npz' with keys arr\_0, arr\_1, arr\_2, arr\_3, arr\_4...

([gh-23357](https://github.com/numpy/numpy/pull/23357))

NumPy now exposes DType classes in `np.dtypes` `` ` ------------------------------------------------ The new ``numpy.dtypes``module now exposes DType classes and will contain future dtype related functionality. Most users should have no need to use these classes directly.  (`gh-23358 <https://github.com/numpy/numpy/pull/23358>`__)  Drop dtype metadata before saving in .npy or .npz files ------------------------------------------------------- Currently, a``\*.npy``file containing a table with a dtype with metadata cannot be read back. Now, `np.save` and `np.savez` drop metadata before saving.  (`gh-23371 <https://github.com/numpy/numpy/pull/23371>`__)``numpy.lib.recfunctions.structured\_to\_unstructured`returns views in more cases ---------------------------------------------------------------------------------`structured\_to\_unstructured`now returns a view, if the stride between the fields is constant. Prior, padding between the fields or a reversed field would lead to a copy. This change only applies to`ndarray`,`memmap`and`recarray``. For all other array subclasses, the behavior remains unchanged.  (`gh-23652 <https://github.com/numpy/numpy/pull/23652>`__)  Signed and unsigned integers always compare correctly ----------------------------------------------------- When``uint64`and`int64`are mixed in NumPy, NumPy typically promotes both to`float64`.  This behavior may be argued about but is confusing for comparisons`==`,`\<=``, since the results returned can be incorrect but the conversion is hidden since the result is a boolean. NumPy will now return the correct results for these by avoiding the cast to float.  (`gh-23713 <https://github.com/numpy/numpy/pull/23713>`__)   Performance improvements and changes ====================================  Faster``np.argsort``on AVX-512 enabled processors --------------------------------------------------- 32-bit and 64-bit quicksort algorithm for np.argsort gain up to 6x speed up on processors that support AVX-512 instruction set.  Thanks to `Intel corporation <https://open.intel.com/>`_ for sponsoring this work.  (`gh-23707 <https://github.com/numpy/numpy/pull/23707>`__)  Faster``np.sort``on AVX-512 enabled processors ------------------------------------------------ Quicksort for 16-bit and 64-bit dtypes gain up to 15x and 9x speed up on processors that support AVX-512 instruction set.  Thanks to `Intel corporation <https://open.intel.com/>`_ for sponsoring this work.  (`gh-22315 <https://github.com/numpy/numpy/pull/22315>`__)``\_\_array\_function\_\_``machinery is now much faster --------------------------------------------------- The overhead of the majority of functions in NumPy is now smaller especially when keyword arguments are used.  This change significantly speeds up many simple function calls.  (`gh-23020 <https://github.com/numpy/numpy/pull/23020>`__)``ufunc.at`can be much faster ------------------------------- Generic`ufunc.at`can be up to 9x faster. The conditions for this speedup:  - operands are aligned - no casting  If ufuncs with appropriate indexed loops on 1d arguments with the above conditions,`ufunc.at`can be up to 60x faster (an additional 7x speedup). Appropriate indexed loops have been added to`add`,`subtract`,`multiply`,`floor\_divide`,`maximum`,`minimum`,`fmax`, and`fmin``.  The internal logic is similar to the logic used for regular ufuncs, which also have fast paths.  Thanks to the `D. E. Shaw group <https://deshaw.com/>`_ for sponsoring this work.  (`gh-23136 <https://github.com/numpy/numpy/pull/23136>`__)  Faster membership test on``NpzFile`------------------------------------- Membership test on`NpzFile``will no longer decompress the archive if it is successful.  (`gh-23661 <https://github.com/numpy/numpy/pull/23661>`__)   Changes =======``[np.r]()\[\]`and`[np.c]()\[\]`with certain scalar values ------------------------------------------------------ In rare cases, using mainly`[np.r]()`with scalars can lead to different results.  The main potential changes are highlighted by the following::      >>> np.r_[np.arange(5, dtype=np.uint8), -1].dtype     int16  # rather than the default integer (int64 or int32)     >>> np.r_[np.arange(5, dtype=np.int8), 255]     array([  0,   1,   2,   3,   4, 255], dtype=int16)  Where the second example returned::      array([ 0,  1,  2,  3,  4, -1], dtype=int8)  The first one is due to a signed integer scalar with an unsigned integer array, while the second is due to`255`not fitting into`int8``and NumPy currently inspecting values to make this work. (Note that the second example is expected to change in the future due to [NEP 50 <NEP50>](#nep-50-<nep50>); it will then raise an error.)  (`gh-22539 <https://github.com/numpy/numpy/pull/22539>`__)  Most NumPy functions are wrapped into a C-callable -------------------------------------------------- To speed up the``\_\_array\_function\_\_``dispatching, most NumPy functions are now wrapped into C-callables and are not proper Python functions or C methods. They still look and feel the same as before (like a Python function), and this should only improve performance and user experience (cleaner tracebacks). However, please inform the NumPy developers if this change confuses your program for some reason.  (`gh-23020 <https://github.com/numpy/numpy/pull/23020>`__)  C++ standard library usage -------------------------- NumPy builds now depend on the C++ standard library, because the``numpy.core.\_multiarray\_umath\`\` extension is linked with the C++ linker.

([gh-23601](https://github.com/numpy/numpy/pull/23601))

---

1.25.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.25.1 Release Notes

NumPy 1.25.1 is a maintenance release that fixes bugs and regressions discovered after the 1.25.0 release. The Python versions supported by this release are 3.9-3.11.

## Contributors

A total of 10 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Andrew Nelson
  - Charles Harris
  - Developer-Ecosystem-Engineering
  - Hood Chatham
  - Nathan Goldbaum
  - Rohit Goswami
  - Sebastian Berg
  - Tim Paine +
  - dependabot\[bot\]
  - matoro +

## Pull requests merged

A total of 14 pull requests were merged for this release.

  - [\#23968](https://github.com/numpy/numpy/pull/23968): MAINT: prepare 1.25.x for further development
  - [\#24036](https://github.com/numpy/numpy/pull/24036): BLD: Port long double identification to C for meson
  - [\#24037](https://github.com/numpy/numpy/pull/24037): BUG: Fix reduction `return NULL` to be `goto fail`
  - [\#24038](https://github.com/numpy/numpy/pull/24038): BUG: Avoid undefined behavior in array.astype()
  - [\#24039](https://github.com/numpy/numpy/pull/24039): BUG: Ensure `__array_ufunc__` works without any kwargs passed
  - [\#24117](https://github.com/numpy/numpy/pull/24117): MAINT: Pin urllib3 to avoid anaconda-client bug.
  - [\#24118](https://github.com/numpy/numpy/pull/24118): TST: Pin pydantic\<2 in Pyodide workflow
  - [\#24119](https://github.com/numpy/numpy/pull/24119): MAINT: Bump pypa/cibuildwheel from 2.13.0 to 2.13.1
  - [\#24120](https://github.com/numpy/numpy/pull/24120): MAINT: Bump actions/checkout from 3.5.2 to 3.5.3
  - [\#24122](https://github.com/numpy/numpy/pull/24122): BUG: Multiply or Divides using SIMD without a full vector can...
  - [\#24127](https://github.com/numpy/numpy/pull/24127): MAINT: testing for IS\_MUSL closes \#24074
  - [\#24128](https://github.com/numpy/numpy/pull/24128): BUG: Only replace dtype temporarily if dimensions changed
  - [\#24129](https://github.com/numpy/numpy/pull/24129): MAINT: Bump actions/setup-node from 3.6.0 to 3.7.0
  - [\#24134](https://github.com/numpy/numpy/pull/24134): BUG: Fix private procedures in f2py modules

---

1.25.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.25.2 Release Notes

NumPy 1.25.2 is a maintenance release that fixes bugs and regressions discovered after the 1.25.1 release. This is the last planned release in the 1.25.x series, the next release will be 1.26.0, which will use the meson build system and support Python 3.12. The Python versions supported by this release are 3.9-3.11.

## Contributors

A total of 13 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Aaron Meurer
  - Andrew Nelson
  - Charles Harris
  - Kevin Sheppard
  - Matti Picus
  - Nathan Goldbaum
  - Peter Hawkins
  - Ralf Gommers
  - Randy Eckenrode +
  - Sam James +
  - Sebastian Berg
  - Tyler Reddy
  - dependabot\[bot\]

## Pull requests merged

A total of 19 pull requests were merged for this release.

  - [\#24148](https://github.com/numpy/numpy/pull/24148): MAINT: prepare 1.25.x for further development
  - [\#24174](https://github.com/numpy/numpy/pull/24174): ENH: Improve clang-cl compliance
  - [\#24179](https://github.com/numpy/numpy/pull/24179): MAINT: Upgrade various build dependencies.
  - [\#24182](https://github.com/numpy/numpy/pull/24182): BLD: use `-ftrapping-math` with Clang on macOS
  - [\#24183](https://github.com/numpy/numpy/pull/24183): BUG: properly handle negative indexes in ufunc\_at fast path
  - [\#24184](https://github.com/numpy/numpy/pull/24184): BUG: PyObject\_IsTrue and PyObject\_Not error handling in setflags
  - [\#24185](https://github.com/numpy/numpy/pull/24185): BUG: histogram small range robust
  - [\#24186](https://github.com/numpy/numpy/pull/24186): MAINT: Update meson.build files from main branch
  - [\#24234](https://github.com/numpy/numpy/pull/24234): MAINT: exclude min, max and round from `np.__all__`
  - [\#24241](https://github.com/numpy/numpy/pull/24241): MAINT: Dependabot updates
  - [\#24242](https://github.com/numpy/numpy/pull/24242): BUG: Fix the signature for np.array\_api.take
  - [\#24243](https://github.com/numpy/numpy/pull/24243): BLD: update OpenBLAS to an intermediate commit
  - [\#24244](https://github.com/numpy/numpy/pull/24244): BUG: Fix reference count leak in str(scalar).
  - [\#24245](https://github.com/numpy/numpy/pull/24245): BUG: fix invalid function pointer conversion error
  - [\#24255](https://github.com/numpy/numpy/pull/24255): BUG: Factor out slow `getenv` call used for memory policy warning
  - [\#24292](https://github.com/numpy/numpy/pull/24292): CI: correct URL in cirrus.star \[skip cirrus\]
  - [\#24293](https://github.com/numpy/numpy/pull/24293): BUG: Fix C types in scalartypes
  - [\#24294](https://github.com/numpy/numpy/pull/24294): BUG: do not modify the input to ufunc\_at
  - [\#24295](https://github.com/numpy/numpy/pull/24295): BUG: Further fixes to indexing loop and added tests

---

1.26.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.26.0 Release Notes

The NumPy 1.26.0 release is a continuation of the 1.25.x release cycle with the addition of Python 3.12.0 support. Python 3.12 dropped distutils, consequently supporting it required finding a replacement for the setup.py/distutils based build system NumPy was using. We have chosen to use the Meson build system instead, and this is the first NumPy release supporting it. This is also the first release that supports Cython 3.0 in addition to retaining 0.29.X compatibility. Supporting those two upgrades was a large project, over 100 files have been touched in this release. The changelog doesn't capture the full extent of the work, special thanks to Ralf Gommers, Sayed Adel, StÃ©fan van der Walt, and Matti Picus who did much of the work in the main development branch.

The highlights of this release are:

  - Python 3.12.0 support.
  - Cython 3.0.0 compatibility.
  - Use of the Meson build system
  - Updated SIMD support
  - f2py fixes, meson and bind(x) support
  - Support for the updated Accelerate BLAS/LAPACK library

The Python versions supported in this release are 3.9-3.12.

## New Features

### Array API v2022.12 support in `numpy.array_api`

  - `numpy.array_api` now full supports the [v2022.12 version](https://data-apis.org/array-api/2022.12) of the array API standard. Note that this does not yet include the optional `fft` extension in the standard.

([gh-23789](https://github.com/numpy/numpy/pull/23789))

### Support for the updated Accelerate BLAS/LAPACK library

Support for the updated Accelerate BLAS/LAPACK library, including ILP64 (64-bit integer) support, in macOS 13.3 has been added. This brings arm64 support, and significant performance improvements of up to 10x for commonly used linear algebra operations. When Accelerate is selected at build time, the 13.3+ version will automatically be used if available.

([gh-24053](https://github.com/numpy/numpy/pull/24053))

### `meson` backend for `f2py`

`f2py` in compile mode (i.e. `f2py -c`) now accepts the `--backend meson` option. This is the default option for Python `3.12` on-wards. Older versions will still default to `--backend distutils`.

To support this in realistic use-cases, in compile mode `f2py` takes a `--dep` flag one or many times which maps to `dependency()` calls in the `meson` backend, and does nothing in the `distutils` backend.

There are no changes for users of `f2py` only as a code generator, i.e. without `-c`.

([gh-24532](https://github.com/numpy/numpy/pull/24532))

### `bind(c)` support for `f2py`

Both functions and subroutines can be annotated with `bind(c)`. `f2py` will handle both the correct type mapping, and preserve the unique label for other `C` interfaces.

**Note:** `bind(c, name = 'routine_name_other_than_fortran_routine')` is not honored by the `f2py` bindings by design, since `bind(c)` with the `name` is meant to guarantee only the same name in `C` and `Fortran`, not in `Python` and `Fortran`.

([gh-24555](https://github.com/numpy/numpy/pull/24555))

## Improvements

### `iso_c_binding` support for `f2py`

Previously, users would have to define their own custom `f2cmap` file to use type mappings defined by the Fortran2003 `iso_c_binding` intrinsic module. These type maps are now natively supported by `f2py`

([gh-24555](https://github.com/numpy/numpy/pull/24555))

## Build system changes

In this release, NumPy has switched to Meson as the build system and meson-python as the build backend. Installing NumPy or building a wheel can be done with standard tools like `pip` and `pypa/build`. The following are supported:

  - Regular installs: `pip install numpy` or (in a cloned repo) `pip install .`
  - Building a wheel: `python -m build` (preferred), or `pip wheel .`
  - Editable installs: `pip install -e . --no-build-isolation`
  - Development builds through the custom CLI implemented with [spin](https://github.com/scientific-python/spin): `spin build`.

All the regular `pip` and `pypa/build` flags (e.g., `--no-build-isolation`) should work as expected.

### NumPy-specific build customization

Many of the NumPy-specific ways of customizing builds have changed. The `NPY_*` environment variables which control BLAS/LAPACK, SIMD, threading, and other such options are no longer supported, nor is a `site.cfg` file to select BLAS and LAPACK. Instead, there are command-line flags that can be passed to the build via `pip`/`build`'s config-settings interface. These flags are all listed in the `meson_options.txt` file in the root of the repo. Detailed documented will be available before the final 1.26.0 release; for now please see [the SciPy "building from source" docs](http://scipy.github.io/devdocs/building/index.html) since most build customization works in an almost identical way in SciPy as it does in NumPy.

### Build dependencies

While the runtime dependencies of NumPy have not changed, the build dependencies have. Because we temporarily vendor Meson and meson-python, there are several new dependencies - please see the `[build-system]` section of `pyproject.toml` for details.

### Troubleshooting

This build system change is quite large. In case of unexpected issues, it is still possible to use a `setup.py`-based build as a temporary workaround (on Python 3.9-3.11, not 3.12), by copying `pyproject.toml.setuppy` to `pyproject.toml`. However, please open an issue with details on the NumPy issue tracker. We aim to phase out `setup.py` builds as soon as possible, and therefore would like to see all potential blockers surfaced early on in the 1.26.0 release cycle.

## Contributors

A total of 20 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - @DWesl
  - Albert Steppi +
  - Bas van Beek
  - Charles Harris
  - Developer-Ecosystem-Engineering
  - Filipe LaÃ­ns +
  - Jake Vanderplas
  - Liang Yan +
  - Marten van Kerkwijk
  - Matti Picus
  - Melissa Weber MendonÃ§a
  - Namami Shanker
  - Nathan Goldbaum
  - Ralf Gommers
  - Rohit Goswami
  - Sayed Adel
  - Sebastian Berg
  - Stefan van der Walt
  - Tyler Reddy
  - Warren Weckesser

## Pull requests merged

A total of 59 pull requests were merged for this release.

  - [\#24305](https://github.com/numpy/numpy/pull/24305): MAINT: Prepare 1.26.x branch for development
  - [\#24308](https://github.com/numpy/numpy/pull/24308): MAINT: Massive update of files from main for numpy 1.26
  - [\#24322](https://github.com/numpy/numpy/pull/24322): CI: fix wheel builds on the 1.26.x branch
  - [\#24326](https://github.com/numpy/numpy/pull/24326): BLD: update openblas to newer version
  - [\#24327](https://github.com/numpy/numpy/pull/24327): TYP: Trim down the `_NestedSequence.__getitem__` signature
  - [\#24328](https://github.com/numpy/numpy/pull/24328): BUG: fix choose refcount leak
  - [\#24337](https://github.com/numpy/numpy/pull/24337): TST: fix running the test suite in builds without BLAS/LAPACK
  - [\#24338](https://github.com/numpy/numpy/pull/24338): BUG: random: Fix generation of nan by dirichlet.
  - [\#24340](https://github.com/numpy/numpy/pull/24340): MAINT: Dependabot updates from main
  - [\#24342](https://github.com/numpy/numpy/pull/24342): MAINT: Add back NPY\_RUN\_MYPY\_IN\_TESTSUITE=1
  - [\#24353](https://github.com/numpy/numpy/pull/24353): MAINT: Update `extbuild.py` from main.
  - [\#24356](https://github.com/numpy/numpy/pull/24356): TST: fix distutils tests for deprecations in recent setuptools...
  - [\#24375](https://github.com/numpy/numpy/pull/24375): MAINT: Update cibuildwheel to version 2.15.0
  - [\#24381](https://github.com/numpy/numpy/pull/24381): MAINT: Fix codespaces setup.sh script
  - [\#24403](https://github.com/numpy/numpy/pull/24403): ENH: Vendor meson for multi-target build support
  - [\#24404](https://github.com/numpy/numpy/pull/24404): BLD: vendor meson-python to make the Windows builds with SIMD...
  - [\#24405](https://github.com/numpy/numpy/pull/24405): BLD, SIMD: The meson CPU dispatcher implementation
  - [\#24406](https://github.com/numpy/numpy/pull/24406): MAINT: Remove versioneer
  - [\#24409](https://github.com/numpy/numpy/pull/24409): REL: Prepare for the NumPy 1.26.0b1 release.
  - [\#24453](https://github.com/numpy/numpy/pull/24453): MAINT: Pin upper version of sphinx.
  - [\#24455](https://github.com/numpy/numpy/pull/24455): ENH: Add prefix to \_ALIGN Macro
  - [\#24456](https://github.com/numpy/numpy/pull/24456): BUG: cleanup warnings \[skip azp\]\[skip circle\]\[skip travis\]\[skip...
  - [\#24460](https://github.com/numpy/numpy/pull/24460): MAINT: Upgrade to spin 0.5
  - [\#24495](https://github.com/numpy/numpy/pull/24495): BUG: `asv dev` has been removed, use `asv run`.
  - [\#24496](https://github.com/numpy/numpy/pull/24496): BUG: Fix meson build failure due to unchanged inplace auto-generated...
  - [\#24521](https://github.com/numpy/numpy/pull/24521): BUG: fix issue with git-version script, needs a shebang to run
  - [\#24522](https://github.com/numpy/numpy/pull/24522): BUG: Use a default assignment for git\_hash \[skip ci\]
  - [\#24524](https://github.com/numpy/numpy/pull/24524): BUG: fix NPY\_cast\_info error handling in choose
  - [\#24526](https://github.com/numpy/numpy/pull/24526): BUG: Fix common block handling in f2py
  - [\#24541](https://github.com/numpy/numpy/pull/24541): CI,TYP: Bump mypy to 1.4.1
  - [\#24542](https://github.com/numpy/numpy/pull/24542): BUG: Fix assumed length f2py regression
  - [\#24544](https://github.com/numpy/numpy/pull/24544): MAINT: Harmonize fortranobject
  - [\#24545](https://github.com/numpy/numpy/pull/24545): TYP: add kind argument to numpy.isin type specification
  - [\#24561](https://github.com/numpy/numpy/pull/24561): BUG: fix comparisons between masked and unmasked structured arrays
  - [\#24590](https://github.com/numpy/numpy/pull/24590): CI: Exclude import libraries from list of DLLs on Cygwin.
  - [\#24591](https://github.com/numpy/numpy/pull/24591): BLD: fix `_umath_linalg` dependencies
  - [\#24594](https://github.com/numpy/numpy/pull/24594): MAINT: Stop testing on ppc64le.
  - [\#24602](https://github.com/numpy/numpy/pull/24602): BLD: meson-cpu: fix SIMD support on platforms with no features
  - [\#24606](https://github.com/numpy/numpy/pull/24606): BUG: Change Cython `binding` directive to "False".
  - [\#24613](https://github.com/numpy/numpy/pull/24613): ENH: Adopt new macOS Accelerate BLAS/LAPACK Interfaces, including...
  - [\#24614](https://github.com/numpy/numpy/pull/24614): DOC: Update building docs to use Meson
  - [\#24615](https://github.com/numpy/numpy/pull/24615): TYP: Add the missing `casting` keyword to `np.clip`
  - [\#24616](https://github.com/numpy/numpy/pull/24616): TST: convert cython test from setup.py to meson
  - [\#24617](https://github.com/numpy/numpy/pull/24617): MAINT: Fixup `fromnumeric.pyi`
  - [\#24622](https://github.com/numpy/numpy/pull/24622): BUG, ENH: Fix `iso_c_binding` type maps and fix `bind(c)`...
  - [\#24629](https://github.com/numpy/numpy/pull/24629): TYP: Allow `binary_repr` to accept any object implementing...
  - [\#24630](https://github.com/numpy/numpy/pull/24630): TYP: Explicitly declare `dtype` and `generic` hashable
  - [\#24637](https://github.com/numpy/numpy/pull/24637): ENH: Refactor the typing "reveal" tests using <span class="title-ref">typing.assert\_type</span>
  - [\#24638](https://github.com/numpy/numpy/pull/24638): MAINT: Bump actions/checkout from 3.6.0 to 4.0.0
  - [\#24647](https://github.com/numpy/numpy/pull/24647): ENH: `meson` backend for `f2py`
  - [\#24648](https://github.com/numpy/numpy/pull/24648): MAINT: Refactor partial load Workaround for Clang
  - [\#24653](https://github.com/numpy/numpy/pull/24653): REL: Prepare for the NumPy 1.26.0rc1 release.
  - [\#24659](https://github.com/numpy/numpy/pull/24659): BLD: allow specifying the long double format to avoid the runtime...
  - [\#24665](https://github.com/numpy/numpy/pull/24665): BLD: fix bug in random.mtrand extension, don't link libnpyrandom
  - [\#24675](https://github.com/numpy/numpy/pull/24675): BLD: build wheels for 32-bit Python on Windows, using MSVC
  - [\#24700](https://github.com/numpy/numpy/pull/24700): BLD: fix issue with compiler selection during cross compilation
  - [\#24701](https://github.com/numpy/numpy/pull/24701): BUG: Fix data stmt handling for complex values in f2py
  - [\#24707](https://github.com/numpy/numpy/pull/24707): TYP: Add annotations for the py3.12 buffer protocol
  - [\#24718](https://github.com/numpy/numpy/pull/24718): DOC: fix a few doc build issues on 1.26.x and update <span class="title-ref">spin docs</span>...

---

1.26.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.26.1 Release Notes

NumPy 1.26.1 is a maintenance release that fixes bugs and regressions discovered after the 1.26.0 release. In addition, it adds new functionality for detecting BLAS and LAPACK when building from source. Highlights are:

  - Improved detection of BLAS and LAPACK libraries for meson builds
  - Pickle compatibility with the upcoming NumPy 2.0.

The 1.26.release series is the last planned minor release series before NumPy 2.0. The Python versions supported by this release are 3.9-3.12.

## Build system changes

### Improved BLAS/LAPACK detection and control

Auto-detection for a number of BLAS and LAPACK is now implemented for Meson. By default, the build system will try to detect MKL, Accelerate (on macOS \>=13.3), OpenBLAS, FlexiBLAS, BLIS and reference BLAS/LAPACK. Support for MKL was significantly improved, and support for FlexiBLAS was added.

New command-line flags are available to further control the selection of the BLAS and LAPACK libraries to build against.

To select a specific library, use the config-settings interface via `pip` or `pypa/build`. E.g., to select `libblas`/`liblapack`, use:

    $ pip install numpy -Csetup-args=-Dblas=blas -Csetup-args=-Dlapack=lapack
    $ # OR
    $ python -m build . -Csetup-args=-Dblas=blas -Csetup-args=-Dlapack=lapack

This works not only for the libraries named above, but for any library that Meson is able to detect with the given name through `pkg-config` or CMake.

Besides `-Dblas` and `-Dlapack`, a number of other new flags are available to control BLAS/LAPACK selection and behavior:

  - `-Dblas-order` and `-Dlapack-order`: a list of library names to search for in order, overriding the default search order.
  - `-Duse-ilp64`: if set to `true`, use ILP64 (64-bit integer) BLAS and LAPACK. Note that with this release, ILP64 support has been extended to include MKL and FlexiBLAS. OpenBLAS and Accelerate were supported in previous releases.
  - `-Dallow-noblas`: if set to `true`, allow NumPy to build with its internal (very slow) fallback routines instead of linking against an external BLAS/LAPACK library. *The default for this flag may be changed to \`\`true\`\` in a future 1.26.x release, however for 1.26.1 we'd prefer to keep it as \`\`false\`\` because if failures to detect an installed library are happening, we'd like a bug report for that, so we can quickly assess whether the new auto-detection machinery needs further improvements.*
  - `-Dmkl-threading`: to select the threading layer for MKL. There are four options: `seq`, `iomp`, `gomp` and `tbb`. The default is `auto`, which selects from those four as appropriate given the version of MKL selected.
  - `-Dblas-symbol-suffix`: manually select the symbol suffix to use for the library - should only be needed for linking against libraries built in a non-standard way.

## New features

### `numpy._core` submodule stubs

`numpy._core` submodule stubs were added to provide compatibility with pickled arrays created using NumPy 2.0 when running Numpy 1.26.

## Contributors

A total of 13 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Andrew Nelson
  - Anton Prosekin +
  - Charles Harris
  - Chongyun Lee +
  - Ivan A. Melnikov +
  - Jake Lishman +
  - Mahder Gebremedhin +
  - Mateusz SokÃ³Å‚
  - Matti Picus
  - Munira Alduraibi +
  - Ralf Gommers
  - Rohit Goswami
  - Sayed Adel

## Pull requests merged

A total of 20 pull requests were merged for this release.

  - [\#24742](https://github.com/numpy/numpy/pull/24742): MAINT: Update cibuildwheel version
  - [\#24748](https://github.com/numpy/numpy/pull/24748): MAINT: fix version string in wheels built with setup.py
  - [\#24771](https://github.com/numpy/numpy/pull/24771): BLD, BUG: Fix build failure for host flags e.g. `-march=native`...
  - [\#24773](https://github.com/numpy/numpy/pull/24773): DOC: Updated the f2py docs to remove a note on -fimplicit-none
  - [\#24776](https://github.com/numpy/numpy/pull/24776): BUG: Fix SIMD f32 trunc test on s390x when baseline is none
  - [\#24785](https://github.com/numpy/numpy/pull/24785): BLD: add libquadmath to licences and other tweaks (\#24753)
  - [\#24786](https://github.com/numpy/numpy/pull/24786): MAINT: Activate `use-compute-credits` for Cirrus.
  - [\#24803](https://github.com/numpy/numpy/pull/24803): BLD: updated vendored-meson/meson for mips64 fix
  - [\#24804](https://github.com/numpy/numpy/pull/24804): MAINT: fix licence path win
  - [\#24813](https://github.com/numpy/numpy/pull/24813): BUG: Fix order of Windows OS detection macros.
  - [\#24831](https://github.com/numpy/numpy/pull/24831): BUG, SIMD: use scalar cmul on bad Apple clang x86\_64 (\#24828)
  - [\#24840](https://github.com/numpy/numpy/pull/24840): BUG: Fix DATA statements for f2py
  - [\#24870](https://github.com/numpy/numpy/pull/24870): API: Add `NumpyUnpickler` for backporting
  - [\#24872](https://github.com/numpy/numpy/pull/24872): MAINT: Xfail test failing on PyPy.
  - [\#24879](https://github.com/numpy/numpy/pull/24879): BLD: fix math func feature checks, fix FreeBSD build, add CI...
  - [\#24899](https://github.com/numpy/numpy/pull/24899): ENH: meson: implement BLAS/LAPACK auto-detection and many CI...
  - [\#24902](https://github.com/numpy/numpy/pull/24902): DOC: add a 1.26.1 release notes section for BLAS/LAPACK build...
  - [\#24906](https://github.com/numpy/numpy/pull/24906): MAINT: Backport `numpy._core` stubs. Remove `NumpyUnpickler`
  - [\#24911](https://github.com/numpy/numpy/pull/24911): MAINT: Bump pypa/cibuildwheel from 2.16.1 to 2.16.2
  - [\#24912](https://github.com/numpy/numpy/pull/24912): BUG: loongarch doesn't use REAL(10)

---

1.26.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.26.2 Release Notes

NumPy 1.26.2 is a maintenance release that fixes bugs and regressions discovered after the 1.26.1 release. The 1.26.release series is the last planned minor release series before NumPy 2.0. The Python versions supported by this release are 3.9-3.12.

## Contributors

A total of 13 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - @stefan6419846
  - @thalassemia +
  - Andrew Nelson
  - Charles Bousseau +
  - Charles Harris
  - Marcel Bargull +
  - Mark Mentovai +
  - Matti Picus
  - Nathan Goldbaum
  - Ralf Gommers
  - Sayed Adel
  - Sebastian Berg
  - William Ayd +

## Pull requests merged

A total of 25 pull requests were merged for this release.

  - [\#24814](https://github.com/numpy/numpy/pull/24814): MAINT: align test\_dispatcher s390x targets with \_umath\_tests\_mtargets
  - [\#24929](https://github.com/numpy/numpy/pull/24929): MAINT: prepare 1.26.x for further development
  - [\#24955](https://github.com/numpy/numpy/pull/24955): ENH: Add Cython enumeration for NPY\_FR\_GENERIC
  - [\#24962](https://github.com/numpy/numpy/pull/24962): REL: Remove Python upper version from the release branch
  - [\#24971](https://github.com/numpy/numpy/pull/24971): BLD: Use the correct Python interpreter when running tempita.py
  - [\#24972](https://github.com/numpy/numpy/pull/24972): MAINT: Remove unhelpful error replacements from `import_array()`
  - [\#24977](https://github.com/numpy/numpy/pull/24977): BLD: use classic linker on macOS, the new one in XCode 15 has...
  - [\#25003](https://github.com/numpy/numpy/pull/25003): BLD: musllinux\_aarch64 \[wheel build\]
  - [\#25043](https://github.com/numpy/numpy/pull/25043): MAINT: Update mailmap
  - [\#25049](https://github.com/numpy/numpy/pull/25049): MAINT: Update meson build infrastructure.
  - [\#25071](https://github.com/numpy/numpy/pull/25071): MAINT: Split up .github/workflows to match main
  - [\#25083](https://github.com/numpy/numpy/pull/25083): BUG: Backport fix build on ppc64 when the baseline set to Power9...
  - [\#25093](https://github.com/numpy/numpy/pull/25093): BLD: Fix features.h detection for Meson builds \[1.26.x Backport\]
  - [\#25095](https://github.com/numpy/numpy/pull/25095): BUG: Avoid intp conversion regression in Cython 3 (backport)
  - [\#25107](https://github.com/numpy/numpy/pull/25107): CI: remove obsolete jobs, and move macOS and conda Azure jobs...
  - [\#25108](https://github.com/numpy/numpy/pull/25108): CI: Add linux\_qemu action and remove travis testing.
  - [\#25112](https://github.com/numpy/numpy/pull/25112): MAINT: Update .spin/cmds.py from main.
  - [\#25113](https://github.com/numpy/numpy/pull/25113): DOC: Visually divide main license and bundled licenses in wheels
  - [\#25115](https://github.com/numpy/numpy/pull/25115): MAINT: Add missing `noexcept` to shuffle helpers
  - [\#25116](https://github.com/numpy/numpy/pull/25116): DOC: Fix license identifier for OpenBLAS
  - [\#25117](https://github.com/numpy/numpy/pull/25117): BLD: improve detection of Netlib libblas/libcblas/liblapack
  - [\#25118](https://github.com/numpy/numpy/pull/25118): MAINT: Make bitfield integers unsigned
  - [\#25119](https://github.com/numpy/numpy/pull/25119): BUG: Make n a long int for np.random.multinomial
  - [\#25120](https://github.com/numpy/numpy/pull/25120): BLD: change default of the `allow-noblas` option to true.
  - [\#25121](https://github.com/numpy/numpy/pull/25121): BUG: ensure passing `np.dtype` to itself doesn't crash

---

1.26.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.26.3 Release Notes

NumPy 1.26.3 is a maintenance release that fixes bugs and regressions discovered after the 1.26.2 release. The most notable changes are the f2py bug fixes. The Python versions supported by this release are 3.9-3.12.

## Compatibility

`f2py` will no longer accept ambiguous `-m` and `.pyf` CLI combinations. When more than one `.pyf` file is passed, an error is raised. When both `-m` and a `.pyf` is passed, a warning is emitted and the `-m` provided name is ignored.

## Improvements

`f2py` now handles `common` blocks which have `kind` specifications from modules. This further expands the usability of intrinsics like `iso_fortran_env` and `iso_c_binding`.

## Contributors

A total of 18 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - @DWesl
  - @Illviljan
  - Alexander Grund
  - Andrea Bianchi +
  - Charles Harris
  - Daniel Vanzo
  - Johann Rohwer +
  - Matti Picus
  - Nathan Goldbaum
  - Peter Hawkins
  - Raghuveer Devulapalli
  - Ralf Gommers
  - Rohit Goswami
  - Sayed Adel
  - Sebastian Berg
  - Stefano Rivera +
  - Thomas A Caswell
  - matoro

## Pull requests merged

A total of 42 pull requests were merged for this release.

  - [\#25130](https://github.com/numpy/numpy/pull/25130): MAINT: prepare 1.26.x for further development
  - [\#25188](https://github.com/numpy/numpy/pull/25188): TYP: add None to `__getitem__` in `numpy.array_api`
  - [\#25189](https://github.com/numpy/numpy/pull/25189): BLD,BUG: quadmath required where available \[f2py\]
  - [\#25190](https://github.com/numpy/numpy/pull/25190): BUG: alpha doesn't use REAL(10)
  - [\#25191](https://github.com/numpy/numpy/pull/25191): BUG: Fix FP overflow error in division when the divisor is scalar
  - [\#25192](https://github.com/numpy/numpy/pull/25192): MAINT: Pin scipy-openblas version.
  - [\#25201](https://github.com/numpy/numpy/pull/25201): BUG: Fix f2py to enable use of string optional inout argument
  - [\#25202](https://github.com/numpy/numpy/pull/25202): BUG: Fix -fsanitize=alignment issue in numpy/\_core/src/multiarray/arraytypes.c.src
  - [\#25203](https://github.com/numpy/numpy/pull/25203): TST: Explicitly pass NumPy path to cython during tests (also...
  - [\#25204](https://github.com/numpy/numpy/pull/25204): BUG: fix issues with `newaxis` and `linalg.solve` in `numpy.array_api`
  - [\#25205](https://github.com/numpy/numpy/pull/25205): BUG: Disallow shadowed modulenames
  - [\#25217](https://github.com/numpy/numpy/pull/25217): BUG: Handle common blocks with kind specifications from modules
  - [\#25218](https://github.com/numpy/numpy/pull/25218): BUG: Fix moving compiled executable to root with f2py -c on Windows
  - [\#25219](https://github.com/numpy/numpy/pull/25219): BUG: Fix single to half-precision conversion on PPC64/VSX3
  - [\#25227](https://github.com/numpy/numpy/pull/25227): TST: f2py: fix issue in test skip condition
  - [\#25240](https://github.com/numpy/numpy/pull/25240): Revert "MAINT: Pin scipy-openblas version."
  - [\#25249](https://github.com/numpy/numpy/pull/25249): MAINT: do not use `long` type
  - [\#25377](https://github.com/numpy/numpy/pull/25377): TST: PyPy needs another gc.collect on latest versions
  - [\#25378](https://github.com/numpy/numpy/pull/25378): CI: Install Lapack runtime on Cygwin.
  - [\#25379](https://github.com/numpy/numpy/pull/25379): MAINT: Bump conda-incubator/setup-miniconda from 2.2.0 to 3.0.1
  - [\#25380](https://github.com/numpy/numpy/pull/25380): BLD: update vendored Meson for AIX shared library fix
  - [\#25419](https://github.com/numpy/numpy/pull/25419): MAINT: Init `base` in cpu\_avx512\_kn
  - [\#25420](https://github.com/numpy/numpy/pull/25420): BUG: Fix failing test\_features on SapphireRapids
  - [\#25422](https://github.com/numpy/numpy/pull/25422): BUG: Fix non-contiguous memory load when ARM/Neon is enabled
  - [\#25428](https://github.com/numpy/numpy/pull/25428): MAINT,BUG: Never import distutils above 3.12 \[f2py\]
  - [\#25452](https://github.com/numpy/numpy/pull/25452): MAINT: make the import-time check for old Accelerate more specific
  - [\#25458](https://github.com/numpy/numpy/pull/25458): BUG: fix macOS version checks for Accelerate support
  - [\#25465](https://github.com/numpy/numpy/pull/25465): MAINT: Bump actions/setup-node and larsoner/circleci-artifacts-redirector-action
  - [\#25466](https://github.com/numpy/numpy/pull/25466): BUG: avoid seg fault from OOB access in RandomState.set\_state()
  - [\#25467](https://github.com/numpy/numpy/pull/25467): BUG: Fix two errors related to not checking for failed allocations
  - [\#25468](https://github.com/numpy/numpy/pull/25468): BUG: Fix regression with `f2py` wrappers when modules and subroutines...
  - [\#25475](https://github.com/numpy/numpy/pull/25475): BUG: Fix build issues on SPR
  - [\#25478](https://github.com/numpy/numpy/pull/25478): BLD: fix uninitialized variable warnings from simd/neon/memory.h
  - [\#25480](https://github.com/numpy/numpy/pull/25480): BUG: Handle `iso_c_type` mappings more consistently
  - [\#25481](https://github.com/numpy/numpy/pull/25481): BUG: Fix module name bug in signature files \[urgent\] \[f2py\]
  - [\#25482](https://github.com/numpy/numpy/pull/25482): BUG: Handle .pyf.src and fix SciPy \[urgent\]
  - [\#25483](https://github.com/numpy/numpy/pull/25483): DOC: `f2py` rewrite with `meson` details
  - [\#25485](https://github.com/numpy/numpy/pull/25485): BUG: Add external library handling for meson \[f2py\]
  - [\#25486](https://github.com/numpy/numpy/pull/25486): MAINT: Run f2py's meson backend with the same python that ran...
  - [\#25489](https://github.com/numpy/numpy/pull/25489): MAINT: Update `numpy/f2py/_backends` from main.
  - [\#25490](https://github.com/numpy/numpy/pull/25490): MAINT: Easy updates of `f2py/*.py` from main.
  - [\#25491](https://github.com/numpy/numpy/pull/25491): MAINT: Update crackfortran.py and f2py2e.py from main

---

1.26.4-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 1.26.4 Release Notes

NumPy 1.26.4 is a maintenance release that fixes bugs and regressions discovered after the 1.26.3 release. The Python versions supported by this release are 3.9-3.12. This is the last planned release in the 1.26.x series.

## Contributors

A total of 13 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Elliott Sales de Andrade
  - Lucas Colley +
  - Mark Ryan +
  - Matti Picus
  - Nathan Goldbaum
  - Ola x Nilsson +
  - Pieter Eendebak
  - Ralf Gommers
  - Sayed Adel
  - Sebastian Berg
  - Stefan van der Walt
  - Stefano Rivera

## Pull requests merged

A total of 19 pull requests were merged for this release.

  - [\#25323](https://github.com/numpy/numpy/pull/25323): BUG: Restore missing asstr import
  - [\#25523](https://github.com/numpy/numpy/pull/25523): MAINT: prepare 1.26.x for further development
  - [\#25539](https://github.com/numpy/numpy/pull/25539): BUG: `numpy.array_api`: fix `linalg.cholesky` upper decomp...
  - [\#25584](https://github.com/numpy/numpy/pull/25584): CI: Bump azure pipeline timeout to 120 minutes
  - [\#25585](https://github.com/numpy/numpy/pull/25585): MAINT, BLD: Fix unused inline functions warnings on clang
  - [\#25599](https://github.com/numpy/numpy/pull/25599): BLD: include fix for MinGW platform detection
  - [\#25618](https://github.com/numpy/numpy/pull/25618): TST: Fix test\_numeric on riscv64
  - [\#25619](https://github.com/numpy/numpy/pull/25619): BLD: fix building for windows ARM64
  - [\#25620](https://github.com/numpy/numpy/pull/25620): MAINT: add `newaxis` to `__all__` in `numpy.array_api`
  - [\#25630](https://github.com/numpy/numpy/pull/25630): BUG: Use large file fallocate on 32 bit linux platforms
  - [\#25643](https://github.com/numpy/numpy/pull/25643): TST: Fix test\_warning\_calls on Python 3.12
  - [\#25645](https://github.com/numpy/numpy/pull/25645): TST: Bump pytz to 2023.3.post1
  - [\#25658](https://github.com/numpy/numpy/pull/25658): BUG: Fix AVX512 build flags on Intel Classic Compiler
  - [\#25670](https://github.com/numpy/numpy/pull/25670): BLD: fix potential issue with escape sequences in `__config__.py`
  - [\#25718](https://github.com/numpy/numpy/pull/25718): CI: pin cygwin python to 3.9.16-1 and fix typing tests \[skip...
  - [\#25720](https://github.com/numpy/numpy/pull/25720): MAINT: Bump cibuildwheel to v2.16.4
  - [\#25748](https://github.com/numpy/numpy/pull/25748): BLD: unvendor meson-python on 1.26.x and upgrade to meson-python...
  - [\#25755](https://github.com/numpy/numpy/pull/25755): MAINT: Include header defining backtrace
  - [\#25756](https://github.com/numpy/numpy/pull/25756): BUG: Fix np.quantile(\[Fraction(2,1)\], 0.5) (\#24711)

---

1.3.0-notes.md

---

# NumPy 1.3.0 Release Notes

This minor includes numerous bug fixes, official python 2.6 support, and several new features such as generalized ufuncs.

## Highlights

### Python 2.6 support

Python 2.6 is now supported on all previously supported platforms, including windows.

<https://www.python.org/dev/peps/pep-0361/>

### Generalized ufuncs

There is a general need for looping over not only functions on scalars but also over functions on vectors (or arrays), as explained on <http://scipy.org/scipy/numpy/wiki/GeneralLoopingFunctions>. We propose to realize this concept by generalizing the universal functions (ufuncs), and provide a C implementation that adds \~500 lines to the numpy code base. In current (specialized) ufuncs, the elementary function is limited to element-by-element operations, whereas the generalized version supports "sub-array" by "sub-array" operations. The Perl vector library PDL provides a similar functionality and its terms are re-used in the following.

Each generalized ufunc has information associated with it that states what the "core" dimensionality of the inputs is, as well as the corresponding dimensionality of the outputs (the element-wise ufuncs have zero core dimensions). The list of the core dimensions for all arguments is called the "signature" of a ufunc. For example, the ufunc numpy.add has signature "(),()-\>()" defining two scalar inputs and one scalar output.

Another example is (see the GeneralLoopingFunctions page) the function inner1d(a,b) with a signature of "(i),(i)-\>()". This applies the inner product along the last axis of each input, but keeps the remaining indices intact. For example, where a is of shape (3,5,N) and b is of shape (5,N), this will return an output of shape (3,5). The underlying elementary function is called 3\*5 times. In the signature, we specify one core dimension "(i)" for each input and zero core dimensions "()" for the output, since it takes two 1-d arrays and returns a scalar. By using the same name "i", we specify that the two corresponding dimensions should be of the same size (or one of them is of size 1 and will be broadcasted).

The dimensions beyond the core dimensions are called "loop" dimensions. In the above example, this corresponds to (3,5).

The usual numpy "broadcasting" rules apply, where the signature determines how the dimensions of each input/output object are split into core and loop dimensions:

While an input array has a smaller dimensionality than the corresponding number of core dimensions, 1's are prepended to its shape. The core dimensions are removed from all inputs and the remaining dimensions are broadcasted; defining the loop dimensions. The output is given by the loop dimensions plus the output core dimensions.

### Experimental Windows 64 bits support

Numpy can now be built on windows 64 bits (amd64 only, not IA64), with both MS compilers and mingw-w64 compilers:

This is *highly experimental*: DO NOT USE FOR PRODUCTION USE. See INSTALL.txt, Windows 64 bits section for more information on limitations and how to build it by yourself.

## New features

### Formatting issues

Float formatting is now handled by numpy instead of the C runtime: this enables locale independent formatting, more robust fromstring and related methods. Special values (inf and nan) are also more consistent across platforms (nan vs IND/NaN, etc...), and more consistent with recent python formatting work (in 2.6 and later).

### Nan handling in max/min

The maximum/minimum ufuncs now reliably propagate nans. If one of the arguments is a nan, then nan is returned. This affects np.min/np.max, amin/amax and the array methods max/min. New ufuncs fmax and fmin have been added to deal with non-propagating nans.

### Nan handling in sign

The ufunc sign now returns nan for the sign of anan.

### New ufuncs

1.  fmax - same as maximum for integer types and non-nan floats. Returns the non-nan argument if one argument is nan and returns nan if both arguments are nan.
2.  fmin - same as minimum for integer types and non-nan floats. Returns the non-nan argument if one argument is nan and returns nan if both arguments are nan.
3.  deg2rad - converts degrees to radians, same as the radians ufunc.
4.  rad2deg - converts radians to degrees, same as the degrees ufunc.
5.  log2 - base 2 logarithm.
6.  exp2 - base 2 exponential.
7.  trunc - truncate floats to nearest integer towards zero.
8.  logaddexp - add numbers stored as logarithms and return the logarithm of the result.
9.  logaddexp2 - add numbers stored as base 2 logarithms and return the base 2 logarithm of the result.

### Masked arrays

Several new features and bug fixes, including:

>   - structured arrays should now be fully supported by MaskedArray (r6463, r6324, r6305, r6300, r6294...)
>   - Minor bug fixes (r6356, r6352, r6335, r6299, r6298)
>   - Improved support for \_\_iter\_\_ (r6326)
>   - made baseclass, sharedmask and hardmask accessible to the user (but read-only)
>   - doc update

### gfortran support on windows

Gfortran can now be used as a fortran compiler for numpy on windows, even when the C compiler is Visual Studio (VS 2005 and above; VS 2003 will NOT work). Gfortran + Visual studio does not work on windows 64 bits (but gcc + gfortran does). It is unclear whether it will be possible to use gfortran and visual studio at all on x64.

### Arch option for windows binary

Automatic arch detection can now be bypassed from the command line for the superpack installed:

> numpy-1.3.0-superpack-win32.exe /arch=nosse

will install a numpy which works on any x86, even if the running computer supports SSE set.

## Deprecated features

### Histogram

The semantics of histogram has been modified to fix long-standing issues with outliers handling. The main changes concern

1.  the definition of the bin edges, now including the rightmost edge, and
2.  the handling of upper outliers, now ignored rather than tallied in the rightmost bin.

The previous behavior is still accessible using <span class="title-ref">new=False</span>, but this is deprecated, and will be removed entirely in 1.4.0.

## Documentation changes

A lot of documentation has been added. Both user guide and references can be built from sphinx.

## New C API

### Multiarray API

The following functions have been added to the multiarray C API:

>   - PyArray\_GetEndianness: to get runtime endianness

### Ufunc API

The following functions have been added to the ufunc API:

>   - PyUFunc\_FromFuncAndDataAndSignature: to declare a more general ufunc (generalized ufunc).

### New defines

New public C defines are available for ARCH specific code through numpy/npy\_cpu.h:

>   -   - NPY\_CPU\_X86: x86 arch (32 bits)
>         
>           - NPY\_CPU\_AMD64: amd64 arch (x86\_64, NOT Itanium)
>           - NPY\_CPU\_PPC: 32 bits ppc
>           - NPY\_CPU\_PPC64: 64 bits ppc
>           - NPY\_CPU\_SPARC: 32 bits sparc
>           - NPY\_CPU\_SPARC64: 64 bits sparc
>           - NPY\_CPU\_S390: S390
>           - NPY\_CPU\_IA64: ia64
>           - NPY\_CPU\_PARISC: PARISC

New macros for CPU endianness has been added as well (see internal changes below for details):

>   - NPY\_BYTE\_ORDER: integer
>   - NPY\_LITTLE\_ENDIAN/NPY\_BIG\_ENDIAN defines

Those provide portable alternatives to glibc endian.h macros for platforms without it.

### Portable NAN, INFINITY, etc...

npy\_math.h now makes available several portable macro to get NAN, INFINITY:

>   - NPY\_NAN: equivalent to NAN, which is a GNU extension
>   - NPY\_INFINITY: equivalent to C99 INFINITY
>   - NPY\_PZERO, NPY\_NZERO: positive and negative zero respectively

Corresponding single and extended precision macros are available as well. All references to NAN, or home-grown computation of NAN on the fly have been removed for consistency.

## Internal changes

### numpy.core math configuration revamp

This should make the porting to new platforms easier, and more robust. In particular, the configuration stage does not need to execute any code on the target platform, which is a first step toward cross-compilation.

<https://www.numpy.org/neps/nep-0003-math_config_clean.html>

### umath refactor

A lot of code cleanup for umath/ufunc code (charris).

### Improvements to build warnings

Numpy can now build with -W -Wall without warnings

<https://www.numpy.org/neps/nep-0002-warnfix.html>

### Separate core math library

The core math functions (sin, cos, etc... for basic C types) have been put into a separate library; it acts as a compatibility layer, to support most C99 maths functions (real only for now). The library includes platform-specific fixes for various maths functions, such as using those versions should be more robust than using your platform functions directly. The API for existing functions is exactly the same as the C99 math functions API; the only difference is the npy prefix (npy\_cos vs cos).

The core library will be made available to any extension in 1.4.0.

### CPU arch detection

npy\_cpu.h defines numpy specific CPU defines, such as NPY\_CPU\_X86, etc... Those are portable across OS and toolchains, and set up when the header is parsed, so that they can be safely used even in the case of cross-compilation (the values is not set when numpy is built), or for multi-arch binaries (e.g. fat binaries on Max OS X).

npy\_endian.h defines numpy specific endianness defines, modeled on the glibc endian.h. NPY\_BYTE\_ORDER is equivalent to BYTE\_ORDER, and one of NPY\_LITTLE\_ENDIAN or NPY\_BIG\_ENDIAN is defined. As for CPU archs, those are set when the header is parsed by the compiler, and as such can be used for cross-compilation and multi-arch binaries.

---

1.4.0-notes.md

---

# NumPy 1.4.0 Release Notes

This minor includes numerous bug fixes, as well as a few new features. It is backward compatible with 1.3.0 release.

## Highlights

  - New datetime dtype support to deal with dates in arrays
  - Faster import time
  - Extended array wrapping mechanism for ufuncs
  - New Neighborhood iterator (C-level only)
  - C99-like complex functions in npymath

## New features

### Extended array wrapping mechanism for ufuncs

An \_\_array\_prepare\_\_ method has been added to ndarray to provide subclasses greater flexibility to interact with ufuncs and ufunc-like functions. ndarray already provided \_\_array\_wrap\_\_, which allowed subclasses to set the array type for the result and populate metadata on the way out of the ufunc (as seen in the implementation of MaskedArray). For some applications it is necessary to provide checks and populate metadata *on the way in*. \_\_array\_prepare\_\_ is therefore called just after the ufunc has initialized the output array but before computing the results and populating it. This way, checks can be made and errors raised before operations which may modify data in place.

### Automatic detection of forward incompatibilities

Previously, if an extension was built against a version N of NumPy, and used on a system with NumPy M \< N, the import\_array was successful, which could cause crashes because the version M does not have a function in N. Starting from NumPy 1.4.0, this will cause a failure in import\_array, so the error will be caught early on.

### New iterators

A new neighborhood iterator has been added to the C API. It can be used to iterate over the items in a neighborhood of an array, and can handle boundaries conditions automatically. Zero and one padding are available, as well as arbitrary constant value, mirror and circular padding.

### New polynomial support

New modules chebyshev and polynomial have been added. The new polynomial module is not compatible with the current polynomial support in numpy, but is much like the new chebyshev module. The most noticeable difference to most will be that coefficients are specified from low to high power, that the low level functions do *not* work with the Chebyshev and Polynomial classes as arguments, and that the Chebyshev and Polynomial classes include a domain. Mapping between domains is a linear substitution and the two classes can be converted one to the other, allowing, for instance, a Chebyshev series in one domain to be expanded as a polynomial in another domain. The new classes should generally be used instead of the low level functions, the latter are provided for those who wish to build their own classes.

The new modules are not automatically imported into the numpy namespace, they must be explicitly brought in with an "import numpy.polynomial" statement.

### New C API

The following C functions have been added to the C API:

> 1.  PyArray\_GetNDArrayCFeatureVersion: return the *API* version of the loaded numpy.
> 2.  PyArray\_Correlate2 - like PyArray\_Correlate, but implements the usual definition of correlation. Inputs are not swapped, and conjugate is taken for complex arrays.
> 3.  PyArray\_NeighborhoodIterNew - a new iterator to iterate over a neighborhood of a point, with automatic boundaries handling. It is documented in the iterators section of the C-API reference, and you can find some examples in the multiarray\_test.c.src file in numpy.core.

### New ufuncs

The following ufuncs have been added to the C API:

> 1.  copysign - return the value of the first argument with the sign copied from the second argument.
> 2.  nextafter - return the next representable floating point value of the first argument toward the second argument.

### New defines

The alpha processor is now defined and available in numpy/npy\_cpu.h. The failed detection of the PARISC processor has been fixed. The defines are:

> 1.  NPY\_CPU\_HPPA: PARISC
> 2.  NPY\_CPU\_ALPHA: Alpha

### Testing

> 1.  deprecated decorator: this decorator may be used to avoid cluttering testing output while testing DeprecationWarning is effectively raised by the decorated test.
> 2.  assert\_array\_almost\_equal\_nulp: new method to compare two arrays of floating point values. With this function, two values are considered close if there are not many representable floating point values in between, thus being more robust than assert\_array\_almost\_equal when the values fluctuate a lot.
> 3.  assert\_array\_max\_ulp: raise an assertion if there are more than N representable numbers between two floating point values.
> 4.  assert\_warns: raise an AssertionError if a callable does not generate a warning of the appropriate class, without altering the warning state.

### Reusing npymath

In 1.3.0, we started putting portable C math routines in npymath library, so that people can use those to write portable extensions. Unfortunately, it was not possible to easily link against this library: in 1.4.0, support has been added to numpy.distutils so that 3rd party can reuse this library. See coremath documentation for more information.

### Improved set operations

In previous versions of NumPy some set functions (intersect1d, setxor1d, setdiff1d and setmember1d) could return incorrect results if the input arrays contained duplicate items. These now work correctly for input arrays with duplicates. setmember1d has been renamed to in1d, as with the change to accept arrays with duplicates it is no longer a set operation, and is conceptually similar to an elementwise version of the Python operator 'in'. All of these functions now accept the boolean keyword assume\_unique. This is False by default, but can be set True if the input arrays are known not to contain duplicates, which can increase the functions' execution speed.

## Improvements

> 1.  numpy import is noticeably faster (from 20 to 30 % depending on the platform and computer)
> 
> 2.  The sort functions now sort nans to the end.
>     
>     >   - Real sort order is \[R, nan\]
>     >   - Complex sort order is \[R + Rj, R + nanj, nan + Rj, nan + nanj\]
>     
>     Complex numbers with the same nan placements are sorted according to the non-nan part if it exists.
> 
> 3.  The type comparison functions have been made consistent with the new sort order of nans. Searchsorted now works with sorted arrays containing nan values.
> 
> 4.  Complex division has been made more resistant to overflow.
> 
> 5.  Complex floor division has been made more resistant to overflow.

## Deprecations

The following functions are deprecated:

> 1.  correlate: it takes a new keyword argument old\_behavior. When True (the default), it returns the same result as before. When False, compute the conventional correlation, and take the conjugate for complex arrays. The old behavior will be removed in NumPy 1.5, and raises a DeprecationWarning in 1.4.
> 2.  unique1d: use unique instead. unique1d raises a deprecation warning in 1.4, and will be removed in 1.5.
> 3.  intersect1d\_nu: use intersect1d instead. intersect1d\_nu raises a deprecation warning in 1.4, and will be removed in 1.5.
> 4.  setmember1d: use in1d instead. setmember1d raises a deprecation warning in 1.4, and will be removed in 1.5.

The following raise errors:

> 1.  When operating on 0-d arrays, `numpy.max` and other functions accept only `axis=0`, `axis=-1` and `axis=None`. Using an out-of-bounds axes is an indication of a bug, so Numpy raises an error for these cases now.
> 2.  Specifying `axis > MAX_DIMS` is no longer allowed; Numpy raises now an error instead of behaving similarly as for `axis=None`.

## Internal changes

### Use C99 complex functions when available

The numpy complex types are now guaranteed to be ABI compatible with C99 complex type, if available on the platform. Moreover, the complex ufunc now use the platform C99 functions instead of our own.

### split multiarray and umath source code

The source code of multiarray and umath has been split into separate logic compilation units. This should make the source code more amenable for newcomers.

### Separate compilation

By default, every file of multiarray (and umath) is merged into one for compilation as was the case before, but if NPY\_SEPARATE\_COMPILATION env variable is set to a non-negative value, experimental individual compilation of each file is enabled. This makes the compile/debug cycle much faster when working on core numpy.

### Separate core math library

New functions which have been added:

>   -   - npy\_copysign
>         
>           - npy\_nextafter
>           - npy\_cpack
>           - npy\_creal
>           - npy\_cimag
>           - npy\_cabs
>           - npy\_cexp
>           - npy\_clog
>           - npy\_cpow
>           - npy\_csqr
>           - npy\_ccos
>           - npy\_csin

---

1.5.0-notes.md

---

# NumPy 1.5.0 Release Notes

## Highlights

### Python 3 compatibility

This is the first NumPy release which is compatible with Python 3. Support for Python 3 and Python 2 is done from a single code base. Extensive notes on changes can be found at <https://web.archive.org/web/20100814160313/http://projects.scipy.org/numpy/browser/trunk/doc/Py3K.txt>.

Note that the Numpy testing framework relies on nose, which does not have a Python 3 compatible release yet. A working Python 3 branch of nose can be found at <https://web.archive.org/web/20100817112505/http://bitbucket.org/jpellerin/nose3/> however.

Porting of SciPy to Python 3 is expected to be completed soon.

### `3118` compatibility

The new buffer protocol described by PEP 3118 is fully supported in this version of Numpy. On Python versions \>= 2.6 Numpy arrays expose the buffer interface, and array(), asarray() and other functions accept new-style buffers as input.

## New features

### Warning on casting complex to real

Numpy now emits a `numpy.ComplexWarning` when a complex number is cast into a real number. For example:

> \>\>\> x = np.array(\[1,2,3\]) \>\>\> x\[:2\] = np.array(\[1+2j, 1-2j\]) ComplexWarning: Casting complex values to real discards the imaginary part

The cast indeed discards the imaginary part, and this may not be the intended behavior in all cases, hence the warning. This warning can be turned off in the standard way:

> \>\>\> import warnings \>\>\> warnings.simplefilter("ignore", np.ComplexWarning)

### Dot method for ndarrays

Ndarrays now have the dot product also as a method, which allows writing chains of matrix products as

> \>\>\> a.dot(b).dot(c)

instead of the longer alternative

> \>\>\> np.dot(a, np.dot(b, c))

### linalg.slogdet function

The slogdet function returns the sign and logarithm of the determinant of a matrix. Because the determinant may involve the product of many small/large values, the result is often more accurate than that obtained by simple multiplication.

### new header

The new header file ndarraytypes.h contains the symbols from ndarrayobject.h that do not depend on the PY\_ARRAY\_UNIQUE\_SYMBOL and NO\_IMPORT/\_ARRAY macros. Broadly, these symbols are types, typedefs, and enumerations; the array function calls are left in ndarrayobject.h. This allows users to include array-related types and enumerations without needing to concern themselves with the macro expansions and their side- effects.

## Changes

### polynomial.polynomial

  - The polyint and polyder functions now check that the specified number integrations or derivations is a non-negative integer. The number 0 is a valid value for both functions.
  - A degree method has been added to the Polynomial class.
  - A trimdeg method has been added to the Polynomial class. It operates like truncate except that the argument is the desired degree of the result, not the number of coefficients.
  - Polynomial.fit now uses None as the default domain for the fit. The default Polynomial domain can be specified by using \[\] as the domain value.
  - Weights can be used in both polyfit and Polynomial.fit
  - A linspace method has been added to the Polynomial class to ease plotting.
  - The polymulx function was added.

### polynomial.chebyshev

  - The chebint and chebder functions now check that the specified number integrations or derivations is a non-negative integer. The number 0 is a valid value for both functions.
  - A degree method has been added to the Chebyshev class.
  - A trimdeg method has been added to the Chebyshev class. It operates like truncate except that the argument is the desired degree of the result, not the number of coefficients.
  - Chebyshev.fit now uses None as the default domain for the fit. The default Chebyshev domain can be specified by using \[\] as the domain value.
  - Weights can be used in both chebfit and Chebyshev.fit
  - A linspace method has been added to the Chebyshev class to ease plotting.
  - The chebmulx function was added.
  - Added functions for the Chebyshev points of the first and second kind.

### histogram

After a two years transition period, the old behavior of the histogram function has been phased out, and the "new" keyword has been removed.

### correlate

The old behavior of correlate was deprecated in 1.4.0, the new behavior (the usual definition for cross-correlation) is now the default.

---

1.6.0-notes.md

---

# NumPy 1.6.0 Release Notes

This release includes several new features as well as numerous bug fixes and improved documentation. It is backward compatible with the 1.5.0 release, and supports Python 2.4 - 2.7 and 3.1 - 3.2.

## Highlights

  - Re-introduction of datetime dtype support to deal with dates in arrays.
  - A new 16-bit floating point type.
  - A new iterator, which improves performance of many functions.

## New features

### New 16-bit floating point type

This release adds support for the IEEE 754-2008 binary16 format, available as the data type `numpy.half`. Within Python, the type behaves similarly to <span class="title-ref">float</span> or <span class="title-ref">double</span>, and C extensions can add support for it with the exposed half-float API.

### New iterator

A new iterator has been added, replacing the functionality of the existing iterator and multi-iterator with a single object and API. This iterator works well with general memory layouts different from C or Fortran contiguous, and handles both standard NumPy and customized broadcasting. The buffering, automatic data type conversion, and optional output parameters, offered by ufuncs but difficult to replicate elsewhere, are now exposed by this iterator.

### Legendre, Laguerre, Hermite, HermiteE polynomials in `numpy.polynomial`

Extend the number of polynomials available in the polynomial package. In addition, a new `window` attribute has been added to the classes in order to specify the range the `domain` maps to. This is mostly useful for the Laguerre, Hermite, and HermiteE polynomials whose natural domains are infinite and provides a more intuitive way to get the correct mapping of values without playing unnatural tricks with the domain.

### Fortran assumed shape array and size function support in `numpy.f2py`

F2py now supports wrapping Fortran 90 routines that use assumed shape arrays. Before such routines could be called from Python but the corresponding Fortran routines received assumed shape arrays as zero length arrays which caused unpredicted results. Thanks to Lorenz HÃ¼depohl for pointing out the correct way to interface routines with assumed shape arrays.

In addition, f2py supports now automatic wrapping of Fortran routines that use two argument `size` function in dimension specifications.

### Other new functions

`numpy.ravel_multi_index` : Converts a multi-index tuple into an array of flat indices, applying boundary modes to the indices.

`numpy.einsum` : Evaluate the Einstein summation convention. Using the Einstein summation convention, many common multi-dimensional array operations can be represented in a simple fashion. This function provides a way compute such summations.

`numpy.count_nonzero` : Counts the number of non-zero elements in an array.

`numpy.result_type` and `numpy.min_scalar_type` : These functions expose the underlying type promotion used by the ufuncs and other operations to determine the types of outputs. These improve upon the `numpy.common_type` and `numpy.mintypecode` which provide similar functionality but do not match the ufunc implementation.

## Changes

### `default error handling`

The default error handling has been change from `print` to `warn` for all except for `underflow`, which remains as `ignore`.

### `numpy.distutils`

Several new compilers are supported for building Numpy: the Portland Group Fortran compiler on OS X, the PathScale compiler suite and the 64-bit Intel C compiler on Linux.

### `numpy.testing`

The testing framework gained `numpy.testing.assert_allclose`, which provides a more convenient way to compare floating point arrays than <span class="title-ref">assert\_almost\_equal</span>, <span class="title-ref">assert\_approx\_equal</span> and <span class="title-ref">assert\_array\_almost\_equal</span>.

### `C API`

In addition to the APIs for the new iterator and half data type, a number of other additions have been made to the C API. The type promotion mechanism used by ufuncs is exposed via `PyArray_PromoteTypes`, `PyArray_ResultType`, and `PyArray_MinScalarType`. A new enumeration `NPY_CASTING` has been added which controls what types of casts are permitted. This is used by the new functions `PyArray_CanCastArrayTo` and `PyArray_CanCastTypeTo`. A more flexible way to handle conversion of arbitrary python objects into arrays is exposed by `PyArray_GetArrayParamsFromObject`.

## Deprecated features

The "normed" keyword in `numpy.histogram` is deprecated. Its functionality will be replaced by the new "density" keyword.

## Removed features

### `numpy.fft`

The functions <span class="title-ref">refft</span>, <span class="title-ref">refft2</span>, <span class="title-ref">refftn</span>, <span class="title-ref">irefft</span>, <span class="title-ref">irefft2</span>, <span class="title-ref">irefftn</span>, which were aliases for the same functions without the 'e' in the name, were removed.

### `numpy.memmap`

The <span class="title-ref">sync()</span> and <span class="title-ref">close()</span> methods of memmap were removed. Use <span class="title-ref">flush()</span> and "del memmap" instead.

### `numpy.lib`

The deprecated functions `numpy.unique1d`, `numpy.setmember1d`, `numpy.intersect1d_nu` and `numpy.lib.ufunclike.log2` were removed.

### `numpy.ma`

Several deprecated items were removed from the `numpy.ma` module:

    * ``numpy.ma.MaskedArray`` "raw_data" method
    * ``numpy.ma.MaskedArray`` constructor "flag" keyword
    * ``numpy.ma.make_mask`` "flag" keyword
    * ``numpy.ma.allclose`` "fill_value" keyword

### `numpy.distutils`

The `numpy.get_numpy_include` function was removed, use `numpy.get_include` instead.

---

1.6.1-notes.md

---

# NumPy 1.6.1 Release Notes

This is a bugfix only release in the 1.6.x series.

## Issues Fixed

  - \#1834: einsum fails for specific shapes
  - \#1837: einsum throws nan or freezes python for specific array shapes
  - \#1838: object \<-\> structured type arrays regression
  - \#1851: regression for SWIG based code in 1.6.0
  - \#1863: Buggy results when operating on array copied with astype()
  - \#1870: Fix corner case of object array assignment
  - \#1843: Py3k: fix error with recarray
  - \#1885: nditer: Error in detecting double reduction loop
  - \#1874: f2py: fix --include\_paths bug
  - \#1749: Fix ctypes.load\_library()
  - \#1895/1896: iter: writeonly operands weren't always being buffered correctly

---

1.6.2-notes.md

---

# NumPy 1.6.2 Release Notes

This is a bugfix release in the 1.6.x series. Due to the delay of the NumPy 1.7.0 release, this release contains far more fixes than a regular NumPy bugfix release. It also includes a number of documentation and build improvements.

## Issues fixed

### `numpy.core`

  - \#2063: make unique() return consistent index
  - \#1138: allow creating arrays from empty buffers or empty slices
  - \#1446: correct note about correspondence vstack and concatenate
  - \#1149: make argmin() work for datetime
  - \#1672: fix allclose() to work for scalar inf
  - \#1747: make np.median() work for 0-D arrays
  - \#1776: make complex division by zero to yield inf properly
  - \#1675: add scalar support for the format() function
  - \#1905: explicitly check for NaNs in allclose()
  - \#1952: allow floating ddof in std() and var()
  - \#1948: fix regression for indexing chararrays with empty list
  - \#2017: fix type hashing
  - \#2046: deleting array attributes causes segfault
  - \#2033: a\*\*2.0 has incorrect type
  - \#2045: make attribute/iterator\_element deletions not segfault
  - \#2021: fix segfault in searchsorted()
  - \#2073: fix float16 \_\_array\_interface\_\_ bug

### `numpy.lib`

  - \#2048: break reference cycle in NpzFile
  - \#1573: savetxt() now handles complex arrays
  - \#1387: allow bincount() to accept empty arrays
  - \#1899: fixed histogramdd() bug with empty inputs
  - \#1793: fix failing npyio test under py3k
  - \#1936: fix extra nesting for subarray dtypes
  - \#1848: make tril/triu return the same dtype as the original array
  - \#1918: use Py\_TYPE to access ob\_type, so it works also on Py3

### `numpy.distutils`

  - \#1261: change compile flag on AIX from -O5 to -O3
  - \#1377: update HP compiler flags
  - \#1383: provide better support for C++ code on HPUX
  - \#1857: fix build for py3k + pip
  - BLD: raise a clearer warning in case of building without cleaning up first
  - BLD: follow build\_ext coding convention in build\_clib
  - BLD: fix up detection of Intel CPU on OS X in system\_info.py
  - BLD: add support for the new X11 directory structure on Ubuntu & co.
  - BLD: add ufsparse to the libraries search path.
  - BLD: add 'pgfortran' as a valid compiler in the Portland Group
  - BLD: update version match regexp for IBM AIX Fortran compilers.

### `numpy.random`

  - BUG: Use npy\_intp instead of long in mtrand

## Changes

### `numpy.f2py`

  - ENH: Introduce new options extra\_f77\_compiler\_args and extra\_f90\_compiler\_args
  - BLD: Improve reporting of fcompiler value
  - BUG: Fix f2py test\_kind.py test

### `numpy.poly`

  - ENH: Add some tests for polynomial printing
  - ENH: Add companion matrix functions
  - DOC: Rearrange the polynomial documents
  - BUG: Fix up links to classes
  - DOC: Add version added to some of the polynomial package modules
  - DOC: Document xxxfit functions in the polynomial package modules
  - BUG: The polynomial convenience classes let different types interact
  - DOC: Document the use of the polynomial convenience classes
  - DOC: Improve numpy reference documentation of polynomial classes
  - ENH: Improve the computation of polynomials from roots
  - STY: Code cleanup in polynomial \[\*\]fromroots functions
  - DOC: Remove references to cast and NA, which were added in 1.7

---

1.7.0-notes.md

---

# NumPy 1.7.0 Release Notes

This release includes several new features as well as numerous bug fixes and refactorings. It supports Python 2.4 - 2.7 and 3.1 - 3.3 and is the last release that supports Python 2.4 - 2.5.

## Highlights

  - `where=` parameter to ufuncs (allows the use of boolean arrays to choose where a computation should be done)
  - `vectorize` improvements (added 'excluded' and 'cache' keyword, general cleanup and bug fixes)
  - `numpy.random.choice` (random sample generating function)

## Compatibility notes

In a future version of numpy, the functions np.diag, np.diagonal, and the diagonal method of ndarrays will return a view onto the original array, instead of producing a copy as they do now. This makes a difference if you write to the array returned by any of these functions. To facilitate this transition, numpy 1.7 produces a FutureWarning if it detects that you may be attempting to write to such an array. See the documentation for np.diagonal for details.

Similar to np.diagonal above, in a future version of numpy, indexing a record array by a list of field names will return a view onto the original array, instead of producing a copy as they do now. As with np.diagonal, numpy 1.7 produces a FutureWarning if it detects that you may be attempting to write to such an array. See the documentation for array indexing for details.

In a future version of numpy, the default casting rule for UFunc out= parameters will be changed from 'unsafe' to 'same\_kind'. (This also applies to in-place operations like a += b, which is equivalent to np.add(a, b, out=a).) Most usages which violate the 'same\_kind' rule are likely bugs, so this change may expose previously undetected errors in projects that depend on NumPy. In this version of numpy, such usages will continue to succeed, but will raise a DeprecationWarning.

Full-array boolean indexing has been optimized to use a different, optimized code path. This code path should produce the same results, but any feedback about changes to your code would be appreciated.

Attempting to write to a read-only array (one with `arr.flags.writeable` set to `False`) used to raise either a RuntimeError, ValueError, or TypeError inconsistently, depending on which code path was taken. It now consistently raises a ValueError.

The \<ufunc\>.reduce functions evaluate some reductions in a different order than in previous versions of NumPy, generally providing higher performance. Because of the nature of floating-point arithmetic, this may subtly change some results, just as linking NumPy to a different BLAS implementations such as MKL can.

If upgrading from 1.5, then generally in 1.6 and 1.7 there have been substantial code added and some code paths altered, particularly in the areas of type resolution and buffered iteration over universal functions. This might have an impact on your code particularly if you relied on accidental behavior in the past.

## New features

### Reduction UFuncs Generalize axis= Parameter

Any ufunc.reduce function call, as well as other reductions like sum, prod, any, all, max and min support the ability to choose a subset of the axes to reduce over. Previously, one could say axis=None to mean all the axes or axis=\# to pick a single axis. Now, one can also say axis=(\#,\#) to pick a list of axes for reduction.

### Reduction UFuncs New keepdims= Parameter

There is a new keepdims= parameter, which if set to True, doesn't throw away the reduction axes but instead sets them to have size one. When this option is set, the reduction result will broadcast correctly to the original operand which was reduced.

### Datetime support

<div class="note">

<div class="title">

Note

</div>

The datetime API is *experimental* in 1.7.0, and may undergo changes in future versions of NumPy.

</div>

There have been a lot of fixes and enhancements to datetime64 compared to NumPy 1.6:

  - the parser is quite strict about only accepting ISO 8601 dates, with a few convenience extensions
  - converts between units correctly
  - datetime arithmetic works correctly
  - business day functionality (allows the datetime to be used in contexts where only certain days of the week are valid)

The notes in [doc/source/reference/arrays.datetime.rst](https://github.com/numpy/numpy/blob/maintenance/1.7.x/doc/source/reference/arrays.datetime.md) (also available in the online docs at [arrays.datetime.html](https://docs.scipy.org/doc/numpy/reference/arrays.datetime.html)) should be consulted for more details.

### Custom formatter for printing arrays

See the new `formatter` parameter of the `numpy.set_printoptions` function.

### New function numpy.random.choice

A generic sampling function has been added which will generate samples from a given array-like. The samples can be with or without replacement, and with uniform or given non-uniform probabilities.

### New function isclose

Returns a boolean array where two arrays are element-wise equal within a tolerance. Both relative and absolute tolerance can be specified.

### Preliminary multi-dimensional support in the polynomial package

Axis keywords have been added to the integration and differentiation functions and a tensor keyword was added to the evaluation functions. These additions allow multi-dimensional coefficient arrays to be used in those functions. New functions for evaluating 2-D and 3-D coefficient arrays on grids or sets of points were added together with 2-D and 3-D pseudo-Vandermonde matrices that can be used for fitting.

### Ability to pad rank-n arrays

A pad module containing functions for padding n-dimensional arrays has been added. The various private padding functions are exposed as options to a public 'pad' function. Example:

    pad(a, 5, mode='mean')

Current modes are `constant`, `edge`, `linear_ramp`, `maximum`, `mean`, `median`, `minimum`, `reflect`, `symmetric`, `wrap`, and `<function>`.

### New argument to searchsorted

The function searchsorted now accepts a 'sorter' argument that is a permutation array that sorts the array to search.

### Build system

Added experimental support for the AArch64 architecture.

### C API

New function `PyArray_FailUnlessWriteable` provides a consistent interface for checking array writeability -- any C code which works with arrays whose WRITEABLE flag is not known to be True a priori, should make sure to call this function before writing.

NumPy C Style Guide added (`doc/C_STYLE_GUIDE.rst`).

## Changes

### General

The function np.concatenate tries to match the layout of its input arrays. Previously, the layout did not follow any particular reason, and depended in an undesirable way on the particular axis chosen for concatenation. A bug was also fixed which silently allowed out of bounds axis arguments.

The ufuncs logical\_or, logical\_and, and logical\_not now follow Python's behavior with object arrays, instead of trying to call methods on the objects. For example the expression (3 and 'test') produces the string 'test', and now np.logical\_and(np.array(3, 'O'), np.array('test', 'O')) produces 'test' as well.

The `.base` attribute on ndarrays, which is used on views to ensure that the underlying array owning the memory is not deallocated prematurely, now collapses out references when you have a view-of-a-view. For example:

    a = np.arange(10)
    b = a[1:]
    c = b[1:]

In numpy 1.6, `c.base` is `b`, and `c.base.base` is `a`. In numpy 1.7, `c.base` is `a`.

To increase backwards compatibility for software which relies on the old behaviour of `.base`, we only 'skip over' objects which have exactly the same type as the newly created view. This makes a difference if you use `ndarray` subclasses. For example, if we have a mix of `ndarray` and `matrix` objects which are all views on the same original `ndarray`:

    a = np.arange(10)
    b = np.asmatrix(a)
    c = b[0, 1:]
    d = c[0, 1:]

then `d.base` will be `b`. This is because `d` is a `matrix` object, and so the collapsing process only continues so long as it encounters other `matrix` objects. It considers `c`, `b`, and `a` in that order, and `b` is the last entry in that list which is a `matrix` object.

### Casting Rules

Casting rules have undergone some changes in corner cases, due to the NA-related work. In particular for combinations of scalar+scalar:

  - the <span class="title-ref">longlong</span> type (<span class="title-ref">q</span>) now stays <span class="title-ref">longlong</span> for operations with any other number (<span class="title-ref">? b h i l q p B H I</span>), previously it was cast as <span class="title-ref">int\_</span> (<span class="title-ref">l</span>). The <span class="title-ref">ulonglong</span> type (<span class="title-ref">Q</span>) now stays as <span class="title-ref">ulonglong</span> instead of <span class="title-ref">uint</span> (<span class="title-ref">L</span>).
  - the <span class="title-ref">timedelta64</span> type (<span class="title-ref">m</span>) can now be mixed with any integer type (<span class="title-ref">b h i l q p B H I L Q P</span>), previously it raised <span class="title-ref">TypeError</span>.

For array + scalar, the above rules just broadcast except the case when the array and scalars are unsigned/signed integers, then the result gets converted to the array type (of possibly larger size) as illustrated by the following examples:

    >>> (np.zeros((2,), dtype=np.uint8) + np.int16(257)).dtype
    dtype('uint16')
    >>> (np.zeros((2,), dtype=np.int8) + np.uint16(257)).dtype
    dtype('int16')
    >>> (np.zeros((2,), dtype=np.int16) + np.uint32(2**17)).dtype
    dtype('int32')

Whether the size gets increased depends on the size of the scalar, for example:

    >>> (np.zeros((2,), dtype=np.uint8) + np.int16(255)).dtype
    dtype('uint8')
    >>> (np.zeros((2,), dtype=np.uint8) + np.int16(256)).dtype
    dtype('uint16')

Also a `complex128` scalar + `float32` array is cast to `complex64`.

In NumPy 1.7 the <span class="title-ref">datetime64</span> type (<span class="title-ref">M</span>) must be constructed by explicitly specifying the type as the second argument (e.g. `np.datetime64(2000, 'Y')`).

## Deprecations

### General

Specifying a custom string formatter with a <span class="title-ref">\_format</span> array attribute is deprecated. The new `formatter` keyword in `numpy.set_printoptions` or `numpy.array2string` can be used instead.

The deprecated imports in the polynomial package have been removed.

`concatenate` now raises DepractionWarning for 1D arrays if `axis != 0`. Versions of numpy \< 1.7.0 ignored axis argument value for 1D arrays. We allow this for now, but in due course we will raise an error.

### C-API

Direct access to the fields of PyArrayObject\* has been deprecated. Direct access has been recommended against for many releases. Expect similar deprecations for PyArray\_Descr\* and other core objects in the future as preparation for NumPy 2.0.

The macros in old\_defines.h are deprecated and will be removed in the next major release (\>= 2.0). The sed script tools/replace\_old\_macros.sed can be used to replace these macros with the newer versions.

You can test your code against the deprecated C API by adding a line composed of `#define NPY_NO_DEPRECATED_API` and the target version number, such as `NPY_1_7_API_VERSION`, before including any NumPy headers.

The `NPY_CHAR` member of the `NPY_TYPES` enum is deprecated and will be removed in NumPy 1.8. See the discussion at [gh-2801](https://github.com/numpy/numpy/issues/2801) for more details.

---

1.7.1-notes.md

---

# NumPy 1.7.1 Release Notes

This is a bugfix only release in the 1.7.x series. It supports Python 2.4 - 2.7 and 3.1 - 3.3 and is the last series that supports Python 2.4 - 2.5.

## Issues fixed

  - gh-2973: Fix <span class="title-ref">1</span> is printed during numpy.test()
  - gh-2983: BUG: gh-2969: Backport memory leak fix 80b3a34.
  - gh-3007: Backport gh-3006
  - gh-2984: Backport fix complex polynomial fit
  - gh-2982: BUG: Make nansum work with booleans.
  - gh-2985: Backport large sort fixes
  - gh-3039: Backport object take
  - gh-3105: Backport nditer fix op axes initialization
  - gh-3108: BUG: npy-pkg-config ini files were missing after Bento build.
  - gh-3124: BUG: PyArray\_LexSort allocates too much temporary memory.
  - gh-3131: BUG: Exported f2py\_size symbol prevents linking multiple f2py modules.
  - gh-3117: Backport gh-2992
  - gh-3135: DOC: Add mention of PyArray\_SetBaseObject stealing a reference
  - gh-3134: DOC: Fix typo in fft docs (the indexing variable is 'm', not 'n').
  - gh-3136: Backport \#3128

---

1.7.2-notes.md

---

# NumPy 1.7.2 Release Notes

This is a bugfix only release in the 1.7.x series. It supports Python 2.4 - 2.7 and 3.1 - 3.3 and is the last series that supports Python 2.4 - 2.5.

## Issues fixed

  - gh-3153: Do not reuse nditer buffers when not filled enough
  - gh-3192: f2py crashes with UnboundLocalError exception
  - gh-442: Concatenate with axis=None now requires equal number of array elements
  - gh-2485: Fix for astype('S') string truncate issue
  - gh-3312: bug in count\_nonzero
  - gh-2684: numpy.ma.average casts complex to float under certain conditions
  - gh-2403: masked array with named components does not behave as expected
  - gh-2495: np.ma.compress treated inputs in wrong order
  - gh-576: add \_\_len\_\_ method to ma.mvoid
  - gh-3364: reduce performance regression of mmap slicing
  - gh-3421: fix non-swapping strided copies in GetStridedCopySwap
  - gh-3373: fix small leak in datetime metadata initialization
  - gh-2791: add platform specific python include directories to search paths
  - gh-3168: fix undefined function and add integer divisions
  - gh-3301: memmap does not work with TemporaryFile in python3
  - gh-3057: distutils.misc\_util.get\_shared\_lib\_extension returns wrong debug extension
  - gh-3472: add module extensions to load\_library search list
  - gh-3324: Make comparison function (gt, ge, ...) respect \_\_array\_priority\_\_
  - gh-3497: np.insert behaves incorrectly with argument 'axis=-1'
  - gh-3541: make preprocessor tests consistent in halffloat.c
  - gh-3458: array\_ass\_boolean\_subscript() writes 'non-existent' data to array
  - gh-2892: Regression in ufunc.reduceat with zero-sized index array
  - gh-3608: Regression when filling struct from tuple
  - gh-3701: add support for Python 3.4 ast.NameConstant
  - gh-3712: do not assume that GIL is enabled in xerbla
  - gh-3712: fix LAPACK error handling in lapack\_litemodule
  - gh-3728: f2py fix decref on wrong object
  - gh-3743: Hash changed signature in Python 3.3
  - gh-3793: scalar int hashing broken on 64 bit python3
  - gh-3160: SandboxViolation easyinstalling 1.7.0 on Mac OS X 10.8.3
  - gh-3871: npy\_math.h has invalid isinf for Solaris with SUNWspro12.2
  - gh-2561: Disable check for oldstyle classes in python3
  - gh-3900: Ensure NotImplemented is passed on in MaskedArray ufunc's
  - gh-2052: del scalar subscript causes segfault
  - gh-3832: fix a few uninitialized uses and memleaks
  - gh-3971: f2py changed string.lowercase to string.ascii\_lowercase for python3
  - gh-3480: numpy.random.binomial raised ValueError for n == 0
  - gh-3992: hypot(inf, 0) shouldn't raise a warning, hypot(inf, inf) wrong result
  - gh-4018: Segmentation fault dealing with very large arrays
  - gh-4094: fix NaT handling in \_strided\_to\_strided\_string\_to\_datetime
  - gh-4051: fix uninitialized use in \_strided\_to\_strided\_string\_to\_datetime
  - gh-4123: lexsort segfault
  - gh-4141: Fix a few issues that show up with python 3.4b1

---

1.8.0-notes.md

---

# NumPy 1.8.0 Release Notes

This release supports Python 2.6 -2.7 and 3.2 - 3.3.

## Highlights

  - New, no 2to3, Python 2 and Python 3 are supported by a common code base.
  - New, gufuncs for linear algebra, enabling operations on stacked arrays.
  - New, inplace fancy indexing for ufuncs with the `.at` method.
  - New, `partition` function, partial sorting via selection for fast median.
  - New, `nanmean`, `nanvar`, and `nanstd` functions skipping NaNs.
  - New, `full` and `full_like` functions to create value initialized arrays.
  - New, `PyUFunc_RegisterLoopForDescr`, better ufunc support for user dtypes.
  - Numerous performance improvements in many areas.

## Dropped Support

Support for Python versions 2.4 and 2.5 has been dropped,

Support for SCons has been removed.

## Future Changes

The Datetime64 type remains experimental in this release. In 1.9 there will probably be some changes to make it more usable.

The diagonal method currently returns a new array and raises a FutureWarning. In 1.9 it will return a readonly view.

Multiple field selection from an array of structured type currently returns a new array and raises a FutureWarning. In 1.9 it will return a readonly view.

The numpy/oldnumeric and numpy/numarray compatibility modules will be removed in 1.9.

## Compatibility notes

The doc/sphinxext content has been moved into its own github repository, and is included in numpy as a submodule. See the instructions in doc/HOWTO\_BUILD\_DOCS.rst for how to access the content.

The hash function of numpy.void scalars has been changed. Previously the pointer to the data was hashed as an integer. Now, the hash function uses the tuple-hash algorithm to combine the hash functions of the elements of the scalar, but only if the scalar is read-only.

Numpy has switched its build system to using 'separate compilation' by default. In previous releases this was supported, but not default. This should produce the same results as the old system, but if you're trying to do something complicated like link numpy statically or using an unusual compiler, then it's possible you will encounter problems. If so, please file a bug and as a temporary workaround you can re-enable the old build system by exporting the shell variable NPY\_SEPARATE\_COMPILATION=0.

For the AdvancedNew iterator the `oa_ndim` flag should now be -1 to indicate that no `op_axes` and `itershape` are passed in. The `oa_ndim == 0` case, now indicates a 0-D iteration and `op_axes` being NULL and the old usage is deprecated. This does not effect the `NpyIter_New` or `NpyIter_MultiNew` functions.

The functions nanargmin and nanargmax now return np.iinfo\['intp'\].min for the index in all-NaN slices. Previously the functions would raise a ValueError for array returns and NaN for scalar returns.

### NPY\_RELAXED\_STRIDES\_CHECKING

There is a new compile time environment variable `NPY_RELAXED_STRIDES_CHECKING`. If this variable is set to 1, then numpy will consider more arrays to be C- or F-contiguous -- for example, it becomes possible to have a column vector which is considered both C- and F-contiguous simultaneously. The new definition is more accurate, allows for faster code that makes fewer unnecessary copies, and simplifies numpy's code internally. However, it may also break third-party libraries that make too-strong assumptions about the stride values of C- and F-contiguous arrays. (It is also currently known that this breaks Cython code using memoryviews, which will be fixed in Cython.) THIS WILL BECOME THE DEFAULT IN A FUTURE RELEASE, SO PLEASE TEST YOUR CODE NOW AGAINST NUMPY BUILT WITH:

    NPY_RELAXED_STRIDES_CHECKING=1 python setup.py install

You can check whether NPY\_RELAXED\_STRIDES\_CHECKING is in effect by running:

    np.ones((10, 1), order="C").flags.f_contiguous

This will be `True` if relaxed strides checking is enabled, and `False` otherwise. The typical problem we've seen so far is C code that works with C-contiguous arrays, and assumes that the itemsize can be accessed by looking at the last element in the `PyArray_STRIDES(arr)` array. When relaxed strides are in effect, this is not true (and in fact, it never was true in some corner cases). Instead, use `PyArray_ITEMSIZE(arr)`.

For more information check the "Internal memory layout of an ndarray" section in the documentation.

### Binary operations with non-arrays as second argument

Binary operations of the form `<array-or-subclass> * <non-array-subclass>` where `<non-array-subclass>` declares an `__array_priority__` higher than that of `<array-or-subclass>` will now unconditionally return *NotImplemented*, giving `<non-array-subclass>` a chance to handle the operation. Previously, <span class="title-ref">NotImplemented</span> would only be returned if `<non-array-subclass>` actually implemented the reversed operation, and after a (potentially expensive) array conversion of `<non-array-subclass>` had been attempted. ([bug](https://github.com/numpy/numpy/issues/3375), [pull request](https://github.com/numpy/numpy/pull/3501))

### Function <span class="title-ref">median</span> used with <span class="title-ref">overwrite\_input</span> only partially sorts array

If <span class="title-ref">median</span> is used with <span class="title-ref">overwrite\_input</span> option the input array will now only be partially sorted instead of fully sorted.

### Fix to financial.npv

The npv function had a bug. Contrary to what the documentation stated, it summed from indexes `1` to `M` instead of from `0` to `M - 1`. The fix changes the returned value. The mirr function called the npv function, but worked around the problem, so that was also fixed and the return value of the mirr function remains unchanged.

### Runtime warnings when comparing NaN numbers

Comparing `NaN` floating point numbers now raises the `invalid` runtime warning. If a `NaN` is expected the warning can be ignored using np.errstate. E.g.:

    with np.errstate(invalid='ignore'):
        operation()

## New Features

### Support for linear algebra on stacked arrays

The gufunc machinery is now used for np.linalg, allowing operations on stacked arrays and vectors. For example:

    >>> a
    array([[[ 1.,  1.],
            [ 0.,  1.]],
    
           [[ 1.,  1.],
            [ 0.,  1.]]])
    
    >>> np.linalg.inv(a)
    array([[[ 1., -1.],
            [ 0.,  1.]],
    
           [[ 1., -1.],
            [ 0.,  1.]]])

### In place fancy indexing for ufuncs

The function `at` has been added to ufunc objects to allow in place ufuncs with no buffering when fancy indexing is used. For example, the following will increment the first and second items in the array, and will increment the third item twice: `numpy.add.at(arr, [0, 1, 2, 2], 1)`

This is what many have mistakenly thought `arr[[0, 1, 2, 2]] += 1` would do, but that does not work as the incremented value of `arr[2]` is simply copied into the third slot in `arr` twice, not incremented twice.

### New functions <span class="title-ref">partition</span> and <span class="title-ref">argpartition</span>

New functions to partially sort arrays via a selection algorithm.

A `partition` by index `k` moves the `k` smallest element to the front of an array. All elements before `k` are then smaller or equal than the value in position `k` and all elements following `k` are then greater or equal than the value in position `k`. The ordering of the values within these bounds is undefined. A sequence of indices can be provided to sort all of them into their sorted position at once iterative partitioning. This can be used to efficiently obtain order statistics like median or percentiles of samples. `partition` has a linear time complexity of `O(n)` while a full sort has `O(n log(n))`.

### New functions <span class="title-ref">nanmean</span>, <span class="title-ref">nanvar</span> and <span class="title-ref">nanstd</span>

New nan aware statistical functions are added. In these functions the results are what would be obtained if nan values were omitted from all computations.

### New functions <span class="title-ref">full</span> and <span class="title-ref">full\_like</span>

New convenience functions to create arrays filled with a specific value; complementary to the existing <span class="title-ref">zeros</span> and <span class="title-ref">zeros\_like</span> functions.

### IO compatibility with large files

Large NPZ files \>2GB can be loaded on 64-bit systems.

### Building against OpenBLAS

It is now possible to build numpy against OpenBLAS by editing site.cfg.

### New constant

Euler's constant is now exposed in numpy as euler\_gamma.

### New modes for qr

New modes 'complete', 'reduced', and 'raw' have been added to the qr factorization and the old 'full' and 'economic' modes are deprecated. The 'reduced' mode replaces the old 'full' mode and is the default as was the 'full' mode, so backward compatibility can be maintained by not specifying the mode.

The 'complete' mode returns a full dimensional factorization, which can be useful for obtaining a basis for the orthogonal complement of the range space. The 'raw' mode returns arrays that contain the Householder reflectors and scaling factors that can be used in the future to apply q without needing to convert to a matrix. The 'economic' mode is simply deprecated, there isn't much use for it and it isn't any more efficient than the 'raw' mode.

### New <span class="title-ref">invert</span> argument to <span class="title-ref">in1d</span>

The function <span class="title-ref">in1d</span> now accepts a <span class="title-ref">invert</span> argument which, when <span class="title-ref">True</span>, causes the returned array to be inverted.

### Advanced indexing using <span class="title-ref">np.newaxis</span>

It is now possible to use <span class="title-ref">np.newaxis</span>/<span class="title-ref">None</span> together with index arrays instead of only in simple indices. This means that `array[np.newaxis, [0, 1]]` will now work as expected and select the first two rows while prepending a new axis to the array.

### C-API

New ufuncs can now be registered with builtin input types and a custom output type. Before this change, NumPy wouldn't be able to find the right ufunc loop function when the ufunc was called from Python, because the ufunc loop signature matching logic wasn't looking at the output operand type. Now the correct ufunc loop is found, as long as the user provides an output argument with the correct output type.

### runtests.py

A simple test runner script `runtests.py` was added. It also builds Numpy via `setup.py build` and can be used to run tests easily during development.

## Improvements

### IO performance improvements

Performance in reading large files was improved by chunking (see also IO compatibility).

### Performance improvements to <span class="title-ref">pad</span>

The <span class="title-ref">pad</span> function has a new implementation, greatly improving performance for all inputs except <span class="title-ref">mode=\<function\></span> (retained for backwards compatibility). Scaling with dimensionality is dramatically improved for rank \>= 4.

### Performance improvements to <span class="title-ref">isnan</span>, <span class="title-ref">isinf</span>, <span class="title-ref">isfinite</span> and <span class="title-ref">byteswap</span>

<span class="title-ref">isnan</span>, <span class="title-ref">isinf</span>, <span class="title-ref">isfinite</span> and <span class="title-ref">byteswap</span> have been improved to take advantage of compiler builtins to avoid expensive calls to libc. This improves performance of these operations by about a factor of two on gnu libc systems.

### Performance improvements via SSE2 vectorization

Several functions have been optimized to make use of SSE2 CPU SIMD instructions.

  -   - Float32 and float64:
        
          - base math (<span class="title-ref">add</span>, <span class="title-ref">subtract</span>, <span class="title-ref">divide</span>, <span class="title-ref">multiply</span>)
          - <span class="title-ref">sqrt</span>
          - <span class="title-ref">minimum/maximum</span>
          - <span class="title-ref">absolute</span>

  -   - Bool:
        
          - <span class="title-ref">logical\_or</span>
          - <span class="title-ref">logical\_and</span>
          - <span class="title-ref">logical\_not</span>

This improves performance of these operations up to 4x/2x for float32/float64 and up to 10x for bool depending on the location of the data in the CPU caches. The performance gain is greatest for in-place operations.

In order to use the improved functions the SSE2 instruction set must be enabled at compile time. It is enabled by default on x86\_64 systems. On x86\_32 with a capable CPU it must be enabled by passing the appropriate flag to the CFLAGS build variable (-msse2 with gcc).

### Performance improvements to <span class="title-ref">median</span>

<span class="title-ref">median</span> is now implemented in terms of <span class="title-ref">partition</span> instead of <span class="title-ref">sort</span> which reduces its time complexity from O(n log(n)) to O(n). If used with the <span class="title-ref">overwrite\_input</span> option the array will now only be partially sorted instead of fully sorted.

### Overridable operand flags in ufunc C-API

When creating a ufunc, the default ufunc operand flags can be overridden via the new op\_flags attribute of the ufunc object. For example, to set the operand flag for the first input to read/write:

PyObject \*ufunc = PyUFunc\_FromFuncAndData(...); ufunc-\>op\_flags\[0\] = NPY\_ITER\_READWRITE;

This allows a ufunc to perform an operation in place. Also, global nditer flags can be overridden via the new iter\_flags attribute of the ufunc object. For example, to set the reduce flag for a ufunc:

ufunc-\>iter\_flags = NPY\_ITER\_REDUCE\_OK;

## Changes

### General

The function np.take now allows 0-d arrays as indices.

The separate compilation mode is now enabled by default.

Several changes to np.insert and np.delete:

  - Previously, negative indices and indices that pointed past the end of the array were simply ignored. Now, this will raise a Future or Deprecation Warning. In the future they will be treated like normal indexing treats them -- negative indices will wrap around, and out-of-bound indices will generate an error.
  - Previously, boolean indices were treated as if they were integers (always referring to either the 0th or 1st item in the array). In the future, they will be treated as masks. In this release, they raise a FutureWarning warning of this coming change.
  - In Numpy 1.7. np.insert already allowed the syntax <span class="title-ref">np.insert(arr, 3, \[1,2,3\])</span> to insert multiple items at a single position. In Numpy 1.8. this is also possible for <span class="title-ref">np.insert(arr, \[3\], \[1, 2, 3\])</span>.

Padded regions from np.pad are now correctly rounded, not truncated.

### C-API Array Additions

Four new functions have been added to the array C-API.

  - PyArray\_Partition
  - PyArray\_ArgPartition
  - PyArray\_SelectkindConverter
  - PyDataMem\_NEW\_ZEROED

### C-API Ufunc Additions

One new function has been added to the ufunc C-API that allows to register an inner loop for user types using the descr.

  - PyUFunc\_RegisterLoopForDescr

### C-API Developer Improvements

The `PyArray_Type` instance creation function `tp_new` now uses `tp_basicsize` to determine how much memory to allocate. In previous releases only `sizeof(PyArrayObject)` bytes of memory were allocated, often requiring C-API subtypes to reimplement `tp_new`.

## Deprecations

The 'full' and 'economic' modes of qr factorization are deprecated.

### General

The use of non-integer for indices and most integer arguments has been deprecated. Previously float indices and function arguments such as axes or shapes were truncated to integers without warning. For example <span class="title-ref">arr.reshape(3., -1)</span> or <span class="title-ref">arr\[0.\]</span> will trigger a deprecation warning in NumPy 1.8., and in some future version of NumPy they will raise an error.

## Authors

This release contains work by the following people who contributed at least one patch to this release. The names are in alphabetical order by first name:

  - 87
  - Adam Ginsburg +
  - Adam Griffiths +
  - Alexander Belopolsky +
  - Alex Barth +
  - Alex Ford +
  - Andreas Hilboll +
  - Andreas Kloeckner +
  - Andreas Schwab +
  - Andrew Horton +
  - argriffing +
  - Arink Verma +
  - Bago Amirbekian +
  - Bartosz Telenczuk +
  - bebert218 +
  - Benjamin Root +
  - Bill Spotz +
  - Bradley M. Froehle
  - Carwyn Pelley +
  - Charles Harris
  - Chris
  - Christian Brueffer +
  - Christoph Dann +
  - Christoph Gohlke
  - Dan Hipschman +
  - Daniel +
  - Dan Miller +
  - daveydave400 +
  - David Cournapeau
  - David Warde-Farley
  - Denis Laxalde
  - dmuellner +
  - Edward Catmur +
  - Egor Zindy +
  - endolith
  - Eric Firing
  - Eric Fode
  - Eric Moore +
  - Eric Price +
  - Fazlul Shahriar +
  - FÃ©lix Hartmann +
  - Fernando Perez
  - Frank B +
  - Frank Breitling +
  - Frederic
  - Gabriel
  - GaelVaroquaux
  - Guillaume Gay +
  - Han Genuit
  - HaroldMills +
  - hklemm +
  - jamestwebber +
  - Jason Madden +
  - Jay Bourque
  - jeromekelleher +
  - JesÃºs GÃ³mez +
  - jmozmoz +
  - jnothman +
  - Johannes SchÃ¶nberger +
  - John Benediktsson +
  - John Salvatier +
  - John Stechschulte +
  - Jonathan Waltman +
  - Joon Ro +
  - Jos de Kloe +
  - Joseph Martinot-Lagarde +
  - Josh Warner (Mac) +
  - Jostein BÃ¸ FlÃ¸ystad +
  - Juan Luis Cano RodrÃ­guez +
  - Julian Taylor +
  - Julien Phalip +
  - K.-Michael Aye +
  - Kumar Appaiah +
  - Lars Buitinck
  - Leon Weber +
  - Luis Pedro Coelho
  - Marcin Juszkiewicz
  - Mark Wiebe
  - Marten van Kerkwijk +
  - Martin Baeuml +
  - Martin Spacek
  - Martin Teichmann +
  - Matt Davis +
  - Matthew Brett
  - Maximilian Albert +
  - m-d-w +
  - Michael Droettboom
  - mwtoews +
  - Nathaniel J. Smith
  - Nicolas Scheffer +
  - Nils Werner +
  - ochoadavid +
  - OndÅ™ej ÄŒertÃ­k
  - ovillellas +
  - Paul Ivanov
  - Pauli Virtanen
  - peterjc
  - Ralf Gommers
  - Raul Cota +
  - Richard Hattersley +
  - Robert Costa +
  - Robert Kern
  - Rob Ruana +
  - Ronan Lamy
  - Sandro Tosi
  - Sascha Peilicke +
  - Sebastian Berg
  - Skipper Seabold
  - Stefan van der Walt
  - Steve +
  - Takafumi Arakaki +
  - Thomas Robitaille +
  - Tomas Tomecek +
  - Travis E. Oliphant
  - Valentin Haenel
  - Vladimir Rutsky +
  - Warren Weckesser
  - Yaroslav Halchenko
  - Yury V. Zaytsev +

A total of 119 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

---

1.8.1-notes.md

---

# NumPy 1.8.1 Release Notes

This is a bugfix only release in the 1.8.x series.

## Issues fixed

  - gh-4276: Fix mean, var, std methods for object arrays
  - gh-4262: remove insecure mktemp usage
  - gh-2385: absolute(complex(inf)) raises invalid warning in python3
  - gh-4024: Sequence assignment doesn't raise exception on shape mismatch
  - gh-4027: Fix chunked reading of strings longer than BUFFERSIZE
  - gh-4109: Fix object scalar return type of 0-d array indices
  - gh-4018: fix missing check for memory allocation failure in ufuncs
  - gh-4156: high order linalg.norm discards imaginary elements of complex arrays
  - gh-4144: linalg: norm fails on longdouble, signed int
  - gh-4094: fix NaT handling in \_strided\_to\_strided\_string\_to\_datetime
  - gh-4051: fix uninitialized use in \_strided\_to\_strided\_string\_to\_datetime
  - gh-4093: Loading compressed .npz file fails under Python 2.6.6
  - gh-4138: segfault with non-native endian memoryview in python 3.4
  - gh-4123: Fix missing NULL check in lexsort
  - gh-4170: fix native-only long long check in memoryviews
  - gh-4187: Fix large file support on 32 bit
  - gh-4152: fromfile: ensure file handle positions are in sync in python3
  - gh-4176: clang compatibility: Typos in conversion\_utils
  - gh-4223: Fetching a non-integer item caused array return
  - gh-4197: fix minor memory leak in memoryview failure case
  - gh-4206: fix build with single-threaded python
  - gh-4220: add versionadded:: 1.8.0 to ufunc.at docstring
  - gh-4267: improve handling of memory allocation failure
  - gh-4267: fix use of capi without gil in ufunc.at
  - gh-4261: Detect vendor versions of GNU Compilers
  - gh-4253: IRR was returning nan instead of valid negative answer
  - gh-4254: fix unnecessary byte order flag change for byte arrays
  - gh-3263: numpy.random.shuffle clobbers mask of a MaskedArray
  - gh-4270: np.random.shuffle not work with flexible dtypes
  - gh-3173: Segmentation fault when 'size' argument to random.multinomial
  - gh-2799: allow using unique with lists of complex
  - gh-3504: fix linspace truncation for integer array scalar
  - gh-4191: get\_info('openblas') does not read libraries key
  - gh-3348: Access violation in \_descriptor\_from\_pep3118\_format
  - gh-3175: segmentation fault with numpy.array() from bytearray
  - gh-4266: histogramdd - wrong result for entries very close to last boundary
  - gh-4408: Fix stride\_stricks.as\_strided function for object arrays
  - gh-4225: fix log1p and exmp1 return for np.inf on windows compiler builds
  - gh-4359: Fix infinite recursion in str.format of flex arrays
  - gh-4145: Incorrect shape of broadcast result with the exponent operator
  - gh-4483: Fix commutativity of {dot,multiply,inner}(scalar, matrix\_of\_objs)
  - gh-4466: Delay npyiter size check when size may change
  - gh-4485: Buffered stride was erroneously marked fixed
  - gh-4354: byte\_bounds fails with datetime dtypes
  - gh-4486: segfault/error converting from/to high-precision datetime64 objects
  - gh-4428: einsum(None, None, None, None) causes segfault
  - gh-4134: uninitialized use for for size 1 object reductions

## Changes

### NDIter

When `NpyIter_RemoveAxis` is now called, the iterator range will be reset.

When a multi index is being tracked and an iterator is not buffered, it is possible to use `NpyIter_RemoveAxis`. In this case an iterator can shrink in size. Because the total size of an iterator is limited, the iterator may be too large before these calls. In this case its size will be set to `-1` and an error issued not at construction time but when removing the multi index, setting the iterator range, or getting the next function.

This has no effect on currently working code, but highlights the necessity of checking for an error return if these conditions can occur. In most cases the arrays being iterated are as large as the iterator so that such a problem cannot occur.

### Optional reduced verbosity for np.distutils

Set `numpy.distutils.system_info.system_info.verbosity = 0` and then calls to `numpy.distutils.system_info.get_info('blas_opt')` will not print anything on the output. This is mostly for other packages using numpy.distutils.

## Deprecations

### C-API

The utility function npy\_PyFile\_Dup and npy\_PyFile\_DupClose are broken by the internal buffering python 3 applies to its file objects. To fix this two new functions npy\_PyFile\_Dup2 and npy\_PyFile\_DupClose2 are declared in npy\_3kcompat.h and the old functions are deprecated. Due to the fragile nature of these functions it is recommended to instead use the python API when possible.

---

1.8.2-notes.md

---

# NumPy 1.8.2 Release Notes

This is a bugfix only release in the 1.8.x series.

## Issues fixed

  - gh-4836: partition produces wrong results for multiple selections in equal ranges
  - gh-4656: Make fftpack.\_raw\_fft threadsafe
  - gh-4628: incorrect argument order to \_copyto in in np.nanmax, np.nanmin
  - gh-4642: Hold GIL for converting dtypes types with fields
  - gh-4733: fix np.linalg.svd(b, compute\_uv=False)
  - gh-4853: avoid unaligned simd load on reductions on i386
  - gh-4722: Fix seg fault converting empty string to object
  - gh-4613: Fix lack of NULL check in array\_richcompare
  - gh-4774: avoid unaligned access for strided byteswap
  - gh-650: Prevent division by zero when creating arrays from some buffers
  - gh-4602: ifort has issues with optimization flag O2, use O1

---

1.9.0-notes.md

---

# NumPy 1.9.0 Release Notes

This release supports Python 2.6 - 2.7 and 3.2 - 3.4.

## Highlights

  - Numerous performance improvements in various areas, most notably indexing and operations on small arrays are significantly faster. Indexing operations now also release the GIL.
  - Addition of <span class="title-ref">nanmedian</span> and <span class="title-ref">nanpercentile</span> rounds out the nanfunction set.

## Dropped Support

  - The oldnumeric and numarray modules have been removed.
  - The doc/pyrex and doc/cython directories have been removed.
  - The doc/numpybook directory has been removed.
  - The numpy/testing/numpytest.py file has been removed together with the importall function it contained.

## Future Changes

  - The numpy/polynomial/polytemplate.py file will be removed in NumPy 1.10.0.
  - Default casting for inplace operations will change to 'same\_kind' in Numpy 1.10.0. This will certainly break some code that is currently ignoring the warning.
  - Relaxed stride checking will be the default in 1.10.0
  - String version checks will break because, e.g., '1.9' \> '1.10' is True. A NumpyVersion class has been added that can be used for such comparisons.
  - The diagonal and diag functions will return writeable views in 1.10.0
  - The <span class="title-ref">S</span> and/or <span class="title-ref">a</span> dtypes may be changed to represent Python strings instead of bytes, in Python 3 these two types are very different.

## Compatibility notes

### The diagonal and diag functions return readonly views.

In NumPy 1.8, the diagonal and diag functions returned readonly copies, in NumPy 1.9 they return readonly views, and in 1.10 they will return writeable views.

### Special scalar float values don't cause upcast to double anymore

In previous numpy versions operations involving floating point scalars containing special values `NaN`, `Inf` and `-Inf` caused the result type to be at least `float64`. As the special values can be represented in the smallest available floating point type, the upcast is not performed anymore.

For example the dtype of:

> `np.array([1.], dtype=np.float32) * float('nan')`

now remains `float32` instead of being cast to `float64`. Operations involving non-special values have not been changed.

### Percentile output changes

If given more than one percentile to compute numpy.percentile returns an array instead of a list. A single percentile still returns a scalar. The array is equivalent to converting the list returned in older versions to an array via `np.array`.

If the `overwrite_input` option is used the input is only partially instead of fully sorted.

### ndarray.tofile exception type

All `tofile` exceptions are now `IOError`, some were previously `ValueError`.

### Invalid fill value exceptions

Two changes to numpy.ma.core.\_check\_fill\_value:

  - When the fill value is a string and the array type is not one of 'OSUV', TypeError is raised instead of the default fill value being used.
  - When the fill value overflows the array type, TypeError is raised instead of OverflowError.

### Polynomial Classes no longer derived from PolyBase

This may cause problems with folks who depended on the polynomial classes being derived from PolyBase. They are now all derived from the abstract base class ABCPolyBase. Strictly speaking, there should be a deprecation involved, but no external code making use of the old baseclass could be found.

### Using numpy.random.binomial may change the RNG state vs. numpy \< 1.9

A bug in one of the algorithms to generate a binomial random variate has been fixed. This change will likely alter the number of random draws performed, and hence the sequence location will be different after a call to distribution.c::rk\_binomial\_btpe. Any tests which rely on the RNG being in a known state should be checked and/or updated as a result.

### Random seed enforced to be a 32 bit unsigned integer

`np.random.seed` and `np.random.RandomState` now throw a `ValueError` if the seed cannot safely be converted to 32 bit unsigned integers. Applications that now fail can be fixed by masking the higher 32 bit values to zero: `seed = seed & 0xFFFFFFFF`. This is what is done silently in older versions so the random stream remains the same.

### Argmin and argmax out argument

The `out` argument to `np.argmin` and `np.argmax` and their equivalent C-API functions is now checked to match the desired output shape exactly. If the check fails a `ValueError` instead of `TypeError` is raised.

### Einsum

Remove unnecessary broadcasting notation restrictions. `np.einsum('ijk,j->ijk', A, B)` can also be written as `np.einsum('ij...,j->ij...', A, B)` (ellipsis is no longer required on 'j')

### Indexing

The NumPy indexing has seen a complete rewrite in this version. This makes most advanced integer indexing operations much faster and should have no other implications. However some subtle changes and deprecations were introduced in advanced indexing operations:

  - Boolean indexing into scalar arrays will always return a new 1-d array. This means that `array(1)[array(True)]` gives `array([1])` and not the original array.
  - Advanced indexing into one dimensional arrays used to have (undocumented) special handling regarding repeating the value array in assignments when the shape of the value array was too small or did not match. Code using this will raise an error. For compatibility you can use `arr.flat[index] = values`, which uses the old code branch. (for example `a = np.ones(10); a[np.arange(10)] = [1, 2, 3]`)
  - The iteration order over advanced indexes used to be always C-order. In NumPy 1.9. the iteration order adapts to the inputs and is not guaranteed (with the exception of a *single* advanced index which is never reversed for compatibility reasons). This means that the result is undefined if multiple values are assigned to the same element. An example for this is `arr[[0, 0], [1, 1]] = [1, 2]`, which may set `arr[0, 1]` to either 1 or 2.
  - Equivalent to the iteration order, the memory layout of the advanced indexing result is adapted for faster indexing and cannot be predicted.
  - All indexing operations return a view or a copy. No indexing operation will return the original array object. (For example `arr[...]`)
  - In the future Boolean array-likes (such as lists of python bools) will always be treated as Boolean indexes and Boolean scalars (including python `True`) will be a legal *boolean* index. At this time, this is already the case for scalar arrays to allow the general `positive = a[a > 0]` to work when `a` is zero dimensional.
  - In NumPy 1.8 it was possible to use `array(True)` and `array(False)` equivalent to 1 and 0 if the result of the operation was a scalar. This will raise an error in NumPy 1.9 and, as noted above, treated as a boolean index in the future.
  - All non-integer array-likes are deprecated, object arrays of custom integer like objects may have to be cast explicitly.
  - The error reporting for advanced indexing is more informative, however the error type has changed in some cases. (Broadcasting errors of indexing arrays are reported as `IndexError`)
  - Indexing with more then one ellipsis (`...`) is deprecated.

### Non-integer reduction axis indexes are deprecated

Non-integer axis indexes to reduction ufuncs like <span class="title-ref">add.reduce</span> or <span class="title-ref">sum</span> are deprecated.

### `promote_types` and string dtype

`promote_types` function now returns a valid string length when given an integer or float dtype as one argument and a string dtype as another argument. Previously it always returned the input string dtype, even if it wasn't long enough to store the max integer/float value converted to a string.

### `can_cast` and string dtype

`can_cast` function now returns False in "safe" casting mode for integer/float dtype and string dtype if the string dtype length is not long enough to store the max integer/float value converted to a string. Previously `can_cast` in "safe" mode returned True for integer/float dtype and a string dtype of any length.

### astype and string dtype

The `astype` method now returns an error if the string dtype to cast to is not long enough in "safe" casting mode to hold the max value of integer/float array that is being casted. Previously the casting was allowed even if the result was truncated.

### <span class="title-ref">npyio.recfromcsv</span> keyword arguments change

<span class="title-ref">npyio.recfromcsv</span> no longer accepts the undocumented <span class="title-ref">update</span> keyword, which used to override the <span class="title-ref">dtype</span> keyword.

### The `doc/swig` directory moved

The `doc/swig` directory has been moved to `tools/swig`.

### The `npy_3kcompat.h` header changed

The unused `simple_capsule_dtor` function has been removed from `npy_3kcompat.h`. Note that this header is not meant to be used outside of numpy; other projects should be using their own copy of this file when needed.

### Negative indices in C-Api `sq_item` and `sq_ass_item` sequence methods

When directly accessing the `sq_item` or `sq_ass_item` PyObject slots for item getting, negative indices will not be supported anymore. `PySequence_GetItem` and `PySequence_SetItem` however fix negative indices so that they can be used there.

### NDIter

When `NpyIter_RemoveAxis` is now called, the iterator range will be reset.

When a multi index is being tracked and an iterator is not buffered, it is possible to use `NpyIter_RemoveAxis`. In this case an iterator can shrink in size. Because the total size of an iterator is limited, the iterator may be too large before these calls. In this case its size will be set to `-1` and an error issued not at construction time but when removing the multi index, setting the iterator range, or getting the next function.

This has no effect on currently working code, but highlights the necessity of checking for an error return if these conditions can occur. In most cases the arrays being iterated are as large as the iterator so that such a problem cannot occur.

This change was already applied to the 1.8.1 release.

### `zeros_like` for string dtypes now returns empty strings

To match the <span class="title-ref">zeros</span> function <span class="title-ref">zeros\_like</span> now returns an array initialized with empty strings instead of an array filled with <span class="title-ref">'0'</span>.

## New Features

### Percentile supports more interpolation options

`np.percentile` now has the interpolation keyword argument to specify in which way points should be interpolated if the percentiles fall between two values. See the documentation for the available options.

### Generalized axis support for median and percentile

`np.median` and `np.percentile` now support generalized axis arguments like ufunc reductions do since 1.7. One can now say axis=(index, index) to pick a list of axes for the reduction. The `keepdims` keyword argument was also added to allow convenient broadcasting to arrays of the original shape.

### Dtype parameter added to `np.linspace` and `np.logspace`

The returned data type from the `linspace` and `logspace` functions can now be specified using the dtype parameter.

### More general `np.triu` and `np.tril` broadcasting

For arrays with `ndim` exceeding 2, these functions will now apply to the final two axes instead of raising an exception.

### `tobytes` alias for `tostring` method

`ndarray.tobytes` and `MaskedArray.tobytes` have been added as aliases for `tostring` which exports arrays as `bytes`. This is more consistent in Python 3 where `str` and `bytes` are not the same.

### Build system

Added experimental support for the ppc64le and OpenRISC architecture.

### Compatibility to python `numbers` module

All numerical numpy types are now registered with the type hierarchy in the python `numbers` module.

### `increasing` parameter added to `np.vander`

The ordering of the columns of the Vandermonde matrix can be specified with this new boolean argument.

### `unique_counts` parameter added to `np.unique`

The number of times each unique item comes up in the input can now be obtained as an optional return value.

### Support for median and percentile in nanfunctions

The `np.nanmedian` and `np.nanpercentile` functions behave like the median and percentile functions except that NaNs are ignored.

### NumpyVersion class added

The class may be imported from numpy.lib and can be used for version comparison when the numpy version goes to 1.10.devel. For example:

    >>> from numpy.lib import NumpyVersion
    >>> if NumpyVersion(np.__version__) < '1.10.0'):
    ...     print('Wow, that is an old NumPy version!')

### Allow saving arrays with large number of named columns

The numpy storage format 1.0 only allowed the array header to have a total size of 65535 bytes. This can be exceeded by structured arrays with a large number of columns. A new format 2.0 has been added which extends the header size to 4 GiB. <span class="title-ref">np.save</span> will automatically save in 2.0 format if the data requires it, else it will always use the more compatible 1.0 format.

### Full broadcasting support for `np.cross`

`np.cross` now properly broadcasts its two input arrays, even if they have different number of dimensions. In earlier versions this would result in either an error being raised, or wrong results computed.

## Improvements

### Better numerical stability for sum in some cases

Pairwise summation is now used in the sum method, but only along the fast axis and for groups of the values \<= 8192 in length. This should also improve the accuracy of var and std in some common cases.

### Percentile implemented in terms of `np.partition`

`np.percentile` has been implemented in terms of `np.partition` which only partially sorts the data via a selection algorithm. This improves the time complexity from `O(nlog(n))` to `O(n)`.

### Performance improvement for `np.array`

The performance of converting lists containing arrays to arrays using `np.array` has been improved. It is now equivalent in speed to `np.vstack(list)`.

### Performance improvement for `np.searchsorted`

For the built-in numeric types, `np.searchsorted` no longer relies on the data type's `compare` function to perform the search, but is now implemented by type specific functions. Depending on the size of the inputs, this can result in performance improvements over 2x.

### Optional reduced verbosity for np.distutils

Set `numpy.distutils.system_info.system_info.verbosity = 0` and then calls to `numpy.distutils.system_info.get_info('blas_opt')` will not print anything on the output. This is mostly for other packages using numpy.distutils.

### Covariance check in `np.random.multivariate_normal`

A `RuntimeWarning` warning is raised when the covariance matrix is not positive-semidefinite.

### Polynomial Classes no longer template based

The polynomial classes have been refactored to use an abstract base class rather than a template in order to implement a common interface. This makes importing the polynomial package faster as the classes do not need to be compiled on import.

### More GIL releases

Several more functions now release the Global Interpreter Lock allowing more efficient parallelization using the `threading` module. Most notably the GIL is now released for fancy indexing, `np.where` and the `random` module now uses a per-state lock instead of the GIL.

### MaskedArray support for more complicated base classes

Built-in assumptions that the baseclass behaved like a plain array are being removed. In particular, `repr` and `str` should now work more reliably.

### C-API

## Deprecations

### Non-integer scalars for sequence repetition

Using non-integer numpy scalars to repeat python sequences is deprecated. For example `np.float_(2) * [1]` will be an error in the future.

### `select` input deprecations

The integer and empty input to `select` is deprecated. In the future only boolean arrays will be valid conditions and an empty `condlist` will be considered an input error instead of returning the default.

### `rank` function

The `rank` function has been deprecated to avoid confusion with `numpy.linalg.matrix_rank`.

### Object array equality comparisons

In the future object array comparisons both <span class="title-ref">==</span> and <span class="title-ref">np.equal</span> will not make use of identity checks anymore. For example:

\>\>\> a = np.array(\[np.array(\[1, 2, 3\]), 1\]) \>\>\> b = np.array(\[np.array(\[1, 2, 3\]), 1\]) \>\>\> a == b

will consistently return False (and in the future an error) even if the array in <span class="title-ref">a</span> and <span class="title-ref">b</span> was the same object.

The equality operator <span class="title-ref">==</span> will in the future raise errors like <span class="title-ref">np.equal</span> if broadcasting or element comparisons, etc. fails.

Comparison with <span class="title-ref">arr == None</span> will in the future do an elementwise comparison instead of just returning False. Code should be using <span class="title-ref">arr is None</span>.

All of these changes will give Deprecation- or FutureWarnings at this time.

### C-API

The utility function npy\_PyFile\_Dup and npy\_PyFile\_DupClose are broken by the internal buffering python 3 applies to its file objects. To fix this two new functions npy\_PyFile\_Dup2 and npy\_PyFile\_DupClose2 are declared in npy\_3kcompat.h and the old functions are deprecated. Due to the fragile nature of these functions it is recommended to instead use the python API when possible.

This change was already applied to the 1.8.1 release.

---

1.9.1-notes.md

---

# NumPy 1.9.1 Release Notes

This is a bugfix only release in the 1.9.x series.

## Issues fixed

  - gh-5184: restore linear edge behaviour of gradient to as it was in \< 1.9. The second order behaviour is available via the <span class="title-ref">edge\_order</span> keyword
  - gh-4007: workaround Accelerate sgemv crash on OSX 10.9
  - gh-5100: restore object dtype inference from iterable objects without <span class="title-ref">len()</span>
  - gh-5163: avoid gcc-4.1.2 (red hat 5) miscompilation causing a crash
  - gh-5138: fix nanmedian on arrays containing inf
  - gh-5240: fix not returning out array from ufuncs with subok=False set
  - gh-5203: copy inherited masks in MaskedArray.\_\_array\_finalize\_\_
  - gh-2317: genfromtxt did not handle filling\_values=0 correctly
  - gh-5067: restore api of npy\_PyFile\_DupClose in python2
  - gh-5063: cannot convert invalid sequence index to tuple
  - gh-5082: Segmentation fault with argmin() on unicode arrays
  - gh-5095: don't propagate subtypes from np.where
  - gh-5104: np.inner segfaults with SciPy's sparse matrices
  - gh-5251: Issue with fromarrays not using correct format for unicode arrays
  - gh-5136: Import dummy\_threading if importing threading fails
  - gh-5148: Make numpy import when run with Python flag '-OO'
  - gh-5147: Einsum double contraction in particular order causes ValueError
  - gh-479: Make f2py work with intent(in out)
  - gh-5170: Make python2 .npy files readable in python3
  - gh-5027: Use 'll' as the default length specifier for long long
  - gh-4896: fix build error with MSVC 2013 caused by C99 complex support
  - gh-4465: Make PyArray\_PutTo respect writeable flag
  - gh-5225: fix crash when using arange on datetime without dtype set
  - gh-5231: fix build in c99 mode

---

1.9.2-notes.md

---

# NumPy 1.9.2 Release Notes

This is a bugfix only release in the 1.9.x series.

## Issues fixed

  - [\#5316](https://github.com/numpy/numpy/issues/5316): fix too large dtype alignment of strings and complex types
  - [\#5424](https://github.com/numpy/numpy/issues/5424): fix ma.median when used on ndarrays
  - [\#5481](https://github.com/numpy/numpy/issues/5481): Fix astype for structured array fields of different byte order
  - [\#5354](https://github.com/numpy/numpy/issues/5354): fix segfault when clipping complex arrays
  - [\#5524](https://github.com/numpy/numpy/issues/5524): allow np.argpartition on non ndarrays
  - [\#5612](https://github.com/numpy/numpy/issues/5612): Fixes ndarray.fill to accept full range of uint64
  - [\#5155](https://github.com/numpy/numpy/issues/5155): Fix loadtxt with comments=None and a string None data
  - [\#4476](https://github.com/numpy/numpy/issues/4476): Masked array view fails if structured dtype has datetime component
  - [\#5388](https://github.com/numpy/numpy/issues/5388): Make RandomState.set\_state and RandomState.get\_state threadsafe
  - [\#5390](https://github.com/numpy/numpy/issues/5390): make seed, randint and shuffle threadsafe
  - [\#5374](https://github.com/numpy/numpy/issues/5374): Fixed incorrect assert\_array\_almost\_equal\_nulp documentation
  - [\#5393](https://github.com/numpy/numpy/issues/5393): Add support for ATLAS \> 3.9.33.
  - [\#5313](https://github.com/numpy/numpy/issues/5313): PyArray\_AsCArray caused segfault for 3d arrays
  - [\#5492](https://github.com/numpy/numpy/issues/5492): handle out of memory in rfftf
  - [\#4181](https://github.com/numpy/numpy/issues/4181): fix a few bugs in the random.pareto docstring
  - [\#5359](https://github.com/numpy/numpy/issues/5359): minor changes to linspace docstring
  - [\#4723](https://github.com/numpy/numpy/issues/4723): fix a compile issues on AIX

---

2.0.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.0.0 Release Notes

NumPy 2.0.0 is the first major release since 2006. It is the result of 11 months of development since the last feature release and is the work of 212 contributors spread over 1078 pull requests. It contains a large number of exciting new features as well as changes to both the Python and C APIs.

This major release includes breaking changes that could not happen in a regular minor (feature) release - including an ABI break, changes to type promotion rules, and API changes which may not have been emitting deprecation warnings in 1.26.x. Key documents related to how to adapt to changes in NumPy 2.0, in addition to these release notes, include:

  - The \[numpy-2-migration-guide\](\#numpy-2-migration-guide)
  - The \[NumPy 2.0-specific advice \<numpy-2-abi-handling\>\](\#numpy-2.0-specific-advice-\<numpy-2-abi-handling\>) in \[for-downstream-package-authors\](\#for-downstream-package-authors)

## Highlights

Highlights of this release include:

  - New features:
      - A new variable-length string dtype, <span class="title-ref">\~numpy.dtypes.StringDType</span> and a new <span class="title-ref">numpy.strings</span> namespace with performant ufuncs for string operations,
      - Support for `float32` and `longdouble` in all <span class="title-ref">numpy.fft</span> functions,
      - Support for the array API standard in the main `numpy` namespace.
  - Performance improvements:
      - Sorting functions (<span class="title-ref">sort</span>, <span class="title-ref">argsort</span>, <span class="title-ref">partition</span>, <span class="title-ref">argpartition</span>) have been accelerated through the use of the Intel x86-simd-sort and Google Highway libraries, and may see large (hardware-specific) speedups,
      - macOS Accelerate support and binary wheels for macOS \>=14, with significant performance improvements for linear algebra operations on macOS, and wheels that are about 3 times smaller,
      - <span class="title-ref">numpy.char</span> fixed-length string operations have been accelerated by implementing ufuncs that also support <span class="title-ref">\~numpy.dtypes.StringDType</span> in addition to the fixed-length string dtypes,
      - A new tracing and introspection API, <span class="title-ref">\~numpy.lib.introspect.opt\_func\_info</span>, to determine which hardware-specific kernels are available and will be dispatched to.
      - <span class="title-ref">numpy.save</span> now uses pickle protocol version 4 for saving arrays with object dtype, which allows for pickle objects larger than 4GB and improves saving speed by about 5% for large arrays.
  - Python API improvements:
      - A clear split between public and private API, with a new \[module structure \<module-structure\>\](\#module-structure-\<module-structure\>), and each public function now available in a single place,
      - Many removals of non-recommended functions and aliases. This should make it easier to learn and use NumPy. The number of objects in the main namespace decreased by \~10% and in `numpy.lib` by \~80%,
      - \[Canonical dtype names \<canonical-python-and-c-types\>\](\#canonical-dtype-names-\<canonical-python-and-c-types\>) and a new <span class="title-ref">\~numpy.isdtype</span> introspection function,
  - C API improvements:
      - A new \[public C API for creating custom dtypes \<dtype-api\>\](\#public-c-api-for-creating-custom-dtypes-\<dtype-api\>),
      - Many outdated functions and macros removed, and private internals hidden to ease future extensibility,
      - New, easier to use, initialization functions: :c\`PyArray\_ImportNumPyAPI\` and :c\`PyUFunc\_ImportUFuncAPI\`.

\- Improved behavior:

>   - Improvements to type promotion behavior was changed by adopting \[NEP 50 \<NEP50\>\](\#nep

  - \----50-\<nep50\>). This fixes many user surprises about promotions which  
    previously often depended on data values of input arrays rather than only their dtypes. Please see the NEP and the \[numpy-2-migration-guide\](\#numpy-2-migration-guide) for details as this change can lead to changes in output dtypes and lower precision results for mixed-dtype operations. - The default integer type on Windows is now `int64` rather than `int32`, matching the behavior on other platforms, - The maximum number of array dimensions is changed from 32 to 64

<!-- end list -->

  - Documentation:
      - The reference guide navigation was significantly improved, and there is now documentation on NumPy's \[module structure \<module-structure\>\](\#module-structure-\<module-structure\>),
      - The \[building from source \<building-from-source\>\](\#building-from-source-\<building-from-source\>) documentation was completely rewritten,

Furthermore there are many changes to NumPy internals, including continuing to migrate code from C to C++, that will make it easier to improve and maintain NumPy in the future.

The "no free lunch" theorem dictates that there is a price to pay for all these API and behavior improvements and better future extensibility. This price is:

1.  Backwards compatibility. There are a significant number of breaking changes to both the Python and C APIs. In the majority of cases, there are clear error messages that will inform the user how to adapt their code. However, there are also changes in behavior for which it was not possible to give such an error message - these cases are all covered in the Deprecation and Compatibility sections below, and in the \[numpy-2-migration-guide\](\#numpy-2-migration-guide).
    
    Note that there is a `ruff` mode to auto-fix many things in Python code.

2.  Breaking changes to the NumPy ABI. As a result, binaries of packages that use the NumPy C API and were built against a NumPy 1.xx release will not work with NumPy 2.0. On import, such packages will see an `ImportError` with a message about binary incompatibility.
    
    It is possible to build binaries against NumPy 2.0 that will work at runtime with both NumPy 2.0 and 1.x. See \[numpy-2-abi-handling\](\#numpy-2-abi-handling) for more details.
    
    **All downstream packages that depend on the NumPy ABI are advised to do a new release built against NumPy 2.0 and verify that that release works with both 2.0 and 1.26 - ideally in the period between 2.0.0rc1 (which will be ABI-stable) and the final 2.0.0 release to avoid problems for their users.**

The Python versions supported by this release are 3.9-3.12.

## NumPy 2.0 Python API removals

  - `np.geterrobj`, `np.seterrobj` and the related ufunc keyword argument `extobj=` have been removed. The preferred replacement for all of these is using the context manager `with np.errstate():`.
    
    ([gh-23922](https://github.com/numpy/numpy/pull/23922))

  - `np.cast` has been removed. The literal replacement for `np.cast[dtype](arg)` is `np.asarray(arg, dtype=dtype)`.

  - `np.source` has been removed. The preferred replacement is `inspect.getsource`.

  - `np.lookfor` has been removed.
    
    ([gh-24144](https://github.com/numpy/numpy/pull/24144))

  - `numpy.who` has been removed. As an alternative for the removed functionality, one can use a variable explorer that is available in IDEs such as Spyder or Jupyter Notebook.
    
    ([gh-24321](https://github.com/numpy/numpy/pull/24321))

  - Warnings and exceptions present in <span class="title-ref">numpy.exceptions</span> (e.g, <span class="title-ref">\~numpy.exceptions.ComplexWarning</span>, <span class="title-ref">\~numpy.exceptions.VisibleDeprecationWarning</span>) are no longer exposed in the main namespace.

  - Multiple niche enums, expired members and functions have been removed from the main namespace, such as: `ERR_*`, `SHIFT_*`, `np.fastCopyAndTranspose`, `np.kernel_version`, `np.numarray`, `np.oldnumeric` and `np.set_numeric_ops`.
    
    ([gh-24316](https://github.com/numpy/numpy/pull/24316))

  - Replaced `from ... import *` in the `numpy/__init__.py` with explicit imports. As a result, these main namespace members got removed: `np.FLOATING_POINT_SUPPORT`, `np.FPE_*`, `np.NINF`, `np.PINF`, `np.NZERO`, `np.PZERO`, `np.CLIP`, `np.WRAP`, `np.WRAP`, `np.RAISE`, `np.BUFSIZE`, `np.UFUNC_BUFSIZE_DEFAULT`, `np.UFUNC_PYVALS_NAME`, `np.ALLOW_THREADS`, `np.MAXDIMS`, `np.MAY_SHARE_EXACT`, `np.MAY_SHARE_BOUNDS`, `add_newdoc`, `np.add_docstring` and `np.add_newdoc_ufunc`.
    
    ([gh-24357](https://github.com/numpy/numpy/pull/24357))

  - Alias `np.float_` has been removed. Use `np.float64` instead.

  - Alias `np.complex_` has been removed. Use `np.complex128` instead.

  - Alias `np.longfloat` has been removed. Use `np.longdouble` instead.

  - Alias `np.singlecomplex` has been removed. Use `np.complex64` instead.

  - Alias `np.cfloat` has been removed. Use `np.complex128` instead.

  - Alias `np.longcomplex` has been removed. Use `np.clongdouble` instead.

  - Alias `np.clongfloat` has been removed. Use `np.clongdouble` instead.

  - Alias `np.string_` has been removed. Use `np.bytes_` instead.

  - Alias `np.unicode_` has been removed. Use `np.str_` instead.

  - Alias `np.Inf` has been removed. Use `np.inf` instead.

  - Alias `np.Infinity` has been removed. Use `np.inf` instead.

  - Alias `np.NaN` has been removed. Use `np.nan` instead.

  - Alias `np.infty` has been removed. Use `np.inf` instead.

  - Alias `np.mat` has been removed. Use `np.asmatrix` instead.

  - `np.issubclass_` has been removed. Use the `issubclass` builtin instead.

  - `np.asfarray` has been removed. Use `np.asarray` with a proper dtype instead.

  - `np.set_string_function` has been removed. Use `np.set_printoptions` instead with a formatter for custom printing of NumPy objects.

  - `np.tracemalloc_domain` is now only available from `np.lib`.

  - `np.recfromcsv` and `np.recfromtxt` were removed from the main namespace. Use `np.genfromtxt` with comma delimiter instead.

  - `np.issctype`, `np.maximum_sctype`, `np.obj2sctype`, `np.sctype2char`, `np.sctypes`, `np.issubsctype` were all removed from the main namespace without replacement, as they where niche members.

  - Deprecated `np.deprecate` and `np.deprecate_with_doc` has been removed from the main namespace. Use `DeprecationWarning` instead.

  - Deprecated `np.safe_eval` has been removed from the main namespace. Use `ast.literal_eval` instead.
    
    ([gh-24376](https://github.com/numpy/numpy/pull/24376))

  - `np.find_common_type` has been removed. Use `numpy.promote_types` or `numpy.result_type` instead. To achieve semantics for the `scalar_types` argument, use `numpy.result_type` and pass `0`, `0.0`, or `0j` as a Python scalar instead.

  - `np.round_` has been removed. Use `np.round` instead.

  - `np.nbytes` has been removed. Use `np.dtype(<dtype>).itemsize` instead.
    
    ([gh-24477](https://github.com/numpy/numpy/pull/24477))

  - `np.compare_chararrays` has been removed from the main namespace. Use `np.char.compare_chararrays` instead.

  - The `charrarray` in the main namespace has been deprecated. It can be imported without a deprecation warning from `np.char.chararray` for now, but we are planning to fully deprecate and remove `chararray` in the future.

  - `np.format_parser` has been removed from the main namespace. Use `np.rec.format_parser` instead.
    
    ([gh-24587](https://github.com/numpy/numpy/pull/24587))

  - Support for seven data type string aliases has been removed from `np.dtype`: `int0`, `uint0`, `void0`, `object0`, `str0`, `bytes0` and `bool8`.
    
    ([gh-24807](https://github.com/numpy/numpy/pull/24807))

  - The experimental `numpy.array_api` submodule has been removed. Use the main `numpy` namespace for regular usage instead, or the separate `array-api-strict` package for the compliance testing use case for which `numpy.array_api` was mostly used.
    
    ([gh-25911](https://github.com/numpy/numpy/pull/25911))

### `__array_prepare__` is removed

UFuncs called `__array_prepare__` before running computations for normal ufunc calls (not generalized ufuncs, reductions, etc.). The function was also called instead of `__array_wrap__` on the results of some linear algebra functions.

It is now removed. If you use it, migrate to `__array_ufunc__` or rely on `__array_wrap__` which is called with a context in all cases, although only after the result array is filled. In those code paths, `__array_wrap__` will now be passed a base class, rather than a subclass array.

([gh-25105](https://github.com/numpy/numpy/pull/25105))

## Deprecations

  - `np.compat` has been deprecated, as Python 2 is no longer supported.

  - `numpy.int8` and similar classes will no longer support conversion of out of bounds python integers to integer arrays. For example, conversion of 255 to int8 will not return -1. `numpy.iinfo(dtype)` can be used to check the machine limits for data types. For example, `np.iinfo(np.uint16)` returns min = 0 and max = 65535.
    
    `np.array(value).astype(dtype)` will give the desired result.

  - `np.safe_eval` has been deprecated. `ast.literal_eval` should be used instead.
    
    ([gh-23830](https://github.com/numpy/numpy/pull/23830))

  - `np.recfromcsv`, `np.recfromtxt`, `np.disp`, `np.get_array_wrap`, `np.maximum_sctype`, `np.deprecate` and `np.deprecate_with_doc` have been deprecated.
    
    ([gh-24154](https://github.com/numpy/numpy/pull/24154))

  - `np.trapz` has been deprecated. Use `np.trapezoid` or a `scipy.integrate` function instead.

  - `np.in1d` has been deprecated. Use `np.isin` instead.

  - Alias `np.row_stack` has been deprecated. Use `np.vstack` directly.
    
    ([gh-24445](https://github.com/numpy/numpy/pull/24445))

  - `__array_wrap__` is now passed `arr, context, return_scalar` and support for implementations not accepting all three are deprecated. Its signature should be `__array_wrap__(self, arr, context=None, return_scalar=False)`
    
    ([gh-25409](https://github.com/numpy/numpy/pull/25409))

  - Arrays of 2-dimensional vectors for `np.cross` have been deprecated. Use arrays of 3-dimensional vectors instead.
    
    ([gh-24818](https://github.com/numpy/numpy/pull/24818))

  - `np.dtype("a")` alias for `np.dtype(np.bytes_)` was deprecated. Use `np.dtype("S")` alias instead.
    
    ([gh-24854](https://github.com/numpy/numpy/pull/24854))

  - Use of keyword arguments `x` and `y` with functions `assert_array_equal` and `assert_array_almost_equal` has been deprecated. Pass the first two arguments as positional arguments instead.
    
    ([gh-24978](https://github.com/numpy/numpy/pull/24978))

### `numpy.fft` deprecations for n-D transforms with None values in arguments

Using `fftn`, `ifftn`, `rfftn`, `irfftn`, `fft2`, `ifft2`, `rfft2` or `irfft2` with the `s` parameter set to a value that is not `None` and the `axes` parameter set to `None` has been deprecated, in line with the array API standard. To retain current behaviour, pass a sequence \[0, ..., k-1\] to `axes` for an array of dimension k.

Furthermore, passing an array to `s` which contains `None` values is deprecated as the parameter is documented to accept a sequence of integers in both the NumPy docs and the array API specification. To use the default behaviour of the corresponding 1-D transform, pass the value matching the default for its `n` parameter. To use the default behaviour for every axis, the `s` argument can be omitted.

([gh-25495](https://github.com/numpy/numpy/pull/25495))

### `np.linalg.lstsq` now defaults to a new `rcond` value

<span class="title-ref">\~numpy.linalg.lstsq</span> now uses the new rcond value of the machine precision times `max(M, N)`. Previously, the machine precision was used but a FutureWarning was given to notify that this change will happen eventually. That old behavior can still be achieved by passing `rcond=-1`.

([gh-25721](https://github.com/numpy/numpy/pull/25721))

## Expired deprecations

  - The `np.core.umath_tests` submodule has been removed from the public API. (Deprecated in NumPy 1.15)
    
    ([gh-23809](https://github.com/numpy/numpy/pull/23809))

  - The `PyDataMem_SetEventHook` deprecation has expired and it is removed. Use `tracemalloc` and the `np.lib.tracemalloc_domain` domain. (Deprecated in NumPy 1.23)
    
    ([gh-23921](https://github.com/numpy/numpy/pull/23921))

  - The deprecation of `set_numeric_ops` and the C functions `PyArray_SetNumericOps` and `PyArray_GetNumericOps` has been expired and the functions removed. (Deprecated in NumPy 1.16)
    
    ([gh-23998](https://github.com/numpy/numpy/pull/23998))

  - The `fasttake`, `fastclip`, and `fastputmask` `ArrFuncs` deprecation is now finalized.

  - The deprecated function `fastCopyAndTranspose` and its C counterpart are now removed.

  - The deprecation of `PyArray_ScalarFromObject` is now finalized.
    
    ([gh-24312](https://github.com/numpy/numpy/pull/24312))

  - `np.msort` has been removed. For a replacement, `np.sort(a, axis=0)` should be used instead.
    
    ([gh-24494](https://github.com/numpy/numpy/pull/24494))

  - `np.dtype(("f8", 1)` will now return a shape 1 subarray dtype rather than a non-subarray one.
    
    ([gh-25761](https://github.com/numpy/numpy/pull/25761))

  - Assigning to the `.data` attribute of an ndarray is disallowed and will raise.

  - `np.binary_repr(a, width)` will raise if width is too small.

  - Using `NPY_CHAR` in `PyArray_DescrFromType()` will raise, use `NPY_STRING` `NPY_UNICODE`, or `NPY_VSTRING` instead.
    
    ([gh-25794](https://github.com/numpy/numpy/pull/25794))

## Compatibility notes

### `loadtxt` and `genfromtxt` default encoding changed

`loadtxt` and `genfromtxt` now both default to `encoding=None` which may mainly modify how `converters` work. These will now be passed `str` rather than `bytes`. Pass the encoding explicitly to always get the new or old behavior. For `genfromtxt` the change also means that returned values will now be unicode strings rather than bytes.

([gh-25158](https://github.com/numpy/numpy/pull/25158))

### `f2py` compatibility notes

  - `f2py` will no longer accept ambiguous `-m` and `.pyf` CLI combinations. When more than one `.pyf` file is passed, an error is raised. When both `-m` and a `.pyf` is passed, a warning is emitted and the `-m` provided name is ignored.
    
    ([gh-25181](https://github.com/numpy/numpy/pull/25181))

  - The `f2py.compile()` helper has been removed because it leaked memory, has been marked as experimental for several years now, and was implemented as a thin `subprocess.run` wrapper. It was also one of the test bottlenecks. See [gh-25122](https://github.com/numpy/numpy/issues/25122) for the full rationale. It also used several `np.distutils` features which are too fragile to be ported to work with `meson`.

  - Users are urged to replace calls to `f2py.compile` with calls to `subprocess.run("python", "-m", "numpy.f2py",...` instead, and to use environment variables to interact with `meson`. [Native files](https://mesonbuild.com/Machine-files.html) are also an option.
    
    ([gh-25193](https://github.com/numpy/numpy/pull/25193))

### Minor changes in behavior of sorting functions

Due to algorithmic changes and use of SIMD code, sorting functions with methods that aren't stable may return slightly different results in 2.0.0 compared to 1.26.x. This includes the default method of <span class="title-ref">\~numpy.argsort</span> and <span class="title-ref">\~numpy.argpartition</span>.

### Removed ambiguity when broadcasting in `np.solve`

The broadcasting rules for `np.solve(a, b)` were ambiguous when `b` had 1 fewer dimensions than `a`. This has been resolved in a backward-incompatible way and is now compliant with the Array API. The old behaviour can be reconstructed by using `np.solve(a, b[..., None])[..., 0]`.

([gh-25914](https://github.com/numpy/numpy/pull/25914))

### Modified representation for `Polynomial`

The representation method for <span class="title-ref">\~numpy.polynomial.polynomial.Polynomial</span> was updated to include the domain in the representation. The plain text and latex representations are now consistent. For example the output of `str(np.polynomial.Polynomial([1, 1], domain=[.1, .2]))` used to be `1.0 + 1.0 x`, but now is `1.0 + 1.0 (-3.0000000000000004 + 20.0 x)`.

([gh-21760](https://github.com/numpy/numpy/pull/21760))

## C API changes

  - The `PyArray_CGT`, `PyArray_CLT`, `PyArray_CGE`, `PyArray_CLE`, `PyArray_CEQ`, `PyArray_CNE` macros have been removed.

  - `PyArray_MIN` and `PyArray_MAX` have been moved from `ndarraytypes.h` to `npy_math.h`.
    
    ([gh-24258](https://github.com/numpy/numpy/pull/24258))

  - A C API for working with <span class="title-ref">numpy.dtypes.StringDType</span> arrays has been exposed. This includes functions for acquiring and releasing mutexes which lock access to the string data, as well as packing and unpacking UTF-8 bytestreams from array entries.

  - `NPY_NTYPES` has been renamed to `NPY_NTYPES_LEGACY` as it does not include new NumPy built-in DTypes. In particular the new string DType will likely not work correctly with code that handles legacy DTypes.
    
    ([gh-25347](https://github.com/numpy/numpy/pull/25347))

  - The C-API now only exports the static inline function versions of the array accessors (previously this depended on using "deprecated API"). While we discourage it, the struct fields can still be used directly.
    
    ([gh-25789](https://github.com/numpy/numpy/pull/25789))

  - NumPy now defines :c\`PyArray\_Pack\` to set an individual memory address. Unlike `PyArray_SETITEM` this function is equivalent to setting an individual array item and does not require a NumPy array input.
    
    ([gh-25954](https://github.com/numpy/numpy/pull/25954))

  - The `->f` slot has been removed from `PyArray_Descr`. If you use this slot, replace accessing it with `PyDataType_GetArrFuncs` (see its documentation and the \[numpy-2-migration-guide\](\#numpy-2-migration-guide)). In some cases using other functions like `PyArray_GETITEM` may be an alternatives.

  - `PyArray_GETITEM` and `PyArray_SETITEM` now require the import of the NumPy API table to be used and are no longer defined in `ndarraytypes.h`.
    
    ([gh-25812](https://github.com/numpy/numpy/pull/25812))

  - Due to runtime dependencies, the definition for functionality accessing the dtype flags was moved from `numpy/ndarraytypes.h` and is only available after including `numpy/ndarrayobject.h` as it requires `import_array()`. This includes `PyDataType_FLAGCHK`, `PyDataType_REFCHK` and `NPY_BEGIN_THREADS_DESCR`.

  - The dtype flags on `PyArray_Descr` must now be accessed through the `PyDataType_FLAGS` inline function to be compatible with both 1.x and 2.x. This function is defined in `npy_2_compat.h` to allow backporting. Most or all users should use `PyDataType_FLAGCHK` which is available on 1.x and does not require backporting. Cython users should use Cython 3. Otherwise access will go through Python unless they use `PyDataType_FLAGCHK` instead.
    
    ([gh-25816](https://github.com/numpy/numpy/pull/25816))

### Datetime functionality exposed in the C API and Cython bindings

The functions `NpyDatetime_ConvertDatetime64ToDatetimeStruct`, `NpyDatetime_ConvertDatetimeStructToDatetime64`, `NpyDatetime_ConvertPyDateTimeToDatetimeStruct`, `NpyDatetime_GetDatetimeISO8601StrLen`, `NpyDatetime_MakeISO8601Datetime`, and `NpyDatetime_ParseISO8601Datetime` have been added to the C API to facilitate converting between strings, Python datetimes, and NumPy datetimes in external libraries.

([gh-21199](https://github.com/numpy/numpy/pull/21199))

### Const correctness for the generalized ufunc C API

The NumPy C API's functions for constructing generalized ufuncs (`PyUFunc_FromFuncAndData`, `PyUFunc_FromFuncAndDataAndSignature`, `PyUFunc_FromFuncAndDataAndSignatureAndIdentity`) take `types` and `data` arguments that are not modified by NumPy's internals. Like the `name` and `doc` arguments, third-party Python extension modules are likely to supply these arguments from static constants. The `types` and `data` arguments are now const-correct: they are declared as `const char *types` and `void *const *data`, respectively. C code should not be affected, but C++ code may be.

([gh-23847](https://github.com/numpy/numpy/pull/23847))

### Larger `NPY_MAXDIMS` and `NPY_MAXARGS`, `NPY_RAVEL_AXIS` introduced

`NPY_MAXDIMS` is now 64, you may want to review its use. This is usually used in a stack allocation, where the increase should be safe. However, we do encourage generally to remove any use of `NPY_MAXDIMS` and `NPY_MAXARGS` to eventually allow removing the constraint completely. For the conversion helper and C-API functions mirroring Python ones such as `take`, `NPY_MAXDIMS` was used to mean `axis=None`. Such usage must be replaced with `NPY_RAVEL_AXIS`. See also \[migration\_maxdims\](\#migration\_maxdims).

([gh-25149](https://github.com/numpy/numpy/pull/25149))

### `NPY_MAXARGS` not constant and `PyArrayMultiIterObject` size change

Since `NPY_MAXARGS` was increased, it is now a runtime constant and not compile-time constant anymore. We expect almost no users to notice this. But if used for stack allocations it now must be replaced with a custom constant using `NPY_MAXARGS` as an additional runtime check.

The `sizeof(PyArrayMultiIterObject)` no longer includes the full size of the object. We expect nobody to notice this change. It was necessary to avoid issues with Cython.

([gh-25271](https://github.com/numpy/numpy/pull/25271))

### Required changes for custom legacy user dtypes

In order to improve our DTypes it is unfortunately necessary to break the ABI, which requires some changes for dtypes registered with `PyArray_RegisterDataType`. Please see the documentation of `PyArray_RegisterDataType` for how to adapt your code and achieve compatibility with both 1.x and 2.x.

([gh-25792](https://github.com/numpy/numpy/pull/25792))

### New Public DType API

The C implementation of the NEP 42 DType API is now public. While the DType API has shipped in NumPy for a few versions, it was only usable in sessions with a special environment variable set. It is now possible to write custom DTypes outside of NumPy using the new DType API and the normal `import_array()` mechanism for importing the numpy C API.

See \[dtype-api\](\#dtype-api) for more details about the API. As always with a new feature, please report any bugs you run into implementing or using a new DType. It is likely that downstream C code that works with dtypes will need to be updated to work correctly with new DTypes.

([gh-25754](https://github.com/numpy/numpy/pull/25754))

### New C-API import functions

We have now added `PyArray_ImportNumPyAPI` and `PyUFunc_ImportUFuncAPI` as static inline functions to import the NumPy C-API tables. The new functions have two advantages over `import_array` and `import_ufunc`:

  - They check whether the import was already performed and are light-weight if not, allowing to add them judiciously (although this is not preferable in most cases).
  - The old mechanisms were macros rather than functions which included a `return` statement.

The `PyArray_ImportNumPyAPI()` function is included in `npy_2_compat.h` for simpler backporting.

([gh-25866](https://github.com/numpy/numpy/pull/25866))

### Structured dtype information access through functions

The dtype structures fields `c_metadata`, `names`, `fields`, and `subarray` must now be accessed through new functions following the same names, such as `PyDataType_NAMES`. Direct access of the fields is not valid as they do not exist for all `PyArray_Descr` instances. The `metadata` field is kept, but the macro version should also be preferred.

([gh-25802](https://github.com/numpy/numpy/pull/25802))

### Descriptor `elsize` and `alignment` access

Unless compiling only with NumPy 2 support, the `elsize` and `alignment` fields must now be accessed via `PyDataType_ELSIZE`, `PyDataType_SET_ELSIZE`, and `PyDataType_ALIGNMENT`. In cases where the descriptor is attached to an array, we advise using `PyArray_ITEMSIZE` as it exists on all NumPy versions. Please see \[migration\_c\_descr\](\#migration\_c\_descr) for more information.

([gh-25943](https://github.com/numpy/numpy/pull/25943))

## NumPy 2.0 C API removals

  - `npy_interrupt.h` and the corresponding macros like `NPY_SIGINT_ON` have been removed. We recommend querying `PyErr_CheckSignals()` or `PyOS_InterruptOccurred()` periodically (these do currently require holding the GIL though).

  - The `noprefix.h` header has been removed. Replace missing symbols with their prefixed counterparts (usually an added `NPY_` or `npy_`).
    
    ([gh-23919](https://github.com/numpy/numpy/pull/23919))

  - `PyUFunc_GetPyVals`, `PyUFunc_handlefperr`, and `PyUFunc_checkfperr` have been removed. If needed, a new backwards compatible function to raise floating point errors could be restored. Reason for removal: there are no known users and the functions would have made `with np.errstate()` fixes much more difficult).
    
    ([gh-23922](https://github.com/numpy/numpy/pull/23922))

  - The `numpy/old_defines.h` which was part of the API deprecated since NumPy 1.7 has been removed. This removes macros of the form `PyArray_CONSTANT`. The [replace\_old\_macros.sed](https://github.com/numpy/numpy/blob/main/tools/replace_old_macros.sed) script may be useful to convert them to the `NPY_CONSTANT` version.
    
    ([gh-24011](https://github.com/numpy/numpy/pull/24011))

  - The `legacy_inner_loop_selector` member of the ufunc struct is removed to simplify improvements to the dispatching system. There are no known users overriding or directly accessing this member.
    
    ([gh-24271](https://github.com/numpy/numpy/pull/24271))

  - `NPY_INTPLTR` has been removed to avoid confusion (see `intp` redefinition).
    
    ([gh-24888](https://github.com/numpy/numpy/pull/24888))

  - The advanced indexing `MapIter` and related API has been removed. The (truly) public part of it was not well tested and had only one known user (Theano). Making it private will simplify improvements to speed up `ufunc.at`, make advanced indexing more maintainable, and was important for increasing the maximum number of dimensions of arrays to 64. Please let us know if this API is important to you so we can find a solution together.
    
    ([gh-25138](https://github.com/numpy/numpy/pull/25138))

  - The `NPY_MAX_ELSIZE` macro has been removed, as it only ever reflected builtin numeric types and served no internal purpose.
    
    ([gh-25149](https://github.com/numpy/numpy/pull/25149))

  - `PyArray_REFCNT` and `NPY_REFCOUNT` are removed. Use `Py_REFCNT` instead.
    
    ([gh-25156](https://github.com/numpy/numpy/pull/25156))

  - `PyArrayFlags_Type` and `PyArray_NewFlagsObject` as well as `PyArrayFlagsObject` are private now. There is no known use-case; use the Python API if needed.

  - `PyArray_MoveInto`, `PyArray_CastTo`, `PyArray_CastAnyTo` are removed use `PyArray_CopyInto` and if absolutely needed `PyArray_CopyAnyInto` (the latter does a flat copy).

  - `PyArray_FillObjectArray` is removed, its only true use was for implementing `np.empty`. Create a new empty array or use `PyArray_FillWithScalar()` (decrefs existing objects).

  - `PyArray_CompareUCS4` and `PyArray_CompareString` are removed. Use the standard C string comparison functions.

  - `PyArray_ISPYTHON` is removed as it is misleading, has no known use-cases, and is easy to replace.

  - `PyArray_FieldNames` is removed, as it is unclear what it would be useful for. It also has incorrect semantics in some possible use-cases.

  - `PyArray_TypestrConvert` is removed, since it seems a misnomer and unlikely to be used by anyone. If you know the size or are limited to few types, just use it explicitly, otherwise go via Python strings.
    
    ([gh-25292](https://github.com/numpy/numpy/pull/25292))

  - `PyDataType_GetDatetimeMetaData` is removed, it did not actually do anything since at least NumPy 1.7.
    
    ([gh-25802](https://github.com/numpy/numpy/pull/25802))

  - `PyArray_GetCastFunc` is removed. Note that custom legacy user dtypes can still provide a castfunc as their implementation, but any access to them is now removed. The reason for this is that NumPy never used these internally for many years. If you use simple numeric types, please just use C casts directly. In case you require an alternative, please let us know so we can create new API such as `PyArray_CastBuffer()` which could use old or new cast functions depending on the NumPy version.
    
    ([gh-25161](https://github.com/numpy/numpy/pull/25161))

## New Features

### `np.add` was extended to work with `unicode` and `bytes` dtypes.

> ([gh-24858](https://github.com/numpy/numpy/pull/24858))

### A new `bitwise_count` function

This new function counts the number of 1-bits in a number. <span class="title-ref">\~numpy.bitwise\_count</span> works on all the numpy integer types and integer-like objects.

`` `python     >>> a = np.array([2**i - 1 for i in range(16)])     >>> np.bitwise_count(a)     array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],           dtype=uint8)  (`gh-19355 <https://github.com/numpy/numpy/pull/19355>`__)   macOS Accelerate support, including the ILP64 ``\` ---------------------------------------------Support for the updated Accelerate BLAS/LAPACK library, including ILP64 (64-bit integer) support, in macOS 13.3 has been added. This brings arm64 support, and significant performance improvements of up to 10x for commonly used linear algebra operations. When Accelerate is selected at build time, or if no explicit BLAS library selection is done, the 13.3+ version will automatically be used if available.

([gh-24053](https://github.com/numpy/numpy/pull/24053))

Binary wheels are also available. On macOS \>=14.0, users who install NumPy from PyPI will get wheels built against Accelerate rather than OpenBLAS.

([gh-25255](https://github.com/numpy/numpy/pull/25255))

### Option to use weights for quantile and percentile functions

A `weights` keyword is now available for <span class="title-ref">\~numpy.quantile</span>, <span class="title-ref">\~numpy.percentile</span>, <span class="title-ref">\~numpy.nanquantile</span> and <span class="title-ref">\~numpy.nanpercentile</span>. Only `method="inverted_cdf"` supports weights.

([gh-24254](https://github.com/numpy/numpy/pull/24254))

### Improved CPU optimization tracking

A new tracer mechanism is available which enables tracking of the enabled targets for each optimized function (i.e., that uses hardware-specific SIMD instructions) in the NumPy library. With this enhancement, it becomes possible to precisely monitor the enabled CPU dispatch targets for the dispatched functions.

A new function named `opt_func_info` has been added to the new namespace <span class="title-ref">numpy.lib.introspect</span>, offering this tracing capability. This function allows you to retrieve information about the enabled targets based on function names and data type signatures.

([gh-24420](https://github.com/numpy/numpy/pull/24420))

### A new Meson backend for `f2py`

`f2py` in compile mode (i.e. `f2py -c`) now accepts the `--backend meson` option. This is the default option for Python \>=3.12. For older Python versions, `f2py` will still default to `--backend distutils`.

To support this in realistic use-cases, in compile mode `f2py` takes a `--dep` flag one or many times which maps to `dependency()` calls in the `meson` backend, and does nothing in the `distutils` backend.

There are no changes for users of `f2py` only as a code generator, i.e. without `-c`.

([gh-24532](https://github.com/numpy/numpy/pull/24532))

### `bind(c)` support for `f2py`

Both functions and subroutines can be annotated with `bind(c)`. `f2py` will handle both the correct type mapping, and preserve the unique label for other C interfaces.

**Note:** `bind(c, name = 'routine_name_other_than_fortran_routine')` is not honored by the `f2py` bindings by design, since `bind(c)` with the `name` is meant to guarantee only the same name in C and Fortran, not in Python and Fortran.

([gh-24555](https://github.com/numpy/numpy/pull/24555))

### A new `strict` option for several testing functions

The `strict` keyword is now available for <span class="title-ref">\~numpy.testing.assert\_allclose</span>, <span class="title-ref">\~numpy.testing.assert\_equal</span>, and <span class="title-ref">\~numpy.testing.assert\_array\_less</span>. Setting `strict=True` will disable the broadcasting behaviour for scalars and ensure that input arrays have the same data type.

([gh-24680](https://github.com/numpy/numpy/pull/24680), [gh-24770](https://github.com/numpy/numpy/pull/24770), [gh-24775](https://github.com/numpy/numpy/pull/24775))

### Add `np.core.umath.find` and `np.core.umath.rfind` UFuncs

Add two `find` and `rfind` UFuncs that operate on unicode or byte strings and are used in `np.char`. They operate similar to `str.find` and `str.rfind`.

([gh-24868](https://github.com/numpy/numpy/pull/24868))

### `diagonal` and `trace` for `numpy.linalg`

<span class="title-ref">numpy.linalg.diagonal</span> and <span class="title-ref">numpy.linalg.trace</span> have been added, which are array API standard-compatible variants of <span class="title-ref">numpy.diagonal</span> and <span class="title-ref">numpy.trace</span>. They differ in the default axis selection which define 2-D sub-arrays.

([gh-24887](https://github.com/numpy/numpy/pull/24887))

### New `long` and `ulong` dtypes

<span class="title-ref">numpy.long</span> and <span class="title-ref">numpy.ulong</span> have been added as NumPy integers mapping to C's `long` and `unsigned long`. Prior to NumPy 1.24, `numpy.long` was an alias to Python's `int`.

([gh-24922](https://github.com/numpy/numpy/pull/24922))

### `svdvals` for `numpy.linalg`

<span class="title-ref">numpy.linalg.svdvals</span> has been added. It computes singular values for (a stack of) matrices. Executing `np.svdvals(x)` is the same as calling `np.svd(x, compute_uv=False, hermitian=False)`. This function is compatible with the array API standard.

([gh-24940](https://github.com/numpy/numpy/pull/24940))

### A new `isdtype` function

<span class="title-ref">numpy.isdtype</span> was added to provide a canonical way to classify NumPy's dtypes in compliance with the array API standard.

([gh-25054](https://github.com/numpy/numpy/pull/25054))

### A new `astype` function

<span class="title-ref">numpy.astype</span> was added to provide an array API standard-compatible alternative to the <span class="title-ref">numpy.ndarray.astype</span> method.

([gh-25079](https://github.com/numpy/numpy/pull/25079))

### Array API compatible functions' aliases

13 aliases for existing functions were added to improve compatibility with the array API standard:

  - Trigonometry: `acos`, `acosh`, `asin`, `asinh`, `atan`, `atanh`, `atan2`.
  - Bitwise: `bitwise_left_shift`, `bitwise_invert`, `bitwise_right_shift`.
  - Misc: `concat`, `permute_dims`, `pow`.
  - In `numpy.linalg`: `tensordot`, `matmul`.

([gh-25086](https://github.com/numpy/numpy/pull/25086))

### New `unique_*` functions

The <span class="title-ref">\~numpy.unique\_all</span>, <span class="title-ref">\~numpy.unique\_counts</span>, <span class="title-ref">\~numpy.unique\_inverse</span>, and <span class="title-ref">\~numpy.unique\_values</span> functions have been added. They provide functionality of <span class="title-ref">\~numpy.unique</span> with different sets of flags. They are array API standard-compatible, and because the number of arrays they return does not depend on the values of input arguments, they are easier to target for JIT compilation.

([gh-25088](https://github.com/numpy/numpy/pull/25088))

### Matrix transpose support for ndarrays

NumPy now offers support for calculating the matrix transpose of an array (or stack of arrays). The matrix transpose is equivalent to swapping the last two axes of an array. Both `np.ndarray` and `np.ma.MaskedArray` now expose a `.mT` attribute, and there is a matching new <span class="title-ref">numpy.matrix\_transpose</span> function.

([gh-23762](https://github.com/numpy/numpy/pull/23762))

### Array API compatible functions for `numpy.linalg`

Six new functions and two aliases were added to improve compatibility with the Array API standard for \`numpy.linalg\`:

  - <span class="title-ref">numpy.linalg.matrix\_norm</span> - Computes the matrix norm of a matrix (or a stack of matrices).

  - <span class="title-ref">numpy.linalg.vector\_norm</span> - Computes the vector norm of a vector (or batch of vectors).

  - <span class="title-ref">numpy.vecdot</span> - Computes the (vector) dot product of two arrays.

  - <span class="title-ref">numpy.linalg.vecdot</span> - An alias for <span class="title-ref">numpy.vecdot</span>.

  - <span class="title-ref">numpy.linalg.matrix\_transpose</span> - An alias for <span class="title-ref">numpy.matrix\_transpose</span>.
    
    ([gh-25155](https://github.com/numpy/numpy/pull/25155))

  - <span class="title-ref">numpy.linalg.outer</span> has been added. It computes the outer product of two vectors. It differs from <span class="title-ref">numpy.outer</span> by accepting one-dimensional arrays only. This function is compatible with the array API standard.
    
    ([gh-25101](https://github.com/numpy/numpy/pull/25101))

  - <span class="title-ref">numpy.linalg.cross</span> has been added. It computes the cross product of two (arrays of) 3-dimensional vectors. It differs from <span class="title-ref">numpy.cross</span> by accepting three-dimensional vectors only. This function is compatible with the array API standard.
    
    ([gh-25145](https://github.com/numpy/numpy/pull/25145))

### A `correction` argument for `var` and `std`

A `correction` argument was added to <span class="title-ref">\~numpy.var</span> and <span class="title-ref">\~numpy.std</span>, which is an array API standard compatible alternative to `ddof`. As both arguments serve a similar purpose, only one of them can be provided at the same time.

([gh-25169](https://github.com/numpy/numpy/pull/25169))

### `ndarray.device` and `ndarray.to_device`

An `ndarray.device` attribute and `ndarray.to_device` method were added to `numpy.ndarray` for array API standard compatibility.

Additionally, `device` keyword-only arguments were added to: <span class="title-ref">\~numpy.asarray</span>, <span class="title-ref">\~numpy.arange</span>, <span class="title-ref">\~numpy.empty</span>, <span class="title-ref">\~numpy.empty\_like</span>, <span class="title-ref">\~numpy.eye</span>, <span class="title-ref">\~numpy.full</span>, <span class="title-ref">\~numpy.full\_like</span>, <span class="title-ref">\~numpy.linspace</span>, <span class="title-ref">\~numpy.ones</span>, <span class="title-ref">\~numpy.ones\_like</span>, <span class="title-ref">\~numpy.zeros</span>, and <span class="title-ref">\~numpy.zeros\_like</span>.

For all these new arguments, only `device="cpu"` is supported.

([gh-25233](https://github.com/numpy/numpy/pull/25233))

### StringDType has been added to NumPy

We have added a new variable-width UTF-8 encoded string data type, implementing a "NumPy array of Python strings", including support for a user-provided missing data sentinel. It is intended as a drop-in replacement for arrays of Python strings and missing data sentinels using the object dtype. See [NEP 55](https://numpy.org/neps/nep-0055-string_dtype.html) and \[the documentation \<stringdtype\>\](\#the documentation-\<stringdtype\>) for more details.

([gh-25347](https://github.com/numpy/numpy/pull/25347))

### New keywords for `cholesky` and `pinv`

The `upper` and `rtol` keywords were added to <span class="title-ref">numpy.linalg.cholesky</span> and <span class="title-ref">numpy.linalg.pinv</span>, respectively, to improve array API standard compatibility.

For <span class="title-ref">\~numpy.linalg.pinv</span>, if neither `rcond` nor `rtol` is specified, the `rcond`'s default is used. We plan to deprecate and remove `rcond` in the future.

([gh-25388](https://github.com/numpy/numpy/pull/25388))

### New keywords for `sort`, `argsort` and `linalg.matrix_rank`

New keyword parameters were added to improve array API standard compatibility:

  - `rtol` was added to <span class="title-ref">\~numpy.linalg.matrix\_rank</span>.
  - `stable` was added to <span class="title-ref">\~numpy.sort</span> and <span class="title-ref">\~numpy.argsort</span>.

([gh-25437](https://github.com/numpy/numpy/pull/25437))

### New `numpy.strings` namespace for string ufuncs

NumPy now implements some string operations as ufuncs. The old `np.char` namespace is still available, and where possible the string manipulation functions in that namespace have been updated to use the new ufuncs, substantially improving their performance.

Where possible, we suggest updating code to use functions in `np.strings` instead of `np.char`. In the future we may deprecate `np.char` in favor of `np.strings`.

([gh-25463](https://github.com/numpy/numpy/pull/25463))

### `numpy.fft` support for different precisions and in-place calculations

The various FFT routines in <span class="title-ref">numpy.fft</span> now do their calculations natively in float, double, or long double precision, depending on the input precision, instead of always calculating in double precision. Hence, the calculation will now be less precise for single and more precise for long double precision. The data type of the output array will now be adjusted accordingly.

Furthermore, all FFT routines have gained an `out` argument that can be used for in-place calculations.

([gh-25536](https://github.com/numpy/numpy/pull/25536))

### configtool and pkg-config support

A new `numpy-config` CLI script is available that can be queried for the NumPy version and for compile flags needed to use the NumPy C API. This will allow build systems to better support the use of NumPy as a dependency. Also, a `numpy.pc` pkg-config file is now included with Numpy. In order to find its location for use with `PKG_CONFIG_PATH`, use `numpy-config --pkgconfigdir`.

([gh-25730](https://github.com/numpy/numpy/pull/25730))

### Array API standard support in the main namespace

The main `numpy` namespace now supports the array API standard. See \[array-api-standard-compatibility\](\#array-api-standard-compatibility) for details.

([gh-25911](https://github.com/numpy/numpy/pull/25911))

## Improvements

### Strings are now supported by `any`, `all`, and the logical ufuncs.

> ([gh-25651](https://github.com/numpy/numpy/pull/25651))

### Integer sequences as the shape argument for `memmap`

<span class="title-ref">numpy.memmap</span> can now be created with any integer sequence as the `shape` argument, such as a list or numpy array of integers. Previously, only the types of tuple and int could be used without raising an error.

([gh-23729](https://github.com/numpy/numpy/pull/23729))

### `errstate` is now faster and context safe

The <span class="title-ref">numpy.errstate</span> context manager/decorator is now faster and safer. Previously, it was not context safe and had (rare) issues with thread-safety.

([gh-23936](https://github.com/numpy/numpy/pull/23936))

### AArch64 quicksort speed improved by using Highway's VQSort

The first introduction of the Google Highway library, using VQSort on AArch64. Execution time is improved by up to 16x in some cases, see the PR for benchmark results. Extensions to other platforms will be done in the future.

([gh-24018](https://github.com/numpy/numpy/pull/24018))

### Complex types - underlying C type changes

  - The underlying C types for all of NumPy's complex types have been changed to use C99 complex types.
  - While this change does not affect the memory layout of complex types, it changes the API to be used to directly retrieve or write the real or complex part of the complex number, since direct field access (as in `c.real` or `c.imag`) is no longer an option. You can now use utilities provided in `numpy/npy_math.h` to do these operations, like this:
      - \`\`\`c  
        npy\_cdouble c; npy\_csetreal(\&c, 1.0); npy\_csetimag(\&c, 0.0); printf("%d + %din", npy\_creal(c), npy\_cimag(c));
  - To ease cross-version compatibility, equivalent macros and a compatibility layer have been added which can be used by downstream packages to continue to support both NumPy 1.x and 2.x. See \[complex-numbers\](\#complex-numbers) for more info.
  - `numpy/npy_common.h` now includes `complex.h`, which means that `complex` is now a reserved keyword.

([gh-24085](https://github.com/numpy/numpy/pull/24085))

`iso_c_binding` support and improved common blocks for `f2py` `` ` ----------------------------------------------------------------- Previously, users would have to define their own custom ``f2cmap`file to use type mappings defined by the Fortran2003`iso\_c\_binding`intrinsic module. These type maps are now natively supported by`f2py``(`gh-24555 <https://github.com/numpy/numpy/pull/24555>`__)``f2py`now handles`common`blocks which have`kind`specifications from modules. This further expands the usability of intrinsics like`iso\_fortran\_env`and`iso\_c\_binding``.  (`gh-25186 <https://github.com/numpy/numpy/pull/25186>`__)   Call``str`automatically on third argument to functions like`assert\_equal``------------------------------------------------------------------------------- The third argument to functions like `~numpy.testing.assert_equal` now has``str`called on it automatically. This way it mimics the built-in`assert`statement, where`assert\_equal(a, b, obj)`works like`assert a == b, obj``.  (`gh-24877 <https://github.com/numpy/numpy/pull/24877>`__)   Support for array-like``atol`/`rtol`in`isclose`,`allclose`--------------------------------------------------------------------- The keywords`atol`and`rtol``in `~numpy.isclose` and `~numpy.allclose` now accept both scalars and arrays. An array, if given, must broadcast to the shapes of the first two array arguments.  (`gh-24878 <https://github.com/numpy/numpy/pull/24878>`__)   Consistent failure messages in test functions --------------------------------------------- Previously, some `numpy.testing` assertions printed messages that referred to the actual and desired results as``x`and`y`. Now, these values are consistently referred to as`ACTUAL`and`DESIRED``.  (`gh-24931 <https://github.com/numpy/numpy/pull/24931>`__)   n-D FFT transforms allow``s\[i\] == -1``--------------------------------------- The `~numpy.fft.fftn`, `~numpy.fft.ifftn`, `~numpy.fft.rfftn`, `~numpy.fft.irfftn`, `~numpy.fft.fft2`, `~numpy.fft.ifft2`, `~numpy.fft.rfft2` and `~numpy.fft.irfft2` functions now use the whole input array along the axis``i`if`s\[i\] == -1``, in line with the array API standard.  (`gh-25495 <https://github.com/numpy/numpy/pull/25495>`__)   Guard PyArrayScalar_VAL and PyUnicodeScalarObject for the limited API ---------------------------------------------------------------------``PyUnicodeScalarObject`holds a`PyUnicodeObject`, which is not available when using`Py\_LIMITED\_API`. Add guards to hide it and consequently also make the`PyArrayScalar\_VAL``macro hidden.  (`gh-25531 <https://github.com/numpy/numpy/pull/25531>`__)   Changes =======  *``np.gradient()``now returns a tuple rather than a list making the   return value immutable.    (`gh-23861 <https://github.com/numpy/numpy/pull/23861>`__)  * Being fully context and thread-safe,``np.errstate`can only   be entered once now.  *`np.setbufsize`is now tied to`np.errstate()`: leaving an`np.errstate`context will also reset the`bufsize``.    (`gh-23936 <https://github.com/numpy/numpy/pull/23936>`__)  * A new public``np.lib.array\_utils`submodule has been introduced and it   currently contains three functions:`byte\_bounds`(moved from`np.lib.utils`),`normalize\_axis\_tuple`and`normalize\_axis\_index``.    (`gh-24540 <https://github.com/numpy/numpy/pull/24540>`__)  * Introduce `numpy.bool` as the new canonical name for NumPy's boolean dtype,   and make `numpy.bool_` an alias to it. Note that until NumPy 1.24,``np.bool`was an alias to Python's builtin`bool``. The new name helps   with array API standard compatibility and is a more intuitive name.    (`gh-25080 <https://github.com/numpy/numpy/pull/25080>`__)  * The``dtype.flags``value was previously stored as a signed integer.   This means that the aligned dtype struct flag lead to negative flags being   set (-128 rather than 128). This flag is now stored unsigned (positive). Code   which checks flags manually may need to adapt.  This may include code   compiled with Cython 0.29.x.    (`gh-25816 <https://github.com/numpy/numpy/pull/25816>`__)   Representation of NumPy scalars changed --------------------------------------- As per [NEP 51 <NEP51>](#nep-51-<nep51>), the scalar representation has been updated to include the type information to avoid confusion with Python scalars.  Scalars are now printed as``np.float64(3.0)`rather than just`3.0`. This may disrupt workflows that store representations of numbers (e.g., to files) making it harder to read them. They should be stored as explicit strings, for example by using`str()`or`f"{scalar\!s}"`. For the time being, affected users can use`np.set\_printoptions(legacy="1.25")``to get the old behavior (with possibly a few exceptions). Documentation of downstream projects may require larger updates, if code snippets are tested.  We are working on tooling for `doctest-plus <https://github.com/scientific-python/pytest-doctestplus/issues/107>`__ to facilitate updates.  (`gh-22449 <https://github.com/numpy/numpy/pull/22449>`__)   Truthiness of NumPy strings changed ----------------------------------- NumPy strings previously were inconsistent about how they defined if the string is``True`or`False`and the definition did not match the one used by Python. Strings are now considered`True`when they are non-empty and`False`when they are empty. This changes the following distinct cases:  * Casts from string to boolean were previously roughly equivalent   to`string\_array.astype(np.int64).astype(bool)`, meaning that only   valid integers could be cast.   Now a string of`"0"`will be considered`True`since it is not empty.   If you need the old behavior, you may use the above step (casting   to integer first) or`string\_array == "0"`(if the input is only ever`0`or`1`).   To get the new result on old NumPy versions use`string\_array \!= ""`. *`np.nonzero(string\_array)`previously ignored whitespace so that   a string only containing whitespace was considered`False`.   Whitespace is now considered`True`.  This change does not affect`np.loadtxt`,`np.fromstring`, or`np.genfromtxt`. The first two still use the integer definition, while`genfromtxt`continues to match for`"true"`(ignoring case). However, if`[np.bool]()`is used as a converter the result will change.  The change does affect`np.fromregex``as it uses direct assignments.  (`gh-23871 <https://github.com/numpy/numpy/pull/23871>`__)   A``mean``keyword was added to var and std function ---------------------------------------------------- Often when the standard deviation is needed the mean is also needed. The same holds for the variance and the mean. Until now the mean is then calculated twice, the change introduced here for the `~numpy.var` and `~numpy.std` functions allows for passing in a precalculated mean as an keyword argument. See the docstrings for details and an example illustrating the speed-up.  (`gh-24126 <https://github.com/numpy/numpy/pull/24126>`__)   Remove datetime64 deprecation warning when constructing with timezone --------------------------------------------------------------------- The `numpy.datetime64` method now issues a UserWarning rather than a DeprecationWarning whenever a timezone is included in the datetime string that is provided.  (`gh-24193 <https://github.com/numpy/numpy/pull/24193>`__)   Default integer dtype is now 64-bit on 64-bit Windows ----------------------------------------------------- The default NumPy integer is now 64-bit on all 64-bit systems as the historic 32-bit default on Windows was a common source of issues. Most users should not notice this. The main issues may occur with code interfacing with libraries written in a compiled language like C.  For more information see [migration_windows_int64](#migration_windows_int64).  (`gh-24224 <https://github.com/numpy/numpy/pull/24224>`__)   Renamed``numpy.core`to`numpy.\_core`----------------------------------------- Accessing`numpy.core`now emits a DeprecationWarning. In practice we have found that most downstream usage of`numpy.core`was to access functionality that is available in the main`numpy`namespace. If for some reason you are using functionality in`numpy.core`that is not available in the main`numpy`namespace, this means you are likely using private NumPy internals. You can still access these internals via`numpy.\_core``without a deprecation warning but we do not provide any backward compatibility guarantees for NumPy internals. Please open an issue if you think a mistake was made and something needs to be made public.  (`gh-24634 <https://github.com/numpy/numpy/pull/24634>`__)  The "relaxed strides" debug build option, which was previously enabled through the``NPY\_RELAXED\_STRIDES\_DEBUG`environment variable or the`-Drelaxed-strides-debug``config-settings flag has been removed.  (`gh-24717 <https://github.com/numpy/numpy/pull/24717>`__)   Redefinition of``np.intp`/`np.uintp`(almost never a change) ---------------------------------------------------------------- Due to the actual use of these types almost always matching the use of`size\_t`/`Py\_ssize\_t`this is now the definition in C. Previously, it matched`intptr\_t`and`uintptr\_t`which would often have been subtly incorrect. This has no effect on the vast majority of machines since the size of these types only differ on extremely niche platforms.  However, it means that:  * Pointers may not necessarily fit into an`intp`typed array anymore.   The`p`and`P`character codes can still be used, however. * Creating`intptr\_t`or`uintptr\_t`typed arrays in C remains possible   in a cross-platform way via`PyArray\_DescrFromType('p')`. * The new character codes`nN`were introduced. * It is now correct to use the Python C-API functions when parsing   to`npy\_intp``typed arguments.  (`gh-24888 <https://github.com/numpy/numpy/pull/24888>`__)``numpy.fft.helper`made private ---------------------------------`numpy.fft.helper`was renamed to`numpy.fft.\_helper``to indicate that it is a private submodule. All public functions exported by it should be accessed from `numpy.fft`.  (`gh-24945 <https://github.com/numpy/numpy/pull/24945>`__)``numpy.linalg.linalg`made private ------------------------------------`numpy.linalg.linalg`was renamed to`numpy.linalg.\_linalg``to indicate that it is a private submodule. All public functions exported by it should be accessed from `numpy.linalg`.  (`gh-24946 <https://github.com/numpy/numpy/pull/24946>`__)   Out-of-bound axis not the same as``axis=None`----------------------------------------------- In some cases`axis=32`or for concatenate any large value was the same as`axis=None`. Except for`concatenate`this was deprecate. Any out of bound axis value will now error, make sure to use`axis=None``.  (`gh-25149 <https://github.com/numpy/numpy/pull/25149>`__)  .. _copy-keyword-changes-2.0:   New``copy`keyword meaning for`array`and`asarray``constructors ----------------------------------------------------------------------- Now `numpy.array` and `numpy.asarray` support three values for``copy`parameter:  *`None`- A copy will only be made if it is necessary. *`True`- Always make a copy. *`False`- Never make a copy. If a copy is required a`ValueError`is raised.  The meaning of`False``changed as it now raises an exception if a copy is needed.  (`gh-25168 <https://github.com/numpy/numpy/pull/25168>`__)   The``\_\_[array](#array_prepare__-is-removed)`special method now takes a`copy`keyword argument. ----------------------------------------------------------------------- NumPy will pass`copy`to the`\_\_array\_\_`special method in situations where it would be set to a non-default value (e.g. in a call to`np.asarray(some\_object, copy=False)`). Currently, if an unexpected keyword argument error is raised after this, NumPy will print a warning and re-try without the`copy`keyword argument. Implementations of objects implementing the`\_\_array\_\_`protocol should accept a`copy``keyword argument with the same meaning as when passed to `numpy.array` or `numpy.asarray`.  (`gh-25168 <https://github.com/numpy/numpy/pull/25168>`__)   Cleanup of initialization of``numpy.dtype`with strings with commas --------------------------------------------------------------------- The interpretation of strings with commas is changed slightly, in that a trailing comma will now always create a structured dtype.  E.g., where previously`np.dtype("i")`and`np.dtype("i,")`were treated as identical, now`np.dtype("i,")`will create a structured dtype, with a single field. This is analogous to`np.dtype("i,i")`creating a structured dtype with two fields, and makes the behaviour consistent with that expected of tuples.  At the same time, the use of single number surrounded by parenthesis to indicate a sub-array shape, like in`np.dtype("(2)i,")`, is deprecated. Instead; one should use`np.dtype("(2,)i")`or`np.dtype("2i")`. Eventually, using a number in parentheses will raise an exception, like is the case for initializations without a comma, like`np.dtype("(2)i")``.  (`gh-25434 <https://github.com/numpy/numpy/pull/25434>`__)   Change in how complex sign is calculated ---------------------------------------- Following the array API standard, the complex sign is now calculated as``z / `(instead of the rather less logical case where the sign of the real part was taken, unless the real part was zero, in which case the sign of the imaginary part was returned).  Like for real numbers, zero is returned if`z==0``.  (`gh-25441 <https://github.com/numpy/numpy/pull/25441>`__)   Return types of functions that returned a list of arrays -------------------------------------------------------- Functions that returned a list of ndarrays have been changed to return a tuple of ndarrays instead. Returning tuples consistently whenever a sequence of arrays is returned makes it easier for JIT compilers like Numba, as well as for static type checkers in some cases, to support these functions. Changed functions are: `~numpy.atleast_1d`, `~numpy.atleast_2d`, `~numpy.atleast_3d`, `~numpy.broadcast_arrays`, `~numpy.meshgrid`, `~numpy.ogrid`, `~numpy.histogramdd`.``np.unique`  `return\_inverse`shape for multi-dimensional inputs ------------------------------------------------------------------- When multi-dimensional inputs are passed to`np.unique`with`return\_inverse=True`, the`unique\_inverse`output is now shaped such that the input can be reconstructed directly using`np.take(unique, unique\_inverse)`when`axis=None`, and`np.take\_along\_axis(unique, unique\_inverse, axis=axis)`otherwise.  .. note::     This change was reverted in 2.0.1 except for`axis=None`.  The correct     reconstruction is always`np.take(unique, unique\_inverse, axis=axis)`.     When 2.0.0 needs to be supported, add`unique\_inverse.reshape(-1)``to code.  (`gh-25553 <https://github.com/numpy/numpy/pull/25553>`__, `gh-25570 <https://github.com/numpy/numpy/pull/25570>`__)``any`and`all`return booleans for object arrays ----------------------------------------------------- The`any`and`all`functions and methods now return booleans also for object arrays.  Previously, they did a reduction which behaved like the Python`or`and`and`operators which evaluates to one of the arguments. You can use`np.logical\_or.reduce`and`np.logical\_and.reduce``to achieve the previous behavior.  (`gh-25712 <https://github.com/numpy/numpy/pull/25712>`__)``np.can\_cast`cannot be called on Python int, float, or complex -----------------------------------------------------------------`np.can\_cast`cannot be called with Python int, float, or complex instances anymore.  This is because NEP 50 means that the result of`can\_cast`must not depend on the value passed in. Unfortunately, for Python scalars whether a cast should be considered`"same\_kind"`or`"safe"`may depend on the context and value so that this is currently not implemented. In some cases, this means you may have to add a specific path for:`if type(obj) in (int, float, complex): ...\`\`.

([gh-26393](https://github.com/numpy/numpy/pull/26393))

---

2.0.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.0.1 Release Notes

NumPy 2.0.1 is a maintenance release that fixes bugs and regressions discovered after the 2.0.0 release. NumPy 2.0.1 is the last planned release in the 2.0.x series, 2.1.0rc1 should be out shortly.

The Python versions supported by this release are 3.9-3.12.

## Improvements

### `np.quantile` with method `closest_observation` chooses nearest even order statistic

This changes the definition of nearest for border cases from the nearest odd order statistic to nearest even order statistic. The numpy implementation now matches other reference implementations.

([gh-26656](https://github.com/numpy/numpy/pull/26656))

## Contributors

A total of 15 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - @vahidmech +
  - Alex Herbert +
  - Charles Harris
  - Giovanni Del Monte +
  - Leo Singer
  - Lysandros Nikolaou
  - Matti Picus
  - Nathan Goldbaum
  - Patrick J. Roddy +
  - Raghuveer Devulapalli
  - Ralf Gommers
  - Rostan Tabet +
  - Sebastian Berg
  - Tyler Reddy
  - Yannik Wicke +

## Pull requests merged

A total of 24 pull requests were merged for this release.

  - [\#26711](https://github.com/numpy/numpy/pull/26711): MAINT: prepare 2.0.x for further development
  - [\#26792](https://github.com/numpy/numpy/pull/26792): TYP: fix incorrect import in `ma/extras.pyi` stub
  - [\#26793](https://github.com/numpy/numpy/pull/26793): DOC: Mention '1.25' legacy printing mode in `set_printoptions`
  - [\#26794](https://github.com/numpy/numpy/pull/26794): DOC: Remove mention of NaN and NAN aliases from constants
  - [\#26821](https://github.com/numpy/numpy/pull/26821): BLD: Fix x86-simd-sort build failure on openBSD
  - [\#26822](https://github.com/numpy/numpy/pull/26822): BUG: Ensure output order follows input in numpy.fft
  - [\#26823](https://github.com/numpy/numpy/pull/26823): TYP: fix missing sys import in numeric.pyi
  - [\#26832](https://github.com/numpy/numpy/pull/26832): DOC: remove hack to override \_add\_newdocs\_scalars (\#26826)
  - [\#26835](https://github.com/numpy/numpy/pull/26835): BUG: avoid side-effect of 'include complex.h'
  - [\#26836](https://github.com/numpy/numpy/pull/26836): BUG: fix max\_rows and chunked string/datetime reading in `loadtxt`
  - [\#26837](https://github.com/numpy/numpy/pull/26837): BUG: fix PyArray\_ImportNumPyAPI under -Werror=strict-prototypes
  - [\#26856](https://github.com/numpy/numpy/pull/26856): DOC: Update some documentation
  - [\#26868](https://github.com/numpy/numpy/pull/26868): BUG: fancy indexing copy
  - [\#26869](https://github.com/numpy/numpy/pull/26869): BUG: Mismatched allocation domains in `PyArray_FillWithScalar`
  - [\#26870](https://github.com/numpy/numpy/pull/26870): BUG: Handle --f77flags and --f90flags for meson \[wheel build\]
  - [\#26887](https://github.com/numpy/numpy/pull/26887): BUG: Fix new DTypes and new string promotion when signature is...
  - [\#26888](https://github.com/numpy/numpy/pull/26888): BUG: remove numpy.f2py from excludedimports
  - [\#26959](https://github.com/numpy/numpy/pull/26959): BUG: Quantile closest\_observation to round to nearest even order
  - [\#26960](https://github.com/numpy/numpy/pull/26960): BUG: Fix off-by-one error in amount of characters in strip
  - [\#26961](https://github.com/numpy/numpy/pull/26961): API: Partially revert unique with return\_inverse
  - [\#26962](https://github.com/numpy/numpy/pull/26962): BUG,MAINT: Fix utf-8 character stripping memory access
  - [\#26963](https://github.com/numpy/numpy/pull/26963): BUG: Fix out-of-bound minimum offset for in1d table method
  - [\#26971](https://github.com/numpy/numpy/pull/26971): BUG: fix f2py tests to work with v2 API
  - [\#26995](https://github.com/numpy/numpy/pull/26995): BUG: Add object cast to avoid warning with limited API

---

2.0.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.0.2 Release Notes

NumPy 2.0.2 is a maintenance release that fixes bugs and regressions discovered after the 2.0.1 release.

The Python versions supported by this release are 3.9-3.12.

## Contributors

A total of 13 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Bruno Oliveira +
  - Charles Harris
  - Chris Sidebottom
  - Christian Heimes +
  - Christopher Sidebottom
  - Mateusz SokÃ³Å‚
  - Matti Picus
  - Nathan Goldbaum
  - Pieter Eendebak
  - Raghuveer Devulapalli
  - Ralf Gommers
  - Sebastian Berg
  - Yair Chuchem +

## Pull requests merged

A total of 19 pull requests were merged for this release.

  - [\#27000](https://github.com/numpy/numpy/pull/27000): REL: Prepare for the NumPy 2.0.1 release \[wheel build\]
  - [\#27001](https://github.com/numpy/numpy/pull/27001): MAINT: prepare 2.0.x for further development
  - [\#27021](https://github.com/numpy/numpy/pull/27021): BUG: cfuncs.py: fix crash when sys.stderr is not available
  - [\#27022](https://github.com/numpy/numpy/pull/27022): DOC: Fix migration note for `alltrue` and `sometrue`
  - [\#27061](https://github.com/numpy/numpy/pull/27061): BUG: use proper input and output descriptor in array\_assign\_subscript...
  - [\#27073](https://github.com/numpy/numpy/pull/27073): BUG: Mirror VQSORT\_ENABLED logic in Quicksort
  - [\#27074](https://github.com/numpy/numpy/pull/27074): BUG: Bump Highway to latest master
  - [\#27077](https://github.com/numpy/numpy/pull/27077): BUG: Off by one in memory overlap check
  - [\#27122](https://github.com/numpy/numpy/pull/27122): BUG: Use the new `npyv_loadable_stride_` functions for ldexp and...
  - [\#27126](https://github.com/numpy/numpy/pull/27126): BUG: Bump Highway to latest
  - [\#27128](https://github.com/numpy/numpy/pull/27128): BUG: add missing error handling in public\_dtype\_api.c
  - [\#27129](https://github.com/numpy/numpy/pull/27129): BUG: fix another cast setup in array\_assign\_subscript
  - [\#27130](https://github.com/numpy/numpy/pull/27130): BUG: Fix building NumPy in FIPS mode
  - [\#27131](https://github.com/numpy/numpy/pull/27131): BLD: update vendored Meson for cross-compilation patches
  - [\#27146](https://github.com/numpy/numpy/pull/27146): MAINT: Scipy openblas 0.3.27.44.4
  - [\#27151](https://github.com/numpy/numpy/pull/27151): BUG: Do not accidentally store dtype metadata in `np.save`
  - [\#27195](https://github.com/numpy/numpy/pull/27195): REV: Revert undef I and document it
  - [\#27213](https://github.com/numpy/numpy/pull/27213): BUG: Fix NPY\_RAVEL\_AXIS on backwards compatible NumPy 2 builds
  - [\#27279](https://github.com/numpy/numpy/pull/27279): BUG: Fix array\_equal for numeric and non-numeric scalar types

---

2.1.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.1.0 Release Notes

NumPy 2.1.0 provides support for the upcoming Python 3.13 release and drops support for Python 3.9. In addition to the usual bug fixes and updated Python support, it helps get us back into our usual release cycle after the extended development of 2.0. The highlights for this release are:

  - Support for the array-api 2023.12 standard.
  - Support for Python 3.13.
  - Preliminary support for free threaded Python 3.13.

Python versions 3.10-3.13 are supported in this release.

## New functions

### New function `numpy.unstack`

A new function `np.unstack(array, axis=...)` was added, which splits an array into a tuple of arrays along an axis. It serves as the inverse of <span class="title-ref">numpy.stack</span>.

([gh-26579](https://github.com/numpy/numpy/pull/26579))

## Deprecations

  - The `fix_imports` keyword argument in `numpy.save` is deprecated. Since NumPy 1.17, `numpy.save` uses a pickle protocol that no longer supports Python 2, and ignored `fix_imports` keyword. This keyword is kept only for backward compatibility. It is now deprecated.
    
    ([gh-26452](https://github.com/numpy/numpy/pull/26452))

  - Passing non-integer inputs as the first argument of <span class="title-ref">bincount</span> is now deprecated, because such inputs are silently cast to integers with no warning about loss of precision.
    
    ([gh-27076](https://github.com/numpy/numpy/pull/27076))

## Expired deprecations

  - Scalars and 0D arrays are disallowed for `numpy.nonzero` and `numpy.ndarray.nonzero`.
    
    ([gh-26268](https://github.com/numpy/numpy/pull/26268))

  - `set_string_function` internal function was removed and `PyArray_SetStringFunction` was stubbed out.
    
    ([gh-26611](https://github.com/numpy/numpy/pull/26611))

## C API changes

### API symbols now hidden but customizable

NumPy now defaults to hide the API symbols it adds to allow all NumPy API usage. This means that by default you cannot dynamically fetch the NumPy API from another library (this was never possible on windows).

If you are experiencing linking errors related to `PyArray_API` or `PyArray_RUNTIME_VERSION`, you can define the `NPY_API_SYMBOL_ATTRIBUTE` to opt-out of this change.

If you are experiencing problems due to an upstream header including NumPy, the solution is to make sure you `#include "numpy/ndarrayobject.h"` before their header and import NumPy yourself based on `including-the-c-api`.

([gh-26103](https://github.com/numpy/numpy/pull/26103))

### Many shims removed from npy\_3kcompat.h

Many of the old shims and helper functions were removed from `npy_3kcompat.h`. If you find yourself in need of these, vendor the previous version of the file into your codebase.

([gh-26842](https://github.com/numpy/numpy/pull/26842))

### New `PyUFuncObject` field `process_core_dims_func`

The field `process_core_dims_func` was added to the structure `PyUFuncObject`. For generalized ufuncs, this field can be set to a function of type `PyUFunc_ProcessCoreDimsFunc` that will be called when the ufunc is called. It allows the ufunc author to check that core dimensions satisfy additional constraints, and to set output core dimension sizes if they have not been provided.

([gh-26908](https://github.com/numpy/numpy/pull/26908))

## New Features

### Preliminary Support for Free-Threaded CPython 3.13

CPython 3.13 will be available as an experimental free-threaded build. See <https://py-free-threading.github.io>, [PEP 703](https://peps.python.org/pep-0703/) and the [CPython 3.13 release notes](https://docs.python.org/3.13/whatsnew/3.13.html#free-threaded-cpython) for more detail about free-threaded Python.

NumPy 2.1 has preliminary support for the free-threaded build of CPython 3.13. This support was enabled by fixing a number of C thread-safety issues in NumPy. Before NumPy 2.1, NumPy used a large number of C global static variables to store runtime caches and other state. We have either refactored to avoid the need for global state, converted the global state to thread-local state, or added locking.

Support for free-threaded Python does not mean that NumPy is thread safe. Read-only shared access to ndarray should be safe. NumPy exposes shared mutable state and we have not added any locking to the array object itself to serialize access to shared state. Care must be taken in user code to avoid races if you would like to mutate the same array in multiple threads. It is certainly possible to crash NumPy by mutating an array simultaneously in multiple threads, for example by calling a ufunc and the `resize` method simultaneously. For now our guidance is: "don't do that". In the future we would like to provide stronger guarantees.

Object arrays in particular need special care, since the GIL previously provided locking for object array access and no longer does. See [Issue \#27199](https://github.com/numpy/numpy/issues/27199) for more information about object arrays in the free-threaded build.

If you are interested in free-threaded Python, for example because you have a multiprocessing-based workflow that you are interested in running with Python threads, we encourage testing and experimentation.

If you run into problems that you suspect are because of NumPy, please [open an issue](https://github.com/numpy/numpy/issues/new/choose), checking first if the bug also occurs in the "regular" non-free-threaded CPython 3.13 build. Many threading bugs can also occur in code that releases the GIL; disabling the GIL only makes it easier to hit threading bugs.

([gh-26157](https://github.com/numpy/numpy/issues/26157#issuecomment-2233864940))

  - `numpy.reshape` and `numpy.ndarray.reshape` now support `shape` and `copy` arguments.
    
    ([gh-26292](https://github.com/numpy/numpy/pull/26292))

  - NumPy now supports DLPack v1, support for older versions will be deprecated in the future.
    
    ([gh-26501](https://github.com/numpy/numpy/pull/26501))

  - `numpy.asanyarray` now supports `copy` and `device` arguments, matching `numpy.asarray`.
    
    ([gh-26580](https://github.com/numpy/numpy/pull/26580))

  - `numpy.printoptions`, `numpy.get_printoptions`, and `numpy.set_printoptions` now support a new option, `override_repr`, for defining custom `repr(array)` behavior.
    
    ([gh-26611](https://github.com/numpy/numpy/pull/26611))

  - `numpy.cumulative_sum` and `numpy.cumulative_prod` were added as Array API compatible alternatives for `numpy.cumsum` and `numpy.cumprod`. The new functions can include a fixed initial (zeros for `sum` and ones for `prod`) in the result.
    
    ([gh-26724](https://github.com/numpy/numpy/pull/26724))

  - `numpy.clip` now supports `max` and `min` keyword arguments which are meant to replace `a_min` and `a_max`. Also, for `np.clip(a)` or `np.clip(a, None, None)` a copy of the input array will be returned instead of raising an error.
    
    ([gh-26724](https://github.com/numpy/numpy/pull/26724))

  - `numpy.astype` now supports `device` argument.
    
    ([gh-26724](https://github.com/numpy/numpy/pull/26724))

### `f2py` can generate freethreading-compatible C extensions

Pass `--freethreading-compatible` to the f2py CLI tool to produce a C extension marked as compatible with the free threading CPython interpreter. Doing so prevents the interpreter from re-enabling the GIL at runtime when it imports the C extension. Note that `f2py` does not analyze fortran code for thread safety, so you must verify that the wrapped fortran code is thread safe before marking the extension as compatible.

([gh-26981](https://github.com/numpy/numpy/pull/26981))

## Improvements

### `histogram` auto-binning now returns bin sizes \>=1 for integer input data

For integer input data, bin sizes smaller than 1 result in spurious empty bins. This is now avoided when the number of bins is computed using one of the algorithms provided by `histogram_bin_edges`.

([gh-12150](https://github.com/numpy/numpy/pull/12150))

### `ndarray` shape-type parameter is now covariant and bound to `tuple[int, ...]`

Static typing for `ndarray` is a long-term effort that continues with this change. It is a generic type with type parameters for the shape and the data type. Previously, the shape type parameter could be any value. This change restricts it to a tuple of ints, as one would expect from using `ndarray.shape`. Further, the shape-type parameter has been changed from invariant to covariant. This change also applies to the subtypes of `ndarray`, e.g. `numpy.ma.MaskedArray`. See the [typing docs](https://typing.readthedocs.io/en/latest/reference/generics.html#variance-of-generic-types) for more information.

([gh-26081](https://github.com/numpy/numpy/pull/26081))

### `np.quantile` with method `closest_observation` chooses nearest even order statistic

This changes the definition of nearest for border cases from the nearest odd order statistic to nearest even order statistic. The numpy implementation now matches other reference implementations.

([gh-26656](https://github.com/numpy/numpy/pull/26656))

### `lapack_lite` is now thread safe

NumPy provides a minimal low-performance version of LAPACK named `lapack_lite` that can be used if no BLAS/LAPACK system is detected at build time.

Until now, `lapack_lite` was not thread safe. Single-threaded use cases did not hit any issues, but running linear algebra operations in multiple threads could lead to errors, incorrect results, or segfaults due to data races.

We have added a global lock, serializing access to `lapack_lite` in multiple threads.

([gh-26750](https://github.com/numpy/numpy/pull/26750))

### The `numpy.printoptions` context manager is now thread and async-safe

In prior versions of NumPy, the printoptions were defined using a combination of Python and C global variables. We have refactored so the state is stored in a python `ContextVar`, making the context manager thread and async-safe.

([gh-26846](https://github.com/numpy/numpy/pull/26846))

### Type hinting `numpy.polynomial`

Starting from the 2.1 release, PEP 484 type annotations have been included for the functions and convenience classes in `numpy.polynomial` and its sub-packages.

([gh-26897](https://github.com/numpy/numpy/pull/26897))

### Improved `numpy.dtypes` type hints

The type annotations for `numpy.dtypes` are now a better reflection of the runtime: The `numpy.dtype` type-aliases have been replaced with specialized `dtype` *subtypes*, and the previously missing annotations for `numpy.dtypes.StringDType` have been added.

([gh-27008](https://github.com/numpy/numpy/pull/27008))

## Performance improvements and changes

  - `numpy.save` now uses pickle protocol version 4 for saving arrays with object dtype, which allows for pickle objects larger than 4GB and improves saving speed by about 5% for large arrays.
    
    ([gh-26388](https://github.com/numpy/numpy/pull/26388))

  - OpenBLAS on x86\_64 and i686 is built with fewer kernels. Based on benchmarking, there are 5 clusters of performance around these kernels: `PRESCOTT NEHALEM SANDYBRIDGE HASWELL SKYLAKEX`.
    
    ([gh-27147](https://github.com/numpy/numpy/pull/27147))

  - OpenBLAS on windows is linked without quadmath, simplifying licensing
    
    ([gh-27147](https://github.com/numpy/numpy/pull/27147))

  - Due to a regression in OpenBLAS on windows, the performance improvements when using multiple threads for OpenBLAS 0.3.26 were reverted.
    
    ([gh-27147](https://github.com/numpy/numpy/pull/27147))

### `ma.cov` and `ma.corrcoef` are now significantly faster

The private function has been refactored along with `ma.cov` and `ma.corrcoef`. They are now significantly faster, particularly on large, masked arrays.

([gh-26285](https://github.com/numpy/numpy/pull/26285))

## Changes

  - As `numpy.vecdot` is now a ufunc it has a less precise signature. This is due to the limitations of ufunc's typing stub.
    
    ([gh-26313](https://github.com/numpy/numpy/pull/26313))

  - `numpy.floor`, `numpy.ceil`, and `numpy.trunc` now won't perform casting to a floating dtype for integer and boolean dtype input arrays.
    
    ([gh-26766](https://github.com/numpy/numpy/pull/26766))

### `ma.corrcoef` may return a slightly different result

A pairwise observation approach is currently used in `ma.corrcoef` to calculate the standard deviations for each pair of variables. This has been changed as it is being used to normalise the covariance, estimated using `ma.cov`, which does not consider the observations for each variable in a pairwise manner, rendering it unnecessary. The normalisation has been replaced by the more appropriate standard deviation for each variable, which significantly reduces the wall time, but will return slightly different estimates of the correlation coefficients in cases where the observations between a pair of variables are not aligned. However, it will return the same estimates in all other cases, including returning the same correlation matrix as `corrcoef` when using a masked array with no masked values.

([gh-26285](https://github.com/numpy/numpy/pull/26285))

### Cast-safety fixes in `copyto` and `full`

`copyto` now uses NEP 50 correctly and applies this to its cast safety. Python integer to NumPy integer casts and Python float to NumPy float casts are now considered "safe" even if assignment may fail or precision may be lost. This means the following examples change slightly:

  -   - `np.copyto(int8_arr, 1000)` previously performed an unsafe/same-kind cast  
        of the Python integer. It will now always raise, to achieve an unsafe cast you must pass an array or NumPy scalar.

  - `np.copyto(uint8_arr, 1000, casting="safe")` will raise an OverflowError rather than a TypeError due to same-kind casting.

  - `np.copyto(float32_arr, 1e300, casting="safe")` will overflow to `inf` (float32 cannot hold `1e300`) rather raising a TypeError.

Further, only the dtype is used when assigning NumPy scalars (or 0-d arrays), meaning that the following behaves differently:

  - `np.copyto(float32_arr, np.float64(3.0), casting="safe")` raises.
  - `np.coptyo(int8_arr, np.int64(100), casting="safe")` raises. Previously, NumPy checked whether the 100 fits the `int8_arr`.

This aligns `copyto`, `full`, and `full_like` with the correct NumPy 2 behavior.

([gh-27091](https://github.com/numpy/numpy/pull/27091))

---

2.1.1-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.1.1 Release Notes

NumPy 2.1.1 is a maintenance release that fixes bugs and regressions discovered after the 2.1.0 release.

The Python versions supported by this release are 3.10-3.13.

## Contributors

A total of 7 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Andrew Nelson
  - Charles Harris
  - Mateusz SokÃ³Å‚
  - Maximilian Weigand +
  - Nathan Goldbaum
  - Pieter Eendebak
  - Sebastian Berg

## Pull requests merged

A total of 10 pull requests were merged for this release.

  - [\#27236](https://github.com/numpy/numpy/pull/27236): REL: Prepare for the NumPy 2.1.0 release \[wheel build\]
  - [\#27252](https://github.com/numpy/numpy/pull/27252): MAINT: prepare 2.1.x for further development
  - [\#27259](https://github.com/numpy/numpy/pull/27259): BUG: revert unintended change in the return value of set\_printoptions
  - [\#27266](https://github.com/numpy/numpy/pull/27266): BUG: fix reference counting bug in \_\_array\_interface\_\_ implementationâ€¦
  - [\#27267](https://github.com/numpy/numpy/pull/27267): TST: Add regression test for missing descr in array-interface
  - [\#27276](https://github.com/numpy/numpy/pull/27276): BUG: Fix \#27256 and \#27257
  - [\#27278](https://github.com/numpy/numpy/pull/27278): BUG: Fix array\_equal for numeric and non-numeric scalar types
  - [\#27287](https://github.com/numpy/numpy/pull/27287): MAINT: Update maintenance/2.1.x after the 2.0.2 release
  - [\#27303](https://github.com/numpy/numpy/pull/27303): BLD: cp311- macosx\_arm64 wheels \[wheel build\]
  - [\#27304](https://github.com/numpy/numpy/pull/27304): BUG: f2py: better handle filtering of public/private subroutines

---

2.1.2-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.1.2 Release Notes

NumPy 2.1.2 is a maintenance release that fixes bugs and regressions discovered after the 2.1.1 release.

The Python versions supported by this release are 3.10-3.13.

## Contributors

A total of 11 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Charles Harris
  - Chris Sidebottom
  - Ishan Koradia +
  - JoÃ£o Eiras +
  - Katie Rust +
  - Marten van Kerkwijk
  - Matti Picus
  - Nathan Goldbaum
  - Peter Hawkins
  - Pieter Eendebak
  - Slava Gorloff +

## Pull requests merged

A total of 14 pull requests were merged for this release.

  - [\#27333](https://github.com/numpy/numpy/pull/27333): MAINT: prepare 2.1.x for further development
  - [\#27400](https://github.com/numpy/numpy/pull/27400): BUG: apply critical sections around populating the dispatch cache
  - [\#27406](https://github.com/numpy/numpy/pull/27406): BUG: Stub out get\_build\_msvc\_version if distutils.msvccompiler...
  - [\#27416](https://github.com/numpy/numpy/pull/27416): BUILD: fix missing include for std::ptrdiff\_t for C++23 language...
  - [\#27433](https://github.com/numpy/numpy/pull/27433): BLD: pin setuptools to avoid breaking numpy.distutils
  - [\#27437](https://github.com/numpy/numpy/pull/27437): BUG: Allow unsigned shift argument for np.roll
  - [\#27439](https://github.com/numpy/numpy/pull/27439): BUG: Disable SVE VQSort
  - [\#27471](https://github.com/numpy/numpy/pull/27471): BUG: rfftn axis bug
  - [\#27479](https://github.com/numpy/numpy/pull/27479): BUG: Fix extra decref of PyArray\_UInt8DType.
  - [\#27480](https://github.com/numpy/numpy/pull/27480): CI: use PyPI not scientific-python-nightly-wheels for CI doc...
  - [\#27481](https://github.com/numpy/numpy/pull/27481): MAINT: Check for SVE support on demand
  - [\#27484](https://github.com/numpy/numpy/pull/27484): BUG: initialize the promotion state to be weak
  - [\#27501](https://github.com/numpy/numpy/pull/27501): MAINT: Bump pypa/cibuildwheel from 2.20.0 to 2.21.2
  - [\#27506](https://github.com/numpy/numpy/pull/27506): BUG: avoid segfault on bad arguments in ndarray.\_\_array\_function\_\_

---

2.1.3-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.1.3 Release Notes

NumPy 2.1.3 is a maintenance release that fixes bugs and regressions discovered after the 2.1.2 release.

The Python versions supported by this release are 3.10-3.13.

## Improvements

  - Fixed a number of issues around promotion for string ufuncs with StringDType arguments. Mixing StringDType and the fixed-width DTypes using the string ufuncs should now generate much more uniform results.
    
    ([gh-27636](https://github.com/numpy/numpy/pull/27636))

## Changes

  - <span class="title-ref">numpy.fix</span> now won't perform casting to a floating data-type for integer and boolean data-type input arrays.
    
    ([gh-26766](https://github.com/numpy/numpy/pull/26766))

## Contributors

A total of 15 people contributed to this release. People with a "+" by their names contributed a patch for the first time.

  - Abhishek Kumar +
  - Austin +
  - Benjamin A. Beasley +
  - Charles Harris
  - Christian Lorentzen
  - Marcel Telka +
  - Matti Picus
  - Michael Davidsaver +
  - Nathan Goldbaum
  - Peter Hawkins
  - Raghuveer Devulapalli
  - Ralf Gommers
  - Sebastian Berg
  - dependabot\[bot\]
  - kp2pml30 +

## Pull requests merged

A total of 21 pull requests were merged for this release.

  - [\#27512](https://github.com/numpy/numpy/pull/27512): MAINT: prepare 2.1.x for further development
  - [\#27537](https://github.com/numpy/numpy/pull/27537): MAINT: Bump actions/cache from 4.0.2 to 4.1.1
  - [\#27538](https://github.com/numpy/numpy/pull/27538): MAINT: Bump pypa/cibuildwheel from 2.21.2 to 2.21.3
  - [\#27539](https://github.com/numpy/numpy/pull/27539): MAINT: MSVC does not support \#warning directive
  - [\#27543](https://github.com/numpy/numpy/pull/27543): BUG: Fix user dtype can-cast with python scalar during promotion
  - [\#27561](https://github.com/numpy/numpy/pull/27561): DEV: bump `python` to 3.12 in environment.yml
  - [\#27562](https://github.com/numpy/numpy/pull/27562): BLD: update vendored Meson to 1.5.2
  - [\#27563](https://github.com/numpy/numpy/pull/27563): BUG: weighted quantile for some zero weights (\#27549)
  - [\#27565](https://github.com/numpy/numpy/pull/27565): MAINT: Use miniforge for macos conda test.
  - [\#27566](https://github.com/numpy/numpy/pull/27566): BUILD: satisfy gcc-13 pendantic errors
  - [\#27569](https://github.com/numpy/numpy/pull/27569): BUG: handle possible error for PyTraceMallocTrack
  - [\#27570](https://github.com/numpy/numpy/pull/27570): BLD: start building Windows free-threaded wheels \[wheel build\]
  - [\#27571](https://github.com/numpy/numpy/pull/27571): BUILD: vendor tempita from Cython
  - [\#27574](https://github.com/numpy/numpy/pull/27574): BUG: Fix warning "differs in levels of indirection" in npy\_atomic.h...
  - [\#27592](https://github.com/numpy/numpy/pull/27592): MAINT: Update Highway to latest
  - [\#27593](https://github.com/numpy/numpy/pull/27593): BUG: Adjust numpy.i for SWIG 4.3 compatibility
  - [\#27616](https://github.com/numpy/numpy/pull/27616): BUG: Fix Linux QEMU CI workflow
  - [\#27668](https://github.com/numpy/numpy/pull/27668): BLD: Do not set \_\_STDC\_VERSION\_\_ to zero during build
  - [\#27669](https://github.com/numpy/numpy/pull/27669): ENH: fix wasm32 runtime type error in numpy.\_core
  - [\#27672](https://github.com/numpy/numpy/pull/27672): BUG: Fix a reference count leak in npy\_find\_descr\_for\_scalar.
  - [\#27673](https://github.com/numpy/numpy/pull/27673): BUG: fixes for StringDType/unicode promoters

---

2.2.0-notes.md

---

<div class="currentmodule">

numpy

</div>

# NumPy 2.2.0 Release Notes

## Highlights

*We'll choose highlights for this release near the end of the release cycle.*

\<\!-- Failed to include notes-towncrier.rst --\>

---

template.md

---

- orphan

<div class="currentmodule">

numpy

</div>

# NumPy 2.xx.x Release Notes

## Highlights

*We'll choose highlights for this release near the end of the release cycle.*

\<\!-- Failed to include notes-towncrier.rst --\>

---

release.md

---

# Release notes

<div class="toctree" data-maxdepth="2">

2.2.0 \<release/2.2.0-notes\> 2.1.3 \<release/2.1.3-notes\> 2.1.2 \<release/2.1.2-notes\> 2.1.1 \<release/2.1.1-notes\> 2.1.0 \<release/2.1.0-notes\> 2.0.2 \<release/2.0.2-notes\> 2.0.1 \<release/2.0.1-notes\> 2.0.0 \<release/2.0.0-notes\> 1.26.4 \<release/1.26.4-notes\> 1.26.3 \<release/1.26.3-notes\> 1.26.2 \<release/1.26.2-notes\> 1.26.1 \<release/1.26.1-notes\> 1.26.0 \<release/1.26.0-notes\> 1.25.2 \<release/1.25.2-notes\> 1.25.1 \<release/1.25.1-notes\> 1.25.0 \<release/1.25.0-notes\> 1.24.4 \<release/1.24.4-notes\> 1.24.3 \<release/1.24.3-notes\> 1.24.2 \<release/1.24.2-notes\> 1.24.1 \<release/1.24.1-notes\> 1.24.0 \<release/1.24.0-notes\> 1.23.5 \<release/1.23.5-notes\> 1.23.4 \<release/1.23.4-notes\> 1.23.3 \<release/1.23.3-notes\> 1.23.2 \<release/1.23.2-notes\> 1.23.1 \<release/1.23.1-notes\> 1.23.0 \<release/1.23.0-notes\> 1.22.4 \<release/1.22.4-notes\> 1.22.3 \<release/1.22.3-notes\> 1.22.2 \<release/1.22.2-notes\> 1.22.1 \<release/1.22.1-notes\> 1.22.0 \<release/1.22.0-notes\> 1.21.6 \<release/1.21.6-notes\> 1.21.5 \<release/1.21.5-notes\> 1.21.4 \<release/1.21.4-notes\> 1.21.3 \<release/1.21.3-notes\> 1.21.2 \<release/1.21.2-notes\> 1.21.1 \<release/1.21.1-notes\> 1.21.0 \<release/1.21.0-notes\> 1.20.3 \<release/1.20.3-notes\> 1.20.2 \<release/1.20.2-notes\> 1.20.1 \<release/1.20.1-notes\> 1.20.0 \<release/1.20.0-notes\> 1.19.5 \<release/1.19.5-notes\> 1.19.4 \<release/1.19.4-notes\> 1.19.3 \<release/1.19.3-notes\> 1.19.2 \<release/1.19.2-notes\> 1.19.1 \<release/1.19.1-notes\> 1.19.0 \<release/1.19.0-notes\> 1.18.5 \<release/1.18.5-notes\> 1.18.4 \<release/1.18.4-notes\> 1.18.3 \<release/1.18.3-notes\> 1.18.2 \<release/1.18.2-notes\> 1.18.1 \<release/1.18.1-notes\> 1.18.0 \<release/1.18.0-notes\> 1.17.5 \<release/1.17.5-notes\> 1.17.4 \<release/1.17.4-notes\> 1.17.3 \<release/1.17.3-notes\> 1.17.2 \<release/1.17.2-notes\> 1.17.1 \<release/1.17.1-notes\> 1.17.0 \<release/1.17.0-notes\> 1.16.6 \<release/1.16.6-notes\> 1.16.5 \<release/1.16.5-notes\> 1.16.4 \<release/1.16.4-notes\> 1.16.3 \<release/1.16.3-notes\> 1.16.2 \<release/1.16.2-notes\> 1.16.1 \<release/1.16.1-notes\> 1.16.0 \<release/1.16.0-notes\> 1.15.4 \<release/1.15.4-notes\> 1.15.3 \<release/1.15.3-notes\> 1.15.2 \<release/1.15.2-notes\> 1.15.1 \<release/1.15.1-notes\> 1.15.0 \<release/1.15.0-notes\> 1.14.6 \<release/1.14.6-notes\> 1.14.5 \<release/1.14.5-notes\> 1.14.4 \<release/1.14.4-notes\> 1.14.3 \<release/1.14.3-notes\> 1.14.2 \<release/1.14.2-notes\> 1.14.1 \<release/1.14.1-notes\> 1.14.0 \<release/1.14.0-notes\> 1.13.3 \<release/1.13.3-notes\> 1.13.2 \<release/1.13.2-notes\> 1.13.1 \<release/1.13.1-notes\> 1.13.0 \<release/1.13.0-notes\> 1.12.1 \<release/1.12.1-notes\> 1.12.0 \<release/1.12.0-notes\> 1.11.3 \<release/1.11.3-notes\> 1.11.2 \<release/1.11.2-notes\> 1.11.1 \<release/1.11.1-notes\> 1.11.0 \<release/1.11.0-notes\> 1.10.4 \<release/1.10.4-notes\> 1.10.3 \<release/1.10.3-notes\> 1.10.2 \<release/1.10.2-notes\> 1.10.1 \<release/1.10.1-notes\> 1.10.0 \<release/1.10.0-notes\> 1.9.2 \<release/1.9.2-notes\> 1.9.1 \<release/1.9.1-notes\> 1.9.0 \<release/1.9.0-notes\> 1.8.2 \<release/1.8.2-notes\> 1.8.1 \<release/1.8.1-notes\> 1.8.0 \<release/1.8.0-notes\> 1.7.2 \<release/1.7.2-notes\> 1.7.1 \<release/1.7.1-notes\> 1.7.0 \<release/1.7.0-notes\> 1.6.2 \<release/1.6.2-notes\> 1.6.1 \<release/1.6.1-notes\> 1.6.0 \<release/1.6.0-notes\> 1.5.0 \<release/1.5.0-notes\> 1.4.0 \<release/1.4.0-notes\> 1.3.0 \<release/1.3.0-notes\>

</div>

---

absolute_beginners.md

---

# NumPy: the absolute basics for beginners

<div class="currentmodule">

numpy

</div>

Welcome to the absolute beginner's guide to NumPy\!

NumPy (**Num**erical **Py**thon) is an open source Python library that's widely used in science and engineering. The NumPy library contains multidimensional array data structures, such as the homogeneous, N-dimensional `ndarray`, and a large library of functions that operate efficiently on these data structures. Learn more about NumPy at \[What is NumPy \<whatisnumpy\>\](\#what-is-numpy-\<whatisnumpy\>), and if you have comments or suggestions, please [reach out](https://numpy.org/community/)\!

## How to import NumPy

After [installing NumPy](https://numpy.org/install/), it may be imported into Python code like:

    import numpy as np

This widespread convention allows access to NumPy features with a short, recognizable prefix (`np.`) while distinguishing NumPy features from others that have the same name.

## Reading the example code

Throughout the NumPy documentation, you will find blocks that look like:

    >>> a = np.array([[1, 2, 3],
    ...               [4, 5, 6]])
    >>> a.shape
    (2, 3)

Text preceded by `>>>` or `...` is **input**, the code that you would enter in a script or at a Python prompt. Everything else is **output**, the results of running your code. Note that `>>>` and `...` are not part of the code and may cause an error if entered at a Python prompt.

## Why use NumPy?

Python lists are excellent, general-purpose containers. They can be "heterogeneous", meaning that they can contain elements of a variety of types, and they are quite fast when used to perform individual operations on a handful of elements.

Depending on the characteristics of the data and the types of operations that need to be performed, other containers may be more appropriate; by exploiting these characteristics, we can improve speed, reduce memory consumption, and offer a high-level syntax for performing a variety of common processing tasks. NumPy shines when there are large quantities of "homogeneous" (same-type) data to be processed on the CPU.

## What is an "array"?

In computer programming, an array is a structure for storing and retrieving data. We often talk about an array as if it were a grid in space, with each cell storing one element of the data. For instance, if each element of the data were a number, we might visualize a "one-dimensional" array like a list:

\[\begin{aligned}
\begin{array}{|c||c|c|c|}
\hline
1 & 5 & 2 & 0 \\
\hline
\end{array}
\end{aligned}\]

A two-dimensional array would be like a table:

\[\begin{aligned}
\begin{array}{|c||c|c|c|}
\hline
1 & 5 & 2 & 0 \\
\hline
8 & 3 & 6 & 1 \\
\hline
1 & 7 & 2 & 9 \\
\hline
\end{array}
\end{aligned}\]

A three-dimensional array would be like a set of tables, perhaps stacked as though they were printed on separate pages. In NumPy, this idea is generalized to an arbitrary number of dimensions, and so the fundamental array class is called `ndarray`: it represents an "N-dimensional array".

Most NumPy arrays have some restrictions. For instance:

  - All elements of the array must be of the same type of data.
  - Once created, the total size of the array can't change.
  - The shape must be "rectangular", not "jagged"; e.g., each row of a two-dimensional array must have the same number of columns.

When these conditions are met, NumPy exploits these characteristics to make the array faster, more memory efficient, and more convenient to use than less restrictive data structures.

For the remainder of this document, we will use the word "array" to refer to an instance of `ndarray`.

## Array fundamentals

One way to initialize an array is using a Python sequence, such as a list. For example:

    >>> a = np.array([1, 2, 3, 4, 5, 6])
    >>> a
    array([1, 2, 3, 4, 5, 6])

Elements of an array can be accessed in \[various ways \<quickstart.indexing-slicing-and-iterating\>\](\#various-ways \<quickstart.indexing-slicing-and-iterating\>). For instance, we can access an individual element of this array as we would access an element in the original list: using the integer index of the element within square brackets.

> \>\>\> a\[0\] 1

\> **Note** \> As with built-in Python sequences, NumPy arrays are "0-indexed": the first element of the array is accessed using index `0`, not `1`.

Like the original list, the array is mutable.

> \>\>\> a\[0\] = 10 \>\>\> a array(\[10, 2, 3, 4, 5, 6\])

Also like the original list, Python slice notation can be used for indexing.

> \>\>\> a\[:3\] array(\[10, 2, 3\])

One major difference is that slice indexing of a list copies the elements into a new list, but slicing an array returns a *view*: an object that refers to the data in the original array. The original array can be mutated using the view.

> \>\>\> b = a\[3:\] \>\>\> b array(\[4, 5, 6\]) \>\>\> b\[0\] = 40 \>\>\> a array(\[ 10, 2, 3, 40, 5, 6\])

See \[basics.copies-and-views\](\#basics.copies-and-views) for a more comprehensive explanation of when array operations return views rather than copies.

Two- and higher-dimensional arrays can be initialized from nested Python sequences:

    >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
    >>> a
    array([[ 1,  2,  3,  4],
           [ 5,  6,  7,  8],
           [ 9, 10, 11, 12]])

In NumPy, a dimension of an array is sometimes referred to as an "axis". This terminology may be useful to disambiguate between the dimensionality of an array and the dimensionality of the data represented by the array. For instance, the array `a` could represent three points, each lying within a four-dimensional space, but `a` has only two "axes".

Another difference between an array and a list of lists is that an element of the array can be accessed by specifying the index along each axis within a *single* set of square brackets, separated by commas. For instance, the element `8` is in row `1` and column `3`:

> \>\>\> a\[1, 3\] 8

\> **Note** \> It is familiar practice in mathematics to refer to elements of a matrix by the row index first and the column index second. This happens to be true for two-dimensional arrays, but a better mental model is to think of the column index as coming *last* and the row index as *second to last*. This generalizes to arrays with *any* number of dimensions.

<div class="note">

<div class="title">

Note

</div>

You might hear of a 0-D (zero-dimensional) array referred to as a "scalar", a 1-D (one-dimensional) array as a "vector", a 2-D (two-dimensional) array as a "matrix", or an N-D (N-dimensional, where "N" is typically an integer greater than 2) array as a "tensor". For clarity, it is best to avoid the mathematical terms when referring to an array because the mathematical objects with these names behave differently than arrays (e.g. "matrix" multiplication is fundamentally different from "array" multiplication), and there are other objects in the scientific Python ecosystem that have these names (e.g. the fundamental data structure of PyTorch is the "tensor").

</div>

## Array attributes

*This section covers the* `ndim`, `shape`, `size`, *and* `dtype` *attributes of an array*.

-----

The number of dimensions of an array is contained in the `ndim` attribute.

> \>\>\> a.ndim 2

The shape of an array is a tuple of non-negative integers that specify the number of elements along each dimension.

> \>\>\> a.shape (3, 4) \>\>\> len(a.shape) == a.ndim True

The fixed, total number of elements in array is contained in the `size` attribute.

> \>\>\> a.size 12 \>\>\> import math \>\>\> a.size == math.prod(a.shape) True

Arrays are typically "homogeneous", meaning that they contain elements of only one "data type". The data type is recorded in the `dtype` attribute.

> \>\>\> a.dtype dtype('int64') \# "int" for integer, "64" for 64-bit

\[Read more about array attributes here \<arrays.ndarray\>\](\#read-more-about-array-attributes-here-\<arrays.ndarray\>) and learn about \[array objects here \<arrays\>\](\#array-objects-here-\<arrays\>).

## How to create a basic array

*This section covers* `np.zeros()`, `np.ones()`, `np.empty()`, `np.arange()`, `np.linspace()`

-----

Besides creating an array from a sequence of elements, you can easily create an array filled with `0`'s:

    >>> np.zeros(2)
    array([0., 0.])

Or an array filled with `1`'s:

    >>> np.ones(2)
    array([1., 1.])

Or even an empty array\! The function `empty` creates an array whose initial content is random and depends on the state of the memory. The reason to use `empty` over `zeros` (or something similar) is speed - just make sure to fill every element afterwards\! :

    >>> # Create an empty array with 2 elements
    >>> np.empty(2) #doctest: +SKIP
    array([3.14, 42.  ])  # may vary

You can create an array with a range of elements:

    >>> np.arange(4)
    array([0, 1, 2, 3])

And even an array that contains a range of evenly spaced intervals. To do this, you will specify the **first number**, **last number**, and the **step size**. :

    >>> np.arange(2, 9, 2)
    array([2, 4, 6, 8])

You can also use `np.linspace()` to create an array with values that are spaced linearly in a specified interval:

    >>> np.linspace(0, 10, num=5)
    array([ 0. ,  2.5,  5. ,  7.5, 10. ])

**Specifying your data type**

While the default data type is floating point (`np.float64`), you can explicitly specify which data type you want using the `dtype` keyword. :

    >>> x = np.ones(2, dtype=np.int64)
    >>> x
    array([1, 1])

\[Learn more about creating arrays here \<quickstart.array-creation\>\](\#learn-more-about-creating-arrays-here-\<quickstart.array-creation\>)

## Adding, removing, and sorting elements

*This section covers* `np.sort()`, `np.concatenate()`

-----

Sorting an array is simple with `np.sort()`. You can specify the axis, kind, and order when you call the function.

If you start with this array:

    >>> arr = np.array([2, 1, 5, 3, 7, 4, 6, 8])

You can quickly sort the numbers in ascending order with:

    >>> np.sort(arr)
    array([1, 2, 3, 4, 5, 6, 7, 8])

In addition to sort, which returns a sorted copy of an array, you can use:

  - <span class="title-ref">argsort</span>, which is an indirect sort along a specified axis,
  - <span class="title-ref">lexsort</span>, which is an indirect stable sort on multiple keys,
  - <span class="title-ref">searchsorted</span>, which will find elements in a sorted array, and
  - <span class="title-ref">partition</span>, which is a partial sort.

To read more about sorting an array, see: <span class="title-ref">sort</span>.

If you start with these arrays:

    >>> a = np.array([1, 2, 3, 4])
    >>> b = np.array([5, 6, 7, 8])

You can concatenate them with `np.concatenate()`. :

    >>> np.concatenate((a, b))
    array([1, 2, 3, 4, 5, 6, 7, 8])

Or, if you start with these arrays:

    >>> x = np.array([[1, 2], [3, 4]])
    >>> y = np.array([[5, 6]])

You can concatenate them with:

    >>> np.concatenate((x, y), axis=0)
    array([[1, 2],
           [3, 4],
           [5, 6]])

In order to remove elements from an array, it's simple to use indexing to select the elements that you want to keep.

To read more about concatenate, see: <span class="title-ref">concatenate</span>.

## How do you know the shape and size of an array?

*This section covers* `ndarray.ndim`, `ndarray.size`, `ndarray.shape`

-----

`ndarray.ndim` will tell you the number of axes, or dimensions, of the array.

`ndarray.size` will tell you the total number of elements of the array. This is the *product* of the elements of the array's shape.

`ndarray.shape` will display a tuple of integers that indicate the number of elements stored along each dimension of the array. If, for example, you have a 2-D array with 2 rows and 3 columns, the shape of your array is `(2, 3)`.

For example, if you create this array:

    >>> array_example = np.array([[[0, 1, 2, 3],
    ...                            [4, 5, 6, 7]],
    ...
    ...                           [[0, 1, 2, 3],
    ...                            [4, 5, 6, 7]],
    ...
    ...                           [[0 ,1 ,2, 3],
    ...                            [4, 5, 6, 7]]])

To find the number of dimensions of the array, run:

    >>> array_example.ndim
    3

To find the total number of elements in the array, run:

    >>> array_example.size
    24

And to find the shape of your array, run:

    >>> array_example.shape
    (3, 2, 4)

## Can you reshape an array?

*This section covers* `arr.reshape()`

-----

**Yes\!**

Using `arr.reshape()` will give a new shape to an array without changing the data. Just remember that when you use the reshape method, the array you want to produce needs to have the same number of elements as the original array. If you start with an array with 12 elements, you'll need to make sure that your new array also has a total of 12 elements.

If you start with this array:

    >>> a = np.arange(6)
    >>> print(a)
    [0 1 2 3 4 5]

You can use `reshape()` to reshape your array. For example, you can reshape this array to an array with three rows and two columns:

    >>> b = a.reshape(3, 2)
    >>> print(b)
    [[0 1]
     [2 3]
     [4 5]]

With `np.reshape`, you can specify a few optional parameters:

    >>> np.reshape(a, shape=(1, 6), order='C')
    array([[0, 1, 2, 3, 4, 5]])

`a` is the array to be reshaped.

`shape` is the new shape you want. You can specify an integer or a tuple of integers. If you specify an integer, the result will be an array of that length. The shape should be compatible with the original shape.

`order:` `C` means to read/write the elements using C-like index order, `F` means to read/write the elements using Fortran-like index order, `A` means to read/write the elements in Fortran-like index order if a is Fortran contiguous in memory, C-like order otherwise. (This is an optional parameter and doesn't need to be specified.)

If you want to learn more about C and Fortran order, you can \[read more about the internal organization of NumPy arrays here \<numpy-internals\>\](\#read-more-about-the-internal-organization-of-numpy-arrays-here-\<numpy-internals\>). Essentially, C and Fortran orders have to do with how indices correspond to the order the array is stored in memory. In Fortran, when moving through the elements of a two-dimensional array as it is stored in memory, the **first** index is the most rapidly varying index. As the first index moves to the next row as it changes, the matrix is stored one column at a time. This is why Fortran is thought of as a **Column-major language**. In C on the other hand, the **last** index changes the most rapidly. The matrix is stored by rows, making it a **Row-major language**. What you do for C or Fortran depends on whether it's more important to preserve the indexing convention or not reorder the data.

\[Learn more about shape manipulation here \<quickstart.shape-manipulation\>\](\#learn-more-about-shape-manipulation-here-\<quickstart.shape-manipulation\>).

## How to convert a 1D array into a 2D array (how to add a new axis to an array)

*This section covers* `np.newaxis`, `np.expand_dims`

-----

You can use `np.newaxis` and `np.expand_dims` to increase the dimensions of your existing array.

Using `np.newaxis` will increase the dimensions of your array by one dimension when used once. This means that a **1D** array will become a **2D** array, a **2D** array will become a **3D** array, and so on.

For example, if you start with this array:

    >>> a = np.array([1, 2, 3, 4, 5, 6])
    >>> a.shape
    (6,)

You can use `np.newaxis` to add a new axis:

    >>> a2 = a[np.newaxis, :]
    >>> a2.shape
    (1, 6)

You can explicitly convert a 1D array to either a row vector or a column vector using `np.newaxis`. For example, you can convert a 1D array to a row vector by inserting an axis along the first dimension:

    >>> row_vector = a[np.newaxis, :]
    >>> row_vector.shape
    (1, 6)

Or, for a column vector, you can insert an axis along the second dimension:

    >>> col_vector = a[:, np.newaxis]
    >>> col_vector.shape
    (6, 1)

You can also expand an array by inserting a new axis at a specified position with `np.expand_dims`.

For example, if you start with this array:

    >>> a = np.array([1, 2, 3, 4, 5, 6])
    >>> a.shape
    (6,)

You can use `np.expand_dims` to add an axis at index position 1 with:

    >>> b = np.expand_dims(a, axis=1)
    >>> b.shape
    (6, 1)

You can add an axis at index position 0 with:

    >>> c = np.expand_dims(a, axis=0)
    >>> c.shape
    (1, 6)

Find more information about \[newaxis here \<arrays.indexing\>\](\#newaxis-here-\<arrays.indexing\>) and `expand_dims` at <span class="title-ref">expand\_dims</span>.

## Indexing and slicing

You can index and slice NumPy arrays in the same ways you can slice Python lists. :

    >>> data = np.array([1, 2, 3])
    
    >>> data[1]
    2
    >>> data[0:2]
    array([1, 2])
    >>> data[1:]
    array([2, 3])
    >>> data[-2:]
    array([2, 3])

You can visualize it this way:

![image](images/np_indexing.png)

You may want to take a section of your array or specific array elements to use in further analysis or additional operations. To do that, you'll need to subset, slice, and/or index your arrays.

If you want to select values from your array that fulfill certain conditions, it's straightforward with NumPy.

For example, if you start with this array:

    >>> a = np.array([[1 , 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can easily print all of the values in the array that are less than 5. :

    >>> print(a[a < 5])
    [1 2 3 4]

You can also select, for example, numbers that are equal to or greater than 5, and use that condition to index an array. :

    >>> five_up = (a >= 5)
    >>> print(a[five_up])
    [ 5  6  7  8  9 10 11 12]

You can select elements that are divisible by 2:

    >>> divisible_by_2 = a[a%2==0]
    >>> print(divisible_by_2)
    [ 2  4  6  8 10 12]

Or you can select elements that satisfy two conditions using the `&` and `|` operators:

    >>> c = a[(a > 2) & (a < 11)]
    >>> print(c)
    [ 3  4  5  6  7  8  9 10]

You can also make use of the logical operators **&** and **|** in order to return boolean values that specify whether or not the values in an array fulfill a certain condition. This can be useful with arrays that contain names or other categorical values. :

    >>> five_up = (a > 5) | (a == 5)
    >>> print(five_up)
    [[False False False False]
     [ True  True  True  True]
     [ True  True  True True]]

You can also use `np.nonzero()` to select elements or indices from an array.

Starting with this array:

    >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can use `np.nonzero()` to print the indices of elements that are, for example, less than 5:

    >>> b = np.nonzero(a < 5)
    >>> print(b)
    (array([0, 0, 0, 0]), array([0, 1, 2, 3]))

In this example, a tuple of arrays was returned: one for each dimension. The first array represents the row indices where these values are found, and the second array represents the column indices where the values are found.

If you want to generate a list of coordinates where the elements exist, you can zip the arrays, iterate over the list of coordinates, and print them. For example:

    >>> list_of_coordinates= list(zip(b[0], b[1]))
    
    >>> for coord in list_of_coordinates:
    ...     print(coord)
    (np.int64(0), np.int64(0))
    (np.int64(0), np.int64(1))
    (np.int64(0), np.int64(2))
    (np.int64(0), np.int64(3))

You can also use `np.nonzero()` to print the elements in an array that are less than 5 with:

    >>> print(a[b])
    [1 2 3 4]

If the element you're looking for doesn't exist in the array, then the returned array of indices will be empty. For example:

    >>> not_there = np.nonzero(a == 42)
    >>> print(not_there)
    (array([], dtype=int64), array([], dtype=int64))

Learn more about \[indexing and slicing here \<quickstart.indexing-slicing-and-iterating\>\](\#indexing-and-slicing-here-\<quickstart.indexing-slicing-and-iterating\>) and \[here \<basics.indexing\>\](\#here-\<basics.indexing\>).

Read more about using the nonzero function at: <span class="title-ref">nonzero</span>.

## How to create an array from existing data

*This section covers* `slicing and indexing`, `np.vstack()`, `np.hstack()`, `np.hsplit()`, `.view()`, `copy()`

-----

You can easily create a new array from a section of an existing array.

Let's say you have this array:

    >>> a = np.array([1,  2,  3,  4,  5,  6,  7,  8,  9, 10])

You can create a new array from a section of your array any time by specifying where you want to slice your array. :

    >>> arr1 = a[3:8]
    >>> arr1
    array([4, 5, 6, 7, 8])

Here, you grabbed a section of your array from index position 3 through index position 8.

You can also stack two existing arrays, both vertically and horizontally. Let's say you have two arrays, `a1` and `a2`:

    >>> a1 = np.array([[1, 1],
    ...                [2, 2]])
    
    >>> a2 = np.array([[3, 3],
    ...                [4, 4]])

You can stack them vertically with `vstack`:

    >>> np.vstack((a1, a2))
    array([[1, 1],
           [2, 2],
           [3, 3],
           [4, 4]])

Or stack them horizontally with `hstack`:

    >>> np.hstack((a1, a2))
    array([[1, 1, 3, 3],
           [2, 2, 4, 4]])

You can split an array into several smaller arrays using `hsplit`. You can specify either the number of equally shaped arrays to return or the columns *after* which the division should occur.

Let's say you have this array:

    >>> x = np.arange(1, 25).reshape(2, 12)
    >>> x
    array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
           [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])

If you wanted to split this array into three equally shaped arrays, you would run:

    >>> np.hsplit(x, 3)
      [array([[ 1,  2,  3,  4],
             [13, 14, 15, 16]]), array([[ 5,  6,  7,  8],
             [17, 18, 19, 20]]), array([[ 9, 10, 11, 12],
             [21, 22, 23, 24]])]

If you wanted to split your array after the third and fourth column, you'd run:

    >>> np.hsplit(x, (3, 4))
      [array([[ 1,  2,  3],
             [13, 14, 15]]), array([[ 4],
             [16]]), array([[ 5,  6,  7,  8,  9, 10, 11, 12],
             [17, 18, 19, 20, 21, 22, 23, 24]])]

\[Learn more about stacking and splitting arrays here \<quickstart.stacking-arrays\>\](\#learn-more-about-stacking-and-splitting-arrays-here-\<quickstart.stacking-arrays\>).

You can use the `view` method to create a new array object that looks at the same data as the original array (a *shallow copy*).

Views are an important NumPy concept\! NumPy functions, as well as operations like indexing and slicing, will return views whenever possible. This saves memory and is faster (no copy of the data has to be made). However it's important to be aware of this - modifying data in a view also modifies the original array\!

Let's say you create this array:

    >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

Now we create an array `b1` by slicing `a` and modify the first element of `b1`. This will modify the corresponding element in `a` as well\! :

    >>> b1 = a[0, :]
    >>> b1
    array([1, 2, 3, 4])
    >>> b1[0] = 99
    >>> b1
    array([99,  2,  3,  4])
    >>> a
    array([[99,  2,  3,  4],
           [ 5,  6,  7,  8],
           [ 9, 10, 11, 12]])

Using the `copy` method will make a complete copy of the array and its data (a *deep copy*). To use this on your array, you could run:

    >>> b2 = a.copy()

\[Learn more about copies and views here \<quickstart.copies-and-views\>\](\#learn-more-about-copies-and-views-here-\<quickstart.copies-and-views\>).

## Basic array operations

*This section covers addition, subtraction, multiplication, division, and more*

-----

Once you've created your arrays, you can start to work with them. Let's say, for example, that you've created two arrays, one called "data" and one called "ones"

![image](images/np_array_dataones.png)

You can add the arrays together with the plus sign.

    >>> data = np.array([1, 2])
    >>> ones = np.ones(2, dtype=int)
    >>> data + ones
    array([2, 3])

![image](images/np_data_plus_ones.png)

You can, of course, do more than just addition\!

    >>> data - ones
    array([0, 1])
    >>> data * data
    array([1, 4])
    >>> data / data
    array([1., 1.])

![image](images/np_sub_mult_divide.png)

Basic operations are simple with NumPy. If you want to find the sum of the elements in an array, you'd use `sum()`. This works for 1D arrays, 2D arrays, and arrays in higher dimensions. :

    >>> a = np.array([1, 2, 3, 4])
    
    >>> a.sum()
    10

To add the rows or the columns in a 2D array, you would specify the axis.

If you start with this array:

    >>> b = np.array([[1, 1], [2, 2]])

You can sum over the axis of rows with:

    >>> b.sum(axis=0)
    array([3, 3])

You can sum over the axis of columns with:

    >>> b.sum(axis=1)
    array([2, 4])

\[Learn more about basic operations here \<quickstart.basic-operations\>\](\#learn-more-about-basic-operations-here-\<quickstart.basic-operations\>).

## Broadcasting

There are times when you might want to carry out an operation between an array and a single number (also called *an operation between a vector and a scalar*) or between arrays of two different sizes. For example, your array (we'll call it "data") might contain information about distance in miles but you want to convert the information to kilometers. You can perform this operation with:

    >>> data = np.array([1.0, 2.0])
    >>> data * 1.6
    array([1.6, 3.2])

![image](images/np_multiply_broadcasting.png)

NumPy understands that the multiplication should happen with each cell. That concept is called **broadcasting**. Broadcasting is a mechanism that allows NumPy to perform operations on arrays of different shapes. The dimensions of your array must be compatible, for example, when the dimensions of both arrays are equal or when one of them is 1. If the dimensions are not compatible, you will get a `ValueError`.

\[Learn more about broadcasting here \<basics.broadcasting\>\](\#learn-more-about-broadcasting-here-\<basics.broadcasting\>).

## More useful array operations

*This section covers maximum, minimum, sum, mean, product, standard deviation, and more*

-----

NumPy also performs aggregation functions. In addition to `min`, `max`, and `sum`, you can easily run `mean` to get the average, `prod` to get the result of multiplying the elements together, `std` to get the standard deviation, and more. :

    >>> data.max()
    2.0
    >>> data.min()
    1.0
    >>> data.sum()
    3.0

![image](images/np_aggregation.png)

Let's start with this array, called "a" :

    >>> a = np.array([[0.45053314, 0.17296777, 0.34376245, 0.5510652],
    ...               [0.54627315, 0.05093587, 0.40067661, 0.55645993],
    ...               [0.12697628, 0.82485143, 0.26590556, 0.56917101]])

It's very common to want to aggregate along a row or column. By default, every NumPy aggregation function will return the aggregate of the entire array. To find the sum or the minimum of the elements in your array, run:

    >>> a.sum()
    4.8595784

Or:

    >>> a.min()
    0.05093587

You can specify on which axis you want the aggregation function to be computed. For example, you can find the minimum value within each column by specifying `axis=0`. :

    >>> a.min(axis=0)
    array([0.12697628, 0.05093587, 0.26590556, 0.5510652 ])

The four values listed above correspond to the number of columns in your array. With a four-column array, you will get four values as your result.

Read more about \[array methods here \<array.ndarray.methods\>\](\#array-methods-here-\<array.ndarray.methods\>).

## Creating matrices

You can pass Python lists of lists to create a 2-D array (or "matrix") to represent them in NumPy. :

    >>> data = np.array([[1, 2], [3, 4], [5, 6]])
    >>> data
    array([[1, 2],
           [3, 4],
           [5, 6]])

![image](images/np_create_matrix.png)

Indexing and slicing operations are useful when you're manipulating matrices:

    >>> data[0, 1]
    2
    >>> data[1:3]
    array([[3, 4],
           [5, 6]])
    >>> data[0:2, 0]
    array([1, 3])

![image](images/np_matrix_indexing.png)

You can aggregate matrices the same way you aggregated vectors:

    >>> data.max()
    6
    >>> data.min()
    1
    >>> data.sum()
    21

![image](images/np_matrix_aggregation.png)

You can aggregate all the values in a matrix and you can aggregate them across columns or rows using the `axis` parameter. To illustrate this point, let's look at a slightly modified dataset:

    >>> data = np.array([[1, 2], [5, 3], [4, 6]])
    >>> data
    array([[1, 2],
           [5, 3],
           [4, 6]])
    >>> data.max(axis=0)
    array([5, 6])
    >>> data.max(axis=1)
    array([2, 5, 6])

![image](images/np_matrix_aggregation_row.png)

Once you've created your matrices, you can add and multiply them using arithmetic operators if you have two matrices that are the same size. :

    >>> data = np.array([[1, 2], [3, 4]])
    >>> ones = np.array([[1, 1], [1, 1]])
    >>> data + ones
    array([[2, 3],
           [4, 5]])

![image](images/np_matrix_arithmetic.png)

You can do these arithmetic operations on matrices of different sizes, but only if one matrix has only one column or one row. In this case, NumPy will use its broadcast rules for the operation. :

    >>> data = np.array([[1, 2], [3, 4], [5, 6]])
    >>> ones_row = np.array([[1, 1]])
    >>> data + ones_row
    array([[2, 3],
           [4, 5],
           [6, 7]])

![image](images/np_matrix_broadcasting.png)

Be aware that when NumPy prints N-dimensional arrays, the last axis is looped over the fastest while the first axis is the slowest. For instance:

    >>> np.ones((4, 3, 2))
    array([[[1., 1.],
            [1., 1.],
            [1., 1.]],
    <BLANKLINE>
           [[1., 1.],
            [1., 1.],
            [1., 1.]],
    <BLANKLINE>
           [[1., 1.],
            [1., 1.],
            [1., 1.]],
    <BLANKLINE>
           [[1., 1.],
            [1., 1.],
            [1., 1.]]])

There are often instances where we want NumPy to initialize the values of an array. NumPy offers functions like `ones()` and `zeros()`, and the `random.Generator` class for random number generation for that. All you need to do is pass in the number of elements you want it to generate:

    >>> np.ones(3)
    array([1., 1., 1.])
    >>> np.zeros(3)
    array([0., 0., 0.])
    >>> rng = np.random.default_rng()  # the simplest way to generate random numbers
    >>> rng.random(3) #doctest: +SKIP
    array([0.63696169, 0.26978671, 0.04097352])

![image](images/np_ones_zeros_random.png)

You can also use `ones()`, `zeros()`, and `random()` to create a 2D array if you give them a tuple describing the dimensions of the matrix:

    >>> np.ones((3, 2))
    array([[1., 1.],
           [1., 1.],
           [1., 1.]])
    >>> np.zeros((3, 2))
    array([[0., 0.],
           [0., 0.],
           [0., 0.]])
    >>> rng.random((3, 2)) #doctest: +SKIP
    array([[0.01652764, 0.81327024],
           [0.91275558, 0.60663578],
           [0.72949656, 0.54362499]])  # may vary

![image](images/np_ones_zeros_matrix.png)

Read more about creating arrays, filled with `0`'s, `1`'s, other values or uninitialized, at \[array creation routines \<routines.array-creation\>\](\#array-creation-routines-\<routines.array-creation\>).

## Generating random numbers

The use of random number generation is an important part of the configuration and evaluation of many numerical and machine learning algorithms. Whether you need to randomly initialize weights in an artificial neural network, split data into random sets, or randomly shuffle your dataset, being able to generate random numbers (actually, repeatable pseudo-random numbers) is essential.

With `Generator.integers`, you can generate random integers from low (remember that this is inclusive with NumPy) to high (exclusive). You can set `endpoint=True` to make the high number inclusive.

You can generate a 2 x 4 array of random integers between 0 and 4 with:

    >>> rng.integers(5, size=(2, 4)) #doctest: +SKIP
    array([[2, 1, 1, 0],
           [0, 0, 0, 4]])  # may vary

\[Read more about random number generation here \<numpyrandom\>\](\#read-more-about-random-number-generation-here-\<numpyrandom\>).

## How to get unique items and counts

*This section covers* `np.unique()`

-----

You can find the unique elements in an array easily with `np.unique`.

For example, if you start with this array:

    >>> a = np.array([11, 11, 12, 13, 14, 15, 16, 17, 12, 13, 11, 14, 18, 19, 20])

you can use `np.unique` to print the unique values in your array:

    >>> unique_values = np.unique(a)
    >>> print(unique_values)
    [11 12 13 14 15 16 17 18 19 20]

To get the indices of unique values in a NumPy array (an array of first index positions of unique values in the array), just pass the `return_index` argument in `np.unique()` as well as your array. :

    >>> unique_values, indices_list = np.unique(a, return_index=True)
    >>> print(indices_list)
    [ 0  2  3  4  5  6  7 12 13 14]

You can pass the `return_counts` argument in `np.unique()` along with your array to get the frequency count of unique values in a NumPy array. :

    >>> unique_values, occurrence_count = np.unique(a, return_counts=True)
    >>> print(occurrence_count)
    [3 2 2 2 1 1 1 1 1 1]

This also works with 2D arrays\! If you start with this array:

    >>> a_2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [1, 2, 3, 4]])

You can find unique values with:

    >>> unique_values = np.unique(a_2d)
    >>> print(unique_values)
    [ 1  2  3  4  5  6  7  8  9 10 11 12]

If the axis argument isn't passed, your 2D array will be flattened.

If you want to get the unique rows or columns, make sure to pass the `axis` argument. To find the unique rows, specify `axis=0` and for columns, specify `axis=1`. :

    >>> unique_rows = np.unique(a_2d, axis=0)
    >>> print(unique_rows)
    [[ 1  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]]

To get the unique rows, index position, and occurrence count, you can use:

    >>> unique_rows, indices, occurrence_count = np.unique(
    ...      a_2d, axis=0, return_counts=True, return_index=True)
    >>> print(unique_rows)
    [[ 1  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]]
    >>> print(indices)
    [0 1 2]
    >>> print(occurrence_count)
    [2 1 1]

To learn more about finding the unique elements in an array, see <span class="title-ref">unique</span>.

## Transposing and reshaping a matrix

*This section covers* `arr.reshape()`, `arr.transpose()`, `arr.T`

-----

It's common to need to transpose your matrices. NumPy arrays have the property `T` that allows you to transpose a matrix.

![image](images/np_transposing_reshaping.png)

You may also need to switch the dimensions of a matrix. This can happen when, for example, you have a model that expects a certain input shape that is different from your dataset. This is where the `reshape` method can be useful. You simply need to pass in the new dimensions that you want for the matrix. :

    >>> data.reshape(2, 3)
    array([[1, 2, 3],
           [4, 5, 6]])
    >>> data.reshape(3, 2)
    array([[1, 2],
           [3, 4],
           [5, 6]])

![image](images/np_reshape.png)

You can also use `.transpose()` to reverse or change the axes of an array according to the values you specify.

If you start with this array:

    >>> arr = np.arange(6).reshape((2, 3))
    >>> arr
    array([[0, 1, 2],
           [3, 4, 5]])

You can transpose your array with `arr.transpose()`. :

    >>> arr.transpose()
    array([[0, 3],
           [1, 4],
           [2, 5]])

You can also use `arr.T`:

    >>> arr.T
    array([[0, 3],
           [1, 4],
           [2, 5]])

To learn more about transposing and reshaping arrays, see <span class="title-ref">transpose</span> and <span class="title-ref">reshape</span>.

## How to reverse an array

*This section covers* `np.flip()`

-----

NumPy's `np.flip()` function allows you to flip, or reverse, the contents of an array along an axis. When using `np.flip()`, specify the array you would like to reverse and the axis. If you don't specify the axis, NumPy will reverse the contents along all of the axes of your input array.

**Reversing a 1D array**

If you begin with a 1D array like this one:

    >>> arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

You can reverse it with:

    >>> reversed_arr = np.flip(arr)

If you want to print your reversed array, you can run:

    >>> print('Reversed Array: ', reversed_arr)
    Reversed Array:  [8 7 6 5 4 3 2 1]

**Reversing a 2D array**

A 2D array works much the same way.

If you start with this array:

    >>> arr_2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can reverse the content in all of the rows and all of the columns with:

    >>> reversed_arr = np.flip(arr_2d)
    >>> print(reversed_arr)
    [[12 11 10  9]
     [ 8  7  6  5]
     [ 4  3  2  1]]

You can easily reverse only the *rows* with:

    >>> reversed_arr_rows = np.flip(arr_2d, axis=0)
    >>> print(reversed_arr_rows)
    [[ 9 10 11 12]
     [ 5  6  7  8]
     [ 1  2  3  4]]

Or reverse only the *columns* with:

    >>> reversed_arr_columns = np.flip(arr_2d, axis=1)
    >>> print(reversed_arr_columns)
    [[ 4  3  2  1]
     [ 8  7  6  5]
     [12 11 10  9]]

You can also reverse the contents of only one column or row. For example, you can reverse the contents of the row at index position 1 (the second row):

    >>> arr_2d[1] = np.flip(arr_2d[1])
    >>> print(arr_2d)
    [[ 1  2  3  4]
     [ 8  7  6  5]
     [ 9 10 11 12]]

You can also reverse the column at index position 1 (the second column):

    >>> arr_2d[:,1] = np.flip(arr_2d[:,1])
    >>> print(arr_2d)
    [[ 1 10  3  4]
     [ 8  7  6  5]
     [ 9  2 11 12]]

Read more about reversing arrays at <span class="title-ref">flip</span>.

## Reshaping and flattening multidimensional arrays

*This section covers* `.flatten()`, `ravel()`

-----

There are two popular ways to flatten an array: `.flatten()` and `.ravel()`. The primary difference between the two is that the new array created using `ravel()` is actually a reference to the parent array (i.e., a "view"). This means that any changes to the new array will affect the parent array as well. Since `ravel` does not create a copy, it's memory efficient.

If you start with this array:

    >>> x = np.array([[1 , 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can use `flatten` to flatten your array into a 1D array. :

    >>> x.flatten()
    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])

When you use `flatten`, changes to your new array won't change the parent array.

For example:

    >>> a1 = x.flatten()
    >>> a1[0] = 99
    >>> print(x)  # Original array
    [[ 1  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]]
    >>> print(a1)  # New array
    [99  2  3  4  5  6  7  8  9 10 11 12]

But when you use `ravel`, the changes you make to the new array will affect the parent array.

For example:

    >>> a2 = x.ravel()
    >>> a2[0] = 98
    >>> print(x)  # Original array
    [[98  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]]
    >>> print(a2)  # New array
    [98  2  3  4  5  6  7  8  9 10 11 12]

Read more about `flatten` at <span class="title-ref">ndarray.flatten</span> and `ravel` at <span class="title-ref">ravel</span>.

## How to access the docstring for more information

*This section covers* `help()`, `?`, `??`

-----

When it comes to the data science ecosystem, Python and NumPy are built with the user in mind. One of the best examples of this is the built-in access to documentation. Every object contains the reference to a string, which is known as the **docstring**. In most cases, this docstring contains a quick and concise summary of the object and how to use it. Python has a built-in `help()` function that can help you access this information. This means that nearly any time you need more information, you can use `help()` to quickly find the information that you need.

For example:

    >>> help(max)
    Help on built-in function max in module builtins:
    <BLANKLINE>
    max(...)
        max(iterable, *[, default=obj, key=func]) -> value
        max(arg1, arg2, *args, *[, key=func]) -> value
    <BLANKLINE>
        With a single iterable argument, return its biggest item. The
        default keyword-only argument specifies an object to return if
        the provided iterable is empty.
        With two or more arguments, return the largest argument.
    <BLANKLINE>

Because access to additional information is so useful, IPython uses the `?` character as a shorthand for accessing this documentation along with other relevant information. IPython is a command shell for interactive computing in multiple languages. [You can find more information about IPython here](https://ipython.org/).

For example:

`` `ipython   In [0]: max?   max(iterable, *[, default=obj, key=func]) -> value   max(arg1, arg2, *args, *[, key=func]) -> value    With a single iterable argument, return its biggest item. The   default keyword-only argument specifies an object to return if   the provided iterable is empty.   With two or more arguments, return the largest argument.   Type:      builtin_function_or_method  You can even use this notation for object methods and objects themselves.  Let's say you create this array::    >>> a = np.array([1, 2, 3, 4, 5, 6])  Then you can obtain a lot of useful information (first details about ``a`itself,`<span class="title-ref"> followed by the docstring of </span><span class="title-ref">ndarray</span><span class="title-ref"> of which </span><span class="title-ref">a</span>\` is an instance):

`` `ipython   In [1]: a?   Type:            ndarray   String form:     [1 2 3 4 5 6]   Length:          6   File:            ~/anaconda3/lib/python3.9/site-packages/numpy/__init__.py   Docstring:       <no docstring>   Class docstring:   ndarray(shape, dtype=float, buffer=None, offset=0,           strides=None, order=None)    An array object represents a multidimensional, homogeneous array   of fixed-size items.  An associated data-type object describes the   format of each element in the array (its byte-order, how many bytes it   occupies in memory, whether it is an integer, a floating point number,   or something else, etc.)    Arrays should be constructed using `array`, `zeros` or `empty` (refer   to the See Also section below).  The parameters given here refer to   a low-level method (`ndarray(...)`) for instantiating an array.    For more information, refer to the `numpy` module and examine the   methods and attributes of an array.    Parameters   ----------   (for the __new__ method; see Notes below)    shape : tuple of ints           Shape of created array.   ...  This also works for functions and other objects that **you** create. Just ``<span class="title-ref"> remember to include a docstring with your function using a string literal (</span><span class="title-ref">""" """</span><span class="title-ref"> or </span><span class="title-ref">''' '''</span>\` around your documentation).

For example, if you create this function:

    >>> def double(a):
    ...   '''Return a * 2'''
    ...   return a * 2

You can obtain information about the function:

`` `ipython   In [2]: double?   Signature: double(a)   Docstring: Return a * 2   File:      ~/Desktop/<ipython-input-23-b5adf20be596>   Type:      function  You can reach another level of information by reading the source code of the ``<span class="title-ref"> object you're interested in. Using a double question mark (</span><span class="title-ref">??</span>\`) allows you to access the source code.

For example:

`` `ipython   In [3]: double??   Signature: double(a)   Source:   def double(a):       '''Return a * 2'''       return a * 2   File:      ~/Desktop/<ipython-input-23-b5adf20be596>   Type:      function  If the object in question is compiled in a language other than Python, using ``<span class="title-ref"> </span><span class="title-ref">??</span><span class="title-ref"> will return the same information as </span><span class="title-ref">?</span>\`. You'll find this with a lot of built-in objects and types, for example:

`` `ipython   In [4]: len?   Signature: len(obj, /)   Docstring: Return the number of items in a container.   Type:      builtin_function_or_method  and :  .. code-block:: ipython    In [5]: len??   Signature: len(obj, /)   Docstring: Return the number of items in a container.   Type:      builtin_function_or_method  have the same output because they were compiled in a programming language other ``\` than Python.

## Working with mathematical formulas

The ease of implementing mathematical formulas that work on arrays is one of the things that make NumPy so widely used in the scientific Python community.

For example, this is the mean square error formula (a central formula used in supervised machine learning models that deal with regression):

![image](images/np_MSE_formula.png)

Implementing this formula is simple and straightforward in NumPy:

![image](images/np_MSE_implementation.png)

What makes this work so well is that `predictions` and `labels` can contain one or a thousand values. They only need to be the same size.

You can visualize it this way:

![image](images/np_mse_viz1.png)

In this example, both the predictions and labels vectors contain three values, meaning `n` has a value of three. After we carry out subtractions the values in the vector are squared. Then NumPy sums the values, and your result is the error value for that prediction and a score for the quality of the model.

![image](images/np_mse_viz2.png)

![image](images/np_MSE_explanation2.png)

## How to save and load NumPy objects

*This section covers* `np.save`, `np.savez`, `np.savetxt`, `np.load`, `np.loadtxt`

-----

You will, at some point, want to save your arrays to disk and load them back without having to re-run the code. Fortunately, there are several ways to save and load objects with NumPy. The ndarray objects can be saved to and loaded from the disk files with `loadtxt` and `savetxt` functions that handle normal text files, `load` and `save` functions that handle NumPy binary files with a **.npy** file extension, and a `savez` function that handles NumPy files with a **.npz** file extension.

The **.npy** and **.npz** files store data, shape, dtype, and other information required to reconstruct the ndarray in a way that allows the array to be correctly retrieved, even when the file is on another machine with different architecture.

If you want to store a single ndarray object, store it as a .npy file using `np.save`. If you want to store more than one ndarray object in a single file, save it as a .npz file using `np.savez`. You can also save several arrays into a single file in compressed npz format with <span class="title-ref">savez\_compressed</span>.

It's easy to save and load an array with `np.save()`. Just make sure to specify the array you want to save and a file name. For example, if you create this array:

    >>> a = np.array([1, 2, 3, 4, 5, 6])

You can save it as "filename.npy" with:

    >>> np.save('filename', a)

You can use `np.load()` to reconstruct your array. :

    >>> b = np.load('filename.npy')

If you want to check your array, you can run:

    >>> print(b)
    [1 2 3 4 5 6]

You can save a NumPy array as a plain text file like a **.csv** or **.txt** file with `np.savetxt`.

For example, if you create this array:

    >>> csv_arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

You can easily save it as a .csv file with the name "new\_file.csv" like this:

    >>> np.savetxt('new_file.csv', csv_arr)

You can quickly and easily load your saved text file using `loadtxt()`:

    >>> np.loadtxt('new_file.csv')
    array([1., 2., 3., 4., 5., 6., 7., 8.])

The `savetxt()` and `loadtxt()` functions accept additional optional parameters such as header, footer, and delimiter. While text files can be easier for sharing, .npy and .npz files are smaller and faster to read. If you need more sophisticated handling of your text file (for example, if you need to work with lines that contain missing values), you will want to use the <span class="title-ref">genfromtxt</span> function.

With <span class="title-ref">savetxt</span>, you can specify headers, footers, comments, and more.

Learn more about \[input and output routines here \<routines.io\>\](\#input-and-output-routines-here-\<routines.io\>).

## Importing and exporting a CSV

It's simple to read in a CSV that contains existing information. The best and easiest way to do this is to use [Pandas](https://pandas.pydata.org). :

    >>> import pandas as pd
    
    >>> # If all of your columns are the same type:
    >>> x = pd.read_csv('music.csv', header=0).values
    >>> print(x)
    [['Billie Holiday' 'Jazz' 1300000 27000000]
     ['Jimmie Hendrix' 'Rock' 2700000 70000000]
     ['Miles Davis' 'Jazz' 1500000 48000000]
     ['SIA' 'Pop' 2000000 74000000]]
    
    >>> # You can also simply select the columns you need:
    >>> x = pd.read_csv('music.csv', usecols=['Artist', 'Plays']).values
    >>> print(x)
    [['Billie Holiday' 27000000]
     ['Jimmie Hendrix' 70000000]
     ['Miles Davis' 48000000]
     ['SIA' 74000000]]

![image](images/np_pandas.png)

It's simple to use Pandas in order to export your array as well. If you are new to NumPy, you may want to create a Pandas dataframe from the values in your array and then write the data frame to a CSV file with Pandas.

If you created this array "a" :

    >>> a = np.array([[-2.58289208,  0.43014843, -1.24082018, 1.59572603],
    ...               [ 0.99027828, 1.17150989,  0.94125714, -0.14692469],
    ...               [ 0.76989341,  0.81299683, -0.95068423, 0.11769564],
    ...               [ 0.20484034,  0.34784527,  1.96979195, 0.51992837]])

You could create a Pandas dataframe :

    >>> df = pd.DataFrame(a)
    >>> print(df)
              0         1         2         3
    0 -2.582892  0.430148 -1.240820  1.595726
    1  0.990278  1.171510  0.941257 -0.146925
    2  0.769893  0.812997 -0.950684  0.117696
    3  0.204840  0.347845  1.969792  0.519928

You can easily save your dataframe with:

    >>> df.to_csv('pd.csv')

And read your CSV with:

    >>> data = pd.read_csv('pd.csv')

![image](images/np_readcsv.png)

You can also save your array with the NumPy `savetxt` method. :

    >>> np.savetxt('np.csv', a, fmt='%.2f', delimiter=',', header='1,  2,  3,  4')

If you're using the command line, you can read your saved CSV any time with a command such as:

    $ cat np.csv
    #  1,  2,  3,  4
    -2.58,0.43,-1.24,1.60
    0.99,1.17,0.94,-0.15
    0.77,0.81,-0.95,0.12
    0.20,0.35,1.97,0.52

Or you can open the file any time with a text editor\!

If you're interested in learning more about Pandas, take a look at the [official Pandas documentation](https://pandas.pydata.org/index.html). Learn how to install Pandas with the [official Pandas installation information](https://pandas.pydata.org/pandas-docs/stable/install.html).

## Plotting arrays with Matplotlib

If you need to generate a plot for your values, it's very simple with [Matplotlib](https://matplotlib.org/).

For example, you may have an array like this one:

    >>> a = np.array([2, 1, 5, 7, 4, 6, 8, 14, 10, 9, 18, 20, 22])

If you already have Matplotlib installed, you can import it with:

    >>> import matplotlib.pyplot as plt
    
    # If you're using Jupyter Notebook, you may also want to run the following
    # line of code to display your code in the notebook:
    
    %matplotlib inline

All you need to do to plot your values is run:

    >>> plt.plot(a)
    
    # If you are running from a command line, you may need to do this:
    # >>> plt.show()

<div class="plot" data-align="center" data-include-source="0">

user/plots/matplotlib1.py

</div>

For example, you can plot a 1D array like this:

    >>> x = np.linspace(0, 5, 20)
    >>> y = np.linspace(0, 10, 20)
    >>> plt.plot(x, y, 'purple') # line
    >>> plt.plot(x, y, 'o')      # dots

<div class="plot" data-align="center" data-include-source="0">

user/plots/matplotlib2.py

</div>

With Matplotlib, you have access to an enormous number of visualization options. :

    >>> fig = plt.figure()
    >>> ax = fig.add_subplot(projection='3d')
    >>> X = np.arange(-5, 5, 0.15)
    >>> Y = np.arange(-5, 5, 0.15)
    >>> X, Y = np.meshgrid(X, Y)
    >>> R = np.sqrt(X**2 + Y**2)
    >>> Z = np.sin(R)
    
    >>> ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis')

<div class="plot" data-align="center" data-include-source="0">

user/plots/matplotlib3.py

</div>

To read more about Matplotlib and what it can do, take a look at [the official documentation](https://matplotlib.org/). For directions regarding installing Matplotlib, see the official [installation section](https://matplotlib.org/users/installing.html).

-----

*Image credits: Jay Alammar https://jalammar.github.io/*

---

basics.broadcasting.md

---

# Broadcasting<span id="basics.broadcasting"></span>

<div class="seealso">

<span class="title-ref">numpy.broadcast</span>

</div>

The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is "broadcast" across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.

NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example:

> \>\>\> import numpy as np \>\>\> a = np.array(\[1.0, 2.0, 3.0\]) \>\>\> b = np.array(\[2.0, 2.0, 2.0\]) \>\>\> a \* b array(\[2., 4., 6.\])

NumPy's broadcasting rule relaxes this constraint when the arrays' shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation:

\>\>\> import numpy as np \>\>\> a = np.array(\[1.0, 2.0, 3.0\]) \>\>\> b = 2.0 \>\>\> a \* b array(\[2., 4., 6.\])

The result is equivalent to the previous example where `b` was an array. We can think of the scalar `b` being *stretched* during the arithmetic operation into an array with the same shape as `a`. The new elements in `b`, as shown in \[broadcasting.figure-1\](\#broadcasting.figure-1), are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies so that broadcasting operations are as memory and computationally efficient as possible.

![](broadcasting_1.png)

The code in the second example is more efficient than that in the first because broadcasting moves less memory around during the multiplication (`b` is a scalar rather than an array).

## General broadcasting rules

When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimension and works its way left. Two dimensions are compatible when

1)  they are equal, or
2)  one of them is 1.

If these conditions are not met, a `ValueError: operands could not be broadcast together` exception is thrown, indicating that the arrays have incompatible shapes.

Input arrays do not need to have the same *number* of dimensions. The resulting array will have the same number of dimensions as the input array with the greatest number of dimensions, where the *size* of each dimension is the largest size of the corresponding dimension among the input arrays. Note that missing dimensions are assumed to have size one.

For example, if you have a `256x256x3` array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:

    Image  (3d array): 256 x 256 x 3
    Scale  (1d array):             3
    Result (3d array): 256 x 256 x 3

When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or "copied" to match the other.

In the following example, both the `A` and `B` arrays have axes with length one that are expanded to a larger size during the broadcast operation:

    A      (4d array):  8 x 1 x 6 x 1
    B      (3d array):      7 x 1 x 5
    Result (4d array):  8 x 7 x 6 x 5

## Broadcastable arrays

<div class="index">

broadcastable

</div>

A set of arrays is called "broadcastable" to the same shape if the above rules produce a valid result.

For example, if `a.shape` is (5,1), `b.shape` is (1,6), `c.shape` is (6,) and `d.shape` is () so that *d* is a scalar, then *a*, *b*, *c*, and *d* are all broadcastable to dimension (5,6); and

  - *a* acts like a (5,6) array where `a[:,0]` is broadcast to the other columns,
  - *b* acts like a (5,6) array where `b[0,:]` is broadcast to the other rows,
  - *c* acts like a (1,6) array and therefore like a (5,6) array where `c[:]` is broadcast to every row, and finally,
  - *d* acts like a (5,6) array where the single value is repeated.

Here are some more examples:

    A      (2d array):  5 x 4
    B      (1d array):      1
    Result (2d array):  5 x 4
    
    A      (2d array):  5 x 4
    B      (1d array):      4
    Result (2d array):  5 x 4
    
    A      (3d array):  15 x 3 x 5
    B      (3d array):  15 x 1 x 5
    Result (3d array):  15 x 3 x 5
    
    A      (3d array):  15 x 3 x 5
    B      (2d array):       3 x 5
    Result (3d array):  15 x 3 x 5
    
    A      (3d array):  15 x 3 x 5
    B      (2d array):       3 x 1
    Result (3d array):  15 x 3 x 5

Here are examples of shapes that do not broadcast:

    A      (1d array):  3
    B      (1d array):  4 # trailing dimensions do not match
    
    A      (2d array):      2 x 1
    B      (3d array):  8 x 4 x 3 # second from last dimensions mismatched

An example of broadcasting when a 1-d array is added to a 2-d array:

    >>> import numpy as np
    >>> a = np.array([[ 0.0,  0.0,  0.0],
    ...               [10.0, 10.0, 10.0],
    ...               [20.0, 20.0, 20.0],
    ...               [30.0, 30.0, 30.0]])
    >>> b = np.array([1.0, 2.0, 3.0])
    >>> a + b
    array([[  1.,   2.,   3.],
            [11.,  12.,  13.],
            [21.,  22.,  23.],
            [31.,  32.,  33.]])
    >>> b = np.array([1.0, 2.0, 3.0, 4.0])
    >>> a + b
    Traceback (most recent call last):
    ValueError: operands could not be broadcast together with shapes (4,3) (4,)

As shown in \[broadcasting.figure-2\](\#broadcasting.figure-2), `b` is added to each row of `a`. In \[broadcasting.figure-3\](\#broadcasting.figure-3), an exception is raised because of the incompatible shapes.

![](broadcasting_2.png)

![](broadcasting_3.png)

Broadcasting provides a convenient way of taking the outer product (or any other outer operation) of two arrays. The following example shows an outer addition operation of two 1-d arrays:

    >>> import numpy as np
    >>> a = np.array([0.0, 10.0, 20.0, 30.0])
    >>> b = np.array([1.0, 2.0, 3.0])
    >>> a[:, np.newaxis] + b
    array([[ 1.,   2.,   3.],
           [11.,  12.,  13.],
           [21.,  22.,  23.],
           [31.,  32.,  33.]])

![](broadcasting_4.png)

Here the `newaxis` index operator inserts a new axis into `a`, making it a two-dimensional `4x1` array. Combining the `4x1` array with `b`, which has shape `(3,)`, yields a `4x3` array.

## A practical example: vector quantization

Broadcasting comes up quite often in real world problems. A typical example occurs in the vector quantization (VQ) algorithm used in information theory, classification, and other related areas. The basic operation in VQ finds the closest point in a set of points, called `codes` in VQ jargon, to a given point, called the `observation`. In the very simple, two-dimensional case shown below, the values in `observation` describe the weight and height of an athlete to be classified. The `codes` represent different classes of athletes.\[1\] Finding the closest point requires calculating the distance between observation and each of the codes. The shortest distance provides the best match. In this example, `codes[0]` is the closest class indicating that the athlete is likely a basketball player.

> \>\>\> from numpy import array, argmin, sqrt, sum \>\>\> observation = array(\[111.0, 188.0\]) \>\>\> codes = array(\[\[102.0, 203.0\], ... \[132.0, 193.0\], ... \[45.0, 155.0\], ... \[57.0, 173.0\]\]) \>\>\> diff = codes - observation \# the broadcast happens here \>\>\> dist = sqrt(sum(diff\*\*2,axis=-1)) \>\>\> argmin(dist) 0

In this example, the `observation` array is stretched to match the shape of the `codes` array:

    Observation      (1d array):      2
    Codes            (2d array):  4 x 2
    Diff             (2d array):  4 x 2

![](broadcasting_5.png)

Typically, a large number of `observations`, perhaps read from a database, are compared to a set of `codes`. Consider this scenario:

    Observation      (2d array):      10 x 3
    Codes            (3d array):   5 x 1 x 3
    Diff             (3d array):  5 x 10 x 3

The three-dimensional array, `diff`, is a consequence of broadcasting, not a necessity for the calculation. Large data sets will generate a large intermediate array that is computationally inefficient. Instead, if each observation is calculated individually using a Python loop around the code in the two-dimensional example above, a much smaller array is used.

Broadcasting is a powerful tool for writing short and usually intuitive code that does its computations very efficiently in C. However, there are cases when broadcasting uses unnecessarily large amounts of memory for a particular algorithm. In these cases, it is better to write the algorithm's outer loop in Python. This may also produce more readable code, as algorithms that use broadcasting tend to become more difficult to interpret as the number of dimensions in the broadcast increases.

**Footnotes**

1.  In this example, weight has more impact on the distance calculation than height because of the larger values. In practice, it is important to normalize the height and weight, often by their standard deviation across the data set, so that both have equal influence on the distance calculation.

---

basics.copies.md

---

# Copies and views

When operating on NumPy arrays, it is possible to access the internal data buffer directly using a \[view \<view\>\](\#view-\<view\>) without copying data around. This ensures good performance but can also cause unwanted problems if the user is not aware of how this works. Hence, it is important to know the difference between these two terms and to know which operations return copies and which return views.

The NumPy array is a data structure consisting of two parts: the `contiguous` data buffer with the actual data elements and the metadata that contains information about the data buffer. The metadata includes data type, strides, and other important information that helps manipulate the <span class="title-ref">.ndarray</span> easily. See the \[numpy-internals\](\#numpy-internals) section for a detailed look.

## View

It is possible to access the array differently by just changing certain metadata like `stride` and `dtype` without changing the data buffer. This creates a new way of looking at the data and these new arrays are called views. The data buffer remains the same, so any changes made to a view reflects in the original copy. A view can be forced through the <span class="title-ref">.ndarray.view</span> method.

## Copy

When a new array is created by duplicating the data buffer as well as the metadata, it is called a copy. Changes made to the copy do not reflect on the original array. Making a copy is slower and memory-consuming but sometimes necessary. A copy can be forced by using <span class="title-ref">.ndarray.copy</span>.

## Indexing operations

<div class="seealso">

\[basics.indexing\](\#basics.indexing)

</div>

Views are created when elements can be addressed with offsets and strides in the original array. Hence, basic indexing always creates views. For example:

    >>> import numpy as np
    >>> x = np.arange(10)
    >>> x
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    >>> y = x[1:3]  # creates a view
    >>> y
    array([1, 2])
    >>> x[1:3] = [10, 11]
    >>> x
    array([ 0, 10, 11,  3,  4,  5,  6,  7,  8,  9])
    >>> y
    array([10, 11])

Here, `y` gets changed when `x` is changed because it is a view.

\[advanced-indexing\](\#advanced-indexing), on the other hand, always creates copies. For example:

    >>> import numpy as np
    >>> x = np.arange(9).reshape(3, 3)
    >>> x
    array([[0, 1, 2],
           [3, 4, 5],
           [6, 7, 8]])
    >>> y = x[[1, 2]]
    >>> y
    array([[3, 4, 5],
           [6, 7, 8]])
    >>> y.base is None
    True
    
    Here, ``y`` is a copy, as signified by the `base <.ndarray.base>`
    attribute. We can also confirm this by assigning new values to ``x[[1, 2]]``
    which in turn will not affect ``y`` at all::
    
    >>> x[[1, 2]] = [[10, 11, 12], [13, 14, 15]]
    >>> x
    array([[ 0,  1,  2],
           [10, 11, 12],
           [13, 14, 15]])
    >>> y
    array([[3, 4, 5],
           [6, 7, 8]])

It must be noted here that during the assignment of `x[[1, 2]]` no view or copy is created as the assignment happens in-place.

## Other operations

The <span class="title-ref">numpy.reshape</span> function creates a view where possible or a copy otherwise. In most cases, the strides can be modified to reshape the array with a view. However, in some cases where the array becomes non-contiguous (perhaps after a <span class="title-ref">.ndarray.transpose</span> operation), the reshaping cannot be done by modifying strides and requires a copy. In these cases, we can raise an error by assigning the new shape to the shape attribute of the array. For example:

    >>> import numpy as np
    >>> x = np.ones((2, 3))
    >>> y = x.T  # makes the array non-contiguous
    >>> y
    array([[1., 1.],
           [1., 1.],
           [1., 1.]])
    >>> z = y.view()
    >>> z.shape = 6
    Traceback (most recent call last):
       ...
    AttributeError: Incompatible shape for in-place modification. Use
    `.reshape()` to make a copy with the desired shape.

Taking the example of another operation, <span class="title-ref">.ravel</span> returns a contiguous flattened view of the array wherever possible. On the other hand, <span class="title-ref">.ndarray.flatten</span> always returns a flattened copy of the array. However, to guarantee a view in most cases, `x.reshape(-1)` may be preferable.

## How to tell if the array is a view or a copy

The <span class="title-ref">base \<.ndarray.base\></span> attribute of the ndarray makes it easy to tell if an array is a view or a copy. The base attribute of a view returns the original array while it returns `None` for a copy.

> \>\>\> import numpy as np \>\>\> x = np.arange(9) \>\>\> x array(\[0, 1, 2, 3, 4, 5, 6, 7, 8\]) \>\>\> y = x.reshape(3, 3) \>\>\> y array(\[\[0, 1, 2\], \[3, 4, 5\], \[6, 7, 8\]\]) \>\>\> y.base \# .reshape() creates a view array(\[0, 1, 2, 3, 4, 5, 6, 7, 8\]) \>\>\> z = y\[\[2, 1\]\] \>\>\> z array(\[\[6, 7, 8\], \[3, 4, 5\]\]) \>\>\> z.base is None \# advanced indexing creates a copy True

Note that the `base` attribute should not be used to determine if an ndarray object is *new*; only if it is a view or a copy of another ndarray.

---

basics.creation.md

---

# Array creation

<div class="seealso">

\[Array creation routines \<routines.array-creation\>\](\#array-creation-routines-\<routines.array-creation\>)

</div>

## Introduction

There are 6 general mechanisms for creating arrays:

1)  Conversion from other Python structures (i.e. lists and tuples)
2)  Intrinsic NumPy array creation functions (e.g. arange, ones, zeros, etc.)
3)  Replicating, joining, or mutating existing arrays
4)  Reading arrays from disk, either from standard or custom formats
5)  Creating arrays from raw bytes through the use of strings or buffers
6)  Use of special library functions (e.g., random)

You can use these methods to create ndarrays or \[structured\_arrays\](\#structured\_arrays). This document will cover general methods for ndarray creation.

## 1\) Converting Python sequences to NumPy arrays

NumPy arrays can be defined using Python sequences such as lists and tuples. Lists and tuples are defined using `[...]` and `(...)`, respectively. Lists and tuples can define ndarray creation:

  - a list of numbers will create a 1D array,
  - a list of lists will create a 2D array,
  - further nested lists will create higher-dimensional arrays. In general, any array object is called an **ndarray** in NumPy.

<!-- end list -->

    >>> import numpy as np
    >>> a1D = np.array([1, 2, 3, 4])
    >>> a2D = np.array([[1, 2], [3, 4]])
    >>> a3D = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

When you use <span class="title-ref">numpy.array</span> to define a new array, you should consider the \[dtype \<basics.types\>\](dtype \<basics.types\>.md) of the elements in the array, which can be specified explicitly. This feature gives you more control over the underlying data structures and how the elements are handled in C/C++ functions. When values do not fit and you are using a `dtype`, NumPy may raise an error:

    >>> import numpy as np
    >>> np.array([127, 128, 129], dtype=np.int8)
    Traceback (most recent call last):
    ...
    OverflowError: Python integer 128 out of bounds for int8

An 8-bit signed integer represents integers from -128 to 127. Assigning the `int8` array to integers outside of this range results in overflow. This feature can often be misunderstood. If you perform calculations with mismatching `dtypes`, you can get unwanted results, for example:

    >>> import numpy as np
    >>> a = np.array([2, 3, 4], dtype=np.uint32)
    >>> b = np.array([5, 6, 7], dtype=np.uint32)
    >>> c_unsigned32 = a - b
    >>> print('unsigned c:', c_unsigned32, c_unsigned32.dtype)
    unsigned c: [4294967293 4294967293 4294967293] uint32
    >>> c_signed32 = a - b.astype(np.int32)
    >>> print('signed c:', c_signed32, c_signed32.dtype)
    signed c: [-3 -3 -3] int64

Notice when you perform operations with two arrays of the same `dtype`: `uint32`, the resulting array is the same type. When you perform operations with different `dtype`, NumPy will assign a new type that satisfies all of the array elements involved in the computation, here `uint32` and `int32` can both be represented in as `int64`.

The default NumPy behavior is to create arrays in either 32 or 64-bit signed integers (platform dependent and matches C `long` size) or double precision floating point numbers. If you expect your integer arrays to be a specific type, then you need to specify the dtype while you create the array.

## 2\) Intrinsic NumPy array creation functions

NumPy has over 40 built-in functions for creating arrays as laid out in the \[Array creation routines \<routines.array-creation\>\](\#array-creation-routines-\<routines.array-creation\>). These functions can be split into roughly three categories, based on the dimension of the array they create:

1)  1D arrays
2)  2D arrays
3)  ndarrays

### 1 - 1D array creation functions

The 1D array creation functions e.g. <span class="title-ref">numpy.linspace</span> and <span class="title-ref">numpy.arange</span> generally need at least two inputs, `start` and `stop`.

<span class="title-ref">numpy.arange</span> creates arrays with regularly incrementing values. Check the documentation for complete information and examples. A few examples are shown:

    >>> import numpy as np
    >>> np.arange(10)
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    >>> np.arange(2, 10, dtype=float)
    array([2., 3., 4., 5., 6., 7., 8., 9.])
    >>> np.arange(2, 3, 0.1)
    array([2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9])

Note: best practice for <span class="title-ref">numpy.arange</span> is to use integer start, end, and step values. There are some subtleties regarding `dtype`. In the second example, the `dtype` is defined. In the third example, the array is `dtype=float` to accommodate the step size of `0.1`. Due to roundoff error, the `stop` value is sometimes included.

<span class="title-ref">numpy.linspace</span> will create arrays with a specified number of elements, and spaced equally between the specified beginning and end values. For example: :

    >>> import numpy as np
    >>> np.linspace(1., 4., 6)
    array([1. ,  1.6,  2.2,  2.8,  3.4,  4. ])

The advantage of this creation function is that you guarantee the number of elements and the starting and end point. The previous `arange(start, stop, step)` will not include the value `stop`.

### 2 - 2D array creation functions

The 2D array creation functions e.g. <span class="title-ref">numpy.eye</span>, <span class="title-ref">numpy.diag</span>, and <span class="title-ref">numpy.vander</span> define properties of special matrices represented as 2D arrays.

`np.eye(n, m)` defines a 2D identity matrix. The elements where i=j (row index and column index are equal) are 1 and the rest are 0, as such:

    >>> import numpy as np
    >>> np.eye(3)
    array([[1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.]])
    >>> np.eye(3, 5)
    array([[1., 0., 0., 0., 0.],
           [0., 1., 0., 0., 0.],
           [0., 0., 1., 0., 0.]])

<span class="title-ref">numpy.diag</span> can define either a square 2D array with given values along the diagonal *or* if given a 2D array returns a 1D array that is only the diagonal elements. The two array creation functions can be helpful while doing linear algebra, as such:

    >>> import numpy as np
    >>> np.diag([1, 2, 3])
    array([[1, 0, 0],
           [0, 2, 0],
           [0, 0, 3]])
    >>> np.diag([1, 2, 3], 1)
    array([[0, 1, 0, 0],
           [0, 0, 2, 0],
           [0, 0, 0, 3],
           [0, 0, 0, 0]])
    >>> a = np.array([[1, 2], [3, 4]])
    >>> np.diag(a)
    array([1, 4])

`vander(x, n)` defines a Vandermonde matrix as a 2D NumPy array. Each column of the Vandermonde matrix is a decreasing power of the input 1D array or list or tuple, `x` where the highest polynomial order is `n-1`. This array creation routine is helpful in generating linear least squares models, as such:

    >>> import numpy as np
    >>> np.vander(np.linspace(0, 2, 5), 2)
    array([[0. , 1. ],
          [0.5, 1. ],
          [1. , 1. ],
          [1.5, 1. ],
          [2. , 1. ]])
    >>> np.vander([1, 2, 3, 4], 2)
    array([[1, 1],
           [2, 1],
           [3, 1],
           [4, 1]])
    >>> np.vander((1, 2, 3, 4), 4)
    array([[ 1,  1,  1,  1],
           [ 8,  4,  2,  1],
           [27,  9,  3,  1],
           [64, 16,  4,  1]])

### 3 - general ndarray creation functions

The ndarray creation functions e.g. <span class="title-ref">numpy.ones</span>, <span class="title-ref">numpy.zeros</span>, and <span class="title-ref">\~numpy.random.Generator.random</span> define arrays based upon the desired shape. The ndarray creation functions can create arrays with any dimension by specifying how many dimensions and length along that dimension in a tuple or list.

<span class="title-ref">numpy.zeros</span> will create an array filled with 0 values with the specified shape. The default dtype is `float64`:

    >>> import numpy as np
    >>> np.zeros((2, 3))
    array([[0., 0., 0.], 
           [0., 0., 0.]])
    >>> np.zeros((2, 3, 2))
    array([[[0., 0.],
            [0., 0.],
            [0., 0.]],
    <BLANKLINE>        
           [[0., 0.],
            [0., 0.],
            [0., 0.]]])

<span class="title-ref">numpy.ones</span> will create an array filled with 1 values. It is identical to `zeros` in all other respects as such:

    >>> import numpy as np
    >>> np.ones((2, 3))
    array([[1., 1., 1.], 
           [1., 1., 1.]])
    >>> np.ones((2, 3, 2))
    array([[[1., 1.],
            [1., 1.],
            [1., 1.]],
    <BLANKLINE>
           [[1., 1.],
            [1., 1.],
            [1., 1.]]])

The <span class="title-ref">\~numpy.random.Generator.random</span> method of the result of `default_rng` will create an array filled with random values between 0 and 1. It is included with the <span class="title-ref">numpy.random</span> library. Below, two arrays are created with shapes (2,3) and (2,3,2), respectively. The seed is set to 42 so you can reproduce these pseudorandom numbers:

    >>> import numpy as np
    >>> from numpy.random import default_rng
    >>> default_rng(42).random((2,3))
    array([[0.77395605, 0.43887844, 0.85859792],
           [0.69736803, 0.09417735, 0.97562235]])
    >>> default_rng(42).random((2,3,2))
    array([[[0.77395605, 0.43887844],
            [0.85859792, 0.69736803],
            [0.09417735, 0.97562235]],
           [[0.7611397 , 0.78606431],
            [0.12811363, 0.45038594],
            [0.37079802, 0.92676499]]])

<span class="title-ref">numpy.indices</span> will create a set of arrays (stacked as a one-higher dimensioned array), one per dimension with each representing variation in that dimension:

    >>> import numpy as np
    >>> np.indices((3,3))
    array([[[0, 0, 0], 
            [1, 1, 1], 
            [2, 2, 2]], 
           [[0, 1, 2], 
            [0, 1, 2], 
            [0, 1, 2]]])

This is particularly useful for evaluating functions of multiple dimensions on a regular grid.

## 3\) Replicating, joining, or mutating existing arrays

Once you have created arrays, you can replicate, join, or mutate those existing arrays to create new arrays. When you assign an array or its elements to a new variable, you have to explicitly <span class="title-ref">numpy.copy</span> the array, otherwise the variable is a view into the original array. Consider the following example:

    >>> import numpy as np
    >>> a = np.array([1, 2, 3, 4, 5, 6])
    >>> b = a[:2]
    >>> b += 1
    >>> print('a =', a, '; b =', b)
    a = [2 3 3 4 5 6] ; b = [2 3]

In this example, you did not create a new array. You created a variable, `b` that viewed the first 2 elements of `a`. When you added 1 to `b` you would get the same result by adding 1 to `a[:2]`. If you want to create a *new* array, use the <span class="title-ref">numpy.copy</span> array creation routine as such:

    >>> import numpy as np
    >>> a = np.array([1, 2, 3, 4])
    >>> b = a[:2].copy()
    >>> b += 1
    >>> print('a = ', a, 'b = ', b)
    a =  [1 2 3 4] b =  [2 3]

For more information and examples look at \[Copies and Views \<quickstart.copies-and-views\>\](\#copies-and-views \<quickstart.copies-and-views\>).

There are a number of routines to join existing arrays e.g. <span class="title-ref">numpy.vstack</span>, <span class="title-ref">numpy.hstack</span>, and <span class="title-ref">numpy.block</span>. Here is an example of joining four 2-by-2 arrays into a 4-by-4 array using `block`:

    >>> import numpy as np
    >>> A = np.ones((2, 2))
    >>> B = np.eye(2, 2)
    >>> C = np.zeros((2, 2))
    >>> D = np.diag((-3, -4))
    >>> np.block([[A, B], [C, D]])
    array([[ 1.,  1.,  1.,  0.],
           [ 1.,  1.,  0.,  1.],
           [ 0.,  0., -3.,  0.],
           [ 0.,  0.,  0., -4.]])

Other routines use similar syntax to join ndarrays. Check the routine's documentation for further examples and syntax.

## 4\) Reading arrays from disk, either from standard or custom formats

This is the most common case of large array creation. The details depend greatly on the format of data on disk. This section gives general pointers on how to handle various formats. For more detailed examples of IO look at \[How to Read and Write files \<how-to-io\>\](\#how-to-read-and-write-files-\<how-to-io\>).

### Standard binary formats

Various fields have standard formats for array data. The following lists the ones with known Python libraries to read them and return NumPy arrays (there may be others for which it is possible to read and convert to NumPy arrays so check the last section as well) :

    HDF5: h5py
    FITS: Astropy

Examples of formats that cannot be read directly but for which it is not hard to convert are those formats supported by libraries like PIL (able to read and write many image formats such as jpg, png, etc).

### Common ASCII formats

Delimited files such as comma separated value (csv) and tab separated value (tsv) files are used for programs like Excel and LabView. Python functions can read and parse these files line-by-line. NumPy has two standard routines for importing a file with delimited data <span class="title-ref">numpy.loadtxt</span> and <span class="title-ref">numpy.genfromtxt</span>. These functions have more involved use cases in \[how-to-io\](how-to-io.md). A simple example given a `simple.csv`:

`` `bash  $ cat simple.csv  x, y  0, 0  1, 1  2, 4  3, 9  Importing ``simple.csv``is accomplished using `numpy.loadtxt`::   >>> import numpy as np  >>> np.loadtxt('simple.csv', delimiter = ',', skiprows = 1) # doctest: +SKIP  array([[0., 0.],         [1., 1.],         [2., 4.],         [3., 9.]])   More generic ASCII files can be read using `scipy.io` and `Pandas``<https://pandas.pydata.org/>.

## 5\) Creating arrays from raw bytes through the use of strings or buffers

There are a variety of approaches one can use. If the file has a relatively simple format then one can write a simple I/O library and use the NumPy `fromfile()` function and `.tofile()` method to read and write NumPy arrays directly (mind your byteorder though\!) If a good C or C++ library exists that read the data, one can wrap that library with a variety of techniques though that certainly is much more work and requires significantly more advanced knowledge to interface with C or C++.

## 6\) Use of special library functions (e.g., SciPy, pandas, and OpenCV)

NumPy is the fundamental library for array containers in the Python Scientific Computing stack. Many Python libraries, including SciPy, Pandas, and OpenCV, use NumPy ndarrays as the common format for data exchange, These libraries can create, operate on, and work with NumPy arrays.

---

basics.dispatch.md

---

# Writing custom array containers

Numpy's dispatch mechanism, introduced in numpy version v1.16 is the recommended approach for writing custom N-dimensional array containers that are compatible with the numpy API and provide custom implementations of numpy functionality. Applications include [dask](http://dask.pydata.org) arrays, an N-dimensional array distributed across multiple nodes, and [cupy](https://docs-cupy.chainer.org/en/stable/) arrays, an N-dimensional array on a GPU.

To get a feel for writing custom array containers, we'll begin with a simple example that has rather narrow utility but illustrates the concepts involved.

\>\>\> import numpy as np \>\>\> class DiagonalArray: ... def \_\_init\_\_(self, N, value): ... self.\_N = N ... self.\_i = value ... def \_\_repr\_\_(self): ... return f"{self.\_\_class\_\_.\_\_name\_\_}(N={self.\_N}, value={self.\_i})" ... def \_\_array\_\_(self, dtype=None, copy=None): ... if copy is False: ... raise ValueError( ... "<span class="title-ref">copy=False</span> isn't supported. A copy is always created." ... ) ... return self.\_i \* np.eye(self.\_N, dtype=dtype)

Our custom array can be instantiated like:

\>\>\> arr = DiagonalArray(5, 1) \>\>\> arr DiagonalArray(N=5, value=1)

We can convert to a numpy array using <span class="title-ref">numpy.array</span> or <span class="title-ref">numpy.asarray</span>, which will call its `__array__` method to obtain a standard `numpy.ndarray`.

\>\>\> np.asarray(arr) array(\[\[1., 0., 0., 0., 0.\], \[0., 1., 0., 0., 0.\], \[0., 0., 1., 0., 0.\], \[0., 0., 0., 1., 0.\], \[0., 0., 0., 0., 1.\]\])

If we operate on `arr` with a numpy function, numpy will again use the `__array__` interface to convert it to an array and then apply the function in the usual way.

\>\>\> np.multiply(arr, 2) array(\[\[2., 0., 0., 0., 0.\], \[0., 2., 0., 0., 0.\], \[0., 0., 2., 0., 0.\], \[0., 0., 0., 2., 0.\], \[0., 0., 0., 0., 2.\]\])

Notice that the return type is a standard `numpy.ndarray`.

\>\>\> type(np.multiply(arr, 2)) \<class 'numpy.ndarray'\>

How can we pass our custom array type through this function? Numpy allows a class to indicate that it would like to handle computations in a custom-defined way through the interfaces `__array_ufunc__` and `__array_function__`. Let's take one at a time, starting with `__array_ufunc__`. This method covers \[ufuncs\](\#ufuncs), a class of functions that includes, for example, <span class="title-ref">numpy.multiply</span> and <span class="title-ref">numpy.sin</span>.

The `__array_ufunc__` receives:

  - `ufunc`, a function like `numpy.multiply`
  - `method`, a string, differentiating between `numpy.multiply(...)` and variants like `numpy.multiply.outer`, `numpy.multiply.accumulate`, and so on. For the common case, `numpy.multiply(...)`, `method == '__call__'`.
  - `inputs`, which could be a mixture of different types
  - `kwargs`, keyword arguments passed to the function

For this example we will only handle the method `__call__`

\>\>\> from numbers import Number \>\>\> class DiagonalArray: ... def \_\_init\_\_(self, N, value): ... self.\_N = N ... self.\_i = value ... def \_\_repr\_\_(self): ... return f"{self.\_\_class\_\_.\_\_name\_\_}(N={self.\_N}, value={self.\_i})" ... def \_\_array\_\_(self, dtype=None, copy=None): ... if copy is False: ... raise ValueError( ... "<span class="title-ref">copy=False</span> isn't supported. A copy is always created." ... ) ... return self.\_i \* np.eye(self.\_N, dtype=dtype) ... def \_\_array\_ufunc\_\_(self, ufunc, method, *inputs,kwargs): ... if method == '\_\_call\_\_': ... N = None ... scalars = \[\] ... for input in inputs: ... if isinstance(input, Number): ... scalars.append(input) ... elif isinstance(input, self.\_\_class\_\_): ... scalars.append(input.\_i) ... if N is not None: ... if N \!= input.\_N: ... raise TypeError("inconsistent sizes") ... else: ... N = input.\_N ... else: ... return NotImplemented ... return self.\_\_class\_\_(N, ufunc(*scalars, \*\*kwargs)) ... else: ... return NotImplemented

Now our custom array type passes through numpy functions.

\>\>\> arr = DiagonalArray(5, 1) \>\>\> np.multiply(arr, 3) DiagonalArray(N=5, value=3) \>\>\> np.add(arr, 3) DiagonalArray(N=5, value=4) \>\>\> np.sin(arr) DiagonalArray(N=5, value=0.8414709848078965)

At this point `arr + 3` does not work.

\>\>\> arr + 3 Traceback (most recent call last): ... TypeError: unsupported operand type(s) for +: 'DiagonalArray' and 'int'

To support it, we need to define the Python interfaces `__add__`, `__lt__`, and so on to dispatch to the corresponding ufunc. We can achieve this conveniently by inheriting from the mixin <span class="title-ref">\~numpy.lib.mixins.NDArrayOperatorsMixin</span>.

\>\>\> import numpy.lib.mixins \>\>\> class DiagonalArray(numpy.lib.mixins.NDArrayOperatorsMixin): ... def \_\_init\_\_(self, N, value): ... self.\_N = N ... self.\_i = value ... def \_\_repr\_\_(self): ... return f"{self.\_\_class\_\_.\_\_name\_\_}(N={self.\_N}, value={self.\_i})" ... def \_\_array\_\_(self, dtype=None, copy=None): ... if copy is False: ... raise ValueError( ... "<span class="title-ref">copy=False</span> isn't supported. A copy is always created." ... ) ... return self.\_i \* np.eye(self.\_N, dtype=dtype) ... def \_\_array\_ufunc\_\_(self, ufunc, method, *inputs,kwargs): ... if method == '\_\_call\_\_': ... N = None ... scalars = \[\] ... for input in inputs: ... if isinstance(input, Number): ... scalars.append(input) ... elif isinstance(input, self.\_\_class\_\_): ... scalars.append(input.\_i) ... if N is not None: ... if N \!= input.\_N: ... raise TypeError("inconsistent sizes") ... else: ... N = input.\_N ... else: ... return NotImplemented ... return self.\_\_class\_\_(N, ufunc(*scalars, \*\*kwargs)) ... else: ... return NotImplemented

\>\>\> arr = DiagonalArray(5, 1) \>\>\> arr + 3 DiagonalArray(N=5, value=4) \>\>\> arr \> 0 DiagonalArray(N=5, value=True)

Now let's tackle `__array_function__`. We'll create dict that maps numpy functions to our custom variants.

\>\>\> HANDLED\_FUNCTIONS = {} \>\>\> class DiagonalArray(numpy.lib.mixins.NDArrayOperatorsMixin): ... def \_\_init\_\_(self, N, value): ... self.\_N = N ... self.\_i = value ... def \_\_repr\_\_(self): ... return f"{self.\_\_class\_\_.\_\_name\_\_}(N={self.\_N}, value={self.\_i})" ... def \_\_array\_\_(self, dtype=None, copy=None): ... if copy is False: ... raise ValueError( ... "<span class="title-ref">copy=False</span> isn't supported. A copy is always created." ... ) ... return self.\_i \* np.eye(self.\_N, dtype=dtype) ... def \_\_array\_ufunc\_\_(self, ufunc, method, *inputs,kwargs): ... if method == '\_\_call\_\_': ... N = None ... scalars = \[\] ... for input in inputs: ... \# In this case we accept only scalar numbers or DiagonalArrays. ... if isinstance(input, Number): ... scalars.append(input) ... elif isinstance(input, self.\_\_class\_\_): ... scalars.append(input.\_i) ... if N is not None: ... if N \!= input.\_N: ... raise TypeError("inconsistent sizes") ... else: ... N = input.\_N ... else: ... return NotImplemented ... return self.\_\_class\_\_(N, ufunc(*scalars, **kwargs)) ... else: ... return NotImplemented ... def \_\_array\_function\_\_(self, func, types, args, kwargs): ... if func not in HANDLED\_FUNCTIONS: ... return NotImplemented ... \# Note: this allows subclasses that don't override ... \# \_\_array\_function\_\_ to handle DiagonalArray objects. ... if not all(issubclass(t, self.\_\_class\_\_) for t in types): ... return NotImplemented ... return HANDLED\_FUNCTIONS\[func\](\*args,**kwargs) ...

A convenient pattern is to define a decorator `implements` that can be used to add functions to `HANDLED_FUNCTIONS`.

\>\>\> def implements(np\_function): ... "Register an \_\_array\_function\_\_ implementation for DiagonalArray objects." ... def decorator(func): ... HANDLED\_FUNCTIONS\[np\_function\] = func ... return func ... return decorator ...

Now we write implementations of numpy functions for `DiagonalArray`. For completeness, to support the usage `arr.sum()` add a method `sum` that calls `numpy.sum(self)`, and the same for `mean`.

\>\>\> @implements(np.sum) ... def sum(arr): ... "Implementation of np.sum for DiagonalArray objects" ... return arr.\_i \* arr.\_N ... \>\>\> @implements(np.mean) ... def mean(arr): ... "Implementation of np.mean for DiagonalArray objects" ... return arr.\_i / arr.\_N ... \>\>\> arr = DiagonalArray(5, 1) \>\>\> np.sum(arr) 5 \>\>\> np.mean(arr) 0.2

If the user tries to use any numpy functions not included in `HANDLED_FUNCTIONS`, a `TypeError` will be raised by numpy, indicating that this operation is not supported. For example, concatenating two `DiagonalArrays` does not produce another diagonal array, so it is not supported.

\>\>\> np.concatenate(\[arr, arr\]) Traceback (most recent call last): ... TypeError: no implementation found for 'numpy.concatenate' on types that implement \_\_array\_function\_\_: \[\<class '\_\_main\_\_.DiagonalArray'\>\]

Additionally, our implementations of `sum` and `mean` do not accept the optional arguments that numpy's implementation does.

\>\>\> np.sum(arr, axis=0) Traceback (most recent call last): ... TypeError: sum() got an unexpected keyword argument 'axis'

The user always has the option of converting to a normal `numpy.ndarray` with <span class="title-ref">numpy.asarray</span> and using standard numpy from there.

\>\>\> np.concatenate(\[np.asarray(arr), np.asarray(arr)\]) array(\[\[1., 0., 0., 0., 0.\], \[0., 1., 0., 0., 0.\], \[0., 0., 1., 0., 0.\], \[0., 0., 0., 1., 0.\], \[0., 0., 0., 0., 1.\], \[1., 0., 0., 0., 0.\], \[0., 1., 0., 0., 0.\], \[0., 0., 1., 0., 0.\], \[0., 0., 0., 1., 0.\], \[0., 0., 0., 0., 1.\]\])

The implementation of `DiagonalArray` in this example only handles the `np.sum` and `np.mean` functions for brevity. Many other functions in the Numpy API are also available to wrap and a full-fledged custom array container can explicitly support all functions that Numpy makes available to wrap.

Numpy provides some utilities to aid testing of custom array containers that implement the `__array_ufunc__` and `__array_function__` protocols in the `numpy.testing.overrides` namespace.

To check if a Numpy function can be overridden via `__array_ufunc__`, you can use \`\~numpy.testing.overrides.allows\_array\_ufunc\_override\`:

\>\>\> from numpy.testing.overrides import allows\_array\_ufunc\_override \>\>\> allows\_array\_ufunc\_override(np.add) True

Similarly, you can check if a function can be overridden via `__array_function__` using <span class="title-ref">\~numpy.testing.overrides.allows\_array\_function\_override</span>.

Lists of every overridable function in the Numpy API are also available via <span class="title-ref">\~numpy.testing.overrides.get\_overridable\_numpy\_array\_functions</span> for functions that support the `__array_function__` protocol and <span class="title-ref">\~numpy.testing.overrides.get\_overridable\_numpy\_ufuncs</span> for functions that support the `__array_ufunc__` protocol. Both functions return sets of functions that are present in the Numpy public API. User-defined ufuncs or ufuncs defined in other libraries that depend on Numpy are not present in these sets.

Refer to the [dask source code](https://github.com/dask/dask) and [cupy source code](https://github.com/cupy/cupy) for more fully-worked examples of custom array containers.

See also \[NEP 18\<neps:nep-0018-array-function-protocol\>\](NEP 18\<neps:nep-0018-array-function-protocol\>.md).

---

basics.indexing.md

---

# Indexing on <span class="title-ref">ndarrays \<.ndarray\></span>

<div class="seealso">

\[Indexing routines \<routines.indexing\>\](\#indexing-routines-\<routines.indexing\>)

</div>

<div class="sectionauthor">

adapted from "Guide to NumPy" by Travis E. Oliphant

</div>

<div class="currentmodule">

numpy

</div>

<div class="index">

indexing, slicing

</div>

<span class="title-ref">ndarrays \<ndarray\></span> can be indexed using the standard Python `x[obj]` syntax, where *x* is the array and *obj* the selection. There are different kinds of indexing available depending on *obj*: basic indexing, advanced indexing and field access.

Most of the following examples show the use of indexing when referencing data in an array. The examples work just as well when assigning to an array. See \[assigning-values-to-indexed-arrays\](\#assigning-values-to-indexed-arrays) for specific examples and explanations on how assignments work.

Note that in Python, `x[(exp1, exp2, ..., expN)]` is equivalent to `x[exp1, exp2, ..., expN]`; the latter is just syntactic sugar for the former.

## Basic indexing

### Single element indexing

Single element indexing works exactly like that for other standard Python sequences. It is 0-based, and accepts negative indices for indexing from the end of the array. :

    >>> x = np.arange(10)
    >>> x[2]
    2
    >>> x[-2]
    8

It is not necessary to separate each dimension's index into its own set of square brackets. :

    >>> x.shape = (2, 5)  # now x is 2-dimensional
    >>> x[1, 3]
    8
    >>> x[1, -1]
    9

Note that if one indexes a multidimensional array with fewer indices than dimensions, one gets a subdimensional array. For example: :

    >>> x[0]
    array([0, 1, 2, 3, 4])

That is, each index specified selects the array corresponding to the rest of the dimensions selected. In the above example, choosing 0 means that the remaining dimension of length 5 is being left unspecified, and that what is returned is an array of that dimensionality and size. It must be noted that the returned array is a `view`, i.e., it is not a copy of the original, but points to the same values in memory as does the original array. In this case, the 1-D array at the first position (0) is returned. So using a single index on the returned array, results in a single element being returned. That is: :

    >>> x[0][2]
    2

So note that `x[0, 2] == x[0][2]` though the second case is more inefficient as a new temporary array is created after the first index that is subsequently indexed by 2.

\> **Note** \> NumPy uses C-order indexing. That means that the last index usually represents the most rapidly changing memory location, unlike Fortran or IDL, where the first index represents the most rapidly changing location in memory. This difference represents a great potential for confusion.

### Slicing and striding

Basic slicing extends Python's basic concept of slicing to N dimensions. Basic slicing occurs when *obj* is a <span class="title-ref">slice</span> object (constructed by `start:stop:step` notation inside of brackets), an integer, or a tuple of slice objects and integers. :py\`Ellipsis\` and <span class="title-ref">newaxis</span> objects can be interspersed with these as well.

<div class="index">

triple: ndarray; special methods; getitem triple: ndarray; special methods; setitem single: ellipsis single: newaxis

</div>

The simplest case of indexing with *N* integers returns an \[array scalar \<arrays.scalars\>\](\#array scalar-\<arrays.scalars\>) representing the corresponding item. As in Python, all indices are zero-based: for the *i*-th index \(n_i\), the valid range is \(0 \le n_i < d_i\) where \(d_i\) is the *i*-th element of the shape of the array. Negative indices are interpreted as counting from the end of the array (*i.e.*, if \(n_i < 0\), it means \(n_i + d_i\)).

All arrays generated by basic slicing are always `views <view>` of the original array.

\> **Note** \> NumPy slicing creates a `view` instead of a copy as in the case of built-in Python sequences such as string, tuple and list. Care must be taken when extracting a small portion from a large array which becomes useless after the extraction, because the small portion extracted contains a reference to the large original array whose memory will not be released until all arrays derived from it are garbage-collected. In such cases an explicit `copy()` is recommended.

The standard rules of sequence slicing apply to basic slicing on a per-dimension basis (including using a step index). Some useful concepts to remember include:

  - The basic slice syntax is `i:j:k` where *i* is the starting index, *j* is the stopping index, and *k* is the step (\(k\neq0\)). This selects the *m* elements (in the corresponding dimension) with index values *i*, *i + k*, ..., *i + (m - 1) k* where \(m = q + (r\neq0)\) and *q* and *r* are the quotient and remainder obtained by dividing *j - i* by *k*: *j - i = q k + r*, so that *i + (m - 1) k \< j*. For example:
    
        >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
        >>> x[1:7:2]
        array([1, 3, 5])

  - Negative *i* and *j* are interpreted as *n + i* and *n + j* where *n* is the number of elements in the corresponding dimension. Negative *k* makes stepping go towards smaller indices. From the above example:
    
        >>> x[-2:10]
        array([8, 9])
        >>> x[-3:3:-1]
        array([7, 6, 5, 4])

  - Assume *n* is the number of elements in the dimension being sliced. Then, if *i* is not given it defaults to 0 for *k \> 0* and *n - 1* for *k \< 0* . If *j* is not given it defaults to *n* for *k \> 0* and *-n-1* for *k \< 0* . If *k* is not given it defaults to 1. Note that `::` is the same as `:` and means select all indices along this axis. From the above example:
    
        >>> x[5:]
        array([5, 6, 7, 8, 9])

  - If the number of objects in the selection tuple is less than *N*, then `:` is assumed for any subsequent dimensions. For example:
    
        >>> x = np.array([[[1],[2],[3]], [[4],[5],[6]]])
        >>> x.shape
        (2, 3, 1)
        >>> x[1:2]
        array([[[4],
                [5],
                [6]]])

  - An integer, *i*, returns the same values as `i:i+1` **except** the dimensionality of the returned object is reduced by
    
    1\. In particular, a selection tuple with the *p*-th element an integer (and all other entries `:`) returns the corresponding sub-array with dimension *N - 1*. If *N = 1* then the returned object is an array scalar. These objects are explained in \[arrays.scalars\](\#arrays.scalars).

  - If the selection tuple has all entries `:` except the *p*-th entry which is a slice object `i:j:k`, then the returned array has dimension *N* formed by stacking, along the *p*-th axis, the sub-arrays returned by integer indexing of elements *i*, *i+k*, ..., *i + (m - 1) k \< j*.

  - Basic slicing with more than one non-`:` entry in the slicing tuple, acts like repeated application of slicing using a single non-`:` entry, where the non-`:` entries are successively taken (with all other non-`:` entries replaced by `:`). Thus, `x[ind1, ..., ind2,:]` acts like `x[ind1][..., ind2, :]` under basic slicing.
    
    <div class="warning">
    
    <div class="title">
    
    Warning
    
    </div>
    
    The above is **not** true for advanced indexing.
    
    </div>

  - You may use slicing to set values in the array, but (unlike lists) you can never grow the array. The size of the value to be set in `x[obj] = value` must be (broadcastable to) the same shape as `x[obj]`.

  - A slicing tuple can always be constructed as *obj* and used in the `x[obj]` notation. Slice objects can be used in the construction in place of the `[start:stop:step]` notation. For example, `x[1:10:5, ::-1]` can also be implemented as `obj = (slice(1, 10, 5), slice(None, None, -1)); x[obj]` . This can be useful for constructing generic code that works on arrays of arbitrary dimensions. See \[dealing-with-variable-indices\](\#dealing-with-variable-indices) for more information.

<div class="index">

pair: ndarray; view

</div>

### Dimensional indexing tools

There are some tools to facilitate the easy matching of array shapes with expressions and in assignments.

:py\`Ellipsis\` expands to the number of `:` objects needed for the selection tuple to index all dimensions. In most cases, this means that the length of the expanded selection tuple is `x.ndim`. There may only be a single ellipsis present. From the above example:

    >>> x[..., 0]
    array([[1, 2, 3],
          [4, 5, 6]])

This is equivalent to:

    >>> x[:, :, 0]
    array([[1, 2, 3],
          [4, 5, 6]])

Each <span class="title-ref">newaxis</span> object in the selection tuple serves to expand the dimensions of the resulting selection by one unit-length dimension. The added dimension is the position of the <span class="title-ref">newaxis</span> object in the selection tuple. <span class="title-ref">newaxis</span> is an alias for `None`, and `None` can be used in place of this with the same result. From the above example:

    >>> x[:, np.newaxis, :, :].shape
    (2, 1, 3, 1)
    >>> x[:, None, :, :].shape
    (2, 1, 3, 1)

This can be handy to combine two arrays in a way that otherwise would require explicit reshaping operations. For example:

    >>> x = np.arange(5)
    >>> x[:, np.newaxis] + x[np.newaxis, :]
    array([[0, 1, 2, 3, 4],
          [1, 2, 3, 4, 5],
          [2, 3, 4, 5, 6],
          [3, 4, 5, 6, 7],
          [4, 5, 6, 7, 8]])

## Advanced indexing

Advanced indexing is triggered when the selection object, *obj*, is a non-tuple sequence object, an <span class="title-ref">ndarray</span> (of data type integer or bool), or a tuple with at least one sequence object or ndarray (of data type integer or bool). There are two types of advanced indexing: integer and Boolean.

Advanced indexing always returns a *copy* of the data (contrast with basic slicing that returns a `view`).

\> **Warning** \> The definition of advanced indexing means that `x[(1, 2, 3),]` is fundamentally different than `x[(1, 2, 3)]`. The latter is equivalent to `x[1, 2, 3]` which will trigger basic selection while the former will trigger advanced indexing. Be sure to understand why this occurs.

### Integer array indexing

Integer array indexing allows selection of arbitrary items in the array based on their *N*-dimensional index. Each integer array represents a number of indices into that dimension.

Negative values are permitted in the index arrays and work as they do with single indices or slices:

    >>> x = np.arange(10, 1, -1)
    >>> x
    array([10,  9,  8,  7,  6,  5,  4,  3,  2])
    >>> x[np.array([3, 3, 1, 8])]
    array([7, 7, 9, 2])
    >>> x[np.array([3, 3, -3, 8])]
    array([7, 7, 4, 2])

If the index values are out of bounds then an `IndexError` is thrown:

    >>> x = np.array([[1, 2], [3, 4], [5, 6]])
    >>> x[np.array([1, -1])]
    array([[3, 4],
          [5, 6]])
    >>> x[np.array([3, 4])]
    Traceback (most recent call last):
      ...
    IndexError: index 3 is out of bounds for axis 0 with size 3

When the index consists of as many integer arrays as dimensions of the array being indexed, the indexing is straightforward, but different from slicing.

Advanced indices always are \[broadcast\<basics.broadcasting\>\](\#broadcast\<basics.broadcasting\>) and iterated as *one*:

    result[i_1, ..., i_M] == x[ind_1[i_1, ..., i_M], ind_2[i_1, ..., i_M],
                               ..., ind_N[i_1, ..., i_M]]

Note that the resulting shape is identical to the (broadcast) indexing array shapes `ind_1, ..., ind_N`. If the indices cannot be broadcast to the same shape, an exception `IndexError: shape mismatch: indexing arrays could not be broadcast together with shapes...` is raised.

Indexing with multidimensional index arrays tend to be more unusual uses, but they are permitted, and they are useful for some problems. Weâ€™ll start with the simplest multidimensional case:

    >>> y = np.arange(35).reshape(5, 7)
    >>> y
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  8,  9, 10, 11, 12, 13],
           [14, 15, 16, 17, 18, 19, 20],
           [21, 22, 23, 24, 25, 26, 27],
           [28, 29, 30, 31, 32, 33, 34]])
    >>> y[np.array([0, 2, 4]), np.array([0, 1, 2])]
    array([ 0, 15, 30])

In this case, if the index arrays have a matching shape, and there is an index array for each dimension of the array being indexed, the resultant array has the same shape as the index arrays, and the values correspond to the index set for each position in the index arrays. In this example, the first index value is 0 for both index arrays, and thus the first value of the resultant array is `y[0, 0]`. The next value is `y[2, 1]`, and the last is `y[4, 2]`.

If the index arrays do not have the same shape, there is an attempt to broadcast them to the same shape. If they cannot be broadcast to the same shape, an exception is raised:

    >>> y[np.array([0, 2, 4]), np.array([0, 1])]
    Traceback (most recent call last):
      ...
    IndexError: shape mismatch: indexing arrays could not be broadcast
    together with shapes (3,) (2,)

The broadcasting mechanism permits index arrays to be combined with scalars for other indices. The effect is that the scalar value is used for all the corresponding values of the index arrays:

    >>> y[np.array([0, 2, 4]), 1]
    array([ 1, 15, 29])

Jumping to the next level of complexity, it is possible to only partially index an array with index arrays. It takes a bit of thought to understand what happens in such cases. For example if we just use one index array with y:

    >>> y[np.array([0, 2, 4])]
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [14, 15, 16, 17, 18, 19, 20],
           [28, 29, 30, 31, 32, 33, 34]])

It results in the construction of a new array where each value of the index array selects one row from the array being indexed and the resultant array has the resulting shape (number of index elements, size of row).

In general, the shape of the resultant array will be the concatenation of the shape of the index array (or the shape that all the index arrays were broadcast to) with the shape of any unused dimensions (those not indexed) in the array being indexed.

**Example**

From each row, a specific element should be selected. The row index is just `[0, 1, 2]` and the column index specifies the element to choose for the corresponding row, here `[0, 1, 0]`. Using both together the task can be solved using advanced indexing:

    >>> x = np.array([[1, 2], [3, 4], [5, 6]])
    >>> x[[0, 1, 2], [0, 1, 0]]
    array([1, 4, 5])

To achieve a behaviour similar to the basic slicing above, broadcasting can be used. The function <span class="title-ref">ix\_</span> can help with this broadcasting. This is best understood with an example.

**Example**

From a 4x3 array the corner elements should be selected using advanced indexing. Thus all elements for which the column is one of `[0, 2]` and the row is one of `[0, 3]` need to be selected. To use advanced indexing one needs to select all elements *explicitly*. Using the method explained previously one could write:

    >>> x = np.array([[ 0,  1,  2],
    ...               [ 3,  4,  5],
    ...               [ 6,  7,  8],
    ...               [ 9, 10, 11]])
    >>> rows = np.array([[0, 0],
    ...                  [3, 3]], dtype=np.intp)
    >>> columns = np.array([[0, 2],
    ...                     [0, 2]], dtype=np.intp)
    >>> x[rows, columns]
    array([[ 0,  2],
           [ 9, 11]])

However, since the indexing arrays above just repeat themselves, broadcasting can be used (compare operations such as `rows[:, np.newaxis] + columns`) to simplify this:

    >>> rows = np.array([0, 3], dtype=np.intp)
    >>> columns = np.array([0, 2], dtype=np.intp)
    >>> rows[:, np.newaxis]
    array([[0],
           [3]])
    >>> x[rows[:, np.newaxis], columns]
    array([[ 0,  2],
           [ 9, 11]])

This broadcasting can also be achieved using the function \`[ix]()\`:

> \>\>\> x\[[np.ix]()(rows, columns)\] array(\[\[ 0, 2\], \[ 9, 11\]\])

Note that without the `np.ix_` call, only the diagonal elements would be selected:

    >>> x[rows, columns]
    array([ 0, 11])

This difference is the most important thing to remember about indexing with multiple advanced indices.

**Example**

A real-life example of where advanced indexing may be useful is for a color lookup table where we want to map the values of an image into RGB triples for display. The lookup table could have a shape (nlookup, 3). Indexing such an array with an image with shape (ny, nx) with dtype=np.uint8 (or any integer type so long as values are with the bounds of the lookup table) will result in an array of shape (ny, nx, 3) where a triple of RGB values is associated with each pixel location.

### Boolean array indexing

This advanced indexing occurs when *obj* is an array object of Boolean type, such as may be returned from comparison operators. A single boolean index array is practically identical to `x[obj.nonzero()]` where, as described above, <span class="title-ref">obj.nonzero() \<ndarray.nonzero\></span> returns a tuple (of length <span class="title-ref">obj.ndim \<ndarray.ndim\></span>) of integer index arrays showing the :py\`True\` elements of *obj*. However, it is faster when `obj.shape == x.shape`.

If `obj.ndim == x.ndim`, `x[obj]` returns a 1-dimensional array filled with the elements of *x* corresponding to the :py\`True\` values of *obj*. The search order will be `row-major`, C-style. An index error will be raised if the shape of *obj* does not match the corresponding dimensions of *x*, regardless of whether those values are :py\`True\` or :py\`False\`.

A common use case for this is filtering for desired element values. For example, one may wish to select all entries from an array which are not \`numpy.nan\`:

    >>> x = np.array([[1., 2.], [np.nan, 3.], [np.nan, np.nan]])
    >>> x[~np.isnan(x)]
    array([1., 2., 3.])

Or wish to add a constant to all negative elements:

    >>> x = np.array([1., -1., -2., 3])
    >>> x[x < 0] += 20
    >>> x
    array([ 1., 19., 18., 3.])

In general if an index includes a Boolean array, the result will be identical to inserting `obj.nonzero()` into the same position and using the integer array indexing mechanism described above. `x[ind_1, boolean_array, ind_2]` is equivalent to `x[(ind_1,) + boolean_array.nonzero() + (ind_2,)]`.

If there is only one Boolean array and no integer indexing array present, this is straightforward. Care must only be taken to make sure that the boolean index has *exactly* as many dimensions as it is supposed to work with.

In general, when the boolean array has fewer dimensions than the array being indexed, this is equivalent to `x[b, ...]`, which means x is indexed by b followed by as many `:` as are needed to fill out the rank of x. Thus the shape of the result is one dimension containing the number of True elements of the boolean array, followed by the remaining dimensions of the array being indexed:

    >>> x = np.arange(35).reshape(5, 7)
    >>> b = x > 20
    >>> b[:, 5]
    array([False, False, False,  True,  True])
    >>> x[b[:, 5]]
    array([[21, 22, 23, 24, 25, 26, 27],
          [28, 29, 30, 31, 32, 33, 34]])

Here the 4th and 5th rows are selected from the indexed array and combined to make a 2-D array.

**Example**

From an array, select all rows which sum up to less or equal two:

    >>> x = np.array([[0, 1], [1, 1], [2, 2]])
    >>> rowsum = x.sum(-1)
    >>> x[rowsum <= 2, :]
    array([[0, 1],
           [1, 1]])

Combining multiple Boolean indexing arrays or a Boolean with an integer indexing array can best be understood with the <span class="title-ref">obj.nonzero() \<ndarray.nonzero\></span> analogy. The function <span class="title-ref">ix\_</span> also supports boolean arrays and will work without any surprises.

**Example**

Use boolean indexing to select all rows adding up to an even number. At the same time columns 0 and 2 should be selected with an advanced integer index. Using the <span class="title-ref">ix\_</span> function this can be done with:

    >>> x = np.array([[ 0,  1,  2],
    ...               [ 3,  4,  5],
    ...               [ 6,  7,  8],
    ...               [ 9, 10, 11]])
    >>> rows = (x.sum(-1) % 2) == 0
    >>> rows
    array([False,  True, False,  True])
    >>> columns = [0, 2]
    >>> x[np.ix_(rows, columns)]
    array([[ 3,  5],
           [ 9, 11]])

Without the `np.ix_` call, only the diagonal elements would be selected.

Or without `np.ix_` (compare the integer array examples):

    >>> rows = rows.nonzero()[0]
    >>> x[rows[:, np.newaxis], columns]
    array([[ 3,  5],
           [ 9, 11]])

**Example**

Use a 2-D boolean array of shape (2, 3) with four True elements to select rows from a 3-D array of shape (2, 3, 5) results in a 2-D result of shape (4, 5):

    >>> x = np.arange(30).reshape(2, 3, 5)
    >>> x
    array([[[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14]],
          [[15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [25, 26, 27, 28, 29]]])
    >>> b = np.array([[True, True, False], [False, True, True]])
    >>> x[b]
    array([[ 0,  1,  2,  3,  4],
          [ 5,  6,  7,  8,  9],
          [20, 21, 22, 23, 24],
          [25, 26, 27, 28, 29]])

### Combining advanced and basic indexing

When there is at least one slice (`:`), ellipsis (`...`) or <span class="title-ref">newaxis</span> in the index (or the array has more dimensions than there are advanced indices), then the behaviour can be more complicated. It is like concatenating the indexing result for each advanced index element.

In the simplest case, there is only a *single* advanced index combined with a slice. For example:

    >>> y = np.arange(35).reshape(5,7)
    >>> y[np.array([0, 2, 4]), 1:3]
    array([[ 1,  2],
           [15, 16],
           [29, 30]])

In effect, the slice and index array operation are independent. The slice operation extracts columns with index 1 and 2, (i.e. the 2nd and 3rd columns), followed by the index array operation which extracts rows with index 0, 2 and 4 (i.e the first, third and fifth rows). This is equivalent to:

    >>> y[:, 1:3][np.array([0, 2, 4]), :]
    array([[ 1,  2],
           [15, 16],
           [29, 30]])

A single advanced index can, for example, replace a slice and the result array will be the same. However, it is a copy and may have a different memory layout. A slice is preferable when it is possible. For example:

    >>> x = np.array([[ 0,  1,  2],
    ...               [ 3,  4,  5],
    ...               [ 6,  7,  8],
    ...               [ 9, 10, 11]])
    >>> x[1:2, 1:3]
    array([[4, 5]])
    >>> x[1:2, [1, 2]]
    array([[4, 5]])

The easiest way to understand a combination of *multiple* advanced indices may be to think in terms of the resulting shape. There are two parts to the indexing operation, the subspace defined by the basic indexing (excluding integers) and the subspace from the advanced indexing part. Two cases of index combination need to be distinguished:

  - The advanced indices are separated by a slice, :py\`Ellipsis\` or <span class="title-ref">newaxis</span>. For example `x[arr1, :, arr2]`.
  - The advanced indices are all next to each other. For example `x[..., arr1, arr2, :]` but *not* `x[arr1, :, 1]` since `1` is an advanced index in this regard.

In the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array (the latter logic is what makes simple advanced indexing behave just like slicing).

**Example**

Suppose `x.shape` is (10, 20, 30) and `ind` is a (2, 5, 2)-shaped indexing <span class="title-ref">intp</span> array, then `result = x[..., ind, :]` has shape (10, 2, 5, 2, 30) because the (20,)-shaped subspace has been replaced with a (2, 5, 2)-shaped broadcasted indexing subspace. If we let *i, j, k* loop over the (2, 5, 2)-shaped subspace then `result[..., i, j, k, :] = x[..., ind[i, j, k], :]`. This example produces the same result as <span class="title-ref">x.take(ind, axis=-2) \<ndarray.take\></span>.

**Example**

Let `x.shape` be (10, 20, 30, 40, 50) and suppose `ind_1` and `ind_2` can be broadcast to the shape (2, 3, 4). Then `x[:, ind_1, ind_2]` has shape (10, 2, 3, 4, 40, 50) because the (20, 30)-shaped subspace from X has been replaced with the (2, 3, 4) subspace from the indices. However, `x[:, ind_1, :, ind_2]` has shape (2, 3, 4, 10, 30, 50) because there is no unambiguous place to drop in the indexing subspace, thus it is tacked-on to the beginning. It is always possible to use <span class="title-ref">.transpose() \<ndarray.transpose\></span> to move the subspace anywhere desired. Note that this example cannot be replicated using <span class="title-ref">take</span>.

**Example**

Slicing can be combined with broadcasted boolean indices:

    >>> x = np.arange(35).reshape(5, 7)
    >>> b = x > 20
    >>> b
    array([[False, False, False, False, False, False, False],
          [False, False, False, False, False, False, False],
          [False, False, False, False, False, False, False],
          [ True,  True,  True,  True,  True,  True,  True],
          [ True,  True,  True,  True,  True,  True,  True]])
    >>> x[b[:, 5], 1:3]
    array([[22, 23],
          [29, 30]])

## Field access

<div class="seealso">

\[structured\_arrays\](\#structured\_arrays)

</div>

If the <span class="title-ref">ndarray</span> object is a structured array the `fields <field>` of the array can be accessed by indexing the array with strings, dictionary-like.

Indexing `x['field-name']` returns a new `view` to the array, which is of the same shape as *x* (except when the field is a sub-array) but of data type `x.dtype['field-name']` and contains only the part of the data in the specified field. Also, \[record array \<arrays.classes.rec\>\](\#record-array-\<arrays.classes.rec\>) scalars can be "indexed" this way.

Indexing into a structured array can also be done with a list of field names, e.g. `x[['field-name1', 'field-name2']]`. As of NumPy 1.16, this returns a view containing only those fields. In older versions of NumPy, it returned a copy. See the user guide section on \[structured\_arrays\](\#structured\_arrays) for more information on multifield indexing.

If the accessed field is a sub-array, the dimensions of the sub-array are appended to the shape of the result. For example:

    >>> x = np.zeros((2, 2), dtype=[('a', np.int32), ('b', np.float64, (3, 3))])
    >>> x['a'].shape
    (2, 2)
    >>> x['a'].dtype
    dtype('int32')
    >>> x['b'].shape
    (2, 2, 3, 3)
    >>> x['b'].dtype
    dtype('float64')

## Flat iterator indexing

<span class="title-ref">x.flat \<ndarray.flat\></span> returns an iterator that will iterate over the entire array (in C-contiguous style with the last index varying the fastest). This iterator object can also be indexed using basic slicing or advanced indexing as long as the selection object is not a tuple. This should be clear from the fact that <span class="title-ref">x.flat \<ndarray.flat\></span> is a 1-dimensional view. It can be used for integer indexing with 1-dimensional C-style-flat indices. The shape of any returned array is therefore the shape of the integer indexing object.

<div class="index">

single: indexing single: ndarray

</div>

## Assigning values to indexed arrays

As mentioned, one can select a subset of an array to assign to using a single index, slices, and index and mask arrays. The value being assigned to the indexed array must be shape consistent (the same shape or broadcastable to the shape the index produces). For example, it is permitted to assign a constant to a slice: :

    >>> x = np.arange(10)
    >>> x[2:7] = 1

or an array of the right size: :

    >>> x[2:7] = np.arange(5)

Note that assignments may result in changes if assigning higher types to lower types (like floats to ints) or even exceptions (assigning complex to floats or ints): :

    >>> x[1] = 1.2
    >>> x[1]
    1
    >>> x[1] = 1.2j  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
      ...
    TypeError: can't convert complex to int

Unlike some of the references (such as array and mask indices) assignments are always made to the original data in the array (indeed, nothing else would make sense\!). Note though, that some actions may not work as one may naively expect. This particular example is often surprising to people: :

    >>> x = np.arange(0, 50, 10)
    >>> x
    array([ 0, 10, 20, 30, 40])
    >>> x[np.array([1, 1, 3, 1])] += 1
    >>> x
    array([ 0, 11, 20, 31, 40])

Where people expect that the 1st location will be incremented by 3. In fact, it will only be incremented by 1. The reason is that a new array is extracted from the original (as a temporary) containing the values at 1, 1, 3, 1, then the value 1 is added to the temporary, and then the temporary is assigned back to the original array. Thus the value of the array at `x[1] + 1` is assigned to `x[1]` three times, rather than being incremented 3 times.

## Dealing with variable numbers of indices within programs

The indexing syntax is very powerful but limiting when dealing with a variable number of indices. For example, if you want to write a function that can handle arguments with various numbers of dimensions without having to write special case code for each number of possible dimensions, how can that be done? If one supplies to the index a tuple, the tuple will be interpreted as a list of indices. For example:

    >>> z = np.arange(81).reshape(3, 3, 3, 3)
    >>> indices = (1, 1, 1, 1)
    >>> z[indices]
    40

So one can use code to construct tuples of any number of indices and then use these within an index.

Slices can be specified within programs by using the slice() function in Python. For example: :

    >>> indices = (1, 1, 1, slice(0, 2))  # same as [1, 1, 1, 0:2]
    >>> z[indices]
    array([39, 40])

Likewise, ellipsis can be specified by code by using the Ellipsis object: :

    >>> indices = (1, Ellipsis, 1)  # same as [1, ..., 1]
    >>> z[indices]
    array([[28, 31, 34],
           [37, 40, 43],
           [46, 49, 52]])

For this reason, it is possible to use the output from the <span class="title-ref">np.nonzero() \<ndarray.nonzero\></span> function directly as an index since it always returns a tuple of index arrays.

Because of the special treatment of tuples, they are not automatically converted to an array as a list would be. As an example: :

    >>> z[[1, 1, 1, 1]]  # produces a large array
    array([[[[27, 28, 29],
             [30, 31, 32], ...
    >>> z[(1, 1, 1, 1)]  # returns a single value
    40

## Detailed notes

These are some detailed notes, which are not of importance for day to day indexing (in no particular order):

  - The native NumPy indexing type is `intp` and may differ from the default integer array type. `intp` is the smallest data type sufficient to safely index any array; for advanced indexing it may be faster than other types.
  - For advanced assignments, there is in general no guarantee for the iteration order. This means that if an element is set more than once, it is not possible to predict the final result.
  - An empty (tuple) index is a full scalar index into a zero-dimensional array. `x[()]` returns a *scalar* if `x` is zero-dimensional and a view otherwise. On the other hand, `x[...]` always returns a view.
  - If a zero-dimensional array is present in the index *and* it is a full integer index the result will be a *scalar* and not a zero-dimensional array. (Advanced indexing is not triggered.)
  - When an ellipsis (`...`) is present but has no size (i.e. replaces zero `:`) the result will still always be an array. A view if no advanced index is present, otherwise a copy.
  - The `nonzero` equivalence for Boolean arrays does not hold for zero dimensional boolean arrays.
  - When the result of an advanced indexing operation has no elements but an individual index is out of bounds, whether or not an `IndexError` is raised is undefined (e.g. `x[[], [123]]` with `123` being out of bounds).
  - When a *casting* error occurs during assignment (for example updating a numerical array using a sequence of strings), the array being assigned to may end up in an unpredictable partially updated state. However, if any other error (such as an out of bounds index) occurs, the array will remain unchanged.
  - The memory layout of an advanced indexing result is optimized for each indexing operation and no particular memory order can be assumed.
  - When using a subclass (especially one which manipulates its shape), the default `ndarray.__setitem__` behaviour will call `__getitem__` for *basic* indexing but not for *advanced* indexing. For such a subclass it may be preferable to call `ndarray.__setitem__` with a *base class* ndarray view on the data. This *must* be done if the subclasses `__getitem__` does not return views.

---

basics.interoperability.md

---

# Interoperability with NumPy

NumPy's ndarray objects provide both a high-level API for operations on array-structured data and a concrete implementation of the API based on \[strided in-RAM storage \<arrays\>\](\#strided-in-ram-storage-\<arrays\>). While this API is powerful and fairly general, its concrete implementation has limitations. As datasets grow and NumPy becomes used in a variety of new environments and architectures, there are cases where the strided in-RAM storage strategy is inappropriate, which has caused different libraries to reimplement this API for their own uses. This includes GPU arrays ([CuPy](https://cupy.dev/)), Sparse arrays (<span class="title-ref">scipy.sparse</span>, [PyData/Sparse](https://sparse.pydata.org/)) and parallel arrays ([Dask](https://docs.dask.org/) arrays) as well as various NumPy-like implementations in deep learning frameworks, like [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/). Similarly, there are many projects that build on top of the NumPy API for labeled and indexed arrays ([XArray](https://xarray.dev/)), automatic differentiation ([JAX](https://jax.readthedocs.io/)), masked arrays (<span class="title-ref">numpy.ma</span>), physical units ([astropy.units](https://docs.astropy.org/en/stable/units/), [pint](https://pint.readthedocs.io/), [unyt](https://unyt.readthedocs.io/)), among others that add additional functionality on top of the NumPy API.

Yet, users still want to work with these arrays using the familiar NumPy API and reuse existing code with minimal (ideally zero) porting overhead. With this goal in mind, various protocols are defined for implementations of multi-dimensional arrays with high-level APIs matching NumPy.

Broadly speaking, there are three groups of features used for interoperability with NumPy:

1.  Methods of turning a foreign object into an ndarray;
2.  Methods of deferring execution from a NumPy function to another array library;
3.  Methods that use NumPy functions and return an instance of a foreign object.

We describe these features below.

## 1\. Using arbitrary objects in NumPy

The first set of interoperability features from the NumPy API allows foreign objects to be treated as NumPy arrays whenever possible. When NumPy functions encounter a foreign object, they will try (in order):

1.  The buffer protocol, described :py\[in the Python C-API documentation \<python:c-api/buffer\>\](in the Python C-API documentation \<python:c-api/buffer\>.md).
2.  The `__array_interface__` protocol, described \[in this page \<arrays.interface\>\](\#in-this-page-\<arrays.interface\>). A precursor to Python's buffer protocol, it defines a way to access the contents of a NumPy array from other C extensions.
3.  The `__array__()` method, which asks an arbitrary object to convert itself into an array.

For both the buffer and the `__array_interface__` protocols, the object describes its memory layout and NumPy does everything else (zero-copy if possible). If that's not possible, the object itself is responsible for returning a `ndarray` from `__array__()`.

\[DLPack \<dlpack:index\>\](DLPack \<dlpack:index\>.md) is yet another protocol to convert foreign objects to NumPy arrays in a language and device agnostic manner. NumPy doesn't implicitly convert objects to ndarrays using DLPack. It provides the function <span class="title-ref">numpy.from\_dlpack</span> that accepts any object implementing the `__dlpack__` method and outputs a NumPy ndarray (which is generally a view of the input object's data buffer). The \[dlpack:python-spec\](\#dlpack:python-spec) page explains the `__dlpack__` protocol in detail.

### The array interface protocol

The \[array interface protocol \<arrays.interface\>\](\#array-interface-protocol-\<arrays.interface\>) defines a way for array-like objects to reuse each other's data buffers. Its implementation relies on the existence of the following attributes or methods:

  - `__array_interface__`: a Python dictionary containing the shape, the element type, and optionally, the data buffer address and the strides of an array-like object;
  - `__array__()`: a method returning the NumPy ndarray copy or a view of an array-like object;

The `__array_interface__` attribute can be inspected directly:

> \>\>\> import numpy as np \>\>\> x = np.array(\[1, 2, 5.0, 8\]) \>\>\> x.\_\_array\_interface\_\_ {'data': (94708397920832, False), 'strides': None, 'descr': \[('', '\<f8')\], 'typestr': '\<f8', 'shape': (4,), 'version': 3}

The `__array_interface__` attribute can also be used to manipulate the object data in place:

> \>\>\> class wrapper(): ... pass ... \>\>\> arr = np.array(\[1, 2, 3, 4\]) \>\>\> buf = arr.\_\_array\_interface\_\_ \>\>\> buf {'data': (140497590272032, False), 'strides': None, 'descr': \[('', '\<i8')\], 'typestr': '\<i8', 'shape': (4,), 'version': 3} \>\>\> buf\['shape'\] = (2, 2) \>\>\> w = wrapper() \>\>\> w.\_\_array\_interface\_\_ = buf \>\>\> new\_arr = np.array(w, copy=False) \>\>\> new\_arr array(\[\[1, 2\], \[3, 4\]\])

We can check that `arr` and `new_arr` share the same data buffer:

> \>\>\> new\_arr\[0, 0\] = 1000 \>\>\> new\_arr array(\[\[1000, 2\], \[ 3, 4\]\]) \>\>\> arr array(\[1000, 2, 3, 4\])

### The `__array__()` method

The `__array__()` method ensures that any NumPy-like object (an array, any object exposing the array interface, an object whose `__array__()` method returns an array or any nested sequence) that implements it can be used as a NumPy array. If possible, this will mean using `__array__()` to create a NumPy ndarray view of the array-like object. Otherwise, this copies the data into a new ndarray object. This is not optimal, as coercing arrays into ndarrays may cause performance problems or create the need for copies and loss of metadata, as the original object and any attributes/behavior it may have had, is lost.

The signature of the method should be `__array__(self, dtype=None, copy=None)`. If a passed `dtype` isn't `None` and different than the object's data type, a casting should happen to a specified type. If `copy` is `None`, a copy should be made only if `dtype` argument enforces it. For `copy=True`, a copy should always be made, where `copy=False` should raise an exception if a copy is needed.

If a class implements the old signature `__array__(self)`, for `np.array(a)` a warning will be raised saying that `dtype` and `copy` arguments are missing.

To see an example of a custom array implementation including the use of `__array__()`, see \[basics.dispatch\](\#basics.dispatch).

### The DLPack Protocol

The \[DLPack \<dlpack:index\>\](DLPack \<dlpack:index\>.md) protocol defines a memory-layout of strided n-dimensional array objects. It offers the following syntax for data exchange:

1.  A <span class="title-ref">numpy.from\_dlpack</span> function, which accepts (array) objects with a `__dlpack__` method and uses that method to construct a new array containing the data from `x`.
2.  `__dlpack__(self, stream=None)` and `__dlpack_device__` methods on the array object, which will be called from within `from_dlpack`, to query what device the array is on (may be needed to pass in the correct stream, e.g. in the case of multiple GPUs) and to access the data.

Unlike the buffer protocol, DLPack allows exchanging arrays containing data on devices other than the CPU (e.g. Vulkan or GPU). Since NumPy only supports CPU, it can only convert objects whose data exists on the CPU. But other libraries, like [PyTorch](https://pytorch.org/) and [CuPy](https://cupy.dev/), may exchange data on GPU using this protocol.

## 2\. Operating on foreign objects without converting

A second set of methods defined by the NumPy API allows us to defer the execution from a NumPy function to another array library.

Consider the following function.

> \>\>\> import numpy as np \>\>\> def f(x): ... return np.mean(np.exp(x))

Note that <span class="title-ref">np.exp \<numpy.exp\></span> is a \[ufunc \<ufuncs-basics\>\](\#ufunc-\<ufuncs-basics\>), which means that it operates on ndarrays in an element-by-element fashion. On the other hand, <span class="title-ref">np.mean \<numpy.mean\></span> operates along one of the array's axes.

We can apply `f` to a NumPy ndarray object directly:

> \>\>\> x = np.array(\[1, 2, 3, 4\]) \>\>\> f(x) 21.1977562209304

We would like this function to work equally well with any NumPy-like array object.

NumPy allows a class to indicate that it would like to handle computations in a custom-defined way through the following interfaces:

  - `__array_ufunc__`: allows third-party objects to support and override \[ufuncs \<ufuncs-basics\>\](\#ufuncs-\<ufuncs-basics\>).
  - `__array_function__`: a catch-all for NumPy functionality that is not covered by the `__array_ufunc__` protocol for universal functions.

As long as foreign objects implement the `__array_ufunc__` or `__array_function__` protocols, it is possible to operate on them without the need for explicit conversion.

### The `__array_ufunc__` protocol

A \[universal function (or ufunc for short) \<ufuncs-basics\>\](\#universal-function-(or-ufunc-for-short)-\<ufuncs-basics\>) is a â€œvectorizedâ€ wrapper for a function that takes a fixed number of specific inputs and produces a fixed number of specific outputs. The output of the ufunc (and its methods) is not necessarily a ndarray, if not all input arguments are ndarrays. Indeed, if any input defines an `__array_ufunc__` method, control will be passed completely to that function, i.e., the ufunc is overridden. The `__array_ufunc__` method defined on that (non-ndarray) object has access to the NumPy ufunc. Because ufuncs have a well-defined structure, the foreign `__array_ufunc__` method may rely on ufunc attributes like `.at()`, `.reduce()`, and others.

A subclass can override what happens when executing NumPy ufuncs on it by overriding the default `ndarray.__array_ufunc__` method. This method is executed instead of the ufunc and should return either the result of the operation, or `NotImplemented` if the operation requested is not implemented.

### The `__array_function__` protocol

To achieve enough coverage of the NumPy API to support downstream projects, there is a need to go beyond `__array_ufunc__` and implement a protocol that allows arguments of a NumPy function to take control and divert execution to another function (for example, a GPU or parallel implementation) in a way that is safe and consistent across projects.

The semantics of `__array_function__` are very similar to `__array_ufunc__`, except the operation is specified by an arbitrary callable object rather than a ufunc instance and method. For more details, see \[NEP18\](\#nep18).

## 3\. Returning foreign objects

A third type of feature set is meant to use the NumPy function implementation and then convert the return value back into an instance of the foreign object. The `__array_finalize__` and `__array_wrap__` methods act behind the scenes to ensure that the return type of a NumPy function can be specified as needed.

The `__array_finalize__` method is the mechanism that NumPy provides to allow subclasses to handle the various ways that new instances get created. This method is called whenever the system internally allocates a new array from an object which is a subclass (subtype) of the ndarray. It can be used to change attributes after construction, or to update meta-information from the â€œparent.â€

The `__array_wrap__` method â€œwraps up the actionâ€ in the sense of allowing any object (such as user-defined functions) to set the type of its return value and update attributes and metadata. This can be seen as the opposite of the `__array__` method. At the end of every object that implements `__array_wrap__`, this method is called on the input object with the highest *array priority*, or the output object if one was specified. The `__array_priority__` attribute is used to determine what type of object to return in situations where there is more than one possibility for the Python type of the returned object. For example, subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the user.

For more information on these methods, see \[basics.subclassing\](\#basics.subclassing) and \[specific-array-subtyping\](\#specific-array-subtyping).

## Interoperability examples

### Example: Pandas `Series` objects

Consider the following:

> \>\>\> import pandas as pd \>\>\> ser = pd.Series(\[1, 2, 3, 4\]) \>\>\> type(ser) pandas.core.series.Series

Now, `ser` is **not** a ndarray, but because it [implements the \_\_array\_ufunc\_\_ protocol](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe-interoperability-with-numpy-functions), we can apply ufuncs to it as if it were a ndarray:

>   - \>\>\> np.exp(ser)  
>     0 2.718282 1 7.389056 2 20.085537 3 54.598150 dtype: float64
> 
>   - \>\>\> np.sin(ser)  
>     0 0.841471 1 0.909297 2 0.141120 3 -0.756802 dtype: float64

We can even do operations with other ndarrays:

>   - \>\>\> np.add(ser, np.array(\[5, 6, 7, 8\]))  
>     0 6 1 8 2 10 3 12 dtype: int64
> 
> \>\>\> f(ser) 21.1977562209304 \>\>\> result = ser.\_\_array\_\_() \>\>\> type(result) numpy.ndarray

### Example: PyTorch tensors

[PyTorch](https://pytorch.org/) is an optimized tensor library for deep learning using GPUs and CPUs. PyTorch arrays are commonly called *tensors*. Tensors are similar to NumPy's ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data.

> \>\>\> import torch \>\>\> data = \[\[1, 2\],\[3, 4\]\] \>\>\> x\_np = np.array(data) \>\>\> x\_tensor = torch.tensor(data)

Note that `x_np` and `x_tensor` are different kinds of objects:

> \>\>\> x\_np array(\[\[1, 2\], \[3, 4\]\]) \>\>\> x\_tensor tensor(\[\[1, 2\], \[3, 4\]\])

However, we can treat PyTorch tensors as NumPy arrays without the need for explicit conversion:

> \>\>\> np.exp(x\_tensor) tensor(\[\[ 2.7183, 7.3891\], \[20.0855, 54.5982\]\], dtype=torch.float64)

Also, note that the return type of this function is compatible with the initial data type.

<div class="admonition">

Warning

While this mixing of ndarrays and tensors may be convenient, it is not recommended. It will not work for non-CPU tensors, and will have unexpected behavior in corner cases. Users should prefer explicitly converting the ndarray to a tensor.

</div>

\> **Note** \> PyTorch does not implement `__array_function__` or `__array_ufunc__`. Under the hood, the `Tensor.__array__()` method returns a NumPy ndarray as a view of the tensor data buffer. See [this issue](https://github.com/pytorch/pytorch/issues/24015) and the [\_\_torch\_function\_\_ implementation](https://github.com/pytorch/pytorch/blob/master/torch/overrides.py) for details.

Note also that we can see `__array_wrap__` in action here, even though `torch.Tensor` is not a subclass of ndarray:

    >>> import torch
    >>> t = torch.arange(4)
    >>> np.abs(t)
    tensor([0, 1, 2, 3])

PyTorch implements `__array_wrap__` to be able to get tensors back from NumPy functions, and we can modify it directly to control which type of objects are returned from these functions.

### Example: CuPy arrays

CuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python. CuPy implements a subset of the NumPy interface by implementing `cupy.ndarray`, [a counterpart to NumPy ndarrays](https://docs.cupy.dev/en/stable/reference/ndarray.html).

> \>\>\> import cupy as cp \>\>\> x\_gpu = cp.array(\[1, 2, 3, 4\])

The `cupy.ndarray` object implements the `__array_ufunc__` interface. This enables NumPy ufuncs to be applied to CuPy arrays (this will defer operation to the matching CuPy CUDA/ROCm implementation of the ufunc):

> \>\>\> np.mean(np.exp(x\_gpu)) array(21.19775622)

Note that the return type of these operations is still consistent with the initial type:

> \>\>\> arr = cp.random.randn(1, 2, 3, 4).astype(cp.float32) \>\>\> result = np.sum(arr) \>\>\> print(type(result)) \<class 'cupy.\_core.core.ndarray'\>

See [this page in the CuPy documentation for details](https://docs.cupy.dev/en/stable/reference/ufunc.html).

`cupy.ndarray` also implements the `__array_function__` interface, meaning it is possible to do operations such as

> \>\>\> a = np.random.randn(100, 100) \>\>\> a\_gpu = cp.asarray(a) \>\>\> qr\_gpu = np.linalg.qr(a\_gpu)

CuPy implements many NumPy functions on `cupy.ndarray` objects, but not all. See [the CuPy documentation](https://docs.cupy.dev/en/stable/user_guide/difference.html) for details.

### Example: Dask arrays

Dask is a flexible library for parallel computing in Python. Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This allows computations on larger-than-memory arrays using multiple cores.

Dask supports `__array__()` and `__array_ufunc__`.

> \>\>\> import dask.array as da \>\>\> x = da.random.normal(1, 0.1, size=(20, 20), chunks=(10, 10)) \>\>\> np.mean(np.exp(x)) dask.array\<mean\_agg-aggregate, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray\> \>\>\> np.mean(np.exp(x)).compute() 5.090097550553843

\> **Note** \> Dask is lazily evaluated, and the result from a computation isn't computed until you ask for it by invoking `compute()`.

See [the Dask array documentation](https://docs.dask.org/en/stable/array.html) and the [scope of Dask arrays interoperability with NumPy arrays](https://docs.dask.org/en/stable/array.html#scope) for details.

### Example: DLPack

Several Python data science libraries implement the `__dlpack__` protocol. Among them are [PyTorch](https://pytorch.org/) and [CuPy](https://cupy.dev/). A full list of libraries that implement this protocol can be found on \[this page of DLPack documentation \<dlpack:index\>\](this page of DLPack documentation \<dlpack:index\>.md).

Convert a PyTorch CPU tensor to NumPy array:

> \>\>\> import torch \>\>\> x\_torch = torch.arange(5) \>\>\> x\_torch tensor(\[0, 1, 2, 3, 4\]) \>\>\> x\_np = np.from\_dlpack(x\_torch) \>\>\> x\_np array(\[0, 1, 2, 3, 4\]) \>\>\> \# note that x\_np is a view of x\_torch \>\>\> x\_torch\[1\] = 100 \>\>\> x\_torch tensor(\[ 0, 100, 2, 3, 4\]) \>\>\> x\_np array(\[ 0, 100, 2, 3, 4\])

The imported arrays are read-only so writing or operating in-place will fail:

> \>\>\> x.flags.writeable False \>\>\> x\_np\[1\] = 1 Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> ValueError: assignment destination is read-only

A copy must be created in order to operate on the imported arrays in-place, but will mean duplicating the memory. Do not do this for very large arrays:

> \>\>\> x\_np\_copy = x\_np.copy() \>\>\> x\_np\_copy.sort() \# works

\> **Note** \> Note that GPU tensors can't be converted to NumPy arrays since NumPy doesn't support GPU devices:

> \>\>\> x\_torch = torch.arange(5, device='cuda') \>\>\> np.from\_dlpack(x\_torch) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> RuntimeError: Unsupported device in DLTensor.
> 
> But, if both libraries support the device the data buffer is on, it is possible to use the `__dlpack__` protocol (e.g. [PyTorch](https://pytorch.org/) and [CuPy](https://cupy.dev/)):
> 
> \>\>\> x\_torch = torch.arange(5, device='cuda') \>\>\> x\_cupy = cupy.from\_dlpack(x\_torch)

Similarly, a NumPy array can be converted to a PyTorch tensor:

> \>\>\> x\_np = np.arange(5) \>\>\> x\_torch = torch.from\_dlpack(x\_np)

Read-only arrays cannot be exported:

> \>\>\> x\_np = np.arange(5) \>\>\> x\_np.flags.writeable = False \>\>\> torch.from\_dlpack(x\_np) \# doctest: +ELLIPSIS Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> File ".../site-packages/torch/utils/dlpack.py", line 63, in from\_dlpack dlpack = ext\_tensor.\_\_dlpack\_\_() TypeError: NumPy currently only supports dlpack for writeable arrays

## Further reading

  - \[arrays.interface\](\#arrays.interface)
  - \[basics.dispatch\](\#basics.dispatch)
  - \[special-attributes-and-methods\](\#special-attributes-and-methods) (details on the `__array_ufunc__` and `__array_function__` protocols)
  - \[basics.subclassing\](\#basics.subclassing) (details on the `__array_wrap__` and `__array_finalize__` methods)
  - \[specific-array-subtyping\](\#specific-array-subtyping) (more details on the implementation of `__array_finalize__`, `__array_wrap__` and `__array_priority__`)
  - \[NumPy roadmap: interoperability \<neps:roadmap\>\](NumPy roadmap: interoperability \<neps:roadmap\>.md)
  - [PyTorch documentation on the Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)

---

basics.io.genfromtxt.md

---

<div class="sectionauthor">

Pierre Gerard-Marchant \<<pierregmcode@gmail.com>\>

</div>

# Importing data with <span class="title-ref">\~numpy.genfromtxt</span>

NumPy provides several functions to create arrays from tabular data. We focus here on the <span class="title-ref">\~numpy.genfromtxt</span> function.

In a nutshell, <span class="title-ref">\~numpy.genfromtxt</span> runs two main loops. The first loop converts each line of the file in a sequence of strings. The second loop converts each string to the appropriate data type. This mechanism is slower than a single loop, but gives more flexibility. In particular, <span class="title-ref">\~numpy.genfromtxt</span> is able to take missing data into account, when other faster and simpler functions like <span class="title-ref">\~numpy.loadtxt</span> cannot.

\> **Note** \> When giving examples, we will use the following conventions:

    >>> import numpy as np
    >>> from io import StringIO

## Defining the input

The only mandatory argument of <span class="title-ref">\~numpy.genfromtxt</span> is the source of the data. It can be a string, a list of strings, a generator or an open file-like object with a `read` method, for example, a file or <span class="title-ref">io.StringIO</span> object. If a single string is provided, it is assumed to be the name of a local or remote file. If a list of strings or a generator returning strings is provided, each string is treated as one line in a file. When the URL of a remote file is passed, the file is automatically downloaded to the current directory and opened.

Recognized file types are text files and archives. Currently, the function recognizes `gzip` and `bz2` (`bzip2`) archives. The type of the archive is determined from the extension of the file: if the filename ends with `'.gz'`, a `gzip` archive is expected; if it ends with `'bz2'`, a `bzip2` archive is assumed.

## Splitting the lines into columns

### The `delimiter` argument

Once the file is defined and open for reading, <span class="title-ref">\~numpy.genfromtxt</span> splits each non-empty line into a sequence of strings. Empty or commented lines are just skipped. The `delimiter` keyword is used to define how the splitting should take place.

Quite often, a single character marks the separation between columns. For example, comma-separated files (CSV) use a comma (`,`) or a semicolon (`;`) as delimiter:

    >>> data = "1, 2, 3\n4, 5, 6"
    >>> np.genfromtxt(StringIO(data), delimiter=",")
    array([[1.,  2.,  3.],
           [4.,  5.,  6.]])

Another common separator is `"\t"`, the tabulation character. However, we are not limited to a single character, any string will do. By default, <span class="title-ref">\~numpy.genfromtxt</span> assumes `delimiter=None`, meaning that the line is split along white spaces (including tabs) and that consecutive white spaces are considered as a single white space.

Alternatively, we may be dealing with a fixed-width file, where columns are defined as a given number of characters. In that case, we need to set `delimiter` to a single integer (if all the columns have the same size) or to a sequence of integers (if columns can have different sizes):

    >>> data = "  1  2  3\n  4  5 67\n890123  4"
    >>> np.genfromtxt(StringIO(data), delimiter=3)
    array([[  1.,    2.,    3.],
           [  4.,    5.,   67.],
           [890.,  123.,    4.]])
    >>> data = "123456789\n   4  7 9\n   4567 9"
    >>> np.genfromtxt(StringIO(data), delimiter=(4, 3, 2))
    array([[1234.,   567.,    89.],
           [   4.,     7.,     9.],
           [   4.,   567.,     9.]])

### The `autostrip` argument

By default, when a line is decomposed into a series of strings, the individual entries are not stripped of leading nor trailing white spaces. This behavior can be overwritten by setting the optional argument `autostrip` to a value of `True`:

    >>> data = "1, abc , 2\n 3, xxx, 4"
    >>> # Without autostrip
    >>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|U5")
    array([['1', ' abc ', ' 2'],
           ['3', ' xxx', ' 4']], dtype='<U5')
    >>> # With autostrip
    >>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|U5", autostrip=True)
    array([['1', 'abc', '2'],
           ['3', 'xxx', '4']], dtype='<U5')

### The `comments` argument

The optional argument `comments` is used to define a character string that marks the beginning of a comment. By default, <span class="title-ref">\~numpy.genfromtxt</span> assumes `comments='#'`. The comment marker may occur anywhere on the line. Any character present after the comment marker(s) is simply ignored:

    >>> data = """#
    ... # Skip me !
    ... # Skip me too !
    ... 1, 2
    ... 3, 4
    ... 5, 6 #This is the third line of the data
    ... 7, 8
    ... # And here comes the last line
    ... 9, 0
    ... """
    >>> np.genfromtxt(StringIO(data), comments="#", delimiter=",")
    array([[1., 2.],
           [3., 4.],
           [5., 6.],
           [7., 8.],
           [9., 0.]])

\> **Note** \> There is one notable exception to this behavior: if the optional argument `names=True`, the first commented line will be examined for names.

## Skipping lines and choosing columns

### The `skip_header` and `skip_footer` arguments

The presence of a header in the file can hinder data processing. In that case, we need to use the `skip_header` optional argument. The values of this argument must be an integer which corresponds to the number of lines to skip at the beginning of the file, before any other action is performed. Similarly, we can skip the last `n` lines of the file by using the `skip_footer` attribute and giving it a value of `n`:

    >>> data = "\n".join(str(i) for i in range(10))
    >>> np.genfromtxt(StringIO(data),)
    array([0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])
    >>> np.genfromtxt(StringIO(data),
    ...               skip_header=3, skip_footer=5)
    array([3.,  4.])

By default, `skip_header=0` and `skip_footer=0`, meaning that no lines are skipped.

### The `usecols` argument

In some cases, we are not interested in all the columns of the data but only a few of them. We can select which columns to import with the `usecols` argument. This argument accepts a single integer or a sequence of integers corresponding to the indices of the columns to import. Remember that by convention, the first column has an index of 0. Negative integers behave the same as regular Python negative indexes.

For example, if we want to import only the first and the last columns, we can use `usecols=(0, -1)`:

    >>> data = "1 2 3\n4 5 6"
    >>> np.genfromtxt(StringIO(data), usecols=(0, -1))
    array([[1.,  3.],
           [4.,  6.]])

If the columns have names, we can also select which columns to import by giving their name to the `usecols` argument, either as a sequence of strings or a comma-separated string:

    >>> data = "1 2 3\n4 5 6"
    >>> np.genfromtxt(StringIO(data),
    ...               names="a, b, c", usecols=("a", "c"))
    array([(1., 3.), (4., 6.)], dtype=[('a', '<f8'), ('c', '<f8')])
    >>> np.genfromtxt(StringIO(data),
    ...               names="a, b, c", usecols=("a, c"))
        array([(1., 3.), (4., 6.)], dtype=[('a', '<f8'), ('c', '<f8')])

## Choosing the data type

The main way to control how the sequences of strings we have read from the file are converted to other types is to set the `dtype` argument. Acceptable values for this argument are:

  - a single type, such as `dtype=float`. The output will be 2D with the given dtype, unless a name has been associated with each column with the use of the `names` argument (see below). Note that `dtype=float` is the default for <span class="title-ref">\~numpy.genfromtxt</span>.
  - a sequence of types, such as `dtype=(int, float, float)`.
  - a comma-separated string, such as `dtype="i4,f8,|U3"`.
  - a dictionary with two keys `'names'` and `'formats'`.
  - a sequence of tuples `(name, type)`, such as `dtype=[('A', int), ('B', float)]`.
  - an existing <span class="title-ref">numpy.dtype</span> object.
  - the special value `None`. In that case, the type of the columns will be determined from the data itself (see below).

In all the cases but the first one, the output will be a 1D array with a structured dtype. This dtype has as many fields as items in the sequence. The field names are defined with the `names` keyword.

When `dtype=None`, the type of each column is determined iteratively from its data. We start by checking whether a string can be converted to a boolean (that is, if the string matches `true` or `false` in lower cases); then whether it can be converted to an integer, then to a float, then to a complex and eventually to a string.

The option `dtype=None` is provided for convenience. However, it is significantly slower than setting the dtype explicitly.

## Setting the names

### The `names` argument

A natural approach when dealing with tabular data is to allocate a name to each column. A first possibility is to use an explicit structured dtype, as mentioned previously:

    >>> data = StringIO("1 2 3\n 4 5 6")
    >>> np.genfromtxt(data, dtype=[(_, int) for _ in "abc"])
    array([(1, 2, 3), (4, 5, 6)],
          dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])

Another simpler possibility is to use the `names` keyword with a sequence of strings or a comma-separated string:

    >>> data = StringIO("1 2 3\n 4 5 6")
    >>> np.genfromtxt(data, names="A, B, C")
    array([(1., 2., 3.), (4., 5., 6.)],
          dtype=[('A', '<f8'), ('B', '<f8'), ('C', '<f8')])

In the example above, we used the fact that by default, `dtype=float`. By giving a sequence of names, we are forcing the output to a structured dtype.

We may sometimes need to define the column names from the data itself. In that case, we must use the `names` keyword with a value of `True`. The names will then be read from the first line (after the `skip_header` ones), even if the line is commented out:

    >>> data = StringIO("So it goes\n#a b c\n1 2 3\n 4 5 6")
    >>> np.genfromtxt(data, skip_header=1, names=True)
    array([(1., 2., 3.), (4., 5., 6.)],
          dtype=[('a', '<f8'), ('b', '<f8'), ('c', '<f8')])

The default value of `names` is `None`. If we give any other value to the keyword, the new names will overwrite the field names we may have defined with the dtype:

    >>> data = StringIO("1 2 3\n 4 5 6")
    >>> ndtype=[('a',int), ('b', float), ('c', int)]
    >>> names = ["A", "B", "C"]
    >>> np.genfromtxt(data, names=names, dtype=ndtype)
    array([(1, 2., 3), (4, 5., 6)],
          dtype=[('A', '<i8'), ('B', '<f8'), ('C', '<i8')])

### The `defaultfmt` argument

If `names=None` but a structured dtype is expected, names are defined with the standard NumPy default of `"f%i"`, yielding names like `f0`, `f1` and so forth:

    >>> data = StringIO("1 2 3\n 4 5 6")
    >>> np.genfromtxt(data, dtype=(int, float, int))
    array([(1, 2., 3), (4, 5., 6)],
          dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<i8')])

In the same way, if we don't give enough names to match the length of the dtype, the missing names will be defined with this default template:

    >>> data = StringIO("1 2 3\n 4 5 6")
    >>> np.genfromtxt(data, dtype=(int, float, int), names="a")
    array([(1, 2., 3), (4, 5., 6)],
          dtype=[('a', '<i8'), ('f0', '<f8'), ('f1', '<i8')])

We can overwrite this default with the `defaultfmt` argument, that takes any format string:

    >>> data = StringIO("1 2 3\n 4 5 6")
    >>> np.genfromtxt(data, dtype=(int, float, int), defaultfmt="var_%02i")
    array([(1, 2., 3), (4, 5., 6)],
          dtype=[('var_00', '<i8'), ('var_01', '<f8'), ('var_02', '<i8')])

\> **Note** \> We need to keep in mind that `defaultfmt` is used only if some names are expected but not defined.

### Validating names

NumPy arrays with a structured dtype can also be viewed as <span class="title-ref">\~numpy.recarray</span>, where a field can be accessed as if it were an attribute. For that reason, we may need to make sure that the field name doesn't contain any space or invalid character, or that it does not correspond to the name of a standard attribute (like `size` or `shape`), which would confuse the interpreter. <span class="title-ref">\~numpy.genfromtxt</span> accepts three optional arguments that provide a finer control on the names:

  - `deletechars`  
    Gives a string combining all the characters that must be deleted from the name. By default, invalid characters are `~!@#$%^&*()-=+~\|]}[{';: /?.>,<`.

  - `excludelist`  
    Gives a list of the names to exclude, such as `return`, `file`, `print`... If one of the input name is part of this list, an underscore character (`'_'`) will be appended to it.

  - `case_sensitive`  
    Whether the names should be case-sensitive (`case_sensitive=True`), converted to upper case (`case_sensitive=False` or `case_sensitive='upper'`) or to lower case (`case_sensitive='lower'`).

## Tweaking the conversion

### The `converters` argument

Usually, defining a dtype is sufficient to define how the sequence of strings must be converted. However, some additional control may sometimes be required. For example, we may want to make sure that a date in a format `YYYY/MM/DD` is converted to a <span class="title-ref">\~datetime.datetime</span> object, or that a string like `xx%` is properly converted to a float between 0 and 1. In such cases, we should define conversion functions with the `converters` arguments.

The value of this argument is typically a dictionary with column indices or column names as keys and a conversion functions as values. These conversion functions can either be actual functions or lambda functions. In any case, they should accept only a string as input and output only a single element of the wanted type.

In the following example, the second column is converted from as string representing a percentage to a float between 0 and 1:

    >>> convertfunc = lambda x: float(x.strip("%"))/100.
    >>> data = "1, 2.3%, 45.\n6, 78.9%, 0"
    >>> names = ("i", "p", "n")
    >>> # General case .....
    >>> np.genfromtxt(StringIO(data), delimiter=",", names=names)
    array([(1., nan, 45.), (6., nan, 0.)],
          dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])

We need to keep in mind that by default, `dtype=float`. A float is therefore expected for the second column. However, the strings `' 2.3%'` and `' 78.9%'` cannot be converted to float and we end up having `np.nan` instead. Let's now use a converter:

    >>> # Converted case ...
    >>> np.genfromtxt(StringIO(data), delimiter=",", names=names,
    ...               converters={1: convertfunc})
    array([(1., 0.023, 45.), (6., 0.789, 0.)],
          dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])

The same results can be obtained by using the name of the second column (`"p"`) as key instead of its index (1):

    >>> # Using a name for the converter ...
    >>> np.genfromtxt(StringIO(data), delimiter=",", names=names,
    ...               converters={"p": convertfunc})
    array([(1., 0.023, 45.), (6., 0.789, 0.)],
          dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])

Converters can also be used to provide a default for missing entries. In the following example, the converter `convert` transforms a stripped string into the corresponding float or into -999 if the string is empty. We need to explicitly strip the string from white spaces as it is not done by default:

    >>> data = "1, , 3\n 4, 5, 6"
    >>> convert = lambda x: float(x.strip() or -999)
    >>> np.genfromtxt(StringIO(data), delimiter=",",
    ...               converters={1: convert})
    array([[   1., -999.,    3.],
           [   4.,    5.,    6.]])

### Using missing and filling values

Some entries may be missing in the dataset we are trying to import. In a previous example, we used a converter to transform an empty string into a float. However, user-defined converters may rapidly become cumbersome to manage.

The <span class="title-ref">\~numpy.genfromtxt</span> function provides two other complementary mechanisms: the `missing_values` argument is used to recognize missing data and a second argument, `filling_values`, is used to process these missing data.

### `missing_values`

By default, any empty string is marked as missing. We can also consider more complex strings, such as `"N/A"` or `"???"` to represent missing or invalid data. The `missing_values` argument accepts three kinds of values:

  - a string or a comma-separated string  
    This string will be used as the marker for missing data for all the columns

  - a sequence of strings  
    In that case, each item is associated to a column, in order.

  - a dictionary  
    Values of the dictionary are strings or sequence of strings. The corresponding keys can be column indices (integers) or column names (strings). In addition, the special key `None` can be used to define a default applicable to all columns.

### `filling_values`

We know how to recognize missing data, but we still need to provide a value for these missing entries. By default, this value is determined from the expected dtype according to this table:

| Expected type | Default     |
| ------------- | ----------- |
| `bool`        | `False`     |
| `int`         | `-1`        |
| `float`       | `np.nan`    |
| `complex`     | `np.nan+0j` |
| `string`      | `'???'`     |

We can get a finer control on the conversion of missing values with the `filling_values` optional argument. Like `missing_values`, this argument accepts different kind of values:

  - a single value  
    This will be the default for all columns

  - a sequence of values  
    Each entry will be the default for the corresponding column

  - a dictionary  
    Each key can be a column index or a column name, and the corresponding value should be a single object. We can use the special key `None` to define a default for all columns.

In the following example, we suppose that the missing values are flagged with `"N/A"` in the first column and by `"???"` in the third column. We wish to transform these missing values to 0 if they occur in the first and second column, and to -999 if they occur in the last column:

    >>> data = "N/A, 2, 3\n4, ,???"
    >>> kwargs = dict(delimiter=",",
    ...               dtype=int,
    ...               names="a,b,c",
    ...               missing_values={0:"N/A", 'b':" ", 2:"???"},
    ...               filling_values={0:0, 'b':0, 2:-999})
    >>> np.genfromtxt(StringIO(data), **kwargs)
    array([(0, 2, 3), (4, 0, -999)],
          dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])

### `usemask`

We may also want to keep track of the occurrence of missing data by constructing a boolean mask, with `True` entries where data was missing and `False` otherwise. To do that, we just have to set the optional argument `usemask` to `True` (the default is `False`). The output array will then be a <span class="title-ref">\~numpy.ma.MaskedArray</span>.

---

basics.io.md

---

# I/O with NumPy

<div class="toctree" data-maxdepth="2">

basics.io.genfromtxt

</div>

---

basics.md

---

# NumPy fundamentals

These documents clarify concepts, design decisions, and technical constraints in NumPy. This is a great place to understand the fundamental NumPy ideas and philosophy.

<div class="toctree" data-maxdepth="1">

basics.creation basics.indexing basics.io basics.types basics.broadcasting basics.copies basics.strings basics.rec basics.ufuncs

</div>

---

basics.rec.md

---

# Structured arrays

## Introduction

Structured arrays are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named `fields <field>`. For example, :

    >>> x = np.array([('Rex', 9, 81.0), ('Fido', 3, 27.0)],
    ...              dtype=[('name', 'U10'), ('age', 'i4'), ('weight', 'f4')])
    >>> x
    array([('Rex', 9, 81.), ('Fido', 3, 27.)],
          dtype=[('name', '<U10'), ('age', '<i4'), ('weight', '<f4')])

Here `x` is a one-dimensional array of length two whose datatype is a structure with three fields: 1. A string of length 10 or less named 'name', 2. a 32-bit integer named 'age', and 3. a 32-bit float named 'weight'.

If you index `x` at position 1 you get a structure:

    >>> x[1]
    np.void(('Fido', 3, 27.0), dtype=[('name', '<U10'), ('age', '<i4'), ('weight', '<f4')])

You can access and modify individual fields of a structured array by indexing with the field name:

    >>> x['age']
    array([9, 3], dtype=int32)
    >>> x['age'] = 5
    >>> x
    array([('Rex', 5, 81.), ('Fido', 5, 27.)],
          dtype=[('name', '<U10'), ('age', '<i4'), ('weight', '<f4')])

Structured datatypes are designed to be able to mimic 'structs' in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.

Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison.

## Structured datatypes

A structured datatype can be thought of as a sequence of bytes of a certain length (the structure's `itemsize`) which is interpreted as a collection of fields. Each field has a name, a datatype, and a byte offset within the structure. The datatype of a field may be any numpy datatype including other structured datatypes, and it may also be a `subarray data type` which behaves like an ndarray of a specified shape. The offsets of the fields are arbitrary, and fields may even overlap. These offsets are usually determined automatically by numpy, but can also be specified.

### Structured datatype creation

Structured datatypes may be created using the function <span class="title-ref">numpy.dtype</span>. There are 4 alternative forms of specification which vary in flexibility and conciseness. These are further documented in the \[Data Type Objects \<arrays.dtypes.constructing\>\](\#data-type-objects-\<arrays.dtypes.constructing\>) reference page, and in summary they are:

1.  A list of tuples, one tuple per field
    
    Each tuple has the form `(fieldname, datatype, shape)` where shape is optional. `fieldname` is a string (or tuple if titles are used, see \[Field Titles \<titles\>\](\#field-titles-\<titles\>) below), `datatype` may be any object convertible to a datatype, and `shape` is a tuple of integers specifying subarray shape.
    
    > \>\>\> np.dtype(\[('x', 'f4'), ('y', np.float32), ('z', 'f4', (2, 2))\]) dtype(\[('x', '\<f4'), ('y', '\<f4'), ('z', '\<f4', (2, 2))\])
    
    If `fieldname` is the empty string `''`, the field will be given a default name of the form `f#`, where `#` is the integer index of the field, counting from 0 from the left:
    
        >>> np.dtype([('x', 'f4'), ('', 'i4'), ('z', 'i8')])
        dtype([('x', '<f4'), ('f1', '<i4'), ('z', '<i8')])
    
    The byte offsets of the fields within the structure and the total structure itemsize are determined automatically.

2\. A string of comma-separated dtype specifications

> In this shorthand notation any of the \[string dtype specifications \<arrays.dtypes.constructing\>\](\#string-dtype-specifications

  - \-----\<arrays.dtypes.constructing\>) may be used in a string and separated by  
    commas. The itemsize and byte offsets of the fields are determined automatically, and the field names are given the default names `f0`, `f1`, etc. :
    
        >>> np.dtype('i8, f4, S3')
        dtype([('f0', '<i8'), ('f1', '<f4'), ('f2', 'S3')])
        >>> np.dtype('3int8, float32, (2, 3)float64')
        dtype([('f0', 'i1', (3,)), ('f1', '<f4'), ('f2', '<f8', (2, 3))])

<!-- end list -->

3.  A dictionary of field parameter arrays
    
    This is the most flexible form of specification since it allows control over the byte-offsets of the fields and the itemsize of the structure.
    
    The dictionary has two required keys, 'names' and 'formats', and four optional keys, 'offsets', 'itemsize', 'aligned' and 'titles'. The values for 'names' and 'formats' should respectively be a list of field names and a list of dtype specifications, of the same length. The optional 'offsets' value should be a list of integer byte-offsets, one for each field within the structure. If 'offsets' is not given the offsets are determined automatically. The optional 'itemsize' value should be an integer describing the total size in bytes of the dtype, which must be large enough to contain all the fields. :
    
        >>> np.dtype({'names': ['col1', 'col2'], 'formats': ['i4', 'f4']})
        dtype([('col1', '<i4'), ('col2', '<f4')])
        >>> np.dtype({'names': ['col1', 'col2'],
        ...           'formats': ['i4', 'f4'],
        ...           'offsets': [0, 4],
        ...           'itemsize': 12})
        dtype({'names': ['col1', 'col2'], 'formats': ['<i4', '<f4'], 'offsets': [0, 4], 'itemsize': 12})
    
    Offsets may be chosen such that the fields overlap, though this will mean that assigning to one field may clobber any overlapping field's data. As an exception, fields of <span class="title-ref">numpy.object\_</span> type cannot overlap with other fields, because of the risk of clobbering the internal object pointer and then dereferencing it.
    
    The optional 'aligned' value can be set to `True` to make the automatic offset computation use aligned offsets (see \[offsets-and-alignment\](\#offsets-and-alignment)), as if the 'align' keyword argument of <span class="title-ref">numpy.dtype</span> had been set to True.
    
    The optional 'titles' value should be a list of titles of the same length as 'names', see \[Field Titles \<titles\>\](\#field-titles-\<titles\>) below.

4.  A dictionary of field names
    
    The keys of the dictionary are the field names and the values are tuples specifying type and offset:
    
        >>> np.dtype({'col1': ('i1', 0), 'col2': ('f4', 1)})
        dtype([('col1', 'i1'), ('col2', '<f4')])
    
    This form was discouraged because Python dictionaries did not preserve order in Python versions before Python 3.6. \[Field Titles \<titles\>\](\#field-titles-\<titles\>) may be specified by using a 3-tuple, see below.

### Manipulating and displaying structured datatypes

The list of field names of a structured datatype can be found in the `names` attribute of the dtype object:

    >>> d = np.dtype([('x', 'i8'), ('y', 'f4')])
    >>> d.names
    ('x', 'y')

The dtype of each individual field can be looked up by name:

    >>> d['x']
    dtype('int64')

The field names may be modified by assigning to the `names` attribute using a sequence of strings of the same length.

The dtype object also has a dictionary-like attribute, `fields`, whose keys are the field names (and \[Field Titles \<titles\>\](\#field-titles-\<titles\>), see below) and whose values are tuples containing the dtype and byte offset of each field. :

    >>> d.fields
    mappingproxy({'x': (dtype('int64'), 0), 'y': (dtype('float32'), 8)})

Both the `names` and `fields` attributes will equal `None` for unstructured arrays. The recommended way to test if a dtype is structured is with <span class="title-ref">if dt.names is not None</span> rather than <span class="title-ref">if dt.names</span>, to account for dtypes with 0 fields.

The string representation of a structured datatype is shown in the "list of tuples" form if possible, otherwise numpy falls back to using the more general dictionary form.

### Automatic byte offsets and alignment

Numpy uses one of two methods to automatically determine the field byte offsets and the overall itemsize of a structured datatype, depending on whether `align=True` was specified as a keyword argument to <span class="title-ref">numpy.dtype</span>.

By default (`align=False`), numpy will pack the fields together such that each field starts at the byte offset the previous field ended, and the fields are contiguous in memory. :

    >>> def print_offsets(d):
    ...     print("offsets:", [d.fields[name][1] for name in d.names])
    ...     print("itemsize:", d.itemsize)
    >>> print_offsets(np.dtype('u1, u1, i4, u1, i8, u2'))
    offsets: [0, 1, 2, 6, 7, 15]
    itemsize: 17

If `align=True` is set, numpy will pad the structure in the same way many C compilers would pad a C-struct. Aligned structures can give a performance improvement in some cases, at the cost of increased datatype size. Padding bytes are inserted between fields such that each field's byte offset will be a multiple of that field's alignment, which is usually equal to the field's size in bytes for simple datatypes, see :c`PyArray_Descr.alignment`. The structure will also have trailing padding added so that its itemsize is a multiple of the largest field's alignment. :

    >>> print_offsets(np.dtype('u1, u1, i4, u1, i8, u2', align=True))
    offsets: [0, 1, 4, 8, 16, 24]
    itemsize: 32

Note that although almost all modern C compilers pad in this way by default, padding in C structs is C-implementation-dependent so this memory layout is not guaranteed to exactly match that of a corresponding struct in a C program. Some work may be needed, either on the numpy side or the C side, to obtain exact correspondence.

If offsets were specified using the optional `offsets` key in the dictionary-based dtype specification, setting `align=True` will check that each field's offset is a multiple of its size and that the itemsize is a multiple of the largest field size, and raise an exception if not.

If the offsets of the fields and itemsize of a structured array satisfy the alignment conditions, the array will have the `ALIGNED` <span class="title-ref">flag \<numpy.ndarray.flags\></span> set.

A convenience function <span class="title-ref">numpy.lib.recfunctions.repack\_fields</span> converts an aligned dtype or array to a packed one and vice versa. It takes either a dtype or structured ndarray as an argument, and returns a copy with fields re-packed, with or without padding bytes.

### Field titles

In addition to field names, fields may also have an associated `title`, an alternate name, which is sometimes used as an additional description or alias for the field. The title may be used to index an array, just like a field name.

To add titles when using the list-of-tuples form of dtype specification, the field name may be specified as a tuple of two strings instead of a single string, which will be the field's title and field name respectively. For example:

    >>> np.dtype([(('my title', 'name'), 'f4')])
    dtype([(('my title', 'name'), '<f4')])

When using the first form of dictionary-based specification, the titles may be supplied as an extra `'titles'` key as described above. When using the second (discouraged) dictionary-based specification, the title can be supplied by providing a 3-element tuple `(datatype, offset, title)` instead of the usual 2-element tuple:

    >>> np.dtype({'name': ('i4', 0, 'my title')})
    dtype([(('my title', 'name'), '<i4')])

The `dtype.fields` dictionary will contain titles as keys, if any titles are used. This means effectively that a field with a title will be represented twice in the fields dictionary. The tuple values for these fields will also have a third element, the field title. Because of this, and because the `names` attribute preserves the field order while the `fields` attribute may not, it is recommended to iterate through the fields of a dtype using the `names` attribute of the dtype, which will not list titles, as in:

    >>> for name in d.names:
    ...     print(d.fields[name][:2])
    (dtype('int64'), 0)
    (dtype('float32'), 8)

### Union types

Structured datatypes are implemented in numpy to have base type <span class="title-ref">numpy.void</span> by default, but it is possible to interpret other numpy types as structured types using the `(base_dtype, dtype)` form of dtype specification described in \[Data Type Objects \<arrays.dtypes.constructing\>\](\#data-type-objects-\<arrays.dtypes.constructing\>). Here, `base_dtype` is the desired underlying dtype, and fields and flags will be copied from `dtype`. This dtype is similar to a 'union' in C.

## Indexing and assignment to structured arrays

### Assigning data to a structured array

There are a number of ways to assign values to a structured array: Using python tuples, using scalar values, or using other structured arrays.

#### Assignment from Python Native Types (Tuples)

The simplest way to assign values to a structured array is using python tuples. Each assigned value should be a tuple of length equal to the number of fields in the array, and not a list or array as these will trigger numpy's broadcasting rules. The tuple's elements are assigned to the successive fields of the array, from left to right:

    >>> x = np.array([(1, 2, 3), (4, 5, 6)], dtype='i8, f4, f8')
    >>> x[1] = (7, 8, 9)
    >>> x
    array([(1, 2., 3.), (7, 8., 9.)],
         dtype=[('f0', '<i8'), ('f1', '<f4'), ('f2', '<f8')])

#### Assignment from Scalars

A scalar assigned to a structured element will be assigned to all fields. This happens when a scalar is assigned to a structured array, or when an unstructured array is assigned to a structured array:

    >>> x = np.zeros(2, dtype='i8, f4, ?, S1')
    >>> x[:] = 3
    >>> x
    array([(3, 3., True, b'3'), (3, 3., True, b'3')],
          dtype=[('f0', '<i8'), ('f1', '<f4'), ('f2', '?'), ('f3', 'S1')])
    >>> x[:] = np.arange(2)
    >>> x
    array([(0, 0., False, b'0'), (1, 1., True, b'1')],
          dtype=[('f0', '<i8'), ('f1', '<f4'), ('f2', '?'), ('f3', 'S1')])

Structured arrays can also be assigned to unstructured arrays, but only if the structured datatype has just a single field:

    >>> twofield = np.zeros(2, dtype=[('A', 'i4'), ('B', 'i4')])
    >>> onefield = np.zeros(2, dtype=[('A', 'i4')])
    >>> nostruct = np.zeros(2, dtype='i4')
    >>> nostruct[:] = twofield
    Traceback (most recent call last):
    ...
    TypeError: Cannot cast array data from dtype([('A', '<i4'), ('B', '<i4')]) to dtype('int32') according to the rule 'unsafe'

#### Assignment from other Structured Arrays

Assignment between two structured arrays occurs as if the source elements had been converted to tuples and then assigned to the destination elements. That is, the first field of the source array is assigned to the first field of the destination array, and the second field likewise, and so on, regardless of field names. Structured arrays with a different number of fields cannot be assigned to each other. Bytes of the destination structure which are not included in any of the fields are unaffected. :

    >>> a = np.zeros(3, dtype=[('a', 'i8'), ('b', 'f4'), ('c', 'S3')])
    >>> b = np.ones(3, dtype=[('x', 'f4'), ('y', 'S3'), ('z', 'O')])
    >>> b[:] = a
    >>> b
    array([(0., b'0.0', b''), (0., b'0.0', b''), (0., b'0.0', b'')],
          dtype=[('x', '<f4'), ('y', 'S3'), ('z', 'O')])

#### Assignment involving subarrays

When assigning to fields which are subarrays, the assigned value will first be broadcast to the shape of the subarray.

### Indexing structured arrays

#### Accessing Individual Fields

Individual fields of a structured array may be accessed and modified by indexing the array with the field name. :

    >>> x = np.array([(1, 2), (3, 4)], dtype=[('foo', 'i8'), ('bar', 'f4')])
    >>> x['foo']
    array([1, 3])
    >>> x['foo'] = 10
    >>> x
    array([(10, 2.), (10, 4.)],
          dtype=[('foo', '<i8'), ('bar', '<f4')])

The resulting array is a view into the original array. It shares the same memory locations and writing to the view will modify the original array. :

    >>> y = x['bar']
    >>> y[:] = 11
    >>> x
    array([(10, 11.), (10, 11.)],
          dtype=[('foo', '<i8'), ('bar', '<f4')])

This view has the same dtype and itemsize as the indexed field, so it is typically a non-structured array, except in the case of nested structures.

> \>\>\> y.dtype, y.shape, y.strides (dtype('float32'), (2,), (12,))

If the accessed field is a subarray, the dimensions of the subarray are appended to the shape of the result:

    >>> x = np.zeros((2, 2), dtype=[('a', np.int32), ('b', np.float64, (3, 3))])
    >>> x['a'].shape
    (2, 2)
    >>> x['b'].shape
    (2, 2, 3, 3)

#### Accessing Multiple Fields

One can index and assign to a structured array with a multi-field index, where the index is a list of field names.

<div class="warning">

<div class="title">

Warning

</div>

The behavior of multi-field indexes changed from Numpy 1.15 to Numpy 1.16.

</div>

The result of indexing with a multi-field index is a view into the original array, as follows:

    >>> a = np.zeros(3, dtype=[('a', 'i4'), ('b', 'i4'), ('c', 'f4')])
    >>> a[['a', 'c']]
    array([(0, 0.), (0, 0.), (0, 0.)],
         dtype={'names': ['a', 'c'], 'formats': ['<i4', '<f4'], 'offsets': [0, 8], 'itemsize': 12})

Assignment to the view modifies the original array. The view's fields will be in the order they were indexed. Note that unlike for single-field indexing, the dtype of the view has the same itemsize as the original array, and has fields at the same offsets as in the original array, and unindexed fields are merely missing.

<div class="warning">

<div class="title">

Warning

</div>

In Numpy 1.15, indexing an array with a multi-field index returned a copy of the result above, but with fields packed together in memory as if passed through <span class="title-ref">numpy.lib.recfunctions.repack\_fields</span>.

The new behavior as of Numpy 1.16 leads to extra "padding" bytes at the location of unindexed fields compared to 1.15. You will need to update any code which depends on the data having a "packed" layout. For instance code such as:

    >>> a[['a', 'c']].view('i8')  # Fails in Numpy 1.16
    Traceback (most recent call last):
       File "<stdin>", line 1, in <module>
    ValueError: When changing to a smaller dtype, its size must be a divisor of the size of original dtype

will need to be changed. This code has raised a `FutureWarning` since Numpy 1.12, and similar code has raised `FutureWarning` since 1.7.

In 1.16 a number of functions have been introduced in the `numpy.lib.recfunctions` module to help users account for this change. These are <span class="title-ref">numpy.lib.recfunctions.repack\_fields</span>. <span class="title-ref">numpy.lib.recfunctions.structured\_to\_unstructured</span>, <span class="title-ref">numpy.lib.recfunctions.unstructured\_to\_structured</span>, <span class="title-ref">numpy.lib.recfunctions.apply\_along\_fields</span>, <span class="title-ref">numpy.lib.recfunctions.assign\_fields\_by\_name</span>, and <span class="title-ref">numpy.lib.recfunctions.require\_fields</span>.

The function <span class="title-ref">numpy.lib.recfunctions.repack\_fields</span> can always be used to reproduce the old behavior, as it will return a packed copy of the structured array. The code above, for example, can be replaced with:

> \>\>\> from numpy.lib.recfunctions import repack\_fields \>\>\> repack\_fields(a\[\['a', 'c'\]\]).view('i8') \# supported in 1.16 array(\[0, 0, 0\])

Furthermore, numpy now provides a new function <span class="title-ref">numpy.lib.recfunctions.structured\_to\_unstructured</span> which is a safer and more efficient alternative for users who wish to convert structured arrays to unstructured arrays, as the view above is often intended to do. This function allows safe conversion to an unstructured type taking into account padding, often avoids a copy, and also casts the datatypes as needed, unlike the view. Code such as:

> \>\>\> b = np.zeros(3, dtype=\[('x', 'f4'), ('y', 'f4'), ('z', 'f4')\]) \>\>\> b\[\['x', 'z'\]\].view('f4') array(\[0., 0., 0., 0., 0., 0., 0., 0., 0.\], dtype=float32)

can be made safer by replacing with:

> \>\>\> from numpy.lib.recfunctions import structured\_to\_unstructured \>\>\> structured\_to\_unstructured(b\[\['x', 'z'\]\]) array(\[\[0., 0.\], \[0., 0.\], \[0., 0.\]\], dtype=float32)

</div>

Assignment to an array with a multi-field index modifies the original array:

    >>> a[['a', 'c']] = (2, 3)
    >>> a
    array([(2, 0, 3.), (2, 0, 3.), (2, 0, 3.)],
          dtype=[('a', '<i4'), ('b', '<i4'), ('c', '<f4')])

This obeys the structured array assignment rules described above. For example, this means that one can swap the values of two fields using appropriate multi-field indexes:

    >>> a[['a', 'c']] = a[['c', 'a']]

#### Indexing with an Integer to get a Structured Scalar

Indexing a single element of a structured array (with an integer index) returns a structured scalar:

    >>> x = np.array([(1, 2., 3.)], dtype='i, f, f')
    >>> scalar = x[0]
    >>> scalar
    np.void((1, 2.0, 3.0), dtype=[('f0', '<i4'), ('f1', '<f4'), ('f2', '<f4')])
    >>> type(scalar)
    <class 'numpy.void'>

Unlike other numpy scalars, structured scalars are mutable and act like views into the original array, such that modifying the scalar will modify the original array. Structured scalars also support access and assignment by field name:

    >>> x = np.array([(1, 2), (3, 4)], dtype=[('foo', 'i8'), ('bar', 'f4')])
    >>> s = x[0]
    >>> s['bar'] = 100
    >>> x
    array([(1, 100.), (3, 4.)],
          dtype=[('foo', '<i8'), ('bar', '<f4')])

Similarly to tuples, structured scalars can also be indexed with an integer:

    >>> scalar = np.array([(1, 2., 3.)], dtype='i, f, f')[0]
    >>> scalar[0]
    np.int32(1)
    >>> scalar[1] = 4

Thus, tuples might be thought of as the native Python equivalent to numpy's structured types, much like native python integers are the equivalent to numpy's integer types. Structured scalars may be converted to a tuple by calling \`numpy.ndarray.item\`:

    >>> scalar.item(), type(scalar.item())
    ((1, 4.0, 3.0), <class 'tuple'>)

### Viewing structured arrays containing objects

In order to prevent clobbering object pointers in fields of <span class="title-ref">object</span> type, numpy currently does not allow views of structured arrays containing objects.

### Structure comparison and promotion

If the dtypes of two void structured arrays are equal, testing the equality of the arrays will result in a boolean array with the dimensions of the original arrays, with elements set to `True` where all fields of the corresponding structures are equal:

    >>> a = np.array([(1, 1), (2, 2)], dtype=[('a', 'i4'), ('b', 'i4')])
    >>> b = np.array([(1, 1), (2, 3)], dtype=[('a', 'i4'), ('b', 'i4')])
    >>> a == b
    array([True, False])

NumPy will promote individual field datatypes to perform the comparison. So the following is also valid (note the `'f4'` dtype for the `'a'` field):

> \>\>\> b = np.array(\[(1.0, 1), (2.5, 2)\], dtype=\[("a", "f4"), ("b", "i4")\]) \>\>\> a == b array(\[True, False\])

To compare two structured arrays, it must be possible to promote them to a common dtype as returned by <span class="title-ref">numpy.result\_type</span> and <span class="title-ref">numpy.promote\_types</span>. This enforces that the number of fields, the field names, and the field titles must match precisely. When promotion is not possible, for example due to mismatching field names, NumPy will raise an error. Promotion between two structured dtypes results in a canonical dtype that ensures native byte-order for all fields:

    >>> np.result_type(np.dtype("i,>i"))
    dtype([('f0', '<i4'), ('f1', '<i4')])
    >>> np.result_type(np.dtype("i,>i"), np.dtype("i,i"))
    dtype([('f0', '<i4'), ('f1', '<i4')])

The resulting dtype from promotion is also guaranteed to be packed, meaning that all fields are ordered contiguously and any unnecessary padding is removed:

    >>> dt = np.dtype("i1,V3,i4,V1")[["f0", "f2"]]
    >>> dt
    dtype({'names': ['f0', 'f2'], 'formats': ['i1', '<i4'], 'offsets': [0, 4], 'itemsize': 9})
    >>> np.result_type(dt)
    dtype([('f0', 'i1'), ('f2', '<i4')])

Note that the result prints without `offsets` or `itemsize` indicating no additional padding. If a structured dtype is created with `align=True` ensuring that `dtype.isalignedstruct` is true, this property is preserved:

    >>> dt = np.dtype("i1,V3,i4,V1", align=True)[["f0", "f2"]]
    >>> dt
    dtype({'names': ['f0', 'f2'], 'formats': ['i1', '<i4'], 'offsets': [0, 4], 'itemsize': 12}, align=True)
    
    >>> np.result_type(dt)
    dtype([('f0', 'i1'), ('f2', '<i4')], align=True)
    >>> np.result_type(dt).isalignedstruct
    True

When promoting multiple dtypes, the result is aligned if any of the inputs is:

    >>> np.result_type(np.dtype("i,i"), np.dtype("i,i", align=True))
    dtype([('f0', '<i4'), ('f1', '<i4')], align=True)

The `<` and `>` operators always return `False` when comparing void structured arrays, and arithmetic and bitwise operations are not supported.

<div class="versionchanged">

1.23 Before NumPy 1.23, a warning was given and `False` returned when promotion to a common dtype failed. Further, promotion was much more restrictive: It would reject the mixed float/integer comparison example above.

</div>

## Record arrays

As an optional convenience numpy provides an ndarray subclass, <span class="title-ref">numpy.recarray</span> that allows access to fields of structured arrays by attribute instead of only by index. Record arrays use a special datatype, <span class="title-ref">numpy.record</span>, that allows field access by attribute on the structured scalars obtained from the array. The `numpy.rec` module provides functions for creating recarrays from various objects. Additional helper functions for creating and manipulating structured arrays can be found in `numpy.lib.recfunctions`.

The simplest way to create a record array is with \`numpy.rec.array \<numpy.rec.array\>\`:

    >>> recordarr = np.rec.array([(1, 2., 'Hello'), (2, 3., "World")],
    ...                    dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'S10')])
    >>> recordarr.bar
    array([2., 3.], dtype=float32)
    >>> recordarr[1:2]
    rec.array([(2, 3., b'World')],
          dtype=[('foo', '<i4'), ('bar', '<f4'), ('baz', 'S10')])
    >>> recordarr[1:2].foo
    array([2], dtype=int32)
    >>> recordarr.foo[1:2]
    array([2], dtype=int32)
    >>> recordarr[1].baz
    b'World'

<span class="title-ref">numpy.rec.array \<numpy.rec.array\></span> can convert a wide variety of arguments into record arrays, including structured arrays:

    >>> arr = np.array([(1, 2., 'Hello'), (2, 3., "World")],
    ...             dtype=[('foo', 'i4'), ('bar', 'f4'), ('baz', 'S10')])
    >>> recordarr = np.rec.array(arr)

The `numpy.rec` module provides a number of other convenience functions for creating record arrays, see \[record array creation routines \<routines.array-creation.rec\>\](\#record-array-creation-routines \<routines.array-creation.rec\>).

A record array representation of a structured array can be obtained using the appropriate \`view \<numpy.ndarray.view\>\`:

    >>> arr = np.array([(1, 2., 'Hello'), (2, 3., "World")],
    ...                dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'S10')])
    >>> recordarr = arr.view(dtype=np.dtype((np.record, arr.dtype)),
    ...                      type=np.recarray)

For convenience, viewing an ndarray as type <span class="title-ref">numpy.recarray</span> will automatically convert to <span class="title-ref">numpy.record</span> datatype, so the dtype can be left out of the view:

    >>> recordarr = arr.view(np.recarray)
    >>> recordarr.dtype
    dtype((numpy.record, [('foo', '<i4'), ('bar', '<f4'), ('baz', 'S10')]))

To get back to a plain ndarray both the dtype and type must be reset. The following view does so, taking into account the unusual case that the recordarr was not a structured type:

    >>> arr2 = recordarr.view(recordarr.dtype.fields or recordarr.dtype, np.ndarray)

Record array fields accessed by index or by attribute are returned as a record array if the field has a structured type but as a plain ndarray otherwise. :

    >>> recordarr = np.rec.array([('Hello', (1, 2)), ("World", (3, 4))],
    ...                 dtype=[('foo', 'S6'),('bar', [('A', int), ('B', int)])])
    >>> type(recordarr.foo)
    <class 'numpy.ndarray'>
    >>> type(recordarr.bar)
    <class 'numpy.rec.recarray'>

Note that if a field has the same name as an ndarray attribute, the ndarray attribute takes precedence. Such fields will be inaccessible by attribute but will still be accessible by index.

### Recarray helper functions

<div class="automodule" data-members="">

numpy.lib.recfunctions

</div>

---

basics.strings.md

---

# Working with Arrays of Strings And Bytes

While NumPy is primarily a numerical library, it is often convenient to work with NumPy arrays of strings or bytes. The two most common use cases are:

  - Working with data loaded or memory-mapped from a data file, where one or more of the fields in the data is a string or bytestring, and the maximum length of the field is known ahead of time. This often is used for a name or label field.
  - Using NumPy indexing and broadcasting with arrays of Python strings of unknown length, which may or may not have data defined for every value.

For the first use case, NumPy provides the fixed-width <span class="title-ref">numpy.void</span>, <span class="title-ref">numpy.str\_</span> and <span class="title-ref">numpy.bytes\_</span> data types. For the second use case, numpy provides <span class="title-ref">numpy.dtypes.StringDType</span>. Below we describe how to work with both fixed-width and variable-width string arrays, how to convert between the two representations, and provide some advice for most efficiently working with string data in NumPy.

## Fixed-width data types

Before NumPy 2.0, the fixed-width <span class="title-ref">numpy.str\_</span>, <span class="title-ref">numpy.bytes\_</span>, and <span class="title-ref">numpy.void</span> data types were the only types available for working with strings and bytestrings in NumPy. For this reason, they are used as the default dtype for strings and bytestrings, respectively:

> \>\>\> np.array(\["hello", "world"\]) array(\['hello', 'world'\], dtype='\<U5')

Here the detected data type is `'<U5'`, or little-endian unicode string data, with a maximum length of 5 unicode code points.

Similarly for bytestrings:

> \>\>\> np.array(\[b"hello", b"world"\]) array(\[b'hello', b'world'\], dtype='|S5')

Since this is a one-byte encoding, the byteorder is <span class="title-ref">'|'</span> (not applicable), and the data type detected is a maximum 5 character bytestring.

You can also use <span class="title-ref">numpy.void</span> to represent bytestrings:

> \>\>\> np.array(\[b"hello", b"world"\]).astype(np.void) array(\[b'x68x65x6Cx6Cx6F', b'x77x6Fx72x6Cx64'\], dtype='|V5')

This is most useful when working with byte streams that are not well represented as bytestrings, and instead are better thought of as collections of 8-bit integers.

## Variable-width strings

<div class="versionadded">

2.0

</div>

\> **Note** \> <span class="title-ref">numpy.dtypes.StringDType</span> is a new addition to NumPy, implemented using the new support in NumPy for flexible user-defined data types and is not as extensively tested in production workflows as the older NumPy data types.

Often, real-world string data does not have a predictable length. In these cases it is awkward to use fixed-width strings, since storing all the data without truncation requires knowing the length of the longest string one would like to store in the array before the array is created.

To support situations like this, NumPy provides <span class="title-ref">numpy.dtypes.StringDType</span>, which stores variable-width string data in a UTF-8 encoding in a NumPy array:

> \>\>\> from numpy.dtypes import StringDType \>\>\> data = \["this is a longer string", "short string"\] \>\>\> arr = np.array(data, dtype=StringDType()) \>\>\> arr array(\['this is a longer string', 'short string'\], dtype=StringDType())

Note that unlike fixed-width strings, `StringDType` is not parameterized by the maximum length of an array element, arbitrarily long or short strings can live in the same array without needing to reserve storage for padding bytes in the short strings.

Also note that unlike fixed-width strings and most other NumPy data types, `StringDType` does not store the string data in the "main" `ndarray` data buffer. Instead, the array buffer is used to store metadata about where the string data are stored in memory. This difference means that code expecting the array buffer to contain string data will not function correctly, and will need to be updated to support `StringDType`.

### Missing data support

Often string datasets are not complete, and a special label is needed to indicate that a value is missing. By default `StringDType` does not have any special support for missing values, besides the fact that empty strings are used to populate empty arrays:

> \>\>\> np.empty(3, dtype=StringDType()) array(\['', '', ''\], dtype=StringDType())

Optionally, you can pass create an instance of `StringDType` with support for missing values by passing `na_object` as a keyword argument for the initializer:

> \>\>\> dt = StringDType(na\_object=None) \>\>\> arr = np.array(\["this array has", None, "as an entry"\], dtype=dt) \>\>\> arr array(\['this array has', None, 'as an entry'\], dtype=StringDType(na\_object=None)) \>\>\> arr\[1\] is None True

The `na_object` can be any arbitrary python object. Common choices are <span class="title-ref">numpy.nan</span>, `float('nan')`, `None`, an object specifically intended to represent missing data like `pandas.NA`, or a (hopefully) unique string like `"__placeholder__"`.

NumPy has special handling for NaN-like sentinels and string sentinels.

#### NaN-like Missing Data Sentinels

A NaN-like sentinel returns itself as the result of arithmetic operations. This includes the python `nan` float and the Pandas missing data sentinel `pd.NA`. NaN-like sentinels inherit these behaviors in string operations. This means that, for example, the result of addition with any other string is the sentinel:

> \>\>\> dt = StringDType(na\_object=np.nan) \>\>\> arr = np.array(\["hello", np.nan, "world"\], dtype=dt) \>\>\> arr + arr array(\['hellohello', nan, 'worldworld'\], dtype=StringDType(na\_object=nan))

Following the behavior of `nan` in float arrays, NaN-like sentinels sort to the end of the array:

> \>\>\> np.sort(arr) array(\['hello', 'world', nan\], dtype=StringDType(na\_object=nan))

#### String Missing Data Sentinels

A string missing data value is an instance of `str` or subtype of `str`. If such an array is passed to a string operation or a cast, "missing" entries are treated as if they have a value given by the string sentinel. Comparison operations similarly use the sentinel value directly for missing entries.

#### Other Sentinels

Other objects, such as `None` are also supported as missing data sentinels. If any missing data are present in an array using such a sentinel, then string operations will raise an error:

> \>\>\> dt = StringDType(na\_object=None) \>\>\> arr = np.array(\["this array has", None, "as an entry"\]) \>\>\> np.sort(arr) Traceback (most recent call last): ... TypeError: '\<' not supported between instances of 'NoneType' and 'str'

### Coercing Non-strings

By default, non-string data are coerced to strings:

> \>\>\> np.array(\[1, object(), 3.4\], dtype=StringDType()) array(\['1', '\<object object at 0x7faa2497dde0\>', '3.4'\], dtype=StringDType())

If this behavior is not desired, an instance of the DType can be created that disables string coercion by setting `coerce=False` in the initializer:

> \>\>\> np.array(\[1, object(), 3.4\], dtype=StringDType(coerce=False)) Traceback (most recent call last): ... ValueError: StringDType only allows string data when string coercion is disabled.

This allows strict data validation in the same pass over the data NumPy uses to create the array. Setting `coerce=True` recovers the default behavior allowing coercion to strings.

### Casting To and From Fixed-Width Strings

`StringDType` supports round-trip casts between <span class="title-ref">numpy.str\_</span>, <span class="title-ref">numpy.bytes\_</span>, and <span class="title-ref">numpy.void</span>. Casting to a fixed-width string is most useful when strings need to be memory-mapped in an ndarray or when a fixed-width string is needed for reading and writing to a columnar data format with a known maximum string length.

In all cases, casting to a fixed-width string requires specifying the maximum allowed string length:

    >>> arr = np.array(["hello", "world"], dtype=StringDType())
    >>> arr.astype(np.str_)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    TypeError: Casting from StringDType to a fixed-width dtype with an
    unspecified size is not currently supported, specify an explicit
    size for the output dtype instead.
    
    The above exception was the direct cause of the following
    exception:
    
    TypeError: cannot cast dtype StringDType() to <class 'numpy.dtypes.StrDType'>.
    >>> arr.astype("U5")
    array(['hello', 'world'], dtype='<U5')

The <span class="title-ref">numpy.bytes\_</span> cast is most useful for string data that is known to contain only ASCII characters, as characters outside this range cannot be represented in a single byte in the UTF-8 encoding and are rejected.

Any valid unicode string can be cast to <span class="title-ref">numpy.str\_</span>, although since <span class="title-ref">numpy.str\_</span> uses a 32-bit UCS4 encoding for all characters, this will often waste memory for real-world textual data that can be well-represented by a more memory-efficient encoding.

Additionally, any valid unicode string can be cast to <span class="title-ref">numpy.void</span>, storing the UTF-8 bytes directly in the output array:

> \>\>\> arr = np.array(\["hello", "world"\], dtype=StringDType()) \>\>\> arr.astype("V5") array(\[b'x68x65x6Cx6Cx6F', b'x77x6Fx72x6Cx64'\], dtype='|V5')

Care must be taken to ensure that the output array has enough space for the UTF-8 bytes in the string, since the size of a UTF-8 bytestream in bytes is not necessarily the same as the number of characters in the string.

---

basics.subclassing.md

---

# Subclassing ndarray

## Introduction

Subclassing ndarray is relatively simple, but it has some complications compared to other Python objects. On this page we explain the machinery that allows you to subclass ndarray, and the implications for implementing a subclass.

### ndarrays and object creation

Subclassing ndarray is complicated by the fact that new instances of ndarray classes can come about in three different ways. These are:

1.  Explicit constructor call - as in `MySubClass(params)`. This is the usual route to Python instance creation.
2.  View casting - casting an existing ndarray as a given subclass
3.  New from template - creating a new instance from a template instance. Examples include returning slices from a subclassed array, creating return types from ufuncs, and copying arrays. See \[new-from-template\](\#new-from-template) for more details

The last two are characteristics of ndarrays - in order to support things like array slicing. The complications of subclassing ndarray are due to the mechanisms numpy has to support these latter two routes of instance creation.

### When to use subclassing

Besides the additional complexities of subclassing a NumPy array, subclasses can run into unexpected behaviour because some functions may convert the subclass to a baseclass and "forget" any additional information associated with the subclass. This can result in surprising behavior if you use NumPy methods or functions you have not explicitly tested.

On the other hand, compared to other interoperability approaches, subclassing can be useful because many things will "just work".

This means that subclassing can be a convenient approach and for a long time it was also often the only available approach. However, NumPy now provides additional interoperability protocols described in "\[Interoperability with NumPy \<basics.interoperability\>\](\#interoperability-with-numpy-\<basics.interoperability\>)". For many use-cases these interoperability protocols may now be a better fit or supplement the use of subclassing.

Subclassing can be a good fit if:

  - you are less worried about maintainability or users other than yourself: Subclass will be faster to implement and additional interoperability can be added "as-needed". And with few users, possible surprises are not an issue.
  - you do not think it is problematic if the subclass information is ignored or lost silently. An example is `np.memmap` where "forgetting" about data being memory mapped cannot lead to a wrong result. An example of a subclass that sometimes confuses users are NumPy's masked arrays. When they were introduced, subclassing was the only approach for implementation. However, today we would possibly try to avoid subclassing and rely only on interoperability protocols.

Note that also subclass authors may wish to study \[Interoperability with NumPy \<basics.interoperability\>\](\#interoperability-with-numpy-\<basics.interoperability\>) to support more complex use-cases or work around the surprising behavior.

`astropy.units.Quantity` and `xarray` are examples for array-like objects that interoperate well with NumPy. Astropy's `Quantity` is an example which uses a dual approach of both subclassing and interoperability protocols.

## View casting

*View casting* is the standard ndarray mechanism by which you take an ndarray of any subclass, and return a view of the array as another (specified) subclass:

\>\>\> import numpy as np \>\>\> \# create a completely useless ndarray subclass \>\>\> class C(np.ndarray): pass \>\>\> \# create a standard ndarray \>\>\> arr = np.zeros((3,)) \>\>\> \# take a view of it, as our useless subclass \>\>\> c\_arr = arr.view(C) \>\>\> type(c\_arr) \<class '\_\_[main](#array_ufunc__-for-ufuncs).C'\>

## Creating new from template

New instances of an ndarray subclass can also come about by a very similar mechanism to \[view-casting\](\#view-casting), when numpy finds it needs to create a new instance from a template instance. The most obvious place this has to happen is when you are taking slices of subclassed arrays. For example:

\>\>\> v = c\_arr\[1:\] \>\>\> type(v) \# the view is of type 'C' \<class '\_\_[main](#array_wrap__-for-ufuncs-and-other-functions).C'\> \>\>\> v is c\_arr \# but it's a new instance False

The slice is a *view* onto the original `c_arr` data. So, when we take a view from the ndarray, we return a new ndarray, of the same class, that points to the data in the original.

There are other points in the use of ndarrays where we need such views, such as copying arrays (`c_arr.copy()`), creating ufunc output arrays (see also \[array-wrap\](\#array-wrap)), and reducing methods (like `c_arr.mean()`).

## Relationship of view casting and new-from-template

These paths both use the same machinery. We make the distinction here, because they result in different input to your methods. Specifically, \[view-casting\](\#view-casting) means you have created a new instance of your array type from any potential subclass of ndarray. \[new-from-template\](\#new-from-template) means you have created a new instance of your class from a pre-existing instance, allowing you - for example - to copy across attributes that are particular to your subclass.

## Implications for subclassing

If we subclass ndarray, we need to deal not only with explicit construction of our array type, but also \[view-casting\](\#view-casting) or \[new-from-template\](\#new-from-template). NumPy has the machinery to do this, and it is this machinery that makes subclassing slightly non-standard.

There are two aspects to the machinery that ndarray uses to support views and new-from-template in subclasses.

The first is the use of the `ndarray.__new__` method for the main work of object initialization, rather then the more usual `__init__` method. The second is the use of the `__array_finalize__` method to allow subclasses to clean up after the creation of views and new instances from templates.

### A brief Python primer on `__new__` and `__init__`

`__new__` is a standard Python method, and, if present, is called before `__init__` when we create a class instance. See the [python \_\_new\_\_ documentation](https://docs.python.org/reference/datamodel.html#object.__new__) for more detail.

For example, consider the following Python code:

\>\>\> class C: ... def \_\_new\_\_(cls, *args): ... print('Cls in \_\_new\_\_:', cls) ... print('Args in \_\_new\_\_:', args) ... \# The \`object\` type \_\_new\_\_ method takes a single argument. ... return object.\_\_new\_\_(cls) ... def \_\_init\_\_(self,*args): ... print('type(self) in \_\_init\_\_:', type(self)) ... print('Args in \_\_init\_\_:', args)

meaning that we get:

\>\>\> c = C('hello') Cls in \_\_new\_\_: \<class '\_\_main\_\_.C'\> Args in \_\_new\_\_: ('hello',) type(self) in \_\_init\_\_: \<class '\_\_main\_\_.C'\> Args in \_\_init\_\_: ('hello',)

When we call `C('hello')`, the `__new__` method gets its own class as first argument, and the passed argument, which is the string `'hello'`. After python calls `__new__`, it usually (see below) calls our `__init__` method, with the output of `__new__` as the first argument (now a class instance), and the passed arguments following.

As you can see, the object can be initialized in the `__new__` method or the `__init__` method, or both, and in fact ndarray does not have an `__init__` method, because all the initialization is done in the `__new__` method.

Why use `__new__` rather than just the usual `__init__`? Because in some cases, as for ndarray, we want to be able to return an object of some other class. Consider the following:

<div class="testcode">

  - class D(C):  
    def \_\_new\_\_(cls, *args): print('D cls is:', cls) print('D args in \_\_new\_\_:', args) return C.\_\_new\_\_(C,*args)
    
      - def \_\_init\_\_(self, \*args):  
        \# we never get here print('In D \_\_init\_\_')

</div>

meaning that:

\>\>\> obj = D('hello') D cls is: \<class 'D'\> D args in \_\_new\_\_: ('hello',) Cls in \_\_new\_\_: \<class 'C'\> Args in \_\_new\_\_: ('hello',) \>\>\> type(obj) \<class 'C'\>

The definition of `C` is the same as before, but for `D`, the `__new__` method returns an instance of class `C` rather than `D`. Note that the `__init__` method of `D` does not get called. In general, when the `__new__` method returns an object of class other than the class in which it is defined, the `__init__` method of that class is not called.

This is how subclasses of the ndarray class are able to return views that preserve the class type. When taking a view, the standard ndarray machinery creates the new ndarray object with something like:

    obj = ndarray.__new__(subtype, shape, ...

where `subtype` is the subclass. Thus the returned view is of the same class as the subclass, rather than being of class `ndarray`.

That solves the problem of returning views of the same type, but now we have a new problem. The machinery of ndarray can set the class this way, in its standard methods for taking views, but the ndarray `__new__` method knows nothing of what we have done in our own `__new__` method in order to set attributes, and so on. (Aside -why not call `obj = subdtype.__new__(...` then? Because we may not have a `__new__` method with the same call signature).

### The role of `__array_finalize__`

`__array_finalize__` is the mechanism that numpy provides to allow subclasses to handle the various ways that new instances get created.

Remember that subclass instances can come about in these three ways:

1.  explicit constructor call (`obj = MySubClass(params)`). This will call the usual sequence of `MySubClass.__new__` then (if it exists) `MySubClass.__init__`.
2.  \[view-casting\](\#view-casting)
3.  \[new-from-template\](\#new-from-template)

Our `MySubClass.__new__` method only gets called in the case of the explicit constructor call, so we can't rely on `MySubClass.__new__` or `MySubClass.__init__` to deal with the view casting and new-from-template. It turns out that `MySubClass.__array_finalize__` *does* get called for all three methods of object creation, so this is where our object creation housekeeping usually goes.

  - For the explicit constructor call, our subclass will need to create a new ndarray instance of its own class. In practice this means that we, the authors of the code, will need to make a call to `ndarray.__new__(MySubClass,...)`, a class-hierarchy prepared call to `super().__new__(cls, ...)`, or do view casting of an existing array (see below)
  - For view casting and new-from-template, the equivalent of `ndarray.__new__(MySubClass,...` is called, at the C level.

The arguments that `__array_finalize__` receives differ for the three methods of instance creation above.

The following code allows us to look at the call sequences and arguments:

<div class="testcode">

import numpy as np

  - class C(np.ndarray):  
    def \_\_new\_\_(cls, *args,kwargs): print('In \_\_new\_\_ with class %s' % cls) return super().\_\_new\_\_(cls,*args, \*\*kwargs)
    
      - def \_\_init\_\_(self, *args,*\*kwargs):  
        \# in practice you probably will not need or want an \_\_init\_\_ \# method for your subclass print('In \_\_init\_\_ with class %s' % self.\_\_class\_\_)
    
      - def \_\_array\_finalize\_\_(self, obj):  
        print('In array\_finalize:') print(' self type is %s' % type(self)) print(' obj type is %s' % type(obj))

</div>

Now:

\>\>\> \# Explicit constructor \>\>\> c = C((10,)) In \_\_new\_\_ with class \<class 'C'\> In array\_finalize: self type is \<class 'C'\> obj type is \<type 'NoneType'\> In \_\_init\_\_ with class \<class 'C'\> \>\>\> \# View casting \>\>\> a = np.arange(10) \>\>\> cast\_a = a.view(C) In array\_finalize: self type is \<class 'C'\> obj type is \<type 'numpy.ndarray'\> \>\>\> \# Slicing (example of new-from-template) \>\>\> cv = c\[:1\] In array\_finalize: self type is \<class 'C'\> obj type is \<class 'C'\>

The signature of `__array_finalize__` is:

    def __array_finalize__(self, obj):

One sees that the `super` call, which goes to `ndarray.__new__`, passes `__array_finalize__` the new object, of our own class (`self`) as well as the object from which the view has been taken (`obj`). As you can see from the output above, the `self` is always a newly created instance of our subclass, and the type of `obj` differs for the three instance creation methods:

  - When called from the explicit constructor, `obj` is `None`
  - When called from view casting, `obj` can be an instance of any subclass of ndarray, including our own.
  - When called in new-from-template, `obj` is another instance of our own subclass, that we might use to update the new `self` instance.

Because `__array_finalize__` is the only method that always sees new instances being created, it is the sensible place to fill in instance defaults for new object attributes, among other tasks.

This may be clearer with an example.

## Simple example - adding an extra attribute to ndarray

<div class="testcode">

import numpy as np

class InfoArray(np.ndarray):

>   - def \_\_new\_\_(subtype, shape, dtype=float, buffer=None, offset=0,  
>     strides=None, order=None, info=None): \# Create the ndarray instance of our type, given the usual \# ndarray input arguments. This will call the standard \# ndarray constructor, but return an object of our type. \# It also triggers a call to InfoArray.\_\_array\_finalize\_\_ obj = super().\_\_new\_\_(subtype, shape, dtype, buffer, offset, strides, order) \# set the new 'info' attribute to the value passed obj.info = info \# Finally, we must return the newly created object: return obj
> 
>   - def \_\_array\_finalize\_\_(self, obj):  
>     \# `self` is a new object resulting from \# ndarray.\_\_new\_\_(InfoArray, ...), therefore it only has \# attributes that the ndarray.\_\_new\_\_ constructor gave it -\# i.e. those of a standard ndarray. \# \# We could have got to the ndarray.\_\_new\_\_ call in 3 ways: \# From an explicit constructor - e.g. InfoArray(): \# obj is None \# (we're in the middle of the InfoArray.\_\_new\_\_ \# constructor, and self.info will be set when we return to \# InfoArray.\_\_new\_\_) if obj is None: return \# From view casting - e.g arr.view(InfoArray): \# obj is arr \# (type(obj) can be InfoArray) \# From new-from-template - e.g infoarr\[:3\] \# type(obj) is InfoArray \# \# Note that it is here, rather than in the \_\_new\_\_ method, \# that we set the default value for 'info', because this \# method sees all creation of default objects - with the \# InfoArray.\_\_new\_\_ constructor, but also with \# arr.view(InfoArray). self.info = getattr(obj, 'info', None) \# We do not need to return anything

</div>

Using the object looks like this:

> \>\>\> obj = InfoArray(shape=(3,)) \# explicit constructor \>\>\> type(obj) \<class 'InfoArray'\> \>\>\> obj.info is None True \>\>\> obj = InfoArray(shape=(3,), info='information') \>\>\> obj.info 'information' \>\>\> v = obj\[1:\] \# new-from-template - here - slicing \>\>\> type(v) \<class 'InfoArray'\> \>\>\> v.info 'information' \>\>\> arr = np.arange(10) \>\>\> cast\_arr = arr.view(InfoArray) \# view casting \>\>\> type(cast\_arr) \<class 'InfoArray'\> \>\>\> cast\_arr.info is None True

This class isn't very useful, because it has the same constructor as the bare ndarray object, including passing in buffers and shapes and so on. We would probably prefer the constructor to be able to take an already formed ndarray from the usual numpy calls to `np.array` and return an object.

## Slightly more realistic example - attribute added to existing array

Here is a class that takes a standard ndarray that already exists, casts as our type, and adds an extra attribute.

<div class="testcode">

import numpy as np

class RealisticInfoArray(np.ndarray):

>   - def \_\_new\_\_(cls, input\_array, info=None):  
>     \# Input array is an already formed ndarray instance \# We first cast to be our class type obj = np.asarray(input\_array).view(cls) \# add the new attribute to the created instance obj.info = info \# Finally, we must return the newly created object: return obj
> 
>   - def \_\_array\_finalize\_\_(self, obj):  
>     \# see InfoArray.\_\_array\_finalize\_\_ for comments if obj is None: return self.info = getattr(obj, 'info', None)

</div>

So:

> \>\>\> arr = np.arange(5) \>\>\> obj = RealisticInfoArray(arr, info='information') \>\>\> type(obj) \<class 'RealisticInfoArray'\> \>\>\> obj.info 'information' \>\>\> v = obj\[1:\] \>\>\> type(v) \<class 'RealisticInfoArray'\> \>\>\> v.info 'information'

## `__array_ufunc__` for ufuncs

A subclass can override what happens when executing numpy ufuncs on it by overriding the default `ndarray.__array_ufunc__` method. This method is executed *instead* of the ufunc and should return either the result of the operation, or `NotImplemented` if the operation requested is not implemented.

The signature of `__array_ufunc__` is:

    def __array_ufunc__(ufunc, method, *inputs, **kwargs):

  - *ufunc* is the ufunc object that was called.
  - *method* is a string indicating how the Ufunc was called, either `"__call__"` to indicate it was called directly, or one of its \[methods\<ufuncs.methods\>\](\#methods\<ufuncs.methods\>): `"reduce"`, `"accumulate"`, `"reduceat"`, `"outer"`, or `"at"`.
  - *inputs* is a tuple of the input arguments to the `ufunc`
  - *kwargs* contains any optional or keyword arguments passed to the function. This includes any `out` arguments, which are always contained in a tuple.

A typical implementation would convert any inputs or outputs that are instances of one's own class, pass everything on to a superclass using `super()`, and finally return the results after possible back-conversion. An example, taken from the test case `test_ufunc_override_with_super` in `_core/tests/test_umath.py`, is the following.

<div class="testcode">

input numpy as np

  - class A(np.ndarray):
    
      - def \_\_array\_ufunc\_\_(self, ufunc, method, *inputs, out=None,*\*kwargs):  
        args = \[\] in\_no = \[\] for i, [input]() in enumerate(inputs): if isinstance([input](), A): in\_no.append(i) args.append([input]().view(np.ndarray)) else: args.append([input]())
        
        outputs = out out\_no = \[\] if outputs: out\_args = \[\] for j, output in enumerate(outputs): if isinstance(output, A): out\_no.append(j) out\_args.append(output.view(np.ndarray)) else: out\_args.append(output) kwargs\['out'\] = tuple(out\_args) else: outputs = (None,) \* ufunc.nout
        
        info = {} if in\_no: info\['inputs'\] = in\_no if out\_no: info\['outputs'\] = out\_no
        
        results = super().\_\_array\_ufunc\_\_(ufunc, method, *args,*\*kwargs) if results is NotImplemented: return NotImplemented
        
          - if method == 'at':
            
              - if isinstance(inputs\[0\], A):  
                inputs\[0\].info = info
            
            return
        
          - if ufunc.nout == 1:  
            results = (results,)
        
          - results = tuple((np.asarray(result).view(A)  
            if output is None else output) for result, output in zip(results, outputs))
        
          - if results and isinstance(results\[0\], A):  
            results\[0\].info = info
        
        return results\[0\] if len(results) == 1 else results

</div>

So, this class does not actually do anything interesting: it just converts any instances of its own to regular ndarray (otherwise, we'd get infinite recursion\!), and adds an `info` dictionary that tells which inputs and outputs it converted. Hence, e.g.,

\>\>\> a = np.arange(5.).view(A) \>\>\> b = np.sin(a) \>\>\> b.info {'inputs': \[0\]} \>\>\> b = np.sin(np.arange(5.), out=(a,)) \>\>\> b.info {'outputs': \[0\]} \>\>\> a = np.arange(5.).view(A) \>\>\> b = np.ones(1).view(A) \>\>\> c = a + b \>\>\> c.info {'inputs': \[0, 1\]} \>\>\> a += b \>\>\> a.info {'inputs': \[0, 1\], 'outputs': \[0\]}

Note that another approach would be to use `getattr(ufunc, methods)(*inputs, **kwargs)` instead of the `super` call. For this example, the result would be identical, but there is a difference if another operand also defines `__array_ufunc__`. E.g., lets assume that we evaluate `np.add(a, b)`, where `b` is an instance of another class `B` that has an override. If you use `super` as in the example, `ndarray.__array_ufunc__` will notice that `b` has an override, which means it cannot evaluate the result itself. Thus, it will return <span class="title-ref">NotImplemented</span> and so will our class `A`. Then, control will be passed over to `b`, which either knows how to deal with us and produces a result, or does not and returns <span class="title-ref">NotImplemented</span>, raising a `TypeError`.

If instead, we replace our `super` call with `getattr(ufunc, method)`, we effectively do `np.add(a.view(np.ndarray), b)`. Again, `B.__array_ufunc__` will be called, but now it sees an `ndarray` as the other argument. Likely, it will know how to handle this, and return a new instance of the `B` class to us. Our example class is not set up to handle this, but it might well be the best approach if, e.g., one were to re-implement `MaskedArray` using `__array_ufunc__`.

As a final note: if the `super` route is suited to a given class, an advantage of using it is that it helps in constructing class hierarchies. E.g., suppose that our other class `B` also used the `super` in its `__array_ufunc__` implementation, and we created a class `C` that depended on both, i.e., `class C(A, B)` (with, for simplicity, not another `__array_ufunc__` override). Then any ufunc on an instance of `C` would pass on to `A.__array_ufunc__`, the `super` call in `A` would go to `B.__array_ufunc__`, and the `super` call in `B` would go to `ndarray.__array_ufunc__`, thus allowing `A` and `B` to collaborate.

## `__array_wrap__` for ufuncs and other functions

Prior to numpy 1.13, the behaviour of ufuncs could only be tuned using `__array_wrap__` and `__array_prepare__` (the latter is now removed). These two allowed one to change the output type of a ufunc, but, in contrast to `__array_ufunc__`, did not allow one to make any changes to the inputs. It is hoped to eventually deprecate these, but `__array_wrap__` is also used by other numpy functions and methods, such as `squeeze`, so at the present time is still needed for full functionality.

Conceptually, `__array_wrap__` "wraps up the action" in the sense of allowing a subclass to set the type of the return value and update attributes and metadata. Let's show how this works with an example. First we return to the simpler example subclass, but with a different name and some print statements:

<div class="testcode">

import numpy as np

class MySubClass(np.ndarray):

>   - def \_\_new\_\_(cls, input\_array, info=None):  
>     obj = np.asarray(input\_array).view(cls) obj.info = info return obj
> 
>   - def \_\_array\_finalize\_\_(self, obj):  
>     print('In \_\_array\_finalize\_\_:') print(' self is %s' % repr(self)) print(' obj is %s' % repr(obj)) if obj is None: return self.info = getattr(obj, 'info', None)
> 
>   - def \_\_array\_wrap\_\_(self, out\_arr, context=None, return\_scalar=False):  
>     print('In \_\_array\_wrap\_\_:') print(' self is %s' % repr(self)) print(' arr is %s' % repr(out\_arr)) \# then just call the parent return super().\_\_array\_wrap\_\_(self, out\_arr, context, return\_scalar)

</div>

We run a ufunc on an instance of our new array:

\>\>\> obj = MySubClass(np.arange(5), info='spam') In \_\_array\_finalize\_\_: self is MySubClass(\[0, 1, 2, 3, 4\]) obj is array(\[0, 1, 2, 3, 4\]) \>\>\> arr2 = np.arange(5)+1 \>\>\> ret = np.add(arr2, obj) In \_\_array\_wrap\_\_: self is MySubClass(\[0, 1, 2, 3, 4\]) arr is array(\[1, 3, 5, 7, 9\]) In \_\_array\_finalize\_\_: self is MySubClass(\[1, 3, 5, 7, 9\]) obj is MySubClass(\[0, 1, 2, 3, 4\]) \>\>\> ret MySubClass(\[1, 3, 5, 7, 9\]) \>\>\> ret.info 'spam'

Note that the ufunc (`np.add`) has called the `__array_wrap__` method with arguments `self` as `obj`, and `out_arr` as the (ndarray) result of the addition. In turn, the default `__array_wrap__` (`ndarray.__array_wrap__`) has cast the result to class `MySubClass`, and called `__array_finalize__` - hence the copying of the `info` attribute. This has all happened at the C level.

But, we could do anything we wanted:

<div class="testcode">

class SillySubClass(np.ndarray):

>   - def \_\_array\_wrap\_\_(self, arr, context=None, return\_scalar=False):  
>     return 'I lost your data'

</div>

\>\>\> arr1 = np.arange(5) \>\>\> obj = arr1.view(SillySubClass) \>\>\> arr2 = np.arange(5) \>\>\> ret = np.multiply(obj, arr2) \>\>\> ret 'I lost your data'

So, by defining a specific `__array_wrap__` method for our subclass, we can tweak the output from ufuncs. The `__array_wrap__` method requires `self`, then an argument - which is the result of the ufunc or another NumPy function - and an optional parameter *context*. This parameter is passed by ufuncs as a 3-element tuple: (name of the ufunc, arguments of the ufunc, domain of the ufunc), but is not passed by other numpy functions. Though, as seen above, it is possible to do otherwise, `__array_wrap__` should return an instance of its containing class. See the masked array subclass for an implementation. `__array_wrap__` is always passed a NumPy array which may or may not be a subclass (usually of the caller).

## Extra gotchas - custom `__del__` methods and ndarray.base

One of the problems that ndarray solves is keeping track of memory ownership of ndarrays and their views. Consider the case where we have created an ndarray, `arr` and have taken a slice with `v = arr[1:]`. The two objects are looking at the same memory. NumPy keeps track of where the data came from for a particular array or view, with the `base` attribute:

\>\>\> \# A normal ndarray, that owns its own data \>\>\> arr = np.zeros((4,)) \>\>\> \# In this case, base is None \>\>\> arr.base is None True \>\>\> \# We take a view \>\>\> v1 = arr\[1:\] \>\>\> \# base now points to the array that it derived from \>\>\> v1.base is arr True \>\>\> \# Take a view of a view \>\>\> v2 = v1\[1:\] \>\>\> \# base points to the original array that it was derived from \>\>\> v2.base is arr True

In general, if the array owns its own memory, as for `arr` in this case, then `arr.base` will be None - there are some exceptions to this - see the numpy book for more details.

The `base` attribute is useful in being able to tell whether we have a view or the original array. This in turn can be useful if we need to know whether or not to do some specific cleanup when the subclassed array is deleted. For example, we may only want to do the cleanup if the original array is deleted, but not the views. For an example of how this can work, have a look at the `memmap` class in `numpy._core`.

## Subclassing and downstream compatibility

When sub-classing `ndarray` or creating duck-types that mimic the `ndarray` interface, it is your responsibility to decide how aligned your APIs will be with those of numpy. For convenience, many numpy functions that have a corresponding `ndarray` method (e.g., `sum`, `mean`, `take`, `reshape`) work by checking if the first argument to a function has a method of the same name. If it exists, the method is called instead of coercing the arguments to a numpy array.

For example, if you want your sub-class or duck-type to be compatible with numpy's `sum` function, the method signature for this object's `sum` method should be the following:

<div class="testcode">

def sum(self, axis=None, dtype=None, out=None, keepdims=False): ...

</div>

This is the exact same method signature for `np.sum`, so now if a user calls `np.sum` on this object, numpy will call the object's own `sum` method and pass in these arguments enumerated above in the signature, and no errors will be raised because the signatures are completely compatible with each other.

If, however, you decide to deviate from this signature and do something like this:

<div class="testcode">

def sum(self, axis=None, dtype=None): ...

</div>

This object is no longer compatible with `np.sum` because if you call `np.sum`, it will pass in unexpected arguments `out` and `keepdims`, causing a TypeError to be raised.

If you wish to maintain compatibility with numpy and its subsequent versions (which might add new keyword arguments) but do not want to surface all of numpy's arguments, your function's signature should accept `**kwargs`. For example:

<div class="testcode">

def sum(self, axis=None, dtype=None, \*\*unused\_kwargs): ...

</div>

This object is now compatible with `np.sum` again because any extraneous arguments (i.e. keywords that are not `axis` or `dtype`) will be hidden away in the `**unused_kwargs` parameter.

---

basics.types.md

---

# Data types

<div class="seealso">

\[Data type objects \<arrays.dtypes\>\](\#data-type-objects-\<arrays.dtypes\>)

</div>

## Array types and conversions between types

NumPy supports a much greater variety of numerical types than Python does. This section shows which are available, and how to modify an array's data-type.

NumPy numerical types are instances of <span class="title-ref">numpy.dtype</span> (data-type) objects, each having unique characteristics. Once you have imported NumPy using `import numpy as np` you can create arrays with a specified dtype using the scalar types in the numpy top-level API, e.g. <span class="title-ref">numpy.bool</span>, <span class="title-ref">numpy.float32</span>, etc.

These scalar types as arguments to the dtype keyword that many numpy functions or methods accept. For example:

    >>> z = np.arange(3, dtype=np.uint8)
    >>> z
    array([0, 1, 2], dtype=uint8)

Array types can also be referred to by character codes, for example:

    >>> np.array([1, 2, 3], dtype='f')
    array([1.,  2.,  3.], dtype=float32)
    >>> np.array([1, 2, 3], dtype='d')
    array([1.,  2.,  3.], dtype=float64)

See \[arrays.dtypes.constructing\](\#arrays.dtypes.constructing) for more information about specifying and constructing data type objects, including how to specify parameters like the byte order.

To convert the type of an array, use the .astype() method. For example: :

    >>> z.astype(np.float64)                 #doctest: +NORMALIZE_WHITESPACE
    array([0.,  1.,  2.])

Note that, above, we could have used the *Python* float object as a dtype instead of <span class="title-ref">numpy.float64</span>. NumPy knows that <span class="title-ref">int</span> refers to <span class="title-ref">numpy.int\_</span>, <span class="title-ref">bool</span> means <span class="title-ref">numpy.bool</span>, that <span class="title-ref">float</span> is <span class="title-ref">numpy.float64</span> and <span class="title-ref">complex</span> is <span class="title-ref">numpy.complex128</span>. The other data-types do not have Python equivalents.

To determine the type of an array, look at the dtype attribute:

    >>> z.dtype
    dtype('uint8')

dtype objects also contain information about the type, such as its bit-width and its byte-order. The data type can also be used indirectly to query properties of the type, such as whether it is an integer:

    >>> d = np.dtype(np.int64)
    >>> d
    dtype('int64')
    
    >>> np.issubdtype(d, np.integer)
    True
    
    >>> np.issubdtype(d, np.floating)
    False

### Numerical Data Types

There are 5 basic numerical types representing booleans (`bool`), integers (`int`), unsigned integers (`uint`) floating point (`float`) and `complex`. A basic numerical type name combined with a numeric bitsize defines a concrete type. The bitsize is the number of bits that are needed to represent a single value in memory. For example, <span class="title-ref">numpy.float64</span> is a 64 bit floating point data type. Some types, such as <span class="title-ref">numpy.int\_</span> and <span class="title-ref">numpy.intp</span>, have differing bitsizes, dependent on the platforms (e.g. 32-bit vs. 64-bit CPU architectures). This should be taken into account when interfacing with low-level code (such as C or Fortran) where the raw memory is addressed.

### Data Types for Strings and Bytes

In addition to numerical types, NumPy also supports storing unicode strings, via the <span class="title-ref">numpy.str\_</span> dtype (`U` character code), null-terminated byte sequences via <span class="title-ref">numpy.bytes\_</span> (`S` character code), and arbitrary byte sequences, via <span class="title-ref">numpy.void</span> (`V` character code).

All of the above are *fixed-width* data types. They are parameterized by a width, in either bytes or unicode points, that a single data element in the array must fit inside. This means that storing an array of byte sequences or strings using this dtype requires knowing or calculating the sizes of the longest text or byte sequence in advance.

As an example, we can create an array storing the words `"hello"` and `"world!"`:

    >>> np.array(["hello", "world!"])
    array(['hello', 'world!'], dtype='<U6')

Here the data type is detected as a unicode string that is a maximum of 6 code points long, enough to store both entries without truncation. If we specify a shorter or longer data type, the string is either truncated or zero-padded to fit in the specified width:

    >>> np.array(["hello", "world!"], dtype="U5")
    array(['hello', 'world'], dtype='<U5')
    >>> np.array(["hello", "world!"], dtype="U7")
    array(['hello', 'world!'], dtype='<U7')

We can see the zero-padding a little more clearly if we use the bytes data type and ask NumPy to print out the bytes in the array buffer:

    >>> np.array(["hello", "world"], dtype="S7").tobytes()
    b'hello\x00\x00world\x00\x00'

Each entry is padded with two extra null bytes. Note however that NumPy cannot tell the difference between intentionally stored trailing nulls and padding nulls:

    >>> x = [b"hello\0\0", b"world"]
    >>> a = np.array(x, dtype="S7")
    >>> print(a[0])
    b"hello"
    >>> a[0] == x[0]
    False

If you need to store and round-trip any trailing null bytes, you will need to use an unstructured void data type:

    >>> a = np.array(x, dtype="V7")
    >>> a
    array([b'\x68\x65\x6C\x6C\x6F\x00\x00', b'\x77\x6F\x72\x6C\x64\x00\x00'],
          dtype='|V7')
    >>> a[0] == np.void(x[0])
    True

Advanced types, not listed above, are explored in section \[structured\_arrays\](\#structured\_arrays).

## Relationship Between NumPy Data Types and C Data Types

NumPy provides both bit sized type names and names based on the names of C types. Since the definition of C types are platform dependent, this means the explicitly bit sized should be preferred to avoid platform-dependent behavior in programs using NumPy.

To ease integration with C code, where it is more natural to refer to platform-dependent C types, NumPy also provides type aliases that correspond to the C types for the platform. Some dtypes have trailing underscore to avoid confusion with builtin python type names, such as <span class="title-ref">numpy.bool\_</span>.

| Canonical Python API name                                                                  | Python API "C-like" name                         | Actual C type                   | Description                                                                                      |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------ | ------------------------------- | ------------------------------------------------------------------------------------------------ |
| <span class="title-ref">numpy.bool</span> or <span class="title-ref">numpy.bool\_</span>   | N/A                                              | `bool` (defined in `stdbool.h`) | Boolean (True or False) stored as a byte.                                                        |
| <span class="title-ref">numpy.int8</span>                                                  | <span class="title-ref">numpy.byte</span>        | `signed char`                   | Platform-defined integer type with 8 bits.                                                       |
| <span class="title-ref">numpy.uint8</span>                                                 | <span class="title-ref">numpy.ubyte</span>       | `unsigned char`                 | Platform-defined integer type with 8 bits without sign.                                          |
| <span class="title-ref">numpy.int16</span>                                                 | <span class="title-ref">numpy.short</span>       | `short`                         | Platform-defined integer type with 16 bits.                                                      |
| <span class="title-ref">numpy.uint16</span>                                                | <span class="title-ref">numpy.ushort</span>      | `unsigned short`                | Platform-defined integer type with 16 bits without sign.                                         |
| <span class="title-ref">numpy.int32</span>                                                 | <span class="title-ref">numpy.intc</span>        | `int`                           | Platform-defined integer type with 32 bits.                                                      |
| <span class="title-ref">numpy.uint32</span>                                                | <span class="title-ref">numpy.uintc</span>       | `unsigned int`                  | Platform-defined integer type with 32 bits without sign.                                         |
| <span class="title-ref">numpy.intp</span>                                                  | N/A                                              | `ssize_t`/`Py_ssize_t`          | Platform-defined integer of size `size_t`; used e.g. for sizes.                                  |
| <span class="title-ref">numpy.uintp</span>                                                 | N/A                                              | `size_t`                        | Platform-defined integer type capable of storing the maximum allocation size.                    |
| N/A                                                                                        | `'p'`                                            | `intptr_t`                      | Guaranteed to hold pointers. Character code only (Python and C).                                 |
| N/A                                                                                        | `'P'`                                            | `uintptr_t`                     | Guaranteed to hold pointers. Character code only (Python and C).                                 |
| <span class="title-ref">numpy.int32</span> or <span class="title-ref">numpy.int64</span>   | <span class="title-ref">numpy.long</span>        | `long`                          | Platform-defined integer type with at least 32 bits.                                             |
| <span class="title-ref">numpy.uint32</span> or <span class="title-ref">numpy.uint64</span> | <span class="title-ref">numpy.ulong</span>       | `unsigned long`                 | Platform-defined integer type with at least 32 bits without sign.                                |
| N/A                                                                                        | <span class="title-ref">numpy.longlong</span>    | `long long`                     | Platform-defined integer type with at least 64 bits.                                             |
| N/A                                                                                        | <span class="title-ref">numpy.ulonglong</span>   | `unsigned long long`            | Platform-defined integer type with at least 64 bits without sign.                                |
| <span class="title-ref">numpy.float16</span>                                               | <span class="title-ref">numpy.half</span>        | N/A                             | Half precision float: sign bit, 5 bits exponent, 10 bits mantissa.                               |
| <span class="title-ref">numpy.float32</span>                                               | <span class="title-ref">numpy.single</span>      | `float`                         | Platform-defined single precision float: typically sign bit, 8 bits exponent, 23 bits mantissa.  |
| <span class="title-ref">numpy.float64</span>                                               | <span class="title-ref">numpy.double</span>      | `double`                        | Platform-defined double precision float: typically sign bit, 11 bits exponent, 52 bits mantissa. |
| `numpy.float96` or <span class="title-ref">numpy.float128</span>                           | <span class="title-ref">numpy.longdouble</span>  | `long double`                   | Platform-defined extended-precision float.                                                       |
| <span class="title-ref">numpy.complex64</span>                                             | <span class="title-ref">numpy.csingle</span>     | `float complex`                 | Complex number, represented by two single-precision floats (real and imaginary components).      |
| <span class="title-ref">numpy.complex128</span>                                            | <span class="title-ref">numpy.cdouble</span>     | `double complex`                | Complex number, represented by two double-precision floats (real and imaginary components).      |
| `numpy.complex192` or <span class="title-ref">numpy.complex256</span>                      | <span class="title-ref">numpy.clongdouble</span> | `long double complex`           | Complex number, represented by two extended-precision floats (real and imaginary components).    |

Since many of these have platform-dependent definitions, a set of fixed-size aliases are provided (See \[sized-aliases\](\#sized-aliases)).

## Array scalars

NumPy generally returns elements of arrays as array scalars (a scalar with an associated dtype). Array scalars differ from Python scalars, but for the most part they can be used interchangeably (the primary exception is for versions of Python older than v2.x, where integer array scalars cannot act as indices for lists and tuples). There are some exceptions, such as when code requires very specific attributes of a scalar or when it checks specifically whether a value is a Python scalar. Generally, problems are easily fixed by explicitly converting array scalars to Python scalars, using the corresponding Python type function (e.g., <span class="title-ref">int</span>, <span class="title-ref">float</span>, <span class="title-ref">complex</span>, <span class="title-ref">str</span>).

The primary advantage of using array scalars is that they preserve the array type (Python may not have a matching scalar type available, e.g. `int16`). Therefore, the use of array scalars ensures identical behaviour between arrays and scalars, irrespective of whether the value is inside an array or not. NumPy scalars also have many of the same methods arrays do.

## Overflow errors

The fixed size of NumPy numeric types may cause overflow errors when a value requires more memory than available in the data type. For example, <span class="title-ref">numpy.power</span> evaluates `100 ** 9` correctly for 64-bit integers, but gives -1486618624 (incorrect) for a 32-bit integer.

> \>\>\> np.power(100, 9, dtype=np.int64) 1000000000000000000 \>\>\> np.power(100, 9, dtype=np.int32) np.int32(-1486618624)

The behaviour of NumPy and Python integer types differs significantly for integer overflows and may confuse users expecting NumPy integers to behave similar to Python's <span class="title-ref">int</span>. Unlike NumPy, the size of Python's <span class="title-ref">int</span> is flexible. This means Python integers may expand to accommodate any integer and will not overflow.

NumPy provides <span class="title-ref">numpy.iinfo</span> and <span class="title-ref">numpy.finfo</span> to verify the minimum or maximum values of NumPy integer and floating point values respectively :

    >>> np.iinfo(int) # Bounds of the default integer on this system.
    iinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)
    >>> np.iinfo(np.int32) # Bounds of a 32-bit integer
    iinfo(min=-2147483648, max=2147483647, dtype=int32)
    >>> np.iinfo(np.int64) # Bounds of a 64-bit integer
    iinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)

If 64-bit integers are still too small the result may be cast to a floating point number. Floating point numbers offer a larger, but inexact, range of possible values.

> \>\>\> np.power(100, 100, dtype=np.int64) \# Incorrect even with 64-bit int 0 \>\>\> np.power(100, 100, dtype=np.float64) 1e+200

## Floating point precision

Many functions in NumPy, especially those in <span class="title-ref">numpy.linalg</span>, involve floating-point arithmetic, which can introduce small inaccuracies due to the way computers represent decimal numbers. For instance, when performing basic arithmetic operations involving floating-point numbers:

> \>\>\> 0.3 - 0.2 - 0.1 \# This does not equal 0 due to floating-point precision -2.7755575615628914e-17

To handle such cases, it's advisable to use functions like <span class="title-ref">np.isclose</span> to compare values, rather than checking for exact equality:

> \>\>\> np.isclose(0.3 - 0.2 - 0.1, 0, rtol=1e-05) \# Check for closeness to 0 True

In this example, <span class="title-ref">np.isclose</span> accounts for the minor inaccuracies that occur in floating-point calculations by applying a relative tolerance, ensuring that results within a small threshold are considered close.

For information about precision in calculations, see [Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html).

## Extended precision

Python's floating-point numbers are usually 64-bit floating-point numbers, nearly equivalent to <span class="title-ref">numpy.float64</span>. In some unusual situations it may be useful to use floating-point numbers with more precision. Whether this is possible in numpy depends on the hardware and on the development environment: specifically, x86 machines provide hardware floating-point with 80-bit precision, and while most C compilers provide this as their `long double` type, MSVC (standard for Windows builds) makes `long double` identical to `double` (64 bits). NumPy makes the compiler's `long double` available as <span class="title-ref">numpy.longdouble</span> (and `np.clongdouble` for the complex numbers). You can find out what your numpy provides with `np.finfo(np.longdouble)`.

NumPy does not provide a dtype with more precision than C's `long double`; in particular, the 128-bit IEEE quad precision data type (FORTRAN's `REAL*16`) is not available.

For efficient memory alignment, <span class="title-ref">numpy.longdouble</span> is usually stored padded with zero bits, either to 96 or 128 bits. Which is more efficient depends on hardware and development environment; typically on 32-bit systems they are padded to 96 bits, while on 64-bit systems they are typically padded to 128 bits. `np.longdouble` is padded to the system default; `np.float96` and `np.float128` are provided for users who want specific padding. In spite of the names, `np.float96` and `np.float128` provide only as much precision as `np.longdouble`, that is, 80 bits on most x86 machines and 64 bits in standard Windows builds.

Be warned that even if <span class="title-ref">numpy.longdouble</span> offers more precision than python <span class="title-ref">float</span>, it is easy to lose that extra precision, since python often forces values to pass through `float`. For example, the `%` formatting operator requires its arguments to be converted to standard python types, and it is therefore impossible to preserve extended precision even if many decimal places are requested. It can be useful to test your code with the value `1 + np.finfo(np.longdouble).eps`.

---

basics.ufuncs.md

---

<div class="sectionauthor">

adapted from "Guide to NumPy" by Travis E. Oliphant

</div>

# Universal functions (<span class="title-ref">.ufunc</span>) basics

<div class="seealso">

\[ufuncs\](\#ufuncs)

</div>

A universal function (or `ufunc` for short) is a function that operates on <span class="title-ref">ndarrays \<numpy.ndarray\></span> in an element-by-element fashion, supporting \[array broadcasting \<ufuncs.broadcasting\>\](\#array-broadcasting-\<ufuncs.broadcasting\>), \[type casting \<ufuncs.casting\>\](\#type casting-\<ufuncs.casting\>), and several other standard features. That is, a ufunc is a "`vectorized <vectorization>`" wrapper for a function that takes a fixed number of specific inputs and produces a fixed number of specific outputs.

In NumPy, universal functions are instances of the <span class="title-ref">numpy.ufunc</span> class. Many of the built-in functions are implemented in compiled C code. The basic ufuncs operate on scalars, but there is also a generalized kind for which the basic elements are sub-arrays (vectors, matrices, etc.), and broadcasting is done over other dimensions. The simplest example is the addition operator:

    >>> np.array([0,2,3,4]) + np.array([1,1,-1,2])
    array([1, 3, 2, 6])

One can also produce custom <span class="title-ref">numpy.ufunc</span> instances using the <span class="title-ref">numpy.frompyfunc</span> factory function.

## Ufunc methods

All ufuncs have four methods. They can be found at \[ufuncs.methods\](\#ufuncs.methods). However, these methods only make sense on scalar ufuncs that take two input arguments and return one output argument. Attempting to call these methods on other ufuncs will cause a <span class="title-ref">ValueError</span>.

The reduce-like methods all take an *axis* keyword, a *dtype* keyword, and an *out* keyword, and the arrays must all have dimension \>= 1. The *axis* keyword specifies the axis of the array over which the reduction will take place (with negative values counting backwards). Generally, it is an integer, though for <span class="title-ref">numpy.ufunc.reduce</span>, it can also be a tuple of `int` to reduce over several axes at once, or `None`, to reduce over all axes. For example:

    >>> x = np.arange(9).reshape(3,3)
    >>> x
    array([[0, 1, 2],
          [3, 4, 5],
          [6, 7, 8]])
    >>> np.add.reduce(x, 1)
    array([ 3, 12, 21])
    >>> np.add.reduce(x, (0, 1))
    36

The *dtype* keyword allows you to manage a very common problem that arises when naively using <span class="title-ref">.ufunc.reduce</span>. Sometimes you may have an array of a certain data type and wish to add up all of its elements, but the result does not fit into the data type of the array. This commonly happens if you have an array of single-byte integers. The *dtype* keyword allows you to alter the data type over which the reduction takes place (and therefore the type of the output). Thus, you can ensure that the output is a data type with precision large enough to handle your output. The responsibility of altering the reduce type is mostly up to you. There is one exception: if no *dtype* is given for a reduction on the "add" or "multiply" operations, then if the input type is an integer (or Boolean) data-type and smaller than the size of the <span class="title-ref">numpy.int\_</span> data type, it will be internally upcast to the <span class="title-ref">.int\_</span> (or <span class="title-ref">numpy.uint</span>) data-type. In the previous example:

    >>> x.dtype 
    dtype('int64')
    >>> np.multiply.reduce(x, dtype=float)
    array([ 0., 28., 80.])

Finally, the *out* keyword allows you to provide an output array (or a tuple of output arrays for multi-output ufuncs). If *out* is given, the *dtype* argument is only used for the internal computations. Considering `x` from the previous example:

    >>> y = np.zeros(3, dtype=int)
    >>> y
    array([0, 0, 0])
    >>> np.multiply.reduce(x, dtype=float, out=y)
    array([ 0, 28, 80])

Ufuncs also have a fifth method, <span class="title-ref">numpy.ufunc.at</span>, that allows in place operations to be performed using advanced indexing. No \[buffering \<use-of-internal-buffers\>\](\#buffering-\<use-of-internal-buffers\>) is used on the dimensions where advanced indexing is used, so the advanced index can list an item more than once and the operation will be performed on the result of the previous operation for that item.

## Output type determination

The output of the ufunc (and its methods) is not necessarily an <span class="title-ref">ndarray \<numpy.ndarray\></span>, if all input arguments are not <span class="title-ref">ndarrays \<numpy.ndarray\></span>. Indeed, if any input defines an `~.class.__array_ufunc__` method, control will be passed completely to that function, i.e., the ufunc is \[overridden \<ufuncs.overrides\>\](\#overridden-\<ufuncs.overrides\>).

If none of the inputs overrides the ufunc, then all output arrays will be passed to the `~.class.__array_wrap__` method of the input (besides <span class="title-ref">ndarrays \<.ndarray\></span>, and scalars) that defines it **and** has the highest `~.class.__array_priority__` of any other input to the universal function. The default `~.class.__array_priority__` of the ndarray is 0.0, and the default `~.class.__array_priority__` of a subtype is 0.0. Matrices have `~.class.__array_priority__` equal to 10.0.

All ufuncs can also take output arguments which must be arrays or subclasses. If necessary, the result will be cast to the data-type(s) of the provided output array(s). If the output has an `~.class.__array_wrap__` method it is called instead of the one found on the inputs.

## Broadcasting

<div class="seealso">

\[Broadcasting basics \<basics.broadcasting\>\](Broadcasting basics \<basics.broadcasting\>.md)

</div>

<div class="index">

broadcasting

</div>

Each universal function takes array inputs and produces array outputs by performing the core function element-wise on the inputs (where an element is generally a scalar, but can be a vector or higher-order sub-array for generalized ufuncs). Standard \[broadcasting rules \<general-broadcasting-rules\>\](\#broadcasting-rules-\<general-broadcasting-rules\>) are applied so that inputs not sharing exactly the same shapes can still be usefully operated on.

By these rules, if an input has a dimension size of 1 in its shape, the first data entry in that dimension will be used for all calculations along that dimension. In other words, the stepping machinery of the `ufunc` will simply not step along that dimension (the \[stride \<memory-layout\>\](\#stride-\<memory-layout\>) will be 0 for that dimension).

## Type casting rules

<div class="index">

pair: ufunc; casting rules

</div>

\> **Note** \> In NumPy 1.6.0, a type promotion API was created to encapsulate the mechanism for determining output types. See the functions <span class="title-ref">numpy.result\_type</span>, <span class="title-ref">numpy.promote\_types</span>, and <span class="title-ref">numpy.min\_scalar\_type</span> for more details.

At the core of every ufunc is a one-dimensional strided loop that implements the actual function for a specific type combination. When a ufunc is created, it is given a static list of inner loops and a corresponding list of type signatures over which the ufunc operates. The ufunc machinery uses this list to determine which inner loop to use for a particular case. You can inspect the <span class="title-ref">.types \<.ufunc.types\></span> attribute for a particular ufunc to see which type combinations have a defined inner loop and which output type they produce (\[character codes \<arrays.scalars.character-codes\>\](\#character-codes-\<arrays.scalars.character-codes\>) are used in said output for brevity).

Casting must be done on one or more of the inputs whenever the ufunc does not have a core loop implementation for the input types provided. If an implementation for the input types cannot be found, then the algorithm searches for an implementation with a type signature to which all of the inputs can be cast "safely." The first one it finds in its internal list of loops is selected and performed, after all necessary type casting. Recall that internal copies during ufuncs (even for casting) are limited to the size of an internal buffer (which is user settable).

\> **Note** \> Universal functions in NumPy are flexible enough to have mixed type signatures. Thus, for example, a universal function could be defined that works with floating-point and integer values. See <span class="title-ref">numpy.ldexp</span> for an example.

By the above description, the casting rules are essentially implemented by the question of when a data type can be cast "safely" to another data type. The answer to this question can be determined in Python with a function call: <span class="title-ref">can\_cast(fromtype, totype) \<numpy.can\_cast\></span>. The example below shows the results of this call for the 24 internally supported types on the author's 64-bit system. You can generate this table for your system with the code given in the example.

**Example**

Code segment showing the "can cast safely" table for a 64-bit system. Generally the output depends on the system; your system might result in a different table.

\>\>\> mark = {False: ' -', True: ' Y'} \>\>\> def print\_table(ntypes): ... print('X ' + ' '.join(ntypes)) ... for row in ntypes: ... print(row, end='') ... for col in ntypes: ... print(mark\[np.can\_cast(row, col)\], end='') ... print() ... \>\>\> print\_table(np.typecodes\['All'\]) X ? b h i l q n p B H I L Q N P e f d g F D G S U V O M m ? Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y - Y b - Y Y Y Y Y Y Y - - - - - - - Y Y Y Y Y Y Y Y Y Y Y - Y h - - Y Y Y Y Y Y - - - - - - - - Y Y Y Y Y Y Y Y Y Y - Y i - - - Y Y Y Y Y - - - - - - - - - Y Y - Y Y Y Y Y Y - Y l - - - - Y Y Y Y - - - - - - - - - Y Y - Y Y Y Y Y Y - Y q - - - - Y Y Y Y - - - - - - - - - Y Y - Y Y Y Y Y Y - Y n - - - - Y Y Y Y - - - - - - - - - Y Y - Y Y Y Y Y Y - Y p - - - - Y Y Y Y - - - - - - - - - Y Y - Y Y Y Y Y Y - Y B - - Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y - Y H - - - Y Y Y Y Y - Y Y Y Y Y Y - Y Y Y Y Y Y Y Y Y Y - Y I - - - - Y Y Y Y - - Y Y Y Y Y - - Y Y - Y Y Y Y Y Y - Y L - - - - - - - - - - - Y Y Y Y - - Y Y - Y Y Y Y Y Y - -Q - - - - - - - - - - - Y Y Y Y - - Y Y - Y Y Y Y Y Y - -N - - - - - - - - - - - Y Y Y Y - - Y Y - Y Y Y Y Y Y - -P - - - - - - - - - - - Y Y Y Y - - Y Y - Y Y Y Y Y Y - -e - - - - - - - - - - - - - - - Y Y Y Y Y Y Y Y Y Y Y - -f - - - - - - - - - - - - - - - - Y Y Y Y Y Y Y Y Y Y - -d - - - - - - - - - - - - - - - - - Y Y - Y Y Y Y Y Y - -g - - - - - - - - - - - - - - - - - - Y - - Y Y Y Y Y - -F - - - - - - - - - - - - - - - - - - - Y Y Y Y Y Y Y - -D - - - - - - - - - - - - - - - - - - - - Y Y Y Y Y Y - -G - - - - - - - - - - - - - - - - - - - - - Y Y Y Y Y - -S - - - - - - - - - - - - - - - - - - - - - - Y Y Y Y - -U - - - - - - - - - - - - - - - - - - - - - - - Y Y Y - -V - - - - - - - - - - - - - - - - - - - - - - - - Y Y - -O - - - - - - - - - - - - - - - - - - - - - - - - - Y - -M - - - - - - - - - - - - - - - - - - - - - - - - Y Y Y -m - - - - - - - - - - - - - - - - - - - - - - - - Y Y - Y

You should note that, while included in the table for completeness, the 'S', 'U', and 'V' types cannot be operated on by ufuncs. Also, note that on a 32-bit system the integer types may have different sizes, resulting in a slightly altered table.

Mixed scalar-array operations use a different set of casting rules that ensure that a scalar cannot "upcast" an array unless the scalar is of a fundamentally different kind of data (i.e., under a different hierarchy in the data-type hierarchy) than the array. This rule enables you to use scalar constants in your code (which, as Python types, are interpreted accordingly in ufuncs) without worrying about whether the precision of the scalar constant will cause upcasting on your large (small precision) array.

## Use of internal buffers

<div class="index">

buffers

</div>

Internally, buffers are used for misaligned data, swapped data, and data that has to be converted from one data type to another. The size of internal buffers is settable on a per-thread basis. There can be up to \(2 (n_{\mathrm{inputs}} + n_{\mathrm{outputs}})\) buffers of the specified size created to handle the data from all the inputs and outputs of a ufunc. The default size of a buffer is 10,000 elements. Whenever buffer-based calculation would be needed, but all input arrays are smaller than the buffer size, those misbehaved or incorrectly-typed arrays will be copied before the calculation proceeds. Adjusting the size of the buffer may therefore alter the speed at which ufunc calculations of various sorts are completed. A simple interface for setting this variable is accessible using the function <span class="title-ref">numpy.setbufsize</span>.

## Error handling

<div class="index">

error handling

</div>

Universal functions can trip special floating-point status registers in your hardware (such as divide-by-zero). If available on your platform, these registers will be regularly checked during calculation. Error handling is controlled on a per-thread basis, and can be configured using the functions <span class="title-ref">numpy.seterr</span> and <span class="title-ref">numpy.seterrcall</span>.

## Overriding ufunc behavior

Classes (including ndarray subclasses) can override how ufuncs act on them by defining certain special methods. For details, see \[arrays.classes\](\#arrays.classes).

---

byteswapping.md

---

# Byte-swapping

## Introduction to byte ordering and ndarrays

The `ndarray` is an object that provides a python array interface to data in memory.

It often happens that the memory that you want to view with an array is not of the same byte ordering as the computer on which you are running Python.

For example, I might be working on a computer with a little-endian CPU -such as an Intel Pentium, but I have loaded some data from a file written by a computer that is big-endian. Let's say I have loaded 4 bytes from a file written by a Sun (big-endian) computer. I know that these 4 bytes represent two 16-bit integers. On a big-endian machine, a two-byte integer is stored with the Most Significant Byte (MSB) first, and then the Least Significant Byte (LSB). Thus the bytes are, in memory order:

1.  MSB integer 1
2.  LSB integer 1
3.  MSB integer 2
4.  LSB integer 2

Let's say the two integers were in fact 1 and 770. Because 770 = 256 \* 3 + 2, the 4 bytes in memory would contain respectively: 0, 1, 3, 2. The bytes I have loaded from the file would have these contents:

\>\>\> big\_end\_buffer = bytearray(\[0,1,3,2\]) \>\>\> big\_end\_buffer bytearray(b'x00x01x03x02')

We might want to use an `ndarray` to access these integers. In that case, we can create an array around this memory, and tell numpy that there are two integers, and that they are 16 bit and big-endian:

\>\>\> import numpy as np \>\>\> big\_end\_arr = np.ndarray(shape=(2,),dtype='\>i2', buffer=big\_end\_buffer) \>\>\> big\_end\_arr\[0\] np.int16(1) \>\>\> big\_end\_arr\[1\] np.int16(770)

Note the array `dtype` above of `>i2`. The `>` means 'big-endian' (`<` is little-endian) and `i2` means 'signed 2-byte integer'. For example, if our data represented a single unsigned 4-byte little-endian integer, the dtype string would be `<u4`.

In fact, why don't we try that?

\>\>\> little\_end\_u4 = np.ndarray(shape=(1,),dtype='\<u4', buffer=big\_end\_buffer) \>\>\> little\_end\_u4\[0\] == 1 \* 256\**1 + 3* 256\**2 + 2* 256\*\*3 True

Returning to our `big_end_arr` - in this case our underlying data is big-endian (data endianness) and we've set the dtype to match (the dtype is also big-endian). However, sometimes you need to flip these around.

\> **Warning** \> Scalars do not include byte order information, so extracting a scalar from an array will return an integer in native byte order. Hence:

> \>\>\> big\_end\_arr\[0\].dtype.byteorder == little\_end\_u4\[0\].dtype.byteorder True
> 
> NumPy intentionally does not attempt to always preserve byte-order and for example converts to native byte-order in <span class="title-ref">numpy.concatenate</span>.

## Changing byte ordering

As you can imagine from the introduction, there are two ways you can affect the relationship between the byte ordering of the array and the underlying memory it is looking at:

  - Change the byte-ordering information in the array dtype so that it interprets the underlying data as being in a different byte order. This is the role of `arr.view(arr.dtype.newbyteorder())`
  - Change the byte-ordering of the underlying data, leaving the dtype interpretation as it was. This is what `arr.byteswap()` does.

The common situations in which you need to change byte ordering are:

1.  Your data and dtype endianness don't match, and you want to change the dtype so that it matches the data.
2.  Your data and dtype endianness don't match, and you want to swap the data so that they match the dtype
3.  Your data and dtype endianness match, but you want the data swapped and the dtype to reflect this

### Data and dtype endianness don't match, change dtype to match data

We make something where they don't match:

\>\>\> wrong\_end\_dtype\_arr = np.ndarray(shape=(2,),dtype='\<i2', buffer=big\_end\_buffer) \>\>\> wrong\_end\_dtype\_arr\[0\] np.int16(256)

The obvious fix for this situation is to change the dtype so it gives the correct endianness:

\>\>\> fixed\_end\_dtype\_arr = wrong\_end\_dtype\_arr.view(np.dtype('\<i2').newbyteorder()) \>\>\> fixed\_end\_dtype\_arr\[0\] np.int16(1)

Note the array has not changed in memory:

\>\>\> fixed\_end\_dtype\_arr.tobytes() == big\_end\_buffer True

### Data and type endianness don't match, change data to match dtype

You might want to do this if you need the data in memory to be a certain ordering. For example you might be writing the memory out to a file that needs a certain byte ordering.

\>\>\> fixed\_end\_mem\_arr = wrong\_end\_dtype\_arr.byteswap() \>\>\> fixed\_end\_mem\_arr\[0\] np.int16(1)

Now the array *has* changed in memory:

\>\>\> fixed\_end\_mem\_arr.tobytes() == big\_end\_buffer False

### Data and dtype endianness match, swap data and dtype

You may have a correctly specified array dtype, but you need the array to have the opposite byte order in memory, and you want the dtype to match so the array values make sense. In this case you just do both of the previous operations:

\>\>\> swapped\_end\_arr = big\_end\_arr.byteswap() \>\>\> swapped\_end\_arr = swapped\_end\_arr.view(swapped\_end\_arr.dtype.newbyteorder()) \>\>\> swapped\_end\_arr\[0\] np.int16(1) \>\>\> swapped\_end\_arr.tobytes() == big\_end\_buffer False

An easier way of casting the data to a specific dtype and byte ordering can be achieved with the ndarray astype method:

\>\>\> swapped\_end\_arr = big\_end\_arr.astype('\<i2') \>\>\> swapped\_end\_arr\[0\] np.int16(1) \>\>\> swapped\_end\_arr.tobytes() == big\_end\_buffer False

---

c-info.beyond-basics.md

---

# Beyond the basics

Â Â Â The voyage of discovery is not in seeking new landscapes but in having  
Â Â Â new eyes.  
Â Â Â --- *Marcel Proust*

Â Â Â Discovery is seeing what everyone else has seen and thinking what no  
Â Â Â one else has thought.  
Â Â Â --- *Albert Szent-Gyorgi*

## Iterating over elements in the array

### Basic iteration

One common algorithmic requirement is to be able to walk over all elements in a multidimensional array. The array iterator object makes this easy to do in a generic way that works for arrays of any dimension. Naturally, if you know the number of dimensions you will be using, then you can always write nested for loops to accomplish the iteration. If, however, you want to write code that works with any number of dimensions, then you can make use of the array iterator. An array iterator object is returned when accessing the .flat attribute of an array.

<div class="index">

single: array iterator

</div>

Basic usage is to call :c\`PyArray\_IterNew\` ( `array` ) where array is an ndarray object (or one of its sub-classes). The returned object is an array-iterator object (the same object returned by the .flat attribute of the ndarray). This object is usually cast to PyArrayIterObject\* so that its members can be accessed. The only members that are needed are `iter->size` which contains the total size of the array, `iter->index`, which contains the current 1-d index into the array, and `iter->dataptr` which is a pointer to the data for the current element of the array. Sometimes it is also useful to access `iter->ao` which is a pointer to the underlying ndarray object.

After processing data at the current element of the array, the next element of the array can be obtained using the macro :c\`PyArray\_ITER\_NEXT\` ( `iter` ). The iteration always proceeds in a C-style contiguous fashion (last index varying the fastest). The :c\`PyArray\_ITER\_GOTO\` ( `iter`, `destination` ) can be used to jump to a particular point in the array, where `destination` is an array of npy\_intp data-type with space to handle at least the number of dimensions in the underlying array. Occasionally it is useful to use :c\`PyArray\_ITER\_GOTO1D\` ( `iter`, `index` ) which will jump to the 1-d index given by the value of `index`. The most common usage, however, is given in the following example.

`` `c     PyObject *obj; /* assumed to be some ndarray object */     PyArrayIterObject *iter;     ...     iter = (PyArrayIterObject *)PyArray_IterNew(obj);     if (iter == NULL) goto fail;   /* Assume fail has clean-up code */     while (iter->index < iter->size) {         /* do something with the data at it->dataptr */         PyArray_ITER_NEXT(it);     }     ...  You can also use :c`PyArrayIter_Check` ( ``obj`) to ensure you have`<span class="title-ref"> an iterator object and :c\`PyArray\_ITER\_RESET</span> ( `iter` ) to reset an iterator object back to the beginning of the array.

It should be emphasized at this point that you may not need the array iterator if your array is already contiguous (using an array iterator will work but will be slower than the fastest code you could write). The major purpose of array iterators is to encapsulate iteration over N-dimensional arrays with arbitrary strides. They are used in many, many places in the NumPy source code itself. If you already know your array is contiguous (Fortran or C), then simply adding the element-size to a running pointer variable will step you through the array very efficiently. In other words, code like this will probably be faster for you in the contiguous case (assuming doubles).

`` `c     npy_intp size;     double *dptr;  /* could make this any variable type */     size = PyArray_SIZE(obj);     dptr = PyArray_DATA(obj);     while(size--) {        /* do something with the data at dptr */        dptr++;     }   Iterating over all but one axis ``\` -------------------------------

A common algorithm is to loop over all elements of an array and perform some function with each element by issuing a function call. As function calls can be time consuming, one way to speed up this kind of algorithm is to write the function so it takes a vector of data and then write the iteration so the function call is performed for an entire dimension of data at a time. This increases the amount of work done per function call, thereby reducing the function-call over-head to a small(er) fraction of the total time. Even if the interior of the loop is performed without a function call it can be advantageous to perform the inner loop over the dimension with the highest number of elements to take advantage of speed enhancements available on micro-processors that use pipelining to enhance fundamental operations.

The :c\`PyArray\_IterAllButAxis\` ( `array`, `&dim` ) constructs an iterator object that is modified so that it will not iterate over the dimension indicated by dim. The only restriction on this iterator object, is that the :c\`PyArray\_ITER\_GOTO1D\` ( `it`, `ind` ) macro cannot be used (thus flat indexing won't work either if you pass this object back to Python --- so you shouldn't do this). Note that the returned object from this routine is still usually cast to PyArrayIterObject \*. All that's been done is to modify the strides and dimensions of the returned iterator to simulate iterating over array\[...,0,...\] where 0 is placed on the \(\textrm{dim}^{\textrm{th}}\) dimension. If dim is negative, then the dimension with the largest axis is found and used.

### Iterating over multiple arrays

Very often, it is desirable to iterate over several arrays at the same time. The universal functions are an example of this kind of behavior. If all you want to do is iterate over arrays with the same shape, then simply creating several iterator objects is the standard procedure. For example, the following code iterates over two arrays assumed to be the same shape and size (actually obj1 just has to have at least as many total elements as does obj2):

`` `c     /* It is already assumed that obj1 and obj2        are ndarrays of the same shape and size.     */     iter1 = (PyArrayIterObject *)PyArray_IterNew(obj1);     if (iter1 == NULL) goto fail;     iter2 = (PyArrayIterObject *)PyArray_IterNew(obj2);     if (iter2 == NULL) goto fail;  /* assume iter1 is DECREF'd at fail */     while (iter2->index < iter2->size)  {         /* process with iter1->dataptr and iter2->dataptr */         PyArray_ITER_NEXT(iter1);         PyArray_ITER_NEXT(iter2);     }   Broadcasting over multiple arrays ``\` ---------------------------------

<div class="index">

single: broadcasting

</div>

When multiple arrays are involved in an operation, you may want to use the same broadcasting rules that the math operations (*i.e.* the ufuncs) use. This can be done easily using the :c`PyArrayMultiIterObject`. This is the object returned from the Python command numpy.broadcast and it is almost as easy to use from C. The function :c\`PyArray\_MultiIterNew\` ( `n`, `...` ) is used (with `n` input objects in place of `...` ). The input objects can be arrays or anything that can be converted into an array. A pointer to a PyArrayMultiIterObject is returned. Broadcasting has already been accomplished which adjusts the iterators so that all that needs to be done to advance to the next element in each array is for PyArray\_ITER\_NEXT to be called for each of the inputs. This incrementing is automatically performed by :c\`PyArray\_MultiIter\_NEXT\` ( `obj` ) macro (which can handle a multiterator `obj` as either a :c`PyArrayMultiIterObject *` or a :c`PyObject *`). The data from input number `i` is available using :c\`PyArray\_MultiIter\_DATA\` ( `obj`, `i` ). An example of using this feature follows.

`` `c     mobj = PyArray_MultiIterNew(2, obj1, obj2);     size = mobj->size;     while(size--) {         ptr1 = PyArray_MultiIter_DATA(mobj, 0);         ptr2 = PyArray_MultiIter_DATA(mobj, 1);         /* code using contents of ptr1 and ptr2 */         PyArray_MultiIter_NEXT(mobj);     }  The function :c`PyArray_RemoveSmallest` ( ``multi`) can be used to`\` take a multi-iterator object and adjust all the iterators so that iteration does not take place over the largest dimension (it makes that dimension of size 1). The code being looped over that makes use of the pointers will very-likely also need the strides data for each of the iterators. This information is stored in multi-\>iters\[i\]-\>strides.

<div class="index">

single: array iterator

</div>

There are several examples of using the multi-iterator in the NumPy source code as it makes N-dimensional broadcasting-code very simple to write. Browse the source for more examples.

## User-defined data-types

NumPy comes with 24 builtin data-types. While this covers a large majority of possible use cases, it is conceivable that a user may have a need for an additional data-type. There is some support for adding an additional data-type into the NumPy system. This additional data-type will behave much like a regular data-type except ufuncs must have 1-d loops registered to handle it separately. Also checking for whether or not other data-types can be cast "safely" to and from this new type or not will always return "can cast" unless you also register which types your new data-type can be cast to and from.

The NumPy source code includes an example of a custom data-type as part of its test suite. The file `_rational_tests.c.src` in the source code directory `numpy/_core/src/umath/` contains an implementation of a data-type that represents a rational number as the ratio of two 32 bit integers.

<div class="index">

pair: dtype; adding new

</div>

### Adding the new data-type

To begin to make use of the new data-type, you need to first define a new Python type to hold the scalars of your new data-type. It should be acceptable to inherit from one of the array scalars if your new type has a binary compatible layout. This will allow your new data type to have the methods and attributes of array scalars. New data-types must have a fixed memory size (if you want to define a data-type that needs a flexible representation, like a variable-precision number, then use a pointer to the object as the data-type). The memory layout of the object structure for the new Python type must be PyObject\_HEAD followed by the fixed-size memory needed for the data-type. For example, a suitable structure for the new Python type is:

`` `c     typedef struct {        PyObject_HEAD;        some_data_type obval;        /* the name can be whatever you want */     } PySomeDataTypeObject;  After you have defined a new Python type object, you must then define ``<span class="title-ref"> a new :c:type:\`PyArray\_Descr</span> structure whose typeobject member will contain a pointer to the data-type you've just defined. In addition, the required functions in the ".f" member must be defined: nonzero, copyswap, copyswapn, setitem, getitem, and cast. The more functions in the ".f" member you define, however, the more useful the new data-type will be. It is very important to initialize unused functions to NULL. This can be achieved using :c\`PyArray\_InitArrFuncs\` (f).

Once a new :c`PyArray_Descr` structure is created and filled with the needed information and useful functions you call :c\`PyArray\_RegisterDataType\` (new\_descr). The return value from this call is an integer providing you with a unique type\_number that specifies your data-type. This type number should be stored and made available by your module so that other modules can use it to recognize your data-type.

Note that this API is inherently thread-unsafe. See <span class="title-ref">thread\_safety</span> for more details about thread safety in NumPy.

### Registering a casting function

You may want to allow builtin (and other user-defined) data-types to be cast automatically to your data-type. In order to make this possible, you must register a casting function with the data-type you want to be able to cast from. This requires writing low-level casting functions for each conversion you want to support and then registering these functions with the data-type descriptor. A low-level casting function has the signature.

An example castfunc is:

`` `c     static void     double_to_float(double *from, float* to, npy_intp n,                     void* ignore1, void* ignore2) {         while (n--) {               (*to++) = (double) *(from++);         }     }  This could then be registered to convert doubles to floats using the ``\` code:

`` `c     doub = PyArray_DescrFromType(NPY_DOUBLE);     PyArray_RegisterCastFunc(doub, NPY_FLOAT,          (PyArray_VectorUnaryFunc *)double_to_float);     Py_DECREF(doub);   Registering coercion rules ``\` --------------------------

By default, all user-defined data-types are not presumed to be safely castable to any builtin data-types. In addition builtin data-types are not presumed to be safely castable to user-defined data-types. This situation limits the ability of user-defined data-types to participate in the coercion system used by ufuncs and other times when automatic coercion takes place in NumPy. This can be changed by registering data-types as safely castable from a particular data-type object. The function :c\`PyArray\_RegisterCanCast\` (from\_descr, totype\_number, scalarkind) should be used to specify that the data-type object from\_descr can be cast to the data-type with type number totype\_number. If you are not trying to alter scalar coercion rules, then use :c`NPY_NOSCALAR` for the scalarkind argument.

If you want to allow your new data-type to also be able to share in the scalar coercion rules, then you need to specify the scalarkind function in the data-type object's ".f" member to return the kind of scalar the new data-type should be seen as (the value of the scalar is available to that function). Then, you can register data-types that can be cast to separately for each scalar kind that may be returned from your user-defined data-type. If you don't register scalar coercion handling, then all of your user-defined data-types will be seen as :c`NPY_NOSCALAR`.

### Registering a ufunc loop

You may also want to register low-level ufunc loops for your data-type so that an ndarray of your data-type can have math applied to it seamlessly. Registering a new loop with exactly the same arg\_types signature, silently replaces any previously registered loops for that data-type.

Before you can register a 1-d loop for a ufunc, the ufunc must be previously created. Then you call :c\`PyUFunc\_RegisterLoopForType\` (...) with the information needed for the loop. The return value of this function is `0` if the process was successful and `-1` with an error condition set if it was not successful.

<div class="index">

pair: dtype; adding new

</div>

## Subtyping the ndarray in C

One of the lesser-used features that has been lurking in Python since 2.2 is the ability to sub-class types in C. This facility is one of the important reasons for basing NumPy off of the Numeric code-base which was already in C. A sub-type in C allows much more flexibility with regards to memory management. Sub-typing in C is not difficult even if you have only a rudimentary understanding of how to create new types for Python. While it is easiest to sub-type from a single parent type, sub-typing from multiple parent types is also possible. Multiple inheritance in C is generally less useful than it is in Python because a restriction on Python sub-types is that they have a binary compatible memory layout. Perhaps for this reason, it is somewhat easier to sub-type from a single parent type.

<div class="index">

pair: ndarray; subtyping

</div>

All C-structures corresponding to Python objects must begin with :c`PyObject_HEAD` (or :c`PyObject_VAR_HEAD`). In the same way, any sub-type must have a C-structure that begins with exactly the same memory layout as the parent type (or all of the parent types in the case of multiple-inheritance). The reason for this is that Python may attempt to access a member of the sub-type structure as if it had the parent structure ( *i.e.* it will cast a given pointer to a pointer to the parent structure and then dereference one of it's members). If the memory layouts are not compatible, then this attempt will cause unpredictable behavior (eventually leading to a memory violation and program crash).

One of the elements in :c`PyObject_HEAD` is a pointer to a type-object structure. A new Python type is created by creating a new type-object structure and populating it with functions and pointers to describe the desired behavior of the type. Typically, a new C-structure is also created to contain the instance-specific information needed for each object of the type as well. For example, :c\`\&PyArray\_Type\<PyArray\_Type\><span class="title-ref"> is a pointer to the type-object table for the ndarray while a :c:expr:\`PyArrayObject \*</span> variable is a pointer to a particular instance of an ndarray (one of the members of the ndarray structure is, in turn, a pointer to the type- object table :c\`\&PyArray\_Type\<PyArray\_Type\><span class="title-ref">). Finally :c\`PyType\_Ready</span> (\<pointer\_to\_type\_object\>) must be called for every new Python type.

### Creating sub-types

To create a sub-type, a similar procedure must be followed except only behaviors that are different require new entries in the type-object structure. All other entries can be NULL and will be filled in by :c\`PyType\_Ready\` with appropriate functions from the parent type(s). In particular, to create a sub-type in C follow these steps:

1.  If needed create a new C-structure to handle each instance of your type. A typical C-structure would be:
    
      - \`\`\`c
        
          - typedef \_new\_struct {  
            PyArrayObject base; /\* new things here \*/
        
        } NewArrayObject;
    
    Notice that the full PyArrayObject is used as the first entry in order to ensure that the binary layout of instances of the new type is identical to the PyArrayObject.

2.  Fill in a new Python type-object structure with pointers to new functions that will over-ride the default behavior while leaving any function that should remain the same unfilled (or NULL). The tp\_name element should be different.

3.  Fill in the tp\_base member of the new type-object structure with a pointer to the (main) parent type object. For multiple-inheritance, also fill in the tp\_bases member with a tuple containing all of the parent objects in the order they should be used to define inheritance. Remember, all parent-types must have the same C-structure for multiple inheritance to work properly.

4.  Call :c\`PyType\_Ready\` (\<pointer\_to\_new\_type\>). If this function returns a negative number, a failure occurred and the type is not initialized. Otherwise, the type is ready to be used. It is generally important to place a reference to the new type into the module dictionary so it can be accessed from Python.

More information on creating sub-types in C can be learned by reading `` ` PEP 253 (available at https://www.python.org/dev/peps/pep-0253).  .. _specific-array-subtyping:  Specific features of ndarray sub-typing ---------------------------------------  Some special methods and attributes are used by arrays in order to facilitate the interoperation of sub-types with the base ndarray type.  The __array_finalize\__ method ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. attribute:: ndarray.__array_finalize__     Several array-creation functions of the ndarray allow    specification of a particular sub-type to be created. This allows    sub-types to be handled seamlessly in many routines. When a    sub-type is created in such a fashion, however, neither the    __new_\_ method nor the __init\__ method gets called. Instead, the    sub-type is allocated and the appropriate instance-structure    members are filled in. Finally, the :obj:`~numpy.class.__array_finalize__`    attribute is looked-up in the object dictionary. If it is present and not    None, then it can be either a :c:type:`PyCapsule` containing a pointer to a    :c`PyArray_FinalizeFunc` or it can be a method taking a single argument    (which could be None)     If the :obj:`~numpy.class.__array_finalize__` attribute is a    :c:type:`PyCapsule`, then the pointer must be a pointer to a function with    the signature: ``\`c (int) (PyArrayObject *, PyObject*)

> The first argument is the newly created sub-type. The second argument (if not NULL) is the "parent" array (if the array was created using slicing or some other operation where a clearly-distinguishable parent is present). This routine can do anything it wants to. It should return a -1 on error and 0 otherwise.
> 
> If the `~numpy.class.__array_finalize__` attribute is not None nor a :c`PyCapsule`, then it must be a Python method that takes the parent array as an argument (which could be None if there is no parent), and returns nothing. Errors in this method will be caught and handled.

The \_\_array\_priority\_\_ attribute \`\`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

<div class="attribute">

ndarray.\_\_array\_priority\_\_

This attribute allows simple but flexible determination of which sub-type should be considered "primary" when an operation involving two or more sub-types arises. In operations where different sub-types are being used, the sub-type with the largest `~numpy.class.__array_priority__` attribute will determine the sub-type of the output(s). If two sub-types have the same `~numpy.class.__array_priority__` then the sub-type of the first argument determines the output. The default `~numpy.class.__array_priority__` attribute returns a value of 0.0 for the base ndarray type and 1.0 for a sub-type. This attribute can also be defined by objects that are not sub-types of the ndarray and can be used to determine which `~numpy.class.__array_wrap__` method should be called for the return output.

</div>

#### The \_\_array\_wrap\_\_ method

<div class="attribute">

ndarray.\_\_array\_wrap\_\_

Any class or type can define this method which should take an ndarray argument and return an instance of the type. It can be seen as the opposite of the `~numpy.class.__array__` method. This method is used by the ufuncs (and other NumPy functions) to allow other objects to pass through. For Python \>2.4, it can also be used to write a decorator that converts a function that works only with ndarrays to one that works with any type with `~numpy.class.__array__` and `~numpy.class.__array_wrap__` methods.

</div>

<div class="index">

pair: ndarray; subtyping

</div>

---

c-info.how-to-extend.md

---

# How to extend NumPy

Â Â Â That which is static and repetitive is boring. That which is dynamic  
Â Â Â and random is confusing. In between lies art.  
Â Â Â --- *John A. Locke*

Â Â Â Science is a differential equation. Religion is a boundary condition.  
Â Â Â --- *Alan Turing*

## Writing an extension module

While the ndarray object is designed to allow rapid computation in Python, it is also designed to be general-purpose and satisfy a wide-variety of computational needs. As a result, if absolute speed is essential, there is no replacement for a well-crafted, compiled loop specific to your application and hardware. This is one of the reasons that numpy includes f2py so that an easy-to-use mechanisms for linking (simple) C/C++ and (arbitrary) Fortran code directly into Python are available. You are encouraged to use and improve this mechanism. The purpose of this section is not to document this tool but to document the more basic steps to writing an extension module that this tool depends on.

<div class="index">

single: extension module

</div>

When an extension module is written, compiled, and installed to somewhere in the Python path (sys.path), the code can then be imported into Python as if it were a standard python file. It will contain objects and methods that have been defined and compiled in C code. The basic steps for doing this in Python are well-documented and you can find more information in the documentation for Python itself available online at [www.python.org](https://www.python.org) .

In addition to the Python C-API, there is a full and rich C-API for NumPy allowing sophisticated manipulations on a C-level. However, for most applications, only a few API calls will typically be used. For example, if you need to just extract a pointer to memory along with some shape information to pass to another calculation routine, then you will use very different calls than if you are trying to create a new array-like type or add a new data type for ndarrays. This chapter documents the API calls and macros that are most commonly used.

## Required subroutine

There is exactly one function that must be defined in your C-code in order for Python to use it as an extension module. The function must be called init{name} where {name} is the name of the module from Python. This function must be declared so that it is visible to code outside of the routine. Besides adding the methods and constants you desire, this subroutine must also contain calls like `import_array()` and/or `import_ufunc()` depending on which C-API is needed. Forgetting to place these commands will show itself as an ugly segmentation fault (crash) as soon as any C-API subroutine is actually called. It is actually possible to have multiple init{name} functions in a single file in which case multiple modules will be defined by that file. However, there are some tricks to get that to work correctly and it is not covered here.

A minimal `init{name}` method looks like:

`` `c     PyMODINIT_FUNC     init{name}(void)     {        (void)Py_InitModule({name}, mymethods);        import_array();     }  The mymethods must be an array (usually statically declared) of ``\` PyMethodDef structures which contain method names, actual C-functions, a variable indicating whether the method uses keyword arguments or not, and docstrings. These are explained in the next section. If you want to add constants to the module, then you store the returned value from Py\_InitModule which is a module object. The most general way to add items to the module is to get the module dictionary using PyModule\_GetDict(module). With the module dictionary, you can add whatever you like to the module manually. An easier way to add objects to the module is to use one of three additional Python C-API calls that do not require a separate extraction of the module dictionary. These are documented in the Python documentation, but repeated here for convenience:

## Defining functions

The second argument passed in to the Py\_InitModule function is a structure that makes it easy to define functions in the module. In the example given above, the mymethods structure would have been defined earlier in the file (usually right before the init{name} subroutine) to:

`` `c     static PyMethodDef mymethods[] = {         { nokeywordfunc,nokeyword_cfunc,           METH_VARARGS,           Doc string},         { keywordfunc, keyword_cfunc,           METH_VARARGS|METH_KEYWORDS,           Doc string},         {NULL, NULL, 0, NULL} /* Sentinel */     }  Each entry in the mymethods array is a :c:type:`PyMethodDef` structure ``\` containing 1) the Python name, 2) the C-function that implements the function, 3) flags indicating whether or not keywords are accepted for this function, and 4) The docstring for the function. Any number of functions may be defined for a single module by adding more entries to this table. The last entry must be all NULL as shown to act as a sentinel. Python looks for this entry to know that all of the functions for the module have been defined.

The last thing that must be done to finish the extension module is to actually write the code that performs the desired functions. There are two kinds of functions: those that don't accept keyword arguments, and those that do.

### Functions without keyword arguments

Functions that don't accept keyword arguments should be written as:

`` `c     static PyObject*     nokeyword_cfunc (PyObject *dummy, PyObject *args)     {         /* convert Python arguments */         /* do function */         /* return something */     }  The dummy argument is not used in this context and can be safely ``<span class="title-ref"> ignored. The \*args\* argument contains all of the arguments passed in to the function as a tuple. You can do anything you want at this point, but usually the easiest way to manage the input arguments is to call :c\`PyArg\_ParseTuple</span> (args, format\_string, addresses\_to\_C\_variables...) or :c\`PyArg\_UnpackTuple\` (tuple, "name", min, max, ...). A good description of how to use the first function is contained in the Python C-API reference manual under section 5.5 (Parsing arguments and building values). You should pay particular attention to the "O&" format which uses converter functions to go between the Python object and the C object. All of the other format functions can be (mostly) thought of as special cases of this general rule. There are several converter functions defined in the NumPy C-API that may be of use. In particular, the :c\`PyArray\_DescrConverter\` function is very useful to support arbitrary data-type specification. This function transforms any valid data-type Python object into a :c`PyArray_Descr *` object. Remember to pass in the address of the C-variables that should be filled in.

There are lots of examples of how to use :c\`PyArg\_ParseTuple\` throughout the NumPy source code. The standard usage is like this:

`` `c     PyObject *input;     PyArray_Descr *dtype;     if (!PyArg_ParseTuple(args, "OO&", &input,                           PyArray_DescrConverter,                           &dtype)) return NULL;  It is important to keep in mind that you get a *borrowed* reference to ``<span class="title-ref"> the object when using the "O" format string. However, the converter functions usually require some form of memory handling. In this example, if the conversion is successful, \*dtype\* will hold a new reference to a :c:expr:\`PyArray\_Descr \*</span> object, while *input* will hold a borrowed reference. Therefore, if this conversion were mixed with another conversion (say to an integer) and the data-type conversion was successful but the integer conversion failed, then you would need to release the reference count to the data-type object before returning. A typical way to do this is to set *dtype* to `NULL` before calling :c\`PyArg\_ParseTuple\` and then use :c\`Py\_XDECREF\` on *dtype* before returning.

After the input arguments are processed, the code that actually does the work is written (likely calling other functions as needed). The final step of the C-function is to return something. If an error is encountered then `NULL` should be returned (making sure an error has actually been set). If nothing should be returned then increment :c\`Py\_None\` and return it. If a single object should be returned then it is returned (ensuring that you own a reference to it first). If multiple objects should be returned then you need to return a tuple. The :c\`Py\_BuildValue\` (format\_string, c\_variables...) function makes it easy to build tuples of Python objects from C variables. Pay special attention to the difference between 'N' and 'O' in the format string or you can easily create memory leaks. The 'O' format string increments the reference count of the :c`PyObject *` C-variable it corresponds to, while the 'N' format string steals a reference to the corresponding :c`PyObject *` C-variable. You should use 'N' if you have already created a reference for the object and just want to give that reference to the tuple. You should use 'O' if you only have a borrowed reference to an object and need to create one to provide for the tuple.

### Functions with keyword arguments

These functions are very similar to functions without keyword arguments. The only difference is that the function signature is:

`` `c     static PyObject*     keyword_cfunc (PyObject *dummy, PyObject *args, PyObject *kwds)     {     ...     }  The kwds argument holds a Python dictionary whose keys are the names ``<span class="title-ref"> of the keyword arguments and whose values are the corresponding keyword-argument values. This dictionary can be processed however you see fit. The easiest way to handle it, however, is to replace the :c\`PyArg\_ParseTuple</span> (args, format\_string, addresses...) function with a call to :c\`PyArg\_ParseTupleAndKeywords\` (args, kwds, format\_string, char \*kwlist\[\], addresses...). The kwlist parameter to this function is a `NULL` -terminated array of strings providing the expected keyword arguments. There should be one string for each entry in the format\_string. Using this function will raise a TypeError if invalid keyword arguments are passed in.

For more help on this function please see section 1.8 (Keyword Parameters for Extension Functions) of the Extending and Embedding tutorial in the Python documentation.

### Reference counting

The biggest difficulty when writing extension modules is reference counting. It is an important reason for the popularity of f2py, weave, Cython, ctypes, etc.... If you mis-handle reference counts you can get problems from memory-leaks to segmentation faults. The only strategy I know of to handle reference counts correctly is blood, sweat, and tears. First, you force it into your head that every Python variable has a reference count. Then, you understand exactly what each function does to the reference count of your objects, so that you can properly use DECREF and INCREF when you need them. Reference counting can really test the amount of patience and diligence you have towards your programming craft. Despite the grim depiction, most cases of reference counting are quite straightforward with the most common difficulty being not using DECREF on objects before exiting early from a routine due to some error. In second place, is the common error of not owning the reference on an object that is passed to a function or macro that is going to steal the reference ( *e.g.* :c\`PyTuple\_SET\_ITEM\`, and most functions that take :c`PyArray_Descr` objects).

<div class="index">

single: reference counting

</div>

Typically you get a new reference to a variable when it is created or is the return value of some function (there are some prominent exceptions, however --- such as getting an item out of a tuple or a dictionary). When you own the reference, you are responsible to make sure that :c\`Py\_DECREF\` (var) is called when the variable is no longer necessary (and no other function has "stolen" its reference). Also, if you are passing a Python object to a function that will "steal" the reference, then you need to make sure you own it (or use :c\`Py\_INCREF\` to get your own reference). You will also encounter the notion of borrowing a reference. A function that borrows a reference does not alter the reference count of the object and does not expect to "hold on "to the reference. It's just going to use the object temporarily. When you use :c\`PyArg\_ParseTuple\` or :c\`PyArg\_UnpackTuple\` you receive a borrowed reference to the objects in the tuple and should not alter their reference count inside your function. With practice, you can learn to get reference counting right, but it can be frustrating at first.

One common source of reference-count errors is the :c\`Py\_BuildValue\` function. Pay careful attention to the difference between the 'N' format character and the 'O' format character. If you create a new object in your subroutine (such as an output array), and you are passing it back in a tuple of return values, then you should most-likely use the 'N' format character in :c\`Py\_BuildValue\`. The 'O' character will increase the reference count by one. This will leave the caller with two reference counts for a brand-new array. When the variable is deleted and the reference count decremented by one, there will still be that extra reference count, and the array will never be deallocated. You will have a reference-counting induced memory leak. Using the 'N' character will avoid this situation as it will return to the caller an object (inside the tuple) with a single reference count.

<div class="index">

single: reference counting

</div>

## Dealing with array objects

Most extension modules for NumPy will need to access the memory for an ndarray object (or one of it's sub-classes). The easiest way to do this doesn't require you to know much about the internals of NumPy. The method is to

1.  Ensure you are dealing with a well-behaved array (aligned, in machine byte-order and single-segment) of the correct type and number of dimensions.
    1.  By converting it from some Python object using :c\`PyArray\_FromAny\` or a macro built on it.
    2.  By constructing a new ndarray of your desired shape and type using :c\`PyArray\_NewFromDescr\` or a simpler macro or function based on it.
2.  Get the shape of the array and a pointer to its actual data.
3.  Pass the data and shape information on to a subroutine or other section of code that actually performs the computation.
4.  If you are writing the algorithm, then I recommend that you use the stride information contained in the array to access the elements of the array (the :c\`PyArray\_GetPtr\` macros make this painless). Then, you can relax your requirements so as not to force a single-segment array and the data-copying that might result.

Each of these sub-topics is covered in the following sub-sections.

### Converting an arbitrary sequence object

The main routine for obtaining an array from any Python object that can be converted to an array is :c\`PyArray\_FromAny\`. This function is very flexible with many input arguments. Several macros make it easier to use the basic function. :c\`PyArray\_FROM\_OTF\` is arguably the most useful of these macros for the most common uses. It allows you to convert an arbitrary Python object to an array of a specific builtin data-type ( *e.g.* float), while specifying a particular set of requirements ( *e.g.* contiguous, aligned, and writeable). The syntax is

  - :c\`PyArray\_FROM\_OTF\`  
    Return an ndarray from any Python object, *obj*, that can be converted to an array. The number of dimensions in the returned array is determined by the object. The desired data-type of the returned array is provided in *typenum* which should be one of the enumerated types. The *requirements* for the returned array can be any combination of standard array flags. Each of these arguments is explained in more detail below. You receive a new reference to the array on success. On failure, `NULL` is returned and an exception is set.
    
      - *obj*  
        The object can be any Python object convertible to an ndarray. If the object is already (a subclass of) the ndarray that satisfies the requirements then a new reference is returned. Otherwise, a new array is constructed. The contents of *obj* are copied to the new array unless the array interface is used so that data does not have to be copied. Objects that can be converted to an array include: 1) any nested sequence object, 2) any object exposing the array interface, 3) any object with an `~numpy.class.__array__` method (which should return an ndarray), and 4) any scalar object (becomes a zero-dimensional array). Sub-classes of the ndarray that otherwise fit the requirements will be passed through. If you want to ensure a base-class ndarray, then use :c\`NPY\_ARRAY\_ENSUREARRAY\` in the requirements flag. A copy is made only if necessary. If you want to guarantee a copy, then pass in :c\`NPY\_ARRAY\_ENSURECOPY\` to the requirements flag.
    
      - *typenum*  
        One of the enumerated types or :c\`NPY\_NOTYPE\` if the data-type should be determined from the object itself. The C-based names can be used:
        
        > :c\`NPY\_BOOL\`, :c\`NPY\_BYTE\`, :c\`NPY\_UBYTE\`, :c\`NPY\_SHORT\`, :c\`NPY\_USHORT\`, :c\`NPY\_INT\`, :c\`NPY\_UINT\`, :c\`NPY\_LONG\`, :c\`NPY\_ULONG\`, :c\`NPY\_LONGLONG\`, :c\`NPY\_ULONGLONG\`, :c\`NPY\_DOUBLE\`, :c\`NPY\_LONGDOUBLE\`, :c\`NPY\_CFLOAT\`, :c\`NPY\_CDOUBLE\`, :c\`NPY\_CLONGDOUBLE\`, :c\`NPY\_OBJECT\`.
        
        Alternatively, the bit-width names can be used as supported on the platform. For example:
        
        > :c\`NPY\_INT8\`, :c\`NPY\_INT16\`, :c\`NPY\_INT32\`, :c\`NPY\_INT64\`, :c\`NPY\_UINT8\`, :c\`NPY\_UINT16\`, :c\`NPY\_UINT32\`, :c\`NPY\_UINT64\`, :c\`NPY\_FLOAT32\`, :c\`NPY\_FLOAT64\`, :c\`NPY\_COMPLEX64\`, :c\`NPY\_COMPLEX128\`.
        
        The object will be converted to the desired type only if it can be done without losing precision. Otherwise `NULL` will be returned and an error raised. Use :c\`NPY\_ARRAY\_FORCECAST\` in the requirements flag to override this behavior.
    
      - *requirements*  
        The memory model for an ndarray admits arbitrary strides in each dimension to advance to the next element of the array. Often, however, you need to interface with code that expects a C-contiguous or a Fortran-contiguous memory layout. In addition, an ndarray can be misaligned (the address of an element is not at an integral multiple of the size of the element) which can cause your program to crash (or at least work more slowly) if you try and dereference a pointer into the array data. Both of these problems can be solved by converting the Python object into an array that is more "well-behaved" for your specific usage.
        
        The requirements flag allows specification of what kind of array is acceptable. If the object passed in does not satisfy this requirements then a copy is made so that the returned object will satisfy the requirements. these ndarray can use a very generic pointer to memory. This flag allows specification of the desired properties of the returned array object. All of the flags are explained in the detailed API chapter. The flags most commonly needed are :c\`NPY\_ARRAY\_IN\_ARRAY\`, :c\`NPY\_ARRAY\_OUT\_ARRAY\`, and :c\`NPY\_ARRAY\_INOUT\_ARRAY\`:
        
          - :c\`NPY\_ARRAY\_IN\_ARRAY\`  
            This flag is useful for arrays that must be in C-contiguous order and aligned. These kinds of arrays are usually input arrays for some algorithm.
        
          - :c\`NPY\_ARRAY\_OUT\_ARRAY\`  
            This flag is useful to specify an array that is in C-contiguous order, is aligned, and can be written to as well. Such an array is usually returned as output (although normally such output arrays are created from scratch).
        
          - :c\`NPY\_ARRAY\_INOUT\_ARRAY\`  
            This flag is useful to specify an array that will be used for both input and output. :c\`PyArray\_ResolveWritebackIfCopy\` must be called before :c\`Py\_DECREF\` at the end of the interface routine to write back the temporary data into the original array passed in. Use of the :c\`NPY\_ARRAY\_WRITEBACKIFCOPY\` flag requires that the input object is already an array (because other objects cannot be automatically updated in this fashion). If an error occurs use :c\`PyArray\_DiscardWritebackIfCopy\` (obj) on an array with these flags set. This will set the underlying base array writable without causing the contents to be copied back into the original array.
        
        Other useful flags that can be OR'd as additional requirements are:
        
          - :c\`NPY\_ARRAY\_FORCECAST\`  
            Cast to the desired type, even if it can't be done without losing information.
        
          - :c\`NPY\_ARRAY\_ENSURECOPY\`  
            Make sure the resulting array is a copy of the original.
        
          - :c\`NPY\_ARRAY\_ENSUREARRAY\`  
            Make sure the resulting object is an actual ndarray and not a sub-class.

\> **Note** \> Whether or not an array is byte-swapped is determined by the data-type of the array. Native byte-order arrays are always requested by :c\`PyArray\_FROM\_OTF\` and so there is no need for a :c\`NPY\_ARRAY\_NOTSWAPPED\` flag in the requirements argument. There is also no way to get a byte-swapped array from this routine.

### Creating a brand-new ndarray

Quite often, new arrays must be created from within extension-module code. Perhaps an output array is needed and you don't want the caller to have to supply it. Perhaps only a temporary array is needed to hold an intermediate calculation. Whatever the need there are simple ways to get an ndarray object of whatever data-type is needed. The most general function for doing this is :c\`PyArray\_NewFromDescr\`. All array creation functions go through this heavily re-used code. Because of its flexibility, it can be somewhat confusing to use. As a result, simpler forms exist that are easier to use. These forms are part of the :c\`PyArray\_SimpleNew\` family of functions, which simplify the interface by providing default values for common use cases.

### Getting at ndarray memory and accessing elements of the ndarray

If obj is an ndarray (:c`PyArrayObject *`), then the data-area of the ndarray is pointed to by the void\* pointer :c\`PyArray\_DATA\` (obj) or the char\* pointer :c\`PyArray\_BYTES\` (obj). Remember that (in general) this data-area may not be aligned according to the data-type, it may represent byte-swapped data, and/or it may not be writeable. If the data area is aligned and in native byte-order, then how to get at a specific element of the array is determined only by the array of npy\_intp variables, :c\`PyArray\_STRIDES\` (obj). In particular, this c-array of integers shows how many **bytes** must be added to the current element pointer to get to the next element in each dimension. For arrays less than 4-dimensions there are `PyArray_GETPTR{k}` (obj, ...) macros where {k} is the integer 1, 2, 3, or 4 that make using the array strides easier. The arguments .... represent {k} non-negative integer indices into the array. For example, suppose `E` is a 3-dimensional ndarray. A (void\*) pointer to the element `E[i,j,k]` is obtained as :c\`PyArray\_GETPTR3\` (E, i, j, k).

As explained previously, C-style contiguous arrays and Fortran-style contiguous arrays have particular striding patterns. Two array flags (:c\`NPY\_ARRAY\_C\_CONTIGUOUS\` and :c\`NPY\_ARRAY\_F\_CONTIGUOUS\`) indicate whether or not the striding pattern of a particular array matches the C-style contiguous or Fortran-style contiguous or neither. Whether or not the striding pattern matches a standard C or Fortran one can be tested Using :c\`PyArray\_IS\_C\_CONTIGUOUS\` (obj) and :c\`PyArray\_ISFORTRAN\` (obj) respectively. Most third-party libraries expect contiguous arrays. But, often it is not difficult to support general-purpose striding. I encourage you to use the striding information in your own code whenever possible, and reserve single-segment requirements for wrapping third-party code. Using the striding information provided with the ndarray rather than requiring a contiguous striding reduces copying that otherwise must be made.

## Example

<div class="index">

single: extension module

</div>

The following example shows how you might write a wrapper that accepts two input arguments (that will be converted to an array) and an output argument (that must be an array). The function returns None and updates the output array. Note the updated use of WRITEBACKIFCOPY semantics for NumPy v1.14 and above

`` `c static PyObject * example_wrapper(PyObject *dummy, PyObject *args) {     PyObject *arg1=NULL, *arg2=NULL, *out=NULL;     PyObject *arr1=NULL, *arr2=NULL, *oarr=NULL;      if (!PyArg_ParseTuple(args, "OOO!", &arg1, &arg2,         &PyArray_Type, &out)) return NULL;      arr1 = PyArray_FROM_OTF(arg1, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);     if (arr1 == NULL) return NULL;     arr2 = PyArray_FROM_OTF(arg2, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);     if (arr2 == NULL) goto fail; #if NPY_API_VERSION >= 0x0000000c     oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_ARRAY_INOUT_ARRAY2); #else     oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_ARRAY_INOUT_ARRAY); #endif     if (oarr == NULL) goto fail;      /* code that makes use of arguments */     /* You will probably need at least        nd = PyArray_NDIM(<..>)    -- number of dimensions        dims = PyArray_DIMS(<..>)  -- npy_intp array of length nd                                      showing length in each dim.        dptr = (double *)PyArray_DATA(<..>) -- pointer to data.         If an error occurs goto fail.      */      Py_DECREF(arr1);     Py_DECREF(arr2); #if NPY_API_VERSION >= 0x0000000c     PyArray_ResolveWritebackIfCopy(oarr); #endif     Py_DECREF(oarr);     Py_INCREF(Py_None);     return Py_None;   fail:     Py_XDECREF(arr1);     Py_XDECREF(arr2); #if NPY_API_VERSION >= 0x0000000c     PyArray_DiscardWritebackIfCopy(oarr); #endif     Py_XDECREF(oarr);     return NULL; } ``\`

---

c-info.md

---

# Using NumPy C-API

<div class="toctree">

c-info.how-to-extend c-info.python-as-glue c-info.ufunc-tutorial c-info.beyond-basics

</div>

---

c-info.python-as-glue.md

---

# Using Python as glue

<div class="warning">

<div class="title">

Warning

</div>

This was written in 2008 as part of the original [Guide to NumPy](https://archive.org/details/NumPyBook) book by Travis E. Oliphant and is out of date.

</div>

Â Â Â There is no conversation more boring than the one where everybody  
Â Â Â agrees.  
Â Â Â --- *Michel de Montaigne*

Â Â Â Duct tape is like the force. It has a light side, and a dark side, and  
Â Â Â it holds the universe together.  
Â Â Â --- *Carl Zwanzig*

Many people like to say that Python is a fantastic glue language. Hopefully, this Chapter will convince you that this is true. The first adopters of Python for science were typically people who used it to glue together large application codes running on super-computers. Not only was it much nicer to code in Python than in a shell script or Perl, in addition, the ability to easily extend Python made it relatively easy to create new classes and types specifically adapted to the problems being solved. From the interactions of these early contributors, Numeric emerged as an array-like object that could be used to pass data between these applications.

As Numeric has matured and developed into NumPy, people have been able to write more code directly in NumPy. Often this code is fast-enough for production use, but there are still times that there is a need to access compiled code. Either to get that last bit of efficiency out of the algorithm or to make it easier to access widely-available codes written in C/C++ or Fortran.

This chapter will review many of the tools that are available for the purpose of accessing code written in other compiled languages. There are many resources available for learning to call other compiled libraries from Python and the purpose of this Chapter is not to make you an expert. The main goal is to make you aware of some of the possibilities so that you will know what to "Google" in order to learn more.

## Calling other compiled libraries from Python

While Python is a great language and a pleasure to code in, its dynamic nature results in overhead that can cause some code ( *i.e.* raw computations inside of for loops) to be up 10-100 times slower than equivalent code written in a static compiled language. In addition, it can cause memory usage to be larger than necessary as temporary arrays are created and destroyed during computation. For many types of computing needs, the extra slow-down and memory consumption can often not be spared (at least for time- or memory-critical portions of your code). Therefore one of the most common needs is to call out from Python code to a fast, machine-code routine (e.g. compiled using C/C++ or Fortran). The fact that this is relatively easy to do is a big reason why Python is such an excellent high-level language for scientific and engineering programming.

There are two basic approaches to calling compiled code: writing an extension module that is then imported to Python using the import command, or calling a shared-library subroutine directly from Python using the [ctypes](https://docs.python.org/3/library/ctypes.html) module. Writing an extension module is the most common method.

\> **Warning** \> Calling C-code from Python can result in Python crashes if you are not careful. None of the approaches in this chapter are immune. You have to know something about the way data is handled by both NumPy and by the third-party library being used.

## Hand-generated wrappers

Extension modules were discussed in \[writing-an-extension\](\#writing-an-extension). The most basic way to interface with compiled code is to write an extension module and construct a module method that calls the compiled code. For improved readability, your method should take advantage of the `PyArg_ParseTuple` call to convert between Python objects and C data-types. For standard C data-types there is probably already a built-in converter. For others you may need to write your own converter and use the `"O&"` format string which allows you to specify a function that will be used to perform the conversion from the Python object to whatever C-structures are needed.

Once the conversions to the appropriate C-structures and C data-types have been performed, the next step in the wrapper is to call the underlying function. This is straightforward if the underlying function is in C or C++. However, in order to call Fortran code you must be familiar with how Fortran subroutines are called from C/C++ using your compiler and platform. This can vary somewhat platforms and compilers (which is another reason f2py makes life much simpler for interfacing Fortran code) but generally involves underscore mangling of the name and the fact that all variables are passed by reference (i.e. all arguments are pointers).

The advantage of the hand-generated wrapper is that you have complete control over how the C-library gets used and called which can lead to a lean and tight interface with minimal over-head. The disadvantage is that you have to write, debug, and maintain C-code, although most of it can be adapted using the time-honored technique of "cutting-pasting-and-modifying" from other extension modules. Because the procedure of calling out to additional C-code is fairly regimented, code-generation procedures have been developed to make this process easier. One of these code-generation techniques is distributed with NumPy and allows easy integration with Fortran and (simple) C code. This package, f2py, will be covered briefly in the next section.

## F2PY

F2PY allows you to automatically construct an extension module that interfaces to routines in Fortran 77/90/95 code. It has the ability to parse Fortran 77/90/95 code and automatically generate Python signatures for the subroutines it encounters, or you can guide how the subroutine interfaces with Python by constructing an interface-definition-file (or modifying the f2py-produced one).

See the \[F2PY documentation \<f2py\>\](\#f2py-documentation-\<f2py\>) for more information and examples.

The f2py method of linking compiled code is currently the most sophisticated and integrated approach. It allows clean separation of Python with compiled code while still allowing for separate distribution of the extension module. The only draw-back is that it requires the existence of a Fortran compiler in order for a user to install the code. However, with the existence of the free-compilers g77, gfortran, and g95, as well as high-quality commercial compilers, this restriction is not particularly onerous. In our opinion, Fortran is still the easiest way to write fast and clear code for scientific computing. It handles complex numbers, and multi-dimensional indexing in the most straightforward way. Be aware, however, that some Fortran compilers will not be able to optimize code as well as good hand-written C-code.

<div class="index">

single: f2py

</div>

## Cython

[Cython](http://cython.org) is a compiler for a Python dialect that adds (optional) static typing for speed, and allows mixing C or C++ code into your modules. It produces C or C++ extensions that can be compiled and imported in Python code.

If you are writing an extension module that will include quite a bit of your own algorithmic code as well, then Cython is a good match. Among its features is the ability to easily and quickly work with multidimensional arrays.

<div class="index">

single: cython

</div>

Notice that Cython is an extension-module generator only. Unlike f2py, it includes no automatic facility for compiling and linking the extension module (which must be done in the usual fashion). It does provide a modified distutils class called `build_ext` which lets you build an extension module from a `.pyx` source. Thus, you could write in a `setup.py` file:

`` `python     from Cython.Distutils import build_ext     from distutils.extension import Extension     from distutils.core import setup     import numpy      setup(name='mine', description='Nothing',           ext_modules=[Extension('filter', ['filter.pyx'],                                  include_dirs=[numpy.get_include()])],           cmdclass = {'build_ext':build_ext})  Adding the NumPy include directory is, of course, only necessary if ``<span class="title-ref"> you are using NumPy arrays in the extension module (which is what we assume you are using Cython for). The distutils extensions in NumPy also include support for automatically producing the extension-module and linking it from a </span><span class="title-ref">.pyx</span><span class="title-ref"> file. It works so that if the user does not have Cython installed, then it looks for a file with the same file-name but a </span><span class="title-ref">.c</span><span class="title-ref"> extension which it then uses instead of trying to produce the </span><span class="title-ref">.c</span>\` file again.

If you just use Cython to compile a standard Python module, then you will get a C extension module that typically runs a bit faster than the equivalent Python module. Further speed increases can be gained by using the `cdef` keyword to statically define C variables.

Let's look at two examples we've seen before to see how they might be implemented using Cython. These examples were compiled into extension modules using Cython 0.21.1.

### Complex addition in Cython

Here is part of a Cython module named `add.pyx` which implements the complex addition functions we previously implemented using f2py:

`` `cython     cimport cython     cimport numpy as np     import numpy as np      # We need to initialize NumPy.     np.import_array()      #@cython.boundscheck(False)     def zadd(in1, in2):         cdef double complex[:] a = in1.ravel()         cdef double complex[:] b = in2.ravel()          out = np.empty(a.shape[0], np.complex64)         cdef double complex[:] c = out.ravel()          for i in range(c.shape[0]):             c[i].real = a[i].real + b[i].real             c[i].imag = a[i].imag + b[i].imag          return out  This module shows use of the ``cimport`statement to load the definitions`<span class="title-ref"> from the </span><span class="title-ref">numpy.pxd</span><span class="title-ref"> header that ships with Cython. It looks like NumPy is imported twice; </span><span class="title-ref">cimport</span><span class="title-ref"> only makes the NumPy C-API available, while the regular </span><span class="title-ref">import</span>\` causes a Python-style import at runtime and makes it possible to call into the familiar NumPy Python API.

The example also demonstrates Cython's "typed memoryviews", which are like NumPy arrays at the C level, in the sense that they are shaped and strided arrays that know their own extent (unlike a C array addressed through a bare pointer). The syntax `double complex[:]` denotes a one-dimensional array (vector) of doubles, with arbitrary strides. A contiguous array of ints would be `int[::1]`, while a matrix of floats would be `float[:, :]`.

Shown commented is the `cython.boundscheck` decorator, which turns bounds-checking for memory view accesses on or off on a per-function basis. We can use this to further speed up our code, at the expense of safety (or a manual check prior to entering the loop).

Other than the view syntax, the function is immediately readable to a Python programmer. Static typing of the variable `i` is implicit. Instead of the view syntax, we could also have used Cython's special NumPy array syntax, but the view syntax is preferred.

### Image filter in Cython

The two-dimensional example we created using Fortran is just as easy to write in Cython:

`` `cython     cimport numpy as np     import numpy as np      np.import_array()      def filter(img):         cdef double[:, :] a = np.asarray(img, dtype=np.double)         out = np.zeros(img.shape, dtype=np.double)         cdef double[:, ::1] b = out          cdef np.npy_intp i, j          for i in range(1, a.shape[0] - 1):             for j in range(1, a.shape[1] - 1):                 b[i, j] = (a[i, j]                            + .5 * (  a[i-1, j] + a[i+1, j]                                    + a[i, j-1] + a[i, j+1])                            + .25 * (  a[i-1, j-1] + a[i-1, j+1]                                     + a[i+1, j-1] + a[i+1, j+1]))          return out  This 2-d averaging filter runs quickly because the loop is in C and ``<span class="title-ref"> the pointer computations are done only as needed. If the code above is compiled as a module </span><span class="title-ref">image</span><span class="title-ref">, then a 2-d image, </span><span class="title-ref">img</span>\`, can be filtered using this code very quickly using:

`` `python     import image     out = image.filter(img)  Regarding the code, two things are of note: firstly, it is impossible to ``<span class="title-ref"> return a memory view to Python. Instead, a NumPy array </span><span class="title-ref">out</span><span class="title-ref"> is first created, and then a view </span><span class="title-ref">b</span><span class="title-ref"> onto this array is used for the computation. Secondly, the view </span><span class="title-ref">b</span><span class="title-ref"> is typed </span><span class="title-ref">double\[:, ::1\]</span>\`. This means 2-d array with contiguous rows, i.e., C matrix order. Specifying the order explicitly can speed up some algorithms since they can skip stride computations.

### Conclusion

Cython is the extension mechanism of choice for several scientific Python libraries, including Scipy, Pandas, SAGE, scikit-image and scikit-learn, as well as the XML processing library LXML. The language and compiler are well-maintained.

There are several disadvantages of using Cython:

1.  When coding custom algorithms, and sometimes when wrapping existing C libraries, some familiarity with C is required. In particular, when using C memory management (`malloc` and friends), it's easy to introduce memory leaks. However, just compiling a Python module renamed to `.pyx` can already speed it up, and adding a few type declarations can give dramatic speedups in some code.
2.  It is easy to lose a clean separation between Python and C which makes re-using your C-code for other non-Python-related projects more difficult.
3.  The C-code generated by Cython is hard to read and modify (and typically compiles with annoying but harmless warnings).

One big advantage of Cython-generated extension modules is that they are easy to distribute. In summary, Cython is a very capable tool for either gluing C code or generating an extension module quickly and should not be over-looked. It is especially useful for people that can't or won't write C or Fortran code.

<div class="index">

single: cython

</div>

## ctypes

[ctypes](https://docs.python.org/3/library/ctypes.html) is a Python extension module, included in the stdlib, that allows you to call an arbitrary function in a shared library directly from Python. This approach allows you to interface with C-code directly from Python. This opens up an enormous number of libraries for use from Python. The drawback, however, is that coding mistakes can lead to ugly program crashes very easily (just as can happen in C) because there is little type or bounds checking done on the parameters. This is especially true when array data is passed in as a pointer to a raw memory location. The responsibility is then on you that the subroutine will not access memory outside the actual array area. But, if you don't mind living a little dangerously ctypes can be an effective tool for quickly taking advantage of a large shared library (or writing extended functionality in your own shared library).

<div class="index">

single: ctypes

</div>

Because the ctypes approach exposes a raw interface to the compiled code it is not always tolerant of user mistakes. Robust use of the ctypes module typically involves an additional layer of Python code in order to check the data types and array bounds of objects passed to the underlying subroutine. This additional layer of checking (not to mention the conversion from ctypes objects to C-data-types that ctypes itself performs), will make the interface slower than a hand-written extension-module interface. However, this overhead should be negligible if the C-routine being called is doing any significant amount of work. If you are a great Python programmer with weak C skills, ctypes is an easy way to write a useful interface to a (shared) library of compiled code.

To use ctypes you must

1.  Have a shared library.
2.  Load the shared library.
3.  Convert the Python objects to ctypes-understood arguments.
4.  Call the function from the library with the ctypes arguments.

### Having a shared library

There are several requirements for a shared library that can be used with ctypes that are platform specific. This guide assumes you have some familiarity with making a shared library on your system (or simply have a shared library available to you). Items to remember are:

  - A shared library must be compiled in a special way ( *e.g.* using the `-shared` flag with gcc).

  - On some platforms (*e.g.* Windows), a shared library requires a .def file that specifies the functions to be exported. For example a mylib.def file might contain:
    
        LIBRARY mylib.dll
        EXPORTS
        cool_function1
        cool_function2
    
    Alternatively, you may be able to use the storage-class specifier `__declspec(dllexport)` in the C-definition of the function to avoid the need for this `.def` file.

There is no standard way in Python distutils to create a standard shared library (an extension module is a "special" shared library Python understands) in a cross-platform manner. Thus, a big disadvantage of ctypes at the time of writing this book is that it is difficult to distribute in a cross-platform manner a Python extension that uses ctypes and includes your own code which should be compiled as a shared library on the users system.

### Loading the shared library

A simple, but robust way to load the shared library is to get the absolute path name and load it using the cdll object of ctypes:

`` `python     lib = ctypes.cdll[<full_path_name>]  However, on Windows accessing an attribute of the ``cdll`method will`<span class="title-ref"> load the first DLL by that name found in the current directory or on the PATH. Loading the absolute path name requires a little finesse for cross-platform work since the extension of shared libraries varies. There is a </span><span class="title-ref">ctypes.util.find\_library</span>\` utility available that can simplify the process of finding the library to load but it is not foolproof. Complicating matters, different platforms have different default extensions used by shared libraries (e.g. .dll -- Windows, .so -- Linux, .dylib -- Mac OS X). This must also be taken into account if you are using ctypes to wrap code that needs to work on several platforms.

NumPy provides a convenience function called `ctypeslib.load_library` (name, path). This function takes the name of the shared library (including any prefix like 'lib' but excluding the extension) and a path where the shared library can be located. It returns a ctypes library object or raises an `OSError` if the library cannot be found or raises an `ImportError` if the ctypes module is not available. (Windows users: the ctypes library object loaded using `load_library` is always loaded assuming cdecl calling convention. See the ctypes documentation under `ctypes.windll` and/or `ctypes.oledll` for ways to load libraries under other calling conventions).

The functions in the shared library are available as attributes of the ctypes library object (returned from `ctypeslib.load_library`) or as items using `lib['func_name']` syntax. The latter method for retrieving a function name is particularly useful if the function name contains characters that are not allowable in Python variable names.

### Converting arguments

Python ints/longs, strings, and unicode objects are automatically converted as needed to equivalent ctypes arguments The None object is also converted automatically to a NULL pointer. All other Python objects must be converted to ctypes-specific types. There are two ways around this restriction that allow ctypes to integrate with other objects.

1.  Don't set the argtypes attribute of the function object and define an `_as_parameter_` method for the object you want to pass in. The `_as_parameter_` method must return a Python int which will be passed directly to the function.
2.  Set the argtypes attribute to a list whose entries contain objects with a classmethod named from\_param that knows how to convert your object to an object that ctypes can understand (an int/long, string, unicode, or object with the `_as_parameter_` attribute).

NumPy uses both methods with a preference for the second method because it can be safer. The ctypes attribute of the ndarray returns an object that has an `_as_parameter_` attribute which returns an integer representing the address of the ndarray to which it is associated. As a result, one can pass this ctypes attribute object directly to a function expecting a pointer to the data in your ndarray. The caller must be sure that the ndarray object is of the correct type, shape, and has the correct flags set or risk nasty crashes if the data-pointer to inappropriate arrays are passed in.

To implement the second method, NumPy provides the class-factory function <span class="title-ref">ndpointer</span> in the `numpy.ctypeslib` module. This class-factory function produces an appropriate class that can be placed in an argtypes attribute entry of a ctypes function. The class will contain a from\_param method which ctypes will use to convert any ndarray passed in to the function to a ctypes-recognized object. In the process, the conversion will perform checking on any properties of the ndarray that were specified by the user in the call to <span class="title-ref">ndpointer</span>. Aspects of the ndarray that can be checked include the data-type, the number-of-dimensions, the shape, and/or the state of the flags on any array passed. The return value of the from\_param method is the ctypes attribute of the array which (because it contains the `_as_parameter_` attribute pointing to the array data area) can be used by ctypes directly.

The ctypes attribute of an ndarray is also endowed with additional attributes that may be convenient when passing additional information about the array into a ctypes function. The attributes **data**, **shape**, and **strides** can provide ctypes compatible types corresponding to the data-area, the shape, and the strides of the array. The data attribute returns a `c_void_p` representing a pointer to the data area. The shape and strides attributes each return an array of ctypes integers (or None representing a NULL pointer, if a 0-d array). The base ctype of the array is a ctype integer of the same size as a pointer on the platform. There are also methods `data_as({ctype})`, `shape_as(<base ctype>)`, and `strides_as(<base ctype>)`. These return the data as a ctype object of your choice and the shape/strides arrays using an underlying base type of your choice. For convenience, the `ctypeslib` module also contains `c_intp` as a ctypes integer data-type whose size is the same as the size of `c_void_p` on the platform (its value is None if ctypes is not installed).

### Calling the function

The function is accessed as an attribute of or an item from the loaded shared-library. Thus, if `./mylib.so` has a function named `cool_function1`, it may be accessed either as:

`` `python     lib = numpy.ctypeslib.load_library('mylib','.')     func1 = lib.cool_function1  # or equivalently     func1 = lib['cool_function1']  In ctypes, the return-value of a function is set to be 'int' by ``\` default. This behavior can be changed by setting the restype attribute of the function. Use None for the restype if the function has no return value ('void'):

`` `python     func1.restype = None  As previously discussed, you can also set the argtypes attribute of ``<span class="title-ref"> the function in order to have ctypes check the types of the input arguments when the function is called. Use the \`ndpointer</span> factory function to generate a ready-made class for data-type, shape, and flags checking on your new function. The <span class="title-ref">ndpointer</span> function has the signature

<div class="function">

ndpointer(dtype=None, ndim=None, shape=None, flags=None)

Keyword arguments with the value `None` are not checked. Specifying a keyword enforces checking of that aspect of the ndarray on conversion to a ctypes-compatible object. The dtype keyword can be any object understood as a data-type object. The ndim keyword should be an integer, and the shape keyword should be an integer or a sequence of integers. The flags keyword specifies the minimal flags that are required on any array passed in. This can be specified as a string of comma separated requirements, an integer indicating the requirement bits OR'd together, or a flags object returned from the flags attribute of an array with the necessary requirements.

</div>

Using an ndpointer class in the argtypes method can make it significantly safer to call a C function using ctypes and the data-area of an ndarray. You may still want to wrap the function in an additional Python wrapper to make it user-friendly (hiding some obvious arguments and making some arguments output arguments). In this process, the `requires` function in NumPy may be useful to return the right kind of array from a given input.

### Complete example

In this example, we will demonstrate how the addition function and the filter function implemented previously using the other approaches can be implemented using ctypes. First, the C code which implements the algorithms contains the functions `zadd`, `dadd`, `sadd`, `cadd`, and `dfilter2d`. The `zadd` function is:

`` `c     /* Add arrays of contiguous data */     typedef struct {double real; double imag;} cdouble;     typedef struct {float real; float imag;} cfloat;     void zadd(cdouble *a, cdouble *b, cdouble *c, long n)     {         while (n--) {             c->real = a->real + b->real;             c->imag = a->imag + b->imag;             a++; b++; c++;         }     }  with similar code for ``cadd`,`dadd`, and`sadd`that handles complex`\` float, double, and float data-types, respectively:

`` `c     void cadd(cfloat *a, cfloat *b, cfloat *c, long n)     {             while (n--) {                     c->real = a->real + b->real;                     c->imag = a->imag + b->imag;                     a++; b++; c++;             }     }     void dadd(double *a, double *b, double *c, long n)     {             while (n--) {                     *c++ = *a++ + *b++;             }     }     void sadd(float *a, float *b, float *c, long n)     {             while (n--) {                     *c++ = *a++ + *b++;             }     }  The ``code.c`file also contains the function`dfilter2d`:  .. code-block:: c      /*      * Assumes b is contiguous and has strides that are multiples of      * sizeof(double)      */     void     dfilter2d(double *a, double *b, ssize_t *astrides, ssize_t *dims)     {         ssize_t i, j, M, N, S0, S1;         ssize_t r, c, rm1, rp1, cp1, cm1;          M = dims[0]; N = dims[1];         S0 = astrides[0]/sizeof(double);         S1 = astrides[1]/sizeof(double);         for (i = 1; i < M - 1; i++) {             r = i*S0;             rp1 = r + S0;             rm1 = r - S0;             for (j = 1; j < N - 1; j++) {                 c = j*S1;                 cp1 = j + S1;                 cm1 = j - S1;                 b[i*N + j] = a[r + c] +                     (a[rp1 + c] + a[rm1 + c] +                      a[r + cp1] + a[r + cm1])*0.5 +                     (a[rp1 + cp1] + a[rp1 + cm1] +                      a[rm1 + cp1] + a[rm1 + cp1])*0.25;             }         }     }  A possible advantage this code has over the Fortran-equivalent code is`<span class="title-ref"> that it takes arbitrarily strided (i.e. non-contiguous arrays) and may also run faster depending on the optimization capability of your compiler. But, it is an obviously more complicated than the simple code in </span><span class="title-ref">filter.f</span>\`. This code must be compiled into a shared library. On my Linux system this is accomplished using:

    gcc -o code.so -shared code.c

Which creates a shared\_library named code.so in the current directory. On Windows don't forget to either add `__declspec(dllexport)` in front of void on the line preceding each function definition, or write a `code.def` file that lists the names of the functions to be exported.

A suitable Python interface to this shared library should be constructed. To do this create a file named interface.py with the following lines at the top:

`` `python     __all__ = ['add', 'filter2d']      import numpy as np     import os      _path = os.path.dirname('__file__')     lib = np.ctypeslib.load_library('code', _path)     _typedict = {'zadd' : complex, 'sadd' : np.single,                  'cadd' : np.csingle, 'dadd' : float}     for name in _typedict.keys():         val = getattr(lib, name)         val.restype = None         _type = _typedict[name]         val.argtypes = [np.ctypeslib.ndpointer(_type,                           flags='aligned, contiguous'),                         np.ctypeslib.ndpointer(_type,                           flags='aligned, contiguous'),                         np.ctypeslib.ndpointer(_type,                           flags='aligned, contiguous,'\                                 'writeable'),                         np.ctypeslib.c_intp]  This code loads the shared library named ``code.{ext}`located in the`\` same path as this file. It then adds a return type of void to the functions contained in the library. It also adds argument checking to the functions in the library so that ndarrays can be passed as the first three arguments along with an integer (large enough to hold a pointer on the platform) as the fourth argument.

Setting up the filtering function is similar and allows the filtering function to be called with ndarray arguments as the first two arguments and with pointers to integers (large enough to handle the strides and shape of an ndarray) as the last two arguments.:

`` `python     lib.dfilter2d.restype=None     lib.dfilter2d.argtypes = [np.ctypeslib.ndpointer(float, ndim=2,                                            flags='aligned'),                               np.ctypeslib.ndpointer(float, ndim=2,                                      flags='aligned, contiguous,'\                                            'writeable'),                               ctypes.POINTER(np.ctypeslib.c_intp),                               ctypes.POINTER(np.ctypeslib.c_intp)]  Next, define a simple selection function that chooses which addition ``\` function to call in the shared library based on the data-type:

`` `python     def select(dtype):         if dtype.char in ['?bBhHf']:             return lib.sadd, single         elif dtype.char in ['F']:             return lib.cadd, csingle         elif dtype.char in ['DG']:             return lib.zadd, complex         else:             return lib.dadd, float         return func, ntype  Finally, the two functions to be exported by the interface can be ``\` written simply as:

`` `python     def add(a, b):         requires = ['CONTIGUOUS', 'ALIGNED']         a = np.asanyarray(a)         func, dtype = select(a.dtype)         a = np.require(a, dtype, requires)         b = np.require(b, dtype, requires)         c = np.empty_like(a)         func(a,b,c,a.size)         return c  and:  .. code-block:: python      def filter2d(a):         a = np.require(a, float, ['ALIGNED'])         b = np.zeros_like(a)         lib.dfilter2d(a, b, a.ctypes.strides, a.ctypes.shape)         return b   Conclusion ``\` ----------

<div class="index">

single: ctypes

</div>

Using ctypes is a powerful way to connect Python with arbitrary C-code. Its advantages for extending Python include

  - clean separation of C code from Python code
    
    >   - no need to learn a new syntax except Python and C
    >   - allows reuse of C code
    >   - functionality in shared libraries written for other purposes can be obtained with a simple Python wrapper and search for the library.

  - easy integration with NumPy through the ctypes attribute

  - full argument checking with the ndpointer class factory

Its disadvantages include

  - It is difficult to distribute an extension module made using ctypes because of a lack of support for building shared libraries in distutils.
  - You must have shared-libraries of your code (no static libraries).
  - Very little support for C++ code and its different library-calling conventions. You will probably need a C wrapper around C++ code to use with ctypes (or just use Boost.Python instead).

Because of the difficulty in distributing an extension module made using ctypes, f2py and Cython are still the easiest ways to extend Python for package creation. However, ctypes is in some cases a useful alternative. This should bring more features to ctypes that should eliminate the difficulty in extending Python and distributing the extension using ctypes.

## Additional tools you may find useful

These tools have been found useful by others using Python and so are included here. They are discussed separately because they are either older ways to do things now handled by f2py, Cython, or ctypes (SWIG, PyFort) or because of a lack of reasonable documentation (SIP, Boost). Links to these methods are not included since the most relevant can be found using Google or some other search engine, and any links provided here would be quickly dated. Do not assume that inclusion in this list means that the package deserves attention. Information about these packages are collected here because many people have found them useful and we'd like to give you as many options as possible for tackling the problem of easily integrating your code.

### SWIG

<div class="index">

single: swig

</div>

Simplified Wrapper and Interface Generator (SWIG) is an old and fairly stable method for wrapping C/C++-libraries to a large variety of other languages. It does not specifically understand NumPy arrays but can be made usable with NumPy through the use of typemaps. There are some sample typemaps in the numpy/tools/swig directory under numpy.i together with an example module that makes use of them. SWIG excels at wrapping large C/C++ libraries because it can (almost) parse their headers and auto-produce an interface. Technically, you need to generate a `.i` file that defines the interface. Often, however, this `.i` file can be parts of the header itself. The interface usually needs a bit of tweaking to be very useful. This ability to parse C/C++ headers and auto-generate the interface still makes SWIG a useful approach to adding functionality from C/C++ into Python, despite the other methods that have emerged that are more targeted to Python. SWIG can actually target extensions for several languages, but the typemaps usually have to be language-specific. Nonetheless, with modifications to the Python-specific typemaps, SWIG can be used to interface a library with other languages such as Perl, Tcl, and Ruby.

My experience with SWIG has been generally positive in that it is relatively easy to use and quite powerful. It has been used often before becoming more proficient at writing C-extensions. However, writing custom interfaces with SWIG is often troublesome because it must be done using the concept of typemaps which are not Python specific and are written in a C-like syntax. Therefore, other gluing strategies are preferred and SWIG would be probably considered only to wrap a very-large C/C++ library. Nonetheless, there are others who use SWIG quite happily.

### SIP

<div class="index">

single: SIP

</div>

SIP is another tool for wrapping C/C++ libraries that is Python specific and appears to have very good support for C++. Riverbank Computing developed SIP in order to create Python bindings to the QT library. An interface file must be written to generate the binding, but the interface file looks a lot like a C/C++ header file. While SIP is not a full C++ parser, it understands quite a bit of C++ syntax as well as its own special directives that allow modification of how the Python binding is accomplished. It also allows the user to define mappings between Python types and C/C++ structures and classes.

### Boost Python

<div class="index">

single: Boost.Python

</div>

Boost is a repository of C++ libraries and Boost.Python is one of those libraries which provides a concise interface for binding C++ classes and functions to Python. The amazing part of the Boost.Python approach is that it works entirely in pure C++ without introducing a new syntax. Many users of C++ report that Boost.Python makes it possible to combine the best of both worlds in a seamless fashion. Using Boost to wrap simple C-subroutines is usually over-kill. Its primary purpose is to make C++ classes available in Python. So, if you have a set of C++ classes that need to be integrated cleanly into Python, consider learning about and using Boost.Python.

### Pyfort

Pyfort is a nice tool for wrapping Fortran and Fortran-like C-code into Python with support for Numeric arrays. It was written by Paul Dubois, a distinguished computer scientist and the very first maintainer of Numeric (now retired). It is worth mentioning in the hopes that somebody will update PyFort to work with NumPy arrays as well which now support either Fortran or C-style contiguous arrays.

---

c-info.ufunc-tutorial.md

---

# Writing your own ufunc

I have the Power\!  
\--- *He-Man*

## Creating a new universal function

<div class="index">

pair: ufunc; adding new

</div>

Before reading this, it may help to familiarize yourself with the basics of C extensions for Python by reading/skimming the tutorials in Section 1 of [Extending and Embedding the Python Interpreter](https://docs.python.org/extending/index.html) and in \[How to extend NumPy \<c-info.how-to-extend\>\](How to extend NumPy \<c-info.how-to-extend\>.md)

The umath module is a computer-generated C-module that creates many ufuncs. It provides a great many examples of how to create a universal function. Creating your own ufunc that will make use of the ufunc machinery is not difficult either. Suppose you have a function that you want to operate element-by-element over its inputs. By creating a new ufunc you will obtain a function that handles

  - broadcasting
  - N-dimensional looping
  - automatic type-conversions with minimal memory usage
  - optional output arrays

It is not difficult to create your own ufunc. All that is required is a 1-d loop for each data-type you want to support. Each 1-d loop must have a specific signature, and only ufuncs for fixed-size data-types can be used. The function call used to create a new ufunc to work on built-in data-types is given below. A different mechanism is used to register ufuncs for user-defined data-types.

In the next several sections we give example code that can be easily modified to create your own ufuncs. The examples are successively more complete or complicated versions of the logit function, a common function in statistical modeling. Logit is also interesting because, due to the magic of IEEE standards (specifically IEEE 754), all of the logit functions created below automatically have the following behavior.

\>\>\> logit(0) -inf \>\>\> logit(1) inf \>\>\> logit(2) nan \>\>\> logit(-2) nan

This is wonderful because the function writer doesn't have to manually propagate infs or nans.

## Example non-ufunc extension

<div class="index">

pair: ufunc; adding new

</div>

For comparison and general edification of the reader we provide a simple implementation of a C extension of `logit` that uses no numpy.

To do this we need two files. The first is the C file which contains the actual code, and the second is the `setup.py` file used to create the module.

>   - \`\`\`c  
>     \#define PY\_SSIZE\_T\_CLEAN \#include \<Python.h\> \#include \<math.h\>
>     
>       - /\*
>         
>           - spammodule.c
>           - This is the C code for a non-numpy Python extension to
>           - define the logit function, where logit(p) = log(p/(1-p)).
>           - This function will not work on numpy arrays automatically.
>           - numpy.vectorize must be called in python to generate
>           - a numpy-friendly function.
>           - 
>           - Details explaining the Python-C API can be found under
>           - 'Extending and Embedding' and 'Python/C API' at
>         
>         \* docs.python.org . \*/
>     
>     /\* This declares the logit function */ static PyObject*spam\_logit(PyObject *self, PyObject*args);
>     
>       - /\*
>         
>           - This tells Python what methods this module has.
>         
>         \* See the Python-C API for more information. \*/
>     
>       - static PyMethodDef SpamMethods\[\] = {
>         
>           - {"logit",  
>             spam\_logit, METH\_VARARGS, "compute logit"},
>         
>         {NULL, NULL, 0, NULL}
>     
>     };
>     
>       - /\*
>         
>           - This actually defines the logit function for
>         
>         \* input args from Python. \*/
>     
>     static PyObject *spam\_logit(PyObject*self, PyObject \*args) { double p;
>     
>     > /\* This parses the Python argument into a double \*/ if(\!PyArg\_ParseTuple(args, "d", \&p)) { return NULL; }
>     > 
>     > /\* THE ACTUAL LOGIT FUNCTION \*/ p = p/(1-p); p = log(p);
>     > 
>     > /*This builds the answer back into a python object*/ return Py\_BuildValue("d", p);
>     
>     }
>     
>     /\* This initiates the module using the above definitions. \*/ static struct PyModuleDef moduledef = { PyModuleDef\_HEAD\_INIT, "spam", NULL, -1, SpamMethods, NULL, NULL, NULL, NULL };
>     
>     PyMODINIT\_FUNC PyInit\_spam(void) { PyObject \*m; m = PyModule\_Create(\&moduledef); if (\!m) { return NULL; } return m; }

To use the `setup.py file`, place `setup.py` and `spammodule.c` `` ` in the same folder. Then ``python setup.py build`will build the module to import, or`python setup.py install`will install the module to your site-packages directory.`\`python ''' setup.py file for spammodule.c

> Calling $python setup.py build\_ext --inplace will build the extension library in the current file.
> 
> Calling $python setup.py build will build a file that looks like ./build/lib\*, where lib\* is a file that begins with lib. The library will be in this file and end with a C library extension, such as .so
> 
> Calling $python setup.py install will install the module in your site-packages file.
> 
> See the setuptools section 'Building Extension Modules' at setuptools.pypa.io for more information. '''
> 
> from setuptools import setup, Extension import numpy as np
> 
> module1 = Extension('spam', sources=\['spammodule.c'\])
> 
> setup(name='spam', version='1.0', ext\_modules=\[module1\])

Once the spam module is imported into python, you can call logit `` ` via ``spam.logit``. Note that the function used above cannot be applied as-is to numpy arrays. To do so we must call :py`numpy.vectorize` on it. For example, if a python interpreter is opened in the file containing the spam library or spam has been installed, one can perform the following commands:  >>> import numpy as np >>> import spam >>> spam.logit(0) -inf >>> spam.logit(1) inf >>> spam.logit(0.5) 0.0 >>> x = np.linspace(0,1,10) >>> spam.logit(x) TypeError: only length-1 arrays can be converted to Python scalars >>> f = np.vectorize(spam.logit) >>> f(x) array([       -inf, -2.07944154, -1.25276297, -0.69314718, -0.22314355,     0.22314355,  0.69314718,  1.25276297,  2.07944154,         inf])  THE RESULTING LOGIT FUNCTION IS NOT FAST!``numpy.vectorize`simply loops over`spam.logit`. The loop is done at the C level, but the numpy array is constantly being parsed and build back up. This is expensive. When the author compared`numpy.vectorize(spam.logit)``against the logit ufuncs constructed below, the logit ufuncs were almost exactly 4 times faster. Larger or smaller speedups are, of course, possible depending on the nature of the function.   .. _`sec:NumPy-one-loop`:  Example NumPy ufunc for one dtype =================================  .. index::    pair: ufunc; adding new  For simplicity we give a ufunc for a single dtype, the``'f8'`  `double`. As in the previous section, we first give the`.c`file and then the`setup.py`file used to create the module containing the ufunc.  The place in the code corresponding to the actual computations for the ufunc are marked with`/\* BEGIN main ufunc computation \*/`and`/\* END main ufunc computation \*/`. The code in between those lines is the primary thing that must be changed to create your own ufunc.`\`c \#define PY\_SSIZE\_T\_CLEAN \#include \<Python.h\> \#include "numpy/ndarraytypes.h" \#include "numpy/ufuncobject.h" \#include "numpy/npy\_3kcompat.h" \#include \<math.h\>

>   - /\*
>     
>       - single\_type\_logit.c
>       - This is the C code for creating your own
>       - NumPy ufunc for a logit function.
>       - 
>       - In this code we only define the ufunc for
>       - a single dtype. The computations that must
>       - be replaced to create a ufunc for
>       - a different function are marked with BEGIN
>       - and END.
>       - 
>       - Details explaining the Python-C API can be found under
>       - 'Extending and Embedding' and 'Python/C API' at
>     
>     \* docs.python.org . \*/
> 
>   - static PyMethodDef LogitMethods\[\] = {  
>     {NULL, NULL, 0, NULL}
> 
> };
> 
> /\* The loop definition must precede the PyMODINIT\_FUNC. \*/
> 
>   - static void double\_logit(char \**args, const npy\_intp*dimensions,  
>     const npy\_intp *steps, void*data)
> 
>   - {  
>     npy\_intp i; npy\_intp n = dimensions\[0\]; char *in = args\[0\],*out = args\[1\]; npy\_intp in\_step = steps\[0\], out\_step = steps\[1\];
>     
>     double tmp;
>     
>       - for (i = 0; i \< n; i++) {  
>         /\* BEGIN main ufunc computation */ tmp =*(double *)in; tmp /= 1 - tmp;*((double *)out) = log(tmp); /* END main ufunc computation \*/
>         
>         in += in\_step; out += out\_step;
>     
>     }
> 
> }
> 
> /\* This a pointer to the above function \*/ PyUFuncGenericFunction funcs\[1\] = {\&double\_logit};
> 
> /\* These are the input and return dtypes of logit.\*/ static const char types\[2\] = {NPY\_DOUBLE, NPY\_DOUBLE};
> 
>   - static struct PyModuleDef moduledef = {  
>     PyModuleDef\_HEAD\_INIT, "npufunc", NULL, -1, LogitMethods, NULL, NULL, NULL, NULL
> 
> };
> 
> PyMODINIT\_FUNC PyInit\_npufunc(void) { PyObject *m,*logit, \*d;
> 
> > import\_array(); import\_umath();
> > 
> > m = PyModule\_Create(\&moduledef); if (\!m) { return NULL; }
> > 
> >   - logit = PyUFunc\_FromFuncAndData(funcs, NULL, types, 1, 1, 1,  
> >     PyUFunc\_None, "logit", "logit\_docstring", 0);
> > 
> > d = PyModule\_GetDict(m);
> > 
> > PyDict\_SetItemString(d, "logit", logit); Py\_DECREF(logit);
> > 
> > return m;
> 
> }

This is a `setup.py file` for the above code. As before, the module `` ` can be build via calling ``python setup.py build`at the command prompt, or installed to site-packages via`python setup.py install`. The module can also be placed into a local folder e.g.`npufunc\_directory`below using`python setup.py build\_ext --inplace`.`\`python ''' setup.py file for single\_type\_logit.c Note that since this is a numpy extension we add an include\_dirs=\[get\_include()\] so that the extension is built with numpy's C/C++ header files.

> Calling $python setup.py build\_ext --inplace will build the extension library in the npufunc\_directory.
> 
> Calling $python setup.py build will build a file that looks like ./build/lib\*, where lib\* is a file that begins with lib. The library will be in this file and end with a C library extension, such as .so
> 
> Calling $python setup.py install will install the module in your site-packages file.
> 
> See the setuptools section 'Building Extension Modules' at setuptools.pypa.io for more information. '''
> 
> from setuptools import setup, Extension from numpy import get\_include
> 
>   - npufunc = Extension('npufunc',  
>     sources=\['single\_type\_logit.c'\], include\_dirs=\[get\_include()\])
> 
> setup(name='npufunc', version='1.0', ext\_modules=\[npufunc\])

After the above has been installed, it can be imported and used as follows.

\>\>\> import numpy as np `` ` >>> import npufunc >>> npufunc.logit(0.5) np.float64(0.0) >>> a = np.linspace(0,1,5) >>> npufunc.logit(a) array([       -inf, -1.09861229,  0.        ,  1.09861229,         inf])    .. _`sec:NumPy-many-loop`:  Example NumPy ufunc with multiple dtypes ========================================  .. index::    pair: ufunc; adding new  We finally give an example of a full ufunc, with inner loops for half-floats, floats, doubles, and long doubles. As in the previous sections we first give the ``.c`file and then the corresponding`setup.py`file.  The places in the code corresponding to the actual computations for the ufunc are marked with`/\* BEGIN main ufunc computation \*/`and`/\* END main ufunc computation \*/`. The code in between those lines is the primary thing that must be changed to create your own ufunc.`\`c \#define PY\_SSIZE\_T\_CLEAN \#include \<Python.h\> \#include "numpy/ndarraytypes.h" \#include "numpy/ufuncobject.h" \#include "numpy/halffloat.h" \#include \<math.h\>

>   - /\*
>     
>       - multi\_type\_logit.c
>       - This is the C code for creating your own
>       - NumPy ufunc for a logit function.
>       - 
>       - Each function of the form type\_logit defines the
>       - logit function for a different numpy dtype. Each
>       - of these functions must be modified when you
>       - create your own ufunc. The computations that must
>       - be replaced to create a ufunc for
>       - a different function are marked with BEGIN
>       - and END.
>       - 
>       - Details explaining the Python-C API can be found under
>       - 'Extending and Embedding' and 'Python/C API' at
>       - docs.python.org .
>     
>     \* \*/
> 
>   - static PyMethodDef LogitMethods\[\] = {  
>     {NULL, NULL, 0, NULL}
> 
> };
> 
> /\* The loop definitions must precede the PyMODINIT\_FUNC. \*/
> 
>   - static void long\_double\_logit(char \**args, const npy\_intp*dimensions,  
>     const npy\_intp *steps, void*data)
> 
>   - {  
>     npy\_intp i; npy\_intp n = dimensions\[0\]; char *in = args\[0\],*out = args\[1\]; npy\_intp in\_step = steps\[0\], out\_step = steps\[1\];
>     
>     long double tmp;
>     
>       - for (i = 0; i \< n; i++) {  
>         /\* BEGIN main ufunc computation */ tmp =*(long double *)in; tmp /= 1 - tmp;*((long double *)out) = logl(tmp); /* END main ufunc computation \*/
>         
>         in += in\_step; out += out\_step;
>     
>     }
> 
> }
> 
>   - static void double\_logit(char \**args, const npy\_intp*dimensions,  
>     const npy\_intp *steps, void*data)
> 
>   - {  
>     npy\_intp i; npy\_intp n = dimensions\[0\]; char *in = args\[0\],*out = args\[1\]; npy\_intp in\_step = steps\[0\], out\_step = steps\[1\];
>     
>     double tmp;
>     
>       - for (i = 0; i \< n; i++) {  
>         /\* BEGIN main ufunc computation */ tmp =*(double *)in; tmp /= 1 - tmp;*((double *)out) = log(tmp); /* END main ufunc computation \*/
>         
>         in += in\_step; out += out\_step;
>     
>     }
> 
> }
> 
>   - static void float\_logit(char \**args, const npy\_intp*dimensions,  
>     const npy\_intp *steps, void*data)
> 
>   - {  
>     npy\_intp i; npy\_intp n = dimensions\[0\]; char *in = args\[0\],*out = args\[1\]; npy\_intp in\_step = steps\[0\], out\_step = steps\[1\];
>     
>     float tmp;
>     
>       - for (i = 0; i \< n; i++) {  
>         /\* BEGIN main ufunc computation */ tmp =*(float *)in; tmp /= 1 - tmp;*((float *)out) = logf(tmp); /* END main ufunc computation \*/
>         
>         in += in\_step; out += out\_step;
>     
>     }
> 
> }
> 
>   - static void half\_float\_logit(char \**args, const npy\_intp*dimensions,  
>     const npy\_intp *steps, void*data)
> 
>   - {  
>     npy\_intp i; npy\_intp n = dimensions\[0\]; char *in = args\[0\],*out = args\[1\]; npy\_intp in\_step = steps\[0\], out\_step = steps\[1\];
>     
>     float tmp;
>     
>     for (i = 0; i \< n; i++) {
>     
>     > /\* BEGIN main ufunc computation */ tmp = npy\_half\_to\_float(*(npy\_half *)in); tmp /= 1 - tmp; tmp = logf(tmp);*((npy\_half *)out) = npy\_float\_to\_half(tmp); /* END main ufunc computation \*/
>     > 
>     > in += in\_step; out += out\_step;
>     
>     }
> 
> }
> 
> /*This gives pointers to the above functions*/ PyUFuncGenericFunction funcs\[4\] = {\&half\_float\_logit, \&float\_logit, \&double\_logit, \&long\_double\_logit};
> 
>   - static const char types\[8\] = {NPY\_HALF, NPY\_HALF,  
>     NPY\_FLOAT, NPY\_FLOAT, NPY\_DOUBLE, NPY\_DOUBLE, NPY\_LONGDOUBLE, NPY\_LONGDOUBLE};
> 
>   - static struct PyModuleDef moduledef = {  
>     PyModuleDef\_HEAD\_INIT, "npufunc", NULL, -1, LogitMethods, NULL, NULL, NULL, NULL
> 
> };
> 
> PyMODINIT\_FUNC PyInit\_npufunc(void) { PyObject *m,*logit, \*d;
> 
> > import\_array(); import\_umath();
> > 
> > m = PyModule\_Create(\&moduledef); if (\!m) { return NULL; }
> > 
> >   - logit = PyUFunc\_FromFuncAndData(funcs, NULL, types, 4, 1, 1,  
> >     PyUFunc\_None, "logit", "logit\_docstring", 0);
> > 
> > d = PyModule\_GetDict(m);
> > 
> > PyDict\_SetItemString(d, "logit", logit); Py\_DECREF(logit);
> > 
> > return m;
> 
> }

This is a `setup.py` file for the above code. As before, the module `` ` can be build via calling ``python setup.py build`at the command prompt, or installed to site-packages via`python setup.py install`.`\`python ''' setup.py file for multi\_type\_logit.c Note that since this is a numpy extension we add an include\_dirs=\[get\_include()\] so that the extension is built with numpy's C/C++ header files. Furthermore, we also have to include the npymath lib for half-float d-type.

> Calling $python setup.py build\_ext --inplace will build the extension library in the current file.
> 
> Calling $python setup.py build will build a file that looks like ./build/lib\*, where lib\* is a file that begins with lib. The library will be in this file and end with a C library extension, such as .so
> 
> Calling $python setup.py install will install the module in your site-packages file.
> 
> See the setuptools section 'Building Extension Modules' at setuptools.pypa.io for more information. '''
> 
> from setuptools import setup, Extension from numpy import get\_include from os import path
> 
> path\_to\_npymath = path.join(get\_include(), '..', 'lib') npufunc = Extension('npufunc', sources=\['multi\_type\_logit.c'\], include\_dirs=\[get\_include()\], \# Necessary for the half-float d-type. library\_dirs=\[path\_to\_npymath\], libraries=\["npymath"\])
> 
> setup(name='npufunc', version='1.0', ext\_modules=\[npufunc\])

After the above has been installed, it can be imported and used as follows.

\>\>\> import numpy as np `` ` >>> import npufunc >>> npufunc.logit(0.5) np.float64(0.0) >>> a = np.linspace(0,1,5) >>> npufunc.logit(a) array([       -inf, -1.09861229,  0.        ,  1.09861229,         inf])    .. _`sec:NumPy-many-arg`:  Example NumPy ufunc with multiple arguments/return values =========================================================  Our final example is a ufunc with multiple arguments. It is a modification of the code for a logit ufunc for data with a single dtype. We compute ``(A \* B, logit(A \* B))`.  We only give the C code as the setup.py file is exactly the same as the`setup.py``file in `Example NumPy ufunc for one dtype`_, except that the line``\`python npufunc = Extension('npufunc', sources=\['single\_type\_logit.c'\], include\_dirs=\[get\_include()\])

is replaced with

> 
> 
> ``` python
> npufunc = Extension('npufunc',
>                     sources=['multi_arg_logit.c'],
>                     include_dirs=[get_include()])
> ```

The C file is given below. The ufunc generated takes two arguments `A` `` ` and ``B`. It returns a tuple whose first element is`A \* B`and whose second element is`logit(A \* B)`. Note that it automatically supports broadcasting, as well as all other properties of a ufunc.`\`c \#define PY\_SSIZE\_T\_CLEAN \#include \<Python.h\> \#include "numpy/ndarraytypes.h" \#include "numpy/ufuncobject.h" \#include "numpy/halffloat.h" \#include \<math.h\>

>   - /\*
>     
>       - multi\_arg\_logit.c
>       - This is the C code for creating your own
>       - NumPy ufunc for a multiple argument, multiple
>       - return value ufunc. The places where the
>       - ufunc computation is carried out are marked
>       - with comments.
>       - 
>       - Details explaining the Python-C API can be found under
>       - 'Extending and Embedding' and 'Python/C API' at
>     
>     \* docs.python.org. \*/
> 
>   - static PyMethodDef LogitMethods\[\] = {  
>     {NULL, NULL, 0, NULL}
> 
> };
> 
> /\* The loop definition must precede the PyMODINIT\_FUNC. \*/
> 
>   - static void double\_logitprod(char \**args, const npy\_intp*dimensions,  
>     const npy\_intp *steps, void*data)
> 
>   - {  
>     npy\_intp i; npy\_intp n = dimensions\[0\]; char *in1 = args\[0\],*in2 = args\[1\]; char *out1 = args\[2\],*out2 = args\[3\]; npy\_intp in1\_step = steps\[0\], in2\_step = steps\[1\]; npy\_intp out1\_step = steps\[2\], out2\_step = steps\[3\];
>     
>     double tmp;
>     
>       - for (i = 0; i \< n; i++) {  
>         /\* BEGIN main ufunc computation */ tmp =*(double *)in1; tmp*= *(double*)in2; *((double*)out1) = tmp; *((double*)out2) = log(tmp / (1 - tmp)); /\* END main ufunc computation \*/
>         
>         in1 += in1\_step; in2 += in2\_step; out1 += out1\_step; out2 += out2\_step;
>     
>     }
> 
> }
> 
> /*This a pointer to the above function*/ PyUFuncGenericFunction funcs\[1\] = {\&double\_logitprod};
> 
> /\* These are the input and return dtypes of logit.\*/
> 
>   - static const char types\[4\] = {NPY\_DOUBLE, NPY\_DOUBLE,  
>     NPY\_DOUBLE, NPY\_DOUBLE};
> 
>   - static struct PyModuleDef moduledef = {  
>     PyModuleDef\_HEAD\_INIT, "npufunc", NULL, -1, LogitMethods, NULL, NULL, NULL, NULL
> 
> };
> 
> PyMODINIT\_FUNC PyInit\_npufunc(void) { PyObject *m,*logit, \*d;
> 
> > import\_array(); import\_umath();
> > 
> > m = PyModule\_Create(\&moduledef); if (\!m) { return NULL; }
> > 
> >   - logit = PyUFunc\_FromFuncAndData(funcs, NULL, types, 1, 2, 2,  
> >     PyUFunc\_None, "logit", "logit\_docstring", 0);
> > 
> > d = PyModule\_GetDict(m);
> > 
> > PyDict\_SetItemString(d, "logit", logit); Py\_DECREF(logit);
> > 
> > return m;
> 
> }

<div id="sec:NumPy-struct-dtype">

Example NumPy ufunc with structured array dtype arguments `` ` =========================================================  This example shows how to create a ufunc for a structured array dtype. For the example we show a trivial ufunc for adding two arrays with dtype ``'u8,u8,u8'``. The process is a bit different from the other examples since a call to :c`PyUFunc_FromFuncAndData` doesn't fully register ufuncs for custom dtypes and structured array dtypes. We need to also call :c`PyUFunc_RegisterLoopForDescr` to finish setting up the ufunc.  We only give the C code as the``setup.py`file is exactly the same as the`setup.py``file in `Example NumPy ufunc for one dtype`_, except that the line``\`python npufunc = Extension('npufunc', sources=\['single\_type\_logit.c'\], include\_dirs=\[get\_include()\])

</div>

is replaced with

> 
> 
> ``` python
> npufunc = Extension('npufunc',
>                     sources=['add_triplet.c'],
>                     include_dirs=[get_include()])
> ```

The C file is given below.

> 
> 
> ``` c
> #define PY_SSIZE_T_CLEAN
> #include <Python.h>
> #include "numpy/ndarraytypes.h"
> #include "numpy/ufuncobject.h"
> #include "numpy/npy_3kcompat.h"
> #include <math.h>
> 
> /*
>  * add_triplet.c
>  * This is the C code for creating your own
>  * NumPy ufunc for a structured array dtype.
>  *
>  * Details explaining the Python-C API can be found under
>  * 'Extending and Embedding' and 'Python/C API' at
>  * docs.python.org.
>  */
> 
> static PyMethodDef StructUfuncTestMethods[] = {
>     {NULL, NULL, 0, NULL}
> };
> 
> /* The loop definition must precede the PyMODINIT_FUNC. */
> 
> static void add_uint64_triplet(char **args, const npy_intp *dimensions,
>                                const npy_intp *steps, void *data)
> {
>     npy_intp i;
>     npy_intp is1 = steps[0];
>     npy_intp is2 = steps[1];
>     npy_intp os = steps[2];
>     npy_intp n = dimensions[0];
>     uint64_t *x, *y, *z;
> 
>     char *i1 = args[0];
>     char *i2 = args[1];
>     char *op = args[2];
> 
>     for (i = 0; i < n; i++) {
> 
>         x = (uint64_t *)i1;
>         y = (uint64_t *)i2;
>         z = (uint64_t *)op;
> 
>         z[0] = x[0] + y[0];
>         z[1] = x[1] + y[1];
>         z[2] = x[2] + y[2];
> 
>         i1 += is1;
>         i2 += is2;
>         op += os;
>     }
> }
> 
> /* This a pointer to the above function */
> PyUFuncGenericFunction funcs[1] = {&add_uint64_triplet};
> 
> /* These are the input and return dtypes of add_uint64_triplet. */
> static const char types[3] = {NPY_UINT64, NPY_UINT64, NPY_UINT64};
> 
> static struct PyModuleDef moduledef = {
>     PyModuleDef_HEAD_INIT,
>     "struct_ufunc_test",
>     NULL,
>     -1,
>     StructUfuncTestMethods,
>     NULL,
>     NULL,
>     NULL,
>     NULL
> };
> 
> PyMODINIT_FUNC PyInit_npufunc(void)
> {
>     PyObject *m, *add_triplet, *d;
>     PyObject *dtype_dict;
>     PyArray_Descr *dtype;
>     PyArray_Descr *dtypes[3];
> 
>     import_array();
>     import_umath();
> 
>     m = PyModule_Create(&moduledef);
>     if (m == NULL) {
>         return NULL;
>     }
> 
>     /* Create a new ufunc object */
>     add_triplet = PyUFunc_FromFuncAndData(NULL, NULL, NULL, 0, 2, 1,
>                                           PyUFunc_None, "add_triplet",
>                                           "add_triplet_docstring", 0);
> 
>     dtype_dict = Py_BuildValue("[(s, s), (s, s), (s, s)]",
>                                "f0", "u8", "f1", "u8", "f2", "u8");
>     PyArray_DescrConverter(dtype_dict, &dtype);
>     Py_DECREF(dtype_dict);
> 
>     dtypes[0] = dtype;
>     dtypes[1] = dtype;
>     dtypes[2] = dtype;
> 
>     /* Register ufunc for structured dtype */
>     PyUFunc_RegisterLoopForDescr(add_triplet,
>                                  dtype,
>                                  &add_uint64_triplet,
>                                  dtypes,
>                                  NULL);
> 
>     d = PyModule_GetDict(m);
> 
>     PyDict_SetItemString(d, "add_triplet", add_triplet);
>     Py_DECREF(add_triplet);
>     return m;
> }
> ```

<div class="index">

pair: ufunc; adding new

</div>

The returned ufunc object is a callable Python object. It should be `` ` placed in a (module) dictionary under the same name as was used in the name argument to the ufunc-creation routine. The following example is adapted from the umath module ``<span class="title-ref">c static PyUFuncGenericFunction atan2\_functions\[\] = { PyUFunc\_ff\_f, PyUFunc\_dd\_d, PyUFunc\_gg\_g, PyUFunc\_OO\_O\_method}; static void \*atan2\_data\[\] = { (void \*)atan2f, (void \*)atan2, (void \*)atan2l, (void \*)"arctan2"}; static const char atan2\_signatures\[\] = { NPY\_FLOAT, NPY\_FLOAT, NPY\_FLOAT, NPY\_DOUBLE, NPY\_DOUBLE, NPY\_DOUBLE, NPY\_LONGDOUBLE, NPY\_LONGDOUBLE, NPY\_LONGDOUBLE NPY\_OBJECT, NPY\_OBJECT, NPY\_OBJECT}; ... /\* in the module initialization code \*/ PyObject \*f, \*dict, \*module; ... dict = PyModule\_GetDict(module); ... f = PyUFunc\_FromFuncAndData(atan2\_functions, atan2\_data, atan2\_signatures, 4, 2, 1, PyUFunc\_None, "arctan2", "a safe and correct arctan(x1/x2)", 0); PyDict\_SetItemString(dict, "arctan2", f); Py\_DECREF(f); ... </span>\`\`

---

how-to-how-to.md

---

# How to write a NumPy how-to

How-tos get straight to the point -- they

  - answer a focused question, or
  - narrow a broad question into focused questions that the user can choose among.

## A stranger has asked for directions...

**"I need to refuel my car."**

## Give a brief but explicit answer

  - *"Three kilometers/miles, take a right at Hayseed Road, it's on your left."*

Add helpful details for newcomers ("Hayseed Road", even though it's the only turnoff at three km/mi). But not irrelevant ones:

  - Don't also give directions from Route 7.
  - Don't explain why the town has only one filling station.

If there's related background (tutorial, explanation, reference, alternative approach), bring it to the user's attention with a link ("Directions from Route 7," "Why so few filling stations?").

## Delegate

  - *"Three km/mi, take a right at Hayseed Road, follow the signs."*

If the information is already documented and succinct enough for a how-to, just link to it, possibly after an introduction ("Three km/mi, take a right").

## If the question is broad, narrow and redirect it

**"I want to see the sights."**

The *See the sights* how-to should link to a set of narrower how-tos:

  - Find historic buildings
  - Find scenic lookouts
  - Find the town center

and these might in turn link to still narrower how-tos -- so the town center page might link to

  - Find the court house
  - Find city hall

By organizing how-tos this way, you not only display the options for people who need to narrow their question, you also have provided answers for users who start with narrower questions ("I want to see historic buildings," "Which way to city hall?").

## If there are many steps, break them up

If a how-to has many steps:

  - Consider breaking a step out into an individual how-to and linking to it.
  - Include subheadings. They help readers grasp what's coming and return where they left off.

## Why write how-tos when there's Stack Overflow, Reddit, Gitter...?

  - We have authoritative answers.
  - How-tos make the site less forbidding to non-experts.
  - How-tos bring people into the site and help them discover other information that's here .
  - Creating how-tos helps us see NumPy usability through new eyes.

## Aren't how-tos and tutorials the same thing?

People use the terms "how-to" and "tutorial" interchangeably, but we draw a distinction, following Daniele Procida's [taxonomy of documentation](https://documentation.divio.com/).

Documentation needs to meet users where they are. *How-tos* offer get-it-done information; the user wants steps to copy and doesn't necessarily want to understand NumPy. *Tutorials* are warm-fuzzy information; the user wants a feel for some aspect of NumPy (and again, may or may not care about deeper knowledge).

We distinguish both tutorials and how-tos from *Explanations*, which are deep dives intended to give understanding rather than immediate assistance, and *References*, which give complete, authoritative data on some concrete part of NumPy (like its API) but aren't obligated to paint a broader picture.

For more on tutorials, see \[numpy-tutorials:content/tutorial-style-guide\](numpy-tutorials:content/tutorial-style-guide.md)

## Is this page an example of a how-to?

Yes -- until the sections with question-mark headings; they explain rather than giving directions. In a how-to, those would be links.

---

how-to-index.md

---

<div class="currentmodule">

numpy

</div>

# How to index <span class="title-ref">ndarrays \<.ndarray\></span>

<div class="seealso">

\[basics.indexing\](\#basics.indexing)

</div>

This page tackles common examples. For an in-depth look into indexing, refer to \[basics.indexing\](\#basics.indexing).

## Access specific/arbitrary rows and columns

Use \[basic-indexing\](\#basic-indexing) features like \[slicing-and-striding\](\#slicing-and-striding), and \[dimensional-indexing-tools\](\#dimensional-indexing-tools).

> \>\>\> a = np.arange(30).reshape(2, 3, 5) \>\>\> a array(\[\[\[ 0, 1, 2, 3, 4\], \[ 5, 6, 7, 8, 9\], \[10, 11, 12, 13, 14\]\], \<BLANKLINE\> \[\[15, 16, 17, 18, 19\], \[20, 21, 22, 23, 24\], \[25, 26, 27, 28, 29\]\]\]) \>\>\> a\[0, 2, :\] array(\[10, 11, 12, 13, 14\]) \>\>\> a\[0, :, 3\] array(\[ 3, 8, 13\])

Note that the output from indexing operations can have different shape from the original object. To preserve the original dimensions after indexing, you can use <span class="title-ref">newaxis</span>. To use other such tools, refer to \[dimensional-indexing-tools\](\#dimensional-indexing-tools).

> \>\>\> a\[0, :, 3\].shape (3,) \>\>\> a\[0, :, 3, np.newaxis\].shape (3, 1) \>\>\> a\[0, :, 3, np.newaxis, np.newaxis\].shape (3, 1, 1)

Variables can also be used to index:

    >>> y = 0
    >>> a[y, :, y+3]
    array([ 3,  8, 13])

Refer to \[dealing-with-variable-indices\](\#dealing-with-variable-indices) to see how to use `python:slice` and :py\`Ellipsis\` in your index variables.

### Index columns

To index columns, you have to index the last axis. Use \[dimensional-indexing-tools\](\#dimensional-indexing-tools) to get the desired number of dimensions:

    >>> a = np.arange(24).reshape(2, 3, 4)
    >>> a
    array([[[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]],
    <BLANKLINE>
           [[12, 13, 14, 15],
            [16, 17, 18, 19],
            [20, 21, 22, 23]]])
    >>> a[..., 3]
    array([[ 3,  7, 11],
           [15, 19, 23]])

To index specific elements in each column, make use of \[advanced-indexing\](\#advanced-indexing) as below:

    >>> arr = np.arange(3*4).reshape(3, 4)
    >>> arr
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> column_indices = [[1, 3], [0, 2], [2, 2]]
    >>> np.arange(arr.shape[0])
    array([0, 1, 2])
    >>> row_indices = np.arange(arr.shape[0])[:, np.newaxis]
    >>> row_indices
    array([[0],
           [1],
           [2]])

Use the `row_indices` and `column_indices` for advanced indexing:

    >>> arr[row_indices, column_indices]
    array([[ 1,  3],
           [ 4,  6],
           [10, 10]])

### Index along a specific axis

Use <span class="title-ref">take</span>. See also <span class="title-ref">take\_along\_axis</span> and <span class="title-ref">put\_along\_axis</span>.

> \>\>\> a = np.arange(30).reshape(2, 3, 5) \>\>\> a array(\[\[\[ 0, 1, 2, 3, 4\], \[ 5, 6, 7, 8, 9\], \[10, 11, 12, 13, 14\]\], \<BLANKLINE\> \[\[15, 16, 17, 18, 19\], \[20, 21, 22, 23, 24\], \[25, 26, 27, 28, 29\]\]\]) \>\>\> np.take(a, \[2, 3\], axis=2) array(\[\[\[ 2, 3\], \[ 7, 8\], \[12, 13\]\], \<BLANKLINE\> \[\[17, 18\], \[22, 23\], \[27, 28\]\]\]) \>\>\> np.take(a, \[2\], axis=1) array(\[\[\[10, 11, 12, 13, 14\]\], \<BLANKLINE\> \[\[25, 26, 27, 28, 29\]\]\])

## Create subsets of larger matrices

Use \[slicing-and-striding\](\#slicing-and-striding) to access chunks of a large array:

    >>> a = np.arange(100).reshape(10, 10)
    >>> a
    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
            [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
            [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
            [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
            [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
            [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
            [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
            [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])
    >>> a[2:5, 2:5]
    array([[22, 23, 24],
           [32, 33, 34],
           [42, 43, 44]])
    >>> a[2:5, 1:3]
    array([[21, 22],
           [31, 32],
           [41, 42]])
    >>> a[:5, :5]
    array([[ 0,  1,  2,  3,  4],
           [10, 11, 12, 13, 14],
           [20, 21, 22, 23, 24],
           [30, 31, 32, 33, 34],
           [40, 41, 42, 43, 44]])

The same thing can be done with advanced indexing in a slightly more complex way. Remember that \[advanced indexing creates a copy \<indexing-operations\>\](\#advanced-indexing-creates-a-copy-\<indexing-operations\>):

    >>> a[np.arange(5)[:, None], np.arange(5)[None, :]]
    array([[ 0,  1,  2,  3,  4],
           [10, 11, 12, 13, 14],
           [20, 21, 22, 23, 24],
           [30, 31, 32, 33, 34],
           [40, 41, 42, 43, 44]])

You can also use <span class="title-ref">mgrid</span> to generate indices:

    >>> indices = np.mgrid[0:6:2]
    >>> indices
    array([0, 2, 4])
    >>> a[:, indices]
    array([[ 0,  2,  4],
           [10, 12, 14],
           [20, 22, 24],
           [30, 32, 34],
           [40, 42, 44],
           [50, 52, 54],
           [60, 62, 64],
           [70, 72, 74],
           [80, 82, 84],
           [90, 92, 94]])

## Filter values

### Non-zero elements

Use <span class="title-ref">nonzero</span> to get a tuple of array indices of non-zero elements corresponding to every dimension:

    >>> z = np.array([[1, 2, 3, 0], [0, 0, 5, 3], [4, 6, 0, 0]])
    >>> z
    array([[1, 2, 3, 0],
           [0, 0, 5, 3],
           [4, 6, 0, 0]])
    >>> np.nonzero(z)
    (array([0, 0, 0, 1, 1, 2, 2]), array([0, 1, 2, 2, 3, 0, 1]))

Use <span class="title-ref">flatnonzero</span> to fetch indices of elements that are non-zero in the flattened version of the ndarray:

    >>> np.flatnonzero(z)
    array([0, 1, 2, 6, 7, 8, 9])

### Arbitrary conditions

Use <span class="title-ref">where</span> to generate indices based on conditions and then use \[advanced-indexing\](\#advanced-indexing).

> \>\>\> a = np.arange(30).reshape(2, 3, 5) \>\>\> indices = np.where(a % 2 == 0) \>\>\> indices (array(\[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1\]), array(\[0, 0, 0, 1, 1, 2, 2, 2, 0, 0, 1, 1, 1, 2, 2\]), array(\[0, 2, 4, 1, 3, 0, 2, 4, 1, 3, 0, 2, 4, 1, 3\])) \>\>\> a\[indices\] array(\[ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28\])

Or, use \[boolean-indexing\](\#boolean-indexing):

    >>> a > 14
    array([[[False, False, False, False, False],
            [False, False, False, False, False],
            [False, False, False, False, False]],
    <BLANKLINE>
           [[ True,  True,  True,  True,  True],
            [ True,  True,  True,  True,  True],
            [ True,  True,  True,  True,  True]]])
    >>> a[a > 14]
    array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])

### Replace values after filtering

Use assignment with filtering to replace desired values:

    >>> p = np.arange(-10, 10).reshape(2, 2, 5)
    >>> p
    array([[[-10,  -9,  -8,  -7,  -6],
            [ -5,  -4,  -3,  -2,  -1]],
    <BLANKLINE>
           [[  0,   1,   2,   3,   4],
            [  5,   6,   7,   8,   9]]])
    >>> q = p < 0
    >>> q
    array([[[ True,  True,  True,  True,  True],
            [ True,  True,  True,  True,  True]],
    <BLANKLINE>
           [[False, False, False, False, False],
            [False, False, False, False, False]]])
    >>> p[q] = 0
    >>> p
    array([[[0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0]],
    <BLANKLINE>
           [[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]]])

## Fetch indices of max/min values

Use <span class="title-ref">argmax</span> and \`argmin\`:

    >>> a = np.arange(30).reshape(2, 3, 5)
    >>> np.argmax(a)
    29
    >>> np.argmin(a)
    0

Use the `axis` keyword to get the indices of maximum and minimum values along a specific axis:

    >>> np.argmax(a, axis=0)
    array([[1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1]])
    >>> np.argmax(a, axis=1)
    array([[2, 2, 2, 2, 2],
           [2, 2, 2, 2, 2]])
    >>> np.argmax(a, axis=2)
    array([[4, 4, 4],
           [4, 4, 4]])
    <BLANKLINE>
    >>> np.argmin(a, axis=1)
    array([[0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0]])
    >>> np.argmin(a, axis=2)
    array([[0, 0, 0],
           [0, 0, 0]])

Set `keepdims` to `True` to keep the axes which are reduced in the result as dimensions with size one:

    >>> np.argmin(a, axis=2, keepdims=True)
    array([[[0],
            [0],
            [0]],
    <BLANKLINE>
           [[0],
            [0],
            [0]]])
    >>> np.argmax(a, axis=1, keepdims=True)
    array([[[2, 2, 2, 2, 2]],
    <BLANKLINE>
           [[2, 2, 2, 2, 2]]])

To get the indices of each maximum or minimum value for each (N-1)-dimensional array in an N-dimensional array, use <span class="title-ref">reshape</span> to reshape the array to a 2D array, apply <span class="title-ref">argmax</span> or <span class="title-ref">argmin</span> along `axis=1` and use <span class="title-ref">unravel\_index</span> to recover the index of the values per slice:

    >>> x = np.arange(2*2*3).reshape(2, 2, 3) % 7  # 3D example array
    >>> x
    array([[[0, 1, 2],
            [3, 4, 5]],
    <BLANKLINE>
           [[6, 0, 1],
            [2, 3, 4]]])
    >>> x_2d = np.reshape(x, (x.shape[0], -1))
    >>> indices_2d = np.argmax(x_2d, axis=1)
    >>> indices_2d
    array([5, 0])
    >>> np.unravel_index(indices_2d, x.shape[1:])
    (array([1, 0]), array([2, 0]))

The first array returned contains the indices along axis 1 in the original array, the second array contains the indices along axis 2. The highest value in `x[0]` is therefore `x[0, 1, 2]`.

## Index the same ndarray multiple times efficiently

It must be kept in mind that basic indexing produces `views <view>` and advanced indexing produces `copies <copy>`, which are computationally less efficient. Hence, you should take care to use basic indexing wherever possible instead of advanced indexing.

## Further reading

Nicolas Rougier's [100 NumPy exercises](https://github.com/rougier/numpy-100) provide a good insight into how indexing is combined with other operations. Exercises [6](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#6-create-a-null-vector-of-size-10-but-the-fifth-value-which-is-1-), [8](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#8-reverse-a-vector-first-element-becomes-last-), [10](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#10-find-indices-of-non-zero-elements-from-120040-), [15](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#15-create-a-2d-array-with-1-on-the-border-and-0-inside-), [16](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#16-how-to-add-a-border-filled-with-0s-around-an-existing-array-), [19](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#19-create-a-8x8-matrix-and-fill-it-with-a-checkerboard-pattern-), [20](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#20-consider-a-678-shape-array-what-is-the-index-xyz-of-the-100th-element-), [45](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#45-create-random-vector-of-size-10-and-replace-the-maximum-value-by-0-), [59](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#59-how-to-sort-an-array-by-the-nth-column-), [64](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#64-consider-a-given-vector-how-to-add-1-to-each-element-indexed-by-a-second-vector-be-careful-with-repeated-indices-), [65](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#65-how-to-accumulate-elements-of-a-vector-x-to-an-array-f-based-on-an-index-list-i-), [70](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#70-consider-the-vector-1-2-3-4-5-how-to-build-a-new-vector-with-3-consecutive-zeros-interleaved-between-each-value-), [71](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#71-consider-an-array-of-dimension-553-how-to-mulitply-it-by-an-array-with-dimensions-55-), [72](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#72-how-to-swap-two-rows-of-an-array-), [76](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#76-consider-a-one-dimensional-array-z-build-a-two-dimensional-array-whose-first-row-is-z0z1z2-and-each-subsequent-row-is--shifted-by-1-last-row-should-be-z-3z-2z-1-), [80](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#80-consider-an-arbitrary-array-write-a-function-that-extract-a-subpart-with-a-fixed-shape-and-centered-on-a-given-element-pad-with-a-fill-value-when-necessary-), [81](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#81-consider-an-array-z--1234567891011121314-how-to-generate-an-array-r--1234-2345-3456--11121314-), [84](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#84-extract-all-the-contiguous-3x3-blocks-from-a-random-10x10-matrix-), [87](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#87-consider-a-16x16-array-how-to-get-the-block-sum-block-size-is-4x4-), [90](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#90-given-an-arbitrary-number-of-vectors-build-the-cartesian-product-every-combinations-of-every-item-), [93](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#93-consider-two-arrays-a-and-b-of-shape-83-and-22-how-to-find-rows-of-a-that-contain-elements-of-each-row-of-b-regardless-of-the-order-of-the-elements-in-b-), [94](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#94-considering-a-10x3-matrix-extract-rows-with-unequal-values-eg-223-) are specially focused on indexing.

---

how-to-io.md

---

<div id="how-to-io">

</div>

<div class="testsetup">

\>\>\> from numpy.testing import temppath \>\>\> with open("csv.txt", "wt") as f: ... \_ = f.write("1, 2, 3n4,, 6n7, 8, 9") \>\>\> with open("fixedwidth.txt", "wt") as f: ... \_ = f.write("1 2 3n44 6n7 88889") \>\>\> with open("nan.txt", "wt") as f: ... \_ = f.write("1 2 3n44 x 6n7 8888 9") \>\>\> with open("skip.txt", "wt") as f: ... \_ = f.write("1 2 3n44 6n7 888 9") \>\>\> with open("tabs.txt", "wt") as f: ... \_ = f.write("1t2t3n44t t6n7t888t9")

</div>

# Reading and writing files

This page tackles common applications; for the full collection of I/O routines, see \[routines.io\](\#routines.io).

## Reading text and [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) files

### With no missing values

Use <span class="title-ref">numpy.loadtxt</span>.

### With missing values

Use <span class="title-ref">numpy.genfromtxt</span>.

<span class="title-ref">numpy.genfromtxt</span> will either

  - return a \[masked array\<maskedarray.generic\>\](\#masked-array\<maskedarray.generic\>) **masking out missing values** (if `usemask=True`), or
  - **fill in the missing value** with the value specified in `filling_values` (default is `np.nan` for float, -1 for int).

#### With non-whitespace delimiters

> \>\>\> with open("csv.txt", "r") as f: ... print(f.read()) 1, 2, 3 4,, 6 7, 8, 9

##### Masked-array output

> \>\>\> np.genfromtxt("csv.txt", delimiter=",", usemask=True) masked\_array( data=\[\[1.0, 2.0, 3.0\], \[4.0, --, 6.0\], \[7.0, 8.0, 9.0\]\], mask=\[\[False, False, False\], \[False, True, False\], \[False, False, False\]\], fill\_value=1e+20)

##### Array output

> \>\>\> np.genfromtxt("csv.txt", delimiter=",") array(\[\[ 1., 2., 3.\], \[ 4., nan, 6.\], \[ 7., 8., 9.\]\])

##### Array output, specified fill-in value

> \>\>\> np.genfromtxt("csv.txt", delimiter=",", dtype=np.int8, filling\_values=99) array(\[\[ 1, 2, 3\], \[ 4, 99, 6\], \[ 7, 8, 9\]\], dtype=int8)

#### Whitespace-delimited

<span class="title-ref">numpy.genfromtxt</span> can also parse whitespace-delimited data files that have missing values if

  - **Each field has a fixed width**: Use the width as the <span class="title-ref">delimiter</span> argument.:
    
        # File with width=4. The data does not have to be justified (for example,
        # the 2 in row 1), the last column can be less than width (for example, the 6
        # in row 2), and no delimiting character is required (for instance 8888 and 9
        # in row 3)
        
        >>> with open("fixedwidth.txt", "r") as f:
        ...    data = (f.read())
        >>> print(data)
        1   2      3
        44      6
        7   88889
        
        # Showing spaces as ^
        >>> print(data.replace(" ","^"))
        1^^^2^^^^^^3
        44^^^^^^6
        7^^^88889
        
        >>> np.genfromtxt("fixedwidth.txt", delimiter=4)
        array([[1.000e+00, 2.000e+00, 3.000e+00],
               [4.400e+01,       nan, 6.000e+00],
               [7.000e+00, 8.888e+03, 9.000e+00]])

  - **A special value (e.g. "x") indicates a missing field**: Use it as the <span class="title-ref">missing\_values</span> argument.
    
    > \>\>\> with open("nan.txt", "r") as f: ... print(f.read()) 1 2 3 44 x 6 7 8888 9
    > 
    > \>\>\> np.genfromtxt("nan.txt", missing\_values="x") array(\[\[1.000e+00, 2.000e+00, 3.000e+00\], \[4.400e+01, nan, 6.000e+00\], \[7.000e+00, 8.888e+03, 9.000e+00\]\])

  - **You want to skip the rows with missing values**: Set <span class="title-ref">invalid\_raise=False</span>.
    
    > \>\>\> with open("skip.txt", "r") as f: ... print(f.read()) 1 2 3 44 6 7 888 9
    > 
    > \>\>\> np.genfromtxt("skip.txt", invalid\_raise=False) \# doctest: +SKIP \_\_main\_\_:1: ConversionWarning: Some errors were detected \! Line \#2 (got 2 columns instead of 3) array(\[\[ 1., 2., 3.\], \[ 7., 888., 9.\]\])

  - **The delimiter whitespace character is different from the whitespace that indicates missing data**. For instance, if columns are delimited by `\t`, then missing data will be recognized if it consists of one or more spaces.:
    
        >>> with open("tabs.txt", "r") as f:
        ...    data = (f.read())
        >>> print(data)
        1       2       3
        44              6
        7       888     9
        
        # Tabs vs. spaces
        >>> print(data.replace("\t","^"))
        1^2^3
        44^ ^6
        7^888^9
        
        >>> np.genfromtxt("tabs.txt", delimiter="\t", missing_values=" +")
        array([[  1.,   2.,   3.],
               [ 44.,  nan,   6.],
               [  7., 888.,   9.]])

## Read a file in .npy or .npz format

Choices:

  - Use <span class="title-ref">numpy.load</span>. It can read files generated by any of <span class="title-ref">numpy.save</span>, <span class="title-ref">numpy.savez</span>, or <span class="title-ref">numpy.savez\_compressed</span>.
  - Use memory mapping. See <span class="title-ref">numpy.lib.format.open\_memmap</span>.

## Write to a file to be read back by NumPy

### Binary

Use <span class="title-ref">numpy.save</span>, or to store multiple arrays <span class="title-ref">numpy.savez</span> or <span class="title-ref">numpy.savez\_compressed</span>.

For \[security and portability \<how-to-io-pickle-file\>\](\#security-and-portability-\<how-to-io-pickle-file\>), set `allow_pickle=False` unless the dtype contains Python objects, which requires pickling.

Masked arrays `can't currently be saved <MaskedArray.tofile>`, nor can other arbitrary array subclasses.

### Human-readable

<span class="title-ref">numpy.save</span> and <span class="title-ref">numpy.savez</span> create binary files. To **write a human-readable file**, use <span class="title-ref">numpy.savetxt</span>. The array can only be 1- or 2-dimensional, and there's no <span class="title-ref"> savetxtz</span> for multiple files.

### Large arrays

See \[how-to-io-large-arrays\](\#how-to-io-large-arrays).

## Read an arbitrarily formatted binary file ("binary blob")

Use a \[structured array \<basics.rec\>\](structured array \<basics.rec\>.md).

**Example:**

The `.wav` file header is a 44-byte block preceding `data_size` bytes of the actual sound data:

    chunk_id         "RIFF"
    chunk_size       4-byte unsigned little-endian integer
    format           "WAVE"
    fmt_id           "fmt "
    fmt_size         4-byte unsigned little-endian integer
    audio_fmt        2-byte unsigned little-endian integer
    num_channels     2-byte unsigned little-endian integer
    sample_rate      4-byte unsigned little-endian integer
    byte_rate        4-byte unsigned little-endian integer
    block_align      2-byte unsigned little-endian integer
    bits_per_sample  2-byte unsigned little-endian integer
    data_id          "data"
    data_size        4-byte unsigned little-endian integer

The `.wav` file header as a NumPy structured dtype:

    wav_header_dtype = np.dtype([
        ("chunk_id", (bytes, 4)), # flexible-sized scalar type, item size 4
        ("chunk_size", "<u4"),    # little-endian unsigned 32-bit integer
        ("format", "S4"),         # 4-byte string, alternate spelling of (bytes, 4)
        ("fmt_id", "S4"),
        ("fmt_size", "<u4"),
        ("audio_fmt", "<u2"),     #
        ("num_channels", "<u2"),  # .. more of the same ...
        ("sample_rate", "<u4"),   #
        ("byte_rate", "<u4"),
        ("block_align", "<u2"),
        ("bits_per_sample", "<u2"),
        ("data_id", "S4"),
        ("data_size", "<u4"),
        #
        # the sound data itself cannot be represented here:
        # it does not have a fixed size
    ])
    
    header = np.fromfile(f, dtype=wave_header_dtype, count=1)[0]

This `.wav` example is for illustration; to read a `.wav` file in real life, use Python's built-in module `wave`.

(Adapted from Pauli Virtanen, \[advanced\_numpy\](\#advanced\_numpy), licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).)

## Write or read large arrays

**Arrays too large to fit in memory** can be treated like ordinary in-memory arrays using memory mapping.

  - Raw array data written with <span class="title-ref">numpy.ndarray.tofile</span> or <span class="title-ref">numpy.ndarray.tobytes</span> can be read with \`numpy.memmap\`:
    
        array = numpy.memmap("mydata/myarray.arr", mode="r", dtype=np.int16, shape=(1024, 1024))

  - Files output by <span class="title-ref">numpy.save</span> (that is, using the numpy format) can be read using <span class="title-ref">numpy.load</span> with the `mmap_mode` keyword argument:
    
        large_array[some_slice] = np.load("path/to/small_array", mmap_mode="r")

Memory mapping lacks features like data chunking and compression; more full-featured formats and libraries usable with NumPy include:

  - **HDF5**: [h5py](https://www.h5py.org/) or [PyTables](https://www.pytables.org/).
  - **Zarr**: [here](https://zarr.readthedocs.io/en/stable/tutorial.html#reading-and-writing-data).
  - **NetCDF**: <span class="title-ref">scipy.io.netcdf\_file</span>.

For tradeoffs among memmap, Zarr, and HDF5, see [pythonspeed.com](https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/).

## Write files for reading by other (non-NumPy) tools

Formats for **exchanging data** with other tools include HDF5, Zarr, and NetCDF (see \[how-to-io-large-arrays\](\#how-to-io-large-arrays)).

## Write or read a JSON file

NumPy arrays and most NumPy scalars are **not** directly [JSON serializable](https://github.com/numpy/numpy/issues/12481). Instead, use a custom <span class="title-ref">json.JSONEncoder</span> for NumPy types, which can be found using your favorite search engine.

## Save/restore using a pickle file

Avoid when possible; \[pickles \<python:library/pickle\>\](pickles \<python:library/pickle\>.md) are not secure against erroneous or maliciously constructed data.

Use <span class="title-ref">numpy.save</span> and <span class="title-ref">numpy.load</span>. Set `allow_pickle=False`, unless the array dtype includes Python objects, in which case pickling is required.

<span class="title-ref">numpy.load</span> and <span class="title-ref">pickle</span> submodule also support unpickling files created with NumPy 1.26.

## Convert from a pandas DataFrame to a NumPy array

See <span class="title-ref">pandas.Series.to\_numpy</span>.

## Save/restore using <span class="title-ref">\~numpy.ndarray.tofile</span> and <span class="title-ref">\~numpy.fromfile</span>

In general, prefer <span class="title-ref">numpy.save</span> and <span class="title-ref">numpy.load</span>.

<span class="title-ref">numpy.ndarray.tofile</span> and <span class="title-ref">numpy.fromfile</span> lose information on endianness and precision and so are unsuitable for anything but scratch storage.

<div class="testcleanup">

\>\>\> import os \>\>\> \# list all files created in testsetup. If needed there are \>\>\> \# conveniences in e.g. astroquery to do this more automatically \>\>\> for filename in \['csv.txt', 'fixedwidth.txt', 'nan.txt', 'skip.txt', 'tabs.txt'\]: ... os.remove(filename)

</div>

---

how-to-partition.md

---

# How to create arrays with regularly-spaced values

There are a few NumPy functions that are similar in application, but which provide slightly different results, which may cause confusion if one is not sure when and how to use them. The following guide aims to list these functions and describe their recommended usage.

The functions mentioned here are

  - <span class="title-ref">numpy.linspace</span>
  - <span class="title-ref">numpy.arange</span>
  - <span class="title-ref">numpy.geomspace</span>
  - <span class="title-ref">numpy.logspace</span>
  - <span class="title-ref">numpy.meshgrid</span>
  - <span class="title-ref">numpy.mgrid</span>
  - <span class="title-ref">numpy.ogrid</span>

## 1D domains (intervals)

### `linspace` vs. `arange`

Both <span class="title-ref">numpy.linspace</span> and <span class="title-ref">numpy.arange</span> provide ways to partition an interval (a 1D domain) into equal-length subintervals. These partitions will vary depending on the chosen starting and ending points, and the **step** (the length of the subintervals).

  - **Use** <span class="title-ref">numpy.arange</span> **if you want integer steps.**
    
    <span class="title-ref">numpy.arange</span> relies on step size to determine how many elements are in the returned array, which excludes the endpoint. This is determined through the `step` argument to `arange`.
    
    Example:
    
        >>> np.arange(0, 10, 2)  # np.arange(start, stop, step)
        array([0, 2, 4, 6, 8])
    
    The arguments `start` and `stop` should be integer or real, but not complex numbers. <span class="title-ref">numpy.arange</span> is similar to the Python built-in :py\`range\`.
    
    Floating-point inaccuracies can make `arange` results with floating-point numbers confusing. In this case, you should use <span class="title-ref">numpy.linspace</span> instead.

  - **Use** <span class="title-ref">numpy.linspace</span> **if you want the endpoint to be included in the result, or if you are using a non-integer step size.**
    
    <span class="title-ref">numpy.linspace</span> *can* include the endpoint and determines step size from the <span class="title-ref">num</span> argument, which specifies the number of elements in the returned array.
    
    The inclusion of the endpoint is determined by an optional boolean argument `endpoint`, which defaults to `True`. Note that selecting `endpoint=False` will change the step size computation, and the subsequent output for the function.
    
    Example:
    
        >>> np.linspace(0.1, 0.2, num=5)  # np.linspace(start, stop, num)
        array([0.1  , 0.125, 0.15 , 0.175, 0.2  ])
        >>> np.linspace(0.1, 0.2, num=5, endpoint=False)
        array([0.1, 0.12, 0.14, 0.16, 0.18])
    
    <span class="title-ref">numpy.linspace</span> can also be used with complex arguments:
    
        >>> np.linspace(1+1.j, 4, 5, dtype=np.complex64)
        array([1.  +1.j  , 1.75+0.75j, 2.5 +0.5j , 3.25+0.25j, 4.  +0.j  ],
              dtype=complex64)

### Other examples

1.  Unexpected results may happen if floating point values are used as `step` in `numpy.arange`. To avoid this, make sure all floating point conversion happens after the computation of results. For example, replace
    
        >>> list(np.arange(0.1,0.4,0.1).round(1))
        [0.1, 0.2, 0.3, 0.4]  # endpoint should not be included!
    
    with
    
        >>> list(np.arange(1, 4, 1) / 10.0)
        [0.1, 0.2, 0.3]  # expected result

2.  Note that
    
        >>> np.arange(0, 1.12, 0.04)
        array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 , 0.24, 0.28, 0.32, 0.36, 0.4 ,
               0.44, 0.48, 0.52, 0.56, 0.6 , 0.64, 0.68, 0.72, 0.76, 0.8 , 0.84,
               0.88, 0.92, 0.96, 1.  , 1.04, 1.08, 1.12])
    
    and
    
        >>> np.arange(0, 1.08, 0.04)
        array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 , 0.24, 0.28, 0.32, 0.36, 0.4 ,
               0.44, 0.48, 0.52, 0.56, 0.6 , 0.64, 0.68, 0.72, 0.76, 0.8 , 0.84,
               0.88, 0.92, 0.96, 1.  , 1.04])
    
    These differ because of numeric noise. When using floating point values, it is possible that `0 + 0.04 * 28 < 1.12`, and so `1.12` is in the interval. In fact, this is exactly the case:
    
        >>> 1.12/0.04
        28.000000000000004
    
    But `0 + 0.04 * 27 >= 1.08` so that 1.08 is excluded:
    
        >>> 1.08/0.04
        27.0
    
    Alternatively, you could use `np.arange(0, 28)*0.04` which would always give you precise control of the end point since it is integral:
    
        >>> np.arange(0, 28)*0.04
        array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 , 0.24, 0.28, 0.32, 0.36, 0.4 ,
               0.44, 0.48, 0.52, 0.56, 0.6 , 0.64, 0.68, 0.72, 0.76, 0.8 , 0.84,
               0.88, 0.92, 0.96, 1.  , 1.04, 1.08])

### `geomspace` and `logspace`

`numpy.geomspace` is similar to `numpy.linspace`, but with numbers spaced evenly on a log scale (a geometric progression). The endpoint is included in the result.

Example:

    >>> np.geomspace(2, 3, num=5)
    array([2.        , 2.21336384, 2.44948974, 2.71080601, 3.        ])

`numpy.logspace` is similar to `numpy.geomspace`, but with the start and end points specified as logarithms (with base 10 as default):

    >>> np.logspace(2, 3, num=5)
    array([ 100.        ,  177.827941  ,  316.22776602,  562.34132519, 1000.        ])

In linear space, the sequence starts at `base ** start` (`base` to the power of `start`) and ends with `base ** stop`:

    >>> np.logspace(2, 3, num=5, base=2)
    array([4.        , 4.75682846, 5.65685425, 6.72717132, 8.        ])

## N-D domains

N-D domains can be partitioned into *grids*. This can be done using one of the following functions.

### `meshgrid`

The purpose of `numpy.meshgrid` is to create a rectangular grid out of a set of one-dimensional coordinate arrays.

Given arrays:

    >>> x = np.array([0, 1, 2, 3])
    >>> y = np.array([0, 1, 2, 3, 4, 5])

`meshgrid` will create two coordinate arrays, which can be used to generate the coordinate pairs determining this grid.:

    >>> xx, yy = np.meshgrid(x, y)
    >>> xx
    array([[0, 1, 2, 3],
           [0, 1, 2, 3],
           [0, 1, 2, 3],
           [0, 1, 2, 3],
           [0, 1, 2, 3],
           [0, 1, 2, 3]])
    >>> yy
    array([[0, 0, 0, 0],
           [1, 1, 1, 1],
           [2, 2, 2, 2],
           [3, 3, 3, 3],
           [4, 4, 4, 4],
           [5, 5, 5, 5]])
    
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(xx, yy, marker='.', color='k', linestyle='none')

<div class="plot" data-align="center" data-include-source="0">

user/plots/meshgrid\_plot.py

</div>

### `mgrid`

`numpy.mgrid` can be used as a shortcut for creating meshgrids. It is not a function, but when indexed, returns a multidimensional meshgrid.

    >>> xx, yy = np.meshgrid(np.array([0, 1, 2, 3]), np.array([0, 1, 2, 3, 4, 5]))
    >>> xx.T, yy.T
    (array([[0, 0, 0, 0, 0, 0],
            [1, 1, 1, 1, 1, 1],
            [2, 2, 2, 2, 2, 2],
            [3, 3, 3, 3, 3, 3]]),
     array([[0, 1, 2, 3, 4, 5],
            [0, 1, 2, 3, 4, 5],
            [0, 1, 2, 3, 4, 5],
            [0, 1, 2, 3, 4, 5]]))
    
    >>> np.mgrid[0:4, 0:6]
    array([[[0, 0, 0, 0, 0, 0],
            [1, 1, 1, 1, 1, 1],
            [2, 2, 2, 2, 2, 2],
            [3, 3, 3, 3, 3, 3]],
    <BLANKLINE>
           [[0, 1, 2, 3, 4, 5],
            [0, 1, 2, 3, 4, 5],
            [0, 1, 2, 3, 4, 5],
            [0, 1, 2, 3, 4, 5]]])

### `ogrid`

Similar to `numpy.mgrid`, `numpy.ogrid` returns an *open* multidimensional meshgrid. This means that when it is indexed, only one dimension of each returned array is greater than 1. This avoids repeating the data and thus saves memory, which is often desirable.

These sparse coordinate grids are intended to be use with \[broadcasting\](\#broadcasting). When all coordinates are used in an expression, broadcasting still leads to a fully-dimensional result array.

    >>> np.ogrid[0:4, 0:6]
    (array([[0],
            [1],
            [2],
            [3]]), array([[0, 1, 2, 3, 4, 5]]))

All three methods described here can be used to evaluate function values on a grid.

    >>> g = np.ogrid[0:4, 0:6]
    >>> zg = np.sqrt(g[0]**2 + g[1]**2)
    >>> g[0].shape, g[1].shape, zg.shape
    ((4, 1), (1, 6), (4, 6))
    >>> m = np.mgrid[0:4, 0:6]
    >>> zm = np.sqrt(m[0]**2 + m[1]**2)
    >>> np.array_equal(zm, zg)
    True

---

how-to-verify-bug.md

---

# Verifying bugs and bug fixes in NumPy

In this how-to you will learn how to:

  - Verify the existence of a bug in NumPy
  - Verify the fix, if any, made for the bug

While you walk through the verification process, you will learn how to:

  - Set up a Python virtual environment (using `virtualenv`)
  - Install appropriate versions of NumPy, first to see the bug in action, then to verify its fix

[Issue 16354](https://github.com/numpy/numpy/issues/16354) is used as an example.

This issue was:

> **Title**: *np.polymul return type is np.float64 or np.complex128 when given an all-zero argument*
> 
> *np.polymul returns an object with type np.float64 when one argument is all zero, and both arguments have type np.int64 or np.float32. Something similar happens with all zero np.complex64 giving result type np.complex128.*
> 
> *This doesn't happen with non-zero arguments; there the result is as expected.*
> 
> *This bug isn't present in np.convolve.*
> 
> **Reproducing code example**:
> 
>     >>> import numpy as np
>     >>> np.__version__
>     '1.18.4'
>     >>> a = np.array([1,2,3])
>     >>> z = np.array([0,0,0])
>     >>> np.polymul(a.astype(np.int64), a.astype(np.int64)).dtype
>     dtype('int64')
>     >>> np.polymul(a.astype(np.int64), z.astype(np.int64)).dtype
>     dtype('float64')
>     >>> np.polymul(a.astype(np.float32), z.astype(np.float32)).dtype
>     dtype('float64')
>     >>> np.polymul(a.astype(np.complex64), z.astype(np.complex64)).dtype
>     dtype('complex128')
>     Numpy/Python version information:
>     >>> import sys, numpy; print(numpy.__version__, sys.version)
>     1.18.4 3.7.5 (default, Nov  7 2019, 10:50:52) [GCC 8.3.0]

## 1\. Set up a virtual environment

Create a new directory, enter into it, and set up a virtual environment using your preferred method. For example, this is how to do it using `virtualenv` on linux or macOS:

    virtualenv venv_np_bug
    source venv_np_bug/bin/activate

This ensures the system/global/default Python/NumPy installation will not be altered.

## 2\. Install the NumPy version in which the bug was reported

The report references NumPy version 1.18.4, so that is the version you need to install in this case.

Since this bug is tied to a release and not a specific commit, a pre-built wheel installed in your virtual environment via `pip` will suffice:

    pip install numpy==1.18.4

Some bugs may require you to build the NumPy version referenced in the issue report. To learn how to do that, visit \[Building from source \<building-from-source\>\](\#building-from-source-\<building-from-source\>).

## 3\. Reproduce the bug

The issue reported in [\#16354](https://github.com/numpy/numpy/issues/16354) is that the wrong `dtype` is returned if one of the inputs of the method <span class="title-ref">numpy.polymul</span> is a zero array.

To reproduce the bug, start a Python terminal, enter the code snippet shown in the bug report, and ensure that the results match those in the issue:

    >>> import numpy as np
    >>> np.__version__
    '...' # 1.18.4
    >>> a = np.array([1,2,3])
    >>> z = np.array([0,0,0])
    >>> np.polymul(a.astype(np.int64), a.astype(np.int64)).dtype
    dtype('int64')
    >>> np.polymul(a.astype(np.int64), z.astype(np.int64)).dtype
    dtype('...') # float64
    >>> np.polymul(a.astype(np.float32), z.astype(np.float32)).dtype
    dtype('...') # float64
    >>> np.polymul(a.astype(np.complex64), z.astype(np.complex64)).dtype
    dtype('...') # complex128

As reported, whenever the zero array, `z` in the example above, is one of the arguments to <span class="title-ref">numpy.polymul</span>, an incorrect `dtype` is returned.

## 4\. Check for fixes in the latest version of NumPy

If the issue report for your bug has not yet been resolved, further action or patches need to be submitted.

In this case, however, the issue was resolved by [PR 17577](https://github.com/numpy/numpy/pull/17577) and is now closed. So you can try to verify the fix.

To verify the fix:

1.  Uninstall the version of NumPy in which the bug still exists:
    
        pip uninstall numpy

2.  Install the latest version of NumPy:
    
        pip install numpy

3.  In your Python terminal, run the reported code snippet you used to verify the existence of the bug and confirm that the issue has been resolved:
    
        >>> import numpy as np
        >>> np.__version__
        '...' # 1.18.4
        >>> a = np.array([1,2,3])
        >>> z = np.array([0,0,0])
        >>> np.polymul(a.astype(np.int64), a.astype(np.int64)).dtype
        dtype('int64')
        >>> np.polymul(a.astype(np.int64), z.astype(np.int64)).dtype
        dtype('int64')
        >>> np.polymul(a.astype(np.float32), z.astype(np.float32)).dtype
        dtype('float32')
        >>> np.polymul(a.astype(np.complex64), z.astype(np.complex64)).dtype
        dtype('complex64')

Note that the correct `dtype` is now returned even when a zero array is one of the arguments to <span class="title-ref">numpy.polymul</span>.

## 5\. Support NumPy development by verifying and fixing bugs

Go to the [NumPy GitHub issues page](https://github.com/numpy/numpy/issues) and see if you can confirm the existence of any other bugs which have not been confirmed yet. In particular, it is useful for the developers to know if a bug can be reproduced on a newer version of NumPy.

Comments verifying the existence of bugs alert the NumPy developers that more than one user can reproduce the issue.

---

howtos_index.md

---

# NumPy how-tos

These documents are intended as recipes to common tasks using NumPy. For detailed reference documentation of the functions and classes contained in the package, see the \[API reference \<reference\>\](\#api-reference-\<reference\>).

<div class="toctree" data-maxdepth="1">

how-to-how-to how-to-io how-to-index how-to-verify-bug how-to-partition

</div>

---

index.md

---

# NumPy user guide

This guide is an overview and explains the important features; details are found in \[reference\](\#reference).

<div class="toctree" data-caption="Getting started" data-maxdepth="1">

whatisnumpy Installation \<<https://numpy.org/install/>\> quickstart absolute\_beginners

</div>

<div class="toctree" data-caption="Fundamentals and usage" data-maxdepth="2">

basics

</div>

<div class="toctree" data-maxdepth="1">

numpy-for-matlab-users NumPy tutorials \<<https://numpy.org/numpy-tutorials/>\> howtos\_index

</div>

<div class="toctree" data-caption="Advanced usage and interoperability" data-maxdepth="1">

c-info ../f2py/index ../dev/underthehood basics.interoperability

</div>

<div class="toctree" hidden="" data-caption="Extras">

../glossary ../release ../numpy\_2\_0\_migration\_guide ../license

</div>

---

install.md

---

- orphan

# Installing NumPy

See [Installing NumPy](https://numpy.org/install/).

---

misc.md

---

- orphan

# Miscellaneous

## IEEE 754 floating point special values

Special values defined in numpy: nan, inf,

NaNs can be used as a poor-man's mask (if you don't care what the original value was)

Note: cannot use equality to test NaNs. E.g.: :

    >>> myarr = np.array([1., 0., np.nan, 3.])
    >>> np.nonzero(myarr == np.nan)
    (array([], dtype=int64),)
    >>> np.nan == np.nan  # is always False! Use special numpy functions instead.
    False
    >>> myarr[myarr == np.nan] = 0. # doesn't work
    >>> myarr
    array([  1.,   0.,  nan,   3.])
    >>> myarr[np.isnan(myarr)] = 0. # use this instead find
    >>> myarr
    array([1.,  0.,  0.,  3.])

Other related special value functions: :

    isinf():    True if value is inf
    isfinite(): True if not nan or inf
    nan_to_num(): Map nan to 0, inf to max float, -inf to min float

The following corresponds to the usual functions except that nans are excluded from the results: :

    nansum()
    nanmax()
    nanmin()
    nanargmax()
    nanargmin()
    
    >>> x = np.arange(10.)
    >>> x[3] = np.nan
    >>> x.sum()
    nan
    >>> np.nansum(x)
    42.0

## How numpy handles numerical exceptions

The default is to `'warn'` for `invalid`, `divide`, and `overflow` and `'ignore'` for `underflow`. But this can be changed, and it can be set individually for different kinds of exceptions. The different behaviors are:

>   - 'ignore' : Take no action when the exception occurs.
>   - 'warn' : Print a <span class="title-ref">RuntimeWarning</span> (via the Python <span class="title-ref">warnings</span> module).
>   - 'raise' : Raise a <span class="title-ref">FloatingPointError</span>.
>   - 'call' : Call a function specified using the <span class="title-ref">seterrcall</span> function.
>   - 'print' : Print a warning directly to `stdout`.
>   - 'log' : Record error in a Log object specified by <span class="title-ref">seterrcall</span>.

These behaviors can be set for all kinds of errors or specific ones:

>   - all : apply to all numeric exceptions
>   - invalid : when NaNs are generated
>   - divide : divide by zero (for integers as well\!)
>   - overflow : floating point overflows
>   - underflow : floating point underflows

Note that integer divide-by-zero is handled by the same machinery. These behaviors are set on a per-thread basis.

## Examples

    >>> oldsettings = np.seterr(all='warn')
    >>> np.zeros(5,dtype=np.float32)/0.
    Traceback (most recent call last):
    ...
    RuntimeWarning: invalid value encountered in divide
    >>> j = np.seterr(under='ignore')
    >>> np.array([1.e-100])**10
    array([0.])
    >>> j = np.seterr(invalid='raise')
    >>> np.sqrt(np.array([-1.]))
    Traceback (most recent call last):
    ...
    FloatingPointError: invalid value encountered in sqrt
    >>> def errorhandler(errstr, errflag):
    ...      print("saw stupid error!")
    >>> np.seterrcall(errorhandler)
    >>> j = np.seterr(all='call')
    >>> np.zeros(5, dtype=np.int32)/0
    saw stupid error!
    array([nan, nan, nan, nan, nan])
    >>> j = np.seterr(**oldsettings) # restore previous
    ...                              # error-handling settings

## Interfacing to C

Only a survey of the choices. Little detail on how each works.

1)  Bare metal, wrap your own C-code manually.

>   - Plusses:
>       - Efficient
>       - No dependencies on other tools
>   - Minuses:
>       - Lots of learning overhead:
>           - need to learn basics of Python C API
>           - need to learn basics of numpy C API
>           - need to learn how to handle reference counting and love it.
>       - Reference counting often difficult to get right.
>           - getting it wrong leads to memory leaks, and worse, segfaults

2)  Cython

>   - Plusses:
>       - avoid learning C API's
>       - no dealing with reference counting
>       - can code in pseudo python and generate C code
>       - can also interface to existing C code
>       - should shield you from changes to Python C api
>       - has become the de-facto standard within the scientific Python community
>       - fast indexing support for arrays
>   - Minuses:
>       - Can write code in non-standard form which may become obsolete
>       - Not as flexible as manual wrapping

3)  ctypes

>   - Plusses:
>       - part of Python standard library
>     
>       - good for interfacing to existing shareable libraries, particularly Windows DLLs
>     
>       - avoids API/reference counting issues
>     
>       - good numpy support: arrays have all these in their ctypes attribute: :
>         
>             a.ctypes.data
>             a.ctypes.data_as
>             a.ctypes.shape
>             a.ctypes.shape_as
>             a.ctypes.strides
>             a.ctypes.strides_as
>   - Minuses:
>       - can't use for writing code to be turned into C extensions, only a wrapper tool.

4)  SWIG (automatic wrapper generator)

>   - Plusses:
>       - around a long time
>       - multiple scripting language support
>       - C++ support
>       - Good for wrapping large (many functions) existing C libraries
>   - Minuses:
>       - generates lots of code between Python and the C code
>       - can cause performance problems that are nearly impossible to optimize out
>       - interface files can be hard to write
>       - doesn't necessarily avoid reference counting issues or needing to know API's

5)  Psyco

>   - Plusses:
>       - Turns pure python into efficient machine code through jit-like optimizations
>       - very fast when it optimizes well
>   - Minuses:
>       - Only on intel (windows?)
>       - Doesn't do much for numpy?

## Interfacing to Fortran:

The clear choice to wrap Fortran code is [f2py](https://docs.scipy.org/doc/numpy/f2py/).

Pyfort is an older alternative, but not supported any longer. Fwrap is a newer project that looked promising but isn't being developed any longer.

## Interfacing to C++:

> 1)  Cython
> 2)  CXX
> 3)  Boost.python
> 4)  SWIG
> 5)  SIP (used mainly in PyQT)

---

numpy-for-matlab-users.md

---

# NumPy for MATLAB users

## Introduction

MATLABÂ® and NumPy have a lot in common, but NumPy was created to work with Python, not to be a MATLAB clone. This guide will help MATLAB users get started with NumPy.

<style>
table.docutils td { border: solid 1px #ccc; }
</style>

## Some key differences

|                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| In MATLAB, the basic type, even for scalars, is a multidimensional array. Array assignments in MATLAB are stored as 2D arrays of double precision floating point numbers, unless you specify the number of dimensions and type. Operations on the 2D instances of these arrays are modeled on matrix operations in linear algebra. | In NumPy, the basic type is a multidimensional `array`. Array assignments in NumPy are usually stored as \[n-dimensional arrays\<arrays\>\](\#n-dimensional-arrays\<arrays\>) with the minimum type required to hold the objects in sequence, unless you specify the number of dimensions and type. NumPy performs operations element-by-element, so multiplying 2D arrays with `*` is not a matrix multiplication -- it's an element-by-element multiplication. (The `@` operator, available since Python 3.5, can be used for conventional matrix multiplication.) |
| MATLAB numbers indices from 1; `a(1)` is the first element. \[See note INDEXING \<numpy-for-matlab-users.notes\>\](\#see-note-indexing-\<numpy-for-matlab-users.notes\>)                                                                                                                                                           | NumPy, like Python, numbers indices from 0; `a[0]` is the first element.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| MATLAB's scripting language was created for linear algebra so the syntax for some array manipulations is more compact than NumPy's. On the other hand, the API for adding GUIs and creating full-fledged applications is more or less an afterthought.                                                                             | NumPy is based on Python, a general-purpose language. The advantage to NumPy is access to Python libraries including: [SciPy](https://www.scipy.org/), [Matplotlib](https://matplotlib.org/), [Pandas](https://pandas.pydata.org/), [OpenCV](https://opencv.org/), and more. In addition, Python is often [embedded as a scripting language](https://en.wikipedia.org/wiki/List_of_Python_software#Embedded_as_a_scripting_language) in other software, allowing NumPy to be used there too.                                                                         |
| MATLAB array slicing uses pass-by-value semantics, with a lazy copy-on-write scheme to prevent creating copies until they are needed. Slicing operations copy parts of the array.                                                                                                                                                  | NumPy array slicing uses pass-by-reference, that does not copy the arguments. Slicing operations are views into an array.                                                                                                                                                                                                                                                                                                                                                                                                                                            |

## Rough equivalents

The table below gives rough equivalents for some common MATLAB expressions. These are similar expressions, not equivalents. For details, see the \[documentation\<reference\>\](\#documentation\<reference\>).

In the table below, it is assumed that you have executed the following commands in Python:

    import numpy as np
    from scipy import io, integrate, linalg, signal
    from scipy.sparse.linalg import cg, eigs

Also assume below that if the Notes talk about "matrix" that the arguments are two-dimensional entities.

### General purpose equivalents

<table>
<thead>
<tr class="header">
<th>MATLAB</th>
<th>NumPy</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>help func</code></td>
<td><code>info(func)</code> or <code>help(func)</code> or <code>func?</code> (in IPython)</td>
<td>get help on the function <em>func</em></td>
</tr>
<tr class="even">
<td><code>which func</code></td>
<td>[see note HELP &lt;numpy-for-matlab-users.notes&gt;](#see-note-help-&lt;numpy-for-matlab-users.notes&gt;)</td>
<td>find out where <em>func</em> is defined</td>
</tr>
<tr class="odd">
<td><code>type func</code></td>
<td><code>np.source(func)</code> or <code>func??</code> (in IPython)</td>
<td>print source for <em>func</em> (if not a native function)</td>
</tr>
<tr class="even">
<td><code>% comment</code></td>
<td><code># comment</code></td>
<td>comment a line of code with the text <code>comment</code></td>
</tr>
<tr class="odd">
<td><pre><code>for i=1:3
    fprintf(&#39;%i\n&#39;,i)
end</code></pre></td>
<td><pre><code>for i in range(1, 4):
   print(i)</code></pre></td>
<td>use a for-loop to print the numbers 1, 2, and 3 using :py`range &lt;range&gt;`</td>
</tr>
<tr class="even">
<td><code>a &amp;&amp; b</code></td>
<td><code>a and b</code></td>
<td>short-circuiting logical AND operator ([Python native operator &lt;python:boolean&gt;](#python-native-operator-&lt;python:boolean&gt;)); scalar arguments only</td>
</tr>
<tr class="odd">
<td><code>a || b</code></td>
<td><code>a or b</code></td>
<td>short-circuiting logical OR operator ([Python native operator &lt;python:boolean&gt;](#python-native-operator-&lt;python:boolean&gt;)); scalar arguments only</td>
</tr>
<tr class="even">
<td><div class="sourceCode" id="cb3"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="op">&gt;&gt;</span> <span class="fl">4</span> <span class="op">==</span> <span class="fl">4</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="va">ans</span> <span class="op">=</span> <span class="fl">1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="op">&gt;&gt;</span> <span class="fl">4</span> <span class="op">==</span> <span class="fl">5</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="va">ans</span> <span class="op">=</span> <span class="fl">0</span></span></code></pre></div></td>
<td><pre><code>&gt;&gt;&gt; 4 == 4
True
&gt;&gt;&gt; 4 == 5
False</code></pre></td>
<td>The [boolean objects &lt;python:bltin-boolean-values&gt;](#boolean-objects-&lt;python:bltin-boolean-values&gt;) in Python are <code>True</code> and <code>False</code>, as opposed to MATLAB logical types of <code>1</code> and <code>0</code>.</td>
</tr>
<tr class="odd">
<td><div class="sourceCode" id="cb5"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="va">a</span><span class="op">=</span><span class="fl">4</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="kw">if</span> <span class="va">a</span><span class="op">==</span><span class="fl">4</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>    <span class="va">fprintf</span>(<span class="ss">&#39;a = 4\n&#39;</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="kw">elseif</span> <span class="va">a</span><span class="op">==</span><span class="fl">5</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>    <span class="va">fprintf</span>(<span class="ss">&#39;a = 5\n&#39;</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a><span class="kw">end</span></span></code></pre></div></td>
<td><pre><code>a = 4
if a == 4:
    print(&#39;a = 4&#39;)
elif a == 5:
    print(&#39;a = 5&#39;)</code></pre></td>
<td>create an if-else statement to check if <code>a</code> is 4 or 5 and print result</td>
</tr>
<tr class="even">
<td><code>1*i</code>, <code>1*j</code>, <code>1i</code>, <code>1j</code></td>
<td><code>1j</code></td>
<td>complex numbers</td>
</tr>
<tr class="odd">
<td><code>eps</code></td>
<td><code>np.finfo(float).eps</code> or <code>np.spacing(1)</code></td>
<td>distance from 1 to the next larger representable real number in double precision</td>
</tr>
<tr class="even">
<td><code>load data.mat</code></td>
<td><code>io.loadmat('data.mat')</code></td>
<td>Load MATLAB variables saved to the file <code>data.mat</code>. (Note: When saving arrays to <code>data.mat</code> in MATLAB/Octave, use a recent binary format. <span class="title-ref">scipy.io.loadmat</span> will create a dictionary with the saved arrays and further information.)</td>
</tr>
<tr class="odd">
<td><code>ode45</code></td>
<td><code>integrate.solve_ivp(f)</code></td>
<td>integrate an ODE with Runge-Kutta 4,5</td>
</tr>
<tr class="even">
<td><code>ode15s</code></td>
<td><code>integrate.solve_ivp(f, method='BDF')</code></td>
<td>integrate an ODE with BDF method</td>
</tr>
</tbody>
</table>

### Linear algebra equivalents

| MATLAB                         | NumPy                                                        | Notes                                                                                                                                                                                                                                           |
| ------------------------------ | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ndims(a)`                     | `np.ndim(a)` or `a.ndim`                                     | number of dimensions of array `a`                                                                                                                                                                                                               |
| `numel(a)`                     | `np.size(a)` or `a.size`                                     | number of elements of array `a`                                                                                                                                                                                                                 |
| `size(a)`                      | `np.shape(a)` or `a.shape`                                   | "size" of array `a`                                                                                                                                                                                                                             |
| `size(a,n)`                    | `a.shape[n-1]`                                               | get the number of elements of the n-th dimension of array `a`. (Note that MATLAB uses 1 based indexing while Python uses 0 based indexing, See note \[INDEXING \<numpy-for-matlab-users.notes\>\](\#indexing-\<numpy-for-matlab-users.notes\>)) |
| `[ 1 2 3; 4 5 6 ]`             | `np.array([[1., 2., 3.], [4., 5., 6.]])`                     | define a 2x3 2D array                                                                                                                                                                                                                           |
| `[ a b; c d ]`                 | `np.block([[a, b], [c, d]])`                                 | construct a matrix from blocks `a`, `b`, `c`, and `d`                                                                                                                                                                                           |
| `a(end)`                       | `a[-1]`                                                      | access last element in MATLAB vector (1xn or nx1) or 1D NumPy array `a` (length n)                                                                                                                                                              |
| `a(2,5)`                       | `a[1, 4]`                                                    | access element in second row, fifth column in 2D array `a`                                                                                                                                                                                      |
| `a(2,:)`                       | `a[1]` or `a[1, :]`                                          | entire second row of 2D array `a`                                                                                                                                                                                                               |
| `a(1:5,:)`                     | `a[0:5]` or `a[:5]` or `a[0:5, :]`                           | first 5 rows of 2D array `a`                                                                                                                                                                                                                    |
| `a(end-4:end,:)`               | `a[-5:]`                                                     | last 5 rows of 2D array `a`                                                                                                                                                                                                                     |
| `a(1:3,5:9)`                   | `a[0:3, 4:9]`                                                | The first through third rows and fifth through ninth columns of a 2D array, `a`.                                                                                                                                                                |
| `a([2,4,5],[1,3])`             | `a[np.ix_([1, 3, 4], [0, 2])]`                               | rows 2,4 and 5 and columns 1 and 3. This allows the matrix to be modified, and doesn't require a regular slice.                                                                                                                                 |
| `a(3:2:21,:)`                  | `a[2:21:2,:]`                                                | every other row of `a`, starting with the third and going to the twenty-first                                                                                                                                                                   |
| `a(1:2:end,:)`                 | `a[::2, :]`                                                  | every other row of `a`, starting with the first                                                                                                                                                                                                 |
| `a(end:-1:1,:)` or `flipud(a)` | `a[::-1,:]`                                                  | `a` with rows in reverse order                                                                                                                                                                                                                  |
| `a([1:end 1],:)`               | `a[np.r_[:len(a),0]]`                                        | `a` with copy of the first row appended to the end                                                                                                                                                                                              |
| `a.'`                          | `a.transpose()` or `a.T`                                     | transpose of `a`                                                                                                                                                                                                                                |
| `a'`                           | `a.conj().transpose()` or `a.conj().T`                       | conjugate transpose of `a`                                                                                                                                                                                                                      |
| `a * b`                        | `a @ b`                                                      | matrix multiply                                                                                                                                                                                                                                 |
| `a .* b`                       | `a * b`                                                      | element-wise multiply                                                                                                                                                                                                                           |
| `a./b`                         | `a/b`                                                        | element-wise divide                                                                                                                                                                                                                             |
| `a.^3`                         | `a**3`                                                       | element-wise exponentiation                                                                                                                                                                                                                     |
| `(a > 0.5)`                    | `(a > 0.5)`                                                  | matrix whose i,jth element is (a\_ij \> 0.5). The MATLAB result is an array of logical values 0 and 1. The NumPy result is an array of the boolean values `False` and `True`.                                                                   |
| `find(a > 0.5)`                | `np.nonzero(a > 0.5)`                                        | find the indices where (`a` \> 0.5)                                                                                                                                                                                                             |
| `a(:,find(v > 0.5))`           | `a[:,np.nonzero(v > 0.5)[0]]`                                | extract the columns of `a` where vector v \> 0.5                                                                                                                                                                                                |
| `a(:,find(v>0.5))`             | `a[:, v.T > 0.5]`                                            | extract the columns of `a` where column vector v \> 0.5                                                                                                                                                                                         |
| `a(a<0.5)=0`                   | `a[a < 0.5]=0`                                               | `a` with elements less than 0.5 zeroed out                                                                                                                                                                                                      |
| `a .* (a>0.5)`                 | `a * (a > 0.5)`                                              | `a` with elements less than 0.5 zeroed out                                                                                                                                                                                                      |
| `a(:) = 3`                     | `a[:] = 3`                                                   | set all values to the same scalar value                                                                                                                                                                                                         |
| `y=x`                          | `y = x.copy()`                                               | NumPy assigns by reference                                                                                                                                                                                                                      |
| `y=x(2,:)`                     | `y = x[1, :].copy()`                                         | NumPy slices are by reference                                                                                                                                                                                                                   |
| `y=x(:)`                       | `y = x.flatten()`                                            | turn array into vector (note that this forces a copy). To obtain the same data ordering as in MATLAB, use `x.flatten('F')`.                                                                                                                     |
| `1:10`                         | `np.arange(1., 11.)` or `np.r_[1.:11.]` or `np.r_[1:10:10j]` | create an increasing vector (see note \[RANGES \<numpy-for-matlab-users.notes\>\](\#ranges                                                                                                                                                      |

\-------\<numpy-for-matlab-users.notes\>))

>   -   - `0:9`
>       - `np.arange(10.)` or `np.r_[:10.]` or `np.r_[:9:10j]`
>       - create an increasing vector (see note \[RANGES \<numpy-for-matlab-users.notes\>\](\#ranges

\-------\<numpy-for-matlab-users.notes\>))

>   -   - `[1:10]'`
>       - `np.arange(1.,11.)[:, np.newaxis]`
>       - create a column vector
> 
>   -   - `zeros(3,4)`
>       - `np.zeros((3, 4))`
>       - 3x4 two-dimensional array full of 64-bit floating point zeros
> 
>   -   - `zeros(3,4,5)`
>       - `np.zeros((3, 4, 5))`
>       - 3x4x5 three-dimensional array full of 64-bit floating point zeros
> 
>   -   - `ones(3,4)`
>       - `np.ones((3, 4))`
>       - 3x4 two-dimensional array full of 64-bit floating point ones
> 
>   -   - `eye(3)`
>       - `np.eye(3)`
>       - 3x3 identity matrix
> 
>   -   - `diag(a)`
>       - `np.diag(a)`
>       - returns a vector of the diagonal elements of 2D array, `a`
> 
>   -   - `diag(v,0)`
>       - `np.diag(v, 0)`
>       - returns a square diagonal matrix whose nonzero values are the elements of vector, `v`
> 
>   -   - ``` matlab
>         rng(42,'twister')
>         rand(3,4)
>         ```
>     
>       - ``` 
>         from numpy.random import default_rng
>         rng = default_rng(42)
>         rng.random((3, 4))
>         ```
>         
>         or older version: `random.rand((3, 4))`
>     
>       - generate a random 3x4 array with default random number generator and seed = 42
> 
>   -   - `linspace(1,3,4)`
>       - `np.linspace(1,3,4)`
>       - 4 equally spaced samples between 1 and 3, inclusive
> 
>   -   - `[x,y]=meshgrid(0:8,0:5)`
>       - `np.mgrid[0:9.,0:6.]` or `np.meshgrid(r_[0:9.],r_[0:6.])`
>       - two 2D arrays: one of x values, the other of y values
> 
>   -   - 
>       - `ogrid[0:9.,0:6.]` or `np.ix_(np.r_[0:9.],np.r_[0:6.]`
>       - the best way to eval functions on a grid
> 
>   -   - `[x,y]=meshgrid([1,2,4],[2,4,5])`
>       - `np.meshgrid([1,2,4],[2,4,5])`
>       - 
>   -   - 
>       - `np.ix_([1,2,4],[2,4,5])`
>       - the best way to eval functions on a grid
> 
>   -   - `repmat(a, m, n)`
>       - `np.tile(a, (m, n))`
>       - create m by n copies of `a`
> 
>   -   - `[a b]`
>       - `np.concatenate((a,b),1)` or `np.hstack((a,b))` or `np.column_stack((a,b))` or `np.c_[a,b]`
>       - concatenate columns of `a` and `b`
> 
>   -   - `[a; b]`
>       - `np.concatenate((a,b))` or `np.vstack((a,b))` or `np.r_[a,b]`
>       - concatenate rows of `a` and `b`
> 
>   -   - `max(max(a))`
>       - `a.max()` or `np.nanmax(a)`
>       - maximum element of `a` (with ndims(a)\<=2 for MATLAB, if there are NaN's, `nanmax` will ignore these and return largest value)
> 
>   -   - `max(a)`
>       - `a.max(0)`
>       - maximum element of each column of array `a`
> 
>   -   - `max(a,[],2)`
>       - `a.max(1)`
>       - maximum element of each row of array `a`
> 
>   -   - `max(a,b)`
>       - `np.maximum(a, b)`
>       - compares `a` and `b` element-wise, and returns the maximum value from each pair
> 
>   -   - `norm(v)`
>       - `np.sqrt(v @ v)` or `np.linalg.norm(v)`
>       - L2 norm of vector `v`
> 
>   -   - `a & b`
>       - `logical_and(a,b)`
>       - element-by-element AND operator (NumPy ufunc) \[See note LOGICOPS \<numpy-for-matlab-users.notes\>\](\#see-note

\-------logicops-\<numpy-for-matlab-users.notes\>)

>   -   - `a | b`
>       - `np.logical_or(a,b)`
>       - element-by-element OR operator (NumPy ufunc) \[See note LOGICOPS \<numpy-for-matlab-users.notes\>\](\#see-note-logicops

\-------\<numpy-for-matlab-users.notes\>)

>   -   - `bitand(a,b)`
>       - `a & b`
>       - bitwise AND operator (Python native and NumPy ufunc)
> 
>   -   - `bitor(a,b)`
>       - `a | b`
>       - bitwise OR operator (Python native and NumPy ufunc)
> 
>   -   - `inv(a)`
>       - `linalg.inv(a)`
>       - inverse of square 2D array `a`
> 
>   -   - `pinv(a)`
>       - `linalg.pinv(a)`
>       - pseudo-inverse of 2D array `a`
> 
>   -   - `rank(a)`
>       - `np.linalg.matrix_rank(a)`
>       - matrix rank of a 2D array `a`
> 
>   -   - `a\b`
>       - `linalg.solve(a, b)` if `a` is square; `linalg.lstsq(a, b)` otherwise
>       - solution of a x = b for x
> 
>   -   - `b/a`
>       - Solve `a.T x.T = b.T` instead
>       - solution of x a = b for x
> 
>   -   - `[U,S,V]=svd(a)`
>       - `U, S, Vh = linalg.svd(a); V = Vh.T`
>       - singular value decomposition of `a`
> 
>   -   - `chol(a)`
>       - `linalg.cholesky(a)`
>       - Cholesky factorization of a 2D array
> 
>   -   - `[V,D]=eig(a)`
>       - `D,V = linalg.eig(a)`
>       - eigenvalues \(\lambda\) and eigenvectors \(v\) of `a`, where \(\mathbf{a} v = \lambda v\)
> 
>   -   - `[V,D]=eig(a,b)`
>       - `D,V = linalg.eig(a, b)`
>       - eigenvalues \(\lambda\) and eigenvectors \(v\) of `a`, `b` where \(\mathbf{a} v = \lambda \mathbf{b} v\)
> 
>   -   - `[V,D]=eigs(a,3)`
>       - `D,V = eigs(a, k=3)`
>       - find the `k=3` largest eigenvalues and eigenvectors of 2D array, `a`
> 
>   -   - `[Q,R]=qr(a,0)`
>       - `Q,R = linalg.qr(a)`
>       - QR decomposition
> 
>   -   - `[L,U,P]=lu(a)` where `a==P'*L*U`
>       - `P,L,U = linalg.lu(a)` where `a == P@L@U`
>       - LU decomposition with partial pivoting (note: P(MATLAB) == transpose(P(NumPy)))
> 
>   -   - `conjgrad`
>       - `cg`
>       - conjugate gradients solver
> 
>   -   - `fft(a)`
>       - `np.fft.fft(a)`
>       - Fourier transform of `a`
> 
>   -   - `ifft(a)`
>       - `np.fft.ifft(a)`
>       - inverse Fourier transform of `a`
> 
>   -   - `sort(a)`
>       - `np.sort(a)` or `a.sort(axis=0)`
>       - sort each column of a 2D array, `a`
> 
>   -   - `sort(a, 2)`
>       - `np.sort(a, axis=1)` or `a.sort(axis=1)`
>       - sort the each row of 2D array, `a`
> 
>   -   - `[b,I]=sortrows(a,1)`
>       - `I = np.argsort(a[:, 0]); b = a[I,:]`
>       - save the array `a` as array `b` with rows sorted by the first column
> 
>   -   - `x = Z\y`
>       - `x = linalg.lstsq(Z, y)`
>       - perform a linear regression of the form \(\mathbf{Zx}=\mathbf{y}\)
> 
>   -   - `decimate(x, q)`
>       - `signal.resample(x, np.ceil(len(x)/q))`
>       - downsample with low-pass filtering
> 
>   -   - `unique(a)`
>       - `np.unique(a)`
>       - a vector of unique values in array `a`
> 
>   -   - `squeeze(a)`
>       - `a.squeeze()`
>       - remove singleton dimensions of array `a`. Note that MATLAB will always return arrays of 2D or higher while NumPy will return arrays of 0D or higher

## Notes

**Submatrix**: Assignment to a submatrix can be done with lists of indices using the `ix_` command. E.g., for 2D array `a`, one might do: `ind=[1, 3];Â a[np.ix_(ind, ind)] += 100`.

**HELP**: There is no direct equivalent of MATLAB's `which` command, but the commands <span class="title-ref">help</span> will usually list the filename where the function is located. Python also has an `inspect` module (do `importÂ inspect`) which provides a `getfile` that often works.

**INDEXING**: MATLAB uses one based indexing, so the initial element of a sequence has index 1. Python uses zero based indexing, so the initial element of a sequence has index 0. Confusion and flamewars arise because each has advantages and disadvantages. One based indexing is consistent with common human language usage, where the "first" element of a sequence has index 1. Zero based indexing [simplifies indexing](https://groups.google.com/group/comp.lang.python/msg/1bf4d925dfbf368?q=g:thl3498076713d&hl=en). See also [a text by prof.dr. Edsger W. Dijkstra](https://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD831.html).

**RANGES**: In MATLAB, `0:5` can be used as both a range literal and a 'slice' index (inside parentheses); however, in Python, constructs like `0:5` can *only* be used as a slice index (inside square brackets). Thus the somewhat quirky `r_` object was created to allow NumPy to have a similarly terse range construction mechanism. Note that `r_` is not called like a function or a constructor, but rather *indexed* using square brackets, which allows the use of Python's slice syntax in the arguments.

**LOGICOPS**: `&` or `|` in NumPy is bitwise AND/OR, while in MATLAB & and `|` are logical AND/OR. The two can appear to work the same, but there are important differences. If you would have used MATLAB's `&` or `|` operators, you should use the NumPy ufuncs `logical_and`/`logical_or`. The notable differences between MATLAB's and NumPy's `&` and `|` operators are:

  - Non-logical {0,1} inputs: NumPy's output is the bitwise AND of the inputs. MATLAB treats any non-zero value as 1 and returns the logical AND. For example `(3 & 4)` in NumPy is `0`, while in MATLAB both `3` and `4` are considered logical true and `(3 & 4)` returns `1`.
  - Precedence: NumPy's & operator is higher precedence than logical operators like `<` and `>`; MATLAB's is the reverse.

If you know you have boolean arguments, you can get away with using NumPy's bitwise operators, but be careful with parentheses, like this: `z = (x > 1) & (x < 2)`. The absence of NumPy operator forms of `logical_and` and `logical_or` is an unfortunate consequence of Python's design.

**RESHAPE and LINEAR INDEXING**: MATLAB always allows multi-dimensional arrays to be accessed using scalar or linear indices, NumPy does not. Linear indices are common in MATLAB programs, e.g. `find()` on a matrix returns them, whereas NumPy's find behaves differently. When converting MATLAB code it might be necessary to first reshape a matrix to a linear sequence, perform some indexing operations and then reshape back. As reshape (usually) produces views onto the same storage, it should be possible to do this fairly efficiently. Note that the scan order used by reshape in NumPy defaults to the 'C' order, whereas MATLAB uses the Fortran order. If you are simply converting to a linear sequence and back this doesn't matter. But if you are converting reshapes from MATLAB code which relies on the scan order, then this MATLAB code: `z = reshape(x,3,4);` should become `z = x.reshape(3,4,order='F').copy()` in NumPy.

## 'array' or 'matrix'? Which should I use?

Historically, NumPy has provided a special matrix type, <span class="title-ref">np.matrix</span>, which is a subclass of ndarray which makes binary operations linear algebra operations. You may see it used in some existing code instead of <span class="title-ref">np.array</span>. So, which one to use?

### Short answer

**Use arrays**.

  - They support multidimensional array algebra that is supported in MATLAB
  - They are the standard vector/matrix/tensor type of NumPy. Many NumPy functions return arrays, not matrices.
  - There is a clear distinction between element-wise operations and linear algebra operations.
  - You can have standard vectors or row/column vectors if you like.

Until Python 3.5 the only disadvantage of using the array type was that you had to use `dot` instead of `*` to multiply (reduce) two tensors (scalar product, matrix vector multiplication etc.). Since Python 3.5 you can use the matrix multiplication `@` operator.

Given the above, we intend to deprecate `matrix` eventually.

### Long answer

NumPy contains both an `array` class and a `matrix` class. The `array` class is intended to be a general-purpose n-dimensional array for many kinds of numerical computing, while `matrix` is intended to facilitate linear algebra computations specifically. In practice there are only a handful of key differences between the two.

  - Operators `*` and `@`, functions `dot()`, and `multiply()`:
      - For `array`, **\`\`\*\`\` means element-wise multiplication**, while **\`\`@\`\` means matrix multiplication**; they have associated functions `multiply()` and `dot()`. (Before Python 3.5, `@` did not exist and one had to use `dot()` for matrix multiplication).
      - For `matrix`, **\`\`\*\`\` means matrix multiplication**, and for element-wise multiplication one has to use the `multiply()` function.
  - Handling of vectors (one-dimensional arrays)
      - For `array`, the **vector shapes 1xN, Nx1, and N are all different things**. Operations like `A[:,1]` return a one-dimensional array of shape N, not a two-dimensional array of shape Nx1. Transpose on a one-dimensional `array` does nothing.
      - For `matrix`, **one-dimensional arrays are always upconverted to 1xN or Nx1 matrices** (row or column vectors). `A[:,1]` returns a two-dimensional matrix of shape Nx1.
  - Handling of higher-dimensional arrays (ndim \> 2)
      - `array` objects **can have number of dimensions \> 2**;
      - `matrix` objects **always have exactly two dimensions**.
  - Convenience attributes
      - `array` **has a .T attribute**, which returns the transpose of the data.
      - `matrix` **also has .H, .I, and .A attributes**, which return the conjugate transpose, inverse, and `asarray()` of the matrix, respectively.
  - Convenience constructor
      - The `array` constructor **takes (nested) Python sequences as initializers**. As in, `array([[1,2,3],[4,5,6]])`.
      - The `matrix` constructor additionally **takes a convenient string initializer**. As in `matrix("[1Â 2Â 3;Â 4Â 5Â 6]")`.

There are pros and cons to using both:

  - `array`
      - `:)` Element-wise multiplication is easy: `A*B`.
      - `:(` You have to remember that matrix multiplication has its own operator, `@`.
      - `:)` You can treat one-dimensional arrays as *either* row or column vectors. `A @ v` treats `v` as a column vector, while `v @ A` treats `v` as a row vector. This can save you having to type a lot of transposes.
      - `:)` `array` is the "default" NumPy type, so it gets the most testing, and is the type most likely to be returned by 3rd party code that uses NumPy.
      - `:)` Is quite at home handling data of any number of dimensions.
      - `:)` Closer in semantics to tensor algebra, if you are familiar with that.
      - `:)` *All* operations (`*`, `/`, `+`, `-` etc.) are element-wise.
      - `:(` Sparse matrices from `scipy.sparse` do not interact as well with arrays.
  - `matrix`
      - `:\\` Behavior is more like that of MATLAB matrices.
      - `<:(` Maximum of two-dimensional. To hold three-dimensional data you need `array` or perhaps a Python list of `matrix`.
      - `<:(` Minimum of two-dimensional. You cannot have vectors. They must be cast as single-column or single-row matrices.
      - `<:(` Since `array` is the default in NumPy, some functions may return an `array` even if you give them a `matrix` as an argument. This shouldn't happen with NumPy functions (if it does it's a bug), but 3rd party code based on NumPy may not honor type preservation like NumPy does.
      - `:)` `A*B` is matrix multiplication, so it looks just like you write it in linear algebra (For Python \>= 3.5 plain arrays have the same convenience with the `@` operator).
      - `<:(` Element-wise multiplication requires calling a function, `multiply(A,B)`.
      - `<:(` The use of operator overloading is a bit illogical: `*` does not work element-wise but `/` does.
      - Interaction with `scipy.sparse` is a bit cleaner.

The `array` is thus much more advisable to use. Indeed, we intend to deprecate `matrix` eventually.

## Customizing your environment

In MATLAB the main tool available to you for customizing the environment is to modify the search path with the locations of your favorite functions. You can put such customizations into a startup script that MATLAB will run on startup.

NumPy, or rather Python, has similar facilities.

  - To modify your Python search path to include the locations of your own modules, define the `PYTHONPATH` environment variable.
  - To have a particular script file executed when the interactive Python interpreter is started, define the `PYTHONSTARTUP` environment variable to contain the name of your startup script.

Unlike MATLAB, where anything on your path can be called immediately, with Python you need to first do an 'import' statement to make functions in a particular file accessible.

For example you might make a startup script that looks like this (Note: this is just an example, not a statement of "best practices"):

    # Make all numpy available via shorter 'np' prefix
    import numpy as np
    #
    # Make the SciPy linear algebra functions available as linalg.func()
    # e.g. linalg.lu, linalg.eig (for general l*B@u==A@u solution)
    from scipy import linalg
    #
    # Define a Hermitian function
    def hermitian(A, **kwargs):
        return np.conj(A,**kwargs).T
    # Make a shortcut for hermitian:
    #    hermitian(A) --> H(A)
    H = hermitian

To use the deprecated <span class="title-ref">matrix</span> and other <span class="title-ref">matlib</span> functions:

    # Make all matlib functions accessible at the top level via M.func()
    import numpy.matlib as M
    # Make some matlib functions accessible directly at the top level via, e.g. rand(3,3)
    from numpy.matlib import matrix,rand,zeros,ones,empty,eye

## Links

Another somewhat outdated MATLAB/NumPy cross-reference can be found at <http://mathesaurus.sf.net/>

An extensive list of tools for scientific work with Python can be found in the [topical software page](https://scipy.org/topical-software.html).

See [List of Python software: scripting](https://en.wikipedia.org/wiki/List_of_Python_software#Embedded_as_a_scripting_language) for a list of software that use Python as a scripting language

MATLABÂ® and SimuLinkÂ® are registered trademarks of The MathWorks, Inc.

---

quickstart.md

---

# NumPy quickstart

<div class="currentmodule">

numpy

</div>

<div class="testsetup">

\>\>\> import numpy as np \>\>\> import sys

</div>

## Prerequisites

You'll need to know a bit of Python. For a refresher, see the [Python tutorial](https://docs.python.org/tutorial/).

To work the examples, you'll need `matplotlib` installed in addition to NumPy.

**Learner profile**

This is a quick overview of arrays in NumPy. It demonstrates how n-dimensional (\(n>=2\)) arrays are represented and can be manipulated. In particular, if you don't know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this article might be of help.

**Learning Objectives**

After reading, you should be able to:

  - Understand the difference between one-, two- and n-dimensional arrays in NumPy;
  - Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops;
  - Understand axis and shape properties for n-dimensional arrays.

## The basics

NumPy's main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called *axes*.

For example, the array for the coordinates of a point in 3D space, `[1, 2, 1]`, has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. The first axis has a length of 2, the second axis has a length of 3.

    [[1., 0., 0.],
     [0., 1., 2.]]

NumPy's array class is called `ndarray`. It is also known by the alias `array`. Note that `numpy.array` is not the same as the Standard Python Library class `array.array`, which only handles one-dimensional arrays and offers less functionality. The more important attributes of an `ndarray` object are:

  - ndarray.ndim  
    the number of axes (dimensions) of the array.

  - ndarray.shape  
    the dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with *n* rows and *m* columns, `shape` will be `(n,m)`. The length of the `shape` tuple is therefore the number of axes, `ndim`.

  - ndarray.size  
    the total number of elements of the array. This is equal to the product of the elements of `shape`.

  - ndarray.dtype  
    an object describing the type of the elements in the array. One can create or specify dtype's using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.float64 are some examples.

  - ndarray.itemsize  
    the size in bytes of each element of the array. For example, an array of elements of type `float64` has `itemsize` 8 (=64/8), while one of type `complex32` has `itemsize` 4 (=32/8). It is equivalent to `ndarray.dtype.itemsize`.

  - ndarray.data  
    the buffer containing the actual elements of the array. Normally, we won't need to use this attribute because we will access the elements in an array using indexing facilities.

### An example

> \>\>\> import numpy as np \>\>\> a = np.arange(15).reshape(3, 5) \>\>\> a array(\[\[ 0, 1, 2, 3, 4\], \[ 5, 6, 7, 8, 9\], \[10, 11, 12, 13, 14\]\]) \>\>\> a.shape (3, 5) \>\>\> a.ndim 2 \>\>\> a.dtype.name 'int64' \>\>\> a.itemsize 8 \>\>\> a.size 15 \>\>\> type(a) \<class 'numpy.ndarray'\> \>\>\> b = np.array(\[6, 7, 8\]) \>\>\> b array(\[6, 7, 8\]) \>\>\> type(b) \<class 'numpy.ndarray'\>

### Array creation

There are several ways to create arrays.

For example, you can create an array from a regular Python list or tuple using the `array` function. The type of the resulting array is deduced from the type of the elements in the sequences.

    >>> import numpy as np
    >>> a = np.array([2, 3, 4])
    >>> a
    array([2, 3, 4])
    >>> a.dtype
    dtype('int64')
    >>> b = np.array([1.2, 3.5, 5.1])
    >>> b.dtype
    dtype('float64')

A frequent error consists in calling `array` with multiple arguments, rather than providing a single sequence as an argument.

    >>> a = np.array(1, 2, 3, 4)    # WRONG
    Traceback (most recent call last):
      ...
    TypeError: array() takes from 1 to 2 positional arguments but 4 were given
    >>> a = np.array([1, 2, 3, 4])  # RIGHT

`array` transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.

    >>> b = np.array([(1.5, 2, 3), (4, 5, 6)])
    >>> b
    array([[1.5, 2. , 3. ],
           [4. , 5. , 6. ]])

The type of the array can also be explicitly specified at creation time:

    >>> c = np.array([[1, 2], [3, 4]], dtype=complex)
    >>> c
    array([[1.+0.j, 2.+0.j],
           [3.+0.j, 4.+0.j]])

Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.

The function `zeros` creates an array full of zeros, the function `ones` creates an array full of ones, and the function `empty` creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is `float64`, but it can be specified via the key word argument `dtype`.

    >>> np.zeros((3, 4))
    array([[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]])
    >>> np.ones((2, 3, 4), dtype=np.int16)
    array([[[1, 1, 1, 1],
            [1, 1, 1, 1],
            [1, 1, 1, 1]],
    <BLANKLINE>
           [[1, 1, 1, 1],
            [1, 1, 1, 1],
            [1, 1, 1, 1]]], dtype=int16)
    >>> np.empty((2, 3)) #doctest: +SKIP
    array([[3.73603959e-262, 6.02658058e-154, 6.55490914e-260],  # may vary
           [5.30498948e-313, 3.14673309e-307, 1.00000000e+000]])

To create sequences of numbers, NumPy provides the `arange` function which is analogous to the Python built-in `range`, but returns an array.

    >>> np.arange(10, 30, 5)
    array([10, 15, 20, 25])
    >>> np.arange(0, 2, 0.3)  # it accepts float arguments
    array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8])

When `arange` is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function `linspace` that receives as an argument the number of elements that we want, instead of the step:

    >>> from numpy import pi
    >>> np.linspace(0, 2, 9)                   # 9 numbers from 0 to 2
    array([0.  , 0.25, 0.5 , 0.75, 1.  , 1.25, 1.5 , 1.75, 2.  ])
    >>> x = np.linspace(0, 2 * pi, 100)        # useful to evaluate function at lots of points
    >>> f = np.sin(x)

<div class="seealso">

<span class="title-ref">array</span>, <span class="title-ref">zeros</span>, <span class="title-ref">zeros\_like</span>, <span class="title-ref">ones</span>, <span class="title-ref">ones\_like</span>, <span class="title-ref">empty</span>, <span class="title-ref">empty\_like</span>, <span class="title-ref">arange</span>, <span class="title-ref">linspace</span>, <span class="title-ref">random.Generator.random</span>, <span class="title-ref">random.Generator.normal</span>, <span class="title-ref">fromfunction</span>, <span class="title-ref">fromfile</span>

</div>

### Printing arrays

When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:

  - the last axis is printed from left to right,
  - the second-to-last is printed from top to bottom,
  - the rest are also printed from top to bottom, with each slice separated from the next by an empty line.

One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices.

    >>> a = np.arange(6)                    # 1d array
    >>> print(a)
    [0 1 2 3 4 5]
    >>> 
    >>> b = np.arange(12).reshape(4, 3)     # 2d array
    >>> print(b)
    [[ 0  1  2]
     [ 3  4  5]
     [ 6  7  8]
     [ 9 10 11]]
    >>> 
    >>> c = np.arange(24).reshape(2, 3, 4)  # 3d array
    >>> print(c)
    [[[ 0  1  2  3]
      [ 4  5  6  7]
      [ 8  9 10 11]]
    <BLANKLINE>
     [[12 13 14 15]
      [16 17 18 19]
      [20 21 22 23]]]

See \[below \<quickstart.shape-manipulation\>\](\#below-\<quickstart.shape-manipulation\>) to get more details on `reshape`.

If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:

    >>> print(np.arange(10000))
    [   0    1    2 ... 9997 9998 9999]
    >>> 
    >>> print(np.arange(10000).reshape(100, 100))
    [[   0    1    2 ...   97   98   99]
     [ 100  101  102 ...  197  198  199]
     [ 200  201  202 ...  297  298  299]
     ...
     [9700 9701 9702 ... 9797 9798 9799]
     [9800 9801 9802 ... 9897 9898 9899]
     [9900 9901 9902 ... 9997 9998 9999]]

To disable this behaviour and force NumPy to print the entire array, you can change the printing options using `set_printoptions`.

    >>> np.set_printoptions(threshold=sys.maxsize)  # sys module should be imported

### Basic operations

Arithmetic operators on arrays apply *elementwise*. A new array is created and filled with the result.

    >>> a = np.array([20, 30, 40, 50])
    >>> b = np.arange(4)
    >>> b
    array([0, 1, 2, 3])
    >>> c = a - b
    >>> c
    array([20, 29, 38, 47])
    >>> b**2
    array([0, 1, 4, 9])
    >>> 10 * np.sin(a)
    array([ 9.12945251, -9.88031624,  7.4511316 , -2.62374854])
    >>> a < 35
    array([ True,  True, False, False])

Unlike in many matrix languages, the product operator `*` operates elementwise in NumPy arrays. The matrix product can be performed using the `@` operator (in python \>=3.5) or the `dot` function or method:

    >>> A = np.array([[1, 1],
    ...               [0, 1]])
    >>> B = np.array([[2, 0],
    ...               [3, 4]])
    >>> A * B     # elementwise product
    array([[2, 0],
           [0, 4]])
    >>> A @ B     # matrix product
    array([[5, 4],
           [3, 4]])
    >>> A.dot(B)  # another matrix product
    array([[5, 4],
           [3, 4]])

Some operations, such as `+=` and `*=`, act in place to modify an existing array rather than create a new one.

    >>> rg = np.random.default_rng(1)  # create instance of default random number generator
    >>> a = np.ones((2, 3), dtype=int)
    >>> b = rg.random((2, 3))
    >>> a *= 3
    >>> a
    array([[3, 3, 3],
           [3, 3, 3]])
    >>> b += a
    >>> b
    array([[3.51182162, 3.9504637 , 3.14415961],
           [3.94864945, 3.31183145, 3.42332645]])
    >>> a += b  # b is not automatically converted to integer type
    Traceback (most recent call last):
        ...
    numpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'

When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).

    >>> a = np.ones(3, dtype=np.int32)
    >>> b = np.linspace(0, pi, 3)
    >>> b.dtype.name
    'float64'
    >>> c = a + b
    >>> c
    array([1.        , 2.57079633, 4.14159265])
    >>> c.dtype.name
    'float64'
    >>> d = np.exp(c * 1j)
    >>> d
    array([ 0.54030231+0.84147098j, -0.84147098+0.54030231j,
           -0.54030231-0.84147098j])
    >>> d.dtype.name
    'complex128'

Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the `ndarray` class.

    >>> a = rg.random((2, 3))
    >>> a
    array([[0.82770259, 0.40919914, 0.54959369],
           [0.02755911, 0.75351311, 0.53814331]])
    >>> a.sum()
    3.1057109529998157
    >>> a.min()
    0.027559113243068367
    >>> a.max()
    0.8277025938204418

By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the `axis` parameter you can apply an operation along the specified axis of an array:

    >>> b = np.arange(12).reshape(3, 4)
    >>> b
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>>
    >>> b.sum(axis=0)     # sum of each column
    array([12, 15, 18, 21])
    >>>
    >>> b.min(axis=1)     # min of each row
    array([0, 4, 8])
    >>>
    >>> b.cumsum(axis=1)  # cumulative sum along each row
    array([[ 0,  1,  3,  6],
           [ 4,  9, 15, 22],
           [ 8, 17, 27, 38]])

### Universal functions

NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called "universal functions" (`ufunc`). Within NumPy, these functions operate elementwise on an array, producing an array as output.

    >>> B = np.arange(3)
    >>> B
    array([0, 1, 2])
    >>> np.exp(B)
    array([1.        , 2.71828183, 7.3890561 ])
    >>> np.sqrt(B)
    array([0.        , 1.        , 1.41421356])
    >>> C = np.array([2., -1., 4.])
    >>> np.add(B, C)
    array([2., 0., 6.])

<div class="seealso">

<span class="title-ref">all</span>, <span class="title-ref">any</span>, <span class="title-ref">apply\_along\_axis</span>, <span class="title-ref">argmax</span>, <span class="title-ref">argmin</span>, <span class="title-ref">argsort</span>, <span class="title-ref">average</span>, <span class="title-ref">bincount</span>, <span class="title-ref">ceil</span>, <span class="title-ref">clip</span>, <span class="title-ref">conj</span>, <span class="title-ref">corrcoef</span>, <span class="title-ref">cov</span>, <span class="title-ref">cross</span>, <span class="title-ref">cumprod</span>, <span class="title-ref">cumsum</span>, <span class="title-ref">diff</span>, <span class="title-ref">dot</span>, <span class="title-ref">floor</span>, <span class="title-ref">inner</span>, <span class="title-ref">invert</span>, <span class="title-ref">lexsort</span>, <span class="title-ref">max</span>, <span class="title-ref">maximum</span>, <span class="title-ref">mean</span>, <span class="title-ref">median</span>, <span class="title-ref">min</span>, <span class="title-ref">minimum</span>, <span class="title-ref">nonzero</span>, <span class="title-ref">outer</span>, <span class="title-ref">prod</span>, <span class="title-ref">re</span>, <span class="title-ref">round</span>, <span class="title-ref">sort</span>, <span class="title-ref">std</span>, <span class="title-ref">sum</span>, <span class="title-ref">trace</span>, <span class="title-ref">transpose</span>, <span class="title-ref">var</span>, <span class="title-ref">vdot</span>, <span class="title-ref">vectorize</span>, <span class="title-ref">where</span>

</div>

### Indexing, slicing and iterating

**One-dimensional** arrays can be indexed, sliced and iterated over, much like [lists](https://docs.python.org/tutorial/introduction.html#lists) and other Python sequences.

    >>> a = np.arange(10)**3
    >>> a
    array([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729])
    >>> a[2]
    8
    >>> a[2:5]
    array([ 8, 27, 64])
    >>> # equivalent to a[0:6:2] = 1000;
    >>> # from start to position 6, exclusive, set every 2nd element to 1000
    >>> a[:6:2] = 1000
    >>> a
    array([1000,    1, 1000,   27, 1000,  125,  216,  343,  512,  729])
    >>> a[::-1]  # reversed a
    array([ 729,  512,  343,  216,  125, 1000,   27, 1000,    1, 1000])
    >>> for i in a:
    ...     print(i**(1 / 3.))
    ...
    9.999999999999998  # may vary
    1.0
    9.999999999999998
    3.0
    9.999999999999998
    4.999999999999999
    5.999999999999999
    6.999999999999999
    7.999999999999999
    8.999999999999998

**Multidimensional** arrays can have one index per axis. These indices are given in a tuple separated by commas:

    >>> def f(x, y):
    ...     return 10 * x + y
    ...
    >>> b = np.fromfunction(f, (5, 4), dtype=int)
    >>> b
    array([[ 0,  1,  2,  3],
           [10, 11, 12, 13],
           [20, 21, 22, 23],
           [30, 31, 32, 33],
           [40, 41, 42, 43]])
    >>> b[2, 3]
    23
    >>> b[0:5, 1]  # each row in the second column of b
    array([ 1, 11, 21, 31, 41])
    >>> b[:, 1]    # equivalent to the previous example
    array([ 1, 11, 21, 31, 41])
    >>> b[1:3, :]  # each column in the second and third row of b
    array([[10, 11, 12, 13],
           [20, 21, 22, 23]])

When fewer indices are provided than the number of axes, the missing indices are considered complete slices`:`

    >>> b[-1]   # the last row. Equivalent to b[-1, :]
    array([40, 41, 42, 43])

The expression within brackets in `b[i]` is treated as an `i` followed by as many instances of `:` as needed to represent the remaining axes. NumPy also allows you to write this using dots as `b[i, ...]`.

The **dots** (`...`) represent as many colons as needed to produce a complete indexing tuple. For example, if `x` is an array with 5 axes, then

  - `x[1, 2, ...]` is equivalent to `x[1, 2, :, :, :]`,
  - `x[..., 3]` to `x[:, :, :, :, 3]` and
  - `x[4, ..., 5, :]` to `x[4, :, :, 5, :]`.

<!-- end list -->

    >>> c = np.array([[[  0,  1,  2],  # a 3D array (two stacked 2D arrays)
    ...                [ 10, 12, 13]],
    ...               [[100, 101, 102],
    ...                [110, 112, 113]]])
    >>> c.shape
    (2, 2, 3)
    >>> c[1, ...]  # same as c[1, :, :] or c[1]
    array([[100, 101, 102],
           [110, 112, 113]])
    >>> c[..., 2]  # same as c[:, :, 2]
    array([[  2,  13],
           [102, 113]])

**Iterating** over multidimensional arrays is done with respect to the first axis:

    >>> for row in b:
    ...     print(row)
    ...
    [0 1 2 3]
    [10 11 12 13]
    [20 21 22 23]
    [30 31 32 33]
    [40 41 42 43]

However, if one wants to perform an operation on each element in the array, one can use the `flat` attribute which is an [iterator](https://docs.python.org/tutorial/classes.html#iterators) over all the elements of the array:

    >>> for element in b.flat:
    ...     print(element)
    ...
    0
    1
    2
    3
    10
    11
    12
    13
    20
    21
    22
    23
    30
    31
    32
    33
    40
    41
    42
    43

<div class="seealso">

\[basics.indexing\](\#basics.indexing), \[arrays.indexing\](\#arrays.indexing) (reference), <span class="title-ref">newaxis</span>, <span class="title-ref">ndenumerate</span>, <span class="title-ref">indices</span>

</div>

## Shape manipulation

### Changing the shape of an array

An array has a shape given by the number of elements along each axis:

    >>> a = np.floor(10 * rg.random((3, 4)))
    >>> a
    array([[3., 7., 3., 4.],
           [1., 4., 2., 2.],
           [7., 2., 4., 9.]])
    >>> a.shape
    (3, 4)

The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array:

    >>> a.ravel()  # returns the array, flattened
    array([3., 7., 3., 4., 1., 4., 2., 2., 7., 2., 4., 9.])
    >>> a.reshape(6, 2)  # returns the array with a modified shape
    array([[3., 7.],
           [3., 4.],
           [1., 4.],
           [2., 2.],
           [7., 2.],
           [4., 9.]])
    >>> a.T  # returns the array, transposed
    array([[3., 1., 7.],
           [7., 4., 2.],
           [3., 2., 4.],
           [4., 2., 9.]])
    >>> a.T.shape
    (4, 3)
    >>> a.shape
    (3, 4)

The order of the elements in the array resulting from `ravel` is normally "C-style", that is, the rightmost index "changes the fastest", so the element after `a[0, 0]` is `a[0, 1]`. If the array is reshaped to some other shape, again the array is treated as "C-style". NumPy normally creates arrays stored in this order, so `ravel` will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions `ravel` and `reshape` can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest.

The <span class="title-ref">reshape</span> function returns its argument with a modified shape, whereas the <span class="title-ref">ndarray.resize</span> method modifies the array itself:

    >>> a
    array([[3., 7., 3., 4.],
           [1., 4., 2., 2.],
           [7., 2., 4., 9.]])
    >>> a.resize((2, 6))
    >>> a
    array([[3., 7., 3., 4., 1., 4.],
           [2., 2., 7., 2., 4., 9.]])

If a dimension is given as `-1` in a reshaping operation, the other dimensions are automatically calculated:

    >>> a.reshape(3, -1)
    array([[3., 7., 3., 4.],
           [1., 4., 2., 2.],
           [7., 2., 4., 9.]])

<div class="seealso">

<span class="title-ref">ndarray.shape</span>, <span class="title-ref">reshape</span>, <span class="title-ref">resize</span>, <span class="title-ref">ravel</span>

</div>

### Stacking together different arrays

Several arrays can be stacked together along different axes:

    >>> a = np.floor(10 * rg.random((2, 2)))
    >>> a
    array([[9., 7.],
           [5., 2.]])
    >>> b = np.floor(10 * rg.random((2, 2)))
    >>> b
    array([[1., 9.],
           [5., 1.]])
    >>> np.vstack((a, b))
    array([[9., 7.],
           [5., 2.],
           [1., 9.],
           [5., 1.]])
    >>> np.hstack((a, b))
    array([[9., 7., 1., 9.],
           [5., 2., 5., 1.]])

The function <span class="title-ref">column\_stack</span> stacks 1D arrays as columns into a 2D array. It is equivalent to <span class="title-ref">hstack</span> only for 2D arrays:

    >>> from numpy import newaxis
    >>> np.column_stack((a, b))  # with 2D arrays
    array([[9., 7., 1., 9.],
           [5., 2., 5., 1.]])
    >>> a = np.array([4., 2.])
    >>> b = np.array([3., 8.])
    >>> np.column_stack((a, b))  # returns a 2D array
    array([[4., 3.],
           [2., 8.]])
    >>> np.hstack((a, b))        # the result is different
    array([4., 2., 3., 8.])
    >>> a[:, newaxis]  # view `a` as a 2D column vector
    array([[4.],
           [2.]])
    >>> np.column_stack((a[:, newaxis], b[:, newaxis]))
    array([[4., 3.],
           [2., 8.]])
    >>> np.hstack((a[:, newaxis], b[:, newaxis]))  # the result is the same
    array([[4., 3.],
           [2., 8.]])

In general, for arrays with more than two dimensions, <span class="title-ref">hstack</span> stacks along their second axes, <span class="title-ref">vstack</span> stacks along their first axes, and <span class="title-ref">concatenate</span> allows for an optional arguments giving the number of the axis along which the concatenation should happen.

**Note**

In complex cases, <span class="title-ref">r\_</span> and <span class="title-ref">c\_</span> are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals `:`. :

    >>> np.r_[1:4, 0, 4]
    array([1, 2, 3, 0, 4])

When used with arrays as arguments, <span class="title-ref">r\_</span> and <span class="title-ref">c\_</span> are similar to <span class="title-ref">vstack</span> and <span class="title-ref">hstack</span> in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate.

<div class="seealso">

<span class="title-ref">hstack</span>, <span class="title-ref">vstack</span>, <span class="title-ref">column\_stack</span>, <span class="title-ref">concatenate</span>, <span class="title-ref">c\_</span>, <span class="title-ref">r\_</span>

</div>

### Splitting one array into several smaller ones

Using <span class="title-ref">hsplit</span>, you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur:

    >>> a = np.floor(10 * rg.random((2, 12)))
    >>> a
    array([[6., 7., 6., 9., 0., 5., 4., 0., 6., 8., 5., 2.],
           [8., 5., 5., 7., 1., 8., 6., 7., 1., 8., 1., 0.]])
    >>> # Split `a` into 3
    >>> np.hsplit(a, 3)
    [array([[6., 7., 6., 9.],
           [8., 5., 5., 7.]]), array([[0., 5., 4., 0.],
           [1., 8., 6., 7.]]), array([[6., 8., 5., 2.],
           [1., 8., 1., 0.]])]
    >>> # Split `a` after the third and the fourth column
    >>> np.hsplit(a, (3, 4))
    [array([[6., 7., 6.],
           [8., 5., 5.]]), array([[9.],
           [7.]]), array([[0., 5., 4., 0., 6., 8., 5., 2.],
           [1., 8., 6., 7., 1., 8., 1., 0.]])]

<span class="title-ref">vsplit</span> splits along the vertical axis, and <span class="title-ref">array\_split</span> allows one to specify along which axis to split.

## Copies and views

When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:

### No copy at all

Simple assignments make no copy of objects or their data.

    >>> a = np.array([[ 0,  1,  2,  3],
    ...               [ 4,  5,  6,  7],
    ...               [ 8,  9, 10, 11]])
    >>> b = a            # no new object is created
    >>> b is a           # a and b are two names for the same ndarray object
    True

Python passes mutable objects as references, so function calls make no copy.

    >>> def f(x):
    ...     print(id(x))
    ...
    >>> id(a)  # id is a unique identifier of an object #doctest: +SKIP
    148293216  # may vary
    >>> f(a)   #doctest: +SKIP
    148293216  # may vary

### View or shallow copy

Different array objects can share the same data. The `view` method creates a new array object that looks at the same data.

    >>> c = a.view()
    >>> c is a
    False
    >>> c.base is a            # c is a view of the data owned by a
    True
    >>> c.flags.owndata
    False
    >>>
    >>> c = c.reshape((2, 6))  # a's shape doesn't change, reassigned c is still a view of a
    >>> a.shape
    (3, 4)
    >>> c[0, 4] = 1234         # a's data changes
    >>> a
    array([[   0,    1,    2,    3],
           [1234,    5,    6,    7],
           [   8,    9,   10,   11]])

Slicing an array returns a view of it:

    >>> s = a[:, 1:3]
    >>> s[:] = 10  # s[:] is a view of s. Note the difference between s = 10 and s[:] = 10
    >>> a
    array([[   0,   10,   10,    3],
           [1234,   10,   10,    7],
           [   8,   10,   10,   11]])

### Deep copy

The `copy` method makes a complete copy of the array and its data.

    >>> d = a.copy()  # a new array object with new data is created
    >>> d is a
    False
    >>> d.base is a  # d doesn't share anything with a
    False
    >>> d[0, 0] = 9999
    >>> a
    array([[   0,   10,   10,    3],
           [1234,   10,   10,    7],
           [   8,   10,   10,   11]])

Sometimes `copy` should be called after slicing if the original array is not required anymore. For example, suppose `a` is a huge intermediate result and the final result `b` only contains a small fraction of `a`, a deep copy should be made when constructing `b` with slicing:

    >>> a = np.arange(int(1e8))
    >>> b = a[:100].copy()
    >>> del a  # the memory of ``a`` can be released.

If `b = a[:100]` is used instead, `a` is referenced by `b` and will persist in memory even if `del a` is executed.

See also \[basics.copies-and-views\](\#basics.copies-and-views).

### Functions and methods overview

Here is a list of some useful NumPy functions and methods names ordered in categories. See \[routines\](\#routines) for the full list.

  - Array Creation  
    <span class="title-ref">arange</span>, <span class="title-ref">array</span>, <span class="title-ref">copy</span>, <span class="title-ref">empty</span>, <span class="title-ref">empty\_like</span>, <span class="title-ref">eye</span>, <span class="title-ref">fromfile</span>, <span class="title-ref">fromfunction</span>, <span class="title-ref">identity</span>, <span class="title-ref">linspace</span>, <span class="title-ref">logspace</span>, <span class="title-ref">mgrid</span>, <span class="title-ref">ogrid</span>, <span class="title-ref">ones</span>, <span class="title-ref">ones\_like</span>, <span class="title-ref">r\_</span>, <span class="title-ref">zeros</span>, <span class="title-ref">zeros\_like</span>

  - Conversions  
    <span class="title-ref">ndarray.astype</span>, <span class="title-ref">atleast\_1d</span>, <span class="title-ref">atleast\_2d</span>, <span class="title-ref">atleast\_3d</span>, <span class="title-ref">mat</span>

  - Manipulations  
    <span class="title-ref">array\_split</span>, <span class="title-ref">column\_stack</span>, <span class="title-ref">concatenate</span>, <span class="title-ref">diagonal</span>, <span class="title-ref">dsplit</span>, <span class="title-ref">dstack</span>, <span class="title-ref">hsplit</span>, <span class="title-ref">hstack</span>, <span class="title-ref">ndarray.item</span>, <span class="title-ref">newaxis</span>, <span class="title-ref">ravel</span>, <span class="title-ref">repeat</span>, <span class="title-ref">reshape</span>, <span class="title-ref">resize</span>, <span class="title-ref">squeeze</span>, <span class="title-ref">swapaxes</span>, <span class="title-ref">take</span>, <span class="title-ref">transpose</span>, <span class="title-ref">vsplit</span>, <span class="title-ref">vstack</span>

  - Questions  
    <span class="title-ref">all</span>, <span class="title-ref">any</span>, <span class="title-ref">nonzero</span>, <span class="title-ref">where</span>

  - Ordering  
    <span class="title-ref">argmax</span>, <span class="title-ref">argmin</span>, <span class="title-ref">argsort</span>, <span class="title-ref">max</span>, <span class="title-ref">min</span>, <span class="title-ref">ptp</span>, <span class="title-ref">searchsorted</span>, <span class="title-ref">sort</span>

  - Operations  
    <span class="title-ref">choose</span>, <span class="title-ref">compress</span>, <span class="title-ref">cumprod</span>, <span class="title-ref">cumsum</span>, <span class="title-ref">inner</span>, <span class="title-ref">ndarray.fill</span>, <span class="title-ref">imag</span>, <span class="title-ref">prod</span>, <span class="title-ref">put</span>, <span class="title-ref">putmask</span>, <span class="title-ref">real</span>, <span class="title-ref">sum</span>

  - Basic Statistics  
    <span class="title-ref">cov</span>, <span class="title-ref">mean</span>, <span class="title-ref">std</span>, <span class="title-ref">var</span>

  - Basic Linear Algebra  
    <span class="title-ref">cross</span>, <span class="title-ref">dot</span>, <span class="title-ref">outer</span>, <span class="title-ref">linalg.svd</span>, <span class="title-ref">vdot</span>

## Less basic

### Broadcasting rules

Broadcasting allows universal functions to deal in a meaningful way with inputs that do not have exactly the same shape.

The first rule of broadcasting is that if all input arrays do not have the same number of dimensions, a "1" will be repeatedly prepended to the shapes of the smaller arrays until all the arrays have the same number of dimensions.

The second rule of broadcasting ensures that arrays with a size of 1 along a particular dimension act as if they had the size of the array with the largest shape along that dimension. The value of the array element is assumed to be the same along that dimension for the "broadcast" array.

After application of the broadcasting rules, the sizes of all arrays must match. More details can be found in \[basics.broadcasting\](\#basics.broadcasting).

## Advanced indexing and index tricks

NumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans.

### Indexing with arrays of indices

    >>> a = np.arange(12)**2  # the first 12 square numbers
    >>> i = np.array([1, 1, 3, 8, 5])  # an array of indices
    >>> a[i]  # the elements of `a` at the positions `i`
    array([ 1,  1,  9, 64, 25])
    >>> 
    >>> j = np.array([[3, 4], [9, 7]])  # a bidimensional array of indices
    >>> a[j]  # the same shape as `j`
    array([[ 9, 16],
           [81, 49]])

When the indexed array `a` is multidimensional, a single array of indices refers to the first dimension of `a`. The following example shows this behavior by converting an image of labels into a color image using a palette.

    >>> palette = np.array([[0, 0, 0],         # black
    ...                     [255, 0, 0],       # red
    ...                     [0, 255, 0],       # green
    ...                     [0, 0, 255],       # blue
    ...                     [255, 255, 255]])  # white
    >>> image = np.array([[0, 1, 2, 0],  # each value corresponds to a color in the palette
    ...                   [0, 3, 4, 0]])
    >>> palette[image]  # the (2, 4, 3) color image
    array([[[  0,   0,   0],
            [255,   0,   0],
            [  0, 255,   0],
            [  0,   0,   0]],
    <BLANKLINE>
           [[  0,   0,   0],
            [  0,   0, 255],
            [255, 255, 255],
            [  0,   0,   0]]])

We can also give indexes for more than one dimension. The arrays of indices for each dimension must have the same shape.

    >>> a = np.arange(12).reshape(3, 4)
    >>> a
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> i = np.array([[0, 1],  # indices for the first dim of `a`
    ...               [1, 2]])
    >>> j = np.array([[2, 1],  # indices for the second dim
    ...               [3, 3]])
    >>> 
    >>> a[i, j]  # i and j must have equal shape
    array([[ 2,  5],
           [ 7, 11]])
    >>> 
    >>> a[i, 2]
    array([[ 2,  6],
           [ 6, 10]])
    >>> 
    >>> a[:, j]
    array([[[ 2,  1],
            [ 3,  3]],
    <BLANKLINE>
           [[ 6,  5],
            [ 7,  7]],
    <BLANKLINE>
           [[10,  9],
            [11, 11]]])

In Python, `arr[i, j]` is exactly the same as `arr[(i, j)]`---so we can put `i` and `j` in a `tuple` and then do the indexing with that.

    >>> l = (i, j)
    >>> # equivalent to a[i, j]
    >>> a[l]
    array([[ 2,  5],
           [ 7, 11]])

However, we can not do this by putting `i` and `j` into an array, because this array will be interpreted as indexing the first dimension of `a`.

    >>> s = np.array([i, j])
    >>> # not what we want
    >>> a[s]
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    IndexError: index 3 is out of bounds for axis 0 with size 3
    >>> # same as `a[i, j]`
    >>> a[tuple(s)]
    array([[ 2,  5],
           [ 7, 11]])

Another common use of indexing with arrays is the search of the maximum value of time-dependent series:

    >>> time = np.linspace(20, 145, 5)  # time scale
    >>> data = np.sin(np.arange(20)).reshape(5, 4)  # 4 time-dependent series
    >>> time
    array([ 20.  ,  51.25,  82.5 , 113.75, 145.  ])
    >>> data
    array([[ 0.        ,  0.84147098,  0.90929743,  0.14112001],
           [-0.7568025 , -0.95892427, -0.2794155 ,  0.6569866 ],
           [ 0.98935825,  0.41211849, -0.54402111, -0.99999021],
           [-0.53657292,  0.42016704,  0.99060736,  0.65028784],
           [-0.28790332, -0.96139749, -0.75098725,  0.14987721]])
    >>> # index of the maxima for each series
    >>> ind = data.argmax(axis=0)
    >>> ind
    array([2, 0, 3, 1])
    >>> # times corresponding to the maxima
    >>> time_max = time[ind]
    >>> 
    >>> data_max = data[ind, range(data.shape[1])]  # => data[ind[0], 0], data[ind[1], 1]...
    >>> time_max
    array([ 82.5 ,  20.  , 113.75,  51.25])
    >>> data_max
    array([0.98935825, 0.84147098, 0.99060736, 0.6569866 ])
    >>> np.all(data_max == data.max(axis=0))
    True

You can also use indexing with arrays as a target to assign to:

    >>> a = np.arange(5)
    >>> a
    array([0, 1, 2, 3, 4])
    >>> a[[1, 3, 4]] = 0
    >>> a
    array([0, 0, 2, 0, 0])

However, when the list of indices contains repetitions, the assignment is done several times, leaving behind the last value:

    >>> a = np.arange(5)
    >>> a[[0, 0, 2]] = [1, 2, 3]
    >>> a
    array([2, 1, 3, 3, 4])

This is reasonable enough, but watch out if you want to use Python's `+=` construct, as it may not do what you expect:

    >>> a = np.arange(5)
    >>> a[[0, 0, 2]] += 1
    >>> a
    array([1, 1, 3, 3, 4])

Even though 0 occurs twice in the list of indices, the 0th element is only incremented once. This is because Python requires `a += 1` to be equivalent to `a = a + 1`.

### Indexing with boolean arrays

When we index arrays with arrays of (integer) indices we are providing the list of indices to pick. With boolean indices the approach is different; we explicitly choose which items in the array we want and which ones we don't.

The most natural way one can think of for boolean indexing is to use boolean arrays that have *the same shape* as the original array:

    >>> a = np.arange(12).reshape(3, 4)
    >>> b = a > 4
    >>> b  # `b` is a boolean with `a`'s shape
    array([[False, False, False, False],
           [False,  True,  True,  True],
           [ True,  True,  True,  True]])
    >>> a[b]  # 1d array with the selected elements
    array([ 5,  6,  7,  8,  9, 10, 11])

This property can be very useful in assignments:

    >>> a[b] = 0  # All elements of `a` higher than 4 become 0
    >>> a
    array([[0, 1, 2, 3],
           [4, 0, 0, 0],
           [0, 0, 0, 0]])

You can look at the following example to see how to use boolean indexing to generate an image of the [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set):

<div class="plot">

\>\>\> import numpy as np \>\>\> import matplotlib.pyplot as plt \>\>\> def mandelbrot(h, w, maxit=20, r=2): ... """Returns an image of the Mandelbrot fractal of size (h,w).""" ... x = np.linspace(-2.5, 1.5, 4\*h+1) ... y = np.linspace(-1.5, 1.5, 3\*w+1) ... A, B = np.meshgrid(x, y) ... C = A + B\*1j ... z = np.zeros\_like(C) ... divtime = maxit + np.zeros(z.shape, dtype=int) ... ... for i in range(maxit): ... z = z\*\*2 + C ... diverge = abs(z) \> r \# who is diverging ... div\_now = diverge & (divtime == maxit) \# who is diverging now ... divtime\[div\_now\] = i \# note when ... z\[diverge\] = r \# avoid diverging too much ... ... return divtime \>\>\> plt.clf() \>\>\> plt.imshow(mandelbrot(400, 400))

</div>

The second way of indexing with booleans is more similar to integer indexing; for each dimension of the array we give a 1D boolean array selecting the slices we want:

    >>> a = np.arange(12).reshape(3, 4)
    >>> b1 = np.array([False, True, True])         # first dim selection
    >>> b2 = np.array([True, False, True, False])  # second dim selection
    >>> 
    >>> a[b1, :]                                   # selecting rows
    array([[ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> 
    >>> a[b1]                                      # same thing
    array([[ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> 
    >>> a[:, b2]                                   # selecting columns
    array([[ 0,  2],
           [ 4,  6],
           [ 8, 10]])
    >>> 
    >>> a[b1, b2]                                  # a weird thing to do
    array([ 4, 10])

Note that the length of the 1D boolean array must coincide with the length of the dimension (or axis) you want to slice. In the previous example, `b1` has length 3 (the number of *rows* in `a`), and `b2` (of length 4) is suitable to index the 2nd axis (columns) of `a`.

### The [ix]()() function

The <span class="title-ref">ix\_</span> function can be used to combine different vectors so as to obtain the result for each n-uplet. For example, if you want to compute all the a+b\*c for all the triplets taken from each of the vectors a, b and c:

    >>> a = np.array([2, 3, 4, 5])
    >>> b = np.array([8, 5, 4])
    >>> c = np.array([5, 4, 6, 8, 3])
    >>> ax, bx, cx = np.ix_(a, b, c)
    >>> ax
    array([[[2]],
    <BLANKLINE>
           [[3]],
    <BLANKLINE>
           [[4]],
    <BLANKLINE>
           [[5]]])
    >>> bx
    array([[[8],
            [5],
            [4]]])
    >>> cx
    array([[[5, 4, 6, 8, 3]]])
    >>> ax.shape, bx.shape, cx.shape
    ((4, 1, 1), (1, 3, 1), (1, 1, 5))
    >>> result = ax + bx * cx
    >>> result
    array([[[42, 34, 50, 66, 26],
            [27, 22, 32, 42, 17],
            [22, 18, 26, 34, 14]],
    <BLANKLINE>
           [[43, 35, 51, 67, 27],
            [28, 23, 33, 43, 18],
            [23, 19, 27, 35, 15]],
    <BLANKLINE>
           [[44, 36, 52, 68, 28],
            [29, 24, 34, 44, 19],
            [24, 20, 28, 36, 16]],
    <BLANKLINE>
           [[45, 37, 53, 69, 29],
            [30, 25, 35, 45, 20],
            [25, 21, 29, 37, 17]]])
    >>> result[3, 2, 4]
    17
    >>> a[3] + b[2] * c[4]
    17

You could also implement the reduce as follows:

    >>> def ufunc_reduce(ufct, *vectors):
    ...    vs = np.ix_(*vectors)
    ...    r = ufct.identity
    ...    for v in vs:
    ...        r = ufct(r, v)
    ...    return r

and then use it as:

    >>> ufunc_reduce(np.add, a, b, c)
    array([[[15, 14, 16, 18, 13],
            [12, 11, 13, 15, 10],
            [11, 10, 12, 14,  9]],
    <BLANKLINE>
           [[16, 15, 17, 19, 14],
            [13, 12, 14, 16, 11],
            [12, 11, 13, 15, 10]],
    <BLANKLINE>
           [[17, 16, 18, 20, 15],
            [14, 13, 15, 17, 12],
            [13, 12, 14, 16, 11]],
    <BLANKLINE>
           [[18, 17, 19, 21, 16],
            [15, 14, 16, 18, 13],
            [14, 13, 15, 17, 12]]])

The advantage of this version of reduce compared to the normal ufunc.reduce is that it makes use of the \[broadcasting rules \<broadcasting-rules\>\](\#broadcasting-rules-\<broadcasting-rules\>) in order to avoid creating an argument array the size of the output times the number of vectors.

### Indexing with strings

See \[structured\_arrays\](\#structured\_arrays).

## Tricks and tips

Here we give a list of short and useful tips.

### "Automatic" reshaping

To change the dimensions of an array, you can omit one of the sizes which will then be deduced automatically:

    >>> a = np.arange(30)
    >>> b = a.reshape((2, -1, 3))  # -1 means "whatever is needed"
    >>> b.shape
    (2, 5, 3)
    >>> b
    array([[[ 0,  1,  2],
            [ 3,  4,  5],
            [ 6,  7,  8],
            [ 9, 10, 11],
            [12, 13, 14]],
    <BLANKLINE>
           [[15, 16, 17],
            [18, 19, 20],
            [21, 22, 23],
            [24, 25, 26],
            [27, 28, 29]]])

### Vector stacking

How do we construct a 2D array from a list of equally-sized row vectors? In MATLAB this is quite easy: if `x` and `y` are two vectors of the same length you only need do `m=[x;y]`. In NumPy this works via the functions `column_stack`, `dstack`, `hstack` and `vstack`, depending on the dimension in which the stacking is to be done. For example:

    >>> x = np.arange(0, 10, 2)
    >>> y = np.arange(5)
    >>> m = np.vstack([x, y])
    >>> m
    array([[0, 2, 4, 6, 8],
           [0, 1, 2, 3, 4]])
    >>> xy = np.hstack([x, y])
    >>> xy
    array([0, 2, 4, 6, 8, 0, 1, 2, 3, 4])

The logic behind those functions in more than two dimensions can be strange.

<div class="seealso">

\[numpy-for-matlab-users\](numpy-for-matlab-users.md)

</div>

### Histograms

The NumPy `histogram` function applied to an array returns a pair of vectors: the histogram of the array and a vector of the bin edges. Beware: `matplotlib` also has a function to build histograms (called `hist`, as in Matlab) that differs from the one in NumPy. The main difference is that `pylab.hist` plots the histogram automatically, while `numpy.histogram` only generates the data.

<div class="plot">

\>\>\> import numpy as np \>\>\> rg = np.random.default\_rng(1) \>\>\> import matplotlib.pyplot as plt \>\>\> \# Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2 \>\>\> mu, sigma = 2, 0.5 \>\>\> v = rg.normal(mu, sigma, 10000) \>\>\> \# Plot a normalized histogram with 50 bins \>\>\> plt.hist(v, bins=50, density=True) \# matplotlib version (plot) (array...) \>\>\> \# Compute the histogram with numpy and then plot it \>\>\> (n, bins) = np.histogram(v, bins=50, density=True) \# NumPy version (no plot) \>\>\> plt.plot(.5 \* (bins\[1:\] + bins\[:-1\]), n) \#doctest: +SKIP

</div>

With Matplotlib \>=3.4 you can also use `plt.stairs(n, bins)`.

## Further reading

  - The [Python tutorial](https://docs.python.org/tutorial/)
  - \[reference\](\#reference)
  - [SciPy Tutorial](https://docs.scipy.org/doc/scipy/tutorial/index.html)
  - [SciPy Lecture Notes](https://scipy-lectures.org)
  - A [matlab, R, IDL, NumPy/SciPy dictionary](https://mathesaurus.sourceforge.net/)
  - \[tutorial-svd \<numpy-tutorials:content/tutorial-svd\>\](tutorial-svd \<numpy-tutorials:content/tutorial-svd\>.md)

---

theory.broadcasting.md

---

- orphan

# Array broadcasting in Numpy

<div class="note">

<div class="title">

Note

</div>

Please refer to the updated \[basics.broadcasting\](basics.broadcasting.md) document.

</div>

---

troubleshooting-importerror.md

---

- orphan

# Troubleshooting

\> **Note** \> Since this information may be updated regularly, please ensure you are viewing the most [up-to-date version](https://numpy.org/devdocs/user/troubleshooting-importerror.html).

## ImportError

In certain cases a failed installation or setup issue can cause you to see the following error message:

    IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!
    
    Importing the numpy c-extensions failed. This error can happen for
    different reasons, often due to issues with your setup.

The error also has additional information to help you troubleshoot:

  - Your Python version
  - Your NumPy version

Please check both of these carefully to see if they are what you expect. You may need to check your `PATH` or `PYTHONPATH` environment variables (see [Check Environment Variables](#check-environment-variables) below).

The following sections list commonly reported issues depending on your setup. If you have an issue/solution that you think should appear please open a NumPy issue so that it will be added.

There are a few commonly reported issues depending on your system/setup. If none of the following tips help you, please be sure to note the following:

  - how you installed Python
  - how you installed NumPy
  - your operating system
  - whether or not you have multiple versions of Python installed
  - if you built from source, your compiler versions and ideally a build log

when investigating further and asking for support.

### Using Python from `conda` (Anaconda)

Please make sure that you have activated your conda environment. See also the [conda user-guide](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment). If you use an external editor/development environment it will have to be set up correctly. See below for solutions for some common setups.

### Using PyCharm with Anaconda Python

There are fairly common issues when using PyCharm together with Anaconda, please see the [PyCharm support](https://www.jetbrains.com/help/pycharm/conda-support-creating-conda-virtual-environment.html)

### Using VS Code with Anaconda Python (or environments)

A commonly reported issue is related to the environment activation within VSCode. Please see the [VSCode support](https://code.visualstudio.com/docs/python/environments) for information on how to correctly set up VSCode with virtual environments or conda.

### Using Eclipse/PyDev with Anaconda Python (or environments)

Please see the [Anaconda Documentation](https://docs.anaconda.com/anaconda/user-guide/tasks/integration/eclipse-pydev/) on how to properly configure Eclipse/PyDev to use Anaconda Python with specific conda environments.

### Raspberry Pi

There are sometimes issues reported on Raspberry Pi setups when installing using `pip3 install` (or `pip` install). These will typically mention:

    libf77blas.so.3: cannot open shared object file: No such file or directory

The solution will be to either:

    sudo apt-get install libatlas-base-dev

to install the missing libraries expected by the self-compiled NumPy (ATLAS is a possible provider of linear algebra).

*Alternatively* use the NumPy provided by Raspbian. In which case run:

    pip3 uninstall numpy  # remove previously installed version
    apt install python3-numpy

### Debug build on Windows

Rather than building your project in `DEBUG` mode on windows, try building in `RELEASE` mode with debug symbols and no optimization. Full `DEBUG` mode on windows changes the names of the DLLs python expects to find, so if you wish to truly work in `DEBUG` mode you will need to recompile the entire stack of python modules you work with including NumPy

### All setups

Occasionally there may be simple issues with old or bad installations of NumPy. In this case you may just try to uninstall and reinstall NumPy. Make sure that NumPy is not found after uninstalling.

### Development setup

If you are using a development setup, make sure to run `git clean -xdf` to delete all files not under version control (be careful not to lose any modifications you made, e.g. `site.cfg`). In many cases files from old builds may lead to incorrect builds.

### Check environment variables

In general how to set and check your environment variables depends on your system. If you can open a correct python shell, you can also run the following in python:

    import os
    print("PYTHONPATH:", os.environ.get('PYTHONPATH'))
    print("PATH:", os.environ.get('PATH'))

This may mainly help you if you are not running the python and/or NumPy version you are expecting to run.

## Downstream ImportError, AttributeError or C-API/ABI incompatibility

If you see a message such as:

    A module that was compiled using NumPy 1.x cannot be run in
    NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
    versions of NumPy, modules must be compiled with NumPy 2.0.
    Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

either as an `ImportError` or with:

    AttributeError: _ARRAY_API not found

or other errors such as:

    RuntimeError: module compiled against API version v1 but this version of numpy is v2

or when a package implemented with Cython:

    ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

This means that a package depending on NumPy was build in a way that is not compatible with the NumPy version found. If this error is due to a recent upgrade to NumPy 2, the easiest solution may be to simply downgrade NumPy to `'numpy<2'`.

To understand the cause, search the traceback (from the back) to find the first line that isn't inside NumPy to see which package has the incompatibility. Note your NumPy version and the version of the incompatible package to help you find the best solution.

There can be various reason for the incompatibility:

  - You have recently upgraded NumPy, most likely to NumPy 2, and the other module now also needs to be upgraded. (NumPy 2 was released in June 2024.)
  - You have version constraints and `pip` may have installed a combination of incompatible packages.
  - You have compiled locally or have copied a compiled extension from elsewhere (which is, in general, a bad idea).

The best solution will usually be to upgrade the failing package:

  - If you installed it for example through `pip`, try upgrading it with `pip install package_name --upgrade`.
  - If it is your own package or it is build locally, you need recompiled for the new NumPy version (for details see \[depending\_on\_numpy\](\#depending\_on\_numpy)). It may be that a reinstall of the package is sufficient to fix it.

When these steps fail, you should inform the package maintainers since they probably need to make a new, compatible, release.

However, upgrading may not always be possible because a compatible version does not yet exist or cannot be installed for other reasons. In that case:

  - Install a compatible NumPy version:
      - Try downgrading NumPy with `pip install 'numpy<2'` (NumPy 2 was released in June 2024).
      - If your NumPy version is old, you can try upgrading it for example with `pip install numpy --upgrade`.
  - Add additional version pins to the failing package to help `pip` resolve compatible versions of NumPy and the package.

## Segfaults or crashes

NumPy tries to use advanced CPU features (SIMD) to speed up operations. If you are getting an "illegal instruction" error or a segfault, one cause could be that the environment claims it can support one or more of these features but actually cannot. This can happen inside a docker image or a VM (qemu, VMWare, ...)

You can use the output of `np.show_runtime()` to show which SIMD features are detected. For instance:

    >>> np.show_runtime()
    WARNING: `threadpoolctl` not found in system! Install it by `pip install \
    threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed
    build information
    [{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                          'found': ['SSSE3',
                                    'SSE41',
                                    'POPCNT',
                                    'SSE42',
                                    'AVX',
                                    'F16C',
                                    'FMA3',
                                    'AVX2'],
                          'not_found': ['AVX512F',
                                        'AVX512CD',
                                        'AVX512_KNL',
                                        'AVX512_KNM',
                                        'AVX512_SKX',
                                        'AVX512_CLX',
                                        'AVX512_CNL',
                                        'AVX512_ICL']}}]

In this case, it shows AVX2 and FMA3 under the `found` section, so you can try disabling them by setting `NPY_DISABLE_CPU_FEATURES="AVX2,FMA3"` in your environment before running python (for cmd.exe on windows):

    >SET NPY_DISABLE_CPU_FEATURES="AVX2,FMA3"
    >python <myprogram.py>

By installing threadpoolctl `np.show_runtime()` will show additional information:

    ...
    {'architecture': 'Zen',
      'filepath': '/tmp/venv3/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
      'internal_api': 'openblas',
      'num_threads': 24,
      'prefix': 'libopenblas',
      'threading_layer': 'pthreads',
      'user_api': 'blas',
      'version': '0.3.21'}]

If you use the wheel from PyPI, it contains code from the OpenBLAS project to speed up matrix operations. This code too can try to use SIMD instructions. It has a different mechanism for choosing which to use, based on a CPU architecture, You can override this architecture by setting `OPENBLAS_CORETYPE`: a minimal value for `x86_64` is `OPENBLAS_CORETYPE=Haswell`. This too needs to be set before running your python (this time for posix):

    $ OPENBLAS_CORETYPE=Haswell python <myprogram.py>

---

whatisnumpy.md

---

# What is NumPy?

NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.

At the core of the NumPy package, is the <span class="title-ref">\~numpy.ndarray</span> object. This encapsulates *n*-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance. There are several important differences between NumPy arrays and the standard Python sequences:

  - NumPy arrays have a fixed size at creation, unlike Python lists (which can grow dynamically). Changing the size of an <span class="title-ref">\~numpy.ndarray</span> will create a new array and delete the original.
  - The elements in a NumPy array are all required to be of the same data type, and thus will be the same size in memory. The exception: one can have arrays of (Python, including NumPy) objects, thereby allowing for arrays of different sized elements.
  - NumPy arrays facilitate advanced mathematical and other types of operations on large numbers of data. Typically, such operations are executed more efficiently and with less code than is possible using Python's built-in sequences.
  - A growing plethora of scientific and mathematical Python-based packages are using NumPy arrays; though these typically support Python-sequence input, they convert such input to NumPy arrays prior to processing, and they often output NumPy arrays. In other words, in order to efficiently use much (perhaps even most) of today's scientific/mathematical Python-based software, just knowing how to use Python's built-in sequence types is insufficient - one also needs to know how to use NumPy arrays.

The points about sequence size and speed are particularly important in scientific computing. As a simple example, consider the case of multiplying each element in a 1-D sequence with the corresponding element in another sequence of the same length. If the data are stored in two Python lists, `a` and `b`, we could iterate over each element:

    c = []
    for i in range(len(a)):
        c.append(a[i]*b[i])

This produces the correct answer, but if `a` and `b` each contain millions of numbers, we will pay the price for the inefficiencies of looping in Python. We could accomplish the same task much more quickly in C by writing (for clarity we neglect variable declarations and initializations, memory allocation, etc.)

    for (i = 0; i < rows; i++) {
      c[i] = a[i]*b[i];
    }

This saves all the overhead involved in interpreting the Python code and manipulating Python objects, but at the expense of the benefits gained from coding in Python. Furthermore, the coding work required increases with the dimensionality of our data. In the case of a 2-D array, for example, the C code (abridged as before) expands to

    for (i = 0; i < rows; i++) {
      for (j = 0; j < columns; j++) {
        c[i][j] = a[i][j]*b[i][j];
      }
    }

NumPy gives us the best of both worlds: element-by-element operations are the "default mode" when an <span class="title-ref">\~numpy.ndarray</span> is involved, but the element-by-element operation is speedily executed by pre-compiled C code. In NumPy

    c = a * b

does what the earlier examples do, at near-C speeds, but with the code simplicity we expect from something based on Python. Indeed, the NumPy idiom is even simpler\! This last example illustrates two of NumPy's features which are the basis of much of its power: vectorization and broadcasting.

## Why is NumPy fast?

Vectorization describes the absence of any explicit looping, indexing, etc., in the code - these things are taking place, of course, just "behind the scenes" in optimized, pre-compiled C code. Vectorized code has many advantages, among which are:

  - vectorized code is more concise and easier to read
  - fewer lines of code generally means fewer bugs
  - the code more closely resembles standard mathematical notation (making it easier, typically, to correctly code mathematical constructs)
  - vectorization results in more "Pythonic" code. Without vectorization, our code would be littered with inefficient and difficult to read `for` loops.

Broadcasting is the term used to describe the implicit element-by-element behavior of operations; generally speaking, in NumPy all operations, not just arithmetic operations, but logical, bit-wise, functional, etc., behave in this implicit element-by-element fashion, i.e., they broadcast. Moreover, in the example above, `a` and `b` could be multidimensional arrays of the same shape, or a scalar and an array, or even two arrays with different shapes, provided that the smaller array is "expandable" to the shape of the larger in such a way that the resulting broadcast is unambiguous. For detailed "rules" of broadcasting see \[Broadcasting \<basics.broadcasting\>\](\#broadcasting-\<basics.broadcasting\>).

## Who else uses NumPy?

NumPy fully supports an object-oriented approach, starting, once again, with <span class="title-ref">\~numpy.ndarray</span>. For example, <span class="title-ref">\~numpy.ndarray</span> is a class, possessing numerous methods and attributes. Many of its methods are mirrored by functions in the outer-most NumPy namespace, allowing the programmer to code in whichever paradigm they prefer. This flexibility has allowed the NumPy array dialect and NumPy <span class="title-ref">\~numpy.ndarray</span> class to become the *de-facto* language of multi-dimensional data interchange used in Python.

---

CONTRIBUTING.rst

---

NumPy\'s Contributing guidelines
================================

Welcome to the NumPy community! We\'re excited to have you here. Whether you\'re new to open source or experienced, your contributions help us grow.

Pull requests (PRs) are always welcome, but making a PR is just the start. Please respond to comments and requests for changes to help move the process forward. Please follow our [Code of Conduct](https://numpy.org/code-of-conduct/), which applies to all interactions, including issues and PRs.

For more, please read <https://www.numpy.org/devdocs/dev/index.html>

Thank you for contributing, and happy coding!


---

INSTALL.rst

---

Building and installing NumPy
=============================

**IMPORTANT**: the below notes are about building NumPy, which for most users is *not* the recommended way to install NumPy. Instead, use either a complete scientific Python distribution (recommended) or a binary installer - see <https://scipy.org/install.html>.

::: {.contents}
:::

Prerequisites
-------------

Building NumPy requires the following installed software:

1)  Python\_\_ 3.10.x or newer.

    Please note that the Python development headers also need to be installed, e.g., on Debian/Ubuntu one needs to install both [python3]{.title-ref} and [python3-dev]{.title-ref}. On Windows and macOS this is normally not an issue.

2)  Cython \>= 3.0.6

3)  pytest\_\_ (optional)

    This is required for testing NumPy, but not for using it.

4)  Hypothesis\_\_ (optional) 5.3.0 or later

    This is required for testing NumPy, but not for using it.

Python\_\_ <https://www.python.org/> pytest\_\_ <https://docs.pytest.org/en/stable/> Hypothesis\_\_ <https://hypothesis.readthedocs.io/en/latest/>

::: {.note}
::: {.title}
Note
:::

If you want to build NumPy in order to work on NumPy itself, use `spin`. For more details, see <https://numpy.org/devdocs/dev/development_environment.html>
:::

::: {.note}
::: {.title}
Note
:::

More extensive information on building NumPy is maintained at <https://numpy.org/devdocs/building/#building-numpy-from-source>
:::

Basic installation
------------------

If this is a clone of the NumPy git repository, then first initialize the `git` submodules:

    git submodule update --init

To install NumPy, run:

    pip install .

This will compile NumPy on all available CPUs and install it into the active environment.

To run the build from the source folder for development purposes, use the `spin` development CLI:

    spin build    # installs in-tree under `build-install/`
    spin ipython  # drop into an interpreter where `import numpy` picks up the local build

Alternatively, use an editable install with:

    pip install -e . --no-build-isolation

See [Requirements for Installing Packages](https://packaging.python.org/tutorials/installing-packages/) for more details.

Choosing compilers
------------------

NumPy needs C and C++ compilers, and for development versions also needs Cython. A Fortran compiler isn\'t needed to build NumPy itself; the `numpy.f2py` tests will be skipped when running the test suite if no Fortran compiler is available.

For more options including selecting compilers, setting custom compiler flags and controlling parallelism, see <https://scipy.github.io/devdocs/building/compilers_and_options.html>

### Windows

On Windows, building from source can be difficult (in particular if you need to build SciPy as well, because that requires a Fortran compiler). Currently, the most robust option is to use MSVC (for NumPy only). If you also need SciPy, you can either use MSVC + Intel Fortran or the Intel compiler suite. Intel itself maintains a good [application note](https://software.intel.com/en-us/articles/numpyscipy-with-intel-mkl) on this.

If you want to use a free compiler toolchain, our current recommendation is to use Docker or Windows subsystem for Linux (WSL). See <https://scipy.github.io/devdocs/dev/contributor/contributor_toc.html#development-environment> for more details.

Building with optimized BLAS support
------------------------------------

Configuring which BLAS/LAPACK is used if you have multiple libraries installed is done via a `--config-settings` CLI flag - if not given, the default choice is OpenBLAS. If your installed library is in a non-standard location, selecting that location is done via a pkg-config `.pc` file. See <https://scipy.github.io/devdocs/building/blas_lapack.html> for more details.

### Windows

The Intel compilers work with Intel MKL, see the application note linked above.

For an overview of the state of BLAS/LAPACK libraries on Windows, see [here](https://mingwpy.github.io/blas_lapack.html).

### macOS

On macOS \>= 13.3, you can use Apple\'s Accelerate library. On older macOS versions, Accelerate has bugs and we recommend using OpenBLAS or (on x86-64) Intel MKL.

### Ubuntu/Debian

For best performance, a development package providing BLAS and CBLAS should be installed. Some of the options available are:

-   `libblas-dev`: reference BLAS (not very optimized)
-   `libatlas-base-dev`: generic tuned ATLAS, it is recommended to tune it to the available hardware, see /usr/share/doc/libatlas3-base/README.Debian for instructions
-   `libopenblas-base`: fast and runtime detected so no tuning required but a very recent version is needed (\>=0.2.15 is recommended). Older versions of OpenBLAS suffered from correctness issues on some CPUs.

The package linked to when numpy is loaded can be chosen after installation via the alternatives mechanism:

    update-alternatives --config libblas.so.3
    update-alternatives --config liblapack.so.3

Or by preloading a specific BLAS library with:

    LD_PRELOAD=/usr/lib/atlas-base/atlas/libblas.so.3 python ...

Build issues
------------

If you run into build issues and need help, the NumPy and SciPy [mailing list](https://scipy.org/scipylib/mailing-lists.html) is the best place to ask. If the issue is clearly a bug in NumPy, please file an issue (or even better, a pull request) at <https://github.com/numpy/numpy>.


---

LICENSE.txt

---

Copyright (c) 2005-2024, NumPy Developers.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
       notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above
       copyright notice, this list of conditions and the following
       disclaimer in the documentation and/or other materials provided
       with the distribution.

    * Neither the name of the NumPy Developers nor the names of any
       contributors may be used to endorse or promote products derived
       from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


---

LICENSES_bundled.txt

---

The NumPy repository and source distributions bundle several libraries that are
compatibly licensed.  We list these here.

Name: lapack-lite
Files: numpy/linalg/lapack_lite/*
License: BSD-3-Clause
  For details, see numpy/linalg/lapack_lite/LICENSE.txt

Name: dragon4
Files: numpy/_core/src/multiarray/dragon4.c
License: MIT
  For license text, see numpy/_core/src/multiarray/dragon4.c

Name: libdivide
Files: numpy/_core/include/numpy/libdivide/*
License: Zlib
  For license text, see numpy/_core/include/numpy/libdivide/LICENSE.txt


Note that the following files are vendored in the repository and sdist but not
installed in built numpy packages:

Name: Meson
Files: vendored-meson/meson/*
License: Apache 2.0
  For license text, see vendored-meson/meson/COPYING

Name: spin
Files: .spin/cmds.py
License: BSD-3
  For license text, see .spin/LICENSE

Name: tempita
Files: numpy/_build_utils/tempita/*
License: MIT
  For details, see numpy/_build_utils/tempita/LICENCE.txt


---

README.md

---

<h1 align="center">
<img src="https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg" width="300">
</h1><br>


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)


NumPy is the fundamental package for scientific computing with Python.

- **Website:** https://www.numpy.org
- **Documentation:** https://numpy.org/doc
- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion
- **Source code:** https://github.com/numpy/numpy
- **Contributing:** https://www.numpy.org/devdocs/dev/index.html
- **Bug reports:** https://github.com/numpy/numpy/issues
- **Report a security vulnerability:** https://tidelift.com/docs/security

It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities

Testing:

NumPy requires `pytest` and `hypothesis`.  Tests can then be run after installation with:

    python -c "import numpy, sys; sys.exit(numpy.test() is False)"

Code of Conduct
----------------------

NumPy is a community-driven open source project developed by a diverse group of
[contributors](https://numpy.org/teams/). The NumPy leadership has made a strong
commitment to creating an open, inclusive, and positive community. Please read the
[NumPy Code of Conduct](https://numpy.org/code-of-conduct/) for guidance on how to interact
with others in a way that makes our community thrive.

Call for Contributions
----------------------

The NumPy project welcomes your expertise and enthusiasm!

Small improvements or fixes are always appreciated. If you are considering larger contributions
to the source code, please contact us through the [mailing
list](https://mail.python.org/mailman/listinfo/numpy-discussion) first.

Writing code isnâ€™t the only way to contribute to NumPy. You can also:
- review pull requests
- help us stay on top of new and old issues
- develop tutorials, presentations, and other educational materials
- maintain and improve [our website](https://github.com/numpy/numpy.org)
- develop graphic design for our brand assets and promotional materials
- translate website content
- help with outreach and onboard new contributors
- write grant proposals and help with other fundraising efforts

For more information about the ways you can contribute to NumPy, visit [our website](https://numpy.org/contribute/). 
If youâ€™re unsure where to start or how your skills fit in, reach out! You can
ask on the mailing list or here, on GitHub, by opening a new issue or leaving a
comment on a relevant issue that is already open.

Our preferred channels of communication are all public, but if youâ€™d like to
speak to us in private first, contact our community coordinators at
numpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com for
an invitation).

We also have a biweekly community call, details of which are announced on the
mailing list. You are very welcome to join.

If you are new to contributing to open source, [this
guide](https://opensource.guide/how-to-contribute/) helps explain why, what,
and how to successfully get involved.


---

THANKS.txt

---

Travis Oliphant for the NumPy core, the NumPy guide, various
    bug-fixes and code contributions.
Paul Dubois, who implemented the original Masked Arrays.
Pearu Peterson for f2py, numpy.distutils and help with code
    organization.
Robert Kern for mtrand, bug fixes, help with distutils, code
    organization, strided tricks and much more.
Eric Jones for planning and code contributions.
Fernando Perez for code snippets, ideas, bugfixes, and testing.
Ed Schofield for matrix.py patches, bugfixes, testing, and docstrings.
Robert Cimrman for array set operations and numpy.distutils help.
John Hunter for code snippets from matplotlib.
Chris Hanley for help with records.py, testing, and bug fixes.
Travis Vaught for administration, community coordination and
    marketing.
Joe Cooper, Jeff Strunk for administration.
Eric Firing for bugfixes.
Arnd Baecker for 64-bit testing.
David Cooke for many code improvements including the auto-generated C-API,
    and optimizations.
Andrew Straw for help with the web-page, documentation, packaging and
    testing.
Alexander Belopolsky (Sasha) for Masked array bug-fixes and tests,
    rank-0 array improvements, scalar math help and other code additions.
Francesc Altet for unicode, work on nested record arrays, and bug-fixes.
Tim Hochberg for getting the build working on MSVC, optimization
    improvements, and code review.
Charles (Chuck) Harris for the sorting code originally written for
    Numarray and for improvements to polyfit, many bug fixes, delving
    into the C code, release management, and documentation.
David Huard for histogram improvements including 2-D and d-D code and
    other bug-fixes.
Stefan van der Walt for numerous bug-fixes, testing and documentation.
Albert Strasheim for documentation, bug-fixes, regression tests and
    Valgrind expertise.
David Cournapeau for build support, doc-and-bug fixes, and code
    contributions including fast_clipping.
Jarrod Millman for release management, community coordination, and code
    clean up.
Chris Burns for work on memory mapped arrays and bug-fixes.
Pauli Virtanen for documentation, bug-fixes, lookfor and the
    documentation editor.
A.M. Archibald for no-copy-reshape code, strided array tricks,
    documentation and bug-fixes.
Pierre Gerard-Marchant for rewriting masked array functionality.
Roberto de Almeida for the buffered array iterator.
Alan McIntyre for updating the NumPy test framework to use nose, improve
    the test coverage, and enhancing the test system documentation.
Joe Harrington for administering the 2008 Documentation Sprint.
Mark Wiebe for the new NumPy iterator, the float16 data type, improved
    low-level data type operations, and other NumPy core improvements.

NumPy is based on the Numeric (Jim Hugunin, Paul Dubois, Konrad
Hinsen, and David Ascher) and NumArray (Perry Greenfield, J Todd
Miller, Rick White and Paul Barrett) projects.  We thank them for
paving the way ahead.

Institutions
------------

Enthought for providing resources and finances for development of NumPy.
UC Berkeley for providing travel money and hosting numerous sprints.
The University of Central Florida for funding the 2008 Documentation Marathon.
The University of Stellenbosch for hosting the buildbot.


---

building_with_meson.md

---

# Building with Meson

_Note: this is for early adopters. It has been tested on Linux and macOS, and
with Python 3.10-3.12. There is one CI job to keep the build stable. This may
have rough edges, please open an issue if you run into a problem._

### Developer build

**Install build tools:** Use one of:

- `mamba env create -f environment.yml && mamba activate numpy-dev`

- `python -m pip install -r requirements/build_requirements.txt`
  *Note: also make sure you have `pkg-config` and the usual system dependencies
  for NumPy*

Then install spin:
- `python -m pip install spin`

**Compile and install:** `spin build`

This builds in the `build/` directory, and installs into the `build-install` directory.

Then run the test suite or a shell via `spin`:
```
spin test
spin ipython
```

Alternatively, to use the package, add it to your `PYTHONPATH`:
```
export PYTHONPATH=${PWD}/build/lib64/python3.10/site-packages  # may vary
pytest --pyargs numpy
```


### pip install

Note that `pip` will use the default build system, which is now Meson.
Commands such as `pip install .` or `pip install --no-build-isolation .`
will work as expected, as does building an sdist or wheel with `python -m build`,
or `pip install -e . --no-build-isolation` for an editable install.
For a more complete developer experience than editable installs, consider using
`spin` instead though (see above).


### Workaround for a hiccup on Fedora

- Fedora does not distribute `openblas.pc`. Install the following file in `~/lib/pkgconfig/openblas.pc`:

```
prefix=/usr
includedir=${prefix}/include
libdir=${prefix}/lib64

Name: openblas
Description: OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version
Version: 0.3.19
Cflags: -I${includedir}/openblas
Libs: -L${libdir} -lopenblas
```

Then build with:

```
spin build -- -Dpkg_config_path=${HOME}/lib/pkgconfig
```
